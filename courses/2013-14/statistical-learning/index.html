<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" 
   "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content=
  "text/html; charset=utf-8" />
  <meta name="KeyWords" content=
  "machine learning, statistics, Korbinian Strimmer" />
  <meta name="Author" content="Korbinian Strimmer" />
  <meta name="Description" content="Course Description" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Statistisches Lernen</title>
  <link rel="stylesheet" type="text/css" href="../../../styles.css" />
</head>

<body>
  <script type="text/javascript">
//<![CDATA[
  <!--  
   if (top.location != self.location) {top.location = self.location}
  //-->
//]]>
</script>

  <div id="page">

    <div id="header">Statistisches Lernen</div>

<div id="main">

 <span class="sidenote">
<i>Modulnr.:</i> 09-INF-BI01 (10 LP) <br /><br />
<b>Vorlesung:</b><br />
<i>Beginn:</i> 22. Oktober 2013 <br />
<i>Zeit:</i> Dienstag 11:00-12:30, Mittwoch 11:00-12:30<br />
<i>Ort:</i> Seminarraum 109, H&auml;rtelstr. 16-18 <br /><br />
<b>R Praktikum:</b><br />
<i>Beginn:</i> 3. Dezember 2013 <br />
<i>Zeit:</i> Dienstag 14:00-19:00<br />
<i>Ort:</i> PC Pool 009 H&auml;rtelstr. 16-18 <br />
    </span>



    <p>Universit&auml;t Leipzig, Wintersemester 2013/14</p>


 <p><strong>Dozenten:</strong></p>

<p>
<i>Vorlesung:</i>
<a href="https://strimmerlab.github.io/korbinian.html">Korbinian Strimmer</a>  und
<a href="http://www.bioinf.uni-leipzig.de/~kristin/">Kristin Reiche</a> <br />
<i>R-Kurs:</i>
<a href="http://www.nowick-lab.info/?page_id=11">Katja Nowick</a> und
<a href="http://www.imise.uni-leipzig.de/Mitarbeiter/Markus.Kreuz.jsp">Markus Kreuz</a><br />

</p>
 
 
   <p><strong>Synopsis:</strong></p>

 
<p>
Ziel des Moduls ist das Erlernen der konzeptionellen Grundlagen des statistischen Lernens ("statistical machine learning") und deren
praktische Anwendung auf bioinformatische Probleme. Die Vorlesung besch&auml;ftigt sich mit
fortgeschrittten statistischen Lernverfahren und Informationstheorie. Das Praktikum vermittelt
praktische Kenntnisse in der Programmierung und Datenanalyse mit der Statistiksoftware <a href="http://r-project.org">R</a>.</p>


 <span class="sidenote">
<b>Vorkenntnisse:</b><br />
Elementarkenntnisse in Statistik sind sehr hilfreich.<br />
Zur Auffrischung empfiehlt sich das Moduls "Grundlagen der Biometrie"  (09-202-4106) von 
<a href="http://www.imise.uni-leipzig.de/Mitarbeiter/Dirk.Hasenclever.jsp"> Dirk Hasenclever</a>
(im WS 2013/14 immer Montag und Donnerstag).
    </span>



<p>Das Modul besteht aus einer 3-st&uuml;ndigen Vorlesung (w&ouml;chentlich) und einem Computer-Praktikum (5 Bl&ouml;cke).  Thematisch werden vor Weihnachten die allgemeinen Grundlagen erarbeitet (sowohl in der VL und im R Kurs).  Nach Weihnachten erfolgt dann die explizite Anwendung auf bioinformatische Problemstellungen.</p>

<p>Am Ende des Semester erfolgt eine <strong>m&uuml;ndlichen Pr&uuml;fung</strong> &uuml;ber den Inhalt der Vorlesung und des R-Praktikums. </p>

<p>
Themen&uuml;berblick:</p>
<ul>
<li>Zufallsvariablen und Wahrscheinlichkeitstheorie</li>
<li>Stochastische Modellierung</li>
<li>Entropy und Information</li>
<li>Maximum likelihood und Bayesianische Inferenz</li>
<li>Clustering und Klassifikation</li>
<li>Resampling Verfahren (Bootstrap und MCMC)</li>
<li>Modellwahl und Hypothesentesten</li>
<li>Hochdimensionale Statistik und Regularisierung</li>
<li>Graphische Modelle</li>
<li>Analyse r&auml;umlich-zeitlich korrellierter Daten</li>
</ul>


    <p><strong>Empfohlene Literatur (Vorlesung):</strong></p>
<ol>
<li>K. P. Murphy. 2012. <a href="http://www.cs.ubc.ca/~murphyk/MLbook/index.html">Machine learning: a probabilistic perspective</a>. MIT Press.<br />
<em>Das zur Zeit aktuellste und sehr umfassende Lehrbuch zu statistischen Verfahren f&uuml;r maschinelles Lernen.</em></li>

<li>T. Hastie, R. Tibshirani, and J. Friedman. 2009. <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/index.html">The elements of statistical learning</a>. 2nd Edition. Springer.<br />
<em>Das Standardlehrbuch zu modernen statistischen Lernverfahren (PDF ist frei verf&uuml;gbar).</em>
</li>


<li> G. James, D. Witten, T. Hastie, and R. Tibshirani. 2013. <a href="http://www-bcf.usc.edu/~gareth/ISL/index.html">An introduction to statistical learning, with Applications in R</a>. Springer.<br />
<em>&Auml;hnlicher Inhalt wie "Elements of Statistical Learning", weniger theoretisch und mit R Code (PDF ist frei verf&uuml;gbar).</em>
</li>

</ol>
    <p><strong>Weitere Literatur (Empfehlung):</strong></p>
<ol>

<li>C. R. Shalizi. 2013. <a href="http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/">Advanced Data Analysis from an Elementary Point of View</a> (freies PDF) </li>


<li>D. J. C. MacKay. 2003. <a href="http://www.inference.phy.cam.ac.uk/mackay/itila/">Information theory, inference, and learning algorithms</a>. Cambridge University Press. (freies PDF) </li>


<li>R. Schutt and C. O'Neil. 2013. <a href="http://columbiadatascience.com/doing-data-science/">Doing data science</a>. O'Reilly. </li>


</ol>


<p><strong>Weitere relevante Links:</strong></p>
<ul>

<li>R Project for Statistical Computing: <a href="http://www.r-project.org">http://www.r-project.org</a> <br />
<em>Eine freie und sehr leistungsf&auml;hige Software f&uuml;r statistische Analysen.</em></li>

<li><a href="http://www.rstudio.com/ide/">RStudio</a> <br />
<em>Platformunabh&auml;ngige graphische Benutzeroberfl&auml;che f&uuml;r R.</em></li>


<li><a href="http://r-users-group.meetup.com/cities/de/leipzig/">Leipzig R statistical computing</a> <br />
<em>Eine Gruppe von enthusiastischen Leipziger R Benutzern.</em></li>

<li><a href="http://www.youtube.com/watch?v=sKiz5smpvE8">The Science of Chance</a> <br />
<em>Sehenswerter BBC Dokumentarfilm &uuml;ber die Entstehung der Statistik als wissenschaftliche Disziplin und den Nutzen in der heutigen Gesellschaft (mit <a href="https://en.wikipedia.org/wiki/David_Spiegelhalter">David Spiegelhalter</a>).</em></li>

</ul>



    <p><strong>Kursplan (R-Kurs):</strong></p>

<p>Der R-Kurs findet an f&uuml;nf Dienstagnachmittagen (14-19 Uhr) ab dem 3. Dezember 2013 statt. 
Genauere Details finden Sie auf <a href="http://www.imise.uni-leipzig.de/Institut/Weiterbildung/R-Kurs/index.jsp">einer eigenen Seite</a>.</p>


   <p><strong>Kursplan (Vorlesung):</strong></p>

<p>Bitte beachten Sie die genauen Wochentage, an denen die VL stattfindet!</p>

<table border="1">
  <tr>
    <th>Woche</th>
    <th>Datum</th>
    <th>Besprochene Konzepte</th>
  </tr>

 <tr>
    <td> </td>
<td> </td>
    <td><strong>Teil I: Vorlesung (Allgemeine Methodik)</strong> 
    
</td>
     </tr>


    <tr>
<td>W1</td>
 <td>-</td>
 <td></td>
  </tr>

    <tr>
<td>W2</td>
 <td>Di 22. Oktober 2013</td>
 <td><strong>Was ist Statistik:</strong> Lernen aus Daten, Entwicklung der Statistik im 20 Jahrhundert. 
<strong>Was ist Wahrscheinlichkeit:</strong> Kolmogorov Axiome, Freqentistische und Bayesianische Interpretation
<strong>Grundbegriffe:</strong> Zufallsvariable, Beobachtungen, Dichtefunktion,
                    Verteilungsfunktion,  
                    Erwartungswert, Varianz,
                    Median, Quantilfunktion,
                     Kovarianz, Korrelation, Identit&auml;ten f&uuml;r Erwartungswert und (Ko)varianz, Unabh&auml;ngigkeit, Variablentransformation, Delta Methode, Jensen
Ungleichung.
<strong>Verteilungen:</strong> <a href="distributions.png">Katalog wichtiger Verteilungen</a>, Normalverteilung, Multivariate Normalverteilung, Beta Verteilung, Exponentialverteilung, Gammaverteilung, Binomialverteilung,
        Poissonverteilung, Lokationsparameter, Skalenparameter,
Varianzstabilisierung.</td>
  </tr>


    <tr>
<td></td>
 <td>Mi 23. Oktober 2013</td>
 <td><strong>Explorative Datenanalyse:</strong> empirische CDF, Histogramm, Box-Plot, Violin Plot, Streu-Plot, qq-Plot, Ausrei&szlig;er 
<strong>Inferenz:</strong> Statistisches Lernen, Probabilistische Modellierung von Daten,
Unsicherheitverteilung Parameter,  Sch&auml;tzfunktion, Eigenschaften von Sch&auml;tzern,
 Bias, MSE, Varianz-Bias Zerlegung, Effizienz, Konsistenz, Stichprobenverteilung.
<strong>  Einfache Sch&auml;tzer:</strong>
empirischer Erwartungswert, empirische Varianz, ECDF und Histogramm als Sch&auml;tzer.
<strong>Computerdemonstration:</strong>
         <ol>
        <li><a href="r-code/varianz.R">Vergleich von Varianzsch&auml;tzern</a> </li>
          </ol>

</td>
  </tr>


    <tr>
<td>W3</td>
 <td>Di 29. Oktober 2013</td>
 <td><strong>Information:</strong> 
Kullback-Leibler Divergenz, Boltzmann Entropie, 
Shannon Entropie, Mutual Information,  Mutual Information zwischen normalverteilten Variablen, Fisher Informationsmatrix.
<strong>Hierarchie Inferenzmethoden:</strong> KL, Maximum likelihood, Kleinste Quadrate, Penalized ML,
Bayes, empirisches Bayes.</td>
  </tr>

    <tr>
<td> </td>
 <td>Mi 30. Oktober 2013</td>
 <td><strong>Likelihood Inferenz:</strong>  Kullback-Leibler Distanz, Approximation bei gro&szlig;en Fallzahlen,
    Maximum-Likelihood, Least-Squares,  Likelihood Funktion,
    Score Funktion, (beobachtete) Fisher Information, Mittelwert als MLE, 
    quadratische Approximation, Likelihood interval, Wald interval, Likelihood ratio,
    Transformationsinvarianz, Optimalit&auml;t f&uuml;r gro&szlig;e Stichproben, Bias.
    Cramer-Rao Ungleichung, Overfitting, Suffizienz.</td>
  </tr>

  <tr>
<td>W4 </td>
 <td>Di 5. November 2013</td>

 <td>
<strong>Frequentistische Fehlerabsch&auml;tzung:</strong> Delta Methode (univariat und multivariat),
    Standardkonfidenzintervalle, Bootstrapverfahren, Bootstrap-Sch&auml;tzer f&uuml;r Varianz und Bias, Bootstrap-Sch&auml;tzer
   f&uuml;r Konfidenzintervall, Bagging, Jacknife, Pr&auml;diktionsfehler, Sch&auml;tzung duch Kreuzvalidierung.

<strong>Computerdemonstration:</strong>
         <ol>
        <li><a href="r-session-1/bootstrap-examples.R">bootstrap-examples.R</a> </li>
          </ol>
</td>
  </tr> 





   <tr>
<td> </td>
 <td>Mi 6. November 2013</td>
 <td><strong>Regularisierung und Shrinkage: </strong>Entscheidungstheorie, Risko, Verlustfunktion,
     Hochdimensionale Inferenz, "small n, large p" Daten (z.B. DNA Chips, Proteomics), 
    Stein-Paradox, 
     James-Stein Sch&auml;tzer, Dominanz, Zul&auml;ssigkeit, Shrinkage, Model Averaging, Bias-Varianz Trade-off,
     Regularisierung,  hierarchische Modelle, empirische Bayes Inferenz, Shrinkage Sch&auml;tzer f&uuml;r Varianz und
     Korrelation. Entscheidungstheorie, Bayes Risko.
<strong>Computerdemonstration:</strong> 
   <ol>
  <li><a href="r-session-2/stein.R">stein.R</a> </li> 
  <li><a href="r-session-2/shrinkage-covariance.R">shrinkage-covariance.R</a> </li> 
 <li>Beispieldaten: <a href="r-session-2/smalldata.txt">smalldata.txt</a>, 
                    <a href="r-session-2/largedata.txt">largedata.txt</a>.</li> 
</ol>
</td>
  </tr>


  

   <tr>
<td>W5</td>
 <td>Di 12. November 2013</td>

 <td>
<strong>Bayesianische Inferenz:</strong> 
    Bayes' Theorem, A Priori Verteilung, A Posteriori Verteilung, Kredibilit&auml;tsintervall,
    Bayesian Learning, Zusammenhang mit Shrinkage (Linearit&auml;t in Exponentialfamilie, Regularisierung),  Wahl der Priori, Kompatibilit&auml;t Priori VT und Likelihood,
Jeffreys prior, Referenz Prior, Posteriori Matching Priors. Maximum Entropie Prior.

</td>
  </tr>

<tr>
<td> </td>
 <td>Mi 13. November 2013</td>

 <td>
<strong>Sampling:</strong>
    Rejection Sampling, Importance Sampling, Markov Chain Monte Carlo (MCMC), Metropolis Algorithmus, Metropolis Hastings, Gibbs Sampling, Reversible Jump MCMC, Hamiltonian MCMC.  Approximations.

<strong>Computerdemonstration:</strong> 
         Alle Beispiele benutzen <a href="http://www.r-project.org">R</a>:
         <ol>
        <li><a href="r-session-1/monte-carlo-pi.R">monte-carlo-pi.R</a> </li>
        <li><a href="r-session-1/monte-carlo-integral.R">monte-carlo-integral.R</a> </li>
         <li><a href="r-session-1/mcmc-examples.R">mcmc-examples.R</a> </li>
         </ol>
</td>
  </tr>

<tr>

<td>W6</td>
 <td>Di 19 November 2013</td>

 <td>
<strong>Statistisches Testen und Modellwahl:</strong> Nullmodell, Alternativverteilung, Mischmodell, 
Wahl des Schwellenwertes, Fisher's p-Werte (nur Nullmodell), Bayesianische Entscheidungsregel 
(Nullmodell plus Alternative), Sensitivit&auml;t, Spezifizit&auml;t, Power, Recall, 
False Discovery Rate, False Nondiscovery Rate, True Discovery Rate, Precision,
multiples Testen.</td>
  </tr>




   <tr>
<td>W7</td>
 <td>Di 26. November 2013</td>
 <td>
<strong>Klassifikationsverfahren:</strong> Pr&auml;diktionsproblem, Mischmodell, Diskriminanzfunktion, Entscheidungsgrenzen, Zentroide, gemeinsame oder getrennte Kovarianzmatrizen,
     Quadratische Diskriminanzanalyse
    (QDA), Lineare Diskriminanzanalyse (LDA), Diagonale Diskriminanzanalyse (DDA), weitere Verfahren (SVM, Naive Bayes,
     logistische Regression), Variablenselection, LDA f&uuml;r zwei Klassen, t-Statistik.
      Regularisierte Diskriminanzanalyse, PAM (Tibshirani), RDA (Hastie).

<strong>Computerdemonstration:</strong> 
         Alle Beispiele benutzen <a href="http://www.r-project.org">R</a>:
         <ol>
        <li><a href="r-session-1/classification-non-nested.R">classification-non-nested.R</a> </li> 
        <li><a href="r-session-1/classification-nested-groups.R">classification-nested-groups.R</a> </li>
         </ol>

</td>
  </tr>

   <tr>
<td>W8</td>
 <td>Di 3. Dezember 2013</td>
 <td>
<strong>Regression:</strong> Lineares Modell, Prediktoren, Response,
     Regressionskoeffizienten, Residual, RSS, Normalengleichung, 
     Least-Squares Sch&auml;tzer, ML Sch&auml;tzer, 
     Zusammenhang Regressionskoeffizient und partieller Korrelation 
     und partieller Varianz, generalisierte lineares Modell (GLM), 
     Link Funktion, Exponentialfamilie, logistische Regression und Logit Link,
     generalisiertes additives Modell (GAM),  Ridge Regression, Lasso Regression,
     L1 und L2 Penalisierung,  Dantzig Selector, lasso und LARS, Elastic Net, Variablenselektion.
</td>
  </tr>



  <tr>
<td>W9</td>
 <td>Di 10. Dezember 2013</td>
 <td><strong>Graphische Modelle:</strong> Korrelation und partielle Korrelation,
  Gaussianische Graphische Modelle, Bayesianische Netzwerke, Kettengraphen.
  Inferenz von graphischen Modellen. Kausalit&auml;t.
</td>
  </tr>


 <tr>
<td>W10</td>
 <td>Di 17. Dezember 2013 <br /> <strong>entf&auml;llt!</strong></td>
 <td>
<strong>Zeitreihenanalyse:</strong>
    Zeitreihe, longitudinale Daten, Trend, Autocovarianz, Autokorrelation,
    Stationarit&auml;t, Variogramm, Korrelogramm, Periodogramm, Spektrum, Sch&auml;tzung
    der Autocorrelation, AR Modell, VAR Modell, State-Space Modell, ARMA, GARCH. 
<strong>R&auml;umliche Statistik:</strong>
     R&auml;umliche Daten, r&auml;umliches Modellieren, r&auml;umliche Kovarianzfunktion,
     geostatistische Modell, Stationarit&auml;t, Istropie, Gauss-Modell, r&auml;umliches GLM,
     r&auml;umliches Variogramm, Matern Kovarianzfunktion, r&auml;umliche Pr&auml;diktion,
     Kriging.<br />
 
     <strong>Computerdemonstration:</strong> 
    <ol>
    <li><a href="r-session-3/geoR-examples.R">geoR-examples.R</a> </li> 
    </ol>
</td>
  </tr>


 <tr>
    <td> </td>
<td> </td>
    <td><strong>Teil II: Vorlesung (Bioinformatische Anwendung)</strong> 
    
</td>
    
   </tr>




   <tr>
<td>W11</td>
    <td>Mi 8. Januar 2014</td>
    <td>Statistische Analyse von Expressionsdaten </td>
    </tr>

   <tr>
<td>W12</td>
    <td>Mi 15. Januar 2014</td>
    <td>Statistische Analyse von Sequenzdaten</td>
    </tr>

   <tr>
<td>W13</td>
    <td>Di 21. Januar 2014</td>
    <td>Wiederholung und Terminvergabe Pr&uuml;fung</td>
    </tr>


   <tr>
<td>W14</td>
    <td></td>
    <td></td>
    </tr>

 <tr>
<td>W15</td>
    <td>Mo 3.2 - Mi 5.2 (nach Vereinbarung)</td>
    <td>
    <strong>M&uuml;ndliche Pr&uuml;fungen.</strong>
   </td>
    
   </tr>







</table>  

</div>

<div id="footer">
<em>Letzte &Auml;nderung:</em>1. Oktober 2013<br />
        <a href="http://validator.w3.org/check?uri=referer">
           <img src="../../../images/valid-xhtml11.png" alt="Valid XHTML 1.1"
           height="31" width="88" /></a>
</div>

</div>
  
</body>
</html>
