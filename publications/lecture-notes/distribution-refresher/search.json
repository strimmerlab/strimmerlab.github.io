[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"notes intended provide quick refresher commonly used univariate multivariate distributions.notes offered supporting information along lecture notes statistical modules teaching Department Mathematics University Manchester.includes current modules:MATH27720 Statistics 2: Likelihood Bayes andMATH38161 Multivariate Statistics,well retired module (offered ):MATH20802 Statistical Methods.Distribution Refresher notes written Korbinian Strimmer 2018–2023. version 12 December 2023.notes updated time time. view current\nversion visit \nonline version Distribution Refresher notes.may also wish download Distribution Refresher notes PDF A4 format printing (double page layout) 6x9 inch PDF use tablets (single page layout).","code":""},{"path":"index.html","id":"about-the-author","chapter":"Welcome","heading":"About the author","text":"Hello! name Korbinian Strimmer Professor Statistics.\nmember Statistics group Department Mathematics\nUniversity Manchester. can find information home page.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"notes licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.","code":""},{"path":"univariate-distributions.html","id":"univariate-distributions","chapter":"1 Univariate distributions","heading":"1 Univariate distributions","text":"","code":""},{"path":"univariate-distributions.html","id":"bernoulli-distribution","chapter":"1 Univariate distributions","heading":"1.1 Bernoulli distribution","text":"Bernoulli distribution \\(\\text{Ber}(\\theta)\\) simplest distribution possible.\nnamed Jacob Bernoulli (1655-1705)\nalso discovered law large numbers.describes discrete binary random variable\ntwo states \\(x=0\\) (“failure”) \\(x=1\\) (“success”),\nparameter \\(\\theta \\[0,1]\\) probability “success”.\nOften Bernoulli distribution also referred “coin tossing” model \ntwo outcomes “heads” “tails”.Correspondingly, probability mass function \\(\\text{Ber}(\\theta)\\) \n\\[\np(x=0) = \\text{Pr}(\\text{\"failure\"}) = 1-\\theta  \n\\]\n\n\\[\np(x=1) = \\text{Pr}(\\text{\"success\"}) = \\theta\n\\]\ncompact way write PMF Bernoulli distribution \n\\[\np(x | \\theta ) = \\theta^{x} (1-\\theta)^{1-x}\n\\]\nlog PMF \n\\[\n\\log p(x | \\theta ) = x \\log \\theta + (1-x)\\log (1-\\theta)\n\\]random variable \\(x\\) follows Bernoulli distribution \nwrite\n\\[\nx \\sim \\text{Ber}(\\theta) \\,.\n\\]\nexpected value \\(\\text{E}(x) = \\theta\\) variance \\(\\text{Var}(x) = \\theta (1 - \\theta)\\).","code":""},{"path":"univariate-distributions.html","id":"binomial-distribution","chapter":"1 Univariate distributions","heading":"1.2 Binomial distribution","text":"Closely related Bernoulli distribution binomial distribution\n\\(\\text{Bin}(n, \\theta)\\) results repeating \nBernoulli experiment \\(n\\) times counting number successes among\n\\(n\\) trials (without keeping track ordering experiments).\nThus, \\(x_1, \\ldots, x_n\\) \\(n\\) independent \\(\\text{Ber}(\\theta)\\) random variables\n\\(y = \\sum_{=1}^n x_i\\) distributed \\(\\text{Bin}(n, \\theta)\\).probability mass function :\n\\[\np(y | n, \\theta) = \\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}\n\\]\n\\(y \\\\{ 0, 1, 2, \\ldots, n\\}\\).\nbinomial coefficient \\(\\binom{n}{x}\\) needed account multiplicity\nways (orderings samples) can observe \\(y\\) sucesses.expected value \\(\\text{E}(y) = n \\theta\\) variance \\(\\text{Var}(y) = n \\theta (1 - \\theta)\\).random variable \\(y\\) follows binomial distribution \nwrite\n\\[\ny \\sim \\text{Bin}(n, \\theta)\\,\n\\]\n\\(n=1\\) reduces Bernoulli distribution \\(\\text{Ber}(\\theta)\\).R PMF binomial distribution called dbinom(). binomial coefficient computed choose().","code":""},{"path":"univariate-distributions.html","id":"normal-distribution","chapter":"1 Univariate distributions","heading":"1.3 Normal distribution","text":"normal distribution important continuous probability distribution.\nalso called Gaussian distribution named Carl Friedrich Gauss (1777–1855).univariate normal distribution \\(N(\\mu, \\sigma^2)\\) two parameters \\(\\mu\\) (location) \\(\\sigma^2\\) (scale):\\[\nx \\sim N(\\mu,\\sigma^2)\n\\]\nmean\n\\[\n\\text{E}(x)=\\mu\n\\]\nvariance\n\\[\n\\text{Var}(x) = \\sigma^2\n\\]Probability density function (PDF):\n\\[\np(x| \\mu, \\sigma^2)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]R density function called dnorm().standard normal distribution \\(N(0, 1)\\) mean 0 variance 1.Plot PDF standard normal:cumulative distribution function (CDF) standard normal \\(N(0,1)\\)\n\n\\[\n\\Phi (x ) = \\int_{-\\infty}^{x} p(x'| \\mu=0, \\sigma^2=1) dx'\n\\]\nanalytic expression \\(\\Phi(x)\\). R function called pnorm().Plot CDF standard normal:inverse \\(\\Phi^{-1}(p)\\) called quantile function standard normal.\nR function called qnorm().","code":""},{"path":"univariate-distributions.html","id":"gamma-distribution-aka-wishart-and-scaled-chi-squared-distribution-and-special-cases-chi-squared-and-exponential-distribution","chapter":"1 Univariate distributions","heading":"1.4 Gamma distribution (aka Wishart and scaled chi-squared distribution) and special cases (chi-squared and exponential distribution)","text":"gamma distribution widely used statistics, appears various\nparameterisations different names, may confusing times.","code":""},{"path":"univariate-distributions.html","id":"standard-parameterisation","chapter":"1 Univariate distributions","heading":"1.4.1 Standard parameterisation","text":"Another important continous distribution gamma distribution\n\\(\\text{Gam}(\\alpha, \\theta)\\).\ntwo parameters \\(\\alpha>0\\) (shape) \\(\\theta>0\\) (scale):\n\\[\nx \\sim\\text{Gam}(\\alpha, \\theta)\n\\]\nmean\n\\[\\text{E}(x)=\\alpha \\theta\\]\nvariance\n\\[\\text{Var}(x) = \\alpha \\theta^2\\]gamma distribution also often used rate\nparameter \\(\\beta=1/\\theta\\) (one needs pay attention parameterisation used).Probability density function (PDF):\n\\[\np(x| \\alpha, \\theta)=\\frac{1}{\\Gamma(\\alpha) \\theta^{\\alpha} } x^{\\alpha-1} e^{-x/\\theta}\n\\]\ndensity gamma distribution available R function dgamma(). cumulative density function pgamma() quantile function qgamma().","code":""},{"path":"univariate-distributions.html","id":"wishart-parameterisation-and-scaled-chi-squared-distribution","chapter":"1 Univariate distributions","heading":"1.4.2 Wishart parameterisation and scaled chi-squared distribution","text":"gamma distribution often used different set parameters\n\\(k=2 \\alpha\\) \\(s^2 =\\theta/2\\) (hence conversely \\(\\alpha = k/2\\) \\(\\theta=2 s^2\\)).\nform known one-dimensional Wishart distribution\n\\[\nW_1\\left(s^2, k \\right)\n\\]\nnamed John Wishart (1898–1954).\nWishart parameterisation mean \n\\[\n\\text{E}(x) = k s^2\n\\]\nvariance\n\\[\n\\text{Var}(x) = 2 k s^4\n\\]Another name one-dimensional Wishart distribution exactly \nparameterisation scaled chi-squared distribution denoted \n\\[\ns^2 \\text{$\\chi^2_{k}$}\n\\]Finally, note often employ Wishart distribution mean parameterisation\n\\(W_1\\left(s^2= \\mu / k, k \\right)\\)\n\\(\\mu = k s^2\\) \\(k\\) (thus \\(\\theta = 2 \\mu /k\\)). \nmean\n\\[\n\\text{E}(x) = \\mu\n\\]\nvariance\n\\[\n\\text{Var}(x) = \\frac{2 \\mu^2}{k}\n\\]","code":""},{"path":"univariate-distributions.html","id":"construction-as-sum-of-squared-normals","chapter":"1 Univariate distributions","heading":"1.4.3 Construction as sum of squared normals","text":"gamma distributed variable can constructed follows.\nAssume \\(k\\) independent normal random variables mean 0\nvariance \\(s^2\\):\n\\[z_1,z_2,\\dots,z_k\\sim N(0,s^2)\\]\nsum squares\n\\[\nx = \\sum_{=1}^{k} z_i^2\n\\]\nfollows\n\\[\nx \\sim \\sigma^2 \\text{$\\chi^2_{k}$} =  W_1\\left( s^2, k \\right)\n\\]\nequivalently\n\\[\nx \\sim \\text{Gam}\\left(\\alpha=\\frac{k}{2}, \\theta = 2 s^2\\right)\n\\]","code":""},{"path":"univariate-distributions.html","id":"special-cases-of-the-gamma-distribution-chi-squared-and-exponential-distribution","chapter":"1 Univariate distributions","heading":"1.4.4 Special cases of the gamma distribution (chi-squared and exponential distribution)","text":"","code":""},{"path":"univariate-distributions.html","id":"chi-squared-distribution","chapter":"1 Univariate distributions","heading":"1.4.4.1 Chi-squared distribution","text":"chi-squared distribution\n\\(\\text{$\\chi^2_{k}$}\\) special one-parameter restriction \ngamma resp. Wishart distribution obtained setting\n\\(s^2=1\\) , equivalently, \\(\\theta = 2\\) \\(\\mu = k\\).mean \\(\\text{E}(x)=k\\) variance \\(\\text{Var}(x)=2k\\). chi-squared distribution \\(\\text{$\\chi^2_{k}$}\\) equals \\(\\text{Gam}(\\alpha=k/2, \\theta=2) = W_1\\left(1, k \\right)\\).plot density chi-squared distribution\ndegrees freedom \\(k=1\\) \\(k=3\\):R density chi-squared distribution given dchisq(). cumulative density function pchisq() \nquantile function qchisq().","code":""},{"path":"univariate-distributions.html","id":"exponential-distribution","chapter":"1 Univariate distributions","heading":"1.4.4.2 Exponential distribution","text":"exponential distribution \\(\\text{Exp}(\\theta)\\) scale parameter \\(\\theta\\)\nanother special one-parameter restriction gamma distribution shape parameter set \n\\(\\alpha=1\\) (equivalently \\(k=2\\)).thus equals\n\\(\\text{Gam}(\\alpha=1, \\theta) = W_1(s^2=\\theta/2, k=2)\\). mean \\(\\theta\\) variance \\(\\theta^2\\).Just like gamma distribution exponential distribution also often specified using rate parameter \\(\\beta= 1/\\theta\\) instead scale parameter \\(\\theta\\).R command dexp() returns \ndensity exponential distribution,\npexp() corresponding cumulative density function qexp() quantile function.","code":""},{"path":"univariate-distributions.html","id":"location-scale-t-distribution-and-special-cases-students-t-and-cauchy-distribution","chapter":"1 Univariate distributions","heading":"1.5 Location-scale \\(t\\)-distribution and special cases (Student’s \\(t\\) and Cauchy distribution)","text":"","code":""},{"path":"univariate-distributions.html","id":"location-scale-t-distribution","chapter":"1 Univariate distributions","heading":"1.5.1 Location-scale \\(t\\)-distribution","text":"location-scale \\(t\\)-distribution \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\) generalisation normal distribution.\nadditional parameter \\(\\nu > 0\\) (degrees freedom) controls probability mass tails. small values \\(\\nu\\) distribution heavy-tailed — indeed heavy \\(\\nu \\leq 1\\) even mean defined\n\\(\\nu \\leq 2\\) variance undefined.probability density \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\) \n\\[\np(x | \\mu, \\tau^2, \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {\\sqrt{\\pi \\nu \\tau^2}  \\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{(x-\\mu)^2}{\\nu \\tau^2} \\right)^{-(\\nu+1)/2}\n\\]\nmean (\\(\\nu>1\\))\n\\[\n\\text{E}(x) = \\mu\n\\]\nvariance (\\(\\nu>2\\))\n\\[\n\\text{Var}(x) = \\tau^2 \\frac{\\nu}{\\nu-2}\n\\]\\(\\nu \\rightarrow \\infty\\) location-scale \\(t\\)-distribution \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\) becomes\nnormal distribution \\(N(\\mu, \\tau^2)\\).R extraDistr package command dlst() returns \ndensity location-scale \\(t\\)-distribution,\nplst() corresponding cumulative density function qlst() quantile function.","code":""},{"path":"univariate-distributions.html","id":"students-t-distribution","chapter":"1 Univariate distributions","heading":"1.5.2 Student’s \\(t\\)-distribution","text":"\\(\\mu=0\\) \\(\\tau^2=1\\) location-scale \\(t\\)-distribution becomes \nStudent’s \\(t\\)-distribution \\(t_\\nu\\)\nmean 0 (\\(\\nu>1\\)) variance \\(\\frac{\\nu}{\\nu-2}\\) (\\(\\nu>2\\)).can thus viewed generalisation standard normal distribution \\(N(0,1)\\).\\(y \\sim t_\\nu\\) \\(x = \\mu + \\tau y\\) distributed \\(x \\sim \\text{lst}(\\mu, \\tau^2, \\nu)\\).\\(\\nu \\rightarrow \\infty\\) \\(t\\)-distribution becomes equal \\(N(0,1)\\).R command dt() returns \ndensity \\(t\\)-distribution,\npt() corresponding cumulative density function qt() quantile function.","code":""},{"path":"univariate-distributions.html","id":"cauchy-and-standard-cauchy-distribution","chapter":"1 Univariate distributions","heading":"1.5.3 Cauchy and standard Cauchy distribution","text":"\\(\\nu=1\\) location-scale \\(t\\)-distribution becomes Cauchy distribution \\(\\text{Cau}(\\mu, \\tau)\\)\ndensity \\(p(x| \\mu, \\tau) = \\frac{\\tau}{\\pi (\\tau^2+(x-\\mu)^2)}\\).\\(\\nu=1\\) \\(t\\)-distribution becomes standard Cauchy distribution\n\\(\\text{Cau}(0, 1)\\) density \\(p(x) = \\frac{1}{\\pi (1+x^2)}\\).","code":""},{"path":"multivariate-distributions.html","id":"multivariate-distributions","chapter":"2 Multivariate distributions","heading":"2 Multivariate distributions","text":"","code":""},{"path":"multivariate-distributions.html","id":"categorical-distribution","chapter":"2 Multivariate distributions","heading":"2.1 Categorical distribution","text":"categorical distribution generalisation Bernoulli distribution \ntwo classes \\(K\\) classes.categorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\) describes\ndiscrete random variable \\(K\\) states (“categories”, “classes”, “bins”) \nparameter vector\n\\(\\boldsymbol \\pi= (\\pi_1, \\ldots, \\pi_K)^T\\) specifies\nprobability class \n\\(\\text{Pr}(\\text{\"class k\"}) = \\pi_k\\).\nparameters satisfy \\(\\pi_k \\[0,1]\\) \n\\(\\sum_{k=1}^K \\pi_k = 1\\), hence \\(K-1\\) independent parameters categorical distribution (\\(K\\)).two main ways numerically represent “class k”:“integer encoding”, .e. corresponding integer \\(k\\).“one hot encoding”, .e. indicator vector\n\\(\\boldsymbol x= (x_1, \\ldots, x_K)^T = (0, 0, \\ldots, 1, \\ldots, 0)^T\\) containing zeros everywhere except element \\(x_k=1\\) position \\(k\\). Thus \\(x_k \\\\{ 0, 1\\}\\) \\(\\sum_{k=1}^K x_k = 1\\).following use “one hot encoding”. Therefore sampling categorical distribution parameters \\(\\boldsymbol \\pi\\)\n\\[\n\\boldsymbol x\\sim \\text{Cat}(\\boldsymbol \\pi)\n\\]\nyields random index vector \\(\\boldsymbol x\\).corresponding probability mass function (PMF)\ncan written conveniently terms \\(x_k\\) \n\\[\np(\\boldsymbol x| \\boldsymbol \\pi) = \\prod_{k=1}^K \\pi_k^{x_k} =\n\\begin{cases}\n   \\pi_k  & \\text{} x_k = 1 \\\\\n\\end{cases}\n\\]\nlog PMF \n\\[\n\\log p(\\boldsymbol x| \\boldsymbol \\pi) = \\sum_{k=1}^K x_k \\log \\pi_k   =\n\\begin{cases}\n   \\log \\pi_k  & \\text{} x_k = 1 \\\\\n\\end{cases}\n\\]order explicit categorical distribution \\(K-1\\) \\(K\\) parameters\nrewrite log-density \n\\(\\pi_K = 1 - \\sum_{k=1}^{K-1} \\pi_k\\) \\(x_K = 1 - \\sum_{k=1}^{K-1} x_k\\) \n\\[\n\\begin{split}\n\\log p(\\boldsymbol x| \\boldsymbol \\pi) & =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + x_K \\log \\pi_K \\\\\n& =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_k  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\\\\n\\end{split}\n\\]\nNote particular reason choose \\(\\pi_K\\) \ndependent probabilities classes,\nplace \\(\\pi_k\\) may selected.\\(K=2\\) categorical distribution reduces Bernoulli \\(\\text{Ber}(\\theta)\\) distribution,\n\\(\\pi_1=\\theta\\) \\(\\pi_2=1-\\theta\\).expected value \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\pi\\), component notation\n\\(\\text{E}(x_k) = \\pi_k\\).\ncovariance matrix \\(\\text{Var}(\\boldsymbol x) = \\text{Diag}(\\boldsymbol \\pi) - \\boldsymbol \\pi\\boldsymbol \\pi^T\\), \ncomponent notation \\(\\text{Var}(x_i) = \\pi_i (1-\\pi_i)\\) \\(\\text{Cov}(x_i, x_j) = -\\pi_i \\pi_j\\).form categorical covariance matrix follows directly definition \nvariance \\(\\text{Var}(\\boldsymbol x) = \\text{E}( \\boldsymbol x\\boldsymbol x^T) - \\text{E}( \\boldsymbol x) \\text{E}( \\boldsymbol x)^T\\)\nnoting \\(x_i^2 = x_i\\) \\(x_i x_j = 0\\) \\(\\neq j\\).\nFurthermore, categorical covariance matrix singular construction, \\(K\\) random variables\n\\(x_1, \\ldots, x_K\\) dependent constraint \\(\\sum_{k=1}^K x_k = 1\\).","code":""},{"path":"multivariate-distributions.html","id":"multinomial-distribution","chapter":"2 Multivariate distributions","heading":"2.2 Multinomial distribution","text":"multinomial distribution \\(\\text{Mult}(n, \\boldsymbol \\pi)\\) arises repeated categorical sampling,\nfashion binomial distribution arises repeated Bernoulli sampling. Thus, \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) \\(n\\) independent \\(\\text{Cat}(\\boldsymbol \\pi)\\) random categorical variables\n\\(\\boldsymbol y= \\sum_{=1}^n \\boldsymbol x_i\\) distributed \\(\\text{Mult}(n, \\boldsymbol \\pi)\\).corresponding PMF describes probability pattern \\(y_1, \\ldots, y_K\\) \nsamples distributed across \\(K\\) classes (\\(n= \\sum_{k=1}^K y_k\\)):\n\\[\np(\\boldsymbol y| n, \\theta) = \\binom{n}{y_1, \\ldots, y_n} \\prod_{k=1}^K \\pi_k^{y_k}\n\\]\n\\(\\binom{n}{y_1, \\ldots, y_n}\\) multinomial coefficient.expected value \\(\\text{E}(\\boldsymbol y) = n \\boldsymbol \\pi\\), component notation\n\\(\\text{E}(y_k) = n \\pi_k\\).\ncovariance matrix \\(\\text{Var}(\\boldsymbol y) = n \\text{Diag}(\\boldsymbol \\pi) - n \\boldsymbol \\pi\\boldsymbol \\pi^T\\), \ncomponent notation \\(\\text{Var}(x_i) = n \\pi_i (1-\\pi_i)\\) \\(\\text{Cov}(x_i, x_j) = -n \\pi_i \\pi_j\\).","code":""},{"path":"multivariate-distributions.html","id":"multivariate-normal-distribution","chapter":"2 Multivariate distributions","heading":"2.3 Multivariate normal distribution","text":"univariate normal distribution random scalar \\(x\\) generalises multivariate normal distribution random vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) mean \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\) covariance matrix \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\). corresponding density \n\\[\np(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) = (2\\pi)^{-\\frac{d}{2}} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}} \\exp\\left({{-\\frac{1}{2}} \\underbrace{\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1 \\times d} \\underbrace{\\boldsymbol \\Sigma^{-1}}_{d \\times d} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d \\times 1} }_{1 \\times 1 = \\text{scalar!}}}\\right)\n\\]expectation \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\) variance \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\).\\(d=1\\) get \\(\\boldsymbol x=x\\), \\(\\boldsymbol \\mu= \\mu\\) \\(\\boldsymbol \\Sigma= \\sigma^2\\) multivariate normal density reduces univariate normal density.","code":""}]
