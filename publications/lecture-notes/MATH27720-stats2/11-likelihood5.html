<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Likelihood-based confidence interval and likelihood ratio – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./12-likelihood6.html" rel="next">
<link href="./10-likelihood4.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ce57f618d012aa58b7306493b9cf9147.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./11-likelihood5.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropy and likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions for statistical models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Relative entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Principle of maximum entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#likelihood-based-confidence-intervals-and-wilks-statistic" id="toc-likelihood-based-confidence-intervals-and-wilks-statistic" class="nav-link active" data-scroll-target="#likelihood-based-confidence-intervals-and-wilks-statistic"><span class="header-section-number">11.1</span> Likelihood-based confidence intervals and Wilks statistic</a>
  <ul class="collapse">
  <li><a href="#general-idea-and-definition-of-wilks-log-likelihood-ratio-statistic" id="toc-general-idea-and-definition-of-wilks-log-likelihood-ratio-statistic" class="nav-link" data-scroll-target="#general-idea-and-definition-of-wilks-log-likelihood-ratio-statistic">General idea and definition of Wilks log-likelihood ratio statistic</a></li>
  <li><a href="#examples-of-the-wilks-log-likelihood-ratio-statistic" id="toc-examples-of-the-wilks-log-likelihood-ratio-statistic" class="nav-link" data-scroll-target="#examples-of-the-wilks-log-likelihood-ratio-statistic">Examples of the Wilks log-likelihood ratio statistic</a></li>
  <li><a href="#quadratic-approximation-of-the-wilks-statistic" id="toc-quadratic-approximation-of-the-wilks-statistic" class="nav-link" data-scroll-target="#quadratic-approximation-of-the-wilks-statistic">Quadratic approximation of the Wilks statistic</a></li>
  <li><a href="#examples-of-quadratic-approximations" id="toc-examples-of-quadratic-approximations" class="nav-link" data-scroll-target="#examples-of-quadratic-approximations">Examples of quadratic approximations</a></li>
  <li><a href="#distribution-of-the-wilks-statistic" id="toc-distribution-of-the-wilks-statistic" class="nav-link" data-scroll-target="#distribution-of-the-wilks-statistic">Distribution of the Wilks statistic</a></li>
  <li><a href="#cutoff-values-for-the-likelihood-ci" id="toc-cutoff-values-for-the-likelihood-ci" class="nav-link" data-scroll-target="#cutoff-values-for-the-likelihood-ci">Cutoff values for the likelihood CI</a></li>
  <li><a href="#likelihood-ratio-test-lrt-using-wilks-statistic" id="toc-likelihood-ratio-test-lrt-using-wilks-statistic" class="nav-link" data-scroll-target="#likelihood-ratio-test-lrt-using-wilks-statistic">Likelihood ratio test (LRT) using Wilks statistic</a></li>
  <li><a href="#origin-of-likelihood-ratio-statistic" id="toc-origin-of-likelihood-ratio-statistic" class="nav-link" data-scroll-target="#origin-of-likelihood-ratio-statistic">Origin of likelihood ratio statistic</a></li>
  </ul></li>
  <li><a href="#generalised-likelihood-ratio-test-glrt" id="toc-generalised-likelihood-ratio-test-glrt" class="nav-link" data-scroll-target="#generalised-likelihood-ratio-test-glrt"><span class="header-section-number">11.2</span> Generalised likelihood ratio test (GLRT)</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./11-likelihood5.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="likelihood-based-confidence-intervals-and-wilks-statistic" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="likelihood-based-confidence-intervals-and-wilks-statistic"><span class="header-section-number">11.1</span> Likelihood-based confidence intervals and Wilks statistic</h2>
<section id="general-idea-and-definition-of-wilks-log-likelihood-ratio-statistic" class="level3">
<h3 class="anchored" data-anchor-id="general-idea-and-definition-of-wilks-log-likelihood-ratio-statistic">General idea and definition of Wilks log-likelihood ratio statistic</h3>
<div id="fig-likelihoodci" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="b">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-likelihoodci-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/likelihoodci.png" class="img-fluid figure-img" style="width:80.0%" data-fig-pos="b">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-likelihoodci-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.1: Construction of a likelihood-based confidence intervals.
</figcaption>
</figure>
</div>
<p>Instead of relying on the normal resp. quadratic approximation, we can also use the log-likelihood directly to find <strong>likelihood confidence intervals</strong> (<a href="#fig-likelihoodci" class="quarto-xref">Figure&nbsp;<span>11.1</span></a>).</p>
<p>Idea: find all <span class="math inline">\(\boldsymbol \theta_0\)</span> that have a log-likelihood that is almost as good as <span class="math inline">\(ell_n(\hat{\boldsymbol \theta}_{ML})\)</span>. <span class="math display">\[\text{CI}= \{\boldsymbol \theta_0: \ell_n(\hat{\boldsymbol \theta}_{ML}) - \ell_n(\boldsymbol \theta_0 ) \leq \Delta\}\]</span> Here <span class="math inline">\(\Delta\)</span> is our tolerated deviation from the maximum log-likelihood. We will see below how to determine a suitable <span class="math inline">\(\Delta\)</span>.</p>
<p>The above leads naturally to the <strong>Wilks log-likelihood ratio statistic</strong> <span class="math inline">\(W(\boldsymbol \theta_0)\)</span> defined as: <span class="math display">\[
\begin{split}
W(\boldsymbol \theta_0) &amp; = 2 \log \left(\frac{L(\hat{\boldsymbol \theta}_{ML}| D)}{L(\boldsymbol \theta_0| D)}\right) \\
&amp; =2(\ell_n(\hat{\boldsymbol \theta}_{ML})-\ell_n(\boldsymbol \theta_0))\\
\end{split}
\]</span> With its help we can write the likelihood CI as follows: <span class="math display">\[\text{CI}= \{\boldsymbol \theta_0: W(\boldsymbol \theta_0) \leq 2 \Delta\}\]</span></p>
<p>The Wilks statistic is named after <a href="https://en.wikipedia.org/wiki/Samuel_S._Wilks">Samuel S. Wilks (1906–1964)</a>.</p>
<p>Advantages of using a likelihood-based CI:</p>
<ul>
<li>not restricted to be symmetric</li>
<li>enables to construct multivariate CIs for parameter vector easily even in non-normal cases</li>
<li>contains normal CI as special case</li>
</ul>
<div id="exm-likelihoodratio" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.1</strong></span> The likelihood ratio statistic:</p>
<p>As alternative to the Wilks log-likelhood ratio statistic <span class="math inline">\(W(\boldsymbol \theta_0)\)</span> one may also use the <strong>likelihood ratio</strong> statistic <span class="math display">\[
\Lambda(\boldsymbol \theta_0)  = \frac{L(\boldsymbol \theta_0| D)}{L(\hat{\boldsymbol \theta}_{ML}| D)}
\]</span> The two statistics can be transformed into each other using <span class="math display">\[
W(\boldsymbol \theta_0) = -2\log \Lambda(\boldsymbol \theta_0)
\]</span> and <span class="math display">\[\Lambda(\boldsymbol \theta_0) =  e^{ - W(\boldsymbol \theta_0) / 2 }
\]</span> Hence large values of <span class="math inline">\(W(\boldsymbol \theta_0\)</span> correspond to small values of <span class="math inline">\(\Lambda(\boldsymbol \theta_0)\)</span> and the other way round.</p>
<p>In this course we will only use <span class="math inline">\(W(\boldsymbol \theta_0)\)</span> as it is both easier to compute and its sampling distribution is easier to obtain.</p>
</div>
</section>
<section id="examples-of-the-wilks-log-likelihood-ratio-statistic" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-the-wilks-log-likelihood-ratio-statistic">Examples of the Wilks log-likelihood ratio statistic</h3>
<div id="exm-wilksproportion" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.2</strong></span> Wilks statistic for the proportion:</p>
<p>The log-likelihood function for the parameter <span class="math inline">\(\theta\)</span> is (cf. <a href="08-likelihood2.html#exm-mleproportion" class="quarto-xref">Example&nbsp;<span>8.1</span></a>) <span class="math display">\[
\ell_n(\theta) = n ( \bar{x} \log \theta + (1-\bar{x}) \log(1-\theta) )
\]</span> Hence the Wilks statistic is with <span class="math inline">\(\hat{\theta}_{ML}=\bar{x}\)</span> <span class="math display">\[
\begin{split}
W(\theta_0) &amp; = 2 ( \ell_n( \hat{\theta}_{ML})  -\ell_n( \theta_0 ) )\\
&amp; = 2 n \left(  \bar{x} \log \left( \frac{  \bar{x}  }{\theta_0}  \right)  
                + (1-\bar{x}) \log \left( \frac{1-\bar{x} }{1-\theta_0}  \right)  
    \right) \\
\end{split}
\]</span></p>
<p>Comparing with <a href="04-entropy2.html#exm-klbernoulli" class="quarto-xref">Example&nbsp;<span>4.4</span></a> we see that in this case the Wilks statistic is essentially (apart from a scale factor <span class="math inline">\(2n\)</span>) the KL divergence between two Bernoulli distributions: <span class="math display">\[
W(\theta_0) =2 n D_{\text{KL}}( \text{Ber}( \hat{\theta}_{ML} ), \text{Ber}(\theta_0)  )
\]</span></p>
</div>
<div id="exm-wilksnormalmean" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.3</strong></span> Wilks statistic for the mean parameter of a normal model:</p>
<p>The Wilks statistic is <span class="math display">\[
W(\mu_0) = \frac{(\bar{x}-\mu_0)^2}{\sigma^2 / n}
\]</span></p>
<p>See Worksheet L2 for a derivation of the Wilks statistic from the normal log-likelihood function.</p>
<p>Note this is the same as the squared Wald statistic discussed in <a href="10-likelihood4.html#exm-waldnormalmean" class="quarto-xref">Example&nbsp;<span>10.6</span></a>.</p>
<p>Comparing with <a href="04-entropy2.html#exm-klnormalequalvar" class="quarto-xref">Example&nbsp;<span>4.5</span></a> we see that in this case the Wilks statistic is essentially (apart from a scale factor <span class="math inline">\(2n\)</span>) the KL divergence between two normal distributions with different means and variance equal to <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[
W(p_0) =2 n D_{\text{KL}}( N( \hat{\mu}_{ML}, \sigma^2 ), N(\mu_0, \sigma^2)  )
\]</span></p>
</div>
<div id="exm-catwilks" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.4</strong></span> Wilks log-likelihood ratio statistic for the categorical distribution:</p>
<p>The Wilks log-likelihood ratio is <span class="math display">\[
W(\boldsymbol p_0) = 2 \left( \ell_n(\hat{\pi}_1^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  )
           - \ell_n(p_1^{0}, \ldots, p_{K-1}^{0}  )  \right)
\]</span> with <span class="math inline">\(\boldsymbol p_0 = c(p_1^{0}, \ldots, p_{K}^{0} )^T\)</span>. As the probabilities sum up to 1 there are only <span class="math inline">\(K-1\)</span> free parameters.</p>
<p>The log-likelihood at the MLE is <span class="math display">\[
\ell_n(\hat{\pi}_1^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  ) =  n   \sum_{k=1}^{K}  \bar{x}_k \log \hat{\pi}_k^{ML}  =  n   \sum_{k=1}^{K}  \bar{x}_k \log \bar{x}_k
\]</span> with <span class="math inline">\(\hat{\pi}_k^{ML} = \frac{n_k}{n} = \bar{x}_k\)</span>. Note that here and in the following the sums run from <span class="math inline">\(1\)</span> to <span class="math inline">\(K\)</span> where the <span class="math inline">\(K\)</span>-th component is always computed from the components <span class="math inline">\(1\)</span> to <span class="math inline">\(K-1\)</span>, as in the previous section. The log-likelihood at <span class="math inline">\(\boldsymbol p_0\)</span> is <span class="math display">\[
\ell_n( p_1^{0}, \ldots, p_{K-1}^{0}   ) =  n   \sum_{k=1}^{K}  \bar{x}_k \log p_k^{0}
\]</span> so that the Wilks statistic becomes <span class="math display">\[
W(\boldsymbol p_0) = 2 n   \sum_{k=1}^{K}  \bar{x}_k \log\left(\frac{\bar{x}_k}{ p_k^{0}} \right)
\]</span> It is asymptotically chi-squared distributed with <span class="math inline">\(K-1\)</span> degrees of freedom.</p>
<p>Note that for this model the Wilks statistic is equal to the KL divergence <span class="math display">\[
W(\boldsymbol p_0) = 2 n D_{\text{KL}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) )
\]</span></p>
<p>The Wilks log-likelihood ratio statistic for the categorical distribution is also known as the <a href="https://en.wikipedia.org/wiki/G-test"><span class="math inline">\(G\)</span> test statistic</a> where <span class="math inline">\(\hat{\boldsymbol \pi}_{ML}\)</span> corresponds to the observed frequencies (as observed in data) and <span class="math inline">\(\boldsymbol p_0\)</span> are the expected frequencies (i.e.&nbsp;hypothesised to be the true frequencies).</p>
<p>Using observed counts <span class="math inline">\(n_k\)</span> and expected counts <span class="math inline">\(n_k^{\text{expect}} = n p_k^{0}\)</span> we can write the Wilks statistic respectively the <span class="math inline">\(G\)</span>-statistic as follows: <span class="math display">\[
W(\boldsymbol p_0) = 2   \sum_{k=1}^{K}  n_k \log\left(\frac{  n_k }{  n_k^{\text{expect}}   } \right)
\]</span></p>
</div>
</section>
<section id="quadratic-approximation-of-the-wilks-statistic" class="level3">
<h3 class="anchored" data-anchor-id="quadratic-approximation-of-the-wilks-statistic">Quadratic approximation of the Wilks statistic</h3>
<p>Recall the <em>quadratic approximation</em> of the log-likelihood function <span class="math inline">\(\ell_n(\boldsymbol \theta_0)\)</span> (= second order Taylor series around the MLE <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span>):</p>
<p><span class="math display">\[\ell_n(\boldsymbol \theta_0)\approx \ell_n(\hat{\boldsymbol \theta}_{ML})-\frac{1}{2}(\boldsymbol \theta_0-\hat{\boldsymbol \theta}_{ML})^T \boldsymbol J_n(\hat{\boldsymbol \theta}_{ML}) (\boldsymbol \theta_0-\hat{\boldsymbol \theta}_{ML})\]</span></p>
<p>With this we can then approximate the Wilks statistic: <span class="math display">\[
\begin{split}
W(\boldsymbol \theta_0) &amp; = 2(\ell_n(\hat{\boldsymbol \theta}_{ML})-\ell_n(\boldsymbol \theta_0))\\
&amp; \approx (\boldsymbol \theta_0-\hat{\boldsymbol \theta}_{ML})^T \boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})(\boldsymbol \theta_0-\hat{\boldsymbol \theta}_{ML})\\
&amp; =t(\boldsymbol \theta_0)^2 \\
\end{split}
\]</span></p>
<p>Thus the <strong>quadratic approximation of the Wilks statistic yields the squared Wald statistic</strong>.</p>
<p>Conversely, the Wilks statistic can be understood a generalisation of the squared Wald statistic.</p>
</section>
<section id="examples-of-quadratic-approximations" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-quadratic-approximations">Examples of quadratic approximations</h3>
<div id="exm-wilksproportionquadraticapprox" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.5</strong></span> Quadratic approximation of the Wilks statistic for a proportion (continued from <a href="#exm-wilksproportion" class="quarto-xref">Example&nbsp;<span>11.2</span></a>):</p>
<p>The Wilks statistic is <span class="math display">\[
W(\theta_0) = 2 n \left(  \bar{x} \log \left( \frac{  \bar{x}  }{\theta_0}  \right)  
                + (1-\bar{x}) \log \left( \frac{1-\bar{x} }{1-\theta_0}  \right)  
    \right)
\]</span> Computing Taylor series of second order (for <span class="math inline">\(p_0\)</span> around <span class="math inline">\(\bar{x}\)</span>) yields the following approximations: <span class="math display">\[
\log \left( \frac{  \bar{x}  }{p_0} \right) \approx -\frac{p_0-\bar{x}}{\bar{x}} + \frac{ ( p_0-\bar{x} )^2    }{2  \bar{x}^2   }
\]</span> and <span class="math display">\[
\log \left( \frac{ 1- \bar{x}  }{1- p_0} \right) \approx \frac{p_0-\bar{x}}{1-\bar{x}} + \frac{ ( p_0-\bar{x} )^2    }{2  (1-\bar{x})^2   }
\]</span> With the above we can approximate the Wilks statistic of the proportion as <span class="math display">\[
\begin{split}
W(p_0) &amp; \approx  2 n \left(  - (p_0-\bar{x})  +\frac{ ( p_0-\bar{x} )^2    }{2  \bar{x}  }
+ (p_0-\bar{x}) + \frac{ ( p_0-\bar{x} )^2    }{2  (1-\bar{x}) } \right)   \\
&amp; = n \left(    \frac{ ( p_0-\bar{x} )^2    }{  \bar{x}  } + \frac{ ( p_0-\bar{x} )^2    }{  (1-\bar{x}) } \right)  \\
&amp; = n \left(    \frac{ ( p_0-\bar{x} )^2    }{  \bar{x} (1-\bar{x})  } \right)   \\
&amp;= t(p_0)^2 \,.
\end{split}
\]</span> This verifies that the quadratic approximation of the Wilks statistic leads back to the squared Wald statistic of <a href="10-likelihood4.html#exm-waldproportion" class="quarto-xref">Example&nbsp;<span>10.5</span></a>.</p>
</div>
<div id="exm-wilksnormalmeanquadraticapprox" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.6</strong></span> Quadratic approximation of the Wilks statistic for the mean parameter of a normal model (continued from <a href="#exm-wilksnormalmean" class="quarto-xref">Example&nbsp;<span>11.3</span></a>):</p>
<p>The normal log-likelihood is already quadratic in the mean parameter (cf. <a href="08-likelihood2.html#exm-mlenormalmean" class="quarto-xref">Example&nbsp;<span>8.2</span></a>). Correspondingly, the Wilks statistic is quadratic in the mean parameter as well. Hence in this particular case the quadratic “approximation” is in fact exact and the Wilks statistic and the squared Wald statistic are identical!</p>
<p>Correspondingly, confidence intervals and tests based on the Wilks statistic are identical to those obtained using the Wald statistic.</p>
</div>
<div id="exm-catwilksquadapprox" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.7</strong></span> Quadratic approximation of the Wilks log-likelihood ratio statistic for the categorical distribution:</p>
<p>Developing the Wilks statistic <span class="math inline">\(W(\boldsymbol p_0)\)</span> around the MLE <span class="math inline">\(\hat{\boldsymbol \pi}_{ML}\)</span> yields the squared Wald statistic which for the categorical distribution is the Neyman chi-squared statistic: <span class="math display">\[
\begin{split}
W(\boldsymbol p_0)&amp; = 2 n D_{\text{KL}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) ) \\
&amp; \approx n D_{\text{Neyman}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) ) \\
&amp; = \sum_{k=1}^K \frac{(n_k-n_k^{\text{expect}} )^2}{n_k} \\
&amp; =  \chi^2_{\text{Neyman}}\\
\end{split}
\]</span></p>
<p>If instead we approximate the KL divergence assuming <span class="math inline">\(\boldsymbol p_0\)</span> as fixed we arrive at <span class="math display">\[
\begin{split}
2 n D_{\text{KL}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) ) &amp;\approx n D_{\text{Pearson}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}),  \text{Cat}(\boldsymbol p_0 ) )\\
&amp; = \sum_{k=1}^K \frac{(n_k-n_k^{\text{expect}})^2}{n_k^{\text{expect}}} \\
&amp; = \chi^2_{\text{Pearson}}
\end{split}
\]</span> which is the well-known Pearson chi-squared statistic (note the <em>expected</em> counts in its denominator).</p>
</div>
</section>
<section id="distribution-of-the-wilks-statistic" class="level3">
<h3 class="anchored" data-anchor-id="distribution-of-the-wilks-statistic">Distribution of the Wilks statistic</h3>
<p>The connection with the squared Wald statistic as quadratic approximation of the Wilks log-likelihood ratio statistic implies that both have asympotically the same distribution.</p>
<p>Hence, under <span class="math inline">\(\boldsymbol \theta_0\)</span> the Wilks statistic is distributed asymptotically as <span class="math display">\[W(\boldsymbol \theta_0) \overset{a}{\sim} \chi^2_d\]</span> where <span class="math inline">\(d\)</span> is the number of parameters in <span class="math inline">\(\boldsymbol \theta\)</span>, i.e.&nbsp;the dimension of the model.</p>
<p>For scalar <span class="math inline">\(\theta\)</span> (i.e.&nbsp;single parameter and <span class="math inline">\(d=1\)</span>) this becomes <span class="math display">\[
W(\theta_0) \overset{a}{\sim} \chi^2_1
\]</span></p>
<p>This fact is known as <strong>Wilks’ theorem</strong>.</p>
</section>
<section id="cutoff-values-for-the-likelihood-ci" class="level3">
<h3 class="anchored" data-anchor-id="cutoff-values-for-the-likelihood-ci">Cutoff values for the likelihood CI</h3>
<div id="tbl-delta" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-delta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;11.1: Cutoff values for construction of likelihood confidence intervals for a single parameter.
</figcaption>
<div aria-describedby="tbl-delta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>coverage <span class="math inline">\(\kappa\)</span></th>
<th><span class="math inline">\(\Delta = \frac{c_{\text{chisq}}}{2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>1.35</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>1.92</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>3.32</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The asymptotic distribution for <span class="math inline">\(W\)</span> is useful to choose a suitable <span class="math inline">\(\Delta\)</span> for the likelihood CI noting that <span class="math inline">\(2 \Delta = c_{\text{chisq}}\)</span> where <span class="math inline">\(c_{\text{chisq}}\)</span> is the critical value from <a href="20-stats.html#tbl-critchisq" class="quarto-xref">Table&nbsp;<span>A.2</span></a> for a specified coverage <span class="math inline">\(\kappa\)</span>. This yields <a href="#tbl-delta" class="quarto-xref">Table&nbsp;<span>11.1</span></a> valid for a scalar parameter.</p>
<p>Hence, in order to calibrate the likelihood interval we in effect compare it with a normal confidence interval.</p>
<div id="exm-likciproportion" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.8</strong></span> Likelihood confidence interval for a proportion:</p>
<p>We continue from <a href="#exm-wilksproportion" class="quarto-xref">Example&nbsp;<span>11.2</span></a>, and as in <a href="10-likelihood4.html#exm-ciproportion" class="quarto-xref">Example&nbsp;<span>10.8</span></a> we asssume we have data with <span class="math inline">\(n = 30\)</span> and <span class="math inline">\(\bar{x} = 0.7\)</span>.</p>
<p>This yields (via numerical root finding) as the 95% likelihood confidence interval the interval <span class="math inline">\([0.524, 0.843]\)</span>. It is similar but not identical to the corresponding asymptotic normal interval <span class="math inline">\([0.536, 0.864]\)</span> obtained in <a href="10-likelihood4.html#exm-ciproportion" class="quarto-xref">Example&nbsp;<span>10.8</span></a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-likelihoodciber" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-likelihoodciber-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="11-likelihood5_files/figure-html/fig-likelihoodciber-1.png" class="img-fluid figure-img" data-fig-pos="t" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-likelihoodciber-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.2: Likelihood-based CI and normal interval for the Bernoulli model.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-likelihoodciber" class="quarto-xref">Figure&nbsp;<span>11.2</span></a> illustrates the relationship between the normal CI and the likelihood CI and also shows the role of the quadratic approximation (see also <a href="10-likelihood4.html#exm-quadapproxproportion" class="quarto-xref">Example&nbsp;<span>10.1</span></a>). Note that:</p>
<ul>
<li>the normal CI is symmetric around the MLE whereas the likelihood CI is not symmetric</li>
<li>the normal CI is identical to the likelihood CI when using the quadratic approximation!</li>
</ul>
</div>
</section>
<section id="likelihood-ratio-test-lrt-using-wilks-statistic" class="level3">
<h3 class="anchored" data-anchor-id="likelihood-ratio-test-lrt-using-wilks-statistic">Likelihood ratio test (LRT) using Wilks statistic</h3>
<p>As in the normal case (with Wald statistic and normal CIs) one can also construct a test using the Wilks statistic:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
H_0: \boldsymbol \theta= \boldsymbol \theta_0\\
H_1: \boldsymbol \theta\neq \boldsymbol \theta_0\\
\end{array}
\begin{array}{ll}
  \text{ True model is } \boldsymbol \theta_0\\
  \text{ True model is } \textbf{not } \boldsymbol \theta_0\\
\end{array}
\begin{array}{ll}
\text{  Null hypothesis}\\
\text{  Alternative hypothesis}\\
\end{array}
\end{align*}\]</span></p>
<p>As test statistic we use the Wilks log likelihood ratio <span class="math inline">\(W(\boldsymbol \theta_0)\)</span>. Extreme values of this test statistic imply evidence against <span class="math inline">\(H_0\)</span>.</p>
<p>Note that the null model is “simple” (= a single parameter value) whereas the alternative model is “composite” (= a set of parameter values).</p>
<p><strong>Remarks:</strong></p>
<ul>
<li>The composite alternative <span class="math inline">\(H_1\)</span> is represented by a single point (the MLE).</li>
<li><strong>Reject</strong> <span class="math inline">\(H_0\)</span> for <strong>large values of <span class="math inline">\(W(\boldsymbol \theta_0)\)</span></strong></li>
<li>under <span class="math inline">\(H_0\)</span> and for large <span class="math inline">\(n\)</span> the statistic <span class="math inline">\(W(\boldsymbol \theta_0)\)</span> is chi-squared distributed, i.e.&nbsp;<span class="math inline">\(W(\boldsymbol \theta_0) \overset{a}{\sim} \chi^2_d\)</span>. This allows to compute critical values (i.e tresholds to declared rejection under a given significance level) and also <span class="math inline">\(p\)</span>-values corresponding to the observed test statistics.</li>
<li>Models <strong>outside</strong> the CI are <strong>rejected</strong></li>
<li>Models <strong>inside</strong> the CI <strong>cannot be rejected</strong>, i.e.&nbsp;they can’t be statistically distinguished from the best alternative model.</li>
</ul>
<p>It can be shown that the likelihood ratio test to compare two simple models is optimal in the sense that for any given specified type I error (=probability of wrongly rejecting <span class="math inline">\(H_0\)</span>, i.e. the significance level) it will maximise the power (=1- type II error, probability of correctly accepting <span class="math inline">\(H_1\)</span>). This is known as the <strong>Neyman-Pearson theorem</strong>.</p>
<p>As we have seen previousy, the likelihood-based confidence interval differs from the the confidence interval based on the quadratic / normal approximation. Correspondingly, tests based on the log-likelihood ratio <span class="math inline">\(W(\boldsymbol \theta_0)\)</span> and on the squared Wald statistic <span class="math inline">\(t(\theta_0)^2\)</span> will also yield different outcomes (e.g.&nbsp;rejection due to lying outside the confidence interval) even though both test statistics share the same asymptotic distribution and critical values.</p>
<div id="exm-liktestproportion" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.9</strong></span> Likelihood test for a proportion:</p>
<p>We continue from <a href="#exm-likciproportion" class="quarto-xref">Example&nbsp;<span>11.8</span></a> with 95% likelihood confidence interval <span class="math inline">\([0.524, 0.843]\)</span>.</p>
<p>The value <span class="math inline">\(p_0=0.5\)</span> is outside the CI and hence can be rejected whereas <span class="math inline">\(p_0=0.8\)</span> is insided the CI and hence cannot be rejected on 5% significance level.</p>
<p>The Wilks statistic for <span class="math inline">\(p_0=0.5\)</span> and <span class="math inline">\(p_0=0.8\)</span> takes on the following values:</p>
<ul>
<li><span class="math inline">\(W(0.5) = 4.94  &gt; 3.84\)</span> hence <span class="math inline">\(p_0=0.5\)</span> can be rejected.</li>
<li><span class="math inline">\(W(0.8) = 1.69  &lt; 3.84\)</span> hence <span class="math inline">\(p_0=0.8\)</span> cannot be rejected.</li>
</ul>
<p>Note that the Wilks statistic at the boundaries of the likelihood confidence interval is equal to the critical value (3.84 corresponding to 5% significance level for a chi-squared distribution with 1 degree of freedom).</p>
<p>Compare also with the normal test for a proportion in <a href="10-likelihood4.html#exm-normaltestproportion" class="quarto-xref">Example&nbsp;<span>10.9</span></a>.</p>
</div>
</section>
<section id="origin-of-likelihood-ratio-statistic" class="level3">
<h3 class="anchored" data-anchor-id="origin-of-likelihood-ratio-statistic">Origin of likelihood ratio statistic</h3>
<p>The likelihood ratio statistic is asymptotically linked to differences in the KL divergences of the two compared models with the underlying true model.</p>
<p>Assume that <span class="math inline">\(F\)</span> is the true (and unknown) data generating model and that <span class="math inline">\(G_{\boldsymbol \theta}\)</span> is a family of models and we would like to compare two candidate models <span class="math inline">\(G_A\)</span> and <span class="math inline">\(G_B\)</span> corresponding to parameters <span class="math inline">\(\boldsymbol \theta_A\)</span> and <span class="math inline">\(\boldsymbol \theta_B\)</span> on the basis of observed data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span>. The KL divergences <span class="math inline">\(D_{\text{KL}}(F, G_A)\)</span> and <span class="math inline">\(D_{\text{KL}}(F, G_B)\)</span> indicate how close each of the models <span class="math inline">\(G_A\)</span> and <span class="math inline">\(G_B\)</span> fit the true <span class="math inline">\(F\)</span>. The difference of the two divergences is a way to measure the relative fit of the two models, and can be computed as <span class="math display">\[
D_{\text{KL}}(F, G_B)-D_{\text{KL}}(F, G_A) = \text{E}_{F} \log \frac{g(x|\boldsymbol \theta_A )}{g(x| \boldsymbol \theta_B)}
\]</span> Replacing <span class="math inline">\(F\)</span> by the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> leads to the large sample approximation <span class="math display">\[
2 n (D_{\text{KL}}(F, G_B)-D_{\text{KL}}(F, G_A))  \approx 2 (\ell_n(\boldsymbol \theta_A) - \ell_n(\boldsymbol \theta_B))
\]</span> Hence, the difference in the log-likelihoods provides an estimate of the difference in the KL divergence of the two models involved.</p>
<p>The Wilks log likelihood ratio statistic <span class="math display">\[
W(\boldsymbol \theta_0) = 2 ( \ell_n( \hat{\boldsymbol \theta}_{ML} ) - \ell_n(\boldsymbol \theta_0) )
\]</span> thus compares the best-fit distribution with <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> as the parameter to the distribution with parameter <span class="math inline">\(\boldsymbol \theta_0\)</span>.</p>
<p>For exponential families the Wilks statistic can also be written in the form of the KL divergence: <span class="math display">\[
W(\boldsymbol \theta_0) = 2n D_{\text{KL}}( F_{\hat{\boldsymbol \theta}_{ML}}, F_{\boldsymbol \theta_0})
\]</span> This has been seen in <a href="#exm-wilksproportion" class="quarto-xref">Example&nbsp;<span>11.2</span></a> and <a href="#exm-wilksnormalmean" class="quarto-xref">Example&nbsp;<span>11.3</span></a>. However, this is not true in general.</p>
</section>
</section>
<section id="generalised-likelihood-ratio-test-glrt" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="generalised-likelihood-ratio-test-glrt"><span class="header-section-number">11.2</span> Generalised likelihood ratio test (GLRT)</h2>
<p>Also known as <strong>maximum likelihood ratio test (MLRT)</strong>. The Generalised Likelihood Ratio Test (GLRT) works just like the standard likelihood ratio test with the difference that now the null model <span class="math inline">\(H_0\)</span> is also a composite model.<br>
<span class="math display">\[\begin{align*}
\begin{array}{ll}
H_0: \boldsymbol \theta\in \omega_0 \subset \Omega \\
H_1: \boldsymbol \theta\in \Omega \\
\end{array}
\begin{array}{ll}
\text{ True model lies in the restricted space }\\
\text{ True model lies in the unstricted space } \\
\end{array}
\end{align*}\]</span></p>
<p>Both <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> are now composite hypotheses. <span class="math inline">\(\Omega\)</span> represents the unrestricted model space with dimension (=number of free parameters) <span class="math inline">\(d = |\Omega|\)</span>. The constrained space <span class="math inline">\(\omega_0\)</span> has degree of freedom <span class="math inline">\(d_0 = |\omega_0|\)</span> with <span class="math inline">\(d_0 &lt; d\)</span>. Note that in the standard LRT the set <span class="math inline">\(\omega_0\)</span> is a simple point with <span class="math inline">\(d_0=0\)</span> as the null model is a simple distribution. Thus, LRT is contained in GLRT as special case!</p>
<p>The corresponding generalised (log) likelihood ratio statistic is given by <span class="math display">\[
\begin{split}
W &amp;= 2\log\left(\frac{L(\hat{\theta}_{ML} |D )}{L(\hat{\theta}_{ML}^0 | D )}\right)\\
&amp; = 2 ( \ell_n( \hat{\theta}_{ML} ) -  \ell_n( \hat{\theta}_{ML}^0 ) )
\end{split}
\]</span> and <span class="math display">\[
\Lambda = \frac{\underset{\theta \in \omega_0}{\max}\, L(\theta| D)}{\underset{\theta \in \Omega}{\max}\, L(\theta | D)}
\]</span> where <span class="math inline">\(L(\hat{\theta}_{ML}| D)\)</span> is the maximised likelihood assuming the full model (with full parameter space <span class="math inline">\(\Omega\)</span>) and <span class="math inline">\(L(\hat{\theta}_{ML}^0| D)\)</span> is the maximised likelihood for the restricted model (with restricted parameter space <span class="math inline">\(\omega_0\)</span>). Hence, to compute the GRLT test statistic we need to perform two optimisations, one for the full and another for the restricted model.</p>
<p><strong>Remarks:</strong></p>
<ul>
<li>MLE in the restricted model space <span class="math inline">\(\omega_0\)</span> is taken as a representative of <span class="math inline">\(H_0\)</span>.</li>
<li>The likelihood is <strong>maximised</strong> in <strong>both numerator</strong> and <strong>denominator</strong>.</li>
<li>The restricted model is a special case of the full model (i.e.&nbsp;the two models are nested).</li>
<li>The asymptotic distribution of <span class="math inline">\(W\)</span> is chi-squared with degree of freedom depending on both <span class="math inline">\(d\)</span> and <span class="math inline">\(d_0\)</span>:</li>
</ul>
<p><span class="math display">\[W \overset{a}{\sim} \text{$\chi^2_{d-d_0}$}\]</span></p>
<ul>
<li><p>This result is due to Wilks (1938) <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Note that it assumes that the true model is contained among the investigated models.</p></li>
<li><p>If <span class="math inline">\(H_0\)</span> is a simple hypothesis (i.e.&nbsp;<span class="math inline">\(d_0=0\)</span>) then the standard LRT (and corresponding CI) is recovered as special case of the GLRT.</p></li>
</ul>
<div id="exm-glrtnormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.10</strong></span> GLRT example:</p>
<p><em>Case-control study:</em> (e.g.&nbsp;“healthy” vs.&nbsp;“disease”)<br>
we observe normal data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> from two groups with sample size <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> (and <span class="math inline">\(n=n_1+n_2\)</span>), with two different means <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> and common variance <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[x_1,\dots,x_{n_1} \sim N(\mu_1, \sigma^2)\]</span> and <span class="math display">\[x_{n_1+1},\dots,x_{n} \sim N(\mu_2, \sigma^2)\]</span></p>
<p>Question: are the two means <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> the same in the two groups?</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
H_0: \mu_1=\mu_2  \text{ (with variance unknown, i.e. treated as nuisance parameter)}
\\
H_1: \mu_1\neq\mu_2\\
\end{array}
\end{align*}\]</span></p>
<p><em>Restricted and full models:</em></p>
<p><span class="math inline">\(\omega_0\)</span>: restricted model with two parameters <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\sigma^2_0\)</span> (so that <span class="math inline">\(x_{1},\dots,x_{n} \sim N(\mu_0, \sigma_0^2)\)</span> ).</p>
<p><span class="math inline">\(\Omega\)</span>: full model with three parameters <span class="math inline">\(\mu_1, \mu_2, \sigma^2\)</span>.</p>
<p><em>Corresponding log-likelihood functions:</em></p>
<p>Restricted model <span class="math inline">\(\omega_0\)</span>: <span class="math display">\[
\ell_n(\mu_0, \sigma_0^2) = -\frac{n}{2} \log(\sigma_0^2)
- \frac{1}{2\sigma_0^2} \sum_{i=1}^n (x_i-\mu_0)^2
\]</span></p>
<p>Full model <span class="math inline">\(\Omega\)</span>: <span class="math display">\[
\begin{split}
\ell_n(\mu_1, \mu_2, \sigma^2) &amp; =
\left(-\frac{n_1}{2} \log(\sigma^2) - \frac{1}{2\sigma^2}  \sum_{i=1}^{n_1} (x_i-\mu_1)^2   \right) + \\
&amp; \phantom{==}
\left(-\frac{n_2}{2} \log(\sigma^2) - \frac{1}{2\sigma^2}  \sum_{i=n_1+1}^{n} (x_i-\mu_2)^2   \right)
\\
&amp;= -\frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \left( \sum_{i=1}^{n_1} (x_i-\mu_1)^2 + \sum_{i=n_1+1}^n (x_i-\mu_2)^2 \right) \\
\end{split}
\]</span></p>
<p><em>Corresponding MLEs:</em></p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\omega_0:\\
\\
\Omega:\\
\\
\end{array}
\begin{array}{ll}
\hat{\mu}_0 = \frac{1}{n}\sum^n_{i=1}x_i\\
\\
\hat{\mu}_1 = \frac{1}{n_1}\sum^{n_1}_{i=1}x_i\\
\hat{\mu}_2 = \frac{1}{n_2}\sum^{n}_{i=n_1+1}x_i\\
\end{array}
\begin{array}{ll}
\widehat{\sigma^2_0} = \frac{1}{n}\sum^n_{i=1}(x_i-\hat{\mu}_0)^2\\
\\
\widehat{\sigma^2} = \frac{1}{n}\left\{\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)^2+\sum^n_{i=n_1+1}(x_i-\hat{\mu}_2)^2\right\}\\
\\
\end{array}
\end{align*}\]</span></p>
<p>Note that the three estimated means are related by <span class="math display">\[
\begin{split}
\hat{\mu}_0  &amp; = \frac{n_1}{n} \hat{\mu}_1 + \frac{n_2}{n} \hat{\mu}_2 \\
             &amp; = \hat{\pi_1} \hat{\mu}_1 +\hat{\pi_2} \hat{\mu}_2 \\
\end{split}
\]</span> so the overall mean is the weighted average of the two individual group means.</p>
<p>Moreover, the two estimated variances are related by <span class="math display">\[
\widehat{\sigma^2_0} =  \hat{\pi_1} \hat{\pi_2} (\hat{\mu}_1 - \hat{\mu}_2)^2 + \widehat{\sigma^2}
\]</span> Note that this is an example of variance decomposition, where</p>
<ul>
<li><span class="math inline">\(\widehat{\sigma^2_0}\)</span> is the estimated total variance,</li>
<li><span class="math inline">\(\hat{\pi_1} \hat{\pi_2} (\hat{\mu}_1 - \hat{\mu}_2)^2\)</span> the estimated between-group (explained) variance, and</li>
<li><span class="math inline">\(\widehat{\sigma^2}\)</span> is the estimated average within-group (unexplained) variance.</li>
</ul>
<p>For the following we also note that <span class="math display">\[
\begin{split}
\frac{ \widehat{\sigma^2_0} }{\widehat{\sigma^2}}
&amp; =  \hat{\pi_1} \hat{\pi_2} \frac{ (\hat{\mu}_1 - \hat{\mu}_2)^2}{\widehat{\sigma^2}} + 1\\
&amp; =  \frac{t^2_{\text{ML}}}{n} +1 \\
&amp; =   \frac{t^2_{\text{UB}}}{n-2} +1 \\
\end{split}
\]</span> where <span class="math display">\[
t_{\text{ML}} = \sqrt{n \hat{\pi_1} \hat{\pi_2} }  \frac{\hat{\mu}_1-\hat{\mu}_2}{ \widehat{\sigma^2}}
\]</span> is the two sample <span class="math inline">\(t\)</span>-statistic based on the ML variance estimate <span class="math inline">\(\widehat{\sigma^2}\)</span> and <span class="math inline">\(t_{\text{UB}} = t_{ML} \sqrt{\frac{n-2}{n}}\)</span> is the conventional two sample <span class="math inline">\(t\)</span>-statistic based on the unbiased variance estimate <span class="math inline">\(\widehat{\sigma^2}_{\text{UB}}=\frac{n}{n-2} \widehat{\sigma^2}\)</span> (see <a href="20-stats.html#sec-tstat" class="quarto-xref"><span>Section A.9</span></a>).</p>
<p><em>Corresponding maximised log-likelihood:</em></p>
<p>Restricted model:</p>
<p><span class="math display">\[\ell_n(\hat{\mu}_0,\widehat{\sigma^2_0}) = -\frac{n}{2} \log(\widehat{\sigma^2_0}) -\frac{n}{2} \]</span></p>
<p>Full model:</p>
<p><span class="math display">\[
\ell_n(\hat{\mu}_1,\hat{\mu}_2,\widehat{\sigma^2} = -\frac{n}{2} \log(\widehat{\sigma^2}) -\frac{n}{2}
\]</span></p>
<p><em>Likelihood ratio statistic:</em></p>
<p><span class="math display">\[
\begin{split}
W &amp; = 2 \left( \ell_n\left(\hat{\mu}_1,\hat{\mu}_2,\widehat{\sigma^2}\right)
    -  \ell_n \left(\hat{\mu}_0,\widehat{\sigma^2_0}\right) \right) \\
&amp; = n\log\left(\frac{\widehat{\sigma^2_0}}{\widehat{\sigma^2}} \right) \\
&amp; = n\log\left(\frac{t^2_{\text{ML}}}{n}+1\right) \\
&amp; = n\log\left(\frac{t^2_{\text{UB}}}{n-2}+1\right) \\
\end{split}
\]</span></p>
<p>Thus, the log-likelihood ratio statistic <span class="math inline">\(W\)</span> is a monotonic function (a one-to-one transformation!) of the (squared) two sample <span class="math inline">\(t\)</span>-statistic!</p>
<p><em>Asymptotic distribution:</em></p>
<p>The degree of freedom of the full model is <span class="math inline">\(d=3\)</span> and that of the constrained model <span class="math inline">\(d_0=2\)</span> so the generalised log likelihood ratio statistic <span class="math inline">\(W\)</span> is distributed asymptotically as <span class="math inline">\(\text{$\chi^2_{1}$}\)</span>. Hence, we reject the null model on 5% significance level for all <span class="math inline">\(W &gt; 3.84\)</span>.</p>
</div>
<p><em>Other application of GLRTs</em></p>
<p>As shown above, the two sample <span class="math inline">\(t\)</span> statistic can be derived as a likelihood ratio statistic.</p>
<p>More generally, it turns out that many other commonly used familiar statistical tests and test statistics can be interpreted as GLRTs. This shows the wide applicability of this procedure.</p>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Wilks, S. S. 1938. <em>The large-sample distribution of the likelihood ratio for testing composite hypotheses.</em> Ann. Math. Statist. <strong>9</strong>:60–62. <a href="https://doi.org/10.1214/aoms/1177732360" class="uri">https://doi.org/10.1214/aoms/1177732360</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./10-likelihood4.html" class="pagination-link" aria-label="Quadratic approximation and normal asymptotics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./12-likelihood6.html" class="pagination-link" aria-label="Optimality properties and conclusion">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>