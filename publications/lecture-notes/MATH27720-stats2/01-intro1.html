<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Statistical learning – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02-intro2.html" rel="next">
<link href="./00-prerequisites.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-9290db0fca16de22067673ab8036b289.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Models</a></li><li class="breadcrumb-item"><a href="./01-intro1.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Information</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Risk and divergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Local divergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Maximum entropy</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-statistics" id="toc-what-is-statistics" class="nav-link active" data-scroll-target="#what-is-statistics"><span class="header-section-number">1.1</span> What is statistics?</a></li>
  <li><a href="#probabilistic-underpinnings" id="toc-probabilistic-underpinnings" class="nav-link" data-scroll-target="#probabilistic-underpinnings"><span class="header-section-number">1.2</span> Probabilistic underpinnings</a>
  <ul class="collapse">
  <li><a href="#randomness-probability-and-uncertainty" id="toc-randomness-probability-and-uncertainty" class="nav-link" data-scroll-target="#randomness-probability-and-uncertainty">Randomness, probability and uncertainty</a></li>
  <li><a href="#probability-theory-versus-statistics" id="toc-probability-theory-versus-statistics" class="nav-link" data-scroll-target="#probability-theory-versus-statistics">Probability theory versus statistics</a></li>
  </ul></li>
  <li><a href="#model-based-learning" id="toc-model-based-learning" class="nav-link" data-scroll-target="#model-based-learning"><span class="header-section-number">1.3</span> Model-based learning</a>
  <ul class="collapse">
  <li><a href="#sketch-of-statistical-learning" id="toc-sketch-of-statistical-learning" class="nav-link" data-scroll-target="#sketch-of-statistical-learning">Sketch of statistical learning</a></li>
  <li><a href="#finding-the-best-models" id="toc-finding-the-best-models" class="nav-link" data-scroll-target="#finding-the-best-models">Finding the best models</a></li>
  <li><a href="#models-and-decomposition-of-uncertainty" id="toc-models-and-decomposition-of-uncertainty" class="nav-link" data-scroll-target="#models-and-decomposition-of-uncertainty">Models and decomposition of uncertainty</a></li>
  </ul></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">1.4</span> Further reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Models</a></li><li class="breadcrumb-item"><a href="./01-intro1.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Learning from data using probabilistic models lies at the heart of statistical learning. This chapter provides an overview over key foundational concepts.</p>
<section id="what-is-statistics" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="what-is-statistics"><span class="header-section-number">1.1</span> What is statistics?</h2>
<p>The following fundamental questions typically arise in any scientific data analysis:</p>
<ul>
<li><p><strong>Optimality</strong>: How do we extract information from data as efficiently and accurately as possible?</p></li>
<li><p><strong>Model fit</strong>: How can we build models that accurately reflect the observed data?</p></li>
<li><p><strong>Interpretability</strong>: How can we construct models that reveal underlying mechanisms and remain understandable?</p></li>
<li><p><strong>Prediction</strong>: How do we use these models and information to make the best possible predictions?</p></li>
</ul>
<p><strong>Statistics</strong> is a mathematical science for reasoning about data and uncertainty. It employs <strong>probabilistic models</strong> to address the questions above, offering a principled framework for learning from data and extracting and for processing information under uncertainty in an optimal way. This includes model selection, approximation and inference.</p>
<p><strong>Machine learning</strong> overlaps substantially with statistics. Rooted in computer science it frequently adopts an engineering-centric perspective and often emphasises algorithmic approaches. Some methods are non-probabilistic while many modern approaches adopt a statistical perspective.</p>
<p><strong>Data science</strong> today comprises elements of both statistics and machine learning and brings together mathematics, computer science and domain-specific expertise (e.g.&nbsp;<strong>biomedical data science</strong>).</p>
<p><strong>Artificial intelligence</strong> (AI) is a branch of computer science that makes substantial use of statistical and machine learning techniques, along with methods such as natural language processing and symbolic reasoning, to create systems that can generate and interpret highly complex data, including language and images, and make decisions to achieve goals.</p>
</section>
<section id="probabilistic-underpinnings" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="probabilistic-underpinnings"><span class="header-section-number">1.2</span> Probabilistic underpinnings</h2>
<section id="randomness-probability-and-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="randomness-probability-and-uncertainty">Randomness, probability and uncertainty</h3>
<p><strong>Random</strong> and <strong>randomness</strong> refer to unpredictable, non-deterministic outcomes or events. Equivalent, more technical terms are <strong>stochastic</strong> and <strong>stochasticity</strong>.</p>
<p>The degree of randomness (or uncertainty, see below) is quantified by the <strong>probability</strong>, or equivalently, by the <strong>chance</strong> of particular outcomes.</p>
<p>An interesting question is the source of the randomness. On a fundamental level, some phenomena are <em>intrinsically</em> random (e.g.&nbsp;radioactive decay, measurement outcomes in quantum theory). However, much <em>apparent</em> randomness arises from our ignorance of the underlying mechanisms. The process may be deterministic in principle but we treat it as random for convenience. For example, a coin flip is often considered random. However, in reality the outcome of a coin flip is fully determined by classical physics.</p>
<p>Randomness that is not intrinsic but stems from a lack of knowledge or understanding, is called <strong>uncertainty</strong>, and corresponding events are <strong>uncertain</strong>. Uncertainty generally decreases if more data and information or a better model is available.</p>
</section>
<section id="probability-theory-versus-statistics" class="level3">
<h3 class="anchored" data-anchor-id="probability-theory-versus-statistics">Probability theory versus statistics</h3>
<p>It is important to recognise the distinct domains of probability theory and statistics.</p>
<p>On the one hand, <strong>probability theory</strong> provides the <strong>mathematical underpinnings</strong> of probability and chance (e.g.&nbsp;probability axioms, measure theory) and corresponding models for randomness and uncertainty (e.g.&nbsp;probability distributions, stochastic processes). Crucially, it is neutral about the sources of randomness and interpretations of probability, and may simply be viewed as <strong>pure mathematics</strong>.</p>
<p>On the other hand, <strong>statistics</strong> uses probabilistic approaches to <strong>learn from observations</strong>, thus linking real-world phenomena with mathematical models. Thus, it is a branch of <strong>applied mathematics</strong>. Importantly, statistics is concerned with <strong>uncertainty</strong> (e.g.&nbsp;about events, predictions, outcomes, model parameters) without assuming that the underlying process is actually random, and to make decisions and predictions under that uncertainty.</p>
<p>The link of statistics with the real world also leads to different interpretations of probability, with the two most common being the <strong>frequentist interpretation</strong> (<strong>ontological</strong>, “every probability is a long running frequency and exists independently from an observer”) and the <strong>Bayesian interpretation</strong> (<strong>epistemological</strong>, “probability is a degree of belief and represent the state of knowledge”), both to be discussed later.</p>
</section>
</section>
<section id="model-based-learning" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="model-based-learning"><span class="header-section-number">1.3</span> Model-based learning</h2>
<section id="sketch-of-statistical-learning" class="level3">
<h3 class="anchored" data-anchor-id="sketch-of-statistical-learning">Sketch of statistical learning</h3>
<p>The aim of statistical learning is to use observed data in an optimal way to learn about the underlying mechanism of the data-generating process. Since data is typically finite but models can be in principle arbitrarily complex there may be issues of overfitting (insufficient data for the complexity of the model) but also under-fitting (model is too simplistic).</p>
<p>We observe data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> assumed to result from an underlying probabilistic model <span class="math inline">\(Q\)</span>, the distribution for&nbsp;<span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
\begin{array}{cc}
\textbf{Real world} \\
\text{True model (unknown)} \\
Q \\
\end{array}
\longrightarrow
\begin{array}{cc}
\textbf{Data}\\
\text{Samples from true model} \\
D = \{x_1, \ldots, x_n\}\\
x_i \sim Q\\
\end{array}
\]</span> The true model underlying the data-generating process is unknown and cannot be observed. However, what we can observe is data <span class="math inline">\(D\)</span> arising from the true model <span class="math inline">\(Q\)</span> by measuring properties of interest (our observations from experiments). Sometimes we can also perturb the model and see what the effect is (interventional study).</p>
<p>To explain the observed data <span class="math inline">\(D\)</span>, and also to predict future data, we consider various competing hypotheses in the form of candidate models <span class="math inline">\(P_{1}, P_{2}, \ldots\)</span>. Often these candidates belong to a parametric family <span class="math inline">\(P(\boldsymbol \theta)\)</span> indexed by a parameter vector <span class="math inline">\(\boldsymbol \theta\)</span>. Individual models such as <span class="math inline">\(P(\boldsymbol \theta_1)\)</span> or <span class="math inline">\(P(\boldsymbol \theta_2)\)</span> correspond to specific parameter values <span class="math inline">\(\boldsymbol \theta_1\)</span> and <span class="math inline">\(\boldsymbol \theta_2\)</span>.</p>
<p>Frequently parameters are chosen such that they allow some interpretation, such as moments or other properties of the distribution. However, intrinsically parameters are just labels and may be changed by any one-to-one transformation.</p>
<p>For statistical learning it is important that models are <strong>identifiable</strong> within a family so that distinct parameters correspond to distinct distributions, as this allows models to be uniquely estimated from data. Specifically, if parameters are identifiable then <span class="math inline">\(P(\boldsymbol \theta_1) = P(\boldsymbol \theta_2)\)</span> implies <span class="math inline">\(\boldsymbol \theta_1 = \boldsymbol \theta_2\)</span>, and conversely if <span class="math inline">\(P(\boldsymbol \theta_1) \neq P(\boldsymbol \theta_2)\)</span> then <span class="math inline">\(\boldsymbol \theta_1 \neq \boldsymbol \theta_2\)</span>. If parameters and distributions are unique within a neighbourhood <span class="math inline">\(\boldsymbol \theta_0+\boldsymbol \varepsilon\)</span> relative to a reference value <span class="math inline">\(\boldsymbol \theta_0\)</span>, rather than globally, they are <strong>locally identifiable</strong> at&nbsp;<span class="math inline">\(\boldsymbol \theta_0\)</span>.</p>
<p>The various candidate models <span class="math inline">\(P(\boldsymbol \theta)\)</span> in the <strong>model world</strong> will at best be good approximations to the true underlying data-generating model&nbsp;<span class="math inline">\(Q\)</span>. In some cases the true model will be part of the model family, i.e.&nbsp;there exists a parameter <span class="math inline">\(\boldsymbol \theta_{\text{true}}\)</span> so that <span class="math inline">\(Q = P(\boldsymbol \theta_{\text{true}})\)</span>. However, more typically we cannot assume that the true underlying model is contained in the family. Nonetheless, even an imperfect candidate model will often provide a useful mathematical approximation and capture some important characteristics of the true model and thus will help to interpret the observed data.</p>
<p><span class="math display">\[
\begin{array}{cc}
\textbf{Statistical Learning}\\
\text{Find model(s) and parameters} \\
\text{approximating the true model} \\
\text{and best explaining both} \\
\text{observed and future data} \\
Q \approx P(\hat{\boldsymbol \theta})\\
\end{array}
\longleftarrow
\begin{array}{cc}
\textbf{Model world} \\
\text{Hypotheses about}\\
\text{data-generating process:} \\
\text{Model } P(\boldsymbol \theta)\\
\text{with parameter(s) } \boldsymbol \theta\\
\text{Ideally, true model is } \\
Q = P(\boldsymbol \theta_{\text{true}})\\
\end{array}
\]</span></p>
<p><strong>The aim of statistical learning is to identify the model(s) that explain the current data and also predict future data (i.e.&nbsp;outcomes of experiments that have not yet been conducted).</strong></p>
<p>Thus a good model provides a good fit to the current data (i.e.&nbsp;it explains current observations well) and also to the future data (i.e.&nbsp;it generalises well).</p>
<p>A large proportion of statistical theory is devoted to finding these “good” models that avoid both <em>overfitting</em> (models being too complex and not generalising well) or <em>under-fitting</em> (models being too simplistic and hence also not predicting well).</p>
<p>Typically the aim is to find an approximating model whose <strong>model complexity</strong> is well matched with the complexity of the unknown true model and also with the complexity of the observed data.</p>
</section>
<section id="finding-the-best-models" class="level3">
<h3 class="anchored" data-anchor-id="finding-the-best-models">Finding the best models</h3>
<p>A core task in statistical learning is to identify those distributions that explain the existing data well and that also generalise well to future yet unseen observations.</p>
<p>In a <strong>nonparametric setting</strong> we may simply rely on the law of large numbers that implies that the empirical distribution <span class="math inline">\(\hat{Q}_n\)</span> constructed from the observed data <span class="math inline">\(D\)</span> converges to the true distribution <span class="math inline">\(Q\)</span> if the sample size is large. We can therefore obtain an <strong>empirical estimator</strong> <span class="math inline">\(\hat{\theta}\)</span> of the functional <span class="math inline">\(\theta = g(Q)\)</span> by <span class="math inline">\(\hat{\theta}= g( \hat{Q}_n )\)</span>, i.e.&nbsp;by substituting the true distribution with the empirical distribution. This allows us, e.g., to get the empirical estimate of the mean <span class="math inline">\(\operatorname{E}_{Q}(x) = \mu\)</span> by <span class="math display">\[
\hat{\operatorname{E}}(x) = \hat{\mu} =  \operatorname{E}_{\hat{Q}_n}(x) = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x}
\]</span> and of the variance <span class="math inline">\(\operatorname{Var}(x) = \sigma^2 = \operatorname{E}_{Q}((x - \mu)^2)\)</span> by <span class="math display">\[
\widehat{\operatorname{Var}}(x) = \widehat{\sigma^2} =
\operatorname{E}_{\hat{Q}_n}((x - \hat{\mu})^2) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2
\]</span> simply by replacing the expectation with the sample average.</p>
<p>For parametric models we need to find estimates of the parameters that correspond to the distributions that best approximate the unknown true data-generating model. One such approach is provided by the <strong>method of maximum likelihood</strong>. More precisely, given a probability distribution <span class="math inline">\(P(\boldsymbol \theta)\)</span> with density or mass function <span class="math inline">\(p(x|\boldsymbol \theta)\)</span> where <span class="math inline">\(\boldsymbol \theta\)</span> is a parameter vector, and <span class="math inline">\(D = \{x_1,\dots,x_n\}\)</span> are the observed iid data (i.e.&nbsp;independent and identically distributed), the <strong>likelihood function</strong> is defined as <span class="math display">\[
L_n(\boldsymbol \theta) = L(\boldsymbol \theta| D ) =\prod_{i=1}^{n} p(x_i|\boldsymbol \theta)
\]</span> The parameter value <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> that maximises the likelihood function for fixed data <span class="math inline">\(D\)</span> is the <strong>maximum likelihood estimate</strong>: <span class="math display">\[
\hat{\boldsymbol \theta}_{ML} = \underset{\boldsymbol \theta}{\arg \max}\, L_n(\boldsymbol \theta)
\]</span></p>
<p>Historically, the likelihood was introduced as “the probability of observing the data given a model with specified parameters <span class="math inline">\(\boldsymbol \theta\)</span>”.</p>
<p>However, this view breaks down for continuous random variables, where the likelihood function is formed from densities rather than probabilities. Furthermore, for discrete random variables the likelihood function typically does not include the additional factor accounting for the permutations of samples that is needed to obtain the actual probability of the data.</p>
<p>Instead, we will see that estimation and inference in statistics are closely linked to information-theoretic concepts such entropy and divergences between distributions. Specifically, maximising the likelihood function is, for large samples, equivalent to minimising the Kullback-Leibler divergence <span class="math inline">\(D_{\text{KL}}(Q,P(\boldsymbol \theta))\)</span> between the unknown true distribution <span class="math inline">\(Q\)</span> and the model <span class="math inline">\(P(\boldsymbol \theta)\)</span>. As a consequence, the method of <strong>maximum likelihood extends empirical estimation to parametric models</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. This connection explains both the optimality properties as well as the limitations of the maximum likelihood approach in statistical learning.</p>
</section>
<section id="models-and-decomposition-of-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="models-and-decomposition-of-uncertainty">Models and decomposition of uncertainty</h3>
<p>When constructing statistical models we will often choose to explain some aspects of the uncertainty while intentionally ignoring others, to create simple yet effective models.</p>
<p>As a result, for any given model, uncertainty decomposes into the sum&nbsp;of</p>
<ol type="i">
<li><strong>reducible uncertainty</strong> (epistemic uncertainty): <strong>explained by model</strong>, and</li>
<li><strong>irreducible uncertainty</strong> (aleatoric / residual / intrinsic uncertainty): <strong>unexplained by model</strong>.</li>
</ol>
<p>Crucially, even with arbitrarily large amounts of data, the total uncertainty cannot always be eliminated fully, since the residual uncertainty depends on the employed model. Consequently, to reduce the unexplained uncertainty the model itself must be changed, but whether this is at all desirable is a different matter (taking into account model complexity, interpretability, etc.).</p>
<p>For example, in linear regression or classification, the decomposition of uncertainty is expressed by the law of total variance with decomposes <strong>total variance</strong> into <strong>explained variance</strong> (“signal”, between-group variance) and <strong>unexplained variance</strong> (“noise”, within-group variance). Additional data improves the accuracy of estimates of the residual and the explained variance but does not eliminate the model’s unexplained variance. To reduce the residual error you need to change the model, e.g.&nbsp;by adding further covariates.</p>
<p>Importantly, intrinsic uncertainty should not be confused with intrinsic randomness: the latter is a claim about nature (fundamental non-determinism), whereas the first is a property of the model (residual uncertainty). Hence, unexplained uncertainty does not imply intrinsic randomness.</p>
</section>
</section>
<section id="further-reading" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">1.4</span> Further reading</h2>
<p>The book “Ten Great Ideas About Chance” by <span class="citation" data-cites="DiaconisSkyrms2018">Diaconis and Skyrms (<a href="bibliography.html#ref-DiaconisSkyrms2018" role="doc-biblioref">2018</a>)</span> provides and overview of the history and philosophical foundations of probability as well as discussing various interpretations of probability.</p>
<p>The book “Probability Theory: The Logic of Science” by <span class="citation" data-cites="Jaynes2003">Jaynes (<a href="bibliography.html#ref-Jaynes2003" role="doc-biblioref">2003</a>)</span> advocates that probability theory, as an extension of logic, is the natural framework for scientific reasoning.</p>
<p>The popular science book “The Theory That Would Not Die” <span class="citation" data-cites="McGrayne2011">(<a href="bibliography.html#ref-McGrayne2011" role="doc-biblioref">McGrayne 2011</a>)</span> focuses on the history of Bayes’ theorem and its importance in statistics.</p>
<p>For a quick recap of essential statistical concepts introduced in earlier statistical modules in year 1 and 2 see <a href="20-stats.html" class="quarto-xref"><span>Appendix A</span></a>.</p>
<div style="page-break-after: always;"></div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>A bit of history
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some important milestones in the development of learning from data are highlighted below:</p>
<ul>
<li><p><strong>Bayesian statistics</strong> dates back to <a href="https://en.wikipedia.org/wiki/Thomas_Bayes">Thomas Bayes (1701–1761)</a> and his famous posthumous <a href="https://en.wikipedia.org/wiki/An_Essay_Towards_Solving_a_Problem_in_the_Doctrine_of_Chances">1763 essay</a> and was further developed by <a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">Pierre-Simon Laplace (1749–1827</a> in the early 19th century.</p></li>
<li><p><strong>Maximum likelihood</strong> was developed by <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald A. Fisher (1890–1962)</a> in the early 20th century, with a seminal paper published in 1922 <span class="citation" data-cites="Fisher1922">(<a href="bibliography.html#ref-Fisher1922" role="doc-biblioref">Fisher 1922</a>)</span>.</p></li>
<li><p>Links of statistical learning with <strong>entropy</strong> were established in the 1940s with roots going back to discoveries in statistical physics in the 1870s. The close link of physics and statistical learning has recently been underlined by the <a href="https://www.nobelprize.org/prizes/physics/2024/summary/">2024 Nobel Prize in Physics</a> awarded to <a href="https://en.wikipedia.org/wiki/John_Hopfield">John J. Hopfield (1933–)</a> and <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Geoffrey E. Hinton (1947-)</a> for advances in artificial neural networks.</p></li>
<li><p>The first artificial <strong>neural network</strong> model appeared in the 1950s as a nonlinear input-output mapping without probabilistic foundations. Progress continued through the 1980s and accelerated after 2010 with <strong>deep learning</strong>. Today neural networks are widely used for complex data analysis and underpin many generative AI systems. Modern views treat them as high-dimensional nonlinear statistical models.</p></li>
<li><p>From the 1960s onwards the foundations of <strong><a href="https://en.wikipedia.org/wiki/Computational_learning_theory">computational learning theory</a></strong> were developed, including later the well-known “support vector machine” (a non-probabilistic model). Other important advances include “ensemble learning” and corresponding algorithmic approaches to classification such as “random forests”.</p></li>
<li><p>Classical statistics focused on settings with few variables and large sample sizes. Since about 2000, large‑scale genomics and other high‑dimensional datasets have driven rapid advances in statistics and machine learning to develop new methods to handle <strong>high‑dimensional data</strong> (many variables with moderate sample sizes) and <strong>big data</strong> (many variables and large sample sizes).</p></li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-DiaconisSkyrms2018" class="csl-entry" role="listitem">
Diaconis, P., and B. Skyrms. 2018. <em>Ten Great Ideas about Chance</em>. Princeton University Press.
</div>
<div id="ref-Fisher1922" class="csl-entry" role="listitem">
Fisher, R. A. 1922. <span>“On the Mathematical Foundations of Theoretical Statistics.”</span> <em>Phil. Trans. R. Soc. A</em> 222: 309–68. <a href="https://doi.org/10.1098/rsta.1922.0009">https://doi.org/10.1098/rsta.1922.0009</a>.
</div>
<div id="ref-Jaynes2003" class="csl-entry" role="listitem">
Jaynes, E. T. 2003. <em>Probability Theory: The Logic of Science</em>. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511790423">https://doi.org/10.1017/CBO9780511790423</a>.
</div>
<div id="ref-McGrayne2011" class="csl-entry" role="listitem">
McGrayne, S. B. 2011. <em>The Theory That Would Not Die</em>. Yale University Press.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Conversely, empirical estimators are, in fact, also likelihood estimators based on an <a href="https://en.wikipedia.org/wiki/Empirical_likelihood">empirical likelihood</a> function constructed from the empirical distribution.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./00-prerequisites.html" class="pagination-link" aria-label="Prerequisites">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Prerequisites</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./02-intro2.html" class="pagination-link" aria-label="Distributions">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>