<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistics 2: Likelihood and Bayes - 2&nbsp; Entropy and KL information</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03-likelihood3.html" rel="next">
<link href="./01-likelihood1.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-likelihood1.html">Likelihood estimation and inference</a></li><li class="breadcrumb-item"><a href="./02-likelihood2.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Entropy and KL information</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Likelihood estimation and inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Overview of statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-likelihood2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Entropy and KL information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Bayesian Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Bayesian learning in practise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#information-theory-and-statistics" id="toc-information-theory-and-statistics" class="nav-link active" data-scroll-target="#information-theory-and-statistics"><span class="header-section-number">2.1</span> Information theory and statistics</a></li>
  <li><a href="#shannon-entropy-and-differential-entropy" id="toc-shannon-entropy-and-differential-entropy" class="nav-link" data-scroll-target="#shannon-entropy-and-differential-entropy"><span class="header-section-number">2.2</span> Shannon entropy and differential entropy</a>
  <ul class="collapse">
  <li><a href="#the-logarithm-and-units-of-information-storage" id="toc-the-logarithm-and-units-of-information-storage" class="nav-link" data-scroll-target="#the-logarithm-and-units-of-information-storage">The logarithm and units of information storage</a></li>
  <li><a href="#surprise-or-surprisal-and-logarithmic-scoring-rule" id="toc-surprise-or-surprisal-and-logarithmic-scoring-rule" class="nav-link" data-scroll-target="#surprise-or-surprisal-and-logarithmic-scoring-rule">Surprise or surprisal and logarithmic scoring rule</a></li>
  <li><a href="#entropy-of-a-distribution" id="toc-entropy-of-a-distribution" class="nav-link" data-scroll-target="#entropy-of-a-distribution">Entropy of a distribution</a></li>
  </ul></li>
  <li><a href="#entropy-examples" id="toc-entropy-examples" class="nav-link" data-scroll-target="#entropy-examples"><span class="header-section-number">2.3</span> Entropy examples</a>
  <ul class="collapse">
  <li><a href="#models-with-single-parameter" id="toc-models-with-single-parameter" class="nav-link" data-scroll-target="#models-with-single-parameter">Models with single parameter</a></li>
  <li><a href="#models-with-multiple-parameters" id="toc-models-with-multiple-parameters" class="nav-link" data-scroll-target="#models-with-multiple-parameters">Models with multiple parameters</a></li>
  </ul></li>
  <li><a href="#colorred-blacktriangleright-maximum-entropy-principle-to-characterise-distributions" id="toc-colorred-blacktriangleright-maximum-entropy-principle-to-characterise-distributions" class="nav-link" data-scroll-target="#colorred-blacktriangleright-maximum-entropy-principle-to-characterise-distributions"><span class="header-section-number">2.4</span> <span class="math inline">\({\color{Red} \blacktriangleright}\)</span> Maximum entropy principle to characterise distributions</a></li>
  <li><a href="#cross-entropy-and-kl-divergence" id="toc-cross-entropy-and-kl-divergence" class="nav-link" data-scroll-target="#cross-entropy-and-kl-divergence"><span class="header-section-number">2.5</span> Cross-entropy and KL divergence</a>
  <ul class="collapse">
  <li><a href="#definition-of-cross-entropy" id="toc-definition-of-cross-entropy" class="nav-link" data-scroll-target="#definition-of-cross-entropy">Definition of cross-entropy</a></li>
  <li><a href="#definition-and-properties-of-kl-divergence" id="toc-definition-and-properties-of-kl-divergence" class="nav-link" data-scroll-target="#definition-and-properties-of-kl-divergence">Definition and properties of KL divergence</a></li>
  <li><a href="#invariance-property-of-kl-divergence" id="toc-invariance-property-of-kl-divergence" class="nav-link" data-scroll-target="#invariance-property-of-kl-divergence">Invariance property of KL divergence</a></li>
  <li><a href="#origin-of-kl-divergence-and-naming-conventions" id="toc-origin-of-kl-divergence-and-naming-conventions" class="nav-link" data-scroll-target="#origin-of-kl-divergence-and-naming-conventions">Origin of KL divergence and naming conventions</a></li>
  <li><a href="#application-in-statistics" id="toc-application-in-statistics" class="nav-link" data-scroll-target="#application-in-statistics">Application in statistics</a></li>
  </ul></li>
  <li><a href="#cross-entropy-and-kl-divergence-examples" id="toc-cross-entropy-and-kl-divergence-examples" class="nav-link" data-scroll-target="#cross-entropy-and-kl-divergence-examples"><span class="header-section-number">2.6</span> Cross-entropy and KL divergence examples</a>
  <ul class="collapse">
  <li><a href="#models-with-a-single-parameter" id="toc-models-with-a-single-parameter" class="nav-link" data-scroll-target="#models-with-a-single-parameter">Models with a single parameter</a></li>
  <li><a href="#models-with-multiple-parameters-1" id="toc-models-with-multiple-parameters-1" class="nav-link" data-scroll-target="#models-with-multiple-parameters-1">Models with multiple parameters</a></li>
  </ul></li>
  <li><a href="#expected-fisher-information" id="toc-expected-fisher-information" class="nav-link" data-scroll-target="#expected-fisher-information"><span class="header-section-number">2.7</span> Expected Fisher information</a>
  <ul class="collapse">
  <li><a href="#definition-of-expected-fisher-information" id="toc-definition-of-expected-fisher-information" class="nav-link" data-scroll-target="#definition-of-expected-fisher-information">Definition of expected Fisher information</a></li>
  <li><a href="#sec-additivityfisher" id="toc-sec-additivityfisher" class="nav-link" data-scroll-target="#sec-additivityfisher">Additivity of Fisher information</a></li>
  <li><a href="#invariance-property-of-the-fisher-information" id="toc-invariance-property-of-the-fisher-information" class="nav-link" data-scroll-target="#invariance-property-of-the-fisher-information">Invariance property of the Fisher information</a></li>
  <li><a href="#sec-covariantfisher" id="toc-sec-covariantfisher" class="nav-link" data-scroll-target="#sec-covariantfisher"><span class="math inline">\({\color{Red} \blacktriangleright}\)</span> Transformation of Fisher information when model parameters change</a></li>
  </ul></li>
  <li><a href="#expected-fisher-information-examples" id="toc-expected-fisher-information-examples" class="nav-link" data-scroll-target="#expected-fisher-information-examples"><span class="header-section-number">2.8</span> Expected Fisher information examples</a>
  <ul class="collapse">
  <li><a href="#models-with-a-single-parameter-1" id="toc-models-with-a-single-parameter-1" class="nav-link" data-scroll-target="#models-with-a-single-parameter-1">Models with a single parameter</a></li>
  <li><a href="#models-with-multiple-parameters-2" id="toc-models-with-multiple-parameters-2" class="nav-link" data-scroll-target="#models-with-multiple-parameters-2">Models with multiple parameters</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-likelihood1.html">Likelihood estimation and inference</a></li><li class="breadcrumb-item"><a href="./02-likelihood2.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Entropy and KL information</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Entropy and KL information</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="information-theory-and-statistics" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="information-theory-and-statistics"><span class="header-section-number">2.1</span> Information theory and statistics</h2>
<p>Information theory and statistical learning is closely linked. The purpose of this chapter is to introduce various important information criteria used in statistics and machine learning. These are all based on entropy and provide the foundation for the method of maximum likelihood as well as Bayesian learning. They also provides the basis for the asymptotic validity of maximum likelihood estimation.</p>
<p>The concept of entropy was first introduced in 1865 by <a href="https://en.wikipedia.org/wiki/Rudolf_Clausius">Rudolph Clausius (1822-1888)</a> in the context of thermodynamics. In physics entropy measures the distribution of energy: if energy is concentrated then the entropy is low, and conversely if energy is spread out the entropy is large. The total energy is conserved (<a href="https://en.wikipedia.org/wiki/First_law_of_thermodynamics">first law of thermodynamics</a>) but with time it will diffuse and thus entropy will increase with time (<a href="https://en.wikipedia.org/wiki/Second_law_of_thermodynamics">second law of thermodynamics</a>).</p>
<p>The modern probabilistic definition of entropy was discovered in the 1870s by <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann">Ludwig Boltzmann (1844–1906)</a> and <a href="https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs">Josiah W. Gibbs (1839–1903)</a>. In statistical mechanics entropy is proportional to the logarithm of the number of microstates (i.e.&nbsp;particular configurations of the system) compatible with the observed macrostate. Typically, in systems where the energy is spread out there are very large numbers of compatible configurations hence this corresponds to large entropy, and conversely, if the energy is concentrated there are only few such configurations, and thus is corresponds to low entropy.</p>
<p>In the 1940–1950’s the notion of entropy turned out to be central also in information theory, a field pioneered by mathematicians such as <a href="https://en.wikipedia.org/wiki/Ralph_Hartley">Ralph Hartley (1888–1970)</a>, <a href="https://en.wikipedia.org/wiki/Solomon_Kullback">Solomon Kullback (1907–1994)</a>, <a href="https://en.wikipedia.org/wiki/Alan_Turing">Alan Turing (1912–1954)</a>, <a href="https://en.wikipedia.org/wiki/Richard_Leibler">Richard Leibler (1914–2003)</a>, <a href="https://en.wikipedia.org/wiki/I._J._Good">Irving J. Good (1916–2009)</a>, <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon (1916–2001)</a>, and <a href="https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes">Edwin T. Jaynes (1922–1998)</a>, and later further explored by <a href="https://de.wikipedia.org/wiki/Shun%E2%80%99ichi_Amari">Shun’ichi Amari (1936–)</a>, <a href="https://en.wikipedia.org/wiki/Imre_Csisz%C3%A1r">Imre Ciszár (1938–)</a>, <a href="https://de.wikipedia.org/wiki/Bradley_Efron">Bradley Efron (1938–)</a>, <a href="https://en.wikipedia.org/wiki/Philip_Dawid">Philip Dawid (1946–)</a> and many others.</p>
<p>Of the above, Turing and Good are affiliated with the University of Manchester.</p>
<p><span class="math display">\[\begin{align*}
\left.
\begin{array}{cc}
\\
\textbf{Entropy} \\
\\
\end{array}
\right.
\left.
\begin{array}{cc}
\\
\nearrow  \\
\searrow  \\
\\
\end{array}
\right.
\begin{array}{ll}
\text{Shannon Entropy} \\
\\
\text{KL information}  \\
\end{array}
\begin{array}{ll}
\text{(Shannon 1948)} \\
\\
\text{(Kullback-Leibler 1951)}  \\
\end{array}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\left.
\begin{array}{ll}
\text{Fisher information} \\
\\
\text{Mutual Information} \\
\end{array}
\right.
\begin{array}{ll}
\rightarrow\text{ Likelihood theory} \\
\\
\rightarrow\text{ Information theory} \\
\end{array}
\begin{array}{ll}
\text{(Fisher 1922)} \\
\\
\text{(Shannon 1948, Lindley 1953)}  \\
\end{array}
\end{align*}\]</span></p>
</section>
<section id="shannon-entropy-and-differential-entropy" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="shannon-entropy-and-differential-entropy"><span class="header-section-number">2.2</span> Shannon entropy and differential entropy</h2>
<section id="the-logarithm-and-units-of-information-storage" class="level3">
<h3 class="anchored" data-anchor-id="the-logarithm-and-units-of-information-storage">The logarithm and units of information storage</h3>
<p>In this module the logarithmic function <span class="math inline">\(\log(x)\)</span> without explicitly stated base always denotes the <em>natural logarithm</em>. For logarithms with respect to base 2 and 10 we write <span class="math inline">\(\log_2(x)\)</span> and <span class="math inline">\(\log_{10}(x)\)</span>, respectively.</p>
<p>Assume we have a discrete variable <span class="math inline">\(x\)</span> for a system with with <span class="math inline">\(K\)</span> possible states <span class="math inline">\(\Omega = \{\omega_1, \ldots, \omega_K\}\)</span> and we have individual storage units available with the capability to index a limited number <span class="math inline">\(a\)</span> of different states. Following the principle underlying common numeral systems, such as the decimal, binary or hexadecimal numbers, using <span class="math inline">\(S = \log_a K\)</span> of such information storage units is sufficient to describe and store the system configuration.</p>
<p>The storage requirement <span class="math inline">\(S\)</span> can also be interpreted as the <strong>code length</strong> or the <strong>cost</strong> needed to describe the system state using an alphabet of size <span class="math inline">\(a\)</span>.</p>
<p>The above tacitly assumes that all <span class="math inline">\(K\)</span> states are treated equally so the storage size / code length / cost requirement associated with each state is constant and the same for all possible <span class="math inline">\(K\)</span> states. With this in mind we can write <span class="math display">\[
S = -\log \left( \frac{1}{K} \right)
\]</span> where <span class="math inline">\(1/K\)</span> is the equal probability of each of the <span class="math inline">\(K\)</span> states.</p>
<div id="exm-units" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1</strong></span> Information storage units:</p>
<p>For <span class="math inline">\(a=2\)</span> the storage units are called “bits” (<a href="https://en.wikipedia.org/wiki/Bit">binary information units</a>), and a single bit can store 2 states. Hence to describe the <span class="math inline">\(K=256\)</span> possible states in a system <span class="math inline">\(8=\log_2 256\)</span> bits (or 1 byte) of storage are sufficient.</p>
<p>For <span class="math inline">\(a=10\)</span> the units are “dits” (decimal information units), so to describe <span class="math inline">\(K=100\)</span> possible states <span class="math inline">\(2=\log_{10} 100\)</span> dits are sufficient, where a single dit can store 10 states.</p>
<p>Finally, if the natural logarithm is used (<span class="math inline">\(a=e\)</span>) the storage units are called “nits” (<a href="https://en.wikipedia.org/wiki/Nat_(unit)">natural information units</a>). In the following we will use “nits” and natural logarithm throughout.</p>
</div>
</section>
<section id="surprise-or-surprisal-and-logarithmic-scoring-rule" class="level3">
<h3 class="anchored" data-anchor-id="surprise-or-surprisal-and-logarithmic-scoring-rule">Surprise or surprisal and logarithmic scoring rule</h3>
<p>In practise, the <span class="math inline">\(K\)</span> states may not all be equally probably, and assume there is a discrete distribution <span class="math inline">\(P\)</span> with probability mass function <span class="math inline">\(p(x)\)</span> to model the state probabilities. In this case, instead of using the same code length to describe each state, we may use <em>variable code lengths</em>, with more probable states assigned shorter codes and less probable states having longer codes. More specifically, generalising from the previous we may use the negative logarithm to map the probability of a state <span class="math inline">\(x\)</span> to a corresponding cost and code length: <span class="math display">\[
S(x, P) = -\log p(x)
\]</span> As we will see below (<a href="#exm-entropycategorical" class="quarto-xref">Example&nbsp;<span>2.8</span></a>, <a href="#exm-entropyuniform" class="quarto-xref">Example&nbsp;<span>2.9</span></a> and <a href="#exm-entropyconcentrated" class="quarto-xref">Example&nbsp;<span>2.10</span></a>) using logarithmic cost allows for expected code lengths that are potentially much smaller than the fixed length <span class="math inline">\(\log K\)</span>, and hence leads to a more space saving representation.</p>
<p>The negative logarithm of the probability <span class="math inline">\(p(x)\)</span> of an event <span class="math inline">\(x\)</span> is known as the <strong>surprise</strong> or <strong>surprisal</strong>. The surprise to observe a certain event (with <span class="math inline">\(p(x)=1\)</span>) is zero, and conversely the surprise to observe an event that is certain not to happen (with <span class="math inline">\(p(x)=0\)</span>) is infinite.</p>
<p>We will apply <span class="math inline">\(S(x, P)\)</span> to both discrete and and continuous variables <span class="math inline">\(x\)</span> and corresponding distributions <span class="math inline">\(P\)</span> and then call it <strong>logarithmic score</strong> or <strong>logarithmic scoring rule</strong> (see also <a href="#exm-scoringrules" class="quarto-xref">Example&nbsp;<span>2.4</span></a>). As densities can take on values larger than 1 the logarithmic score <span class="math inline">\(S(x, P)\)</span> may therefore become negative when <span class="math inline">\(P\)</span> is a continuous distribution.</p>
<div id="exm-logodds" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2</strong></span> Log-odds ratio and surprise:</p>
<p>The commonly used <strong>log-odds ratio</strong> of the probability <span class="math inline">\(p\)</span> of an event is the difference of the surprise of the complementary event (with probability <span class="math inline">\(1-p\)</span>) and the surprise of the event:</p>
<p><span class="math display">\[
\begin{split}
\text{logit}(p) &amp;= \log\left( \frac{p}{1-p} \right) \\
&amp;= -\log(1-p) - ( -\log p)\\
\end{split}
\]</span></p>
</div>
<div id="exm-logscorenormdist" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3</strong></span> Logarithmic score and normal distribution:</p>
<p>If we quote in the logarithmic scoring rule the normal distribution <span class="math inline">\(P = N(\mu, \sigma^2)\)</span> with density <span class="math inline">\(p(x |\mu, \sigma^2)= \frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\)</span> we get as score <span class="math display">\[
S\left(x,N(\mu, \sigma^2 )\right) = \frac{1}{2} \left( \log(2\pi\sigma^2) + \frac{(x-\mu)^2}{\sigma^2}\right)
\]</span> For fixed variance <span class="math inline">\(\sigma^2\)</span> this is equivalent to the squared error from the parameter&nbsp;<span class="math inline">\(\mu\)</span>.</p>
</div>
<div id="exm-scoringrules" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4</strong></span> <span class="math inline">\({\color{Red} \blacktriangleright}\)</span> General scoring rules:</p>
<p>The function <span class="math inline">\(S(x, P) = -\log p(x)\)</span> is an important example of a <strong><a href="https://en.wikipedia.org/wiki/Scoring_rule">scoring rule</a></strong> for a probabilistic forecast represented by model <span class="math inline">\(P\)</span> evaluated on the observation <span class="math inline">\(x\)</span>. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>While one can devise many different scoring rules the logarithmic scoring rule stands out as it has a number of unique and favourable properties (e.g.&nbsp;Hartley 1928, Shannon 1948, Good 1952, Bernardo 1979). In particular, it is the only scoring rule that is both <em>proper</em>, i.e.&nbsp;the expected score is minimised when the quoted model <span class="math inline">\(P\)</span> is identical to the data generating model, and <em>local</em> in that the score depends only on the value of the density/probability mass function at <span class="math inline">\(x\)</span>.</p>
</div>
</section>
<section id="entropy-of-a-distribution" class="level3">
<h3 class="anchored" data-anchor-id="entropy-of-a-distribution">Entropy of a distribution</h3>
<p>The entropy of the distribution <span class="math inline">\(P\)</span> is defined as the functional <span class="math display">\[
\begin{split}
H(P) &amp;= \text{E}_P\left( S(x, P) \right) \\
     &amp;= - \text{E}_P\left(\log p(x)\right) \\
\end{split}
\]</span> i.e.&nbsp;as the expected logarithmic score when the data are generated by <span class="math inline">\(P\)</span> and the model <span class="math inline">\(P\)</span> is evaluated on the observations. As will be clear from the examples, entropy measures the <strong>spread of the probability mass</strong> across a distribution. If the probability mass is locally concentrated the entropy will be low, and conversely, if the probability mass is spread out the entropy will be large.</p>
<p>The entropy of a discrete probability distribution <span class="math inline">\(P\)</span> with probability mass function <span class="math inline">\(p(x)\)</span> with <span class="math inline">\(x \in \Omega\)</span> is called <strong>Shannon entropy</strong> (1948) <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. In statistical physics, the Shannon entropy is known as <a href="https://en.wikipedia.org/wiki/Entropy_(statistical_thermodynamics)#Gibbs_entropy_formula">Gibbs entropy (1878)</a>:</p>
<p><span class="math display">\[
H(P) = - \sum_{x \in \Omega} \log p(x) \, p(x)
\]</span> The entropy of a discrete distribution is the <strong>expected surprise</strong>. We can also interpret it as the <strong>expected cost</strong> or <strong>expected code length</strong> when the data are generated according to model <span class="math inline">\(P\)</span> and we are also using model <span class="math inline">\(P\)</span> to describe the data. Furthermore, it also has a combinatorial interpretation (see <a href="#exm-entropymultinomial" class="quarto-xref">Example&nbsp;<span>2.12</span></a>).</p>
<p>As <span class="math inline">\(p(x) \in [0,1]\)</span> and hence <span class="math inline">\(-\log p(x) \geq 0\)</span> by construction Shannon entropy is bounded below and must be larger or equal to 0.</p>
<p>Applying the definition of entropy to a continuous probability distribution <span class="math inline">\(P\)</span> with density <span class="math inline">\(p(x)\)</span> yields the <strong>differential entropy</strong>: <span class="math display">\[
H(P) = -\text{E}_P(\log p(x)) = - \int_x \log p(x) \, p(x) \, dx
\]</span> Because the logarithm is taken of a density, which in contrast to a probability can assume values larger than one, differential entropy can be negative.</p>
<p>Furthermore, since for continuous random variables the shape of the density typically changes under <strong>variable transformation</strong>, say from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span>, the <strong>differential entropy will change</strong> as well under such a transformation so that <span class="math inline">\(H(P_y) \neq H(P_x)\)</span>.</p>
</section>
</section>
<section id="entropy-examples" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="entropy-examples"><span class="header-section-number">2.3</span> Entropy examples</h2>
<section id="models-with-single-parameter" class="level3">
<h3 class="anchored" data-anchor-id="models-with-single-parameter">Models with single parameter</h3>
<div id="exm-entropygeom" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5</strong></span> The Shannon entropy of the geometric distribution <span class="math inline">\(F_x = \text{Geom}(\theta)\)</span> with probability mass function <span class="math inline">\(p(x|\theta) = \theta (1-\theta)^{x-1}\)</span>, <span class="math inline">\(\theta \in [0,1]\)</span>, support <span class="math inline">\(x \in \{1, 2, \ldots \}\)</span> and <span class="math inline">\(\text{E}(x)= 1/\theta\)</span> is <span class="math display">\[
\begin{split}
H(F_x) &amp;= - \text{E}\left( \log \theta + (x-1) \log(1-\theta)   \right)\\
       &amp;= -\log \theta+ \left(\frac{1}{\theta}-1\right)\log(1-\theta)\\
       &amp;= -\frac{\theta \log \theta + (1-\theta) \log(1-\theta) }{\theta}
\end{split}
\]</span> Using the identity <span class="math inline">\(0\times\log(0)=0\)</span> we see that the entropy of the geometric distribution for <span class="math inline">\(\theta = 1\)</span> equals 0, i.e.&nbsp;it achieves the minimum possible Shannon entropy. Conversely, as <span class="math inline">\(\theta \rightarrow 0\)</span> it diverges to infinity.</p>
</div>
<div id="exm-entropyunif" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6</strong></span> Consider the uniform distribution <span class="math inline">\(F_x = U(0, a)\)</span> with <span class="math inline">\(a&gt;0\)</span>, support from <span class="math inline">\(0\)</span> to <span class="math inline">\(a\)</span> and density <span class="math inline">\(p(x) = 1/a\)</span>. The corresponding differential entropy is <span class="math display">\[
\begin{split}
H( F_x ) &amp;= - \int_0^a \log\left(\frac{1}{a}\right) \, \frac{1}{a}  dx \\
             &amp;=  \log a  \int_0^a \frac{1}{a} dx \\
             &amp;= \log a \,.
\end{split}
\]</span> Note that for <span class="math inline">\(0 &lt; a &lt; 1\)</span> the differential entropy is negative.</p>
</div>
<div id="exm-entropyvartrans" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.7</strong></span> Starting with the uniform distribution <span class="math inline">\(F_x = U(0, a)\)</span> from <a href="#exm-entropyunif" class="quarto-xref">Example&nbsp;<span>2.6</span></a> the variable <span class="math inline">\(x\)</span> is changed to <span class="math inline">\(y = x^2\)</span> yielding the distribution <span class="math inline">\(F_y\)</span> with support from <span class="math inline">\(0\)</span> to <span class="math inline">\(a^2\)</span> and density <span class="math inline">\(p(y) = 1/\left(2 a \sqrt{y}\right)\)</span>.</p>
<p>The corresponding differential entropy is <span class="math display">\[
\begin{split}
H( F_y ) &amp;=  \int_0^{a^2}  \log \left(2 a \sqrt{y}\right) \, 1/\left(2 a \sqrt{y}\right) dy \\
         &amp;= \left[ \sqrt{y}/a \, \left(\log \left( 2 a \sqrt{y} \right)-1\right)  \right]_{y=0}^{y=a^2} \\
             &amp;= \log \left(2 a^2\right) -1 \,.
\end{split}
\]</span> This is negative for <span class="math inline">\(0 &lt; a &lt; \sqrt{e/2}\approx 1.1658\)</span>. As expected <span class="math inline">\(H( F_y ) \neq H( F_x )\)</span> as differential entropy is not invariant against variable transformations.</p>
</div>
</section>
<section id="models-with-multiple-parameters" class="level3">
<h3 class="anchored" data-anchor-id="models-with-multiple-parameters">Models with multiple parameters</h3>
<div id="exm-entropycategorical" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.8</strong></span> The Shannon entropy of the categorical distribution <span class="math inline">\(P\)</span> with <span class="math inline">\(K\)</span> categories with class probabilities <span class="math inline">\(p_1, \ldots, p_K\)</span> is <span class="math display">\[
H(P) = - \sum_{k=1}^{K } \log(p_k)\, p_k
\]</span></p>
<p>As <span class="math inline">\(P\)</span> is discrete <span class="math inline">\(H(P)\)</span> is bounded below by 0. Furthermore, it is also bounded above by <span class="math inline">\(\log K\)</span>. This can be seen by maximising Shannon entropy with regard to the <span class="math inline">\(p_k\)</span> under the constraint <span class="math inline">\(\sum_{k=1}^K p_k= 1\)</span>, e.g., by constrained optimisation using Lagrange multipliers. Hence for a categorical distribution <span class="math inline">\(P\)</span> with <span class="math inline">\(K\)</span> categories we have <span class="math display">\[
0 \leq  H(P) \leq \log K
\]</span> The maximum is achieved for the discrete uniform distribution (<a href="#exm-entropyuniform" class="quarto-xref">Example&nbsp;<span>2.9</span></a>) and the minimum for a concentrated categorical distribution (<a href="#exm-entropyconcentrated" class="quarto-xref">Example&nbsp;<span>2.10</span></a>).</p>
</div>
<div id="exm-entropyuniform" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.9</strong></span> Entropy for the discrete uniform distribution <span class="math inline">\(U_K\)</span>:</p>
<p>Let <span class="math inline">\(p_1=p_2= \ldots = p_K = \frac{1}{K}\)</span>. Then <span class="math display">\[H(U_K) = - \sum_{k=1}^{K}\log\left(\frac{1}{K}\right)\, \frac{1}{K}  = \log K\]</span></p>
<p>Note that <span class="math inline">\(\log K\)</span> is the largest value the Shannon entropy can assume with <span class="math inline">\(K\)</span> classes and indicates maximum spread of probability mass.</p>
</div>
<div id="exm-entropyconcentrated" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.10</strong></span> Entropy for a categorical distribution with concentrated probability mass:</p>
<p>Let <span class="math inline">\(p_1=1\)</span> and <span class="math inline">\(p_2=p_3=\ldots=p_K=0\)</span>. Using <span class="math inline">\(0\times\log(0)=0\)</span> we obtain for the Shannon entropy <span class="math display">\[H(P) = \log(1)\times 1 + \log(0)\times 0 + \dots = 0\]</span></p>
<p>Note that 0 is the smallest value that Shannon entropy can assume and that it corresponds to maximum concentration of probability mass.</p>
</div>
<div id="exm-entropynormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.11</strong></span> Differential entropy of the normal distribution:</p>
<p>The log density of the univariate normal <span class="math inline">\(N(\mu, \sigma^2)\)</span> distribution is <span class="math inline">\(\log p(x |\mu, \sigma^2) = -\frac{1}{2} \left(  \log(2\pi\sigma^2)  + \frac{(x-\mu)^2}{\sigma^2} \right)\)</span> with <span class="math inline">\(\sigma^2 &gt; 0\)</span>. The corresponding differential entropy is with <span class="math inline">\(\text{E}((x-\mu)^2) = \sigma^2\)</span> <span class="math display">\[
\begin{split}
H(P) &amp; = -\text{E}\left( \log p(x |\mu, \sigma^2) \right)\\
&amp; = \frac{1}{2} \left( \log(2 \pi \sigma^2)+1\right) \,. \\
\end{split}
\]</span> Note that <span class="math inline">\(H(P)\)</span> only depends on the variance parameter and not on the mean parameter. This intuitively clear as only the variance controls the concentration of the probability mass. The entropy grows with the variance as the probability mass becomes more spread out and less concentrated around the mean. For <span class="math inline">\(\sigma^2 &lt; 1/(2 \pi e) \approx 0.0585\)</span> the differential entropy is negative.</p>
</div>
<div id="exm-entropymultinomial" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.12</strong></span> <span class="math inline">\({\color{Red} \blacktriangleright}\)</span> Entropy of a categorical distribution and the multinomial coefficient:</p>
<p>Let <span class="math inline">\(\hat{Q}\)</span> be the empirical categorical distribution with <span class="math inline">\(\hat{q}_k = n_k/n\)</span> the observed frequencies with <span class="math inline">\(n_k\)</span> counts in class <span class="math inline">\(k\)</span> and <span class="math inline">\(n=\sum_{k=1}^K\)</span> total counts.</p>
<p>The number of possible permutation of <span class="math inline">\(n\)</span> items of <span class="math inline">\(K\)</span> distinct types is given by the multinomial coefficient <span class="math display">\[
W = \binom{n}{n_1, \ldots, n_K} = \frac {n!}{n_1! \times n_2! \times\ldots \times n_K! }
\]</span></p>
<p>It turns out that for large <span class="math inline">\(n\)</span> both quantities are directly linked: <span class="math display">\[
H(\hat{Q})  \approx \frac{1}{n} \log W
\]</span></p>
<p>Recall the Moivre-Sterling formula which for large <span class="math inline">\(n\)</span> allow to approximate the factorial by <span class="math display">\[
\log n! \approx  n \log n  -n
\]</span> With this <span class="math display">\[
\begin{split}
\log W &amp;= \log n! - \sum_{k=1}^K \log n_k!\\
&amp; \approx    n \log n  -n - \sum_{k=1}^K (n_k \log n_k  -n_k) \\
&amp; = \sum_{k=1}^K n_k \log n - \sum_{k=1}^K n_k \log n_k\\
&amp; = - n \sum_{k=1}^K \frac{n_k}{n} \log\left( \frac{n_k}{n} \right)\\
&amp; = -n \sum_{k=1}^K \log (\hat{q}_k) \, \hat{q}_k  
\end{split}
\]</span></p>
<p>The above combinatorial derivation of entropy is one of the cornerstones of statistical mechanics and is credited to Boltzmann (1877) and Gibbs (1878). The number of elements <span class="math inline">\(n_1, \ldots, n_K\)</span> in each of the <span class="math inline">\(K\)</span> classes corresponds to the macrostate and any of the <span class="math inline">\(W\)</span> different allocations of the <span class="math inline">\(n\)</span> elements to the <span class="math inline">\(K\)</span> classes to an underlying microstate. The multinomial coefficient, and hence entropy, is largest when there are only small differences (or none) among the <span class="math inline">\(n_i\)</span>, i.e.&nbsp;when samples are equally spread across the <span class="math inline">\(K\)</span> bins.</p>
<p>In statistics the above derivation of entropy was rediscovered by Wallis (1962).</p>
</div>
</section>
</section>
<section id="colorred-blacktriangleright-maximum-entropy-principle-to-characterise-distributions" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="colorred-blacktriangleright-maximum-entropy-principle-to-characterise-distributions"><span class="header-section-number">2.4</span> <span class="math inline">\({\color{Red} \blacktriangleright}\)</span> Maximum entropy principle to characterise distributions</h2>
<p>Both Shannon entropy and differential entropy are useful to characterise distributions:</p>
<p>As seen in the examples above, <strong>large entropy</strong> implies that the <strong>distribution is spread out</strong> whereas <strong>small entropy</strong> indicates that the <strong>distribution is concentrated</strong>.</p>
<p>Correspondingly, <strong>maximum entropy distributions</strong> can be considered <strong>minimally informative</strong> about a random variable. The higher the entropy the more spread out (and hence more uninformative) the distribution. Conversely, low entropy implies that the probability mass is concentrated and thus the distribution is more informative about the random variable.</p>
<p>Examples:</p>
<ol type="1">
<li><p>The <strong>discrete uniform distribution</strong> is the <strong>maximum entropy distribution</strong> among all discrete distributions.</p></li>
<li><p>the maximum entropy distribution of a continuous random variable with support <span class="math inline">\([-\infty, \infty]\)</span> with a specific mean and variance is the <strong>normal distribution</strong>.</p></li>
<li><p>the maximum entropy distribution among all continuous distributions supported in <span class="math inline">\([0, \infty]\)</span> with a specified mean is the <strong>exponential distribution</strong>.</p></li>
</ol>
<p>Using maximum entropy to characterise maximally uninformative distributions was advocated by <a href="https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes">Edwin T. Jaynes (1922–1998)</a> (who also proposed to use maximum entropy in the context of finding Bayesian priors). The maximum entropy principle in statistical physics goes back to Boltzmann.</p>
<p>A list of maximum entropy distribution is given here: <a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution" class="uri">https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution</a>.</p>
<p>Many distributions commonly used in statistical modelling are exponential families. Intriguingly, these distribution are all maximum entropy distributions, so there is a very close link between the principle of maximum entropy and common model choices in statistics and machine learning.</p>
</section>
<section id="cross-entropy-and-kl-divergence" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="cross-entropy-and-kl-divergence"><span class="header-section-number">2.5</span> Cross-entropy and KL divergence</h2>
<section id="definition-of-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="definition-of-cross-entropy">Definition of cross-entropy</h3>
<p>If we modify the definition of entropy such that the expectation is taken with regard to a different distribution <span class="math inline">\(Q\)</span> we arrive at the <strong>cross-entropy</strong><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <span class="math display">\[
\begin{split}
H(Q, P) &amp; =\text{E}_Q\left( S(x, P) \right)\\
&amp; = -\text{E}_Q\left( \log p(x)  \right)\\
\end{split}
\]</span> i.e.&nbsp;the expected logarithmic score when the data are generated by <span class="math inline">\(Q\)</span> and model <span class="math inline">\(P\)</span> is evaluated on the observations. Thus, cross-entropy is a functional of two distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>.</p>
<p>For two discrete distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> with probability mass functions <span class="math inline">\(q(x)\)</span> and <span class="math inline">\(p(x)\)</span> with <span class="math inline">\(x\in \Omega\)</span> the cross-entropy is computed as the weighted sum <span class="math display">\[
H(Q, P) = - \sum_{x \in \Omega}  \log p(x) \, q(x)
\]</span> It can be interpreted as the expected cost or expected code length when when the data are generated according to model <span class="math inline">\(Q\)</span> and but we use model <span class="math inline">\(P\)</span> to describe the data.</p>
<p>For two continuous distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> with densities <span class="math inline">\(q(x)\)</span> and <span class="math inline">\(p(x)\)</span> we compute the integral <span class="math display">\[H(Q, P) =- \int_x  \log p(x)\, q(x) \, dx\]</span></p>
<p>Note that</p>
<ul>
<li>Cross-entropy is not symmetric with regard to <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>, because the expectation is taken with reference to <span class="math inline">\(Q\)</span>.</li>
<li>By construction if both distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are identical cross-entropy reduces to entropy, i.e.&nbsp;<span class="math inline">\(H(Q, Q) = H(Q)\)</span>.</li>
<li>Like entropy <strong>cross-entropy changes under variable transformation</strong> for continuous random variables, say from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span>, hence <span class="math inline">\(H(Q_y, P_y) \neq H(Q_x, P_x)\)</span>.</li>
</ul>
<p>A crucial property of the cross-entropy <span class="math inline">\(H(Q, P)\)</span> is that it is bounded below by the entropy of <span class="math inline">\(Q\)</span>, therefore <span class="math display">\[
H(Q, P) \geq H(Q)
\]</span> with equality only if <span class="math inline">\(Q=P\)</span>. This is known as <a href="https://en.wikipedia.org/wiki/Gibbs%27_inequality"><strong>Gibbs’ inequality</strong></a>. For a proof see Worksheet E1.</p>
<p>Essentially this means that when data are generated under model <span class="math inline">\(Q\)</span> and encoded with model <span class="math inline">\(P\)</span> there is always an extra cost, or penalty, to use model <span class="math inline">\(P\)</span> rather than the correct model <span class="math inline">\(Q\)</span>.</p>
<div id="exm-scoringrulegibbs" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.13</strong></span> <span class="math inline">\({\color{Red} \blacktriangleright}\)</span> Scoring rules and Gibbs’ inequality:</p>
<p>The logarithmic scoring rule <span class="math inline">\(S(x, P) = -\log p(x)\)</span> is called <em>proper</em> because the corresponding expected score, i.e.&nbsp;the cross-entropy <span class="math inline">\(H(Q, P)\)</span>, satisfies the Gibbs’ inequality, and thus the expected score is minimised for <span class="math inline">\(P=Q\)</span>.</p>
</div>
</section>
<section id="definition-and-properties-of-kl-divergence" class="level3">
<h3 class="anchored" data-anchor-id="definition-and-properties-of-kl-divergence">Definition and properties of KL divergence</h3>
<p>The <strong>KL divergence</strong> is defined as <span class="math display">\[
\begin{split}
D_{\text{KL}}(Q,P)  &amp;= H(Q, P)-H(Q) \\
&amp; = \text{E}_Q  \left(  S(x, P) - S(x, Q)  \right) \\
&amp; = \text{E}_Q\log\left(\frac{q(x)}{p(x)}\right) \\
\end{split}
\]</span> Hence, KL divergence <span class="math inline">\(D_{\text{KL}}(Q,P)\)</span> is simply a recalibrated cross-entropy, but it is arguably more fundamental than both entropy and cross-entropy.</p>
<p>The KL divergence <span class="math inline">\(D_{\text{KL}}(Q,P)\)</span> is the expected difference in logarithmic scores when the data are generated by <span class="math inline">\(Q\)</span> and models <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> are evaluated on the observations. <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> can be interpreted as the additional cost if <span class="math inline">\(P\)</span> is used instead <span class="math inline">\(Q\)</span> to describe data from <span class="math inline">\(Q\)</span>. If <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are identical there is no extra cost and <span class="math inline">\(D_{\text{KL}}(Q,P)=0\)</span>. Conversely, if they are not identical then there is an additional cost and <span class="math inline">\(D_{\text{KL}}(Q,P)&gt; 0\)</span>.</p>
<p><span class="math inline">\(D_{\text{KL}}(Q,P)\)</span> thus serves as a measure of the <strong>divergence</strong><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> of distribution <span class="math inline">\(P\)</span> from distribution <span class="math inline">\(Q\)</span>. The use of the term “divergence” rather than “distance” is a reminder the distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are not interchangeable in <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span>.</p>
<p>The KL divergence has a number of important properties inherited from cross-entropy:</p>
<ol type="1">
<li><span class="math inline">\(D_{\text{KL}}(Q, P) \neq D_{\text{KL}}(Q, P)\)</span>, i.e.&nbsp;the KL divergence is not symmetric, <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> cannot be interchanged. This follows from the same property of cross-entropy.</li>
<li><span class="math inline">\(D_{\text{KL}}(Q, P)\geq 0\)</span>, follows from Gibbs’ inequality and proof via <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality"><strong>Jensen’s inequality</strong></a>.</li>
<li><span class="math inline">\(D_{\text{KL}}(Q, P) = 0\)</span> if and only if <span class="math inline">\(P=Q\)</span>, i.e., the KL divergence is zero if and only if <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are identical. Also follows from Gibbs’ inequality.</li>
</ol>
<p>For more details and proofs of properties 2 and 3 see Worksheet E1.</p>
</section>
<section id="invariance-property-of-kl-divergence" class="level3">
<h3 class="anchored" data-anchor-id="invariance-property-of-kl-divergence">Invariance property of KL divergence</h3>
<p>A further crucial property of KL divergence is the <strong>invariance property</strong>:</p>
<ol start="4" type="1">
<li><span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> is <strong>invariant under general variable transformations</strong>, with <span class="math inline">\(D_{\text{KL}}(Q_y, P_y) =D_{\text{KL}}(Q_x, P_x)\)</span> under a change of variables from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span>.</li>
</ol>
<p>Thus, KL divergence does not change when the sample space is reparameterised.</p>
<p>In the definition of KL divergence the expectation is taken over a <em>ratio of densities</em> (or ratio of probabilities for discrete random variables). This creates the invariance under variable transformation as the Jacobian determinant that changes both densities cancel out.</p>
<p>For more details and proof of the invariance property see Worksheet E1.</p>
</section>
<section id="origin-of-kl-divergence-and-naming-conventions" class="level3">
<h3 class="anchored" data-anchor-id="origin-of-kl-divergence-and-naming-conventions">Origin of KL divergence and naming conventions</h3>
<p>Historically, KL divergence was first discovered by Boltzmann (1878) <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> in physics in a discrete setting (see <a href="#exm-empiricalcatkl" class="quarto-xref">Example&nbsp;<span>2.20</span></a>).</p>
<p>In statistics and information theory it was introduced by Kullback and Leibler (1951) <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> but note that Good (1979) <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> credits Turing with the first statistical application in 1940/1941 in the field of cryptography.</p>
<p>The KL divergence is also known as <strong>KL information</strong> or <strong>KL information number</strong> named after two of the original authors (Kullback and Leibler) who themselves referred to this quantity as <strong>discrimination information</strong>. Another common name is <strong>information divergence</strong> or short <strong><span class="math inline">\(\symbfit I\)</span>-divergence</strong>. Some authors (e.g.&nbsp;Efron) call twice the KL divergence <span class="math inline">\(2 D_{\text{KL}}(Q, P) = D(Q, P)\)</span> the <strong>deviance</strong> of <span class="math inline">\(P\)</span> from <span class="math inline">\(Q\)</span>. In the more general context of scoring rules the divergence is also called <strong>discrepancy</strong>. Furthermore, KL divergence is also very frequently called <strong>relative entropy</strong>. However, especially in older literature the KL divergence is referred to as “cross-entropy” but this use is discouraged to avoid confusion with the related but distinct definition of cross-entropy above.</p>
<p>Perhaps it should be called Boltzmann-Turing-Kullback-Leibler information divergence or short BTKL divergence.</p>
<p>There also exist various notations for KL divergence in the literature. Here we use <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> but you often find as well <span class="math inline">\(\text{KL}(Q || P)\)</span> and <span class="math inline">\(I^{KL}(Q; P)\)</span>.</p>
</section>
<section id="application-in-statistics" class="level3">
<h3 class="anchored" data-anchor-id="application-in-statistics">Application in statistics</h3>
<p>In statistics the typical roles of the distribution <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> in <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> are:</p>
<ul>
<li><span class="math inline">\(Q\)</span> is the (unknown) underlying true model for the data generating process</li>
<li><span class="math inline">\(P\)</span> is the approximating model (typically a parametric distribution family)</li>
</ul>
<p>Optimising (i.e.&nbsp;minimising) the KL divergence with regard to <span class="math inline">\(P\)</span> amounts to <em>approximation</em> and optimising with regard to <span class="math inline">\(Q\)</span> to <em>imputation</em>. Later we will see how this leads to the method of maximum likelihood and to Bayesian learning, respectively.</p>
</section>
</section>
<section id="cross-entropy-and-kl-divergence-examples" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="cross-entropy-and-kl-divergence-examples"><span class="header-section-number">2.6</span> Cross-entropy and KL divergence examples</h2>
<section id="models-with-a-single-parameter" class="level3">
<h3 class="anchored" data-anchor-id="models-with-a-single-parameter">Models with a single parameter</h3>
<div id="exm-klbernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.14</strong></span> KL divergence between two Bernoulli distributions <span class="math inline">\(\text{Ber}(\theta_1)\)</span> and <span class="math inline">\(\text{Ber}(\theta_2)\)</span>:</p>
<p>The “success” probabilities for the two distributions are <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, respectively, and the complementary “failure” probabilities are <span class="math inline">\(1-\theta_1\)</span> and <span class="math inline">\(1-\theta_2\)</span>. With this we get for the KL divergence <span class="math display">\[
D_{\text{KL}}(\text{Ber}(\theta_1), \text{Ber}(\theta_2))=\theta_1 \log\left( \frac{\theta_1}{\theta_2}\right) + (1-\theta_1) \log\left(\frac{1-\theta_1}{1-\theta_2}\right)
\]</span></p>
</div>
<div id="exm-klnormalequalvar" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.15</strong></span> KL divergence between two univariate normals with different means and common variance:</p>
<p>Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2)\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>.</p>
<p>Then we get <span class="math display">\[D_{\text{KL}}(F_{\text{ref}}, F )=\frac{1}{2} \left(\frac{(\mu-\mu_{\text{ref}})^2}{\sigma^2}\right)\]</span></p>
<p>Thus, the squared Euclidean distance is a special case of KL divergence. Note that in this case the KL divergence is symmetric.</p>
</div>
</section>
<section id="models-with-multiple-parameters-1" class="level3">
<h3 class="anchored" data-anchor-id="models-with-multiple-parameters-1">Models with multiple parameters</h3>
<div id="exm-catkl" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.16</strong></span> KL divergence between two categorical distributions with <span class="math inline">\(K\)</span> classes:</p>
<p>With <span class="math inline">\(Q=\text{Cat}(\symbfit q)\)</span> and <span class="math inline">\(P=\text{Cat}(\symbfit p)\)</span> and corresponding probabilities <span class="math inline">\(q_1,\dots,q_K\)</span> and <span class="math inline">\(p_1,\dots,p_K\)</span> satisfying <span class="math inline">\(\sum_{i=1}^K q_i = 1\)</span> and <span class="math inline">\(\sum_{i=1}^K p_i =1\)</span> we get:</p>
<p><span class="math display">\[\begin{equation*}
D_{\text{KL}}(Q, P)=\sum_{i=1}^K q_i\log\left(\frac{q_i}{p_i}\right)
\end{equation*}\]</span></p>
<p>To be explicit that there are only <span class="math inline">\(K-1\)</span> parameters in a categorical distribution we can also write <span class="math display">\[\begin{equation*}
D_{\text{KL}}(Q, P)=\sum_{i=1}^{K-1} q_i\log\left(\frac{q_i}{p_i}\right)  + q_K\log\left(\frac{q_K}{p_K}\right)
\end{equation*}\]</span> with <span class="math inline">\(q_K=\left(1- \sum_{i=1}^{K-1} q_i\right)\)</span> and <span class="math inline">\(p_K=\left(1- \sum_{i=1}^{K-1} p_i\right)\)</span>.</p>
</div>
<div id="exm-crossentropynormals" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.17</strong></span> Cross-entropy between two normals:</p>
<p>Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2_{\text{ref}})\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>. The cross-entropy <span class="math inline">\(H(F_{\text{ref}}, F)\)</span> is <span class="math display">\[
\begin{split}
H(F_{\text{ref}}, F) &amp;=  -\text{E}_{F_{\text{ref}}} \left( \log p(x |\mu, \sigma^2) \right)\\
&amp;=  \frac{1}{2}  \text{E}_{F_{\text{ref}}} \left(  \log(2\pi\sigma^2)  + \frac{(x-\mu)^2}{\sigma^2} \right) \\
&amp;= \frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 }
+\frac{\sigma^2_{\text{ref}}}{\sigma^2}  +\log(2 \pi \sigma^2) \right)  \\
\end{split}
\]</span> using <span class="math inline">\(\text{E}_{F_{\text{ref}}} ((x-\mu)^2) = (\mu_{\text{ref}}-\mu)^2 + \sigma^2_{\text{ref}}\)</span>.</p>
</div>
<div id="exm-crossentropylowerbound" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.18</strong></span> Entropy as lower bound of cross-entropy:</p>
<p>If <span class="math inline">\(\mu_{\text{ref}} = \mu\)</span> and <span class="math inline">\(\sigma^2_{\text{ref}} = \sigma^2\)</span> then the cross-entropy <span class="math inline">\(H(F_{\text{ref}},F)\)</span> in <a href="#exm-crossentropynormals" class="quarto-xref">Example&nbsp;<span>2.17</span></a> degenerates to the differential entropy <span class="math inline">\(H(F_{\text{ref}}) = \frac{1}{2} \left(\log( 2 \pi \sigma^2_{\text{ref}}) +1 \right)\)</span>.</p>
<p>See also <a href="#exm-entropynormal" class="quarto-xref">Example&nbsp;<span>2.11</span></a>.</p>
</div>
<div id="exm-klnormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.19</strong></span> KL divergence between two univariate normals with different means and variances:</p>
<p>Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2_{\text{ref}})\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>. Then <span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\text{ref}},F) &amp;= H(F_{\text{ref}}, F) - H(F_{\text{ref}}) \\
&amp;= \frac{1}{2} \left(   \frac{(\mu-\mu_{\text{ref}})^2}{\sigma^2}  + \frac{\sigma_{\text{ref}}^2}{\sigma^2}
-\log\left(\frac{\sigma_{\text{ref}}^2}{\sigma^2}\right)-1  
   \right) \\
\end{split}
\]</span></p>
<p>If variances are equal then we recover the previous <a href="#exm-klnormalequalvar" class="quarto-xref">Example&nbsp;<span>2.15</span></a> as special case.</p>
</div>
<div id="exm-empiricalcatkl" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.20</strong></span> <span class="math inline">\({\color{Red} \blacktriangleright}\)</span> KL divergence as negative log-probability:</p>
<p>Assume <span class="math inline">\(\hat{Q}\)</span> is an empirical categorical distribution based on observed counts <span class="math inline">\(n_k\)</span> (see <a href="#exm-entropymultinomial" class="quarto-xref">Example&nbsp;<span>2.12</span></a>) and <span class="math inline">\(P\)</span> is a second categorical distribution.</p>
<p>The KL divergence is then <span class="math display">\[
\begin{split}
D_{\text{KL}}(\hat{Q}, P) &amp; =H(\hat{Q}, P) - H(\hat{Q})\\
&amp; = -\sum_{i=1}^K  \log ( p_i) \, \hat{q}_i  - H(\hat{Q})\\
&amp; = -\frac{1}{n} \sum_{i=1}^K n_i  \log p_i  - H(\hat{Q})\\
\end{split}
\]</span></p>
<p>For large <span class="math inline">\(n\)</span> we may use the multinomial coefficient <span class="math inline">\(W = \binom{n}{n_1, \ldots, n_K}\)</span> to obtain the entropy of <span class="math inline">\(\hat{Q}\)</span> (see <a href="#exm-entropymultinomial" class="quarto-xref">Example&nbsp;<span>2.12</span></a>). This results in <span class="math display">\[
\begin{split}
D_{\text{KL}}(\hat{Q}, P) &amp;\approx -\frac{1}{n} \left( \sum_{i=1}^K n_i \log p_i + \log W   \right)\\
&amp; = -\frac{1}{n} \log \left( W \times \prod_{i=1}^K  p_i^{n_i}    \right)\\
&amp; = -\frac{1}{n} \log  \text{Pr}(n_1, \ldots, n_k| \symbfit p) \\
\end{split}
\]</span> Hence the KL divergence is directly linked to the multinomial probability of the observed counts <span class="math inline">\(n_1, \ldots, n_k\)</span> under the model <span class="math inline">\(P\)</span>. This derivation of KL divergence as negative log-probability of a macrostate is due to Boltzmann (1878).</p>
</div>
</section>
</section>
<section id="expected-fisher-information" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="expected-fisher-information"><span class="header-section-number">2.7</span> Expected Fisher information</h2>
<section id="definition-of-expected-fisher-information" class="level3">
<h3 class="anchored" data-anchor-id="definition-of-expected-fisher-information">Definition of expected Fisher information</h3>
<p>KL information measures the divergence of two distributions. Previously we have seen examples of KL divergence between two distributions belonging to the same family. We now consider the KL divergence of two such distributions separated in parameter space only by some small <span class="math inline">\(\symbfit \varepsilon\)</span>.</p>
<p>Specifically, we consider the function <span class="math display">\[h(\symbfit \varepsilon) = D_{\text{KL}}(F_{\symbfit \theta}, F_{\symbfit \theta+\symbfit \varepsilon})  = \text{E}_{F_{\symbfit \theta}}\left(  \log f(\symbfit x| \symbfit \theta)  - \log f(\symbfit x| \symbfit \theta+\symbfit \varepsilon)   \right)\]</span> where <span class="math inline">\(\symbfit \theta\)</span> is kept constant and <span class="math inline">\(\symbfit \varepsilon\)</span> is varying. Assuming that <span class="math inline">\(f(\symbfit x| \symbfit \theta)\)</span> is twice differentiable with regard to <span class="math inline">\(\symbfit \theta\)</span> we can approximate <span class="math inline">\(h(\symbfit \varepsilon)\)</span> quadratically by <span class="math inline">\(h(\symbfit \varepsilon) \approx h(0) + \nabla h(0)^T\symbfit \varepsilon+ \frac{1}{2} \symbfit \varepsilon^T \, \nabla \nabla^T h(0) \,\symbfit \varepsilon\)</span>.</p>
<p>From the properties of the KL divergence we know that <span class="math inline">\(D_{\text{KL}}(F_{\symbfit \theta}, F_{\symbfit \theta+\symbfit \varepsilon})\geq 0\)</span> and that it becomes zero only if <span class="math inline">\(\symbfit \varepsilon=0\)</span>. Thus, by construction the function <span class="math inline">\(h(\symbfit \varepsilon)\)</span> achieves a true minimum at <span class="math inline">\(\symbfit \varepsilon=0\)</span> (with <span class="math inline">\(h(0)=0\)</span>), has a vanishing gradient at <span class="math inline">\(\symbfit \varepsilon=0\)</span> and a positive definite Hessian matrix at <span class="math inline">\(\symbfit \varepsilon=0\)</span>. Therefore in the quadratic approximation of <span class="math inline">\(h(\symbfit \varepsilon)\)</span> around <span class="math inline">\(\symbfit \varepsilon=0\)</span> above the first two terms (constant and linear) vanish and only the quadratic term remains: <span class="math display">\[
h(\symbfit \varepsilon) \approx \frac{1}{2} \symbfit \varepsilon^T \, \nabla \nabla^T h(0) \, \symbfit \varepsilon
\]</span> The Hessian matrix of <span class="math inline">\(h(\symbfit \varepsilon)\)</span> evaluated at <span class="math inline">\(\symbfit \varepsilon=0\)</span> is the negative expected Hessian matrix of the log-density at <span class="math inline">\(\symbfit \theta\)</span> <span class="math display">\[\symbfit I^{\text{Fisher}}(\symbfit \theta) = \nabla \nabla^T h(0) =  -\text{E}_{F_{\symbfit \theta}} \nabla \nabla^T  \log f(\symbfit x| \symbfit \theta)\]</span> It is called the <strong>expected Fisher information</strong> at <span class="math inline">\(\symbfit \theta\)</span>, or short <strong>Fisher information</strong>. Hence, the KL divergence can be locally approximated by <span class="math display">\[
D_{\text{KL}}(F_{\symbfit \theta}, F_{\symbfit \theta+\symbfit \varepsilon})\approx \frac{1}{2} \symbfit \varepsilon^T  \symbfit I^{\text{Fisher}}(\symbfit \theta) \symbfit \varepsilon
\]</span></p>
<p>We may also vary the first argument in the KL divergence. It is straightforward to show that this leads to the same approximation to second order in <span class="math inline">\(\symbfit \varepsilon\)</span>: <span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\symbfit \theta+\symbfit \varepsilon}, F_{\symbfit \theta})
&amp;\approx \frac{1}{2}\symbfit \varepsilon^T \symbfit I^{\text{Fisher}}(\symbfit \theta)\, \symbfit \varepsilon\\
\end{split}
\]</span></p>
<p>Hence, the KL divergence, while generally not symmetric in its arguments, is still locally symmetric.</p>
<p>Computing the expected Fisher information involves no observed data, it is purely a property of the model family <span class="math inline">\(F_{\symbfit \theta}\)</span>. In the next chapter we will study a related quantity, the <em>observed Fisher information</em> that in contrast to the expected Fisher information is a function of the observed data.</p>
<div id="exm-fimmetrictensor" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.21</strong></span> <span class="math inline">\({\color{Red} \blacktriangleright}\)</span> Fisher information as metric tensor:</p>
<p>In the field of <em>information geometry</em><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> sets of distributions are studied using tools from differential geometry. It turns out that distribution families are manifolds and that the expected Fisher information matrix plays the role of the (symmetric!) <a href="https://en.wikipedia.org/wiki/Fisher_information_metric">metric tensor</a> on this manifold.</p>
</div>
</section>
<section id="sec-additivityfisher" class="level3">
<h3 class="anchored" data-anchor-id="sec-additivityfisher">Additivity of Fisher information</h3>
<p>We may wish to compute the expected Fisher information based on a set of independent identically distributed (iid) random variables.</p>
<p>Assume that a random variable <span class="math inline">\(x \sim F_{\symbfit \theta}\)</span> has log-density <span class="math inline">\(\log f(x| \symbfit \theta)\)</span> and expected Fisher information <span class="math inline">\(\symbfit I^{\text{Fisher}}(\symbfit \theta)\)</span>. The expected Fisher information <span class="math inline">\(\symbfit I_{x_1, \ldots, x_n}^{\text{Fisher}}(\symbfit \theta)\)</span> for a set of iid random variables <span class="math inline">\(x_1, \ldots, x_n \sim F_{\symbfit \theta}\)</span> is computed from the joint log-density <span class="math inline">\(\log f(x_1, \ldots, x_n) = \sum_{i}^n \log f(x_i| \symbfit \theta)\)</span>. This yields <span class="math display">\[
\begin{split}
\symbfit I_{x_1, \ldots, x_n}^{\text{Fisher}}(\symbfit \theta) &amp;= -\text{E}_{F_{\symbfit \theta}} \nabla \nabla^T  \sum_{i}^n \log f(x_i| \symbfit \theta)\\
&amp;= \sum_{i=1}^n  \symbfit I^{\text{Fisher}}(\symbfit \theta) =n  \symbfit I^{\text{Fisher}}(\symbfit \theta) \\
\end{split}
\]</span> Hence, the expected Fisher information for a set of <span class="math inline">\(n\)</span> iid random variables is the <span class="math inline">\(n\)</span> times the Fisher information of a single variable.</p>
</section>
<section id="invariance-property-of-the-fisher-information" class="level3">
<h3 class="anchored" data-anchor-id="invariance-property-of-the-fisher-information">Invariance property of the Fisher information</h3>
<p>Like KL divergence the <strong>expected Fisher information is invariant against change of parameterisation of the sample space</strong>, say from variable <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span> and from distribution <span class="math inline">\(F_x\)</span> to <span class="math inline">\(F_y\)</span>. This is easy to see as the KL divergence itself is invariant against such reparameterisation. Hence the function <span class="math inline">\(h(\symbfit \varepsilon)\)</span> above is invariant and thus also its curvature, and hence the expected Fisher information.</p>
<p>More formally, when the sample space is changed the density gains a factor in the form of the Jacobian determinant for this transformation. However, as this factor does not depend of the model parameters it does not change the first and second derivative of the log-density with regard to the model parameters.</p>
</section>
<section id="sec-covariantfisher" class="level3">
<h3 class="anchored" data-anchor-id="sec-covariantfisher"><span class="math inline">\({\color{Red} \blacktriangleright}\)</span> Transformation of Fisher information when model parameters change</h3>
<p>The Fisher information <span class="math inline">\(\symbfit I^{\text{Fisher}}(\symbfit \theta)\)</span> depends on the parameter <span class="math inline">\(\symbfit \theta\)</span>. If we use a different parameterisation of the underlying parametric distribution family, say <span class="math inline">\(\symbfit \zeta\)</span> with a map <span class="math inline">\(\symbfit \theta(\symbfit \zeta)\)</span> from <span class="math inline">\(\symbfit \zeta\)</span> to <span class="math inline">\(\symbfit \theta\)</span>, then the Fisher information changes according to the chain rule in calculus.</p>
<p>To find the resulting Fisher information in terms of the new parameter <span class="math inline">\(\symbfit \zeta\)</span> we need to use the Jacobian matrix <span class="math inline">\(D \symbfit \theta(\symbfit \zeta)\)</span>. This matrix contains the gradients for each component of the map <span class="math inline">\(\symbfit \theta(\symbfit \zeta)\)</span> in its rows: <span class="math display">\[
D \symbfit \theta(\symbfit \zeta) =
\begin{pmatrix}\nabla^T \theta_1(\symbfit \zeta)\\ \nabla^T \theta_2(\symbfit \zeta) \\ \vdots \\  \end{pmatrix}
\]</span></p>
<p>With the above the Fisher information for <span class="math inline">\(\symbfit \theta\)</span> is then transformed to the Fisher information for <span class="math inline">\(\symbfit \zeta\)</span> applying the chain rule for the Hessian matrix: <span class="math display">\[
\symbfit I^{\text{Fisher}}(\symbfit \zeta)   = (D \symbfit \theta(\symbfit \zeta))^T \, \symbfit I^{\text{Fisher}}(\symbfit \theta) \rvert_{\symbfit \theta= \symbfit \theta(\symbfit \zeta)}  \, D \symbfit \theta(\symbfit \zeta)
\]</span> This type of transformation is also known as <em>covariant transformation</em>, in this case for the Fisher information metric tensor.</p>
</section>
</section>
<section id="expected-fisher-information-examples" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="expected-fisher-information-examples"><span class="header-section-number">2.8</span> Expected Fisher information examples</h2>
<section id="models-with-a-single-parameter-1" class="level3">
<h3 class="anchored" data-anchor-id="models-with-a-single-parameter-1">Models with a single parameter</h3>
<div id="exm-expectedfisherbernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.22</strong></span> Expected Fisher information for the Bernoulli distribution:</p>
<p>The log-probability mass function of the Bernoulli <span class="math inline">\(\text{Ber}(\theta)\)</span> distribution is <span class="math display">\[
\log p(x | \theta) = x \log(\theta) + (1-x) \log(1-\theta)
\]</span> where <span class="math inline">\(\theta\)</span> is the probability of “success”. The second derivative with regard to the parameter <span class="math inline">\(\theta\)</span> is <span class="math display">\[
\frac{d^2}{d\theta^2} \log p(x | \theta)  =  -\frac{x}{\theta^2}-  \frac{1-x}{(1-\theta)^2}
\]</span> Since <span class="math inline">\(\text{E}(x) = \theta\)</span> we get as Fisher information <span class="math display">\[
\begin{split}
I^{\text{Fisher}}(\theta) &amp; = -\text{E}\left(\frac{d^2}{d\theta^2} \log p(x | \theta)  \right)\\
                           &amp;= \frac{\theta}{\theta^2}+  \frac{1-\theta}{(1-\theta)^2} \\
                            &amp;= \frac{1}{\theta(1-\theta)}\\
\end{split}
\]</span></p>
</div>
<div id="exm-quadapproxklbernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.23</strong></span> Quadratic approximations of the KL divergence between two Bernoulli distributions:</p>
<p>From <a href="#exm-klbernoulli" class="quarto-xref">Example&nbsp;<span>2.14</span></a> we have as KL divergence <span class="math display">\[
D_{\text{KL}}\left (\text{Ber}(\theta_1), \text{Ber}(\theta_2) \right)=\theta_1 \log\left( \frac{\theta_1}{\theta_2}\right) + (1-\theta_1) \log\left(\frac{1-\theta_1}{1-\theta_2}\right)
\]</span> and from <a href="#exm-expectedfisherbernoulli" class="quarto-xref">Example&nbsp;<span>2.22</span></a> the corresponding expected Fisher information.</p>
<p>The quadratic approximation implies that <span class="math display">\[
D_{\text{KL}}\left( \text{Ber}(\theta), \text{Ber}(\theta + \varepsilon) \right) \approx \frac{\varepsilon^2}{2}  I^{\text{Fisher}}(\theta) =  \frac{\varepsilon^2}{2 \theta (1-\theta)}
\]</span> and also that <span class="math display">\[
D_{\text{KL}}\left( \text{Ber}(\theta+\varepsilon), \text{Ber}(\theta) \right) \approx \frac{\varepsilon^2}{2} I^{\text{Fisher}}(\theta) =  \frac{\varepsilon^2}{2 \theta (1-\theta)}
\]</span></p>
<p>In Worksheet E1 this is verified by using a second order Taylor series applied to the KL divergence.</p>
</div>
<div id="exm-expectedfishernormknownvar" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.24</strong></span> Expected Fisher information for the normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span> with known variance.</p>
<p>The log-density is <span class="math display">\[
\log f(x | \mu, \sigma^2) = -\frac{1}{2} \log(\sigma^2)
-\frac{1}{2 \sigma^2} (x-\mu)^2 - \frac{1}{2}\log(2 \pi)
\]</span> The second derivative with respect to <span class="math inline">\(\mu\)</span> is <span class="math display">\[
\frac{d^2}{d\mu^2} \log f(x | \mu, \sigma^2) = -\frac{1}{\sigma^2}
\]</span> Therefore the expected Fisher information is <span class="math display">\[
\symbfit I^{\text{Fisher}}\left(\mu\right) = \frac{1}{\sigma^2}
\]</span></p>
</div>
</section>
<section id="models-with-multiple-parameters-2" class="level3">
<h3 class="anchored" data-anchor-id="models-with-multiple-parameters-2">Models with multiple parameters</h3>
<div id="exm-expectedfishernormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.25</strong></span> Expected Fisher information for the normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<p>The log-density is <span class="math display">\[
\log f(x | \mu, \sigma^2) = -\frac{1}{2} \log(\sigma^2)
-\frac{1}{2 \sigma^2} (x-\mu)^2 - \frac{1}{2}\log(2 \pi)
\]</span> The gradient with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> (!) is the vector <span class="math display">\[
\nabla \log f(x | \mu, \sigma^2) =
\begin{pmatrix}
\frac{1}{\sigma^2} (x-\mu) \\
- \frac{1}{2 \sigma^2} + \frac{1}{2 \sigma^4} (x- \mu)^2 \\
\end{pmatrix}
\]</span> Hint for calculating the gradient: replace <span class="math inline">\(\sigma^2\)</span> by <span class="math inline">\(v\)</span> and then take the partial derivative with regard to <span class="math inline">\(v\)</span>, then substitute back.</p>
<p>The corresponding Hessian matrix is <span class="math display">\[
\nabla \nabla^T \log f(x | \mu, \sigma^2) =
\begin{pmatrix}
-\frac{1}{\sigma^2} &amp; -\frac{1}{\sigma^4} (x-\mu)\\
-\frac{1}{\sigma^4} (x-\mu) &amp;  \frac{1}{2\sigma^4} - \frac{1}{\sigma^6}(x- \mu)^2 \\
\end{pmatrix}
\]</span> As <span class="math inline">\(\text{E}(x) = \mu\)</span> we have <span class="math inline">\(\text{E}(x-\mu) =0\)</span>. Furthermore, with <span class="math inline">\(\text{E}( (x-\mu)^2 ) =\sigma^2\)</span> we see that <span class="math inline">\(\text{E}\left(\frac{1}{\sigma^6}(x- \mu)^2\right) = \frac{1}{\sigma^4}\)</span>. Therefore the expected Fisher information matrix as the negative expected Hessian matrix is <span class="math display">\[
\symbfit I^{\text{Fisher}}\left(\mu,\sigma^2\right) = \begin{pmatrix} \frac{1}{\sigma^2} &amp; 0 \\ 0 &amp; \frac{1}{2\sigma^4} \end{pmatrix}
\]</span></p>
</div>
<div id="exm-catexpectfisher" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.26</strong></span> <span class="math inline">\({\color{Red} \blacktriangleright}\)</span> Expected Fisher information of the categorical distribution:</p>
<p>The log-probability mass function for the categorical distribution with <span class="math inline">\(K\)</span> classes and <span class="math inline">\(K-1\)</span> free parameters <span class="math inline">\(\pi_1, \ldots, \pi_{K-1}\)</span> is <span class="math display">\[
\begin{split}
\log p(\symbfit x| \pi_1, \ldots, \pi_{K-1}  ) &amp; =\sum_{k=1}^{K-1}  x_k \log \pi_k    + x_K \log \pi_K \\
&amp; =\sum_{k=1}^{K-1}  x_k \log \pi_k    + \left( 1 - \sum_{k=1}^{K-1} x_k  \right) \log \left( 1 - \sum_{k=1}^{K-1} \pi_k \right) \\
\end{split}
\]</span></p>
<p>From the log-probability mass function we compute the Hessian matrix of second order partial derivatives <span class="math inline">\(\nabla \nabla^T \log p(\symbfit x| \pi_1, \ldots, \pi_{K-1} )\)</span> with regard to <span class="math inline">\(\pi_1, \ldots, \pi_{K-1}\)</span>:</p>
<ul>
<li><p>The diagonal entries of the Hessian matrix (with <span class="math inline">\(i=1, \ldots, K-1\)</span>) are <span class="math display">\[
\frac{\partial^2}{\partial \pi_i^2} \log p(\symbfit x|\pi_1, \ldots, \pi_{K-1}) =
-\frac{x_i}{\pi_i^2}-\frac{x_K}{\pi_K^2}
\]</span></p></li>
<li><p>the off-diagonal entries are (with <span class="math inline">\(j=1, \ldots, K-1\)</span> and <span class="math inline">\(j \neq i\)</span>) <span class="math display">\[
\frac{\partial^2}{\partial \pi_i \partial \pi_j} \log p(\symbfit x|\pi_1, \ldots, \pi_{K-1}) =
-\frac{ x_K}{\pi_K^2}
\]</span></p></li>
</ul>
<p>Recalling that <span class="math inline">\(\text{E}(x_i) = \pi_i\)</span> we obtain the expected Fisher information matrix for a categorical distribution as <span class="math inline">\(K-1  \times K-1\)</span> dimensional matrix <span class="math display">\[
\begin{split}
\symbfit I^{\text{Fisher}}\left( \pi_1, \ldots, \pi_{K-1}  \right) &amp;=
-\text{E}\left( \nabla \nabla^T \log p(\symbfit x| \pi_1, \ldots, \pi_{K-1}) \right) \\
&amp; =
\begin{pmatrix}
\frac{1}{\pi_1} + \frac{1}{\pi_K} &amp; \cdots &amp; \frac{1}{\pi_K} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{1}{\pi_K} &amp; \cdots &amp; \frac{1}{\pi_{K-1}} + \frac{1}{\pi_K} \\
\end{pmatrix}\\
&amp; = \text{Diag}\left( \frac{1}{\pi_1} , \ldots,  \frac{1}{\pi_{K-1}}   \right) + \frac{1}{\pi_K} \symbfup 1\\
\end{split}
\]</span></p>
<p>For <span class="math inline">\(K=2\)</span> and <span class="math inline">\(\pi_1=\theta\)</span> this reduces to the expected Fisher information of a Bernoulli variable, see <a href="#exm-expectedfisherbernoulli" class="quarto-xref">Example&nbsp;<span>2.22</span></a>. <span class="math display">\[
\begin{split}
I^{\text{Fisher}}(\theta) &amp; =  \left(\frac{1}{\theta} + \frac{1}{1-\theta} \right) \\
  &amp;= \frac{1}{\theta (1-\theta)} \\
\end{split}
\]</span></p>
</div>
<div id="exm-catquadapproxkl" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.27</strong></span> <span class="math inline">\({\color{Red} \blacktriangleright}\)</span> Quadratic approximation of KL divergence of the categorical distribution and the Neyman and Pearson divergence:</p>
<p>We now consider the local approximation of the KL divergence <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> between the categorical distribution <span class="math inline">\(Q=\text{Cat}(\symbfit q)\)</span> with probabilities <span class="math inline">\(\symbfit q=(q_1, \ldots, q_K)^T\)</span> with the categorical distribution <span class="math inline">\(P=\text{Cat}(\symbfit p)\)</span> with probabilities <span class="math inline">\(\symbfit p= (p_1, \ldots, p_K)^T\)</span>.</p>
<p>From <a href="#exm-catkl" class="quarto-xref">Example&nbsp;<span>2.16</span></a> we already know the KL divergence and from <a href="#exm-catexpectfisher" class="quarto-xref">Example&nbsp;<span>2.26</span></a> the corresponding expected Fisher information.</p>
<p>First, we keep the first argument <span class="math inline">\(Q\)</span> fixed and assume that <span class="math inline">\(P\)</span> is a perturbed version of <span class="math inline">\(Q\)</span> with <span class="math inline">\(\symbfit p= \symbfit q+\symbfit \varepsilon\)</span>. Note that the perturbations <span class="math inline">\(\symbfit \varepsilon=(\varepsilon_1, \ldots, \varepsilon_K)^T\)</span> satisfy <span class="math inline">\(\sum_{k=1}^K \varepsilon_k = 0\)</span> because <span class="math inline">\(\sum_{k=1}^K q_i=1\)</span> and <span class="math inline">\(\sum_{k=1}^K p_i=1\)</span>. Thus <span class="math inline">\(\varepsilon_K = -\sum_{k=1}^{K-1} \varepsilon_k\)</span>. Then <span class="math display">\[
\begin{split}
D_{\text{KL}}(\text{Cat}(\symbfit q), \text{Cat}(\symbfit q+\symbfit \varepsilon))
&amp;  \approx \frac{1}{2} (\varepsilon_1, \ldots,  \varepsilon_{K-1}) \,
\symbfit I^{\text{Fisher}}\left( q_1, \ldots, q_{K-1}  \right)
\begin{pmatrix} \varepsilon_1 \\ \vdots \\  \varepsilon_{K-1}\\
\end{pmatrix} \\
&amp;= \frac{1}{2} \left( \sum_{k=1}^{K-1} \frac{\varepsilon_k^2}{q_k}   + \frac{ \left(\sum_{k=1}^{K-1} \varepsilon_k\right)^2}{q_K} \right)  \\
&amp;= \frac{1}{2}  \sum_{k=1}^{K} \frac{\varepsilon_k^2}{q_k}\\
&amp;= \frac{1}{2}  \sum_{k=1}^{K} \frac{(q_k-p_k)^2}{q_k}\\
&amp; = \frac{1}{2} D_{\text{Neyman}}(Q, P)\\
\end{split}
\]</span> Similarly, if we keep <span class="math inline">\(P\)</span> fixed and consider <span class="math inline">\(Q\)</span> as a perturbed version of <span class="math inline">\(P\)</span> we get <span class="math display">\[
\begin{split}
D_{\text{KL}}(\text{Cat}(\symbfit p+\symbfit \varepsilon), \text{Cat}(\symbfit p))
&amp;\approx \frac{1}{2}  \sum_{k=1}^{K} \frac{(q_k-p_k)^2}{p_k}\\
&amp;= \frac{1}{2} D_{\text{Pearson}}(Q, P)
\end{split}
\]</span> Note that in both approximations we divide by the probabilities of the distribution that is kept fixed.</p>
<p>Note the appearance of the <em>Pearson <span class="math inline">\(\chi^2\)</span> divergence</em> and the <em>Neyman <span class="math inline">\(\chi^2\)</span> divergence</em> in the above. Both are, like the KL divergence, part of the family of <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergences</a>. The Neyman <span class="math inline">\(\chi^2\)</span> divergence is also known as the reverse Pearson divergence as <span class="math inline">\(D_{\text{Neyman}}(Q, P) = D_{\text{Pearson}}(P, Q)\)</span>.</p>
</div>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>We use the convention that scoring rules are negatively oriented (e.g.&nbsp;Dawid 2007) with the aim to minimise the score (cost, code length, surprise). However, some authors prefer the positively oriented convention with a reversed sign in the definition of <span class="math inline">\(S(x, P)\)</span> so the score represents a <strong>reward</strong> that is maximised (e.g.&nbsp;Gneiting and Raftery 2007).<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Shannon, C. E. 1948. A mathematical theory of communication. Bell System Technical Journal <strong>27</strong>:379–423. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x" class="uri">https://doi.org/10.1002/j.1538-7305.1948.tb01338.x</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>This follows the current and widely accepted usage of the term cross-entropy. However, in some typically older literature cross-entropy may refer instead to the related but different KL divergence discussed further below.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Note that <a href="https://en.wikipedia.org/wiki/Divergence_(statistics)">divergence between distributions</a> is not related to and should not be confused with the <a href="https://en.wikipedia.org/wiki/Divergence">divergence vector operator</a> used in vector calculus.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Boltzmann, L. 1878. Weitere Bemerkungen über einige Probleme der mechanischen Wärmetheorie. Wien Ber. <strong>78</strong>:7–46. <a href="https://doi.org/10.1017/CBO9781139381437.013" class="uri">https://doi.org/10.1017/CBO9781139381437.013</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Kullback, S., and R. A. Leibler. 1951. On information and sufficiency. Ann. Math. Statist. <strong>22</strong> 79–86. <a href="https://doi.org/10.1214/aoms/1177729694" class="uri">https://doi.org/10.1214/aoms/1177729694</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Good, I. J. 1979. Studies in the history of probability. XXXVII. A. M. Turing’s statistical work in world war II. Biometrika, 66:393–396. <a href="https://doi.org/10.1093/biomet/66.2.393" class="uri">https://doi.org/10.1093/biomet/66.2.393</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>A recent review is given, e.g., in: Nielsen, F. 2020. <em>An elementary introduction to information geometry.</em> Entropy <strong>22</strong>:1100. <a href="https://doi.org/10.3390/e22101100" class="uri">https://doi.org/10.3390/e22101100</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01-likelihood1.html" class="pagination-link" aria-label="Overview of statistical learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Overview of statistical learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03-likelihood3.html" class="pagination-link" aria-label="Maximum likelihood estimation">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>These notes were written by <a href="https://strimmerlab.github.io/korbinian.html">Korbinian Strimmer</a> using <a href="https://quarto.org">Quarto</a>,</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>