<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>12&nbsp; Optimality properties and conclusion – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./13-bayes1.html" rel="next">
<link href="./11-likelihood5.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-bf90d58e07b16a5a5517af5259b97af0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./12-likelihood6.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropy and likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Divergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Principle of maximum entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#properties-of-maximum-likelihood-encountered-so-far" id="toc-properties-of-maximum-likelihood-encountered-so-far" class="nav-link active" data-scroll-target="#properties-of-maximum-likelihood-encountered-so-far"><span class="header-section-number">12.1</span> Properties of maximum likelihood encountered so far</a></li>
  <li><a href="#summarising-data-and-the-concept-of-minimal-sufficiency" id="toc-summarising-data-and-the-concept-of-minimal-sufficiency" class="nav-link" data-scroll-target="#summarising-data-and-the-concept-of-minimal-sufficiency"><span class="header-section-number">12.2</span> Summarising data and the concept of (minimal) sufficiency</a>
  <ul class="collapse">
  <li><a href="#induced-partioning-of-data-space-and-likelihood-equivalence" id="toc-induced-partioning-of-data-space-and-likelihood-equivalence" class="nav-link" data-scroll-target="#induced-partioning-of-data-space-and-likelihood-equivalence">Induced partioning of data space and likelihood equivalence</a></li>
  <li><a href="#minimal-sufficient-statistics" id="toc-minimal-sufficient-statistics" class="nav-link" data-scroll-target="#minimal-sufficient-statistics">Minimal sufficient statistics</a></li>
  <li><a href="#example-normal-distribution" id="toc-example-normal-distribution" class="nav-link" data-scroll-target="#example-normal-distribution">Example: normal distribution</a></li>
  <li><a href="#mles-of-parameters-of-an-exponential-family-are-minimal-sufficient-statistics" id="toc-mles-of-parameters-of-an-exponential-family-are-minimal-sufficient-statistics" class="nav-link" data-scroll-target="#mles-of-parameters-of-an-exponential-family-are-minimal-sufficient-statistics">MLEs of parameters of an exponential family are minimal sufficient statistics</a></li>
  </ul></li>
  <li><a href="#concluding-remarks-on-maximum-likelihood" id="toc-concluding-remarks-on-maximum-likelihood" class="nav-link" data-scroll-target="#concluding-remarks-on-maximum-likelihood"><span class="header-section-number">12.3</span> Concluding remarks on maximum likelihood</a>
  <ul class="collapse">
  <li><a href="#application-of-kl-divergence-in-statistics" id="toc-application-of-kl-divergence-in-statistics" class="nav-link" data-scroll-target="#application-of-kl-divergence-in-statistics">Application of KL divergence in statistics</a></li>
  <li><a href="#what-happens-if-n-is-small" id="toc-what-happens-if-n-is-small" class="nav-link" data-scroll-target="#what-happens-if-n-is-small">What happens if <span class="math inline">\(n\)</span> is small?</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection">Model selection</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./12-likelihood6.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="properties-of-maximum-likelihood-encountered-so-far" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="properties-of-maximum-likelihood-encountered-so-far"><span class="header-section-number">12.1</span> Properties of maximum likelihood encountered so far</h2>
<ol type="1">
<li>MLE is a special case of KL divergence minimisation <em>valid for large samples</em>.</li>
<li>MLE can be seen as generalisation of least squares (and conversely, least squares is a special case of ML).</li>
</ol>
<p><span class="math display">\[\begin{align*}
\begin{array}{cc}
\text{Kullback and Leibler (1951)}\\
\textbf{Entropy learning: minimise  } D_{\text{KL}}(F,P(\boldsymbol \theta))\\
\downarrow\\
\text{large } n\\
\downarrow\\
\text{Fisher 1922}\\
\textbf{Maximise Likelihood  } L(\boldsymbol \theta|D)\\
\downarrow\\
\text{normal model}\\
\downarrow\\
\text{Gauss 1805}\\
\textbf{Minimise squared error  } \sum_i (x_i-\theta)^2\\
\end{array}
\end{align*}\]</span></p>
<ol start="3" type="1">
<li><p>Given a model, derivation of the MLE is basically automatic (only optimisation required)!</p></li>
<li><p>MLEs are <strong>consistent</strong>, i.e.&nbsp;if the true underlying model <span class="math inline">\(F\)</span> is contained in the set of specified candidates models <span class="math inline">\(P(\boldsymbol \theta)\)</span> then the MLE will converge to the true model corresponding to parameter <span class="math inline">\(\boldsymbol \theta_{\text{true}}\)</span>.</p></li>
<li><p>Correspondingly, <strong>MLEs are asympotically unbiased</strong>.</p></li>
<li><p>However, MLEs are <em>not</em> necessarily unbiased in finite samples (e.g.&nbsp;the MLE of the variance parameter in the normal distribution).</p></li>
<li><p>The maximum likelihood is invariant against parameter transformations.</p></li>
<li><p>In regular situations (when local quadratic approximation is possible) MLEs are <strong>asympotically normally distributed</strong>, with the asymptotic variance determined by the observed Fisher information.</p></li>
<li><p>In regular situations and for large sample size MLEs are <strong>asympotically optimally efficient</strong> (Cramer-Rao theorem): For large samples the MLE achieves the lowest possible variance possible in an estimator — this is the so-called Cramer-Rao lower bound. The variance decreases to zero with <span class="math inline">\(n \rightarrow \infty\)</span> typically with rate <span class="math inline">\(1/n\)</span>.</p></li>
<li><p>The likelihood ratio can be used to construct optimal tests (in the sense of the Neyman-Pearson theorem).</p></li>
</ol>
</section>
<section id="summarising-data-and-the-concept-of-minimal-sufficiency" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="summarising-data-and-the-concept-of-minimal-sufficiency"><span class="header-section-number">12.2</span> Summarising data and the concept of (minimal) sufficiency</h2>
<section id="induced-partioning-of-data-space-and-likelihood-equivalence" class="level3">
<h3 class="anchored" data-anchor-id="induced-partioning-of-data-space-and-likelihood-equivalence">Induced partioning of data space and likelihood equivalence</h3>
<p>Every sufficient statistic <span class="math inline">\(t(D)\)</span> induces a partitioning of the space of data sets by clustering all hypothetical outcomes for which the statistic <span class="math inline">\(t(D)\)</span> assumes the same value <span class="math inline">\(t\)</span>: <span class="math display">\[\mathcal{X}_t = \{D: t(D) = t\}\]</span> The <strong>data sets in <span class="math inline">\(\mathcal{X}_t\)</span> are equivalent in terms of the sufficient statistic <span class="math inline">\(t(D)\)</span></strong>. Note that this implies that <span class="math inline">\(t(D)\)</span> is not a 1:1 transformation of <span class="math inline">\(D\)</span>. Instead of <span class="math inline">\(n\)</span> data points <span class="math inline">\(x_1, \ldots, x_n\)</span> as few as one or two summaries (such as empirical mean and variance) may be sufficient to fully convey all the information in the data about the model parameters. Thus, transforming data <span class="math inline">\(D\)</span> using a sufficient statistic <span class="math inline">\(t(D)\)</span> may result in substantial <strong>data reduction</strong>.</p>
<p>Two data sets <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> for which the ratio of the corresponding likelihoods <span class="math inline">\(L(\boldsymbol \theta| D_1 )/L(\boldsymbol \theta| D_2)\)</span> does not depend on <span class="math inline">\(\boldsymbol \theta\)</span> (so the two likelihoods are proportional to each other by a constant) are called <strong>likelihood equivalent</strong> because a likelihood-based procedure to learn about <span class="math inline">\(\boldsymbol \theta\)</span> will draw identical conclusions from <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span>. For data sets <span class="math inline">\(D_1, D_2 \in \mathcal{X}_t\)</span> which are equivalent with respect to a sufficient statistic <span class="math inline">\(T\)</span> it follows directly from the Fisher-Pearson factorisation <span class="math display">\[
L(\boldsymbol \theta| D) = h( \boldsymbol t(D) , \boldsymbol \theta) \, k(D)
\]</span> that the ratio <span class="math display">\[L(\boldsymbol \theta| D_1 )/L(\boldsymbol \theta| D_2) = k(D_1)/ k(D_2)\]</span> and thus is constant with regard to <span class="math inline">\(\boldsymbol \theta\)</span>. As a result, all <strong>data sets in <span class="math inline">\(\mathcal{X}_t\)</span> are likelihood equivalent</strong>. However, the converse is not true: depending on the sufficient statistics there usually will be many likelihood equivalent data sets that are not part of the same set <span class="math inline">\(\mathcal{X}_t\)</span>.</p>
</section>
<section id="minimal-sufficient-statistics" class="level3">
<h3 class="anchored" data-anchor-id="minimal-sufficient-statistics">Minimal sufficient statistics</h3>
<p>Of particular interest is therefore to find those sufficient statistics that achieve the coarsest partitioning of the sample space and thus may allow the highest data reduction. Specifically, a <strong>minimal sufficient statistic</strong> is a sufficient statistic for which all likelihood equivalent data sets also are equivalent under this statistic.</p>
<p>Therefore, to check whether a sufficient statistic <span class="math inline">\(t(D)\)</span> is minimal sufficient we need to verify whether for any two likelihood equivalent data sets <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> it also follows that <span class="math inline">\(t(D_1) = t(D_2)\)</span>. If this holds true then <span class="math inline">\(T\)</span> is a minimal sufficient statistic.</p>
<p>An equivalent non-operational definition is that a minimal sufficient statistic <span class="math inline">\(t(D)\)</span> is a sufficient statistic that can be computed from any other sufficient statistic <span class="math inline">\(S(D)\)</span>. This follows from the above directly: assume any sufficient statistic <span class="math inline">\(S(D)\)</span>, this defines a corresponding set <span class="math inline">\(\mathcal{X}_s\)</span> of likelihood equivalent data sets. By implication any <span class="math inline">\(D_1, D_2 \in \mathcal{X}_s\)</span> will necessarily also be in <span class="math inline">\(\mathcal{X}_t\)</span>, thus whenever <span class="math inline">\(S(D_1)=S(D_2)\)</span> we also have <span class="math inline">\(t(D_1)=t(D_2)\)</span>, and therefore <span class="math inline">\(t(D_1)\)</span> is a function of <span class="math inline">\(S(D_1)\)</span>.</p>
<p>A trivial but <strong>important example of a minimal sufficient statistic is the likelihood function itself</strong> since by definition it can be computed from any set of sufficient statistics. Thus the likelihood function <span class="math inline">\(L(\boldsymbol \theta)\)</span> captures all information about <span class="math inline">\(\boldsymbol \theta\)</span> that is available in the data. In other words, it provides an <em>optimal summary</em> of the observed data with regard to a model. Note that in Bayesian statistics (to be discussed in Part 2 of the module) the likelihood function is used as proxy/summary of the data.</p>
</section>
<section id="example-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="example-normal-distribution">Example: normal distribution</h3>
<div id="exm-suffstatnormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 12.1</strong></span> Sufficient statistics for the parameters of the normal distribution:</p>
<p>The normal model <span class="math inline">\(N(\mu, \sigma^2)\)</span> with parameter vector <span class="math inline">\(\boldsymbol \theta= (\mu, \sigma^2)^T\)</span> and log-likelihood <span class="math display">\[
\ell_n(\boldsymbol \theta) = -\frac{n}{2} \log(2 \pi \sigma^2)  - \frac{1}{2 \sigma^2} \sum_{i=1}^n (x_i-\mu)^2
\]</span> One possible set of minimal sufficient statistics for <span class="math inline">\(\boldsymbol \theta\)</span> are <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\overline{x^2}\)</span>, and with these we can rewrite the log-likelihood function without any reference to the original data <span class="math inline">\(x_1, \ldots, x_n\)</span> as follows <span class="math display">\[
\ell_n(\boldsymbol \theta) = -\frac{n}{2} \log(2 \pi \sigma^2)
-\frac{n}{2 \sigma^2} (\overline{x^2} - 2 \bar{x} \mu + \mu^2)
\]</span> An alternative set of minimal sufficient statistics for <span class="math inline">\(\boldsymbol \theta\)</span> consists of <span class="math inline">\(s^2 = \overline{x^2} - \bar{x}^2 = \widehat{\sigma^2}_{ML}\)</span> as and <span class="math inline">\(\bar{x} = \hat{\mu}_{ML}\)</span>. The log-likelihood written in terms of <span class="math inline">\(s^2\)</span> and <span class="math inline">\(\bar{x}\)</span> is <span class="math display">\[
\ell_n(\boldsymbol \theta) = -\frac{n}{2} \log(2 \pi \sigma^2)
-\frac{n}{2 \sigma^2} (s^2 + (\bar{x} - \mu)^2 )
\]</span></p>
<p>Note that in this example the dimension of the parameter vector <span class="math inline">\(\boldsymbol \theta\)</span> equals the dimension of the minimal sufficient statistic, and furthermore, that the MLEs of the parameters are in fact minimal sufficient!</p>
</div>
</section>
<section id="mles-of-parameters-of-an-exponential-family-are-minimal-sufficient-statistics" class="level3">
<h3 class="anchored" data-anchor-id="mles-of-parameters-of-an-exponential-family-are-minimal-sufficient-statistics">MLEs of parameters of an exponential family are minimal sufficient statistics</h3>
<p>The conclusion from <a href="#exm-suffstatnormal" class="quarto-xref">Example&nbsp;<span>12.1</span></a> holds true more generally: <strong>in an exponential family model</strong> (such as the normal distribution as particular important case) <strong>the MLEs of the parameters are minimal sufficient statistics</strong>. Thus, there will typically be substantial dimension reduction from the raw data to the sufficient statistics.</p>
<p>However, outside exponential families the MLE is not necessarily a minimal sufficient statistic, and may not even be a sufficient statistic. This is because <strong>a (minimal) sufficient statistic of the same dimension as the parameters does not always exist</strong>. A classic example is the Cauchy distribution for which the minimal sufficient statistics are the ordered observations, thus the MLE of the parameters do not constitute sufficient statistics, let alone minimal sufficient statistics. However, the MLE is of course still a function of the minimal sufficient statistic.</p>
<p>In summary, the likelihood function acts as perfect data summariser (i.e.&nbsp;as minimal sufficient statistic), and in exponential families (e.g.&nbsp;normal distribution) the MLEs of the parameters <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> are minimal sufficient.</p>
<p>Finally, while sufficiency is clearly a useful concept for data reduction one needs to keep in mind that this is always in reference to a specific model. Therefore, unless one strongly believes in a certain model it is generally a good idea to keep (and not discard!) the original data.</p>
</section>
</section>
<section id="concluding-remarks-on-maximum-likelihood" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="concluding-remarks-on-maximum-likelihood"><span class="header-section-number">12.3</span> Concluding remarks on maximum likelihood</h2>
<section id="application-of-kl-divergence-in-statistics" class="level3">
<h3 class="anchored" data-anchor-id="application-of-kl-divergence-in-statistics">Application of KL divergence in statistics</h3>
<p>In statistics the typical roles of the distribution <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> in the KL divergence <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> are:</p>
<ul>
<li><span class="math inline">\(Q\)</span> is the (unknown) underlying true model for the data-generating process</li>
<li><span class="math inline">\(P\)</span> is the approximating model (typically a parametric distribution family)</li>
</ul>
<p>Optimising (i.e.&nbsp;minimising) the KL divergence with regard to <span class="math inline">\(P\)</span> amounts to <em>approximation</em> and optimising with regard to <span class="math inline">\(Q\)</span> to <em>imputation</em>.</p>
<p>In previous chapters we have seen how the KL divergence leads to maximum likelihood (via minimum empirical risk) and also allows to choose distribution families (via maximum entropy). Later we will also see how KL divergence is linked to Bayesian learning.</p>
<p>Since the KL divergence is not symmetric there two distinct ways to minimise the divergence between a fixed <span class="math inline">\(F_0\)</span> and the family <span class="math inline">\(F(\boldsymbol \theta)\)</span> (see <a href="#fig-klopt" class="quarto-xref">Figure&nbsp;<span>12.1</span></a>). minimising the parameter <span class="math inline">\(\boldsymbol \theta\)</span> in <span class="math inline">\(D_{\text{KL}}(\hat{F}_0,F(\boldsymbol \theta))\)</span> (“forward KL”) and in <span class="math inline">\(D_{\text{KL}}(F(\boldsymbol \theta), \hat{F}_0)\)</span> (“backward KL”).</p>
<p>Each way has different properties:</p>
<div id="fig-klopt" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-klopt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-klopt" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-klfwd" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-klfwd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/entropy-forward-kl.png" class="img-fluid figure-img" data-ref-parent="fig-klopt">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-klfwd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Forward KL
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-klopt" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-klrev" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-klrev-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/entropy-reverse-kl.png" class="img-fluid figure-img" data-ref-parent="fig-klopt">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-klrev-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Reverse KL
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-klopt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.1: Illustration of (a) forward KL and b) reverse KL optimisation.
</figcaption>
</figure>
</div>
<ol type="a">
<li><p><strong>forward KL</strong>, <strong>approximation KL</strong>: <span class="math inline">\(\min_{\boldsymbol \theta}  D_{\text{KL}}(F_0,F(\boldsymbol \theta))\)</span></p>
<p>Here we keep the first argument fixed and minimise KL by changing the second argument. This is also called an “M (Moment) projection”. It has a <strong>zero avoiding</strong> property: <span class="math inline">\(f(x| \boldsymbol \theta)&gt;0 \text{ whenever } f_0(x)&gt;0\)</span>.</p>
<p>This procedure is mean-seeking and inclusive, i.e.&nbsp;when there are multiple modes in the density of <span class="math inline">\(F_0\)</span> a fitted unimodal density <span class="math inline">\(F(\hat{\boldsymbol \theta})\)</span> will seek to cover all modes (<strong>mass covering</strong> property).</p>
<p>Maximum likelihood is based on “forward KL”.</p></li>
<li><p><strong>reverse KL</strong>, <strong>inference KL</strong>: <span class="math inline">\(\min_{\boldsymbol \theta} D_{\text{KL}}(F(\boldsymbol \theta),F_0)\)</span></p>
<p>Here we keep the second argument fixed and minimise KL by changing the first argument. This is also called an “I (Information) projection”. It has a <strong>zero forcing</strong> property: <span class="math inline">\(f(x| \boldsymbol \theta)=0 \text{ whenever } f_0(x)=0\)</span>.</p>
<p>This procedure is mode-seeking and exclusive, i.e.&nbsp;when there are multiple modes in the density of <span class="math inline">\(F_0\)</span> a fitted unimodal density <span class="math inline">\(F(\hat{\boldsymbol \theta})\)</span> will seek out one mode to the exclusion of the others (<strong>mode attracting</strong> property).</p>
<p>Bayesian updating and variational Bayes approximations use “reverse KL”.</p></li>
</ol>
</section>
<section id="what-happens-if-n-is-small" class="level3">
<h3 class="anchored" data-anchor-id="what-happens-if-n-is-small">What happens if <span class="math inline">\(n\)</span> is small?</h3>
<p>From the long list of optimality properties of ML it is clear that for large sample size <span class="math inline">\(n\)</span> the best estimator will typically be the MLE.</p>
<p>However, for <strong>small sample size it is indeed possible (and necessary) to improve over the MLE</strong> (e.g.&nbsp;via Bayesian estimation or regularisation). Some of these ideas will be discussed in Part II.</p>
<ul>
<li>Likelihood will <em>overfit</em>!</li>
</ul>
<p>Alternative methods need to be used:</p>
<ul>
<li>regularised/penalised likelihood</li>
<li>Bayesian methods</li>
</ul>
<p>which are essentially two sides of the same coin.</p>
<p>Classic example of a simple non-ML estimator that is better than the MLE: <strong>Stein’s example / Stein paradox</strong> (C. Stein, 1955):</p>
<ul>
<li><p>Problem setting: estimation of the mean in multivariate case</p></li>
<li><p>Maximum likelihood estimation breaks down! <span class="math inline">\(\rightarrow\)</span> average (=MLE) is worse in terms of MSE than Stein estimator.</p></li>
<li><p>For small <span class="math inline">\(n\)</span> the asymptotic distributions for the MLE and for the LRT are not accurate, so for inference in these situations the distributions may need to be obtained by simulation (e.g.&nbsp;parametric or nonparametric bootstrap).</p></li>
</ul>
</section>
<section id="model-selection" class="level3">
<h3 class="anchored" data-anchor-id="model-selection">Model selection</h3>
<ul>
<li><p>CI are sets of models that are not statistically distinguishable from the best ML model</p></li>
<li><p>in doubt, choose the simplest model compatible with data</p></li>
<li><p>better prediction, avoids overfitting</p></li>
<li><p>Useful for model exploration and model building.</p></li>
<li><p>Note that, by construction, the model with more parameters always has a higher likelihood, implying likelihood favours complex models</p></li>
<li><p>Complex model may overfit!<br>
</p></li>
<li><p>For comparison of models penalised likelihood or Bayesian approaches may be necessary</p></li>
<li><p>Model selection in small samples and high dimension is challenging</p></li>
<li><p>Recall that the aim in statistics is <strong>not</strong> about rejecting models (this is easy as for large sample size any model will be rejected!)<br>
</p></li>
<li><p>Instead, the aim is model building, i.e.&nbsp;to find a model that <strong>explains the data well</strong> and that <strong>predicts well</strong>!<br>
</p></li>
<li><p>Typically, this will not be the best-fit ML model, but rather a simpler model that is close enough to the best / most complex model.</p></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./11-likelihood5.html" class="pagination-link" aria-label="Likelihood-based confidence interval and likelihood ratio">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./13-bayes1.html" class="pagination-link" aria-label="Conditioning and Bayes rule">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>