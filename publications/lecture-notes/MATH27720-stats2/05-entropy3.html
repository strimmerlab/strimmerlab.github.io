<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Local divergence – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06-entropy4.html" rel="next">
<link href="./04-entropy2.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-9290db0fca16de22067673ab8036b289.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-entropy1.html">Information</a></li><li class="breadcrumb-item"><a href="./05-entropy3.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Local divergence</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Information</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Risk and divergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Local divergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Maximum entropy</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-expectedfisherinfo" id="toc-sec-expectedfisherinfo" class="nav-link active" data-scroll-target="#sec-expectedfisherinfo"><span class="header-section-number">5.1</span> Fisher information</a>
  <ul class="collapse">
  <li><a href="#local-quadratic-approximation-of-kl-divergence" id="toc-local-quadratic-approximation-of-kl-divergence" class="nav-link" data-scroll-target="#local-quadratic-approximation-of-kl-divergence">Local quadratic approximation of KL divergence</a></li>
  <li><a href="#parameter-identifiability" id="toc-parameter-identifiability" class="nav-link" data-scroll-target="#parameter-identifiability">Parameter identifiability</a></li>
  <li><a href="#sec-additivityfisher" id="toc-sec-additivityfisher" class="nav-link" data-scroll-target="#sec-additivityfisher">Additivity of Fisher information</a></li>
  <li><a href="#invariance-property-of-fisher-information" id="toc-invariance-property-of-fisher-information" class="nav-link" data-scroll-target="#invariance-property-of-fisher-information">Invariance property of Fisher information</a></li>
  <li><a href="#scalar-examples-single-parameter-models" id="toc-scalar-examples-single-parameter-models" class="nav-link" data-scroll-target="#scalar-examples-single-parameter-models">Scalar examples — single parameter models</a></li>
  <li><a href="#matrix-examples-multiple-parameter-models" id="toc-matrix-examples-multiple-parameter-models" class="nav-link" data-scroll-target="#matrix-examples-multiple-parameter-models">Matrix examples — multiple parameter models</a></li>
  </ul></li>
  <li><a href="#colorred-blacktriangleright-reparametrisation" id="toc-colorred-blacktriangleright-reparametrisation" class="nav-link" data-scroll-target="#colorred-blacktriangleright-reparametrisation"><span class="header-section-number">5.2</span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Reparametrisation</a>
  <ul class="collapse">
  <li><a href="#sec-covariantfisher" id="toc-sec-covariantfisher" class="nav-link" data-scroll-target="#sec-covariantfisher"><span class="math inline">\(\color{Red} \blacktriangleright\)</span> Transformation of Fisher information reparametrisation</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples">Examples</a></li>
  </ul></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">5.3</span> Further reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-entropy1.html">Information</a></li><li class="breadcrumb-item"><a href="./05-entropy3.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Local divergence</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Local divergence</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter introduces the Fisher information matrix as the local curvature (the Hessian matrix) of the Kullback-Leibler divergence, serving as the local second-order sensitivity matrix for model parameters.</p>
<section id="sec-expectedfisherinfo" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-expectedfisherinfo"><span class="header-section-number">5.1</span> Fisher information</h2>
<section id="local-quadratic-approximation-of-kl-divergence" class="level3">
<h3 class="anchored" data-anchor-id="local-quadratic-approximation-of-kl-divergence">Local quadratic approximation of KL divergence</h3>
<p>The Kullback-Leibler (KL) number measures the divergence between two distributions. We now study the KL divergence of two distributions within a parametric family separate only by some small&nbsp;<span class="math inline">\(\boldsymbol \varepsilon\)</span>.</p>
<p>Specifically, we consider <span class="math display">\[
\begin{split}
D_{\text{KL}}(P(\boldsymbol \theta), P(\boldsymbol \theta+\boldsymbol \varepsilon)) &amp;= \operatorname{E}_{P(\boldsymbol \theta)}\left(  \log p(\boldsymbol x| \boldsymbol \theta)  - \log p(\boldsymbol x| \boldsymbol \theta+\boldsymbol \varepsilon)   \right)\\
&amp; = h(\boldsymbol \theta+\boldsymbol \varepsilon) \\
\end{split}
\]</span> where <span class="math inline">\(\boldsymbol \theta\)</span> is kept constant and <span class="math inline">\(\boldsymbol \varepsilon\)</span> is varying. Assuming that the pdmf <span class="math inline">\(p(\boldsymbol x| \boldsymbol \theta)\)</span> is <strong>twice differentiable</strong> with regard to <span class="math inline">\(\boldsymbol \theta\)</span> we can approximate the function <span class="math inline">\(h(\boldsymbol \theta+\boldsymbol \varepsilon)\)</span> quadratically by <span class="math display">\[
h(\boldsymbol \theta+\boldsymbol \varepsilon) \approx h(\boldsymbol \theta) + \nabla h(\boldsymbol \theta)^T\boldsymbol \varepsilon+ \frac{1}{2} \boldsymbol \varepsilon^T \, \nabla \nabla^T h(\boldsymbol \theta) \,\boldsymbol \varepsilon
\]</span></p>
<p>From the familiar properties of the KL divergence we conclude</p>
<ul>
<li><span class="math inline">\(D_{\text{KL}}(P(\boldsymbol \theta), P(\boldsymbol \theta+\boldsymbol \varepsilon))\geq 0\)</span> and</li>
<li>with equality only if <span class="math inline">\(\boldsymbol \varepsilon=0\)</span>.</li>
</ul>
<p>Thus, by construction the function <span class="math inline">\(h(\boldsymbol \theta+\boldsymbol \varepsilon)\)</span> assumes a minimum at <span class="math inline">\(\boldsymbol \varepsilon=0\)</span> with <span class="math inline">\(h(\boldsymbol \theta)=0\)</span> and a vanishing gradient <span class="math inline">\(\nabla h(\boldsymbol \theta) = 0\)</span>. Therefore, in the quadratic approximation of <span class="math inline">\(h(\boldsymbol \theta+\boldsymbol \varepsilon)\)</span> around <span class="math inline">\(\boldsymbol \theta\)</span> the first two terms (constant and linear) vanish and only the quadratic term remains: <span class="math display">\[
h(\boldsymbol \theta+\boldsymbol \varepsilon) \approx  \frac{1}{2} \boldsymbol \varepsilon^T \, \nabla \nabla^T h(\boldsymbol \theta) \,\boldsymbol \varepsilon
\]</span></p>
<p>Furthermore, the Hessian matrix of <span class="math inline">\(h(\boldsymbol \theta+\boldsymbol \varepsilon)\)</span> evaluated at <span class="math inline">\(\boldsymbol \varepsilon=0\)</span> is given by <span class="math display">\[
\begin{split}
\nabla \nabla^T h(\boldsymbol \theta) &amp;=  -\operatorname{E}_{P(\boldsymbol \theta)} \nabla \nabla^T  \log p(\boldsymbol x| \boldsymbol \theta) \\
&amp;= \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)
\end{split}
\]</span></p>
<p>This matrix is known as the <strong>Fisher information</strong> at <span class="math inline">\(\boldsymbol \theta\)</span>, denoted by <span class="math inline">\(\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\)</span>. It is also called <strong>expected Fisher information</strong> to emphasise that it is computed as the mean Hessian of the negative log-pdmf. The Fisher information matrix is always <strong>symmetric</strong> and <strong>positive semidefinite</strong>.</p>
<p>With its help the KL divergence can be locally approximated by <span class="math display">\[
D_{\text{KL}}(P(\boldsymbol \theta), P(\boldsymbol \theta+\boldsymbol \varepsilon))\approx \frac{1}{2} \boldsymbol \varepsilon^T  \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) \boldsymbol \varepsilon
\]</span></p>
<p>We may also vary the first argument in the KL divergence. It is straightforward to show that this leads to the same approximation to second order in <span class="math inline">\(\boldsymbol \varepsilon\)</span>: <span class="math display">\[
\begin{split}
D_{\text{KL}}(P(\boldsymbol \theta+\boldsymbol \varepsilon), P(\boldsymbol \theta))
&amp;\approx \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\, \boldsymbol \varepsilon\\
\end{split}
\]</span></p>
<p>Hence, although the KL divergence is not symmetric in its arguments in general, it is symmetric to second order (locally symmetric).</p>
<div id="nte-fimmetrictensor" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;5.1: <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Fisher information as metric tensor
</div>
</div>
<div class="callout-body-container callout-body">
<p>In <strong>information geometry</strong> probability distributions are studied using tools from differential geometry. From this geometric perspective, smoothly parametrised distribution families <span class="math inline">\(P(\boldsymbol \theta)\)</span> are viewed as <strong>manifolds</strong>. In KL divergence geometry the Fisher information <span class="math inline">\(\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\)</span> serves as <a href="https://en.wikipedia.org/wiki/Fisher_information_metric"><strong>metric tensor</strong></a>, measuring local distances between nearby distributions.</p>
<p>Other types of divergences among distributions induce related geometries, with local metrics similarly obtained by quadratic approximation.</p>
</div>
</div>
</section>
<section id="parameter-identifiability" class="level3">
<h3 class="anchored" data-anchor-id="parameter-identifiability">Parameter identifiability</h3>
<p>For a <strong>regular model</strong> the Fisher information is <strong>positive definite</strong> (with only positive eigenvalues) and hence <strong>parameters are locally identifiable</strong>. Recall that a positive definite Hessian implies that <span class="math inline">\(h(\boldsymbol \theta+ \boldsymbol \varepsilon)\)</span> has a true minimum at&nbsp;<span class="math inline">\(\boldsymbol \theta\)</span>.</p>
<p>Conversely, for a <strong>singular statistical model</strong> the Fisher information matrix is <strong>singular</strong> (some or all of its eigenvalues vanish) at some parameter values. This indicates <strong>local non-identifiability</strong> arising, e.g., from overparametrisation, parameters linked by exact constraints, lower dimensional latent structure, parameters on boundaries or other regularity failures.</p>
</section>
<section id="sec-additivityfisher" class="level3">
<h3 class="anchored" data-anchor-id="sec-additivityfisher">Additivity of Fisher information</h3>
<p>We may wish to compute the Fisher information based on a set of independent identically distributed (iid) random variables.</p>
<p>Assume that a random variable <span class="math inline">\(x \sim P(\boldsymbol \theta)\)</span> has log-pdmf <span class="math inline">\(\log p(x| \boldsymbol \theta)\)</span> and Fisher information <span class="math inline">\(\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\)</span>. The Fisher information <span class="math inline">\(\boldsymbol I_{x_1, \ldots, x_n}^{\text{Fisher}}(\boldsymbol \theta)\)</span> for a set of iid random variables <span class="math inline">\(x_1, \ldots, x_n \sim P(\boldsymbol \theta)\)</span> is computed from the joint log-pdmf <span class="math inline">\(\log p(x_1, \ldots, x_n) = \sum_{i}^n \log p(x_i| \boldsymbol \theta)\)</span>. This yields <span class="math display">\[
\begin{split}
\boldsymbol I_{x_1, \ldots, x_n}^{\text{Fisher}}(\boldsymbol \theta) &amp;= -\operatorname{E}_{P(\boldsymbol \theta)} \nabla \nabla^T  \sum_{i}^n \log p(x_i| \boldsymbol \theta)\\
&amp;= \sum_{i=1}^n  \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) =n  \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) \\
\end{split}
\]</span> Hence, the Fisher information for a set of <span class="math inline">\(n\)</span> iid random variables equals <span class="math inline">\(n\)</span> times the Fisher information of a single variable.</p>
</section>
<section id="invariance-property-of-fisher-information" class="level3">
<h3 class="anchored" data-anchor-id="invariance-property-of-fisher-information">Invariance property of Fisher information</h3>
<p>Like KL divergence the <strong>Fisher information is invariant under a change of variables in the sample space</strong>, such as from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span> and from distribution <span class="math inline">\(F_x\)</span> to <span class="math inline">\(F_y\)</span>. This is easy to see as the KL divergence itself is invariant under such reparametrisation, and thus also its curvature, and hence the Fisher information.</p>
<p>More specifically, when the sample space is changed the density will gain a factor in the form of the Jacobian determinant according to this transformation. However, since this factor does not depend on the model parameters, the first and second derivatives of the log-density with regard to the model parameters are not affected by it.</p>
<p>See also <a href="07-likelihood1.html#sec-mlregular" class="quarto-xref"><span>Section 7.4</span></a> for related sample space invariance of the gradient and curvature of the log-likelihood and <a href="09-likelihood3.html" class="quarto-xref"><span>Chapter 9</span></a> for the sample invariance of observed Fisher information.</p>
</section>
<section id="scalar-examples-single-parameter-models" class="level3">
<h3 class="anchored" data-anchor-id="scalar-examples-single-parameter-models">Scalar examples — single parameter models</h3>
<div id="exm-exfisherbernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1</strong></span> Fisher information for the Bernoulli distribution:</p>
<p>The log-pmf for the Bernoulli distribution <span class="math inline">\(\operatorname{Ber}(\theta)\)</span> is <span class="math display">\[
\log p(x | \theta) = x \log \theta + (1-x) \log(1-\theta)
\]</span> where <span class="math inline">\(\theta\)</span> is the probability of “success”. The second derivative with regard to the parameter <span class="math inline">\(\theta\)</span> is <span class="math display">\[
\frac{d^2}{d\theta^2} \log p(x | \theta)  =  -\frac{x}{\theta^2}-  \frac{1-x}{(1-\theta)^2}
\]</span> Since <span class="math inline">\(\operatorname{E}(x) = \theta\)</span> we get as Fisher information <span class="math display">\[
\begin{split}
I^{\text{Fisher}}(\theta) &amp; = -\operatorname{E}\left(\frac{d^2}{d\theta^2} \log p(x | \theta)  \right)\\
                           &amp;= \frac{\theta}{\theta^2}+  \frac{1-\theta}{(1-\theta)^2} \\
                            &amp;= \frac{1}{\theta(1-\theta)}\\
\end{split}
\]</span></p>
</div>
<div id="exm-quadapproxklbernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.2</strong></span> Quadratic approximations of the KL divergence between two Bernoulli distributions:</p>
<p>From <a href="04-entropy2.html#exm-klbernoulli" class="quarto-xref">Example&nbsp;<span>4.5</span></a> we have as KL divergence <span class="math display">\[
D_{\text{KL}}\left (\operatorname{Ber}(\theta_1), \operatorname{Ber}(\theta_2) \right)=\theta_1 \log\left( \frac{\theta_1}{\theta_2}\right) + (1-\theta_1) \log\left(\frac{1-\theta_1}{1-\theta_2}\right)
\]</span> and from <a href="#exm-exfisherbernoulli" class="quarto-xref">Example&nbsp;<span>5.1</span></a> the corresponding Fisher information.</p>
<p>The quadratic approximation implies that <span class="math display">\[
D_{\text{KL}}\left( \operatorname{Ber}(\theta), \operatorname{Ber}(\theta + \varepsilon) \right) \approx \frac{\varepsilon^2}{2}  I^{\text{Fisher}}(\theta) =  \frac{\varepsilon^2}{2 \theta (1-\theta)}
\]</span> and also that <span class="math display">\[
D_{\text{KL}}\left( \operatorname{Ber}(\theta+\varepsilon), \operatorname{Ber}(\theta) \right) \approx \frac{\varepsilon^2}{2} I^{\text{Fisher}}(\theta) =  \frac{\varepsilon^2}{2 \theta (1-\theta)}
\]</span></p>
<p>In Worksheet E1 this is verified by using a second order Taylor series applied to the KL divergence.</p>
</div>
<div id="exm-exfishernormknownvar" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.3</strong></span> Fisher information for the normal distribution with known variance.</p>
<p>The log-pdf for <span class="math inline">\(N(\mu, \sigma^2)\)</span> is <span class="math display">\[
\log p(x | \mu, \sigma^2) = -\frac{1}{2} \log \sigma^2
-\frac{1}{2 \sigma^2} (x-\mu)^2 - \frac{1}{2}\log(2 \pi)
\]</span> The second derivative with respect to <span class="math inline">\(\mu\)</span> is <span class="math display">\[
\frac{d^2}{d\mu^2} \log p(x | \mu, \sigma^2) = -\frac{1}{\sigma^2}
\]</span> Therefore the Fisher information is <span class="math display">\[
\boldsymbol I^{\text{Fisher}}\left(\mu\right) = \frac{1}{\sigma^2}
\]</span></p>
</div>
</section>
<section id="matrix-examples-multiple-parameter-models" class="level3">
<h3 class="anchored" data-anchor-id="matrix-examples-multiple-parameter-models">Matrix examples — multiple parameter models</h3>
<div id="exm-exfishernormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.4</strong></span> Fisher information for the normal distribution.</p>
<p>The log-pdf for <span class="math inline">\(N(\mu, \sigma^2)\)</span> is <span class="math display">\[
\log p(x | \mu, \sigma^2) = -\frac{1}{2} \log \sigma^2
-\frac{1}{2 \sigma^2} (x-\mu)^2 - \frac{1}{2}\log(2 \pi)
\]</span> The gradient with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> (!) is the vector <span class="math display">\[
\nabla \log p(x | \mu, \sigma^2) =
\begin{pmatrix}
\frac{1}{\sigma^2} (x-\mu) \\
- \frac{1}{2 \sigma^2} + \frac{1}{2 \sigma^4} (x- \mu)^2 \\
\end{pmatrix}
\]</span> Hint for calculating the gradient: replace <span class="math inline">\(\sigma^2\)</span> by <span class="math inline">\(v\)</span> and then take the partial derivative with regard to <span class="math inline">\(v\)</span>, then substitute back.</p>
<p>The corresponding Hessian matrix is <span class="math display">\[
\nabla \nabla^T \log p(x | \mu, \sigma^2) =
\begin{pmatrix}
-\frac{1}{\sigma^2} &amp; -\frac{1}{\sigma^4} (x-\mu)\\
-\frac{1}{\sigma^4} (x-\mu) &amp;  \frac{1}{2\sigma^4} - \frac{1}{\sigma^6}(x- \mu)^2 \\
\end{pmatrix}
\]</span> As <span class="math inline">\(\operatorname{E}(x) = \mu\)</span> we have <span class="math inline">\(\operatorname{E}(x-\mu) =0\)</span>. Furthermore, with <span class="math inline">\(\operatorname{E}( (x-\mu)^2 ) =\sigma^2\)</span> we see that <span class="math inline">\(\operatorname{E}\left(\frac{1}{\sigma^6}(x- \mu)^2\right) = \frac{1}{\sigma^4}\)</span>. Therefore the Fisher information matrix as the negative expected Hessian matrix is <span class="math display">\[
\boldsymbol I^{\text{Fisher}}\left(\mu,\sigma^2\right) = \begin{pmatrix} \frac{1}{\sigma^2} &amp; 0 \\ 0 &amp; \frac{1}{2\sigma^4} \end{pmatrix}
\]</span></p>
</div>
<div id="exm-exfisherexpfam" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.5</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Fisher information for the canonical parameter of an exponential family:</p>
<p>Review <a href="02-intro2.html#sec-expfamintro" class="quarto-xref"><span>Section 2.4</span></a> before studying this example.</p>
<p>Assume <span class="math inline">\(P(\boldsymbol \eta)\)</span> is an exponential family with canonical parameter vector <span class="math inline">\(\boldsymbol \eta\)</span>, canonical statistics <span class="math inline">\(\boldsymbol t(x)\)</span> and log-partition function <span class="math inline">\(a(\boldsymbol \eta)\)</span> with log-pdmf <span class="math inline">\(\log p(x|\boldsymbol \eta) =  \boldsymbol \eta^T \boldsymbol t(x) + \log h(x) - a(\boldsymbol \eta)\)</span>.</p>
<p>If we take second derivatives with regard to <span class="math inline">\(\boldsymbol \eta\)</span> all terms except for the last vanish: <span class="math display">\[
\nabla \nabla^T  \log p(x | \boldsymbol \eta) = - \nabla \nabla^T a(\boldsymbol \eta) = -\boldsymbol \Sigma_{\boldsymbol t}(\boldsymbol \eta)
\]</span> Then the Fisher information is <span class="math display">\[
\begin{split}
\boldsymbol I^{\text{Fisher}}(\boldsymbol \eta) &amp;= -\operatorname{E}_{P(\boldsymbol \eta)} \nabla \nabla^T  \log p(x | \boldsymbol \eta)\\
&amp; = \operatorname{E}_{P(\boldsymbol \eta)} \boldsymbol \Sigma_{\boldsymbol t}(\boldsymbol \eta)\\
&amp;= \boldsymbol \Sigma_{\boldsymbol t}(\boldsymbol \eta)
\end{split}
\]</span></p>
<p>Hence, the Fisher information for the canonical parameter in an exponential family is the variance of the canonical statistics.</p>
<p>See <a href="#exm-exfisherefexparam" class="quarto-xref">Example&nbsp;<span>5.8</span></a> for a related but different result for the expectation parameters.</p>
</div>
</section>
</section>
<section id="colorred-blacktriangleright-reparametrisation" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="colorred-blacktriangleright-reparametrisation"><span class="header-section-number">5.2</span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Reparametrisation</h2>
<section id="sec-covariantfisher" class="level3">
<h3 class="anchored" data-anchor-id="sec-covariantfisher"><span class="math inline">\(\color{Red} \blacktriangleright\)</span> Transformation of Fisher information reparametrisation</h3>
<p>The Fisher information <span class="math inline">\(\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\)</span> depends on the parameter <span class="math inline">\(\boldsymbol \theta\)</span>. If we use a different parametrisation of the underlying distribution family, say <span class="math inline">\(\boldsymbol \zeta\)</span> instead of <span class="math inline">\(\boldsymbol \theta\)</span>, with a map <span class="math inline">\(\boldsymbol \theta(\boldsymbol \zeta)\)</span> from <span class="math inline">\(\boldsymbol \zeta\)</span> to <span class="math inline">\(\boldsymbol \theta\)</span>, then the Fisher information changes according to the chain rule in calculus.</p>
<p>To find the resulting Fisher information in terms of the new parameter <span class="math inline">\(\boldsymbol \zeta\)</span> we need to use the Jacobian matrix <span class="math inline">\(D \boldsymbol \theta(\boldsymbol \zeta)\)</span>. This matrix contains the gradients for each component of the map <span class="math inline">\(\boldsymbol \theta(\boldsymbol \zeta)\)</span> in its rows: <span class="math display">\[
D \boldsymbol \theta(\boldsymbol \zeta) =
\begin{pmatrix}\nabla^T \theta_1(\boldsymbol \zeta)\\ \nabla^T \theta_2(\boldsymbol \zeta) \\ \vdots \\  \end{pmatrix}
\]</span></p>
<p>With the above the Fisher information for <span class="math inline">\(\boldsymbol \theta\)</span> is then transformed to the Fisher information for <span class="math inline">\(\boldsymbol \zeta\)</span> applying the chain rule for the Hessian matrix: <span class="math display">\[
\boldsymbol I^{\text{Fisher}}(\boldsymbol \zeta)   = (D \boldsymbol \theta(\boldsymbol \zeta))^T \, \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) \rvert_{\boldsymbol \theta= \boldsymbol \theta(\boldsymbol \zeta)}  \, D \boldsymbol \theta(\boldsymbol \zeta)
\]</span> This type of transformation is also known as <em>covariant transformation</em>.</p>
</section>
<section id="examples" class="level3">
<h3 class="anchored" data-anchor-id="examples">Examples</h3>
<div id="exm-expfishtransbernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.6</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Fisher information for the Bernoulli distribution from canonical to conventional parametrisation by change of variables:</p>
<p>From <a href="02-intro2.html#exm-expfambernoulli" class="quarto-xref">Example&nbsp;<span>2.2</span></a> and <a href="#exm-exfisherexpfam" class="quarto-xref">Example&nbsp;<span>5.5</span></a> the Fisher information for the Bernoulli distribution using the canonical parameter <span class="math inline">\(\eta\)</span> is <span class="math display">\[
I^{\text{Fisher}}(\eta)=a''(\eta) = \frac{ e^{\eta}}{(e^{\eta}+1)^2}
\]</span> The map to the canonical parameter <span class="math inline">\(\eta\)</span> from the conventional parameter <span class="math inline">\(\theta\)</span> is the logit function <span class="math inline">\(\eta(\theta) = \log\left(  \frac{\theta}{1-\theta}\right)\)</span> with Jacobian <span class="math display">\[
D \eta(\theta) = \eta(\theta)'= \frac{1}{\theta (1-\theta)}
\]</span> Using the chain rule to obtain the Fisher information for <span class="math inline">\(\theta\)</span> with <span class="math display">\[
I^{\text{Fisher}}(\eta) \rvert_{\eta = \eta(\theta)}   =\theta (1-\theta)
\]</span> yields <span class="math display">\[
I^{\text{Fisher}}(\theta) = (D\eta(\theta))^2 \, I^{\text{Fisher}}(\eta) \rvert_{\eta = \eta(\theta)}   =  \frac{1}{\theta (1-\theta)}
\]</span> which agrees with the result obtained by direct calculation in <a href="#exm-exfisherbernoulli" class="quarto-xref">Example&nbsp;<span>5.1</span></a>.</p>
<p>Note that the Fisher information for the mean parameter <span class="math inline">\(\theta\)</span> is the inverse of the Fisher information for the canonical parameter <span class="math inline">\(\eta\)</span>.</p>
</div>
<div id="exm-expfishtransnormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.7</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Fisher information for the normal distribution from canonical to conventional parametrisation by change of variables:</p>
<p>From <a href="02-intro2.html#exm-expfamnormal" class="quarto-xref">Example&nbsp;<span>2.3</span></a> and <a href="#exm-exfisherexpfam" class="quarto-xref">Example&nbsp;<span>5.5</span></a> the Fisher information matrix for the normal distribution using canonical parameters <span class="math inline">\(\boldsymbol \eta\)</span> is <span class="math display">\[
\boldsymbol I^{\text{Fisher}}(\boldsymbol \eta)=  \nabla \nabla^T a(\boldsymbol \eta) = \begin{pmatrix}
                 \frac{1}{1-2\eta_2} &amp; \frac{2 \eta_1}{(1-2 \eta_2)^2} \\
                 \frac{2 \eta_1}{(1-2 \eta_2)^2} &amp;  \frac{4\eta_1^2 -4 \eta_2 +2 }{(1-2 \eta_2)^3} \\
                 \end{pmatrix}
\]</span> The map to the canonical parameter <span class="math inline">\(\boldsymbol \eta\)</span> from the conventional parameters <span class="math inline">\(\boldsymbol \theta=c(\mu, \sigma^2)^T\)</span> is <span class="math inline">\(\boldsymbol \eta= (\eta_1, \eta_2)^T  = (\frac{\mu}{\sigma^2}, \frac{1}{2} - \frac{1}{2 \sigma^2} )^T\)</span> with Jacobian matrix <span class="math display">\[
D\boldsymbol \eta(\boldsymbol \theta)=  \begin{pmatrix}
                 \frac{1}{\sigma^2} &amp; -\frac{\mu}{\sigma^4} \\
                 0 &amp;  \frac{1 }{2\sigma^4} \\
                 \end{pmatrix}
\]</span> With <span class="math display">\[
\boldsymbol I^{\text{Fisher}}(\boldsymbol \eta) \rvert_{\boldsymbol \eta= \boldsymbol \eta(\boldsymbol \theta)} = \begin{pmatrix}
                 \sigma^2 &amp;  2 \mu \sigma^2 \\
                  2 \mu \sigma^2 &amp;  4 \mu^2 \sigma^2 + 2 \sigma^4 \\
                 \end{pmatrix}
\]</span> this yields the Fisher information for <span class="math inline">\(\boldsymbol \theta=c(\mu, \sigma^2)^T\)</span> as <span class="math display">\[
\begin{split}
\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)  &amp; = (D \boldsymbol \eta(\boldsymbol \theta))^T \, \boldsymbol I^{\text{Fisher}}(\boldsymbol \eta)  \rvert_{\boldsymbol \eta= \boldsymbol \eta(\boldsymbol \theta)} \, D \boldsymbol \eta(\boldsymbol \theta)\\
&amp;= \begin{pmatrix} \frac{1}{\sigma^2} &amp; 0 \\ 0 &amp; \frac{1}{2\sigma^4} \end{pmatrix}
\end{split}
\]</span> which agrees with the result obtained by direct calculation in <a href="#exm-exfishernormal" class="quarto-xref">Example&nbsp;<span>5.4</span></a>.</p>
</div>
<div id="exm-exfisherefexparam" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.8</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Fisher information for the expectation parameter of an exponential family:</p>
<p>An alternative parametrisation of an exponential family is provided by the means <span class="math inline">\(\boldsymbol \mu_{\boldsymbol t}\)</span> of the canonical statistics <span class="math inline">\(\boldsymbol t(\boldsymbol x)\)</span>. These expectation parameters are given by <span class="math inline">\(\boldsymbol \mu_{\boldsymbol t}(\boldsymbol \eta)= \nabla a(\boldsymbol \eta)\)</span>.</p>
<p>The Jacobian for the transformation <span class="math inline">\(\boldsymbol \mu_{\boldsymbol t}(\boldsymbol \eta)\)</span> is <span class="math inline">\(D \boldsymbol \mu_{\boldsymbol t}(\boldsymbol \eta) = D \nabla a(\boldsymbol \eta) = \nabla \nabla^T a(\boldsymbol \eta) = \boldsymbol \Sigma_{\boldsymbol t}(\boldsymbol \eta)\)</span>. Hence, the Jacobian for the inverse transformation <span class="math inline">\(\boldsymbol \eta(\boldsymbol \mu_{\boldsymbol t})\)</span> is <span class="math inline">\(D \boldsymbol \eta(\boldsymbol \mu_{\boldsymbol t}) = \boldsymbol \Sigma_{\boldsymbol t}(\boldsymbol \eta(\boldsymbol \mu_{\boldsymbol t}))^{-1}\)</span>.</p>
<p>The Fisher information transforms covariantly under change of parameter. With <span class="math display">\[
\boldsymbol I^{\text{Fisher}}(\boldsymbol \eta) \rvert_{\boldsymbol \eta= \boldsymbol \eta(\boldsymbol \mu_{\boldsymbol t})} =  \boldsymbol \Sigma_{\boldsymbol t}(\boldsymbol \eta(\boldsymbol \mu_{\boldsymbol t}))
\]</span> this yields the Fisher information for the expectation parameters <span class="math inline">\(\boldsymbol \mu_{\boldsymbol t}\)</span> <span class="math display">\[
\begin{split}
\boldsymbol I^{\text{Fisher}}(\boldsymbol \mu_{\boldsymbol t})  &amp; = (D \boldsymbol \eta(\boldsymbol \mu_{\boldsymbol t}))^T \, \boldsymbol I^{\text{Fisher}}(\boldsymbol \eta)  \rvert_{\boldsymbol \eta= \boldsymbol \eta(\boldsymbol \mu_{\boldsymbol t})} \, D \boldsymbol \eta(\boldsymbol \mu_{\boldsymbol t})\\
&amp;= \boldsymbol \Sigma_{\boldsymbol t}(\boldsymbol \eta(\boldsymbol \mu_{\boldsymbol t}))^{-1}
\end{split}
\]</span> Hence the Fisher information for the expectation parameters <span class="math inline">\(\boldsymbol \mu_{\boldsymbol t}\)</span> is the inverse of the variance of the canonical statistics, and therefore the inverse of the Fisher information for the canonical parameters <span class="math inline">\(\boldsymbol \eta\)</span>.</p>
<p>This relationship has been encountered before in <a href="#exm-expfishtransbernoulli" class="quarto-xref">Example&nbsp;<span>5.6</span></a> for the special case of the Bernoulli distribution.</p>
</div>
</section>
</section>
<section id="further-reading" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">5.3</span> Further reading</h2>
<p><span class="citation" data-cites="Amari2016">Amari (<a href="bibliography.html#ref-Amari2016" role="doc-biblioref">2016</a>)</span> is a recent book and standard reference on information geometry.</p>
<p>For metrics associated with proper scoring rules see <span class="citation" data-cites="DawidMusio2014">Dawid and Musio (<a href="bibliography.html#ref-DawidMusio2014" role="doc-biblioref">2014</a>)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>A bit of history
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Fisher information matrix was originally introduced by <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald A. Fisher (1890–1962)</a> in the 1920s to measure the precision of an estimator.</p>
<p><a href="https://en.wikipedia.org/wiki/C._R._Rao">C. Radhakrishna Rao (1920–2023)</a> showed in 1945 that the Fisher information matrix defines a local metric tensor on the parameter space and established its interpretation as a local sensitivity measure.</p>
<p>This insight later helped lead to the development of information geometry and to the study of singular or non-regular models in statistics.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Amari2016" class="csl-entry" role="listitem">
Amari, S.-I. 2016. <em>Information Geometry and Its Applications</em>. Springer. <a href="https://doi.org/10.1007/978-4-431-55978-8">https://doi.org/10.1007/978-4-431-55978-8</a>.
</div>
<div id="ref-DawidMusio2014" class="csl-entry" role="listitem">
Dawid, A. P., and M. Musio. 2014. <span>“Theory and Applications of Proper Scoring Rules.”</span> <em>METRON</em> 72: 169–83. <a href="https://doi.org/10.1007/s40300-014-0039-y">https://doi.org/10.1007/s40300-014-0039-y</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04-entropy2.html" class="pagination-link" aria-label="Risk and divergence">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Risk and divergence</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./06-entropy4.html" class="pagination-link" aria-label="Maximum entropy">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Maximum entropy</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>