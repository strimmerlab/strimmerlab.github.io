<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Expected Fisher information – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06-entropy4.html" rel="next">
<link href="./04-entropy2.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./05-entropy3.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropy and likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions for statistical models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Relative entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Principle of maximum entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#expected-fisher-information" id="toc-expected-fisher-information" class="nav-link active" data-scroll-target="#expected-fisher-information"><span class="header-section-number">5.1</span> Expected Fisher information</a>
  <ul class="collapse">
  <li><a href="#definition-of-expected-fisher-information" id="toc-definition-of-expected-fisher-information" class="nav-link" data-scroll-target="#definition-of-expected-fisher-information">Definition of expected Fisher information</a></li>
  <li><a href="#sec-additivityfisher" id="toc-sec-additivityfisher" class="nav-link" data-scroll-target="#sec-additivityfisher">Additivity of Fisher information</a></li>
  <li><a href="#invariance-property-of-the-fisher-information" id="toc-invariance-property-of-the-fisher-information" class="nav-link" data-scroll-target="#invariance-property-of-the-fisher-information">Invariance property of the Fisher information</a></li>
  <li><a href="#sec-covariantfisher" id="toc-sec-covariantfisher" class="nav-link" data-scroll-target="#sec-covariantfisher"><span class="math inline">\(\color{Red} \blacktriangleright\)</span> Transformation of Fisher information when model parameters change</a></li>
  </ul></li>
  <li><a href="#expected-fisher-information-examples" id="toc-expected-fisher-information-examples" class="nav-link" data-scroll-target="#expected-fisher-information-examples"><span class="header-section-number">5.2</span> Expected Fisher information examples</a>
  <ul class="collapse">
  <li><a href="#models-with-a-single-parameter" id="toc-models-with-a-single-parameter" class="nav-link" data-scroll-target="#models-with-a-single-parameter">Models with a single parameter</a></li>
  <li><a href="#models-with-multiple-parameters" id="toc-models-with-multiple-parameters" class="nav-link" data-scroll-target="#models-with-multiple-parameters">Models with multiple parameters</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./05-entropy3.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="expected-fisher-information" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="expected-fisher-information"><span class="header-section-number">5.1</span> Expected Fisher information</h2>
<section id="definition-of-expected-fisher-information" class="level3">
<h3 class="anchored" data-anchor-id="definition-of-expected-fisher-information">Definition of expected Fisher information</h3>
<p>KL information measures the divergence of two distributions. Previously we have seen examples of KL divergence between two distributions belonging to the same family. We now consider the KL divergence of two such distributions separated in parameter space only by some small&nbsp;<span class="math inline">\(\boldsymbol \varepsilon\)</span>.</p>
<p>Specifically, we consider the function <span class="math display">\[
\begin{split}
h(\boldsymbol \theta+\boldsymbol \varepsilon) &amp; = D_{\text{KL}}(F_{\boldsymbol \theta}, F_{\boldsymbol \theta+\boldsymbol \varepsilon}) \\
&amp;= \text{E}_{F_{\boldsymbol \theta}}\left(  \log f(\boldsymbol x| \boldsymbol \theta)  - \log f(\boldsymbol x| \boldsymbol \theta+\boldsymbol \varepsilon)   \right)\\
\end{split}
\]</span> where <span class="math inline">\(\boldsymbol \theta\)</span> is kept constant and <span class="math inline">\(\boldsymbol \varepsilon\)</span> is varying. Assuming that <span class="math inline">\(f(\boldsymbol x| \boldsymbol \theta)\)</span> is twice differentiable with regard to <span class="math inline">\(\boldsymbol \theta\)</span> we can approximate <span class="math inline">\(h(\boldsymbol \theta+\boldsymbol \varepsilon)\)</span> quadratically by <span class="math display">\[
h(\boldsymbol \theta+\boldsymbol \varepsilon) \approx h(\boldsymbol \theta) + \nabla h(\boldsymbol \theta)^T\boldsymbol \varepsilon+ \frac{1}{2} \boldsymbol \varepsilon^T \, \nabla \nabla^T h(\boldsymbol \theta) \,\boldsymbol \varepsilon
\]</span></p>
<p>From the properties of the KL divergence we know that <span class="math inline">\(D_{\text{KL}}(F_{\boldsymbol \theta}, F_{\boldsymbol \theta+\boldsymbol \varepsilon})\geq 0\)</span> and that it becomes zero only if <span class="math inline">\(\boldsymbol \varepsilon=0\)</span>. Thus, by construction the function <span class="math inline">\(h(\boldsymbol \theta+\boldsymbol \varepsilon)\)</span> achieves for <span class="math inline">\(\boldsymbol \varepsilon=0\)</span></p>
<ol type="i">
<li>a true minimum with <span class="math inline">\(h(\boldsymbol \theta)=0\)</span>,</li>
<li>a vanishing gradient with <span class="math inline">\(\nabla h(\boldsymbol \theta) = 0\)</span>, and</li>
<li>a positive definite Hessian matrix with <span class="math inline">\(\nabla \nabla^T h(\boldsymbol \theta) =  -\text{E}_{F_{\boldsymbol \theta}} \nabla \nabla^T  \log f(\boldsymbol x| \boldsymbol \theta)\)</span>.</li>
</ol>
<p>Therefore in the quadratic approximation of <span class="math inline">\(h(\boldsymbol \theta+\boldsymbol \varepsilon)\)</span> around <span class="math inline">\(\boldsymbol \theta\)</span> above the first two terms (constant and linear) vanish and only the quadratic term remains. The Hessian matrix evaluated at <span class="math inline">\(\boldsymbol \theta\)</span> <span class="math display">\[
\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) =   -\text{E}_{F_{\boldsymbol \theta}} \nabla \nabla^T  \log f(\boldsymbol x| \boldsymbol \theta)
\]</span> is called <strong>expected Fisher information</strong> for <span class="math inline">\(\boldsymbol \theta\)</span>, or short <strong>Fisher information</strong>. Hence, the KL divergence can be locally approximated by <span class="math display">\[
D_{\text{KL}}(F_{\boldsymbol \theta}, F_{\boldsymbol \theta+\boldsymbol \varepsilon})\approx \frac{1}{2} \boldsymbol \varepsilon^T  \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) \boldsymbol \varepsilon
\]</span></p>
<p>We may also vary the first argument in the KL divergence. It is straightforward to show that this leads to the same approximation to second order in <span class="math inline">\(\boldsymbol \varepsilon\)</span>: <span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\boldsymbol \theta+\boldsymbol \varepsilon}, F_{\boldsymbol \theta})
&amp;\approx \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\, \boldsymbol \varepsilon\\
\end{split}
\]</span></p>
<p>Hence, the KL divergence, while generally not symmetric in its arguments, is still locally symmetric.</p>
<p>Computing the expected Fisher information involves no observed data, it is purely a property of the model family <span class="math inline">\(F_{\boldsymbol \theta}\)</span>. In <a href="09-likelihood3.html" class="quarto-xref"><span>Chapter 9</span></a> we will study a related quantity, the <em>observed Fisher information</em> that in contrast to the expected Fisher information is a function of the observed data.</p>
<div id="exm-fimmetrictensor" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Fisher information as metric tensor:</p>
<p>In the field of <em>information geometry</em><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> sets of distributions are studied using tools from differential geometry. It turns out that distribution families are manifolds and that the expected Fisher information matrix plays the role of the (symmetric!) <a href="https://en.wikipedia.org/wiki/Fisher_information_metric">metric tensor</a> on this manifold.</p>
</div>
</section>
<section id="sec-additivityfisher" class="level3">
<h3 class="anchored" data-anchor-id="sec-additivityfisher">Additivity of Fisher information</h3>
<p>We may wish to compute the expected Fisher information based on a set of independent identically distributed (iid) random variables.</p>
<p>Assume that a random variable <span class="math inline">\(x \sim F_{\boldsymbol \theta}\)</span> has log-density <span class="math inline">\(\log f(x| \boldsymbol \theta)\)</span> and expected Fisher information <span class="math inline">\(\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\)</span>. The expected Fisher information <span class="math inline">\(\boldsymbol I_{x_1, \ldots, x_n}^{\text{Fisher}}(\boldsymbol \theta)\)</span> for a set of iid random variables <span class="math inline">\(x_1, \ldots, x_n \sim F_{\boldsymbol \theta}\)</span> is computed from the joint log-density <span class="math inline">\(\log f(x_1, \ldots, x_n) = \sum_{i}^n \log f(x_i| \boldsymbol \theta)\)</span>. This yields <span class="math display">\[
\begin{split}
\boldsymbol I_{x_1, \ldots, x_n}^{\text{Fisher}}(\boldsymbol \theta) &amp;= -\text{E}_{F_{\boldsymbol \theta}} \nabla \nabla^T  \sum_{i}^n \log f(x_i| \boldsymbol \theta)\\
&amp;= \sum_{i=1}^n  \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) =n  \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) \\
\end{split}
\]</span> Hence, the expected Fisher information for a set of <span class="math inline">\(n\)</span> iid random variables is the <span class="math inline">\(n\)</span> times the Fisher information of a single variable.</p>
</section>
<section id="invariance-property-of-the-fisher-information" class="level3">
<h3 class="anchored" data-anchor-id="invariance-property-of-the-fisher-information">Invariance property of the Fisher information</h3>
<p>Like KL divergence the <strong>expected Fisher information is invariant against change of parametrisation of the sample space</strong>, say from variable <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span> and from distribution <span class="math inline">\(F_x\)</span> to <span class="math inline">\(F_y\)</span>. This is easy to see as the KL divergence itself is invariant against such reparametrisation, and thus also its curvature, and hence the expected Fisher information.</p>
<p>More specifically, when the sample space is changed the density gains a factor in the form of the Jacobian determinant according to this transformation. Since this factor does not depend of the model parameters and it does not change the first and second derivatives of the log-density with regard to the model parameters.</p>
<p>See also <a href="07-likelihood1.html#sec-mlregular" class="quarto-xref"><span>Section 7.4</span></a> for related sample space invariance of the gradient and curvature of the log-likelihood and <a href="09-likelihood3.html" class="quarto-xref"><span>Chapter 9</span></a> for the sample invariance of observed Fisher information.</p>
</section>
<section id="sec-covariantfisher" class="level3">
<h3 class="anchored" data-anchor-id="sec-covariantfisher"><span class="math inline">\(\color{Red} \blacktriangleright\)</span> Transformation of Fisher information when model parameters change</h3>
<p>The Fisher information <span class="math inline">\(\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\)</span> depends on the parameter <span class="math inline">\(\boldsymbol \theta\)</span>. If we use a different parameterisation of the underlying parametric distribution family, say <span class="math inline">\(\boldsymbol \zeta\)</span> with a map <span class="math inline">\(\boldsymbol \theta(\boldsymbol \zeta)\)</span> from <span class="math inline">\(\boldsymbol \zeta\)</span> to <span class="math inline">\(\boldsymbol \theta\)</span>, then the Fisher information changes according to the chain rule in calculus.</p>
<p>To find the resulting Fisher information in terms of the new parameter <span class="math inline">\(\boldsymbol \zeta\)</span> we need to use the Jacobian matrix <span class="math inline">\(D \boldsymbol \theta(\boldsymbol \zeta)\)</span>. This matrix contains the gradients for each component of the map <span class="math inline">\(\boldsymbol \theta(\boldsymbol \zeta)\)</span> in its rows: <span class="math display">\[
D \boldsymbol \theta(\boldsymbol \zeta) =
\begin{pmatrix}\nabla^T \theta_1(\boldsymbol \zeta)\\ \nabla^T \theta_2(\boldsymbol \zeta) \\ \vdots \\  \end{pmatrix}
\]</span></p>
<p>With the above the Fisher information for <span class="math inline">\(\boldsymbol \theta\)</span> is then transformed to the Fisher information for <span class="math inline">\(\boldsymbol \zeta\)</span> applying the chain rule for the Hessian matrix: <span class="math display">\[
\boldsymbol I^{\text{Fisher}}(\boldsymbol \zeta)   = (D \boldsymbol \theta(\boldsymbol \zeta))^T \, \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) \rvert_{\boldsymbol \theta= \boldsymbol \theta(\boldsymbol \zeta)}  \, D \boldsymbol \theta(\boldsymbol \zeta)
\]</span> This type of transformation is also known as <em>covariant transformation</em>, in this case for the Fisher information metric tensor.</p>
</section>
</section>
<section id="expected-fisher-information-examples" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="expected-fisher-information-examples"><span class="header-section-number">5.2</span> Expected Fisher information examples</h2>
<section id="models-with-a-single-parameter" class="level3">
<h3 class="anchored" data-anchor-id="models-with-a-single-parameter">Models with a single parameter</h3>
<div id="exm-expectedfisherbernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.2</strong></span> Expected Fisher information for the Bernoulli distribution:</p>
<p>The log-probability mass function of the Bernoulli <span class="math inline">\(\text{Ber}(\theta)\)</span> distribution is <span class="math display">\[
\log p(x | \theta) = x \log(\theta) + (1-x) \log(1-\theta)
\]</span> where <span class="math inline">\(\theta\)</span> is the probability of “success”. The second derivative with regard to the parameter <span class="math inline">\(\theta\)</span> is <span class="math display">\[
\frac{d^2}{d\theta^2} \log p(x | \theta)  =  -\frac{x}{\theta^2}-  \frac{1-x}{(1-\theta)^2}
\]</span> Since <span class="math inline">\(\text{E}(x) = \theta\)</span> we get as Fisher information <span class="math display">\[
\begin{split}
I^{\text{Fisher}}(\theta) &amp; = -\text{E}\left(\frac{d^2}{d\theta^2} \log p(x | \theta)  \right)\\
                           &amp;= \frac{\theta}{\theta^2}+  \frac{1-\theta}{(1-\theta)^2} \\
                            &amp;= \frac{1}{\theta(1-\theta)}\\
\end{split}
\]</span></p>
</div>
<div id="exm-quadapproxklbernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.3</strong></span> Quadratic approximations of the KL divergence between two Bernoulli distributions:</p>
<p>From <a href="04-entropy2.html#exm-klbernoulli" class="quarto-xref">Example&nbsp;<span>4.4</span></a> we have as KL divergence <span class="math display">\[
D_{\text{KL}}\left (\text{Ber}(\theta_1), \text{Ber}(\theta_2) \right)=\theta_1 \log\left( \frac{\theta_1}{\theta_2}\right) + (1-\theta_1) \log\left(\frac{1-\theta_1}{1-\theta_2}\right)
\]</span> and from <a href="#exm-expectedfisherbernoulli" class="quarto-xref">Example&nbsp;<span>5.2</span></a> the corresponding expected Fisher information.</p>
<p>The quadratic approximation implies that <span class="math display">\[
D_{\text{KL}}\left( \text{Ber}(\theta), \text{Ber}(\theta + \varepsilon) \right) \approx \frac{\varepsilon^2}{2}  I^{\text{Fisher}}(\theta) =  \frac{\varepsilon^2}{2 \theta (1-\theta)}
\]</span> and also that <span class="math display">\[
D_{\text{KL}}\left( \text{Ber}(\theta+\varepsilon), \text{Ber}(\theta) \right) \approx \frac{\varepsilon^2}{2} I^{\text{Fisher}}(\theta) =  \frac{\varepsilon^2}{2 \theta (1-\theta)}
\]</span></p>
<p>In Worksheet E1 this is verified by using a second order Taylor series applied to the KL divergence.</p>
</div>
<div id="exm-expectedfishernormknownvar" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.4</strong></span> Expected Fisher information for the normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span> with known variance.</p>
<p>The log-density is <span class="math display">\[
\log f(x | \mu, \sigma^2) = -\frac{1}{2} \log(\sigma^2)
-\frac{1}{2 \sigma^2} (x-\mu)^2 - \frac{1}{2}\log(2 \pi)
\]</span> The second derivative with respect to <span class="math inline">\(\mu\)</span> is <span class="math display">\[
\frac{d^2}{d\mu^2} \log f(x | \mu, \sigma^2) = -\frac{1}{\sigma^2}
\]</span> Therefore the expected Fisher information is <span class="math display">\[
\boldsymbol I^{\text{Fisher}}\left(\mu\right) = \frac{1}{\sigma^2}
\]</span></p>
</div>
</section>
<section id="models-with-multiple-parameters" class="level3">
<h3 class="anchored" data-anchor-id="models-with-multiple-parameters">Models with multiple parameters</h3>
<div id="exm-expectedfishernormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.5</strong></span> Expected Fisher information for the normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<p>The log-density is <span class="math display">\[
\log f(x | \mu, \sigma^2) = -\frac{1}{2} \log(\sigma^2)
-\frac{1}{2 \sigma^2} (x-\mu)^2 - \frac{1}{2}\log(2 \pi)
\]</span> The gradient with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> (!) is the vector <span class="math display">\[
\nabla \log f(x | \mu, \sigma^2) =
\begin{pmatrix}
\frac{1}{\sigma^2} (x-\mu) \\
- \frac{1}{2 \sigma^2} + \frac{1}{2 \sigma^4} (x- \mu)^2 \\
\end{pmatrix}
\]</span> Hint for calculating the gradient: replace <span class="math inline">\(\sigma^2\)</span> by <span class="math inline">\(v\)</span> and then take the partial derivative with regard to <span class="math inline">\(v\)</span>, then substitute back.</p>
<p>The corresponding Hessian matrix is <span class="math display">\[
\nabla \nabla^T \log f(x | \mu, \sigma^2) =
\begin{pmatrix}
-\frac{1}{\sigma^2} &amp; -\frac{1}{\sigma^4} (x-\mu)\\
-\frac{1}{\sigma^4} (x-\mu) &amp;  \frac{1}{2\sigma^4} - \frac{1}{\sigma^6}(x- \mu)^2 \\
\end{pmatrix}
\]</span> As <span class="math inline">\(\text{E}(x) = \mu\)</span> we have <span class="math inline">\(\text{E}(x-\mu) =0\)</span>. Furthermore, with <span class="math inline">\(\text{E}( (x-\mu)^2 ) =\sigma^2\)</span> we see that <span class="math inline">\(\text{E}\left(\frac{1}{\sigma^6}(x- \mu)^2\right) = \frac{1}{\sigma^4}\)</span>. Therefore the expected Fisher information matrix as the negative expected Hessian matrix is <span class="math display">\[
\boldsymbol I^{\text{Fisher}}\left(\mu,\sigma^2\right) = \begin{pmatrix} \frac{1}{\sigma^2} &amp; 0 \\ 0 &amp; \frac{1}{2\sigma^4} \end{pmatrix}
\]</span></p>
</div>
<div id="exm-catexpectfisher" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.6</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Expected Fisher information of the categorical distribution:</p>
<p>The log-probability mass function for the categorical distribution with <span class="math inline">\(K\)</span> classes and <span class="math inline">\(K-1\)</span> free parameters <span class="math inline">\(\pi_1, \ldots, \pi_{K-1}\)</span> is <span class="math display">\[
\begin{split}
\log p(\boldsymbol x| \pi_1, \ldots, \pi_{K-1}  ) &amp; =\sum_{k=1}^{K-1}  x_k \log \pi_k    + x_K \log \pi_K \\
&amp; =\sum_{k=1}^{K-1}  x_k \log \pi_k    + \left( 1 - \sum_{k=1}^{K-1} x_k  \right) \log \left( 1 - \sum_{k=1}^{K-1} \pi_k \right) \\
\end{split}
\]</span></p>
<p>From the log-probability mass function we compute the Hessian matrix of second order partial derivatives <span class="math inline">\(\nabla \nabla^T \log p(\boldsymbol x| \pi_1, \ldots, \pi_{K-1} )\)</span> with regard to <span class="math inline">\(\pi_1, \ldots, \pi_{K-1}\)</span>:</p>
<ul>
<li><p>The diagonal entries of the Hessian matrix (with <span class="math inline">\(i=1, \ldots, K-1\)</span>) are <span class="math display">\[
\frac{\partial^2}{\partial \pi_i^2} \log p(\boldsymbol x|\pi_1, \ldots, \pi_{K-1}) =
-\frac{x_i}{\pi_i^2}-\frac{x_K}{\pi_K^2}
\]</span></p></li>
<li><p>the off-diagonal entries are (with <span class="math inline">\(j=1, \ldots, K-1\)</span> and <span class="math inline">\(j \neq i\)</span>) <span class="math display">\[
\frac{\partial^2}{\partial \pi_i \partial \pi_j} \log p(\boldsymbol x|\pi_1, \ldots, \pi_{K-1}) =
-\frac{ x_K}{\pi_K^2}
\]</span></p></li>
</ul>
<p>Recalling that <span class="math inline">\(\text{E}(x_i) = \pi_i\)</span> we obtain the expected Fisher information matrix for a categorical distribution as <span class="math inline">\(K-1  \times K-1\)</span> dimensional matrix <span class="math display">\[
\begin{split}
\boldsymbol I^{\text{Fisher}}\left( \pi_1, \ldots, \pi_{K-1}  \right) &amp;=
-\text{E}\left( \nabla \nabla^T \log p(\boldsymbol x| \pi_1, \ldots, \pi_{K-1}) \right) \\
&amp; =
\begin{pmatrix}
\frac{1}{\pi_1} + \frac{1}{\pi_K} &amp; \cdots &amp; \frac{1}{\pi_K} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{1}{\pi_K} &amp; \cdots &amp; \frac{1}{\pi_{K-1}} + \frac{1}{\pi_K} \\
\end{pmatrix}\\
&amp; = \text{Diag}\left( \frac{1}{\pi_1} , \ldots,  \frac{1}{\pi_{K-1}}   \right) + \frac{1}{\pi_K} \mathbf 1\\
\end{split}
\]</span></p>
<p>For <span class="math inline">\(K=2\)</span> and <span class="math inline">\(\pi_1=\theta\)</span> this reduces to the expected Fisher information of a Bernoulli variable, see <a href="#exm-expectedfisherbernoulli" class="quarto-xref">Example&nbsp;<span>5.2</span></a>. <span class="math display">\[
\begin{split}
I^{\text{Fisher}}(\theta) &amp; =  \left(\frac{1}{\theta} + \frac{1}{1-\theta} \right) \\
  &amp;= \frac{1}{\theta (1-\theta)} \\
\end{split}
\]</span></p>
</div>
<div id="exm-catquadapproxkl" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.7</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Quadratic approximation of KL divergence of the categorical distribution and the Neyman and Pearson divergence:</p>
<p>We now consider the local approximation of the KL divergence <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> between the categorical distribution <span class="math inline">\(Q=\text{Cat}(\boldsymbol q)\)</span> with probabilities <span class="math inline">\(\boldsymbol q=(q_1, \ldots, q_K)^T\)</span> with the categorical distribution <span class="math inline">\(P=\text{Cat}(\boldsymbol p)\)</span> with probabilities <span class="math inline">\(\boldsymbol p= (p_1, \ldots, p_K)^T\)</span>.</p>
<p>From <a href="04-entropy2.html#exm-catkl" class="quarto-xref">Example&nbsp;<span>4.6</span></a> we already know the KL divergence and from <a href="#exm-catexpectfisher" class="quarto-xref">Example&nbsp;<span>5.6</span></a> the corresponding expected Fisher information.</p>
<p>First, we keep the first argument <span class="math inline">\(Q\)</span> fixed and assume that <span class="math inline">\(P\)</span> is a perturbed version of <span class="math inline">\(Q\)</span> with <span class="math inline">\(\boldsymbol p= \boldsymbol q+\boldsymbol \varepsilon\)</span>. Note that the perturbations <span class="math inline">\(\boldsymbol \varepsilon=(\varepsilon_1, \ldots, \varepsilon_K)^T\)</span> satisfy <span class="math inline">\(\sum_{k=1}^K \varepsilon_k = 0\)</span> because <span class="math inline">\(\sum_{k=1}^K q_i=1\)</span> and <span class="math inline">\(\sum_{k=1}^K p_i=1\)</span>. Thus <span class="math inline">\(\varepsilon_K = -\sum_{k=1}^{K-1} \varepsilon_k\)</span>. Then <span class="math display">\[
\begin{split}
D_{\text{KL}}(\text{Cat}(\boldsymbol q), \text{Cat}(\boldsymbol q+\boldsymbol \varepsilon))
&amp;  \approx \frac{1}{2} (\varepsilon_1, \ldots,  \varepsilon_{K-1}) \,
\boldsymbol I^{\text{Fisher}}\left( q_1, \ldots, q_{K-1}  \right)
\begin{pmatrix} \varepsilon_1 \\ \vdots \\  \varepsilon_{K-1}\\
\end{pmatrix} \\
&amp;= \frac{1}{2} \left( \sum_{k=1}^{K-1} \frac{\varepsilon_k^2}{q_k}   + \frac{ \left(\sum_{k=1}^{K-1} \varepsilon_k\right)^2}{q_K} \right)  \\
&amp;= \frac{1}{2}  \sum_{k=1}^{K} \frac{\varepsilon_k^2}{q_k}\\
&amp;= \frac{1}{2}  \sum_{k=1}^{K} \frac{(q_k-p_k)^2}{q_k}\\
&amp; = \frac{1}{2} D_{\text{Neyman}}(Q, P)\\
\end{split}
\]</span> Similarly, if we keep <span class="math inline">\(P\)</span> fixed and consider <span class="math inline">\(Q\)</span> as a perturbed version of <span class="math inline">\(P\)</span> we get <span class="math display">\[
\begin{split}
D_{\text{KL}}(\text{Cat}(\boldsymbol p+\boldsymbol \varepsilon), \text{Cat}(\boldsymbol p))
&amp;\approx \frac{1}{2}  \sum_{k=1}^{K} \frac{(q_k-p_k)^2}{p_k}\\
&amp;= \frac{1}{2} D_{\text{Pearson}}(Q, P)
\end{split}
\]</span> Note that in both approximations we divide by the probabilities of the distribution that is kept fixed.</p>
<p>Note the appearance of the <em>Pearson <span class="math inline">\(\chi^2\)</span> divergence</em> and the <em>Neyman <span class="math inline">\(\chi^2\)</span> divergence</em> in the above. Both are, like the KL divergence, part of the family of <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergences</a>. The Neyman <span class="math inline">\(\chi^2\)</span> divergence is also known as the reverse Pearson divergence as <span class="math inline">\(D_{\text{Neyman}}(Q, P) = D_{\text{Pearson}}(P, Q)\)</span>.</p>
</div>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>A recent review is given, e.g., in: Nielsen, F. 2020. <em>An elementary introduction to information geometry.</em> Entropy <strong>22</strong>:1100. <a href="https://doi.org/10.3390/e22101100" class="uri">https://doi.org/10.3390/e22101100</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04-entropy2.html" class="pagination-link" aria-label="Relative entropy">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Relative entropy</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./06-entropy4.html" class="pagination-link" aria-label="Principle of maximum entropy">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Principle of maximum entropy</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>