<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Relative entropy – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-entropy3.html" rel="next">
<link href="./03-entropy1.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-0c40499cbe776dcbb59891309e44f779.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./04-entropy2.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Relative entropy</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropy and likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions for statistical models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Relative entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Principle of maximum entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#cross-entropy" id="toc-cross-entropy" class="nav-link active" data-scroll-target="#cross-entropy"><span class="header-section-number">4.1</span> Cross-entropy</a>
  <ul class="collapse">
  <li><a href="#definition-of-cross-entropy" id="toc-definition-of-cross-entropy" class="nav-link" data-scroll-target="#definition-of-cross-entropy">Definition of cross-entropy</a></li>
  <li><a href="#properties-of-cross-entropy" id="toc-properties-of-cross-entropy" class="nav-link" data-scroll-target="#properties-of-cross-entropy">Properties of cross-entropy</a></li>
  <li><a href="#gibbs-inequality" id="toc-gibbs-inequality" class="nav-link" data-scroll-target="#gibbs-inequality">Gibbs’ inequality</a></li>
  </ul></li>
  <li><a href="#boltzmann-relative-entropy-and-kl-divergence" id="toc-boltzmann-relative-entropy-and-kl-divergence" class="nav-link" data-scroll-target="#boltzmann-relative-entropy-and-kl-divergence"><span class="header-section-number">4.2</span> Boltzmann relative entropy and KL divergence</a>
  <ul class="collapse">
  <li><a href="#boltzmann-entropy-aka-relative-entropy" id="toc-boltzmann-entropy-aka-relative-entropy" class="nav-link" data-scroll-target="#boltzmann-entropy-aka-relative-entropy">Boltzmann entropy aka relative entropy</a></li>
  <li><a href="#definition-of-kl-divergence" id="toc-definition-of-kl-divergence" class="nav-link" data-scroll-target="#definition-of-kl-divergence">Definition of KL divergence</a></li>
  <li><a href="#properties-of-kl-divergence-and-boltzmann-relative-entropy" id="toc-properties-of-kl-divergence-and-boltzmann-relative-entropy" class="nav-link" data-scroll-target="#properties-of-kl-divergence-and-boltzmann-relative-entropy">Properties of KL divergence and Boltzmann relative entropy</a></li>
  <li><a href="#invariance-and-data-processing-properties" id="toc-invariance-and-data-processing-properties" class="nav-link" data-scroll-target="#invariance-and-data-processing-properties">Invariance and data processing properties</a></li>
  <li><a href="#further-properties" id="toc-further-properties" class="nav-link" data-scroll-target="#further-properties">Further properties</a></li>
  <li><a href="#origin-of-boltzmann-relative-entropy-and-kl-divergence-and-naming-conventions" id="toc-origin-of-boltzmann-relative-entropy-and-kl-divergence-and-naming-conventions" class="nav-link" data-scroll-target="#origin-of-boltzmann-relative-entropy-and-kl-divergence-and-naming-conventions">Origin of Boltzmann relative entropy and KL divergence and naming conventions</a></li>
  </ul></li>
  <li><a href="#kl-divergence-examples" id="toc-kl-divergence-examples" class="nav-link" data-scroll-target="#kl-divergence-examples"><span class="header-section-number">4.3</span> KL divergence examples</a>
  <ul class="collapse">
  <li><a href="#models-with-a-single-parameter" id="toc-models-with-a-single-parameter" class="nav-link" data-scroll-target="#models-with-a-single-parameter">Models with a single parameter</a></li>
  <li><a href="#models-with-multiple-parameters" id="toc-models-with-multiple-parameters" class="nav-link" data-scroll-target="#models-with-multiple-parameters">Models with multiple parameters</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./04-entropy2.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Relative entropy</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Relative entropy</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="cross-entropy" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="cross-entropy"><span class="header-section-number">4.1</span> Cross-entropy</h2>
<section id="definition-of-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="definition-of-cross-entropy">Definition of cross-entropy</h3>
<p>Given the scoring rule <span class="math inline">\(S(x, P)\)</span> we now compute its expectation assuming <span class="math inline">\(x \sim Q\)</span>, i.e.&nbsp;with regard to another distribution <span class="math inline">\(Q\)</span>: <span class="math display">\[
\begin{split}
H(Q, P) &amp; =\text{E}_Q\left( S(x, P) \right)\\
&amp; = -\text{E}_Q\left( \log p(x)  \right)\\
\end{split}
\]</span> For the logarithmic scoring rule this is called the <strong>cross-entropy</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>In the above the distribution <span class="math inline">\(Q\)</span> represent the data-generating process (note the expectation <span class="math inline">\(\text{E}_Q\)</span> with regard to <span class="math inline">\(Q\)</span>) and the distribution <span class="math inline">\(P\)</span> is the the model that is evaluated on the observations (via <span class="math inline">\(-\log p(x)\)</span>). Thus, cross-entropy is a functional of two distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>.</p>
<p>For two discrete distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> with probability mass functions <span class="math inline">\(q(x)\)</span> and <span class="math inline">\(p(x)\)</span> with <span class="math inline">\(x\in \Omega\)</span> the cross-entropy is computed as the weighted sum <span class="math display">\[
H(Q, P) = - \sum_{x \in \Omega}  \log p(x) \, q(x)
\]</span> It can be interpreted as the expected cost or expected code length when the data are generated according to model <span class="math inline">\(Q\)</span> (“sender”, “encoder”) and but we use model <span class="math inline">\(P\)</span> to describe the data (“receiver”, “decoder”).</p>
<p>For two continuous distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> with densities <span class="math inline">\(q(x)\)</span> and <span class="math inline">\(p(x)\)</span> we compute the integral <span class="math display">\[H(Q, P) =- \int_x  \log p(x)\, q(x) \, dx\]</span></p>
<div id="exm-crossentropynormals" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1</strong></span> Cross-entropy between two normals:</p>
<p>Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2_{\text{ref}})\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>. The cross-entropy <span class="math inline">\(H(F_{\text{ref}}, F)\)</span> is <span class="math display">\[
\begin{split}
H(F_{\text{ref}}, F) &amp;=  -\text{E}_{F_{\text{ref}}} \left( \log p(x |\mu, \sigma^2) \right)\\
&amp;=  \frac{1}{2}  \text{E}_{F_{\text{ref}}} \left(  \log(2\pi\sigma^2)  + \frac{(x-\mu)^2}{\sigma^2} \right) \\
&amp;= \frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 }
+\frac{\sigma^2_{\text{ref}}}{\sigma^2}  +\log(2 \pi \sigma^2) \right)  \\
\end{split}
\]</span> using <span class="math inline">\(\text{E}_{F_{\text{ref}}} ((x-\mu)^2) = (\mu_{\text{ref}}-\mu)^2 + \sigma^2_{\text{ref}}\)</span>.</p>
</div>
</section>
<section id="properties-of-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-cross-entropy">Properties of cross-entropy</h3>
<ul>
<li>Cross-entropy is not symmetric with regard to <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>, because the expectation is taken with reference to <span class="math inline">\(Q\)</span>.</li>
<li>If both distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are identical, cross-entropy reduces to entropy, i.e.&nbsp;<span class="math inline">\(H(Q, Q) = H(Q)\)</span>.</li>
<li>Like differential entropy <strong>cross-entropy changes under variable transformation</strong> for continuous random variables, say from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span>, hence <span class="math inline">\(H(Q_y, P_y) \neq H(Q_x, P_x)\)</span>.</li>
</ul>
</section>
<section id="gibbs-inequality" class="level3">
<h3 class="anchored" data-anchor-id="gibbs-inequality">Gibbs’ inequality</h3>
<p>A crucial further property of the cross-entropy <span class="math inline">\(H(Q, P)\)</span> is that it is bounded below by the entropy of <span class="math inline">\(Q\)</span>, therefore <span class="math display">\[
H(Q, P) \geq H(Q)
\]</span> with equality only if <span class="math inline">\(Q=P\)</span>. This is known as <a href="https://en.wikipedia.org/wiki/Gibbs%27_inequality"><strong>Gibbs’ inequality</strong></a>.</p>
<p>This follows from <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality"><strong>Jensen’s inequality</strong></a>. For details see Worksheet E1.</p>
<p>Essentially this means that when data are generated (encoded) under model <span class="math inline">\(Q\)</span> and described (decoded) using model <span class="math inline">\(P\)</span> there is always an extra cost, or penalty, to employ the approximating model <span class="math inline">\(P\)</span> rather than the correct model <span class="math inline">\(Q\)</span> .</p>
<div id="exm-scoringrulegibbs" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Logarithmic scoring rule is strictly proper:</p>
<p>Following the definition (see <a href="03-entropy1.html#nte-scoringrules" class="quarto-xref">Note&nbsp;<span>3.1</span></a>) as result of the Gibbs’ inequality, the logarithmic scoring rule <span class="math inline">\(S(x, P) = -\log p(x)\)</span> is <em>strictly proper</em> because its risk, the cross-entropy <span class="math inline">\(H(Q, P)\)</span>, is uniquely minimised at <span class="math inline">\(P=Q\)</span>, with the minimum risk being the entropy <span class="math inline">\(H(Q)\)</span> of the data-generating model&nbsp;<span class="math inline">\(Q\)</span>.</p>
</div>
<div id="exm-crossentropylowerbound" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.3</strong></span> Normal differential entropy as lower bound of normal cross-entropy:</p>
<p>Revisit the cross-entropy <span class="math inline">\(H(F_{\text{ref}},F)\)</span> in <a href="#exm-crossentropynormals" class="quarto-xref">Example&nbsp;<span>4.1</span></a>. Setting <span class="math inline">\(\mu_{\text{ref}} = \mu\)</span> and <span class="math inline">\(\sigma^2_{\text{ref}} = \sigma^2\)</span> the normal cross-entropy degenerates to the normal differential entropy <span class="math inline">\(H(F_{\text{ref}}) = \frac{1}{2} \left(\log( 2 \pi \sigma^2_{\text{ref}}) +1 \right)\)</span> as obtained in <a href="03-entropy1.html#exm-entropynormal" class="quarto-xref">Example&nbsp;<span>3.10</span></a>.</p>
</div>
</section>
</section>
<section id="boltzmann-relative-entropy-and-kl-divergence" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="boltzmann-relative-entropy-and-kl-divergence"><span class="header-section-number">4.2</span> Boltzmann relative entropy and KL divergence</h2>
<section id="boltzmann-entropy-aka-relative-entropy" class="level3">
<h3 class="anchored" data-anchor-id="boltzmann-entropy-aka-relative-entropy">Boltzmann entropy aka relative entropy</h3>
<p>The <strong>Boltzmann entropy</strong> of a distribution <span class="math inline">\(Q\)</span> relative to a distribution <span class="math inline">\(P\)</span> is given by <span class="math display">\[
\begin{split}
B(Q, P) &amp;=  H(Q) - H(Q, P) \\
&amp;=  -\text{E}_Q\log\left(\frac{q(x)}{p(x)}\right) \\
\end{split}
\]</span> The Boltzmann entropy is also known as <strong>relative entropy</strong>.</p>
<p>As a consequence of the Gibbs’s inequality we see that the Boltzmann entropy is always non-positive, <span class="math inline">\(B(Q, P) \leq 0\)</span>. In <a href="#exm-empiricalcatkl" class="quarto-xref">Example&nbsp;<span>4.8</span></a> it is shown that <span class="math inline">\(B(Q, P)\)</span> can be interpreted as a log-probability.</p>
<p>By construction, the Boltzmann entropy of <span class="math inline">\(Q\)</span> relative to a uniform distribution <span class="math inline">\(U\)</span> (with constant probability density mass function) is <span class="math display">\[
B(Q, U) = H(Q) + \text{const.}
\]</span> i.e.&nbsp;it is equal to the entropy of <span class="math inline">\(Q\)</span> apart from a constant.</p>
</section>
<section id="definition-of-kl-divergence" class="level3">
<h3 class="anchored" data-anchor-id="definition-of-kl-divergence">Definition of KL divergence</h3>
<p>The <strong>KL divergence</strong> is defined as the negative of the Boltzmann relative entropy as <span class="math display">\[
\begin{split}
D_{\text{KL}}(Q,P)  &amp;= H(Q, P)-H(Q) \\
            &amp; = \text{E}_Q\log\left(\frac{q(x)}{p(x)}\right)\\
\end{split}
\]</span> As a consequence of the Gibbs inequality the KL divergence is always non-negative: <span class="math inline">\(D_{\text{KL}}(Q, H) \geq 0\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>The KL divergence <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> can be interpreted as the additional cost if model <span class="math inline">\(P\)</span> is used instead <span class="math inline">\(Q\)</span> to describe data from <span class="math inline">\(Q\)</span>. If <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are identical there is no extra cost and <span class="math inline">\(D_{\text{KL}}(Q,P)=0\)</span>. However, if they are not identical then there is an additional cost and <span class="math inline">\(D_{\text{KL}}(Q,P)&gt; 0\)</span>.</p>
<p><span class="math inline">\(D_{\text{KL}}(Q,P)\)</span> thus measures the <strong>divergence</strong><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> between the two distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>. The use of the term “divergence” rather than “distance” is a reminder that <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are not interchangeable in <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span>.</p>
</section>
<section id="properties-of-kl-divergence-and-boltzmann-relative-entropy" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-kl-divergence-and-boltzmann-relative-entropy">Properties of KL divergence and Boltzmann relative entropy</h3>
<p>Boltzmann relative entropy and KL divergence differ only by sign and thus share a number of key properties inherited from cross-entropy:</p>
<ol type="1">
<li><span class="math inline">\(D_{\text{KL}}(Q, P) \neq D_{\text{KL}}(Q, P)\)</span>, i.e.&nbsp;the KL divergence is not symmetric, <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> cannot be interchanged. This follows from the same property of cross-entropy.</li>
<li><span class="math inline">\(D_{\text{KL}}(Q, P)\geq 0\)</span>, follows from Gibbs’ inequality and proof via <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality"><strong>Jensen’s inequality</strong></a>.</li>
<li><span class="math inline">\(D_{\text{KL}}(Q, P) = 0\)</span> if and only if <span class="math inline">\(P=Q\)</span>, i.e., the KL divergence is zero if and only if <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are identical. Also follows from Gibbs’ inequality.</li>
</ol>
<p>For more details and proofs of properties 2 and 3 see Worksheet E1.</p>
<p>Typically, we wish to minimise KL divergence <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> and maximise Boltzmann relative entropy <span class="math inline">\(B(Q, P)\)</span>.</p>
</section>
<section id="invariance-and-data-processing-properties" class="level3">
<h3 class="anchored" data-anchor-id="invariance-and-data-processing-properties">Invariance and data processing properties</h3>
<p>A further crucial property of KL divergence is its <strong>invariance property</strong>:</p>
<ol start="4" type="1">
<li><span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> is <strong>invariant under general invertible variable transformations</strong> , so that <span class="math inline">\(D_{\text{KL}}(Q_y, P_y) =D_{\text{KL}}(Q_x, P_x)\)</span> under a change of random variable from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span>. Hence, KL divergence does not change when the sample space is reparametrised.</li>
</ol>
<p>This is a remarkable property<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> as it holds not just for discrete but also for continuous random variables. For comparison, recall that both differential entropy as well as cross-entropy for continuous random variables are not transformation invariant.</p>
<p>In the definition of KL divergence the expectation is taken over a <em>ratio of two densities</em>. The invariance is created because the Jacobian determinant changes both densities in the same way under variable transformation and thus cancel out. For more details and more formal proof of the invariance property see Worksheet&nbsp;E1.</p>
<p>More broadly, the KL divergence satisfies the <strong>data processing inequality</strong><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> i.e.&nbsp;applying a stochastic or deterministic transformation to the underlying random variables cannot increase the KL divergence <span class="math inline">\(D_{\text{KL}}(Q,P)\)</span> between <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>. Thus, by processing data you cannot increase information about which distribution generated the data.</p>
<p>Coordinate transformations can be viewed as a special case of data processing, and for <span class="math inline">\(D_{\text{KL}}(Q,P)\)</span> the data-processing inequality under general invertible transformations becomes an identity.</p>
</section>
<section id="further-properties" class="level3">
<h3 class="anchored" data-anchor-id="further-properties">Further properties</h3>
<p>The KL divergence and cross-entropy inherit a number of further useful properties from proper scoring rules. We will not cover these in this text. For example, there are various <strong>decompositions</strong> for the risk and the divergence satisfies a <strong>generalised Pythagorean theorem</strong>.</p>
<p>In summary, KL divergence stands out among divergences between distributions due to many valuable and in some cases unique properties. It is therefore not surprising that it plays a central role in statistics and machine learning.</p>
</section>
<section id="origin-of-boltzmann-relative-entropy-and-kl-divergence-and-naming-conventions" class="level3">
<h3 class="anchored" data-anchor-id="origin-of-boltzmann-relative-entropy-and-kl-divergence-and-naming-conventions">Origin of Boltzmann relative entropy and KL divergence and naming conventions</h3>
<p>Boltzmann relative entropy was first discovered by Boltzmann (1878)<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> in physics in a discrete setting in the context of statistical mechanics (see <a href="#exm-empiricalcatkl" class="quarto-xref">Example&nbsp;<span>4.8</span></a>). In statistics and information theory KL divergence was formally introduced by Kullback and Leibler (1951)<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. Good (1979)<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> credits Turing with the first statistical application in 1940/1941 in the field of cryptography.</p>
<p>The KL divergence is also known as <strong>KL information</strong> or <strong>KL information number</strong> named after two of the original authors (Kullback and Leibler) who themselves referred to this quantity as <strong>discrimination information</strong>. Another common name is <strong>information divergence</strong> or short <strong><span class="math inline">\(\boldsymbol I\)</span>-divergence</strong>. Some authors (e.g.&nbsp;Efron) call twice the KL divergence <span class="math inline">\(2 D_{\text{KL}}(Q, P) = D(Q, P)\)</span> the <strong>deviance</strong> of <span class="math inline">\(P\)</span> from <span class="math inline">\(Q\)</span>.</p>
<p>There also exist various notations for KL divergence in the literature. Here we use <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> but you will often find both <span class="math inline">\(\text{KL}(Q || P)\)</span> and <span class="math inline">\(I^{KL}(Q; P)\)</span>.</p>
<p>Especially in older literature the KL divergence is also referred to as “cross-entropy”. This use is outdated and only leads to confusion with the related but different definition of cross-entropy above.</p>
<p>Furthermore, KL divergence is also frequently referred to as “relative entropy” however this use also leads to confusion as KL divergence is normally minimised whereas entropy and relative entropy (i.e.&nbsp;Boltzmann entropy) is normally maximised. Shannon (1948) defined “relative entropy” yet differently again as the ratio of Shannon-Gibbs entropy relative to its maximum value, i.e.&nbsp;as standardised entropy.</p>
<p>In this text relative entropy always refers to <strong>Boltzmann entropy</strong> with the opposite orientation compared to <strong>KL divergence</strong>.</p>
</section>
</section>
<section id="kl-divergence-examples" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="kl-divergence-examples"><span class="header-section-number">4.3</span> KL divergence examples</h2>
<section id="models-with-a-single-parameter" class="level3">
<h3 class="anchored" data-anchor-id="models-with-a-single-parameter">Models with a single parameter</h3>
<div id="exm-klbernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.4</strong></span> KL divergence between two Bernoulli distributions <span class="math inline">\(\text{Ber}(\theta_1)\)</span> and <span class="math inline">\(\text{Ber}(\theta_2)\)</span>:</p>
<p>The “success” probabilities for the two distributions are <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, respectively, and the complementary “failure” probabilities are <span class="math inline">\(1-\theta_1\)</span> and <span class="math inline">\(1-\theta_2\)</span>. With this we get for the KL divergence <span class="math display">\[
D_{\text{KL}}(\text{Ber}(\theta_1), \text{Ber}(\theta_2))=\theta_1 \log\left( \frac{\theta_1}{\theta_2}\right) + (1-\theta_1) \log\left(\frac{1-\theta_1}{1-\theta_2}\right)
\]</span></p>
</div>
<div id="exm-klnormalequalvar" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.5</strong></span> KL divergence between two univariate normals with different means and common variance:</p>
<p>Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2)\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>.</p>
<p>Then we get <span class="math display">\[D_{\text{KL}}(F_{\text{ref}}, F )=\frac{1}{2} \left(\frac{(\mu-\mu_{\text{ref}})^2}{\sigma^2}\right)\]</span></p>
<p>Thus, the squared Euclidean distance is a special case of KL divergence. Note that in this case the KL divergence is symmetric.</p>
</div>
</section>
<section id="models-with-multiple-parameters" class="level3">
<h3 class="anchored" data-anchor-id="models-with-multiple-parameters">Models with multiple parameters</h3>
<div id="exm-catkl" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.6</strong></span> KL divergence between two categorical distributions with <span class="math inline">\(K\)</span> classes:</p>
<p>With <span class="math inline">\(Q=\text{Cat}(\boldsymbol q)\)</span> and <span class="math inline">\(P=\text{Cat}(\boldsymbol p)\)</span> and corresponding probabilities <span class="math inline">\(q_1,\dots,q_K\)</span> and <span class="math inline">\(p_1,\dots,p_K\)</span> satisfying <span class="math inline">\(\sum_{i=1}^K q_i = 1\)</span> and <span class="math inline">\(\sum_{i=1}^K p_i =1\)</span> we get:</p>
<p><span class="math display">\[\begin{equation*}
D_{\text{KL}}(Q, P)=\sum_{i=1}^K q_i\log\left(\frac{q_i}{p_i}\right)
\end{equation*}\]</span></p>
<p>To be explicit that there are only <span class="math inline">\(K-1\)</span> parameters in a categorical distribution we can also write <span class="math display">\[\begin{equation*}
D_{\text{KL}}(Q, P)=\sum_{i=1}^{K-1} q_i\log\left(\frac{q_i}{p_i}\right)  + q_K\log\left(\frac{q_K}{p_K}\right)
\end{equation*}\]</span> with <span class="math inline">\(q_K=\left(1- \sum_{i=1}^{K-1} q_i\right)\)</span> and <span class="math inline">\(p_K=\left(1- \sum_{i=1}^{K-1} p_i\right)\)</span>.</p>
</div>
<div id="exm-klnormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.7</strong></span> KL divergence between two univariate normals with different means and variances:</p>
<p>Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2_{\text{ref}})\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>. Then <span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\text{ref}},F) &amp;= H(F_{\text{ref}}, F) - H(F_{\text{ref}}) \\
&amp;= \frac{1}{2} \left(   \frac{(\mu-\mu_{\text{ref}})^2}{\sigma^2}  + \frac{\sigma_{\text{ref}}^2}{\sigma^2}
-\log\left(\frac{\sigma_{\text{ref}}^2}{\sigma^2}\right)-1  
   \right) \\
\end{split}
\]</span></p>
<p>If variances are equal then we recover the previous <a href="#exm-klnormalequalvar" class="quarto-xref">Example&nbsp;<span>4.5</span></a> as special case.</p>
</div>
<div id="exm-empiricalcatkl" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.8</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Boltzmann relative entropy as log-probability:</p>
<p>Assume <span class="math inline">\(\hat{Q}\)</span> is an empirical categorical distribution based on observed counts <span class="math inline">\(n_k\)</span> (see <a href="03-entropy1.html#exm-entropymultinomial" class="quarto-xref">Example&nbsp;<span>3.11</span></a>) and <span class="math inline">\(P\)</span> is a second categorical distribution.</p>
<p>The KL divergence is then <span class="math display">\[
\begin{split}
B(\hat{Q}, P) &amp; = H(\hat{Q}) -H(\hat{Q}, P) \\
&amp; = H(\hat{Q})  +  \sum_{i=1}^K  \log ( p_i) \, \hat{q}_i   \\
&amp; = H(\hat{Q})  +   \frac{1}{n} \sum_{i=1}^K n_i  \log p_i  \\
\end{split}
\]</span></p>
<p>For large <span class="math inline">\(n\)</span> we may use the multinomial coefficient <span class="math inline">\(W = \binom{n}{n_1, \ldots, n_K}\)</span> to obtain the entropy of <span class="math inline">\(\hat{Q}\)</span> (see <a href="03-entropy1.html#exm-entropymultinomial" class="quarto-xref">Example&nbsp;<span>3.11</span></a>). This results in <span class="math display">\[
\begin{split}
B(\hat{Q}, P) &amp;\approx \frac{1}{n} \left( \log W  + \sum_{i=1}^K n_i \log p_i   \right)\\
&amp; = \frac{1}{n} \log \left( W \times \prod_{i=1}^K  p_i^{n_i}    \right)\\
&amp; = \frac{1}{n} \log  \text{Pr}(n_1, \ldots, n_K| \,\boldsymbol p) \\
\end{split}
\]</span> Hence the Boltzmann relative entropy is directly linked to the multinomial probability of the observed counts <span class="math inline">\(n_1, \ldots, n_k\)</span> under the model <span class="math inline">\(P\)</span>. This derivation of the Boltzmann relative entropy as log-probability of a macrostate is due to Boltzmann (1878). See also Akaike (1985) for a historical account.</p>
</div>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>For other scoring rules it is called the <strong>risk</strong> of <span class="math inline">\(P\)</span> under the true model <span class="math inline">\(Q\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This follows the current accepted usage. However, in some (typically older) literature the term cross-entropy may refer instead to the KL divergence.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>For any proper scoring rule <span class="math inline">\(S(x, P)\)</span> the associated <strong>divergence</strong> or <strong>discrepancy</strong> is defined in the same fashion: <span class="math inline">\(D(Q,P) = \text{E}_Q  \left(  S(x, P) \right) - \text{E}_Q  \left(  S(x, Q)  \right) \geq 0\)</span>, i.e.&nbsp;the difference between the risk and the minimum risk. Such divergences closely correspond to <a href="https://en.wikipedia.org/wiki/Bregman_divergence"><strong>Bregman divergences</strong></a>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Note that <a href="https://en.wikipedia.org/wiki/Divergence_(statistics)">divergence between distributions</a> is unrelated to the vector calculus <a href="https://en.wikipedia.org/wiki/Divergence">divergence vector operator</a> used in vector calculus.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Even more striking, this property only holds for the KL divergence, not for any other divergence induced by a proper scoring rule, making the KL divergence sole invariant Bregman divergence.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Furthermore, the KL divergence is also the only <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergence</a> (of which the KL divergence is a principal example) that is invariant against coordinate transformation.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The data processing inequality also holds for all <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergences</a> but is notably <em>not</em> satisfied by divergences of other proper scoring rules (and thus other Bregman divergences).<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Boltzmann, L. 1878. Weitere Bemerkungen über einige Probleme der mechanischen Wärmetheorie. Wien Ber. <strong>78</strong>:7–46. <a href="https://doi.org/10.1017/CBO9781139381437.013" class="uri">https://doi.org/10.1017/CBO9781139381437.013</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Kullback, S., and R. A. Leibler. 1951. On information and sufficiency. Ann. Math. Statist. <strong>22</strong> 79–86. <a href="https://doi.org/10.1214/aoms/1177729694" class="uri">https://doi.org/10.1214/aoms/1177729694</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Good, I. J. 1979. Studies in the history of probability. XXXVII. A. M. Turing’s statistical work in world war II. Biometrika, 66:393–396. <a href="https://doi.org/10.1093/biomet/66.2.393" class="uri">https://doi.org/10.1093/biomet/66.2.393</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-entropy1.html" class="pagination-link" aria-label="Entropy">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-entropy3.html" class="pagination-link" aria-label="Expected Fisher information">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>