<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Divergence – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-entropy3.html" rel="next">
<link href="./03-entropy1.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-bf90d58e07b16a5a5517af5259b97af0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./04-entropy2.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Divergence</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropy and likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Divergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Principle of maximum entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#scoring-rules-and-risk" id="toc-scoring-rules-and-risk" class="nav-link active" data-scroll-target="#scoring-rules-and-risk"><span class="header-section-number">4.1</span> Scoring rules and risk</a>
  <ul class="collapse">
  <li><a href="#mean-log-loss-or-cross-entropy" id="toc-mean-log-loss-or-cross-entropy" class="nav-link" data-scroll-target="#mean-log-loss-or-cross-entropy">Mean log-loss or cross-entropy</a></li>
  <li><a href="#properties-of-the-mean-log-loss" id="toc-properties-of-the-mean-log-loss" class="nav-link" data-scroll-target="#properties-of-the-mean-log-loss">Properties of the mean log-loss</a></li>
  <li><a href="#gibbs-inequality" id="toc-gibbs-inequality" class="nav-link" data-scroll-target="#gibbs-inequality">Gibbs’ inequality</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples">Examples</a></li>
  </ul></li>
  <li><a href="#kl-divergence-and-boltzmann-entropy" id="toc-kl-divergence-and-boltzmann-entropy" class="nav-link" data-scroll-target="#kl-divergence-and-boltzmann-entropy"><span class="header-section-number">4.2</span> KL divergence and Boltzmann entropy</a>
  <ul class="collapse">
  <li><a href="#definition-of-kl-divergence" id="toc-definition-of-kl-divergence" class="nav-link" data-scroll-target="#definition-of-kl-divergence">Definition of KL divergence</a></li>
  <li><a href="#boltzmann-entropy-or-relative-entropy" id="toc-boltzmann-entropy-or-relative-entropy" class="nav-link" data-scroll-target="#boltzmann-entropy-or-relative-entropy">Boltzmann entropy or relative entropy</a></li>
  <li><a href="#properties-of-kl-divergence-and-boltzmann-entropy" id="toc-properties-of-kl-divergence-and-boltzmann-entropy" class="nav-link" data-scroll-target="#properties-of-kl-divergence-and-boltzmann-entropy">Properties of KL divergence and Boltzmann entropy</a></li>
  <li><a href="#invariance-and-data-processing-properties" id="toc-invariance-and-data-processing-properties" class="nav-link" data-scroll-target="#invariance-and-data-processing-properties">Invariance and data processing properties</a></li>
  <li><a href="#further-properties" id="toc-further-properties" class="nav-link" data-scroll-target="#further-properties">Further properties</a></li>
  </ul></li>
  <li><a href="#kl-divergence-examples" id="toc-kl-divergence-examples" class="nav-link" data-scroll-target="#kl-divergence-examples"><span class="header-section-number">4.3</span> KL divergence examples</a>
  <ul class="collapse">
  <li><a href="#models-with-a-single-parameter" id="toc-models-with-a-single-parameter" class="nav-link" data-scroll-target="#models-with-a-single-parameter">Models with a single parameter</a></li>
  <li><a href="#models-with-multiple-parameters" id="toc-models-with-multiple-parameters" class="nav-link" data-scroll-target="#models-with-multiple-parameters">Models with multiple parameters</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./04-entropy2.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Divergence</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-divergence" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Divergence</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter introduces entropy measures involving two distributions, such as the mean log-loss or cross-entropy, the Kullback-Leibler (KL) divergence and Boltzmann entropy or relative entropy.</p>
<section id="scoring-rules-and-risk" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="scoring-rules-and-risk"><span class="header-section-number">4.1</span> Scoring rules and risk</h2>
<section id="mean-log-loss-or-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="mean-log-loss-or-cross-entropy">Mean log-loss or cross-entropy</h3>
<p>The <strong>risk</strong> of <span class="math inline">\(P\)</span> under <span class="math inline">\(Q\)</span> associated with a scoring rule <span class="math inline">\(S(x, P)\)</span> is the expected loss <span class="math display">\[
R_Q(P) =\text{E}_Q\left( S(x, P) \right)
\]</span> For the logarithmic scoring rule the risk is the <strong>mean log-loss</strong> or <strong>cross-entropy</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="math display">\[
R_Q(P) = - \text{E}_Q \log p(x) = H(Q,P).
\]</span> The risk <span class="math inline">\(R_Q(P)\)</span> is thus a functional of two distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>, where <span class="math inline">\(Q\)</span> represents the data-generating process (note the expectation <span class="math inline">\(\text{E}_Q\)</span> with regard to <span class="math inline">\(Q\)</span>) and the distribution <span class="math inline">\(P\)</span> is the model that is evaluated on the observations.</p>
<p>For two discrete distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> with pmf <span class="math inline">\(q(x)\)</span> and <span class="math inline">\(p(x)\)</span> with <span class="math inline">\(x\in \Omega\)</span> the mean log-loss is computed as the weighted sum <span class="math display">\[
H(Q, P) = - \sum_{x \in \Omega}  \log p(x) \, q(x)
\]</span> It can be interpreted as the expected cost or expected code length when the data are generated according to model <span class="math inline">\(Q\)</span> (“sender”, “encoder”) and but we use model <span class="math inline">\(P\)</span> to describe the data (“receiver”, “decoder”).</p>
<p>For two continuous distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> with pdfs <span class="math inline">\(q(x)\)</span> and <span class="math inline">\(p(x)\)</span> we compute the integral <span class="math display">\[
H(Q, P) =- \int_x  \log p(x)\, q(x) \, dx
\]</span></p>
</section>
<section id="properties-of-the-mean-log-loss" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-the-mean-log-loss">Properties of the mean log-loss</h3>
<ul>
<li><p>The mean log-loss is not symmetric with regard to <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>, because the expectation is taken with reference to <span class="math inline">\(Q\)</span>.</p></li>
<li><p>If both distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are identical, the mean log-loss reduces to entropy, i.e.&nbsp;<span class="math inline">\(H(Q, Q) = H(Q)\)</span>.</p></li>
<li><p>Like differential entropy the <strong>mean log-loss changes under variable change</strong> for continuous random variables, such as from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span>, hence <span class="math inline">\(H(Q_y, P_y) \neq H(Q_x, P_x)\)</span>.</p></li>
</ul>
</section>
<section id="gibbs-inequality" class="level3">
<h3 class="anchored" data-anchor-id="gibbs-inequality">Gibbs’ inequality</h3>
<p>A crucial further property of the mean log-loss <span class="math inline">\(H(Q, P)\)</span> is that it is bounded below by the entropy of <span class="math inline">\(Q\)</span>, therefore <span class="math display">\[
H(Q, P) \geq H(Q)
\]</span> with equality only if <span class="math inline">\(P=Q\)</span>. This is known as <a href="https://en.wikipedia.org/wiki/Gibbs%27_inequality"><strong>Gibbs’ inequality</strong></a>.</p>
<p>This lower bound can be established<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> using <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality"><strong>Jensen’s inequality</strong></a> for convex functions (recall that <span class="math inline">\(-\log(x)\)</span> is strictly convex).</p>
<p>As a consequence, when data are generated (encoded) under model <span class="math inline">\(Q\)</span> and described (decoded) using model <span class="math inline">\(P\)</span> there is always an extra cost, or penalty, to employ the approximating model <span class="math inline">\(P\)</span> rather than the correct model <span class="math inline">\(Q\)</span>.</p>
<p>More generally, proper scoring rules (see <a href="03-entropy1.html#nte-scoringrules" class="quarto-xref">Note&nbsp;<span>3.1</span></a>) satisfy the inequality <span class="math inline">\(R_Q(P) \geq R_Q(Q)\)</span>. For strictly proper scoring rules the minimum is achieved uniquely at the true distribution <span class="math inline">\(Q\)</span> (see <a href="#nte-scoringrulegibbs" class="quarto-xref">Note&nbsp;<span>4.1</span></a>).</p>
</section>
<section id="examples" class="level3">
<h3 class="anchored" data-anchor-id="examples">Examples</h3>
<div id="exm-crossentropynormals" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1</strong></span> Mean log-loss for two normal distributions:</p>
<p>Assume <span class="math inline">\(P_1=N(\mu_1,\sigma^2_1)\)</span> and <span class="math inline">\(P_2=N(\mu_2,\sigma^2_2)\)</span>. The mean log-loss is <span class="math display">\[
\begin{split}
H(P_1, P_2) &amp;=  -\text{E}_{P_1} \left( \log p(x |\mu_2, \sigma^2_2) \right)\\
&amp;=  \frac{1}{2}  \text{E}_{P_1} \left(  \log(2\pi\sigma^2_2)  + \frac{(x-\mu_2)^2}{\sigma^2_2} \right) \\
&amp;= \frac{1}{2} \left( \frac{(\mu_1 - \mu_2)^2}{ \sigma^2_2 }
+\frac{\sigma^2_1}{\sigma^2_2}  +\log(2 \pi \sigma^2_2) \right)  \\
\end{split}
\]</span> using <span class="math inline">\(\text{E}_{P_1} ((x-\mu_2)^2) = (\mu_1-\mu_2)^2 + \sigma^2_1\)</span>.</p>
</div>
<div id="exm-normcrossenttwoentropy" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2</strong></span> Normal mean log-loss reduces to normal differential entropy:</p>
<p>The mean log-loss for two normal distributions <span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_2\)</span> is (<a href="#exm-crossentropynormals" class="quarto-xref">Example&nbsp;<span>4.1</span></a>) <span class="math display">\[
H(P_1,P_2) = \frac{1}{2} \left( \frac{(\mu_1 - \mu_2)^2}{ \sigma^2_2 }
+\frac{\sigma^2_1}{\sigma^2_2}  +\log(2 \pi \sigma^2_2) \right)
\]</span> Setting <span class="math inline">\(P_1=P_2 = P\)</span> with <span class="math inline">\(\mu_1 = \mu_2 = \mu\)</span> and <span class="math inline">\(\sigma^2_1 =  \sigma^2_2 = \sigma^2\)</span> the mean log-loss degenerates to <span class="math display">\[
H(P) = \frac{1}{2} \left(\log( 2 \pi \sigma^2) +1 \right)
\]</span> which is the normal differential entropy (<a href="03-entropy1.html#exm-entropynormal" class="quarto-xref">Example&nbsp;<span>3.11</span></a>).</p>
</div>
<div id="nte-scoringrulegibbs" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;4.1: <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Logarithmic scoring rule is strictly proper
</div>
</div>
<div class="callout-body-container callout-body">
<p>The mean log-loss <span class="math inline">\(H(Q, P)\)</span> is the risk associated with the logarithmic scoring rule <span class="math inline">\(S(x, P) = -\log p(x)\)</span>. Gibbs’ inequality states that the mean log-loss <span class="math inline">\(H(Q, P)\)</span> is uniquely minimised at <span class="math inline">\(P=Q\)</span>, with the minimum risk being the entropy <span class="math inline">\(H(Q)\)</span> of the data-generating model&nbsp;<span class="math inline">\(Q\)</span>. Therefore, the logarithmic scoring rule is <em>strictly proper</em> (see <a href="03-entropy1.html#nte-scoringrules" class="quarto-xref">Note&nbsp;<span>3.1</span></a>).</p>
</div>
</div>
</section>
</section>
<section id="kl-divergence-and-boltzmann-entropy" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="kl-divergence-and-boltzmann-entropy"><span class="header-section-number">4.2</span> KL divergence and Boltzmann entropy</h2>
<section id="definition-of-kl-divergence" class="level3">
<h3 class="anchored" data-anchor-id="definition-of-kl-divergence">Definition of KL divergence</h3>
<p>The <strong>divergence</strong><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> between <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> associated with a proper scoring rule <span class="math inline">\(S(x, P)\)</span> is defined as the difference between the risk and the minimum risk<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>: <span class="math display">\[
D(Q,P) = R_Q(P) - R_Q(Q) \geq 0
\]</span></p>
<p>For the logarithmic scoring rule this yields the <strong>Kullback-Leibler (KL) divergence</strong> between <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>: <span class="math display">\[
\begin{split}
D_{\text{KL}}(Q,P)  &amp;= H(Q, P)-H(Q) \\
            &amp; = \text{E}_Q\log\left(\frac{q(x)}{p(x)}\right)\\
\end{split}
\]</span> The KL divergence is always non-negative, <span class="math inline">\(D_{\text{KL}}(Q, H) \geq 0\)</span>, as a consequence of Gibbs’ inequality.</p>
<p>The KL divergence <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> can be interpreted as the additional risk or cost incurred if model <span class="math inline">\(P\)</span> is used instead <span class="math inline">\(Q\)</span> to describe data that actually follow <span class="math inline">\(Q\)</span>. If <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are identical there is no extra cost and <span class="math inline">\(D_{\text{KL}}(Q,P)=0\)</span>. However, if they differ then there is an additional cost and <span class="math inline">\(D_{\text{KL}}(Q,P)&gt; 0\)</span>.</p>
<p>The use of the term “divergence” rather than “distance” is a reminder that <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are not interchangeable in <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span>.</p>
<p>The KL divergence is also known as <strong>KL information</strong> , <strong>KL information number</strong> or <strong>discrimination information</strong>. Further names are <strong>information divergence</strong> or <strong><span class="math inline">\(\boldsymbol I\)</span>-divergence</strong>. Some authors refer to <span class="math inline">\(2 D_{\text{KL}}(Q, P) = D(Q, P)\)</span> as the <strong>deviance</strong> of <span class="math inline">\(P\)</span> from <span class="math inline">\(Q\)</span>.</p>
<p>Various notations for KL divergence exist, we use <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> but <span class="math inline">\(\text{KL}(Q || P)\)</span> and <span class="math inline">\(I^{KL}(Q; P)\)</span> are also common.</p>
</section>
<section id="boltzmann-entropy-or-relative-entropy" class="level3">
<h3 class="anchored" data-anchor-id="boltzmann-entropy-or-relative-entropy">Boltzmann entropy or relative entropy</h3>
<p>The <strong>Boltzmann entropy</strong> of a distribution <span class="math inline">\(Q\)</span> relative to a distribution <span class="math inline">\(P\)</span> is given by <span class="math display">\[
\begin{split}
B(Q, P) &amp;=  H(Q) - H(Q, P) \\
&amp;=  -\text{E}_Q\log\left(\frac{q(x)}{p(x)}\right) \\
\end{split}
\]</span></p>
<p>Boltzmann entropy is the negative of the KL divergence, and hence it is always non-positive, <span class="math inline">\(B(Q, P) \leq 0\)</span>.</p>
<p>Boltzmann entropy can be interpreted as <strong>relative entropy</strong>, containing entropy as a special case without a change of sign (<a href="#exm-boltzmannuniform" class="quarto-xref">Example&nbsp;<span>4.8</span></a>).</p>
<p>Furthermore, <a href="#exm-empiricalcatkl" class="quarto-xref">Example&nbsp;<span>4.9</span></a> shows that <span class="math inline">\(B(Q, P)\)</span> can be interpreted as a log-probability and that it is closely linked with the log-likelihood.</p>
<p>The terminology to refer to the <em>negative</em> KL divergence as Boltzmann entropy and as relative entropy follows <span class="citation" data-cites="Akaike1985">Akaike (<a href="bibliography.html#ref-Akaike1985" role="doc-biblioref">1985</a>)</span> and is in line with the conventions in statistical physics, where this quantity is also known as relative Boltzmann-Gibbs entropy<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> (see <a href="#nte-klnames" class="quarto-xref">Note&nbsp;<span>4.2</span></a>).</p>
<div id="nte-klnames" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;4.2: Usage of cross-entropy and relative entropy
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are inconsistent and ambiguous uses of the terms “cross-entropy” and “relative entropy” in the literature.</p>
<p>In these notes, “cross-entropy” denotes the <strong>mean log-loss</strong> and “relative entropy” refers to <strong>Boltzmann entropy</strong>.</p>
<p>Our use of “cross-entropy” follows common modern usage. However, many statistical and engineering sources use the term to mean KL divergence or relative entropy.</p>
<p>The term “relative entropy” is also ambiguous. We follow the convention in statistical mechanics and refer to relative entropy as the <em>negative</em> of KL divergence, denoted here as Boltzmann entropy, since it exhibits the correct orientation for maximisation principles and also contains entropy as special case without a change of sign. However, many authors, particularly in computer science, equate relative entropy directly with KL divergence. <span class="citation" data-cites="Shannon1948">Shannon (<a href="bibliography.html#ref-Shannon1948" role="doc-biblioref">1948</a>)</span> used relative entropy to denote entropy standardised by its maximum value.</p>
</div>
</div>
</section>
<section id="properties-of-kl-divergence-and-boltzmann-entropy" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-kl-divergence-and-boltzmann-entropy">Properties of KL divergence and Boltzmann entropy</h3>
<p>Boltzmann entropy and KL divergence differ only by sign and thus share a number of key properties inherited from mean log-loss:</p>
<ul>
<li><p><span class="math inline">\(D_{\text{KL}}(Q, P) \neq D_{\text{KL}}(Q, P)\)</span>, i.e.&nbsp;the KL divergence is not symmetric, <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> cannot be interchanged. This follows from the same property of the mean log-loss.</p></li>
<li><p><span class="math inline">\(D_{\text{KL}}(Q, P)\geq 0\)</span>, follows from Gibbs’ inequality and proof via <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality"><strong>Jensen’s inequality</strong></a>.</p></li>
<li><p><span class="math inline">\(D_{\text{KL}}(Q, P) = 0\)</span> if and only if <span class="math inline">\(P=Q\)</span>, i.e., the KL divergence is zero if and only if <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are identical. Also follows from Gibbs’ inequality.</p></li>
</ul>
<p>Typically, we will <strong>minimise quantities related to risk</strong>:</p>
<ul>
<li>KL divergence and mean log-loss</li>
</ul>
<p>and, conversely, <strong>maximise quantities related to entropy</strong>:</p>
<ul>
<li>Boltzmann entropy (and log-likelihood) and Shannon-Gibbs entropy.</li>
</ul>
</section>
<section id="invariance-and-data-processing-properties" class="level3">
<h3 class="anchored" data-anchor-id="invariance-and-data-processing-properties">Invariance and data processing properties</h3>
<p>A further crucial property of KL divergence is its <strong>invariance property</strong>:</p>
<ol start="4" type="1">
<li><span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> is <strong>invariant under general invertible variable transformations</strong> , so that <span class="math inline">\(D_{\text{KL}}(Q_y, P_y) =D_{\text{KL}}(Q_x, P_x)\)</span> under a change of random variable from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span>. Hence, KL divergence does not change when the sample space is reparametrised.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></li>
</ol>
<p>This is a remarkable property<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> as it holds not just for discrete but also for continuous random variables. For comparison, recall that both differential entropy as well as mean log-loss for continuous random variables are not transformation invariant.</p>
<p>In the definition of KL divergence the expectation is taken over a <em>ratio of two densities</em>. The invariance is created because the Jacobian determinant changes both densities in the same way under variable transformation and thus cancel out.</p>
<p>More broadly, the KL divergence satisfies the <strong>data processing inequality</strong><a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, i.e.&nbsp;applying a stochastic or deterministic transformation to the underlying random variables cannot increase the KL divergence <span class="math inline">\(D_{\text{KL}}(Q,P)\)</span> between <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>. Thus, by processing data you cannot increase information about which distribution generated the data.</p>
<p>Coordinate transformations can be viewed as a special case of data processing, and for <span class="math inline">\(D_{\text{KL}}(Q,P)\)</span> the data-processing inequality under general invertible transformations becomes an identity.</p>
</section>
<section id="further-properties" class="level3">
<h3 class="anchored" data-anchor-id="further-properties">Further properties</h3>
<p>The KL divergence and mean log-loss inherit a number of further useful properties from proper scoring rules. We will not cover these in this text. For example, there are various <strong>decompositions</strong> for the risk and the divergence satisfies a <strong>generalised Pythagorean theorem</strong>.</p>
<p>In summary, KL divergence stands out among divergences between distributions due to many valuable and in some cases unique properties. It is therefore not surprising that it plays a central role in statistics and machine learning.</p>
</section>
</section>
<section id="kl-divergence-examples" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="kl-divergence-examples"><span class="header-section-number">4.3</span> KL divergence examples</h2>
<section id="models-with-a-single-parameter" class="level3">
<h3 class="anchored" data-anchor-id="models-with-a-single-parameter">Models with a single parameter</h3>
<div id="exm-klbernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.3</strong></span> KL divergence between two Bernoulli distributions <span class="math inline">\(B_1 = \text{Ber}(\theta_1)\)</span> and <span class="math inline">\(B_2 = \text{Ber}(\theta_2)\)</span>:</p>
<p>The “success” probabilities for the two distributions are <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, respectively, and the complementary “failure” probabilities are <span class="math inline">\(1-\theta_1\)</span> and <span class="math inline">\(1-\theta_2\)</span>. With this we get for the KL divergence <span class="math display">\[
D_{\text{KL}}(B_1, B_2)=\theta_1 \log\left( \frac{\theta_1}{\theta_2}\right) + (1-\theta_1) \log\left(\frac{1-\theta_1}{1-\theta_2}\right)
\]</span></p>
</div>
<div id="exm-klnormalequalvar" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.4</strong></span> KL divergence between two univariate normals with different means and common variance:</p>
<p>Assume <span class="math inline">\(P_1=N(\mu_1,\sigma^2)\)</span> and <span class="math inline">\(P_2=N(\mu_2,\sigma^2)\)</span>. Setting <span class="math inline">\(\sigma^2_1 =\sigma^2_2 = \sigma^2\)</span> in the more general case of the KL divergence between two normals (<a href="#exm-klnormal" class="quarto-xref">Example&nbsp;<span>4.7</span></a>) yields <span class="math display">\[D_{\text{KL}}(P_1, P_2 )=\frac{1}{2\sigma^2} (\mu_1-\mu_2)^2
\]</span> which, apart from a scale factor, is the <strong>squared Euclidean distance</strong> or <strong>squared loss</strong> between the two means <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span>. Note that in this case the KL divergence is symmetric with regard to the two mean parameters.</p>
</div>
<div id="exm-klnormalequalmean" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.5</strong></span> KL divergence between two univariate normals with common mean and different variances:</p>
<p>Assume <span class="math inline">\(P_1=N(\mu,\sigma^2_1\)</span> and <span class="math inline">\(P_2=N(\mu,\sigma^2_2)\)</span>. Setting <span class="math inline">\(\mu_1 = \mu_2 = \mu\)</span> in the more general case of the KL divergence between two normals (<a href="#exm-klnormal" class="quarto-xref">Example&nbsp;<span>4.7</span></a>) yields</p>
<p><span class="math display">\[
D_{\text{KL}}(P_1,P_2)
= \frac{1}{2}
\left(   \frac{\sigma^2_1}{\sigma^2_2}
-\log\left(\frac{\sigma^2_1}{\sigma^2_2}\right)-1  \right)
\]</span> This is a convex function of the ratio <span class="math inline">\(\sigma_1^2/\sigma^2_2\)</span> of the two variances. Apart from the scale factor this is known as <strong>Stein’s loss</strong> between the two variances <span class="math inline">\(\sigma^2_1\)</span> and <span class="math inline">\(\sigma^2_2\)</span>.</p>
</div>
</section>
<section id="models-with-multiple-parameters" class="level3">
<h3 class="anchored" data-anchor-id="models-with-multiple-parameters">Models with multiple parameters</h3>
<div id="exm-catkl" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.6</strong></span> KL divergence between two categorical distributions with <span class="math inline">\(K\)</span> classes:</p>
<p>With <span class="math inline">\(Q=\text{Cat}(\boldsymbol q)\)</span> and <span class="math inline">\(P=\text{Cat}(\boldsymbol p)\)</span> and corresponding probabilities <span class="math inline">\(q_1,\dots,q_K\)</span> and <span class="math inline">\(p_1,\dots,p_K\)</span> satisfying <span class="math inline">\(\sum_{i=1}^K q_i = 1\)</span> and <span class="math inline">\(\sum_{i=1}^K p_i =1\)</span> we get:</p>
<p><span class="math display">\[\begin{equation*}
D_{\text{KL}}(Q, P)=\sum_{i=1}^K q_i\log\left(\frac{q_i}{p_i}\right)
\end{equation*}\]</span></p>
<p>To be explicit that there are only <span class="math inline">\(K-1\)</span> parameters in a categorical distribution we can also write <span class="math display">\[\begin{equation*}
D_{\text{KL}}(Q, P)=\sum_{i=1}^{K-1} q_i\log\left(\frac{q_i}{p_i}\right)  + q_K\log\left(\frac{q_K}{p_K}\right)
\end{equation*}\]</span> with <span class="math inline">\(q_K=\left(1- \sum_{i=1}^{K-1} q_i\right)\)</span> and <span class="math inline">\(p_K=\left(1- \sum_{i=1}^{K-1} p_i\right)\)</span>.</p>
</div>
<div id="exm-klnormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.7</strong></span> KL divergence between two univariate normals with different means and variances:</p>
<p>Assume <span class="math inline">\(P_1=N(\mu_1,\sigma^2_1)\)</span> and <span class="math inline">\(P_2=N(\mu_2,\sigma^2_2)\)</span>. Then with <a href="03-entropy1.html#exm-entropynormal" class="quarto-xref">Example&nbsp;<span>3.11</span></a> (entropy of normal) and <a href="#exm-crossentropynormals" class="quarto-xref">Example&nbsp;<span>4.1</span></a> (mean log-loss between two normals) we get <span class="math display">\[
\begin{split}
D_{\text{KL}}(P_1,P_2) &amp;= H(P_1, P_2) - H(P_1) \\
&amp;= \frac{1}{2} \left(   \frac{(\mu_1-\mu_2)^2}{\sigma^2_2}  + \frac{\sigma^2_1}{\sigma^2_2}
-\log\left(\frac{\sigma^2_1}{\sigma^2_2}\right)-1  
   \right) \\
\end{split}
\]</span></p>
<p>For the two special cases of equal variances (<span class="math inline">\(\sigma^2_1  =\sigma^2_2 = \sigma^2\)</span>) and equal means <span class="math inline">\((\mu_1 = \mu_2 = \mu\)</span>) see <a href="#exm-klnormalequalvar" class="quarto-xref">Example&nbsp;<span>4.4</span></a> and <a href="#exm-klnormalequalmean" class="quarto-xref">Example&nbsp;<span>4.5</span></a>.</p>
</div>
<div id="exm-boltzmannuniform" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.8</strong></span> Boltzmann entropy reduces to entropy:</p>
<p>The mean log-loss <span class="math inline">\(H(Q, U)\)</span> for true model <span class="math inline">\(Q\)</span> and a uniform distribution <span class="math inline">\(U\)</span> with constant pdmf is <span class="math display">\[
H(Q, U) = \text{const.}
\]</span> and does not depend on <span class="math inline">\(Q\)</span>.</p>
<p>The Boltzmann entropy of <span class="math inline">\(Q\)</span> relative to a uniform distribution <span class="math inline">\(U\)</span> is therefore <span class="math display">\[
B(Q, U) = H(Q) + \text{const.}
\]</span> Thus, up to a constant Boltzmann entropy is equal to the entropy of <span class="math inline">\(Q\)</span>, and crucially, both quantities share the same sign (and hence orientation).</p>
</div>
<div id="exm-empiricalcatkl" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.9</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Boltzmann entropy as log-probability:</p>
<p>Assume <span class="math inline">\(\hat{Q}\)</span> is an empirical categorical distribution based on observed counts <span class="math inline">\(n_k\)</span> (see <a href="03-entropy1.html#exm-entropymultinomial" class="quarto-xref">Example&nbsp;<span>3.12</span></a>) and <span class="math inline">\(P\)</span> is a second categorical distribution.</p>
<p>The Boltzmann entropy is then <span class="math display">\[
\begin{split}
B(\hat{Q}, P) &amp; = H(\hat{Q}) -H(\hat{Q}, P) \\
&amp; = H(\hat{Q})  +  \sum_{i=1}^K  \log ( p_i) \, \hat{q}_i   \\
&amp; = H(\hat{Q})  +   \frac{1}{n} \sum_{i=1}^K n_i  \log p_i  \\
\end{split}
\]</span></p>
<p>For large <span class="math inline">\(n\)</span> we may use the multinomial coefficient <span class="math inline">\(W = \binom{n}{n_1, \ldots, n_K}\)</span> to obtain the entropy of <span class="math inline">\(\hat{Q}\)</span> (see <a href="03-entropy1.html#exm-entropymultinomial" class="quarto-xref">Example&nbsp;<span>3.12</span></a>). This results in <span class="math display">\[
\begin{split}
B(\hat{Q}, P) &amp;\approx \frac{1}{n} \left( \log W  + \sum_{i=1}^K n_i \log p_i   \right)\\
&amp; = \frac{1}{n} \log \left( W \times \prod_{i=1}^K  p_i^{n_i}    \right)\\
&amp; = \frac{1}{n} \log  \text{Pr}(n_1, \ldots, n_K| \,\boldsymbol p) \\
\end{split}
\]</span> Hence the Boltzmann entropy is directly linked to the multinomial probability of the observed counts <span class="math inline">\(n_1, \ldots, n_k\)</span> under the model <span class="math inline">\(P\)</span>. Note the Boltzmann entropy is essentially the multinomial log-likelihood.</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>A bit of history
</div>
</div>
<div class="callout-body-container callout-body">
<p>The concept of relative entropy and the insight that entropy is log- probability (see <a href="#exm-empiricalcatkl" class="quarto-xref">Example&nbsp;<span>4.9</span></a>) is due to Boltzmann, see <span class="citation" data-cites="Akaike1985">Akaike (<a href="bibliography.html#ref-Akaike1985" role="doc-biblioref">1985</a>)</span> for a historical account.</p>
<p>In statistics KL divergence was formally introduced by <span class="citation" data-cites="KullbackLeibler1951">Kullback and Leibler (<a href="bibliography.html#ref-KullbackLeibler1951" role="doc-biblioref">1951</a>)</span>, even though <span class="citation" data-cites="Good1979">Good (<a href="bibliography.html#ref-Good1979" role="doc-biblioref">1979</a>)</span> credits Turing with an earlier application in 1940/1941 in the field of cryptography.</p>
<p><span class="citation" data-cites="DawidMusio2014">Dawid and Musio (<a href="bibliography.html#ref-DawidMusio2014" role="doc-biblioref">2014</a>)</span> review entropy and divergence associated with general proper scoring rules.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Akaike1985" class="csl-entry" role="listitem">
Akaike, H. 1985. <span>“Prediction and Entropy.”</span> In <em>A Celebration of Statistics</em>, edited by A. C. Atkinson and S. E. Fienberg, 1–24. Springer. <a href="https://doi.org/10.1007/978-1-4613-8560-8_1">https://doi.org/10.1007/978-1-4613-8560-8_1</a>.
</div>
<div id="ref-DawidMusio2014" class="csl-entry" role="listitem">
Dawid, A. P., and M. Musio. 2014. <span>“Theory and Applications of Proper Scoring Rules.”</span> <em>METRON</em> 72: 169–83. <a href="https://doi.org/10.1007/s40300-014-0039-y">https://doi.org/10.1007/s40300-014-0039-y</a>.
</div>
<div id="ref-Good1979" class="csl-entry" role="listitem">
Good, I. J. 1979. <span>“<span class="nocase">A. M. Turing’s</span> Statistical Work in World War&nbsp;<span>II</span>.”</span> <em>Ann. Math. Statist.</em> 66: 393–96. <a href="https://doi.org/10.1093/biomet/66.2.393">https://doi.org/10.1093/biomet/66.2.393</a>.
</div>
<div id="ref-KullbackLeibler1951" class="csl-entry" role="listitem">
Kullback, S., and R. A. Leibler. 1951. <span>“On Information and Sufficiency.”</span> <em>Ann. Math. Statist.</em> 22: 79–86. <a href="https://doi.org/10.1214/aoms/1177729694">https://doi.org/10.1214/aoms/1177729694</a>.
</div>
<div id="ref-PachterYangDill2024" class="csl-entry" role="listitem">
Pachter, J. A., Y.-J. Yang, and K. A. Dill. 2024. <span>“Entropy, Irreversibility and Inference at the Foundations of Statistical Physics.”</span> <em>Nat. Rev. Physics</em> 6: 382–93. <a href="https://doi.org/10.1038/s42254-024-00720-5">https://doi.org/10.1038/s42254-024-00720-5</a>.
</div>
<div id="ref-Shannon1948" class="csl-entry" role="listitem">
Shannon, C. E. 1948. <span>“A Mathematical Theory of Communication.”</span> <em>Bell Syst. Tech. J.</em> 27: 379–423. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">https://doi.org/10.1002/j.1538-7305.1948.tb01338.x</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>There are multiple, inconsistent uses of the term “cross-entropy” in the literature, so we will mostly use “mean log-loss” for clarity (see <a href="#nte-klnames" class="quarto-xref">Note&nbsp;<span>4.2</span></a>).<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>For details see Worksheet E1.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Note that <a href="https://en.wikipedia.org/wiki/Divergence_(statistics)">divergence between distributions</a> is unrelated to the <a href="https://en.wikipedia.org/wiki/Divergence">divergence vector operator</a> from vector calculus.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Such divergences closely correspond to <a href="https://en.wikipedia.org/wiki/Bregman_divergence"><strong>Bregman divergences</strong></a>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>See, e.g., Eq.&nbsp;14 in <span class="citation" data-cites="PachterYangDill2024">Pachter, Yang, and Dill (<a href="bibliography.html#ref-PachterYangDill2024" role="doc-biblioref">2024</a>)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>For a proof of the invariance property see Worksheet&nbsp;E1.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Even more striking, this property only holds for the KL divergence, not for any other divergence induced by a proper scoring rule, making the KL divergence the sole invariant Bregman divergence.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Furthermore, the KL divergence is also the only <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergence</a> (of which the KL divergence is a principal example) that is invariant against coordinate transformations.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The data processing inequality also holds for all <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergences</a> but is notably <em>not</em> satisfied by divergences of other proper scoring rules (and thus other Bregman divergences).<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-entropy1.html" class="pagination-link" aria-label="Entropy">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-entropy3.html" class="pagination-link" aria-label="Expected Fisher information">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>