<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Risk and divergence – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-entropy3.html" rel="next">
<link href="./03-entropy1.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-9290db0fca16de22067673ab8036b289.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-entropy1.html">Information</a></li><li class="breadcrumb-item"><a href="./04-entropy2.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Risk and divergence</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Information</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Risk and divergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Local divergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Maximum entropy</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#risk" id="toc-risk" class="nav-link active" data-scroll-target="#risk"><span class="header-section-number">4.1</span> Risk</a>
  <ul class="collapse">
  <li><a href="#mean-log-loss-or-cross-entropy" id="toc-mean-log-loss-or-cross-entropy" class="nav-link" data-scroll-target="#mean-log-loss-or-cross-entropy">Mean log-loss or cross-entropy</a></li>
  <li><a href="#mixture-preserving" id="toc-mixture-preserving" class="nav-link" data-scroll-target="#mixture-preserving">Mixture preserving</a></li>
  <li><a href="#sec-properness" id="toc-sec-properness" class="nav-link" data-scroll-target="#sec-properness">Properness inequality</a></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation">Interpretation</a></li>
  <li><a href="#mean-log-loss-for-discrete-and-continuous-distributions" id="toc-mean-log-loss-for-discrete-and-continuous-distributions" class="nav-link" data-scroll-target="#mean-log-loss-for-discrete-and-continuous-distributions">Mean log-loss for discrete and continuous distributions</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples">Examples</a></li>
  </ul></li>
  <li><a href="#divergence" id="toc-divergence" class="nav-link" data-scroll-target="#divergence"><span class="header-section-number">4.2</span> Divergence</a>
  <ul class="collapse">
  <li><a href="#kullback-leibler-divergence" id="toc-kullback-leibler-divergence" class="nav-link" data-scroll-target="#kullback-leibler-divergence">Kullback-Leibler divergence</a></li>
  <li><a href="#convexity" id="toc-convexity" class="nav-link" data-scroll-target="#convexity">Convexity</a></li>
  <li><a href="#interpretation-1" id="toc-interpretation-1" class="nav-link" data-scroll-target="#interpretation-1">Interpretation</a></li>
  <li><a href="#invariance-under-a-change-of-variables" id="toc-invariance-under-a-change-of-variables" class="nav-link" data-scroll-target="#invariance-under-a-change-of-variables">Invariance under a change of variables</a></li>
  <li><a href="#data-processing-inequality" id="toc-data-processing-inequality" class="nav-link" data-scroll-target="#data-processing-inequality">Data processing inequality</a></li>
  <li><a href="#further-properties" id="toc-further-properties" class="nav-link" data-scroll-target="#further-properties">Further properties</a></li>
  <li><a href="#examples-1" id="toc-examples-1" class="nav-link" data-scroll-target="#examples-1">Examples</a></li>
  </ul></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">4.3</span> Further reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-entropy1.html">Information</a></li><li class="breadcrumb-item"><a href="./04-entropy2.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Risk and divergence</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-divergence" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Risk and divergence</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter introduces two measures of risk and divergence: the mean log-loss, also known as cross-entropy, and the Kullback-Leibler (KL) divergence, also known as relative entropy.</p>
<section id="risk" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="risk"><span class="header-section-number">4.1</span> Risk</h2>
<section id="mean-log-loss-or-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="mean-log-loss-or-cross-entropy">Mean log-loss or cross-entropy</h3>
<p>The <strong>risk</strong> is defined as the <strong>expected loss</strong>. For the logarithmic scoring rule <span class="math inline">\(S(x, P) = -\log p(x)\)</span> the risk of <span class="math inline">\(P\)</span> under <span class="math inline">\(Q\)</span> is the <strong>mean log-loss</strong> <span class="math display">\[
H(Q,P) = - \operatorname{E}_Q \log p(x)
\]</span> which is also known as <strong>cross-entropy</strong> (see <a href="#nte-cenames" class="quarto-xref">Note&nbsp;<span>4.2</span></a>).</p>
<p>The risk is a functional of two distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>, where <span class="math inline">\(Q\)</span> represents the data-generating process (note the expectation <span class="math inline">\(\operatorname{E}_Q\)</span> with regard to <span class="math inline">\(Q\)</span>) and the distribution <span class="math inline">\(P\)</span> is the model that is evaluated on the observations. Hence, the two arguments are not interchangeable and <span class="math inline">\(H(Q, P\)</span>) is <em>not symmetric</em> with regard to <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>.</p>
<p>The risk, like entropy, is <strong>only defined up to an affine transformation with a positive scaling factor</strong>, yielding the equivalence class <span class="math display">\[
H(Q, P)^{\text{equiv}}= - k \operatorname{E}_Q\left(\log p(x)\right)   + c
\]</span> For fixed <span class="math inline">\(Q\)</span> all <strong>equivalent risks</strong> have the same minimiser for <span class="math inline">\(P\)</span>.</p>
<div id="nte-cenames" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;4.1: Terminology for cross-entropy
</div>
</div>
<div class="callout-body-container callout-body">
<p>The term “cross-entropy” is used inconsistently in the literature, so we will generally use “mean log-loss” or “risk” instead.</p>
<p>In contemporary usage, “cross-entropy” typically refers to the mean log-loss (a risk to be minimised). However, many authors have also used “cross-entropy” to mean the KL divergence (relative entropy).</p>
</div>
</div>
</section>
<section id="mixture-preserving" class="level3">
<h3 class="anchored" data-anchor-id="mixture-preserving">Mixture preserving</h3>
<p>The risk is <strong>mixture preserving in Q</strong> meaning that <span class="math display">\[
H( Q_{\lambda}, P ) = (1-\lambda) H(Q_0, P) + \lambda H(Q_1, P)
\]</span> for the mixture <span class="math inline">\(Q_{\lambda}=(1-\lambda) Q_0 + \lambda Q_1\)</span> with <span class="math inline">\(0 &lt; \lambda &lt; 1\)</span> and <span class="math inline">\(Q_0 \neq Q_1\)</span>. This follows from the linearity of expectation.</p>
<p>Hence, unlike entropy <span class="math inline">\(H(Q)\)</span>, the mean log-loss <span class="math inline">\(H(Q, P)\)</span> is <em>not</em> concave in&nbsp;<span class="math inline">\(Q\)</span>.</p>
</section>
<section id="sec-properness" class="level3">
<h3 class="anchored" data-anchor-id="sec-properness">Properness inequality</h3>
<p>By construction, for <span class="math inline">\(P=Q\)</span> the mean log-loss reduces to the information entropy <span class="math inline">\(H(Q) = H(Q,Q)\)</span>. Moreover, <span class="math inline">\(H(Q, P)\)</span> is uniquely minimised for <span class="math inline">\(P=Q\)</span> so that the mean log-loss also satisfies the <strong>properness inequality</strong> <span class="math display">\[
H(Q, P) \geq H(Q)
\]</span> with equality only if <span class="math inline">\(P=Q\)</span>. This is also known as <a href="https://en.wikipedia.org/wiki/Gibbs%27_inequality"><strong>Gibbs’ inequality</strong></a>.</p>
<p>That information entropy is the unique lower bound of the mean log-loss can be established using <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality"><strong>Jensen’s inequality</strong></a> for convex functions (recall that <span class="math inline">\(-\log x\)</span> is strictly convex).</p>
<p>The logarithmic scoring rule (log-loss) is called <strong>strictly proper</strong> because the associated risk (mean log-loss) satisfies the <strong>properness inequality</strong>.</p>
<p>Properness is what makes the log-scoring rule (and indeed other proper scoring rules) useful as it allows to identify the underlying true model by risk minimisation.</p>
<p>If a scoring rule is (strictly) proper, then the associated entropy is (strictly) concave (see <a href="03-entropy1.html#sec-entropyconcave" class="quarto-xref"><span>Section 3.2.2</span></a>).</p>
</section>
<section id="interpretation" class="level3">
<h3 class="anchored" data-anchor-id="interpretation">Interpretation</h3>
<p>The mean log-loss can be interpreted as the expected cost or expected code length when the data are generated according to model <span class="math inline">\(Q\)</span> (“sender”, “encoder”) and but we employ model <span class="math inline">\(P\)</span> to describe the data (“receiver”, “decoder”).</p>
<p>As a consequence of properness, when data are generated (encoded) under model <span class="math inline">\(Q\)</span> and described (decoded) using model <span class="math inline">\(P\)</span> there is always an extra cost, or penalty, to employ the approximating model <span class="math inline">\(P\)</span> rather than the correct model <span class="math inline">\(Q\)</span>.</p>
</section>
<section id="mean-log-loss-for-discrete-and-continuous-distributions" class="level3">
<h3 class="anchored" data-anchor-id="mean-log-loss-for-discrete-and-continuous-distributions">Mean log-loss for discrete and continuous distributions</h3>
<p>For two discrete distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> with pmf <span class="math inline">\(q(x)\)</span> and <span class="math inline">\(p(x)\)</span> with <span class="math inline">\(x\in \Omega\)</span> the mean log-loss is computed as the weighted sum <span class="math display">\[
H(Q, P) = - \sum_{x \in \Omega} q(x) \, \log p(x)  
\]</span> Similarly, for two continuous distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> with pdfs <span class="math inline">\(q(x)\)</span> and <span class="math inline">\(p(x)\)</span> we compute the integral <span class="math display">\[
H(Q, P) =- \int_x q(x) \, \log p(x) \, dx
\]</span></p>
<p>Like differential entropy the <strong>mean log-loss is not invariant under a change of variables</strong> for continuous random variables, such as from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span>, hence <span class="math inline">\(H(Q_y, P_y) \neq H(Q_x, P_x)\)</span>.</p>
</section>
<section id="examples" class="level3">
<h3 class="anchored" data-anchor-id="examples">Examples</h3>
<div id="exm-crossentropynormals" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1</strong></span> Mean log-loss for two normal distributions:</p>
<p>Assume <span class="math inline">\(P_1=N(\mu_1,\sigma^2_1)\)</span> and <span class="math inline">\(P_2=N(\mu_2,\sigma^2_2)\)</span>. The mean log-loss is <span class="math display">\[
\begin{split}
H(P_1, P_2) &amp;=  -\operatorname{E}_{P_1} \left( \log p(x |\mu_2, \sigma^2_2) \right)\\
&amp;=  \frac{1}{2}  \operatorname{E}_{P_1} \left(  \log(2\pi\sigma^2_2)  + \frac{(x-\mu_2)^2}{\sigma^2_2} \right) \\
&amp;= \frac{1}{2} \left( \frac{(\mu_1 - \mu_2)^2}{ \sigma^2_2 }
+\frac{\sigma^2_1}{\sigma^2_2}  +\log(2 \pi \sigma^2_2) \right)  \\
\end{split}
\]</span> using <span class="math inline">\(\operatorname{E}_{P_1} ((x-\mu_2)^2) = (\mu_1-\mu_2)^2 + \sigma^2_1\)</span>.</p>
</div>
<div id="exm-normcrossenttwoentropy" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2</strong></span> Normal mean log-loss reduces to normal differential entropy:</p>
<p>The mean log-loss for two normal distributions <span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_2\)</span> is (<a href="#exm-crossentropynormals" class="quarto-xref">Example&nbsp;<span>4.1</span></a>) <span class="math display">\[
H(P_1,P_2) = \frac{1}{2} \left( \frac{(\mu_1 - \mu_2)^2}{ \sigma^2_2 }
+\frac{\sigma^2_1}{\sigma^2_2}  +\log(2 \pi \sigma^2_2) \right)
\]</span> Setting <span class="math inline">\(P_1=P_2 = P\)</span> with <span class="math inline">\(\mu_1 = \mu_2 = \mu\)</span> and <span class="math inline">\(\sigma^2_1 =  \sigma^2_2 = \sigma^2\)</span> the mean log-loss degenerates to <span class="math display">\[
H(P) = \frac{1}{2} \left(\log( 2 \pi \sigma^2) +1 \right)
\]</span> which is the normal differential entropy (<a href="03-entropy1.html#exm-entropynormal" class="quarto-xref">Example&nbsp;<span>3.9</span></a>).</p>
</div>
<div id="exm-empiricalmeanlogloss" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.3</strong></span> Empirical mean log-loss and log-likelihood</p>
<p>In the <strong>empirical mean log-loss</strong> the expectation is taken with regard to the empirical distribution <span class="math inline">\(\hat{Q}_n\)</span>. For a model family <span class="math inline">\(P(\boldsymbol \theta)\)</span> it is <span class="math display">\[
\begin{split}
H(\hat{Q}_n, P(\boldsymbol \theta)) &amp; = - \operatorname{E}_{\hat{Q}_n} (\log p(x|\boldsymbol \theta))  \\
                  &amp; = -\frac{1}{n} \sum_{i=1}^n \log p(x_i | \boldsymbol \theta) \\
                  &amp; = -\frac{1}{n} \ell_n ({\boldsymbol \theta})
\end{split}
\]</span> The empirical risk is equal to the negative log-likelihood <span class="math inline">\(\ell_n ({\boldsymbol \theta})\)</span> standardised by the sample size <span class="math inline">\(n\)</span>. Conversely, the <strong>log-likelihood</strong> is the <strong>negative empirical risk based on the log-loss multiplied by sample size <span class="math inline">\(n\)</span></strong>.</p>
<p>Therefore, maximising the log-likelihood is equal to minimising the empirical risk based on the log-loss</p>
</div>
<div id="exm-meanloglossexpfam" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.4</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Mean log-loss for an exponential family:</p>
<p>Assume <span class="math inline">\(P(\boldsymbol \eta)\)</span> is an exponential family with canonical parameters <span class="math inline">\(\boldsymbol \eta\)</span>, canonical statistics <span class="math inline">\(\boldsymbol t(x)\)</span> and log-partition function <span class="math inline">\(a(\boldsymbol \eta)\)</span> with log-pdmf<br>
<span class="math inline">\(\log p(x|\boldsymbol \eta) =  \langle \boldsymbol \eta,  \boldsymbol t(x)\rangle + \log h(x) - a(\boldsymbol \eta)\)</span>.</p>
<p>Then with <span class="math inline">\(\operatorname{E}_{ P(\boldsymbol \eta_1)}(\boldsymbol t(x))=\boldsymbol \mu_{\boldsymbol t}(\boldsymbol \eta_1)\)</span> the mean log-loss is <span class="math display">\[
\begin{split}
H(P(\boldsymbol \eta_1), P(\boldsymbol \eta_2) ) &amp;= -\operatorname{E}_{P(\boldsymbol \eta_1)}( \log p(x|\boldsymbol \eta_2) ) \\
&amp;=  a(\boldsymbol \eta_2) -\langle \boldsymbol \eta_2,  \boldsymbol \mu_{\boldsymbol t}(\boldsymbol \eta_1) \rangle -\operatorname{E}_{P(\boldsymbol \eta_1)}( \log h(x) )
\end{split}
\]</span> For <span class="math inline">\(\boldsymbol \eta_1 = \boldsymbol \eta_2 = \boldsymbol \eta\)</span> it reduces to <span class="math display">\[
H(P(\boldsymbol \eta) ) =  a(\boldsymbol \eta) -\langle \boldsymbol \eta,  \boldsymbol \mu_{\boldsymbol t}(\boldsymbol \eta) \rangle -\operatorname{E}_{P(\boldsymbol \eta)}( \log h(x) )
\]</span> the entropy of an exponential family (<a href="03-entropy1.html#exm-entropyexpfam" class="quarto-xref">Example&nbsp;<span>3.10</span></a>).</p>
</div>
</section>
</section>
<section id="divergence" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="divergence"><span class="header-section-number">4.2</span> Divergence</h2>
<section id="kullback-leibler-divergence" class="level3">
<h3 class="anchored" data-anchor-id="kullback-leibler-divergence">Kullback-Leibler divergence</h3>
<p>The <strong>divergence</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> between the two distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> based on the logarithmic scoring rule is the difference between the mean log-loss (risk) and the information entropy (minimum risk). This yields the <strong>Kullback-Leibler (KL) divergence</strong> between <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>: <span class="math display">\[
\begin{split}
D_{\text{KL}}(Q,P)  &amp;= H(Q, P)-H(Q) \\
            &amp; = \operatorname{E}_Q\log\left(\frac{q(x)}{p(x)}\right)\\
\end{split}
\]</span> By construction, as a consequence of properness, the KL divergence is always non-negative, <span class="math inline">\(D_{\text{KL}}(Q, H) \geq 0\)</span>. Furthermore, <span class="math inline">\(D_{\text{KL}}(Q, P) = 0\)</span> only for <span class="math inline">\(P=Q\)</span> (strict properness)</p>
<p>The use of the term “divergence” rather than “distance” is a reminder that <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are not interchangeable.</p>
<p>Using the KL divergence, we can write the mean log-loss as <span class="math display">\[
\begin{split}
H(Q, P) &amp;= \underbrace{D_{\text{KL}}(Q,P)}_{\geq 0} + H(Q)  \\
            &amp; \geq H(Q) \\
\end{split}
\]</span> which also includes the properness inequality.</p>
<p>The divergence is only <strong>defined up to a positive scaling factor</strong> <span class="math inline">\(k\)</span> that effectively determines the base of the logarithm: <span class="math display">\[
D_{\text{KL}}(Q,P)^{\text{equiv}}=  H(Q, P)^{\text{equiv}}-H(Q)^{\text{equiv}}  =  k \operatorname{E}_Q\log\left(\frac{q(x)}{p(x)}\right)\\
\]</span></p>
<div id="nte-cenames" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;4.2: Terminology for KL divergence
</div>
</div>
<div class="callout-body-container callout-body">
<p>Various notations exist for KL divergence, we use <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> but <span class="math inline">\(\text{KL}(Q || P)\)</span> and <span class="math inline">\(I^{KL}(Q; P)\)</span> are also common.</p>
<p>The KL divergence is also called <strong>KL information</strong>, <strong>KL information number</strong> or <strong>discrimination information</strong>. Other names include <strong>information divergence</strong> or <strong><span class="math inline">\(\boldsymbol I\)</span>-divergence</strong>. Some authors call <span class="math inline">\(2 D_{\text{KL}}(Q, P) = D(Q, P)\)</span> the <strong>deviance</strong> of <span class="math inline">\(P\)</span> from <span class="math inline">\(Q\)</span>.</p>
<p>KL divergence is frequently termed <strong>relative entropy</strong>, but this label is ambiguous as some authors use it for the <strong>negative of KL divergence</strong> (i.e.&nbsp;for information entropy with prior measure). <span class="citation" data-cites="Shannon1948">Shannon (<a href="bibliography.html#ref-Shannon1948" role="doc-biblioref">1948</a>)</span> also used the “relative” entropy to mean entropy standardised by its maximum.</p>
</div>
</div>
</section>
<section id="convexity" class="level3">
<h3 class="anchored" data-anchor-id="convexity">Convexity</h3>
<p>By construction, <span class="math inline">\(D_{\text{KL}}(Q,P)\)</span> is strictly convex in <span class="math inline">\(Q\)</span> as <span class="math inline">\(H(Q)\)</span> is strictly concave in <span class="math inline">\(Q\)</span> and <span class="math inline">\(H(Q, P)\)</span> is mixture-preserving.</p>
</section>
<section id="interpretation-1" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-1">Interpretation</h3>
<p>The KL divergence <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> can be interpreted as the excess risk or additional cost incurred if model <span class="math inline">\(P\)</span> is used to describe data that actually follow <span class="math inline">\(Q\)</span>. If <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are identical there is no extra cost and <span class="math inline">\(D_{\text{KL}}(Q,P)=0\)</span>. However, if they differ then there is an additional cost and <span class="math inline">\(D_{\text{KL}}(Q,P)&gt; 0\)</span>.</p>
</section>
<section id="invariance-under-a-change-of-variables" class="level3">
<h3 class="anchored" data-anchor-id="invariance-under-a-change-of-variables">Invariance under a change of variables</h3>
<p>A further crucial property of KL divergence is its <strong>invariance property</strong> with respect to reparametrisation of the sample space. Specifically, <span class="math inline">\(D_{\text{KL}}(Q, P)\)</span> is <strong>invariant under general invertible variable transformations</strong> , so that <span class="math inline">\(D_{\text{KL}}(Q_y, P_y) =D_{\text{KL}}(Q_x, P_x)\)</span> under a change of the random variable from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span>.</p>
<p>This is a remarkable property as it holds not just for discrete but also for continuous random variables. For comparison, recall that both differential entropy as well as mean log-loss for continuous random variables are not invariant under a change of variables.</p>
<p>In the definition of KL divergence the expectation is taken over a <em>ratio of two densities</em>. The invariance is created because the Jacobian determinant changes both densities in the same way under variable transformation and thus cancel out<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> .</p>
</section>
<section id="data-processing-inequality" class="level3">
<h3 class="anchored" data-anchor-id="data-processing-inequality">Data processing inequality</h3>
<p>More generally, the KL divergence also satisfies the <strong>data processing inequality</strong> (DPI). Applying a transformation (stochastic or deterministic, possibly coarsening) that produces <span class="math inline">\(y\)</span> from <span class="math inline">\(x\)</span>, cannot increase the divergence, so that <span class="math inline">\(D_{\text{KL}}(Q_x,P_x) \geq D_{\text{KL}}(Q_y,P_y)\)</span>. Note that a change of variables is a special case of data processing (with identity).</p>
<p>Divergences between distributions satisfying the DPI (and hence being invariant under a change of variables) form the class of <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergences</a>. The KL divergence is the <em>only</em> divergence induced by a proper scoring rule (i.e.&nbsp;the only Bregman divergence) that is also an <span class="math inline">\(f\)</span>-divergence.</p>
</section>
<section id="further-properties" class="level3">
<h3 class="anchored" data-anchor-id="further-properties">Further properties</h3>
<p>The KL divergence and mean log-loss inherit a number of further useful properties from proper scoring rules. We will not cover these in this text. For example, there are various <strong>decompositions</strong> for the risk and the divergence satisfies a <strong>generalised Pythagorean theorem</strong>.</p>
<p>In summary, KL divergence (and the underlying log-loss) stands out among divergences between distributions that is features many unique and desirable properties in a single quantity. It is therefore not surprising that it plays a central role in statistics and machine learning.</p>
</section>
<section id="examples-1" class="level3">
<h3 class="anchored" data-anchor-id="examples-1">Examples</h3>
<div id="exm-klbernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.5</strong></span> KL divergence between two Bernoulli distributions <span class="math inline">\(B_1 = \operatorname{Ber}(\theta_1)\)</span> and <span class="math inline">\(B_2 = \operatorname{Ber}(\theta_2)\)</span>:</p>
<p>The “success” probabilities for the two distributions are <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, respectively, and the complementary “failure” probabilities are <span class="math inline">\(1-\theta_1\)</span> and <span class="math inline">\(1-\theta_2\)</span>. With this we get for the KL divergence <span class="math display">\[
D_{\text{KL}}(B_1, B_2)=\theta_1 \log\left( \frac{\theta_1}{\theta_2}\right) + (1-\theta_1) \log\left(\frac{1-\theta_1}{1-\theta_2}\right)
\]</span></p>
</div>
<div id="exm-catkl" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.6</strong></span> KL divergence between two categorical distributions with <span class="math inline">\(K\)</span> classes:</p>
<p>With <span class="math inline">\(Q=\operatorname{Cat}(\boldsymbol q)\)</span> and <span class="math inline">\(P=\operatorname{Cat}(\boldsymbol p)\)</span> and corresponding probabilities <span class="math inline">\(q_1,\dots,q_K\)</span> and <span class="math inline">\(p_1,\dots,p_K\)</span> satisfying <span class="math inline">\(\sum_{i=1}^K q_i = 1\)</span> and <span class="math inline">\(\sum_{i=1}^K p_i =1\)</span> we get:</p>
<p><span class="math display">\[\begin{equation*}
D_{\text{KL}}(Q, P)=\sum_{i=1}^K q_i\log\left(\frac{q_i}{p_i}\right)
\end{equation*}\]</span></p>
<p>To be explicit that there are only <span class="math inline">\(K-1\)</span> parameters in a categorical distribution we can also write <span class="math display">\[\begin{equation*}
D_{\text{KL}}(Q, P)=\sum_{i=1}^{K-1} q_i\log\left(\frac{q_i}{p_i}\right)  + q_K\log\left(\frac{q_K}{p_K}\right)
\end{equation*}\]</span> with <span class="math inline">\(q_K=\left(1- \sum_{i=1}^{K-1} q_i\right)\)</span> and <span class="math inline">\(p_K=\left(1- \sum_{i=1}^{K-1} p_i\right)\)</span>.</p>
</div>
<div id="exm-klnormalequalvar" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.7</strong></span> KL divergence between two univariate normals with different means and common variance:</p>
<p>Assume <span class="math inline">\(P_1=N(\mu_1,\sigma^2)\)</span> and <span class="math inline">\(P_2=N(\mu_2,\sigma^2)\)</span>. Setting <span class="math inline">\(\sigma^2_1 =\sigma^2_2 = \sigma^2\)</span> in the more general case of the KL divergence between two normals (<a href="#exm-klnormal" class="quarto-xref">Example&nbsp;<span>4.9</span></a>) yields <span class="math display">\[D_{\text{KL}}(P_1, P_2 )=\frac{1}{2\sigma^2} (\mu_1-\mu_2)^2
\]</span> which, apart from a scale factor, is the <strong>squared Euclidean distance</strong> or <strong>squared loss</strong> between the two means <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span>. Note that in this case the KL divergence is symmetric with regard to the two mean parameters.</p>
</div>
<div id="exm-klnormalequalmean" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.8</strong></span> KL divergence between two univariate normals with common mean and different variances:</p>
<p>Assume <span class="math inline">\(P_1=N(\mu,\sigma^2_1\)</span> and <span class="math inline">\(P_2=N(\mu,\sigma^2_2)\)</span>. Setting <span class="math inline">\(\mu_1 = \mu_2 = \mu\)</span> in the more general case of the KL divergence between two normals (<a href="#exm-klnormal" class="quarto-xref">Example&nbsp;<span>4.9</span></a>) yields</p>
<p><span class="math display">\[
D_{\text{KL}}(P_1,P_2)
= \frac{1}{2}
\left(   \frac{\sigma^2_1}{\sigma^2_2}
-\log\left(\frac{\sigma^2_1}{\sigma^2_2}\right)-1  \right)
\]</span> This is a convex function of the ratio <span class="math inline">\(\sigma_1^2/\sigma^2_2\)</span> of the two variances. Apart from the scale factor this is known as <strong>Stein’s loss</strong> between the two variances <span class="math inline">\(\sigma^2_1\)</span> and <span class="math inline">\(\sigma^2_2\)</span>.</p>
</div>
<div id="exm-klnormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.9</strong></span> KL divergence between two univariate normals with different means and variances:</p>
<p>Assume <span class="math inline">\(P_1=N(\mu_1,\sigma^2_1)\)</span> and <span class="math inline">\(P_2=N(\mu_2,\sigma^2_2)\)</span>. Then with <a href="03-entropy1.html#exm-entropynormal" class="quarto-xref">Example&nbsp;<span>3.9</span></a> (entropy of normal) and <a href="#exm-crossentropynormals" class="quarto-xref">Example&nbsp;<span>4.1</span></a> (mean log-loss between two normals) we get <span class="math display">\[
\begin{split}
D_{\text{KL}}(P_1,P_2) &amp;= H(P_1, P_2) - H(P_1) \\
&amp;= \frac{1}{2} \left(   \frac{(\mu_1-\mu_2)^2}{\sigma^2_2}  + \frac{\sigma^2_1}{\sigma^2_2}
-\log\left(\frac{\sigma^2_1}{\sigma^2_2}\right)-1  
   \right) \\
\end{split}
\]</span></p>
<p>For the two special cases of equal variances (<span class="math inline">\(\sigma^2_1  =\sigma^2_2 = \sigma^2\)</span>) and equal means <span class="math inline">\((\mu_1 = \mu_2 = \mu\)</span>) see <a href="#exm-klnormalequalvar" class="quarto-xref">Example&nbsp;<span>4.7</span></a> and <a href="#exm-klnormalequalmean" class="quarto-xref">Example&nbsp;<span>4.8</span></a>.</p>
</div>
<div id="exm-klexpfam" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.10</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> KL divergence between two members of an exponential family:</p>
<p>Assume <span class="math inline">\(P(\boldsymbol \eta)\)</span> is an exponential family with canonical parameters <span class="math inline">\(\boldsymbol \eta\)</span>, canonical statistics <span class="math inline">\(\boldsymbol t(x)\)</span> and log-partition function <span class="math inline">\(a(\boldsymbol \eta)\)</span> with log-pdmf<br>
<span class="math inline">\(\log p(x|\boldsymbol \eta) =  \langle \boldsymbol \eta,  \boldsymbol t(x)\rangle + \log h(x) - a(\boldsymbol \eta)\)</span>.</p>
<p>Then with <span class="math inline">\(\operatorname{E}_{ P(\boldsymbol \eta_1)}(\boldsymbol t(x))=\boldsymbol \mu_{\boldsymbol t}(\boldsymbol \eta_1)\)</span> the KL divergence between two distributions <span class="math inline">\(P(\boldsymbol \eta_1)\)</span> and <span class="math inline">\(P(\boldsymbol \eta_2)\)</span> in this family is <span class="math display">\[
\begin{split}
D_{\text{KL}}(P(\boldsymbol \eta_1),  P(\boldsymbol \eta_2) ) &amp;= \operatorname{E}_{P(\boldsymbol \eta_1)}( \log p(x|\boldsymbol \eta_1) - \log p(x|\boldsymbol \eta_2)) \\
&amp;= \langle \boldsymbol \eta_1-\boldsymbol \eta_2,  \boldsymbol \mu_{\boldsymbol t}(\boldsymbol \eta_1)\rangle - \left( a(\boldsymbol \eta_1) - a(\boldsymbol \eta_2) \right)
\end{split}
\]</span></p>
<p>This can also be obtained by computing <span class="math display">\[
D_{\text{KL}}(P(\boldsymbol \eta_1),  P(\boldsymbol \eta_2) ) = H(P(\boldsymbol \eta_1), P(\boldsymbol \eta_2) )  - H(P(\boldsymbol \eta_1) )
\]</span> using the results from <a href="03-entropy1.html#exm-entropyexpfam" class="quarto-xref">Example&nbsp;<span>3.10</span></a> and <a href="#exm-meanloglossexpfam" class="quarto-xref">Example&nbsp;<span>4.4</span></a>.</p>
</div>
</section>
</section>
<section id="further-reading" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">4.3</span> Further reading</h2>
<p>A brief overview of proper scoring rules along with corresponding divergences is found in the supplementary <a href="https://strimmerlab.github.io/publications/lecture-notes/probability-distribution-refresher/index.html">Probability and Distribution Refresher notes</a>.</p>
<p>A broad review of scoring rules, including a discussion of properness, is found, e.g., in <span class="citation" data-cites="Winkler1996">Winkler (<a href="bibliography.html#ref-Winkler1996" role="doc-biblioref">1996</a>)</span>.</p>
<p>See also: <a href="https://en.wikipedia.org/wiki/Bregman_divergence">Bregman divergences (Wikipedia)</a> and <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergences (Wikipedia)</a></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>A bit of history
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://en.wikipedia.org/wiki/Solomon_Kullback">Solomon Kullback (1907–1994)</a> and <a href="https://en.wikipedia.org/wiki/Richard_Leibler">Richard Leibler (1914–2003)</a> introduced KL divergence in statistics in <span class="citation" data-cites="KullbackLeibler1951">Kullback and Leibler (<a href="bibliography.html#ref-KullbackLeibler1951" role="doc-biblioref">1951</a>)</span>. An earlier application of KL divergence in 1940/1941 is credited to <a href="https://en.wikipedia.org/wiki/Alan_Turing">Alan Turing (1912–1954)</a> by <span class="citation" data-cites="Good1979">Good (<a href="bibliography.html#ref-Good1979" role="doc-biblioref">1979</a>)</span>.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Good1979" class="csl-entry" role="listitem">
Good, I. J. 1979. <span>“Studies in the History of Probability and Statistics. <span>XXXVII</span> <span class="nocase">A. M. Turing’s</span> Statistical Work in World War&nbsp;<span>II</span>.”</span> <em>Ann. Math. Statist.</em> 66: 393–96. <a href="https://doi.org/10.1093/biomet/66.2.393">https://doi.org/10.1093/biomet/66.2.393</a>.
</div>
<div id="ref-KullbackLeibler1951" class="csl-entry" role="listitem">
Kullback, S., and R. A. Leibler. 1951. <span>“On Information and Sufficiency.”</span> <em>Ann. Math. Statist.</em> 22: 79–86. <a href="https://doi.org/10.1214/aoms/1177729694">https://doi.org/10.1214/aoms/1177729694</a>.
</div>
<div id="ref-Shannon1948" class="csl-entry" role="listitem">
Shannon, C. E. 1948. <span>“A Mathematical Theory of Communication.”</span> <em>Bell Syst. Tech. J.</em> 27: 379–423. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">https://doi.org/10.1002/j.1538-7305.1948.tb01338.x</a>.
</div>
<div id="ref-Winkler1996" class="csl-entry" role="listitem">
Winkler, R. L. 1996. <span>“Scoring Rules and the Evaluation of Probabilities (with Discussion).”</span> <em>Test</em> 5: 1–69. <a href="https://doi.org/10.1007/BF02562681">https://doi.org/10.1007/BF02562681</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The <a href="https://en.wikipedia.org/wiki/Divergence_(statistics)">divergence between distributions</a> is unrelated to the <a href="https://en.wikipedia.org/wiki/Divergence">divergence vector operator</a> from vector calculus.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>For a proof of the invariance property of KL divergence see Worksheet&nbsp;E1.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-entropy1.html" class="pagination-link" aria-label="Entropy">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-entropy3.html" class="pagination-link" aria-label="Local divergence">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Local divergence</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>