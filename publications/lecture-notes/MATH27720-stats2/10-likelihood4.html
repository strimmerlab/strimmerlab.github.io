<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Quadratic approximation and normal asymptotics – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./11-likelihood5.html" rel="next">
<link href="./09-likelihood3.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-0c40499cbe776dcbb59891309e44f779.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./10-likelihood4.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropy and likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions for statistical models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Relative entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Principle of maximum entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#approximate-distribution-of-maximum-likelihood-estimates" id="toc-approximate-distribution-of-maximum-likelihood-estimates" class="nav-link active" data-scroll-target="#approximate-distribution-of-maximum-likelihood-estimates"><span class="header-section-number">10.1</span> Approximate distribution of maximum likelihood estimates</a>
  <ul class="collapse">
  <li><a href="#quadratic-log-likelihood-of-the-multivariate-normal-model" id="toc-quadratic-log-likelihood-of-the-multivariate-normal-model" class="nav-link" data-scroll-target="#quadratic-log-likelihood-of-the-multivariate-normal-model">Quadratic log-likelihood of the multivariate normal model</a></li>
  <li><a href="#quadratic-approximation-of-a-log-likelihood-function" id="toc-quadratic-approximation-of-a-log-likelihood-function" class="nav-link" data-scroll-target="#quadratic-approximation-of-a-log-likelihood-function">Quadratic approximation of a log-likelihood function</a></li>
  <li><a href="#asymptotic-normality-of-maximum-likelihood-estimates" id="toc-asymptotic-normality-of-maximum-likelihood-estimates" class="nav-link" data-scroll-target="#asymptotic-normality-of-maximum-likelihood-estimates">Asymptotic normality of maximum likelihood estimates</a></li>
  <li><a href="#remarks-on-the-asympotics" id="toc-remarks-on-the-asympotics" class="nav-link" data-scroll-target="#remarks-on-the-asympotics">Remarks on the asympotics</a></li>
  <li><a href="#information-inequality-and-asymptotic-optimal-efficiency" id="toc-information-inequality-and-asymptotic-optimal-efficiency" class="nav-link" data-scroll-target="#information-inequality-and-asymptotic-optimal-efficiency">Information inequality and asymptotic optimal efficiency</a></li>
  <li><a href="#non-regular-models" id="toc-non-regular-models" class="nav-link" data-scroll-target="#non-regular-models">Non-regular models</a></li>
  </ul></li>
  <li><a href="#quantifying-the-uncertainty-of-maximum-likelihood-estimates" id="toc-quantifying-the-uncertainty-of-maximum-likelihood-estimates" class="nav-link" data-scroll-target="#quantifying-the-uncertainty-of-maximum-likelihood-estimates"><span class="header-section-number">10.2</span> Quantifying the uncertainty of maximum likelihood estimates</a>
  <ul class="collapse">
  <li><a href="#estimating-the-variance-of-mles" id="toc-estimating-the-variance-of-mles" class="nav-link" data-scroll-target="#estimating-the-variance-of-mles">Estimating the variance of MLEs</a></li>
  <li><a href="#examples-for-the-estimated-variance-and-asymptotic-normal-distribution" id="toc-examples-for-the-estimated-variance-and-asymptotic-normal-distribution" class="nav-link" data-scroll-target="#examples-for-the-estimated-variance-and-asymptotic-normal-distribution">Examples for the estimated variance and asymptotic normal distribution</a></li>
  <li><a href="#wald-statistic" id="toc-wald-statistic" class="nav-link" data-scroll-target="#wald-statistic">Wald statistic</a></li>
  <li><a href="#examples-of-the-squared-wald-statistic" id="toc-examples-of-the-squared-wald-statistic" class="nav-link" data-scroll-target="#examples-of-the-squared-wald-statistic">Examples of the (squared) Wald statistic</a></li>
  <li><a href="#normal-confidence-intervals-using-the-wald-statistic" id="toc-normal-confidence-intervals-using-the-wald-statistic" class="nav-link" data-scroll-target="#normal-confidence-intervals-using-the-wald-statistic">Normal confidence intervals using the Wald statistic</a></li>
  <li><a href="#normal-tests-using-the-wald-statistic" id="toc-normal-tests-using-the-wald-statistic" class="nav-link" data-scroll-target="#normal-tests-using-the-wald-statistic">Normal tests using the Wald statistic</a></li>
  <li><a href="#examples-for-normal-confidence-intervals-and-corresponding-tests" id="toc-examples-for-normal-confidence-intervals-and-corresponding-tests" class="nav-link" data-scroll-target="#examples-for-normal-confidence-intervals-and-corresponding-tests">Examples for normal confidence intervals and corresponding tests</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro1.html">Entropy and likelihood</a></li><li class="breadcrumb-item"><a href="./10-likelihood4.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="approximate-distribution-of-maximum-likelihood-estimates" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="approximate-distribution-of-maximum-likelihood-estimates"><span class="header-section-number">10.1</span> Approximate distribution of maximum likelihood estimates</h2>
<section id="quadratic-log-likelihood-of-the-multivariate-normal-model" class="level3">
<h3 class="anchored" data-anchor-id="quadratic-log-likelihood-of-the-multivariate-normal-model">Quadratic log-likelihood of the multivariate normal model</h3>
<p>Assume we observe a single sample <span class="math inline">\(\boldsymbol x\sim  N(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> with known covariance. Noting that the multivariate normal density is <span class="math display">\[
f(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma) = (2\pi)^{-\frac{d}{2}} \det(\boldsymbol \Sigma)^{-\frac{1}{2}}
\exp\left(-\frac{1}{2} (\boldsymbol x-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu) \right)
\]</span> the corresponding log-likelihood for <span class="math inline">\(\boldsymbol \mu\)</span> is <span class="math display">\[
\ell_1(\boldsymbol \mu) = C - \frac{1}{2}(\boldsymbol x-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu)
\]</span> where <span class="math inline">\(C\)</span> is a constant that does not depend on <span class="math inline">\(\boldsymbol \mu\)</span>. Note that the log-likelihood is a quadratic function (both for <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol \mu\)</span>) and the maximum of the function lies at <span class="math inline">\(\boldsymbol \mu= \boldsymbol x\)</span> with value <span class="math inline">\(C\)</span>.</p>
</section>
<section id="quadratic-approximation-of-a-log-likelihood-function" class="level3">
<h3 class="anchored" data-anchor-id="quadratic-approximation-of-a-log-likelihood-function">Quadratic approximation of a log-likelihood function</h3>
<div id="fig-approxlogl" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-approxlogl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/approxlogl.png" class="img-fluid figure-img" style="width:70.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-approxlogl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.1: Quadratic approximation of the log-likelihood function.
</figcaption>
</figure>
</div>
<p>Now consider the quadratic approximation of a general log-likelihood function <span class="math inline">\(\ell_n(\boldsymbol \theta)\)</span> for <span class="math inline">\(\boldsymbol \theta\)</span> around the MLE <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> (<a href="#fig-approxlogl" class="quarto-xref">Figure&nbsp;<span>10.1</span></a>).</p>
<p>We assume the underlying model is regular and that <span class="math inline">\(\nabla \ell_n(\hat{\boldsymbol \theta}_{ML}) = 0\)</span>, i.e.&nbsp;the gradient at the maximum vanishes. The Taylor series approximation of scalar-valued function <span class="math inline">\(f(\boldsymbol x)\)</span> around <span class="math inline">\(\boldsymbol x_0\)</span> is <span class="math display">\[
f(\boldsymbol x) = f(\boldsymbol x_0) + \nabla^T f(\boldsymbol x_0)\, (\boldsymbol x-\boldsymbol x_0) + \frac{1}{2}
(\boldsymbol x-\boldsymbol x_0)^T \nabla \nabla^T f(\boldsymbol x_0) (\boldsymbol x-\boldsymbol x_0) + \ldots
\]</span> Applied to the log-likelihood function this yields</p>
<p><span class="math display">\[\ell_n(\boldsymbol \theta) \approx \ell_n(\hat{\boldsymbol \theta}_{ML} )- \frac{1}{2}(\hat{\boldsymbol \theta}_{ML}- \boldsymbol \theta)^T J_n(\hat{\boldsymbol \theta}_{ML})(\hat{\boldsymbol \theta}_{ML}-\boldsymbol \theta)\]</span></p>
<p>This is a quadratic function with maximum at <span class="math inline">\(( \hat{\boldsymbol \theta}_{ML}, \ell_n(\hat{\boldsymbol \theta}_{ML}) )\)</span>. Note the appearance of the observed Fisher information <span class="math inline">\(J_n(\hat{\boldsymbol \theta}_{ML})\)</span> in the quadratic term. There is no linear term because of the vanishing gradient at the MLE.</p>
<p>Crucially, this approximated log-likelihood takes the same form as if <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> was sampled from a multivariate normal distribution with mean <span class="math inline">\(\boldsymbol \theta\)</span> and with covariance given by the <em>inverse</em> observed Fisher information.</p>
<p>Note that this requires a positive definite observed Fisher information matrix so that <span class="math inline">\(J_n(\hat{\boldsymbol \theta}_{ML})\)</span> is actually invertible!</p>
<div id="exm-quadapproxproportion" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.1</strong></span> Quadratic approximation of the log-likelihood for a proportion:</p>
<p>From <a href="08-likelihood2.html#exm-mleproportion" class="quarto-xref">Example&nbsp;<span>8.1</span></a> we have the log-likelihood <span class="math display">\[
\ell_n(p) = n \left( \bar{x} \log p + (1-\bar{x}) \log(1-p) \right)
\]</span> and the MLE <span class="math display">\[
\hat{p}_{ML} = \bar{x}
\]</span> and from <a href="09-likelihood3.html#exm-obsfisherproportion" class="quarto-xref">Example&nbsp;<span>9.1</span></a> the observed Fisher information <span class="math display">\[
\begin{split}
J_n(\hat{p}_{ML}) = \frac{n}{\bar{x} (1-\bar{x})}
\end{split}
\]</span> The log-likelihood at the MLE is <span class="math display">\[
\ell_n(\hat{p}_{ML}) = n \left( \bar{x} \log \bar{x} + (1-\bar{x}) \log(1-\bar{x}) \right)
\]</span> This allows us to construct the quadratic approximation of the log-likelihood around the MLE as <span class="math display">\[
\begin{split}
\ell_n(p) &amp; \approx  \ell_n(\hat{p}_{ML}) - \frac{1}{2} J_n(\hat{p}_{ML}) (p-\hat{p}_{ML})^2 \\
   &amp;= n \left( \bar{x} \log \bar{x} + (1-\bar{x}) \log(1-\bar{x}) - \frac{(p-\bar{x})^2}{2 \bar{x} (1-\bar{x})}  \right) \\
&amp;=  C + \frac{ \bar{x} p -\frac{1}{2} p^2}{ \bar{x} (1-\bar{x})/n} \\
\end{split}
\]</span> The constant <span class="math inline">\(C\)</span> does not depend on <span class="math inline">\(p\)</span>, its function is to match the approximate log-likelihood at the MLE with that of the corresponding original log-likelihood. The approximate log-likelihood takes on the form of a normal log-likelihood (<a href="08-likelihood2.html#exm-mlenormalmean" class="quarto-xref">Example&nbsp;<span>8.2</span></a>) for one observation of <span class="math inline">\(\hat{p}_{ML}=\bar{x}\)</span> from <span class="math inline">\(N\left(p, \frac{\bar{x} (1-\bar{x})}{n} \right)\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-approxberlogl" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-approxberlogl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-likelihood4_files/figure-html/fig-approxberlogl-1.png" class="img-fluid figure-img" data-fig-pos="t" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-approxberlogl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.2: Quadratic approximation of the log-likelihood for a Bernoulli model.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-approxberlogl" class="quarto-xref">Figure&nbsp;<span>10.2</span></a> shows the Bernoulli log-likelihood function and its quadratic approximation illustrated for data with <span class="math inline">\(n = 30\)</span> and <span class="math inline">\(\bar{x} = 0.7\)</span>:</p>
</div>
</section>
<section id="asymptotic-normality-of-maximum-likelihood-estimates" class="level3">
<h3 class="anchored" data-anchor-id="asymptotic-normality-of-maximum-likelihood-estimates">Asymptotic normality of maximum likelihood estimates</h3>
<p>Intuitively, it makes sense to associate large amount of curvature of the log-likelihood at the MLE with low variance of the MLE (and conversely, low amount of curvature with high variance).</p>
<p>From the above we see that for regular models:</p>
<ul>
<li>normality implies a quadratic log-likelihood,</li>
<li>conversely, taking an quadratic approximation of the log-likelihood implies approximate normality, and</li>
<li>in the quadratic approximation <strong>the inverse observed Fisher information plays the role of the covariance</strong> of the MLE.</li>
</ul>
<p>This suggests the following theorem:</p>
<p><strong>Asymptotically, the MLE of the parameters of a regular model is normally distributed around the true parameter and with covariance equal to the inverse of the observed Fisher information</strong>:</p>
<p><span class="math display">\[\hat{\boldsymbol \theta}_{ML} \overset{a}{\sim}\underbrace{N_d}_{\text{multivariate normal}}\left(\underbrace{\boldsymbol \theta_{\text{true}}}_{\text{mean vector}},\underbrace{\boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})^{-1}}_{\text{ covariance matrix}}\right)\]</span></p>
<p>This theorem about the distributional properties of MLEs greatly enhances the usefulness of the method of maximum likelihood. It implies that in regular settings maximum likelihood is not just a method for obtaining point estimates but also also provides estimates of their uncertainty.</p>
</section>
<section id="remarks-on-the-asympotics" class="level3">
<h3 class="anchored" data-anchor-id="remarks-on-the-asympotics">Remarks on the asympotics</h3>
<p>However, we need to clarify what “asymptotic” actually means in the context of the above theorem:</p>
<ol type="1">
<li><p>Primarily, it means to have sufficient sample size so that the log-likelihood <span class="math inline">\(\ell_n(\boldsymbol \theta)\)</span> is sufficiently well approximated by a quadratic function around <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span>. The better the local quadratic approximation the better the normal approximation!</p></li>
<li><p>In a regular model with positive definite observed Fisher information matrix this is guaranteed for large sample size <span class="math inline">\(n \rightarrow \infty\)</span> thanks to the central limit theorem).</p></li>
<li><p>However, <span class="math inline">\(n\)</span> going to infinity is in fact not always required for the normal approximation to hold! Depending on the particular model a good local fit to a quadratic log-likelihood may be available also for finite <span class="math inline">\(n\)</span>. As a trivial example, for the normal log-likelihood it is valid for any <span class="math inline">\(n\)</span>.</p></li>
<li><p>In the other hand, in non-regular models (with nondifferentiable log-likelihood at the MLE and/or a singular Fisher information matrix) no amount of data, not even <span class="math inline">\(n\rightarrow \infty\)</span>, will make the quadratic approximation work.</p></li>
</ol>
<p>Remarks:</p>
<ul>
<li>The asymptotic normality of MLEs was first discussed in Fisher (1925) <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
</ul>
<ul>
<li><p>The technical details of the above considerations are worked out in the theory of <a href="https://en.wikipedia.org/wiki/Local_asymptotic_normality">locally asymptotically normal (LAN) models</a> pioneered in 1960 by <a href="https://en.wikipedia.org/wiki/Lucien_Le_Cam">Lucien LeCam (1924–2000)</a>.</p></li>
<li><p>There are also methods to obtain higher-order (higher than quadratic and thus non-normal) asymptotic approximations. These relate to so-called <a href="https://en.wikipedia.org/wiki/Saddlepoint_approximation_method">saddle point approximations</a>.</p></li>
</ul>
</section>
<section id="information-inequality-and-asymptotic-optimal-efficiency" class="level3">
<h3 class="anchored" data-anchor-id="information-inequality-and-asymptotic-optimal-efficiency">Information inequality and asymptotic optimal efficiency</h3>
<p>Assume now that <span class="math inline">\(\hat{\boldsymbol \theta}\)</span> is an arbitrary and unbiased estimator for <span class="math inline">\(\boldsymbol \theta\)</span> and the underlying data-generating model is regular with density <span class="math inline">\(f(\boldsymbol x| \boldsymbol \theta)\)</span>.</p>
<p><a href="https://en.wikipedia.org/wiki/Harald_Cram%C3%A9r">H. Cramér (1893–1985)</a>, <a href="https://en.wikipedia.org/wiki/C._R._Rao">C. R. Rao (1920–)</a> and others demonstrated in 1945 the so-called <strong>information inequality</strong>, <span class="math display">\[
\text{Var}(\hat{\boldsymbol \theta}) \geq \frac{1}{n} \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta_{\text{true}})^{-1}
\]</span> which puts a lower bound on the variance of an estimator for <span class="math inline">\(\boldsymbol \theta\)</span>. (Note for <span class="math inline">\(d&gt;1\)</span> this is a matrix inequality, meaning that the difference matrix is positive semidefinite).</p>
<p>For large sample size with <span class="math inline">\(n \rightarrow \infty\)</span> and <span class="math inline">\(\hat{\boldsymbol \theta}_{ML} \rightarrow \boldsymbol \theta\)</span> the observed Fisher information becomes <span class="math inline">\(J_n(\hat{\boldsymbol \theta}) \rightarrow n \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\)</span> and therefore we can write the asymptotic distribution of <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> as <span class="math display">\[
\hat{\boldsymbol \theta}_{ML} \overset{a}{\sim} N_d\left(  \boldsymbol \theta_{\text{true}},  \frac{1}{n} \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta_{\text{true}})^{-1}  \right)
\]</span> This means that for large <span class="math inline">\(n\)</span> in regular models <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> achieves the lowest variance possible according to the Cramér-Rao information inequality. In other words, for large sample size maximum likelihood is optimally efficient and thus the best available estimator will in fact be the MLE!</p>
<p>However, as we will see later this does not hold for small sample size where it is indeed possible (and necessary) to improve over the MLE (e.g.&nbsp;via Bayesian estimation or regularisation).</p>
</section>
<section id="non-regular-models" class="level3">
<h3 class="anchored" data-anchor-id="non-regular-models">Non-regular models</h3>
<p>For non-regular models the asymptotic normality does not hold. Instead, the (asympotic) distribution of the MLE must be obtained in by other means.</p>
<div id="exm-nonregulardist" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.2</strong></span> Distribution of MLE for upper bound <span class="math inline">\(\theta\)</span> of the uniform distribution:</p>
<p>This continues <a href="08-likelihood2.html#exm-nonregular" class="quarto-xref">Example&nbsp;<span>8.4</span></a> to find the distribution of <span class="math inline">\(\hat{\theta}_{ML}\)</span>. Due to a discontinuity in the density at the MLE the observed Fisher information cannot be computed and the normal approximation for the distribution of <span class="math inline">\(\hat{\theta}_{ML}\)</span> is not valid.</p>
<p>Nonetheless, one can still obtain the sampling distribution of <span class="math inline">\(\hat{\theta}_{ML}=x_{[n]}\)</span>. However, <em>not</em> via asymptotic arguments but instead by understanding that <span class="math inline">\(x_{[n]}\)</span> is an order statistic (see <a href="https://en.wikipedia.org/wiki/Order_statistic" class="uri">https://en.wikipedia.org/wiki/Order_statistic</a>) with the following properties: <span class="math display">\[\begin{align*}
\begin{array}{cc}
x_{[n]}\sim \theta \, \text{Beta}(n,1)\\
\\
\text{E}\left(x_{[n]}\right)=\frac{n}{n+1} \theta\\
\\
\text{Var}\left(x_{[n]}\right)=\frac{n}{(n+1)^2(n+2)}\theta^2\\
\end{array}
\begin{array}{ll}
\text{"n-th order statistic" }\\
\\
\\
\\
\approx \frac{\theta^2}{n^2}\\
\end{array}
\end{align*}\]</span></p>
<p>Note that the variance decreases with <span class="math inline">\(\frac{1}{n^2}\)</span> which is much faster than the usual <span class="math inline">\(\frac{1}{n}\)</span> of an “efficient” estimator. Correspondingly, <span class="math inline">\(\hat{\theta}_{ML}\)</span> is a so-called “super efficient” estimator.</p>
</div>
</section>
</section>
<section id="quantifying-the-uncertainty-of-maximum-likelihood-estimates" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="quantifying-the-uncertainty-of-maximum-likelihood-estimates"><span class="header-section-number">10.2</span> Quantifying the uncertainty of maximum likelihood estimates</h2>
<section id="estimating-the-variance-of-mles" class="level3">
<h3 class="anchored" data-anchor-id="estimating-the-variance-of-mles">Estimating the variance of MLEs</h3>
<p>In the previous section we saw that MLEs are asymptotically normally distributed, with the inverse Fisher information (both expected and observed) linked to the asymptotic variance.</p>
<p>This leads to the question whether to use the observed Fisher information <span class="math inline">\(J_n(\hat{\boldsymbol \theta}_{ML})\)</span> or the expected Fisher information at the MLE <span class="math inline">\(n \boldsymbol I^{\text{Fisher}}( \hat{\boldsymbol \theta}_{ML} )\)</span> to estimate the variance of the MLE?</p>
<ul>
<li>Clearly, for <span class="math inline">\(n\rightarrow \infty\)</span> both can be used interchangeably.</li>
<li>However, they can be very different for finite <span class="math inline">\(n\)</span> in particular for models that are not exponential families.</li>
<li>Also normality may occur well before <span class="math inline">\(n\)</span> goes to <span class="math inline">\(\infty\)</span>.</li>
</ul>
<p>Therefore one needs to choose between the two, considering also that</p>
<ul>
<li>the expected Fisher information at the MLE is the average curvature at the MLE, whereas the observed Fisher information is the actual observed curvature, and</li>
<li>the observed Fisher information naturally occurs in the quadratic approximation of the log-likelihood.</li>
</ul>
<p>All in all, the observed Fisher information as estimator of the variance is more appropriate as it is based on the actual observed data and also works for large <span class="math inline">\(n\)</span> (in which case it yields the same result as using expected Fisher information): <span class="math display">\[
\widehat{\text{Var}}(\hat{\boldsymbol \theta}_{ML}) = \boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})^{-1}
\]</span> and its square-root as the estimate of the standard deviation <span class="math display">\[
\widehat{\text{SD}}(\hat{\boldsymbol \theta}_{ML}) = \boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})^{-1/2}
\]</span> Note that in the above we use <em>matrix inversion</em> and the (inverse) <em>matrix square root</em>.</p>
<p>The reasons for preferring observed Fisher information are made mathematically precise in a classic paper by Efron and Hinkley (1978) <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> .</p>
</section>
<section id="examples-for-the-estimated-variance-and-asymptotic-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="examples-for-the-estimated-variance-and-asymptotic-normal-distribution">Examples for the estimated variance and asymptotic normal distribution</h3>
<div id="exm-distproportion" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.3</strong></span> Estimated variance and distribution of the MLE of a proportion:</p>
<p>From <a href="08-likelihood2.html#exm-mleproportion" class="quarto-xref">Example&nbsp;<span>8.1</span></a> and <a href="09-likelihood3.html#exm-obsfisherproportion" class="quarto-xref">Example&nbsp;<span>9.1</span></a> we know the MLE <span class="math display">\[
\hat{p}_{ML} = \bar{x} = \frac{k}{n}
\]</span> and the corresponding observed Fisher information <span class="math display">\[
J_n(\hat{p}_{ML})=\frac{n}{\hat{p}_{ML}(1-\hat{p}_{ML})}
\]</span> The estimated variance of the MLE is therefore <span class="math display">\[
\widehat{\text{Var}}(   \hat{p}_{ML}  ) = \frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n}
\]</span> and the corresponding asymptotic normal distribution is <span class="math display">\[
\hat{p}_{ML} \overset{a}{\sim} N\left(p,   \frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n}   \right)
\]</span></p>
</div>
<div id="exm-distnormalmean" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.4</strong></span> Estimated variance and distribution of the MLE of the mean parameter for the normal distribution with known variance:</p>
<p>From <a href="08-likelihood2.html#exm-mlenormalmean" class="quarto-xref">Example&nbsp;<span>8.2</span></a> and <a href="09-likelihood3.html#exm-obsfishernormalmean" class="quarto-xref">Example&nbsp;<span>9.2</span></a> we know that <span class="math display">\[\hat{\mu}_{ML} =\bar{x}\]</span> and that the corresponding observed Fisher information at <span class="math inline">\(\hat{\mu}_{ML}\)</span> is <span class="math display">\[J_n(\hat{\mu}_{ML})=\frac{n}{\sigma^2}\]</span></p>
<p>The estimated variance of the MLE is therefore <span class="math display">\[
\widehat{\text{Var}}(\hat{\mu}_{ML}) = \frac{\sigma^2}{n}
\]</span> and the corresponding asymptotic normal distribution is <span class="math display">\[
\hat{\mu}_{ML} \sim N\left(\mu,\frac{\sigma^2}{n}\right)
\]</span></p>
<p>Note that in this case the distribution is not asymptotic but is <strong>exact</strong>, i.e.&nbsp;valid also for small <span class="math inline">\(n\)</span> (as long as the data <span class="math inline">\(x_i\)</span> are actually from <span class="math inline">\(N(\mu, \sigma^2)\)</span>!).</p>
</div>
</section>
<section id="wald-statistic" class="level3">
<h3 class="anchored" data-anchor-id="wald-statistic">Wald statistic</h3>
<p>Centering the MLE <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> with <span class="math inline">\(\boldsymbol \theta_0\)</span> followed by standardising with <span class="math inline">\(\widehat{\text{SD}}(\hat{\boldsymbol \theta}_{ML})\)</span> yields the <strong>Wald statistic</strong> (named after <a href="https://en.wikipedia.org/wiki/Abraham_Wald">Abraham Wald, 1902–1950</a>): <span class="math display">\[
\begin{split}
\boldsymbol t(\boldsymbol \theta_0) &amp; = \widehat{\text{SD}}(\hat{\boldsymbol \theta}_{ML})^{-1}(\hat{\boldsymbol \theta}_{ML}-\boldsymbol \theta_0)\\
&amp; = \boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})^{1/2}(\hat{\boldsymbol \theta}_{ML}-\boldsymbol \theta_0)\\
\end{split}
\]</span> The <strong>squared Wald statistic</strong> is a scalar defined as <span class="math display">\[
\begin{split}
t(\boldsymbol \theta_0)^2 &amp;= \boldsymbol t(\boldsymbol \theta_0)^T \boldsymbol t(\boldsymbol \theta_0) \\
&amp;=
(\hat{\boldsymbol \theta}_{ML}-\boldsymbol \theta_0)^T
\boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})
(\hat{\boldsymbol \theta}_{ML}-\boldsymbol \theta_0)\\
\end{split}
\]</span> Note that in the literature both <span class="math inline">\(\boldsymbol t(\boldsymbol \theta_0)\)</span> and <span class="math inline">\(t(\boldsymbol \theta_0)^2\)</span> are commonly referred to as Wald statistics. In this text we use the qualifier “squared” if we refer to the latter.</p>
<p>We now assume that the true underlying parameter is <span class="math inline">\(\boldsymbol \theta_0\)</span>. Since the MLE is asymptotically normal the Wald statistic is asymptotically <strong>standard normal</strong> distributed: <span class="math display">\[\begin{align*}
\begin{array}{cc}
\boldsymbol t(\boldsymbol \theta_0) \overset{a}{\sim}\\
t(\theta_0) \overset{a}{\sim}\\
\end{array}
\begin{array}{ll}
N_d(\mathbf 0_d,\boldsymbol I_d)\\
N(0,1)\\
\end{array}
\begin{array}{ll}
  \text{for vector } \boldsymbol \theta\\
  \text{for scalar } \theta\\
\end{array}
\end{align*}\]</span> Correspondingly, the <strong>squared</strong> Wald statistic is chi-squared distributed: <span class="math display">\[\begin{align*}
\begin{array}{cc}
t(\boldsymbol \theta_0)^2 \\
t(\theta_0)^2\\
\end{array}
\begin{array}{ll}
\overset{a}{\sim}\chi^2_d\\
\overset{a}{\sim}\chi^2_1\\
\end{array}
\begin{array}{ll}
  \text{for vector } \boldsymbol \theta\\
  \text{for scalar } \theta\\
\end{array}
\end{align*}\]</span> The degree of freedom of the chi-squared distribution is the dimension <span class="math inline">\(d\)</span> of the parameter vector <span class="math inline">\(\boldsymbol \theta\)</span>.</p>
</section>
<section id="examples-of-the-squared-wald-statistic" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-the-squared-wald-statistic">Examples of the (squared) Wald statistic</h3>
<div id="exm-waldproportion" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.5</strong></span> Wald statistic for a proportion:</p>
<p>We continue from <a href="#exm-distproportion" class="quarto-xref">Example&nbsp;<span>10.3</span></a>. With <span class="math inline">\(\hat{p}_{ML} = \bar{x}\)</span> and <span class="math inline">\(\widehat{\text{Var}}(   \hat{p}_{ML}  ) = \frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n}\)</span> and thus <span class="math inline">\(\widehat{\text{SD}}(   \hat{p}_{ML}  ) =\sqrt{ \frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n} }\)</span> we get as <strong>Wald statistic</strong>:</p>
<p><span class="math display">\[
t(p_0) = \frac{\bar{x}-p_0}{ \sqrt{\bar{x}(1-\bar{x}) / n }  }\overset{a}{\sim} N(0,1)
\]</span></p>
<p>The <strong>squared Wald statistic</strong> is: <span class="math display">\[t(p_0)^2 = n \frac{(\bar{x}-p_0)^2}{ \bar{x}(1-\bar{x})   }\overset{a}{\sim} \chi^2_1 \]</span></p>
</div>
<div id="exm-waldnormalmean" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.6</strong></span> Wald statistic for the mean parameter of a normal distribution with known variance:</p>
<p>We continue from <a href="#exm-distnormalmean" class="quarto-xref">Example&nbsp;<span>10.4</span></a>. With <span class="math inline">\(\hat{\mu}_{ML} =\bar{x}\)</span> and <span class="math inline">\(\widehat{\text{Var}}(\hat{\mu}_{ML}) = \frac{\sigma^2}{n}\)</span> and thus <span class="math inline">\(\widehat{\text{SD}}(\hat{\mu}_{ML}) = \frac{\sigma}{\sqrt{n}}\)</span> we get as <strong>Wald statistic</strong>:</p>
<p><span class="math display">\[t(\mu_0) = \frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}}\sim N(0,1)\]</span> Note this is the one sample <span class="math inline">\(t\)</span>-statistic with given <span class="math inline">\(\sigma\)</span>. The <strong>squared Wald statistic</strong> is: <span class="math display">\[t(\mu_0)^2 = \frac{(\bar{x}-\mu_0)^2}{\sigma^2 / n}\sim \chi^2_1 \]</span></p>
<p>Again, in this instance this is the exact distribution, not just the asymptotic one.</p>
<p>Using the Wald statistic or the squared Wald statistic we can test whether a particular <span class="math inline">\(\mu_0\)</span> can be rejected as underlying true parameter, and we can also construct corresponding confidence intervals.</p>
</div>
<div id="exm-catwald" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.7</strong></span> Wald statistic for the categorical distribution:</p>
<p>The squared Wald statistic is <span class="math display">\[
\begin{split}
t(\boldsymbol p_0)^2 &amp;=
(\hat{\pi}_{1}^{ML}-p_1^0, \ldots,  \hat{\pi}_{K-1}^{ML}-p_{K-1}^0)   \boldsymbol J_n(\hat{\pi}_{1}^{ML}, \ldots, \hat{\pi}_{K-1}^{ML} ) \begin{pmatrix} \hat{\pi}_{1}^{ML}-p_1^0 \\
\vdots \\
\hat{\pi}_{K-1}^{ML}-p_{K-1}^0\\
\end{pmatrix}\\
&amp;= n  \left( \sum_{k=1}^{K-1} \frac{(\hat{\pi}_{k}^{ML}-p_{k}^0)^2}{\hat{\pi}_{k}^{ML}}   + \frac{ \left(\sum_{k=1}^{K-1} (\hat{\pi}_{k}^{ML}-p_{k}^0)\right)^2}{\hat{\pi}_{K}^{ML}} \right)  \\
&amp;= n  \left( \sum_{k=1}^{K} \frac{(\hat{\pi}_{k}^{ML}-p_{k}^0)^2}{\hat{\pi}_{k}^{ML}}    \right)  \\
&amp; = n D_{\text{Neyman}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) )
\end{split}
\]</span></p>
<p>With <span class="math inline">\(n_1, \ldots, n_K\)</span> the observed counts with <span class="math inline">\(n =  \sum_{k=1}^K  n_k\)</span> and <span class="math inline">\(\hat{\pi}_k^{ML} = \frac{n_k}{n} = \bar{x}_k\)</span>, and <span class="math inline">\(n_1^{\text{expect}}, \ldots, n_K^{\text{expect}}\)</span> the expected counts <span class="math inline">\(n_k^{\text{expect}} = n p_k^{0}\)</span> under <span class="math inline">\(\boldsymbol p_0\)</span> we can write the squared Wald statistic as follows: <span class="math display">\[
t(\boldsymbol p_0)^2 = \sum_{k=1}^K \frac{(n_k-n_k^{\text{expect}} )^2}{n_k} =  \chi^2_{\text{Neyman}}
\]</span> This is known as the Neyman chi-squared statistic (note the <em>observed</em> counts in its denominator) and it is asymptotically distributed as <span class="math inline">\(\chi^2_{K-1}\)</span> because there are <span class="math inline">\(K-1\)</span> free parameters in <span class="math inline">\(\boldsymbol p_0\)</span>.</p>
</div>
</section>
<section id="normal-confidence-intervals-using-the-wald-statistic" class="level3">
<h3 class="anchored" data-anchor-id="normal-confidence-intervals-using-the-wald-statistic">Normal confidence intervals using the Wald statistic</h3>
<p>See <a href="20-stats.html#sec-ci" class="quarto-xref"><span>Section A.10</span></a> to review relevant background from year 1.</p>
<div id="fig-normalciml" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normalciml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/normalciml.png" class="img-fluid figure-img" style="width:90.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normalciml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.3: Construction of a 95% symmetric normal confidence interval for a maximum likelihood estimate.
</figcaption>
</figure>
</div>
<p>The asymptotic normality of MLEs derived from regular models enables us to construct a corresponding normal confidence interval (<a href="#fig-normalciml" class="quarto-xref">Figure&nbsp;<span>10.3</span></a>). For example, to construct the asymptotic normal CI for the MLE of a scalar parameter <span class="math inline">\(\theta\)</span> we use the MLE <span class="math inline">\(\hat{\theta}_{ML}\)</span> as estimate of the mean and its standard deviation <span class="math inline">\(\widehat{\text{SD}}(\hat{\theta}_{ML})\)</span> computed from the observed Fisher information: <span class="math display">\[
\text{CI}=[\hat{\theta}_{ML} \pm c_{\text{normal}} \widehat{\text{SD}}(\hat{\theta}_{ML})]
\]</span> Here <span class="math inline">\(c_{normal}\)</span> is a critical value for the standard-normal symmetric confidence interval chosen to achieve the desired nominal coverage. The critical values are computed using the inverse standard normal distribution function via <span class="math inline">\(c_{\text{normal}}=\Phi^{-1}\left(\frac{1+\kappa}{2}\right)\)</span>. A list of critical values for the standard normal distribution is found in <a href="20-stats.html#tbl-critnorm" class="quarto-xref">Table&nbsp;<span>A.1</span></a>. For example, for a CI with 95% coverage one uses the factor 1.96 so that <span class="math display">\[\text{CI}=[\hat{\theta}_{ML} \pm 1.96\, \widehat{\text{SD}}(\hat{\theta}_{ML}) ]\]</span></p>
<p>The normal CI can be expressed using Wald statistic as follows: <span class="math display">\[
\text{CI}=\{\theta_0:  | t(\theta_0)| &lt; c_{\text{normal}} \}
\]</span></p>
<p>Similary, it can also be expressed using the squared Wald statistic: <span class="math display">\[
\text{CI}=\{\theta_0:   t(\boldsymbol \theta_0)^2 &lt; c_{\text{chisq}} \}
\]</span> Note that this form facilitates the construction of normal confidence intervals for a parameter vector <span class="math inline">\(\boldsymbol \theta_0\)</span>.</p>
<p>A list of critical values for the chi-squared distribution with one degree of freedom is found in <a href="20-stats.html#tbl-critchisq" class="quarto-xref">Table&nbsp;<span>A.2</span></a>.</p>
<p>The following lists contains the critical values resulting from the chi-squared distribution with degree of freedom <span class="math inline">\(m=1\)</span> for the three most common choices of coverage <span class="math inline">\(\kappa\)</span> for a normal CI for a univariate parameter: For example, for a 95% interval the critical value equals 3.84 (which is the square of the critical value 1.96 for the standard normal).</p>
</section>
<section id="normal-tests-using-the-wald-statistic" class="level3">
<h3 class="anchored" data-anchor-id="normal-tests-using-the-wald-statistic">Normal tests using the Wald statistic</h3>
<p>Finally, recall the <strong>duality between confidence intervals and statistical tests</strong>. Specifically, a confidence interval with coverage <span class="math inline">\(\kappa\)</span> can be also used for testing as follows:</p>
<ul>
<li>for every <span class="math inline">\(\theta_0\)</span> inside the CI the data do not allow to reject the hypothesis that <span class="math inline">\(\theta_0\)</span> is the true parameter with significance level <span class="math inline">\(\alpha=1-\kappa\)</span>.</li>
<li>Conversely, all values <span class="math inline">\(\theta_0\)</span> outside the CI can be rejected to be the true parameter with significance level <span class="math inline">\(\alpha=1-\kappa\)</span> .</li>
</ul>
<p>Hence, in order to test whether <span class="math inline">\(\boldsymbol \theta_0\)</span> is the true underlying parameter value we can compute the corresponding (squared) Wald statistic, find the desired critical value and then decide on rejection.</p>
</section>
<section id="examples-for-normal-confidence-intervals-and-corresponding-tests" class="level3">
<h3 class="anchored" data-anchor-id="examples-for-normal-confidence-intervals-and-corresponding-tests">Examples for normal confidence intervals and corresponding tests</h3>
<div id="exm-ciproportion" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.8</strong></span> Asymptotic normal confidence interval for a proportion:</p>
<p>We continue from <a href="#exm-distproportion" class="quarto-xref">Example&nbsp;<span>10.3</span></a> and <a href="#exm-waldproportion" class="quarto-xref">Example&nbsp;<span>10.5</span></a>. Assume we observe <span class="math inline">\(n=30\)</span> measurements with average <span class="math inline">\(\bar{x} = 0.7\)</span>. Then <span class="math inline">\(\hat{p}_{ML} = \bar{x} = 0.7\)</span> and <span class="math inline">\(\widehat{\text{SD}}(\hat{p}_{ML}) = \sqrt{ \frac{ \bar{x}(1-\bar{x})}{n} } \approx 0.084\)</span>.</p>
<p>The symmetric asymptotic normal CI for <span class="math inline">\(p\)</span> with 95% coverage is given by <span class="math inline">\(\hat{p}_{ML} \pm 1.96 \, \widehat{\text{SD}}(\hat{p}_{ML})\)</span> which for the present data results in the interval <span class="math inline">\([0.536, 0.864]\)</span>.</p>
</div>
<div id="exm-normaltestproportion" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.9</strong></span> Asymptotic normal test for a proportion:</p>
<p>We continue from <a href="#exm-ciproportion" class="quarto-xref">Example&nbsp;<span>10.8</span></a>.</p>
<p>We now consider two possible values (<span class="math inline">\(p_0=0.5\)</span> and <span class="math inline">\(p_0=0.8\)</span>) as potentially true underlying proportion.</p>
<p>The value <span class="math inline">\(p_0=0.8\)</span> lies inside the 95% confidence interval <span class="math inline">\([0.536, 0.864]\)</span>. This implies we cannot reject the hypthesis that this is the true underlying parameter on 5% significance level. In contrast, <span class="math inline">\(p_0=0.5\)</span> is outside the confidence interval so we can indeed reject this value. In other words, data plus model exclude this value as statistically implausible.</p>
<p>This can be verified more directly by computing the corresponding (squared) Wald statistics (see <a href="#exm-waldproportion" class="quarto-xref">Example&nbsp;<span>10.5</span></a>) and comparing them with the relevant critical value (3.84 from chi-squared distribution for 5% significance level):</p>
<ul>
<li><span class="math inline">\(t(0.5)^2 = \frac{(0.7-0.5)^2}{0.084^2} = 5.71  &gt; 3.84\)</span> hence <span class="math inline">\(p_0=0.5\)</span> can be rejected.</li>
<li><span class="math inline">\(t(0.8)^2 = \frac{(0.7-0.8)^2}{0.084^2} = 1.43  &lt; 3.84\)</span> hence <span class="math inline">\(p_0=0.8\)</span> cannot be rejected.</li>
</ul>
<p>Note that the squared Wald statistic at the boundaries of the normal confidence interval is equal to the critical value.</p>
</div>
<div id="exm-cinormalmean" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.10</strong></span> Normal confidence interval for the mean:</p>
<p>We continue from <a href="#exm-distnormalmean" class="quarto-xref">Example&nbsp;<span>10.4</span></a> and <a href="#exm-waldnormalmean" class="quarto-xref">Example&nbsp;<span>10.6</span></a>. Assume that we observe <span class="math inline">\(n=25\)</span> measurements with average <span class="math inline">\(\bar{x} = 10\)</span>, from a normal with unknown mean and variance <span class="math inline">\(\sigma^2=4\)</span>.</p>
<p>Then <span class="math inline">\(\hat{\mu}_{ML} = \bar{x} = 10\)</span> and <span class="math inline">\(\widehat{\text{SD}}(\hat{\mu}_{ML}) = \sqrt{ \frac{ \sigma^2}{n} } = \frac{2}{5}\)</span>.</p>
<p>The symmetric asymptotic normal CI for <span class="math inline">\(p\)</span> with 95% coverage is given by <span class="math inline">\(\hat{\mu}_{ML} \pm 1.96 \, \widehat{\text{SD}}(\hat{\mu}_{ML})\)</span> which for the present data results in the interval <span class="math inline">\([9.216, 10.784]\)</span>.</p>
</div>
<div id="exm-normaltestnormalmean" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.11</strong></span> Normal test for the mean:</p>
<p>We continue from <a href="#exm-cinormalmean" class="quarto-xref">Example&nbsp;<span>10.10</span></a>.</p>
<p>We now consider two possible values (<span class="math inline">\(\mu_0=9.5\)</span> and <span class="math inline">\(\mu_0=11\)</span>) as potentially true underlying mean parameter.</p>
<p>The value <span class="math inline">\(\mu_0=9.5\)</span> lies inside the 95% confidence interval <span class="math inline">\([9.216, 10.784]\)</span>. This implies we cannot reject the hypthesis that this is the true underlying parameter on 5% significance level. In contrast, <span class="math inline">\(\mu_0=11\)</span> is outside the confidence interval so we can indeed reject this value. In other words, data plus model exclude this value as a statistically implausible.</p>
<p>This can be verified more directly by computing the corresponding (squared) Wald statistics (see <a href="#exm-waldnormalmean" class="quarto-xref">Example&nbsp;<span>10.6</span></a>) and comparing them with the relevant critical values:</p>
<ul>
<li><span class="math inline">\(t(9.5)^2 = \frac{(10-9.5)^2}{4/25}= 1.56  &lt; 3.84\)</span> hence <span class="math inline">\(\mu_0=9.5\)</span> cannot be rejected.</li>
<li><span class="math inline">\(t(11)^2 = \frac{(10-11)^2}{4/25} = 6.25 &gt; 3.84\)</span> hence <span class="math inline">\(\mu_0=11\)</span> can be rejected.</li>
</ul>
<p>The squared Wald statistic at the boundaries of the confidence interval equals the critical value.</p>
<p>Note that this is the standard one-sample test of the mean, and that it is exact, not an approximation.</p>
</div>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Fisher R. A. 1925. <em>Theory of statistical estimation.</em> Math. Proc. Cambridge Philos. Soc. <strong>22</strong>:700–725. <a href="https://doi.org/10.1017/S0305004100009580" class="uri">https://doi.org/10.1017/S0305004100009580</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Efron, B., and D. V. Hinkley. 1978. <em>Assessing the accuracy of the maximum likelihood estimator: observed versus expected Fisher information.</em> Biometrika <strong>65</strong>:457–482. <a href="https://doi.org/10.1093/biomet/65.3.457" class="uri">https://doi.org/10.1093/biomet/65.3.457</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./09-likelihood3.html" class="pagination-link" aria-label="Observed Fisher information">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./11-likelihood5.html" class="pagination-link" aria-label="Likelihood-based confidence interval and likelihood ratio">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>