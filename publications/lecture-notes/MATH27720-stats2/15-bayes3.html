<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; Essentials of Bayesian statistics – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./16-bayes4.html" rel="next">
<link href="./14-bayes2.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-bf90d58e07b16a5a5517af5259b97af0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./13-bayes1.html">Bayesian statistics</a></li><li class="breadcrumb-item"><a href="./15-bayes3.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropy and likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions for statistical models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Relative entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Principle of maximum entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#principle-of-bayesian-learning" id="toc-principle-of-bayesian-learning" class="nav-link active" data-scroll-target="#principle-of-bayesian-learning"><span class="header-section-number">15.1</span> Principle of Bayesian learning</a>
  <ul class="collapse">
  <li><a href="#from-prior-to-posterior-distribution" id="toc-from-prior-to-posterior-distribution" class="nav-link" data-scroll-target="#from-prior-to-posterior-distribution">From prior to posterior distribution</a></li>
  <li><a href="#zero-forcing-property" id="toc-zero-forcing-property" class="nav-link" data-scroll-target="#zero-forcing-property">Zero forcing property</a></li>
  <li><a href="#bayesian-update-and-likelihood" id="toc-bayesian-update-and-likelihood" class="nav-link" data-scroll-target="#bayesian-update-and-likelihood">Bayesian update and likelihood</a></li>
  <li><a href="#sequential-updates" id="toc-sequential-updates" class="nav-link" data-scroll-target="#sequential-updates">Sequential updates</a></li>
  <li><a href="#summaries-of-posterior-distributions-and-credible-intervals" id="toc-summaries-of-posterior-distributions-and-credible-intervals" class="nav-link" data-scroll-target="#summaries-of-posterior-distributions-and-credible-intervals">Summaries of posterior distributions and credible intervals</a></li>
  <li><a href="#practical-application-of-bayes-statistics-on-the-computer" id="toc-practical-application-of-bayes-statistics-on-the-computer" class="nav-link" data-scroll-target="#practical-application-of-bayes-statistics-on-the-computer">Practical application of Bayes statistics on the computer</a></li>
  </ul></li>
  <li><a href="#some-background-on-bayesian-statistics" id="toc-some-background-on-bayesian-statistics" class="nav-link" data-scroll-target="#some-background-on-bayesian-statistics"><span class="header-section-number">15.2</span> Some background on Bayesian statistics</a>
  <ul class="collapse">
  <li><a href="#bayesian-interpretation-of-probability" id="toc-bayesian-interpretation-of-probability" class="nav-link" data-scroll-target="#bayesian-interpretation-of-probability">Bayesian interpretation of probability</a></li>
  <li><a href="#historical-developments" id="toc-historical-developments" class="nav-link" data-scroll-target="#historical-developments">Historical developments</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./13-bayes1.html">Bayesian statistics</a></li><li class="breadcrumb-item"><a href="./15-bayes3.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="principle-of-bayesian-learning" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="principle-of-bayesian-learning"><span class="header-section-number">15.1</span> Principle of Bayesian learning</h2>
<section id="from-prior-to-posterior-distribution" class="level3">
<h3 class="anchored" data-anchor-id="from-prior-to-posterior-distribution">From prior to posterior distribution</h3>
<p>Bayesian statistical learning applies Bayes’ theorem to update our state of knowledge about a parameter in the light of data.</p>
<p>Ingredients:</p>
<ul>
<li><span class="math inline">\(\boldsymbol \theta\)</span> parameter(s) of interest, unknown and fixed.</li>
<li>prior distribution with density <span class="math inline">\(p(\boldsymbol \theta)\)</span> describing the <em>uncertainty</em> (not randomness!) about <span class="math inline">\(\boldsymbol \theta\)</span></li>
<li>data-generating process <span class="math inline">\(p(x | \boldsymbol \theta)\)</span></li>
</ul>
<p>Note the <strong>model underlying the Bayesian approach is the joint distribution</strong> <span class="math display">\[
p(\boldsymbol \theta, x) = p(\boldsymbol \theta) p(x | \boldsymbol \theta)
\]</span> as both a prior distribution over the parameters as well as a data-generating process have to be specified.</p>
<p>Question: new information in the form of a new observation <span class="math inline">\(x\)</span> arrives - how does the uncertainty about <span class="math inline">\(\boldsymbol \theta\)</span> change?</p>
<div id="fig-bayesupdate" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bayesupdate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/bayesian-update.png" class="img-fluid figure-img" style="width:90.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bayesupdate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.1: Bayesian learning by updating the prior distribution to the posterior distribution.
</figcaption>
</figure>
</div>
<p>Answer: use Bayes’ theorem to <strong>update the prior density to the posterior density</strong> (see <a href="#fig-bayesupdate" class="quarto-xref">Figure&nbsp;<span>15.1</span></a>).</p>
<p><span class="math display">\[
\underbrace{p(\boldsymbol \theta| x)}_{\text{posterior} } = \underbrace{p(\boldsymbol \theta)}_{\text{prior}} \frac{p(x | \boldsymbol \theta) }{ p(x)}
\]</span></p>
<p>For the denominator in Bayes formula we need to compute <span class="math inline">\(p(x)\)</span>. This is obtained by<br>
<span class="math display">\[
\begin{split}
p(x) &amp;= \int_{\boldsymbol \theta} p(x , \boldsymbol \theta) d\boldsymbol \theta\\
&amp;= \int_{\boldsymbol \theta} p(x | \boldsymbol \theta) p(\boldsymbol \theta) d\boldsymbol \theta\\
\end{split}
\]</span> i.e.&nbsp;by marginalisation of the parameter <span class="math inline">\(\boldsymbol \theta\)</span> from the joint distribution of <span class="math inline">\(\boldsymbol \theta\)</span> and <span class="math inline">\(x\)</span>. (For discrete <span class="math inline">\(\boldsymbol \theta\)</span> replace the integral by a sum). Depending on the context this quantity is either called the</p>
<ul>
<li><strong>normalisation constant</strong> as it ensures that the posterior density <span class="math inline">\(p(\boldsymbol \theta| x)\)</span> integrates to one.</li>
<li><strong>prior predictive density</strong> of the data <span class="math inline">\(x\)</span> given the model <span class="math inline">\(M\)</span> before seeing any data. To emphasise the implicit conditioning on a model we may write <span class="math inline">\(p(x| M)\)</span>. Since all parameters have been integrated out <span class="math inline">\(M\)</span> in fact refers to a model <em>class</em>.</li>
<li><strong>marginal likelihood</strong> of the underlying <strong>model</strong> (class) <span class="math inline">\(M\)</span> given data <span class="math inline">\(x\)</span>. To emphasise this may write <span class="math inline">\(L(M| x)\)</span>. Sometimes it is also called <strong>model likelihood</strong>.</li>
</ul>
</section>
<section id="zero-forcing-property" class="level3">
<h3 class="anchored" data-anchor-id="zero-forcing-property">Zero forcing property</h3>
<p>It is easy to see that if in Bayes rule the prior density/probability is zero for some parameter value <span class="math inline">\(\boldsymbol \theta\)</span> then the posterior density/probability will remain at zero for that <span class="math inline">\(\boldsymbol \theta\)</span>, regardless of any data collected. This <strong>zero-forcing property</strong> of the Bayes update rule has been called <strong>Cromwell’s rule</strong> by <a href="https://en.wikipedia.org/wiki/Dennis_Lindley">Dennis Lindley (1923–2013)</a>. Therefore, assigning prior density/probability 0 to an event should be avoided.</p>
<p>Note that this implies that assigning prior probability 1 should be avoided, too.</p>
</section>
<section id="bayesian-update-and-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-update-and-likelihood">Bayesian update and likelihood</h3>
<p>After <em>independent and identically distributed</em> (iid) data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> have been observed the Bayesian posterior is computed by<br>
<span class="math display">\[
\underbrace{p(\boldsymbol \theta| D) }_{\text{posterior} } = \underbrace{p(\boldsymbol \theta)}_{\text{prior}} \frac{ L(\boldsymbol \theta| D) }{ p(D)}
\]</span> involving the likelihood <span class="math inline">\(L(\boldsymbol \theta| D) = \prod_{i=1}^n p(x_i | \boldsymbol \theta)\)</span> and the marginal likelihoood <span class="math inline">\(p(D) = \int_{\boldsymbol \theta} p(\boldsymbol \theta) L(\boldsymbol \theta| D) d\boldsymbol \theta\)</span> with <span class="math inline">\(\boldsymbol \theta\)</span> integrated out.</p>
<p>The marginal likelihood serves as a standardising factor so that the posterior density for <span class="math inline">\(\boldsymbol \theta\)</span> integrates to 1: <span class="math display">\[
\int_{\boldsymbol \theta} p(\boldsymbol \theta| D) d\boldsymbol \theta= \frac{1}{p(D)} \int_{\boldsymbol \theta} p(\boldsymbol \theta) L(\boldsymbol \theta| D) d\boldsymbol \theta= 1
\]</span> Unfortunately, the integral to compute the marginal likelihood is typically analytically intractable and requires numerical integration and/or approximation.</p>
<p>Comparing likelihood and Bayes procedures note that</p>
<ul>
<li>conducting a Bayesian statistical analysis requires integration respectively averaging (to compute the marginal likelihood)</li>
<li>in contrast to a likelihood analysis that requires optimisation (to find the maximum likelihood).</li>
</ul>
</section>
<section id="sequential-updates" class="level3">
<h3 class="anchored" data-anchor-id="sequential-updates">Sequential updates</h3>
<p>Note that the Bayesian update procedure can be repeated again and again: we can use the posterior as our new prior and then update it with further data. Thus, we may also update the posterior density sequentially, with the data points <span class="math inline">\(x_1, \ldots, x_n\)</span> arriving one after the other, by computing first <span class="math inline">\(p(\boldsymbol \theta| x_1)\)</span>, then <span class="math inline">\(p(\boldsymbol \theta| x_1, x_2)\)</span> and so on until we reach <span class="math inline">\(p(\boldsymbol \theta| x_1, \ldots, x_n) = p(\boldsymbol \theta| D)\)</span>.</p>
<p>For example, for the first update we have <span class="math display">\[
p(\boldsymbol \theta| x_1) =  p(\boldsymbol \theta)   \frac{p(x_1 | \boldsymbol \theta)  }{p(x_1)}
\]</span> with <span class="math inline">\(p(x_1) =\int_{\boldsymbol \theta} p(x_1 | \boldsymbol \theta) p(\boldsymbol \theta) d\boldsymbol \theta\)</span>.</p>
<p>The second update yields <span class="math display">\[
\begin{split}
p(\boldsymbol \theta| x_1, x_2) &amp;=  p(\boldsymbol \theta| x_1)   \frac{p(x_2 | \boldsymbol \theta, x_1)  }{p(x_2| x_1)}\\
&amp;= p(\boldsymbol \theta| x_1)   \frac{p(x_2 | \boldsymbol \theta)  }{p(x_2| x_1)}\\
&amp;=  p(\boldsymbol \theta) \frac{  p(x_1 | \boldsymbol \theta)    p(x_2 | \boldsymbol \theta)  }{p(x_1) p(x_2| x_1)}\\
\end{split}
\]</span> with <span class="math inline">\(p(x_2| x_1) = \int_{\boldsymbol \theta} p(x_2 | \boldsymbol \theta) p(\boldsymbol \theta| x_1) d\boldsymbol \theta\)</span>.</p>
<p>Note that <em>given <span class="math inline">\(\boldsymbol \theta\)</span></em> the <span class="math inline">\(x_i\)</span> are independent, hence <span class="math inline">\(p(x_2 | \boldsymbol \theta, x_1) = p(x_2 | \boldsymbol \theta)\)</span> and the joint distribution given <span class="math inline">\(\boldsymbol \theta\)</span> is <span class="math inline">\(p(x_1, x_2|\boldsymbol \theta) = p(x_1 | \boldsymbol \theta) p(x_2 | \boldsymbol \theta)\)</span>. In contrast, the joint distribution with <span class="math inline">\(\boldsymbol \theta\)</span> integrated out is <span class="math inline">\(p(x_1, x_2) = p(x_1) p(x_2| x_1) \neq p(x_1) p(x_2)\)</span> so without specification of <span class="math inline">\(\boldsymbol \theta\)</span> out the <span class="math inline">\(x_i\)</span> are dependent.</p>
<p>The final step is <span class="math display">\[
\begin{split}
p(\boldsymbol \theta| D)  = p(\boldsymbol \theta| x_1, \ldots, x_n) &amp;=   p(\boldsymbol \theta) \frac{ \prod_{i=1}^n p(x_i | \boldsymbol \theta)  }{ p(D)  }\\
\end{split}
\]</span> with the marginal likelihood factorising into <span class="math display">\[
p(D) = \prod_{i=1}^n p(x_i| x_{&lt;i})
\]</span> with <span class="math display">\[
p(x_i| x_{&lt;i}) = \int_{\boldsymbol \theta} p(x_i | \boldsymbol \theta) p(\boldsymbol \theta| x_{&lt;i}) d\boldsymbol \theta
\]</span> The last factor is the <strong>posterior predictive density</strong> of the new data <span class="math inline">\(x_i\)</span> after seeing data <span class="math inline">\(x_1, \ldots, x_{i-1}\)</span> (given the model class <span class="math inline">\(M\)</span>).</p>
<p>Intuitively, we can understand why the probability of the new <span class="math inline">\(x_i\)</span> depends on the previously observed data points. This is because the uncertainty about the model parameter <span class="math inline">\(\boldsymbol \theta\)</span> depends on how many data points we have already observed, and typically with more data the uncertainty about <span class="math inline">\(\boldsymbol \theta\)</span> decreases. Therefore the marginal likelihood <span class="math inline">\(p(D)\)</span> is <em>not</em> simply the product of the marginal densities <span class="math inline">\(p(x_i)\)</span> at each <span class="math inline">\(x_i\)</span> but instead the product of the conditional densities <span class="math inline">\(p(x_i| x_{&lt;i})\)</span>.</p>
<p>Only when the parameter is fully known, and there is no uncertainty about <span class="math inline">\(\boldsymbol \theta\)</span>, the observations <span class="math inline">\(x_i\)</span> are independent. This leads back to the standard likelihood where we condition on a particular <span class="math inline">\(\boldsymbol \theta\)</span> and the likelihood is the product <span class="math inline">\(p(D| \boldsymbol \theta) = \prod_{i=1}^n p(x_i| \boldsymbol \theta)\)</span>.</p>
</section>
<section id="summaries-of-posterior-distributions-and-credible-intervals" class="level3">
<h3 class="anchored" data-anchor-id="summaries-of-posterior-distributions-and-credible-intervals">Summaries of posterior distributions and credible intervals</h3>
<p><strong>The Bayesian estimate is the full complete posterior distribution!</strong></p>
<p>However, it is useful to summarise aspects of the posterior distribution:</p>
<ul>
<li>Posterior mean <span class="math inline">\(\text{E}(\boldsymbol \theta| D)\)</span></li>
<li>Posterior variance <span class="math inline">\(\text{Var}(\boldsymbol \theta| D)\)</span></li>
<li>Posterior mode etc.</li>
</ul>
<p>In particular the mean of the posterior distribution is often taken as a <em>Bayesian point estimate</em>.</p>
<div id="fig-bayescredible" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bayescredible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/bayes-credible.png" class="img-fluid figure-img" style="width:60.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bayescredible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.2: Bayesian credible interval based on the posterior distribution.
</figcaption>
</figure>
</div>
<p>The posterior distribution also allows to define <strong>credible regions</strong> or <strong>credible intervals</strong>. These are the <strong>Bayesian equivalent to confidence intervals</strong> and are constructed by finding the areas of highest probability mass (say 95%) in the posterior distribution (<a href="#fig-bayescredible" class="quarto-xref">Figure&nbsp;<span>15.2</span></a>).</p>
<p>Bayesian credible intervals, unlike frequentist confidence intervals, are thus very easy to interpret as they simply correspond to the part of the parameter space in which we can find the parameter with a given specified probability. In contrast, in frequentist statistics it does not make sense to assign a probability to a parameter value.</p>
<p>Note that there are typically many credible intervals with the given specified coverage <span class="math inline">\(\alpha\)</span> (say 95%). Therefore, we may need further criteria to construct these intervals.</p>
<p>For univariate parameter <span class="math inline">\(\theta\)</span> a <strong>two-sided equal-tail credible interval</strong> is obtained by finding the corresponding lower <span class="math inline">\(1-\alpha/2\)</span> and upper <span class="math inline">\(\alpha/2\)</span> quantiles. Typically this type of credible interval is easy to compute. However, note that the density values at the left and right boundary points of such an interval are typically different. Also this does not generalise well to a multivariate parameter <span class="math inline">\(\boldsymbol \theta\)</span>.</p>
<p>As alternative, a <strong>highest posterior density (HPD)</strong> credible interval of coverage <span class="math inline">\(\alpha\)</span> is found by identifying the shortest interval (i.e.&nbsp;with smallest support) for the given <span class="math inline">\(\alpha\)</span> probability mass. Any point within an HDP credible interval has higher density than a point outside the HDP credible interval. Correspondingly, the density at the boundary of an HPD credible interval is constant taking on the same value everywhere along the boundary.</p>
<p>A Bayesian HPD credible interval is constructed in a similar fashion as a likelihood-based confidence interval, starting from the mode of the posterior density and then looking for a common threshold value for the density to define the boundary of the credible interval. When the posterior density has multiple modes the HPD interval may be disjoint. HPD intervals are also well defined for multivariate <span class="math inline">\(\boldsymbol \theta\)</span> with the boundaries given by the contour lines of the posterior density resulting from the threshold value.</p>
<p>In the Worksheet B1 examples for both types of credible intervals are given and compared visually.</p>
</section>
<section id="practical-application-of-bayes-statistics-on-the-computer" class="level3">
<h3 class="anchored" data-anchor-id="practical-application-of-bayes-statistics-on-the-computer">Practical application of Bayes statistics on the computer</h3>
<p>As we have seen Bayesian learning is <em>conceptually straightforward</em>:</p>
<ol type="1">
<li>Specify prior uncertainty <span class="math inline">\(p(\boldsymbol \theta\)</span>) about the parameters of interest <span class="math inline">\(\boldsymbol \theta\)</span>.</li>
<li>Specify the data-generating process for a specified parameter: <span class="math inline">\(p(x | \boldsymbol \theta)\)</span>.</li>
<li>Apply Bayes’ theorem to update prior uncertainty in the light of the new data.</li>
</ol>
<p>In practice, however, computing the posterior distribution can be <em>computationally very demanding</em>, especially for complex models.</p>
<p>For this reason specialised software packages have been developed for computational Bayesian modelling, for example:</p>
<ul>
<li><p>Bayesian statistics in R: <a href="https://cran.r-project.org/web/views/Bayesian.html" class="uri">https://cran.r-project.org/web/views/Bayesian.html</a></p></li>
<li><p>Stan probabilistic programming language (interfaces with R, Python, Julia and other languages) — <a href="https://mc-stan.org" class="uri">https://mc-stan.org</a></p></li>
<li><p>Bayesian statistics in Python: <a href="https://github.com/pymc-devs/pymc">PyMC</a> using <a href="https://pytensor.readthedocs.io">PyTensor</a> as backend, <a href="http://num.pyro.ai/">NumPyro</a> using <a href="https://jax.readthedocs.io">JAX</a> as backend, <a href="https://www.tensorflow.org/probability/examples/TensorFlow_Probability_on_JAX">TensorFlow Probability on JAX</a> using <a href="https://jax.readthedocs.io">JAX</a> as backend, <a href="http://docs.pyro.ai/">Pyro</a> using <a href="https://pytorch.org/">PyTorch</a> as backend, <a href="https://www.tensorflow.org/probability/">TensorFlow Probability</a> using <a href="https://www.tensorflow.org/">Tensorflow</a> as backend.</p></li>
<li><p>Bayesian statistics in Julia: <a href="https://turinglang.org/">Turing.jl</a></p></li>
<li><p>Bayesian hierarchical modelling with <a href="https://www.mrc-bsu.cam.ac.uk/software">BUGS</a>, <a href="https://mcmc-jags.sourceforge.io/">JAGS</a> and <a href="https://r-nimble.org/">NIMBLE</a>.</p></li>
</ul>
<p>In addition to numerical procedures to sample from the posterior distribution there are also many procedures aiming to approximate the Bayesian posterior, employing the <a href="https://en.wikipedia.org/wiki/Laplace%27s_method">Laplace approximation</a>, integrated nested Laplace approximation (INLA), <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational Bayes</a> etc.</p>
</section>
</section>
<section id="some-background-on-bayesian-statistics" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="some-background-on-bayesian-statistics"><span class="header-section-number">15.2</span> Some background on Bayesian statistics</h2>
<section id="bayesian-interpretation-of-probability" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-interpretation-of-probability">Bayesian interpretation of probability</h3>
<section id="what-makes-you-bayesian" class="level4">
<h4 class="anchored" data-anchor-id="what-makes-you-bayesian">What makes you “Bayesian”?</h4>
<p>If you use Bayes’ theorem are you therefore automatically a Bayesian? No!!</p>
<p>Bayes’ theorem is a mathematical fact from probability theory. Hence, Bayes’ theorem is valid for everyone, whichever form for statistical learning your are subscribing (such as frequentist ideas, likelihood methods, entropy learning, Bayesian learning).</p>
<p>As we discuss now the key difference between Bayesian and frequentist statistical learning lies in the differences in <em>interpretation of probability</em>, not in the mathematical formalism for probability (which includes Bayes’ theorem).</p>
</section>
<section id="mathematics-of-probability" class="level4">
<h4 class="anchored" data-anchor-id="mathematics-of-probability">Mathematics of probability</h4>
<p>The mathematics of probability in its modern foundation was developed by <a href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov">Andrey Kolmogorov (1903–1987)</a>. In this book <a href="https://en.wikipedia.org/wiki/Probability_axioms">Foundations of the Theory of Probability (1933)</a> he establishes probability in terms of set theory/ measure theory. This theory provides a coherent mathematical framework to work with probabilities.</p>
<p>However, Kolmogorov’s theory does <em>not</em> provide an interpretation of probability!</p>
<p><span class="math inline">\(\rightarrow\)</span> The Kolmogorov framework is the basis for both the frequentist and the Bayesian interpretation of probability.</p>
</section>
<section id="interpretations-of-probability" class="level4">
<h4 class="anchored" data-anchor-id="interpretations-of-probability">Interpretations of probability</h4>
<p>Essentially, there are two major commonly used interpretation of probability in statistics - the <strong>frequentist interpretation</strong> and the <strong>Bayesian interpretation</strong>.</p>
<p><strong>A: Frequentist interpretation</strong></p>
<p>Probability is interpreted as a relative frequency (of an event in a long-running series of identically repeated experiments, ideally based on infinitely many trials)</p>
<p>Note that the law of large numbers provides justification to interpret long-running relative frequencies as probabilities. However, the converse assumption — and the key frequentist assumption! — that all probabilities have a frequentist interpretation does <em>not</em> follow from the law of large numbers.</p>
<p>By definition, the frequentist view of probability is very restrictive. For example, frequentist probability cannot be used to describe events that occur only a single time. Frequentist probability thus can only be applied asymptotically, for large samples.</p>
<p>The frequentist view of probability is also called the <em>ontological view</em> of probability as it assumes that probability “exists” independently of an observer and our knowledge.</p>
<p>In turn, ontological probability implies actual inherent randomness which, as we have discussed already in the introduction, amounts only to a small fraction of settings in which probability is used.</p>
<p>As a result, in classical frequentist statistics there are constructs other than probability to assess the uncertainty about parameters (e.g.&nbsp;confidence intervals, sampling distributions).</p>
<p><strong>B: Bayesian probability</strong></p>
<p>“Probability does not exist” — famous quote by <a href="https://en.wikipedia.org/wiki/Bruno_de_Finetti">Bruno de Finetti (1906–1985)</a>, a Bayesian statistician.</p>
<p>What does this mean?</p>
<p>Probability is a <strong>description of the state of knowledge</strong> and of <strong>uncertainty</strong>.</p>
<p>Probability is thus an <em>epistemological quantity</em> that is assigned and that changes with more information rather than something that is an inherent property.</p>
<p>Note that this notion of probability does not require any repeated experiments. The Bayesian interpretation of probability is valid regardless of sample size or the number or repetitions of an experiment.</p>
<p>Epistemiological probability can be applied both to single repetitions and to large number of trials, in the latter case (by means of the law of large numbers) it agrees numerically with frequentist probability.</p>
<p><strong>Hence, the key difference between frequentist and Bayesian approaches is not the use of Bayes’ theorem. Rather it is whether you consider probability as ontological (frequentist) or epistemological entity (Bayesian).</strong></p>
<p>As a result, in Bayesian statistics to assess the uncertainty about parameters one can use probability directy (in the form of prior and posterior distributions).</p>
</section>
</section>
<section id="historical-developments" class="level3">
<h3 class="anchored" data-anchor-id="historical-developments">Historical developments</h3>
<ul>
<li>Bayesian statistics is named after <a href="https://de.wikipedia.org/wiki/Thomas_Bayes">Thomas Bayes</a> (1701-1761). His paper <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> introducing the famous theorem was published only after his death (1763).</li>
</ul>
<ul>
<li><a href="https://de.wikipedia.org/wiki/Pierre-Simon_Laplace">Pierre-Simon Laplace</a> (1749-1827) was the first to practically use Bayes’ theorem for statistical calculations, and he also independently discovered Bayes’ theorem in 1774 <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
</ul>
<ul>
<li><p>This activity was then called “<a href="https://en.wikipedia.org/wiki/Inverse_probability">inverse probability</a>” and not “Bayesian statistics”.</p></li>
<li><p>Between 1900 and 1940 classical mathematical statistics was developed and the field was heavily influenced and dominated by <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">R.A. Fisher</a> (who invented likelihood theory and ANOVA, among other things - he was also working in biology and was professor of genetics). Fisher was very much opposed to Bayesian statistics.</p></li>
<li><p>1931 <a href="https://en.wikipedia.org/wiki/Bruno_de_Finetti">Bruno de Finetti</a> publishes his “<a href="https://en.wikipedia.org/wiki/De_Finetti%27s_theorem">representation theorem</a>”. This shows that the joint distribution of a sequence of exchangeable events (i.e.&nbsp;where the ordering can be permuted) can be represented by a mixture distribution that can be constructed via Bayes’ theorem. (Note that exchangeability is a weaker condition than i.i.d.) This theorem is often used as a justification of Bayesian statistics (along with the so-called Dutch book argument, also by de Finetti).</p></li>
<li><p>1933 publication of <a href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov">Andrey Kolmogorov</a>’s book on probability theory.</p></li>
<li><p>1946 Cox theorem by <a href="https://en.wikipedia.org/wiki/Richard_Threlkeld_Cox">Richard T. Cox (1898–1991)</a>: the aim to generalise classical logic from TRUE/FALSE statements to continuous measures of uncertainty inevitably leads to probability theory and Bayesian learning! This justification of Bayesian statistics was later popularised by <a href="https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes">Edwin T. Jaynes (1922–1998)</a> in various books (1959, 2003).</p></li>
<li><p>1955 Stein Paradox - <a href="https://en.wikipedia.org/wiki/Charles_M._Stein">Charles M. Stein (1920–2016)</a> publishes a paper on the Stein estimator — an estimator of the mean that dominates the ML estimator (i.e.&nbsp;the sample average). The Stein estimator is better in terms of MSE than the ML estimator, which was very puzzling at that time but it is easy to understand from a Bayesian perspective.</p></li>
<li><p>Only from the 1950s the use of the term “Bayesian statistics” became prevalent — see Fienberg (2006) <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p></li>
</ul>
<p>Due to advances in personal computing from 1970 onwards Bayesian learning has become more pervasive!</p>
<ul>
<li>Computers allow to do the complex (numerical) calculations needed in Bayesian statistics .</li>
<li>Metropolis-Hastings algorithm published in 1970 (which allows to sample from a posterior distribution without explicitly computing the marginal likelihood).</li>
<li>Development of regularised estimation techniques such as penalised likelihood in regression (e.g.&nbsp;ridge regression 1970).</li>
<li>penalised likelihood via KL divergence for model selection (Akaike 1973).</li>
<li>A lot of work on interpreting Stein estimators as empirical Bayes estimators (Efron and Morris 1975)</li>
<li>regularisation originally was only meant to make singular systems/matrices invertible, but then it turned out regularisation has also a Bayesian interpretation.</li>
<li>Reference priors (Bernardo 1979) proposed as default priors for models with multiple parameters.</li>
<li>The EM algorithm (published in 1977) uses Bayes theorem for imputing the distribution of the latent variables.</li>
</ul>
<p>Another boost was in the 1990/2000s when in science (e.g.&nbsp;genomics) many complex and high-dimensional data set were becoming the norm, not the exception.</p>
<ul>
<li>Classical statistical methods cannot be used in this setting (overfitting!) so new methods were developed for high-dimensional data analysis, many with a direct link to Bayesian statistics</li>
<li>1996 lasso (L1 regularised) regression invented by <a href="https://en.wikipedia.org/wiki/Robert_Tibshirani">Robert Tibshirani</a>.</li>
<li>Machine learning methods for non-parametric and extremely highly parametric models (neural network) require either explicit or implicit regularisation.</li>
<li>Many Bayesians in this field, many using <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational Bayes techniques</a> which may be viewed as generalisation of the EM algorithm and are also linked to methods used in statistical physics.</li>
</ul>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Bayes, T. 1763. <em>An essay towards solving a problem in the doctrine of chances</em>. The Philosophical Transactions <strong>53</strong>:370–418. <a href="https://doi.org/10.1098/rstl.1763.0053" class="uri">https://doi.org/10.1098/rstl.1763.0053</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Laplace, P.-S. 1774. <em>Mémoire sur la probabilité de causes par les évenements</em>. Mémoires de mathématique et de physique, présentés à l’Académie Royale des sciences par divers savants et lus dans ses assemblées. Paris, Imprimerie Royale, pp.&nbsp;621–657.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Fienberg, S. E. 2006. <em>When did Bayesian inference become “Bayesian”?</em> Bayesian Analysis <strong>1</strong>:1–40. <a href="https://doi.org/10.1214/06-BA101" class="uri">https://doi.org/10.1214/06-BA101</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./14-bayes2.html" class="pagination-link" aria-label="Models with latent variables and missing data">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./16-bayes4.html" class="pagination-link" aria-label="Bayesian learning in practice">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practice</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>