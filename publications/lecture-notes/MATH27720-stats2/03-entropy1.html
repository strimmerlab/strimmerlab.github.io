<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Entropy – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04-entropy2.html" rel="next">
<link href="./02-intro2.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-9290db0fca16de22067673ab8036b289.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-entropy1.html">Information</a></li><li class="breadcrumb-item"><a href="./03-entropy1.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Information</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Risk and divergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Local divergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Maximum entropy</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#code-length-and-log-loss" id="toc-code-length-and-log-loss" class="nav-link active" data-scroll-target="#code-length-and-log-loss"><span class="header-section-number">3.1</span> Code length and log-loss</a>
  <ul class="collapse">
  <li><a href="#information-storage-units" id="toc-information-storage-units" class="nav-link" data-scroll-target="#information-storage-units">Information storage units</a></li>
  <li><a href="#constant-code-lengths" id="toc-constant-code-lengths" class="nav-link" data-scroll-target="#constant-code-lengths">Constant code lengths</a></li>
  <li><a href="#variable-code-lengths" id="toc-variable-code-lengths" class="nav-link" data-scroll-target="#variable-code-lengths">Variable code lengths</a></li>
  </ul></li>
  <li><a href="#information-entropy" id="toc-information-entropy" class="nav-link" data-scroll-target="#information-entropy"><span class="header-section-number">3.2</span> Information entropy</a>
  <ul class="collapse">
  <li><a href="#entropy-of-a-distribution" id="toc-entropy-of-a-distribution" class="nav-link" data-scroll-target="#entropy-of-a-distribution">Entropy of a distribution</a></li>
  <li><a href="#sec-entropyconcave" id="toc-sec-entropyconcave" class="nav-link" data-scroll-target="#sec-entropyconcave">Concavity</a></li>
  <li><a href="#information-entropy-for-discrete-distributions" id="toc-information-entropy-for-discrete-distributions" class="nav-link" data-scroll-target="#information-entropy-for-discrete-distributions">Information entropy for discrete distributions</a></li>
  <li><a href="#information-entropy-for-continuous-distributions" id="toc-information-entropy-for-continuous-distributions" class="nav-link" data-scroll-target="#information-entropy-for-continuous-distributions">Information entropy for continuous distributions</a></li>
  <li><a href="#interpretation-of-entropy" id="toc-interpretation-of-entropy" class="nav-link" data-scroll-target="#interpretation-of-entropy">Interpretation of entropy</a></li>
  </ul></li>
  <li><a href="#colorred-blacktriangleright-entropy-in-physics" id="toc-colorred-blacktriangleright-entropy-in-physics" class="nav-link" data-scroll-target="#colorred-blacktriangleright-entropy-in-physics"><span class="header-section-number">3.3</span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Entropy in physics</a>
  <ul class="collapse">
  <li><a href="#entropy-as-macroscopic-phenomenon" id="toc-entropy-as-macroscopic-phenomenon" class="nav-link" data-scroll-target="#entropy-as-macroscopic-phenomenon">Entropy as macroscopic phenomenon</a></li>
  <li><a href="#colorred-blacktriangleright-combinatorial-derivation" id="toc-colorred-blacktriangleright-combinatorial-derivation" class="nav-link" data-scroll-target="#colorred-blacktriangleright-combinatorial-derivation"><span class="math inline">\(\color{Red} \blacktriangleright\)</span> Combinatorial derivation</a></li>
  </ul></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">3.4</span> Further reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-entropy1.html">Information</a></li><li class="breadcrumb-item"><a href="./03-entropy1.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-entropy" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Entropy, a fundamental concept that originated in physics, is central also in information theory and statistical learning. This chapter introduces information entropy through log-loss and combinatorics.</p>
<section id="code-length-and-log-loss" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="code-length-and-log-loss"><span class="header-section-number">3.1</span> Code length and log-loss</h2>
<section id="information-storage-units" class="level3">
<h3 class="anchored" data-anchor-id="information-storage-units">Information storage units</h3>
<p>When we record information on paper or on a computer, we need an alphabet, whose symbols act as units of stored information:</p>
<ul>
<li><p>For an alphabet of size <span class="math inline">\(b=2\)</span> (e.g.&nbsp;symbols <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>) the storage unit is called <strong>bit</strong> (binary digit). To represent <span class="math inline">\(K=256\)</span> possible states requires <span class="math inline">\(\log_2 256 = 8\)</span> bits (or 1 byte) of storage.</p></li>
<li><p>For an alphabet of size <span class="math inline">\(b=10\)</span> (e.g.&nbsp;Arabic numerals) the unit is called <strong>dit</strong> (decimal digit). To represent <span class="math inline">\(K=100\)</span> possible states requires <span class="math inline">\(\log_{10} 100 = 2\)</span> dits.</p></li>
</ul>
<p>Bit and dit are <strong>units of information</strong> to describe information content. One dit is equal to <span class="math inline">\(\log_2 10 \approx 3.32\)</span> bits.</p>
<p>If the natural logarithm is used (<span class="math inline">\(b=e\)</span>) the unit of information is called <strong>nat</strong>. This is the standard unit of information used in statistics. It is equal to <span class="math inline">\(\log_2 e \approx 1.44\)</span> bits.</p>
</section>
<section id="constant-code-lengths" class="level3">
<h3 class="anchored" data-anchor-id="constant-code-lengths">Constant code lengths</h3>
<p>We now assume a discrete variable <span class="math inline">\(x\)</span> with <span class="math inline">\(K\)</span> possible outcomes <span class="math inline">\(\Omega = \{\omega_1, \ldots, \omega_K\}\)</span>. In order to record a realised state of <span class="math inline">\(x\)</span> using an alphabet of size <span class="math inline">\(b\)</span> we require <span class="math display">\[
S = \log_b K
\]</span> units of information. <span class="math inline">\(S\)</span> is called the <strong>code length</strong> or the <strong>cost</strong> to describe the state of <span class="math inline">\(x\)</span>. The base <span class="math inline">\(b\)</span> can be larger or smaller than <span class="math inline">\(K\)</span>.</p>
<p>In the above we have tacitly assumed that storage size, code length, and cost requirements are fixed and identical for all possible <span class="math inline">\(K\)</span> states. This can be made more explicit by writing <span class="math display">\[
S = -\log_b \left( \frac{1}{K} \right)
\]</span> where <span class="math inline">\(1/K\)</span> is the equal probability of each of the <span class="math inline">\(K\)</span> states.</p>
</section>
<section id="variable-code-lengths" class="level3">
<h3 class="anchored" data-anchor-id="variable-code-lengths">Variable code lengths</h3>
<p>In practice the <span class="math inline">\(K\)</span> states are usually not equally likely. For example, <span class="math inline">\(x\)</span> might represent letters in a text message, each occurring with different frequency. To account for that we use a distribution <span class="math inline">\(P\)</span> with pmf <span class="math inline">\(p(x)\)</span> and allow variable code lengths per state.</p>
<p>Generalising the case of constant code lengths, we use the negative logarithm to map the probability of a state <span class="math inline">\(x\)</span> to a corresponding cost and code length: <span class="math display">\[
S(x, P) = -\log p(x)
\]</span> This is called the <strong>log-loss</strong> or the <strong>logarithmic scoring rule</strong>. Under this rule, more probable states under model <span class="math inline">\(P\)</span> get shorter codes (smaller loss/cost) and less probable states get longer codes (larger loss/cost).</p>
<p>As we will see in the examples (<a href="#exm-entropycategorical" class="quarto-xref">Example&nbsp;<span>3.3</span></a>, <a href="#exm-entropydiscunif" class="quarto-xref">Example&nbsp;<span>3.4</span></a> and <a href="#exm-entropyconcentrated" class="quarto-xref">Example&nbsp;<span>3.5</span></a>) using variable code lengths lets the expected code length be much smaller than for a fixed-length encoding.</p>
<p>The log-loss underpins much of statistical estimation and inference through likelihood and Bayesian methods. It is the leading example of a <strong>proper scoring rule</strong>, a broader class of loss functions suitable for principled statistical analysis (<a href="#nte-properscoringrules" class="quarto-xref">Note&nbsp;<span>3.1</span></a>).</p>
<p>Scoring rules are only defined up to an equivalence class <span class="math inline">\(S^{\text{equiv}}(x, P) = k S(x, P) + c(x)\)</span>, where <span class="math inline">\(k\)</span> is a positive scaling factor. All <strong>equivalent scoring rules</strong> have the same minimiser for fixed <span class="math inline">\(x\)</span>.</p>
<p>In the equivalence class for the log-loss, the scaling factor <span class="math inline">\(k\)</span> determines the base of the logarithm. In the following, we set <span class="math inline">\(b=e\)</span> and use the natural logarithm and nats as units of information throughout. We will also apply the log-loss to both discrete and continuous variables <span class="math inline">\(x\)</span> and corresponding models&nbsp;<span class="math inline">\(P\)</span>.</p>
<div id="nte-properscoringrules" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.1: <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Proper scoring rules
</div>
</div>
<div class="callout-body-container callout-body">
<p>The logarithmic scoring rule <span class="math inline">\(S(x, P) = -\log p(x)\)</span> is a particularly important loss function to evaluate probabilistic models. It is unique as the only <strong>local</strong> strictly proper scoring rule, solely depending on the value of the pdmf at the observed outcome, and not on any other features of the distribution.</p>
<p>However, instead of the negative logarithm one may also employ other suitable loss functions as scoring rule, while still preserving the useful properties of the log-loss. This leads to the wider class of <strong>proper scoring rules</strong>. In machine learning these are related to <strong>Bregman divergences</strong>.</p>
<p>Using proper scoring rules other than the log-loss may be preferable, e.g., for computational convenience and simplicity, for numerical stability for robustness to outliers or interpretability.</p>
</div>
</div>
<div id="exm-surprise" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1</strong></span> Surprise and log-odds:</p>
<p>The negative logarithm of a probability or density is called the <strong>surprise</strong> or <strong>surprisal</strong>. The log-loss thus returns the surprise for the outcomes of a random variable.</p>
<p>For a discrete random variable, the surprise <span class="math inline">\(-\log p\)</span> to observe an outcome with probability <span class="math inline">\(p=1\)</span> is zero, and conversely the surprise to observe an outcome with probability <span class="math inline">\(p=0\)</span> is infinite.</p>
<p>The <strong>log-odds</strong> <span class="math display">\[
\log\left( \frac{p}{1-p} \right) = -\log(1-p) - ( -\log p)
\]</span> is the difference between the surprise of the complementary event (probability <span class="math inline">\(1-p\)</span>) and the surprise of the event (probability <span class="math inline">\(p\)</span>):</p>
</div>
<div id="exm-logscorenormdist" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2</strong></span> Log-loss for a normal model:</p>
<p>If we quote in the log-loss <span class="math inline">\(S(x, P) = -\log p(x)\)</span> the normal distribution <span class="math inline">\(P = N(\mu, \sigma^2)\)</span> with density <span class="math inline">\(p(x |\mu, \sigma^2)= \frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\)</span> we get <span class="math display">\[
S\left(x,P\right) = \frac{1}{2} \left( \log(2\pi\sigma^2) + \frac{(x-\mu)^2}{\sigma^2}\right)
\]</span> For a normal model with fixed variance the the log-loss is thus equivalent to the squared distance between <span class="math inline">\(x\)</span> and&nbsp;<span class="math inline">\(\mu\)</span>.</p>
</div>
</section>
</section>
<section id="information-entropy" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="information-entropy"><span class="header-section-number">3.2</span> Information entropy</h2>
<section id="entropy-of-a-distribution" class="level3">
<h3 class="anchored" data-anchor-id="entropy-of-a-distribution">Entropy of a distribution</h3>
<p>The expectation of the log-loss <span class="math display">\[
\begin{split}
H(P) &amp;= \operatorname{E}_P\left( S(x, P) \right) \\
     &amp;= - \operatorname{E}_P\left(\log p(x)\right)
\end{split}
\]</span> is the <strong>information entropy</strong> <span class="math inline">\(H(P)\)</span> of the distribution&nbsp;<span class="math inline">\(P\)</span>.</p>
<p>The notation <span class="math inline">\(H(P)\)</span> emphasises that entropy is a functional of the distribution <span class="math inline">\(P\)</span>. Note that in this expression <span class="math inline">\(P\)</span> serves two distinct roles, first as the data-generating model (the expectation is taken with respect to <span class="math inline">\(P\)</span>) and second as the model <span class="math inline">\(P\)</span> quoted in the log-loss.</p>
<p>Information entropy is <strong>only defined up to an affine transformation with a positive scaling factor</strong> as the underlying loss function is itself also only defined up to an equivalence class <span class="math display">\[
H(P)^{\text{equiv}}= - k \operatorname{E}_P\left(\log p(x)\right)   + c
\]</span> The scaling factor <span class="math inline">\(k\)</span> determines the units (or the base of the logarithm) and the additive constant <span class="math inline">\(c = \operatorname{E}_P( c(x))\)</span> can be chosen to fix a reference value (e.g.&nbsp;typically either set the maximum or minimum entropy to zero).</p>
<div id="nte-cenames" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.2: Other names for information entropy
</div>
</div>
<div class="callout-body-container callout-body">
<p>Information entropy is also called <strong>Shannon entropy</strong> (for discrete distributions) and <strong>differential entropy</strong> (for continuous distribution).</p>
<p>In physics, the formula for information entropy is primarily known as <strong>Gibbs entropy</strong>, but also referred to as <strong>Boltzmann-Gibbs entropy</strong> (BG entropy) or <strong>Boltzmann-Gibbs-Shannon entropy</strong> (BGS entropy) entropy. Physical entropy also carries the <strong>Boltzmann constant</strong> <span class="math inline">\(k_B=1.380649 \times 10^{-23} J/K\)</span> as scaling factor, giving physical entropy units of energy/temperature.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>We will mostly use the term <strong>information entropy</strong>.</p>
</div>
</div>
</section>
<section id="sec-entropyconcave" class="level3">
<h3 class="anchored" data-anchor-id="sec-entropyconcave">Concavity</h3>
<p>A key property of the <strong>entropy</strong> <span class="math inline">\(H(P)\)</span> is that it is <strong>strictly concave</strong> in <span class="math inline">\(P\)</span>. This means that <span class="math display">\[
H( P_{\lambda} ) &gt; (1-\lambda) H(P_0) + \lambda H(P_1)
\]</span> for the mixture <span class="math inline">\(P_{\lambda} = (1-\lambda) P_0 + \lambda P_1\)</span> with <span class="math inline">\(0 &lt; \lambda &lt; 1\)</span> and <span class="math inline">\(P_0 \neq P_1\)</span>, and follows from the fact that <span class="math inline">\(-x \log(x)\)</span> is a strictly concave function.</p>
<p>Strict concavity implies that mixing states (represented by <span class="math inline">\(P_0\)</span> and <span class="math inline">\(P_1\)</span>) raises the entropy. Further, it ensures that the maximum is unique (if a maximum exists).</p>
<p>Concavity is a result of the properness of the log-loss (<a href="04-entropy2.html#sec-properness" class="quarto-xref"><span>Section 4.1.3</span></a>). Specifically, the entropy is (strictly) concave because the underlying scoring rule is (strictly) proper.</p>
</section>
<section id="information-entropy-for-discrete-distributions" class="level3">
<h3 class="anchored" data-anchor-id="information-entropy-for-discrete-distributions">Information entropy for discrete distributions</h3>
<p>The information entropy <span class="math inline">\(H(P)\)</span> of a discrete probability distribution <span class="math inline">\(P\)</span> with pmf <span class="math inline">\(p(x)\)</span> with <span class="math inline">\(x \in \Omega\)</span> is the <strong>Shannon entropy</strong>: <span class="math display">\[
H(P) = - \sum_{x} p(x) \log p(x)
\]</span></p>
<p>By construction, since all <span class="math inline">\(p(x) \in [0,1]\)</span> and hence <span class="math inline">\(-\log p(x) \geq 0\)</span>, the Shannon entropy is <strong>non-negative</strong>.</p>
<div id="exm-entropycategorical" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3</strong></span> Entropy of the categorical distribution:</p>
<p>For <span class="math inline">\(P=\operatorname{Cat}(\boldsymbol p)\)</span> with class probabilities <span class="math inline">\(\boldsymbol p=(p_1, \ldots, p_K)^T\)</span> the information entropy of <span class="math inline">\(P\)</span> is <span class="math display">\[
H(P) = - \sum_{k=1}^{K } p_k \log p_k
\]</span> The maximum entropy (<span class="math inline">\(\log K\)</span>) is achieved for the discrete uniform distribution (<a href="#exm-entropydiscunif" class="quarto-xref">Example&nbsp;<span>3.4</span></a>) and the minimum entropy (<span class="math inline">\(0\)</span>) for a concentrated categorical distribution (<a href="#exm-entropyconcentrated" class="quarto-xref">Example&nbsp;<span>3.5</span></a>). Hence for a categorical distribution <span class="math inline">\(P\)</span> with <span class="math inline">\(K\)</span> categories we have <span class="math display">\[
0 \leq  H(P) \leq \log K
\]</span></p>
</div>
<div id="exm-entropydiscunif" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.4</strong></span> Entropy of the discrete uniform distribution:</p>
<p>The discrete uniform distribution is <span class="math inline">\(U_K=\operatorname{Cat}(\boldsymbol p=\mathbf 1_K/K)\)</span> so that <span class="math inline">\(p_1=p_2= \ldots = p_K = \frac{1}{K}\)</span>. Then <span class="math display">\[
H(U_K) = - \sum_{k=1}^{K}  \frac{1}{K}  \log\left(\frac{1}{K}\right) = \log K
\]</span></p>
<p>Note that <span class="math inline">\(\log K\)</span> is the largest value the information entropy can assume with <span class="math inline">\(K\)</span> classes (cf. <a href="06-entropy4.html#exm-maxentdiscunif" class="quarto-xref">Example&nbsp;<span>6.1</span></a>) and indicates maximum spread of probability mass.</p>
</div>
<div id="exm-entropyconcentrated" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.5</strong></span> Entropy of a categorical distribution with concentrated probability mass:</p>
<p>Let <span class="math inline">\(p_1=1\)</span> and <span class="math inline">\(p_2=p_3=\ldots=p_K=0\)</span>. Using <span class="math inline">\(0\times\log 0=0\)</span> we obtain for the information entropy of <span class="math inline">\(P=\operatorname{Cat}(\boldsymbol p)\)</span> <span class="math display">\[
H(P) = 1 \times \log 1  + 0 \times \log 0 + \dots = 0
\]</span></p>
<p>Note that zero is the smallest value that the entropy can assume for a categorical distribution and that it corresponds to maximum concentration of probability mass.</p>
</div>
<div id="exm-entropygeom" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.6</strong></span> Entropy of the geometric distribution:</p>
<p>With <span class="math inline">\(P = \operatorname{Geom}(\theta)\)</span> and <span class="math inline">\(\theta \in [0,1]\)</span>, support <span class="math inline">\(x \in \{1, 2, \ldots \}\)</span> and pmf <span class="math inline">\(p(x|\theta) = \theta (1-\theta)^{x-1}\)</span>, with <span class="math inline">\(\operatorname{E}(x)= 1/\theta\)</span> and <span class="math inline">\(\operatorname{Var}(x) = (1-\theta)/(\theta^2)\)</span>, the entropy is <span class="math display">\[
\begin{split}
H(P) &amp;= - \operatorname{E}\left( \log \theta + (x-1) \log(1-\theta)   \right)\\
       &amp;= -\log \theta+ \left(\frac{1}{\theta}-1\right)\log(1-\theta)\\
       &amp;= -\frac{\theta \log \theta + (1-\theta) \log(1-\theta) }{\theta}
\end{split}
\]</span> Using the identity <span class="math inline">\(0\times\log 0=0\)</span> we see that the entropy of the geometric distribution for <span class="math inline">\(\theta = 1\)</span> equals 0, i.e.&nbsp;it achieves the minimum possible information entropy for a discrete distribution. Conversely, as <span class="math inline">\(\theta \rightarrow 0\)</span> it diverges to infinity.</p>
</div>
</section>
<section id="information-entropy-for-continuous-distributions" class="level3">
<h3 class="anchored" data-anchor-id="information-entropy-for-continuous-distributions">Information entropy for continuous distributions</h3>
<p>The entropy <span class="math inline">\(H(P)\)</span> of a continuous distribution <span class="math inline">\(P\)</span> with pdf <span class="math inline">\(p(x)\)</span> is the <strong>differential entropy</strong>: <span class="math display">\[
H(P) = - \int_x  p(x)  \log p(x)  \, dx
\]</span></p>
<p>Differential entropy <strong>can be negative</strong> because the logarithm is taken of a density value, and unlike probabilities, densities are not constrained to be less than or equal to one.</p>
<p>Under a <strong>change of variables</strong>, such as from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span>, the shape of the pdf typically changes, so that differential entropy changes as well and is <strong>not invariant</strong>: <span class="math inline">\(H(P_y) \neq H(P_x)\)</span>.</p>
<div id="exm-entropyunif" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.7</strong></span> Entropy of the uniform distribution:</p>
<p>Consider the uniform distribution <span class="math inline">\(P_x = \operatorname{Unif}(0, a)\)</span> with <span class="math inline">\(a&gt;0\)</span>, support <span class="math inline">\(x \in [0, a]\)</span> and density <span class="math inline">\(p(x) = 1/a\)</span>. The corresponding entropy is <span class="math display">\[
\begin{split}
H( P_x ) &amp;= - \int_0^a \frac{1}{a} \, \log\left(\frac{1}{a}\right) \,   dx \\
             &amp;= \frac{1}{a} \, \log a \,\int_0^a  dx \\
             &amp;= \log a \,.
\end{split}
\]</span> The range of the entropy is unbounded for a positive <span class="math inline">\(a\)</span>, and for <span class="math inline">\(0 &lt; a &lt; 1\)</span> the entropy assumes a negative value.</p>
</div>
<div id="exm-entropyvartrans" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.8</strong></span> Differential entropy under a change of variables:</p>
<p>Starting with the uniform distribution <span class="math inline">\(P_x = \operatorname{Unif}(0, a)\)</span> from <a href="#exm-entropyunif" class="quarto-xref">Example&nbsp;<span>3.7</span></a> the variable <span class="math inline">\(x\)</span> is changed to <span class="math inline">\(y = x^2\)</span> yielding the distribution <span class="math inline">\(P_y\)</span> with support from <span class="math inline">\(0\)</span> to <span class="math inline">\(a^2\)</span> and density <span class="math inline">\(p(y) = 1/\left(2 a \sqrt{y}\right)\)</span>.</p>
<p>The corresponding entropy is <span class="math display">\[
\begin{split}
H( P_y ) &amp;=  \int_0^{a^2} 1/\left(2 a \sqrt{y}\right) \, \log \left(2 a \sqrt{y}\right) dy \\
         &amp;= \left[ \sqrt{y}/a \, \left(\log \left( 2 a \sqrt{y} \right)-1\right)  \right]_{y=0}^{y=a^2} \\
             &amp;= \log \left(2 a^2\right) -1 \,.
\end{split}
\]</span> The range of entropy is unbounded, and is negative for <span class="math inline">\(0 &lt; a &lt; \sqrt{e/2}\approx 1.1658\)</span>. As expected <span class="math inline">\(H( P_y ) \neq H( P_x )\)</span> as differential entropy is not invariant under a change of variables.</p>
</div>
<div id="exm-entropynormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.9</strong></span> Entropy of the normal distribution:</p>
<p>The log-density of the univariate normal distribution <span class="math inline">\(P = N(\mu, \sigma^2)\)</span> is <span class="math inline">\(\log p(x |\mu, \sigma^2) = -\frac{1}{2} \left(  \log(2\pi\sigma^2)  + \frac{(x-\mu)^2}{\sigma^2} \right)\)</span> with <span class="math inline">\(\sigma^2 &gt; 0\)</span>. The corresponding entropy is with <span class="math inline">\(\operatorname{E}((x-\mu)^2) = \sigma^2\)</span> <span class="math display">\[
\begin{split}
H(P) &amp; = -\operatorname{E}\left( \log p(x |\mu, \sigma^2) \right)\\
&amp; = \frac{1}{2} \left( \log(2 \pi \sigma^2)+1\right) \,. \\
\end{split}
\]</span> Importantly, the entropy of a normal distribution depends only on its variance&nbsp;<span class="math inline">\(\sigma^2\)</span>, not on its mean&nbsp;<span class="math inline">\(\mu\)</span>. This is intuitively clear as the variance controls the concentration of probability mass. A large variance means that the probability mass is more spread out and thus less concentrated around the mean.</p>
<p>Since the variance <span class="math inline">\(\sigma^2\)</span> ranges from 0 to infinity, the entropy of the normal distribution is unbounded ranging from negative to positive infinity, taking on the value of zero at <span class="math inline">\(\sigma^2 &lt; 1/(2 \pi e) \approx 0.0585\)</span>.</p>
</div>
<div id="exm-entropyexpfam" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.10</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Entropy of an exponential family:</p>
<p>Assume <span class="math inline">\(P(\boldsymbol \eta)\)</span> is an exponential family with canonical parameters <span class="math inline">\(\boldsymbol \eta\)</span>, canonical statistics <span class="math inline">\(\boldsymbol t(x)\)</span> and log-partition function <span class="math inline">\(a(\boldsymbol \eta)\)</span> with log-pdmf<br>
<span class="math inline">\(\log p(x|\boldsymbol \eta) =  \langle \boldsymbol \eta,  \boldsymbol t(x)\rangle + \log h(x) - a(\boldsymbol \eta)\)</span>.</p>
<p>Then with <span class="math inline">\(\operatorname{E}_{ P(\boldsymbol \eta)}(\boldsymbol t(x))=\boldsymbol \mu_{\boldsymbol t}(\boldsymbol \eta)\)</span> the entropy is <span class="math display">\[
\begin{split}
H(P(\boldsymbol \eta) ) &amp;= -\operatorname{E}_{P(\boldsymbol \eta)}( \log p(x|\boldsymbol \eta) ) \\
&amp;=  a(\boldsymbol \eta) -\langle \boldsymbol \eta,  \boldsymbol \mu_{\boldsymbol t}(\boldsymbol \eta)\rangle   - \operatorname{E}_{P(\boldsymbol \eta)}( \log h(x) )
\end{split}
\]</span></p>
</div>
</section>
<section id="interpretation-of-entropy" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-entropy">Interpretation of entropy</h3>
<p>Information entropy <span class="math inline">\(H(P)\)</span> can be interpreted as the <strong>expected cost</strong> or <strong>expected code length</strong> when the data are generated according to model <span class="math inline">\(P\)</span> (“sender”, “encoder”) and the same model <span class="math inline">\(P\)</span> is used to describe the data (“receiver”, “decoder”). As will be discussed in <a href="04-entropy2.html" class="quarto-xref"><span>Chapter 4</span></a> in more detail, using different models for “sending” and “receiving” always increases the cost, hence the entropy is a lower bound on that cost.</p>
<p>Furthermore, information entropy <span class="math inline">\(H(P)\)</span> quantifies the <strong>dispersal of probability mass</strong> within the distribution <span class="math inline">\(P\)</span>. When probability mass is concentrated, entropy is low (<a href="#exm-entropyconcentrated" class="quarto-xref">Example&nbsp;<span>3.5</span></a>). Conversely, when the probability mass is more broadly distributed, the entropy is high (<a href="#exm-entropydiscunif" class="quarto-xref">Example&nbsp;<span>3.4</span></a>).</p>
<p>The notion of entropy as measure of spread is also supported in continuous settings, e.g. the entropy of the normal distribution (<a href="#exm-entropynormal" class="quarto-xref">Example&nbsp;<span>3.9</span></a>) is a monotonically increasing function of the variance.</p>
<p>A distribution <span class="math inline">\(P\)</span> with low entropy and concentrated probability mass is highly informative. Conversely, a distribution with high entropy and dispersed probability mass is uninformative. This is the basis of the principle of maximum entropy (<a href="06-entropy4.html" class="quarto-xref"><span>Chapter 6</span></a>).</p>
</section>
</section>
<section id="colorred-blacktriangleright-entropy-in-physics" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="colorred-blacktriangleright-entropy-in-physics"><span class="header-section-number">3.3</span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Entropy in physics</h2>
<section id="entropy-as-macroscopic-phenomenon" class="level3">
<h3 class="anchored" data-anchor-id="entropy-as-macroscopic-phenomenon">Entropy as macroscopic phenomenon</h3>
<p>The concept of entropy was introduced in the mid-19th century within <strong>thermodynamics</strong>, a phenomenological theory that quantifies heat, work, energy, and the laws governing their transformations. Entropy determines how much of the internal energy is available to do work. Low entropy implies energy is concentrated and available for work, and high entropy means energy is spread out and not available for work. Systems at equilibrium are characterised by maximum entropy (subject to constraints).</p>
<p>In <strong>statistical mechanics</strong>, entropy and the laws of thermodynamics are explained as emergent, probabilistic consequences of the behaviour of many-particle systems. A <strong>microstate</strong> provides a complete specification of the current state of the system which evolves by transitioning from one such state to the other.</p>
<p>In the <strong>Gibbs approach</strong>, hypothetical ensembles of systems are considered, represented by a probability distribution over all microstates. If that distribution is concentrated on relatively few microstates the entropy is low. Conversely, if it is spread over many microstates the entropy is large. Once in equilibrium, the distribution and its entropy is stationary. Its information entropy corresponds the conventional thermodynamic entropy.</p>
<p>In the <strong>Boltzmann approach</strong>, entropy arises from coarse graining and the introduction of <strong>macrostates</strong> (e.g., particle counts or energy levels). The entropy of a specific system (not of an ensemble!) is computed from its current macrostate, by determining the number of underlying microstates compatible with it. This yields the standard formula for information entropy (see <a href="#exm-combinatorialentropy" class="quarto-xref">Example&nbsp;<span>3.11</span></a>) applied to a sampled distribution. Equilibrium corresponds to the typical macrostate with overwhelmingly many compatible microstates (maximum entropy) and is the state in which the system spends most of its time. Entropy fluctuates as the system evolves and continues to do so even in equilibrium. This interpretation explains irreversibility as a statistical phenomenon.</p>
<p>Both the Gibbs and Boltzmann approaches support the interpretation of entropy as a measure of dispersal and use the same functional form of entropy.</p>
<p>A further common physical interpretation of entropy is as a measure of disorder, with low entropy corresponding to “order” and high entropy to “disorder”. However, this interpretation is now discouraged because those everyday terms are ambiguous. Many apparently ordered systems can have large entropy, while some disordered-looking systems can have low entropy.</p>
<p>Finally, in physics the concavity of entropy is a crucial property as it underpins the <a href="https://en.wikipedia.org/wiki/Second_law_of_thermodynamics">second law of thermodynamics</a>.</p>
</section>
<section id="colorred-blacktriangleright-combinatorial-derivation" class="level3">
<h3 class="anchored" data-anchor-id="colorred-blacktriangleright-combinatorial-derivation"><span class="math inline">\(\color{Red} \blacktriangleright\)</span> Combinatorial derivation</h3>
<p>Information entropy can also be obtained by combinatorial considerations. This was the route taken by Boltzmann in 1877 in the early days of statistical mechanics.</p>
<div id="exm-combinatorialentropy" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.11</strong></span> <span class="math inline">\(\color{Red} \blacktriangleright\)</span> Combinatorial derivation of information entropy:</p>
<p>Let <span class="math inline">\(D=\{n_1, \ldots, n_K\}\)</span> be counts for <span class="math inline">\(K\)</span> classes with <span class="math inline">\(n = \sum_{k=1}^K n_k\)</span> and <span class="math inline">\(\hat{Q} = \operatorname{Cat}(\hat{\boldsymbol q})\)</span> be the corresponding empirical categorical distribution with class frequencies <span class="math inline">\(\hat{q}_k = n_k/n\)</span>.</p>
<p>The entropy of <span class="math inline">\(\hat{Q}\)</span> is <span class="math inline">\(H(\hat{Q}) = -\sum_{k=1}^K \hat{q}_k  \log \hat{q}_k\)</span> (<a href="#exm-entropycategorical" class="quarto-xref">Example&nbsp;<span>3.3</span></a>).</p>
<p>The macrostate corresponds to the counts <span class="math inline">\(D\)</span> or equivalently the distribution <span class="math inline">\(\hat{Q}\)</span>. Each particular allocation of the <span class="math inline">\(n\)</span> elements to the <span class="math inline">\(K\)</span> groups constitutes a microstate.</p>
<p>The number of allocations of <span class="math inline">\(n\)</span> distinct items to <span class="math inline">\(K\)</span> groups compatible with the given counts <span class="math inline">\(D\)</span> is given by the <strong>multinomial coefficient</strong> <span class="math display">\[
\begin{split}
W_K &amp;= \binom{n}{n_1, \ldots, n_K} \\
    &amp;= \frac {n!}{n_1! \, n_2! \, \ldots \, n_K! }
\end{split}
\]</span> This represents the <strong>multiplicity</strong> of the macrostate.</p>
<p>Using the de Moivre-Sterling formula we can, for large <span class="math inline">\(n\)</span>, approximate the factorial according to <span class="math display">\[
\log n! \approx  n \log n  -n
\]</span> and get <span class="math display">\[
\begin{split}
\log W_K &amp;= \log n! - \sum_{k=1}^K \log n_k!\\
&amp; \approx    n \log n  -n - \sum_{k=1}^K (n_k \log n_k  -n_k) \\
&amp; = \sum_{k=1}^K n_k \log n - \sum_{k=1}^K n_k \log n_k\\
&amp; = - n \sum_{k=1}^K \frac{n_k}{n} \log\left( \frac{n_k}{n} \right)\\
&amp; = -n \sum_{k=1}^K \hat{q}_k  \log \hat{q}_k   \\
&amp; = n H(\hat{Q})
\end{split}
\]</span></p>
<p>Therefore, information entropy and the multinomial coefficient and are directly related <span class="math display">\[
H(\hat{Q})  \approx \frac{1}{n} \log W_K
\]</span> with equality for <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>The number of permutations <span class="math inline">\(W_K\)</span> is far greater when items are uniformly “spread out” over the <span class="math inline">\(K\)</span> categories than when they are “concentrated”. Hence, the combinatorial perspective supports the interpretation of entropy as a measure of dispersal.</p>
</div>
</section>
</section>
<section id="further-reading" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">3.4</span> Further reading</h2>
<p>For a justification of the logarithmic loss function see, e.g., <span class="citation" data-cites="Good1952">Good (<a href="bibliography.html#ref-Good1952" role="doc-biblioref">1952</a>)</span> and <span class="citation" data-cites="Bernardo1979">Bernardo (<a href="bibliography.html#ref-Bernardo1979" role="doc-biblioref">1979</a>)</span>.</p>
<p>A brief overview of proper scoring rules is found in the supplementary <a href="https://strimmerlab.github.io/publications/lecture-notes/probability-distribution-refresher/index.html">Probability and Distribution Refresher notes</a>.</p>
<p><span class="citation" data-cites="Akaike1985">Akaike (<a href="bibliography.html#ref-Akaike1985" role="doc-biblioref">1985</a>)</span> provides an historical account from a statistical perspective of how Boltzmann arrived in 1877 at the formula for information entropy.</p>
<p><span class="citation" data-cites="Lemons2013">Lemons (<a href="bibliography.html#ref-Lemons2013" role="doc-biblioref">2013</a>)</span> offers a friendly introduction to entropy in physics and in information theory. This book also discusses the combinatorial derivation of entropy (<a href="#exm-combinatorialentropy" class="quarto-xref">Example&nbsp;<span>3.11</span></a>) as well as various interpretations and applications of entropy.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>A bit of history
</div>
</div>
<div class="callout-body-container callout-body">
<p>Entropy as a thermodynamic quantity was introduced in 1865 by <a href="https://en.wikipedia.org/wiki/Rudolf_Clausius">Rudolph Clausius (1822-1888)</a>, along with the first and second law of thermodynamics.</p>
<p>The probabilistic and combinatorial underpinnings of entropy were developed in the period from 1870 to 1900 by <a href="https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs">Josiah W. Gibbs (1839–1903)</a> and <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann">Ludwig Boltzmann (1844–1906)</a>. The specific formula for information entropy was found by Boltzmann in 1877.</p>
<p>By the mid-20th century the notion of information entropy was established in statistics. <a href="https://en.wikipedia.org/wiki/Ralph_Hartley">Ralph V. L. Hartley (1888–1970)</a> and <a href="https://en.wikipedia.org/wiki/I._J._Good">Irving J. Good (1916–2009)</a> argued for logarithmic measures of information and <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon (1916–2001)</a> introduced entropy in information theory <span class="citation" data-cites="Shannon1948">(<a href="bibliography.html#ref-Shannon1948" role="doc-biblioref">Shannon 1948</a>)</span>.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Akaike1985" class="csl-entry" role="listitem">
Akaike, H. 1985. <span>“Prediction and Entropy.”</span> In <em>A Celebration of Statistics</em>, edited by A. C. Atkinson and S. E. Fienberg, 1–24. Springer. <a href="https://doi.org/10.1007/978-1-4613-8560-8_1">https://doi.org/10.1007/978-1-4613-8560-8_1</a>.
</div>
<div id="ref-Bernardo1979" class="csl-entry" role="listitem">
Bernardo, J. M. 1979. <span>“Expected Information as Expected Utility.”</span> <em>Ann. Stat.</em> 7: 686–90. <a href="https://doi.org/10.1214/aos/1176344689">https://doi.org/10.1214/aos/1176344689</a>.
</div>
<div id="ref-Good1952" class="csl-entry" role="listitem">
Good, I. J. 1952. <span>“Rational Decisions.”</span> <em>J. R. Statist. Soc. B</em> 14: 107–14. <a href="https://doi.org/10.1111/j.2517-6161.1952.tb00104.x">https://doi.org/10.1111/j.2517-6161.1952.tb00104.x</a>.
</div>
<div id="ref-Lemons2013" class="csl-entry" role="listitem">
Lemons, D. S. 2013. <em>A Student’s Guide to Entropy</em>. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511984556">https://doi.org/10.1017/CBO9780511984556</a>.
</div>
<div id="ref-Shannon1948" class="csl-entry" role="listitem">
Shannon, C. E. 1948. <span>“A Mathematical Theory of Communication.”</span> <em>Bell Syst. Tech. J.</em> 27: 379–423. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">https://doi.org/10.1002/j.1538-7305.1948.tb01338.x</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The Boltzmann constant <span class="math inline">\(k_B\)</span> is one of <strong>seven defining constants</strong> in the <a href="https://en.wikipedia.org/wiki/International_System_of_Units"><strong>International System of Units</strong></a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02-intro2.html" class="pagination-link" aria-label="Distributions">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04-entropy2.html" class="pagination-link" aria-label="Risk and divergence">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Risk and divergence</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>