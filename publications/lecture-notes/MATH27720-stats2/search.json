[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 2: Statistical Learning with Likelihood and Bayes",
    "section": "",
    "text": "Welcome\nThese are the lecture notes for MATH27720 Statistics 2, a course for second year mathematics students at the Department of Mathematics of the University of Manchester.\nThe course text was written by Korbinian Strimmer from 2023–2025. This version is from 15 Feb 2025.\nIf you have any questions, comments, or corrections please get in touch! 1",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Statistics 2: Statistical Learning with Likelihood and Bayes",
    "section": "Updates",
    "text": "Updates\nThe notes will be updated from time to time.\nThe most current version is found at the web page for the\n\nonline version of the MATH27720 Statistics 2 lecture notes.\n\nThere you can also download the MATH27720 Statistics 2 lecture notes as\n\nPDF in A4 format for printing (double page layout).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Statistics 2: Statistical Learning with Likelihood and Bayes",
    "section": "License",
    "text": "License\nThese notes are licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Statistics 2: Statistical Learning with Likelihood and Bayes",
    "section": "",
    "text": "Email address: korbinian.strimmer@manchester.ac.uk↩︎",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "00-preface.html",
    "href": "00-preface.html",
    "title": "Preface",
    "section": "",
    "text": "About the author\nHello! My name is Korbinian Strimmer and I am a Professor in Statistics. I am a member of the Statistics group at the Department of Mathematics of the University of Manchester. You can find more information about me on my home page.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#about-the-module",
    "href": "00-preface.html#about-the-module",
    "title": "Preface",
    "section": "About the module",
    "text": "About the module\nThe notes are for the version of MATH27720 Statistics 2 taught in spring 2025 at the University of Manchester.\nThe MATH27720 Statistics 2 module is designed to run over the course of 10 weeks. It has the following two part structure:\n\nEntropy and likelihood (W1–W5)\nBayesian statistics (W6–W10)\n\nThis module focuses on conceptual understanding and methods, not on theory. Specifically, you will learn about the foundations of statistical learning using likelihood and Bayesian approaches and also how these are underpinned by entropy.\nThe presentation in this course is non-technical and most sections and examples will be easily accessible for a year 2 mathematics student. Sections and examples marked \\(\\color{Red} \\blacktriangleright\\) are either conceptually or technically a bit more advanced (e.g. involving more complicated matrix operations). These examples may be omitted on first reading.\nIf you are a University of Manchester student and enrolled in this module you will find additional support material on Blackboard:\n\na weekly learning plan,\nworksheets with examples and solutions (and R code), and\nexam papers of previous years.\n\nFurthermore, a MATH27720 Statistics 2 online reading list is hosted by the University of Manchester library.\n\n\n\n\n\n\nNote 1: Aims of this module\n\n\n\nIn a nutshell, the two key aims of the MATH27720 Statistics 2 module are\n\nto provide a principled introduction to maximum likelihood and Bayesian statistical analysis and\nto demonstrate that statistics offers a well founded and coherent theory of information, rather than just seemingly unrelated collections of “recipes” for data analysis.\n\nThe first part of the module (Weeks 1–5) we will explore the method of maximum likelihood both practically and more theoretically in terms of its foundations.\nThe second part of this module (Weeks 6–10) focuses on the Bayesian approach to statistical estimation and inference that can be viewed as a natural extension of likelihood-based statistical analysis that overcomes some of the limitations of maximum likelihood.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#acknowledgements",
    "href": "00-preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese notes are based in part on my earlier notes for MATH20802 Statistical Methods which was last run in Spring 2023. Many thanks to Beatriz Costa Gomes for her help in creating the 2019 version of the lecture notes when I was teaching the MATH20802 module for the first time and to Kristijonas Raudys for his extensive feedback on the 2020 version.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-prerequisites.html",
    "href": "00-prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Matrices and calculus\nStatistics is a mathematical science that requires practical working knowledge not only in probability but also in vector and matrix algebra as well as in the calculus of functions of several variables.\nThe MATH27720 Statistics 2 module (year 2, semester 2) builds on the earlier mandatory probability and statistics modules offered by the University of Manchester such as Statistics 1 and Probability 1 (year 1) as well as on Probability 2 (year 2, semester 1). Many students will also have attendend the Practical Statistics and the Linear Regression modules (year 2, semester 1).\nBelow you find a number of resources to help you to refresh your knowledge in these areas.\nFor a refresher of the essentials in matrix algebra and calculus please refer to the supplementary Matrix and Calculus Refresher notes.\nBelow you find topics that are particularly relevant.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "00-prerequisites.html#matrices-and-calculus",
    "href": "00-prerequisites.html#matrices-and-calculus",
    "title": "Prerequisites",
    "section": "",
    "text": "Vectors and matrices\n\nVector and matrix notation\nVector algebra\nEigenvectors and eigenvalues for a real symmetric matrix\nPositive and negative definiteness of a real symmetric matrix (containing only positive or only negative eigenvalues)\nMatrix inverse\n\n\n\nFunctions\n\nGradient vector\nHessian matrix\nConditions for a local extremum of a function\nConvex and concave functions\nLinear and quadratic approximation\n\n\n\nLogarithms\nIn these notes the logarithmic function \\(\\log(x)\\), when the base is not without explicitly stated, always refer to the natural logarithm. For logarithms with respect to base 2 and 10 we denote them as \\(\\log_2(x)\\) and \\(\\log_{10}(x)\\), respectively.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "00-prerequisites.html#combinatorics-probability-and-distributions",
    "href": "00-prerequisites.html#combinatorics-probability-and-distributions",
    "title": "Prerequisites",
    "section": "Combinatorics, probability and distributions",
    "text": "Combinatorics, probability and distributions\nFor a detailed introduction of concepts in combinatorics probability see the lecture notes for Probability 1 and Probability 2.\nA review of the essentials can also be found in the supplementary Probability and Distribution Refresher notes. These supplementary notes also provide details of probability distributions frequently employed in statistical analysis.\n2  Distributions for statistical models briefly revisits the distributions relevant for this module.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "00-prerequisites.html#statistics",
    "href": "00-prerequisites.html#statistics",
    "title": "Prerequisites",
    "section": "Statistics",
    "text": "Statistics\nFor a refresher of statistical concepts discussed in earlier statistics courses and required in this module see Appendix A — Statistics refresher.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "01-intro1.html",
    "href": "01-intro1.html",
    "title": "1  Statistical learning",
    "section": "",
    "text": "1.1 How to learn from data?\nThe two fundamental questions when learning from data are\nTo achieve this goal, several theories of information have been emerging. Statistics is the oldest science of information and is concerned with using probabilistic models to offer a principled ways to learn from data and to extract and process information under uncertainty. However, there are various other theories of information, with some emphasising approximation while others concentrate on algorithmic approaches and on non-probabilistic methods. The domain of machine learning shares significant overlap with statistics. Rooted in computer science rather than mathematics, it frequently adopts a more engineering-centric perspective. Artificial intelligence (AI) is a branch of computer science that makes substantial use of statistical and machine learning techniques. The emerging field of data science today comprises both statistics and machine learning and brings together mathematics, computer science and area-specific applications, such as biomedical data science.\nSome important milestones in the development of learning from data are highlighted below:",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical learning</span>"
    ]
  },
  {
    "objectID": "01-intro1.html#how-to-learn-from-data",
    "href": "01-intro1.html#how-to-learn-from-data",
    "title": "1  Statistical learning",
    "section": "",
    "text": "how to extract information from data in an optimal way, and\nhow to make the best possible predictions based on this information.\n\n\n\n\nStarting as early as 1763, the Bayesian school of learning was started which later turned out to be closely linked with the theory of likelihood estimation formulated in 1922.\nLinks of statistical learning with entropy were established in the 1940s with roots going back to discoveries in statistical physics in the 1870s. The close link of physics and statistical learning has recently been underlined by the 2024 Nobel Prize in Physics awarded to J. J. Hopfield and G. Hinton for advances in artificial neural networks.\nIt was also in the 1950s that the first model of artificial neural network arises, essentially a nonlinear input-output map with no underlying probabilistic modelling. This field saw another leap in the 1980s and further progressed from 2010 onwards with the development of deep learning. It is one of the most popular (and most effective) methods for analysing complex data and also underlies many generative AI models. Despite their non-probabilistic origins, modern interpretations of neural networks now view them as high-dimensional nonlinear statistical models.\nIn the 1960s further advanced theories of information were developed under the umbrella of computational learning, most notably the Vapnik-Chernov theory, with the most prominent example of the “support vector machine” (another non-probabilistic model) devised in the 1990s. Other important advances include “ensemble learning” and corresponding algorithmic approaches to classification such as “random forests”.\nClassical statistics has focused on data sets with a low number of variables and a large sample size. With the advent of large-scale genomics and the availability other high-dimensional data in the last 20 years there has been a surge of developments in both statistics and in machine learning to develop new methods to analyse high-dimensional data (large dimension, large number of variables) and big data (large dimension as well as large sample size).",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical learning</span>"
    ]
  },
  {
    "objectID": "01-intro1.html#randomness-versus-uncertainty",
    "href": "01-intro1.html#randomness-versus-uncertainty",
    "title": "1  Statistical learning",
    "section": "1.2 Randomness versus uncertainty",
    "text": "1.2 Randomness versus uncertainty\nWhen exploring statistics (or any other field related to information) it is important to recognise that there is a fundamental difference between probability theory and statistics, and that relates to the distinction between “randomness” and “uncertainty”.\nOn the one hand, probability theory studies randomness, by developing mathematical models for randomness (such as probability distributions), and studying corresponding mathematical properties such as asymptotic behaviour. Probability theory can be viewed as a branch of measure theory, and as such it belongs to the domain of pure mathematics.\nOn the other hand, statistics, and related areas of machine learning and data science, is not at all concerned with randomness. Instead the focus is on learning from data using mathematical models that represent our understanding about the world. Hence, statistics uses probability as a tool to describe uncertainty. Importantly, that uncertainty (e.g. about events, predictions, outcomes, model parameters) is mostly due to our ignorance and lack of knowledge of the true underlying processes but not necessarily because the underlying process is actually random. As soon as new data or information becomes available, the state of knowledge and the uncertainty changes. Hence uncertainty is an epistemological property. The enormous success of statistical methods is indeed due to the fact that they provide optimal procedures for learning from data and the same time allow to model and update this uncertainty.\nIn short, statistics is about describing the state of knowledge of the world, which may be uncertain and incomplete, and to make decisions and predictions in face of uncertainty. This uncertainty can stem from randomness but more frequently arises from our ignorance (and sometimes this ignorance even helps to create a simple yet effective model).",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical learning</span>"
    ]
  },
  {
    "objectID": "01-intro1.html#probabilistic-models-data-and-statistical-learning",
    "href": "01-intro1.html#probabilistic-models-data-and-statistical-learning",
    "title": "1  Statistical learning",
    "section": "1.3 Probabilistic models, data and statistical learning",
    "text": "1.3 Probabilistic models, data and statistical learning\nThe aim of statistical learning is to use observed data in an optimal way to learn about the underlying mechanism of the data-generating process. Since data is typically finite but models can be in principle arbitrarily complex there may be issues of over-fitting (insufficient data for the complexity of the model) but also under-fitting (model is too simplistic).\nWe observe data \\(D = \\{x_1, \\ldots, x_n\\}\\) assumed to result from an underlying true data-generating model \\(F_{\\text{true}}\\), the distribution for \\(x\\).\nTo explain the observed data, and also to predict future data, we will make hypotheses in the form of candidate models \\(P_{1}, P_{2}, \\ldots\\). Often these candidate models form a model family \\(P_{\\boldsymbol \\theta}\\) indexed by a parameter vector \\(\\boldsymbol \\theta\\), with specific values for each model so that we can also write \\(P_{\\boldsymbol \\theta_1}, P_{\\boldsymbol \\theta_2}, \\ldots\\) for the various models.\nFrequently parameters are chosen such that they allow some interpretation, such as moments or other properties of the distribution. However, intrinsically parameters are just labels and may be changed by any one-to-one transformation. For statistical learning it is necessary that models are identifiable within a family, i.e. each distinct model is identified by a unique parameter so that \\(P_{\\boldsymbol \\theta_1} = P_{\\boldsymbol \\theta_2}\\) implies \\(\\boldsymbol \\theta_1 = \\boldsymbol \\theta_2\\), and conversely if \\(P_{\\boldsymbol \\theta_1} \\neq P_{\\boldsymbol \\theta_2}\\) then \\(\\boldsymbol \\theta_1 \\neq \\boldsymbol \\theta_2\\).\nThe true model underlying the data generating process is unknown and cannot be observed. However, what we can observe is data \\(D\\) from the true model \\(F\\) by measuring properties of interest (our observations from experiments). Sometimes we can also perturb the model and see what the effect is (interventional study).\nThe various candidate models \\(P_{\\boldsymbol \\theta}\\) in the model world will at best be good approximations to the true underlying data generating model \\(F\\). In some cases the true model will be part of the model family, i.e. there exists a parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) so that \\(F = P_{\\boldsymbol \\theta_{\\text{true}}}\\). However, more typically we cannot assume that the true underlying model is contained in the family. Nonetheless, even an imperfect candidate model will often provide a useful mathematical approximation and capture some important characteristics of the true model and thus will help to interpret the observed data.\n\\[\n\\begin{array}{cc}\n\\textbf{Real world} \\\\\n\\text{true model (unknown)} \\\\\nF \\\\\n\\end{array}\n\\longrightarrow\n\\begin{array}{cc}\n\\textbf{Data}\\\\\n\\text{Samples from true model} \\\\\nx_1, \\ldots, x_n\\\\\n\\end{array}\n\\] \\[\n\\begin{array}{cc}\n\\textbf{Statistical Learning}\\\\\n\\text{Find model(s) and parameters} \\\\\n\\text{approximating the true model} \\\\\n\\text{and best explaining both} \\\\\n\\text{observed and future data} \\\\\nF \\approx P_{\\hat{\\boldsymbol \\theta}}\\\\\n\\end{array}\n\\longleftarrow\n\\begin{array}{cc}\n\\textbf{Model world} \\\\\n\\text{Hypotheses about}\\\\\n\\text{data generating process:} \\\\\n\\text{Model } P_{\\boldsymbol \\theta}\\\\\n\\text{with parameter(s) } \\boldsymbol \\theta\\\\\n\\text{Ideally, true model is } \\\\\nF = P_{\\boldsymbol \\theta_{\\text{true}}}\\\\\n\\end{array}\n\\]\nThe aim of statistical learning is to identify the model(s) that explain the current data and also predict future data (i.e. predict outcome of experiments that have not been conducted yet).\nThus a good model provides a good fit to the current data (i.e. it explains current observations well) and also to the future data (i.e. it generalises well).\nA large proportion of statistical theory is devoted to finding these “good” models that avoid both over-fitting (models being too complex and not generalising well) or under-fitting (models being too simplistic and hence also not predicting well).\nTypically the aim is to find an approximating model whose model complexity is well matched with the complexity of the unknown true model and also with the complexity of the observed data.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical learning</span>"
    ]
  },
  {
    "objectID": "01-intro1.html#finding-the-best-models",
    "href": "01-intro1.html#finding-the-best-models",
    "title": "1  Statistical learning",
    "section": "1.4 Finding the best models",
    "text": "1.4 Finding the best models\nA core task in statistical learning is to identify those distributions that explain the existing data well and that also generalise well to future yet unseen observations.\nIn a non-parametric setting we may simply rely on the law of large numbers that implies that the empirical distribution \\(\\hat{F}_n\\) constructed from the observed data \\(D\\) converges to the true distribution \\(F\\) if the sample size is large. We can therefore obtain an empirical estimator \\(\\hat{\\theta}\\) of the functional \\(\\theta = g(F)\\) by \\(\\hat{\\theta}= g( \\hat{F}_n )\\), i.e. by substituting the true distribution with the empirical distribution. This allows us, e.g., to get the empirical estimate of the mean \\(\\text{E}_{F}(x) = \\mu\\) by \\[\n\\hat{\\text{E}}(x) = \\hat{\\mu} =  \\text{E}_{\\hat{F}_n}(x) = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{x}\n\\] and of the variance \\(\\text{Var}(x) = \\sigma^2 = \\text{E}_{F}((x - \\mu)^2)\\) by \\[\n\\widehat{\\text{Var}}(x) = \\widehat{\\sigma^2} =\n\\text{E}_{\\hat{F}_n}((x - \\hat{\\mu})^2) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\] simply by replacing the expectation with the sample average.\nFor parametric models we need to find estimates of the parameters that correspond to the distributions that best approximate the unknown true data generating model. One such approach is provided by the method of maximum likelihood. More precisely, given a probability distribution \\(P_{\\boldsymbol \\theta}\\) with density or mass function \\(p(x|\\boldsymbol \\theta)\\) where \\(\\boldsymbol \\theta\\) is a parameter vector, and \\(D = \\{x_1,\\dots,x_n\\}\\) are the observed iid data (i.e. independent and identically distributed), the likelihood function is defined as \\[\nL_n(\\boldsymbol \\theta| D ) =\\prod_{i=1}^{n} p(x_i|\\boldsymbol \\theta)\n\\] The parameter value \\(\\hat{\\boldsymbol \\theta}_{ML}\\) that maximises the likelihood function for fixed data \\(D\\) is the maximum likelihood estimate: \\[\n\\hat{\\boldsymbol \\theta}_{ML} = \\underset{\\boldsymbol \\theta}{\\arg \\max}\\, L_n(\\boldsymbol \\theta|D)\n\\]\nHistorically, the likelihood was introduced as the probability to observe the data given the model with specified parameters \\(\\boldsymbol \\theta\\). However, this view is incorrect as this interpretation of the likelihood breaks down for continuous random variables which use densities rather than probabilities in the likelihood. Furthermore even for discrete random variables an additional factor accounting for the possible permutations of samples is needed to obtain the actual probability of the data. Instead, as will soon become evident, the basis of the method of maximum likelihood is fundamentally linked to entropy.\nSpecifically, we will see that the likelihood is closely linked to the cross-entropy between the unknown true distribution \\(F\\) and the model \\(P_{\\boldsymbol \\theta}\\). As a consequence the method of maximum likelihood extends empirical estimation to parametric models1. This insight illuminates both the optimality characteristics as well as the limitations of the maximum likelihood approach to statistical learning.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical learning</span>"
    ]
  },
  {
    "objectID": "01-intro1.html#further-reading",
    "href": "01-intro1.html#further-reading",
    "title": "1  Statistical learning",
    "section": "1.5 Further reading",
    "text": "1.5 Further reading\nThe popular science book “The Theory That Would Not Die” McGrayne (2011) focuses on the history of Bayes’ theorem and its importance in statistics. In a similar fashion, “The Master Algorithm” by Domingos (2015) provides an informal overview over the various schools of information science.\nThe book “Ten Great Ideas About Chance” by Diaconis and Skyrms (2018) offers a gentle introduction to various viewpoints of probability, discussing randomness and uncertainty including the frequentist ontological perspective versus the epistemological Bayesian perspective.\nFor a quick recap of essential statistical concepts introduced in earlier statistical modules in year 1 and 2 see Appendix A.\n\n\n\n\nDiaconis, P., and B. Skyrms. 2018. Ten Great Ideas about Chance. Princeton University Press.\n\n\nDomingos, P. 2015. The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books.\n\n\nMcGrayne, S. B. 2011. The Theory That Would Not Die. Yale University Press.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical learning</span>"
    ]
  },
  {
    "objectID": "01-intro1.html#footnotes",
    "href": "01-intro1.html#footnotes",
    "title": "1  Statistical learning",
    "section": "",
    "text": "Conversely, empirical estimators are, in fact, also likelihood estimators based on an empirical likelihood function constructed from the empirical distribution.↩︎",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical learning</span>"
    ]
  },
  {
    "objectID": "02-intro2.html",
    "href": "02-intro2.html",
    "title": "2  Distributions for statistical models",
    "section": "",
    "text": "2.1 Common chararacterics of distributions\nChoosing the appropriate distributions for statistical modeling is a crucial aspect of probabilistic data analysis. This chapter explores various factors to consider when selecting suitable distributions and also reviews the key distributions covered in this module.\nDistributions can be differentiated by a number of characteristics.\nFirstly, by the type of random variable:\nSecondly, by the support of the random variable, with typical ranges such as:\nThe choice of support will depend on the intended use of the random variable in the model. Common applications include\nThese interpretations apply not only to the random variable itself but also to the parameter of a distribution family. For instance, we might select a distribution that allows the samples to be interpreted as proportions (such as the Beta distribution). Alternatively, we may wish to choose a distribution family in which a parameter represents a proportion (such as the Bernoulli distribution).\nA third consideration may be the general shape of the distribution:\nA further characteristic of a distribution family is the number of parameters, with choices such as\nA distribution family is comprised of a finite or infinite number of distributions corresponding to particular instances of the parameter values.\nIn data analysis our goal is to employ and develop models that are sufficiently complex to capture the essential features of the observations while also preventing overfitting of the data. As a result, models with fewer parameters are generally preferred over models with larger number of parameters, especially if both have similar explanatory power, i.e. similar capacity to both explain the observed data and to predict future observations.\nLastly, it is important to take into account the general structure of the distribution:\nModels with simpler structure can be preferable when the sample size is small and there are fewer observations, and conversely nonparametric approaches with fewer assumptions about the data generating process may be more appropriate when there is an abundance of data.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distributions for statistical models</span>"
    ]
  },
  {
    "objectID": "02-intro2.html#common-chararacterics-of-distributions",
    "href": "02-intro2.html#common-chararacterics-of-distributions",
    "title": "2  Distributions for statistical models",
    "section": "",
    "text": "discrete versus continuous\nunivariate versus multivariate\n\n\n\nfinite discrete support, e.g. \\(\\{1, 2, \\ldots, n\\}\\)\ninfinite discrete support, e.g. \\(\\{1, 2, \\ldots\\}\\)\n\\([0,1]\\)\n\\([-\\infty, \\infty]\\)\n\\([0, \\infty]\\)\n\n\n\nproportion\nlocation\nscale\nmean\nvariance\nspread\nconcentration\nshape\nrate\n(squared) correlation\n\n\n\n\nsymmetric or asymmetric\nleft or right skewed\nshort tails or long tails\nunimodal or multimodal\n\n\n\nsingle parameter\nmultiple parameters\nmultiple types of parameters (e.g. location+scale)\n\n\n\n\n\nparametric versus nonparametric models\nexponential family versus non-exponential family\nspecial exponential families, e.g. Gibbs family, natural exponential family (NEF)",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distributions for statistical models</span>"
    ]
  },
  {
    "objectID": "02-intro2.html#commonly-used-basic-distributions",
    "href": "02-intro2.html#commonly-used-basic-distributions",
    "title": "2  Distributions for statistical models",
    "section": "2.2 Commonly used basic distributions",
    "text": "2.2 Commonly used basic distributions\nIn this module we will often make use of the following common univariate distributions:\n\nBinomial distribution \\(\\text{Bin}(n, \\theta)\\), with support \\(\\{0, 1, \\ldots, n\\}\\).\nAs special case (\\(n=1)\\) is:\n\nBernoulli distribution \\(\\text{Ber}(\\theta)\\), with support \\(\\{0, 1\\}\\).\n\nBeta distribution \\(\\text{Beta}(\\alpha, \\beta)\\), with support \\([0, 1]\\).\nNormal distribution \\(N(\\mu, \\sigma^2)\\), with support \\([-\\infty, \\infty]\\).\nGamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\), with support \\([0, \\infty]\\). It is also known as univariate Wishart distribution \\(\\text{Wis}\\left(s^2, k \\right)\\).\nSpecial cases of the gamma/Wishart distribution are:\n\nscaled chi-squared distribution \\(s^2 \\text{$\\chi^2_{k}$}\\) (discrete \\(k\\))\nchi-squared distribution \\(\\text{$\\chi^2_{k}$}\\) (discrete \\(k\\), \\(s^2=1\\))\nexponential distribution \\(\\text{Exp}(\\theta)\\) (\\(\\alpha=1\\))\n\nInverse gamma distribution \\(\\text{IG}(\\alpha, \\beta)\\), with support \\([0, \\infty]\\). Also know as univariate inverse Wishart distribution \\(\\text{IW}(\\psi, k)\\).\n\nAll the above distributions are so-called exponential families. As such they can be written in a particular structural form. Exponential families have many useful properties that facilitate statistical analysis.\n\nLocation-scale \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mu, \\tau^2)\\), with support \\([-\\infty, \\infty]\\).\nSpecial cases of the location-scale \\(t\\)-distribution are:\n\nStudent’s \\(t\\)-distribution \\(t_\\nu\\)\nCauchy distribution \\(\\text{Cau}(\\mu, \\tau)\\)\n\nThe location-scale \\(t\\)-distribution is generalisation of the normal distribution but with more probability mass in the tails. Depending on the choice of the degrees of freedom \\(\\nu\\), not all moments of the distribution may exist. Furthermore, it’s not an exponential family.\n\nFor all of the above univariate distribution there exist corresponding multivariate variants. In this module we will make use of the following multivariate distributions:\n\nMultinomial distribution \\(\\text{Mult}(n, \\boldsymbol \\pi)\\), generalising the binomial distribution.\nSpecial case (\\(n=1)\\):\n\nCategorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\), generalising the Bernoulli distribution.\n\nMultivariate normal distribution \\(N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\), generalising the univariate normal distribution.\n\nA distribution family can be parameterised in multiple equivalent ways. Typically, there is a standard parameterisation, and also a mean parameterisation, where one of the parameters can be interpreted as the mean. Sometimes, the same distribution is referred to by different names and there are various default parameterisations.\nImportantly, any parameterisation is a matter of choice and simply provides as an alternative means to index the elementary distributions within the family. However, certain parameterisations may be more interpretable or offer computational advantages.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distributions for statistical models</span>"
    ]
  },
  {
    "objectID": "02-intro2.html#choosing-the-right-distribution",
    "href": "02-intro2.html#choosing-the-right-distribution",
    "title": "2  Distributions for statistical models",
    "section": "2.3 Choosing the right distribution",
    "text": "2.3 Choosing the right distribution\nWhen choosing a distribution for a data model we typically aim to to align the characteristics of the distribution with the observations. For instance, if the data exhibit long tails, we will need to use a long-tailed model. Additionally, there may be a mechanistic rationale, such as derived from physics, that the underlying process adheres to a particular distribution\nIn many cases, the central limit theorem justifies using a normal distribution.\nAnother approach to selecting a distribution family involves constraining specific properties of the distribution, such as its mean and variance, and then selecting a model family that maximises the spread of the probability mass. This method is closely related to the maximum entropy principle, which will be discussed in more detail later, and is also in fact one of the reasons why exponential families are favoured in statistical modelling.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distributions for statistical models</span>"
    ]
  },
  {
    "objectID": "02-intro2.html#building-complex-statistical-models",
    "href": "02-intro2.html#building-complex-statistical-models",
    "title": "2  Distributions for statistical models",
    "section": "2.4 Building complex statistical models",
    "text": "2.4 Building complex statistical models\nStatistical analysis often utilises models that consist of numerous random variables. In practice, these can be quite intricate, featuring hierarchical or network-like structures that connect observed and latent variables, and may also display nonlinear functional relationships. Despite their complexity, even the most sophisticated statistical models are constructed from more fundamental components.\nSpecifically, the large class of graphical models provide a principled means to form complex joint distributions for observed and unobserved random variables built from more elementary components. This include regression models, mixture models and compound models (continuous version of mixture models) as well as more general network-like and hierarchically structured models.\nIn these complex models some of the underlying elementary distributions will serve to model the observed output while others represent internal variables or account for the uncertainty regarding a parameter (in a Bayesian context).\nIn statistical course units in year 3 and year 4 you will discuss and learn about many types of advanced models, related for instance to\n\nmultivariate statistics and machine learning\ntemporal and spatial modelling, and\ngeneralised linear and nonparametric models.\n\nMuch of statistics is concerned with methods to quantify how well a model fits to data and how well it predicts future observations, and this allows to build successive models and compare them in a systematic and principled fashion.\nFinally, it is worth recalling that all distributions (and models in general) are best considered as approximations of the true unknown data generating process. Hence, the focus of any data analysis will be to find the model that captures the essential properties at an appropriate level of detail1.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distributions for statistical models</span>"
    ]
  },
  {
    "objectID": "02-intro2.html#further-reading",
    "href": "02-intro2.html#further-reading",
    "title": "2  Distributions for statistical models",
    "section": "2.5 Further reading",
    "text": "2.5 Further reading\nFor details about the above-mentioned distributions, and their parameterisations, see the supplementary Probability and Distribution Refresher notes.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distributions for statistical models</span>"
    ]
  },
  {
    "objectID": "02-intro2.html#footnotes",
    "href": "02-intro2.html#footnotes",
    "title": "2  Distributions for statistical models",
    "section": "",
    "text": "The fact that it’s possible to model the world at one length scale independently from what’s happening at other length scales is a general phenomenon in nature known in physics as decoupling of scales.↩︎",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distributions for statistical models</span>"
    ]
  },
  {
    "objectID": "03-entropy1.html",
    "href": "03-entropy1.html",
    "title": "3  Entropy",
    "section": "",
    "text": "3.1 Information storage and scoring rules\nEntropy, a fundamental concept that originated in physics, plays a crucial role in information theory and statistical learning. This chapter introduces entropy via the route of scoring rules.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Entropy</span>"
    ]
  },
  {
    "objectID": "03-entropy1.html#information-storage-and-scoring-rules",
    "href": "03-entropy1.html#information-storage-and-scoring-rules",
    "title": "3  Entropy",
    "section": "",
    "text": "Information storage with constant code length\nWe study a discrete variable \\(x\\) that can assume \\(K\\) possible states \\(\\Omega = \\{\\omega_1, \\ldots, \\omega_K\\}\\). Our aim is to record the current state of the variable \\(x\\) using one or more information storage units, each of which can register one of \\(A\\) different states. \\(A\\) is called alphabet size of the unit, and can be large or smaller than \\(K\\).\nFollowing the principle of common numeral systems, such as the decimal, binary or hexadecimal numbers, it is easy to verify that in order to describe the state of \\(x\\) we require \\[\nS = \\log_A K\n\\] storage units. \\(S\\) is called the code length or the cost to describe the state of \\(x\\).\nIn the above we have tacitly assumed that all \\(K\\) states are equal and that storage size, code length, and cost requirement associated with each state \\(\\omega_i \\in \\Omega\\) is constant and the same for all possible \\(K\\) states. This can be made more explicit by writing \\[\nS = -\\log_A \\left( \\frac{1}{K} \\right)\n\\] where \\(1/K\\) is the equal probability of each of the \\(K\\) states.\n\nExample 3.1 Information storage units:\nFor and alphabet of size \\(A=2\\) (e.g. 0, 1) the storage units are called “bits” (binary digits). Hence to describe \\(K=256\\) possible states \\(\\log_2 256 = 8\\) bits (or 1 byte) of storage are sufficient.\nFor \\(A=10\\) the units are “dits” (decimal digits), so to describe \\(K=100\\) possible states \\(\\log_{10} 100 = 2\\) dits are sufficient, where a single dit has an alphabet of size \\(A=10\\) (e.g. arabic numerals).\nFinally, if the natural logarithm is used (\\(A=e\\)) the storage units are called “nits” (natural digits). In the following we will use “nits” and the natural logarithm throughout.\n\n\n\nVariable code length and scoring rules\nIn practise, the \\(K\\) states are often not equally probable. For example \\(x\\) might be a random random variable describing the letters in a text message. Hence, rather than assuming equal probabilities we use a discrete distribution \\(P\\) with probability mass function \\(p(x)\\) to model the state probabilities.\nIn this case, instead of using the same code length to describe each state, we use variable code lengths, with more probable states having shorter codes and less probable states assigned longer codes. Specifically, generalising from the previous we may wish to employ the negative logarithm to map the probability of a state \\(x\\) to a corresponding cost and code length: \\[\nS(x, P) = -\\log p(x)\n\\] This is called the logarithmic score or logarithmic scoring rule1.\nAs we will see below (Example 3.8, Example 3.9 and Example 3.10) using variable code lengths allows for expected code lengths that are potentially much smaller than using a fixed length \\(\\log K\\) for all states, and hence leads to a more space saving representation.\nWe will apply the logarithmic score \\(S(x, P) = -\\log p(x)\\) to both discrete and continuous variables \\(x\\) and corresponding distributions \\(P\\). As densities can take on values larger than 1 the logarithmic score may become negative when \\(P\\) is a continuous distribution.\nWhile there are other possibilities for suitable scoring rules we will exclusively rely on the logarithmic scoring rule as it the default scoring rule underlying classical likelihood and Bayes inference methods due to its unique characteristics (see Note 3.1).\n\n\n\n\n\n\nNote 3.1: \\(\\color{Red} \\blacktriangleright\\) General scoring rules\n\n\n\nThe function \\(S(x, P) = -\\log p(x)\\) is an example of a scoring rule for a probabilistic forecast represented by model \\(P\\) evaluated on the observation \\(x\\).\nThe logarithmic scoring rule is uniquely characterised by a number of favourable properties (e.g. Hartley 1928, Shannon 1948, Good 1952, Bernardo 1979). In particular, it is the only scoring rule that is both\n\nproper, i.e. the expected score is minimised when the quoted model \\(P\\) is identical to the data generating model, and\nlocal in that the score depends only on the value of the probability density mass function at \\(x\\).\n\nSee also Example 4.2.\n\n\n\nExample 3.2 Surprise:\nThe negative logarithm of a probability \\(-\\log p\\) is called the surprise or surprisal. The surprise to observe a certain outcome (with probability \\(p=1\\)) is zero, and conversely the surprise to observe an outcome that cannot happen (with probability \\(p=0\\)) is infinite.\n\n\nExample 3.3 Log-odds ratio:\nThe odds of an event with probability \\(p\\) are given by the ratio \\(\\frac{p}{1-p}\\).\nThe log-odds ratio is therefore the difference of the surprise of the complementary event (with probability \\(1-p\\)) and the surprise of the event (with probability \\(p\\)): \\[\n\\begin{split}\n\\text{logit}(p) &= \\log\\left( \\frac{p}{1-p} \\right) \\\\\n&= -\\log(1-p) - ( -\\log p)\\\\\n\\end{split}\n\\]\n\n\nExample 3.4 Logarithmic score and normal distribution:\nIf we quote in the logarithmic scoring rule the normal distribution \\(P = N(\\mu, \\sigma^2)\\) with density \\(p(x |\\mu, \\sigma^2)= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\) we get as score \\[\nS\\left(x,N(\\mu, \\sigma^2 )\\right) = \\frac{1}{2} \\left( \\log(2\\pi\\sigma^2) + \\frac{(x-\\mu)^2}{\\sigma^2}\\right)\n\\] For fixed variance \\(\\sigma^2\\) the logarithmic score is thus equivalent to the squared distance between \\(x\\) and \\(\\mu\\).",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Entropy</span>"
    ]
  },
  {
    "objectID": "03-entropy1.html#expected-score-and-entropy",
    "href": "03-entropy1.html#expected-score-and-entropy",
    "title": "3  Entropy",
    "section": "3.2 Expected score and entropy",
    "text": "3.2 Expected score and entropy\n\nEntropy of a distribution\nGiven the scoring rule \\(S(x, P)\\) we can compute its expectation with regard to \\(P\\): \\[\n\\begin{split}\nH(P) &= \\text{E}_P\\left( S(x, P) \\right) \\\\\n     &= - \\text{E}_P\\left(\\log p(x)\\right) \\\\\n\\end{split}\n\\] This expected score is called the entropy of the distribution \\(P\\).\nIn the above the distribution \\(P\\) occurs in two distinct roles. First, it is the model generating the data (note the expectation \\(\\text{E}_P\\) with regard to \\(P\\)). Second, it is also the model that is evaluated on the observations (via \\(-\\log p(x)\\)). Thus, entropy is a functional of the distributions \\(P\\).\n\n\nShannon-Gibbs entropy\nThe entropy of a discrete probability distribution \\(P\\) with probability mass function \\(p(x)\\) with \\(x \\in \\Omega\\) is called Shannon entropy (1948)2. In statistical physics, the Shannon entropy is known as the Gibbs entropy (1878):\n\\[\nH(P) = - \\sum_{x \\in \\Omega} \\log p(x) \\, p(x)\n\\] The entropy of a discrete distribution is the expected surprise. We can also interpret it as the expected cost or expected code length when the data are generated according to model \\(P\\) (“sender”, “encoder” ) and we are using the same model \\(P\\) to describe the data (“receiver”, “decoder”).\nFurthermore, Shannon-Gibbs entropy also has a combinatorial interpretation (see Example 3.12).\nAs \\(p(x) \\in [0,1]\\) and hence \\(-\\log p(x) \\geq 0\\) by construction Shannon-Gibbs entropy is bounded below and must be larger or equal to 0.\n\n\nDifferential entropy\nApplying the definition of entropy to a continuous probability distribution \\(P\\) with density \\(p(x)\\) yields the differential entropy: \\[\nH(P) = - \\int_x \\log p(x) \\, p(x) \\, dx\n\\] Differential entropy can be negative because the logarithm is applied to a density, which, unlike a probability, can take on values greater than one.\nMoreover, for continuous random variables, the shape of the density typically changes under variable transformation, such as from \\(x\\) to \\(y\\), the differential entropy will change as well and is not invariant under such a transformation so that \\(H(P_y) \\neq H(P_x)\\).\n\n\n\n\n\n\nNote 3.2: \\(\\color{Red} \\blacktriangleright\\) Interpretation of entropy — spread versus disorder\n\n\n\nTraditionally, entropy has often been considered as a measure of order, with large entropy corresponding to disorder and low entropy to order. However, this classic interpretation is now viewed as outdated as there are numerous systems that appear ordered but have large entropy, and vice versa.\nA better intuition about entropy is the notion of spread or dispersal which is now preferred in Physics3 and this also aligns best with the interpretation of entropy in Statistics and Machine Learning.\nAs will become clear from the examples, entropy quantifies the spread of the probability mass within a distribution. When the probability mass is concentrated within a specific area, the entropy is low; conversely, when the probability mass is more broadly distributed, the entropy is high.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Entropy</span>"
    ]
  },
  {
    "objectID": "03-entropy1.html#entropy-examples",
    "href": "03-entropy1.html#entropy-examples",
    "title": "3  Entropy",
    "section": "3.3 Entropy examples",
    "text": "3.3 Entropy examples\n\nModels with single parameter\n\nExample 3.5 The Shannon-Gibbs entropy of the geometric distribution \\(F_x = \\text{Geom}(\\theta)\\) with probability mass function \\(p(x|\\theta) = \\theta (1-\\theta)^{x-1}\\), \\(\\theta \\in [0,1]\\), support \\(x \\in \\{1, 2, \\ldots \\}\\) and \\(\\text{E}(x)= 1/\\theta\\) is \\[\n\\begin{split}\nH(F_x) &= - \\text{E}\\left( \\log \\theta + (x-1) \\log(1-\\theta)   \\right)\\\\\n       &= -\\log \\theta+ \\left(\\frac{1}{\\theta}-1\\right)\\log(1-\\theta)\\\\\n       &= -\\frac{\\theta \\log \\theta + (1-\\theta) \\log(1-\\theta) }{\\theta}\n\\end{split}\n\\] Using the identity \\(0\\times\\log(0)=0\\) we see that the entropy of the geometric distribution for \\(\\theta = 1\\) equals 0, i.e. it achieves the minimum possible Shannon-Gibbs entropy. Conversely, as \\(\\theta \\rightarrow 0\\) it diverges to infinity.\n\n\nExample 3.6 Consider the uniform distribution \\(F_x = U(0, a)\\) with \\(a&gt;0\\), support from \\(0\\) to \\(a\\) and density \\(p(x) = 1/a\\). The corresponding differential entropy is \\[\n\\begin{split}\nH( F_x ) &= - \\int_0^a \\log\\left(\\frac{1}{a}\\right) \\, \\frac{1}{a}  dx \\\\\n             &=  \\log a  \\int_0^a \\frac{1}{a} dx \\\\\n             &= \\log a \\,.\n\\end{split}\n\\] Note that for \\(0 &lt; a &lt; 1\\) the differential entropy is negative.\n\n\nExample 3.7 Starting with the uniform distribution \\(F_x = U(0, a)\\) from Example 3.6 the variable \\(x\\) is changed to \\(y = x^2\\) yielding the distribution \\(F_y\\) with support from \\(0\\) to \\(a^2\\) and density \\(p(y) = 1/\\left(2 a \\sqrt{y}\\right)\\).\nThe corresponding differential entropy is \\[\n\\begin{split}\nH( F_y ) &=  \\int_0^{a^2}  \\log \\left(2 a \\sqrt{y}\\right) \\, 1/\\left(2 a \\sqrt{y}\\right) dy \\\\\n         &= \\left[ \\sqrt{y}/a \\, \\left(\\log \\left( 2 a \\sqrt{y} \\right)-1\\right)  \\right]_{y=0}^{y=a^2} \\\\\n             &= \\log \\left(2 a^2\\right) -1 \\,.\n\\end{split}\n\\] This is negative for \\(0 &lt; a &lt; \\sqrt{e/2}\\approx 1.1658\\). As expected \\(H( F_y ) \\neq H( F_x )\\) as differential entropy is not invariant against variable transformations.\n\n\n\nModels with multiple parameters\n\nExample 3.8 Entropy of the categorical distribution \\(P\\) with \\(K\\) categories.\nAssuming class probabilities \\(p_1, \\ldots, p_K\\) the Shannon-Gibbs entropy is \\[\nH(P) = - \\sum_{k=1}^{K } \\log(p_k)\\, p_k\n\\]\nAs \\(P\\) is discrete \\(H(P)\\) is bounded below by 0. Furthermore, it is also bounded above by \\(\\log K\\) (cf. Example 6.1). Hence for a categorical distribution \\(P\\) with \\(K\\) categories we have \\[\n0 \\leq  H(P) \\leq \\log K\n\\] The maximum is achieved for the discrete uniform distribution (Example 3.9) and the minimum for a concentrated categorical distribution (Example 3.10).\n\n\nExample 3.9 Entropy of the discrete uniform distribution \\(U_K\\):\nLet \\(p_1=p_2= \\ldots = p_K = \\frac{1}{K}\\). Then \\[H(U_K) = - \\sum_{k=1}^{K}\\log\\left(\\frac{1}{K}\\right)\\, \\frac{1}{K}  = \\log K\\]\nNote that \\(\\log K\\) is the largest value the Shannon-Gibbs entropy can assume with \\(K\\) classes (cf. Example 6.1) and indicates maximum spread of probability mass.\n\n\nExample 3.10 Entropy of a categorical distribution with concentrated probability mass:\nLet \\(p_1=1\\) and \\(p_2=p_3=\\ldots=p_K=0\\). Using \\(0\\times\\log(0)=0\\) we obtain for the Shannon-Gibbs entropy \\[H(P) = \\log(1)\\times 1 + \\log(0)\\times 0 + \\dots = 0\\]\nNote that 0 is the smallest value that Shannon-Gibbs entropy can assume and that it corresponds to maximum concentration of probability mass.\n\n\nExample 3.11 Differential entropy of the normal distribution:\nThe log density of the univariate normal \\(N(\\mu, \\sigma^2)\\) distribution is \\(\\log p(x |\\mu, \\sigma^2) = -\\frac{1}{2} \\left(  \\log(2\\pi\\sigma^2)  + \\frac{(x-\\mu)^2}{\\sigma^2} \\right)\\) with \\(\\sigma^2 &gt; 0\\). The corresponding differential entropy is with \\(\\text{E}((x-\\mu)^2) = \\sigma^2\\) \\[\n\\begin{split}\nH(P) & = -\\text{E}\\left( \\log p(x |\\mu, \\sigma^2) \\right)\\\\\n& = \\frac{1}{2} \\left( \\log(2 \\pi \\sigma^2)+1\\right) \\,. \\\\\n\\end{split}\n\\] Note that \\(H(P)\\) only depends on the variance parameter and not on the mean parameter. This intuitively clear as only the variance controls the concentration of the probability mass. The entropy grows with the variance as the probability mass becomes more spread out and less concentrated around the mean. For \\(\\sigma^2 &lt; 1/(2 \\pi e) \\approx 0.0585\\) the differential entropy is negative.\n\n\nExample 3.12 \\(\\color{Red} \\blacktriangleright\\) Entropy of a categorical distribution and the multinomial coefficient:\nLet \\(\\hat{Q}\\) be the empirical categorical distribution with \\(\\hat{q}_k = n_k/n\\) the observed frequencies with \\(n_k\\) counts in class \\(k\\) and \\(n=\\sum_{k=1}^K\\) total counts.\nThe number of possible permutation of \\(n\\) items of \\(K\\) distinct types is given by the multinomial coefficient \\[\nW = \\binom{n}{n_1, \\ldots, n_K} = \\frac {n!}{n_1! \\times n_2! \\times\\ldots \\times n_K! }\n\\]\nIt turns out that for large \\(n\\) both quantities are directly linked: \\[\nH(\\hat{Q})  \\approx \\frac{1}{n} \\log W\n\\]\nRecall the Moivre-Sterling formula which for large \\(n\\) allow to approximate the factorial by \\[\n\\log n! \\approx  n \\log n  -n\n\\] With this \\[\n\\begin{split}\n\\log W &= \\log n! - \\sum_{k=1}^K \\log n_k!\\\\\n& \\approx    n \\log n  -n - \\sum_{k=1}^K (n_k \\log n_k  -n_k) \\\\\n& = \\sum_{k=1}^K n_k \\log n - \\sum_{k=1}^K n_k \\log n_k\\\\\n& = - n \\sum_{k=1}^K \\frac{n_k}{n} \\log\\left( \\frac{n_k}{n} \\right)\\\\\n& = -n \\sum_{k=1}^K \\log (\\hat{q}_k) \\, \\hat{q}_k  \\\\\n& = n H(\\hat{Q})\n\\end{split}\n\\]\nThe above combinatorial derivation of entropy is one of the cornerstones of statistical mechanics and is credited to Boltzmann (1877) and Gibbs (1878). The number of elements \\(n_1, \\ldots, n_K\\) in each of the \\(K\\) classes corresponds to the macrostate and any of the \\(W\\) different allocations of the \\(n\\) elements to the \\(K\\) classes to an underlying microstate. The multinomial coefficient, and hence entropy, is largest when there are only small differences (or none) among the \\(n_i\\), i.e. when the individual elements are equally spread across the \\(K\\) bins.\nIn statistics the above derivation of entropy was rediscovered by Wallis (1962).\n\n\n\nA bit of history\nThe concept of entropy was first introduced in 1865 by Rudolph Clausius (1822-1888) in the context of thermodynamics. In physics entropy measures the distribution of energy: if energy is concentrated then the entropy is low, and conversely if energy is spread out the entropy is large. The total energy is conserved (first law of thermodynamics) but with time it will diffuse and thus entropy will increase with time (second law of thermodynamics).\nThe modern probabilistic definition of entropy was discovered in the 1870s by Ludwig Boltzmann (1844–1906) and Josiah W. Gibbs (1839–1903). In statistical mechanics entropy is proportional to the logarithm of the number of microstates (i.e. particular configurations of the system) compatible with the observed macrostate. Typically, in systems where the energy is spread out there are very large numbers of compatible configurations hence this corresponds to large entropy, and conversely, if the energy is concentrated there are only few such configurations, and thus is corresponds to low entropy.\nIn the 1940–1950’s the notion of entropy turned out to be central also in information theory, a field pioneered by mathematicians such as Ralph Hartley (1888–1970), Solomon Kullback (1907–1994), Alan Turing (1912–1954), Richard Leibler (1914–2003), Irving J. Good (1916–2009), Claude Shannon (1916–2001), and Edwin T. Jaynes (1922–1998), and later further explored by Shun’ichi Amari (1936–), Imre Ciszár (1938–), Bradley Efron (1938–), Philip Dawid (1946–) and many others.\nOf the above, Turing and Good were affiliated with the University of Manchester.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Entropy</span>"
    ]
  },
  {
    "objectID": "03-entropy1.html#footnotes",
    "href": "03-entropy1.html#footnotes",
    "title": "3  Entropy",
    "section": "",
    "text": "We follow the convention that scoring rules are negatively oriented (e.g. Dawid 2007) with the aim to minimise the score (cost, code length, surprise). However, some authors prefer the positively oriented convention with a reversed sign in the definition of \\(S(x, P)\\) so the score represents a reward that is maximised (e.g. Gneiting and Raftery 2007).↩︎\nShannon, C. E. 1948. A mathematical theory of communication. Bell System Technical Journal 27:379–423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x↩︎\nFor example, see: H. S. Leff. 2007. Entropy, its language, and interpretation. Found. Phys. 37: 1744–1766, D. S. Lemons. 2013. A student’s guide to entropy. CUP, and https://en.wikipedia.org/wiki/Entropy_(energy_dispersal)↩︎",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Entropy</span>"
    ]
  },
  {
    "objectID": "04-entropy2.html",
    "href": "04-entropy2.html",
    "title": "4  Relative entropy",
    "section": "",
    "text": "4.1 Cross-entropy",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Relative entropy</span>"
    ]
  },
  {
    "objectID": "04-entropy2.html#cross-entropy",
    "href": "04-entropy2.html#cross-entropy",
    "title": "4  Relative entropy",
    "section": "",
    "text": "Definition of cross-entropy\nGiven the scoring rule \\(S(x, P)\\) we may also wish to compute its expectation with regard to a different distribution \\(Q\\): \\[\n\\begin{split}\nH(Q, P) & =\\text{E}_Q\\left( S(x, P) \\right)\\\\\n& = -\\text{E}_Q\\left( \\log p(x)  \\right)\\\\\n\\end{split}\n\\] This is called the cross-entropy1.\nIn the above the distribution \\(Q\\) is the model generating the data (note the expectation \\(\\text{E}_Q\\) with regard to \\(Q\\)) and the distribution \\(P\\) is the the model that is evaluated on the observations (via \\(-\\log p(x)\\)). Thus, cross-entropy is a functional of two distributions \\(Q\\) and \\(P\\).\nFor two discrete distributions \\(Q\\) and \\(P\\) with probability mass functions \\(q(x)\\) and \\(p(x)\\) with \\(x\\in \\Omega\\) the cross-entropy is computed as the weighted sum \\[\nH(Q, P) = - \\sum_{x \\in \\Omega}  \\log p(x) \\, q(x)\n\\] It can be interpreted as the expected cost or expected code length when the data are generated according to model \\(Q\\) (“sender”, “encoder”) and but we use model \\(P\\) to describe the data (“receiver”, “decoder”).\nFor two continuous distributions \\(Q\\) and \\(P\\) with densities \\(q(x)\\) and \\(p(x)\\) we compute the integral \\[H(Q, P) =- \\int_x  \\log p(x)\\, q(x) \\, dx\\]\n\nExample 4.1 Cross-entropy between two normals:\nAssume \\(F_{\\text{ref}}=N(\\mu_{\\text{ref}},\\sigma^2_{\\text{ref}})\\) and \\(F=N(\\mu,\\sigma^2)\\). The cross-entropy \\(H(F_{\\text{ref}}, F)\\) is \\[\n\\begin{split}\nH(F_{\\text{ref}}, F) &=  -\\text{E}_{F_{\\text{ref}}} \\left( \\log p(x |\\mu, \\sigma^2) \\right)\\\\\n&=  \\frac{1}{2}  \\text{E}_{F_{\\text{ref}}} \\left(  \\log(2\\pi\\sigma^2)  + \\frac{(x-\\mu)^2}{\\sigma^2} \\right) \\\\\n&= \\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 }\n+\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2}  +\\log(2 \\pi \\sigma^2) \\right)  \\\\\n\\end{split}\n\\] using \\(\\text{E}_{F_{\\text{ref}}} ((x-\\mu)^2) = (\\mu_{\\text{ref}}-\\mu)^2 + \\sigma^2_{\\text{ref}}\\).\n\n\n\nProperties of cross-entropy\n\nCross-entropy is not symmetric with regard to \\(Q\\) and \\(P\\), because the expectation is taken with reference to \\(Q\\).\nBy construction if both distributions \\(Q\\) and \\(P\\) are identical cross-entropy reduces to entropy, i.e. \\(H(Q, Q) = H(Q)\\).\nLike differential entropy cross-entropy changes under variable transformation for continuous random variables, say from \\(x\\) to \\(y\\), hence \\(H(Q_y, P_y) \\neq H(Q_x, P_x)\\).\n\n\n\nGibbs’ inequality\nA crucial further property of the cross-entropy \\(H(Q, P)\\) is that it is bounded below by the entropy of \\(Q\\), therefore \\[\nH(Q, P) \\geq H(Q)\n\\] with equality only if \\(Q=P\\). This is known as Gibbs’ inequality.\nThis follows from Jensen’s inequality. For details see Worksheet E1.\nEssentially this means that when data are generated (encoded) under model \\(Q\\) and described (decoded) using model \\(P\\) there is always an extra cost, or penalty, to employ the approximating model \\(P\\) rather than the correct model \\(Q\\) .\n\nExample 4.2 \\(\\color{Red} \\blacktriangleright\\) Proper scoring rules and Gibbs’ inequality:\nThe logarithmic scoring rule \\(S(x, P) = -\\log p(x)\\) is called proper because the corresponding expected score, i.e. the cross-entropy \\(H(Q, P)\\), satisfies Gibbs’ inequality. Hence, the expected logarithmic score rule is minimised for \\(P=Q\\).\nAny general scoring rule (see Note 3.1) for which the expected score is minimised when \\(P=Q\\) is called proper. It is this property that makes proper scoring rules (and thus entropy) useful in statistics and machine learning as it allows to indentify the best approximating models \\(P\\).\n\n\nExample 4.3 Normal differential entropy as lower bound of normal cross-entropy:\nRevisit the cross-entropy \\(H(F_{\\text{ref}},F)\\) in Example 4.1. Setting \\(\\mu_{\\text{ref}} = \\mu\\) and \\(\\sigma^2_{\\text{ref}} = \\sigma^2\\) the normal cross-entropy degenerates to the normal differential entropy \\(H(F_{\\text{ref}}) = \\frac{1}{2} \\left(\\log( 2 \\pi \\sigma^2_{\\text{ref}}) +1 \\right)\\) as obtained in Example 3.11.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Relative entropy</span>"
    ]
  },
  {
    "objectID": "04-entropy2.html#boltzmann-relative-entropy-and-kl-divergence",
    "href": "04-entropy2.html#boltzmann-relative-entropy-and-kl-divergence",
    "title": "4  Relative entropy",
    "section": "4.2 Boltzmann relative entropy and KL divergence",
    "text": "4.2 Boltzmann relative entropy and KL divergence\n\nBoltzmann entropy aka relative entropy\nThe Boltzmann entropy of a distribution \\(Q\\) relative to a distribution \\(P\\) is given by \\[\n\\begin{split}\nB(Q, P) &=  -\\text{E}_Q\\log\\left(\\frac{q(x)}{p(x)}\\right) \\\\\n        &= H(Q) - H(Q, P) \\\\\n\\end{split}\n\\] The Boltzmann entropy is also known as relative entropy.\nAs a consequence of the Gibbs’s inequality we see that the Boltzman entropy is always non-positive, \\(B(Q, P) \\leq 0\\). In Example 4.8 it is shown that \\(B(Q, P)\\) can be interpreted as a log-probability.\nBy construction, the Boltzmann entropy of \\(Q\\) relative to a uniform distribution \\(U\\) (with constant probability density mass function) is \\[\nB(Q, U) = H(Q) + \\text{const.}\n\\] i.e. it is equal to the entropy of \\(Q\\) apart from a constant.\n\n\nDefinition of KL divergence\nThe KL divergence is defined as the negative of the Boltzmann relative entropy as \\[\n\\begin{split}\nD_{\\text{KL}}(Q,P)  & = \\text{E}_Q\\log\\left(\\frac{q(x)}{p(x)}\\right)\\\\\n&= H(Q, P)-H(Q) \\\\\n& = \\text{E}_Q  \\left(  S(x, P) - S(x, Q)  \\right) \\\\\n\\end{split}\n\\] As a consequence of the Gibbs inequality the KL divergence is always non-negative: \\(D_{\\text{KL}}(Q, H) \\geq 0\\).\nThe KL divergence \\(D_{\\text{KL}}(Q,P)\\) is the expected difference in logarithmic scores when the data are generated by \\(Q\\) (note the expectation \\(\\text{E}_Q\\) with regard to \\(Q\\)) and models \\(P\\) and \\(Q\\) are evaluated on the observations. \\(D_{\\text{KL}}(Q, P)\\) can be interpreted as the additional cost if \\(P\\) is used instead \\(Q\\) to describe data from \\(Q\\). If \\(Q\\) and \\(P\\) are identical there is no extra cost and \\(D_{\\text{KL}}(Q,P)=0\\). Conversely, if they are not identical then there is an additional cost and \\(D_{\\text{KL}}(Q,P)&gt; 0\\).\n\\(D_{\\text{KL}}(Q,P)\\) thus serves as a measure of the divergence2 of the two distribution \\(P\\) and \\(Q\\). The use of the term “divergence” rather than “distance” is a reminder the \\(Q\\) and \\(P\\) are not interchangeable in \\(D_{\\text{KL}}(Q, P)\\).\n\n\nProperties of KL divergence and Boltzmann relative entropy\nBoltzmann relative entropy and KL divergence differ only by sign and thus share a number of key properties inherited from cross-entropy:\n\n\\(D_{\\text{KL}}(Q, P) \\neq D_{\\text{KL}}(Q, P)\\), i.e. the KL divergence is not symmetric, \\(Q\\) and \\(P\\) cannot be interchanged. This follows from the same property of cross-entropy.\n\\(D_{\\text{KL}}(Q, P)\\geq 0\\), follows from Gibbs’ inequality and proof via Jensen’s inequality.\n\\(D_{\\text{KL}}(Q, P) = 0\\) if and only if \\(P=Q\\), i.e., the KL divergence is zero if and only if \\(Q\\) and \\(P\\) are identical. Also follows from Gibbs’ inequality.\n\nFor more details and proofs of properties 2 and 3 see Worksheet E1.\nTypically, we wish to minimise KL divergence \\(D_{\\text{KL}}(Q, P)\\) and maximise Boltzmann relative entropy \\(B(Q, P)\\).\nA further crucial property of KL divergence is the invariance property:\n\n\\(D_{\\text{KL}}(Q, P)\\) is invariant under general variable transformations, with \\(D_{\\text{KL}}(Q_y, P_y) =D_{\\text{KL}}(Q_x, P_x)\\) under a change of variables from \\(x\\) to \\(y\\).\n\nThus, KL divergence does not change when the sample space is reparameterised.\nIn the definition of KL divergence the expectation is taken over a ratio of densities (or ratio of probabilities for discrete random variables). This creates the invariance under variable transformation as the Jacobian determinant that changes both densities cancel out.\nFor more details and proof of the invariance property see Worksheet E1.\n\n\nOrigin of Boltzmann relative entropy and KL divergence and naming conventions\nBoltzmann relative entropy was first discovered by Boltzmann (1878)3 in physics in a discrete setting in the context of statistical mechanics (see Example 4.8). In statistics and information theory KL divergence was formally introduced by Kullback and Leibler (1951)4. Good (1979)5 credits Turing with the first statistical application in 1940/1941 in the field of cryptography.\nThe KL divergence is also known as KL information or KL information number named after two of the original authors (Kullback and Leibler) who themselves referred to this quantity as discrimination information. Another common name is information divergence or short \\(\\boldsymbol I\\)-divergence. Some authors (e.g. Efron) call twice the KL divergence \\(2 D_{\\text{KL}}(Q, P) = D(Q, P)\\) the deviance of \\(P\\) from \\(Q\\). In the context of general scoring rules the divergence is also called discrepancy.\nThere also exist various notations for KL divergence in the literature. Here we use \\(D_{\\text{KL}}(Q, P)\\) but you will often find both \\(\\text{KL}(Q || P)\\) and \\(I^{KL}(Q; P)\\).\nEspecially in older literature the KL divergence is also referred to as “cross-entropy”. This use is outdated and only leads to confusion with the related but different definition of cross-entropy above.\nFurthermore, KL divergence is also frequently referred to as “relative entropy” however this use also leads to confusion as KL divergence is normally minimised whereas entropy and relative entropy (i.e. Boltzmann entropy) is normally maximised. Shannon (1948) defined “relative entropy” yet differently again as the ratio of Shannon-Gibbs entropy relative to its maximum value, i.e. as standardised entropy.\nIn this text relative entropy always refers to Boltzmann entropy with the opposite sign as KL divergence.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Relative entropy</span>"
    ]
  },
  {
    "objectID": "04-entropy2.html#kl-divergence-examples",
    "href": "04-entropy2.html#kl-divergence-examples",
    "title": "4  Relative entropy",
    "section": "4.3 KL divergence examples",
    "text": "4.3 KL divergence examples\n\nModels with a single parameter\n\nExample 4.4 KL divergence between two Bernoulli distributions \\(\\text{Ber}(\\theta_1)\\) and \\(\\text{Ber}(\\theta_2)\\):\nThe “success” probabilities for the two distributions are \\(\\theta_1\\) and \\(\\theta_2\\), respectively, and the complementary “failure” probabilities are \\(1-\\theta_1\\) and \\(1-\\theta_2\\). With this we get for the KL divergence \\[\nD_{\\text{KL}}(\\text{Ber}(\\theta_1), \\text{Ber}(\\theta_2))=\\theta_1 \\log\\left( \\frac{\\theta_1}{\\theta_2}\\right) + (1-\\theta_1) \\log\\left(\\frac{1-\\theta_1}{1-\\theta_2}\\right)\n\\]\n\n\nExample 4.5 KL divergence between two univariate normals with different means and common variance:\nAssume \\(F_{\\text{ref}}=N(\\mu_{\\text{ref}},\\sigma^2)\\) and \\(F=N(\\mu,\\sigma^2)\\).\nThen we get \\[D_{\\text{KL}}(F_{\\text{ref}}, F )=\\frac{1}{2} \\left(\\frac{(\\mu-\\mu_{\\text{ref}})^2}{\\sigma^2}\\right)\\]\nThus, the squared Euclidean distance is a special case of KL divergence. Note that in this case the KL divergence is symmetric.\n\n\n\nModels with multiple parameters\n\nExample 4.6 KL divergence between two categorical distributions with \\(K\\) classes:\nWith \\(Q=\\text{Cat}(\\boldsymbol q)\\) and \\(P=\\text{Cat}(\\boldsymbol p)\\) and corresponding probabilities \\(q_1,\\dots,q_K\\) and \\(p_1,\\dots,p_K\\) satisfying \\(\\sum_{i=1}^K q_i = 1\\) and \\(\\sum_{i=1}^K p_i =1\\) we get:\n\\[\\begin{equation*}\nD_{\\text{KL}}(Q, P)=\\sum_{i=1}^K q_i\\log\\left(\\frac{q_i}{p_i}\\right)\n\\end{equation*}\\]\nTo be explicit that there are only \\(K-1\\) parameters in a categorical distribution we can also write \\[\\begin{equation*}\nD_{\\text{KL}}(Q, P)=\\sum_{i=1}^{K-1} q_i\\log\\left(\\frac{q_i}{p_i}\\right)  + q_K\\log\\left(\\frac{q_K}{p_K}\\right)\n\\end{equation*}\\] with \\(q_K=\\left(1- \\sum_{i=1}^{K-1} q_i\\right)\\) and \\(p_K=\\left(1- \\sum_{i=1}^{K-1} p_i\\right)\\).\n\n\nExample 4.7 KL divergence between two univariate normals with different means and variances:\nAssume \\(F_{\\text{ref}}=N(\\mu_{\\text{ref}},\\sigma^2_{\\text{ref}})\\) and \\(F=N(\\mu,\\sigma^2)\\). Then \\[\n\\begin{split}\nD_{\\text{KL}}(F_{\\text{ref}},F) &= H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) \\\\\n&= \\frac{1}{2} \\left(   \\frac{(\\mu-\\mu_{\\text{ref}})^2}{\\sigma^2}  + \\frac{\\sigma_{\\text{ref}}^2}{\\sigma^2}\n-\\log\\left(\\frac{\\sigma_{\\text{ref}}^2}{\\sigma^2}\\right)-1  \n   \\right) \\\\\n\\end{split}\n\\]\nIf variances are equal then we recover the previous Example 4.5 as special case.\n\n\nExample 4.8 \\(\\color{Red} \\blacktriangleright\\) Boltzmann relative entropy as log-probability:\nAssume \\(\\hat{Q}\\) is an empirical categorical distribution based on observed counts \\(n_k\\) (see Example 3.12) and \\(P\\) is a second categorical distribution.\nThe KL divergence is then \\[\n\\begin{split}\nB(\\hat{Q}, P) & = H(\\hat{Q}) -H(\\hat{Q}, P) \\\\\n& = H(\\hat{Q})  +  \\sum_{i=1}^K  \\log ( p_i) \\, \\hat{q}_i   \\\\\n& = H(\\hat{Q})  +   \\frac{1}{n} \\sum_{i=1}^K n_i  \\log p_i  \\\\\n\\end{split}\n\\]\nFor large \\(n\\) we may use the multinomial coefficient \\(W = \\binom{n}{n_1, \\ldots, n_K}\\) to obtain the entropy of \\(\\hat{Q}\\) (see Example 3.12). This results in \\[\n\\begin{split}\nB(\\hat{Q}, P) &\\approx \\frac{1}{n} \\left( \\log W  + \\sum_{i=1}^K n_i \\log p_i   \\right)\\\\\n& = \\frac{1}{n} \\log \\left( W \\times \\prod_{i=1}^K  p_i^{n_i}    \\right)\\\\\n& = \\frac{1}{n} \\log  \\text{Pr}(n_1, \\ldots, n_K| \\,\\boldsymbol p) \\\\\n\\end{split}\n\\] Hence the Boltzmann relative entropy is directly linked to the multinomial probability of the observed counts \\(n_1, \\ldots, n_k\\) under the model \\(P\\). This derivation of the Boltzmann relative entropy as log-probability of a macrostate is due to Boltzmann (1878). See also Akaike (1985) for a historical account.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Relative entropy</span>"
    ]
  },
  {
    "objectID": "04-entropy2.html#footnotes",
    "href": "04-entropy2.html#footnotes",
    "title": "4  Relative entropy",
    "section": "",
    "text": "This follows the current and widely accepted usage of the term cross-entropy. However, in some typically older literature cross-entropy can sometimes refer instead to the related but different KL divergence discussed further below.↩︎\nNote that divergence between distributions is not related to and should not be confused with the divergence vector operator used in vector calculus.↩︎\nBoltzmann, L. 1878. Weitere Bemerkungen über einige Probleme der mechanischen Wärmetheorie. Wien Ber. 78:7–46. https://doi.org/10.1017/CBO9781139381437.013↩︎\nKullback, S., and R. A. Leibler. 1951. On information and sufficiency. Ann. Math. Statist. 22 79–86. https://doi.org/10.1214/aoms/1177729694↩︎\nGood, I. J. 1979. Studies in the history of probability. XXXVII. A. M. Turing’s statistical work in world war II. Biometrika, 66:393–396. https://doi.org/10.1093/biomet/66.2.393↩︎",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Relative entropy</span>"
    ]
  },
  {
    "objectID": "05-entropy3.html",
    "href": "05-entropy3.html",
    "title": "5  Expected Fisher information",
    "section": "",
    "text": "5.1 Expected Fisher information",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expected Fisher information</span>"
    ]
  },
  {
    "objectID": "05-entropy3.html#expected-fisher-information",
    "href": "05-entropy3.html#expected-fisher-information",
    "title": "5  Expected Fisher information",
    "section": "",
    "text": "Definition of expected Fisher information\nKL information measures the divergence of two distributions. Previously we have seen examples of KL divergence between two distributions belonging to the same family. We now consider the KL divergence of two such distributions separated in parameter space only by some small \\(\\boldsymbol \\varepsilon\\).\nSpecifically, we consider the function \\[\n\\begin{split}\nh(\\boldsymbol \\theta+\\boldsymbol \\varepsilon) & = D_{\\text{KL}}(F_{\\boldsymbol \\theta}, F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon}) \\\\\n&= \\text{E}_{F_{\\boldsymbol \\theta}}\\left(  \\log f(\\boldsymbol x| \\boldsymbol \\theta)  - \\log f(\\boldsymbol x| \\boldsymbol \\theta+\\boldsymbol \\varepsilon)   \\right)\\\\\n\\end{split}\n\\] where \\(\\boldsymbol \\theta\\) is kept constant and \\(\\boldsymbol \\varepsilon\\) is varying. Assuming that \\(f(\\boldsymbol x| \\boldsymbol \\theta)\\) is twice differentiable with regard to \\(\\boldsymbol \\theta\\) we can approximate \\(h(\\boldsymbol \\theta+\\boldsymbol \\varepsilon)\\) quadratically by \\[\nh(\\boldsymbol \\theta+\\boldsymbol \\varepsilon) \\approx h(\\boldsymbol \\theta) + \\nabla h(\\boldsymbol \\theta)^T\\boldsymbol \\varepsilon+ \\frac{1}{2} \\boldsymbol \\varepsilon^T \\, \\nabla \\nabla^T h(\\boldsymbol \\theta) \\,\\boldsymbol \\varepsilon\n\\]\nFrom the properties of the KL divergence we know that \\(D_{\\text{KL}}(F_{\\boldsymbol \\theta}, F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon})\\geq 0\\) and that it becomes zero only if \\(\\boldsymbol \\varepsilon=0\\). Thus, by construction the function \\(h(\\boldsymbol \\theta+\\boldsymbol \\varepsilon)\\) achieves for \\(\\boldsymbol \\varepsilon=0\\)\n\na true minimum with \\(h(\\boldsymbol \\theta)=0\\),\na vanishing gradient with \\(\\nabla h(\\boldsymbol \\theta) = 0\\), and\na positive definite Hessian matrix with \\(\\nabla \\nabla^T h(\\boldsymbol \\theta) =  -\\text{E}_{F_{\\boldsymbol \\theta}} \\nabla \\nabla^T  \\log f(\\boldsymbol x| \\boldsymbol \\theta)\\).\n\nTherefore in the quadratic approximation of \\(h(\\boldsymbol \\theta+\\boldsymbol \\varepsilon)\\) around \\(\\boldsymbol \\theta\\) above the first two terms (constant and linear) vanish and only the quadratic term remains. The Hessian matrix evaluated at \\(\\boldsymbol \\theta\\) \\[\n\\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta) =   -\\text{E}_{F_{\\boldsymbol \\theta}} \\nabla \\nabla^T  \\log f(\\boldsymbol x| \\boldsymbol \\theta)\n\\] is called expected Fisher information for \\(\\boldsymbol \\theta\\), or short Fisher information. Hence, the KL divergence can be locally approximated by \\[\nD_{\\text{KL}}(F_{\\boldsymbol \\theta}, F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon})\\approx \\frac{1}{2} \\boldsymbol \\varepsilon^T  \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta) \\boldsymbol \\varepsilon\n\\]\nWe may also vary the first argument in the KL divergence. It is straightforward to show that this leads to the same approximation to second order in \\(\\boldsymbol \\varepsilon\\): \\[\n\\begin{split}\nD_{\\text{KL}}(F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon}, F_{\\boldsymbol \\theta})\n&\\approx \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)\\, \\boldsymbol \\varepsilon\\\\\n\\end{split}\n\\]\nHence, the KL divergence, while generally not symmetric in its arguments, is still locally symmetric.\nComputing the expected Fisher information involves no observed data, it is purely a property of the model family \\(F_{\\boldsymbol \\theta}\\). In Chapter 9 we will study a related quantity, the observed Fisher information that in contrast to the expected Fisher information is a function of the observed data.\n\n\n\n\n\n\nNote 5.1: \\(\\color{Red} \\blacktriangleright\\) Fisher information as metric tensor\n\n\n\nIn the field of information geometry1 sets of distributions are studied using tools from differential geometry. It turns out that distribution families are manifolds and that the expected Fisher information matrix plays the role of the (symmetric!) metric tensor on this manifold.\n\n\n\n\nAdditivity of Fisher information\nWe may wish to compute the expected Fisher information based on a set of independent identically distributed (iid) random variables.\nAssume that a random variable \\(x \\sim F_{\\boldsymbol \\theta}\\) has log-density \\(\\log f(x| \\boldsymbol \\theta)\\) and expected Fisher information \\(\\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)\\). The expected Fisher information \\(\\boldsymbol I_{x_1, \\ldots, x_n}^{\\text{Fisher}}(\\boldsymbol \\theta)\\) for a set of iid random variables \\(x_1, \\ldots, x_n \\sim F_{\\boldsymbol \\theta}\\) is computed from the joint log-density \\(\\log f(x_1, \\ldots, x_n) = \\sum_{i}^n \\log f(x_i| \\boldsymbol \\theta)\\). This yields \\[\n\\begin{split}\n\\boldsymbol I_{x_1, \\ldots, x_n}^{\\text{Fisher}}(\\boldsymbol \\theta) &= -\\text{E}_{F_{\\boldsymbol \\theta}} \\nabla \\nabla^T  \\sum_{i}^n \\log f(x_i| \\boldsymbol \\theta)\\\\\n&= \\sum_{i=1}^n  \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta) =n  \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta) \\\\\n\\end{split}\n\\] Hence, the expected Fisher information for a set of \\(n\\) iid random variables is the \\(n\\) times the Fisher information of a single variable.\n\n\nInvariance property of the Fisher information\nLike KL divergence the expected Fisher information is invariant against change of parametrisation of the sample space, say from variable \\(x\\) to \\(y\\) and from distribution \\(F_x\\) to \\(F_y\\). This is easy to see as the KL divergence itself is invariant against such reparametrisation, and thus also its curvature, and hence the expected Fisher information.\nMore specifically, when the sample space is changed the density will gain a factor in the form of the Jacobian determinant according to this transformation. However, since this factor does not depend on the model parameters, the first and second derivatives of the log-density with regard to the model parameters are not affected by it.\nSee also Section 7.4 for related sample space invariance of the gradient and curvature of the log-likelihood and Chapter 9 for the sample invariance of observed Fisher information.\n\n\n\\(\\color{Red} \\blacktriangleright\\) Transformation of Fisher information when model parameters change\nThe Fisher information \\(\\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)\\) depends on the parameter \\(\\boldsymbol \\theta\\). If we use a different parameterisation of the underlying parametric distribution family, say \\(\\boldsymbol \\zeta\\) with a map \\(\\boldsymbol \\theta(\\boldsymbol \\zeta)\\) from \\(\\boldsymbol \\zeta\\) to \\(\\boldsymbol \\theta\\), then the Fisher information changes according to the chain rule in calculus.\nTo find the resulting Fisher information in terms of the new parameter \\(\\boldsymbol \\zeta\\) we need to use the Jacobian matrix \\(D \\boldsymbol \\theta(\\boldsymbol \\zeta)\\). This matrix contains the gradients for each component of the map \\(\\boldsymbol \\theta(\\boldsymbol \\zeta)\\) in its rows: \\[\nD \\boldsymbol \\theta(\\boldsymbol \\zeta) =\n\\begin{pmatrix}\\nabla^T \\theta_1(\\boldsymbol \\zeta)\\\\ \\nabla^T \\theta_2(\\boldsymbol \\zeta) \\\\ \\vdots \\\\  \\end{pmatrix}\n\\]\nWith the above the Fisher information for \\(\\boldsymbol \\theta\\) is then transformed to the Fisher information for \\(\\boldsymbol \\zeta\\) applying the chain rule for the Hessian matrix: \\[\n\\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\zeta)   = (D \\boldsymbol \\theta(\\boldsymbol \\zeta))^T \\, \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta) \\rvert_{\\boldsymbol \\theta= \\boldsymbol \\theta(\\boldsymbol \\zeta)}  \\, D \\boldsymbol \\theta(\\boldsymbol \\zeta)\n\\] This type of transformation is also known as covariant transformation, in this case for the Fisher information metric tensor.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expected Fisher information</span>"
    ]
  },
  {
    "objectID": "05-entropy3.html#expected-fisher-information-examples",
    "href": "05-entropy3.html#expected-fisher-information-examples",
    "title": "5  Expected Fisher information",
    "section": "5.2 Expected Fisher information examples",
    "text": "5.2 Expected Fisher information examples\n\nModels with a single parameter\n\nExample 5.1 Expected Fisher information for the Bernoulli distribution:\nThe log-probability mass function of the Bernoulli \\(\\text{Ber}(\\theta)\\) distribution is \\[\n\\log p(x | \\theta) = x \\log(\\theta) + (1-x) \\log(1-\\theta)\n\\] where \\(\\theta\\) is the probability of “success”. The second derivative with regard to the parameter \\(\\theta\\) is \\[\n\\frac{d^2}{d\\theta^2} \\log p(x | \\theta)  =  -\\frac{x}{\\theta^2}-  \\frac{1-x}{(1-\\theta)^2}\n\\] Since \\(\\text{E}(x) = \\theta\\) we get as Fisher information \\[\n\\begin{split}\nI^{\\text{Fisher}}(\\theta) & = -\\text{E}\\left(\\frac{d^2}{d\\theta^2} \\log p(x | \\theta)  \\right)\\\\\n                           &= \\frac{\\theta}{\\theta^2}+  \\frac{1-\\theta}{(1-\\theta)^2} \\\\\n                            &= \\frac{1}{\\theta(1-\\theta)}\\\\\n\\end{split}\n\\]\n\n\nExample 5.2 Quadratic approximations of the KL divergence between two Bernoulli distributions:\nFrom Example 4.4 we have as KL divergence \\[\nD_{\\text{KL}}\\left (\\text{Ber}(\\theta_1), \\text{Ber}(\\theta_2) \\right)=\\theta_1 \\log\\left( \\frac{\\theta_1}{\\theta_2}\\right) + (1-\\theta_1) \\log\\left(\\frac{1-\\theta_1}{1-\\theta_2}\\right)\n\\] and from Example 5.1 the corresponding expected Fisher information.\nThe quadratic approximation implies that \\[\nD_{\\text{KL}}\\left( \\text{Ber}(\\theta), \\text{Ber}(\\theta + \\varepsilon) \\right) \\approx \\frac{\\varepsilon^2}{2}  I^{\\text{Fisher}}(\\theta) =  \\frac{\\varepsilon^2}{2 \\theta (1-\\theta)}\n\\] and also that \\[\nD_{\\text{KL}}\\left( \\text{Ber}(\\theta+\\varepsilon), \\text{Ber}(\\theta) \\right) \\approx \\frac{\\varepsilon^2}{2} I^{\\text{Fisher}}(\\theta) =  \\frac{\\varepsilon^2}{2 \\theta (1-\\theta)}\n\\]\nIn Worksheet E1 this is verified by using a second order Taylor series applied to the KL divergence.\n\n\nExample 5.3 Expected Fisher information for the normal distribution \\(N(\\mu, \\sigma^2)\\) with known variance.\nThe log-density is \\[\n\\log f(x | \\mu, \\sigma^2) = -\\frac{1}{2} \\log(\\sigma^2)\n-\\frac{1}{2 \\sigma^2} (x-\\mu)^2 - \\frac{1}{2}\\log(2 \\pi)\n\\] The second derivative with respect to \\(\\mu\\) is \\[\n\\frac{d^2}{d\\mu^2} \\log f(x | \\mu, \\sigma^2) = -\\frac{1}{\\sigma^2}\n\\] Therefore the expected Fisher information is \\[\n\\boldsymbol I^{\\text{Fisher}}\\left(\\mu\\right) = \\frac{1}{\\sigma^2}\n\\]\n\n\n\nModels with multiple parameters\n\nExample 5.4 Expected Fisher information for the normal distribution \\(N(\\mu, \\sigma^2)\\).\nThe log-density is \\[\n\\log f(x | \\mu, \\sigma^2) = -\\frac{1}{2} \\log(\\sigma^2)\n-\\frac{1}{2 \\sigma^2} (x-\\mu)^2 - \\frac{1}{2}\\log(2 \\pi)\n\\] The gradient with respect to \\(\\mu\\) and \\(\\sigma^2\\) (!) is the vector \\[\n\\nabla \\log f(x | \\mu, \\sigma^2) =\n\\begin{pmatrix}\n\\frac{1}{\\sigma^2} (x-\\mu) \\\\\n- \\frac{1}{2 \\sigma^2} + \\frac{1}{2 \\sigma^4} (x- \\mu)^2 \\\\\n\\end{pmatrix}\n\\] Hint for calculating the gradient: replace \\(\\sigma^2\\) by \\(v\\) and then take the partial derivative with regard to \\(v\\), then substitute back.\nThe corresponding Hessian matrix is \\[\n\\nabla \\nabla^T \\log f(x | \\mu, \\sigma^2) =\n\\begin{pmatrix}\n-\\frac{1}{\\sigma^2} & -\\frac{1}{\\sigma^4} (x-\\mu)\\\\\n-\\frac{1}{\\sigma^4} (x-\\mu) &  \\frac{1}{2\\sigma^4} - \\frac{1}{\\sigma^6}(x- \\mu)^2 \\\\\n\\end{pmatrix}\n\\] As \\(\\text{E}(x) = \\mu\\) we have \\(\\text{E}(x-\\mu) =0\\). Furthermore, with \\(\\text{E}( (x-\\mu)^2 ) =\\sigma^2\\) we see that \\(\\text{E}\\left(\\frac{1}{\\sigma^6}(x- \\mu)^2\\right) = \\frac{1}{\\sigma^4}\\). Therefore the expected Fisher information matrix as the negative expected Hessian matrix is \\[\n\\boldsymbol I^{\\text{Fisher}}\\left(\\mu,\\sigma^2\\right) = \\begin{pmatrix} \\frac{1}{\\sigma^2} & 0 \\\\ 0 & \\frac{1}{2\\sigma^4} \\end{pmatrix}\n\\]\n\n\nExample 5.5 \\(\\color{Red} \\blacktriangleright\\) Expected Fisher information of the categorical distribution:\nThe log-probability mass function for the categorical distribution with \\(K\\) classes and \\(K-1\\) free parameters \\(\\pi_1, \\ldots, \\pi_{K-1}\\) is \\[\n\\begin{split}\n\\log p(\\boldsymbol x| \\pi_1, \\ldots, \\pi_{K-1}  ) & =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + x_K \\log \\pi_K \\\\\n& =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_k  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\\\\n\\end{split}\n\\]\nFrom the log-probability mass function we compute the Hessian matrix of second order partial derivatives \\(\\nabla \\nabla^T \\log p(\\boldsymbol x| \\pi_1, \\ldots, \\pi_{K-1} )\\) with regard to \\(\\pi_1, \\ldots, \\pi_{K-1}\\):\n\nThe diagonal entries of the Hessian matrix (with \\(i=1, \\ldots, K-1\\)) are \\[\n\\frac{\\partial^2}{\\partial \\pi_i^2} \\log p(\\boldsymbol x|\\pi_1, \\ldots, \\pi_{K-1}) =\n-\\frac{x_i}{\\pi_i^2}-\\frac{x_K}{\\pi_K^2}\n\\]\nthe off-diagonal entries are (with \\(j=1, \\ldots, K-1\\) and \\(j \\neq i\\)) \\[\n\\frac{\\partial^2}{\\partial \\pi_i \\partial \\pi_j} \\log p(\\boldsymbol x|\\pi_1, \\ldots, \\pi_{K-1}) =\n-\\frac{ x_K}{\\pi_K^2}\n\\]\n\nRecalling that \\(\\text{E}(x_i) = \\pi_i\\) we obtain the expected Fisher information matrix for a categorical distribution as \\(K-1  \\times K-1\\) dimensional matrix \\[\n\\begin{split}\n\\boldsymbol I^{\\text{Fisher}}\\left( \\pi_1, \\ldots, \\pi_{K-1}  \\right) &=\n-\\text{E}\\left( \\nabla \\nabla^T \\log p(\\boldsymbol x| \\pi_1, \\ldots, \\pi_{K-1}) \\right) \\\\\n& =\n\\begin{pmatrix}\n\\frac{1}{\\pi_1} + \\frac{1}{\\pi_K} & \\cdots & \\frac{1}{\\pi_K} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{\\pi_K} & \\cdots & \\frac{1}{\\pi_{K-1}} + \\frac{1}{\\pi_K} \\\\\n\\end{pmatrix}\\\\\n& = \\text{Diag}\\left( \\frac{1}{\\pi_1} , \\ldots,  \\frac{1}{\\pi_{K-1}}   \\right) + \\frac{1}{\\pi_K} \\mathbf 1\\\\\n\\end{split}\n\\]\nFor \\(K=2\\) and \\(\\pi_1=\\theta\\) this reduces to the expected Fisher information of a Bernoulli variable, see Example 5.1. \\[\n\\begin{split}\nI^{\\text{Fisher}}(\\theta) & =  \\left(\\frac{1}{\\theta} + \\frac{1}{1-\\theta} \\right) \\\\\n  &= \\frac{1}{\\theta (1-\\theta)} \\\\\n\\end{split}\n\\]\n\n\nExample 5.6 \\(\\color{Red} \\blacktriangleright\\) Quadratic approximation of KL divergence of the categorical distribution and the Neyman and Pearson divergence:\nWe now consider the local approximation of the KL divergence \\(D_{\\text{KL}}(Q, P)\\) between the categorical distribution \\(Q=\\text{Cat}(\\boldsymbol q)\\) with probabilities \\(\\boldsymbol q=(q_1, \\ldots, q_K)^T\\) with the categorical distribution \\(P=\\text{Cat}(\\boldsymbol p)\\) with probabilities \\(\\boldsymbol p= (p_1, \\ldots, p_K)^T\\).\nFrom Example 4.6 we already know the KL divergence and from Example 5.5 the corresponding expected Fisher information.\nFirst, we keep the first argument \\(Q\\) fixed and assume that \\(P\\) is a perturbed version of \\(Q\\) with \\(\\boldsymbol p= \\boldsymbol q+\\boldsymbol \\varepsilon\\). Note that the perturbations \\(\\boldsymbol \\varepsilon=(\\varepsilon_1, \\ldots, \\varepsilon_K)^T\\) satisfy \\(\\sum_{k=1}^K \\varepsilon_k = 0\\) because \\(\\sum_{k=1}^K q_i=1\\) and \\(\\sum_{k=1}^K p_i=1\\). Thus \\(\\varepsilon_K = -\\sum_{k=1}^{K-1} \\varepsilon_k\\). Then \\[\n\\begin{split}\nD_{\\text{KL}}(\\text{Cat}(\\boldsymbol q), \\text{Cat}(\\boldsymbol q+\\boldsymbol \\varepsilon))\n&  \\approx \\frac{1}{2} (\\varepsilon_1, \\ldots,  \\varepsilon_{K-1}) \\,\n\\boldsymbol I^{\\text{Fisher}}\\left( q_1, \\ldots, q_{K-1}  \\right)\n\\begin{pmatrix} \\varepsilon_1 \\\\ \\vdots \\\\  \\varepsilon_{K-1}\\\\\n\\end{pmatrix} \\\\\n&= \\frac{1}{2} \\left( \\sum_{k=1}^{K-1} \\frac{\\varepsilon_k^2}{q_k}   + \\frac{ \\left(\\sum_{k=1}^{K-1} \\varepsilon_k\\right)^2}{q_K} \\right)  \\\\\n&= \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{\\varepsilon_k^2}{q_k}\\\\\n&= \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{(q_k-p_k)^2}{q_k}\\\\\n& = \\frac{1}{2} D_{\\text{Neyman}}(Q, P)\\\\\n\\end{split}\n\\] Similarly, if we keep \\(P\\) fixed and consider \\(Q\\) as a perturbed version of \\(P\\) we get \\[\n\\begin{split}\nD_{\\text{KL}}(\\text{Cat}(\\boldsymbol p+\\boldsymbol \\varepsilon), \\text{Cat}(\\boldsymbol p))\n&\\approx \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{(q_k-p_k)^2}{p_k}\\\\\n&= \\frac{1}{2} D_{\\text{Pearson}}(Q, P)\n\\end{split}\n\\] Note that in both approximations we divide by the probabilities of the distribution that is kept fixed.\nNote the appearance of the Pearson \\(\\chi^2\\) divergence and the Neyman \\(\\chi^2\\) divergence in the above. Both are, like the KL divergence, part of the family of \\(f\\)-divergences. The Neyman \\(\\chi^2\\) divergence is also known as the reverse Pearson divergence as \\(D_{\\text{Neyman}}(Q, P) = D_{\\text{Pearson}}(P, Q)\\).",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expected Fisher information</span>"
    ]
  },
  {
    "objectID": "05-entropy3.html#footnotes",
    "href": "05-entropy3.html#footnotes",
    "title": "5  Expected Fisher information",
    "section": "",
    "text": "A recent review is given, e.g., in: Nielsen, F. 2020. An elementary introduction to information geometry. Entropy 22:1100. https://doi.org/10.3390/e22101100↩︎",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expected Fisher information</span>"
    ]
  },
  {
    "objectID": "06-entropy4.html",
    "href": "06-entropy4.html",
    "title": "6  Principle of maximum entropy",
    "section": "",
    "text": "6.1 \\(\\color{Red} \\blacktriangleright\\) Maximum entropy principle to characterise distributions\nBoth Shannon entropy and differential entropy are useful to characterise distributions.\nAs discussed in Chapter 3, large entropy implies that the distribution is spread out whereas small entropy indicates that the distribution is concentrated.\nCorrespondingly, maximum entropy distributions can be considered minimally informative about a random variable. The higher the entropy the more spread out (and hence more uninformative) the distribution. Conversely, low entropy implies that the probability mass is concentrated and thus the distribution is more informative about the random variable.\nExamples:\nUsing maximum entropy to characterise maximally uninformative distributions was advocated by Edwin T. Jaynes (1922–1998) (who also proposed to use maximum entropy in the context of finding Bayesian priors). The maximum entropy principle in statistical physics goes back to Boltzmann.\nA list of maximum entropy distribution is given here: https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution.\nMany distributions commonly used in statistical modelling are exponential families. Intriguingly, these distribution are all maximum entropy distributions, so there is a very close link between the principle of maximum entropy and common model choices in statistics and machine learning.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Principle of maximum entropy</span>"
    ]
  },
  {
    "objectID": "06-entropy4.html#colorred-blacktriangleright-maximum-entropy-principle-to-characterise-distributions",
    "href": "06-entropy4.html#colorred-blacktriangleright-maximum-entropy-principle-to-characterise-distributions",
    "title": "6  Principle of maximum entropy",
    "section": "",
    "text": "The discrete uniform distribution is the maximum entropy distribution among all discrete distributions.\nthe maximum entropy distribution of a continuous random variable with support \\([-\\infty, \\infty]\\) with a specific mean and variance is the normal distribution.\nthe maximum entropy distribution among all continuous distributions supported in \\([0, \\infty]\\) with a specified mean is the exponential distribution.\n\n\n\n\n\nExample 6.1 \\(\\color{Red} \\blacktriangleright\\) Discrete uniform distribution as maximum entropy distribution:\nAssume \\(G\\) is an categorical distribution with \\(K\\) classes and probabilities \\(g_i\\). We now show that \\(G\\) has maximum entropy when \\(G\\) is the discrete uniform distribution.\nLet \\(P=U_K\\) be the discrete uniform with equal probabilities \\(p_i=1/K\\). The entropy of \\(P\\) is \\(H(P) = \\log K\\) (see also Example 3.9). The cross-entropy \\(H(G, P) = - \\text{E}_G \\log p_i = \\log K\\). Note that both entropies are identical.\nFrom Gibbs’ inequality we know that \\(H(G, P) \\geq H(G)\\). Since in our case \\(H(G, P) = H(P)\\) it follows directly that \\(H(P) \\geq H(G)\\), i.e. the discrete uniform distribution \\(P\\) achieves the maximum entropy. Furthermore, any other distribution \\(G\\) will have lower entropy, with equality only if \\(G=P\\) and thus only if \\(g_i=1/K\\).\n\n\nExample 6.2 \\(\\color{Red} \\blacktriangleright\\) Exponential distribution as maximum entropy distribution:\nAssume \\(G\\) is an continous distribution for \\(x\\) with support \\([0, \\infty]\\) and with specified mean \\(\\text{E}(x) = \\theta\\). We now show that \\(G\\) has maximum entropy when \\(G\\) is the exponential distribution.\nThe log-density of the exponential distribution \\(P\\) with scale parameter \\(\\theta\\) is \\(\\log p(x | \\theta) =  x/\\theta -\\log \\theta\\). The differential entropy of \\(P\\) is \\(H(P) = -\\text{E}_P \\log p(x | \\theta) = 1 +\\log \\theta\\) as \\(\\text{E}_P(x) = \\theta\\). The cross-entropy \\(H(G, P) =  -\\text{E}_G \\log p(x | \\theta) = 1 +\\log \\theta\\) as \\(\\text{E}_G(x) = \\theta\\). Note that both entropies are identical.\nFrom Gibbs’ inequality we know that \\(H(G, P) \\geq H(G)\\). Since in our case \\(H(G, P) = H(P)\\) it follows directly that \\(H(P) \\geq H(G)\\), i.e. the exponential distribution \\(P\\) achieves the maximum entropy. Furthermore, any other distribution \\(G\\) will have lower entropy, with equality only if \\(G=P\\).",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Principle of maximum entropy</span>"
    ]
  },
  {
    "objectID": "07-likelihood1.html",
    "href": "07-likelihood1.html",
    "title": "7  Principle of maximum likelihood",
    "section": "",
    "text": "7.1 Overview",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Principle of maximum likelihood</span>"
    ]
  },
  {
    "objectID": "07-likelihood1.html#overview",
    "href": "07-likelihood1.html#overview",
    "title": "7  Principle of maximum likelihood",
    "section": "",
    "text": "Outline of maximum likelihood estimation\nMaximum likelihood is a very general method for fitting probabilistic models to data that generalises the method of least-squares. It plays a very important role in statistics and was mainly developed by R.A. Fisher in the early 20th century. 1\nIn a nutshell, the starting points in a maximum likelihood analysis are\n\nthe observed data \\(D = \\{x_1,\\ldots,x_n\\}\\) with \\(n\\) independent and identically distributed (iid) samples, with the ordering irrelevant, and a\na model \\(P_{\\boldsymbol \\theta}\\) with corresponding probability density or probability mass function \\(p(x|\\boldsymbol \\theta)\\) and parameters \\(\\boldsymbol \\theta\\)\n\nFrom model and data the likelihood function (note upper case “L”) is constructed as \\[\nL_n(\\boldsymbol \\theta|D)=\\prod_{i=1}^{n} p(x_i|\\boldsymbol \\theta)\n\\] Equivalently, the log-likelihood function (note lower case “l”) is \\[\nl_n(\\boldsymbol \\theta|D)=\\sum_{i=1}^n \\log p(x_i|\\boldsymbol \\theta)\n\\] The likelihood is multiplicative and the log-likelihood additive over the samples \\(x_i\\).\n\n\n\n\n\n\nFigure 7.1: Finding the maximum likelihood estimate by maximisation.\n\n\n\nThe maximum likelihood estimate (MLE) \\(\\hat{\\boldsymbol \\theta}^{ML}\\) is then found by maximising the (log)-likelihood function with regard to the parameters \\(\\boldsymbol \\theta\\) (see Figure 7.1): \\[\n\\hat{\\boldsymbol \\theta}_{ML} = \\underset{\\boldsymbol \\theta}{\\arg \\max}\\, l_n(\\boldsymbol \\theta|D)\n\\]\nHence, once the model is chosen and data are collected, finding the MLE and thus fitting the model to the data is an optimisation problem.\nDepending on the complexity of the likelihood function and the number of the parameters finding the maximum likelihood can be very difficult. On the other hand, for likelihood functions constructed from common distribution families, such as exponential families, maximum likelihood estimation is very straightforward and can even be done analytically (this is the case for most examples we encounter in this course).\nIn practise in application to more complex models the optimisation required for maximum likelihood analysis is done on the computer, typically on the log-likelihood rather than on the likelihood function in order to avoid problems with the computer representation of small floating point numbers. Suitable optimisation algorithm may rely only on function values without requiring derivatives, or use in addition gradient and possibly curvature information. In recent years there has been a lot of progress in high-dimensional optimisation using combined numerical and analytical approaches (e.g. using automatic differentiation) and stochastic approximations (e.g. stochastic gradient descent).\n\n\nOrigin of the method of maximum likelihood\nHistorically, the likelihood has often interpreted and justified as the probability of the data given the model. However, while providing an intuitive understanding this is not strictly correct. First, this interpretation only applies to discrete random variables. Second, since the samples \\(x_1, \\ldots, x_n\\) are typically exchangeable (i.e. permutation invariant) even in this case one would still need to add a factor accounting for the multiplicity of the possible orderings of the samples to obtain the correct probability of the data. Third, the interpretation of likelihood as probability of the data completely breaks down for continuous random variables because then \\(p(x |\\boldsymbol \\theta)\\) is a density, not a probability.\nNext, we will see that maximum likelihood estimation is a well-justified method that arises naturally from an entropy perspective. More specifically, the maximum likelihood estimate corresponds to the distribution \\(P_{\\boldsymbol \\theta}\\) that is closest in terms of KL divergence to the unknown true data generating model as represented by the observed data and the empirical distribution.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Principle of maximum likelihood</span>"
    ]
  },
  {
    "objectID": "07-likelihood1.html#from-entropy-learning-to-maximum-likelihood",
    "href": "07-likelihood1.html#from-entropy-learning-to-maximum-likelihood",
    "title": "7  Principle of maximum likelihood",
    "section": "7.2 From entropy learning to maximum likelihood",
    "text": "7.2 From entropy learning to maximum likelihood\n\nThe KL divergence between true model and approximating model\nAssume we have observations \\(D = \\{x_1, \\ldots, x_n\\}\\). The data are sampled from \\(F\\), the true but unknown data generating distribution. We also specify a family of distributions \\(P_{\\boldsymbol \\theta}\\) indexed by \\(\\boldsymbol \\theta\\) to approximate \\(F\\).\nThe KL divergence \\(D_{\\text{KL}}(F,P_{\\boldsymbol \\theta})\\) measures the divergence of the approximation \\(P_{\\boldsymbol \\theta}\\) from the unknown true model \\(F\\). It can be written as \\[\n\\begin{split}\nD_{\\text{KL}}(F,P_{\\boldsymbol \\theta}) &= H(F,P_{\\boldsymbol \\theta}) - H(F) \\\\\n&= \\underbrace{- \\text{E}_{F}\\log p_{\\boldsymbol \\theta}(x)}_{\\text{cross-entropy}}\n-(\\underbrace{-\\text{E}_{F}\\log f(x)}_{\\text{entropy of $F$, does not depend on $\\boldsymbol \\theta$}})\\\\\n\\end{split}\n\\]\nHowever, since we do not know \\(F\\) we cannot actually compute this divergence. Nonetheless, we may use the empirical distribution \\(\\hat{F}_n\\) — a function of the observed data — as approximation for \\(F\\), and in this way we arrive at an approximation for \\(D_{\\text{KL}}(F,P_{\\boldsymbol \\theta})\\) that becomes more and more accurate with growing sample size.\n\nRecall the “Law of Large Numbers” :\n\nThe empirical distribution \\(\\hat{F}_n\\) based on observed data \\(D=\\{x_1, \\ldots, x_n\\}\\) converges strongly (almost surely) to the true underlying distribution \\(F\\) as \\(n \\rightarrow \\infty\\): \\[\n\\hat{F}_n\\overset{a. s.}{\\to} F\n\\]\nCorrespondingly, for \\(n \\rightarrow \\infty\\) the average \\(\\text{E}_{\\hat{F}_n}(h(x)) = \\frac{1}{n} \\sum_{i=1}^n h(x_i)\\) converges to the expectation \\(\\text{E}_{F}(h(x))\\).\n\n\nHence, for large sample size \\(n\\) we can approximate cross-entropy and as a result the KL divergence. The cross-entropy \\(H(F, P_{\\boldsymbol \\theta})\\) is approximated by the empirical cross-entropy where the expectation is taken with regard to \\(\\hat{F}_n\\) rather than \\(F\\): \\[\n\\begin{split}\nH(F, P_{\\boldsymbol \\theta}) & \\approx H(\\hat{F}_n, P_{\\boldsymbol \\theta}) \\\\\n                  & = - \\text{E}_{\\hat{F}_n} (\\log p(x|\\boldsymbol \\theta))  \\\\\n                  & = -\\frac{1}{n} \\sum_{i=1}^n \\log p(x_i | \\boldsymbol \\theta) \\\\\n                  & = -\\frac{1}{n} l_n ({\\boldsymbol \\theta}| D)\n\\end{split}\n\\] The empirical cross-entropy is equal to the negative log-likelihood standardised by the sample size \\(n\\). Conversely, the log-likelihood is the negative empirical cross-entropy multiplied by sample size \\(n\\).\n\n\nMinimum KL divergence and maximum likelihood\nIf we knew \\(F\\) we would simply minimise \\(D_{\\text{KL}}(F, P_{\\boldsymbol \\theta})\\) to find the particular model \\(P_{\\boldsymbol \\theta}\\) that is closest to the true model, or equivalent, we would minimise the cross-entropy \\(H(F, P_{\\boldsymbol \\theta})\\). However, since we actually don’t know \\(F\\) this is not possible.\nHowever, for large sample size \\(n\\) when the empirical distribution \\(\\hat{F}_n\\) is a good approximation for \\(F\\), we can use the results from the previous section. Thus, instead of minimising the KL divergence \\(D_{\\text{KL}}(F, P_{\\boldsymbol \\theta})\\) we simply minimise \\(H(\\hat{F}_n, P_{\\boldsymbol \\theta})\\) which is the same as maximising the log-likelihood \\(l_n ({\\boldsymbol \\theta}| D)\\).\nConversely, this implies that maximising the likelihood with regard to the \\(\\boldsymbol \\theta\\) is equivalent ( asymptotically for large \\(n\\)!) to minimising the KL divergence of the approximating model and the unknown true model.\n\\[\n\\begin{split}\n\\hat{\\boldsymbol \\theta}_{ML} &= \\underset{\\boldsymbol \\theta}{\\arg \\max}\\,\\, l_n(\\boldsymbol \\theta| D) \\\\\n&= \\underset{\\boldsymbol \\theta}{\\arg \\min}\\,\\, H(\\hat{F}_n, P_{\\boldsymbol \\theta}) \\\\\n&\\approx \\underset{\\boldsymbol \\theta}{\\arg \\min}\\,\\, D_{\\text{KL}}(F, P_{\\boldsymbol \\theta}) \\\\\n\\end{split}\n\\]\nTherefore, the reasoning behind the method of maximum likelihood is that it minimises a large sample approximation of the KL divergence of the candidate model \\(P_{\\boldsymbol \\theta}\\) from the unkown true model \\(F\\). In other words, maximum likelihood estimators are minimum empirical KL divergence estimators.\nAs the KL divergence is a functional of the true distribution \\(F\\) maximum likelihood provides empirical estimators for parametric models.\nAs a consequence of the close link of maximum likelihood and KL divergence maximum likelihood inherits for large \\(n\\) (and only then!) all the optimality properties from KL divergence.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Principle of maximum likelihood</span>"
    ]
  },
  {
    "objectID": "07-likelihood1.html#properties-of-maximum-likelihood-estimation",
    "href": "07-likelihood1.html#properties-of-maximum-likelihood-estimation",
    "title": "7  Principle of maximum likelihood",
    "section": "7.3 Properties of maximum likelihood estimation",
    "text": "7.3 Properties of maximum likelihood estimation\n\nConsistency of maximum likelihood estimates\nOne important property of the method of maximum likelihood is that in general it produces consistent estimates. This means that estimates are well behaved so that they become more accurate with more data and in the limit of infinite data converge to the true parameters.\nSpecifically, if the true underlying model \\(F_{\\text{true}}\\) is contained in the set of specified candidates models \\(P_{\\boldsymbol \\theta}\\) \\[\n\\underbrace{F_{\\text{true}}}_{\\text{true model}} \\subset \\underbrace{P_{\\boldsymbol \\theta}}_{\\text{specified models}}\n\\] so that there is a parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) for which \\(F_{\\text{true}} = P_{\\boldsymbol \\theta_{\\text{true}}}\\), then \\[\\hat{\\boldsymbol \\theta}_{ML} \\overset{\\text{large }n}{\\longrightarrow} \\boldsymbol \\theta_{\\text{true}}\n\\]\nThis is a consequence of \\(D_{\\text{KL}}(F_{\\text{true}},P_{\\boldsymbol \\theta})\\rightarrow 0\\) for \\(P_{\\boldsymbol \\theta} \\rightarrow F_{\\text{true}}\\), and that maximisation of the likelihood function is for large \\(n\\) equivalent to minimising the KL divergence.\nThus given sufficient data the maximum likelihood estimate of the parameters of the model will converge to the true value of the parameters. Note that this also assumes that the model and in particular the number of parameters is fixed. As a consequence of consistency, maximum likelihood estimates are asympotically unbiased. As we will see in the examples they can still be biased in finite samples.\nNote that even if the candidate model family \\(P_{\\boldsymbol \\theta}\\) is misspecified (so that it does not contain the actual true model), the maximum likelihood estimate is still optimal in the sense in that it will identify the model in the model family that is closest in terms of empirical KL divergence.\nFinally, it is possible to find inconsistent maximum likelihood estimates, but this occurs only in situations when the MLE lies at a boundary or when there are singularities in the likelihood function (in both cases the models are not regular). Furthermore, models are inconsistent by construction when the number of parameters grows with sample size (e.g. in the famous Neyman-Scott paradox) as the data available per parameter does not decrease.\n\n\nInvariance property of the maximum likelihood\nThe maximum likelihood invariance principle states that the achieved maximum likelihood is invariant against reparameterisation of the model parameters. This property is shared by the KL divergence minimisation procedure as the achieved minimum KL divergence is also invariant against the change of parameters.\nRecall that the model parameter is just an arbitrary label to index a specific distribution within a distribution family, and changing that label does not affect the maximum (likelihood) or the minimum (KL divergence). For example, consider a function \\(h_x(x)\\) with a maximum at \\(x_{\\max} = \\text{arg max } h_x(x)\\). Now we relabel the argument using \\(y = g(x)\\) where \\(g\\) is an invertible function. Then the function in terms of \\(y\\) is \\(h_y(y) = h_x( g^{-1}(y))\\). and clearly this function has a maximum at \\(y_{\\max} =  g(x_{\\max})\\) since \\(h_y(y_{\\max}) = h_x(g^{-1}(y_{\\max} ) ) = h_x( x_{\\max} )\\). Furthermore, the achieved maximum value is the same.\nIn application to maximum likelihood, assume we transform a parameter \\(\\theta\\) into another parameter \\(\\omega\\) using some invertible function \\(g()\\) so that \\(\\omega= g(\\theta)\\). Then the maximum likelihood estimate \\(\\hat{\\omega}_{ML}\\) of the new parameter \\(\\omega\\) is simply the transformation of the maximum likelihood estimate \\(\\hat{\\theta}_{ML}\\) of the original parameter \\(\\theta\\) with \\(\\hat{\\omega}_{ML}= g( \\hat{\\theta}_{ML})\\). The achieved maximum likelihood is the same in both cases.\nThe invariance property can be very useful in practise because it is often easier (and sometimes numerically more stable) to maximise the likelihood for a different set of parameters.\nSee Worksheet L1 for an example application of the invariance principle.\n\n\nSufficient statistics\nAnother important concept are so-called sufficient statistics to summarise the information available in the data about a parameter in a model.\nA statistic \\(\\boldsymbol t(D)\\) is called a sufficient statistic for the model parameters \\(\\boldsymbol \\theta\\) if the corresponding likelihood function can be written using only \\(\\boldsymbol t(D)\\) in the terms that involve \\(\\boldsymbol \\theta\\) such that \\[\nL(\\boldsymbol \\theta| D) = h( \\boldsymbol t(D) , \\boldsymbol \\theta) \\, k(D) \\,,\n\\] where \\(h()\\) and \\(k()\\) are positive-valued functions. This is known as the Fisher-Pearson factorisation. Equivalently on log-scale this becomes \\[\nl_n(\\boldsymbol \\theta| D) = \\log h( \\boldsymbol t(D) , \\boldsymbol \\theta) + \\log k(D) \\,.\n\\]\nBy construction, estimation and inference about \\(\\boldsymbol \\theta\\) based on the factorised likelihood \\(L(\\boldsymbol \\theta)\\) is mediated through the sufficient statistic \\(\\boldsymbol t(D)\\) and does not require knowledge of the original data \\(D\\). Instead, the sufficient statistic \\(\\boldsymbol t(D)\\) contains all the information in \\(D\\) required to learn about the parameter \\(\\boldsymbol \\theta\\).\nNote that a sufficient statistic always exists since the data \\(D\\) are themselves sufficient statistics, with \\(\\boldsymbol t(D) = D\\). However, in practise one aims to find sufficient statistics that summarise the data \\(D\\) and hence provide data reduction. This will become clear in the examples below.\nFurthermore, sufficient statistics are not unique since applying a one-to-one transformation to \\(\\boldsymbol t(D)\\) yields another sufficient statistic.\nTherefore, if the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\) of \\(\\boldsymbol \\theta\\) exists and is unique then the MLE is a unique function of the sufficient statistic \\(\\boldsymbol t(D)\\). If the MLE is not unique then it can be chosen to be function of \\(\\boldsymbol t(D)\\).",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Principle of maximum likelihood</span>"
    ]
  },
  {
    "objectID": "07-likelihood1.html#sec-mlregular",
    "href": "07-likelihood1.html#sec-mlregular",
    "title": "7  Principle of maximum likelihood",
    "section": "7.4 Maximum likelihood estimation for regular models",
    "text": "7.4 Maximum likelihood estimation for regular models\n\nRegular models\nA regular model is one that is well-behaved and well-suited for model fitting by optimisation. In particular this requires that:\n\nthe support does not depend on the parameters,\nthe model is identifiable (in particular the model is not overparameterised and has a minimal set of parameters),\nthe density/probability mass function and hence the log-likelihood function is twice differentiable everywhere with regard to the parameters,\nthe maximum (peak) of the likelihood function lies inside the parameter space and not at a boundary,\nthe second derivative of the log-likelihood at the maximum is negative and not zero (for multiple parameters: the Hessian matrix at the maximum is negative definite and not singular)\n\nMost models considered in this course are regular.\n\n\nMaximum likelihood estimation in regular models\nFor a regular model maximum likelihood estimation and the necessary optimisation is greatly simplified by being able to using gradient and curvature information.\nIn order to maximise \\(l_n(\\boldsymbol \\theta|D)\\) one may use the score function \\(\\boldsymbol S(\\boldsymbol \\theta)\\) which is the first derivative of the log-likelihood function with regard to the parameters 2\n\\[\\begin{align*}\n\\begin{array}{cc}\nS_n(\\theta) = \\frac{d l_n(\\theta|D)}{d \\theta}\\\\\n\\\\\n\\\\\n\\boldsymbol S_n(\\boldsymbol \\theta)=\\nabla l_n(\\boldsymbol \\theta|D)\\\\\n\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{scalar parameter $\\theta$: first derivative}\\\\\n\\text{of log-likelihood function}\\\\\n\\\\\n\\text{gradient if } \\boldsymbol \\theta\\text{ is a vector}\\\\\n\\text{(i.e. if there's more than one parameter)}\\\\\n\\end{array}\n\\end{align*}\\]\nIn this case a necessary (but not sufficient) condition for the MLE is that \\[\n\\boldsymbol S_n(\\hat{\\boldsymbol \\theta}_{ML}) = 0\n\\]\nTo demonstrate that the log-likelihood function actually achieves a maximum at \\(\\hat{\\boldsymbol \\theta}_{ML}\\) the curvature at the MLE must negative, i.e. that the log-likelihood must be locally concave at the MLE.\nIn the case of a single parameter (scalar \\(\\theta\\)) this requires to check that the second derivative of the log-likelihood function with regard to the parameter is negative: \\[\n\\frac{d^2 l_n(\\hat{\\theta}_{ML}| D)}{d \\theta^2} &lt;0\n\\] In the case of a parameter vector (multivariate \\(\\boldsymbol \\theta\\)) you need to compute the Hessian matrix (matrix of second order derivatives) at the MLE: \\[\n\\nabla \\nabla^T l_n(\\hat{\\boldsymbol \\theta}_{ML}| D)\n\\] and then verify that this matrix is negative definite (i.e. all its eigenvalues must be negative). For multivariate parameter vector \\(\\boldsymbol \\theta\\) of dimension \\(d\\) the Hessian is a matrix of size \\(d \\times d\\).\n\n\nInvariance of score function and second derivative of the log-likelihood\nThe score function \\(\\boldsymbol S_n(\\boldsymbol \\theta)\\) is invariant against transformation of the sample space. Assume \\(\\boldsymbol x\\) has log-density \\(\\log f_{\\boldsymbol x}(\\boldsymbol x| \\boldsymbol \\theta)\\) then the log-density for \\(\\boldsymbol y\\) is \\[\n\\log f_{\\boldsymbol y}(\\boldsymbol y| \\boldsymbol \\theta) = \\log |\\det\\left( D\\boldsymbol x(\\boldsymbol y) \\right)| +  \\log f_{\\boldsymbol x}\\left( \\boldsymbol x(\\boldsymbol y)| \\boldsymbol \\theta\\right)\n\\] where \\(D\\boldsymbol x(\\boldsymbol y)\\) is the Jacobian matrix of the inverse transformation \\(\\boldsymbol x(\\boldsymbol y)\\). When taking the derivative of the log-likelihood function with regard to the parameter \\(\\boldsymbol \\theta\\) the first term containing the Jacobian determinant vanishes. Hence the score function \\(\\boldsymbol S_n(\\boldsymbol \\theta)\\) is not affected by a change of variables.\nAs a consequence, the second derivative of log-likelihood function with regard to \\(\\boldsymbol \\theta\\) is also invariant against transformations of the sample space.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Principle of maximum likelihood</span>"
    ]
  },
  {
    "objectID": "07-likelihood1.html#footnotes",
    "href": "07-likelihood1.html#footnotes",
    "title": "7  Principle of maximum likelihood",
    "section": "",
    "text": "Aldrich J. 1997. R. A. Fisher and the Making of Maximum Likelihood 1912–1922. Statist. Sci. 12:162–176. https://doi.org/10.1214/ss/1030037906↩︎\nThe score function \\(\\boldsymbol S_n(\\boldsymbol \\theta)\\) as the gradient of the log-likelihood function must not be confused with the scoring rule \\(S(x, P)\\) mentioned in the introduction to entropy and KL divergence, cf. Note 3.1.↩︎",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Principle of maximum likelihood</span>"
    ]
  },
  {
    "objectID": "08-likelihood2.html",
    "href": "08-likelihood2.html",
    "title": "8  Maximum likelihood estimation in practise",
    "section": "",
    "text": "8.1 Likelihood estimation for a single parameter\nNext, maximum likelihood estimation is illustrated on a number of examples. Among others we discuss three basic problems, namely how to estimate a proportion, the mean and the variance in the likelihood framework. We also consider an example of non-regular model (Example 8.4).\nIn the following we illustrate likelihood estimation for models with a single parameter. In this case the score function and the second derivative of the log-likelihood are all scalar-valued like the log-likelihood function itself.\nNote that to analyse the coin tossing experiment and to estimate \\(\\theta\\) we may equally well use the binomial distribution \\(\\text{Bin}(n, \\theta)\\) as model for the number of successes. This results in the same MLE for \\(\\theta\\) but the likelihood function based on the binomial PMF includes the binomial coefficient. However, as it does not depend on \\(\\theta\\) it disappears in the score function and has no influence in the derivation of the MLE.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum likelihood estimation in practise</span>"
    ]
  },
  {
    "objectID": "08-likelihood2.html#likelihood-estimation-for-a-single-parameter",
    "href": "08-likelihood2.html#likelihood-estimation-for-a-single-parameter",
    "title": "8  Maximum likelihood estimation in practise",
    "section": "",
    "text": "Example 8.1 Maximum likelihood estimation for the Bernoulli model:\nWe aim to estimate the true proportion \\(\\theta\\) in a Bernoulli experiment with binary outcomes, say the proportion of “successes” vs. “failures” or of “heads” vs. “tails” in a coin tossing experiment.\n\nBernoulli model \\(\\text{Ber}(\\theta)\\): \\(\\text{Pr}(\\text{\"success\"}) = \\theta\\) and \\(\\text{Pr}(\\text{\"failure\"}) = 1-\\theta\\).\nThe “success” is indicated by outcome \\(x=1\\) and the “failure” by \\(x=0\\).\nWe conduct \\(n\\) trials and record \\(n_1\\) successes and \\(n-n_1\\) failures.\nParameter: \\(\\theta\\) probability of “success”.\n\nWhat is the MLE of \\(\\theta\\)?\n\nthe observations \\(D=\\{x_1, \\ldots, x_n\\}\\) take on values 0 or 1.\nthe average of the data points is \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\frac{n_1}{n}\\).\nthe probability mass function (PMF) of the Bernoulli distribution \\(\\text{Ber}(\\theta)\\) is: \\[\np(x| \\theta) = \\theta^x (1-\\theta)^{1-x} =\n\\begin{cases}\n\\theta &  \\text{if $x=1$ }\\\\\n1-\\theta & \\text{if $x=0$} \\\\\n\\end{cases}\n\\]\nlog-PMF: \\[\n\\log p(x| \\theta) =  x \\log(\\theta) + (1-x) \\log(1 - \\theta)\n\\]\nlog-likelihood function: \\[\n\\begin{split}\nl_n(\\theta| D) & = \\sum_{i=1}^n \\log p(x_i| \\theta) \\\\\n    & = n_1 \\log \\theta + (n-n_1) \\log(1-\\theta) \\\\\n    & = n \\left( \\bar{x} \\log \\theta + (1-\\bar{x}) \\log(1-\\theta) \\right) \\\\\n\\end{split}\n\\] Note that the log-likelihood depends on the data only via \\(\\bar{x}\\). Thus, \\(t(D) = \\bar{x}\\) is a sufficient statistic for the parameter \\(\\theta\\). In fact it is also a minimally sufficient statistic as will be discussed in more detail later.\nScore function: \\[\nS_n(\\theta)=  \\frac{dl_n(\\theta| D)}{d\\theta}= n \\left( \\frac{\\bar{x}}{\\theta}-\\frac{1-\\bar{x}}{1-\\theta} \\right)\n\\]\nMaximum likelihood estimate: Setting \\(S_n(\\hat{\\theta}_{ML})=0\\) yields as solution \\[\n\\hat{\\theta}_{ML} = \\bar{x} = \\frac{n_1}{n}\n\\]\nWith \\(\\frac{dS_n(\\theta)}{d\\theta} = -n \\left( \\frac{\\bar{x}}{\\theta^2} + \\frac{1-\\bar{x}}{(1-\\theta)^2} \\right) &lt;0\\) the optimum corresponds indeed to the maximum of the (log-)likelihood function as this is negative for \\(\\hat{\\theta}_{ML}\\) (and indeed for any \\(\\theta\\)).\nThe maximum likelihood estimator of \\(\\theta\\) is therefore identical to the frequency of the successes among all observations.\n\n\n\n\nExample 8.2 Maximum likelihood estimation for the normal distribution with unknown mean and known variance:\n\n\\(x \\sim N(\\mu,\\sigma^2)\\) with \\(\\text{E}(x)=\\mu\\) and \\(\\text{Var}(x) = \\sigma^2\\)\nthe parameter to be estimated is \\(\\mu\\) whereas \\(\\sigma^2\\) is known.\n\nWhat’s the MLE of the parameter \\(\\mu\\)?\n\nthe data \\(D= \\{x_1, \\ldots, x_n\\}\\) are all real in the range \\(x_i \\in [-\\infty, \\infty]\\).\nthe average \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) is real as well.\nDensity: \\[ p(x| \\mu)=\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]\nLog-Density: \\[\\log p(x| \\mu) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\]\nLog-likelihood function: \\[\n\\begin{split}\nl_n(\\mu| D) &= \\sum_{i=1}^n \\log p(x_i| \\mu)\\\\\n&=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2}\\log(2 \\pi \\sigma^2) }_{\\text{constant term, does not depend on } \\mu \\text{, can be removed}}\\\\\n&=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i^2 - 2 x_i \\mu+\\mu^2)  + \\text{ const.}\\\\\n&=\\frac{n}{\\sigma^2}  ( \\bar{x} \\mu  - \\frac{1}{2}\\mu^2)  \\underbrace{ - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n   x_i^2 }_{\\text{another constant term}}   + \\text{ const.}\\\\\n\\end{split}\n\\] Note how the non-constant terms of the log-likelihood depend on the data only through \\(\\bar{x}\\). Hence \\(t(D) =\\bar{x}\\) this is a sufficient statistic for \\(\\mu\\).\nScore function: \\[\nS_n(\\mu) =\n\\frac{n}{\\sigma^2} ( \\bar{x}- \\mu)\n\\]\nMaximum likelihood estimate: \\[S_n(\\hat{\\mu}_{ML})=0 \\Rightarrow \\hat{\\mu}_{ML} = \\bar{x}\\]\nWith \\(\\frac{dS_n(\\mu)}{d\\mu} = -\\frac{n}{\\sigma^2}&lt;0\\) the optimum is indeed the maximum\n\nThe constant term in the log-likelihood function collects all terms that do not depend on the parameter of interest. After taking the first derivative with regard to the parameter the constant term disappears thus it has no influence in maximum likelhood estimation. Therefore constant terms can be dropped from the log-likelihood function.\n\n\nExample 8.3 Maximum likelihood estimation for the normal distribution with known mean and unknown variance:\n\n\\(x \\sim N(\\mu,\\sigma^2)\\) with \\(\\text{E}(x)=\\mu\\) and \\(\\text{Var}(x) = \\sigma^2\\)\n\\(\\sigma^2\\) needs to be estimated whereas the mean \\(\\mu\\) is known\n\nWhat’s the MLE of \\(\\sigma^2\\)?\n\nthe data \\(D= \\{x_1, \\ldots, x_n\\}\\) are all real in the range \\(x_i \\in [-\\infty, \\infty]\\).\nthe average of the squared centred data \\(\\overline{(x-\\mu)^2} = \\frac{1}{n} \\sum_{i=1}^n (x_i-\\mu)^2 \\geq 0\\) is non-negative.\nDensity: \\[ p(x| \\sigma^2)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]\nLog-Density: \\[\\log p(x | \\sigma^2) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\]\nLog-likelihood function: \\[\n\\begin{split}\nl_n(\\sigma^2 | D) & = l_n(\\mu, \\sigma^2 | D) = \\sum_{i=1}^n \\log p(x_i| \\sigma^2)\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2} \\log(2 \\pi) }_{\\text{constant not depending on } \\sigma^2}\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{n}{2\\sigma^2}  \\overline{(x-\\mu)^2}  + C\\\\\n\\end{split}\n\\] Note how the log-likelihood function depends on the data only through \\(\\overline{(x-\\mu)^2}\\). Hence \\(t(D) = \\overline{(x-\\mu)^2}\\) is a sufficient statistic for \\(\\sigma^2\\).\nScore function: \\[\nS_n(\\sigma^2) =\n-\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4}    \\overline{(x-\\mu)^2}\n\\]\nNote that to obtain the score function the derivative needs to be taken with regard to the variance parameter \\(\\sigma^2\\) — not with regard to \\(\\sigma\\)! As a trick, relabel \\(\\sigma^2 = v\\) in the log-likelihood function, then take the derivative with regard to \\(v\\), then backsubstitute \\(v=\\sigma^2\\) in the final result.\nMaximum likelihood estimate: \\[\nS_n(\\widehat{\\sigma^2}_{ML})=0 \\Rightarrow\n\\] \\[\n\\widehat{\\sigma^2}_{ML}\n=\\overline{(x-\\mu)^2} = \\frac{1}{n}\\sum_{i=1}^n (x_i-\\mu)^2\n\\]\nTo confirm that we actually have maximum we need to verify that the second derivative of log-likelihood at the optimum is negative. With \\(\\frac{dS_n(\\sigma^2)}{d\\sigma^2} =\n-\\frac{n}{2\\sigma^4} \\left(\\frac{2}{\\sigma^2}  \\overline{(x-\\mu)^2} -1\\right)\\) and hence \\(\\frac{dS_n( \\widehat{\\sigma^2}_{ML}  )}{d\\sigma^2} =\n-\\frac{n}{2} \\left(\\widehat{\\sigma^2}_{ML} \\right)^{-2}&lt;0\\) the optimum is indeed the maximum.\n\n\n\nExample 8.4 Uniform distribution with upper bound \\(\\theta\\):\nThis is an example of a non-regular model, as the parameter \\(\\theta\\) determines the support of the model.\n\n\\(x \\sim U(0,\\theta)\\) with \\(\\theta &gt; 0\\)\nthe data \\(D= \\{x_1, \\ldots, x_n\\}\\) are all real in the range \\(x_i \\in [0, \\theta]\\).\nby \\(x_{[i]}\\) we denote the ordered observations with \\(0 \\leq x_{[1]} &lt; x_{[2]} &lt; \\ldots &lt; x_{[n]} \\leq \\theta\\) with \\(x_{[n]} = \\max(x_1,\\dots,x_n)\\).\n\nWe would like to obtain the maximum likelihood estimator \\(\\hat{\\theta}_{ML}\\).\n\nThe probability density function of \\(U(0,\\theta)\\) is \\[p(x|\\theta) =\\begin{cases}\n  \\frac{1}{\\theta} &\\text{if } x \\in [0,\\theta] \\\\\n  0              & \\text{otherwise.}\n\\end{cases}\n\\]\nthe corresponding the log-density is \\[\n\\log p(x|\\theta) =\\begin{cases}\n  - \\log \\theta &\\text{if } x \\in [0,\\theta] \\\\\n  - \\infty              & \\text{otherwise.}\n\\end{cases}\n\\]\nthe log-likelihood function is \\[\nl_n(\\theta| D) =\\begin{cases}\n-n\\log \\theta  &\\text{for } \\theta \\geq  x_{[n]}\\\\\n- \\infty       & \\text{otherwise}\n\\end{cases}\n\\] since all observed data \\(D =\\{x_1, \\ldots, x_n\\}\\) lie in the interval \\([0,\\theta]\\). Note that the log-likelihood is a function of \\(x_{[n]}\\) only so this single data point is the sufficient statistic \\(t(D) =x_{[n]}\\).\nthe log-likelihood function remains at value \\(-\\infty\\) until \\(\\theta = x_{[n]}\\), where it jumps to \\(-n\\log x_{[n]}\\) and then it decreases monotonically with increasing \\(\\theta &gt; x_{[n]}\\). Hence the log-likelihood function has a maximum at \\(\\hat{\\theta}_{ML}=x_{[n]}\\).\nDue to the discontinuity at \\(x_{[n]}\\) the log-likelihood \\(l_n(\\theta| D)\\) is not differentiable at \\(\\hat{\\theta}_{ML}\\). and hence the maximum cannot be found by setting the score function equal to zero as in a regular model.\nIn addition, there is no quadratic approximation around \\(\\hat{\\theta}_{ML}\\) and therefore the observed Fisher information cannot be computed either.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum likelihood estimation in practise</span>"
    ]
  },
  {
    "objectID": "08-likelihood2.html#likelihood-estimation-for-multiple-parameters",
    "href": "08-likelihood2.html#likelihood-estimation-for-multiple-parameters",
    "title": "8  Maximum likelihood estimation in practise",
    "section": "8.2 Likelihood estimation for multiple parameters",
    "text": "8.2 Likelihood estimation for multiple parameters\nIf there are several parameters likelihood estimation is conceptually no different from the case of a single parameter. However, the score function is now vector-valued and the second derivative of the log-likelihood is a matrix-valued function.\n\nExample 8.5 Normal distribution with mean and variance both unknown:\n\n\\(x \\sim N(\\mu,\\sigma^2)\\) with \\(\\text{E}(x)=\\mu\\) and \\(\\text{Var}(x) = \\sigma^2\\)\nboth \\(\\mu\\) and \\(\\sigma^2\\) need to be estimated.\n\nWhat’s the MLE of the parameter vector \\(\\boldsymbol \\theta= (\\mu,\\sigma^2)^T\\)?\n\nthe data \\(D= \\{x_1, \\ldots, x_n\\}\\) are all real in the range \\(x_i \\in [-\\infty, \\infty]\\).\nthe average \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) is real as well.\nthe average of the squared data \\(\\overline{x^2} = \\frac{1}{n} \\sum_{i=1}^n x_i^2 \\geq 0\\) is non-negative.\nDensity: \\[ f(x| \\mu, \\sigma^2)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]\nLog-Density: \\[\\log f(x | \\mu, \\sigma^2) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\]\nLog-likelihood function: \\[\n\\begin{split}\nl_n(\\boldsymbol \\theta| D) & = l_n(\\mu, \\sigma^2 | D) = \\sum_{i=1}^n \\log f(x_i| \\mu, \\sigma^2)\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2} \\log(2 \\pi) }_{\\text{constant not depending on }\\mu \\text{ or } \\sigma^2}\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{n}{2\\sigma^2}  ( \\overline{x^2} -2 \\bar{x} \\mu + \\mu^2)  + C\\\\\n\\end{split}\n\\] Note how the log-likelihood function depends on the data only through \\(\\bar{x}\\) and \\(\\overline{x^2}\\). Hence, \\(\\boldsymbol t(D) = (\\bar{x},  \\overline{x^2})^T\\) are sufficient statistics for \\(\\boldsymbol \\theta\\).\nScore function \\(\\boldsymbol S_n\\), gradient of \\(l_n(\\boldsymbol \\theta| D)\\): \\[\n\\begin{split}\n\\boldsymbol S_n(\\boldsymbol \\theta) &= \\boldsymbol S_n(\\mu,\\sigma^2)  \\\\\n& =\\nabla l_n(\\mu, \\sigma^2| D) \\\\\n&=\n\\begin{pmatrix}\n\\frac{n}{\\sigma^2} (\\bar{x}-\\mu) \\\\\n-\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4}   \\left( \\overline{x^2} - 2\\bar{x} \\mu +\\mu^2 \\right)  \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\]\nMaximum likelihood estimate: \\[\n\\boldsymbol S_n(\\hat{\\boldsymbol \\theta}_{ML})=0 \\Rightarrow\n\\] \\[\n\\hat{\\boldsymbol \\theta}_{ML}=\n\\begin{pmatrix}\n\\hat{\\mu}_{ML}  \\\\\n\\widehat{\\sigma^2}_{ML} \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\bar{x} \\\\\n\\overline{x^2} -\\bar{x}^2\\\\\n\\end{pmatrix}\n\\] The ML estimate of the variance can also be written \\(\\widehat{\\sigma^2}_{ML} = \\overline{x^2} -\\bar{x}^2 =\\overline{(x-\\bar{x})^2}  =\n\\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})^2\\).\nTo confirm that we actually have a maximum we need to verify that the eigenvalues of the Hessian matrix at the optimum are all negative. This is indeed the case, for details see Example 9.4.\n\n\n\nExample 8.6 \\(\\color{Red} \\blacktriangleright\\) Maximum likelihood estimates of the parameters of the multivariate normal distribution:\nThe results from Example 8.5 can be generalised to the multivariate normal distribution:\n\n\\(\\boldsymbol x\\sim N(\\boldsymbol \\mu,\\boldsymbol \\Sigma)\\) with \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu\\) and \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\)\nboth \\(\\boldsymbol \\mu\\) and \\(\\boldsymbol \\Sigma\\) need to be estimated.\n\nWith\n\nthe data \\(D= \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) containing real vector-valued observations,\n\nthe maximum likelihood can be written as follows:\nMLE for the mean: \\[\n\\hat{\\boldsymbol \\mu}_{ML} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k = \\bar{\\boldsymbol x}\n\\]\nMLE for the covariance: \\[\n\\underbrace{\\widehat{\\boldsymbol \\Sigma}_{ML}}_{d \\times d} = \\frac{1}{n}\\sum^{n}_{k=1} \\underbrace{\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)}_{d \\times 1} \\; \\underbrace{\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)^T}_{1 \\times d}\\] Note the factor \\(\\frac{1}{n}\\) in the estimator of the covariance matrix.\nWith \\(\\overline{\\boldsymbol x\\boldsymbol x^T} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k \\boldsymbol x_k^T\\) we can also write \\[\n\\widehat{\\boldsymbol \\Sigma}_{ML} = \\overline{\\boldsymbol x\\boldsymbol x^T} - \\bar{\\boldsymbol x} \\bar{\\boldsymbol x}^T\n\\]\nHence, the MLEs correspond to the well-known empirical estimates.\nThe derivation of the MLEs is discussed in more detail in the module MATH38161 Multivariate Statistics and Machine Learning.\n\n\nExample 8.7 \\(\\color{Red} \\blacktriangleright\\) Maximum likelihood estimation of the parameters of the categorical distribution:\nMaximum likelihood estimation of the parameters of \\(\\text{Cat}(\\boldsymbol \\pi)\\) at first seems a trivial extension of the Bernoulli model (cf. Example 8.1) but this a bit more complicated because of the constraint on the allowed values of \\(\\boldsymbol \\pi\\) so there are only \\(K-1\\) free parameters and not \\(K\\). Hence we either need to optimise with regard to a specific set of \\(K-1\\) parameters (which is what we do below) or use a constrained optimisation procedure to enforce that \\(\\sum_{k=1}^K \\pi_k = 1\\) (e.g using Lagrange multipliers).\n\nThe data: We observe \\(n\\) samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\). The data matrix of dimension \\(n \\times K\\) is \\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)^T = (x_{ik})\\). It contains each \\(\\boldsymbol x_i = (x_{i1}, \\ldots, x_{iK})^T\\). The corresponding summary (minimal sufficient) statistics are \\(\\boldsymbol t(D) = \\bar{\\boldsymbol x} = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol x_i = (\\bar{x}_1, \\ldots, \\bar{x}_K)^T\\) with \\(\\bar{x}_k = \\frac{1}{n} \\sum_{i=1}^n x_{ik}\\). We can also write \\(\\bar{x}_{K} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k}\\). The number of samples for class \\(k\\) is \\(n_k = n \\bar{x}_k\\) with \\(\\sum_{k=1}^K n_k = n\\).\nThe log-likelihood is \\[\n\\begin{split}\nl_n(\\pi_1, \\ldots, \\pi_{K-1}) & = \\sum_{i=1}^n \\log f(\\boldsymbol x_i) \\\\\n& =\\sum_{i=1}^n \\left( \\sum_{k=1}^{K-1}  x_{ik} \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_{ik}  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\right)\\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right) \\log\\left(1 - \\sum_{k=1}^{K-1} \\pi_k\\right) \\right) \\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\bar{x}_K \\log \\pi_K \\right) \\\\  \n\\end{split}\n\\]\nScore function (gradient) \\[\n\\begin{split}\n\\boldsymbol S_n(\\pi_1, \\ldots, \\pi_{K-1}) &=  \\nabla l_n(\\pi_1, \\ldots, \\pi_{K-1} ) \\\\\n& =\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial \\pi_1} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\vdots\\\\\n\\frac{\\partial}{\\partial \\pi_{K-1}} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\end{pmatrix}\\\\\n& = n\n\\begin{pmatrix}\n\\frac{\\bar{x}_1}{\\pi_1}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\vdots\\\\\n\\frac{\\bar{x}_{K-1}}{\\pi_{K-1}}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\] Note in particular the need for the second term that arises because \\(\\pi_K\\) depends on all the \\(\\pi_1, \\ldots, \\pi_{K-1}\\).\nMaximum likelihood estimate: Setting \\(\\boldsymbol S_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML})=0\\) yields \\(K-1\\) equations \\[\n\\bar{x}_i \\left(1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML}\\right)  = \\hat{\\pi}_i^{ML} \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right)\n\\] for \\(i=1, \\ldots, K-1\\) and with solution \\[\n\\hat{\\pi}_i^{ML} = \\bar{x}_i\n\\] It also follows that \\[\n\\hat{\\pi}_K^{ML} = 1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} = \\bar{x}_K\n\\] The maximum likelihood estimator is therefore the frequency of the occurrence of a class among the \\(n\\) samples.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum likelihood estimation in practise</span>"
    ]
  },
  {
    "objectID": "08-likelihood2.html#further-properties-of-ml",
    "href": "08-likelihood2.html#further-properties-of-ml",
    "title": "8  Maximum likelihood estimation in practise",
    "section": "8.3 Further properties of ML",
    "text": "8.3 Further properties of ML\n\nRelationship of maximum likelihood with least squares estimation\nIn Example 8.2 the form of the log-likelihood function is a function of the sum of squared differences. Maximising \\(l_n(\\mu| D) =-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\) is equivalent to minimising \\(\\sum_{i=1}^n(x_i-\\mu)^2\\). Hence, finding the mean by maximum likelihood assuming a normal model is equivalent to least-squares estimation!\nNote that least-squares estimation has been in use at least since the early 1800s 1 and thus predates maximum likelihood (1922). Due to its simplicity it is still very popular in particular in regression and the link with maximum likelihood and normality allows to understand why it usually works well.\nSee also Example 3.4 and Example 4.5 for further links of the normal distribution with squared error.\n\n\nBias of maximum likelihood estimates\nExample 8.5 is interesting because it shows that maximum likelihood can result in both biased and as well as unbiased estimators.\nRecall that \\(x \\sim N(\\mu, \\sigma^2)\\). As a result \\[\\hat{\\mu}_{ML}=\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\] with \\(\\text{E}( \\hat{\\mu}_{ML} ) = \\mu\\) and \\[\n\\widehat{\\sigma^2}_{\\text{ML}} \\sim\nW_1\\left(s^2 = \\frac{\\sigma^2}{n}, k=n-1\\right)\n\\] (see Section A.8) with mean \\(\\text{E}(\\widehat{\\sigma^2}_{ML}) = \\frac{n-1}{n} \\, \\sigma^2\\).\nTherefore, the MLE of \\(\\mu\\) is unbiased as\n\\[\n\\text{Bias}(\\hat{\\mu}_{ML}) = \\text{E}( \\hat{\\mu}_{ML} ) - \\mu = 0\n\\] In contrast, however, the MLE of \\(\\sigma^2\\) is negatively biased because \\[\n\\text{Bias}(\\widehat{\\sigma^2}_{ML}) = \\text{E}( \\widehat{\\sigma^2}_{ML} ) - \\sigma^2 = -\\frac{1}{n} \\, \\sigma^2\n\\]\nThus, in the case of the variance parameter of the normal distribution the MLE is not recovering the well-known unbiased estimator of the variance\n\\[\n\\widehat{\\sigma^2}_{UB} = \\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2 = \\frac{n}{n-1} \\widehat{\\sigma^2}_{ML}\n\\] In other words, the unbiased variance estimate is not a maximum likelihood estimate!\nTherefore it is worth keeping in mind that maximum likelihood can result in biased estimates for finite \\(n\\). For large \\(n\\), however, the bias disappears as MLEs are consistent.\n\n\n\\(\\color{Red} \\blacktriangleright\\) Minimal sufficient statistics and maximal data reduction\nIn all the examples discussed above the sufficient statistic was typically either \\(\\bar{x}\\) and \\(\\overline{x^2}\\) (or both). This is not a coincidence since all of the examples are exponential families with canonical statistics \\(x\\) and \\(x^2\\), and in exponential families a sufficient statistic can be obtained as the average of the canonical statistics.\nCrucially, in the above examples the identified sufficient statistics are also minimal sufficient statistics where the dimension of sufficient statistic is equal to the dimension of the parameter vector, and as such as low as possible. Minimal sufficient statistics provide maximal data reduction as will be discussed later.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum likelihood estimation in practise</span>"
    ]
  },
  {
    "objectID": "08-likelihood2.html#footnotes",
    "href": "08-likelihood2.html#footnotes",
    "title": "8  Maximum likelihood estimation in practise",
    "section": "",
    "text": "Stigler, S. M. 1981. Gauss and the invention of least squares. Ann. Statist. 9:465–474. https://doi.org/10.1214/aos/1176345451↩︎",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum likelihood estimation in practise</span>"
    ]
  },
  {
    "objectID": "09-likelihood3.html",
    "href": "09-likelihood3.html",
    "title": "9  Observed Fisher information",
    "section": "",
    "text": "9.1 Definition of the observed Fisher information\nFigure 9.1: Flat and sharp log-likelihood function.\nVisual inspection of the log-likelihood function (e.g. Figure 9.1) suggests that it contains more information about the parameter \\(\\boldsymbol \\theta\\) than just the location of the maximum point at \\(\\hat{\\boldsymbol \\theta}_{ML}\\).\nIn particular, in a regular model the curvature of the log-likelihood function at the MLE seems to be related to the accuracy of \\(\\hat{\\boldsymbol \\theta}_{ML}\\): if the likelihood surface is flat near the maximum (low curvature) then if is more difficult to find the optimal parameter (also numerically). Conversely, if the likelihood surface is sharply peaked (strong curvature) then the maximum point is well defined.\nThe curvature can be quantified by the second-order derivatives (Hessian matrix) of the log-likelihood function.\nAccordingly, the observed Fisher information (matrix) is defined as the negative curvature at the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\): \\[\n{\\boldsymbol J_n}(\\hat{\\boldsymbol \\theta}_{ML}) = -\\nabla \\nabla^T l_n(\\hat{\\boldsymbol \\theta}_{ML}| D)\n\\]\nSometimes this is simply called the “observed information”. To avoid confusion with the expected Fisher information \\[\n\\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta) = -\\text{E}_{F_{\\boldsymbol \\theta}} \\left( \\nabla \\nabla^T \\log f(x|\\boldsymbol \\theta)\\right)\n\\] introduced earlier it is necessary to always use the qualifier “observed” when referring to \\({\\boldsymbol J_n}(\\hat{\\boldsymbol \\theta}_{ML})\\).\nWe will see in more detail later that the observed Fisher information plays an important role at quantifying the uncertainty of a maximum likelihood estimate.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Observed Fisher information</span>"
    ]
  },
  {
    "objectID": "09-likelihood3.html#definition-of-the-observed-fisher-information",
    "href": "09-likelihood3.html#definition-of-the-observed-fisher-information",
    "title": "9  Observed Fisher information",
    "section": "",
    "text": "Transformation properties\nAs a consequence of the invariance of the score function and curvature function the observed Fisher information is invariant against transformations of the sample space. This is the same invariance also shown by the expected Fisher information and by the KL divergence.\n\\(\\color{Red} \\blacktriangleright\\) Like the expected Fisher information the observed Fisher information (as a Hessian matrix) transforms covariantly under change of model parameters — see Section 5.1.4.\n\n\nRelationship between observed and expected Fisher information\nThe observed Fisher information \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) and the expected Fisher information \\(\\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)\\) are related but also two clearly different entities.\nCurvature based:\n\nBoth types of Fisher information are based on computing second order derivatives (Hessian matrix), thus both are based on the curvature of a function.\n\nTransformation properties:\n\nBoth quantities are invariant against changes of the parameterisation of the sample space.\n\\(\\color{Red} \\blacktriangleright\\) Both transform covariantly when changing the parameter of the distribution.\n\nData-based vs. model only:\n\nThe observed Fisher information is computed from the log-likelihood function. Therefore it takes both the model and the observed data \\(D\\) into account and explicitly depends on the sample size \\(n\\). It contains estimates of the parameters but not the parameters themselves. While the curvature of the log-likelihood function may be computed for any point of the log-likelihood function the observed Fisher information specifically refers to the curvature at the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\). It is linked to the (asymptotic) variance of the MLE (see the examples and as will be discussed in more detail later).\nIn contrast, the expected Fisher information is derived directly from the log-density of the model family. It does not depend on the observed data, and thus does not depend on sample size. It makes sense and can be computed for any value of the parameters. It describes the geometry of the space of the model family, and is the local approximation of KL information.\n\nLarge sample equivalence:\n\nAssume that for large sample size \\(n\\) the MLE converges to \\(\\hat{\\boldsymbol \\theta}_{ML} \\rightarrow \\boldsymbol \\theta_0\\). It follows from the construction of the observed Fisher information and the law of large numbers that asymptotically for large sample size \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\rightarrow n \\boldsymbol I^{\\text{Fisher}}( \\boldsymbol \\theta_0 )\\) (i.e. the expected Fisher information for a set of iid random variables, see Section 5.1.2.\n\n\\(\\color{Red} \\blacktriangleright\\) Finite sample equivalence for exponential families:\n\nIn a very important class of models, namely for exponential families, we find that \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) = n \\boldsymbol I^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) is valid also for finite sample size \\(n\\). This can be directly seen from the special instances of exponential families such as the Bernoulli distribution (Example 5.1 and Example 9.1), the normal distribution with one parameter (Example 5.3 and Example 9.2), the normal distribution with two parameters (Example 5.4 and Example 9.4) and the categorical distribution (Example 5.5 and Example 9.5).\n\n\nHowever, exponential families are an exception. In a general model \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\neq n \\boldsymbol I^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) for finite sample size \\(n\\). As an example consider the location-scale \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mu, \\tau^2)\\) with unknown median parameter \\(\\mu\\) and known scale parameter \\(\\tau^2\\) and given degree of freedom \\(\\nu\\). This is not an exponential family model (unless \\(\\nu \\rightarrow \\infty\\) when it becomes the normal distribution). It can be shown that the expected Fisher information is \\(I^{\\text{Fisher}}(\\mu )=\\frac{\\nu+1}{\\nu+3} \\frac{1}{\\tau^2}\\) but the observed Fisher information \\(J_n(\\hat{\\mu}_{ML}) \\neq n \\frac{\\nu+1}{\\nu+3} \\frac{1}{\\tau^2}\\) with a maximum likelihood estimate \\(\\hat{\\mu}_{ML}\\) that can only be computed numerically with no closed form available.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Observed Fisher information</span>"
    ]
  },
  {
    "objectID": "09-likelihood3.html#observed-fisher-information-examples",
    "href": "09-likelihood3.html#observed-fisher-information-examples",
    "title": "9  Observed Fisher information",
    "section": "9.2 Observed Fisher information examples",
    "text": "9.2 Observed Fisher information examples\n\nModels with a single parameter\n\nExample 9.1 Observed Fisher information for the Bernoulli model \\(\\text{Ber}(\\theta)\\):\nWe continue Example 8.1. The negative second derivative of the log-likelihood function is \\[\n-\\frac{d S_n(\\theta)}{d\\theta}=n \\left( \\frac{ \\bar{x} }{\\theta^2} + \\frac{1 - \\bar{x} }{(1-\\theta)^2} \\right)\n\\] The observed Fisher information is therefore \\[\n\\begin{split}\nJ_n(\\hat{\\theta}_{ML}) & = n \\left(\\frac{ \\bar{x} }{\\hat{\\theta}_{ML}^2} + \\frac{ 1 - \\bar{x} }{  (1-\\hat{\\theta}_{ML})^2  } \\right) \\\\\n  & = n \\left(\\frac{1}{\\hat{\\theta}_{ML}} + \\frac{1}{1-\\hat{\\theta}_{ML}} \\right) \\\\\n  &= \\frac{n}{\\hat{\\theta}_{ML} (1-\\hat{\\theta}_{ML})} \\\\\n\\end{split}\n\\]\nThe inverse of the observed Fisher information is: \\[J_n(\\hat{\\theta}_{ML})^{-1}=\\frac{\\hat{\\theta}_{ML}(1-\\hat{\\theta}_{ML})}{n}\\]\nCompare this with \\(\\text{Var}\\left(\\frac{x}{n}\\right) = \\frac{\\theta(1-\\theta)}{n}\\) for \\(x \\sim \\text{Bin}(n, \\theta)\\).\n\n\nExample 9.2 Observed Fisher information for the normal distribution with unknown mean and known variance:\nThis is the continuation of Example 8.2. The negative second derivative of the score function is \\[\n-\\frac{d S_n(\\mu)}{d\\mu}= \\frac{n}{\\sigma^2}\n\\] The observed Fisher information at the MLE is therefore \\[\nJ_n(\\hat{\\mu}_{ML}) = \\frac{n}{\\sigma^2}\n\\] and the inverse of the observed Fisher information is \\[\nJ_n(\\hat{\\mu}_{ML})^{-1} = \\frac{\\sigma^2}{n}\n\\]\nFor \\(x_i \\sim N(\\mu, \\sigma^2)\\) we have \\(\\text{Var}(x_i) = \\sigma^2\\) and hence \\(\\text{Var}(\\bar{x}) = \\frac{\\sigma^2}{n}\\), which is equal to the inverse observed Fisher information.\n\n\nExample 9.3 Observed Fisher information for the normal distribution with known mean and unknown variance:\nThis is the continuation of Example 8.3.\n\nCorrespondingly, the observed Fisher information is \\[\nJ_n(\\widehat{\\sigma^2}_{ML}) = \\frac{n}{2} \\left(\\widehat{\\sigma^2}_{ML} \\right)^{-2}\n\\] and its inverse is \\[\nJ_n(\\widehat{\\sigma^2}_{ML})^{-1} = \\frac{2}{n} \\left(\\widehat{\\sigma^2}_{ML} \\right)^{2}\n\\]\n\nWith \\(x_i \\sim N(\\mu, \\sigma^2)\\) the empirical variance \\(\\widehat{\\sigma^2}_{ML}\\) follows a one-dimensional Wishart distribution \\[\n\\widehat{\\sigma^2}_{\\text{ML}} \\sim\nW\\left(s^2 = \\frac{\\sigma^2}{n}, k=n-1\\right)\n\\] (see Section A.8) and hence has variance \\(\\text{Var}(\\widehat{\\sigma^2}_{ML}) = \\frac{n-1}{n} \\, \\frac{2 \\sigma ^4}{n}\\). For large \\(n\\) this becomes \\(\\text{Var}\\left(\\widehat{\\sigma^2}_{ML}\\right)\\overset{a}{=} \\frac{2}{n} \\left(\\sigma^2\\right)^2\\) which is (apart from the “hat”) the inverse of the observed Fisher information.\n\n\n\nModels with multiple parameters\n\nExample 9.4 Observed Fisher information for the normal distribution with mean and variance parameter:\nThis is the continuation of Example 8.5.\nThe Hessian matrix of the log-likelihood function is \\[\\nabla \\nabla^T l_n(\\mu,\\sigma^2| D) =\n\\begin{pmatrix}\n    - \\frac{n}{\\sigma^2}&  -\\frac{n}{\\sigma^4} (\\bar{x} -\\mu)\\\\\n    - \\frac{n}{\\sigma^4} (\\bar{x} -\\mu) & \\frac{n}{2\\sigma^4}-\\frac{n}{\\sigma^6} \\left(\\overline{x^2} - 2 \\mu \\bar{x} + \\mu^2\\right) \\\\\n    \\end{pmatrix}\n\\] The negative Hessian at the MLE, i.e. at \\(\\hat{\\mu}_{ML} = \\bar{x}\\) and \\(\\widehat{\\sigma^2}_{ML} = \\overline{x^2} -\\bar{x}^2\\), yields the observed Fisher information matrix: \\[\n\\boldsymbol J_n(\\hat{\\mu}_{ML},\\widehat{\\sigma^2}_{ML}) = \\begin{pmatrix}\n    \\frac{n}{\\widehat{\\sigma^2}_{ML}}&0 \\\\\n    0 & \\frac{n}{2(\\widehat{\\sigma^2}_{ML})^2}\n    \\end{pmatrix}\n\\] The observed Fisher information matrix is diagonal with positive entries. Therefore its eigenvalues are all positive as required for a maximum, because for a diagonal matrix the eigenvalues are simply the the entries on the diagonal.\nThe inverse of the observed Fisher information matrix is \\[\n\\boldsymbol J_n(\\hat{\\mu}_{ML},\\widehat{\\sigma^2}_{ML})^{-1} = \\begin{pmatrix}\n    \\frac{\\widehat{\\sigma^2}_{ML}}{n}& 0\\\\\n    0 & \\frac{2(\\widehat{\\sigma^2}_{ML})^2}{n}\n    \\end{pmatrix}\n\\]\n\n\nExample 9.5 \\(\\color{Red} \\blacktriangleright\\) Observed Fisher information of the categorical distribution:\nWe continue Example 8.7. We first need to compute the negative Hessian matrix of the log likelihood function \\(- \\nabla \\nabla^T l_n(\\pi_1, \\ldots, \\pi_{K-1} )\\) and then evaluate it at the MLEs \\(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}\\).\nThe diagonal entries of the Hessian matrix (with \\(i=1, \\ldots, K-1\\)) are \\[\n\\frac{\\partial^2}{\\partial \\pi_i^2} l_n(\\pi_1, \\ldots, \\pi_{K-1} ) =\n-n \\left( \\frac{\\bar{x}_i}{\\pi_i^2} +\\frac{\\bar{x}_K}{\\pi_K^2}\\right)\n\\] and its off-diagonal entries are (with \\(j=1, \\ldots, K-1\\)) \\[\n\\frac{\\partial^2}{\\partial \\pi_i \\partial \\pi_j} l_n(\\pi_1, \\ldots, \\pi_{K-1} ) =\n-\\frac{n \\bar{x}_K}{\\pi_K^2}\n\\] Thus, the observed Fisher information matrix at the MLE for a categorical distribution is the \\(K-1 \\times K-1\\) dimensional matrix \\[\n\\begin{split}\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) &=\nn\n\\begin{pmatrix}\n\\frac{1}{\\hat{\\pi}_1^{ML}} + \\frac{1}{\\hat{\\pi}_K^{ML}} & \\cdots & \\frac{1}{\\hat{\\pi}_K^{ML}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{\\hat{\\pi}_K^{ML}} & \\cdots & \\frac{1}{\\hat{\\pi}_{K-1}^{ML}} + \\frac{1}{\\hat{\\pi}_K^{ML}} \\\\\n\\end{pmatrix} \\\\\n& = n \\text{Diag}\\left( \\frac{1}{\\hat{\\pi}_1^{ML}} , \\ldots, \\frac{1}{\\hat{\\pi}_{K-1}^{ML}}   \\right) + \\frac{n}{\\hat{\\pi}_K^{ML}}\\mathbf 1\\\\\n\\end{split}\n\\]\nFor \\(K=2\\) (cf. Example 9.1) this reduces to the observed Fisher information of a Bernoulli variable \\[\n\\begin{split}\nJ_n(\\hat{\\theta}_{ML}) & = n \\left(\\frac{1}{\\hat{\\theta}_{ML}} + \\frac{1}{1-\\hat{\\theta}_{ML}} \\right) \\\\\n  &= \\frac{n}{\\hat{\\theta}_{ML} (1-\\hat{\\theta}_{ML})} \\\\\n\\end{split}\n\\]\nThe inverse of the observed Fisher information is: \\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  )^{-1} =\n\\frac{1}{n}\n\\begin{pmatrix}\n\\hat{\\pi}_1^{ML} (1- \\hat{\\pi}_1^{ML} )  & \\cdots & -  \\hat{\\pi}_{1}^{ML} \\hat{\\pi}_{K-1}^{ML}   \\\\\n\\vdots & \\ddots & \\vdots \\\\\n-  \\hat{\\pi}_{K-1}^{ML} \\hat{\\pi}_{1}^{ML} & \\cdots & \\hat{\\pi}_{K-1}^{ML} (1- \\hat{\\pi}_{K-1}^{ML} )  \\\\\n\\end{pmatrix}\n\\]\nTo show that this is indeed the inverse we use the Woodbury matrix identity\n\\[\n(\\boldsymbol A+ \\boldsymbol U\\boldsymbol B\\boldsymbol V)^{-1} = \\boldsymbol A^{-1} - \\boldsymbol A^{-1} \\boldsymbol U(\\boldsymbol B^{-1} + \\boldsymbol V\\boldsymbol A^{-1} \\boldsymbol U)^{-1} \\boldsymbol V\\boldsymbol A^{-1}\n\\] with\n\n\\(B=1\\),\n\\(\\boldsymbol u= (\\pi_1, \\ldots, \\pi_{K-1})^T\\),\n\\(\\boldsymbol v=-\\boldsymbol u^T\\),\n\\(\\boldsymbol A= \\text{Diag}(\\boldsymbol u)\\) and its inverse \\(\\boldsymbol A^{-1} = \\text{Diag}(\\pi_1^{-1}, \\ldots, \\pi_{K-1}^{-1})\\).\n\nThen \\(\\boldsymbol A^{-1} \\boldsymbol u= \\mathbf 1_{K-1}\\) and \\(1-\\boldsymbol u^T \\boldsymbol A^{-1} \\boldsymbol u= \\pi_K\\). With this \\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  )^{-1} = \\frac{1}{n}\n\\left( \\boldsymbol A- \\boldsymbol u\\boldsymbol u^T \\right)\n\\] and \\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) = n \\left( \\boldsymbol A^{-1} + \\frac{1}{\\pi_K} \\mathbf 1_{K-1 \\times K-1}  \\right)\n\\]",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Observed Fisher information</span>"
    ]
  },
  {
    "objectID": "10-likelihood4.html",
    "href": "10-likelihood4.html",
    "title": "10  Quadratic approximation and normal asymptotics",
    "section": "",
    "text": "10.1 Approximate distribution of maximum likelihood estimates",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quadratic approximation and normal asymptotics</span>"
    ]
  },
  {
    "objectID": "10-likelihood4.html#approximate-distribution-of-maximum-likelihood-estimates",
    "href": "10-likelihood4.html#approximate-distribution-of-maximum-likelihood-estimates",
    "title": "10  Quadratic approximation and normal asymptotics",
    "section": "",
    "text": "Quadratic log-likelihood of the multivariate normal model\nAssume we observe a single sample \\(\\boldsymbol x\\sim  N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) with known covariance. Noting that the multivariate normal density is \\[\nf(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) = (2\\pi)^{-\\frac{d}{2}} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}}\n\\exp\\left(-\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu) \\right)\n\\] the corresponding log-likelihood for \\(\\boldsymbol \\mu\\) is \\[\nl_1(\\boldsymbol \\mu| \\boldsymbol x) = C - \\frac{1}{2}(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)\n\\] where \\(C\\) is a constant that does not depend on \\(\\boldsymbol \\mu\\). Note that the log-likelihood is a quadratic function (both for \\(\\boldsymbol x\\) and \\(\\boldsymbol \\mu\\)) and the maximum of the function lies at \\(\\boldsymbol \\mu= \\boldsymbol x\\) with value \\(C\\).\n\n\nQuadratic approximation of a log-likelihood function\n\n\n\n\n\n\nFigure 10.1: Quadratic approximation of the log-likelihood function.\n\n\n\nNow consider the quadratic approximation of a general log-likelihood function \\(l_n(\\boldsymbol \\theta| D)\\) for \\(\\boldsymbol \\theta\\) around the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\) (Figure 10.1).\nWe assume the underlying model is regular and that \\(\\nabla l_n(\\hat{\\boldsymbol \\theta}_{ML} | D) = 0\\), i.e. the gradient at the maximum vanishes. The Taylor series approximation of scalar-valued function \\(f(\\boldsymbol x)\\) around \\(\\boldsymbol x_0\\) is \\[\nf(\\boldsymbol x) = f(\\boldsymbol x_0) + \\nabla^T f(\\boldsymbol x_0)\\, (\\boldsymbol x-\\boldsymbol x_0) + \\frac{1}{2}\n(\\boldsymbol x-\\boldsymbol x_0)^T \\nabla \\nabla^T f(\\boldsymbol x_0) (\\boldsymbol x-\\boldsymbol x_0) + \\ldots\n\\] Applied to the log-likelihood function this yields\n\\[l_n(\\boldsymbol \\theta| D) \\approx l_n(\\hat{\\boldsymbol \\theta}_{ML} | D)- \\frac{1}{2}(\\hat{\\boldsymbol \\theta}_{ML}- \\boldsymbol \\theta)^T J_n(\\hat{\\boldsymbol \\theta}_{ML})(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta)\\]\nThis is a quadratic function with maximum at \\(( \\hat{\\boldsymbol \\theta}_{ML}, l_n(\\hat{\\boldsymbol \\theta}_{ML} | D) )\\). Note the appearance of the observed Fisher information \\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) in the quadratic term. There is no linear term because of the vanishing gradient at the MLE.\nCrucially, this approximated log-likelihood takes the same form as if \\(\\hat{\\boldsymbol \\theta}_{ML}\\) was sampled from a multivariate normal distribution with mean \\(\\boldsymbol \\theta\\) and with covariance given by the inverse observed Fisher information.\nNote that this requires a positive definite observed Fisher information matrix so that \\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) is actually invertible!\n\nExample 10.1 Quadratic approximation of the log-likelihood for a proportion:\nFrom Example 8.1 we have the log-likelihood \\[\nl_n(p | D) = n \\left( \\bar{x} \\log p + (1-\\bar{x}) \\log(1-p) \\right)\n\\] and the MLE \\[\n\\hat{p}_{ML} = \\bar{x}\n\\] and from Example 9.1 the observed Fisher information \\[\n\\begin{split}\nJ_n(\\hat{p}_{ML}) = \\frac{n}{\\bar{x} (1-\\bar{x})}\n\\end{split}\n\\] The log-likelihood at the MLE is \\[\nl_n(\\hat{p}_{ML} | D) = n \\left( \\bar{x} \\log \\bar{x} + (1-\\bar{x}) \\log(1-\\bar{x}) \\right)\n\\] This allows us to construct the quadratic approximation of the log-likelihood around the MLE as \\[\n\\begin{split}\nl_n(p| D) & \\approx  l_n(\\hat{p}_{ML} | D) - \\frac{1}{2} J_n(\\hat{p}_{ML}) (p-\\hat{p}_{ML})^2 \\\\\n   &= n \\left( \\bar{x} \\log \\bar{x} + (1-\\bar{x}) \\log(1-\\bar{x}) - \\frac{(p-\\bar{x})^2}{2 \\bar{x} (1-\\bar{x})}  \\right) \\\\\n&=  C + \\frac{ \\bar{x} p -\\frac{1}{2} p^2}{ \\bar{x} (1-\\bar{x})/n} \\\\\n\\end{split}\n\\] The constant \\(C\\) does not depend on \\(p\\), its function is to match the approximate log-likelihood at the MLE with that of the corresponding original log-likelihood. The approximate log-likelihood takes on the form of a normal log-likelihood (Example 8.2) for one observation of \\(\\hat{p}_{ML}=\\bar{x}\\) from \\(N\\left(p, \\frac{\\bar{x} (1-\\bar{x})}{n} \\right)\\).\n\n\n\n\n\n\n\n\nFigure 10.2: Quadratic approximation of the log-likelihood for a Bernoulli model.\n\n\n\n\n\nFigure 10.2 shows the Bernoulli log-likelihood function and its quadratic approximation illustrated for data with \\(n = 30\\) and \\(\\bar{x} = 0.7\\):\n\n\n\nAsymptotic normality of maximum likelihood estimates\nIntuitively, it makes sense to associate large amount of curvature of the log-likelihood at the MLE with low variance of the MLE (and conversely, low amount of curvature with high variance).\nFrom the above we see that for regular models:\n\nnormality implies a quadratic log-likelihood,\nconversely, taking an quadratic approximation of the log-likelihood implies approximate normality, and\nin the quadratic approximation the inverse observed Fisher information plays the role of the covariance of the MLE.\n\nThis suggests the following theorem:\nAsymptotically, the MLE of the parameters of a regular model is normally distributed around the true parameter and with covariance equal to the inverse of the observed Fisher information:\n\\[\\hat{\\boldsymbol \\theta}_{ML} \\overset{a}{\\sim}\\underbrace{N_d}_{\\text{multivariate normal}}\\left(\\underbrace{\\boldsymbol \\theta}_{\\text{mean vector}},\\underbrace{\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1}}_{\\text{ covariance matrix}}\\right)\\]\nThis theorem about the distributional properties of MLEs greatly enhances the usefulness of the method of maximum likelihood. It implies that in regular settings maximum likelihood is not just a method for obtaining point estimates but also also provides estimates of their uncertainty.\n\n\nRemarks on the asympotics\nHowever, we need to clarify what “asymptotic” actually means in the context of the above theorem:\n\nPrimarily, it means to have sufficient sample size so that the log-likelihood \\(l_n(\\boldsymbol \\theta)\\) is sufficiently well approximated by a quadratic function around \\(\\hat{\\boldsymbol \\theta}_{ML}\\). The better the local quadratic approximation the better the normal approximation!\nIn a regular model with positive definite observed Fisher information matrix this is guaranteed for large sample size \\(n \\rightarrow \\infty\\) thanks to the central limit theorem).\nHowever, \\(n\\) going to infinity is in fact not always required for the normal approximation to hold! Depending on the particular model a good local fit to a quadratic log-likelihood may be available also for finite \\(n\\). As a trivial example, for the normal log-likelihood it is valid for any \\(n\\).\nIn the other hand, in non-regular models (with nondifferentiable log-likelihood at the MLE and/or a singular Fisher information matrix) no amount of data, not even \\(n\\rightarrow \\infty\\), will make the quadratic approximation work.\n\nRemarks:\n\nThe asymptotic normality of MLEs was first discussed in Fisher (1925) 1\n\n\nThe technical details of the above considerations are worked out in the theory of locally asymptotically normal (LAN) models pioneered in 1960 by Lucien LeCam (1924–2000).\nThere are also methods to obtain higher-order (higher than quadratic and thus non-normal) asymptotic approximations. These relate to so-called saddle point approximations.\n\n\n\nInformation inequality and asymptotic optimal efficiency\nAssume now that \\(\\hat{\\boldsymbol \\theta}\\) is an arbitrary and unbiased estimator for \\(\\boldsymbol \\theta\\) and the underlying data generating model is regular with density \\(f(\\boldsymbol x| \\boldsymbol \\theta)\\).\nH. Cramér (1893–1985), C. R. Rao (1920–) and others demonstrated in 1945 the so-called information inequality, \\[\n\\text{Var}(\\hat{\\boldsymbol \\theta}) \\geq \\frac{1}{n} \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)^{-1}\n\\] which puts a lower bound on the variance of an estimator for \\(\\boldsymbol \\theta\\). (Note for \\(d&gt;1\\) this is a matrix inequality, meaning that the difference matrix is positive semidefinite).\nFor large sample size with \\(n \\rightarrow \\infty\\) and \\(\\hat{\\boldsymbol \\theta}_{ML} \\rightarrow \\boldsymbol \\theta\\) the observed Fisher information becomes \\(J_n(\\hat{\\boldsymbol \\theta}) \\rightarrow n \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)\\) and therefore we can write the asymptotic distribution of \\(\\hat{\\boldsymbol \\theta}_{ML}\\) as \\[\n\\hat{\\boldsymbol \\theta}_{ML} \\overset{a}{\\sim} N_d\\left(  \\boldsymbol \\theta,  \\frac{1}{n} \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)^{-1}  \\right)\n\\] This means that for large \\(n\\) in regular models \\(\\hat{\\boldsymbol \\theta}_{ML}\\) achieves the lowest variance possible according to the Cramér-Rao information inequality. In other words, for large sample size maximum likelihood is optimally efficient and thus the best available estimator will in fact be the MLE!\nHowever, as we will see later this does not hold for small sample size where it is indeed possible (and necessary) to improve over the MLE (e.g. via Bayesian estimation or regularisation).\n\n\nNon-regular models\nFor non-regular models the asymptotic normality does not hold. Instead, the (asympotic) distribution of the MLE must be obtained in by other means.\n\nExample 10.2 Distribution of MLE for upper bound \\(\\theta\\) of the uniform distribution:\nThis continues Example 8.4 to find the distribution of \\(\\hat{\\theta}_{ML}\\). Due to a discontinuity in the density at the MLE the observed Fisher information cannot be computed and the normal approximation for the distribution of \\(\\hat{\\theta}_{ML}\\) is not valid.\nNonetheless, one can still obtain the sampling distribution of \\(\\hat{\\theta}_{ML}=x_{[n]}\\). However, not via asymptotic arguments but instead by understanding that \\(x_{[n]}\\) is an order statistic (see https://en.wikipedia.org/wiki/Order_statistic) with the following properties: \\[\\begin{align*}\n\\begin{array}{cc}\nx_{[n]}\\sim \\theta \\, \\text{Beta}(n,1)\\\\\n\\\\\n\\text{E}(x_{[n]})=\\frac{n}{n+1} \\theta\\\\\n\\\\\n\\text{Var}(x_{[n]})=\\frac{n}{(n+1)^2(n+2)}\\theta^2\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{\"n-th order statistic\" }\\\\\n\\\\\n\\\\\n\\\\\n\\approx \\frac{\\theta^2}{n^2}\\\\\n\\end{array}\n\\end{align*}\\]\nNote that the variance decreases with \\(\\frac{1}{n^2}\\) which is much faster than the usual \\(\\frac{1}{n}\\) of an “efficient” estimator. Correspondingly, \\(\\hat{\\theta}_{ML}\\) is a so-called “super efficient” estimator.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quadratic approximation and normal asymptotics</span>"
    ]
  },
  {
    "objectID": "10-likelihood4.html#quantifying-the-uncertainty-of-maximum-likelihood-estimates",
    "href": "10-likelihood4.html#quantifying-the-uncertainty-of-maximum-likelihood-estimates",
    "title": "10  Quadratic approximation and normal asymptotics",
    "section": "10.2 Quantifying the uncertainty of maximum likelihood estimates",
    "text": "10.2 Quantifying the uncertainty of maximum likelihood estimates\n\nEstimating the variance of MLEs\nIn the previous section we saw that MLEs are asymptotically normally distributed, with the inverse Fisher information (both expected and observed) linked to the asymptotic variance.\nThis leads to the question whether to use the observed Fisher information \\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) or the expected Fisher information at the MLE \\(n \\boldsymbol I^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) to estimate the variance of the MLE?\n\nClearly, for \\(n\\rightarrow \\infty\\) both can be used interchangeably.\nHowever, they can be very different for finite \\(n\\) in particular for models that are not exponential families.\nAlso normality may occur well before \\(n\\) goes to \\(\\infty\\).\n\nTherefore one needs to choose between the two, considering also that\n\nthe expected Fisher information at the MLE is the average curvature at the MLE, whereas the observed Fisher information is the actual observed curvature, and\nthe observed Fisher information naturally occurs in the quadratic approximation of the log-likelihood.\n\nAll in all, the observed Fisher information as estimator of the variance is more appropriate as it is based on the actual observed data and also works for large \\(n\\) (in which case it yields the same result as using expected Fisher information): \\[\n\\widehat{\\text{Var}}(\\hat{\\boldsymbol \\theta}_{ML}) = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1}\n\\] and its square-root as the estimate of the standard deviation \\[\n\\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML}) = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1/2}\n\\] Note that in the above we use matrix inversion and the (inverse) matrix square root.\nThe reasons for preferring observed Fisher information are made mathematically precise in a classic paper by Efron and Hinkley (1978) 2 .\n\n\nExamples for the estimated variance and asymptotic normal distribution\n\nExample 10.3 Estimated variance and distribution of the MLE of a proportion:\nFrom Example 8.1 and Example 9.1 we know the MLE \\[\n\\hat{p}_{ML} = \\bar{x} = \\frac{k}{n}\n\\] and the corresponding observed Fisher information \\[\nJ_n(\\hat{p}_{ML})=\\frac{n}{\\hat{p}_{ML}(1-\\hat{p}_{ML})}\n\\] The estimated variance of the MLE is therefore \\[\n\\widehat{\\text{Var}}(   \\hat{p}_{ML}  ) = \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\n\\] and the corresponding asymptotic normal distribution is \\[\n\\hat{p}_{ML} \\overset{a}{\\sim} N\\left(p,   \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}   \\right)\n\\]\n\n\nExample 10.4 Estimated variance and distribution of the MLE of the mean parameter for the normal distribution with known variance:\nFrom Example 8.2 and Example 9.2 we know that \\[\\hat{\\mu}_{ML} =\\bar{x}\\] and that the corresponding observed Fisher information at \\(\\hat{\\mu}_{ML}\\) is \\[J_n(\\hat{\\mu}_{ML})=\\frac{n}{\\sigma^2}\\]\nThe estimated variance of the MLE is therefore \\[\n\\widehat{\\text{Var}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma^2}{n}\n\\] and the corresponding asymptotic normal distribution is \\[\n\\hat{\\mu}_{ML} \\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right)\n\\]\nNote that in this case the distribution is not asymptotic but is exact, i.e. valid also for small \\(n\\) (as long as the data \\(x_i\\) are actually from \\(N(\\mu, \\sigma^2)\\)!).\n\n\n\nWald statistic\nCentering the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\) with \\(\\boldsymbol \\theta_0\\) followed by standardising with \\(\\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML})\\) yields the Wald statistic (named after Abraham Wald, 1902–1950): \\[\n\\begin{split}\n\\boldsymbol t(\\boldsymbol \\theta_0) & = \\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML})^{-1}(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\\n& = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{1/2}(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\\n\\end{split}\n\\] The squared Wald statistic is a scalar defined as \\[\n\\begin{split}\nt(\\boldsymbol \\theta_0)^2 &= \\boldsymbol t(\\boldsymbol \\theta_0)^T \\boldsymbol t(\\boldsymbol \\theta_0) \\\\\n&=\n(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)^T\n\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})\n(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\\n\\end{split}\n\\] Note that in the literature both \\(\\boldsymbol t(\\boldsymbol \\theta_0)\\) and \\(t(\\boldsymbol \\theta_0)^2\\) are commonly referred to as Wald statistics. In this text we use the qualifier “squared” if we refer to the latter.\nWe now assume that the true underlying parameter is \\(\\boldsymbol \\theta_0\\). Since the MLE is asymptotically normal the Wald statistic is asymptotically standard normal distributed: \\[\\begin{align*}\n\\begin{array}{cc}\n\\boldsymbol t(\\boldsymbol \\theta_0) \\overset{a}{\\sim}\\\\\nt(\\theta_0) \\overset{a}{\\sim}\\\\\n\\end{array}\n\\begin{array}{ll}\nN_d(\\mathbf 0_d,\\boldsymbol I_d)\\\\\nN(0,1)\\\\\n\\end{array}\n\\begin{array}{ll}\n  \\text{for vector } \\boldsymbol \\theta\\\\\n  \\text{for scalar } \\theta\\\\\n\\end{array}\n\\end{align*}\\] Correspondingly, the squared Wald statistic is chi-squared distributed: \\[\\begin{align*}\n\\begin{array}{cc}\nt(\\boldsymbol \\theta_0)^2 \\\\\nt(\\theta_0)^2\\\\\n\\end{array}\n\\begin{array}{ll}\n\\overset{a}{\\sim}\\chi^2_d\\\\\n\\overset{a}{\\sim}\\chi^2_1\\\\\n\\end{array}\n\\begin{array}{ll}\n  \\text{for vector } \\boldsymbol \\theta\\\\\n  \\text{for scalar } \\theta\\\\\n\\end{array}\n\\end{align*}\\] The degree of freedom of the chi-squared distribution is the dimension \\(d\\) of the parameter vector \\(\\boldsymbol \\theta\\).\n\n\nExamples of the (squared) Wald statistic\n\nExample 10.5 Wald statistic for a proportion:\nWe continue from Example 10.3. With \\(\\hat{p}_{ML} = \\bar{x}\\) and \\(\\widehat{\\text{Var}}(   \\hat{p}_{ML}  ) = \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\\) and thus \\(\\widehat{\\text{SD}}(   \\hat{p}_{ML}  ) =\\sqrt{ \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n} }\\) we get as Wald statistic:\n\\[\nt(p_0) = \\frac{\\bar{x}-p_0}{ \\sqrt{\\bar{x}(1-\\bar{x}) / n }  }\\overset{a}{\\sim} N(0,1)\n\\]\nThe squared Wald statistic is: \\[t(p_0)^2 = n \\frac{(\\bar{x}-p_0)^2}{ \\bar{x}(1-\\bar{x})   }\\overset{a}{\\sim} \\chi^2_1 \\]\n\n\nExample 10.6 Wald statistic for the mean parameter of a normal distribution with known variance:\nWe continue from Example 10.4. With \\(\\hat{\\mu}_{ML} =\\bar{x}\\) and \\(\\widehat{\\text{Var}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma^2}{n}\\) and thus \\(\\widehat{\\text{SD}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma}{\\sqrt{n}}\\) we get as Wald statistic:\n\\[t(\\mu_0) = \\frac{\\bar{x}-\\mu_0}{\\sigma / \\sqrt{n}}\\sim N(0,1)\\] Note this is the one sample \\(t\\)-statistic with given \\(\\sigma\\). The squared Wald statistic is: \\[t(\\mu_0)^2 = \\frac{(\\bar{x}-\\mu_0)^2}{\\sigma^2 / n}\\sim \\chi^2_1 \\]\nAgain, in this instance this is the exact distribution, not just the asymptotic one.\nUsing the Wald statistic or the squared Wald statistic we can test whether a particular \\(\\mu_0\\) can be rejected as underlying true parameter, and we can also construct corresponding confidence intervals.\n\n\nExample 10.7 Wald statistic for the categorical distribution:\nThe squared Wald statistic is \\[\n\\begin{split}\nt(\\boldsymbol p_0)^2 &=\n(\\hat{\\pi}_{1}^{ML}-p_1^0, \\ldots,  \\hat{\\pi}_{K-1}^{ML}-p_{K-1}^0)   \\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML} ) \\begin{pmatrix} \\hat{\\pi}_{1}^{ML}-p_1^0 \\\\\n\\vdots \\\\\n\\hat{\\pi}_{K-1}^{ML}-p_{K-1}^0\\\\\n\\end{pmatrix}\\\\\n&= n  \\left( \\sum_{k=1}^{K-1} \\frac{(\\hat{\\pi}_{k}^{ML}-p_{k}^0)^2}{\\hat{\\pi}_{k}^{ML}}   + \\frac{ \\left(\\sum_{k=1}^{K-1} (\\hat{\\pi}_{k}^{ML}-p_{k}^0)\\right)^2}{\\hat{\\pi}_{K}^{ML}} \\right)  \\\\\n&= n  \\left( \\sum_{k=1}^{K} \\frac{(\\hat{\\pi}_{k}^{ML}-p_{k}^0)^2}{\\hat{\\pi}_{k}^{ML}}    \\right)  \\\\\n& = n D_{\\text{Neyman}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) )\n\\end{split}\n\\]\nWith \\(n_1, \\ldots, n_K\\) the observed counts with \\(n =  \\sum_{k=1}^K  n_k\\) and \\(\\hat{\\pi}_k^{ML} = \\frac{n_k}{n} = \\bar{x}_k\\), and \\(n_1^{\\text{expect}}, \\ldots, n_K^{\\text{expect}}\\) the expected counts \\(n_k^{\\text{expect}} = n p_k^{0}\\) under \\(\\boldsymbol p_0\\) we can write the squared Wald statistic as follows: \\[\nt(\\boldsymbol p_0)^2 = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}} )^2}{n_k} =  \\chi^2_{\\text{Neyman}}\n\\] This is known as the Neyman chi-squared statistic (note the observed counts in its denominator) and it is asymptotically distributed as \\(\\chi^2_{K-1}\\) because there are \\(K-1\\) free parameters in \\(\\boldsymbol p_0\\).\n\n\n\nNormal confidence intervals using the Wald statistic\nSee Section A.10 to review relevant background from year 1.\n\n\n\n\n\n\nFigure 10.3: Construction of a 95% symmetric normal confidence interval for a maximum likelihood estimate.\n\n\n\nThe asymptotic normality of MLEs derived from regular models enables us to construct a corresponding normal confidence interval (Figure 10.3). For example, to construct the asymptotic normal CI for the MLE of a scalar parameter \\(\\theta\\) we use the MLE \\(\\hat{\\theta}_{ML}\\) as estimate of the mean and its standard deviation \\(\\widehat{\\text{SD}}(\\hat{\\theta}_{ML})\\) computed from the observed Fisher information: \\[\n\\text{CI}=[\\hat{\\theta}_{ML} \\pm c_{\\text{normal}} \\widehat{\\text{SD}}(\\hat{\\theta}_{ML})]\n\\] Here \\(c_{normal}\\) is a critical value for the standard-normal symmetric confidence interval chosen to achieve the desired nominal coverage. The critical values are computed using the inverse standard normal distribution function via \\(c_{\\text{normal}}=\\Phi^{-1}\\left(\\frac{1+\\kappa}{2}\\right)\\). A list of critical values for the standard normal distribution is found in Table A.1. For example, for a CI with 95% coverage one uses the factor 1.96 so that \\[\\text{CI}=[\\hat{\\theta}_{ML} \\pm 1.96\\, \\widehat{\\text{SD}}(\\hat{\\theta}_{ML}) ]\\]\nThe normal CI can be expressed using Wald statistic as follows: \\[\n\\text{CI}=\\{\\theta_0:  | t(\\theta_0)| &lt; c_{\\text{normal}} \\}\n\\]\nSimilary, it can also be expressed using the squared Wald statistic: \\[\n\\text{CI}=\\{\\theta_0:   t(\\boldsymbol \\theta_0)^2 &lt; c_{\\text{chisq}} \\}\n\\] Note that this form facilitates the construction of normal confidence intervals for a parameter vector \\(\\boldsymbol \\theta_0\\).\nA list of critical values for the chi-squared distribution with one degree of freedom is found in Table A.2.\nThe following lists contains the critical values resulting from the chi-squared distribution with degree of freedom \\(m=1\\) for the three most common choices of coverage \\(\\kappa\\) for a normal CI for a univariate parameter: For example, for a 95% interval the critical value equals 3.84 (which is the square of the critical value 1.96 for the standard normal).\n\n\nNormal tests using the Wald statistic\nFinally, recall the duality between confidence intervals and statistical tests. Specifically, a confidence interval with coverage \\(\\kappa\\) can be also used for testing as follows:\n\nfor every \\(\\theta_0\\) inside the CI the data do not allow to reject the hypothesis that \\(\\theta_0\\) is the true parameter with significance level \\(1-\\kappa\\).\nConversely, all values \\(\\theta_0\\) outside the CI can be rejected to be the true parameter with significance level \\(1-\\kappa\\) .\n\nHence, in order to test whether \\(\\boldsymbol \\theta_0\\) is the true underlying parameter value we can compute the corresponding (squared) Wald statistic, find the desired critical value and then decide on rejection.\n\n\nExamples for normal confidence intervals and corresponding tests\n\nExample 10.8 Asymptotic normal confidence interval for a proportion:\nWe continue from Example 10.3 and Example 10.5. Assume we observe \\(n=30\\) measurements with average \\(\\bar{x} = 0.7\\). Then \\(\\hat{p}_{ML} = \\bar{x} = 0.7\\) and \\(\\widehat{\\text{SD}}(\\hat{p}_{ML}) = \\sqrt{ \\frac{ \\bar{x}(1-\\bar{x})}{n} } \\approx 0.084\\).\nThe symmetric asymptotic normal CI for \\(p\\) with 95% coverage is given by \\(\\hat{p}_{ML} \\pm 1.96 \\, \\widehat{\\text{SD}}(\\hat{p}_{ML})\\) which for the present data results in the interval \\([0.536, 0.864]\\).\n\n\nExample 10.9 Asymptotic normal test for a proportion:\nWe continue from Example 10.8.\nWe now consider two possible values (\\(p_0=0.5\\) and \\(p_0=0.8\\)) as potentially true underlying proportion.\nThe value \\(p_0=0.8\\) lies inside the 95% confidence interval \\([0.536, 0.864]\\). This implies we cannot reject the hypthesis that this is the true underlying parameter on 5% significance level. In contrast, \\(p_0=0.5\\) is outside the confidence interval so we can indeed reject this value. In other words, data plus model exclude this value as statistically implausible.\nThis can be verified more directly by computing the corresponding (squared) Wald statistics (see Example 10.5) and comparing them with the relevant critical value (3.84 from chi-squared distribution for 5% significance level):\n\n\\(t(0.5)^2 = \\frac{(0.7-0.5)^2}{0.084^2} = 5.71  &gt; 3.84\\) hence \\(p_0=0.5\\) can be rejected.\n\\(t(0.8)^2 = \\frac{(0.7-0.8)^2}{0.084^2} = 1.43  &lt; 3.84\\) hence \\(p_0=0.8\\) cannot be rejected.\n\nNote that the squared Wald statistic at the boundaries of the normal confidence interval is equal to the critical value.\n\n\nExample 10.10 Normal confidence interval for the mean:\nWe continue from Example 10.4 and Example 10.6. Assume that we observe \\(n=25\\) measurements with average \\(\\bar{x} = 10\\), from a normal with unknown mean and variance \\(\\sigma^2=4\\).\nThen \\(\\hat{\\mu}_{ML} = \\bar{x} = 10\\) and \\(\\widehat{\\text{SD}}(\\hat{\\mu}_{ML}) = \\sqrt{ \\frac{ \\sigma^2}{n} } = \\frac{2}{5}\\).\nThe symmetric asymptotic normal CI for \\(p\\) with 95% coverage is given by \\(\\hat{\\mu}_{ML} \\pm 1.96 \\, \\widehat{\\text{SD}}(\\hat{\\mu}_{ML})\\) which for the present data results in the interval \\([9.216, 10.784]\\).\n\n\nExample 10.11 Normal test for the mean:\nWe continue from Example 10.10.\nWe now consider two possible values (\\(\\mu_0=9.5\\) and \\(\\mu_0=11\\)) as potentially true underlying mean parameter.\nThe value \\(\\mu_0=9.5\\) lies inside the 95% confidence interval \\([9.216, 10.784]\\). This implies we cannot reject the hypthesis that this is the true underlying parameter on 5% significance level. In contrast, \\(\\mu_0=11\\) is outside the confidence interval so we can indeed reject this value. In other words, data plus model exclude this value as a statistically implausible.\nThis can be verified more directly by computing the corresponding (squared) Wald statistics (see Example 10.6) and comparing them with the relevant critical values:\n\n\\(t(9.5)^2 = \\frac{(10-9.5)^2}{4/25}= 1.56  &lt; 3.84\\) hence \\(\\mu_0=9.5\\) cannot be rejected.\n\\(t(11)^2 = \\frac{(10-11)^2}{4/25} = 6.25 &gt; 3.84\\) hence \\(\\mu_0=11\\) can be rejected.\n\nThe squared Wald statistic at the boundaries of the confidence interval equals the critical value.\nNote that this is the standard one-sample test of the mean, and that it is exact, not an approximation.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quadratic approximation and normal asymptotics</span>"
    ]
  },
  {
    "objectID": "10-likelihood4.html#footnotes",
    "href": "10-likelihood4.html#footnotes",
    "title": "10  Quadratic approximation and normal asymptotics",
    "section": "",
    "text": "Fisher R. A. 1925. Theory of statistical estimation. Math. Proc. Cambridge Philos. Soc. 22:700–725. https://doi.org/10.1017/S0305004100009580↩︎\nEfron, B., and D. V. Hinkley. 1978. Assessing the accuracy of the maximum likelihood estimator: observed versus expected Fisher information. Biometrika 65:457–482. https://doi.org/10.1093/biomet/65.3.457↩︎",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quadratic approximation and normal asymptotics</span>"
    ]
  },
  {
    "objectID": "11-likelihood5.html",
    "href": "11-likelihood5.html",
    "title": "11  Likelihood-based confidence interval and likelihood ratio",
    "section": "",
    "text": "11.1 Likelihood-based confidence intervals and Wilks statistic",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Likelihood-based confidence interval and likelihood ratio</span>"
    ]
  },
  {
    "objectID": "11-likelihood5.html#likelihood-based-confidence-intervals-and-wilks-statistic",
    "href": "11-likelihood5.html#likelihood-based-confidence-intervals-and-wilks-statistic",
    "title": "11  Likelihood-based confidence interval and likelihood ratio",
    "section": "",
    "text": "General idea and definition of Wilks log-likelihood ratio statistic\n\n\n\n\n\n\nFigure 11.1: Construction of a likelihood-based confidence intervals.\n\n\n\nInstead of relying on the normal resp. quadratic approximation, we can also use the log-likelihood directly to find likelihood confidence intervals (Figure 11.1).\nIdea: find all \\(\\boldsymbol \\theta_0\\) that have a log-likelihood that is almost as good as \\(l_n(\\hat{\\boldsymbol \\theta}_{ML} | D)\\). \\[\\text{CI}= \\{\\boldsymbol \\theta_0: l_n(\\hat{\\boldsymbol \\theta}_{ML}| D) - l_n(\\boldsymbol \\theta_0 | D) \\leq \\Delta\\}\\] Here \\(\\Delta\\) is our tolerated deviation from the maximum log-likelihood. We will see below how to determine a suitable \\(\\Delta\\).\nThe above leads naturally to the Wilks log-likelihood ratio statistic \\(W(\\boldsymbol \\theta_0)\\) defined as: \\[\n\\begin{split}\nW(\\boldsymbol \\theta_0) & = 2 \\log \\left(\\frac{L(\\hat{\\boldsymbol \\theta}_{ML}| D)}{L(\\boldsymbol \\theta_0| D)}\\right) \\\\\n& =2(l_n(\\hat{\\boldsymbol \\theta}_{ML}| D)-l_n(\\boldsymbol \\theta_0 |D))\\\\\n\\end{split}\n\\] With its help we can write the likelihood CI as follows: \\[\\text{CI}= \\{\\boldsymbol \\theta_0: W(\\boldsymbol \\theta_0) \\leq 2 \\Delta\\}\\]\nThe Wilks statistic is named after Samuel S. Wilks (1906–1964).\nAdvantages of using a likelihood-based CI:\n\nnot restricted to be symmetric\nenables to construct multivariate CIs for parameter vector easily even in non-normal cases\ncontains normal CI as special case\n\n\nExample 11.1 The likelihood ratio statistic:\nAs alternative to the Wilks log-likelhood ratio statistic \\(W(\\boldsymbol \\theta_0)\\) one may also use the likelihood ratio statistic \\[\n\\Lambda(\\boldsymbol \\theta_0)  = \\frac{L(\\boldsymbol \\theta_0| D)}{L(\\hat{\\boldsymbol \\theta}_{ML}| D)}\n\\] The two statistics can be transformed into each other using \\[\nW(\\boldsymbol \\theta_0) = -2\\log \\Lambda(\\boldsymbol \\theta_0)\n\\] and \\[\\Lambda(\\boldsymbol \\theta_0) =  e^{ - W(\\boldsymbol \\theta_0) / 2 }\n\\] Hence large values of \\(W(\\boldsymbol \\theta_0\\) correspond to small values of \\(\\Lambda(\\boldsymbol \\theta_0)\\) and the other way round.\nIn this course we will only use \\(W(\\boldsymbol \\theta_0)\\) as it is both easier to compute and its sampling distribution is easier to obtain.\n\n\n\nExamples of the Wilks log-likelihood ratio statistic\n\nExample 11.2 Wilks statistic for the proportion:\nThe log-likelihood for the parameter \\(\\theta\\) is (cf. Example 8.1) \\[\nl_n(\\theta| D) = n ( \\bar{x} \\log \\theta + (1-\\bar{x}) \\log(1-\\theta) )\n\\] Hence the Wilks statistic is with \\(\\hat{\\theta}_{ML}=\\bar{x}\\) \\[\n\\begin{split}\nW(\\theta_0) & = 2 ( l_n( \\hat{\\theta}_{ML} | D)  -l_n( \\theta_0 | D ) )\\\\\n& = 2 n \\left(  \\bar{x} \\log \\left( \\frac{  \\bar{x}  }{\\theta_0}  \\right)  \n                + (1-\\bar{x}) \\log \\left( \\frac{1-\\bar{x} }{1-\\theta_0}  \\right)  \n    \\right) \\\\\n\\end{split}\n\\]\nComparing with Example 4.4 we see that in this case the Wilks statistic is essentially (apart from a scale factor \\(2n\\)) the KL divergence between two Bernoulli distributions: \\[\nW(\\theta_0) =2 n D_{\\text{KL}}( \\text{Ber}( \\hat{\\theta}_{ML} ), \\text{Ber}(\\theta_0)  )\n\\]\n\n\nExample 11.3 Wilks statistic for the mean parameter of a normal model:\nThe Wilks statistic is \\[\nW(\\mu_0) = \\frac{(\\bar{x}-\\mu_0)^2}{\\sigma^2 / n}\n\\]\nSee Worksheet L2 for a derivation of the Wilks statistic from the normal log-likelihood function.\nNote this is the same as the squared Wald statistic discussed in Example 10.6.\nComparing with Example 4.5 we see that in this case the Wilks statistic is essentially (apart from a scale factor \\(2n\\)) the KL divergence between two normal distributions with different means and variance equal to \\(\\sigma^2\\): \\[\nW(p_0) =2 n D_{\\text{KL}}( N( \\hat{\\mu}_{ML}, \\sigma^2 ), N(\\mu_0, \\sigma^2)  )\n\\]\n\n\nExample 11.4 Wilks log-likelihood ratio statistic for the categorical distribution:\nThe Wilks log-likelihood ratio is \\[\nW(\\boldsymbol p_0) = 2 (l_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) - l_n(p_1^{0}, \\ldots, p_{K-1}^{0}    ))\n\\] with \\(\\boldsymbol p_0 = c(p_1^{0}, \\ldots, p_{K}^{0} )^T\\). As the probabilities sum up to 1 there are only \\(K-1\\) free parameters.\nThe log-likelihood at the MLE is \\[\nl_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log \\hat{\\pi}_k^{ML}  =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log \\bar{x}_k\n\\] with \\(\\hat{\\pi}_k^{ML} = \\frac{n_k}{n} = \\bar{x}_k\\). Note that here and in the following the sums run from \\(1\\) to \\(K\\) where the \\(K\\)-th component is always computed from the components \\(1\\) to \\(K-1\\), as in the previous section. The log-likelihood at \\(\\boldsymbol p_0\\) is \\[l_n( p_1^{0}, \\ldots, p_{K-1}^{0}    ) =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log p_k^{0}\n\\] so that the Wilks statistic becomes \\[\nW(\\boldsymbol p_0) = 2 n   \\sum_{k=1}^{K}  \\bar{x}_k \\log\\left(\\frac{\\bar{x}_k}{ p_k^{0}} \\right)\n\\] It is asymptotically chi-squared distributed with \\(K-1\\) degrees of freedom.\nNote that for this model the Wilks statistic is equal to the KL divergence \\[\nW(\\boldsymbol p_0) = 2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) )\n\\]\nThe Wilks log-likelihood ratio statistic for the categorical distribution is also known as the \\(G\\) test statistic where \\(\\hat{\\boldsymbol \\pi}_{ML}\\) corresponds to the observed frequencies (as observed in data) and \\(\\boldsymbol p_0\\) are the expected frequencies (i.e. hypothesised to be the true frequencies).\nUsing observed counts \\(n_k\\) and expected counts \\(n_k^{\\text{expect}} = n p_k^{0}\\) we can write the Wilks statistic respectively the \\(G\\)-statistic as follows: \\[\nW(\\boldsymbol p_0) = 2   \\sum_{k=1}^{K}  n_k \\log\\left(\\frac{  n_k }{  n_k^{\\text{expect}}   } \\right)\n\\]\n\n\n\nQuadratic approximation of the Wilks statistic\nRecall the quadratic approximation of the log-likelihood function \\(l_n(\\boldsymbol \\theta_0| D)\\) (= second order Taylor series around the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\)):\n\\[l_n(\\boldsymbol \\theta_0| D)\\approx l_n(\\hat{\\boldsymbol \\theta}_{ML}| D)-\\frac{1}{2}(\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})^T \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) (\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})\\]\nWith this we can then approximate the Wilks statistic: \\[\n\\begin{split}\nW(\\boldsymbol \\theta_0) & = 2(l_n(\\hat{\\boldsymbol \\theta}_{ML}| D)-l_n(\\boldsymbol \\theta_0| D))\\\\\n& \\approx (\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})^T \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})(\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})\\\\\n& =t(\\boldsymbol \\theta_0)^2 \\\\\n\\end{split}\n\\]\nThus the quadratic approximation of the Wilks statistic yields the squared Wald statistic.\nConversely, the Wilks statistic can be understood a generalisation of the squared Wald statistic.\n\n\nExamples of quadratic approximations\n\nExample 11.5 Quadratic approximation of the Wilks statistic for a proportion (continued from Example 11.2):\nA Taylor series of second order (for \\(p_0\\) around \\(\\bar{x}\\)) yields \\[\n\\log \\left( \\frac{  \\bar{x}  }{p_0} \\right) \\approx -\\frac{p_0-\\bar{x}}{\\bar{x}} + \\frac{ ( p_0-\\bar{x} )^2    }{2  \\bar{x}^2   }\n\\] and \\[\n\\log \\left( \\frac{ 1- \\bar{x}  }{1- p_0} \\right) \\approx \\frac{p_0-\\bar{x}}{1-\\bar{x}} + \\frac{ ( p_0-\\bar{x} )^2    }{2  (1-\\bar{x})^2   }\n\\] With this we can approximate the Wilks statistic of the proportion as \\[\n\\begin{split}\nW(p_0) & \\approx  2 n \\left(  - (p_0-\\bar{x})  +\\frac{ ( p_0-\\bar{x} )^2    }{2  \\bar{x}  }\n+ (p_0-\\bar{x}) + \\frac{ ( p_0-\\bar{x} )^2    }{2  (1-\\bar{x}) } \\right)   \\\\\n& = n \\left(    \\frac{ ( p_0-\\bar{x} )^2    }{  \\bar{x}  } + \\frac{ ( p_0-\\bar{x} )^2    }{  (1-\\bar{x}) } \\right)  \\\\\n& = n \\left(    \\frac{ ( p_0-\\bar{x} )^2    }{  \\bar{x} (1-\\bar{x})  } \\right)   \\\\\n&= t(p_0)^2 \\,.\n\\end{split}\n\\] This verifies that the quadratic approximation of the Wilks statistic leads back to the squared Wald statistic of Example 10.5.\n\n\nExample 11.6 Quadratic approximation of the Wilks statistic for the mean parameter of a normal model (continued from Example 11.3):\nThe normal log-likelihood is already quadratic in the mean parameter (cf. Example 8.2). Correspondingly, the Wilks statistic is quadratic in the mean parameter as well. Hence in this particular case the quadratic “approximation” is in fact exact and the Wilks statistic and the squared Wald statistic are identical!\nCorrespondingly, confidence intervals and tests based on the Wilks statistic are identical to those obtained using the Wald statistic.\n\n\nExample 11.7 Quadratic approximation of the Wilks log-likelihood ratio statistic for the categorical distribution:\nDeveloping the Wilks statistic \\(W(\\boldsymbol p_0)\\) around the MLE \\(\\hat{\\boldsymbol \\pi}_{ML}\\) yields the squared Wald statistic which for the categorical distribution is the Neyman chi-squared statistic: \\[\n\\begin{split}\nW(\\boldsymbol p_0)& = 2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) \\\\\n& \\approx n D_{\\text{Neyman}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) \\\\\n& = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}} )^2}{n_k} \\\\\n& =  \\chi^2_{\\text{Neyman}}\\\\\n\\end{split}\n\\]\nIf instead we approximate the KL divergence assuming \\(\\boldsymbol p_0\\) as fixed we arrive at \\[\n\\begin{split}\n2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) &\\approx n D_{\\text{Pearson}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}),  \\text{Cat}(\\boldsymbol p_0 ) )\\\\\n& = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}})^2}{n_k^{\\text{expect}}} \\\\\n& = \\chi^2_{\\text{Pearson}}\n\\end{split}\n\\] which is the well-known Pearson chi-squared statistic (note the expected counts in its denominator).\n\n\n\nDistribution of the Wilks statistic\nThe connection with the squared Wald statistic as quadratic approximation of the Wilks log-likelihood ratio statistic implies that both have asympotically the same distribution.\nHence, under \\(\\boldsymbol \\theta_0\\) the Wilks statistic is distributed asymptotically as \\[W(\\boldsymbol \\theta_0) \\overset{a}{\\sim} \\chi^2_d\\] where \\(d\\) is the number of parameters in \\(\\boldsymbol \\theta\\), i.e. the dimension of the model.\nFor scalar \\(\\theta\\) (i.e. single parameter and \\(d=1\\)) this becomes \\[\nW(\\theta_0) \\overset{a}{\\sim} \\chi^2_1\n\\]\nThis fact is known as Wilks’ theorem.\n\n\nCutoff values for the likelihood CI\n\n\n\nTable 11.1: Cutoff values for construction of likelihood confidence intervals for a single parameter.\n\n\n\n\n\ncoverage \\(\\kappa\\)\n\\(\\Delta = \\frac{c_{\\text{chisq}}}{2}\\)\n\n\n\n\n0.9\n1.35\n\n\n0.95\n1.92\n\n\n0.99\n3.32\n\n\n\n\n\n\nThe asymptotic distribution for \\(W\\) is useful to choose a suitable \\(\\Delta\\) for the likelihood CI noting that \\(2 \\Delta = c_{\\text{chisq}}\\) where \\(c_{\\text{chisq}}\\) is the critical value from Table A.2 for a specified coverage \\(\\kappa\\). This yields Table 11.1 valid for a scalar parameter.\nHence, in order to calibrate the likelihood interval we in effect compare it with a normal confidence interval.\n\nExample 11.8 Likelihood confidence interval for a proportion:\nWe continue from Example 11.2, and as in Example 10.8 we asssume we have data with \\(n = 30\\) and \\(\\bar{x} = 0.7\\).\nThis yields (via numerical root finding) as the 95% likelihood confidence interval the interval \\([0.524, 0.843]\\). It is similar but not identical to the corresponding asymptotic normal interval \\([0.536, 0.864]\\) obtained in Example 10.8.\n\n\n\n\n\n\n\n\nFigure 11.2: Likelihood-based CI and normal interval for the Bernoulli model.\n\n\n\n\n\nFigure 11.2 illustrates the relationship between the normal CI and the likelihood CI and also shows the role of the quadratic approximation (see also Example 10.1). Note that:\n\nthe normal CI is symmetric around the MLE whereas the likelihood CI is not symmetric\nthe normal CI is identical to the likelihood CI when using the quadratic approximation!\n\n\n\n\nLikelihood ratio test (LRT) using Wilks statistic\nAs in the normal case (with Wald statistic and normal CIs) one can also construct a test using the Wilks statistic:\n\\[\\begin{align*}\n\\begin{array}{ll}\nH_0: \\boldsymbol \\theta= \\boldsymbol \\theta_0\\\\\nH_1: \\boldsymbol \\theta\\neq \\boldsymbol \\theta_0\\\\\n\\end{array}\n\\begin{array}{ll}\n  \\text{ True model is } \\boldsymbol \\theta_0\\\\\n  \\text{ True model is } \\textbf{not } \\boldsymbol \\theta_0\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{  Null hypothesis}\\\\\n\\text{  Alternative hypothesis}\\\\\n\\end{array}\n\\end{align*}\\]\nAs test statistic we use the Wilks log likelihood ratio \\(W(\\boldsymbol \\theta_0)\\). Extreme values of this test statistic imply evidence against \\(H_0\\).\nNote that the null model is “simple” (= a single parameter value) whereas the alternative model is “composite” (= a set of parameter values).\nRemarks:\n\nThe composite alternative \\(H_1\\) is represented by a single point (the MLE).\nReject \\(H_0\\) for large values of \\(W(\\boldsymbol \\theta_0)\\)\nunder \\(H_0\\) and for large \\(n\\) the statistic \\(W(\\boldsymbol \\theta_0)\\) is chi-squared distributed, i.e. \\(W(\\boldsymbol \\theta_0) \\overset{a}{\\sim} \\chi^2_d\\). This allows to compute critical values (i.e tresholds to declared rejection under a given significance level) and also \\(p\\)-values corresponding to the observed test statistics.\nModels outside the CI are rejected\nModels inside the CI cannot be rejected, i.e. they can’t be statistically distinguished from the best alternative model.\n\nIt can be shown that the likelihood ratio test to compare two simple models is optimal in the sense that for any given specified type I error (=probability of wrongly rejecting \\(H_0\\), i.e. the significance level) it will maximise the power (=1- type II error, probability of correctly accepting \\(H_1\\)). This is known as the Neyman-Pearson theorem.\nAs we have seen previousy, the likelihood-based confidence interval differs from the the confidence interval based on the quadratic / normal approximation. Correspondingly, tests based on the log-likelihood ratio \\(W(\\boldsymbol \\theta_0)\\) and on the squared Wald statistic \\(t(\\theta_0)^2\\) will also yield different outcomes (e.g. rejection due to lying outside the confidence interval) even though both test statistics share the same asymptotic distribution and critical values.\n\nExample 11.9 Likelihood test for a proportion:\nWe continue from Example 11.8 with 95% likelihood confidence interval \\([0.524, 0.843]\\).\nThe value \\(p_0=0.5\\) is outside the CI and hence can be rejected whereas \\(p_0=0.8\\) is insided the CI and hence cannot be rejected on 5% significance level.\nThe Wilks statistic for \\(p_0=0.5\\) and \\(p_0=0.8\\) takes on the following values:\n\n\\(W(0.5) = 4.94  &gt; 3.84\\) hence \\(p_0=0.5\\) can be rejected.\n\\(W(0.8) = 1.69  &lt; 3.84\\) hence \\(p_0=0.8\\) cannot be rejected.\n\nNote that the Wilks statistic at the boundaries of the likelihood confidence interval is equal to the critical value (3.84 corresponding to 5% significance level for a chi-squared distribution with 1 degree of freedom).\nCompare also with the normal test for a proportion in Example 10.9.\n\n\n\nOrigin of likelihood ratio statistic\nThe likelihood ratio statistic is asymptotically linked to differences in the KL divergences of the two compared models with the underlying true model.\nAssume that \\(F\\) is the true (and unknown) data generating model and that \\(G_{\\boldsymbol \\theta}\\) is a family of models and we would like to compare two candidate models \\(G_A\\) and \\(G_B\\) corresponding to parameters \\(\\boldsymbol \\theta_A\\) and \\(\\boldsymbol \\theta_B\\) on the basis of observed data \\(D = \\{x_1, \\ldots, x_n\\}\\). The KL divergences \\(D_{\\text{KL}}(F, G_A)\\) and \\(D_{\\text{KL}}(F, G_B)\\) indicate how close each of the models \\(G_A\\) and \\(G_B\\) fit the true \\(F\\). The difference of the two divergences is a way to measure the relative fit of the two models, and can be computed as \\[\nD_{\\text{KL}}(F, G_B)-D_{\\text{KL}}(F, G_A) = \\text{E}_{F} \\log \\frac{g(x|\\boldsymbol \\theta_A )}{g(x| \\boldsymbol \\theta_B)}\n\\] Replacing \\(F\\) by the empirical distribution \\(\\hat{F}_n\\) leads to the large sample approximation \\[\n2 n (D_{\\text{KL}}(F, G_B)-D_{\\text{KL}}(F, G_A))  \\approx 2 (l_n(\\boldsymbol \\theta_A| D) - l_n(\\boldsymbol \\theta_B| D))\n\\] Hence, the difference in the log-likelihoods provides an estimate of the difference in the KL divergence of the two models involved.\nThe Wilks log likelihood ratio statistic \\[\nW(\\boldsymbol \\theta_0) = 2 ( l_n( \\hat{\\boldsymbol \\theta}_{ML}| D ) - l_n(\\boldsymbol \\theta_0| D) )\n\\] thus compares the best-fit distribution with \\(\\hat{\\boldsymbol \\theta}_{ML}\\) as the parameter to the distribution with parameter \\(\\boldsymbol \\theta_0\\).\nFor exponential families the Wilks statistic can also be written in the form of the KL divergence: \\[\nW(\\boldsymbol \\theta_0) = 2n D_{\\text{KL}}( F_{\\hat{\\boldsymbol \\theta}_{ML}}, F_{\\boldsymbol \\theta_0})\n\\] This has been seen in Example 11.2 and Example 11.3. However, this is not true in general.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Likelihood-based confidence interval and likelihood ratio</span>"
    ]
  },
  {
    "objectID": "11-likelihood5.html#generalised-likelihood-ratio-test-glrt",
    "href": "11-likelihood5.html#generalised-likelihood-ratio-test-glrt",
    "title": "11  Likelihood-based confidence interval and likelihood ratio",
    "section": "11.2 Generalised likelihood ratio test (GLRT)",
    "text": "11.2 Generalised likelihood ratio test (GLRT)\nAlso known as maximum likelihood ratio test (MLRT). The Generalised Likelihood Ratio Test (GLRT) works just like the standard likelihood ratio test with the difference that now the null model \\(H_0\\) is also a composite model.\n\\[\\begin{align*}\n\\begin{array}{ll}\nH_0: \\boldsymbol \\theta\\in \\omega_0 \\subset \\Omega \\\\\nH_1: \\boldsymbol \\theta\\in \\omega_1  = \\Omega \\setminus \\omega_0\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{ True model lies in restricted model space }\\\\\n\\text{ True model is not the restricted model space } \\\\\n\\end{array}\n\\end{align*}\\]\nBoth \\(H_0\\) and \\(H_1\\) are now composite hypotheses. \\(\\Omega\\) represents the unrestricted model space with dimension (=number of free parameters) \\(d = |\\Omega|\\). The constrained space \\(\\omega_0\\) has degree of freedom \\(d_0 = |\\omega_0|\\) with \\(d_0 &lt; d\\). Note that in the standard LRT the set \\(\\omega_0\\) is a simple point with \\(d_0=0\\) as the null model is a simple distribution. Thus, LRT is contained in GLRT as special case!\nThe corresponding generalised (log) likelihood ratio statistic is given by\n\\[\nW = 2\\log\\left(\\frac{L(\\hat{\\theta}_{ML} |D )}{L(\\hat{\\theta}_{ML}^0 | D )}\\right)\n\\text{ and }\n\\Lambda = \\frac{\\underset{\\theta \\in \\omega_0}{\\max}\\, L(\\theta| D)}{\\underset{\\theta \\in \\Omega}{\\max}\\, L(\\theta | D)}\n\\]\nwhere \\(L(\\hat{\\theta}_{ML}| D)\\) is the maximised likelihood assuming the full model (with parameter space \\(\\Omega\\)) and \\(L(\\hat{\\theta}_{ML}^0| D)\\) is the maximised likelihood for the restricted model (with parameter space \\(\\omega_0\\)). Hence, to compute the GRLT test statistic we need to perform two optimisations, one for the full and another for the restricted model.\nRemarks:\n\nMLE in the restricted model space \\(\\omega_0\\) is taken as a representative of \\(H_0\\).\nThe likelihood is maximised in both numerator and denominator.\nThe restricted model is a special case of the full model (i.e. the two models are nested).\nThe asymptotic distribution of \\(W\\) is chi-squared with degree of freedom depending on both \\(d\\) and \\(d_0\\):\n\n\\[W \\overset{a}{\\sim} \\text{$\\chi^2_{d-d_0}$}\\]\n\nThis result is due to Wilks (1938) 1. Note that it assumes that the true model is contained among the investigated models.\nIf \\(H_0\\) is a simple hypothesis (i.e. \\(d_0=0\\)) then the standard LRT (and corresponding CI) is recovered as special case of the GLRT.\n\n\nExample 11.10 GLRT example:\nCase-control study: (e.g. “healthy” vs. “disease”)\nwe observe normal data \\(D = \\{x_1, \\ldots, x_n\\}\\) from two groups with sample size \\(n_1\\) and \\(n_2\\) (and \\(n=n_1+n_2\\)), with two different means \\(\\mu_1\\) and \\(\\mu_2\\) and common variance \\(\\sigma^2\\):\n\\[x_1,\\dots,x_{n_1} \\sim N(\\mu_1, \\sigma^2)\\] and \\[x_{n_1+1},\\dots,x_{n} \\sim N(\\mu_2, \\sigma^2)\\]\nQuestion: are the two means \\(\\mu_1\\) and \\(\\mu_2\\) the same in the two groups?\n\\[\\begin{align*}\n\\begin{array}{ll}\nH_0: \\mu_1=\\mu_2  \\text{ (with variance unknown, i.e. treated as nuisance parameter)}\n\\\\\nH_1: \\mu_1\\neq\\mu_2\\\\\n\\end{array}\n\\end{align*}\\]\nRestricted and full models:\n\\(\\omega_0\\): restricted model with two parameters \\(\\mu_0\\) and \\(\\sigma^2_0\\) (so that \\(x_{1},\\dots,x_{n} \\sim N(\\mu_0, \\sigma_0^2)\\) ).\n\\(\\Omega\\): full model with three parameters \\(\\mu_1, \\mu_2, \\sigma^2\\).\nCorresponding log-likelihood functions:\nRestricted model \\(\\omega_0\\): \\[\n\\log L(\\mu_0, \\sigma_0^2 | D) = -\\frac{n}{2} \\log(\\sigma_0^2)\n- \\frac{1}{2\\sigma_0^2} \\sum_{i=1}^n (x_i-\\mu_0)^2\n\\]\nFull model \\(\\Omega\\): \\[\n\\begin{split}\n\\log L(\\mu_1, \\mu_2, \\sigma^2 | D) & =\n\\left(-\\frac{n_1}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_{i=1}^{n_1} (x_i-\\mu_1)^2   \\right) + \\\\\n& \\phantom{==}\n\\left(-\\frac{n_2}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_{i=n_1+1}^{n} (x_i-\\mu_2)^2   \\right)\n\\\\\n&= -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n_1} (x_i-\\mu_1)^2 + \\sum_{i=n_1+1}^n (x_i-\\mu_2)^2 \\right) \\\\\n\\end{split}\n\\]\nCorresponding MLEs:\n\\[\\begin{align*}\n\\begin{array}{ll}\n\\omega_0:\\\\\n\\\\\n\\Omega:\\\\\n\\\\\n\\end{array}\n\\begin{array}{ll}\n\\hat{\\mu}_0 = \\frac{1}{n}\\sum^n_{i=1}x_i\\\\\n\\\\\n\\hat{\\mu}_1 = \\frac{1}{n_1}\\sum^{n_1}_{i=1}x_i\\\\\n\\hat{\\mu}_2 = \\frac{1}{n_2}\\sum^{n}_{i=n_1+1}x_i\\\\\n\\end{array}\n\\begin{array}{ll}\n\\widehat{\\sigma^2_0} = \\frac{1}{n}\\sum^n_{i=1}(x_i-\\hat{\\mu}_0)^2\\\\\n\\\\\n\\widehat{\\sigma^2} = \\frac{1}{n}\\left\\{\\sum^{n_1}_{i=1}(x_i-\\hat{\\mu}_1)^2+\\sum^n_{i=n_1+1}(x_i-\\hat{\\mu}_2)^2\\right\\}\\\\\n\\\\\n\\end{array}\n\\end{align*}\\]\nNote that the three estimated means are related by \\[\n\\begin{split}\n\\hat{\\mu}_0  & = \\frac{n_1}{n} \\hat{\\mu}_1 + \\frac{n_2}{n} \\hat{\\mu}_2 \\\\\n             & = \\hat{\\pi_1} \\hat{\\mu}_1 +\\hat{\\pi_2} \\hat{\\mu}_2 \\\\\n\\end{split}\n\\] so the overall mean is the weighted average of the two individual group means.\nMoreover, the two estimated variances are related by \\[\n\\widehat{\\sigma^2_0} =  \\hat{\\pi_1} \\hat{\\pi_2} (\\hat{\\mu}_1 - \\hat{\\mu}_2)^2 + \\widehat{\\sigma^2}\n\\] Note that this is an example of variance decomposition, where\n\n\\(\\widehat{\\sigma^2_0}\\) is the estimated total variance,\n\\(\\hat{\\pi_1} \\hat{\\pi_2} (\\hat{\\mu}_1 - \\hat{\\mu}_2)^2\\) the estimated between-group (explained) variance, and\n\\(\\widehat{\\sigma^2}\\) is the estimated average within-group (unexplained) variance.\n\nFor the following we also note that \\[\n\\begin{split}\n\\frac{ \\widehat{\\sigma^2_0} }{\\widehat{\\sigma^2}}\n& =  \\hat{\\pi_1} \\hat{\\pi_2} \\frac{ (\\hat{\\mu}_1 - \\hat{\\mu}_2)^2}{\\widehat{\\sigma^2}} + 1\\\\\n& =  \\frac{t^2_{\\text{ML}}}{n} +1 \\\\\n& =   \\frac{t^2_{\\text{UB}}}{n-2} +1 \\\\\n\\end{split}\n\\] where \\[\nt_{\\text{ML}} = \\sqrt{n \\hat{\\pi_1} \\hat{\\pi_2} }  \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\widehat{\\sigma^2}}\n\\] is the two sample \\(t\\)-statistic based on the ML variance estimate \\(\\widehat{\\sigma^2}\\) and \\(t_{\\text{UB}} = t_{ML} \\sqrt{\\frac{n-2}{n}}\\) is the conventional two sample \\(t\\)-statistic based on the unbiased variance estimate \\(\\widehat{\\sigma^2}_{\\text{UB}}=\\frac{n}{n-2} \\widehat{\\sigma^2}\\) (see Section A.9).\nCorresponding maximised log-likelihood:\nRestricted model:\n\\[\\log L(\\hat{\\mu}_0,\\widehat{\\sigma^2_0}| D) = -\\frac{n}{2} \\log(\\widehat{\\sigma^2_0}) -\\frac{n}{2} \\]\nFull model:\n\\[\n\\log L(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2}| D) = -\\frac{n}{2} \\log(\\widehat{\\sigma^2}) -\\frac{n}{2}\n\\]\nLikelihood ratio statistic:\n\\[\n\\begin{split}\nW & = 2\\log\\left(\\frac{L(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2} | D)}{L(\\hat{\\mu}_0,\\widehat{\\sigma^2_0} | D)}\\right)\\\\\n& = 2 \\log L\\left(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2}| D\\right) - 2 \\log L\\left(\\hat{\\mu}_0,\\widehat{\\sigma^2_0}| D\\right) \\\\\n& = n\\log\\left(\\frac{\\widehat{\\sigma^2_0}}{\\widehat{\\sigma^2}} \\right) \\\\\n& = n\\log\\left(\\frac{t^2_{\\text{ML}}}{n}+1\\right) \\\\\n& = n\\log\\left(\\frac{t^2_{\\text{UB}}}{n-2}+1\\right) \\\\\n\\end{split}\n\\]\nThus, the log-likelihood ratio statistic \\(W\\) is a monotonic function (a one-to-one transformation!) of the (squared) two sample \\(t\\)-statistic!\nAsymptotic distribution:\nThe degree of freedom of the full model is \\(d=3\\) and that of the constrained model \\(d_0=2\\) so the generalised log likelihood ratio statistic \\(W\\) is distributed asymptotically as \\(\\text{$\\chi^2_{1}$}\\). Hence, we reject the null model on 5% significance level for all \\(W &gt; 3.84\\).\n\nOther application of GLRTs\nAs shown above, the two sample \\(t\\) statistic can be derived as a likelihood ratio statistic.\nMore generally, it turns out that many other commonly used familiar statistical tests and test statistics can be interpreted as GLRTs. This shows the wide applicability of this procedure.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Likelihood-based confidence interval and likelihood ratio</span>"
    ]
  },
  {
    "objectID": "11-likelihood5.html#footnotes",
    "href": "11-likelihood5.html#footnotes",
    "title": "11  Likelihood-based confidence interval and likelihood ratio",
    "section": "",
    "text": "Wilks, S. S. 1938. The large-sample distribution of the likelihood ratio for testing composite hypotheses. Ann. Math. Statist. 9:60–62. https://doi.org/10.1214/aoms/1177732360↩︎",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Likelihood-based confidence interval and likelihood ratio</span>"
    ]
  },
  {
    "objectID": "12-likelihood6.html",
    "href": "12-likelihood6.html",
    "title": "12  Optimality properties and conclusion",
    "section": "",
    "text": "12.1 Properties of maximum likelihood encountered so far\n\\[\\begin{align*}\n\\begin{array}{cc}\n\\text{Kullback-Leibler 1951}\\\\\n\\textbf{Entropy learning: minimise  } D_{\\text{KL}}(F_{\\text{true}},F_{\\boldsymbol \\theta})\\\\\n\\downarrow\\\\\n\\text{large } n\\\\\n\\downarrow\\\\\n\\text{Fisher 1922}\\\\\n\\textbf{Maximise Likelihood  } L(\\boldsymbol \\theta|D)\\\\\n\\downarrow\\\\\n\\text{normal model}\\\\\n\\downarrow\\\\\n\\text{Gauss 1805}\\\\\n\\textbf{Minimise squared error  } \\sum_i (x_i-\\theta)^2\\\\\n\\end{array}\n\\end{align*}\\]",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optimality properties and conclusion</span>"
    ]
  },
  {
    "objectID": "12-likelihood6.html#properties-of-maximum-likelihood-encountered-so-far",
    "href": "12-likelihood6.html#properties-of-maximum-likelihood-encountered-so-far",
    "title": "12  Optimality properties and conclusion",
    "section": "",
    "text": "MLE is a special case of KL divergence minimisation valid for large samples.\nMLE can be seen as generalisation of least squares (and conversely, least squares is a special case of ML).\n\n\n\nGiven a model, derivation of the MLE is basically automatic (only optimisation required)!\nMLEs are consistent, i.e. if the true underlying model \\(F_{\\text{true}}\\) with parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) is contained in the set of specified candidates models \\(F_{\\boldsymbol \\theta}\\) then the MLE will converge to the true model.\nCorrespondingly, MLEs are asympotically unbiased.\nHowever, MLEs are not necessarily unbiased in finite samples (e.g. the MLE of the variance parameter in the normal distribution).\nThe maximum likelihood is invariant against parameter transformations.\nIn regular situations (when local quadratic approximation is possible) MLEs are asympotically normally distributed, with the asymptotic variance determined by the observed Fisher information.\nIn regular situations and for large sample size MLEs are asympotically optimally efficient (Cramer-Rao theorem): For large samples the MLE achieves the lowest possible variance possible in an estimator — this is the so-called Cramer-Rao lower bound. The variance decreases to zero with \\(n \\rightarrow \\infty\\) typically with rate \\(1/n\\).\nThe likelihood ratio can be used to construct optimal tests (in the sense of the Neyman-Pearson theorem).",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optimality properties and conclusion</span>"
    ]
  },
  {
    "objectID": "12-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency",
    "href": "12-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency",
    "title": "12  Optimality properties and conclusion",
    "section": "12.2 Summarising data and the concept of (minimal) sufficiency",
    "text": "12.2 Summarising data and the concept of (minimal) sufficiency\n\nInduced partioning of data space and likelihood equivalence\nEvery sufficient statistic \\(t(D)\\) induces a partitioning of the space of data sets by clustering all hypothetical outcomes for which the statistic \\(t(D)\\) assumes the same value \\(t\\): \\[\\mathcal{X}_t = \\{D: t(D) = t\\}\\] The data sets in \\(\\mathcal{X}_t\\) are equivalent in terms of the sufficient statistic \\(t(D)\\). Note that this implies that \\(t(D)\\) is not a 1:1 transformation of \\(D\\). Instead of \\(n\\) data points \\(x_1, \\ldots, x_n\\) as few as one or two summaries (such as empirical mean and variance) may be sufficient to fully convey all the information in the data about the model parameters. Thus, transforming data \\(D\\) using a sufficient statistic \\(t(D)\\) may result in substantial data reduction.\nTwo data sets \\(D_1\\) and \\(D_2\\) for which the ratio of the corresponding likelihoods \\(L(\\boldsymbol \\theta| D_1 )/L(\\boldsymbol \\theta| D_2)\\) does not depend on \\(\\boldsymbol \\theta\\) (so the two likelihoods are proportional to each other by a constant) are called likelihood equivalent because a likelihood-based procedure to learn about \\(\\boldsymbol \\theta\\) will draw identical conclusions from \\(D_1\\) and \\(D_2\\). For data sets \\(D_1, D_2 \\in \\mathcal{X}_t\\) which are equivalent with respect to a sufficient statistic \\(T\\) it follows directly from the Fisher-Pearson factorisation that the ratio \\[L(\\boldsymbol \\theta| D_1 )/L(\\boldsymbol \\theta| D_2) = k(D_1)/ k(D_2)\\] and thus is constant with regard to \\(\\boldsymbol \\theta\\). As a result, all data sets in \\(\\mathcal{X}_t\\) are likelihood equivalent. However, the converse is not true: depending on the sufficient statistics there usually will be many likelihood equivalent data sets that are not part of the same set \\(\\mathcal{X}_t\\).\n\n\nMinimal sufficient statistics\nOf particular interest is therefore to find those sufficient statistics that achieve the coarsest partitioning of the sample space and thus may allow the highest data reduction. Specifically, a minimal sufficient statistic is a sufficient statistic for which all likelihood equivalent data sets also are equivalent under this statistic.\nTherefore, to check whether a sufficient statistic \\(t(D)\\) is minimally sufficient we need to verify whether for any two likelihood equivalent data sets \\(D_1\\) and \\(D_2\\) it also follows that \\(t(D_1) = t(D_2)\\). If this holds true then \\(T\\) is a minimally sufficient statistic.\nAn equivalent non-operational definition is that a minimal sufficient statistic \\(t(D)\\) is a sufficient statistic that can be computed from any other sufficient statistic \\(S(D)\\). This follows from the above directly: assume any sufficient statistic \\(S(D)\\), this defines a corresponding set \\(\\mathcal{X}_s\\) of likelihood equivalent data sets. By implication any \\(D_1, D_2 \\in \\mathcal{X}_s\\) will necessarily also be in \\(\\mathcal{X}_t\\), thus whenever \\(S(D_1)=S(D_2)\\) we also have \\(t(D_1)=t(D_2)\\), and therefore \\(t(D_1)\\) is a function of \\(S(D_1)\\).\nA trivial but important example of a minimal sufficient statistic is the likelihood function itself since by definition it can be computed from any set of sufficient statistics. Thus the likelihood function \\(L(\\boldsymbol \\theta)\\) captures all information about \\(\\boldsymbol \\theta\\) that is available in the data. In other words, it provides an optimal summary of the observed data with regard to a model. Note that in Bayesian statistics (to be discussed in Part 2 of the module) the likelihood function is used as proxy/summary of the data.\n\n\nExample: normal distribution\n\nExample 12.1 Sufficient statistics for the parameters of the normal distribution:\nThe normal model \\(N(\\mu, \\sigma^2)\\) with parameter vector \\(\\boldsymbol \\theta= (\\mu, \\sigma^2)^T\\) and log-likelihood \\[\nl_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2)  - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (x_i-\\mu)^2\n\\] One possible set of minimal sufficient statistics for \\(\\boldsymbol \\theta\\) are \\(\\bar{x}\\) and \\(\\overline{x^2}\\), and with these we can rewrite the log-likelihood function without any reference to the original data \\(x_1, \\ldots, x_n\\) as follows \\[\nl_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2)\n-\\frac{n}{2 \\sigma^2} (\\overline{x^2} - 2 \\bar{x} \\mu + \\mu^2)\n\\] An alternative set of minimal sufficient statistics for \\(\\boldsymbol \\theta\\) consists of \\(s^2 = \\overline{x^2} - \\bar{x}^2 = \\widehat{\\sigma^2}_{ML}\\) as and \\(\\bar{x} = \\hat{\\mu}_{ML}\\). The log-likelihood written in terms of \\(s^2\\) and \\(\\bar{x}\\) is \\[\nl_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2)\n-\\frac{n}{2 \\sigma^2} (s^2 + (\\bar{x} - \\mu)^2 )\n\\]\nNote that in this example the dimension of the parameter vector \\(\\boldsymbol \\theta\\) equals the dimension of the minimal sufficient statistic, and furthermore, that the MLEs of the parameters are in fact minimal sufficient!\n\n\n\nMLEs of parameters of an exponential family are minimal sufficient statistics\nThe conclusion from Example 12.1 holds true more generally: in an exponential family model (such as the normal distribution as particular important case) the MLEs of the parameters are minimal sufficient statistics. Thus, there will typically be substantial dimension reduction from the raw data to the sufficient statistics.\nHowever, outside exponential families the MLE is not necessarily a minimal sufficient statistic, and may not even be a sufficient statistic. This is because a (minimal) sufficient statistic of the same dimension as the parameters does not always exist. A classic example is the Cauchy distribution for which the minimal sufficient statistics are the ordered observations, thus the MLE of the parameters do not constitute sufficient statistics, let alone minimal sufficient statistics. However, the MLE is of course still a function of the minimal sufficient statistic.\nIn summary, the likelihood function acts as perfect data summariser (i.e. as minimally sufficient statistic), and in exponential families (e.g. normal distribution) the MLEs of the parameters \\(\\hat{\\boldsymbol \\theta}_{ML}\\) are minimal sufficient.\nFinally, while sufficiency is clearly a useful concept for data reduction one needs to keep in mind that this is always in reference to a specific model. Therefore, unless one strongly believes in a certain model it is generally a good idea to keep (and not discard!) the original data.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optimality properties and conclusion</span>"
    ]
  },
  {
    "objectID": "12-likelihood6.html#concluding-remarks-on-maximum-likelihood",
    "href": "12-likelihood6.html#concluding-remarks-on-maximum-likelihood",
    "title": "12  Optimality properties and conclusion",
    "section": "12.3 Concluding remarks on maximum likelihood",
    "text": "12.3 Concluding remarks on maximum likelihood\n\nApplication of KL divergence in statistics\nIn statistics the typical roles of the distribution \\(Q\\) and \\(P\\) in the KL divergence \\(D_{\\text{KL}}(Q, P)\\) are:\n\n\\(Q\\) is the (unknown) underlying true model for the data generating process\n\\(P\\) is the approximating model (typically a parametric distribution family)\n\nOptimising (i.e. minimising) the KL divergence with regard to \\(P\\) amounts to approximation and optimising with regard to \\(Q\\) to imputation.\nIn previous chapters we have seen how the KL divergence leads to the maximum likelihood (via minimum cross-entropy) and also allows to choose distribution families (via maximum entropy) Later we will also see how KL divergence is linked to Bayesian learning, respectively.\nSince the KL divergence is not symmetric there two distinct ways to minimise the divergence between a fixed \\(F_0\\) and the family \\(F_{\\boldsymbol \\theta}\\) (see Figure 12.1). minimising the parameter \\(\\boldsymbol \\theta\\) in \\(D_{\\text{KL}}(\\hat{F}_0,F_{\\boldsymbol \\theta})\\) (“forward KL”) and in \\(D_{\\text{KL}}(F_{\\boldsymbol \\theta}, \\hat{F}_0)\\) (“backward KL”).\nEach way has different properties:\n\n\n\n\n\n\n\n\n\n\n\n(a) Forward KL\n\n\n\n\n\n\n\n\n\n\n\n(b) Reverse KL\n\n\n\n\n\n\n\nFigure 12.1: Illustration of (a) forward KL and b) reverse KL optimisation.\n\n\n\n\nforward KL, approximation KL: \\(\\min_{\\boldsymbol \\theta}  D_{\\text{KL}}(F_0,F_{\\boldsymbol \\theta})\\)\nHere we keep the first argument fixed and minimise KL by changing the second argument. This is also called an “M (Moment) projection”. It has a zero avoiding property: \\(f_{\\boldsymbol \\theta}(x)&gt;0 \\text{ whenever } f_0(x)&gt;0\\).\nThis procedure is mean-seeking and inclusive, i.e. when there are multiple modes in the density of \\(F_0\\) a fitted unimodal density \\(F_{\\hat{\\boldsymbol \\theta}}\\) will seek to cover all modes (mass covering property).\nMaximum likelihood is based on “forward KL”.\nreverse KL, inference KL: \\(\\min_{\\boldsymbol \\theta} D_{\\text{KL}}(F_{\\boldsymbol \\theta},F_0)\\)\nHere we keep the second argument fixed and minimise KL by changing the first argument. This is also called an “I (Information) projection”. It has a zero forcing property: \\(f_{\\boldsymbol \\theta}(x)=0 \\text{ whenever } f_0(x)=0\\).\nThis procedure is mode-seeking and exclusive, i.e. when there are multiple modes in the density of \\(F_0\\) a fitted unimodal density \\(F_{\\hat{\\boldsymbol \\theta}}\\) will seek out one mode to the exclusion of the others (mode attracting property).\nBayesian updating and variational Bayes approximations use “reverse KL”.\n\n\n\nWhat happens if \\(n\\) is small?\nFrom the long list of optimality properties of ML it is clear that for large sample size \\(n\\) the best estimator will typically be the MLE.\nHowever, for small sample size it is indeed possible (and necessary) to improve over the MLE (e.g. via Bayesian estimation or regularisation). Some of these ideas will be discussed in Part II.\n\nLikelihood will overfit!\n\nAlternative methods need to be used:\n\nregularised/penalised likelihood\nBayesian methods\n\nwhich are essentially two sides of the same coin.\nClassic example of a simple non-ML estimator that is better than the MLE: Stein’s example / Stein paradox (C. Stein, 1955):\n\nProblem setting: estimation of the mean in multivariate case\nMaximum likelihood estimation breaks down! \\(\\rightarrow\\) average (=MLE) is worse in terms of MSE than Stein estimator.\nFor small \\(n\\) the asymptotic distributions for the MLE and for the LRT are not accurate, so for inference in these situations the distributions may need to be obtained by simulation (e.g. parametric or nonparametric bootstrap).\n\n\n\nModel selection\n\nCI are sets of models that are not statistically distinguishable from the best ML model\nin doubt, choose the simplest model compatible with data\nbetter prediction, avoids overfitting\nUseful for model exploration and model building.\nNote that, by construction, the model with more parameters always has a higher likelihood, implying likelihood favours complex models\nComplex model may overfit!\n\nFor comparison of models penalised likelihood or Bayesian approaches may be necessary\nModel selection in small samples and high dimension is challenging\nRecall that the aim in statistics is not about rejecting models (this is easy as for large sample size any model will be rejected!)\n\nInstead, the aim is model building, i.e. to find a model that explains the data well and that predicts well!\n\nTypically, this will not be the best-fit ML model, but rather a simpler model that is close enough to the best / most complex model.",
    "crumbs": [
      "Entropy and likelihood",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optimality properties and conclusion</span>"
    ]
  },
  {
    "objectID": "13-bayes1.html",
    "href": "13-bayes1.html",
    "title": "13  Conditioning and Bayes rule",
    "section": "",
    "text": "13.1 Conditional probability\nIn this chapter we review conditional probabilities. Conditional probability is essential for Bayesian statistical modelling.\nAssume we have two random variables \\(x\\) and \\(y\\) with a joint density (or joint PMF) \\(p(x,y)\\). By definition \\(\\int_{x,y} p(x,y) dx dy = 1\\).\nThe marginal densities for the individual \\(x\\) and \\(y\\) are given by \\(p(x) = \\int_y p(x,y) dy\\) and \\(p(y) = \\int_x f(x,y) dx\\). Thus, marginal densities are obtained from the joint density by integrating over all possible states of the variable that is removed. Like the joint density the marginal densities also integrate to 1, i.e. \\(\\int_x p(x) dx = 1\\) and \\(\\int_y p(y) dy = 1\\).\nAs alternative to integrating out a random variable in the joint density \\(p(x,y)\\) we may wish to keep it fixed at some value, we may want to keep \\(y\\) fixed at \\(y_0\\). In this case \\(p(x, y=y_0)\\) is proportional to the conditional density (or PMF) given by the ratio \\[\np(x | y=y_0) = \\frac{p(x, y=y_0)}{p(y=y_0)}\n\\] In this formula the denominator \\(p(y=y_0) = \\int_x p(x, y=y_0) dx\\) ensures that \\(\\int_x p(x | y=y_0) dx = 1\\), thus it renormalises \\(p(x, y=y_0)\\) so that it is a proper density integrating to one.\nTo simplify notation, the particular value on which a variable is conditioned is often left unspecified so we just write \\(p(x | y)\\).",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conditioning and Bayes rule</span>"
    ]
  },
  {
    "objectID": "13-bayes1.html#bayes-theorem",
    "href": "13-bayes1.html#bayes-theorem",
    "title": "13  Conditioning and Bayes rule",
    "section": "13.2 Bayes’ theorem",
    "text": "13.2 Bayes’ theorem\nThomas Bayes (1701-1761) was the first to state Bayes’ theorem on conditional probabilities.\nUsing the definition of conditional probabilities we see that the joint density can be written as the product of marginal and conditional density in two different ways: \\[\np(x,y) = p(x| y) p(y) = p(y | x) p(x)\n\\]\nThis directly leads to Bayes’ theorem: \\[\np(x | y) = p(y | x) \\frac{ p(x) }{ p(y)}\n\\] This rule relates the two possible conditional densities (or conditional probability mass functions) for two random variables \\(x\\) and \\(y\\). It thus allows to reverse the ordering of conditioning.\nBayes’s theorem was published in 1763 only after his death by Richard Price (1723-1791):\nPierre-Simon Laplace independently published Bayes’ theorem in 1774 and he was in fact the first to routinely apply it to statistical calculations.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conditioning and Bayes rule</span>"
    ]
  },
  {
    "objectID": "13-bayes1.html#conditional-mean-and-variance",
    "href": "13-bayes1.html#conditional-mean-and-variance",
    "title": "13  Conditioning and Bayes rule",
    "section": "13.3 Conditional mean and variance",
    "text": "13.3 Conditional mean and variance\nThe mean \\(\\text{E}(x| y)\\) and variance \\(\\text{Var}(x|y)\\) of the conditional distribution with density \\(p(x|y)\\) are called conditional mean and conditional variance.\nThe law of total expectation states that \\[\n\\text{E}(x) = \\text{E}( \\text{E}(x| y) )\n\\]\nThe law of total variance states that \\[\n\\text{Var}(x) = \\text{Var}(\\text{E}(x| y)) + \\text{E}(\\text{Var}(x|y))\n\\] The first term on the right-hand side is the “explained” variance or “between-group” variance, and the second term is the “unexplained” variance or “mean within group” variance.\n\nExample 13.1 Mean and variance of a mixture model:\nAssume \\(K\\) groups indicated by a discrete variable \\(y = 1, 2, \\ldots, K\\) with probability \\(p(y) = \\pi_y\\). In each group the observations \\(x\\) follow a density \\(p(x|y)\\) with conditional mean \\(E(x|y) = \\mu_y\\) and conditional variance \\(\\text{Var}(x| y)= \\sigma^2_y\\). The joint density for \\(x\\) and \\(y\\) is \\(p(x, y) = \\pi_y p(x|y)\\). The marginal density for \\(x\\) is \\(\\sum_{y=1}^K \\pi_y p(x|y)\\). This is called a mixture model.\nThe total mean \\(\\text{E}(x) = \\mu_0\\) is equal to \\(\\sum_{y=1}^K \\pi_y \\mu_y\\).\nThe total variance \\(\\text{Var}(x) = \\sigma^2_0\\) is equal to \\[\n\\sum_{y=1}^K \\pi_y (\\mu_y - \\mu_0)^2 + \\sum_{y=1}^K \\pi_y \\sigma^2_y\n\\]",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conditioning and Bayes rule</span>"
    ]
  },
  {
    "objectID": "13-bayes1.html#conditional-entropy-and-entropy-chain-rules",
    "href": "13-bayes1.html#conditional-entropy-and-entropy-chain-rules",
    "title": "13  Conditioning and Bayes rule",
    "section": "13.4 Conditional entropy and entropy chain rules",
    "text": "13.4 Conditional entropy and entropy chain rules\nFor the entropy of the joint distribution we find that \\[\n\\begin{split}\nH( P_{x,y}) &= -\\text{E}_{P_{x,y}} \\log p(x, y) \\\\\n&= -\\text{E}_{P_x} \\text{E}_{P_{y| x}} (\\log p(x) + \\log p(y| x))\\\\\n&= -\\text{E}_{P_x} \\log p(x) - \\text{E}_{P_x} \\text{E}_{P_{y| x}} \\log p(y| x)\\\\\n&= H(P_{x}) + H(P_{y| x} ) \\\\\n\\end{split}\n\\] thus it decomposes into the entropy of the marginal distribution and the conditional entropy defined as \\[\nH(P_{y| x} ) = - \\text{E}_{P_x} \\text{E}_{P_{y| x}} \\log p(y| x)\n\\] Note that to simplify notation by convention the expectation \\(\\text{E}_{P_{x}}\\) over the variable \\(x\\) that we condition on (\\(x\\)) is implicitly assumed.\nSimilarly, for the cross-entropy we get \\[\n\\begin{split}\nH(Q_{x,y} , P_{x, y}) &= -\\text{E}_{Q_{x,y}} \\log  p(x, y) \\\\\n&= -\\text{E}_{Q_x} \\text{E}_{Q_{y| x}} \\log \\left(\\, p(x)\\, p(y| x)\\, \\right)\\\\\n  &= -\\text{E}_{Q_x} \\log p(x)    -\\text{E}_{Q_x} \\text{E}_{Q_{y| x}}  \\log  p(y| x)         \\\\\n&= H(Q_x, P_x)  +  H(Q_{y|x}, P_{y|x})\n\\end{split}\n\\] where the conditional cross-entropy is defined as \\[\nH(Q_{y|x}, P_{y|x})= -\\text{E}_{Q_x} \\text{E}_{Q_{y| x}}  \\log  p(y| x)\n\\] Note again the implicit expectation \\(\\text{E}_{Q_x}\\) over \\(x\\) implied in this notation.\nThe KL divergence between the joint distributions can be decomposed as follows: \\[\n\\begin{split}\nD_{\\text{KL}}(Q_{x,y} , P_{x, y}) &= \\text{E}_{Q_{x,y}} \\log \\left(\\frac{ q(x, y) }{ p(x, y) }\\right)\\\\\n&= \\text{E}_{Q_x} \\text{E}_{Q_{y| x}} \\log \\left(\\frac{ q(x) q(y| x) }{ p(x) p(y| x) }\\right)\\\\\n&= \\text{E}_{Q_x} \\log \\left(\\frac{ q(x) }{ p(x) }\\right)      + \\text{E}_{Q_x} \\text{E}_{Q_{y| x}}  \\log \\left(\\frac{  q(y| x) }{ p(y| x) }\\right)          \\\\\n&= D_{\\text{KL}}(Q_{x} , P_{x}) +   D_{\\text{KL}}(Q_{y| x} , P_{y|x}) \\\\\n\\end{split}\n\\] with the conditional KL divergence or conditional KL divergence defined as \\[\nD_{\\text{KL}}(Q_{y| x} , P_{y|x})  = \\text{E}_{Q_x} \\text{E}_{Q_{y| x}}  \\log \\left(\\frac{  q(y| x) }{ p(y| x) }\\right)\n\\] (again the expectation \\(\\text{E}_{Q_{x}}\\) is usually dropped for convenience). The conditional KL divergence can also be computed from the conditional (cross-)entropies by the familiar relationship \\[\nD_{\\text{KL}}(Q_{y| x} , P_{y|x}) = H(Q_{y|x}, P_{y|x})\n-  H(Q_{y| x})\n\\]\nThe above decompositions for the entropy, the cross-entropy and KL divergence are known as entropy chain rules.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conditioning and Bayes rule</span>"
    ]
  },
  {
    "objectID": "13-bayes1.html#entropy-bounds-for-the-marginal-variables",
    "href": "13-bayes1.html#entropy-bounds-for-the-marginal-variables",
    "title": "13  Conditioning and Bayes rule",
    "section": "13.5 Entropy bounds for the marginal variables",
    "text": "13.5 Entropy bounds for the marginal variables\nThe chain rule for KL divergence directly shows that \\[\n\\begin{split}\n\\underbrace{D_{\\text{KL}}(Q_{x,y} , P_{x, y})}_{\\text{upper bound}} &= D_{\\text{KL}}(Q_{x} , P_{x}) + \\underbrace{  D_{\\text{KL}}(Q_{y| x} , P_{y|x})   }_{\\geq 0}\\\\\n&\\geq D_{\\text{KL}}(Q_{x} , P_{x})\n\\end{split}\n\\] This means that the KL divergence between the joint distributions forms an upper bound for the KL divergence between the marginal distributions, with the difference given by the conditional KL divergence \\(D_{\\text{KL}}(Q_{y| x} , P_{y|x})\\).\nEquivalently, we can state an upper bound for the marginal cross-entropy: \\[\n\\begin{split}\n\\underbrace{H(Q_{x,y} , P_{x, y}) - H(Q_{y| x} )}_{\\text{upper bound}} &= H(Q_{x}, P_{x})  + \\underbrace{ D_{\\text{KL}}(Q_{y| x} , P_{y|x}) }_{\\geq 0}\\\\\n& \\geq H(Q_{x}, P_{x}) \\\\\n\\end{split}\n\\] Instead of an upper bound we may as well express this as lower bound for the negative marginal cross-entropy \\[.\n\\begin{split}\n- H(Q_{x}, P_{x}) &= \\underbrace{ - H(Q_{x} Q_{y| x} , P_{x, y})  + H(Q_{y| x} )}_{\\text{lower bound}}  + \\underbrace{ D_{\\text{KL}}(Q_{y| x} , P_{y|x})}_{\\geq 0}\\\\\n& \\geq F\\left( Q_{x}, Q_{y| x},  P_{x, y}\\right)\\\\\n\\end{split}\n\\]\nSince entropy and KL divergence is closedly linked with maximum likelihood the above bounds play a major role in statistical learning of models with unobserved latent variables (here \\(y\\)). They form the basis of important methods such as the EM algorithm as well as of variational Bayes.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conditioning and Bayes rule</span>"
    ]
  },
  {
    "objectID": "14-bayes2.html",
    "href": "14-bayes2.html",
    "title": "14  Models with latent variables and missing data",
    "section": "",
    "text": "14.1 Complete data log-likelihood versus observed data log-likelihood\nIt is frequently the case that we need to employ models where not all variables are observable and the corresponding data are missing.\nFor example consider two random variables \\(x\\) and \\(y\\) with a joint density \\[\np(x, y| \\boldsymbol \\theta)\n\\] and parameters \\(\\boldsymbol \\theta\\). If we observe data \\(D_x = \\{ x_1, \\ldots, x_n\\}\\) and \\(D_y = \\{ y_1, \\ldots, y_n\\}\\) for \\(n\\) samples we can use the complete data log-likelihood \\[\nl_n(\\boldsymbol \\theta| D_x, D_y) = \\sum_{i=1}^n  \\log p(x_i, y_i| \\boldsymbol \\theta)\n\\] to estimate \\(\\boldsymbol \\theta\\). Recall that \\[\nl_n(\\boldsymbol \\theta| D_x, D_y) =-n H(\\hat{Q}_{x,y}, P_{x, y|\\boldsymbol \\theta})\n\\] where \\(\\hat{Q}_{x,y}\\) is the empirical joint distribution based on both \\(D_x\\) and \\(D_y\\) and \\(P_{x, y|\\boldsymbol \\theta}\\) the joint model, so maximising the complete data log-likelihood minimises the cross-entropy \\(H(\\hat{Q}_{x,y}, P_{x, y|\\boldsymbol \\theta})\\).\nNow assume that \\(y\\) is not observable and hence is a so-called latent variable. Then we don’t have observations \\(D_y\\) and therefore cannot use the complete data likelihood. Instead, for maximum likelihood estimation with missing data we need to use the observed data log-likelihood.\nFrom the joint density we obtain the marginal density for \\(x\\) by integrating out the unobserved variable \\(y\\): \\[\np(x | \\boldsymbol \\theta) = \\int_y  p(x, y| \\boldsymbol \\theta) dy\n\\] Using the marginal model we then compute the observed data log-likelihood \\[\nl_n(\\boldsymbol \\theta| D_x) = \\sum_{i=1}^n  \\log p(x_i| \\boldsymbol \\theta) =\\sum_{i=1}^n \\log \\int_y  p(x_i, y| \\boldsymbol \\theta) dy\n\\] Note that only the data \\(D_x\\) are used.\nMaximum likelihood estimation based on the marginal model proceeds as usual by maximising the corresponding observed data likelihood function which is \\[\nl_n(\\boldsymbol \\theta| D_x) = -n H(\\hat{Q}_{x}, P_{x|\\boldsymbol \\theta})\n\\] where \\(\\hat{Q}_{x}\\) is the empirical distribution based only on \\(D_x\\) and \\(P_{x|\\boldsymbol \\theta}\\) is the model family. Hence, maximising the observed data log-likelihood minimises the cross-entropy \\(H(\\hat{Q}_{x}, P_{x|\\boldsymbol \\theta})\\).",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models with latent variables and missing data</span>"
    ]
  },
  {
    "objectID": "14-bayes2.html#complete-data-log-likelihood-versus-observed-data-log-likelihood",
    "href": "14-bayes2.html#complete-data-log-likelihood-versus-observed-data-log-likelihood",
    "title": "14  Models with latent variables and missing data",
    "section": "",
    "text": "Example 14.1 Two group normal mixture model:\nAssume we have two groups labelled by \\(y=1\\) and \\(y=2\\) (thus the variable \\(y\\) is discrete). The data \\(x\\) observed in each group are normal with means \\(\\mu_1\\) and \\(\\mu_2\\) and variances \\(\\sigma^2_1\\) and \\(\\sigma^2_2\\), respectively. The probability of group \\(1\\) is \\(\\pi_1 = p\\) and the probability of group \\(2\\) is \\(\\pi_2=1-p\\). The density of the joint model for \\(x\\) and \\(y\\) is \\[\np(x, y| \\boldsymbol \\theta)  = \\pi_y N(x| \\mu_y, \\sigma_y)\n\\] The model parameters are \\(\\boldsymbol \\theta= (p, \\mu_1, \\mu_2, \\sigma^2_1, \\sigma^2_2)^T\\) and they can be inferred from the complete data comprised of \\(D_x = \\{x_1, \\ldots, x_n\\}\\) and the group allocations \\(D_y=\\{y_1, \\ldots, y_n\\}\\) of each sample using the complete data log-likelihood \\[\nl_n(\\boldsymbol \\theta| D_x, D_y  ) =\\sum_{i=1}^n  \\log \\pi_{y_i} + \\sum_{i=1}^n \\log  N(x_i| \\mu_{y_i}, \\sigma_{y_i})\n\\]\nHowever, typically we do not know the class allocation \\(y\\) and thus we need to use the marginal model for \\(x\\) alone which has density \\[\n\\begin{split}\np(x| \\boldsymbol \\theta) &= \\sum_{y=1}^2 \\pi_y N(\\mu_y, \\sigma^2_y) \\\\\n&= p N(x| \\mu_1, \\sigma^2_1) + (1-p)  N(x | \\mu_2, \\sigma^2_2)\\\\\n\\end{split}\n\\] This is an example of a two-component mixture model. The corresponding observed data log-likelihood is \\[\nl_n(\\boldsymbol \\theta| D_x ) = \\sum_{i=1}^n  \\log \\sum_{y=1}^2 \\pi_y N(x |\\mu_y, \\sigma^2_y)\n\\] Note that the form of the observed data log-likelihood is more complex than that of the complete data log-likelihood because it contains the logarithm of a sum that cannot be simplified. It is used to estimate the model parameters \\(\\boldsymbol \\theta\\) from \\(D_x\\) without requiring knowledge of the class allocations \\(D_y\\).\n\n\nExample 14.2 Alternative computation of the observed data likelihood:\nAn alternative way to arrive at the observed data likelihood is to marginalise the complete data likelihood. \\[\nL_n(\\boldsymbol \\theta| D_x, D_y) = \\prod_{i=1}^n p(x_i, y_i| \\boldsymbol \\theta)\n\\] and \\[\nL_n(\\boldsymbol \\theta| D_x) = \\int_{y_1, \\ldots, y_n} \\prod_{i=1}^n p(x_i, y_i| \\boldsymbol \\theta) dy_1 \\ldots dy_n\n\\] The integration (sum) and the multiplication can be interchanged as per Generalised Distributive Law leading to \\[\nL_n(\\boldsymbol \\theta| D_x) =  \\prod_{i=1}^n \\int_{y} p(x_i, y| \\boldsymbol \\theta) dy\n\\] which is the same as constructing the likelihood from the marginal density.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models with latent variables and missing data</span>"
    ]
  },
  {
    "objectID": "14-bayes2.html#estimation-of-the-unobservable-latent-states-using-bayes-theorem",
    "href": "14-bayes2.html#estimation-of-the-unobservable-latent-states-using-bayes-theorem",
    "title": "14  Models with latent variables and missing data",
    "section": "14.2 Estimation of the unobservable latent states using Bayes theorem",
    "text": "14.2 Estimation of the unobservable latent states using Bayes theorem\nAfter estimating the marginal model it is straightforward to obtain a probabilistic prediction about the state of the latent variables \\(y_1, \\ldots, y_n\\). Since \\[\np(x, y | \\boldsymbol \\theta) = p( x|\\boldsymbol \\theta) \\, p(y | x, \\boldsymbol \\theta) =  p( y|\\boldsymbol \\theta) \\, p(x | y, \\boldsymbol \\theta)\n\\] given an estimate \\(\\hat{\\boldsymbol \\theta}\\) we are able to compute for each observation \\(x_i\\) \\[\np(y_i | x_i , \\hat{\\boldsymbol \\theta}) = \\frac{p(x_i, y_i | \\hat{\\boldsymbol \\theta} ) }{p(x_i|\\hat{\\boldsymbol \\theta})}\n=\\frac{  p( y_i|\\hat{\\boldsymbol \\theta}) \\, p(x_i | y_i, \\hat{\\boldsymbol \\theta})     }{p(x_i|\\hat{\\boldsymbol \\theta})}\n\\] the probabilities / densities of all states of \\(y_i\\) (note this an application of Bayes’ theorem).\n\nExample 14.3 Latent states of two group normal mixture model:\nContinuing from Example 14.1 above we assume the marginal model has been fitted with parameter values \\(\\hat{\\boldsymbol \\theta} = (\\hat{p},\\hat{\\mu}_1, \\hat{\\mu}_2, \\widehat{\\sigma^2_1}, \\widehat{\\sigma^2_2} )^T\\). Then for each sample \\(x_i\\) we can get probabilistic prediction about group assocation of each sample by \\[\np(y_i | x_i, \\hat{\\boldsymbol \\theta}) = \\frac{\\hat{\\pi}_{y_i} N(x_i| \\hat{\\mu}_{y_i}, \\widehat{\\sigma^2_{y_i}})}{\\hat{p} N(x_i| \\hat{\\mu}_1, \\widehat{\\sigma^2_1}) + (1-\\hat{p})  N(x_i | \\hat{\\mu}_2,  \\widehat{\\sigma^2_2})}\n\\]",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models with latent variables and missing data</span>"
    ]
  },
  {
    "objectID": "14-bayes2.html#em-algorithm",
    "href": "14-bayes2.html#em-algorithm",
    "title": "14  Models with latent variables and missing data",
    "section": "14.3 EM Algorithm",
    "text": "14.3 EM Algorithm\nComputing and maximising the observed data log-likelihood can be difficult because of the integration over the unobserved variable (or summation in case of a discrete latent variable). In contrast, the complete data log-likelihood function may be easier to compute.\nThe widely used EM algorithm, formally described by Dempster and others (1977) but also used before, addresses this problem and maximises the observed data log-likelihood indirectly in an iterative procedure comprising two steps:\n\nFirst (“E” step), the missing data \\(D_y\\) is imputed using Bayes’ theorem. This provides probabilities (“soft allocations”) for each possible state of the latent variable.\nSubsequently (“M” step), the expected complete data log-likelihood function is computed, where the expectation is taken with regard to the distribution over the latent states, and it is maximised with regard to \\(\\boldsymbol \\theta\\) to estimate the model parameters.\n\nThe EM algorithm leads to the exact same estimates as if the observed data log-likelihood would be optimised directly. Therefore the EM algorithm is in fact not an approximation, it is just a different way to find the MLEs.\nThe EM algorithm and application to clustering is discussed in more detail in the module MATH38161 Multivariate Statistics and Machine Learning.\nIn a nutshell, the justication for the EM algorithm follows from the entropy chain rules and the corresponding bounds, such as \\(D_{\\text{KL}}(Q_{x,y} , P_{x, y}) \\geq D_{\\text{KL}}(Q_{x} , P_{x})\\) (see previous chapter). Given observed data for \\(x\\) we know the empirical distribution \\(\\hat{Q}_x\\). Hence, by minimising \\(D_{\\text{KL}}( \\hat{Q}_{x} Q_{y| x}, P_{x, y}^{\\boldsymbol \\theta})\\) iteratively\n\nwith regard to \\(Q_{y| x}\\) (“E” step) and\nwith regard to the parameters \\(\\boldsymbol \\theta\\) of \\(P_{x, y}^{\\boldsymbol \\theta}\\) (“M” step”)\n\none minimises \\(D_{\\text{KL}}(\\hat{Q}_{x} , P_{x}^{\\boldsymbol \\theta})\\) with regard to the parameters of \\(P_{x}^{\\boldsymbol \\theta}\\).\nInterestingly, in the “E” step the first argument of the KL divergence is optimised (“I” projection) and in the “M” step the second argument (“M” projection).\nAlternatively, instead of bounding the marginal KL divergence one can also either minimise the upper bound of the cross-entropy or maximise the lower bound of the negative cross-entropy. All of these three procedures yield the same EM algorithm.\nNote that the optimisation of the entropy bound in the “E” step requires variational calculus since the argument is a distribution! The EM algorithm is therefore in fact a special case of a variational Bayes algorithm since it not only provides estimates of \\(\\boldsymbol \\theta\\) but also yields the distribution of the latent states by means of the calculus of variations.\nFinally, in the above we see that we can learn about unobservable states by means of Bayes theorem. By extending this same principle to learning about parameters and models we arrive at Bayesian learning.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models with latent variables and missing data</span>"
    ]
  },
  {
    "objectID": "15-bayes3.html",
    "href": "15-bayes3.html",
    "title": "15  Essentials of Bayesian statistics",
    "section": "",
    "text": "15.1 Principle of Bayesian learning",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Essentials of Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "15-bayes3.html#principle-of-bayesian-learning",
    "href": "15-bayes3.html#principle-of-bayesian-learning",
    "title": "15  Essentials of Bayesian statistics",
    "section": "",
    "text": "From prior to posterior distribution\nBayesian statistical learning applies Bayes’ theorem to update our state of knowledge about a parameter in the light of data.\nIngredients:\n\n\\(\\boldsymbol \\theta\\) parameter(s) of interest, unknown and fixed.\nprior distribution with density \\(p(\\boldsymbol \\theta)\\) describing the uncertainty (not randomness!) about \\(\\boldsymbol \\theta\\)\ndata generating process \\(p(x | \\boldsymbol \\theta)\\)\n\nNote the model underlying the Bayesian approach is the joint distribution \\[\np(\\boldsymbol \\theta, x) = p(\\boldsymbol \\theta) p(x | \\boldsymbol \\theta)\n\\] as both a prior distribution over the parameters as well as a data generating process have to be specified.\nQuestion: new information in the form of new observation \\(x\\) arrives - how does the uncertainty about \\(\\boldsymbol \\theta\\) change?\n\n\n\n\n\n\nFigure 15.1: Bayesian learning by updating the prior distribution to the posterior distribution.\n\n\n\nAnswer: use Bayes’ theorem to update the prior density to the posterior density (see Figure 15.1).\n\\[\n\\underbrace{p(\\boldsymbol \\theta| x)}_{\\text{posterior} } = \\underbrace{p(\\boldsymbol \\theta)}_{\\text{prior}} \\frac{p(x | \\boldsymbol \\theta) }{ p(x)}\n\\]\nFor the denominator in Bayes formula we need to compute \\(p(x)\\). This is obtained by\n\\[\n\\begin{split}\np(x) &= \\int_{\\boldsymbol \\theta} p(x , \\boldsymbol \\theta) d\\boldsymbol \\theta\\\\\n&= \\int_{\\boldsymbol \\theta} p(x | \\boldsymbol \\theta) p(\\boldsymbol \\theta) d\\boldsymbol \\theta\\\\\n\\end{split}\n\\] i.e. by marginalisation of the parameter \\(\\boldsymbol \\theta\\) from the joint distribution of \\(\\boldsymbol \\theta\\) and \\(x\\). (For discrete \\(\\boldsymbol \\theta\\) replace the integral by a sum). Depending on the context this quantity is either called the\n\nnormalisation constant as it ensures that the posterior density \\(p(\\boldsymbol \\theta| x)\\) integrates to one.\nprior predictive density of the data \\(x\\) given the model \\(M\\) before seeing any data. To emphasise the implicit conditioning on a model we may write \\(p(x| M)\\). Since all parameters have been integrated out \\(M\\) in fact refers to a model class.\nmarginal likelihood of the underlying model (class) \\(M\\) given data \\(x\\). To emphasise this may write \\(L(M| x)\\). Sometimes it is also called model likelihood.\n\n\n\nZero forcing property\nIt is easy to see that if in Bayes rule the prior density/probability is zero for some parameter value \\(\\boldsymbol \\theta\\) then the posterior density/probability will remain at zero for that \\(\\boldsymbol \\theta\\), regardless of any data collected. This zero-forcing property of the Bayes update rule has been called Cromwell’s rule by Dennis Lindley (1923–2013). Therefore, assigning prior density/probability 0 to an event should be avoided.\nNote that this implies that assigning prior probability 1 should be avoided, too.\n\n\nBayesian update and likelihood\nAfter independent and identically distributed data \\(D = \\{x_1, \\ldots, x_n\\}\\) have been observed the Bayesian posterior is computed by\n\\[\n\\underbrace{p(\\boldsymbol \\theta| D) }_{\\text{posterior} } = \\underbrace{p(\\boldsymbol \\theta)}_{\\text{prior}} \\frac{ L(\\boldsymbol \\theta| D) }{ p(D)}\n\\] involving the likelihood \\(L(\\boldsymbol \\theta| D) = \\prod_{i=1}^n p(x_i | \\boldsymbol \\theta)\\) and the marginal likelihoood \\(p(D) = \\int_{\\boldsymbol \\theta} p(\\boldsymbol \\theta) L(\\boldsymbol \\theta| D) d\\boldsymbol \\theta\\) with \\(\\boldsymbol \\theta\\) integrated out.\nThe marginal likelihood serves as a standardising factor so that the posterior density for \\(\\boldsymbol \\theta\\) integrates to 1: \\[\n\\int_{\\boldsymbol \\theta} p(\\boldsymbol \\theta| D) d\\boldsymbol \\theta= \\frac{1}{p(D)} \\int_{\\boldsymbol \\theta} p(\\boldsymbol \\theta) L(\\boldsymbol \\theta| D) d\\boldsymbol \\theta= 1\n\\] Unfortunately, the integral to compute the marginal likelihood is typically analytically intractable and requires numerical integration and/or approximation.\nComparing likelihood and Bayes procedures note that\n\nconducting a Bayesian statistical analysis requires integration respectively averaging (to compute the marginal likelihood)\nin contrast to a likelihood analysis that requires optimisation (to find the maximum likelihood).\n\n\n\nSequential updates\nNote that the Bayesian update procedure can be repeated again and again: we can use the posterior as our new prior and then update it with further data. Thus, we may also update the posterior density sequentially, with the data points \\(x_1, \\ldots, x_n\\) arriving one after the other, by computing first \\(p(\\boldsymbol \\theta| x_1)\\), then \\(p(\\boldsymbol \\theta| x_1, x_2)\\) and so on until we reach \\(p(\\boldsymbol \\theta| x_1, \\ldots, x_n) = p(\\boldsymbol \\theta| D)\\).\nFor example, for the first update we have \\[\np(\\boldsymbol \\theta| x_1) =  p(\\boldsymbol \\theta)   \\frac{p(x_1 | \\boldsymbol \\theta)  }{p(x_1)}\n\\] with \\(p(x_1) =\\int_{\\boldsymbol \\theta} p(x_1 | \\boldsymbol \\theta) p(\\boldsymbol \\theta) d\\boldsymbol \\theta\\). The second update yields \\[\n\\begin{split}\np(\\boldsymbol \\theta| x_1, x_2) &=  p(\\boldsymbol \\theta| x_1)   \\frac{p(x_2 | \\boldsymbol \\theta, x_1)  }{p(x_2| x_1)}\\\\\n&= p(\\boldsymbol \\theta| x_1)   \\frac{p(x_2 | \\boldsymbol \\theta)  }{p(x_2| x_1)}\\\\\n&=  p(\\boldsymbol \\theta) \\frac{  p(x_1 | \\boldsymbol \\theta)    p(x_2 | \\boldsymbol \\theta)  }{p(x_1) p(x_2| x_1)}\\\\\n\\end{split}\n\\] with \\(p(x_2| x_1) = \\int_{\\boldsymbol \\theta} p(x_2 | \\boldsymbol \\theta) p(\\boldsymbol \\theta| x_1) d\\boldsymbol \\theta\\). The final step is \\[\n\\begin{split}\np(\\boldsymbol \\theta| D)  = p(\\boldsymbol \\theta| x_1, \\ldots, x_n) &=   p(\\boldsymbol \\theta) \\frac{ \\prod_{i=1}^n p(x_i | \\boldsymbol \\theta)  }{ p(D)  }\\\\\n\\end{split}\n\\] with the marginal likelihood factorising into \\[\np(D) = \\prod_{i=1}^n p(x_i| x_{&lt;i})\n\\] with \\[\np(x_i| x_{&lt;i}) = \\int_{\\boldsymbol \\theta} p(x_i | \\boldsymbol \\theta) p(\\boldsymbol \\theta| x_{&lt;i}) d\\boldsymbol \\theta\n\\] The last factor is the posterior predictive density of the new data \\(x_i\\) after seeing data \\(x_1, \\ldots, x_{i-1}\\) (given the model class \\(M\\)). It is straightforward to understand why the probability of the new \\(x_i\\) depends on the previously observed data points — because the uncertainty about the model parameter \\(\\boldsymbol \\theta\\) depends on how much data we have already observed. Therefore the marginal likelihood \\(p(D)\\) is not simply the product of the marginal densities \\(p(x_i)\\) at each \\(x_i\\) but instead the product of the conditional densities \\(p(x_i| x_{&lt;i})\\).\nOnly when the parameter is fully known and there is no uncertainty about \\(\\boldsymbol \\theta\\) the observations \\(x_i\\) are independent. This leads back to the standard likelihood where we condition on a particular \\(\\boldsymbol \\theta\\) and the likelihood is the product \\(p(D| \\boldsymbol \\theta) = \\prod_{i=1}^n p(x_i| \\boldsymbol \\theta)\\).\n\n\nSummaries of posterior distributions and credible intervals\nThe Bayesian estimate is the full complete posterior distribution!\nHowever, it is useful to summarise aspects of the posterior distribution:\n\nPosterior mean \\(\\text{E}(\\boldsymbol \\theta| D)\\)\nPosterior variance \\(\\text{Var}(\\boldsymbol \\theta| D)\\)\nPosterior mode etc.\n\nIn particular the mean of the posterior distribution is often taken as a Bayesian point estimate.\n\n\n\n\n\n\nFigure 15.2: Bayesian credible interval based on the posterior distribution.\n\n\n\nThe posterior distribution also allows to define credible regions or credible intervals. These are the Bayesian equivalent to confidence intervals and are constructed by finding the areas of highest probability mass (say 95%) in the posterior distribution (Figure 15.2).\nBayesian credible intervals, unlike frequentist confidence intervals, are thus very easy to interpret as they simply correspond to the part of the parameter space in which we can find the parameter with a given specified probability. In contrast, in frequentist statistics it does not make sense to assign a probability to a parameter value.\nNote that there are typically many credible intervals with the given specified coverage \\(\\alpha\\) (say 95%). Therefore, we may need further criteria to construct these intervals.\nFor univariate parameter \\(\\theta\\) a two-sided equal-tail credible interval is obtained by finding the corresponding lower \\(1-\\alpha/2\\) and upper \\(\\alpha/2\\) quantiles. Typically this type of credible interval is easy to compute. However, note that the density values at the left and right boundary points of such an interval are typically different. Also this does not generalise well to a multivariate parameter \\(\\boldsymbol \\theta\\).\nAs alternative, a highest posterior density (HPD) credible interval of coverage \\(\\alpha\\) is found by identifying the shortest interval (i.e. with smallest support) for the given \\(\\alpha\\) probability mass. Any point within an HDP credible interval has higher density than a point outside the HDP credible interval. Correspondingly, the density at the boundary of an HPD credible interval is constant taking on the same value everywhere along the boundary.\nA Bayesian HPD credible interval is constructed in a similar fashion as a likelihood-based confidence interval, starting from the mode of the posterior density and then looking for a common threshold value for the density to define the boundary of the credible interval. When the posterior density has multiple modes the HPD interval may be disjoint. HPD intervals are also well defined for multivariate \\(\\boldsymbol \\theta\\) with the boundaries given by the contour lines of the posterior density resulting from the threshold value.\nIn the Worksheet B1 examples for both types of credible intervals are given and compared visually.\n\n\nPractical application of Bayes statistics on the computer\nAs we have seen Bayesian learning is conceptually straightforward:\n\nSpecify prior uncertainty \\(p(\\boldsymbol \\theta\\)) about the parameters of interest \\(\\boldsymbol \\theta\\).\nSpecify the data generating process for a specified parameter: \\(p(x | \\boldsymbol \\theta)\\).\nApply Bayes’ theorem to update prior uncertainty in the light of the new data.\n\nIn practise, however, computing the posterior distribution can be computationally very demanding, especially for complex models.\nFor this reason specialised software packages have been developed for computational Bayesian modelling, for example:\n\nBayesian statistics in R: https://cran.r-project.org/web/views/Bayesian.html\nStan probabilistic programming language (interfaces with R, Python, Julia and other languages) — https://mc-stan.org\nBayesian statistics in Python: PyMC using PyTensor as backend, NumPyro using JAX as backend, TensorFlow Probability on JAX using JAX as backend, Pyro using PyTorch as backend, TensorFlow Probability using Tensorflow as backend.\nBayesian statistics in Julia: Turing.jl\nBayesian hierarchical modelling with BUGS, JAGS and NIMBLE.\n\nIn addition to numerical procedures to sample from the posterior distribution there are also many procedures aiming to approximate the Bayesian posterior, employing the Laplace approximation, integrated nested Laplace approximation (INLA), variational Bayes etc.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Essentials of Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "15-bayes3.html#some-background-on-bayesian-statistics",
    "href": "15-bayes3.html#some-background-on-bayesian-statistics",
    "title": "15  Essentials of Bayesian statistics",
    "section": "15.2 Some background on Bayesian statistics",
    "text": "15.2 Some background on Bayesian statistics\n\nBayesian interpretation of probability\n\nWhat makes you “Bayesian”?\nIf you use Bayes’ theorem are you therefore automatically a Bayesian? No!!\nBayes’ theorem is a mathematical fact from probability theory. Hence, Bayes’ theorem is valid for everyone, whichever form for statistical learning your are subscribing (such as frequentist ideas, likelihood methods, entropy learning, Bayesian learning).\nAs we discuss now the key difference between Bayesian and frequentist statistical learning lies in the differences in interpretation of probability, not in the mathematical formalism for probability (which includes Bayes’ theorem).\n\n\nMathematics of probability\nThe mathematics of probability in its modern foundation was developed by Andrey Kolmogorov (1903–1987). In this book Foundations of the Theory of Probability (1933) he establishes probability in terms of set theory/ measure theory. This theory provides a coherent mathematical framework to work with probabilities.\nHowever, Kolmogorov’s theory does not provide an interpretation of probability!\n\\(\\rightarrow\\) The Kolmogorov framework is the basis for both the frequentist and the Bayesian interpretation of probability.\n\n\nInterpretations of probability\nEssentially, there are two major commonly used interpretation of probability in statistics - the frequentist interpretation and the Bayesian interpretation.\nA: Frequentist interpretation\nprobability = frequency (of an event in a long-running series of identically repeated experiments)\nThis is the ontological view of probability (i.e. probability “exists” and is identical to something that can be observed.).\nIt is also a very restrictive view of probability. For example, frequentist probability cannot be used to describe events that occur only a single time. Frequentist probability thus can only be applied asymptotically, for large samples!\nThe law of large numbers provides justification to interpret long-running limits of frequencies as probabilities. However, the con- verse assumption that all probabilities have a frequentist interpretation does not follow from the law of large numbers.\nB: Bayesian probability\n“Probability does not exist” — famous quote by Bruno de Finetti (1906–1985), a Bayesian statistician.\nWhat does this mean?\nProbability is a description of the state of knowledge and of uncertainty.\nProbability is thus an epistemological quantity that is assigned and that changes rather than something that is an inherent property of an object.\nNote that this does not require any repeated experiments. The Bayesian interpretation of probability is valid regardless of sample size or the number or repetitions of an experiment.\nHence, the key difference between frequentist and Bayesian approaches is not the use of Bayes’ theorem. Rather it is whether you consider probability as ontological (frequentist) or epistemological entity (Bayesian).\n\n\n\nHistorical developments\n\nBayesian statistics is named after Thomas Bayes (1701-1761). His paper 1 introducing the famous theorem was published only after his death (1763).\n\n\nPierre-Simon Laplace (1749-1827) was the first to practically use Bayes’ theorem for statistical calculations, and he also independently discovered Bayes’ theorem in 1774 2\n\n\nThis activity was then called “inverse probability” and not “Bayesian statistics”.\nBetween 1900 and 1940 classical mathematical statistics was developed and the field was heavily influenced and dominated by R.A. Fisher (who invented likelihood theory and ANOVA, among other things - he was also working in biology and was professor of genetics). Fisher was very much opposed to Bayesian statistics.\n1931 Bruno de Finetti publishes his “representation theorem”. This shows that the joint distribution of a sequence of exchangeable events (i.e. where the ordering can be permuted) can be represented by a mixture distribution that can be constructed via Bayes’ theorem. (Note that exchangeability is a weaker condition than i.i.d.) This theorem is often used as a justification of Bayesian statistics (along with the so-called Dutch book argument, also by de Finetti).\n1933 publication of Andrey Kolmogorov’s book on probability theory.\n1946 Cox theorem by Richard T. Cox (1898–1991): the aim to generalise classical logic from TRUE/FALSE statements to continuous measures of uncertainty inevitably leads to probability theory and Bayesian learning! This justification of Bayesian statistics was later popularised by Edwin T. Jaynes (1922–1998) in various books (1959, 2003).\n1955 Stein Paradox - Charles M. Stein (1920–2016) publishes a paper on the Stein estimator — an estimator of the mean that dominates the ML estimator (i.e. the sample average). The Stein estimator is better in terms of MSE than the ML estimator, which was very puzzling at that time but it is easy to understand from a Bayesian perspective.\nOnly from the 1950s the use of the term “Bayesian statistics” became prevalent — see Fienberg (2006) 3\n\nDue to advances in personal computing from 1970 onwards Bayesian learning has become more pervasive!\n\nComputers allow to do the complex (numerical) calculations needed in Bayesian statistics .\nMetropolis-Hastings algorithm published in 1970 (which allows to sample from a posterior distribution without explicitly computing the marginal likelihood).\nDevelopment of regularised estimation techniques such as penalised likelihood in regression (e.g. ridge regression 1970).\npenalised likelihood via KL divergence for model selection (Akaike 1973).\nA lot of work on interpreting Stein estimators as empirical Bayes estimators (Efron and Morris 1975)\nregularisation originally was only meant to make singular systems/matrices invertible, but then it turned out regularisation has also a Bayesian interpretation.\nReference priors (Bernardo 1979) proposed as default priors for models with multiple parameters.\nThe EM algorithm (published in 1977) uses Bayes theorem for imputing the distribution of the latent variables.\n\nAnother boost was in the 1990/2000s when in science (e.g. genomics) many complex and high-dimensional data set were becoming the norm, not the exception.\n\nClassical statistical methods cannot be used in this setting (overfitting!) so new methods were developed for high-dimensional data analysis, many with a direct link to Bayesian statistics\n1996 lasso (L1 regularised) regression invented by Robert Tibshirani.\nMachine learning methods for non-parametric and extremely highly parametric models (neural network) require either explicit or implicit regularisation.\nMany Bayesians in this field, many using variational Bayes techniques which may be viewed as generalisation of the EM algorithm and are also linked to methods used in statistical physics.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Essentials of Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "15-bayes3.html#footnotes",
    "href": "15-bayes3.html#footnotes",
    "title": "15  Essentials of Bayesian statistics",
    "section": "",
    "text": "Bayes, T. 1763. An essay towards solving a problem in the doctrine of chances. The Philosophical Transactions 53:370–418. https://doi.org/10.1098/rstl.1763.0053↩︎\nLaplace, P.-S. 1774. Mémoire sur la probabilité de causes par les évenements. Mémoires de mathématique et de physique, présentés à l’Académie Royale des sciences par divers savants et lus dans ses assemblées. Paris, Imprimerie Royale, pp. 621–657.↩︎\nFienberg, S. E. 2006. When did Bayesian inference become “Bayesian”? Bayesian Analysis 1:1–40. https://doi.org/10.1214/06-BA101↩︎",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Essentials of Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "16-bayes4.html",
    "href": "16-bayes4.html",
    "title": "16  Bayesian learning in practise",
    "section": "",
    "text": "16.1 Estimating a proportion using the beta-binomial model\nIn this chapter we discuss how three basic problems, namely how to estimate a proportion, the mean and the variance in a Bayesian framework.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian learning in practise</span>"
    ]
  },
  {
    "objectID": "16-bayes4.html#estimating-a-proportion-using-the-beta-binomial-model",
    "href": "16-bayes4.html#estimating-a-proportion-using-the-beta-binomial-model",
    "title": "16  Bayesian learning in practise",
    "section": "",
    "text": "Binomial likelihood\nIn order to apply Bayes’ theorem we first need to find a suitable likelihood. We use the Bernoulli model as in Example 8.1:\nRepeated Bernoulli experiment (binomial model):\nBernoulli data generating process: \\[\nx  \\sim \\text{Ber}(\\theta)\n\\]\n\n\\(x \\in \\{0, 1\\}\\) (e.g. “success” vs. “failure”)\nThe “success” is indicated by outcome \\(x=1\\) and the “failure” by \\(x=0\\)\nParameter: \\(\\theta\\) is the probability of “success”\nprobability mass function (PMF): \\(\\text{Pr}(x=1) = \\theta\\), \\(\\text{Pr}(x=0) = 1-\\theta\\)\nMean: \\(\\text{E}(x) = \\theta\\)\nVariance \\(\\text{Var}(x) = \\theta (1-\\theta)\\)\n\nBinomial model \\(\\text{Bin}(n,\\theta)\\) (sum of \\(n\\) Bernoulli experiments):\n\n\\(y \\in \\{0, 1, \\ldots, n\\} = \\sum_{i=1}^n x_i\\)\nMean: \\(\\text{E}(y) = n \\theta\\)\nVariance: \\(\\text{Var}(y) = n \\theta (1-\\theta)\\)\nMean of standardised \\(y\\): \\(\\text{E}(y/n) = \\theta\\)\nVariance of standardised \\(y\\): \\(\\text{Var}(y/n) = \\frac{\\theta (1-\\theta)}{n}\\)\n\nMaximum likelihood estimate of \\(\\theta\\):\n\nWe conduct \\(n\\) Bernoulli trials and observe data \\(D = \\{x_1, \\ldots, x_n\\}\\) with average \\(\\bar{x}\\) and \\(n_1\\) successes and \\(n_2 = n-n_1\\) failures.\nBinomial likelihood: \\[\nL(\\theta|D) = \\begin{pmatrix} n \\\\ n_1 \\end{pmatrix} \\theta^{n_1} (1-\\theta)^{n_2}\n\\] Note that the binomial coefficient arises as the ordering of the \\(x_i\\) is irrelevant but it may be discarded as is does not contain the parameter \\(\\theta\\).\nFrom Example 8.1 we know that the maximum likelihood estimate of the proportion \\(\\theta\\) is the frequency \\[\\hat{\\theta}_{ML} = \\frac{n_1}{n} = \\bar{x}\\] Thus, the MLE \\(\\hat{\\theta}_{ML}\\) can be expressed as an average (of the individual data points). This seemingly trivial fact is important for Bayesian estimation of \\(\\theta\\) using linear shrinkage, as will become evident below.\n\n\n\nBeta prior distribution\nIn Bayesian statistics we need not only to specify the data generating process but also a prior distribution over the parameters of the likelihood function.\nTherefore, we need to explicitly specify our prior uncertainty about \\(\\theta\\).\nThe parameter \\(\\theta\\) has support \\([0,1]\\). Therefore we may use a beta distribution \\(\\text{Beta}(\\alpha_1, \\alpha_2)\\) as prior for \\(\\theta\\) (see the Probability and Distribution Refresher notes for properties of this distribution). We will see below that the beta distribution is a natural choice as a prior in conjunction with a binomial likelihood.\nThe parameters of a prior (here \\(\\alpha_1 \\geq 0\\) and \\(\\alpha_2 \\geq 0\\)) are also known as the hyperparameters of the model to distinguish them from the parameters of the likelihood function (here \\(\\theta\\)).\nWe write for the prior distribution \\[\n\\theta \\sim \\text{Beta}(\\alpha_1, \\alpha_2)\n\\] with density \\[\np(\\theta) = \\frac{1}{B(\\alpha_1, \\alpha_2)} \\theta^{\\alpha_1-1} (1-\\theta)^{\\alpha_2-1}\n\\]\nIn terms of mean parameterisation \\(\\text{Beta}(\\mu_0, k_0)\\) this corresponds to:\n\nThe prior concentration parameter is set to \\(k_0  = \\alpha_1 + \\alpha_2\\)\nThe prior mean parameter is set to \\(\\mu_0 = \\alpha_1 / k_0\\).\n\nThe prior mean is therefore \\[\n\\text{E}(\\theta) = \\mu_0\n\\] and the prior variance \\[\n\\text{Var}(\\theta)  = \\frac{\\mu_0 (1-\\mu_0)}{k_0 + 1}\n\\]\nIt is important that this does not actually mean that \\(\\theta\\) is random. It only means that we model the uncertainty about \\(\\theta\\) using a beta-distributed random variable. The flexibility of the beta distribution allows to accommodate a large variety of possible scenarios for our prior knowledge using just two parameters.\nNote the mean and variance of the beta prior and the mean and variance of the standardised binomial variable \\(y/n\\) have the same form. This is further indication that the binomial likelihood and the beta prior are well matched — see the discussion below about “conjugate priors”.\n\n\nComputing the posterior distribution\nAfter observing data \\(D = \\{x_1, \\ldots, x_n\\}\\) with \\(n_1\\) “successes” and \\(n_2 = n-n_1\\) “failures” we can compute the posterior density over \\(\\theta\\) using Bayes’ theorem: \\[\np(\\theta| D) = \\frac{p(\\theta) L(\\theta | D) }{p(D)}\n\\]\nApplying Bayes’ theorem results in the posterior distribution: \\[\n\\theta| D \\sim \\text{Beta}(\\alpha_1+n_1, \\alpha_2+n_2)\n\\] with density \\[\np(\\theta| D) = \\frac{1}{B(\\alpha_1+n_1, \\alpha_2+n_2)} \\theta^{\\alpha_1+n_1-1} (1-\\theta)^{\\alpha_2+n_2-1}\n\\] (For a proof see Worksheet B1.)\nIn the corresponding mean parameterisation \\(\\text{Beta}(\\mu_1, k_1)\\) this results in the following updates:\n\nThe concentration parameter is updated to \\(k_1  = k_0+n\\)\nThe mean parameter is updated to \\[\n\\mu_1 = \\frac{\\alpha_1 + n_1}{k_1}\n\\] This can be written as \\[\n\\begin{split}\n\\mu_1 & =  \\frac{\\alpha_1}{k_1}  + \\frac{n_1}{k_1}\\\\\n    & =  \\frac{k_0}{k_1} \\frac{\\alpha_1}{k_0}   + \\frac{n}{k_1} \\frac{n_1}{n}\\\\\n    & = \\lambda \\mu_0 + (1-\\lambda) \\hat{\\theta}_{ML}\\\\\n\\end{split}\n\\] with \\(\\lambda = \\frac{k_0}{k_1}\\). Hence, \\(\\mu_1\\) is a convex combination of the prior mean and the MLE.\n\nTherefore, the posterior mean is \\[\n\\text{E}(\\theta | D) = \\mu_1\n\\] and the posterior variance is \\[\n\\text{Var}(\\theta | D)\n= \\frac{\\mu_1 (1-\\mu_1)}{k_1+1 }\n\\]",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian learning in practise</span>"
    ]
  },
  {
    "objectID": "16-bayes4.html#properties-of-bayesian-learning",
    "href": "16-bayes4.html#properties-of-bayesian-learning",
    "title": "16  Bayesian learning in practise",
    "section": "16.2 Properties of Bayesian learning",
    "text": "16.2 Properties of Bayesian learning\nThe beta-binomial model, even though it is one of the simplest possible models, already allows to observe a number of important features and properties of Bayesian learning. Many of these apply also to other models as we will see later.\n\nPrior acting as pseudodata\nIn the expression for the mean and variance you can see that the concentration parameter \\(k_0=\\alpha_1 + \\alpha_2\\) behaves like an implicit sample size connected with the prior information about \\(\\theta\\).\nSpecifically, \\(\\alpha_1\\) and \\(\\alpha_2\\) act as pseudocounts that influence both the posterior mean and the posterior variance, exactly in the same way as conventional observations.\nFor example, the larger \\(k_0\\) (and thus the larger \\(\\alpha_1\\) and \\(\\alpha_2\\)) the smaller is the posterior variance, with variance decreasing proportional to the inverse of \\(k_0\\). If the prior is highly concentrated, i.e. if it has low variance and large precision (=inverse variance) then the implicit data size \\(k_0\\) is large. Conversely, if the prior has large variance, then the prior is vague and the implicit data size \\(k_0\\) is small.\nHence, a prior has the same effect as if one would add data — but without actually adding data! This is precisely this why a prior acts as a regulariser and prevents overfitting, because it increases the effective sample size.\nAnother interpretation is that a prior summarises data that may have been available previously as observations.\n\n\nLinear shrinkage of mean\nIn the beta-binomial model the posterior mean is a convex combination (i.e. the weighted average) of the ML estimate and the prior mean as can be seen from the update formula \\[\n\\mu_1 = \\lambda \\mu_0 + (1-\\lambda) \\hat{\\theta}_{ML}\n\\] with weight \\(\\lambda \\in [0,1]\\) \\[\n\\lambda = \\frac{k_0}{k_1} \\,.\n\\] Thus, the posterior mean \\(\\mu_1\\) is a linearly adjusted \\(\\hat{\\theta}_{ML}\\). The factor \\(\\lambda\\) is called the shrinkage intensity — note that this is the ratio of the “prior sample size” (\\(k_0\\)) and the “effective total sample size” (\\(k_1\\)).\n\nThis adjustment of the MLE is called shrinkage, because the \\(\\hat{\\theta}_{ML}\\) is “shrunk” towards the prior mean \\(\\mu_0\\) (which is often called the “target”, and sometimes the target is zero, and then the terminology “shrinking” makes most sense).\nIf the shrinkage intensity is zero (\\(\\lambda = 0\\)) then the ML point estimator is recovered. This happens when \\(\\alpha_1=0\\) and \\(\\alpha_2=0\\) or for \\(n \\rightarrow \\infty\\).\nRemark: using maximum likelihood to estimate \\(\\theta\\) (for moderate or small \\(n\\)) is the same as Bayesian posterior mean estimation using the beta-binomial model with prior \\(\\alpha_1=0\\) and \\(\\alpha_2=0\\). This prior is extremely “u-shaped” and the implicit prior for the ML estimation. Would you use such a prior intentionally?\nIf the shrinkage intensity is large (\\(\\lambda \\rightarrow 1\\)) then the posterior mean corresponds to the prior. This happens if \\(n=0\\) or if \\(k_0\\) is very large (implying that the prior is sharply concentrated around the prior mean).\nSince the ML estimate \\(\\hat{\\theta}_{ML}\\) is unbiased the Bayesian point estimate is biased (for finite \\(n\\)!). And the bias is induced by the prior mean deviating from the true mean. This is also true more generally as Bayesian learning typically produces biased estimators (but asymptotically they will be unbiased like in ML).\nThe fact that the posterior mean is a linear combination of the MLE and the prior mean is not a coincidence. In fact, this is true for all distributions that are exponential families, see e.g. Diaconis and Ylvisaker (1979)1. Crucially, exponential families can always be parameterised such that the corresponding MLEs are expressed as averages of functions of the data (more technically: the MLE of the mean parameter in an EF is the average of the canonical statistic). In conjunction with a particular type of prior (conjugate priors, always existing for exponential families, see below) this allows to write the update from the prior to posterior mean as a linear adjustment of the MLE.\nFurthermore, it is possible (and indeed quite useful for computational reasons!) to formulate Bayes learning assuming only first and second moments (i.e. without full distributions) and in terms of linear shrinkage, see e.g. Hartigan (1969)2. The resulting theory is called “Bayes linear statistics” (Goldstein and Wooff, 2007)3.\n\n\n\nConjugacy of prior and posterior distribution\nIn the beta-binomial model for estimating the proportion \\(\\theta\\) the choice of the beta distribution as prior distribution along with the binomial likelihood resulted in having the beta distribution as posterior distribution as well.\nIf the prior and posterior belong to the same distributional family the prior is called a conjugate prior. This will be the case if the prior has the same functional form as the likelihood. Therefore one also says that the prior is conjugate for the likelihood.\nIt can be shown that conjugate priors exist for all likelihood functions that are based on data generating models that are exponential families.\nIn the beta-binomial model the likelihood is based on the binomial distribution and has the following form (only terms depending on the parameter \\(\\theta\\) are shown): \\[\n\\theta^{n_1} (1-\\theta)^{n_2}\n\\] The form of the beta prior is (again, only showing terms depending on \\(\\theta\\)): \\[\n\\theta^{\\alpha_1-1} (1-\\theta)^{\\alpha_2-1}\n\\] Since the posterior is proportional to the product of prior and likelihood the posterior will have exactly the same form as the prior: \\[\n\\theta^{\\alpha_1+n_1-1} (1-\\theta)^{\\alpha_2+n_2-1}\n\\] Choosing the prior distribution from a family conjugate for the likelihood greatly simplifies Bayesian analysis since the Bayes formula can then be written in form of an update formula for the parameters of the beta distribution: \\[\n\\alpha_1 \\rightarrow \\alpha_1 + n_1  = \\alpha_1 + n \\hat{\\theta}_{ML}\n\\] \\[\n\\alpha_2 \\rightarrow \\alpha_2 + n_2 = \\alpha_2 + n (1-\\hat{\\theta}_{ML})\n\\]\nThus, conjugate prior distributions are very convenient choices. However, in their application it must be ensured that the prior distribution is flexible enough to encapsulate all prior information that may be available. In cases where this is not the case alternative priors should be used (and most likely this will then require to compute the posterior distribution numerically rather than analytically).\n\n\nLarge sample limits of mean and variance\nIf \\(n\\) is large and \\(n &gt;&gt; \\alpha, \\beta\\) then \\(\\lambda \\rightarrow 0\\) and hence the posterior mean and variance become asympotically\n\\[\n\\text{E}(\\theta| D)  \\overset{a}{=} \\frac{n_1 }{n} = \\hat{\\theta}_{ML}\n\\] and \\[\n\\text{Var}(\\theta| D) \\overset{a}{=}   \\frac{\\hat{\\theta}_{ML} (1-\\hat{\\theta}_{ML})}{n}\n\\]\nThus, if the sample size is large then the Bayes’ estimator turns into the ML estimator! Specifically, the posterior mean becomes the ML point estimate, and the posterior variance is equal to the asymptotic variance computed via the observed Fisher information.\nThus, for large \\(n\\) the data dominate and any details about the prior (such as the settings of the hyperparameters \\(\\alpha_1\\) and \\(\\alpha_2\\)) become irrelevant!\n\n\nAsymptotic normality of the posterior distribution\nAlso known as Bayesian Central Limit Theorem (CLT).\nUnder some regularity conditions (such as regular likelihood and positive prior probability for all parameter values, finite number of parameters, etc.) for large sample size the Bayesian posterior distribution converges to a normal distribution centred around the MLE and with the variance of the MLE:\n\\[\n\\text{for large $n$:  }  p(\\boldsymbol \\theta| D) \\to N(\\hat{\\boldsymbol \\theta}_{ML}, \\text{Var}(\\hat{\\boldsymbol \\theta}_{ML}) )\n\\]\nSo not only are the posterior mean and variance converging to the MLE and the variance of the MLE for large sample size, but also the posterior distribution itself converges to the sampling distribution!\nThis holds generally in many regular cases, not just in the simple case above.\nThe Bayesian CLT is generally known as the Bernstein-von Mises theorem (who discovered it at around 1920–30), but special cases were already known by Laplace.\nIn the Worksheet B1 the asymptotic convergence of the posterior distribution to a normal distribution is demonstrated graphically.\n\n\nPosterior variance for finite \\(n\\)\nFrom the Bayesian posterior we can obtain a Bayesian point estimate for the proportion \\(\\theta\\) by computing the posterior mean \\[\n\\text{E}(\\theta | D) = \\frac{\\alpha_1+n_1}{k_1} = \\hat{\\theta}_{\\text{Bayes}}\n\\] along with the posterior variance \\[\n\\text{Var}(\\theta | D) = \\frac{\\hat{\\theta}_{\\text{Bayes}} (1-\\hat{\\theta}_{\\text{Bayes}})}{k_1+1}\n\\]\nAsymptotically for large \\(n\\) the posterior mean becomes the maximum likelihood estimate (MLE), and the posterior variance becomes the asymptotic variance of the MLE. Thus, for large \\(n\\) the Bayesian point estimate will be indistinguishable from the MLE and shares its favourable properties.\nIn addition, for finite sample size the posterior variance will typically be smaller than both the asymptotic posterior variance (for large \\(n\\)) and the prior variance, showing that combining the information available in the prior and in the data leads to a more efficient estimate.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian learning in practise</span>"
    ]
  },
  {
    "objectID": "16-bayes4.html#estimating-the-mean-using-the-normal-normal-model",
    "href": "16-bayes4.html#estimating-the-mean-using-the-normal-normal-model",
    "title": "16  Bayesian learning in practise",
    "section": "16.3 Estimating the mean using the normal-normal model",
    "text": "16.3 Estimating the mean using the normal-normal model\n\nNormal likelihood\nAs in Example 8.2 where we estimated the mean parameter by maximum likelihood we assume as data-generating model the normal distribution with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2\\): \\[\nx \\sim N(\\mu, \\sigma^2)\n\\] We observe \\(n\\) samples \\(D = \\{x_1, \\ldots x_n\\}\\). This yields using maximum likelihood the estimate \\(\\hat{\\mu}_{ML} = \\bar{x}\\).\nWe note that the MLE \\(\\hat\\mu_{ML}\\) is expressed as an average of the data points, which is what enables the linear shrinkage seen below.\n\n\nNormal prior distribution\nThe normal distribution is the conjugate distribution for the mean parameter of a normal likelihood, so if we use a normal prior then posterior for \\(\\mu\\) is normal as well.\nTo model the uncertainty about \\(\\mu\\) we use the normal distribution in the form \\(N(\\mu, \\sigma^2/k)\\) with a mean parameter \\(\\mu\\) and a concentration parameter \\(k &gt; 0\\) (remember that \\(\\sigma^2\\) is given and is also used in the likelihood).\nSpecifically, we use as normal prior distribution for the mean \\[\n\\mu \\sim N\\left(\\mu_0, \\frac{\\sigma^2}{k_0}\\right)\n\\]\n\nThe prior concentration parameter is set to \\(k_0\\)\nThe prior mean parameter is set to \\(\\mu_0\\)\n\nHence the prior mean is \\[\n\\text{E}(\\mu) =  \\mu_0\n\\] and the prior variance \\[\n\\text{Var}(\\mu)  = \\frac{\\sigma^2}{k_0}\n\\] where the concentration parameter \\(k_0\\) corresponds the implied sample size of the prior. Note that \\(k_0\\) does not need to be an integer value.\n\n\nNormal posterior distribution\nAfter observing data \\(D\\) the posterior distribution is also normal with updated parameters \\(\\mu=\\mu_1\\) and \\(k_1\\) \\[\n\\mu | D \\sim N\\left(\\mu_1, \\frac{\\sigma^2}{k_1}\\right)\n\\]\n\nThe posterior concentration parameter is updated to \\(k_1 = k_0 +n\\)\nThe posterior mean parameter is updated to \\[\n\\mu_1 = \\lambda \\mu_0 + (1-\\lambda) \\hat\\mu_{ML}\n\\] with \\(\\lambda = \\frac{k_0}{k_1}\\). This can be seen as linear shrinkage of \\(\\hat\\mu_{ML}\\) towards the prior mean \\(\\mu_0\\).\n\n(For a proof see Worksheet B2.)\nThe posterior mean is \\[\n\\text{E}(\\mu | D) = \\mu_1\n\\] and the posterior variance is \\[\n\\text{Var}(\\mu | D)  = \\frac{\\sigma^2}{k_1}\n\\]\n\n\nLarge sample asymptotics\nFor \\(n\\) large and \\(n &gt;&gt; k_0\\) the shrinkage intensity \\(\\lambda \\rightarrow 0\\) and \\(k_1 \\rightarrow n\\). As a result \\[\n\\text{E}(\\mu |  D) \\overset{a}{=}  \\hat\\mu_{ML}\n\\] \\[\n\\text{Var}(\\mu |  D) \\overset{a}{=} \\frac{\\sigma^2}{n}\n\\] i.e. we recover the MLE and its asymptotic variance!\nNote that for finite \\(n\\) the posterior variance \\(\\frac{\\sigma^2}{n+k_0}\\) is smaller than both the asymptotic variance \\(\\frac{\\sigma^2}{n}\\) of the MLE and the prior variance \\(\\frac{\\sigma^2}{k_0}\\).",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian learning in practise</span>"
    ]
  },
  {
    "objectID": "16-bayes4.html#estimating-the-variance-using-the-inverse-gamma-normal-model",
    "href": "16-bayes4.html#estimating-the-variance-using-the-inverse-gamma-normal-model",
    "title": "16  Bayesian learning in practise",
    "section": "16.4 Estimating the variance using the inverse-gamma-normal model",
    "text": "16.4 Estimating the variance using the inverse-gamma-normal model\n\nNormal likelihood\nAs data generating model we use normal distribution \\[\nx  \\sim N(\\mu, \\sigma^2)\n\\] with unknown variance \\(\\sigma^2\\) and known mean \\(\\mu\\). This yields as maximum likelihood estimate for the variance \\[\n\\widehat{\\sigma^2}_{ML}= \\frac{1}{n}\\sum_{i=1}^n (x_i-\\mu)^2\n\\]\nNote that, again, the MLE is an average (of a quadratic function of the individual data points). This enables linear shrinkage of the MLE as seen below.\n\n\nIG prior distribution\nTo model the uncertainty about the variance we use the inverse-gamma (IG) distribution, also known as inverse Wishart (IW) distribution the (see Probability and Distribution Refresher notes for details of this distribution). The IG distribution is conjugate for the variance parameter in the normal likelihood, hence both the prior and the posterior distribution are IG.\nAs we use the Wishart parameterisation we may equally well call this an inverse Wishart (IW) prior, and the whole model IW-normal model.\nSpecifically, as prior distribution for \\(\\sigma^2\\) we assume using the mean parameter \\(\\mu\\) and concentration parameter \\(\\kappa\\): \\[\n\\sigma^2 \\sim  W^{-1}(\\psi=\\kappa_0 \\sigma^2_0, \\nu=\\kappa_0+2)\n\\]\n\nThe prior concentration parameter is set to \\(\\kappa_0\\)\nThe prior mean parameter is set to \\(\\sigma^2_0\\)\n\nThe corresponding prior mean is \\[\n\\text{E}(\\sigma^2) = \\sigma^2_0\n\\] and the prior variance is \\[\n\\text{Var}(\\sigma^2) = \\frac{2 \\sigma_0^4}{\\kappa_0-2}\n\\] (note that \\(\\kappa_0 &gt; 2\\) for the variance to exist)\n\n\nIG posterior distribution\nAfter observing \\(D = \\{ x_1 \\ldots, x_n\\}\\) the posterior distribution is also IG with updated parameters: \\[\n\\sigma^2| D \\sim W^{-1}_1(\\psi=\\kappa_1 \\sigma^2_1, \\nu=\\kappa_1+2)\n\\]\n\nThe posterior concentration parameter is updated to \\(\\kappa_1 = \\kappa_0+n\\)\nThe posterior mean parameter update follows the standard linear shrinkage rule: \\[\n\\sigma^2_1 =  \\lambda \\sigma^2_0 + (1-\\lambda) \\widehat{\\sigma^2}_{ML}\n\\] with \\(\\lambda=\\frac{\\kappa_0}{\\kappa_1}\\).\n\nThe posterior mean is \\[\n\\text{E}(\\sigma^2 | D) = \\sigma^2_1\n\\] and the posterior variance \\[\n\\text{Var}(\\sigma^2 | D) = \\frac{ 2 \\sigma^4_1}{\\kappa_1-2}\n\\]\n\n\nLarge sample asymptotics\nFor large sample size \\(n\\) with \\(n &gt;&gt; \\kappa_0\\) the shrinkage intensity vanishes (\\(\\lambda \\rightarrow 0\\)) and therefore \\(\\sigma^2_1 \\rightarrow  \\widehat{\\sigma^2}_{ML}\\). We also find that \\(\\kappa_1-2 \\rightarrow n\\).\nThis results in the asymptotic posterior mean \\[\n\\text{E}(\\sigma^2 |  D) \\overset{a}{=}  \\widehat{\\sigma^2}_{ML}\n\\] and the asymptotic posterior variance \\[\n\\text{Var}(\\sigma^2 |  D) \\overset{a}{=} \\frac{2 (\\widehat{\\sigma^2}_{ML})^2}{n}\n\\] Thus we recover the MLE of \\(\\sigma^2\\) and its asymptotic variance.\n\n\nOther equivalent update rules\nAbove the update rule from prior to posterior inverse gamma distribution is stated for the mean parameterisation:\n\n\\(\\kappa_0 \\rightarrow \\kappa_1 = \\kappa_0+n\\)\n\\(\\sigma^2_0 \\rightarrow\n\\sigma^2_1 = \\lambda \\sigma^2_0 + (1-\\lambda) \\widehat{\\sigma^2}_{ML}\\) with \\(\\lambda=\\frac{\\kappa_0}{\\kappa_1}\\)\n\nThis has the advantage that the mean of the inverse gamma distribution is updated directly, and that the prior and posterior variance is also straightforward to compute.\nThe same update rule can also be expressed in terms of the other parameterisations. In terms of the conventional parameters \\(\\alpha\\) and \\(\\beta\\) of the inverse gamma distribution the update rule is\n\n\\(\\alpha_0  \\rightarrow \\alpha_1 = \\alpha_0 +\\frac{n}{2}\\)\n\\(\\beta_0  \\rightarrow  \\beta_1 = \\beta_0 + \\frac{n}{2} \\widehat{\\sigma^2}_{ML}\n= \\beta_0 + \\frac{1}{2} \\sum_{i=1}^n (x_i-\\mu)^2\\)\n\nFor the parameters \\(\\psi\\) and \\(\\nu\\) of the univariate inverse Wishart distribution the update rule is\n\n\\(\\nu_0  \\rightarrow \\nu_1 = \\nu_0 +n\\)\n\\(\\psi_0  \\rightarrow  \\psi_1 = \\psi_0 + n \\widehat{\\sigma^2}_{ML}\n= \\psi_0 + \\sum_{i=1}^n (x_i-\\mu)^2\\)\n\nFor the parameters \\(\\tau^2\\) and \\(\\nu\\) of the scaled inverse chi-squared distribution the update rule is\n\n\\(\\nu_0  \\rightarrow \\nu_1 = \\nu_0 +n\\)\n\\(\\tau^2_0  \\rightarrow  \\tau^2_1 = \\frac{\\nu_0}{\\nu_1} \\tau^2_0 + \\frac{n}{\\nu_1} \\widehat{\\sigma^2}_{ML}\\)\n\n(See Worksheet B2 for proof of the equivalence of all the above update rules.)",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian learning in practise</span>"
    ]
  },
  {
    "objectID": "16-bayes4.html#estimating-the-precision-using-the-gamma-normal-model",
    "href": "16-bayes4.html#estimating-the-precision-using-the-gamma-normal-model",
    "title": "16  Bayesian learning in practise",
    "section": "16.5 Estimating the precision using the gamma-normal model",
    "text": "16.5 Estimating the precision using the gamma-normal model\n\nMLE of the precision\nInstead of estimating the variance \\(\\sigma^2\\) we may wish to estimate the precision \\(w = 1/\\sigma^2\\), i.e. the inverse of variance.\nAs above the data generating model is a normal distribution \\[\nx  \\sim N(\\mu, 1/w)\n\\] with unknown precision \\(w\\) and known mean \\(\\mu\\). This yields as maximum likelihood estimate (easily derived thanks to the invariance principle) \\[\n\\hat{w}_{ML} =  \\frac{ 1}{\\widehat{\\sigma^2}_{ML} } = \\frac{1}{\\frac{1}{n}\\sum_{i=1}^n (x_i-\\mu)^2}\n\\] Crucially, the MLE of the precision \\(w\\) is not an average itself (instead, it is a function of an average). As a consequence, as seen below, the posterior mean of \\(w\\) cannot be written as linear adjustment of the MLE.\n\n\nGamma / Wishart prior\nFor modelling the variance we have used an inverse gamma (inverse Wishart) distribution for the prior and posterior distributions. Thus, in order to model the precision we therefore now use a gamma (Wishart) distribution.\nSpecifically, we use the Wishart distribution in the mean parameterisation (see Probability and Distribution Refresher notes for details): \\[\nw \\sim  W_1(s^2 = w_0/k_0, k=k_0)\n\\]\n\nThe prior concentration parameter is set to \\(k_0\\)\nThe prior mean parameter is set to \\(w_0\\)\n\nThe corresponding prior mean of the precision is \\[\n\\text{E}(w) = w_0\n\\] and the prior variance is \\[\n\\text{Var}(w) = 2 w_0^2/ k_0\n\\]\n\n\nGamma / Wishart posterior\nAfter observing \\(D = \\{ x_1 \\ldots, x_n\\}\\) the posterior distribution is also gamma resp. Wishart with updated parameters:\n\\[\nw | D \\sim   W(s^2 = w_1/k_1, k=k_1)\n\\]\n\nThe posterior concentration parameter is updated to \\(k_1 = k_0+n\\)\nThe posterior mean parameter update follows the rule: \\[\n\\frac{1}{w_1} =  \\lambda \\frac{1}{w_0}  + (1-\\lambda)  \\frac{1}{\\hat{w}_{ML}}\n\\] with \\(\\lambda = \\frac{k_0}{k_1}\\). Crucially, the linear update is applied to the inverse of the precision but not to the precision itself. This is because the MLE of the precision parameter cannot be expressed as an average.\nEquivalent update rules are for the inverse scale parameter \\(s^2\\) \\[\n\\frac{1}{s^2_1} =  \\frac{1}{s^2_0}  + n \\widehat{\\sigma^2}_{ML}\n\\] and for the rate parameter \\(\\beta = 1/(2 s^2)\\) of the gamma distribution \\[\n\\beta_1 =  \\beta_0 + \\frac{n}{2} \\widehat{\\sigma^2}_{ML}\n\\] This is the form you will find most often in textbooks.\n\nThe posterior mean is \\[\n\\text{E}(w | D) = w_1\n\\] and the posterior variance \\[\n\\text{Var}(w | D) = 2 w_1^2/ k_1\n\\]",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian learning in practise</span>"
    ]
  },
  {
    "objectID": "16-bayes4.html#footnotes",
    "href": "16-bayes4.html#footnotes",
    "title": "16  Bayesian learning in practise",
    "section": "",
    "text": "Diaconis, P., and D Ylvisaker. 1979. Conjugate Priors for Exponential Families. Ann. Statist. 7:269–281. https://doi.org/10.1214/aos/1176344611↩︎\nHartigan, J. A. 1969. Linear Bayesian methods. J. Roy. Statist. Soc. B 31:446-454 https://doi.org/10.1111/j.2517-6161.1969.tb00804.x↩︎\nGoldstein, M., and D. Wooff. 2007. Bayes Linear Statistics: Theory and Methods. Wiley. https://doi.org/10.1002/9780470065662↩︎",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian learning in practise</span>"
    ]
  },
  {
    "objectID": "17-bayes5.html",
    "href": "17-bayes5.html",
    "title": "17  Bayesian model comparison",
    "section": "",
    "text": "17.1 Marginal likelihood as model likelihood",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian model comparison</span>"
    ]
  },
  {
    "objectID": "17-bayes5.html#marginal-likelihood-as-model-likelihood",
    "href": "17-bayes5.html#marginal-likelihood-as-model-likelihood",
    "title": "17  Bayesian model comparison",
    "section": "",
    "text": "Simple and composite models\nIn the introduction of the Bayesian learning we already encountered the marginal likelihood \\(p(D | M)\\) of a model class \\(M\\) in the denominator of Bayes’ rule: \\[\np(\\boldsymbol \\theta| D, M) =  \\frac{p(\\boldsymbol \\theta| M)  p(D | \\boldsymbol \\theta, M) }{p(D | M)}\n\\] Computing this marginal likelihood is different for simple and composite models.\nA model is called “simple” if it directly corresponds to a specific distribution, say, a normal distribution with fixed mean and variance, or a binomial distribution with a given probability for the two classes. Thus, a simple model is a point in the model space described by the parameters of a distribution family (e.g. \\(\\mu\\) and \\(\\sigma^2\\) for the normal family \\(N(\\mu, \\sigma^2\\)). For a simple model \\(M\\) the density \\(p(D | M)\\) corresponds to standard likelihood of \\(M\\) and there are no free parameters.\nOn the other hand, a model is “composite” if it is composed of simple models. This can be a finite set, or it can be comprised of infinite number of simpple models. Thus a composite model represent a model class. For example, a normal distribution with a given mean but unspecified variance, or a binomial model with unspecified class probability, is a composite model.\nIf \\(M\\) is a composite model, with the underlying simple models indexed by a parameter \\(\\boldsymbol \\theta\\), the likelihood of the model is obtained by marginalisation over \\(\\boldsymbol \\theta\\): \\[\n\\begin{split}\np(D | M) &= \\int_{\\boldsymbol \\theta} p(D | \\boldsymbol \\theta, M) p(\\boldsymbol \\theta| M) d\\boldsymbol \\theta\\\\\n             &= \\int_{\\boldsymbol \\theta} p(D , \\boldsymbol \\theta| M) d\\boldsymbol \\theta\\\\\n\\end{split}\n\\] i.e. we integrate over all parameter values \\(\\boldsymbol \\theta\\).\nIf the distribution over the parameter \\(\\boldsymbol \\theta\\) of a model is strongly concentrated around a specific value \\(\\boldsymbol \\theta_0\\) then the composite model degenerates to a simple point model, and the marginal likelihood becomes the likelihood of the parameter \\(\\boldsymbol \\theta_0\\) under that model.\n\nExample 17.1 Beta-binomial distribution:\nAssume that likelihood is binomial with mean parameter \\(\\theta\\). If \\(\\theta\\) follows a Beta distribution then the marginal likelihood with \\(\\theta\\) integrated out is the beta-binomial distribution (see also Worksheet B2). This is an example of a compound probability distribution.\n\n\n\nLog-marginal likelihood as penalised maximum log-likelihood\nBy rearranging Bayes’ rule we see that \\[\n\\log p(D | M) =  \\log p(D | \\boldsymbol \\theta, M) - \\log  \\frac{ p(\\boldsymbol \\theta| D, M) }{p(\\boldsymbol \\theta| M)  }\n\\] The above is valid for all \\(\\boldsymbol \\theta\\).\nAssuming concentration of the posterior around the MLE \\(\\hat{\\boldsymbol \\theta}_{\\text{ML}}\\) we will have \\(p(\\hat{\\boldsymbol \\theta}_{\\text{ML}} | D, M)&gt; p(\\hat{\\boldsymbol \\theta}_{\\text{ML}}| M)\\) and thus \\[\n\\log p(D | M) =  \\underbrace{\\log p(D | \\hat{\\boldsymbol \\theta}_{\\text{ML}}, M)}_{\\text{maximum log-likelihood}}\n- \\underbrace{ \\log  \\frac{ p( \\hat{\\boldsymbol \\theta}_{\\text{ML}} | D, M) }{p( \\hat{\\boldsymbol \\theta}_{\\text{ML}}| M)  } }_{\\text{penalty &gt; 0}}\n\\] Therefore, the log-marginal likelihood is essentially a penalised version of the maximum log-likelihood, and the penalty depends on the concentration of the posterior around the MLE.\n\n\nModel complexity and Occams razor\nIntriguingly, the penality implicit in the log-marginal likelihood is linked to the complexity of the model, in particular to the number of parameters of \\(M\\). We will see this directly in the Schwarz approximation of the log-marginal likelihood discussed below.\nThus, the averaging over \\(\\boldsymbol \\theta\\) in the marginal likelihood has the effect of automatically penalising complex models. Therefore, when comparing models using the marginal likelihood a complex model may be ranked below simpler models. In contrast, when selecting a model by comparing maximum likelihood directly the model with the highest number of parameters always wins over simpler models. Hence, the penalisation implicit in the marginal likelihood prevents overfitting that occurs with maximum likelihood.\nThe principle of preferring a less complex model is called Occam’s razor or the law of parsimony.\nWhen choosing models a simpler model is often preferable over a more complex model, because the simpler model is typically better suited to both explaining the currently observed data as well as future data, whereas a complex model will typically only excel in fitting the current data but will perform poorly in prediction.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian model comparison</span>"
    ]
  },
  {
    "objectID": "17-bayes5.html#the-bayes-factor-for-comparing-two-models",
    "href": "17-bayes5.html#the-bayes-factor-for-comparing-two-models",
    "title": "17  Bayesian model comparison",
    "section": "17.2 The Bayes factor for comparing two models",
    "text": "17.2 The Bayes factor for comparing two models\n\nDefinition of the Bayes factor\nThe Bayes factor is the ratio of the likelihoods of the two models: \\[\nB_{12} = \\frac{p(D | M_1)}{p(D | M_2)}\n\\]\nThe log-Bayes factor \\(\\log B_{12}\\) is also called the weight of evidence for \\(M_1\\) over \\(M_2\\).\n\n\nBayes theorem in terms of the Bayes factor\nWe would like to compare two models \\(M_1\\) and \\(M_2\\). Before seeing data \\(D\\) we can check their Prior odds (= ratio of prior probabilities of the models \\(M_1\\) and \\(M_2\\)):\n\\[\\frac{\\text{Pr}(M_1)}{\\text{Pr}(M_2)}\\]\nAfter seeing data \\(D = \\{x_1, \\ldots, x_n\\}\\) we arrive at the Posterior odds (= ratio of posterior probabilities): \\[\\frac{\\text{Pr}(M_1 | D)}{\\text{Pr}(M_2  | D)}\\]\nUsing Bayes Theorem \\(\\text{Pr}(M_i | D) = \\text{Pr}(M_i) \\frac{p(D | M_i)  }{p(D)}\\) we can rewrite the posterior odds as \\[\n\\underbrace{\\frac{\\text{Pr}(M_1 | D)}{\\text{Pr}(M_2 | D)}}_{\\text{posterior odds}} = \\underbrace{\\frac{p(D | M_1)}{p(D | M_2)}}_{\\text{Bayes factor $B_{12}$}} \\,\n\\underbrace{\\frac{\\text{Pr}(M_1)}{\\text{Pr}(M_2)}}_{\\text{prior odds}}\n\\]\nThe Bayes factor is the multiplicative factor that updates the prior odds to the posterior odds.\nOn the log scale we see that\n\\[\n\\text{log-posterior odds = weight of evidence + log-prior odds}\n\\]\n\n\nInterpretive scales for the Bayes factor\n\n\n\nTable 17.1: Scale for the Bayes factor according to Jeffreys (1961).\n\n\n\n\n\n\n\n\n\n\n\\(B_{12}\\)\n\\(\\log B_{12}\\)\nevidence in favour of \\(M_1\\) versus \\(M_2\\)\n\n\n\n\n&gt; 100\n&gt; 4.6\ndecisive\n\n\n10 to 100\n2.3 to 4.6\nstrong\n\n\n3.2 to 10\n1.16 to 2.3\nsubstantial\n\n\n1 to 3.2\n0 to 1.16\nnot worth more than a bare mention\n\n\n\n\n\n\nFollowing Harold Jeffreys (1961) 1 one may interpret the strength of the Bayes factor as listed in Table 17.1.\n\n\n\nTable 17.2: Scale for the Bayes factor according to Kass and Raftery (1995).\n\n\n\n\n\n\n\n\n\n\n\\(B_{12}\\)\n\\(\\log B_{12}\\)\nevidence in favour of \\(M_1\\) versus \\(M_2\\)\n\n\n\n\n&gt; 150\n&gt; 5\nvery strong\n\n\n20 to 150\n3 to 5\nstrong\n\n\n3 to 20\n1 to 3\npositive\n\n\n1 to 3\n0 to 1\nnot worth more than a bare mention\n\n\n\n\n\n\nMore recently, Kass and Raftery (1995) 2 proposed to use the a slightly modified scale (Table 17.2).\n\n\nBayes factor versus likelihood ratio\nIf both \\(M_1\\) and \\(M_2\\) are simple models then the Bayes factor is identical to the likelihood ratio of the two models.\nHowever, if one of the two models is composite then the Bayes factor and the generalised likelihood ratio differ: In the Bayes factor the representative of a composite model is the model average of the simple models indexed by \\(\\boldsymbol \\theta\\), with weights taken from the prior distribution over the simple models contained in \\(M\\). In contrast, in the generalised likelihood ratio statistic the representative of a composite model is chosen by maximisation.\nThus, for composite models, the Bayes factor does not equal the corresponding generalised likelihood ratio statistic. In fact, the key difference is that the Bayes factor is a penalised version of the likelihood ratio, with the penality depending on the difference in complexity (number of parameters) of the two models",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian model comparison</span>"
    ]
  },
  {
    "objectID": "17-bayes5.html#approximate-computations",
    "href": "17-bayes5.html#approximate-computations",
    "title": "17  Bayesian model comparison",
    "section": "17.3 Approximate computations",
    "text": "17.3 Approximate computations\nThe marginal likelihood and the Bayes factor can be difficult to compute in practise. Therefore, a number of approximations have been developed. The most important is the so-called Schwarz (1978) approximation of the log-marginal likelihood. It is used to approximate the log-Bayes factor and also yields the BIC (Bayesian information criterion) which can be interpreted as penalised maximum likelihood.\n\nSchwarz (1978) approximation of log-marginal likelihood\nThe logarithm of the marginal likelihood of a model can be approximated following Schwarz (1978) 3 as follows: \\[\n\\log p(D | M) \\approx l_n^M(\\hat{\\boldsymbol \\theta}_{ML}^{M}) - \\frac{1}{2} d_M \\log n  \n\\] where \\(d_M\\) is the dimension of the model \\(M\\) (number of parameters in \\(\\boldsymbol \\theta\\) belonging to \\(M\\)) and \\(n\\) is the sample size and \\(\\hat{\\boldsymbol \\theta}_{ML}^{M}\\) is the MLE. For a simple model \\(d_M=0\\) so then there is no approximation as in this case the marginal likelihood equals the likelihood.\nThe above formula can be obtained by quadratic approximation of the likelihood assuming large \\(n\\) and assuming that the prior is locally uniform around the MLE. The Schwarz (1978) approximation is therefore a special case of a Laplace approximation.\nNote that the approximation is the maximum log-likelihood minus a penalty that depends on the model complexity (as measured by dimension \\(d\\)), hence this is an example of penalised ML! Also note that the distribution over the parameter \\(\\boldsymbol \\theta\\) is not required in the approximation.\n\n\nBayesian information criterion (BIC)\nThe BIC (Bayesian information criterion) of the model \\(M\\) is the approximated log-marginal likelihood times the factor -2:\n\\[\nBIC(M) = -2 l_n^M(\\hat{\\boldsymbol \\theta}_{ML}^{M}) + d_M \\log n\n\\]\nThus, when comparing models one aimes to maximise the marginal likelihood or, as approximation, minimise the BIC.\nThe reason for the factor “-2” is simply to have a quantity that is on the same scale as the Wilks log likelihood ratio. Some people / software packages also use the factor “2”.\n\n\nApproximating the weight of evidence (log-Bayes factor) with BIC\nUsing BIC (twice) the log-Bayes factor can be approximated as \\[\n\\begin{split}\n2 \\log B_{12} &\\approx -BIC(M_1) + BIC(M_2) \\\\\n&=2 \\left( l_n^{M_{1}}(\\hat{\\boldsymbol \\theta}_{ML}^{M_{1}}) - l_n^{M_{2}}(\\hat{\\boldsymbol \\theta}_{ML}^{M_{2}}) \\right) - \\log(n) (d_{M_{1}}-d_{M_{2}}) \\\\\n\\end{split}\n\\] i.e. it is the penalised log-likelihood ratio of model \\(M_1\\) vs. \\(M_2\\).",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian model comparison</span>"
    ]
  },
  {
    "objectID": "17-bayes5.html#bayesian-testing-using-false-discovery-rates",
    "href": "17-bayes5.html#bayesian-testing-using-false-discovery-rates",
    "title": "17  Bayesian model comparison",
    "section": "17.4 Bayesian testing using false discovery rates",
    "text": "17.4 Bayesian testing using false discovery rates\nWe introduce False Discovery Rates (FDR) as a Bayesian method to distinguish a null model from an alternative model. This is closely linked with classical frequentist multiple testing procedures.\n\nSetup for testing a null model \\(H_0\\) versus an alternative model \\(H_A\\)\nWe consider two models:\n\\(H_0:\\) null model, with density \\(f_0(x)\\) and distribution \\(F_0(x)\\)\n\\(H_A:\\) alternative model, with density \\(f_A(x)\\) and distribution \\(F_A(x)\\)\nAim: given observations \\(x_1, \\ldots, x_n\\) we would like to decide for each \\(x_i\\) whether it belongs to \\(H_0\\) or \\(H_A\\).\nThis is done by a critical decision threshold \\(x_c\\): if \\(x_i &gt; x_c\\) then \\(x_i\\) is called “significant” and otherwise called “not significant”.\nIn classical statistics one of the the most widely used approach to find the decision threshold is by computing \\(p\\)-values from the \\(x_i\\) (this uses only the null model but not the alternative model), and then thresholding the \\(p\\)-values a a certain level (say 5%). If \\(n\\) is large then often the test is modified by adjusting the \\(p\\)-values or the threshold (e.g. if Bonferroni correction).\nNote that this procedure ignores any information we may have about the alternative model!\n\n\nTest errors\n\nTrue and false positives and negatives\nFor any decision threshold \\(x_c\\) we can distinguish the following errors:\n\nFalse positives (FP), “false alarm”, type I error: \\(x_i\\) belongs to null but is called “significant”\nFalse negative (FN), “miss”, type II error: \\(x_i\\) belongs to alternative, but is called “not significant”\n\nIn addition we have:\n\nTrue positives (TP), “hits”: belongs to alternative and is called “significant”\nTrue negatives (TN), “correct rejections”: belongs to null and is called “not significant”\n\n\n\nSpecificity and Sensitivity\nFrom counts of TP, TN, FN, FP we can derive further quantities:\n\nFalse Positive Rate FPR, false alarm rate, type I error probability: \\(FPR=\\alpha_I = \\frac{FP}{TN+FP} = 1- TNR\\)\nFalse Negative Rate FNR, miss rate, type II error probability: \\(FNR =\\alpha_{II} = \\frac{FN}{TP+FN}  = 1- TPR\\)\nTrue Negative Rate TNR, specificity: \\(TNR= \\frac{TN}{TN+FP} = 1- FPR = 1-\\alpha_I\\)\nTrue Positive Rate TPR, sensitivity, power, recall: \\(TPR= \\frac{TP}{TP+FN} = 1- FNR =1-\\alpha_{II}\\)\nAccuracy: \\(ACC =  \\frac{TP+TN}{TP+TN+FP+FN}\\)\n\nAnother common way to choose the decision threshold \\(x_d\\) in classical statistics is to balance sensitivity/power vs. specificity (maximising both power and specificity, or equivalently, minimising both false positive and false negative rates). ROC curves plot TPR/sensitivity vs. FPR = 1-specificity.\n\n\nFDR and FNDR\nIt is possible to link the above with the observed counts of TP, FP, TN, FN:\n\nFalse Discovery Rate (FDR): \\(FDR = \\frac{FP}{FP+TP}\\)\nFalse Nondiscovery Rate (FNDR): \\(FNDR = \\frac{FN}{TN+FN}\\)\nPositive predictive value (PPV), True Discovery Rate (TDR), precision: \\(PPV = \\frac{TP}{FP+TP} = 1-FDR\\)\nNegative predictive value (NPV): \\(NPV = \\frac{TN}{TN+FN} = 1-FNDR\\)\n\nIn order to choose the decision threshold it is natural to balance FDR and FDNR (or PPV and NPV), by minimising both FDR and FNDR or maximising both PPV and NPV.\nIn machine learning it is common to use “precision-recall plots” that plot precision (=PPV, TDR) vs. recall (=power, sensitivity).\n\n\n\nBayesian perspective\n\nTwo component mixture model\nIn the Bayesian perspective the problem of choosing the decision threshold is related to computing the posterior probability \\[\\text{Pr}(H_0 | x_i) , \\] i.e. probability of the null model given the observation \\(x_i\\), or equivalently computing \\[\\text{Pr}(H_A | x_i) = 1- \\text{Pr}(H_0 | x_i)\\] the probability of the alternative model given the observation \\(x_i\\).\nThis is done by assuming a mixture model \\[\nf(x) = \\pi_0 f_0(x) + (1-\\pi_0) f_A(x)\n\\] where \\(\\pi_0 = \\text{Pr}(H_0)\\) is the prior probability of \\(H_0\\) and. \\(\\pi_A = 1- \\pi_0 = \\text{Pr}(H_A)\\) the prior probabiltiy of \\(H_A\\).\nNote that the weights \\(\\pi_0\\) can in fact be estimated from the observations by fitting the mixture distribution to the observations \\(x_1, \\ldots, x_n\\) (so it is effectively an empirical Bayes method where the prior is informed by the data).\n\n\nLocal FDR\nThe posterior probability of the null model given a data point is then given by \\[\\text{Pr}(H_0 | x_i) = \\frac{\\pi_0 f_0(x_i)}{f(x_i)} = LFDR(x_i)\\] This quantity is also known as the local FDR or local False Discovery Rate.\nIn the given one-sided setup the local FDR is large (close to 1) for small \\(x\\), and will become close to 0 for large \\(x\\). A common decision rule is given by thresholding local false discovery rates: if \\(LFDR(x_i) &lt; 0.1\\) the \\(x_i\\) is called significant.\n\n\nq-values\nIn correspondence to \\(p\\)-values one can also define tail-area based false discovery rates: \\[\nFdr(x_i) = \\text{Pr}(H_0 | X &gt; x_i) = \\frac{\\pi_0 F_0(x_i)}{F(x_i)}\n\\]\nThese are called q-values, or simply False Discovery Rates (FDR). Intriguingly, these also have a frequentist interpretation as adjusted p-values (using a Benjamini-Hochberg adjustment procedure).\n\n\n\nSoftware\nThere are a number of R packages to compute (local) FDR values:\nFor example:\n\nlocfdr\nqvalue\nfdrtool\n\nand many more.\nUsing FDR values for screening is especially useful in high-dimensional settings (e.g. when analysing genomic and other high-throughput data).\nFDR values have both a Bayesian as well as frequentist interpretation, providing further evidence that good classical statistical methods do have a Bayesian interpretation.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian model comparison</span>"
    ]
  },
  {
    "objectID": "17-bayes5.html#footnotes",
    "href": "17-bayes5.html#footnotes",
    "title": "17  Bayesian model comparison",
    "section": "",
    "text": "Jeffreys, H. Theory of Probability. 3rd ed. Oxford University Press.↩︎\nKass, R.E., and A.E. Raftery. 1995. Bayes factors. JASA 90:773–795. https://doi.org/10.1080/01621459.1995.10476572↩︎\nSchwarz, G. 1978. Estimating the dimension of a model. Ann. Statist. 6:461–464. https://doi.org/10.1214/aos/1176344136↩︎",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian model comparison</span>"
    ]
  },
  {
    "objectID": "18-bayes6.html",
    "href": "18-bayes6.html",
    "title": "18  Choosing priors in Bayesian analysis",
    "section": "",
    "text": "18.1 Choosing a prior",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Choosing priors in Bayesian analysis</span>"
    ]
  },
  {
    "objectID": "18-bayes6.html#choosing-a-prior",
    "href": "18-bayes6.html#choosing-a-prior",
    "title": "18  Choosing priors in Bayesian analysis",
    "section": "",
    "text": "Prior as part of the model\nIt is essential in a Bayesian analysis to specify your prior uncertainty about the model parameters. Note that this is simply part of the modelling process! Thus in a Bayesian approach the data analyst needs to be more explicit about all modelling assumptions.\nTypically, when choosing a suitable prior distribution we consider the overall form (shape and domain) of the distribution as well as its key characteristics such as the mean and variance. As we have learned the precision (inverse variance) of the prior may often be viewed as implied sample size.\nFor large sample size \\(n\\) the posterior mean converges to the maximum likelihood estimate (and the posterior distribution to normal distribution centered around the MLE), so for large \\(n\\) we may ignore specifying a prior.\nHowever, for small \\(n\\) it is essential that a prior is specified. In non-Bayesian approaches this prior is still there but it is either implicit (maximum likelihood estimation) or specified via a penality (penalised maximum likelihood estimation).\n\n\nSome guidelines\nSo the question remains what are good ways to choose a prior? Two useful ways are:\n\nUse a weakly informative prior. This means that you do have an idea (even if only vague) about the suitable values of the parameter of interest, and you use a corresponding prior (for example with moderate variance) to model the uncertainty. This acknowledges that there are no uninformative priors and but also aims that the prior does not dominate the likelihood (i.e. the data). The result is a weakly regularised estimator. Note that it is often desirable that the prior adds information (if only a little) so that it can act as a regulariser.\nEmpirical Bayes methods can often be used to determine one or all of the hyperparameters (i.e. the parameters in the prior) from the observed data. There are several ways to do this, one of them is to tune the shrinkage parameter \\(\\lambda\\) to achieve minimum MSE. We discuss this further below.\n\nFurthermore, there also exist many proposals advocating so-called “uninformative priors” or “objective priors”. However, there are no actually unformative priors, since a prior distribution that looks uninformative (i.e. “flat”) in one coordinate system can be informative in another — this is a simple consequence of the rule for transformation of probability densities. As a result, often the suggested objective priors are in fact improper, i.e. are not actually probability distributions!",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Choosing priors in Bayesian analysis</span>"
    ]
  },
  {
    "objectID": "18-bayes6.html#default-priors-or-uninformative-priors",
    "href": "18-bayes6.html#default-priors-or-uninformative-priors",
    "title": "18  Choosing priors in Bayesian analysis",
    "section": "18.2 Default priors or uninformative priors",
    "text": "18.2 Default priors or uninformative priors\nObjective or for default priors are attempts 1) to automatise specification of a prior and 2) to find uniformative priors.\n\nJeffreys prior\nThe most well-known non-informative prior is given by a proposal by Harold Jeffreys (1891–1989) in 1946 1.\nSpecifically, this prior is constructed from the expected Fisher information and thus promises automatic construction of objective uninformative priors using the likelihood: \\[\np(\\boldsymbol \\theta) \\propto \\sqrt{\\det \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)}\n\\]\nThe reasoning underlying this prior is invariance against transformation of the coordinate system of the parameters.\nFor the Beta-Binomial model the Jeffreys prior corresponds to \\(\\text{Beta}(\\frac{1}{2}, \\frac{1}{2})\\). Note this is not the uniform distribution but a U-shaped prior.\nFor the normal-normal model it corresponds to the flat improper prior \\(p(\\mu) =1\\).\nFor the IG-normal model the Jeffreys prior is the improper prior \\(p(\\sigma^2) = \\frac{1}{\\sigma^2}\\).\nThis already illustrates the main problem with this type of prior – namely that it often is improper, i.e. the prior distribution is not actually a probability distribution (i.e. the density does not integrate to 1).\nAnother issue is that Jeffreys priors are usually not conjugate which complicates the update from the prior to the posterior.\nFurthermore, if there are multiple parameters (\\(\\boldsymbol \\theta\\) is a vector) then Jeffreys priors do not usually lead to sensible priors.\n\n\nReference priors\nAn alternative to Jeffreys priors are the so-called reference priors developed by Bernardo (1979) 2. This type of priors aims to choose the prior such that there is maximal “correlation” between the data and the parameter. More precisely, the mutual information between \\(\\theta\\) and \\(x\\) is maximised (i.e. the the expected KL divergence between the posterior and prior distribution). The underlying motivation is that the data and parameters should be maximally linked (thereby minimising the influence of the prior).\nFor univariate settings the reference priors are identical to Jeffreys priors. However, reference prior also provide reasonable priors in multivariate settings.\nIn both Jeffreys’ and the reference prior approach the choice of prior is by expectation over the data, i.e. not for the specific data set at hand (this can be seen both as a positive and negative!).",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Choosing priors in Bayesian analysis</span>"
    ]
  },
  {
    "objectID": "18-bayes6.html#empirical-bayes",
    "href": "18-bayes6.html#empirical-bayes",
    "title": "18  Choosing priors in Bayesian analysis",
    "section": "18.3 Empirical Bayes",
    "text": "18.3 Empirical Bayes\nIn empirical Bayes the data analysist specifies a family of prior distribution (say a Beta distribution with free parameters), and then the data at hand are used to find an optimal choise for the hyper-parameters (hence the name “empirical”). Thus the hyper-parameters are not specified but themselves estimated.\n\nType II maximum likelihood\nIn particular, assuming data \\(D\\), a likelihood \\(p(D|\\boldsymbol \\theta)\\) for some model with parameters \\(\\boldsymbol \\theta\\) as well as a prior \\(p(\\boldsymbol \\theta| \\lambda)\\) for \\(\\boldsymbol \\theta\\) with hyper-parameter \\(\\lambda\\) the marginal likelihood now depends on \\(\\lambda\\): \\[\np(D | \\lambda) = \\int_{\\boldsymbol \\theta}  p(D|\\boldsymbol \\theta) p(\\boldsymbol \\theta| \\lambda) d\\boldsymbol \\theta\n\\] We can therefore use maximum (marginal) likelihood find optimal values of \\(\\lambda\\) given the data.\nSince maximum-likelihood is used in a second level step (the hyper-parameters) this type of empirical Bayes is also often called “type II maximum likelihood”.\n\n\nShrinkage estimation using empirical risk minimisation\nAn alternative (but related) way to estimate hyper-parameters is by minimising the empirical risk.\nIn the examples for Bayesian estimation that we have considered so far the posterior mean of the parameter of interest was obtained by linear shrinkage \\[\n\\hat\\theta_{\\text{shrink}} = \\text{E}( \\theta | D) = \\lambda \\theta_0 + (1-\\lambda) \\hat\\theta_{\\text{ML}}\n\\] of the MLE \\(\\hat\\theta_{\\text{ML}}\\) towards the prior mean \\(\\theta_0\\), with shrinkage intensity \\(\\lambda=\\frac{k_0}{k_1}\\) determined by the ratio of the prior and posterior concentration parameters \\(k_0\\) and \\(k_1\\).\nThe resulting point estimate \\(\\hat\\theta_{\\text{shrink}}\\) is called shrinkage estimate and is a convex combination of \\(\\theta_0\\) and \\(\\hat\\theta_{\\text{ML}}\\). The prior mean \\(\\theta_0\\) is also called the “target”.\nThe hyperparameter in this setting is \\(k_0\\) (linked to the precision of the prior) and or equivalently the shrinkage intensity \\(\\lambda\\).\nAn optimal value for \\(\\lambda\\) can be obtained by minimising the mean squared error of the estimator \\(\\hat\\theta_{\\text{shrink}}\\).\nIn particular, by construction, the target \\(\\theta_0\\) has low or even zero variance but non-vanishing and potentially large bias, whereas the MLE \\(\\hat\\theta_{\\text{ML}}\\) will have low or zero bias but a substantial variance. By combinining these two estimators with opposite properties the aim is to achieve a bias-variance tradeoff so that the resulting estimator \\(\\hat\\theta_{\\text{shrink}}\\) has lower MSE than either \\(\\theta_0\\) and \\(\\hat\\theta_{\\text{ML}}\\).\nSpecifically, the aim is to find \\[\n\\lambda^{\\star} = \\underset{\\lambda}{\\arg \\min \\ }  \n\\text{E}\\left( ( \\theta - \\hat\\theta_{\\text{shrink}} )^2\\right)\n\\]\nIt turns out that this can be minimised without knowing the actual true value of \\(\\theta\\) and the result for an unbiased \\(\\hat\\theta_{\\text{ML}}\\) is \\[\n\\lambda^{\\star} = \\frac{\\text{Var}(\\hat\\theta_{\\text{ML}})}{\\text{E}( (\\hat\\theta_{\\text{ML}} - \\theta_0)^2 )}\n\\] Hence, the shrinkage intensity will be small if the variance of the MLE is small and/or if the target and the MLE differ substantially. On the other hand, if the variance of the MLE is large and/or the target is close to the MLE the shrinkage intensity will be large.\nChoosing the shrinkage parameter by optimising expected risk (here mean squared error) is also a form empirical Bayes.\n\nExample 18.1 James-Stein estimator:\nEmpirical risk minimisation to estimate the shrinkage parameter of the normal-normal model for a single observation yields the James-Stein estimator (1955).\nSpecifically, James and Stein propose the following estimate for the multivariate mean \\(\\boldsymbol \\mu\\) of using a single sample \\(\\boldsymbol x\\) drawn from the multivariate normal \\(N_d(\\boldsymbol \\mu, \\boldsymbol I)\\): \\[\n\\hat{\\boldsymbol \\mu}_{JS} = \\left(1-\\frac{d-2}{||\\boldsymbol x||^2}\\right) \\boldsymbol x\n\\] Here, we recognise \\(\\hat{\\boldsymbol \\mu}_{ML} = \\boldsymbol x\\), \\(\\boldsymbol \\mu_0=0\\) and shrinkage intensity \\(\\lambda^{\\star}=\\frac{d-2}{||\\boldsymbol x||^2}\\).\nEfron and Morris (1972) and Lindley and Smith (1972) later generalised the James-Stein estimator to the case of multiple observations \\(\\boldsymbol x_1, \\ldots \\boldsymbol x_n\\) and target \\(\\boldsymbol \\mu_0\\), yielding an empirical Bayes estimate of \\(\\mu\\) based on the normal-normal model.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Choosing priors in Bayesian analysis</span>"
    ]
  },
  {
    "objectID": "18-bayes6.html#footnotes",
    "href": "18-bayes6.html#footnotes",
    "title": "18  Choosing priors in Bayesian analysis",
    "section": "",
    "text": "Jeffreys, H. 1946. An invariant form for the prior probability in estimation problems. Proc. Roy. Soc. A 186:453–461. https://doi.org/10.1098/rspa.1946.0056↩︎\nBernardo, J. M. 1979. Reference posterior distributions for Bayesian inference (with discussion). JRSS B 41:113–147. https://doi.org/10.1111/j.2517-6161.1979.tb01066.x↩︎",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Choosing priors in Bayesian analysis</span>"
    ]
  },
  {
    "objectID": "19-bayes7.html",
    "href": "19-bayes7.html",
    "title": "19  Optimality properties and summary",
    "section": "",
    "text": "19.1 Bayesian statistics in a nutshell",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Optimality properties and summary</span>"
    ]
  },
  {
    "objectID": "19-bayes7.html#bayesian-statistics-in-a-nutshell",
    "href": "19-bayes7.html#bayesian-statistics-in-a-nutshell",
    "title": "19  Optimality properties and summary",
    "section": "",
    "text": "Bayesian statistics explicitly models the uncertainty about the parameters of interest by probability\nIn the light of new evidence (observed data) the uncertainty is updated, i.e. the prior distribution is combined via Bayes rule with the likelihood to form the posterior distribution\nIf the posterior distribution is in same family as the prior \\(\\rightarrow\\) conjugate prior.\nIn an exponential family the Bayesian update of the mean is always expressible as linear shrinkage of the MLE.\nFor large sample size the posterior mean becomes maximum likelihood estimator and the prior playes no role.\nConversely, for small sample size if no data is available the posterior stays close the prior.\n\n\nAdvantages\n\nAdding prior information has regularisation properties. This is very important in more complex models with many parameters, e.g., in the estimation of a covariance matrix (to avoid singularity).\nImproves small-sample accuracy (e.g. MSE)\nBayesian estimators tend to perform better than MLEs - this is not surprising as they use the observed data plus the extra information available in the prior.\nBayesian credible intervals are conceptually much more simple than frequentist confidence intervals.\n\n\n\nFrequentist properties of Bayesian estimators\nA Bayesian point estimator (e.g. the posterior mean) can also be assessed by its frequentist properties.\n\nFirst, by construction due to introducing a prior the Bayesian estimator will be biased for finite \\(n\\) even if the MLE is unbiased.\nSecond, intriguingly it turns out that the sampling variance of the Bayes point estimator (not to be confused with the posterior variance!) can be smaller than the variance of the MLE. This depends on the choice of the shrinkage parameter \\(\\lambda\\) that also determines the posterior variance.\n\nAs a result, Bayesian estimators may have smaller MSE (=squared bias + variance) than the ML estimator for finite \\(n\\).\nIn statistical decision theory this is called the theorem of admissibility of Bayes rules. It states that under mild conditions every admissible estimation rule (i.e. one that dominates all other estimators with regard to some expected loss, such as the MSE) is in fact a Bayes estimator with some prior.\nUnfortunately, this theorem does not tell which prior is needed to achive optimality, however an optimal estimator can often be found by tuning the hyperparameters.\n\n\nSpecifying the prior — problem or advantage?\nIn Bayesian statistics the data analyst needs to be very explicit about the modelling assumptions:\nModel = data generating process (likelihood) + prior uncertainty (prior distribution)\nNote that alternative statistical methods can often be interpreted as Bayesian methods assuming a specific implicit prior!\nFor example, likelihood estimation for the binomial model is equivalent to Bayes estimation using the Beta-Binomial model with a \\(\\text{Beta}(0,0)\\) prior (=Haldane prior).\nHowever, when choosing a prior explicitly for this model, interestingly most analysts would rather use a flat prior \\(\\text{Beta}(1,1)\\) (=Laplace prior) with implicit sample size \\(k_0=2\\) or a transformation-invariant prior \\(\\text{Beta}(1/2, 1/2)\\) (=Jeffreys prior) with implicit sample size \\(k_0=1\\) rather than the Haldane prior!\n\\(\\rightarrow\\) be aware about the implicit priors!!\nBetter to acknowledge that a prior is being used (even if implicit!)\nBeing specific about all your assumptions is enforced by the Bayesian approach.\nSpecifying a prior is thus best understood as an intrinsic part of model specification. It helps to improve inference and it may only be ignored if there is lots of data.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Optimality properties and summary</span>"
    ]
  },
  {
    "objectID": "19-bayes7.html#optimality-of-bayesian-inference",
    "href": "19-bayes7.html#optimality-of-bayesian-inference",
    "title": "19  Optimality properties and summary",
    "section": "19.2 Optimality of Bayesian inference",
    "text": "19.2 Optimality of Bayesian inference\nThe optimality of Bayesian model making use of full model specification (likelihood plus prior) can be shown from a number of different perspectives. Correspondingly, there are many theorems that prove (or at least indicate) this optimality:\n\nRichard Cox’s theorem: generalising classical logic invariably leads to Bayesian inference.\nde Finetti’s representation theorem: joint distribution of exchangeable observations can always be expressed as weighted mixture over a prior distribution for the parameter of the model. This implies the existence of the prior distribution and the requirement of a Bayesian approach.\nFrequentist decision theory: all admissible decision rules are Bayes rules!\nEntropy perspective: The posterior density (a function!) is obtained as a result of optimising an entropy criterion. Bayesian updating may thus be viewed as a variational optimisation problem. Specifically, Bayes theorem is the minimal update when new information arrives in form of observations (see below).\n\nRemark: there exist a number of further (often somewhat esoteric) suggestions for propagating uncertainty such as “fuzzy logic”, imprecise probabilities, etc. These contradict Bayesian learning and are thus in direct violation of the above theorems.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Optimality properties and summary</span>"
    ]
  },
  {
    "objectID": "19-bayes7.html#connection-with-entropy-learning",
    "href": "19-bayes7.html#connection-with-entropy-learning",
    "title": "19  Optimality properties and summary",
    "section": "19.3 Connection with entropy learning",
    "text": "19.3 Connection with entropy learning\nThe Bayesian update rule is a very general form of learning when the new information arrives in the form of data. But actually there is an even more general principle of which the Bayesian update rule is just a special case: the principle of minimal information update (e.g. Jaynes 1959, 2003) or principle of minimum information discrimination (MDI) (Kullback 1959).\nIt can be summarised as follows: Change your beliefs only as much as necessary to be coherent with new evidence!\nUnder this principle of “inertia of beliefs” when new information arrives the uncertainty about a parameter is only minimally adjusted, only as much as needed to account for the new information. To implement this principle KL divergence is a natural measure to quantify the change of the underlying beliefs. This is known as entropy learning.\nThe Bayes rule emerges a special case of entropy learning:\n\nThe KL divergence between the joint posterior \\(Q_{x,\\boldsymbol \\theta}\\) and joint prior distribution \\(P_{x,\\boldsymbol \\theta}\\) is computed, with the posterior distribution \\(Q_{\\boldsymbol \\theta|x}\\) as free parameter.\nThe conditional distribution \\(Q_{\\boldsymbol \\theta|x}\\) is found by minimising the KL divergence \\(D_{\\text{KL}}(Q_{x,\\boldsymbol \\theta},  P_{x,\\boldsymbol \\theta})\\).\nThe optimal solution to this variational optimisation problem is given by Bayes’ rule!\n\nThis application of the KL divergence is an example of reverse KL optimisation (aka \\(I\\)-projection, see Part I of the notes). Intriguingly, this explains the zero forcing property of Bayes’ rule (because that this is a general property of an \\(I\\)-projection).\nApplying entropy learning therefore includes Bayesian learning as special case:\n\nIf information arrives in form of data \\(\\rightarrow\\) update prior by Bayes’ theorem (Bayesian learning).\n\nInterestingly, entropy learning will lead to other update rules for other types of information:\n\nIf information arrives in the form of another distribution \\(\\rightarrow\\) update using R. Jeffrey’s rule of conditioning (1965).\nIf the information is presented in the form of constraints \\(\\rightarrow\\) Kullback’s principle of minimum MDI (1959), E. T. Jaynes maximum entropy (MaxEnt) principle (1957).\n\nThis shows (again) how fundamentally important KL divergence is in statistics. It not only leads to likelihood inference (via forward KL) but also to Bayesian learning, as well as to other forms of information updating (via reverse KL).\nFurthermore, in Bayesian statistics the KL divergence is useful to choose priors (e.g. reference priors) and it also helps in (Bayesian) experimental design to quantify the information provided by an experiment.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Optimality properties and summary</span>"
    ]
  },
  {
    "objectID": "19-bayes7.html#conclusion",
    "href": "19-bayes7.html#conclusion",
    "title": "19  Optimality properties and summary",
    "section": "19.4 Conclusion",
    "text": "19.4 Conclusion\nBayesian statistics offers a coherent framework for statistical learning from data, with methods for\n\nestimation\ntesting\nmodel building\n\nThere are a number of theorems that show that “optimal” estimators (defined in various ways) are all Bayesian.\nIt is conceptually very simple — but can be computationally very involved!\nIt provides a coherent generalisation of classical TRUE/FALSE logic (and therefore does not suffer from some of the inconsistencies prevalent in frequentist statistics).\nBayesian statistics is a non-asymptotic theory, it works for any sample size. Asympotically (large \\(n\\)) it is consistent and converges to the true model (like ML!). But Bayesian reasoning can also be applied to events that take place only once — no assumption of hypothetical infinitely many repetitions as in frequentist statistics is needed.\nMoreover, many classical (frequentist) procedures may be viewed as approximations to Bayesian methods and estimators, so using classical approaches in the correct application domain is perfectly in line with the Bayesian framework.\nBayesian estimation and inference also automatically regularises (via the prior) which is important for complex models and when there is the problem of overfitting.",
    "crumbs": [
      "Bayesian statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Optimality properties and summary</span>"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Agresti, A., and M. Kateri. 2022. Foundations of Statistics for Data\nScientists. Chapman; Hall/CRC.\n\n\nDiaconis, P., and B. Skyrms. 2018. Ten Great Ideas about\nChance. Princeton University Press.\n\n\nDomingos, P. 2015. The Master Algorithm: How the Quest for the\nUltimate Learning Machine Will Remake Our World. Basic Books.\n\n\nGelman, A., J. B. Carlin, H. A. Stern, D. B. Dunson, A. Vehtari, and D.\nB. Rubin. 2014. Bayesian Data Analysis. 3rd ed. CRC Press.\n\n\nHeard, N. 2021. An Introduction to Bayesian Inference, Methods and\nComputation. Springer.\n\n\nHeld, L., and D. S. Bové. 2020. Applied Statistical Inference:\nLikelihood and Bayes. Second. Springer.\n\n\nMcGrayne, S. B. 2011. The Theory That Would Not Die. Yale\nUniversity Press.",
    "crumbs": [
      "Bayesian statistics",
      "Bibliography"
    ]
  },
  {
    "objectID": "20-stats.html",
    "href": "20-stats.html",
    "title": "Appendix A — Statistics refresher",
    "section": "",
    "text": "A.1 Data and statistics as functions of data\nBelow you find a brief overview over some relevant concepts in statistics that you should be familiar with from earlier modules.\nBroadly, by “data” we refer to quantitative observations and measurements collected in experiments.\nWe denote the observed data by \\(D =\\{x_1, \\ldots, x_n\\}\\) where \\(n\\) denotes the number of data points (the sample size). Each data point can be scalar or a multivariate quantity.\nGenerally, a statistic \\(t(D)\\) is function of the observed data \\(D\\). The statistic \\(t(D)\\) can be of any type and value (scalar, vector, matrix etc. — even a function). \\(t(D)\\) is called a summary statistic if it describes important aspects of the data such as location (e.g. the average \\(\\text{avg}(D) =\\bar{x}\\), the median) or scale (e.g. standard deviation, interquartile range).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Statistics refresher</span>"
    ]
  },
  {
    "objectID": "20-stats.html#statistical-learning",
    "href": "20-stats.html#statistical-learning",
    "title": "Appendix A — Statistics refresher",
    "section": "A.2 Statistical learning",
    "text": "A.2 Statistical learning\nThe aim in statistics, and by extension in data science and machine learning, is to use data to learn about and better understand the world. It is a key feature of statistics to employ probabilistic models for that purpose.\nLet denote data models by \\(p(x| \\theta)\\) where \\(\\theta\\) represents the parameters of the model. Often (but not always) \\(\\theta\\) can be interpreted as or is associated with some manifest property of the model. If there is only a single parameter we write \\(\\theta\\) (scalar parameter). If we wish to highlight that there are multiple parameters we write \\(\\boldsymbol \\theta\\) (in bold type).\nSpecifically, our aim is to identify the best model(s) for the data in order to both\n\nexplain the current data, and\nto enable good prediction of future data.\n\nBy choosing a sufficiently complex model the first aim (to explain the observed data) is often easily achieved. However, if the model is too complex it can fail to address the second aim (to predict unseen data well). Thus, when choosing a model we would like to avoid both the problem of underfitting (i.e. choosing an overly simplistic model) as well overfitting (i.e. choosing an overly complex model). Finding this balances will also help with interpreting the fitted model.\nTypically, we focus the analysis to a specific model family with a some parameter \\(\\theta\\).\nAn estimator for \\(\\theta\\) is a function \\(\\hat{\\theta}(D)\\) of the data that maps the data (input) to an informed guess (output) about \\(\\theta\\).\n\nA point estimator provides a single number for each parameter\nAn interval estimator provides a set of possible values for each parameter.\n\nInterval estimators can be linked to the concept of testing specified values for a parameter. Specifically a confidence interval contains all parameter values that are not significantly different from the best parameter.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Statistics refresher</span>"
    ]
  },
  {
    "objectID": "20-stats.html#sampling-properties-of-a-point-estimator",
    "href": "20-stats.html#sampling-properties-of-a-point-estimator",
    "title": "Appendix A — Statistics refresher",
    "section": "A.3 Sampling properties of a point estimator",
    "text": "A.3 Sampling properties of a point estimator\nA point estimator \\(\\hat\\theta\\) depends on the data, hence it exhibits sampling variation, i.e. estimate will be different for a new set of observations.\nThus \\(\\hat\\theta\\) can be seen as a random variable, and its distribution is called sampling distribution (across different experiments).\nProperties of this distribution can be used to evaluate how far the estimator deviates (on average across different experiments) from the true value:\n\\[\\begin{align*}\n\\begin{array}{rr}\n\\text{Bias:}\\\\\n\\text{Variance:}\\\\\n\\text{Mean squared error:}\\\\\n\\\\\n\\end{array}\n\\begin{array}{rr}\n\\text{Bias}(\\hat{\\theta})\\\\\n\\text{Var}(\\hat{\\theta})\\\\\n\\text{MSE}(\\hat{\\theta})\\\\\n\\\\\n\\end{array}\n\\begin{array}{ll}\n=\\text{E}(\\hat{\\theta})-\\theta\\\\\n=\\text{E}\\left((\\hat{\\theta}-\\text{E}(\\hat{\\theta}))^2\\right)\\\\\n=\\text{E}((\\hat{\\theta}-\\theta)^2)\\\\\n=\\text{Var}(\\hat{\\theta})+\\text{Bias}(\\hat{\\theta})^2\\\\\n\\end{array}\n\\end{align*}\\]\nThe last identity about MSE follows from \\(\\text{E}(x^2)=\\text{Var}(x)+\\text{E}(x)^2\\).\nAt first sight it seems desirable to focus on unbiased (for finite sample size \\(n\\)) estimators. However, requiring strict unbiasedness is not always a good idea. In many situations it is better to accept some bias in an estimator in order to achieve a smaller variance and an overall smaller MSE. This is called bias-variance tradeoff — as more bias is traded for smaller variance (or, conversely, less bias is traded for higher variance). This is also related to the above mentioned problems of underfitting (large bias) and overfitting (large variance).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Statistics refresher</span>"
    ]
  },
  {
    "objectID": "20-stats.html#efficiency-and-consistency-of-an-estimator",
    "href": "20-stats.html#efficiency-and-consistency-of-an-estimator",
    "title": "Appendix A — Statistics refresher",
    "section": "A.4 Efficiency and consistency of an estimator",
    "text": "A.4 Efficiency and consistency of an estimator\nTypically, \\(\\text{Bias}\\), \\(\\text{Var}\\) and \\(\\text{MSE}\\) all decrease with increasing sample size so that with more data \\(n \\to \\infty\\) the errors become smaller and smaller.\nEfficiency: An estimator \\(\\hat\\theta_A\\) is said to more efficient than estimator \\(\\hat\\theta_B\\) if for same sample size \\(n\\) it has smaller error (e.g. MSE) than the competing estimator.\nThe typical rate of decrease in variance of a good estimator is \\(\\frac{1}{n}\\) and the rate of decrease in the standard deviation is \\(\\frac{1}{\\sqrt{n}}\\). Note that this implies that to get one digit more accuracy in an estimate (standard deviation decreasing by factor of 10) we need 100 times more data!\nConsistency: \\(\\hat{\\theta}\\) is called consistent if \\[\n\\text{MSE}(\\hat{\\theta}) \\longrightarrow 0 \\text{ with $n\\rightarrow \\infty$ }\n\\]\nConsistency is an essential yet relatively weak requirement for any reasonable estimator. Among all consistent estimators we generally choose those that are most efficient, meaning that they exhibit the smallest variance and/or MSE for a given finite \\(n\\).\nConsistency implies that, given an infinite amount of data, the true model can be accurately identified, provided that the model class includes the actual data-generating model. If the model class does not encompass the true model, strict consistency cannot be attained. Nevertheless, our goal remains to choose a model that is closest to the true model and approximates it as best as possible.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Statistics refresher</span>"
    ]
  },
  {
    "objectID": "20-stats.html#sec-lawlargenumbers",
    "href": "20-stats.html#sec-lawlargenumbers",
    "title": "Appendix A — Statistics refresher",
    "section": "A.5 Law of large numbers",
    "text": "A.5 Law of large numbers\nThe law of large numbers, discovered by Jacob Bernoulli (1655-1705), asserts that, if the mean exists, the sample average will converge to the mean as the sample size \\(n\\) becomes large. Therefore, when the mean is defined, it can be approximated by the empirical mean for sufficiently large values of \\(n\\).\nA variant of the law of large numbers is that the empirical distribution \\(\\hat{F}_n\\) convergences strongly to \\(F\\) (Section A.6).\nAs a result, with \\(n \\rightarrow \\infty\\) there’s also convergence of the average of a function of the observed samples to the corresponding expectation of the function of the random variable: \\[\n\\text{E}_{\\hat{F}_n}(h(x)) =\n\\frac{1}{n} \\sum_{i=1}^n h(x_i) \\to \\text{E}_{F}(h(x))\n\\] Hence, the law of large numbers also ensures that empirical estimators (Section A.7) will converge to the corresponding true values for sufficiently large \\(n\\).\nMoreover, the law of large numbers provides a justification for interpreting large-sample limits of frequencies as probabilities. However, the converse assumption that all probabilities can be interpreted in frequentist manner does not follow from the law of large numbers or from the axioms of probability.\nFinally, it is worth pointing out that the law of large number doesn’t say anything about the finite sample properties of an estimator, it is only concerned with the asymptotic domain (large \\(n\\)).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Statistics refresher</span>"
    ]
  },
  {
    "objectID": "20-stats.html#sec-empiricalcdf",
    "href": "20-stats.html#sec-empiricalcdf",
    "title": "Appendix A — Statistics refresher",
    "section": "A.6 Empirical distribution function",
    "text": "A.6 Empirical distribution function\nSuppose we observe data \\(D=\\{x_1, \\ldots, x_n\\}\\) with each \\(x_i \\sim F\\) sampled independently and identically. The empirical cumulative distribution function \\(\\hat{F}_n(x)\\) based on data \\(D\\) is then given by \\[\n\\hat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n [x_i \\leq x]  \n\\] where \\([A]\\) is the indicator function in Iverson bracket notation which equals 1 if \\(A\\) is true and 0 otherwise. Thus \\(\\hat{F}_n(x)\\) counts how many observations \\(x_i \\in D\\) are smaller or equal than \\(x\\) and then standardises by the total number of samples \\(n\\).\nThe empirical distribution function is monotonically non-decreasing from 0 to 1 in discrete steps.\nIn R the empirical distribution function is computed by ecdf().\nCrucially, the empirical distribution \\(\\hat{F}_n\\) converges strongly (almost surely) to the underlying distribution \\(F\\) as \\(n \\rightarrow \\infty\\): \\[\n\\hat{F}_n\\overset{a. s.}{\\to} F\n\\] The Glivenko–Cantelli theorem additionally asserts that the convergence is uniform.\nThis theorem is a variant of the law of large numbers (Section A.5) applied to the whole distribution, rather than just to the mean.\nAs a result, we may use the empirical distribution \\(\\hat{F}_n\\) based on data \\(D\\) as an estimate of the underlying unknown true distribution \\(F\\). From the convergence theorems we know that \\(\\hat{F}_n\\) is consistent.\nHowever, for \\(\\hat{F}_n\\) to work well as an estimate of \\(F\\) the number of observations \\(n\\) must be sufficiently large so that the approximation provided by \\(\\hat{F}_n\\) is adequate.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Statistics refresher</span>"
    ]
  },
  {
    "objectID": "20-stats.html#sec-empiricalestimators",
    "href": "20-stats.html#sec-empiricalestimators",
    "title": "Appendix A — Statistics refresher",
    "section": "A.7 Empirical estimators",
    "text": "A.7 Empirical estimators\nThe fact that for large sample size \\(n\\) the empirical distribution \\(\\hat{F}_n\\) may be used as a substitute for the unknown \\(F\\) allows us to easily construct empirical estimators.\nSpecifically, parameters of a model can typically be expressed as a functional of the distribution \\(\\theta = g(F)\\). An empirical estimator \\(\\hat{\\theta}\\) is constructed by substituting the true distribution by the empirical distribution \\(\\hat{\\theta}= g( \\hat{F}_n )\\).\nAn example is the mean \\(\\text{E}_F(x)\\) with regard to \\(F\\). The empirical mean is the expectation with regard to the empirical distribution which equals the average of the samples: \\[\n\\hat{\\text{E}}(x) = \\hat{\\mu} =  \\text{E}_{\\hat{F}_n}(x) = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{x}\n\\]\nSimilarly, other empirical estimators can be constructed simply by replacing the expectation in the definition of the quantity of interest by the sample average. For example, the empirical variance with unknown mean is given by \\[\n\\widehat{\\text{Var}}(x) = \\widehat{\\sigma^2} =\n\\text{E}_{\\hat{F}_n}((x - \\hat{\\mu})^2) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\] Note the factor \\(1/n\\) before the summation sign. We can also write the empirical variance in terms of \\(\\overline{x^2} =\\frac{1}{n}\\sum^{n}_{k=1} x^2\\) as \\[\n\\widehat{\\text{Var}}(x) = \\overline{x^2} - \\bar{x}^2\n\\]\nBy construction, as a result of the strong convergence of \\(\\hat{F}_n\\) to \\(F\\) empirical estimators are consistent, with their MSE, variance and bias all decreasing to zero with large sample size \\(n\\). However, for finite sample size they do have a finite variance and may also be biased.\nFor example, the empirical variance given above is biased with \\(\\text{Bias}(\\widehat{\\sigma^2}) = -\\sigma^2/n\\). Note this bias decreases with \\(n\\). An unbiased estimator can be obtained by rescaling the empirical estimator by the factor \\(n/(n-1)\\): \\[\n\\widehat{\\sigma^2}_{\\text{UB}} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i -\\bar{x})^2\n\\]\nThe empirical estimators for the mean and variance can also be obtained for random vectors \\(\\boldsymbol x\\). In this case the data \\(D = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n \\}\\) is comprised of \\(n\\) vector-valued observations.\nFor the mean get \\[\n\\hat{\\boldsymbol \\mu} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k = \\bar{\\boldsymbol x}\n\\] and for the covariance \\[\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\sum^{n}_{k=1} \\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right) \\; \\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)^T\\] Note the factor \\(\\frac{1}{n}\\) in the estimator of the covariance matrix.\nWith \\(\\overline{\\boldsymbol x\\boldsymbol x^T} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k \\boldsymbol x_k^T\\) we can also write \\[\n\\widehat{\\boldsymbol \\Sigma} = \\overline{\\boldsymbol x\\boldsymbol x^T} - \\bar{\\boldsymbol x} \\bar{\\boldsymbol x}^T\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Statistics refresher</span>"
    ]
  },
  {
    "objectID": "20-stats.html#sec-distmeanvarest",
    "href": "20-stats.html#sec-distmeanvarest",
    "title": "Appendix A — Statistics refresher",
    "section": "A.8 Sampling distribution of mean and variance estimators for normal data",
    "text": "A.8 Sampling distribution of mean and variance estimators for normal data\nIf the underlying distribution family of \\(D = \\{x_1, \\ldots, x_n\\}\\) is known we can often obtain the exact distribution of an estimator.\nFor example, assuming normal distribution \\(x_i \\sim N(\\mu, \\sigma^2)\\) we can derive the sampling distribution for the empirical mean and variance:\n\nThe empirical estimator of the mean parameter \\(\\mu\\) is given by \\(\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\\). Under the normal assumption the distribution of \\(\\hat{\\mu}\\) is \\[\n\\hat{\\mu} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\] Thus \\(\\text{E}(\\hat{\\mu}) = \\mu\\) and \\(\\text{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n}\\). The estimate \\(\\hat{\\mu}\\) is unbiased as \\(\\text{E}(\\hat{\\mu})-\\mu = 0\\). The mean squared error of \\(\\hat{\\mu}\\) is \\(\\text{MSE}(\\hat{\\mu}) = \\frac{\\sigma^2}{n}\\).\nThe empirical variance \\(\\widehat{\\sigma^2} = \\frac{1}{n} \\sum_{i=1}^n (x_i -\\bar{x})^2\\) for normal data follows a one-dimensional Wishart distribution \\[\n\\widehat{\\sigma^2} \\sim\nW\\left(s^2 = \\frac{\\sigma^2}{n}, k=n-1\\right)\n\\] Thus, \\(\\text{E}( \\widehat{\\sigma^2} ) = \\frac{n-1}{n}\\sigma^2\\) and \\(\\text{Var}( \\widehat{\\sigma^2}_{\\text{ML}}  ) = \\frac{2(n-1)}{n^2}\\sigma^4\\). The estimate \\(\\widehat{\\sigma^2}\\) is biased since \\(\\text{E}( \\widehat{\\sigma^2}_{\\text{ML}}  )-\\sigma^2 = -\\frac{1}{n}\\sigma^2\\). The mean squared error is \\(\\text{MSE}( \\widehat{\\sigma^2}) = \\frac{2(n-1)}{n^2}\\sigma^4 +\\frac{1}{n^2}\\sigma^4 =\\frac{2 n-1}{n^2}\\sigma^4\\).\nThe unbiased variance estimate \\(\\widehat{\\sigma^2}_{\\text{UB}} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i -\\bar{x})^2\\) for normal data follows a one-dimensional Wishart distribution \\[\n\\widehat{\\sigma^2}_{\\text{UB}} \\sim\nW\\left(s^2 = \\frac{\\sigma^2}{n-1}, k = n-1 \\right)\n\\] Thus, \\(\\text{E}( \\widehat{\\sigma^2}_{\\text{UB}}  ) = \\sigma^2\\) and \\(\\text{Var}( \\widehat{\\sigma^2}_{\\text{UB}}  ) = \\frac{2}{n-1}\\sigma^4\\). The estimate \\(\\widehat{\\sigma^2}_{\\text{ML}}\\) is unbiased since \\(\\text{E}( \\widehat{\\sigma^2}_{\\text{UB}}  )-\\sigma^2 =0\\). The mean squared error is \\(\\text{MSE}( \\widehat{\\sigma^2}_{\\text{UB}} ) =\\frac{2}{n-1}\\sigma^4\\).\nInterestingly, for any \\(n&gt;1\\) we find that \\(\\text{Var}\\left( \\widehat{\\sigma^2}_{\\text{UB}}  \\right) &gt; \\text{Var}\\left( \\widehat{\\sigma^2}_{\\text{ML}}  \\right)\\) and \\(\\text{MSE}\\left( \\widehat{\\sigma^2}_{\\text{UB}} \\right) &gt; \\text{MSE}\\left( \\widehat{\\sigma^2}_{\\text{ML}} \\right)\\) so that the biased empirical estimator has both lower variance and lower mean squared error than the unbiased estimator.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Statistics refresher</span>"
    ]
  },
  {
    "objectID": "20-stats.html#sec-tstat",
    "href": "20-stats.html#sec-tstat",
    "title": "Appendix A — Statistics refresher",
    "section": "A.9 \\(t\\)-statistics",
    "text": "A.9 \\(t\\)-statistics\n\nOne sample \\(t\\)-statistic\nSuppose we observe \\(n\\) independent data points \\(x_1, \\ldots, x_n \\sim N(\\mu, \\sigma^2)\\). Then the average \\(\\bar{x} = \\sum_{i=1}^n x_i\\) is distributed as \\(\\bar{x} \\sim N(\\mu, \\sigma^2/n)\\) and correspondingly \\[\nz = \\frac{\\bar{x}-\\mu}{\\sqrt{\\sigma^2/n}} \\sim N(0, 1)\n\\]\nNote that \\(z\\) uses the known variance \\(\\sigma^2\\).\nIf the variance is unknown and is estimated by the unbiased variance\n\\[\ns^2_{\\text{UB}} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i -\\bar{x})^2\n\\] then one arrives at the one sample \\(t\\)-statistic \\[\nt_{\\text{UB}} = \\frac{\\bar{x}-\\mu}{\\sqrt{s^2_{\\text{UB}}/n}} \\sim \\text{$t_{n-1}$} \\,.\n\\] It is distributed according to a Student’s \\(t\\)-distribution with \\(n-1\\) degrees of freedom, with mean 0 for \\(n&gt;2\\) and variance \\((n-1)/(n-3)\\) for \\(n&gt;3\\).\nIf instead of the unbiased estimate the empirical variance (i.e. the maximum likelihood estimate, ML) \\[\ns^2_{\\text{ML}} = \\frac{1}{n} \\sum_{i=1}^n (x_i -\\bar{x})^2 = \\frac{n-1}{n} s^2_{\\text{UB}}\n\\] is used then this leads to a slightly different statistic \\[\nt_{\\text{ML}} = \\frac{\\bar{x}-\\mu}{ \\sqrt{ s^2_{\\text{ML}}/n}}  = \\sqrt{\\frac{n}{n-1}} t_{\\text{UB}}\n\\] with \\[\nt_{\\text{ML}} \\sim \\text{$t_{n-1}$}\\left(0, \\tau^2=\\frac{n}{n-1}\\right)\n\\] Thus, \\(t_{\\text{ML}}\\) follows a location-scale \\(t\\)-distribution, with mean 0 for \\(n&gt;2\\) and variance \\(n/(n-3)\\) for \\(n&gt;3\\).\n\n\nTwo sample \\(t\\)-statistic with common variance\nNow suppose we observe normal data \\(D = \\{x_1, \\ldots, x_n\\}\\) from two groups with sample size \\(n_1\\) and \\(n_2\\) (and \\(n=n_1+n_2\\)) with two different means \\(\\mu_1\\) and \\(\\mu_2\\) and common variance \\(\\sigma^2\\): \\[x_1,\\dots,x_{n_1} \\sim N(\\mu_1, \\sigma^2)\\] and \\[x_{n_1+1},\\dots,x_{n} \\sim N(\\mu_2, \\sigma^2)\\] Then \\(\\hat{\\mu}_1 = \\frac{1}{n_1}\\sum^{n_1}_{i=1}x_i\\) and \\(\\hat{\\mu}_2 = \\frac{1}{n_2}\\sum^{n}_{i=n_1+1}x_i\\) are the sample averages within each group.\nThe common variance \\(\\sigma^2\\) may be estimated either by the unbiased estimate \\[\ns^2_{\\text{UB}} = \\frac{1}{n-2} \\left(\\sum^{n_1}_{i=1}(x_i-\\hat{\\mu}_1)^2+\n\\sum^n_{i=n_1+1}(x_i-\\hat{\\mu}_2)^2\\right)\n\\] (note the factor \\(n-2\\)) or by the empirical estimate (ML)\n\\[\ns^2_{\\text{ML}} = \\frac{1}{n} \\left(\\sum^{n_1}_{i=1}(x_i-\\hat{\\mu}_1)^2+\\sum^n_{i=n_1+1}(x_i-\\hat{\\mu}_2)^2\\right) =\\frac{n-2}{n} s^2_{\\text{UB}}\n\\] The estimator for the common variance is a often referred to as pooled variance estimate as information is pooled from two groups to obtain the estimate.\nUsing the unbiased pooled variance estimate the two sample \\(t\\)-statistic is given by \\[\nt_{\\text{UB}} = \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ \\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)  s^2_{\\text{UB}}}  }\n= \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ \\left(\\frac{n}{n_1 n_2} \\right) s^2_{\\text{UB}} }  }\n\\] In terms of empirical frequencies \\(\\hat{\\pi}_1 = \\frac{n_1}{n}\\) and \\(\\hat{\\pi}_2 = \\frac{n_2}{n}\\) it can also be written as \\[\nt_{\\text{UB}} = \\sqrt{n} \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{  \\left(\\frac{1}{\\hat{\\pi}_1}+\\frac{1}{\\hat{\\pi}_2}\\right) s^2_{\\text{UB}} }}\n= \\sqrt{n\\hat{\\pi}_1 \\hat{\\pi}_2} \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ s^2_{\\text{UB}}}}\n\\] The two sample \\(t\\)-statistic is distributed as \\[\nt_{\\text{UB}} \\sim \\text{$t_{n-2}$}\n\\] i.e. according to a Student’s \\(t\\)-distribution with \\(n-2\\) degrees of freedom, with mean 0 for \\(n&gt;3\\) and variance \\((n-2)/(n-4)\\) for \\(n&gt;4\\). Large values of the two sample \\(t\\)-statistic indicates that there are indeed two groups rather than just one.\nThe two sample \\(t\\)-statistic using the empirical (ML) pooled estimate of the variance is \\[\n\\begin{split}\nt_{\\text{ML}} & = \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ \\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)  s^2_{\\text{ML}}  }   }\n= \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ \\left(\\frac{n}{n_1 n_2}\\right) s^2_{\\text{ML}}  }   }\\\\\n& =\\sqrt{n} \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ \\left(\\frac{1}{\\hat{\\pi}_1}+\\frac{1}{\\hat{\\pi}_2}\\right) s^2_{\\text{ML}} }}\n= \\sqrt{n \\hat{\\pi}_1 \\hat{\\pi}_2 } \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ s^2_{\\text{ML}}}}\\\\\n& = \\sqrt{\\frac{n}{n-2}} t_{\\text{UB}}\n\\end{split}\n\\] with \\[\nt_{\\text{ML}} \\sim \\text{$t_{n-2}$}\\left(0, \\tau^2=\\frac{n}{n-2}\\right)\n\\] Thus, \\(t_{\\text{ML}}\\) follows a location-scale \\(t\\)-distribution, with mean 0 for \\(n&gt;3\\) and variance \\(n/(n-4)\\) for \\(n&gt;4\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Statistics refresher</span>"
    ]
  },
  {
    "objectID": "20-stats.html#sec-ci",
    "href": "20-stats.html#sec-ci",
    "title": "Appendix A — Statistics refresher",
    "section": "A.10 Confidence intervals",
    "text": "A.10 Confidence intervals\n\nGeneral concept\nA confidence interval (CI) is an interval estimate \\(\\widehat{\\text{CI}}(x_1, \\ldots, x_n)\\) that depends on data and has a random (sampling) variation as well as a frequentist interpretation.\n\n\n\n\n\n\nFigure A.1: Coverage property of confidence intervals.\n\n\n\nA key property of a confidence interval is its coverage probability \\[\n\\kappa = \\text{Pr}( \\theta \\in \\widehat{\\text{CI}})\n\\] whih describes how often — in repeated identical experiments — the estimated CI overlaps the true parameter value \\(\\theta\\), i.e. how often it will “cover” \\(\\theta\\) (see Figure A.1). In the above \\(\\theta\\) is fixed and \\(\\widehat{\\text{CI}}\\) is random. Note that \\(\\kappa\\) is explicitly not the probability that the true value is contained in a specific instance of an estimated CI. Specifically, any particular CI based on a specific data set either covers \\(\\theta\\) or it doesn’t.\nHence, a coverage probability \\(\\kappa=0.95\\) (95%) means that in 100 repeated experiments 95 out of the 100 the corresponding estimated confidence intervals will contain the (unknown) true value.\nIt is trivial to create a CI with high coverage, simply by assuming a wide interval. Therefore, a useful confidence interval must be both compact and have high coverage.\nFinally, there is also a direct relationship between confidence intervals and statistical testing procedures. Specifically, a confidence interval can be interpreted as the set of parameter values that cannot be rejected. The complement \\(1-\\kappa\\) is called the rejection probability.\n\n\nSymmetric normal confidence interval\n\n\n\n\n\n\nFigure A.2: Construction of a symmetric two-sided normal confidence interval.\n\n\n\nFor a normally distributed univariate random variable it is straightforward to construct a symmetric two-sided CI with a given desired coverage \\(\\kappa\\) (Figure A.2).\nA symmetric normal CI with nominal coverage \\(\\kappa\\) for\n\na scalar parameter \\(\\theta\\)\nwith normally distributed estimate \\(\\hat{\\theta} \\sim N(\\theta, \\sigma^2)\\)\n\nis given by \\[\n\\widehat{\\text{CI}}=[\\hat{\\theta} \\pm c \\sigma]\n\\] where \\(c\\) is chosen to achieve the desired coverage probability \\[\n\\kappa = \\text{Pr}(\\hat{\\theta} - c \\sigma \\leq \\theta  \\leq  \\hat{\\theta} + c \\sigma)\n\\]\n\n\n\nTable A.1: Critical values for the normal distribution.\n\n\n\n\n\nCoverage \\(\\kappa\\)\nCritical value \\(c\\)\n\n\n\n\n0.90\n1.64\n\n\n0.95\n1.96\n\n\n0.99\n2.58\n\n\n\n\n\n\nTable A.1 lists the critical values \\(c\\) for the three most commonly used values of \\(\\kappa\\). It is useful to memorise these values as they are used frequently.\nThe critical values are obtained as follows. For a normal random variable \\(x \\sim N(\\mu, \\sigma^2)\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\) and density function \\(p(x|\\mu, \\sigma^2)\\) we can compute the probability \\[\n\\text{Pr}(x \\leq \\mu + c \\sigma) =  \\int_{-\\infty}^{\\mu+c\\sigma} p(x|\\mu, \\sigma^2) dx  = \\Phi (c) = \\frac{1+\\kappa}{2}\n\\] where \\(\\Phi(c)\\) is the cumulative distribution function (CDF) of the standard normal \\(N(0,1)\\) distribution. Its inverse \\(\\Phi^{-1}\\) is the standard normal quantile function. The critical value \\(c\\) is obtained by \\[\nc=\\Phi^{-1}\\left(\\frac{1+\\kappa}{2}\\right)\n\\]\n\n\nOne-sided confidence interval based on the chi-squared distribution\n\n\n\n\n\n\nFigure A.3: Construction of a one-sided confidence interval based on a chi-squared distribution with one degree of freedom.\n\n\n\nFor a chi-squared distributed statistic commonly a one-sided CI of the form \\([0, c ]\\) is used with nominal coverage probability \\(\\kappa = \\text{Pr}(x \\leq c)\\) (see Figure A.3). As before we obtain \\(c\\) from the quantile function, i.e. by inverting the CDF of the chi-squared distribution.\n\n\n\nTable A.2: Critical values for the chi-squared distribution with one degree of freedom.\n\n\n\n\n\nCoverage \\(\\kappa\\)\nCritical value \\(c\\)\n\n\n\n\n0.90\n2.71\n\n\n0.95\n3.84\n\n\n0.99\n6.63\n\n\n\n\n\n\nTable A.2 lists the critical values for the three most common choices of the coverage probability \\(\\kappa\\) for a chi-squared distribution with one degree of freedom. Note that these critical values are the squared values of the corresponding thresholds in Table A.1 (with small discrepancies due to rounding).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Statistics refresher</span>"
    ]
  },
  {
    "objectID": "21-further-study.html",
    "href": "21-further-study.html",
    "title": "Appendix B — Further study",
    "section": "",
    "text": "B.1 Recommended reading\nIn this module we can only touch the surface of likelihood and Bayes inference. As a starting point for further reading the following text books are recommended.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Further study</span>"
    ]
  },
  {
    "objectID": "21-further-study.html#recommended-reading",
    "href": "21-further-study.html#recommended-reading",
    "title": "Appendix B — Further study",
    "section": "",
    "text": "Held and Bové (2020) Applied Statistical Inference: Likelihood and Bayes (2nd edition). Springer.\nAgresti and Kateri (2022) Foundations of Statistics for Data Scientists. Chapman and Hall/CRC.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Further study</span>"
    ]
  },
  {
    "objectID": "21-further-study.html#additional-references",
    "href": "21-further-study.html#additional-references",
    "title": "Appendix B — Further study",
    "section": "B.2 Additional references",
    "text": "B.2 Additional references\n\nHeard (2021) An Introduction to Bayesian Inference, Methods and Computation. Springer.\nGelman et al. (2014) Bayesian data analysis (3rd edition). CRC Press.\n\n\n\n\n\nAgresti, A., and M. Kateri. 2022. Foundations of Statistics for Data Scientists. Chapman; Hall/CRC.\n\n\nGelman, A., J. B. Carlin, H. A. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. 2014. Bayesian Data Analysis. 3rd ed. CRC Press.\n\n\nHeard, N. 2021. An Introduction to Bayesian Inference, Methods and Computation. Springer.\n\n\nHeld, L., and D. S. Bové. 2020. Applied Statistical Inference: Likelihood and Bayes. Second. Springer.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Further study</span>"
    ]
  }
]