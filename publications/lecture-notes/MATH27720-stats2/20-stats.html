<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Appendix A — Statistics refresher – Statistics 2: Statistical Learning with Likelihood and Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./21-further-study.html" rel="next">
<link href="./bibliography.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c63e616a2164fee75212f002d4510366.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./20-stats.html">Appendices</a></li><li class="breadcrumb-item"><a href="./20-stats.html"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics 2: Statistical Learning with Likelihood and Bayes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-prerequisites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prerequisites</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropy and likelihood</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-intro2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributions for statistical models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-entropy1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-entropy2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Relative entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-entropy3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expected Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-entropy4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Principle of maximum entropy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-likelihood1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Principle of maximum likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-likelihood2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Maximum likelihood estimation in practise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-likelihood3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Observed Fisher information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-likelihood4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quadratic approximation and normal asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-likelihood5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Likelihood-based confidence interval and likelihood ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-likelihood6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Optimality properties and conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayesian statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Conditioning and Bayes rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Models with latent variables and missing data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Essentials of Bayesian statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-bayes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian learning in practise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-bayes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian model comparison</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-bayes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing priors in Bayesian analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-bayes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optimality properties and summary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-stats.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#data-and-statistics-as-functions-of-data" id="toc-data-and-statistics-as-functions-of-data" class="nav-link active" data-scroll-target="#data-and-statistics-as-functions-of-data"><span class="header-section-number">A.1</span> Data and statistics as functions of data</a></li>
  <li><a href="#statistical-learning" id="toc-statistical-learning" class="nav-link" data-scroll-target="#statistical-learning"><span class="header-section-number">A.2</span> Statistical learning</a></li>
  <li><a href="#sampling-properties-of-a-point-estimator" id="toc-sampling-properties-of-a-point-estimator" class="nav-link" data-scroll-target="#sampling-properties-of-a-point-estimator"><span class="header-section-number">A.3</span> Sampling properties of a point estimator</a></li>
  <li><a href="#efficiency-and-consistency-of-an-estimator" id="toc-efficiency-and-consistency-of-an-estimator" class="nav-link" data-scroll-target="#efficiency-and-consistency-of-an-estimator"><span class="header-section-number">A.4</span> Efficiency and consistency of an estimator</a></li>
  <li><a href="#sec-lawlargenumbers" id="toc-sec-lawlargenumbers" class="nav-link" data-scroll-target="#sec-lawlargenumbers"><span class="header-section-number">A.5</span> Law of large numbers</a></li>
  <li><a href="#sec-empiricalcdf" id="toc-sec-empiricalcdf" class="nav-link" data-scroll-target="#sec-empiricalcdf"><span class="header-section-number">A.6</span> Empirical distribution function</a></li>
  <li><a href="#sec-empiricalestimators" id="toc-sec-empiricalestimators" class="nav-link" data-scroll-target="#sec-empiricalestimators"><span class="header-section-number">A.7</span> Empirical estimators</a></li>
  <li><a href="#sec-distmeanvarest" id="toc-sec-distmeanvarest" class="nav-link" data-scroll-target="#sec-distmeanvarest"><span class="header-section-number">A.8</span> Sampling distribution of mean and variance estimators for normal data</a></li>
  <li><a href="#sec-tstat" id="toc-sec-tstat" class="nav-link" data-scroll-target="#sec-tstat"><span class="header-section-number">A.9</span> <span class="math inline">\(t\)</span>-statistics</a>
  <ul class="collapse">
  <li><a href="#one-sample-t-statistic" id="toc-one-sample-t-statistic" class="nav-link" data-scroll-target="#one-sample-t-statistic">One sample <span class="math inline">\(t\)</span>-statistic</a></li>
  <li><a href="#two-sample-t-statistic-with-common-variance" id="toc-two-sample-t-statistic-with-common-variance" class="nav-link" data-scroll-target="#two-sample-t-statistic-with-common-variance">Two sample <span class="math inline">\(t\)</span>-statistic with common variance</a></li>
  </ul></li>
  <li><a href="#sec-ci" id="toc-sec-ci" class="nav-link" data-scroll-target="#sec-ci"><span class="header-section-number">A.10</span> Confidence intervals</a>
  <ul class="collapse">
  <li><a href="#general-concept" id="toc-general-concept" class="nav-link" data-scroll-target="#general-concept">General concept</a></li>
  <li><a href="#sec-normci" id="toc-sec-normci" class="nav-link" data-scroll-target="#sec-normci">Symmetric normal confidence interval</a></li>
  <li><a href="#sec-chisqci" id="toc-sec-chisqci" class="nav-link" data-scroll-target="#sec-chisqci">One-sided confidence interval based on the chi-squared distribution</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./20-stats.html">Appendices</a></li><li class="breadcrumb-item"><a href="./20-stats.html"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statistics refresher</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-statrefresh" class="quarto-section-identifier">Appendix A — Statistics refresher</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Below you find a brief overview over some relevant concepts in statistics that you should be familiar with from earlier modules.</p>
<section id="data-and-statistics-as-functions-of-data" class="level2" data-number="A.1">
<h2 data-number="A.1" class="anchored" data-anchor-id="data-and-statistics-as-functions-of-data"><span class="header-section-number">A.1</span> Data and statistics as functions of data</h2>
<p>Broadly, by “data” we refer to quantitative observations and measurements collected in experiments.</p>
<p>We denote the observed data by <span class="math inline">\(D =\{x_1, \ldots, x_n\}\)</span> where <span class="math inline">\(n\)</span> denotes the number of data points (the <strong>sample size</strong>). Each data point can be scalar or a multivariate quantity.</p>
<p>Generally, a <strong>statistic</strong> <span class="math inline">\(t(D)\)</span> is function of the observed data <span class="math inline">\(D\)</span>. The statistic <span class="math inline">\(t(D)\)</span> can be of any type and value (scalar, vector, matrix etc. — even a function). <span class="math inline">\(t(D)\)</span> is called a <em>summary statistic</em> if it describes important aspects of the data such as location (e.g.&nbsp;the average <span class="math inline">\(\text{avg}(D) =\bar{x}\)</span>, the median) or scale (e.g.&nbsp;standard deviation, interquartile range).</p>
</section>
<section id="statistical-learning" class="level2" data-number="A.2">
<h2 data-number="A.2" class="anchored" data-anchor-id="statistical-learning"><span class="header-section-number">A.2</span> Statistical learning</h2>
<p>The aim in statistics, and by extension in data science and machine learning, is to use data to learn about and better understand the world. It is a key feature of statistics to employ probabilistic models for that purpose.</p>
<p>Let denote data models by <span class="math inline">\(p(x| \theta)\)</span> where <span class="math inline">\(\theta\)</span> represents the parameters of the model. Often (but not always) <span class="math inline">\(\theta\)</span> can be interpreted as or is associated with some manifest property of the model. If there is only a single parameter we write <span class="math inline">\(\theta\)</span> (scalar parameter). If we wish to highlight that there are multiple parameters we write <span class="math inline">\(\boldsymbol \theta\)</span> (in bold type).</p>
<p>Specifically, our aim is to identify the best model(s) for the data in order to both</p>
<ul>
<li>explain the current data, and</li>
<li>to enable good prediction of future data.</li>
</ul>
<p>By choosing a sufficiently complex model the first aim (to explain the observed data) is often easily achieved. However, if the model is too complex it can fail to address the second aim (to predict unseen data well). Thus, when choosing a model we would like to avoid both the problem of <strong>underfitting</strong> (i.e.&nbsp;choosing an overly simplistic model) as well <strong>overfitting</strong> (i.e.&nbsp;choosing an overly complex model). Finding this balances will also help with interpreting the fitted model.</p>
<p>Typically, we focus the analysis to a specific model family with a some parameter <span class="math inline">\(\theta\)</span>.<br>
An <strong>estimator for <span class="math inline">\(\theta\)</span></strong> is a function <span class="math inline">\(\hat{\theta}(D)\)</span> of the data that maps the data (input) to an informed guess (output) about <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li>A <strong>point estimator</strong> provides a single number for each parameter</li>
<li>An <strong>interval estimator</strong> provides a set of possible values for each parameter.</li>
</ul>
<p>Interval estimators can be linked to the concept of testing specified values for a parameter. Specifically a confidence interval contains all parameter values that are not significantly different from the best parameter.</p>
</section>
<section id="sampling-properties-of-a-point-estimator" class="level2" data-number="A.3">
<h2 data-number="A.3" class="anchored" data-anchor-id="sampling-properties-of-a-point-estimator"><span class="header-section-number">A.3</span> Sampling properties of a point estimator</h2>
<p>A point estimator <span class="math inline">\(\hat\theta\)</span> depends on the data, hence it exhibits <strong>sampling variation</strong>, i.e.&nbsp;estimate will be different for a new set of observations.</p>
<p>Thus <span class="math inline">\(\hat\theta\)</span> can be seen as a random variable, and its distribution is called <strong>sampling distribution</strong> (across different experiments).</p>
<p>Properties of this distribution can be used to evaluate how far the estimator deviates (on average across different experiments) from the true value:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{rr}
\text{Bias:}\\
\text{Variance:}\\
\text{Mean squared error:}\\
\\
\end{array}
\begin{array}{rr}
\text{Bias}(\hat{\theta})\\
\text{Var}(\hat{\theta})\\
\text{MSE}(\hat{\theta})\\
\\
\end{array}
\begin{array}{ll}
=\text{E}(\hat{\theta})-\theta\\
=\text{E}\left((\hat{\theta}-\text{E}(\hat{\theta}))^2\right)\\
=\text{E}((\hat{\theta}-\theta)^2)\\
=\text{Var}(\hat{\theta})+\text{Bias}(\hat{\theta})^2\\
\end{array}
\end{align*}\]</span></p>
<p>The last identity about MSE follows from <span class="math inline">\(\text{E}(x^2)=\text{Var}(x)+\text{E}(x)^2\)</span>.</p>
<p>At first sight it seems desirable to focus on unbiased (for finite sample size <span class="math inline">\(n\)</span>) estimators. However, requiring strict unbiasedness is not always a good idea. In many situations it is better to accept some bias in an estimator in order to achieve a smaller variance and an overall smaller MSE. This is called <strong>bias-variance tradeoff</strong> — as more bias is traded for smaller variance (or, conversely, less bias is traded for higher variance). This is also related to the above mentioned problems of underfitting (large bias) and overfitting (large variance).</p>
</section>
<section id="efficiency-and-consistency-of-an-estimator" class="level2" data-number="A.4">
<h2 data-number="A.4" class="anchored" data-anchor-id="efficiency-and-consistency-of-an-estimator"><span class="header-section-number">A.4</span> Efficiency and consistency of an estimator</h2>
<p>Typically, <span class="math inline">\(\text{Bias}\)</span>, <span class="math inline">\(\text{Var}\)</span> and <span class="math inline">\(\text{MSE}\)</span> all decrease with increasing sample size so that with more data <span class="math inline">\(n \to \infty\)</span> the errors become smaller and smaller.</p>
<p><strong>Efficiency</strong>: An estimator <span class="math inline">\(\hat\theta_A\)</span> is said to more efficient than estimator <span class="math inline">\(\hat\theta_B\)</span> if for same sample size <span class="math inline">\(n\)</span> it has smaller error (e.g.&nbsp;MSE) than the competing estimator.</p>
<p>The typical rate of decrease in variance of a good estimator is <span class="math inline">\(\frac{1}{n}\)</span> and the rate of decrease in the standard deviation is <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span>. Note that this implies that to get one digit more accuracy in an estimate (standard deviation decreasing by factor of 10) we need 100 times more data!</p>
<p><strong>Consistency</strong>: <span class="math inline">\(\hat{\theta}\)</span> is called consistent if <span class="math display">\[
\text{MSE}(\hat{\theta}) \longrightarrow 0 \text{ with $n\rightarrow \infty$ }
\]</span></p>
<p>Consistency is an essential yet relatively weak requirement for any reasonable estimator. Among all consistent estimators we generally choose those that are most <strong>efficient</strong>, meaning that they exhibit the smallest variance and/or MSE for a given finite <span class="math inline">\(n\)</span>.</p>
<p>Consistency implies that, given an infinite amount of data, the true model can be accurately identified, provided that the model class includes the actual data-generating model. If the model class does not encompass the true model, strict consistency cannot be attained. Nevertheless, our goal remains to choose a model that is closest to the true model and approximates it as best as possible.</p>
</section>
<section id="sec-lawlargenumbers" class="level2" data-number="A.5">
<h2 data-number="A.5" class="anchored" data-anchor-id="sec-lawlargenumbers"><span class="header-section-number">A.5</span> Law of large numbers</h2>
<p>The <strong>law of large numbers</strong>, discovered by <a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Jacob Bernoulli (1655-1705)</a>, asserts that, if the mean exists, the sample average will converge to the mean as the sample size <span class="math inline">\(n\)</span> becomes large. Therefore, when the mean is defined, it can be approximated by the empirical mean for sufficiently large values of <span class="math inline">\(n\)</span>.</p>
<p>A variant of the law of large numbers is that the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> convergences strongly to <span class="math inline">\(F\)</span> (<a href="#sec-empiricalcdf" class="quarto-xref"><span>Section A.6</span></a>).</p>
<p>As a result, with <span class="math inline">\(n \rightarrow \infty\)</span> there’s also convergence of the average of a function of the observed samples to the corresponding expectation of the function of the random variable: <span class="math display">\[
\text{E}_{\hat{F}_n}(h(x)) =
\frac{1}{n} \sum_{i=1}^n h(x_i) \to \text{E}_{F}(h(x))
\]</span> Hence, the law of large numbers also ensures that empirical estimators (<a href="#sec-empiricalestimators" class="quarto-xref"><span>Section A.7</span></a>) will converge to the corresponding true values for sufficiently large <span class="math inline">\(n\)</span>.</p>
<p>Moreover, the law of large numbers provides a <strong>justification for interpreting large-sample limits of frequencies as probabilities</strong>. However, <strong>the converse</strong> assumption that all probabilities can be interpreted in frequentist manner <strong>does not follow</strong> from the law of large numbers or from the axioms of probability.</p>
<p>Finally, it is worth pointing out that the law of large number doesn’t say anything about the finite sample properties of an estimator, it is only concerned with the asymptotic domain (large <span class="math inline">\(n\)</span>).</p>
</section>
<section id="sec-empiricalcdf" class="level2" data-number="A.6">
<h2 data-number="A.6" class="anchored" data-anchor-id="sec-empiricalcdf"><span class="header-section-number">A.6</span> Empirical distribution function</h2>
<p>Suppose we observe data <span class="math inline">\(D=\{x_1, \ldots, x_n\}\)</span> with each <span class="math inline">\(x_i \sim F\)</span> sampled independently and identically. The empirical cumulative distribution function <span class="math inline">\(\hat{F}_n(x)\)</span> based on data <span class="math inline">\(D\)</span> is then given by <span class="math display">\[
\hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n [x_i \leq x]  
\]</span> where <span class="math inline">\([A]\)</span> is the indicator function in Iverson bracket notation which equals 1 if <span class="math inline">\(A\)</span> is true and 0 otherwise. Thus <span class="math inline">\(\hat{F}_n(x)\)</span> counts how many observations <span class="math inline">\(x_i \in D\)</span> are smaller or equal than <span class="math inline">\(x\)</span> and then standardises by the total number of samples <span class="math inline">\(n\)</span>.</p>
<p>The empirical distribution function is monotonically non-decreasing from 0 to 1 in discrete steps.</p>
<p>In R the empirical distribution function is computed by <code>ecdf()</code>.</p>
<p>Crucially, the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> converges strongly (almost surely) to the underlying distribution <span class="math inline">\(F\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>: <span class="math display">\[
\hat{F}_n\overset{a. s.}{\to} F
\]</span> The <a href="https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem">Glivenko–Cantelli theorem</a> additionally asserts that the convergence is uniform.</p>
<p>This theorem is a variant of the <strong>law of large numbers</strong> (<a href="#sec-lawlargenumbers" class="quarto-xref"><span>Section A.5</span></a>) applied to the whole distribution, rather than just to the mean.</p>
<p>As a result, we may use the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> based on data <span class="math inline">\(D\)</span> as an estimate of the underlying unknown true distribution <span class="math inline">\(F\)</span>. From the convergence theorems we know that <span class="math inline">\(\hat{F}_n\)</span> is consistent.</p>
<p>However, for <span class="math inline">\(\hat{F}_n\)</span> to work well as an estimate of <span class="math inline">\(F\)</span> the number of observations <span class="math inline">\(n\)</span> must be sufficiently large so that the approximation provided by <span class="math inline">\(\hat{F}_n\)</span> is adequate.</p>
</section>
<section id="sec-empiricalestimators" class="level2" data-number="A.7">
<h2 data-number="A.7" class="anchored" data-anchor-id="sec-empiricalestimators"><span class="header-section-number">A.7</span> Empirical estimators</h2>
<p>The fact that for large sample size <span class="math inline">\(n\)</span> the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> may be used as a substitute for the unknown <span class="math inline">\(F\)</span> allows us to easily construct empirical estimators.</p>
<p>Specifically, parameters of a model can typically be expressed as a functional of the distribution <span class="math inline">\(\theta = g(F)\)</span>. An <strong>empirical estimator</strong> <span class="math inline">\(\hat{\theta}\)</span> is constructed by substituting the true distribution by the empirical distribution <span class="math inline">\(\hat{\theta}= g( \hat{F}_n )\)</span>.</p>
<p>An example is the mean <span class="math inline">\(\text{E}_F(x)\)</span> with regard to <span class="math inline">\(F\)</span>. The <strong>empirical mean</strong> is the expectation with regard to the empirical distribution which equals the <strong>average of the samples</strong>: <span class="math display">\[
\hat{\text{E}}(x) = \hat{\mu} =  \text{E}_{\hat{F}_n}(x) = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x}
\]</span></p>
<p>Similarly, other empirical estimators can be constructed simply by replacing the expectation in the definition of the quantity of interest by the sample average. For example, the <strong>empirical variance</strong> with unknown mean is given by <span class="math display">\[
\widehat{\text{Var}}(x) = \widehat{\sigma^2} =
\text{E}_{\hat{F}_n}((x - \hat{\mu})^2) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2
\]</span> Note the factor <span class="math inline">\(1/n\)</span> before the summation sign. We can also write the empirical variance in terms of <span class="math inline">\(\overline{x^2} =\frac{1}{n}\sum^{n}_{k=1} x^2\)</span> as <span class="math display">\[
\widehat{\text{Var}}(x) = \overline{x^2} - \bar{x}^2
\]</span></p>
<p>By construction, as a result of the strong convergence of <span class="math inline">\(\hat{F}_n\)</span> to <span class="math inline">\(F\)</span> empirical estimators are consistent, with their MSE, variance and bias all decreasing to zero with large sample size <span class="math inline">\(n\)</span>. However, for finite sample size they do have a finite variance and may also be biased.</p>
<p>For example, the empirical variance given above is biased with <span class="math inline">\(\text{Bias}(\widehat{\sigma^2}) = -\sigma^2/n\)</span>. Note this bias decreases with <span class="math inline">\(n\)</span>. An unbiased estimator can be obtained by rescaling the empirical estimator by the factor <span class="math inline">\(n/(n-1)\)</span>: <span class="math display">\[
\widehat{\sigma^2}_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\bar{x})^2
\]</span></p>
<p>The empirical estimators for the mean and variance can also be obtained for random vectors <span class="math inline">\(\boldsymbol x\)</span>. In this case the data <span class="math inline">\(D = \{\boldsymbol x_1, \ldots, \boldsymbol x_n \}\)</span> is comprised of <span class="math inline">\(n\)</span> vector-valued observations.</p>
<p>For the mean get <span class="math display">\[
\hat{\boldsymbol \mu} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k = \bar{\boldsymbol x}
\]</span> and for the covariance <span class="math display">\[\widehat{\boldsymbol \Sigma} = \frac{1}{n}\sum^{n}_{k=1} \left(\boldsymbol x_k-\bar{\boldsymbol x}\right) \; \left(\boldsymbol x_k-\bar{\boldsymbol x}\right)^T\]</span> Note the factor <span class="math inline">\(\frac{1}{n}\)</span> in the estimator of the covariance matrix.</p>
<p>With <span class="math inline">\(\overline{\boldsymbol x\boldsymbol x^T} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k \boldsymbol x_k^T\)</span> we can also write <span class="math display">\[
\widehat{\boldsymbol \Sigma} = \overline{\boldsymbol x\boldsymbol x^T} - \bar{\boldsymbol x} \bar{\boldsymbol x}^T
\]</span></p>
</section>
<section id="sec-distmeanvarest" class="level2" data-number="A.8">
<h2 data-number="A.8" class="anchored" data-anchor-id="sec-distmeanvarest"><span class="header-section-number">A.8</span> Sampling distribution of mean and variance estimators for normal data</h2>
<p>If the underlying distribution family of <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> is known we can often obtain the exact distribution of an estimator.</p>
<p>For example, assuming normal distribution <span class="math inline">\(x_i \sim N(\mu, \sigma^2)\)</span> we can derive the sampling distribution for the empirical mean and variance:</p>
<ul>
<li><p>The empirical estimator of the mean parameter <span class="math inline">\(\mu\)</span> is given by <span class="math inline">\(\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i\)</span>. Under the normal assumption the distribution of <span class="math inline">\(\hat{\mu}\)</span> is <span class="math display">\[
\hat{\mu} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
\]</span> Thus <span class="math inline">\(\text{E}(\hat{\mu}) = \mu\)</span> and <span class="math inline">\(\text{Var}(\hat{\mu}) = \frac{\sigma^2}{n}\)</span>. The estimate <span class="math inline">\(\hat{\mu}\)</span> is unbiased as <span class="math inline">\(\text{E}(\hat{\mu})-\mu = 0\)</span>. The mean squared error of <span class="math inline">\(\hat{\mu}\)</span> is <span class="math inline">\(\text{MSE}(\hat{\mu}) = \frac{\sigma^2}{n}\)</span>.</p></li>
<li><p>The empirical variance <span class="math inline">\(\widehat{\sigma^2} = \frac{1}{n} \sum_{i=1}^n (x_i -\bar{x})^2\)</span> for normal data follows a one-dimensional Wishart distribution <span class="math display">\[
\widehat{\sigma^2} \sim
\text{Wis}\left(s^2 = \frac{\sigma^2}{n}, k=n-1\right)
\]</span> Thus, <span class="math inline">\(\text{E}( \widehat{\sigma^2} ) = \frac{n-1}{n}\sigma^2\)</span> and <span class="math inline">\(\text{Var}( \widehat{\sigma^2}_{\text{ML}}  ) = \frac{2(n-1)}{n^2}\sigma^4\)</span>. The estimate <span class="math inline">\(\widehat{\sigma^2}\)</span> is biased since <span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{ML}}  )-\sigma^2 = -\frac{1}{n}\sigma^2\)</span>. The mean squared error is <span class="math inline">\(\text{MSE}( \widehat{\sigma^2}) = \frac{2(n-1)}{n^2}\sigma^4 +\frac{1}{n^2}\sigma^4 =\frac{2 n-1}{n^2}\sigma^4\)</span>.</p></li>
<li><p>The unbiased variance estimate <span class="math inline">\(\widehat{\sigma^2}_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\bar{x})^2\)</span> for normal data follows a one-dimensional Wishart distribution <span class="math display">\[
\widehat{\sigma^2}_{\text{UB}} \sim
\text{Wis}\left(s^2 = \frac{\sigma^2}{n-1}, k = n-1 \right)
\]</span> Thus, <span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{UB}}  ) = \sigma^2\)</span> and <span class="math inline">\(\text{Var}( \widehat{\sigma^2}_{\text{UB}}  ) = \frac{2}{n-1}\sigma^4\)</span>. The estimate <span class="math inline">\(\widehat{\sigma^2}_{\text{ML}}\)</span> is unbiased since <span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{UB}}  )-\sigma^2 =0\)</span>. The mean squared error is <span class="math inline">\(\text{MSE}( \widehat{\sigma^2}_{\text{UB}} ) =\frac{2}{n-1}\sigma^4\)</span>.</p>
<p>Interestingly, for any <span class="math inline">\(n&gt;1\)</span> we find that <span class="math inline">\(\text{Var}\left( \widehat{\sigma^2}_{\text{UB}}  \right) &gt; \text{Var}\left( \widehat{\sigma^2}_{\text{ML}}  \right)\)</span> and <span class="math inline">\(\text{MSE}\left( \widehat{\sigma^2}_{\text{UB}} \right) &gt; \text{MSE}\left( \widehat{\sigma^2}_{\text{ML}} \right)\)</span> so that the biased empirical estimator has both lower variance and lower mean squared error than the unbiased estimator.</p></li>
</ul>
</section>
<section id="sec-tstat" class="level2" data-number="A.9">
<h2 data-number="A.9" class="anchored" data-anchor-id="sec-tstat"><span class="header-section-number">A.9</span> <span class="math inline">\(t\)</span>-statistics</h2>
<section id="one-sample-t-statistic" class="level3">
<h3 class="anchored" data-anchor-id="one-sample-t-statistic">One sample <span class="math inline">\(t\)</span>-statistic</h3>
<p>Suppose we observe <span class="math inline">\(n\)</span> independent data points <span class="math inline">\(x_1, \ldots, x_n \sim N(\mu, \sigma^2)\)</span>. Then the average <span class="math inline">\(\bar{x} = \sum_{i=1}^n x_i\)</span> is distributed as <span class="math inline">\(\bar{x} \sim N(\mu, \sigma^2/n)\)</span> and correspondingly <span class="math display">\[
z = \frac{\bar{x}-\mu}{\sqrt{\sigma^2/n}} \sim N(0, 1)
\]</span></p>
<p>Note that <span class="math inline">\(z\)</span> uses the <em>known variance</em> <span class="math inline">\(\sigma^2\)</span>.</p>
<p>If the variance is unknown and is estimated by the <em>unbiased variance</em><br>
<span class="math display">\[
s^2_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\bar{x})^2
\]</span> then one arrives at the one sample <span class="math inline">\(t\)</span>-statistic <span class="math display">\[
t_{\text{UB}} = \frac{\bar{x}-\mu}{\sqrt{s^2_{\text{UB}}/n}} \sim \text{$t_{n-1}$} \,.
\]</span> It is distributed according to a Student’s <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom, with mean 0 for <span class="math inline">\(n&gt;2\)</span> and variance <span class="math inline">\((n-1)/(n-3)\)</span> for <span class="math inline">\(n&gt;3\)</span>.</p>
<p>If instead of the unbiased estimate the <em>empirical variance</em> (i.e.&nbsp;the maximum likelihood estimate, ML) <span class="math display">\[
s^2_{\text{ML}} = \frac{1}{n} \sum_{i=1}^n (x_i -\bar{x})^2 = \frac{n-1}{n} s^2_{\text{UB}}
\]</span> is used then this leads to a slightly different statistic <span class="math display">\[
t_{\text{ML}} = \frac{\bar{x}-\mu}{ \sqrt{ s^2_{\text{ML}}/n}}  = \sqrt{\frac{n}{n-1}} t_{\text{UB}}
\]</span> with <span class="math display">\[
t_{\text{ML}} \sim \text{$t_{n-1}$}\left(0, \tau^2=\frac{n}{n-1}\right)
\]</span> Thus, <span class="math inline">\(t_{\text{ML}}\)</span> follows a location-scale <span class="math inline">\(t\)</span>-distribution, with mean 0 for <span class="math inline">\(n&gt;2\)</span> and variance <span class="math inline">\(n/(n-3)\)</span> for <span class="math inline">\(n&gt;3\)</span>.</p>
</section>
<section id="two-sample-t-statistic-with-common-variance" class="level3">
<h3 class="anchored" data-anchor-id="two-sample-t-statistic-with-common-variance">Two sample <span class="math inline">\(t\)</span>-statistic with common variance</h3>
<p>Now suppose we observe normal data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> from two groups with sample size <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> (and <span class="math inline">\(n=n_1+n_2\)</span>) with two different means <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> and common variance <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[x_1,\dots,x_{n_1} \sim N(\mu_1, \sigma^2)\]</span> and <span class="math display">\[x_{n_1+1},\dots,x_{n} \sim N(\mu_2, \sigma^2)\]</span> Then <span class="math inline">\(\hat{\mu}_1 = \frac{1}{n_1}\sum^{n_1}_{i=1}x_i\)</span> and <span class="math inline">\(\hat{\mu}_2 = \frac{1}{n_2}\sum^{n}_{i=n_1+1}x_i\)</span> are the sample averages within each group.</p>
<p>The common variance <span class="math inline">\(\sigma^2\)</span> may be estimated either by the <em>unbiased estimate</em> <span class="math display">\[
s^2_{\text{UB}} = \frac{1}{n-2} \left(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)^2+
\sum^n_{i=n_1+1}(x_i-\hat{\mu}_2)^2\right)
\]</span> (note the factor <span class="math inline">\(n-2\)</span>) or by the <em>empirical estimate</em> (ML)<br>
<span class="math display">\[
s^2_{\text{ML}} = \frac{1}{n} \left(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)^2+\sum^n_{i=n_1+1}(x_i-\hat{\mu}_2)^2\right) =\frac{n-2}{n} s^2_{\text{UB}}
\]</span> The estimator for the common variance is a often referred to as <em>pooled variance estimate</em> as information is pooled from two groups to obtain the estimate.</p>
<p>Using the unbiased pooled variance estimate the two sample <span class="math inline">\(t\)</span>-statistic is given by <span class="math display">\[
t_{\text{UB}} = \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{1}{n_1}+\frac{1}{n_2}\right)  s^2_{\text{UB}}}  }
= \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{n}{n_1 n_2} \right) s^2_{\text{UB}} }  }
\]</span> In terms of empirical frequencies <span class="math inline">\(\hat{\pi}_1 = \frac{n_1}{n}\)</span> and <span class="math inline">\(\hat{\pi}_2 = \frac{n_2}{n}\)</span> it can also be written as <span class="math display">\[
t_{\text{UB}} = \sqrt{n} \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{  \left(\frac{1}{\hat{\pi}_1}+\frac{1}{\hat{\pi}_2}\right) s^2_{\text{UB}} }}
= \sqrt{n\hat{\pi}_1 \hat{\pi}_2} \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ s^2_{\text{UB}}}}
\]</span> The two sample <span class="math inline">\(t\)</span>-statistic is distributed as <span class="math display">\[
t_{\text{UB}} \sim \text{$t_{n-2}$}
\]</span> i.e.&nbsp;according to a Student’s <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, with mean 0 for <span class="math inline">\(n&gt;3\)</span> and variance <span class="math inline">\((n-2)/(n-4)\)</span> for <span class="math inline">\(n&gt;4\)</span>. Large values of the two sample <span class="math inline">\(t\)</span>-statistic indicates that there are indeed two groups rather than just one.</p>
<p>The two sample <span class="math inline">\(t\)</span>-statistic using the empirical (ML) pooled estimate of the variance is <span class="math display">\[
\begin{split}
t_{\text{ML}} &amp; = \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{1}{n_1}+\frac{1}{n_2}\right)  s^2_{\text{ML}}  }   }
= \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{n}{n_1 n_2}\right) s^2_{\text{ML}}  }   }\\
&amp; =\sqrt{n} \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{1}{\hat{\pi}_1}+\frac{1}{\hat{\pi}_2}\right) s^2_{\text{ML}} }}
= \sqrt{n \hat{\pi}_1 \hat{\pi}_2 } \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ s^2_{\text{ML}}}}\\
&amp; = \sqrt{\frac{n}{n-2}} t_{\text{UB}}
\end{split}
\]</span> with <span class="math display">\[
t_{\text{ML}} \sim \text{$t_{n-2}$}\left(0, \tau^2=\frac{n}{n-2}\right)
\]</span> Thus, <span class="math inline">\(t_{\text{ML}}\)</span> follows a location-scale <span class="math inline">\(t\)</span>-distribution, with mean 0 for <span class="math inline">\(n&gt;3\)</span> and variance <span class="math inline">\(n/(n-4)\)</span> for <span class="math inline">\(n&gt;4\)</span>.</p>
</section>
</section>
<section id="sec-ci" class="level2" data-number="A.10">
<h2 data-number="A.10" class="anchored" data-anchor-id="sec-ci"><span class="header-section-number">A.10</span> Confidence intervals</h2>
<section id="general-concept" class="level3">
<h3 class="anchored" data-anchor-id="general-concept">General concept</h3>
<p>A <strong>confidence</strong> interval (CI) is an <strong>interval estimate</strong> <span class="math inline">\(\widehat{\text{CI}}(x_1, \ldots, x_n)\)</span> that depends on data and has a random (sampling) variation as well as a <strong>frequentist</strong> interpretation.</p>
<div id="fig-coverageci" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coverageci-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/coverageci.png" class="img-fluid figure-img" style="width:40.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coverageci-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;A.1: Coverage property of confidence intervals.
</figcaption>
</figure>
</div>
<p>A key property of a confidence interval is its <strong>coverage probability</strong> <span class="math display">\[
\kappa = \text{Pr}( \theta \in \widehat{\text{CI}})
\]</span> which describes how often — in repeated identical experiments — the estimated confidence interval overlaps the true parameter value <span class="math inline">\(\theta\)</span>, i.e.&nbsp;how often it will “cover” <span class="math inline">\(\theta\)</span> (see <a href="#fig-coverageci" class="quarto-xref">Figure&nbsp;<span>A.1</span></a>). In the above <span class="math inline">\(\theta\)</span> is fixed and <span class="math inline">\(\widehat{\text{CI}}\)</span> is random. Note that <span class="math inline">\(\kappa\)</span> is explicitly <strong>not</strong> the probability that the true value is contained in a specific instance of an estimated confidence interval. Specifically, any particular confidence interval either covers <span class="math inline">\(\theta\)</span> or it doesn’t.</p>
<p>For example, a coverage probability <span class="math inline">\(\kappa=0.95\)</span> (95%) implies that in 95 out of 100 repeated experiments the corresponding estimated confidence interval will contain the (unknown) true value.</p>
<p>It is trivial to create a confidence with high coverage, simply by assuming a wide interval. Therefore, a useful confidence interval must be both <strong>compact</strong> and have <strong>high coverage</strong>.</p>
<p>Finally, there is also a direct relationship between confidence intervals and statistical testing procedures. Specifically, a confidence interval can be interpreted as the set of parameter values that cannot be rejected. The complement <span class="math inline">\(\alpha=1-\kappa\)</span> is called the <strong>rejection probability</strong> or <strong>significance level</strong>.</p>
</section>
<section id="sec-normci" class="level3">
<h3 class="anchored" data-anchor-id="sec-normci">Symmetric normal confidence interval</h3>
<div id="fig-normaltwosidedci" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="h">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normaltwosidedci-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/normaltwosidedci.png" class="img-fluid figure-img" style="width:80.0%" data-fig-pos="h">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normaltwosidedci-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;A.2: Construction of a symmetric two-sided normal confidence interval.
</figcaption>
</figure>
</div>
<p>For a normally distributed univariate random variable it is straightforward to construct a symmetric two-sided confidence interval with a given desired coverage <span class="math inline">\(\kappa\)</span> (<a href="#fig-normaltwosidedci" class="quarto-xref">Figure&nbsp;<span>A.2</span></a>). The confidence interval corresponds to the central part of the density and contains probability mass <span class="math inline">\(\kappa = 1-\alpha\)</span> whereas both tails each contain mass <span class="math inline">\((1-\kappa)/2 = \alpha/2\)</span> and correspond together to the rejection region.</p>
<p>A <strong>symmetric normal confidence interval</strong> with nominal coverage <span class="math inline">\(\kappa\)</span> for</p>
<ul>
<li>a scalar parameter <span class="math inline">\(\theta\)</span></li>
<li>with normally distributed estimate <span class="math inline">\(\hat{\theta} \sim N(\theta, \sigma^2)\)</span></li>
</ul>
<p>is given by <span class="math display">\[
\widehat{\text{CI}}=[\hat{\theta} \pm c \sigma]
\]</span> where the critical value <span class="math inline">\(c\)</span> is chosen to achieve the desired coverage probability <span class="math display">\[
\kappa = \text{Pr}(\hat{\theta} - c \sigma \leq \theta  \leq  \hat{\theta} + c \sigma)
\]</span> The critical value <span class="math inline">\(c\)</span> is obtained as the <span class="math inline">\((1+\kappa)/2 = 1- \alpha/2\)</span> quantile <span class="math inline">\(z_{(1+\kappa)/2} = z_{1- \alpha/2}\)</span> of the standard normal distribution <span class="math inline">\(N(0,1)\)</span> so that <span class="math display">\[
c=\Phi^{-1}\left(\frac{1+\kappa}{2}\right) = \Phi^{-1}\left(1- \frac{\alpha}{2}\right)
\]</span> where where <span class="math inline">\(\Phi(c)\)</span> is the cumulative distribution function (CDF) of the standard normal <span class="math inline">\(N(0,1)\)</span> distribution. Its inverse <span class="math inline">\(\Phi^{-1}\)</span> is the standard normal quantile function.</p>
<div id="tbl-critnorm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-critnorm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;A.1: Critical values for the standard normal distribution.
</figcaption>
<div aria-describedby="tbl-critnorm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span></th>
<th>Critical value <span class="math inline">\(c\)</span></th>
<th>Quantile <span class="math inline">\(z_{(1+\kappa)/2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.90</td>
<td>1.6449</td>
<td><span class="math inline">\(z_{0.95}\)</span></td>
</tr>
<tr class="even">
<td>0.95</td>
<td>1.9600</td>
<td><span class="math inline">\(z_{0.975}\)</span></td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>2.5758</td>
<td><span class="math inline">\(z_{0.995}\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-critnorm" class="quarto-xref">Table&nbsp;<span>A.1</span></a> lists the critical values <span class="math inline">\(c\)</span> for the three most commonly used values of <span class="math inline">\(\kappa\)</span>. It is useful to memorise these values as they are used frequently.</p>
</section>
<section id="sec-chisqci" class="level3">
<h3 class="anchored" data-anchor-id="sec-chisqci">One-sided confidence interval based on the chi-squared distribution</h3>
<div id="fig-chisqci" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chisqci-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chisqci.png" class="img-fluid figure-img" style="width:50.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chisqci-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;A.3: Construction of a one-sided confidence interval based on a chi-squared distribution with one degree of freedom.
</figcaption>
</figure>
</div>
<p>For a chi-squared distributed statistic commonly a one-sided confidence interval of the form <span class="math inline">\([0, c ]\)</span> is used with nominal coverage probability <span class="math inline">\(\kappa = \text{Pr}(x \leq c)\)</span> (see <a href="#fig-chisqci" class="quarto-xref">Figure&nbsp;<span>A.3</span></a>). The right tail contains <span class="math inline">\(1-\kappa = \alpha\)</span> probability mass.</p>
<p>We obtain the critical value <span class="math inline">\(c\)</span> as the <span class="math inline">\(\kappa=1-\alpha\)</span> quantile of the chi-squared distribution by using the quantile function, i.e. by inverting the CDF of the chi-squared distribution.</p>
<div id="tbl-critchisq" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-critchisq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;A.2: Critical values for the chi-squared distribution with one degree of freedom.
</figcaption>
<div aria-describedby="tbl-critchisq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span></th>
<th>Critical value <span class="math inline">\(c\)</span></th>
<th>Quantile <span class="math inline">\(x_{\kappa}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.90</td>
<td>2.7055</td>
<td><span class="math inline">\(x_{.90}\)</span></td>
</tr>
<tr class="even">
<td>0.95</td>
<td>3.8415</td>
<td><span class="math inline">\(x_{.95}\)</span></td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>6.6349</td>
<td><span class="math inline">\(x_{.99}\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-critchisq" class="quarto-xref">Table&nbsp;<span>A.2</span></a> lists the critical values for the three most common choices of the coverage probability <span class="math inline">\(\kappa\)</span> for a chi-squared distribution with one degree of freedom. Note that these critical values are the squared values of the corresponding thresholds in <a href="#tbl-critnorm" class="quarto-xref">Table&nbsp;<span>A.1</span></a> (with small discrepancies due to rounding).</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH27720-stats2");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./bibliography.html" class="pagination-link" aria-label="Bibliography">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Bibliography</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./21-further-study.html" class="pagination-link" aria-label="Further study">
        <span class="nav-page-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Further study</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>