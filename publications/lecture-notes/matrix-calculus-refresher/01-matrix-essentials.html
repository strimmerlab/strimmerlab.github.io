<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Matrix essentials â€“ Matrix and Calculus Refresher</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02-vector-calculus.html" rel="next">
<link href="./00-preface.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3c04d35918bfbae480bb424d60ad250e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-matrix-essentials.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Matrix essentials</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Matrix and Calculus Refresher</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-matrix-essentials.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Matrix essentials</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-vector-calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Vector and matrix calculus</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">1.1</span> Overview</a></li>
  <li><a href="#matrix-basics" id="toc-matrix-basics" class="nav-link" data-scroll-target="#matrix-basics"><span class="header-section-number">1.2</span> Matrix basics</a>
  <ul class="collapse">
  <li><a href="#matrix-notation" id="toc-matrix-notation" class="nav-link" data-scroll-target="#matrix-notation">Matrix notation</a></li>
  <li><a href="#notation-for-random-vectors-and-matrices" id="toc-notation-for-random-vectors-and-matrices" class="nav-link" data-scroll-target="#notation-for-random-vectors-and-matrices">Notation for random vectors and matrices</a></li>
  <li><a href="#special-matrices" id="toc-special-matrices" class="nav-link" data-scroll-target="#special-matrices">Special matrices</a></li>
  </ul></li>
  <li><a href="#simple-matrix-operations" id="toc-simple-matrix-operations" class="nav-link" data-scroll-target="#simple-matrix-operations"><span class="header-section-number">1.3</span> Simple matrix operations</a>
  <ul class="collapse">
  <li><a href="#matrix-addition-and-multiplication" id="toc-matrix-addition-and-multiplication" class="nav-link" data-scroll-target="#matrix-addition-and-multiplication">Matrix addition and multiplication</a></li>
  <li><a href="#matrix-transpose" id="toc-matrix-transpose" class="nav-link" data-scroll-target="#matrix-transpose">Matrix transpose</a></li>
  </ul></li>
  <li><a href="#matrix-summaries" id="toc-matrix-summaries" class="nav-link" data-scroll-target="#matrix-summaries"><span class="header-section-number">1.4</span> Matrix summaries</a>
  <ul class="collapse">
  <li><a href="#row-column-and-grand-sum" id="toc-row-column-and-grand-sum" class="nav-link" data-scroll-target="#row-column-and-grand-sum">Row, column and grand sum</a></li>
  <li><a href="#matrix-trace" id="toc-matrix-trace" class="nav-link" data-scroll-target="#matrix-trace">Matrix trace</a></li>
  <li><a href="#row-column-and-grand-sum-of-squares" id="toc-row-column-and-grand-sum-of-squares" class="nav-link" data-scroll-target="#row-column-and-grand-sum-of-squares">Row, column and grand sum of squares</a></li>
  <li><a href="#sum-of-squared-diagonal-entries" id="toc-sum-of-squared-diagonal-entries" class="nav-link" data-scroll-target="#sum-of-squared-diagonal-entries">Sum of squared diagonal entries</a></li>
  <li><a href="#frobenius-inner-product" id="toc-frobenius-inner-product" class="nav-link" data-scroll-target="#frobenius-inner-product">Frobenius inner product</a></li>
  <li><a href="#euclidean-norm" id="toc-euclidean-norm" class="nav-link" data-scroll-target="#euclidean-norm">Euclidean norm</a></li>
  <li><a href="#determinant-of-a-matrix" id="toc-determinant-of-a-matrix" class="nav-link" data-scroll-target="#determinant-of-a-matrix">Determinant of a matrix</a></li>
  </ul></li>
  <li><a href="#matrix-inverse" id="toc-matrix-inverse" class="nav-link" data-scroll-target="#matrix-inverse"><span class="header-section-number">1.5</span> Matrix inverse</a>
  <ul class="collapse">
  <li><a href="#inversion-of-square-matrix" id="toc-inversion-of-square-matrix" class="nav-link" data-scroll-target="#inversion-of-square-matrix">Inversion of square matrix</a></li>
  <li><a href="#inversion-of-structured-matrices" id="toc-inversion-of-structured-matrices" class="nav-link" data-scroll-target="#inversion-of-structured-matrices">Inversion of structured matrices</a></li>
  </ul></li>
  <li><a href="#orthogonal-matrices" id="toc-orthogonal-matrices" class="nav-link" data-scroll-target="#orthogonal-matrices"><span class="header-section-number">1.6</span> Orthogonal matrices</a>
  <ul class="collapse">
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties">Properties</a></li>
  <li><a href="#semi-orthogonal-matrices" id="toc-semi-orthogonal-matrices" class="nav-link" data-scroll-target="#semi-orthogonal-matrices">Semi-orthogonal matrices</a></li>
  <li><a href="#generating-orthogonal-matrices" id="toc-generating-orthogonal-matrices" class="nav-link" data-scroll-target="#generating-orthogonal-matrices">Generating orthogonal matrices</a></li>
  <li><a href="#permutation-matrix" id="toc-permutation-matrix" class="nav-link" data-scroll-target="#permutation-matrix">Permutation matrix</a></li>
  </ul></li>
  <li><a href="#eigenvalues-and-eigenvectors" id="toc-eigenvalues-and-eigenvectors" class="nav-link" data-scroll-target="#eigenvalues-and-eigenvectors"><span class="header-section-number">1.7</span> Eigenvalues and eigenvectors</a>
  <ul class="collapse">
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition">Definition</a></li>
  <li><a href="#finding-eigenvalues-and-vectors" id="toc-finding-eigenvalues-and-vectors" class="nav-link" data-scroll-target="#finding-eigenvalues-and-vectors">Finding eigenvalues and vectors</a></li>
  <li><a href="#eigenequation-in-matrix-notation" id="toc-eigenequation-in-matrix-notation" class="nav-link" data-scroll-target="#eigenequation-in-matrix-notation">Eigenequation in matrix notation</a></li>
  <li><a href="#permutation-of-eigenvalues" id="toc-permutation-of-eigenvalues" class="nav-link" data-scroll-target="#permutation-of-eigenvalues">Permutation of eigenvalues</a></li>
  <li><a href="#similar-matrices" id="toc-similar-matrices" class="nav-link" data-scroll-target="#similar-matrices">Similar matrices</a></li>
  <li><a href="#defective-matrix" id="toc-defective-matrix" class="nav-link" data-scroll-target="#defective-matrix">Defective matrix</a></li>
  <li><a href="#eigenvalues-of-a-diagonal-or-triangular-matrix" id="toc-eigenvalues-of-a-diagonal-or-triangular-matrix" class="nav-link" data-scroll-target="#eigenvalues-of-a-diagonal-or-triangular-matrix">Eigenvalues of a diagonal or triangular matrix</a></li>
  <li><a href="#eigenvalues-and-vectors-of-a-symmetric-matrix" id="toc-eigenvalues-and-vectors-of-a-symmetric-matrix" class="nav-link" data-scroll-target="#eigenvalues-and-vectors-of-a-symmetric-matrix">Eigenvalues and vectors of a symmetric matrix</a></li>
  <li><a href="#eigenvalues-of-orthogonal-matrices" id="toc-eigenvalues-of-orthogonal-matrices" class="nav-link" data-scroll-target="#eigenvalues-of-orthogonal-matrices">Eigenvalues of orthogonal matrices</a></li>
  <li><a href="#positive-definite-matrices" id="toc-positive-definite-matrices" class="nav-link" data-scroll-target="#positive-definite-matrices">Positive definite matrices</a></li>
  </ul></li>
  <li><a href="#matrix-decompositions" id="toc-matrix-decompositions" class="nav-link" data-scroll-target="#matrix-decompositions"><span class="header-section-number">1.8</span> Matrix decompositions</a>
  <ul class="collapse">
  <li><a href="#diagonalisation-and-eigenvalue-decomposition" id="toc-diagonalisation-and-eigenvalue-decomposition" class="nav-link" data-scroll-target="#diagonalisation-and-eigenvalue-decomposition">Diagonalisation and eigenvalue decomposition</a></li>
  <li><a href="#orthogonal-eigenvalue-decomposition" id="toc-orthogonal-eigenvalue-decomposition" class="nav-link" data-scroll-target="#orthogonal-eigenvalue-decomposition">Orthogonal eigenvalue decomposition</a></li>
  <li><a href="#singular-value-decomposition" id="toc-singular-value-decomposition" class="nav-link" data-scroll-target="#singular-value-decomposition">Singular value decomposition</a></li>
  <li><a href="#polar-decomposition" id="toc-polar-decomposition" class="nav-link" data-scroll-target="#polar-decomposition">Polar decomposition</a></li>
  <li><a href="#cholesky-decomposition" id="toc-cholesky-decomposition" class="nav-link" data-scroll-target="#cholesky-decomposition">Cholesky decomposition</a></li>
  </ul></li>
  <li><a href="#matrix-summaries-based-on-eigenvalues-and-singular-values" id="toc-matrix-summaries-based-on-eigenvalues-and-singular-values" class="nav-link" data-scroll-target="#matrix-summaries-based-on-eigenvalues-and-singular-values"><span class="header-section-number">1.9</span> Matrix summaries based on eigenvalues and singular values</a>
  <ul class="collapse">
  <li><a href="#trace-and-determinant-computed-from-eigenvalues" id="toc-trace-and-determinant-computed-from-eigenvalues" class="nav-link" data-scroll-target="#trace-and-determinant-computed-from-eigenvalues">Trace and determinant computed from eigenvalues</a></li>
  <li><a href="#eigenvalues-of-a-squared-matrix" id="toc-eigenvalues-of-a-squared-matrix" class="nav-link" data-scroll-target="#eigenvalues-of-a-squared-matrix">Eigenvalues of a squared matrix</a></li>
  <li><a href="#rank-and-condition-number" id="toc-rank-and-condition-number" class="nav-link" data-scroll-target="#rank-and-condition-number">Rank and condition number</a></li>
  </ul></li>
  <li><a href="#functions-of-symmetric-matrices" id="toc-functions-of-symmetric-matrices" class="nav-link" data-scroll-target="#functions-of-symmetric-matrices"><span class="header-section-number">1.10</span> Functions of symmetric matrices</a>
  <ul class="collapse">
  <li><a href="#definition-of-a-matrix-function" id="toc-definition-of-a-matrix-function" class="nav-link" data-scroll-target="#definition-of-a-matrix-function">Definition of a matrix function</a></li>
  <li><a href="#identities-for-the-matrix-exponential-and-logarithm" id="toc-identities-for-the-matrix-exponential-and-logarithm" class="nav-link" data-scroll-target="#identities-for-the-matrix-exponential-and-logarithm">Identities for the matrix exponential and logarithm</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Matrix essentials</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="overview" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">1.1</span> Overview</h2>
<p>In statistics we will frequently make use of matrix calculations and matrix notation.</p>
<p>Throughout we mostly work with <strong>real matrices</strong>, i.e.&nbsp;we assume all matrix elements are real numbers. However, one important matrix decomposition â€” the eigenvalue decomposition â€” can yield complex-valued matrices even when applied to real matrices. Thus occasionally we will also need to deal also with complex numbers.</p>
<p>For further details on matrix theory please consult the lecture notes of related modules (e.g.&nbsp;linear algebra).</p>
</section>
<section id="matrix-basics" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="matrix-basics"><span class="header-section-number">1.2</span> Matrix basics</h2>
<section id="matrix-notation" class="level3">
<h3 class="anchored" data-anchor-id="matrix-notation">Matrix notation</h3>
<p>In matrix notation we distinguish between scalars, vectors, and matrices:</p>
<p><strong>Scalar</strong>: <span class="math inline">\(x\)</span>, <span class="math inline">\(X\)</span>, lower or upper case, plain type.</p>
<p><strong>Vector</strong>: <span class="math inline">\(\boldsymbol x\)</span>, lower case, bold type. In handwriting an arrow <span class="math inline">\(\vec{x}\)</span> indicates a vector.</p>
<p>In component notation we write <span class="math inline">\(\boldsymbol x= \begin{pmatrix} x_1 \\ \vdots\\ x_d\end{pmatrix}\)</span>. By default, a vector is a column vector, i.e.&nbsp;the elements are arranged in a column and index of the components <span class="math inline">\(x_i\)</span> refers to the row.</p>
<p>The <strong>transpose</strong> of a vector (indicated by the superscript <span class="math inline">\(T\)</span>) turns it into a row vector. To save space we can write the column vector <span class="math inline">\(\boldsymbol x\)</span> as <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_d)^T\)</span> so that <span class="math inline">\(\boldsymbol x^T\)</span> is a row vector.</p>
<p><strong>Matrix</strong>: <span class="math inline">\(\boldsymbol X\)</span>, upper case, bold type. In handwriting an underscore <span class="math inline">\(\underline{X}\)</span> indicates a matrix.</p>
<p>In component notation we write <span class="math inline">\(\boldsymbol X= (x_{ij})\)</span>. By convention, the first index (here <span class="math inline">\(i\)</span>) of the scalar elements <span class="math inline">\(x_{ij}\)</span> denotes the row and the second index (here <span class="math inline">\(j\)</span>) the column of the matrix. For <span class="math inline">\(n\)</span> the number of rows and <span class="math inline">\(d\)</span> the number of columns we can view the matrix <span class="math inline">\(\boldsymbol X= (\boldsymbol x_1, \ldots, \boldsymbol x_d)\)</span> either as being composed of <span class="math inline">\(d\)</span> column vectors <span class="math inline">\(\boldsymbol x_j = \begin{pmatrix} x_{1j} \\ \vdots \\ x_{nj}\\ \end{pmatrix}\)</span> or <span class="math inline">\(\boldsymbol X= \begin{pmatrix} \boldsymbol z_1^T \\ \vdots \\ \boldsymbol z_n^T\end{pmatrix}\)</span> being composed of <span class="math inline">\(n\)</span> row vectors <span class="math inline">\(\boldsymbol z_i^T = (x_{i1},  \ldots,  x_{id})\)</span>.</p>
<p>A (column) vector of dimension <span class="math inline">\(d\)</span> is a matrix of size <span class="math inline">\(d\times 1\)</span>. A row vector of dimension <span class="math inline">\(d\)</span> is a matrix of size <span class="math inline">\(1\times d\)</span>. A scalar is of dimension <span class="math inline">\(1\)</span> and is a matrix of size <span class="math inline">\(1 \times 1\)</span>.</p>
</section>
<section id="notation-for-random-vectors-and-matrices" class="level3">
<h3 class="anchored" data-anchor-id="notation-for-random-vectors-and-matrices">Notation for random vectors and matrices</h3>
<p>A <strong>random matrix</strong> (vector) is a matrix (vector) whose elements are random variables.</p>
<p>It is common practise in univariate statistics to distinguish random variables and their fixed realisations by using upper case versus lower case. However, this convention breaks down when working with matrices and vectors.</p>
<p>Therefore, when working with multivariate random quantities it is best practise (see e.g.&nbsp;Mardia et al.&nbsp;1979)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> to always state explicitly whether a matrix, vector or scalar is a random variable, especially when this is not obvious from the context.</p>
</section>
<section id="special-matrices" class="level3">
<h3 class="anchored" data-anchor-id="special-matrices">Special matrices</h3>
<p><span class="math inline">\(\boldsymbol I_d\)</span> is the <strong>identity matrix</strong>. It is a square matrix of size <span class="math inline">\(d \times d\)</span> with the diagonal filled with 1 and off-diagonals filled with 0. <span class="math display">\[\boldsymbol I_d =
\begin{pmatrix}
    1 &amp; 0 &amp; 0 &amp; \dots &amp; 0\\
    0 &amp; 1 &amp; 0 &amp; \dots &amp; 0\\
    0 &amp; 0 &amp; 1 &amp;   &amp; 0\\
    \vdots &amp; \vdots &amp; &amp; \ddots &amp;  \\
    0 &amp; 0 &amp; 0 &amp;  &amp; 1 \\
\end{pmatrix}\]</span></p>
<p><span class="math inline">\(\mathbf 1\)</span> is a matrix that contains only ones. Most often it is used in the form of a column vector with <span class="math inline">\(d\)</span> rows: <span class="math display">\[\mathbf 1_d =
\begin{pmatrix}
    1 \\
    1 \\
    1 \\
    \vdots   \\
    1  \\
\end{pmatrix}\]</span></p>
<p>Similarly, <span class="math inline">\(\mathbf 0\)</span> is a matrix that contains only zeros. Most often it is used in the form of a column vector with <span class="math inline">\(d\)</span> rows: <span class="math display">\[\mathbf 0_d =
\begin{pmatrix}
    0 \\
    0 \\
    0 \\
    \vdots   \\
    0  \\
\end{pmatrix}\]</span></p>
<p>A <strong>diagonal matrix</strong> is a matrix where all off-diagonal elements are zero. By <span class="math inline">\(\text{Diag}(\boldsymbol A)\)</span> we access the diagonal elements of a matrix as vector and by <span class="math inline">\(\text{Diag}(a_1, \ldots, a_d)\)</span> we specify a diagonal matrix by listing the diagonal elements.</p>
<p>Any matrix can be partitioned into blocks or sub-matrices. A <strong>block-structured matrix</strong> or <strong>block matrix</strong> partitioning rows and columns into two groups has the form <span class="math display">\[
\boldsymbol A= \begin{pmatrix} \boldsymbol A_{11} &amp; \boldsymbol A_{12} \\ \boldsymbol A_{21} &amp; \boldsymbol A_{22} \\ \end{pmatrix} \, ,
\]</span> where <span class="math inline">\(\boldsymbol A_{11}\)</span>, <span class="math inline">\(\boldsymbol A_{22}\)</span>, <span class="math inline">\(\boldsymbol A_{12}\)</span> and <span class="math inline">\(\boldsymbol A_{21}\)</span> are themselves matrices. If <span class="math inline">\(\boldsymbol A\)</span> is symmetric (hence square) then <span class="math inline">\(\boldsymbol A_{11}\)</span> and <span class="math inline">\(\boldsymbol A_{22}\)</span> must also be symmetric and <span class="math inline">\(\boldsymbol A_{21} = \boldsymbol A_{12}^T\)</span>.</p>
<p>A <strong>block diagonal</strong> matrix is a symmetric block matrix with vanishing off-diagonal blocks and symmetric sub-matrices along the diagonal.</p>
<p>A <strong>triangular matrix</strong> is a square matrix whose elements either below or above the diagonal are all zero (upper vs.&nbsp;lower triangular matrix).</p>
</section>
</section>
<section id="simple-matrix-operations" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="simple-matrix-operations"><span class="header-section-number">1.3</span> Simple matrix operations</h2>
<section id="matrix-addition-and-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="matrix-addition-and-multiplication">Matrix addition and multiplication</h3>
<p>Matrices behave much like common numbers. For example, we can add matrices <span class="math inline">\(\boldsymbol C= \boldsymbol A+ \boldsymbol B\)</span> and multiply matrices <span class="math inline">\(\boldsymbol C= \boldsymbol A\boldsymbol B\)</span>.</p>
<p>For <strong>matrix addition</strong> <span class="math inline">\(\boldsymbol C= \boldsymbol A+ \boldsymbol B\)</span> we add the corresponding elements <span class="math inline">\(c_{ij} = a_{ij} + b_{ij}\)</span>. For matrix addition <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> must have the same dimensions, i.e. the same number of rows and columns.</p>
<p>The <strong>dot product</strong>, or <strong>scalar product</strong>, of two vectors <span class="math inline">\(\boldsymbol a\)</span> and <span class="math inline">\(\boldsymbol b\)</span> is a scalar given by <span class="math inline">\(\boldsymbol a\cdot \boldsymbol b= \langle \boldsymbol a, \boldsymbol b\rangle  = \boldsymbol a^T \boldsymbol b= \boldsymbol b^T \boldsymbol a= \sum_{i=1}^d a_{i} b_{i}\)</span>.</p>
<p><strong>Matrix multiplication</strong> <span class="math inline">\(\boldsymbol C= \boldsymbol A\boldsymbol B\)</span> is obtained by setting <span class="math inline">\(c_{ij} = \sum_{k=1}^m a_{ik} b_{kj}\)</span> where <span class="math inline">\(m\)</span> is the number of columns of <span class="math inline">\(\boldsymbol A\)</span> and the number of rows in <span class="math inline">\(\boldsymbol B\)</span>. Thus, <span class="math inline">\(\boldsymbol C\)</span> contains all possible dot products of the row vectors in <span class="math inline">\(\boldsymbol A\)</span> with the column vectors in <span class="math inline">\(\boldsymbol B\)</span>. For matrix multiplication the number of columns in <span class="math inline">\(\boldsymbol A\)</span> must match the number of rows in <span class="math inline">\(\boldsymbol B\)</span>. Note that matrix multiplication in general (for <span class="math inline">\(m &gt; 1\)</span>) does not commute, i.e.&nbsp;<span class="math inline">\(\boldsymbol A\boldsymbol B\neq \boldsymbol B\boldsymbol A\)</span>.</p>
</section>
<section id="matrix-transpose" class="level3">
<h3 class="anchored" data-anchor-id="matrix-transpose">Matrix transpose</h3>
<p>The matrix transpose <span class="math inline">\(\boldsymbol A^T\)</span> indicate by the superscript <span class="math inline">\(T\)</span> interchanges rows and columns of a matrix. The transpose is a linear operator <span class="math inline">\((\boldsymbol A+ \boldsymbol B)^T = \boldsymbol A^T + \boldsymbol B^T\)</span> and applied to a matrix product it reverses the ordering, i.e.&nbsp;<span class="math inline">\((\boldsymbol A\boldsymbol B)^T =\boldsymbol B^T \boldsymbol A^T\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol A= \boldsymbol A^T\)</span> then <span class="math inline">\(\boldsymbol A\)</span> is symmetric (and square).</p>
<p>By construction given a rectangular <span class="math inline">\(\boldsymbol A\)</span> the matrices <span class="math inline">\(\boldsymbol A^T \boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T\)</span> are symmetric with non-negative diagonal.</p>
</section>
</section>
<section id="matrix-summaries" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="matrix-summaries"><span class="header-section-number">1.4</span> Matrix summaries</h2>
<section id="row-column-and-grand-sum" class="level3">
<h3 class="anchored" data-anchor-id="row-column-and-grand-sum">Row, column and grand sum</h3>
<p>Assume a matrix <span class="math inline">\(\boldsymbol A\)</span> of size <span class="math inline">\(n \times m\)</span>.</p>
<p>The sum over the <span class="math inline">\(m\)</span> entries of row <span class="math inline">\(i\)</span> is <span class="math inline">\(\sum_{j=1}^m a_{ij}\)</span>. In matrix notation the <span class="math inline">\(n\)</span> row sums are given by <span class="math inline">\(\boldsymbol A\, \mathbf 1_m\)</span>.</p>
<p>The sum over the <span class="math inline">\(n\)</span> entries of column <span class="math inline">\(j\)</span> is <span class="math inline">\(\sum_{i=1}^n a_{ij}\)</span>. In matrix notation the <span class="math inline">\(m\)</span> column sums are <span class="math inline">\(\boldsymbol A^T \mathbf 1_n\)</span>.</p>
<p>The grand sum of all matrix entries of <span class="math inline">\(\boldsymbol A\)</span> is obtained by <span class="math display">\[
\sum_{i=1}^n \sum_{j=1}^m a_{ij} = \mathbf 1_n^T \, \boldsymbol A\, \mathbf 1_m
\]</span></p>
</section>
<section id="matrix-trace" class="level3">
<h3 class="anchored" data-anchor-id="matrix-trace">Matrix trace</h3>
<p>The trace of the matrix is the sum of the diagonal elements <span class="math inline">\(\text{Tr}(\boldsymbol A) = \sum a_{ii}\)</span>.</p>
<p>The trace is invariant against transposition, i.e. <span class="math display">\[
\text{Tr}(\boldsymbol A) = \text{Tr}(\boldsymbol A^T )
\]</span></p>
<p>A useful identity for the matrix trace of the product of two matrices is <span class="math display">\[
\text{Tr}(\boldsymbol A\boldsymbol B) = \text{Tr}( \boldsymbol B\boldsymbol A)  
\]</span></p>
<p>Intriguingly, the trace of a matrix equals the sum of the eigenvalues of the matrix (see further below).</p>
</section>
<section id="row-column-and-grand-sum-of-squares" class="level3">
<h3 class="anchored" data-anchor-id="row-column-and-grand-sum-of-squares">Row, column and grand sum of squares</h3>
<p>The sum over the <span class="math inline">\(m\)</span> squared entries of row <span class="math inline">\(i\)</span> is <span class="math inline">\(\sum_{j=1}^m a_{ij}^2\)</span>. In matrix notation the <span class="math inline">\(n\)</span> row sums of squares are given by <span class="math inline">\(\text{Diag}( \boldsymbol A\boldsymbol A^T)\)</span>.</p>
<p>The sum over the <span class="math inline">\(n\)</span> squared entries of column <span class="math inline">\(j\)</span> is <span class="math inline">\(\sum_{i=1}^n a_{ij}^2\)</span>. In matrix notation the <span class="math inline">\(m\)</span> column sums of squares are <span class="math inline">\(\text{Diag}( \boldsymbol A^T \boldsymbol A)\)</span>.</p>
<p>The grand sum of all squared elements of <span class="math inline">\(\boldsymbol A\)</span> is obtained by <span class="math display">\[
\sum_{i=1}^n \sum_{j=1}^m a_{ij}^2 = \text{Tr}(\boldsymbol A^T \boldsymbol A) = \text{Tr}(\boldsymbol A\boldsymbol A^T)
\]</span> This is also known as the squared Frobenius norm of <span class="math inline">\(\boldsymbol A\)</span> (see below).</p>
</section>
<section id="sum-of-squared-diagonal-entries" class="level3">
<h3 class="anchored" data-anchor-id="sum-of-squared-diagonal-entries">Sum of squared diagonal entries</h3>
<p>The sum of the squared entries on the diagonal is in matrix notation <span class="math display">\[
\text{Diag}(\boldsymbol A)^T \text{Diag}(\boldsymbol A) = \sum_{i=1}^{\min(n,m)} a_{ii}^2
\]</span></p>
</section>
<section id="frobenius-inner-product" class="level3">
<h3 class="anchored" data-anchor-id="frobenius-inner-product">Frobenius inner product</h3>
<p>The <strong>Frobenius inner product</strong> between two rectangular matrices of the same dimension is the scalar <span class="math display">\[
\begin{split}
\langle \boldsymbol A, \boldsymbol B\rangle &amp;=  \text{Tr}(\boldsymbol A\boldsymbol B^T) = \text{Tr}(\boldsymbol B\boldsymbol A^T)\\
&amp;=  \text{Tr}(\boldsymbol A^T \boldsymbol B) = \text{Tr}(\boldsymbol B^T \boldsymbol A)\\
&amp;= \sum_{i,j} a_{ij} b_{ij} \,.
\end{split}
\]</span> This generalises the dot product between two vectors. Note that the dot product can therefore also be written as the trace of a matrix <span class="math display">\[
\langle \boldsymbol a, \boldsymbol b\rangle = \text{Tr}( \boldsymbol a\boldsymbol b^T ) = \text{Tr}( \boldsymbol b\boldsymbol a^T) \,.
\]</span></p>
</section>
<section id="euclidean-norm" class="level3">
<h3 class="anchored" data-anchor-id="euclidean-norm">Euclidean norm</h3>
<p>The <strong>squared Euclidean norm</strong> or the <strong>squared length</strong> of the vector <span class="math inline">\(\boldsymbol a\)</span> is the dot product <span class="math inline">\(||\boldsymbol a||^2_2 = \boldsymbol a\cdot \boldsymbol a=  \langle \boldsymbol a, \boldsymbol a\rangle =   \boldsymbol a^T \boldsymbol a= \boldsymbol a\boldsymbol a^T = \sum_{i=1}^d a_i^2\)</span>.</p>
<p>The <strong>squared Frobenius norm</strong> is a generalisation of the squared Euclidean vector norm to a rectangular matrix and is the sum of the squares of all its elements. Using the trace it can be written as <span class="math display">\[
\begin{split}
||\boldsymbol A||_F^2 &amp;= \langle \boldsymbol A, \boldsymbol A\rangle \\
&amp;= \text{Tr}(\boldsymbol A^T \boldsymbol A) = \text{Tr}(\boldsymbol A\boldsymbol A^T) \\
&amp;= \sum_{i,j} a_{ij}^2 \,.
\end{split}
\]</span></p>
<p>A useful identity for the <strong>squared Frobenius norm of the difference of two matrices</strong> is <span class="math display">\[
\begin{split}
||\boldsymbol A- \boldsymbol B||_F^2 &amp;= ||\boldsymbol A||_F^2 + ||\boldsymbol B||_F^2  - 2 \langle \boldsymbol A, \boldsymbol B\rangle \\
&amp;= \text{Tr}(\boldsymbol A^T \boldsymbol A) +\text{Tr}(\boldsymbol B^T \boldsymbol B) - 2 \text{Tr}(\boldsymbol A^T \boldsymbol B) \\
&amp;= \sum_{i,j} (a_{ij}-b_{ij})^2 \,.
\end{split}
\]</span></p>
<p>The Frobenius norm of a matrix <span class="math inline">\(||\boldsymbol A||_F\)</span> is not to be confused with the induced <span class="math inline">\(2\)</span>-norm of a matrix <span class="math inline">\(||\boldsymbol A||_2\)</span>. The latter equals the maximum absolute eigenvalue of the matrix, with <span class="math inline">\(||\boldsymbol A||_2 \leq ||\boldsymbol A||_F\)</span>.</p>
</section>
<section id="determinant-of-a-matrix" class="level3">
<h3 class="anchored" data-anchor-id="determinant-of-a-matrix">Determinant of a matrix</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is a square matrix the determinant <span class="math inline">\(\det(\boldsymbol A)\)</span> is a scalar measuring the volume spanned by the column vectors in <span class="math inline">\(\boldsymbol A\)</span> with the sign determined by the orientation of the vectors.</p>
<p>If <span class="math inline">\(\det(\boldsymbol A) \neq 0\)</span> the matrix <span class="math inline">\(\boldsymbol A\)</span> is non-singular or non-degenerate. Conversely, if <span class="math inline">\(\det(\boldsymbol A) =0\)</span> the matrix <span class="math inline">\(\boldsymbol A\)</span> is singular or degenerate.</p>
<p>Intriguingly, the determinant of <span class="math inline">\(\boldsymbol A\)</span> is the product of the eigenvalues of <span class="math inline">\(\boldsymbol A\)</span> (see further below).</p>
<p>One way to compute the determinant of a matrix <span class="math inline">\(\boldsymbol A\)</span> is the Laplace cofactor expansion approach that proceeds recursively based on the determinants of the sub-matrices <span class="math inline">\(\boldsymbol A_{-i,-j}\)</span> obtained by deleting row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> from <span class="math inline">\(\boldsymbol A\)</span>. Specifically, at each level we compute the</p>
<ol type="1">
<li>cofactor expansion either
<ol type="a">
<li>along the <span class="math inline">\(i\)</span>-th row â€” pick any row <span class="math inline">\(i\)</span>: <span class="math display">\[\det(\boldsymbol A) = \sum_{j=1}^d a_{ij} (-1)^{i+j} \det(\boldsymbol A_{-i,-j})  \text{ , or}\]</span></li>
<li>along the <span class="math inline">\(j\)</span>-th column â€” pick any <span class="math inline">\(j\)</span>: <span class="math display">\[\det(\boldsymbol A) = \sum_{i=1}^d a_{ij} (-1)^{i+j} \det(\boldsymbol A_{-i,-j})\]</span>.</li>
</ol></li>
<li>Then repeat until the sub-matrix is a scalar <span class="math inline">\(a\)</span> and <span class="math inline">\(\det(a)=a \,.\)</span></li>
</ol>
<p>The recursive nature of this algorithm leads to a complexity of order <span class="math inline">\(O(d!)\)</span> so it is not practical except for very small <span class="math inline">\(d\)</span>. Therefore, in practice other more efficient algorithms for computing determinants are used but these still have algorithmic complexity in the order of <span class="math inline">\(O(d^3)\)</span> so for large dimensions obtaining determinants is very expensive.</p>
<p>However, some specially structured matrices do allow for very fast calculation.</p>
<p>The determinant of a <strong>triangular matrix</strong> (and thus also of a <strong>diagonal matrix</strong>) <span class="math display">\[
\boldsymbol A= \begin{pmatrix}
a_{11} &amp; 0       &amp; \cdots &amp; 0\\
a_{21} &amp; a_{22}  &amp; \cdots &amp; 0\\
\vdots  &amp; \vdots &amp; \ddots &amp; 0 \\
a_{d1} &amp; a_{d2} &amp; \cdots &amp; a_{dd} \\
\end{pmatrix}
\]</span> is the product of its diagonal elements, i.e.&nbsp;<span class="math inline">\(\det(\boldsymbol A) = \prod_{i=1}^d a_{ii}\)</span>.</p>
<p>For a two-dimensional matrix <span class="math inline">\(\boldsymbol A= \begin{pmatrix} a_{11} &amp; a_{12} \\  a_{21} &amp; a_{22} \\\end{pmatrix}\)</span> the determinant is <span class="math inline">\(\det(A) = a_{11} a_{22} - a_{12} a_{21}\)</span>.</p>
<p>For a block-structured square matrix <span class="math display">\[
\boldsymbol A= \begin{pmatrix} \boldsymbol A_{11} &amp; \boldsymbol A_{12} \\ \boldsymbol A_{21} &amp; \boldsymbol A_{22} \\ \end{pmatrix} \, ,
\]</span> where the matrices on the diagonal <span class="math inline">\(\boldsymbol A_{11}\)</span> and <span class="math inline">\(\boldsymbol A_{22}\)</span> are themselves square but <span class="math inline">\(\boldsymbol A_{12}\)</span> and <span class="math inline">\(\boldsymbol A_{21}\)</span> may be rectangular, the determinant is <span class="math display">\[
\det(\boldsymbol A) = \det(\boldsymbol A_{22}) \det(\boldsymbol C_1) = \det(\boldsymbol A_{11}) \det(\boldsymbol C_2)
\]</span> with the (Schur complement of <span class="math inline">\(\boldsymbol A_{22}\)</span>) <span class="math display">\[
\boldsymbol C_1 = \boldsymbol A_{11} -  \boldsymbol A_{12}  \boldsymbol A_{22}^{-1}  \boldsymbol A_{21}
\]</span> and (Schur complement of <span class="math inline">\(\boldsymbol A_{11}\)</span>) <span class="math display">\[
\boldsymbol C_2 = \boldsymbol A_{22} -  \boldsymbol A_{21}  \boldsymbol A_{11}^{-1}  \boldsymbol A_{12}
\]</span> Note that <span class="math inline">\(\boldsymbol C_1\)</span> and <span class="math inline">\(\boldsymbol C_2\)</span> are square matrices.</p>
<p>For a block-diagonal matrix <span class="math inline">\(\boldsymbol A\)</span> with <span class="math inline">\(\boldsymbol A_{12} = 0\)</span> and <span class="math inline">\(\boldsymbol A_{21} = 0\)</span> the determinant is <span class="math inline">\(\det(\boldsymbol A) =  \det(\boldsymbol A_{11})  \det(\boldsymbol A_{22})\)</span>, i.e.&nbsp;the product of the determinants of the sub-matrices along the diagonal.</p>
<p>Determinants have a multiplicative property, <span class="math display">\[\det(\boldsymbol A\boldsymbol B) = \det(\boldsymbol B\boldsymbol A) = \det(\boldsymbol A) \det(\boldsymbol B) \,.\]</span> In the above <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> are both square and of the same dimension.</p>
<p>For rectangular <span class="math inline">\(\boldsymbol A\)</span> (<span class="math inline">\(n \times m\)</span>) and rectangular <span class="math inline">\(\boldsymbol B\)</span> (<span class="math inline">\(m \times n\)</span>) with <span class="math inline">\(m \geq n\)</span> this generalises to the Cauchy-Binet formula <span class="math display">\[
\det(\boldsymbol A\boldsymbol B) = \sum_{w} \det(\boldsymbol A_{,w}) \det(\boldsymbol B_{w,})
\]</span> where the summation is over all <span class="math inline">\(\binom{m}{n}\)</span> index subsets <span class="math inline">\(w\)</span> of size <span class="math inline">\(n\)</span> taken from <span class="math inline">\(\{1, \ldots, m\}\)</span> keeping the ordering and <span class="math inline">\(\boldsymbol A_{,w}\)</span> and <span class="math inline">\(\boldsymbol B_{w,}\)</span> are the corresponding square <span class="math inline">\(n \times n\)</span> sub-matrices. If <span class="math inline">\(m &lt; n\)</span> then <span class="math inline">\(\det(\boldsymbol A\boldsymbol B) = 0\)</span>.</p>
<p>For scalar <span class="math inline">\(a\)</span> <span class="math inline">\(\det(a \boldsymbol B) = a^d \det(\boldsymbol B)\)</span> where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(\boldsymbol B\)</span>.</p>
<p>Another important identity is <span class="math display">\[\det(\boldsymbol I_n + \boldsymbol A\boldsymbol B) = \det(\boldsymbol I_m + \boldsymbol B\boldsymbol A)\]</span> where <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> are rectangular matrices. This is called the Weinstein-Aronszajn determinant identity (also credited to Sylvester).</p>
</section>
</section>
<section id="matrix-inverse" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="matrix-inverse"><span class="header-section-number">1.5</span> Matrix inverse</h2>
<section id="inversion-of-square-matrix" class="level3">
<h3 class="anchored" data-anchor-id="inversion-of-square-matrix">Inversion of square matrix</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is a square matrix then the inverse matrix <span class="math inline">\(\boldsymbol A^{-1}\)</span> is a matrix such that <span class="math display">\[\boldsymbol A^{-1} \boldsymbol A= \boldsymbol A\boldsymbol A^{-1}=  \boldsymbol I\, .\]</span> Only non-singular matrices with <span class="math inline">\(\det(\boldsymbol A) \neq 0\)</span> are invertible.</p>
<p>As <span class="math inline">\(\det(\boldsymbol A^{-1} \boldsymbol A) = \det(\boldsymbol I) = 1\)</span> the determinant of the inverse matrix equals the inverse determinant, <span class="math display">\[\det(\boldsymbol A^{-1}) = \det(\boldsymbol A)^{-1} \,.\]</span></p>
<p>The transpose of the inverse is the inverse of the transpose as <span class="math display">\[
\begin{split}
(\boldsymbol A^{-1})^T &amp;= (\boldsymbol A^{-1})^T \,  \boldsymbol A^T (\boldsymbol A^{T})^{-1}   \\
&amp;= (\boldsymbol A\boldsymbol A^{-1})^T \, (\boldsymbol A^{T})^{-1} = (\boldsymbol A^{T})^{-1} \,. \\
\end{split}
\]</span></p>
<p>The inverse of a matrix product <span class="math inline">\((\boldsymbol A\boldsymbol B)^{-1} = \boldsymbol B^{-1} \boldsymbol A^{-1}\)</span> is the product of the individual matrix inverses in reverse order.</p>
<p>There are many different algorithms to compute the inverse of a matrix (which is essentially a problem of solving a system of equations). The computational complexity of matrix inversion is of the order <span class="math inline">\(O(d^3)\)</span> where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(\boldsymbol A\)</span>. Therefore matrix inversion is very costly in higher dimensions.</p>
<div id="exm-invmattwobytwo" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1</strong></span> Inversion of a <span class="math inline">\(2 \times 2\)</span> matrix:</p>
<p>The inverse of the matrix <span class="math inline">\(A = \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}\)</span> is <span class="math inline">\(A^{-1} = \frac{1}{ad-bc} \begin{pmatrix} d &amp; -b \\ -c &amp; a \end{pmatrix}\)</span></p>
</div>
</section>
<section id="inversion-of-structured-matrices" class="level3">
<h3 class="anchored" data-anchor-id="inversion-of-structured-matrices">Inversion of structured matrices</h3>
<p>However, for specially structured matrices inversion can be done effectively:</p>
<ul>
<li>The inverse of a <strong>diagonal matrix</strong> is another diagonal matrix obtained by inverting the diagonal elements.</li>
<li>More generally, the inverse of a <strong>block-diagonal matrix</strong> is obtained by individually inverting the blocks along the diagonal.</li>
</ul>
<p>The <strong>Woodbury matrix identity</strong> simplifies the inversion of matrices that can be written as <span class="math inline">\(\boldsymbol A+ \boldsymbol U\boldsymbol B\boldsymbol V\)</span> where <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> are both square and <span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span> are suitable rectangular matrices: <span class="math display">\[
(\boldsymbol A+ \boldsymbol U\boldsymbol B\boldsymbol V)^{-1} = \boldsymbol A^{-1} - \boldsymbol A^{-1} \boldsymbol U(\boldsymbol B^{-1} + \boldsymbol V\boldsymbol A^{-1} \boldsymbol U)^{-1} \boldsymbol V\boldsymbol A^{-1}
\]</span> Typically, the inverse <span class="math inline">\(\boldsymbol A^{-1}\)</span> is either already known or can be easily obtained and the dimension of <span class="math inline">\(\boldsymbol B\)</span> is much lower than that of <span class="math inline">\(\boldsymbol A\)</span>.</p>
<p>The class of matrices that can be most easily inverted are <strong>orthogonal matrices</strong> whose inverse is obtained simply by transposing the matrix.</p>
</section>
</section>
<section id="orthogonal-matrices" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="orthogonal-matrices"><span class="header-section-number">1.6</span> Orthogonal matrices</h2>
<section id="properties" class="level3">
<h3 class="anchored" data-anchor-id="properties">Properties</h3>
<p>An orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> is a square matrix with the property that <span class="math inline">\(\boldsymbol Q^T = \boldsymbol Q^{-1}\)</span>, i.e. the transpose is also the inverse. This implies that <span class="math inline">\(\boldsymbol Q\boldsymbol Q^T = \boldsymbol Q^T \boldsymbol Q= \boldsymbol I\)</span>.</p>
<p>Both the column and the row vectors in <span class="math inline">\(\boldsymbol Q\)</span> all have length 1. This implies that each element <span class="math inline">\(q_{ij}\)</span> of <span class="math inline">\(\boldsymbol Q\)</span> can only take a value in the interval <span class="math inline">\([-1, 1]\)</span>.</p>
<p>The identity matrix <span class="math inline">\(\boldsymbol I\)</span> is the simplest example of an orthogonal matrix.</p>
<p>The squared Euclidean and Frobenius norm is preserved when a vector <span class="math inline">\(\boldsymbol a\)</span> or matrix <span class="math inline">\(\boldsymbol A\)</span> is multiplied with an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span>: <span class="math display">\[
|| \boldsymbol Q\boldsymbol a||^2_2 = (\boldsymbol Q\boldsymbol a)^T \boldsymbol Q\boldsymbol a= \boldsymbol a^T \boldsymbol a= || \boldsymbol a||^2_2
\]</span> and <span class="math display">\[
|| \boldsymbol Q\boldsymbol A||^2_F = \text{Tr}\left((\boldsymbol Q\boldsymbol A)^T \boldsymbol Q\boldsymbol A\right) = \text{Tr}\left(\boldsymbol A^T \boldsymbol A\right) = || \boldsymbol A||^2_F
\]</span></p>
<p>Multiplication of <span class="math inline">\(\boldsymbol Q\)</span> with a vector results in a new vector of the same length but with a change in direction (unless <span class="math inline">\(\boldsymbol Q=\boldsymbol I\)</span>). An orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> can thus be interpreted geometrically as an operator performing rotation, reflection and/or permutation.</p>
<p>The product <span class="math inline">\(\boldsymbol Q_3 = \boldsymbol Q_1 \boldsymbol Q_2\)</span> of two orthogonal matrices <span class="math inline">\(\boldsymbol Q_1\)</span> and <span class="math inline">\(\boldsymbol Q_2\)</span> yields another orthogonal matrix as <span class="math inline">\(\boldsymbol Q_3 \boldsymbol Q_3^T = \boldsymbol Q_1 \boldsymbol Q_2  (\boldsymbol Q_1 \boldsymbol Q_2)^T = \boldsymbol Q_1 \boldsymbol Q_2 \boldsymbol Q_2^T \boldsymbol Q_1^T = \boldsymbol I\)</span>.</p>
<p>The determinant <span class="math inline">\(\det(\boldsymbol Q)\)</span> of an orthogonal matrix is either +1 or -1, because <span class="math inline">\(\boldsymbol Q\boldsymbol Q^T = \boldsymbol I\)</span> and thus <span class="math inline">\(\det(\boldsymbol Q)\det(\boldsymbol Q^T) = \det(\boldsymbol Q)^2 = \det(\boldsymbol I) = 1\)</span>.</p>
<p>The set of all orthogonal matrices of dimension <span class="math inline">\(d\)</span> together with multiplication form a group called the orthogonal group <span class="math inline">\(O(d)\)</span>. The subset of orthogonal matrices with <span class="math inline">\(\det(\boldsymbol Q)=1\)</span> are called rotation matrices and form with multiplication the special orthogonal group <span class="math inline">\(SO(d)\)</span>. Orthogonal matrices with <span class="math inline">\(\det(\boldsymbol Q)=-1\)</span> are rotation-reflection matrices.</p>
</section>
<section id="semi-orthogonal-matrices" class="level3">
<h3 class="anchored" data-anchor-id="semi-orthogonal-matrices">Semi-orthogonal matrices</h3>
<p>A rectangular <span class="math inline">\(d \times k\)</span> matrix <span class="math inline">\(\boldsymbol Q\)</span> is semi-orthogonal if for <span class="math inline">\(k &lt; d\)</span> the <span class="math inline">\(k\)</span> column vectors are orthonormal and hence <span class="math inline">\(\boldsymbol Q^T \boldsymbol Q= \boldsymbol I_k\)</span>, or if for <span class="math inline">\(k &gt; d\)</span> the <span class="math inline">\(d\)</span> row vectors are orthonormal with <span class="math inline">\(\boldsymbol Q\boldsymbol Q^T = \boldsymbol I_d\)</span>.</p>
<p>The set of all (semi)-orthogonal matrices <span class="math inline">\(\boldsymbol Q\)</span> with <span class="math inline">\(k \leq d\)</span> column vectors is known as the Stiefel manifold <span class="math inline">\(\text{St}(d, k)\)</span>.</p>
</section>
<section id="generating-orthogonal-matrices" class="level3">
<h3 class="anchored" data-anchor-id="generating-orthogonal-matrices">Generating orthogonal matrices</h3>
<p>In two dimensions <span class="math inline">\((d=2)\)</span> all orthogonal matrices <span class="math inline">\(\boldsymbol R\)</span> representing rotations with <span class="math inline">\(\det(\boldsymbol R)=1\)</span> are given by <span class="math display">\[
\boldsymbol R(\theta) =
\begin{pmatrix}
\cos \theta &amp; -\sin \theta \\
\sin \theta &amp; \cos \theta
\end{pmatrix}
\]</span> and those representing rotation-reflections <span class="math inline">\(\boldsymbol G\)</span> with <span class="math inline">\(\det(\boldsymbol G)=-1\)</span> by <span class="math display">\[
\boldsymbol G(\theta) =
\begin{pmatrix}
\cos \theta &amp; \sin \theta \\
\sin \theta &amp; -\cos \theta
\end{pmatrix}\,.
\]</span> Every orthogonal matrix of dimension <span class="math inline">\(d=2\)</span> can be represented as the product of at most two rotation-reflection matrices because <span class="math display">\[
\boldsymbol R(\theta) = \boldsymbol G(\theta)\, \boldsymbol G(0) =  
\begin{pmatrix}
\cos \theta &amp; \sin \theta \\
\sin \theta &amp; -\cos \theta
\end{pmatrix}
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; -1
\end{pmatrix}\,.
\]</span> Thus, the matrix <span class="math inline">\(\boldsymbol G\)</span> is a generator of two-dimensional orthogonal matrices. Note that <span class="math inline">\(\boldsymbol G(\theta)\)</span> is symmetric, orthogonal and has determinant -1.</p>
<p>More generally, and applicable in arbitrary dimension, the role of generator is taken by the Householder reflection matrix <span class="math display">\[
\boldsymbol Q_{HH}(\boldsymbol v) = \boldsymbol I- 2 \boldsymbol v\boldsymbol v^T
\]</span> where <span class="math inline">\(\boldsymbol v\)</span> is a vector of unit length (with <span class="math inline">\(\boldsymbol v^T \boldsymbol v=1\)</span>) orthogonal to the reflection hyperplane. Note that <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v) = \boldsymbol Q_{HH}(-\boldsymbol v)\)</span>. By construction the matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span> is symmetric, orthogonal and has determinant -1.</p>
<p>It can be shown that any <span class="math inline">\(d\)</span>-dimensional orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> can be represented as the product of at most <span class="math inline">\(d\)</span> Householder reflection matrices. The two-dimensional generator <span class="math inline">\(\boldsymbol G(\theta)\)</span> is recovered as the Householder matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span> with <span class="math inline">\(\boldsymbol v= \begin{pmatrix} -\sin  \frac{\theta}{2} \\ \cos \frac{\theta}{2} \end{pmatrix}\)</span> or <span class="math inline">\(\boldsymbol v= \begin{pmatrix} \sin  \frac{\theta}{2} \\ -\cos \frac{\theta}{2} \end{pmatrix}\)</span>.</p>
</section>
<section id="permutation-matrix" class="level3">
<h3 class="anchored" data-anchor-id="permutation-matrix">Permutation matrix</h3>
<p>A special type of an orthogonal matrix is a permutation matrix <span class="math inline">\(\boldsymbol P\)</span> created by permuting rows and/or columns of the identity matrix <span class="math inline">\(\boldsymbol I\)</span>. Thus, each row and column of <span class="math inline">\(\boldsymbol P\)</span> contains exactly one entry of 1, but not necessarily on the diagonal.</p>
<p>If a permutation matrix <span class="math inline">\(\boldsymbol P\)</span> is multiplied with a matrix <span class="math inline">\(\boldsymbol A\)</span> it acts as an operator permuting the columns (<span class="math inline">\(\boldsymbol A\boldsymbol P\)</span>) or the rows (<span class="math inline">\(\boldsymbol P\boldsymbol A\)</span>). For a set of <span class="math inline">\(d\)</span> elements there exist <span class="math inline">\(d!\)</span> permutations. Thus, for dimension <span class="math inline">\(d\)</span> there are <span class="math inline">\(d!\)</span> possible permutation matrices (including the identity matrix).</p>
<p>The determinant of a permutation matrix is either +1 or -1. The product of two permutation matrices yields another permutation matrix.</p>
<p>Symmetric permutation matrices correspond to self-inverse permutations (i.e.&nbsp;the permutation matrix is its own inverse), and are also called permutation involutions. They can have determinant +1 and -1.</p>
<p>A transposition is a permutation where only two elements are exchanged. Thus, in a transposition matrix <span class="math inline">\(\boldsymbol T\)</span> exactly two rows and/or columns are exchanged compared to identity matrix <span class="math inline">\(\boldsymbol I\)</span>. Transpositions are self-inverse, and transposition matrices are symmetric. There are <span class="math inline">\(\frac{d (d-1)}{2}\)</span> different transposition matrices. The determinant of a transposition matrix is <span class="math inline">\(\det(\boldsymbol T)= -1\)</span>.</p>
<p>Note that the transposition matrix is an instance of a Householder matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span> with vector <span class="math inline">\(\boldsymbol v\)</span> filled with zeros except for two elements that have value <span class="math inline">\(\frac{\sqrt{2}}{2}\)</span> and <span class="math inline">\(-\frac{\sqrt{2}}{2}\)</span>.</p>
<p>Any permutation of <span class="math inline">\(d\)</span> elements can be generated by a series of at most <span class="math inline">\(d-1\)</span> transpositions. Correspondingly, any permutation matrix <span class="math inline">\(\boldsymbol P\)</span> can be constructed by multiplication of the identity matrix with at most <span class="math inline">\(d-1\)</span> transposition matrices. If the number of transpositions is even then <span class="math inline">\(\det(\boldsymbol P) = 1\)</span> otherwise for an uneven number <span class="math inline">\(\det(\boldsymbol P) = -1\)</span>. This is called the sign or signature of the permutation.</p>
<p>The set of all permutations form the symmetric group <span class="math inline">\(S_d\)</span>, the subset of even permutations (with positive sign and <span class="math inline">\(\det(\boldsymbol P)=1\)</span>) the alternating group <span class="math inline">\(A_d\)</span>.</p>
</section>
</section>
<section id="eigenvalues-and-eigenvectors" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="eigenvalues-and-eigenvectors"><span class="header-section-number">1.7</span> Eigenvalues and eigenvectors</h2>
<section id="definition" class="level3">
<h3 class="anchored" data-anchor-id="definition">Definition</h3>
<p>Assume a square matrix <span class="math inline">\(\boldsymbol A\)</span> of size <span class="math inline">\(d \times d\)</span>. A vector <span class="math inline">\(\boldsymbol u\neq 0\)</span> is called an eigenvector of the matrix <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\lambda\)</span> the corresponding eigenvalue if<br>
<span class="math display">\[\boldsymbol A\boldsymbol u= \boldsymbol u\lambda \, .\]</span> This is called <strong>eigenvalue equation</strong> or <strong>eigenequation</strong>.</p>
</section>
<section id="finding-eigenvalues-and-vectors" class="level3">
<h3 class="anchored" data-anchor-id="finding-eigenvalues-and-vectors">Finding eigenvalues and vectors</h3>
<p>To find the eigenvalues and eigenvectors the eigenequation is rewritten as <span class="math display">\[(\boldsymbol A-\boldsymbol I\lambda ) \; \boldsymbol u= \mathbf 0\,.\]</span> For this equation to hold for an eigenvector <span class="math inline">\(\boldsymbol u\neq 0\)</span> with eigenvalue <span class="math inline">\(\lambda\)</span> implies that the matrix <span class="math inline">\(\boldsymbol A-\boldsymbol I\lambda\)</span> is singular. Correspondingly, its determinant must vanish <span class="math display">\[\det(\boldsymbol A-\boldsymbol I\lambda ) =0 \,.\]</span> This is called the <em>characteristic equation</em> of the matrix <span class="math inline">\(\boldsymbol A\)</span>, and its solution yields the <span class="math inline">\(d\)</span> eigenvalues <span class="math inline">\(\lambda_1, \ldots, \lambda_d\)</span>. Note the eigenvalues need not be distinct and they may be complex even if the matrix <span class="math inline">\(\boldsymbol A\)</span> is real.</p>
<p>If there are complex eigenvalues, for a real matrix those eigenvalues come in conjugate pairs. Hence, for a complex <span class="math inline">\(\lambda_1 = r e^{i \phi}\)</span> there will also be a corresponding complex eigenvalue <span class="math inline">\(\lambda_2 = r e^{-i \phi}\)</span>.</p>
<p>Given the eigenvalues we then solve the eigenequation for the corresponding non-zero eigenvectors <span class="math inline">\(\boldsymbol u_1, \ldots, \boldsymbol u_d\)</span>. Note that eigenvectors of real matrices can have complex components. Also the eigenvector is only defined by the eigenequation up to a scalar. By convention eigenvectors are therefore typically standardised to unit length but this still leaves a sign ambiguity for real eigenvectors and implies that complex eigenvectors are defined only up to a factor with modulus 1.</p>
</section>
<section id="eigenequation-in-matrix-notation" class="level3">
<h3 class="anchored" data-anchor-id="eigenequation-in-matrix-notation">Eigenequation in matrix notation</h3>
<p>With the matrix <span class="math display">\[\boldsymbol U= (\boldsymbol u_1, \ldots, \boldsymbol u_d)\]</span> containing the standardised eigenvectors in the columns and the diagonal matrix <span class="math display">\[\boldsymbol \Lambda= \begin{pmatrix}
    \lambda_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}
\end{pmatrix}\]</span> containing the eigenvalues (typically sorted in order of magnitude) the eigenvalue equation can be written as <span class="math display">\[\boldsymbol A\boldsymbol U= \boldsymbol U\boldsymbol \Lambda\,.\]</span></p>
</section>
<section id="permutation-of-eigenvalues" class="level3">
<h3 class="anchored" data-anchor-id="permutation-of-eigenvalues">Permutation of eigenvalues</h3>
<p>If eigenvalues are not in order, we may apply a permutation matrix <span class="math inline">\(\boldsymbol P\)</span> to arrange them in order. With <span class="math inline">\(\boldsymbol \Lambda^{\text{sort}} = \boldsymbol P^T \boldsymbol \Lambda\boldsymbol P\)</span> as the sorted eigenvalues and <span class="math inline">\(\boldsymbol U^{\text{sort}} = \boldsymbol U\boldsymbol P\)</span> as the corresponding eigenvectors the eigenequation becomes <span class="math display">\[\boldsymbol A\boldsymbol U^{\text{sort}} =  \boldsymbol A\boldsymbol U\boldsymbol P= \boldsymbol U\boldsymbol \Lambda\boldsymbol P=  \boldsymbol U\boldsymbol P\boldsymbol P^T \boldsymbol \Lambda\boldsymbol P=  \boldsymbol U^{\text{sort}} \boldsymbol \Lambda^{\text{sort}} \,.\]</span></p>
</section>
<section id="similar-matrices" class="level3">
<h3 class="anchored" data-anchor-id="similar-matrices">Similar matrices</h3>
<p>Two matrices <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> are called <strong>similar</strong> if they share the same eigenvalues.</p>
<p>From <span class="math inline">\(\boldsymbol A\)</span> with eigenvalues <span class="math inline">\(\boldsymbol \Lambda\)</span> and eigenvectors <span class="math inline">\(\boldsymbol U\)</span> we can construct a similar <span class="math inline">\(\boldsymbol B\)</span> via the <strong>similarity transformation</strong> <span class="math inline">\(\boldsymbol B= \boldsymbol M\boldsymbol A\boldsymbol M^{-1}\)</span> where <span class="math inline">\(\boldsymbol M\)</span> is an invertible matrix.</p>
<p>Then <span class="math inline">\(\boldsymbol \Lambda\)</span> are the eigenvalues of <span class="math inline">\(\boldsymbol B\)</span> and <span class="math inline">\(\boldsymbol V= \boldsymbol M\boldsymbol U\)</span> its eigenvectors as <span class="math display">\[
\boldsymbol B\boldsymbol V= \boldsymbol M\boldsymbol A\boldsymbol M^{-1} \boldsymbol M\boldsymbol U= \boldsymbol M\boldsymbol A\boldsymbol U= \boldsymbol M\boldsymbol U\boldsymbol \Lambda= \boldsymbol V\boldsymbol \Lambda\,.
\]</span></p>
</section>
<section id="defective-matrix" class="level3">
<h3 class="anchored" data-anchor-id="defective-matrix">Defective matrix</h3>
<p>In most cases the eigenvectors <span class="math inline">\(\boldsymbol u_i\)</span> will be linearly independent so that they form a basis to span a <span class="math inline">\(d\)</span> dimensional space.</p>
<p>However, if this is not the case and the matrix <span class="math inline">\(\boldsymbol A\)</span> does not have a complete basis of eigenvectors, then the matrix is called defective. In this case the matrix <span class="math inline">\(\boldsymbol U\)</span> containing the eigenvectors is singular and <span class="math inline">\(\det(\boldsymbol U)=0\)</span>.</p>
<p>An example of a defective matrix is <span class="math inline">\(\begin{pmatrix}
1 &amp;1 \\
0 &amp; 1 \\
\end{pmatrix}\)</span> which has determinant 1 so that it can be inverted and its column vectors do form a complete basis but has only one distinct eigenvector <span class="math inline">\((1,0)^T\)</span> so that the eigenvector basis is incomplete.</p>
</section>
<section id="eigenvalues-of-a-diagonal-or-triangular-matrix" class="level3">
<h3 class="anchored" data-anchor-id="eigenvalues-of-a-diagonal-or-triangular-matrix">Eigenvalues of a diagonal or triangular matrix</h3>
<p>In the special case that <span class="math inline">\(\boldsymbol A\)</span> is diagonal or a triangular matrix the eigenvalues are easily determined. This follows from the simple form of their determinants as the product of the diagonal elements. Hence for these matrices the characteristic equation becomes <span class="math inline">\(\prod_{i}^d (a_{ii} -\lambda) = 0\)</span> and has solution <span class="math inline">\(\lambda_i=a_{ii}\)</span>, i.e.&nbsp;the eigenvalues are equal to the diagonal elements.</p>
</section>
<section id="eigenvalues-and-vectors-of-a-symmetric-matrix" class="level3">
<h3 class="anchored" data-anchor-id="eigenvalues-and-vectors-of-a-symmetric-matrix">Eigenvalues and vectors of a symmetric matrix</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is symmetric, i.e.&nbsp;<span class="math inline">\(\boldsymbol A= \boldsymbol A^T\)</span>, then its eigenvalues and eigenvectors have special properties:</p>
<ol type="i">
<li>all eigenvalues of <span class="math inline">\(\boldsymbol A\)</span> are real,</li>
<li>the eigenvectors are orthogonal, i.e <span class="math inline">\(\boldsymbol u_i^T \boldsymbol u_j = 0\)</span> for <span class="math inline">\(i \neq j\)</span>, and real. Thus, the matrix <span class="math inline">\(\boldsymbol U\)</span> containing the standardised orthonormal eigenvectors is orthogonal.</li>
<li><span class="math inline">\(\boldsymbol A\)</span> is never defective as <span class="math inline">\(\boldsymbol U\)</span> forms a complete basis.</li>
</ol>
<p>Furthermore, for a symmetric matrix <span class="math inline">\(\boldsymbol A\)</span> with diagonal elements <span class="math inline">\(p_1 \geq \ldots \geq p_d\)</span> and eigenvalues <span class="math inline">\(\lambda_1 \geq \ldots \geq \lambda_d\)</span> (note both written in decreasing order) the sum of the largest <span class="math inline">\(k\)</span> eigenvalues forms an upper bound of the sum of the largest <span class="math inline">\(k\)</span> diagonal elements: <span class="math display">\[
\sum_{i}^k \lambda_i \geq \sum_{i}^k p_i
\]</span> This theorem is due to Schur (1923) <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. The equality holds for <span class="math inline">\(k=d\)</span> (as the trace of <span class="math inline">\(\boldsymbol A\)</span> equals the sum of its eigenvalues) and for any <span class="math inline">\(k\)</span> if <span class="math inline">\(\boldsymbol A\)</span> is diagonal (as in this case of the diagonal elements equal the eigenvalues).</p>
</section>
<section id="eigenvalues-of-orthogonal-matrices" class="level3">
<h3 class="anchored" data-anchor-id="eigenvalues-of-orthogonal-matrices">Eigenvalues of orthogonal matrices</h3>
<p>The eigenvalues of an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> are not necessarily real but they all have modulus 1 and lie on the unit circle . Thus, the eigenvalues of <span class="math inline">\(\boldsymbol Q\)</span> all have the form <span class="math inline">\(\lambda = e^{i \phi} = \cos \phi + i \sin \phi\)</span>.</p>
<p>In any real matrix complex eigenvalues come in conjugate pairs. Hence if an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> has the complex eigenvalue <span class="math inline">\(e^{i \phi}\)</span> it also has an complex eigenvalue <span class="math inline">\(e^{-i \phi} =\cos \phi - i \sin \phi\)</span>. The product of these two conjugate eigenvalues is 1. Thus, an orthogonal matrix of uneven dimension has at least one real eigenvalue (+1 or -1).</p>
<p>The eigenvalues of a Hausholder matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span> are all real (recall that it is symmetric!). In fact, in dimension <span class="math inline">\(d\)</span> its eigenvalues are -1 (one time) and 1 ( <span class="math inline">\(d-1\)</span> times). Since a transposition matrix <span class="math inline">\(\boldsymbol T\)</span> is a special Householder matrix they have the same eigenvalues.</p>
</section>
<section id="positive-definite-matrices" class="level3">
<h3 class="anchored" data-anchor-id="positive-definite-matrices">Positive definite matrices</h3>
<p>If all eigenvalues of a square matrix <span class="math inline">\(\boldsymbol A\)</span> are real and <span class="math inline">\(\lambda_i \geq 0\)</span> then <span class="math inline">\(\boldsymbol A\)</span> is called <em>positive semi-definite</em>. If all eigenvalues are strictly positive <span class="math inline">\(\lambda_i &gt; 0\)</span> then <span class="math inline">\(\boldsymbol A\)</span> is called <em>positive definite</em>.</p>
<p>Note that a matrix does not need to be symmetric to be positive definite, e.g. <span class="math inline">\(\begin{pmatrix}
2 &amp; 3 \\
1 &amp; 4 \\
\end{pmatrix}\)</span> has positive eigenvalues 5 and 1. It also has a complete set of eigenvectors and is diagonalisable.</p>
<p>A symmetric matrix <span class="math inline">\(\boldsymbol A\)</span> is positive definite if the quadratic form <span class="math inline">\(\boldsymbol x^T \boldsymbol A\boldsymbol x&gt; 0\)</span> for any non-zero <span class="math inline">\(\boldsymbol x\)</span>, and it is positive semi-definite if <span class="math inline">\(\boldsymbol x^T \boldsymbol A\boldsymbol x\geq 0\)</span>. This holds also the other way around: a symmetric positive definite matrix (with positive eigenvalues) has a positive quadratic form, and a symmetric positive semi-definite matrix (with non-negative eigenvalues) a non-negative quadratic form.</p>
<p>A symmetric positive definite matrix always has a positive diagonal (this can be seen by setting <span class="math inline">\(\boldsymbol x\)</span> above to a unit vector with 1 at a single position, and 0 at all other elements). However, just requiring a positive diagonal is too weak to ensure positive definiteness of a symmetric matrix, for example <span class="math inline">\(\begin{pmatrix}
1 &amp;10 \\
10 &amp; 1 \\
\end{pmatrix}\)</span> has a negative eigenvalue of -9. On the other hand, a symmetric matrix is indeed positive definite if it is strictly diagonally dominant, i.e.&nbsp;if all its diagonal elements are positive and are larger than the absolute value of any of the corresponding row or column elements. However, diagonal dominance is too restrictive as criterion to characterise all symmetric positive definite matrices, since there are many symmetric matrices that are positive definite but not diagonally dominant, such as <span class="math inline">\(\begin{pmatrix}
1 &amp; 2 \\
2 &amp; 5 \\
\end{pmatrix}\)</span>.</p>
<p>Finally, the sum of a symmetric positive semi-definite matrix <span class="math inline">\(\boldsymbol A\)</span> and a symmetric positive definite matrix <span class="math inline">\(\boldsymbol B\)</span> is itself symmetric positive definite because the corresponding quadratic form <span class="math inline">\(\boldsymbol x^T ( \boldsymbol A+\boldsymbol B) \boldsymbol x=
\boldsymbol x^T \boldsymbol A\boldsymbol x+ \boldsymbol x^T \boldsymbol B\boldsymbol x&gt; 0\)</span> is positive. Similarly, the sum of two symmetric positive (semi)-definite matrices is itself symmetric positive (semi)-definite.</p>
</section>
</section>
<section id="matrix-decompositions" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="matrix-decompositions"><span class="header-section-number">1.8</span> Matrix decompositions</h2>
<section id="diagonalisation-and-eigenvalue-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="diagonalisation-and-eigenvalue-decomposition">Diagonalisation and eigenvalue decomposition</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is a square non-defective matrix then the eigensystem <span class="math inline">\(\boldsymbol U\)</span> is invertible and we can rewrite the eigenvalue equation to <span class="math display">\[\boldsymbol A= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1} \,.\]</span> This is called the <strong>eigendecomposition</strong>, or <strong>spectral decomposition</strong>, of <span class="math inline">\(\boldsymbol A\)</span> and equivalently <span class="math display">\[\boldsymbol \Lambda= \boldsymbol U^{-1} \boldsymbol A\boldsymbol U\]</span> is the diagonalisation of <span class="math inline">\(\boldsymbol A\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is defective (i.e.&nbsp;<span class="math inline">\(\boldsymbol U\)</span> is singular) one can still <em>approximately</em> diagonalise <span class="math inline">\(\boldsymbol A\)</span> as there always exists a similarity transformation to <span class="math inline">\(\boldsymbol J= \boldsymbol M\boldsymbol A\boldsymbol M^{-1}\)</span> where <span class="math inline">\(\boldsymbol M\)</span> is a invertible matrix and <span class="math inline">\(\boldsymbol J\)</span> has Jordan canonical form, i.e.&nbsp;<span class="math inline">\(\boldsymbol J\)</span> is upper triangular with the (potentially complex) eigenvalues on the diagonal and some non-zero entries equal to 1 immediately above the main diagonal.</p>
</section>
<section id="orthogonal-eigenvalue-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="orthogonal-eigenvalue-decomposition">Orthogonal eigenvalue decomposition</h3>
<p>For symmetric <span class="math inline">\(\boldsymbol A\)</span> with real eigenvalues and orthogonal matrix <span class="math inline">\(\boldsymbol U\)</span> the spectral decomposition becomes <span class="math display">\[\boldsymbol A= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\]</span> and <span class="math display">\[\boldsymbol \Lambda= \boldsymbol U^T \boldsymbol A\boldsymbol U\,.\]</span> This special case is known as the orthogonal diagonalisation of <span class="math inline">\(\boldsymbol A\)</span>.</p>
<p>The orthogonal decomposition for symmetric <span class="math inline">\(\boldsymbol A\)</span> is unique apart from the signs of the eigenvectors (columns of <span class="math inline">\(\boldsymbol U\)</span>). Thus, in a computer application depending on the specific implementation of a numerical algorithm for eigenvalue decomposition the signs may vary.</p>
</section>
<section id="singular-value-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="singular-value-decomposition">Singular value decomposition</h3>
<p>The <strong>singular value decomposition</strong> (SVD) is a generalisation of the orthogonal eigenvalue decomposition for symmetric matrices.</p>
<p>Any (!) rectangular matrix <span class="math inline">\(\boldsymbol A\)</span> of size <span class="math inline">\(n\times d\)</span> can be factored into the product <span class="math display">\[\boldsymbol A= \boldsymbol U\boldsymbol D\boldsymbol V^T\]</span> where <span class="math inline">\(\boldsymbol U\)</span> is a <span class="math inline">\(n \times n\)</span> orthogonal matrix, <span class="math inline">\(\boldsymbol V\)</span> is a second <span class="math inline">\(d \times d\)</span> orthogonal matrix and <span class="math inline">\(\boldsymbol D\)</span> is a diagonal but rectangular matrix of size <span class="math inline">\(n\times d\)</span> with <span class="math inline">\(m=min(n,d)\)</span> real diagonal elements <span class="math inline">\(d_1, \ldots
d_m\)</span>. The <span class="math inline">\(d_i\)</span> are called singular values, and appear along the diagonal in <span class="math inline">\(\boldsymbol D\)</span> by order of magnitude.</p>
<p>The SVD is unique apart from the signs of the columns vectors in <span class="math inline">\(\boldsymbol U\)</span>, <span class="math inline">\(\boldsymbol V\)</span> and <span class="math inline">\(\boldsymbol D\)</span> (you can freely specify the column signs of any two of the three matrices). By convention the signs are chosen such that the singular values in <span class="math inline">\(\boldsymbol D\)</span> are all non-negative, which leaves ambiguity in columns signs of <span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span>. Alternatively, one may fix the columns signs of <span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span>, e.g.&nbsp;by requiring a positive diagonal, which then determines the sign of the singular values (thus allowing for negative singular values as well).</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is symmetric then the SVD and the orthogonal eigenvalue decomposition coincide (apart from different sign conventions for singular values, eigenvalues and eigenvectors).</p>
<p>Since <span class="math inline">\(\boldsymbol A^T \boldsymbol A= \boldsymbol V\boldsymbol D^T \boldsymbol D\boldsymbol V^T\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T = \boldsymbol U\boldsymbol D\boldsymbol D^T \boldsymbol U^T\)</span> the squared singular values correspond to the eigenvalues of <span class="math inline">\(\boldsymbol A^T \boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T\)</span>. It also follows that <span class="math inline">\(\boldsymbol A^T \boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T\)</span> are both positive semi-definite symmetric matrices, and that <span class="math inline">\(\boldsymbol V\)</span> and <span class="math inline">\(\boldsymbol U\)</span> contain the respective sets of eigenvectors.</p>
</section>
<section id="polar-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="polar-decomposition">Polar decomposition</h3>
<p>Any square matrix <span class="math inline">\(\boldsymbol A\)</span> can be factored into the product <span class="math display">\[
\boldsymbol A= \boldsymbol Q\boldsymbol B
\]</span> of an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> and a symmetric positive semi-definite matrix <span class="math inline">\(\boldsymbol B\)</span>.</p>
<p>This follows from the SVD of <span class="math inline">\(\boldsymbol A\)</span> given as <span class="math display">\[
\begin{split}
\boldsymbol A&amp;= \boldsymbol U\boldsymbol D\boldsymbol V^T \\
    &amp;= ( \boldsymbol U\boldsymbol V^T ) ( \boldsymbol V\boldsymbol D\boldsymbol V^T ) \\
    &amp;= \boldsymbol Q\boldsymbol B\\
\end{split}
\]</span> with non-negative <span class="math inline">\(\boldsymbol D\)</span>. Note that this decomposition is unique as the sign ambiguities in the columns of <span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span> cancel out in <span class="math inline">\(\boldsymbol Q\)</span> and <span class="math inline">\(\boldsymbol B\)</span>.</p>
</section>
<section id="cholesky-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="cholesky-decomposition">Cholesky decomposition</h3>
<p>A symmetric positive definite matrix <span class="math inline">\(\boldsymbol A\)</span> can be decomposed into a product of a triangular matrix <span class="math inline">\(\boldsymbol L\)</span> with its transpose <span class="math display">\[
\boldsymbol A= \boldsymbol L\boldsymbol L^T \,.
\]</span> Here, <span class="math inline">\(\boldsymbol L\)</span> is a lower triangular matrix with positive diagonal elements.</p>
<p>This decomposition is unique and is called <strong>Cholesky factorisation</strong>. It is often used to check whether a symmetric matrix is positive definite as it is algorithmically less demanding than eigenvalue decomposition.</p>
<p>Note that some implementations of the Cholesky decomposition (e.g.&nbsp;in R) use upper triangular matrices <span class="math inline">\(\boldsymbol K\)</span> with positive diagonal so that <span class="math inline">\(\boldsymbol A= \boldsymbol K^T \boldsymbol K\)</span> and <span class="math inline">\(\boldsymbol L= \boldsymbol K^T\)</span>.</p>
</section>
</section>
<section id="matrix-summaries-based-on-eigenvalues-and-singular-values" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="matrix-summaries-based-on-eigenvalues-and-singular-values"><span class="header-section-number">1.9</span> Matrix summaries based on eigenvalues and singular values</h2>
<section id="trace-and-determinant-computed-from-eigenvalues" class="level3">
<h3 class="anchored" data-anchor-id="trace-and-determinant-computed-from-eigenvalues">Trace and determinant computed from eigenvalues</h3>
<p>The eigendecomposition <span class="math inline">\(\boldsymbol A=\boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1}\)</span> allows to establish a link between the trace and the determinant and the eigenvalues of a matrix.</p>
<p>Specifically, <span class="math display">\[
\begin{split}
\text{Tr}(\boldsymbol A) &amp; = \text{Tr}(\boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1}  ) =
\text{Tr}( \boldsymbol \Lambda\boldsymbol U^{-1} \boldsymbol U) \\
&amp;= \text{Tr}( \boldsymbol \Lambda) = \sum_{i=1}^d \lambda_i \\
\end{split}
\]</span> thus the trace of a square matrix <span class="math inline">\(\boldsymbol A\)</span> is equal to the <em>sum</em> of its eigenvalues. Likewise, <span class="math display">\[
\begin{split}
\det(\boldsymbol A) &amp; = \det(\boldsymbol U) \det(\boldsymbol \Lambda) \det(\boldsymbol U^{-1}  ) \\
&amp;=\det( \boldsymbol \Lambda) = \prod_{i=1}^d \lambda_i \\
\end{split}
\]</span> therefore the determinant of <span class="math inline">\(\boldsymbol A\)</span> is the <em>product</em> of the eigenvalues.</p>
<p>The relationship between the eigenvalues of a square matrix and the trace and the determinant of that matrix is shown above for diagonalisable matrices. However, it holds more generally for any square matrix, i.e.&nbsp;also for defective matrices. For the latter the Jordan canonical form <span class="math inline">\(\boldsymbol J\)</span> replaces <span class="math inline">\(\boldsymbol \Lambda\)</span> (in both cases the eigenvalues are simply the entries on the diagonal).</p>
<p>If any of the eigenvalues are equal to zero then <span class="math inline">\(\det(\boldsymbol A) = 0\)</span> and as hence <span class="math inline">\(\boldsymbol A\)</span> is singular and not invertible.</p>
<p>The trace and determinant of a real matrix are always real even though the individual eigenvalues may be complex.</p>
</section>
<section id="eigenvalues-of-a-squared-matrix" class="level3">
<h3 class="anchored" data-anchor-id="eigenvalues-of-a-squared-matrix">Eigenvalues of a squared matrix</h3>
<p>From the eigendecomposition <span class="math inline">\(\boldsymbol A=\boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1}\)</span> it is easy to see that the eigenvalues of <span class="math inline">\(\boldsymbol A^2\)</span> are simply the squared eigenvalues of <span class="math inline">\(\boldsymbol A\)</span> as <span class="math display">\[
\boldsymbol A^2 = \boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1} \boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1} = \boldsymbol U\boldsymbol \Lambda^2 \boldsymbol U^{-1}
\]</span> As a result we can compute the trace of <span class="math inline">\(\boldsymbol A^2\)</span> as the sum of the squared eigenvalues of <span class="math inline">\(\boldsymbol A\)</span>, i.e.&nbsp;<span class="math inline">\(\text{Tr}(\boldsymbol A^2) =  \sum_{i=1}^d \lambda_i^2\)</span>, and the determinant as the product of squared eigenvalues, i.e <span class="math inline">\(\det(\boldsymbol A^2) = \prod_{i=1}^d \lambda_i^2\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is symmetric then <span class="math inline">\(\text{Tr}(\boldsymbol A^2) = \text{Tr}(\boldsymbol A\boldsymbol A^T) = || A ||^2_F = \sum_{i=1}^d \sum_{j=1}^d a_{ij}^2\)</span>. This leads to the identity <span class="math display">\[
\sum_{i=1}^d \lambda_i^2 =  \sum_{i=1}^d \sum_{j=1}^d a_{ij}^2
\]</span> between the sum of the squared eigenvalues and the sum of all squared entries of a symmetric matrix <span class="math inline">\(\boldsymbol A\)</span>.</p>
</section>
<section id="rank-and-condition-number" class="level3">
<h3 class="anchored" data-anchor-id="rank-and-condition-number">Rank and condition number</h3>
<p>The rank is the dimension of the space spanned by both the column and row vectors. A rectangular matrix of dimension <span class="math inline">\(n \times d\)</span> will have rank of at most <span class="math inline">\(m = \min(n, d)\)</span>, and if the maximum is indeed achieved then it has full rank.</p>
<p>The condition number describes how well- or ill-conditioned a full rank matrix is. For example, for a square matrix a large condition number implies that the matrix is close to being singular and thus ill-conditioned. If the condition number is infinite then the matrix is not full rank.</p>
<p>The rank and condition of a matrix can both be determined from the <span class="math inline">\(m\)</span> singular values <span class="math inline">\(d_1, \ldots, d_m\)</span> of a matrix obtained by SVD:</p>
<ol type="i">
<li>The rank is number of non-zero singular values.</li>
<li>The condition number is the ratio of the largest singular value divided by the smallest singular value (absolute values if signs are allowed).</li>
</ol>
<p>If a square matrix <span class="math inline">\(\boldsymbol A\)</span> is singular then the condition number is infinite, and it will not have full rank. On the other hand, a non-singular square matrix, such as a positive definite matrix, has full rank.</p>
</section>
</section>
<section id="functions-of-symmetric-matrices" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="functions-of-symmetric-matrices"><span class="header-section-number">1.10</span> Functions of symmetric matrices</h2>
<p>We focus on <em>symmetric</em> square matrices <span class="math inline">\(\boldsymbol A=\boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span> which are always diagonalisable with real eigenvalues <span class="math inline">\(\boldsymbol \Lambda\)</span> and orthogonal eigenvectors <span class="math inline">\(\boldsymbol U\)</span>.</p>
<section id="definition-of-a-matrix-function" class="level3">
<h3 class="anchored" data-anchor-id="definition-of-a-matrix-function">Definition of a matrix function</h3>
<p>Assume a real-valued function <span class="math inline">\(f(a)\)</span> of a real number <span class="math inline">\(a\)</span>. Then the corresponding matrix function <span class="math inline">\(f(\boldsymbol A)\)</span> is defined as <span class="math display">\[
f(\boldsymbol A) =  \boldsymbol Uf(\boldsymbol \Lambda) \boldsymbol U^T =  \boldsymbol U\begin{pmatrix}
    f(\lambda_{1}) &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; f(\lambda_{d})
\end{pmatrix} \boldsymbol U^T
\]</span> where the function <span class="math inline">\(f(a)\)</span> is applied to the eigenvalues of <span class="math inline">\(\boldsymbol A\)</span>. By construction <span class="math inline">\(f(\boldsymbol A)\)</span> is real, symmetric and has real eigenvalues <span class="math inline">\(f(\lambda_i)\)</span>.</p>
<p>Examples:</p>
<div id="exm-matpower" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2</strong></span> Matrix power: <span class="math inline">\(f(a) = a^p\)</span> (with <span class="math inline">\(p\)</span> a real number)</p>
</div>
<p>Special cases of matrix power include :</p>
<ul>
<li>Matrix inversion: <span class="math inline">\(f(a) = a^{-1}\)</span><br>
Note that if the matrix <span class="math inline">\(\boldsymbol A\)</span> is singular, i.e.&nbsp;contains one or more eigenvalues <span class="math inline">\(\lambda_i=0\)</span>, then <span class="math inline">\(\boldsymbol A^{-1}\)</span> is not defined and therefore <span class="math inline">\(\boldsymbol A\)</span> is not invertible.</li>
</ul>
<p>However, a so-called <em>pseudoinverse</em> can still be computed, by inverting the non-zero eigenvalues, and keeping the zero eigenvalues as zero.</p>
<ul>
<li>Matrix square root: <span class="math inline">\(f(a) = a^{1/2}\)</span><br>
Since there are multiple solutions to the square root there are also multiple matrix square roots. The principal matrix square root is obtained by using the positive square roots of all the eigenvalues. Thus the <strong>principal matrix square root</strong> of a positive semi-definite matrix is also positive semi-definite and it is unique.</li>
</ul>
<div id="exm-matexp" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.3</strong></span> Matrix exponential: <span class="math inline">\(f(a) = \exp(a)\)</span><br>
Note that because <span class="math inline">\(\exp(a) \geq 0\)</span> for all real <span class="math inline">\(a\)</span> the matrix <span class="math inline">\(\exp(\boldsymbol A)\)</span> is positive semi-definite. Thus, the matrix exponential can be used to generate positive semi-definite matrices.</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> commute, i.e.&nbsp;if <span class="math inline">\(\boldsymbol A\boldsymbol B= \boldsymbol B\boldsymbol A\)</span>, then <span class="math inline">\(\exp(\boldsymbol A+\boldsymbol B) = \exp(\boldsymbol A) \exp(\boldsymbol B)\)</span>. However, this is not the case otherwise!</p>
</div>
<div id="exm-matlog" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.4</strong></span> Matrix logarithm: <span class="math inline">\(f(a) = \log(a)\)</span><br>
As the logarithm requires <span class="math inline">\(a &gt;0\)</span> the matrix <span class="math inline">\(\boldsymbol A\)</span> needs to be positive definite for <span class="math inline">\(\log(\boldsymbol A)\)</span> to be defined.</p>
</div>
</section>
<section id="identities-for-the-matrix-exponential-and-logarithm" class="level3">
<h3 class="anchored" data-anchor-id="identities-for-the-matrix-exponential-and-logarithm">Identities for the matrix exponential and logarithm</h3>
<p>The above give rise to useful identities:</p>
<ol type="1">
<li><p>For any symmetric matrix <span class="math inline">\(\boldsymbol A\)</span> we have <span class="math display">\[
\det(\exp(\boldsymbol A)) = \exp(\text{Tr}(\boldsymbol A))
\]</span> because <span class="math inline">\(\prod_i \exp(\lambda_i) = \exp( \sum_i \lambda_i)\)</span> where <span class="math inline">\(\lambda_i\)</span> are the eigenvalues of <span class="math inline">\(\boldsymbol A\)</span>.</p></li>
<li><p>If we take the logarithm on both sides and replace <span class="math inline">\(\exp(\boldsymbol A)=\boldsymbol B\)</span> we get another identity for a symmetric positive definite matrix <span class="math inline">\(\boldsymbol B\)</span>: <span class="math display">\[
\log \det(\boldsymbol B) = \text{Tr}(\log(\boldsymbol B))
\]</span> because <span class="math inline">\(\log( \prod_i \lambda_i)  = \sum_i \log(\lambda_i)\)</span> where <span class="math inline">\(\lambda_i\)</span> are the eigenvalues of <span class="math inline">\(\boldsymbol B\)</span>.</p></li>
</ol>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Mardia, K. V., J. T. Kent and J. M. Bibby. 1979. <em>Multivariate Analysis</em>. Academic Press.<a href="#fnref1" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn2"><p>Schur, I. 1923. Ãœber eine Klasse von Mittelbildungen mit Anwendungen auf die Determinantentheorie. Sitzungsber. Berl. Math. Ges. <strong>22</strong>:9â€“29.<a href="#fnref2" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/matrix-calculus-refresher\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./00-preface.html" class="pagination-link" aria-label="Preface">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./02-vector-calculus.html" class="pagination-link" aria-label="Vector and matrix calculus">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Vector and matrix calculus</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>