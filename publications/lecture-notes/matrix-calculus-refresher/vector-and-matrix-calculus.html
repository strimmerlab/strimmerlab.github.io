<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>2 Vector and matrix calculus | Matrix and Calculus Refresher</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.38 with bs4_book()">
<meta property="og:title" content="2 Vector and matrix calculus | Matrix and Calculus Refresher">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="2 Vector and matrix calculus | Matrix and Calculus Refresher">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="2.1 First order vector derivatives  2.1.1 Derivative and gradient The derivative of a scalar-valued function \(h(\boldsymbol x)\) with regard to its vector argument \(\boldsymbol x= (x_1, \ldots,...">
<meta property="og:description" content="2.1 First order vector derivatives  2.1.1 Derivative and gradient The derivative of a scalar-valued function \(h(\boldsymbol x)\) with regard to its vector argument \(\boldsymbol x= (x_1, \ldots,...">
<meta name="twitter:description" content="2.1 First order vector derivatives  2.1.1 Derivative and gradient The derivative of a scalar-valued function \(h(\boldsymbol x)\) with regard to its vector argument \(\boldsymbol x= (x_1, \ldots,...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Matrix and Calculus Refresher</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="matrix-essentials.html"><span class="header-section-number">1</span> Matrix essentials</a></li>
<li><a class="active" href="vector-and-matrix-calculus.html"><span class="header-section-number">2</span> Vector and matrix calculus</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="vector-and-matrix-calculus" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Vector and matrix calculus<a class="anchor" aria-label="anchor" href="#vector-and-matrix-calculus"><i class="fas fa-link"></i></a>
</h1>
<div id="first-order-vector-derivatives" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> First order vector derivatives<a class="anchor" aria-label="anchor" href="#first-order-vector-derivatives"><i class="fas fa-link"></i></a>
</h2>
<div id="derivative-and-gradient" class="section level3" number="2.1.1">
<h3>
<span class="header-section-number">2.1.1</span> Derivative and gradient<a class="anchor" aria-label="anchor" href="#derivative-and-gradient"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>derivative</strong> of a scalar-valued function
<span class="math inline">\(h(\boldsymbol x)\)</span> with regard to its vector argument <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_d)^T\)</span>
is the <em>row vector</em>
<span class="math display">\[
D h(\boldsymbol x) =  \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x}
= \begin{pmatrix}
\frac{\partial h(\boldsymbol x)}{\partial x_1} &amp;
\cdots &amp;
\frac{\partial h(\boldsymbol x)}{\partial x_d} \\
\end{pmatrix}
\]</span></p>
<p>The above notation follows the <strong>numerator layout</strong> convention, where the dimension
of the numerator (here 1) determines the number of rows and the dimension of the denominator
(here <span class="math inline">\(d\)</span>) the number of columns of the resulting matrix
(see <a href="https://en.wikipedia.org/wiki/Matrix_calculus" class="uri">https://en.wikipedia.org/wiki/Matrix_calculus</a> for details). For a scalar
function this results in a vector of the same dimension as <span class="math inline">\(\boldsymbol x^T\)</span>.</p>
<p>The <strong>gradient</strong> of <span class="math inline">\(h(\boldsymbol x)\)</span> is a <em>column vector</em> and the transpose of the derivative
<span class="math display">\[
\text{grad } h(\boldsymbol x) = \left( \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} \right)^T \\
\]</span>
It is often written using the <strong>nabla operator</strong> <span class="math inline">\(\nabla\)</span> as
<span class="math display">\[
\nabla h(\boldsymbol x) = \begin{pmatrix}
\frac{\partial h(\boldsymbol x)}{\partial x_1} \\
\vdots\\
\frac{\partial h(\boldsymbol x)}{\partial x_d}
\end{pmatrix}
\]</span>
with
<span class="math display">\[
\nabla = \begin{pmatrix}
\frac{\partial }{\partial x_1} \\
\vdots\\
\frac{\partial }{\partial x_d}
\end{pmatrix}
\]</span></p>
<p>Note that
<span class="math display">\[
(\nabla h(\boldsymbol x))^T = \nabla^T h(\boldsymbol x) = D h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x}
\]</span></p>
<div class="example">
<p><span id="exm:gradientexamples" class="example"><strong>Example 2.1  </strong></span>Examples for the gradient and derivative:</p>
<ul>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol a^T \boldsymbol x+ b\)</span>.<br>
Then <span class="math inline">\(\nabla h(\boldsymbol x) = \boldsymbol a\)</span> and <span class="math inline">\(D h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol a^T\)</span>.</li>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol x^T \boldsymbol x\)</span>.<br>
Then <span class="math inline">\(\nabla h(\boldsymbol x) = 2 \boldsymbol x\)</span> and <span class="math inline">\(D h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = 2 \boldsymbol x^T\)</span>.</li>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol x^T \boldsymbol A\boldsymbol x\)</span>.<br>
Then <span class="math inline">\(\nabla h(\boldsymbol x) = (\boldsymbol A+ \boldsymbol A^T) \boldsymbol x\)</span> and
<span class="math inline">\(D h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol x^T (\boldsymbol A+ \boldsymbol A^T)\)</span>.</li>
</ul>
</div>
</div>
<div id="jacobian-matrix" class="section level3" number="2.1.2">
<h3>
<span class="header-section-number">2.1.2</span> Jacobian matrix<a class="anchor" aria-label="anchor" href="#jacobian-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>Similarly, we can also compute the <strong>derivative</strong> of
a vector-valued function
<span class="math display">\[
\boldsymbol h(\boldsymbol x) = ( h_1(\boldsymbol x), \ldots, h_m(\boldsymbol x) )^T
\]</span>
with regard to <span class="math inline">\(\boldsymbol x\)</span>. This yields (again in numerator layout convention) a matrix of size <span class="math inline">\(m\)</span> rows and <span class="math inline">\(d\)</span> columns
whose rows contain the derivatives of the components of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span>.
<span class="math display">\[
\begin{split}
D\boldsymbol h(\boldsymbol x) &amp;= \frac{\partial \boldsymbol h(\boldsymbol x)}{\partial \boldsymbol x}  = \left(\frac{\partial h_i(\boldsymbol x)}{\partial x_j}\right) \\
&amp;=\begin{pmatrix}
\frac{\partial h_1(\boldsymbol x)}{\partial x_1} &amp; \cdots  &amp; \frac{\partial h_1(\boldsymbol x)}{\partial x_d} \\
\vdots &amp;\ddots &amp; \vdots \\
\frac{\partial h_m(\boldsymbol x)}{\partial x_1} &amp; \cdots &amp; \frac{\partial h_m(\boldsymbol x)}{\partial x_d} \\
\end{pmatrix} \\
&amp;=
\left( {\begin{array}{c}
\nabla^T h_1(\boldsymbol x)   \\
\vdots   \\
\nabla^T h_m(\boldsymbol x)  \\
\end{array} } \right) \\
&amp;= \boldsymbol J_{\boldsymbol h}(\boldsymbol x)
\end{split}
\]</span></p>
<p>This matrix is also called the <strong>Jacobian matrix</strong></p>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 2.2  </strong></span><span class="math inline">\(\boldsymbol h(\boldsymbol x)=\boldsymbol A^T \boldsymbol x+ \boldsymbol b\)</span>. Then <span class="math inline">\(D \boldsymbol h(\boldsymbol x) = \frac{\partial \boldsymbol h(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol J_{\boldsymbol h}(\boldsymbol x) =\boldsymbol A^T\)</span>.</p>
</div>
<p>If <span class="math inline">\(m=d\)</span> then the Jacobian matrix is a square and this
allows to compute the <strong>determinant of the Jacobian matrix</strong>.
Both the Jacobian matrix and the Jacobian determinant are often called “the Jacobian”
so one needs to determine from the context whether this refers to the matrix or the determinant.</p>
<p>If <span class="math inline">\(\boldsymbol y= \boldsymbol h(\boldsymbol x)\)</span> is an invertible function with <span class="math inline">\(\boldsymbol x= \boldsymbol h^{-1}(\boldsymbol y)\)</span>
then the Jacobian matrix is invertible and the inverted matrix is the
Jacobian matrix of the inverse function:
<span class="math display">\[
D\boldsymbol x(\boldsymbol y) = \left( D\boldsymbol y(\boldsymbol x)\right)^{-1} \rvert_{\boldsymbol x= \boldsymbol x(\boldsymbol y)}
\]</span>
or in alternative notation
<span class="math display">\[
\frac{\partial \boldsymbol x(\boldsymbol y)}{\partial \boldsymbol y} = \left. \left( \frac{\partial \boldsymbol y(\boldsymbol x)}{\partial \boldsymbol x}  \right)^{-1} \right\rvert_{\boldsymbol x= \boldsymbol x(\boldsymbol y)}
\]</span></p>
<p>In this case the Jacobian determinant of the backtransformation can be computed
as the inverse of the Jacobian determinant of the original function:
<span class="math display">\[
\det D\boldsymbol x(\boldsymbol y) = \det \left( D\boldsymbol y(\boldsymbol x)\right)^{-1} \rvert_{\boldsymbol x= \boldsymbol x(\boldsymbol y)}
\]</span>
and
<span class="math display">\[
\det \left(\frac{\partial \boldsymbol x(\boldsymbol y)}{\partial \boldsymbol y}\right) = \left. \det \left( \frac{\partial \boldsymbol y(\boldsymbol x)}{\partial \boldsymbol x}  \right)^{-1} \right\rvert_{\boldsymbol x= \boldsymbol x(\boldsymbol y)}
\]</span></p>
</div>
</div>
<div id="second-order-vector-derivatives" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Second order vector derivatives<a class="anchor" aria-label="anchor" href="#second-order-vector-derivatives"><i class="fas fa-link"></i></a>
</h2>
<p>The matrix of all second order partial derivates of scalar-valued
function with vector-valued argument is called the <strong>Hessian matrix</strong>:
<span class="math display">\[
\begin{split}
\nabla \nabla^T h(\boldsymbol x) &amp;=
\begin{pmatrix}
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_1^2}
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_1 \partial x_2}
     &amp; \cdots
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_1 \partial x_d} \\
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_2 \partial x_1}
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_2^2}
     &amp; \cdots
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_2 \partial x_d} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_d \partial x_1}
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_d \partial x_2}  
     &amp; \cdots
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_d^2}
\end{pmatrix} \\
&amp;= \left(\frac{\partial h(\boldsymbol x)}{\partial x_i \partial x_j}\right) \\
&amp; =   \frac{\partial }{\partial \boldsymbol x} \left( \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x}\right)^T \\
&amp; = D (Dh(\boldsymbol x))^T \\
\end{split}
\]</span>
By construction the Hessian matrix is square and symmetric.</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 2.3  </strong></span><span class="math inline">\(h(\boldsymbol x)=\boldsymbol x^T \boldsymbol A\boldsymbol x\)</span>. Then <span class="math inline">\(\nabla \nabla^T h(\boldsymbol x) = (\boldsymbol A+ \boldsymbol A^T)\)</span>.</p>
</div>
</div>
<div id="chain-rules-for-gradient-vector-and-hessian-matrix" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Chain rules for gradient vector and Hessian matrix<a class="anchor" aria-label="anchor" href="#chain-rules-for-gradient-vector-and-hessian-matrix"><i class="fas fa-link"></i></a>
</h2>
<p>Suppose <span class="math inline">\(h(\boldsymbol x)\)</span> is a scalar-valued function and <span class="math inline">\(g(\boldsymbol y) = h(\boldsymbol x(\boldsymbol y))\)</span>
is a composite scalar-valued function where <span class="math inline">\(\boldsymbol x(\boldsymbol y)\)</span> a map from <span class="math inline">\(\boldsymbol y\)</span> to <span class="math inline">\(\boldsymbol x\)</span>.</p>
<p>The gradient of the composite function <span class="math inline">\(g(\boldsymbol y) = h(\boldsymbol x(\boldsymbol y))\)</span> can be
computed from the gradient of <span class="math inline">\(h(\boldsymbol x)\)</span> and
the Jacobian matrix for <span class="math inline">\(\boldsymbol x(\boldsymbol y)\)</span> as follows:
<span class="math display">\[
\nabla g(\boldsymbol y)  =   (D\boldsymbol x(\boldsymbol y))^T\,  \nabla h(\boldsymbol x)   \rvert_{\boldsymbol x= \boldsymbol x(\boldsymbol y)}
\]</span>
and
<span class="math display">\[
\nabla g(\boldsymbol y)  =   \left(\frac{\partial \boldsymbol x(\boldsymbol y)}{\partial \boldsymbol y}\right)^T\,  \nabla h(\boldsymbol x)   \rvert_{\boldsymbol x= \boldsymbol x(\boldsymbol y)}
\]</span></p>
<p>Similarly, the Hessian matrix of <span class="math inline">\(g(\boldsymbol y)\)</span> can be computed from the Hessian
of <span class="math inline">\(h(\boldsymbol x)\)</span> and the Jacobian matrix for <span class="math inline">\(\boldsymbol x(\boldsymbol y)\)</span>:
<span class="math display">\[
\nabla \nabla^T g(\boldsymbol y) = (D \boldsymbol x(\boldsymbol y))^T\, \nabla \nabla^T h(\boldsymbol x)\rvert_{\boldsymbol x= \boldsymbol x(\boldsymbol y)}  \,   D\boldsymbol x(\boldsymbol y)
\]</span>
and
<span class="math display">\[
\nabla \nabla^T g(\boldsymbol y) = \left(\frac{\partial \boldsymbol x(\boldsymbol y)}{\partial \boldsymbol y}\right)^T\,
\nabla \nabla^T h(\boldsymbol x)\rvert_{\boldsymbol x= \boldsymbol x(\boldsymbol y)}  \,  \frac{\partial \boldsymbol x(\boldsymbol y)}{\partial \boldsymbol y}
\]</span></p>
</div>
<div id="first-order-matrix-derivatives" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> First order matrix derivatives<a class="anchor" aria-label="anchor" href="#first-order-matrix-derivatives"><i class="fas fa-link"></i></a>
</h2>
<p>The derivative of a scalar-valued function <span class="math inline">\(h(\boldsymbol X)\)</span> with regard to a matrix argument <span class="math inline">\(\boldsymbol X= (x_{ij})\)</span>
is defined as below and and results (in numerator layout convention) in a matrix of the same dimension as <span class="math inline">\(\boldsymbol X^T\)</span>:
<span class="math display">\[
D h(\boldsymbol X) = \frac{\partial h(\boldsymbol X)}{\partial \boldsymbol X}  = \left(\frac{\partial h(\boldsymbol X)}{\partial x_{ji}}\right)
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 2.4  </strong></span><span class="math inline">\(\frac{\partial \text{Tr}(\boldsymbol A^T \boldsymbol X)}{\partial \boldsymbol X} = \boldsymbol A^T\)</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example 2.5  </strong></span><span class="math inline">\(\frac{\partial \text{Tr}(\boldsymbol A^T \boldsymbol X\boldsymbol B)}{\partial \boldsymbol X} = \boldsymbol B\boldsymbol A^T\)</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 2.6  </strong></span><span class="math inline">\(\frac{\partial \text{Tr}(\boldsymbol X^T \boldsymbol A\boldsymbol X)}{\partial \boldsymbol X} = \boldsymbol X^T (\boldsymbol A+ \boldsymbol A^T)\)</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example 2.7  </strong></span><span class="math inline">\(\frac{\partial \log \det(\boldsymbol X)}{\partial \boldsymbol X} = \frac{\partial \text{Tr}(\log \boldsymbol X)}{\partial \boldsymbol X} = \boldsymbol X^{-1}\)</span></p>
</div>
</div>
<div id="linear-and-quadratic-approximation" class="section level2" number="2.5">
<h2>
<span class="header-section-number">2.5</span> Linear and quadratic approximation<a class="anchor" aria-label="anchor" href="#linear-and-quadratic-approximation"><i class="fas fa-link"></i></a>
</h2>
<p>A linear and quadratic approximation of a differentiable function is
given by a Taylor series of first and second order, respectively.</p>
<ol style="list-style-type: lower-roman">
<li><p>Linear and quadratic approximation of a scalar-valued function of a scalar:
<span class="math display">\[
h(x) \approx h(x_0) + h'(x_0) \, (x-x_0) + \frac{1}{2} h''(x_0) \, (x-x_0)^2
\]</span>
Note that <span class="math inline">\(h'(x_0) = h'(x) \,|\, x_0\)</span> is first derivative of <span class="math inline">\(h(x)\)</span> evaluated at <span class="math inline">\(x_0\)</span> and
<span class="math inline">\(h''(x_0) = h''(x) \,|\, x_0\)</span> is the second derivative of <span class="math inline">\(h(x)\)</span> evaluated <span class="math inline">\(x_0\)</span>.<br>
With <span class="math inline">\(x = x_0+ \varepsilon\)</span> the approximation can also be written as
<span class="math display">\[
h(x_0+ \varepsilon) \approx h(x_0) + h'(x_0) \, \varepsilon + \frac{1}{2} h''(x_0)\, \varepsilon^2
\]</span>
The first two terms on the right comprise the linear approximation, all three terms the quadratic approximation.</p></li>
<li><p>Linear and quadratic approximation of a scalar-valued function of a vector:
<span class="math display">\[
h(\boldsymbol x) \approx h(\boldsymbol x_0) + \nabla^T h(\boldsymbol x_0)\, (\boldsymbol x-\boldsymbol x_0) + \frac{1}{2}
(\boldsymbol x-\boldsymbol x_0)^T \, \nabla \nabla^T h(\boldsymbol x_0) \, (\boldsymbol x-\boldsymbol x_0)
\]</span>
Note that <span class="math inline">\(\nabla^T h(\boldsymbol x_0)\)</span> is the transposed gradient (i.e the vector derivative)
of <span class="math inline">\(h(\boldsymbol x)\)</span> evaluated at <span class="math inline">\(\boldsymbol x_0\)</span> and <span class="math inline">\(\nabla \nabla^T h(\boldsymbol x_0)\)</span> the Hessian matrix of <span class="math inline">\(h(\boldsymbol x)\)</span> evaluated at <span class="math inline">\(\boldsymbol x_0\)</span>.
With <span class="math inline">\(\boldsymbol x= \boldsymbol x_0+ \boldsymbol \varepsilon\)</span> this approximation can also be written as
<span class="math display">\[
h(\boldsymbol x_0+ \boldsymbol \varepsilon) \approx h(\boldsymbol x_0) + \nabla^T h(\boldsymbol x_0)\, \boldsymbol \varepsilon+ \frac{1}{2} \boldsymbol \varepsilon^T \, \nabla \nabla^T h(\boldsymbol x_0) \,\boldsymbol \varepsilon
\]</span>
The first two terms on the right comprise the linear approximation, all three terms the quadratic approximation.</p></li>
<li><p>Linear approximation of a vector-valued function of a vector:
<span class="math display">\[
\boldsymbol h(\boldsymbol x) \approx \boldsymbol h(\boldsymbol x_0) + D \boldsymbol h(\boldsymbol x_0) \, (\boldsymbol x-\boldsymbol x_0)
\]</span>
Note that <span class="math inline">\(D \boldsymbol h(\boldsymbol x_0)\)</span> is Jacobian matrix (i.e the vector derivative)
of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> evaluated at <span class="math inline">\(\boldsymbol x_0\)</span>.
With <span class="math inline">\(\boldsymbol x= \boldsymbol x_0+ \boldsymbol \varepsilon\)</span> this approximation can also be written as
<span class="math display">\[
\boldsymbol h(\boldsymbol x_0+ \boldsymbol \varepsilon) \approx h(\boldsymbol x_0) + D \boldsymbol h(\boldsymbol x_0) \, \boldsymbol \varepsilon
\]</span></p></li>
</ol>
<div class="example">
<p><span id="exm:taylorexamples" class="example"><strong>Example 2.8  </strong></span>Examples of Taylor series approximations of second order:</p>
<ul>
<li><span class="math inline">\(\log(x_0+\varepsilon) \approx \log(x_0) + \frac{\varepsilon}{x_0} - \frac{\varepsilon^2}{2 x_0^2}\)</span></li>
<li><span class="math inline">\(\frac{x_0}{x_0+\varepsilon} \approx 1 - \frac{\varepsilon}{x_0} + \frac{\varepsilon^2}{ x_0^2}\)</span></li>
</ul>
</div>
<div class="example">
<p><span id="exm:approxextremum" class="example"><strong>Example 2.9  </strong></span>Around a local extremum <span class="math inline">\(\boldsymbol x_0\)</span> (maximum or minimum) where the gradient vanishes (<span class="math inline">\(h(\boldsymbol x_0) = 0\)</span>)
the quadratic approximation of the function <span class="math inline">\(h(\boldsymbol x)\)</span> simplifies to<br><span class="math display">\[
h(\boldsymbol x_0+ \boldsymbol \varepsilon) \approx h(\boldsymbol x_0) +  \frac{1}{2} \boldsymbol \varepsilon^T \nabla \nabla^T h(\boldsymbol x_0) \boldsymbol \varepsilon
\]</span></p>
</div>
</div>
<div id="conditions-for-a-local-extremum-of-a-function" class="section level2" number="2.6">
<h2>
<span class="header-section-number">2.6</span> Conditions for a local extremum of a function<a class="anchor" aria-label="anchor" href="#conditions-for-a-local-extremum-of-a-function"><i class="fas fa-link"></i></a>
</h2>
<p>To check if <span class="math inline">\(x_0\)</span> or <span class="math inline">\(\boldsymbol x_0\)</span> is a local extremum, i.e. a local maximum or a local minimum,
of a differentiable function <span class="math inline">\(h(x)\)</span> or <span class="math inline">\(h(\boldsymbol x)\)</span> we can use the following conditions:</p>
<p>For a function of a single variable:</p>
<ol style="list-style-type: lower-roman">
<li>First derivative is zero at the extremum: <span class="math inline">\(h'(x_0) = 0\)</span>.</li>
<li>If the second derivative <span class="math inline">\(h''(x_0) &lt; 0\)</span> at the extremum is negative then it is a maximum.</li>
<li>If the second derivative <span class="math inline">\(h''(x_0) &gt; 0\)</span> at the extremum is positive it is a minimum.</li>
</ol>
<p>Note that conditions ii) and iii) are sufficient but not necessary. For a minimum, it is necessary that the second derivative is non-negative, and for a maximum that the second derivative is non-positive.</p>
<p>For a function of several variables:</p>
<ol style="list-style-type: lower-roman">
<li>Gradient vanishes at extremum: <span class="math inline">\(\nabla h(\boldsymbol x_0)=0\)</span>.</li>
<li>If the Hessian <span class="math inline">\(\nabla \nabla^T h(\boldsymbol x_0)\)</span> is negative definite (= all eigenvalues of Hessian matrix are negative) then the extremum is a maximum.</li>
<li>If the Hessian is positive definite (= all eigenvalues of Hessian matrix are positive) then the extremum is a minimum.</li>
</ol>
<p>Again, conditions ii) and iii) are sufficient but not necessary. For a minimum
it is necessary that the Hessian is positive semi-definite, and for a maximum
that the Hessian is negative semi-definite.</p>
<div class="example">
<p><span id="exm:minzerocurvature" class="example"><strong>Example 2.10  </strong></span>Minimum with vanishing second derivative:</p>
<p><span class="math inline">\(x^4\)</span> clearly has a minimum at <span class="math inline">\(x_0=0\)</span>. As required the first derivative <span class="math inline">\(4 x^3\)</span> vanishes
at <span class="math inline">\(x_0=0\)</span>. However, the second derivative <span class="math inline">\(12 x^2\)</span> also vanishes at <span class="math inline">\(x_0=0\)</span>, showing
that a positive second derivative is not necessary for a minimum.</p>
</div>
</div>
<div id="convex-and-concave-functions" class="section level2" number="2.7">
<h2>
<span class="header-section-number">2.7</span> Convex and concave functions<a class="anchor" aria-label="anchor" href="#convex-and-concave-functions"><i class="fas fa-link"></i></a>
</h2>
<p>A function <span class="math inline">\(h(\boldsymbol x)\)</span> is <strong>convex</strong> if for all <span class="math inline">\(\boldsymbol x_1\)</span> and <span class="math inline">\(\boldsymbol x_2\)</span> the
line segment from point <span class="math inline">\((\boldsymbol x_1, h(\boldsymbol x_1))\)</span>
to point <span class="math inline">\((\boldsymbol x_2, h(\boldsymbol x_2))\)</span> never lies below the function. Moreover, the function is strictly convex if the line segment always lies above the curve, apart from the two end points:<br><span class="math display">\[
\lambda h(\boldsymbol x_1) + (1-\lambda) h(\boldsymbol x_2) \geq h(\lambda \boldsymbol x_1 + (1-\lambda) \boldsymbol x_2)
\]</span>
for all <span class="math inline">\(\lambda \in [0, 1]\)</span>.</p>
<p>Equivalently, a differentiable function <span class="math inline">\(h(\boldsymbol x)\)</span> is convex (strictly convex) if for all <span class="math inline">\(\boldsymbol x_0\)</span> the function <span class="math inline">\(h(\boldsymbol x)\)</span> never lies below (always lies above, except at <span class="math inline">\(\boldsymbol x_0\)</span>) the linear approximation through the point <span class="math inline">\((\boldsymbol x_0, h(\boldsymbol x_0))\)</span>:
<span class="math display">\[
h(\boldsymbol x) \geq h(\boldsymbol x_0) + \nabla^T h(\boldsymbol x_0)\, (\boldsymbol x-\boldsymbol x_0)
\]</span></p>
<p><strong>For a convex function a vanishing gradient at <span class="math inline">\(\boldsymbol x_0\)</span> indicates a minimum at <span class="math inline">\(\boldsymbol x_0\)</span>.</strong>
Furthermore, any local minimum must also be a global minimum (for a differentiable function this follows directly from the last inequality).
For a strictly convex function the minimum is unique so there is at most one local/global minimum in that case.</p>
<p>If <span class="math inline">\(h(\boldsymbol x)\)</span> is convex, then <span class="math inline">\(-h(\boldsymbol x)\)</span> is <strong>concave</strong>, and the criteria above can be
adapted accordingly to check for concavity and strict concavity, as well as to identify
local/global maxima.</p>
<p>(Strictly) convex and concave functions are convenient objective functions in optimisation as it is straightforward to find their local/global extrema, both analytically and numerically.</p>
<p>As the shape of a convex function resembles that of a valley, one way to memorise that fact is that a <strong>va</strong>lley is con<strong>ve</strong>x.</p>
<div class="example">
<p><span id="exm:convexexample" class="example"><strong>Example 2.11  </strong></span>Convex functions:</p>
<p>This is a convex function but not a strictly convex function:</p>
<ul>
<li><span class="math inline">\(\max(x^2, | x | )\)</span></li>
</ul>
<p>The following are strictly convex functions:</p>
<ul>
<li>
<span class="math inline">\(x^2\)</span>,</li>
<li>
<span class="math inline">\(x^4\)</span>,</li>
<li>
<span class="math inline">\(e^x\)</span>,</li>
<li>
<span class="math inline">\(x \log(x)\)</span> for <span class="math inline">\(x&gt;0\)</span>.</li>
</ul>
<p>On the other hand, this is not a convex function:</p>
<ul>
<li>
<span class="math inline">\(\frac{1}{x^2}\)</span> for all <span class="math inline">\(x \neq 0\)</span>.</li>
</ul>
<p>However, the function in last example is strictly convex if the domain is restricted
to either <span class="math inline">\(x &gt;0\)</span> or <span class="math inline">\(x&lt;0\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:concaveexample" class="example"><strong>Example 2.12  </strong></span>Concave functions:</p>
<p>The following are strictly concave functions:</p>
<ul>
<li>
<span class="math inline">\(-x^2\)</span>,</li>
<li>
<span class="math inline">\(\log(x)\)</span> for <span class="math inline">\(x&gt;0\)</span>,</li>
<li>
<span class="math inline">\(\sqrt{x}\)</span> for <span class="math inline">\(x&gt;0\)</span>.</li>
</ul>
</div>

</div>
</div>







  <div class="chapter-nav">
<div class="prev"><a href="matrix-essentials.html"><span class="header-section-number">1</span> Matrix essentials</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#vector-and-matrix-calculus"><span class="header-section-number">2</span> Vector and matrix calculus</a></li>
<li>
<a class="nav-link" href="#first-order-vector-derivatives"><span class="header-section-number">2.1</span> First order vector derivatives</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#derivative-and-gradient"><span class="header-section-number">2.1.1</span> Derivative and gradient</a></li>
<li><a class="nav-link" href="#jacobian-matrix"><span class="header-section-number">2.1.2</span> Jacobian matrix</a></li>
</ul>
</li>
<li><a class="nav-link" href="#second-order-vector-derivatives"><span class="header-section-number">2.2</span> Second order vector derivatives</a></li>
<li><a class="nav-link" href="#chain-rules-for-gradient-vector-and-hessian-matrix"><span class="header-section-number">2.3</span> Chain rules for gradient vector and Hessian matrix</a></li>
<li><a class="nav-link" href="#first-order-matrix-derivatives"><span class="header-section-number">2.4</span> First order matrix derivatives</a></li>
<li><a class="nav-link" href="#linear-and-quadratic-approximation"><span class="header-section-number">2.5</span> Linear and quadratic approximation</a></li>
<li><a class="nav-link" href="#conditions-for-a-local-extremum-of-a-function"><span class="header-section-number">2.6</span> Conditions for a local extremum of a function</a></li>
<li><a class="nav-link" href="#convex-and-concave-functions"><span class="header-section-number">2.7</span> Convex and concave functions</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Matrix and Calculus Refresher</strong>" was written by Korbinian Strimmer. It was last built on 5 March 2024.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
