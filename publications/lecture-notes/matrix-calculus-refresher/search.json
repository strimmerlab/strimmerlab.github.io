[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matrix and Calculus Refresher",
    "section": "",
    "text": "Welcome\nThe Matrix and Calculus Refresher notes were written by Korbinian Strimmer from 2018–2024. This version is from 3 June 2024.\nIf you have any questions, comments, or corrections please get in touch! 1",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Matrix and Calculus Refresher",
    "section": "Updates",
    "text": "Updates\nThe notes will be updated from time to time.\nThe most current version is found at the web page for the\n\nonline version of the Matrix and Calculus Refresher notes.\n\nThere you can also download the download the Matrix and Calculus Refresher notes as\n\nPDF in A4 format for printing (double page layout), or as\n6x9 inch PDF for use on tablets (single page layout).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Matrix and Calculus Refresher",
    "section": "License",
    "text": "License\nThese notes are licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Matrix and Calculus Refresher",
    "section": "",
    "text": "Email address: korbinian.strimmer@manchester.ac.uk↩︎",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "00-preface.html",
    "href": "00-preface.html",
    "title": "Preface",
    "section": "",
    "text": "About the author\nHello! My name is Korbinian Strimmer and I am a Professor in Statistics. I am a member of the Statistics group at the Department of Mathematics of the University of Manchester. You can find more information about me on my home page.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#about-the-notes",
    "href": "00-preface.html#about-the-notes",
    "title": "Preface",
    "section": "About the notes",
    "text": "About the notes\nIn statistics and machine learning we make frequent use of matrix notation, matrix algebra, matrix decompositions and also of vector and matrix calculus. The aim of these supplementary notes is to provide a refresher for students to quickly gain a working knowledge about matrices and the calculus of functions with several variables.\nThe notes are supporting information for a number of lecture notes of statistical courses I am or have been teaching at the Department of Mathematics of the University of Manchester.\nThis includes the currently offered modules:\n\nMATH27720 Statistics 2: Likelihood and Bayes and\nMATH38161 Multivariate Statistics,\n\nas well as the retired module (not offered any more):\n\nMATH20802 Statistical Methods.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html",
    "href": "01-matrix-essentials.html",
    "title": "1  Matrix essentials",
    "section": "",
    "text": "1.1 Overview\nIn statistics we will frequently make use of matrix calculations and matrix notation.\nThroughout we mostly work with real matrices, i.e. we assume all matrix elements are real numbers. However, one important matrix decomposition — the eigenvalue decomposition — can yield complex-valued matrices even when applied to real matrices. Thus occasionally we will also need to deal also with complex numbers.\nFor further details on matrix theory please consult the lecture notes of related modules (e.g. linear algebra).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#matrix-basics",
    "href": "01-matrix-essentials.html#matrix-basics",
    "title": "1  Matrix essentials",
    "section": "1.2 Matrix basics",
    "text": "1.2 Matrix basics\n\nMatrix notation\nIn matrix notation we distinguish between scalars, vectors, and matrices:\nScalar: \\(x\\), \\(X\\), lower or upper case, plain type.\nVector: \\(\\boldsymbol x\\), lower case, bold type. In handwriting an arrow \\(\\vec{x}\\) indicates a vector.\nIn component notation we write \\(\\boldsymbol x= \\begin{pmatrix} x_1 \\\\ \\vdots\\\\ x_d\\end{pmatrix}\\). By default, a vector is a column vector, i.e. the elements are arranged in a column and index of the components \\(x_i\\) refers to the row.\nThe transpose of a vector (indicated by the superscript \\(T\\)) turns it into a row vector. To save space we can write the column vector \\(\\boldsymbol x\\) as \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\) so that \\(\\boldsymbol x^T\\) is a row vector.\nMatrix: \\(\\boldsymbol X\\), upper case, bold type. In handwriting an underscore \\(\\underline{X}\\) indicates a matrix.\nIn component notation we write \\(\\boldsymbol X= (x_{ij})\\). By convention, the first index (here \\(i\\)) of the scalar elements \\(x_{ij}\\) denotes the row and the second index (here \\(j\\)) the column of the matrix. For \\(n\\) the number of rows and \\(d\\) the number of columns we can view the matrix \\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_d)\\) either as being composed of \\(d\\) column vectors \\(\\boldsymbol x_j = \\begin{pmatrix} x_{1j} \\\\ \\vdots \\\\ x_{nj}\\\\ \\end{pmatrix}\\) or \\(\\boldsymbol X= \\begin{pmatrix} \\boldsymbol z_1^T \\\\ \\vdots \\\\ \\boldsymbol z_n^T\\end{pmatrix}\\) being composed of \\(n\\) row vectors \\(\\boldsymbol z_i^T = (x_{i1},  \\ldots,  x_{id})\\).\nA (column) vector of dimension \\(d\\) is a matrix of size \\(d\\times 1\\). A row vector of dimension \\(d\\) is a matrix of size \\(1\\times d\\). A scalar is of dimension \\(1\\) and is a matrix of size \\(1 \\times 1\\).\n\n\nNotation for random vectors and matrices\nA random matrix (vector) is a matrix (vector) whose elements are random variables.\nIt is common practise in univariate statistics to distinguish random variables and their fixed realisations by using upper case versus lower case. However, this convention breaks down when working with matrices and vectors.\nTherefore, when working with multivariate random quantities it is best practise (see e.g. Mardia et al. 1979)1 to always state explicitly whether a matrix, vector or scalar is a random variable, especially when this is not obvious from the context.\n\n\nSpecial matrices\n\\(\\boldsymbol I_d\\) is the identity matrix. It is a square matrix of size \\(d \\times d\\) with the diagonal filled with 1 and off-diagonals filled with 0. \\[\\boldsymbol I_d =\n\\begin{pmatrix}\n    1 & 0 & 0 & \\dots & 0\\\\\n    0 & 1 & 0 & \\dots & 0\\\\\n    0 & 0 & 1 &   & 0\\\\\n    \\vdots & \\vdots & & \\ddots &  \\\\\n    0 & 0 & 0 &  & 1 \\\\\n\\end{pmatrix}\\]\n\\(\\mathbf 1\\) is a matrix that contains only ones. Most often it is used in the form of a column vector with \\(d\\) rows: \\[\\mathbf 1_d =\n\\begin{pmatrix}\n    1 \\\\\n    1 \\\\\n    1 \\\\\n    \\vdots   \\\\\n    1  \\\\\n\\end{pmatrix}\\]\nSimilarly, \\(\\mathbf 0\\) is a matrix that contains only zeros. Most often it is used in the form of a column vector with \\(d\\) rows: \\[\\mathbf 0_d =\n\\begin{pmatrix}\n    0 \\\\\n    0 \\\\\n    0 \\\\\n    \\vdots   \\\\\n    0  \\\\\n\\end{pmatrix}\\]\nA diagonal matrix is a matrix where all off-diagonal elements are zero. By \\(\\text{Diag}(\\boldsymbol A)\\) we access the diagonal elements of a matrix as vector and by \\(\\text{Diag}(a_1, \\ldots, a_d)\\) we specify a diagonal matrix by listing the diagonal elements.\nAny matrix can be partitioned into blocks or sub-matrices. A block-structured matrix or block matrix partitioning rows and columns into two groups has the form \\[\n\\boldsymbol A= \\begin{pmatrix} \\boldsymbol A_{11} & \\boldsymbol A_{12} \\\\ \\boldsymbol A_{21} & \\boldsymbol A_{22} \\\\ \\end{pmatrix} \\, ,\n\\] where \\(\\boldsymbol A_{11}\\), \\(\\boldsymbol A_{22}\\), \\(\\boldsymbol A_{12}\\) and \\(\\boldsymbol A_{21}\\) are themselves matrices. If \\(\\boldsymbol A\\) is symmetric (hence square) then \\(\\boldsymbol A_{11}\\) and \\(\\boldsymbol A_{22}\\) must also be symmetric and \\(\\boldsymbol A_{21} = \\boldsymbol A_{12}^T\\).\nA block diagonal matrix is a symmetric block matrix with vanishing off-diagonal blocks and symmetric sub-matrices along the diagonal.\nA triangular matrix is a square matrix whose elements either below or above the diagonal are all zero (upper vs. lower triangular matrix).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#simple-matrix-operations",
    "href": "01-matrix-essentials.html#simple-matrix-operations",
    "title": "1  Matrix essentials",
    "section": "1.3 Simple matrix operations",
    "text": "1.3 Simple matrix operations\n\nMatrix addition and multiplication\nMatrices behave much like common numbers. For example, we can add matrices \\(\\boldsymbol C= \\boldsymbol A+ \\boldsymbol B\\) and multiply matrices \\(\\boldsymbol C= \\boldsymbol A\\boldsymbol B\\).\nFor matrix addition \\(\\boldsymbol C= \\boldsymbol A+ \\boldsymbol B\\) we add the corresponding elements \\(c_{ij} = a_{ij} + b_{ij}\\). For matrix addition \\(\\boldsymbol A\\) and \\(\\boldsymbol B\\) must have the same dimensions, i.e. the same number of rows and columns.\nThe dot product, or scalar product, of two vectors \\(\\boldsymbol a\\) and \\(\\boldsymbol b\\) is a scalar given by \\(\\boldsymbol a\\cdot \\boldsymbol b= \\langle \\boldsymbol a, \\boldsymbol b\\rangle  = \\boldsymbol a^T \\boldsymbol b= \\boldsymbol b^T \\boldsymbol a= \\sum_{i=1}^d a_{i} b_{i}\\).\nMatrix multiplication \\(\\boldsymbol C= \\boldsymbol A\\boldsymbol B\\) is obtained by setting \\(c_{ij} = \\sum_{k=1}^m a_{ik} b_{kj}\\) where \\(m\\) is the number of columns of \\(\\boldsymbol A\\) and the number of rows in \\(\\boldsymbol B\\). Thus, \\(\\boldsymbol C\\) contains all possible dot products of the row vectors in \\(\\boldsymbol A\\) with the column vectors in \\(\\boldsymbol B\\). For matrix multiplication the number of columns in \\(\\boldsymbol A\\) must match the number of rows in \\(\\boldsymbol B\\). Note that matrix multiplication in general (for \\(m &gt; 1\\)) does not commute, i.e. \\(\\boldsymbol A\\boldsymbol B\\neq \\boldsymbol B\\boldsymbol A\\).\n\n\nMatrix transpose\nThe matrix transpose \\(\\boldsymbol A^T\\) indicate by the superscript \\(T\\) interchanges rows and columns of a matrix. The transpose is a linear operator \\((\\boldsymbol A+ \\boldsymbol B)^T = \\boldsymbol A^T + \\boldsymbol B^T\\) and applied to a matrix product it reverses the ordering, i.e. \\((\\boldsymbol A\\boldsymbol B)^T =\\boldsymbol B^T \\boldsymbol A^T\\).\nIf \\(\\boldsymbol A= \\boldsymbol A^T\\) then \\(\\boldsymbol A\\) is symmetric (and square).\nBy construction given a rectangular \\(\\boldsymbol A\\) the matrices \\(\\boldsymbol A^T \\boldsymbol A\\) and \\(\\boldsymbol A\\boldsymbol A^T\\) are symmetric with non-negative diagonal.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#matrix-summaries",
    "href": "01-matrix-essentials.html#matrix-summaries",
    "title": "1  Matrix essentials",
    "section": "1.4 Matrix summaries",
    "text": "1.4 Matrix summaries\n\nRow, column and grand sum\nAssume a matrix \\(\\boldsymbol A\\) of size \\(n \\times m\\).\nThe sum over the \\(m\\) entries of row \\(i\\) is \\(\\sum_{j=1}^m a_{ij}\\). In matrix notation the \\(n\\) row sums are given by \\(\\boldsymbol A\\, \\mathbf 1_m\\).\nThe sum over the \\(n\\) entries of column \\(j\\) is \\(\\sum_{i=1}^n a_{ij}\\). In matrix notation the \\(m\\) column sums are \\(\\boldsymbol A^T \\mathbf 1_n\\).\nThe grand sum of all matrix entries of \\(\\boldsymbol A\\) is obtained by \\[\n\\sum_{i=1}^n \\sum_{j=1}^m a_{ij} = \\mathbf 1_n^T \\, \\boldsymbol A\\, \\mathbf 1_m\n\\]\n\n\nMatrix trace\nThe trace of the matrix is the sum of the diagonal elements \\(\\text{Tr}(\\boldsymbol A) = \\sum a_{ii}\\).\nThe trace is invariant against transposition, i.e. \\[\n\\text{Tr}(\\boldsymbol A) = \\text{Tr}(\\boldsymbol A^T )\n\\]\nA useful identity for the matrix trace of the product of two matrices is \\[\n\\text{Tr}(\\boldsymbol A\\boldsymbol B) = \\text{Tr}( \\boldsymbol B\\boldsymbol A)  \n\\]\nIntriguingly, the trace of a matrix equals the sum of the eigenvalues of the matrix (see further below).\n\n\nRow, column and grand sum of squares\nThe sum over the \\(m\\) squared entries of row \\(i\\) is \\(\\sum_{j=1}^m a_{ij}^2\\). In matrix notation the \\(n\\) row sums of squares are given by \\(\\text{Diag}( \\boldsymbol A\\boldsymbol A^T)\\).\nThe sum over the \\(n\\) squared entries of column \\(j\\) is \\(\\sum_{i=1}^n a_{ij}^2\\). In matrix notation the \\(m\\) column sums of squares are \\(\\text{Diag}( \\boldsymbol A^T \\boldsymbol A)\\).\nThe grand sum of all squared elements of \\(\\boldsymbol A\\) is obtained by \\[\n\\sum_{i=1}^n \\sum_{j=1}^m a_{ij}^2 = \\text{Tr}(\\boldsymbol A^T \\boldsymbol A) = \\text{Tr}(\\boldsymbol A\\boldsymbol A^T)\n\\] This is also known as the squared Frobenius norm of \\(\\boldsymbol A\\) (see below).\n\n\nSum of squared diagonal entries\nThe sum of the squared entries on the diagonal is in matrix notation \\[\n\\text{Diag}(\\boldsymbol A)^T \\text{Diag}(\\boldsymbol A) = \\sum_{i=1}^{\\min(n,m)} a_{ii}^2\n\\]\n\n\nFrobenius inner product\nThe Frobenius inner product between two rectangular matrices of the same dimension is the scalar \\[\n\\begin{split}\n\\langle \\boldsymbol A, \\boldsymbol B\\rangle &=  \\text{Tr}(\\boldsymbol A\\boldsymbol B^T) = \\text{Tr}(\\boldsymbol B\\boldsymbol A^T)\\\\\n&=  \\text{Tr}(\\boldsymbol A^T \\boldsymbol B) = \\text{Tr}(\\boldsymbol B^T \\boldsymbol A)\\\\\n&= \\sum_{i,j} a_{ij} b_{ij} \\,.\n\\end{split}\n\\] This generalises the dot product between two vectors. Note that the dot product can therefore also be written as the trace of a matrix \\[\n\\langle \\boldsymbol a, \\boldsymbol b\\rangle = \\text{Tr}( \\boldsymbol a\\boldsymbol b^T ) = \\text{Tr}( \\boldsymbol b\\boldsymbol a^T) \\,.\n\\]\n\n\nEuclidean norm\nThe squared Euclidean norm or the squared length of the vector \\(\\boldsymbol a\\) is the dot product \\(||\\boldsymbol a||^2_2 = \\boldsymbol a\\cdot \\boldsymbol a=  \\langle \\boldsymbol a, \\boldsymbol a\\rangle =   \\boldsymbol a^T \\boldsymbol a= \\boldsymbol a\\boldsymbol a^T = \\sum_{i=1}^d a_i^2\\).\nThe squared Frobenius norm is a generalisation of the squared Euclidean vector norm to a rectangular matrix and is the sum of the squares of all its elements. Using the trace it can be written as \\[\n\\begin{split}\n||\\boldsymbol A||_F^2 &= \\langle \\boldsymbol A, \\boldsymbol A\\rangle \\\\\n&= \\text{Tr}(\\boldsymbol A^T \\boldsymbol A) = \\text{Tr}(\\boldsymbol A\\boldsymbol A^T) \\\\\n&= \\sum_{i,j} a_{ij}^2 \\,.\n\\end{split}\n\\]\nA useful identity for the squared Frobenius norm of the difference of two matrices is \\[\n\\begin{split}\n||\\boldsymbol A- \\boldsymbol B||_F^2 &= ||\\boldsymbol A||_F^2 + ||\\boldsymbol B||_F^2  - 2 \\langle \\boldsymbol A, \\boldsymbol B\\rangle \\\\\n&= \\text{Tr}(\\boldsymbol A^T \\boldsymbol A) +\\text{Tr}(\\boldsymbol B^T \\boldsymbol B) - 2 \\text{Tr}(\\boldsymbol A^T \\boldsymbol B) \\\\\n&= \\sum_{i,j} (a_{ij}-b_{ij})^2 \\,.\n\\end{split}\n\\]\nThe Frobenius norm of a matrix \\(||\\boldsymbol A||_F\\) is not to be confused with the induced \\(2\\)-norm of a matrix \\(||\\boldsymbol A||_2\\). The latter equals the maximum absolute eigenvalue of the matrix, with \\(||\\boldsymbol A||_2 \\leq ||\\boldsymbol A||_F\\).\n\n\nDeterminant of a matrix\nIf \\(\\boldsymbol A\\) is a square matrix the determinant \\(\\det(\\boldsymbol A)\\) is a scalar measuring the volume spanned by the column vectors in \\(\\boldsymbol A\\) with the sign determined by the orientation of the vectors.\nIf \\(\\det(\\boldsymbol A) \\neq 0\\) the matrix \\(\\boldsymbol A\\) is non-singular or non-degenerate. Conversely, if \\(\\det(\\boldsymbol A) =0\\) the matrix \\(\\boldsymbol A\\) is singular or degenerate.\nIntriguingly, the determinant of \\(\\boldsymbol A\\) is the product of the eigenvalues of \\(\\boldsymbol A\\) (see further below).\nOne way to compute the determinant of a matrix \\(\\boldsymbol A\\) is the Laplace cofactor expansion approach that proceeds recursively based on the determinants of the sub-matrices \\(\\boldsymbol A_{-i,-j}\\) obtained by deleting row \\(i\\) and column \\(j\\) from \\(\\boldsymbol A\\). Specifically, at each level we compute the\n\ncofactor expansion either\n\nalong the \\(i\\)-th row — pick any row \\(i\\): \\[\\det(\\boldsymbol A) = \\sum_{j=1}^d a_{ij} (-1)^{i+j} \\det(\\boldsymbol A_{-i,-j})  \\text{ , or}\\]\nalong the \\(j\\)-th column — pick any \\(j\\): \\[\\det(\\boldsymbol A) = \\sum_{i=1}^d a_{ij} (-1)^{i+j} \\det(\\boldsymbol A_{-i,-j})\\].\n\nThen repeat until the sub-matrix is a scalar \\(a\\) and \\(\\det(a)=a \\,.\\)\n\nThe recursive nature of this algorithm leads to a complexity of order \\(O(d!)\\) so it is not practical except for very small \\(d\\). Therefore, in practice other more efficient algorithms for computing determinants are used but these still have algorithmic complexity in the order of \\(O(d^3)\\) so for large dimensions obtaining determinants is very expensive.\nHowever, some specially structured matrices do allow for very fast calculation.\nThe determinant of a triangular matrix (and thus also of a diagonal matrix) \\[\n\\boldsymbol A= \\begin{pmatrix}\na_{11} & 0       & \\cdots & 0\\\\\na_{21} & a_{22}  & \\cdots & 0\\\\\n\\vdots  & \\vdots & \\ddots & 0 \\\\\na_{d1} & a_{d2} & \\cdots & a_{dd} \\\\\n\\end{pmatrix}\n\\] is the product of its diagonal elements, i.e. \\(\\det(\\boldsymbol A) = \\prod_{i=1}^d a_{ii}\\).\nFor a two-dimensional matrix \\(\\boldsymbol A= \\begin{pmatrix} a_{11} & a_{12} \\\\  a_{21} & a_{22} \\\\\\end{pmatrix}\\) the determinant is \\(\\det(A) = a_{11} a_{22} - a_{12} a_{21}\\).\nFor a block-structured square matrix \\[\n\\boldsymbol A= \\begin{pmatrix} \\boldsymbol A_{11} & \\boldsymbol A_{12} \\\\ \\boldsymbol A_{21} & \\boldsymbol A_{22} \\\\ \\end{pmatrix} \\, ,\n\\] where the matrices on the diagonal \\(\\boldsymbol A_{11}\\) and \\(\\boldsymbol A_{22}\\) are themselves square but \\(\\boldsymbol A_{12}\\) and \\(\\boldsymbol A_{21}\\) may be rectangular, the determinant is \\[\n\\det(\\boldsymbol A) = \\det(\\boldsymbol A_{22}) \\det(\\boldsymbol C_1) = \\det(\\boldsymbol A_{11}) \\det(\\boldsymbol C_2)\n\\] with the (Schur complement of \\(\\boldsymbol A_{22}\\)) \\[\n\\boldsymbol C_1 = \\boldsymbol A_{11} -  \\boldsymbol A_{12}  \\boldsymbol A_{22}^{-1}  \\boldsymbol A_{21}\n\\] and (Schur complement of \\(\\boldsymbol A_{11}\\)) \\[\n\\boldsymbol C_2 = \\boldsymbol A_{22} -  \\boldsymbol A_{21}  \\boldsymbol A_{11}^{-1}  \\boldsymbol A_{12}\n\\] Note that \\(\\boldsymbol C_1\\) and \\(\\boldsymbol C_2\\) are square matrices.\nFor a block-diagonal matrix \\(\\boldsymbol A\\) with \\(\\boldsymbol A_{12} = 0\\) and \\(\\boldsymbol A_{21} = 0\\) the determinant is \\(\\det(\\boldsymbol A) =  \\det(\\boldsymbol A_{11})  \\det(\\boldsymbol A_{22})\\), i.e. the product of the determinants of the sub-matrices along the diagonal.\nDeterminants have a multiplicative property, \\[\\det(\\boldsymbol A\\boldsymbol B) = \\det(\\boldsymbol B\\boldsymbol A) = \\det(\\boldsymbol A) \\det(\\boldsymbol B) \\,.\\] In the above \\(\\boldsymbol A\\) and \\(\\boldsymbol B\\) are both square and of the same dimension.\nFor rectangular \\(\\boldsymbol A\\) (\\(n \\times m\\)) and rectangular \\(\\boldsymbol B\\) (\\(m \\times n\\)) with \\(m \\geq n\\) this generalises to the Cauchy-Binet formula \\[\n\\det(\\boldsymbol A\\boldsymbol B) = \\sum_{w} \\det(\\boldsymbol A_{,w}) \\det(\\boldsymbol B_{w,})\n\\] where the summation is over all \\(\\binom{m}{n}\\) index subsets \\(w\\) of size \\(n\\) taken from \\(\\{1, \\ldots, m\\}\\) keeping the ordering and \\(\\boldsymbol A_{,w}\\) and \\(\\boldsymbol B_{w,}\\) are the corresponding square \\(n \\times n\\) sub-matrices. If \\(m &lt; n\\) then \\(\\det(\\boldsymbol A\\boldsymbol B) = 0\\).\nFor scalar \\(a\\) \\(\\det(a \\boldsymbol B) = a^d \\det(\\boldsymbol B)\\) where \\(d\\) is the dimension of \\(\\boldsymbol B\\).\nAnother important identity is \\[\\det(\\boldsymbol I_n + \\boldsymbol A\\boldsymbol B) = \\det(\\boldsymbol I_m + \\boldsymbol B\\boldsymbol A)\\] where \\(\\boldsymbol A\\) and \\(\\boldsymbol B\\) are rectangular matrices. This is called the Weinstein-Aronszajn determinant identity (also credited to Sylvester).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#matrix-inverse",
    "href": "01-matrix-essentials.html#matrix-inverse",
    "title": "1  Matrix essentials",
    "section": "1.5 Matrix inverse",
    "text": "1.5 Matrix inverse\n\nInversion of square matrix\nIf \\(\\boldsymbol A\\) is a square matrix then the inverse matrix \\(\\boldsymbol A^{-1}\\) is a matrix such that \\[\\boldsymbol A^{-1} \\boldsymbol A= \\boldsymbol A\\boldsymbol A^{-1}=  \\boldsymbol I\\, .\\] Only non-singular matrices with \\(\\det(\\boldsymbol A) \\neq 0\\) are invertible.\nAs \\(\\det(\\boldsymbol A^{-1} \\boldsymbol A) = \\det(\\boldsymbol I) = 1\\) the determinant of the inverse matrix equals the inverse determinant, \\[\\det(\\boldsymbol A^{-1}) = \\det(\\boldsymbol A)^{-1} \\,.\\]\nThe transpose of the inverse is the inverse of the transpose as \\[\n\\begin{split}\n(\\boldsymbol A^{-1})^T &= (\\boldsymbol A^{-1})^T \\,  \\boldsymbol A^T (\\boldsymbol A^{T})^{-1}   \\\\\n&= (\\boldsymbol A\\boldsymbol A^{-1})^T \\, (\\boldsymbol A^{T})^{-1} = (\\boldsymbol A^{T})^{-1} \\,. \\\\\n\\end{split}\n\\]\nThe inverse of a matrix product \\((\\boldsymbol A\\boldsymbol B)^{-1} = \\boldsymbol B^{-1} \\boldsymbol A^{-1}\\) is the product of the individual matrix inverses in reverse order.\nThere are many different algorithms to compute the inverse of a matrix (which is essentially a problem of solving a system of equations). The computational complexity of matrix inversion is of the order \\(O(d^3)\\) where \\(d\\) is the dimension of \\(\\boldsymbol A\\). Therefore matrix inversion is very costly in higher dimensions.\n\nExample 1.1 Inversion of a \\(2 \\times 2\\) matrix:\nThe inverse of the matrix \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\) is \\(A^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\\)\n\n\n\nInversion of structured matrices\nHowever, for specially structured matrices inversion can be done effectively:\n\nThe inverse of a diagonal matrix is another diagonal matrix obtained by inverting the diagonal elements.\nMore generally, the inverse of a block-diagonal matrix is obtained by individually inverting the blocks along the diagonal.\n\nThe Woodbury matrix identity simplifies the inversion of matrices that can be written as \\(\\boldsymbol A+ \\boldsymbol U\\boldsymbol B\\boldsymbol V\\) where \\(\\boldsymbol A\\) and \\(\\boldsymbol B\\) are both square and \\(\\boldsymbol U\\) and \\(\\boldsymbol V\\) are suitable rectangular matrices: \\[\n(\\boldsymbol A+ \\boldsymbol U\\boldsymbol B\\boldsymbol V)^{-1} = \\boldsymbol A^{-1} - \\boldsymbol A^{-1} \\boldsymbol U(\\boldsymbol B^{-1} + \\boldsymbol V\\boldsymbol A^{-1} \\boldsymbol U)^{-1} \\boldsymbol V\\boldsymbol A^{-1}\n\\] Typically, the inverse \\(\\boldsymbol A^{-1}\\) is either already known or can be easily obtained and the dimension of \\(\\boldsymbol B\\) is much lower than that of \\(\\boldsymbol A\\).\nThe class of matrices that can be most easily inverted are orthogonal matrices whose inverse is obtained simply by transposing the matrix.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#orthogonal-matrices",
    "href": "01-matrix-essentials.html#orthogonal-matrices",
    "title": "1  Matrix essentials",
    "section": "1.6 Orthogonal matrices",
    "text": "1.6 Orthogonal matrices\n\nProperties\nAn orthogonal matrix \\(\\boldsymbol Q\\) is a square matrix with the property that \\(\\boldsymbol Q^T = \\boldsymbol Q^{-1}\\), i.e. the transpose is also the inverse. This implies that \\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol I\\).\nBoth the column and the row vectors in \\(\\boldsymbol Q\\) all have length 1. This implies that each element \\(q_{ij}\\) of \\(\\boldsymbol Q\\) can only take a value in the interval \\([-1, 1]\\).\nThe identity matrix \\(\\boldsymbol I\\) is the simplest example of an orthogonal matrix.\nThe squared Euclidean and Frobenius norm is preserved when a vector \\(\\boldsymbol a\\) or matrix \\(\\boldsymbol A\\) is multiplied with an orthogonal matrix \\(\\boldsymbol Q\\): \\[\n|| \\boldsymbol Q\\boldsymbol a||^2_2 = (\\boldsymbol Q\\boldsymbol a)^T \\boldsymbol Q\\boldsymbol a= \\boldsymbol a^T \\boldsymbol a= || \\boldsymbol a||^2_2\n\\] and \\[\n|| \\boldsymbol Q\\boldsymbol A||^2_F = \\text{Tr}\\left((\\boldsymbol Q\\boldsymbol A)^T \\boldsymbol Q\\boldsymbol A\\right) = \\text{Tr}\\left(\\boldsymbol A^T \\boldsymbol A\\right) = || \\boldsymbol A||^2_F\n\\]\nMultiplication of \\(\\boldsymbol Q\\) with a vector results in a new vector of the same length but with a change in direction (unless \\(\\boldsymbol Q=\\boldsymbol I\\)). An orthogonal matrix \\(\\boldsymbol Q\\) can thus be interpreted geometrically as an operator performing rotation, reflection and/or permutation.\nThe product \\(\\boldsymbol Q_3 = \\boldsymbol Q_1 \\boldsymbol Q_2\\) of two orthogonal matrices \\(\\boldsymbol Q_1\\) and \\(\\boldsymbol Q_2\\) yields another orthogonal matrix as \\(\\boldsymbol Q_3 \\boldsymbol Q_3^T = \\boldsymbol Q_1 \\boldsymbol Q_2  (\\boldsymbol Q_1 \\boldsymbol Q_2)^T = \\boldsymbol Q_1 \\boldsymbol Q_2 \\boldsymbol Q_2^T \\boldsymbol Q_1^T = \\boldsymbol I\\).\nThe determinant \\(\\det(\\boldsymbol Q)\\) of an orthogonal matrix is either +1 or -1, because \\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol I\\) and thus \\(\\det(\\boldsymbol Q)\\det(\\boldsymbol Q^T) = \\det(\\boldsymbol Q)^2 = \\det(\\boldsymbol I) = 1\\).\nThe set of all orthogonal matrices of dimension \\(d\\) together with multiplication form a group called the orthogonal group \\(O(d)\\). The subset of orthogonal matrices with \\(\\det(\\boldsymbol Q)=1\\) are called rotation matrices and form with multiplication the special orthogonal group \\(SO(d)\\). Orthogonal matrices with \\(\\det(\\boldsymbol Q)=-1\\) are rotation-reflection matrices.\n\n\nSemi-orthogonal matrices\nA rectangular \\(d \\times k\\) matrix \\(\\boldsymbol Q\\) is semi-orthogonal if for \\(k &lt; d\\) the \\(k\\) column vectors are orthonormal and hence \\(\\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol I_k\\), or if for \\(k &gt; d\\) the \\(d\\) row vectors are orthonormal with \\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol I_d\\).\nThe set of all (semi)-orthogonal matrices \\(\\boldsymbol Q\\) with \\(k \\leq d\\) column vectors is known as the Stiefel manifold \\(\\text{St}(d, k)\\).\n\n\nGenerating orthogonal matrices\nIn two dimensions \\((d=2)\\) all orthogonal matrices \\(\\boldsymbol R\\) representing rotations with \\(\\det(\\boldsymbol R)=1\\) are given by \\[\n\\boldsymbol R(\\theta) =\n\\begin{pmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{pmatrix}\n\\] and those representing rotation-reflections \\(\\boldsymbol G\\) with \\(\\det(\\boldsymbol G)=-1\\) by \\[\n\\boldsymbol G(\\theta) =\n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix}\\,.\n\\] Every orthogonal matrix of dimension \\(d=2\\) can be represented as the product of at most two rotation-reflection matrices because \\[\n\\boldsymbol R(\\theta) = \\boldsymbol G(\\theta)\\, \\boldsymbol G(0) =  \n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}\\,.\n\\] Thus, the matrix \\(\\boldsymbol G\\) is a generator of two-dimensional orthogonal matrices. Note that \\(\\boldsymbol G(\\theta)\\) is symmetric, orthogonal and has determinant -1.\nMore generally, and applicable in arbitrary dimension, the role of generator is taken by the Householder reflection matrix \\[\n\\boldsymbol Q_{HH}(\\boldsymbol v) = \\boldsymbol I- 2 \\boldsymbol v\\boldsymbol v^T\n\\] where \\(\\boldsymbol v\\) is a vector of unit length (with \\(\\boldsymbol v^T \\boldsymbol v=1\\)) orthogonal to the reflection hyperplane. Note that \\(\\boldsymbol Q_{HH}(\\boldsymbol v) = \\boldsymbol Q_{HH}(-\\boldsymbol v)\\). By construction the matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\) is symmetric, orthogonal and has determinant -1.\nIt can be shown that any \\(d\\)-dimensional orthogonal matrix \\(\\boldsymbol Q\\) can be represented as the product of at most \\(d\\) Householder reflection matrices. The two-dimensional generator \\(\\boldsymbol G(\\theta)\\) is recovered as the Householder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\) with \\(\\boldsymbol v= \\begin{pmatrix} -\\sin  \\frac{\\theta}{2} \\\\ \\cos \\frac{\\theta}{2} \\end{pmatrix}\\) or \\(\\boldsymbol v= \\begin{pmatrix} \\sin  \\frac{\\theta}{2} \\\\ -\\cos \\frac{\\theta}{2} \\end{pmatrix}\\).\n\n\nPermutation matrix\nA special type of an orthogonal matrix is a permutation matrix \\(\\boldsymbol P\\) created by permuting rows and/or columns of the identity matrix \\(\\boldsymbol I\\). Thus, each row and column of \\(\\boldsymbol P\\) contains exactly one entry of 1, but not necessarily on the diagonal.\nIf a permutation matrix \\(\\boldsymbol P\\) is multiplied with a matrix \\(\\boldsymbol A\\) it acts as an operator permuting the columns (\\(\\boldsymbol A\\boldsymbol P\\)) or the rows (\\(\\boldsymbol P\\boldsymbol A\\)). For a set of \\(d\\) elements there exist \\(d!\\) permutations. Thus, for dimension \\(d\\) there are \\(d!\\) possible permutation matrices (including the identity matrix).\nThe determinant of a permutation matrix is either +1 or -1. The product of two permutation matrices yields another permutation matrix.\nSymmetric permutation matrices correspond to self-inverse permutations (i.e. the permutation matrix is its own inverse), and are also called permutation involutions. They can have determinant +1 and -1.\nA transposition is a permutation where only two elements are exchanged. Thus, in a transposition matrix \\(\\boldsymbol T\\) exactly two rows and/or columns are exchanged compared to identity matrix \\(\\boldsymbol I\\). Transpositions are self-inverse, and transposition matrices are symmetric. There are \\(\\frac{d (d-1)}{2}\\) different transposition matrices. The determinant of a transposition matrix is \\(\\det(\\boldsymbol T)= -1\\).\nNote that the transposition matrix is an instance of a Householder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\) with vector \\(\\boldsymbol v\\) filled with zeros except for two elements that have value \\(\\frac{\\sqrt{2}}{2}\\) and \\(-\\frac{\\sqrt{2}}{2}\\).\nAny permutation of \\(d\\) elements can be generated by a series of at most \\(d-1\\) transpositions. Correspondingly, any permutation matrix \\(\\boldsymbol P\\) can be constructed by multiplication of the identity matrix with at most \\(d-1\\) transposition matrices. If the number of transpositions is even then \\(\\det(\\boldsymbol P) = 1\\) otherwise for an uneven number \\(\\det(\\boldsymbol P) = -1\\). This is called the sign or signature of the permutation.\nThe set of all permutations form the symmetric group \\(S_d\\), the subset of even permutations (with positive sign and \\(\\det(\\boldsymbol P)=1\\)) the alternating group \\(A_d\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#eigenvalues-and-eigenvectors",
    "href": "01-matrix-essentials.html#eigenvalues-and-eigenvectors",
    "title": "1  Matrix essentials",
    "section": "1.7 Eigenvalues and eigenvectors",
    "text": "1.7 Eigenvalues and eigenvectors\n\nDefinition\nAssume a square matrix \\(\\boldsymbol A\\) of size \\(d \\times d\\). A vector \\(\\boldsymbol u\\neq 0\\) is called an eigenvector of the matrix \\(\\boldsymbol A\\) and \\(\\lambda\\) the corresponding eigenvalue if\n\\[\\boldsymbol A\\boldsymbol u= \\boldsymbol u\\lambda \\, .\\] This is called eigenvalue equation or eigenequation.\n\n\nFinding eigenvalues and vectors\nTo find the eigenvalues and eigenvectors the eigenequation is rewritten as \\[(\\boldsymbol A-\\boldsymbol I\\lambda ) \\; \\boldsymbol u= \\mathbf 0\\,.\\] For this equation to hold for an eigenvector \\(\\boldsymbol u\\neq 0\\) with eigenvalue \\(\\lambda\\) implies that the matrix \\(\\boldsymbol A-\\boldsymbol I\\lambda\\) is singular. Correspondingly, its determinant must vanish \\[\\det(\\boldsymbol A-\\boldsymbol I\\lambda ) =0 \\,.\\] This is called the characteristic equation of the matrix \\(\\boldsymbol A\\), and its solution yields the \\(d\\) eigenvalues \\(\\lambda_1, \\ldots, \\lambda_d\\). Note the eigenvalues need not be distinct and they may be complex even if the matrix \\(\\boldsymbol A\\) is real.\nIf there are complex eigenvalues, for a real matrix those eigenvalues come in conjugate pairs. Hence, for a complex \\(\\lambda_1 = r e^{i \\phi}\\) there will also be a corresponding complex eigenvalue \\(\\lambda_2 = r e^{-i \\phi}\\).\nGiven the eigenvalues we then solve the eigenequation for the corresponding non-zero eigenvectors \\(\\boldsymbol u_1, \\ldots, \\boldsymbol u_d\\). Note that eigenvectors of real matrices can have complex components. Also the eigenvector is only defined by the eigenequation up to a scalar. By convention eigenvectors are therefore typically standardised to unit length but this still leaves a sign ambiguity for real eigenvectors and implies that complex eigenvectors are defined only up to a factor with modulus 1.\n\n\nEigenequation in matrix notation\nWith the matrix \\[\\boldsymbol U= (\\boldsymbol u_1, \\ldots, \\boldsymbol u_d)\\] containing the standardised eigenvectors in the columns and the diagonal matrix \\[\\boldsymbol \\Lambda= \\begin{pmatrix}\n    \\lambda_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}\n\\end{pmatrix}\\] containing the eigenvalues (typically sorted in order of magnitude) the eigenvalue equation can be written as \\[\\boldsymbol A\\boldsymbol U= \\boldsymbol U\\boldsymbol \\Lambda\\,.\\]\n\n\nPermutation of eigenvalues\nIf eigenvalues are not in order, we may apply a permutation matrix \\(\\boldsymbol P\\) to arrange them in order. With \\(\\boldsymbol \\Lambda^{\\text{sort}} = \\boldsymbol P^T \\boldsymbol \\Lambda\\boldsymbol P\\) as the sorted eigenvalues and \\(\\boldsymbol U^{\\text{sort}} = \\boldsymbol U\\boldsymbol P\\) as the corresponding eigenvectors the eigenequation becomes \\[\\boldsymbol A\\boldsymbol U^{\\text{sort}} =  \\boldsymbol A\\boldsymbol U\\boldsymbol P= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol P=  \\boldsymbol U\\boldsymbol P\\boldsymbol P^T \\boldsymbol \\Lambda\\boldsymbol P=  \\boldsymbol U^{\\text{sort}} \\boldsymbol \\Lambda^{\\text{sort}} \\,.\\]\n\n\nSimilar matrices\nTwo matrices \\(\\boldsymbol A\\) and \\(\\boldsymbol B\\) are called similar if they share the same eigenvalues.\nFrom \\(\\boldsymbol A\\) with eigenvalues \\(\\boldsymbol \\Lambda\\) and eigenvectors \\(\\boldsymbol U\\) we can construct a similar \\(\\boldsymbol B\\) via the similarity transformation \\(\\boldsymbol B= \\boldsymbol M\\boldsymbol A\\boldsymbol M^{-1}\\) where \\(\\boldsymbol M\\) is an invertible matrix.\nThen \\(\\boldsymbol \\Lambda\\) are the eigenvalues of \\(\\boldsymbol B\\) and \\(\\boldsymbol V= \\boldsymbol M\\boldsymbol U\\) its eigenvectors as \\[\n\\boldsymbol B\\boldsymbol V= \\boldsymbol M\\boldsymbol A\\boldsymbol M^{-1} \\boldsymbol M\\boldsymbol U= \\boldsymbol M\\boldsymbol A\\boldsymbol U= \\boldsymbol M\\boldsymbol U\\boldsymbol \\Lambda= \\boldsymbol V\\boldsymbol \\Lambda\\,.\n\\]\n\n\nDefective matrix\nIn most cases the eigenvectors \\(\\boldsymbol u_i\\) will be linearly independent so that they form a basis to span a \\(d\\) dimensional space.\nHowever, if this is not the case and the matrix \\(\\boldsymbol A\\) does not have a complete basis of eigenvectors, then the matrix is called defective. In this case the matrix \\(\\boldsymbol U\\) containing the eigenvectors is singular and \\(\\det(\\boldsymbol U)=0\\).\nAn example of a defective matrix is \\(\\begin{pmatrix}\n1 &1 \\\\\n0 & 1 \\\\\n\\end{pmatrix}\\) which has determinant 1 so that it can be inverted and its column vectors do form a complete basis but has only one distinct eigenvector \\((1,0)^T\\) so that the eigenvector basis is incomplete.\n\n\nEigenvalues of a diagonal or triangular matrix\nIn the special case that \\(\\boldsymbol A\\) is diagonal or a triangular matrix the eigenvalues are easily determined. This follows from the simple form of their determinants as the product of the diagonal elements. Hence for these matrices the characteristic equation becomes \\(\\prod_{i}^d (a_{ii} -\\lambda) = 0\\) and has solution \\(\\lambda_i=a_{ii}\\), i.e. the eigenvalues are equal to the diagonal elements.\n\n\nEigenvalues and vectors of a symmetric matrix\nIf \\(\\boldsymbol A\\) is symmetric, i.e. \\(\\boldsymbol A= \\boldsymbol A^T\\), then its eigenvalues and eigenvectors have special properties:\n\nall eigenvalues of \\(\\boldsymbol A\\) are real,\nthe eigenvectors are orthogonal, i.e \\(\\boldsymbol u_i^T \\boldsymbol u_j = 0\\) for \\(i \\neq j\\), and real. Thus, the matrix \\(\\boldsymbol U\\) containing the standardised orthonormal eigenvectors is orthogonal.\n\\(\\boldsymbol A\\) is never defective as \\(\\boldsymbol U\\) forms a complete basis.\n\nFurthermore, for a symmetric matrix \\(\\boldsymbol A\\) with diagonal elements \\(p_1 \\geq \\ldots \\geq p_d\\) and eigenvalues \\(\\lambda_1 \\geq \\ldots \\geq \\lambda_d\\) (note both written in decreasing order) the sum of the largest \\(k\\) eigenvalues forms an upper bound of the sum of the largest \\(k\\) diagonal elements: \\[\n\\sum_{i}^k \\lambda_i \\geq \\sum_{i}^k p_i\n\\] This theorem is due to Schur (1923) 2. The equality holds for \\(k=d\\) (as the trace of \\(\\boldsymbol A\\) equals the sum of its eigenvalues) and for any \\(k\\) if \\(\\boldsymbol A\\) is diagonal (as in this case of the diagonal elements equal the eigenvalues).\n\n\nEigenvalues of orthogonal matrices\nThe eigenvalues of an orthogonal matrix \\(\\boldsymbol Q\\) are not necessarily real but they all have modulus 1 and lie on the unit circle . Thus, the eigenvalues of \\(\\boldsymbol Q\\) all have the form \\(\\lambda = e^{i \\phi} = \\cos \\phi + i \\sin \\phi\\).\nIn any real matrix complex eigenvalues come in conjugate pairs. Hence if an orthogonal matrix \\(\\boldsymbol Q\\) has the complex eigenvalue \\(e^{i \\phi}\\) it also has an complex eigenvalue \\(e^{-i \\phi} =\\cos \\phi - i \\sin \\phi\\). The product of these two conjugate eigenvalues is 1. Thus, an orthogonal matrix of uneven dimension has at least one real eigenvalue (+1 or -1).\nThe eigenvalues of a Hausholder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\) are all real (recall that it is symmetric!). In fact, in dimension \\(d\\) its eigenvalues are -1 (one time) and 1 ( \\(d-1\\) times). Since a transposition matrix \\(\\boldsymbol T\\) is a special Householder matrix they have the same eigenvalues.\n\n\nPositive definite matrices\nIf all eigenvalues of a square matrix \\(\\boldsymbol A\\) are real and \\(\\lambda_i \\geq 0\\) then \\(\\boldsymbol A\\) is called positive semi-definite. If all eigenvalues are strictly positive \\(\\lambda_i &gt; 0\\) then \\(\\boldsymbol A\\) is called positive definite.\nNote that a matrix does not need to be symmetric to be positive definite, e.g. \\(\\begin{pmatrix}\n2 & 3 \\\\\n1 & 4 \\\\\n\\end{pmatrix}\\) has positive eigenvalues 5 and 1. It also has a complete set of eigenvectors and is diagonalisable.\nA symmetric matrix \\(\\boldsymbol A\\) is positive definite if the quadratic form \\(\\boldsymbol x^T \\boldsymbol A\\boldsymbol x&gt; 0\\) for any non-zero \\(\\boldsymbol x\\), and it is positive semi-definite if \\(\\boldsymbol x^T \\boldsymbol A\\boldsymbol x\\geq 0\\). This holds also the other way around: a symmetric positive definite matrix (with positive eigenvalues) has a positive quadratic form, and a symmetric positive semi-definite matrix (with non-negative eigenvalues) a non-negative quadratic form.\nA symmetric positive definite matrix always has a positive diagonal (this can be seen by setting \\(\\boldsymbol x\\) above to a unit vector with 1 at a single position, and 0 at all other elements). However, just requiring a positive diagonal is too weak to ensure positive definiteness of a symmetric matrix, for example \\(\\begin{pmatrix}\n1 &10 \\\\\n10 & 1 \\\\\n\\end{pmatrix}\\) has a negative eigenvalue of -9. On the other hand, a symmetric matrix is indeed positive definite if it is strictly diagonally dominant, i.e. if all its diagonal elements are positive and are larger than the absolute value of any of the corresponding row or column elements. However, diagonal dominance is too restrictive as criterion to characterise all symmetric positive definite matrices, since there are many symmetric matrices that are positive definite but not diagonally dominant, such as \\(\\begin{pmatrix}\n1 & 2 \\\\\n2 & 5 \\\\\n\\end{pmatrix}\\).\nFinally, the sum of a symmetric positive semi-definite matrix \\(\\boldsymbol A\\) and a symmetric positive definite matrix \\(\\boldsymbol B\\) is itself symmetric positive definite because the corresponding quadratic form \\(\\boldsymbol x^T ( \\boldsymbol A+\\boldsymbol B) \\boldsymbol x=\n\\boldsymbol x^T \\boldsymbol A\\boldsymbol x+ \\boldsymbol x^T \\boldsymbol B\\boldsymbol x&gt; 0\\) is positive. Similarly, the sum of two symmetric positive (semi)-definite matrices is itself symmetric positive (semi)-definite.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#matrix-decompositions",
    "href": "01-matrix-essentials.html#matrix-decompositions",
    "title": "1  Matrix essentials",
    "section": "1.8 Matrix decompositions",
    "text": "1.8 Matrix decompositions\n\nDiagonalisation and eigenvalue decomposition\nIf \\(\\boldsymbol A\\) is a square non-defective matrix then the eigensystem \\(\\boldsymbol U\\) is invertible and we can rewrite the eigenvalue equation to \\[\\boldsymbol A= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1} \\,.\\] This is called the eigendecomposition, or spectral decomposition, of \\(\\boldsymbol A\\) and equivalently \\[\\boldsymbol \\Lambda= \\boldsymbol U^{-1} \\boldsymbol A\\boldsymbol U\\] is the diagonalisation of \\(\\boldsymbol A\\).\nIf \\(\\boldsymbol A\\) is defective (i.e. \\(\\boldsymbol U\\) is singular) one can still approximately diagonalise \\(\\boldsymbol A\\) as there always exists a similarity transformation to \\(\\boldsymbol J= \\boldsymbol M\\boldsymbol A\\boldsymbol M^{-1}\\) where \\(\\boldsymbol M\\) is a invertible matrix and \\(\\boldsymbol J\\) has Jordan canonical form, i.e. \\(\\boldsymbol J\\) is upper triangular with the (potentially complex) eigenvalues on the diagonal and some non-zero entries equal to 1 immediately above the main diagonal.\n\n\nOrthogonal eigenvalue decomposition\nFor symmetric \\(\\boldsymbol A\\) with real eigenvalues and orthogonal matrix \\(\\boldsymbol U\\) the spectral decomposition becomes \\[\\boldsymbol A= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\] and \\[\\boldsymbol \\Lambda= \\boldsymbol U^T \\boldsymbol A\\boldsymbol U\\,.\\] This special case is known as the orthogonal diagonalisation of \\(\\boldsymbol A\\).\nThe orthogonal decomposition for symmetric \\(\\boldsymbol A\\) is unique apart from the signs of the eigenvectors (columns of \\(\\boldsymbol U\\)). Thus, in a computer application depending on the specific implementation of a numerical algorithm for eigenvalue decomposition the signs may vary.\n\n\nSingular value decomposition\nThe singular value decomposition (SVD) is a generalisation of the orthogonal eigenvalue decomposition for symmetric matrices.\nAny (!) rectangular matrix \\(\\boldsymbol A\\) of size \\(n\\times d\\) can be factored into the product \\[\\boldsymbol A= \\boldsymbol U\\boldsymbol D\\boldsymbol V^T\\] where \\(\\boldsymbol U\\) is a \\(n \\times n\\) orthogonal matrix, \\(\\boldsymbol V\\) is a second \\(d \\times d\\) orthogonal matrix and \\(\\boldsymbol D\\) is a diagonal but rectangular matrix of size \\(n\\times d\\) with \\(m=min(n,d)\\) real diagonal elements \\(d_1, \\ldots\nd_m\\). The \\(d_i\\) are called singular values, and appear along the diagonal in \\(\\boldsymbol D\\) by order of magnitude.\nThe SVD is unique apart from the signs of the columns vectors in \\(\\boldsymbol U\\), \\(\\boldsymbol V\\) and \\(\\boldsymbol D\\) (you can freely specify the column signs of any two of the three matrices). By convention the signs are chosen such that the singular values in \\(\\boldsymbol D\\) are all non-negative, which leaves ambiguity in columns signs of \\(\\boldsymbol U\\) and \\(\\boldsymbol V\\). Alternatively, one may fix the columns signs of \\(\\boldsymbol U\\) and \\(\\boldsymbol V\\), e.g. by requiring a positive diagonal, which then determines the sign of the singular values (thus allowing for negative singular values as well).\nIf \\(\\boldsymbol A\\) is symmetric then the SVD and the orthogonal eigenvalue decomposition coincide (apart from different sign conventions for singular values, eigenvalues and eigenvectors).\nSince \\(\\boldsymbol A^T \\boldsymbol A= \\boldsymbol V\\boldsymbol D^T \\boldsymbol D\\boldsymbol V^T\\) and \\(\\boldsymbol A\\boldsymbol A^T = \\boldsymbol U\\boldsymbol D\\boldsymbol D^T \\boldsymbol U^T\\) the squared singular values correspond to the eigenvalues of \\(\\boldsymbol A^T \\boldsymbol A\\) and \\(\\boldsymbol A\\boldsymbol A^T\\). It also follows that \\(\\boldsymbol A^T \\boldsymbol A\\) and \\(\\boldsymbol A\\boldsymbol A^T\\) are both positive semi-definite symmetric matrices, and that \\(\\boldsymbol V\\) and \\(\\boldsymbol U\\) contain the respective sets of eigenvectors.\n\n\nPolar decomposition\nAny square matrix \\(\\boldsymbol A\\) can be factored into the product \\[\n\\boldsymbol A= \\boldsymbol Q\\boldsymbol B\n\\] of an orthogonal matrix \\(\\boldsymbol Q\\) and a symmetric positive semi-definite matrix \\(\\boldsymbol B\\).\nThis follows from the SVD of \\(\\boldsymbol A\\) given as \\[\n\\begin{split}\n\\boldsymbol A&= \\boldsymbol U\\boldsymbol D\\boldsymbol V^T \\\\\n    &= ( \\boldsymbol U\\boldsymbol V^T ) ( \\boldsymbol V\\boldsymbol D\\boldsymbol V^T ) \\\\\n    &= \\boldsymbol Q\\boldsymbol B\\\\\n\\end{split}\n\\] with non-negative \\(\\boldsymbol D\\). Note that this decomposition is unique as the sign ambiguities in the columns of \\(\\boldsymbol U\\) and \\(\\boldsymbol V\\) cancel out in \\(\\boldsymbol Q\\) and \\(\\boldsymbol B\\).\n\n\nCholesky decomposition\nA symmetric positive definite matrix \\(\\boldsymbol A\\) can be decomposed into a product of a triangular matrix \\(\\boldsymbol L\\) with its transpose \\[\n\\boldsymbol A= \\boldsymbol L\\boldsymbol L^T \\,.\n\\] Here, \\(\\boldsymbol L\\) is a lower triangular matrix with positive diagonal elements.\nThis decomposition is unique and is called Cholesky factorisation. It is often used to check whether a symmetric matrix is positive definite as it is algorithmically less demanding than eigenvalue decomposition.\nNote that some implementations of the Cholesky decomposition (e.g. in R) use upper triangular matrices \\(\\boldsymbol K\\) with positive diagonal so that \\(\\boldsymbol A= \\boldsymbol K^T \\boldsymbol K\\) and \\(\\boldsymbol L= \\boldsymbol K^T\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#matrix-summaries-based-on-eigenvalues-and-singular-values",
    "href": "01-matrix-essentials.html#matrix-summaries-based-on-eigenvalues-and-singular-values",
    "title": "1  Matrix essentials",
    "section": "1.9 Matrix summaries based on eigenvalues and singular values",
    "text": "1.9 Matrix summaries based on eigenvalues and singular values\n\nTrace and determinant computed from eigenvalues\nThe eigendecomposition \\(\\boldsymbol A=\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1}\\) allows to establish a link between the trace and the determinant and the eigenvalues of a matrix.\nSpecifically, \\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol A) & = \\text{Tr}(\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1}  ) =\n\\text{Tr}( \\boldsymbol \\Lambda\\boldsymbol U^{-1} \\boldsymbol U) \\\\\n&= \\text{Tr}( \\boldsymbol \\Lambda) = \\sum_{i=1}^d \\lambda_i \\\\\n\\end{split}\n\\] thus the trace of a square matrix \\(\\boldsymbol A\\) is equal to the sum of its eigenvalues. Likewise, \\[\n\\begin{split}\n\\det(\\boldsymbol A) & = \\det(\\boldsymbol U) \\det(\\boldsymbol \\Lambda) \\det(\\boldsymbol U^{-1}  ) \\\\\n&=\\det( \\boldsymbol \\Lambda) = \\prod_{i=1}^d \\lambda_i \\\\\n\\end{split}\n\\] therefore the determinant of \\(\\boldsymbol A\\) is the product of the eigenvalues.\nThe relationship between the eigenvalues of a square matrix and the trace and the determinant of that matrix is shown above for diagonalisable matrices. However, it holds more generally for any square matrix, i.e. also for defective matrices. For the latter the Jordan canonical form \\(\\boldsymbol J\\) replaces \\(\\boldsymbol \\Lambda\\) (in both cases the eigenvalues are simply the entries on the diagonal).\nIf any of the eigenvalues are equal to zero then \\(\\det(\\boldsymbol A) = 0\\) and as hence \\(\\boldsymbol A\\) is singular and not invertible.\nThe trace and determinant of a real matrix are always real even though the individual eigenvalues may be complex.\n\n\nEigenvalues of a squared matrix\nFrom the eigendecomposition \\(\\boldsymbol A=\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1}\\) it is easy to see that the eigenvalues of \\(\\boldsymbol A^2\\) are simply the squared eigenvalues of \\(\\boldsymbol A\\) as \\[\n\\boldsymbol A^2 = \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1} \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1} = \\boldsymbol U\\boldsymbol \\Lambda^2 \\boldsymbol U^{-1}\n\\] As a result we can compute the trace of \\(\\boldsymbol A^2\\) as the sum of the squared eigenvalues of \\(\\boldsymbol A\\), i.e. \\(\\text{Tr}(\\boldsymbol A^2) =  \\sum_{i=1}^d \\lambda_i^2\\), and the determinant as the product of squared eigenvalues, i.e \\(\\det(\\boldsymbol A^2) = \\prod_{i=1}^d \\lambda_i^2\\).\nIf \\(\\boldsymbol A\\) is symmetric then \\(\\text{Tr}(\\boldsymbol A^2) = \\text{Tr}(\\boldsymbol A\\boldsymbol A^T) = || A ||^2_F = \\sum_{i=1}^d \\sum_{j=1}^d a_{ij}^2\\). This leads to the identity \\[\n\\sum_{i=1}^d \\lambda_i^2 =  \\sum_{i=1}^d \\sum_{j=1}^d a_{ij}^2\n\\] between the sum of the squared eigenvalues and the sum of all squared entries of a symmetric matrix \\(\\boldsymbol A\\).\n\n\nRank and condition number\nThe rank is the dimension of the space spanned by both the column and row vectors. A rectangular matrix of dimension \\(n \\times d\\) will have rank of at most \\(m = \\min(n, d)\\), and if the maximum is indeed achieved then it has full rank.\nThe condition number describes how well- or ill-conditioned a full rank matrix is. For example, for a square matrix a large condition number implies that the matrix is close to being singular and thus ill-conditioned. If the condition number is infinite then the matrix is not full rank.\nThe rank and condition of a matrix can both be determined from the \\(m\\) singular values \\(d_1, \\ldots, d_m\\) of a matrix obtained by SVD:\n\nThe rank is number of non-zero singular values.\nThe condition number is the ratio of the largest singular value divided by the smallest singular value (absolute values if signs are allowed).\n\nIf a square matrix \\(\\boldsymbol A\\) is singular then the condition number is infinite, and it will not have full rank. On the other hand, a non-singular square matrix, such as a positive definite matrix, has full rank.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#functions-of-symmetric-matrices",
    "href": "01-matrix-essentials.html#functions-of-symmetric-matrices",
    "title": "1  Matrix essentials",
    "section": "1.10 Functions of symmetric matrices",
    "text": "1.10 Functions of symmetric matrices\nWe focus on symmetric square matrices \\(\\boldsymbol A=\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) which are always diagonalisable with real eigenvalues \\(\\boldsymbol \\Lambda\\) and orthogonal eigenvectors \\(\\boldsymbol U\\).\n\nDefinition of a matrix function\nAssume a real-valued function \\(f(a)\\) of a real number \\(a\\). Then the corresponding matrix function \\(f(\\boldsymbol A)\\) is defined as \\[\nf(\\boldsymbol A) =  \\boldsymbol Uf(\\boldsymbol \\Lambda) \\boldsymbol U^T =  \\boldsymbol U\\begin{pmatrix}\n    f(\\lambda_{1}) & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & f(\\lambda_{d})\n\\end{pmatrix} \\boldsymbol U^T\n\\] where the function \\(f(a)\\) is applied to the eigenvalues of \\(\\boldsymbol A\\). By construction \\(f(\\boldsymbol A)\\) is real, symmetric and has real eigenvalues \\(f(\\lambda_i)\\).\nExamples:\n\nExample 1.2 Matrix power: \\(f(a) = a^p\\) (with \\(p\\) a real number)\n\nSpecial cases of matrix power include :\n\nMatrix inversion: \\(f(a) = a^{-1}\\)\nNote that if the matrix \\(\\boldsymbol A\\) is singular, i.e. contains one or more eigenvalues \\(\\lambda_i=0\\), then \\(\\boldsymbol A^{-1}\\) is not defined and therefore \\(\\boldsymbol A\\) is not invertible.\n\nHowever, a so-called pseudoinverse can still be computed, by inverting the non-zero eigenvalues, and keeping the zero eigenvalues as zero.\n\nMatrix square root: \\(f(a) = a^{1/2}\\)\nSince there are multiple solutions to the square root there are also multiple matrix square roots. The principal matrix square root is obtained by using the positive square roots of all the eigenvalues. Thus the principal matrix square root of a positive semi-definite matrix is also positive semi-definite and it is unique.\n\n\nExample 1.3 Matrix exponential: \\(f(a) = \\exp(a)\\)\nNote that because \\(\\exp(a) \\geq 0\\) for all real \\(a\\) the matrix \\(\\exp(\\boldsymbol A)\\) is positive semi-definite. Thus, the matrix exponential can be used to generate positive semi-definite matrices.\nIf \\(\\boldsymbol A\\) and \\(\\boldsymbol B\\) commute, i.e. if \\(\\boldsymbol A\\boldsymbol B= \\boldsymbol B\\boldsymbol A\\), then \\(\\exp(\\boldsymbol A+\\boldsymbol B) = \\exp(\\boldsymbol A) \\exp(\\boldsymbol B)\\). However, this is not the case otherwise!\n\n\nExample 1.4 Matrix logarithm: \\(f(a) = \\log(a)\\)\nAs the logarithm requires \\(a &gt;0\\) the matrix \\(\\boldsymbol A\\) needs to be positive definite for \\(\\log(\\boldsymbol A)\\) to be defined.\n\n\n\nIdentities for the matrix exponential and logarithm\nThe above give rise to useful identities:\n\nFor any symmetric matrix \\(\\boldsymbol A\\) we have \\[\n\\det(\\exp(\\boldsymbol A)) = \\exp(\\text{Tr}(\\boldsymbol A))\n\\] because \\(\\prod_i \\exp(\\lambda_i) = \\exp( \\sum_i \\lambda_i)\\) where \\(\\lambda_i\\) are the eigenvalues of \\(\\boldsymbol A\\).\nIf we take the logarithm on both sides and replace \\(\\exp(\\boldsymbol A)=\\boldsymbol B\\) we get another identity for a symmetric positive definite matrix \\(\\boldsymbol B\\): \\[\n\\log \\det(\\boldsymbol B) = \\text{Tr}(\\log(\\boldsymbol B))\n\\] because \\(\\log( \\prod_i \\lambda_i)  = \\sum_i \\log(\\lambda_i)\\) where \\(\\lambda_i\\) are the eigenvalues of \\(\\boldsymbol B\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#footnotes",
    "href": "01-matrix-essentials.html#footnotes",
    "title": "1  Matrix essentials",
    "section": "",
    "text": "Mardia, K. V., J. T. Kent and J. M. Bibby. 1979. Multivariate Analysis. Academic Press.↩︎\nSchur, I. 1923. Über eine Klasse von Mittelbildungen mit Anwendungen auf die Determinantentheorie. Sitzungsber. Berl. Math. Ges. 22:9–29.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html",
    "href": "02-vector-calculus.html",
    "title": "2  Vector and matrix calculus",
    "section": "",
    "text": "2.1 First order vector derivatives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#first-order-vector-derivatives",
    "href": "02-vector-calculus.html#first-order-vector-derivatives",
    "title": "2  Vector and matrix calculus",
    "section": "",
    "text": "Derivative and gradient\nThe derivative of a scalar-valued function \\(h(\\boldsymbol x)\\) with regard to its vector argument \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\) is the row vector \\[\nD h(\\boldsymbol x) =  \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x}\n= \\begin{pmatrix}\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_1} &\n\\cdots &\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_d} \\\\\n\\end{pmatrix}\n\\]\nThe above notation follows the numerator layout convention, where the dimension of the numerator (here 1) determines the number of rows and the dimension of the denominator (here \\(d\\)) the number of columns of the resulting matrix (see https://en.wikipedia.org/wiki/Matrix_calculus for details). For a scalar function this results in a vector of the same dimension as \\(\\boldsymbol x^T\\).\nThe gradient of \\(h(\\boldsymbol x)\\) is a column vector and the transpose of the derivative \\[\n\\text{grad } h(\\boldsymbol x) = \\left( \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} \\right)^T\n\\] It is often written using the nabla operator \\(\\nabla\\) as \\[\n\\nabla h(\\boldsymbol x) = \\begin{pmatrix}\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_1} \\\\\n\\vdots\\\\\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_d}\n\\end{pmatrix}\n\\] with \\[\n\\nabla = \\begin{pmatrix}\n\\frac{\\partial }{\\partial x_1} \\\\\n\\vdots\\\\\n\\frac{\\partial }{\\partial x_d}\n\\end{pmatrix}\n\\]\nNote that \\[\n(\\nabla h(\\boldsymbol x))^T = \\nabla^T h(\\boldsymbol x) = D h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x}\n\\]\n\nExample 2.1 Examples for the gradient and derivative:\n\n\\(h(\\boldsymbol x)=\\boldsymbol a^T \\boldsymbol x+ b\\).\nThen \\(\\nabla h(\\boldsymbol x) = \\boldsymbol a\\) and \\(D h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol a^T\\).\n\\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol x\\).\nThen \\(\\nabla h(\\boldsymbol x) = 2 \\boldsymbol x\\) and \\(D h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = 2 \\boldsymbol x^T\\).\n\\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol A\\boldsymbol x\\).\nThen \\(\\nabla h(\\boldsymbol x) = (\\boldsymbol A+ \\boldsymbol A^T) \\boldsymbol x\\) and \\(D h(\\boldsymbol x) =  \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol x^T (\\boldsymbol A+ \\boldsymbol A^T)\\).\n\n\n\n\nJacobian matrix\nSimilarly, we can also compute the derivative of a vector-valued function \\[\n\\boldsymbol h(\\boldsymbol x) = ( h_1(\\boldsymbol x), \\ldots, h_m(\\boldsymbol x) )^T\n\\] with regard to \\(\\boldsymbol x\\). This yields (again in numerator layout convention) a matrix of size \\(m\\) rows and \\(d\\) columns whose rows contain the derivatives of the components of \\(\\boldsymbol h(\\boldsymbol x)\\). \\[\n\\begin{split}\nD\\boldsymbol h(\\boldsymbol x) &= \\frac{\\partial \\boldsymbol h(\\boldsymbol x)}{\\partial \\boldsymbol x}  = \\left(\\frac{\\partial h_i(\\boldsymbol x)}{\\partial x_j}\\right) \\\\\n&=\\begin{pmatrix}\n\\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_1} & \\cdots  & \\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_d} \\\\\n\\vdots &\\ddots & \\vdots \\\\\n\\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_1} & \\cdots & \\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_d} \\\\\n\\end{pmatrix} \\\\\n&=\n\\left( {\\begin{array}{c}\n\\nabla^T h_1(\\boldsymbol x)   \\\\\n\\vdots   \\\\\n\\nabla^T h_m(\\boldsymbol x)  \\\\\n\\end{array} } \\right) \\\\\n&= \\boldsymbol J_{\\boldsymbol h}(\\boldsymbol x)\n\\end{split}\n\\]\nThis matrix is also called the Jacobian matrix\n\nExample 2.2 \\(\\boldsymbol h(\\boldsymbol x)=\\boldsymbol A^T \\boldsymbol x+ \\boldsymbol b\\). Then \\(D \\boldsymbol h(\\boldsymbol x) = \\frac{\\partial \\boldsymbol h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol J_{\\boldsymbol h}(\\boldsymbol x) =\\boldsymbol A^T\\).\n\nIf \\(m=d\\) then the Jacobian matrix is a square and this allows to compute the determinant of the Jacobian matrix. Both the Jacobian matrix and the Jacobian determinant are often called “the Jacobian” so one needs to determine from the context whether this refers to the matrix or the determinant.\nIf \\(\\boldsymbol y= \\boldsymbol h(\\boldsymbol x)\\) is an invertible function with \\(\\boldsymbol x= \\boldsymbol h^{-1}(\\boldsymbol y)\\) then the Jacobian matrix is invertible and the inverted matrix is the Jacobian matrix of the inverse function: \\[\nD\\boldsymbol x(\\boldsymbol y) = \\left( D\\boldsymbol y(\\boldsymbol x)\\right)^{-1} \\rvert_{\\boldsymbol x= \\boldsymbol x(\\boldsymbol y)}\n\\] or in alternative notation \\[\n\\frac{\\partial \\boldsymbol x(\\boldsymbol y)}{\\partial \\boldsymbol y} = \\left. \\left( \\frac{\\partial \\boldsymbol y(\\boldsymbol x)}{\\partial \\boldsymbol x}  \\right)^{-1} \\right\\rvert_{\\boldsymbol x= \\boldsymbol x(\\boldsymbol y)}\n\\]\nIn this case the Jacobian determinant of the back-transformation can be computed as the inverse of the Jacobian determinant of the original function: \\[\n\\det D\\boldsymbol x(\\boldsymbol y) = \\det \\left( D\\boldsymbol y(\\boldsymbol x)\\right)^{-1} \\rvert_{\\boldsymbol x= \\boldsymbol x(\\boldsymbol y)}\n\\] and \\[\n\\det \\left(\\frac{\\partial \\boldsymbol x(\\boldsymbol y)}{\\partial \\boldsymbol y}\\right) = \\left. \\det \\left( \\frac{\\partial \\boldsymbol y(\\boldsymbol x)}{\\partial \\boldsymbol x}  \\right)^{-1} \\right\\rvert_{\\boldsymbol x= \\boldsymbol x(\\boldsymbol y)}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#second-order-vector-derivatives",
    "href": "02-vector-calculus.html#second-order-vector-derivatives",
    "title": "2  Vector and matrix calculus",
    "section": "2.2 Second order vector derivatives",
    "text": "2.2 Second order vector derivatives\nThe matrix of all second order partial derivates of scalar-valued function with vector-valued argument is called the Hessian matrix: \\[\n\\begin{split}\n\\nabla \\nabla^T h(\\boldsymbol x) &=\n\\begin{pmatrix}\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1^2}\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1 \\partial x_2}\n     & \\cdots\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1 \\partial x_d} \\\\\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2 \\partial x_1}\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2^2}\n     & \\cdots\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2 \\partial x_d} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d \\partial x_1}\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d \\partial x_2}  \n     & \\cdots\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d^2}\n\\end{pmatrix} \\\\\n&= \\left(\\frac{\\partial h(\\boldsymbol x)}{\\partial x_i \\partial x_j}\\right) \\\\\n& =   \\frac{\\partial }{\\partial \\boldsymbol x} \\left( \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x}\\right)^T \\\\\n& = D (Dh(\\boldsymbol x))^T \\\\\n\\end{split}\n\\] By construction the Hessian matrix is square and symmetric.\n\nExample 2.3 \\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol A\\boldsymbol x\\). Then \\(\\nabla \\nabla^T h(\\boldsymbol x)  =  (\\boldsymbol A+ \\boldsymbol A^T)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#chain-rules-for-gradient-vector-and-hessian-matrix",
    "href": "02-vector-calculus.html#chain-rules-for-gradient-vector-and-hessian-matrix",
    "title": "2  Vector and matrix calculus",
    "section": "2.3 Chain rules for gradient vector and Hessian matrix",
    "text": "2.3 Chain rules for gradient vector and Hessian matrix\nSuppose \\(h(\\boldsymbol x)\\) is a scalar-valued function and \\(g(\\boldsymbol y) = h(\\boldsymbol x(\\boldsymbol y))\\) is a composite scalar-valued function where \\(\\boldsymbol x(\\boldsymbol y)\\) a map from \\(\\boldsymbol y\\) to \\(\\boldsymbol x\\).\nThe gradient of the composite function \\(g(\\boldsymbol y) = h(\\boldsymbol x(\\boldsymbol y))\\) can be computed from the gradient of \\(h(\\boldsymbol x)\\) and the Jacobian matrix for \\(\\boldsymbol x(\\boldsymbol y)\\) as follows: \\[\n\\nabla g(\\boldsymbol y)  =   (D\\boldsymbol x(\\boldsymbol y))^T\\,  \\nabla h(\\boldsymbol x)   \\rvert_{\\boldsymbol x= \\boldsymbol x(\\boldsymbol y)}\n\\] and \\[\n\\nabla g(\\boldsymbol y)  =   \\left(\\frac{\\partial \\boldsymbol x(\\boldsymbol y)}{\\partial \\boldsymbol y}\\right)^T\\,  \\nabla h(\\boldsymbol x)   \\rvert_{\\boldsymbol x= \\boldsymbol x(\\boldsymbol y)}\n\\]\nSimilarly, the Hessian matrix of \\(g(\\boldsymbol y)\\) can be computed from the Hessian of \\(h(\\boldsymbol x)\\) and the Jacobian matrix for \\(\\boldsymbol x(\\boldsymbol y)\\): \\[\n\\nabla \\nabla^T g(\\boldsymbol y) = (D \\boldsymbol x(\\boldsymbol y))^T\\, \\nabla \\nabla^T h(\\boldsymbol x)\\rvert_{\\boldsymbol x= \\boldsymbol x(\\boldsymbol y)}  \\,   D\\boldsymbol x(\\boldsymbol y)\n\\] and \\[\n\\nabla \\nabla^T g(\\boldsymbol y) = \\left(\\frac{\\partial \\boldsymbol x(\\boldsymbol y)}{\\partial \\boldsymbol y}\\right)^T\\,\n\\nabla \\nabla^T h(\\boldsymbol x)\\rvert_{\\boldsymbol x= \\boldsymbol x(\\boldsymbol y)}  \\,  \\frac{\\partial \\boldsymbol x(\\boldsymbol y)}{\\partial \\boldsymbol y}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#first-order-matrix-derivatives",
    "href": "02-vector-calculus.html#first-order-matrix-derivatives",
    "title": "2  Vector and matrix calculus",
    "section": "2.4 First order matrix derivatives",
    "text": "2.4 First order matrix derivatives\nThe derivative of a scalar-valued function \\(h(\\boldsymbol X)\\) with regard to a matrix argument \\(\\boldsymbol X= (x_{ij})\\) is defined as below and and results (in numerator layout convention) in a matrix of the same dimension as \\(\\boldsymbol X^T\\): \\[\nD h(\\boldsymbol X) = \\frac{\\partial h(\\boldsymbol X)}{\\partial \\boldsymbol X}  = \\left(\\frac{\\partial h(\\boldsymbol X)}{\\partial x_{ji}}\\right)\n\\]\n\nExample 2.4 Examples for first order matrix derivatives:\n\n\\(\\frac{\\partial \\text{Tr}(\\boldsymbol A^T \\boldsymbol X)}{\\partial \\boldsymbol X} = \\boldsymbol A^T\\)\n\\(\\frac{\\partial \\text{Tr}(\\boldsymbol A^T \\boldsymbol X\\boldsymbol B)}{\\partial \\boldsymbol X} = \\boldsymbol B\\boldsymbol A^T\\)\n\\(\\frac{\\partial \\text{Tr}(\\boldsymbol X^T \\boldsymbol A\\boldsymbol X)}{\\partial \\boldsymbol X} = \\boldsymbol X^T (\\boldsymbol A+ \\boldsymbol A^T)\\)\n\\(\\frac{\\partial \\log \\det(\\boldsymbol X)}{\\partial \\boldsymbol X} = \\frac{\\partial \\text{Tr}(\\log \\boldsymbol X)}{\\partial \\boldsymbol X} = \\boldsymbol X^{-1}\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#linear-and-quadratic-approximation",
    "href": "02-vector-calculus.html#linear-and-quadratic-approximation",
    "title": "2  Vector and matrix calculus",
    "section": "2.5 Linear and quadratic approximation",
    "text": "2.5 Linear and quadratic approximation\nA linear and quadratic approximation of a differentiable function is given by a Taylor series of first and second order, respectively.\n\nLinear and quadratic approximation of a scalar-valued function of a scalar: \\[\nh(x) \\approx h(x_0) + h'(x_0) \\, (x-x_0) + \\frac{1}{2} h''(x_0) \\, (x-x_0)^2\n\\] Note that \\(h'(x_0) = h'(x) \\,|\\, x_0\\) is first derivative of \\(h(x)\\) evaluated at \\(x_0\\) and \\(h''(x_0) = h''(x) \\,|\\, x_0\\) is the second derivative of \\(h(x)\\) evaluated \\(x_0\\).\nWith \\(x = x_0+ \\varepsilon\\) the approximation can also be written as \\[\nh(x_0+ \\varepsilon) \\approx h(x_0) + h'(x_0) \\, \\varepsilon + \\frac{1}{2} h''(x_0)\\, \\varepsilon^2\n\\] The first two terms on the right comprise the linear approximation, all three terms the quadratic approximation.\nLinear and quadratic approximation of a scalar-valued function of a vector: \\[\nh(\\boldsymbol x) \\approx h(\\boldsymbol x_0) + \\nabla^T h(\\boldsymbol x_0)\\, (\\boldsymbol x-\\boldsymbol x_0) + \\frac{1}{2}\n(\\boldsymbol x-\\boldsymbol x_0)^T \\, \\nabla \\nabla^T h(\\boldsymbol x_0) \\, (\\boldsymbol x-\\boldsymbol x_0)\n\\] Note that \\(\\nabla^T h(\\boldsymbol x_0)\\) is the transposed gradient (i.e the vector derivative) of \\(h(\\boldsymbol x)\\) evaluated at \\(\\boldsymbol x_0\\) and \\(\\nabla \\nabla^T h(\\boldsymbol x_0)\\) the Hessian matrix of \\(h(\\boldsymbol x)\\) evaluated at \\(\\boldsymbol x_0\\). With \\(\\boldsymbol x= \\boldsymbol x_0+ \\boldsymbol \\varepsilon\\) this approximation can also be written as \\[\nh(\\boldsymbol x_0+ \\boldsymbol \\varepsilon) \\approx h(\\boldsymbol x_0) + \\nabla^T h(\\boldsymbol x_0)\\, \\boldsymbol \\varepsilon+ \\frac{1}{2} \\boldsymbol \\varepsilon^T \\, \\nabla \\nabla^T h(\\boldsymbol x_0) \\,\\boldsymbol \\varepsilon\n\\] The first two terms on the right comprise the linear approximation, all three terms the quadratic approximation.\nLinear approximation of a vector-valued function of a vector: \\[\n\\boldsymbol h(\\boldsymbol x) \\approx \\boldsymbol h(\\boldsymbol x_0) + D \\boldsymbol h(\\boldsymbol x_0) \\, (\\boldsymbol x-\\boldsymbol x_0)\n\\] Note that \\(D \\boldsymbol h(\\boldsymbol x_0)\\) is Jacobian matrix (i.e the vector derivative) of \\(\\boldsymbol h(\\boldsymbol x)\\) evaluated at \\(\\boldsymbol x_0\\). With \\(\\boldsymbol x= \\boldsymbol x_0+ \\boldsymbol \\varepsilon\\) this approximation can also be written as \\[\n\\boldsymbol h(\\boldsymbol x_0+ \\boldsymbol \\varepsilon) \\approx h(\\boldsymbol x_0) + D \\boldsymbol h(\\boldsymbol x_0) \\, \\boldsymbol \\varepsilon\n\\]\n\n\nExample 2.5 Examples of Taylor series approximations of second order:\n\n\\(\\log(x_0+\\varepsilon) \\approx \\log(x_0) + \\frac{\\varepsilon}{x_0} - \\frac{\\varepsilon^2}{2 x_0^2}\\)\n\\(\\frac{x_0}{x_0+\\varepsilon} \\approx 1 - \\frac{\\varepsilon}{x_0} + \\frac{\\varepsilon^2}{ x_0^2}\\)\n\n\n\nExample 2.6 Around a local extremum \\(\\boldsymbol x_0\\) (maximum or minimum) where the gradient vanishes (\\(h(\\boldsymbol x_0) = 0\\)) the quadratic approximation of the function \\(h(\\boldsymbol x)\\) simplifies to\n\\[\nh(\\boldsymbol x_0+ \\boldsymbol \\varepsilon) \\approx h(\\boldsymbol x_0) +  \\frac{1}{2} \\boldsymbol \\varepsilon^T \\nabla \\nabla^T h(\\boldsymbol x_0) \\boldsymbol \\varepsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#conditions-for-a-local-extremum-of-a-function",
    "href": "02-vector-calculus.html#conditions-for-a-local-extremum-of-a-function",
    "title": "2  Vector and matrix calculus",
    "section": "2.6 Conditions for a local extremum of a function",
    "text": "2.6 Conditions for a local extremum of a function\nTo check if \\(x_0\\) or \\(\\boldsymbol x_0\\) is a local extremum, i.e. a local maximum or a local minimum, of a differentiable function \\(h(x)\\) or \\(h(\\boldsymbol x)\\) we can use the following conditions:\nFor a function of a single variable:\n\nFirst derivative is zero at the extremum: \\(h'(x_0) = 0\\).\nIf the second derivative \\(h''(x_0) &lt; 0\\) at the extremum is negative then it is a maximum.\nIf the second derivative \\(h''(x_0) &gt; 0\\) at the extremum is positive it is a minimum.\n\nNote that conditions ii) and iii) are sufficient but not necessary. For a minimum, it is necessary that the second derivative is non-negative, and for a maximum that the second derivative is non-positive.\nFor a function of several variables:\n\nGradient vanishes at extremum: \\(\\nabla h(\\boldsymbol x_0)=0\\).\nIf the Hessian \\(\\nabla \\nabla^T h(\\boldsymbol x_0)\\) is negative definite (= all eigenvalues of Hessian matrix are negative) then the extremum is a maximum.\nIf the Hessian is positive definite (= all eigenvalues of Hessian matrix are positive) then the extremum is a minimum.\n\nAgain, conditions ii) and iii) are sufficient but not necessary. For a minimum it is necessary that the Hessian is positive semi-definite, and for a maximum that the Hessian is negative semi-definite.\n\nExample 2.7 Minimum with vanishing second derivative:\n\\(x^4\\) clearly has a minimum at \\(x_0=0\\). As required the first derivative \\(4 x^3\\) vanishes at \\(x_0=0\\). However, the second derivative \\(12 x^2\\) also vanishes at \\(x_0=0\\), showing that a positive second derivative is not necessary for a minimum.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#convex-and-concave-functions",
    "href": "02-vector-calculus.html#convex-and-concave-functions",
    "title": "2  Vector and matrix calculus",
    "section": "2.7 Convex and concave functions",
    "text": "2.7 Convex and concave functions\nA function \\(h(\\boldsymbol x)\\) is convex if for all \\(\\boldsymbol x_1\\) and \\(\\boldsymbol x_2\\) the line segment from point \\((\\boldsymbol x_1, h(\\boldsymbol x_1))\\) to point \\((\\boldsymbol x_2, h(\\boldsymbol x_2))\\) never lies below the function. Moreover, the function is strictly convex if the line segment always lies above the curve, apart from the two end points:\n\\[\n\\lambda h(\\boldsymbol x_1) + (1-\\lambda) h(\\boldsymbol x_2) \\geq h(\\lambda \\boldsymbol x_1 + (1-\\lambda) \\boldsymbol x_2)\n\\] for all \\(\\lambda \\in [0, 1]\\).\nEquivalently, a differentiable function \\(h(\\boldsymbol x)\\) is convex (strictly convex) if for all \\(\\boldsymbol x_0\\) the function \\(h(\\boldsymbol x)\\) never lies below (always lies above, except at \\(\\boldsymbol x_0\\)) the linear approximation through the point \\((\\boldsymbol x_0, h(\\boldsymbol x_0))\\): \\[\nh(\\boldsymbol x) \\geq h(\\boldsymbol x_0) + \\nabla^T h(\\boldsymbol x_0)\\, (\\boldsymbol x-\\boldsymbol x_0)\n\\]\nFor a convex function a vanishing gradient at \\(\\boldsymbol x_0\\) indicates a minimum at \\(\\boldsymbol x_0\\). Furthermore, any local minimum must also be a global minimum (for a differentiable function this follows directly from the last inequality). For a strictly convex function the minimum is unique so there is at most one local/global minimum in that case.\nIf \\(h(\\boldsymbol x)\\) is convex, then \\(-h(\\boldsymbol x)\\) is concave, and the criteria above can be adapted accordingly to check for concavity and strict concavity, as well as to identify local/global maxima.\n(Strictly) convex and concave functions are convenient objective functions in optimisation as it is straightforward to find their local/global extrema, both analytically and numerically.\nAs the shape of a convex function resembles that of a valley, one way to memorise that fact is that a valley is convex.\n\nExample 2.8 Convex functions:\nThis is a convex function but not a strictly convex function:\n\n\\(\\max(x^2, | x |  )\\)\n\nThe following are strictly convex functions:\n\n\\(x^2\\),\n\\(x^4\\),\n\\(e^x\\),\n\\(x \\log(x)\\) for \\(x&gt;0\\).\n\nOn the other hand, this is not a convex function:\n\n\\(\\frac{1}{x^2}\\) for all \\(x \\neq 0\\).\n\nHowever, the function in last example is strictly convex if the domain is restricted to either \\(x &gt;0\\) or \\(x&lt;0\\).\n\n\nExample 2.9 Concave functions:\nThe following are strictly concave functions:\n\n\\(-x^2\\),\n\\(\\log(x)\\) for \\(x&gt;0\\),\n\\(\\sqrt{x}\\) for \\(x&gt;0\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  }
]