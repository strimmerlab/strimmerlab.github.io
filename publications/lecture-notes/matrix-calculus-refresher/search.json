[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matrix and Calculus Refresher",
    "section": "",
    "text": "Welcome\nThe Matrix and Calculus Refresher notes were written by Korbinian Strimmer from 2018–2024. This version is from 6 March 2024.\nIf you have any questions, comments, or corrections please get in touch! 1",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Matrix and Calculus Refresher",
    "section": "Updates",
    "text": "Updates\nThe notes will be updated from time to time. To view the current version visit the\n\nonline version of the Matrix and Calculus Refresher notes.\n\nYou may also wish to download the Matrix and Calculus Refresher notes as\n\nPDF in A4 format for printing (double page layout), or as\n6x9 inch PDF for use on tablets (single page layout).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Matrix and Calculus Refresher",
    "section": "License",
    "text": "License\nThese notes are licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Matrix and Calculus Refresher",
    "section": "",
    "text": "Email address: korbinian.strimmer@manchester.ac.uk↩︎",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "00-preface.html",
    "href": "00-preface.html",
    "title": "Preface",
    "section": "",
    "text": "About the author\nHello! My name is Korbinian Strimmer and I am a Professor in Statistics. I am a member of the Statistics group at the Department of Mathematics of the University of Manchester. You can find more information about me on my home page.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#about-the-notes",
    "href": "00-preface.html#about-the-notes",
    "title": "Preface",
    "section": "About the notes",
    "text": "About the notes\nIn statistics and machine learning we make frequent use of matrix notation, matrix algebra, matrix decompositions and also of vector and matrix calculus. The aim of these supplementary notes is to provide a refresher for students to quickly gain a working knowledge about matrices and the calculus of functions with several variables.\nThe notes are supporting information for a number of lecture notes of statistical courses I am or have been teaching at the Department of Mathematics of the University of Manchester.\nThis includes the currently offered modules:\n\nMATH27720 Statistics 2: Likelihood and Bayes and\nMATH38161 Multivariate Statistics,\n\nas well as the retired module (not offered any more):\n\nMATH20802 Statistical Methods.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html",
    "href": "01-matrix-essentials.html",
    "title": "1  Matrix essentials",
    "section": "",
    "text": "1.1 Overview\nIn statistics we will frequently make use of matrix calculations and matrix notation.\nThroughout we mostly work with real matrices, i.e. we assume all matrix elements are real numbers. However, one important matrix decomposition — the eigenvalue decomposition — can yield complex-valued matrices even when applied to real matrices. Thus occasionally we will also need to deal also with complex numbers.\nFor further details on matrix theory please consult the lecture notes of related modules (e.g. linear algebra).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#matrix-basics",
    "href": "01-matrix-essentials.html#matrix-basics",
    "title": "1  Matrix essentials",
    "section": "1.2 Matrix basics",
    "text": "1.2 Matrix basics\n\n1.2.1 Matrix notation\nIn matrix notation we distinguish between scalars, vectors, and matrices:\nScalar: \\(x\\), \\(X\\), lower or upper case, plain type.\nVector: \\(\\symbfit x\\), lower case, bold type. In handwriting an arrow \\(\\vec{x}\\) indicates a vector.\nIn component notation we write \\(\\symbfit x= \\begin{pmatrix} x_1 \\\\ \\vdots\\\\ x_d\\end{pmatrix}\\). By default, a vector is a column vector, i.e. the elements are arranged in a column and index of the components \\(x_i\\) refers to the row.\nThe transpose of a vector (indicated by the superscript \\(T\\)) turns it into a row vector. To save space we can write the column vector \\(\\symbfit x\\) as \\(\\symbfit x= (x_1, \\ldots, x_d)^T\\) so that \\(\\symbfit x^T\\) is a row vector.\nMatrix: \\(\\symbfit X\\), upper case, bold type. In handwriting an underscore \\(\\underline{X}\\) indicates a matrix.\nIn component notation we write \\(\\symbfit X= (x_{ij})\\). By convention, the first index (here \\(i\\)) of the scalar elements \\(x_{ij}\\) denotes the row and the second index (here \\(j\\)) the column of the matrix. For \\(n\\) the number of rows and \\(d\\) the number of columns we can view the matrix \\(\\symbfit X= (\\symbfit x_1, \\ldots, \\symbfit x_d)\\) either as being composed of \\(d\\) column vectors \\(\\symbfit x_j = \\begin{pmatrix} x_{1j} \\\\ \\vdots \\\\ x_{nj}\\\\ \\end{pmatrix}\\) or \\(\\symbfit X= \\begin{pmatrix} \\symbfit z_1^T \\\\ \\vdots \\\\ \\symbfit z_n^T\\end{pmatrix}\\) being composed of \\(n\\) row vectors \\(\\symbfit z_i^T = (x_{i1},  \\ldots,  x_{id})\\).\nA (column) vector of dimension \\(d\\) is a matrix of size \\(d\\times 1\\). A row vector of dimension \\(d\\) is a matrix of size \\(1\\times d\\). A scalar is of dimension \\(1\\) and is a matrix of size \\(1 \\times 1\\).\n\n\n1.2.2 Notation for random vectors and matrices\nA random matrix (vector) is a matrix (vector) whose elements are random variables.\nIt is common practise in univariate statistics to distinguish random variables and their fixed realisations by using upper case versus lower case. However, this convention breaks down when working with matrices and vectors.\nTherefore, when working with multivariate random quantities it is best practise (see e.g. Mardia et al. 1979)1 to always state explicitly whether a matrix, vector or scalar is a random variable, especially when this is not obvious from the context.\n\n\n1.2.3 Special matrices\n\\(\\symbfit I_d\\) is the identity matrix. It is a square matrix of size \\(d \\times d\\) with the diagonal filled with 1 and off-diagonals filled with 0. \\[\\symbfit I_d =\n\\begin{pmatrix}\n    1 & 0 & 0 & \\dots & 0\\\\\n    0 & 1 & 0 & \\dots & 0\\\\\n    0 & 0 & 1 &   & 0\\\\\n    \\vdots & \\vdots & & \\ddots &  \\\\\n    0 & 0 & 0 &  & 1 \\\\\n\\end{pmatrix}\\]\n\\(\\symbfup 1\\) is a matrix that contains only ones. Most often it is used in the form of a column vector with \\(d\\) rows: \\[\\symbfup 1_d =\n\\begin{pmatrix}\n    1 \\\\\n    1 \\\\\n    1 \\\\\n    \\vdots   \\\\\n    1  \\\\\n\\end{pmatrix}\\]\nSimilarly, \\(\\symbfup 0\\) is a matrix that contains only zeros. Most often it is used in the form of a column vector with \\(d\\) rows: \\[\\symbfup 0_d =\n\\begin{pmatrix}\n    0 \\\\\n    0 \\\\\n    0 \\\\\n    \\vdots   \\\\\n    0  \\\\\n\\end{pmatrix}\\]\nA diagonal matrix is a matrix where all off-diagonal elements are zero. By \\(\\text{Diag}(\\symbfit A)\\) we access the diagonal elements of a matrix as vector and by \\(\\text{Diag}(a_1, \\ldots, a_d)\\) we specify a diagonal matrix by listing the diagonal elements.\nAny matrix can be partitioned into blocks or submatrices. A block-structured matrix or block matrix partioning rows and columns into two groups has the form \\[\n\\symbfit A= \\begin{pmatrix} \\symbfit A_{11} & \\symbfit A_{12} \\\\ \\symbfit A_{21} & \\symbfit A_{22} \\\\ \\end{pmatrix} \\, ,\n\\] where \\(\\symbfit A_{11}\\), \\(\\symbfit A_{22}\\), \\(\\symbfit A_{12}\\) and \\(\\symbfit A_{21}\\) are themselves matrices. If \\(\\symbfit A\\) is symmetric (hence square) then \\(\\symbfit A_{11}\\) and \\(\\symbfit A_{22}\\) must also be symmetric and \\(\\symbfit A_{21} = \\symbfit A_{12}^T\\).\nA block diagonal matrix is a symmetric block matrix with vanishing off-diagonal blocks and symmetric submatrices along the diagonal.\nA triangular matrix is a square matrix whose elements either below or above the diagonal are all zero (upper vs. lower triangular matrix).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#simple-matrix-operations",
    "href": "01-matrix-essentials.html#simple-matrix-operations",
    "title": "1  Matrix essentials",
    "section": "1.3 Simple matrix operations",
    "text": "1.3 Simple matrix operations\n\n1.3.1 Matrix addition and multiplication\nMatrices behave much like common numbers. For example, we can add matrices \\(\\symbfit C= \\symbfit A+ \\symbfit B\\) and multiply matrices \\(\\symbfit C= \\symbfit A\\symbfit B\\).\nFor matrix addition \\(\\symbfit C= \\symbfit A+ \\symbfit B\\) we add the corresponding elements \\(c_{ij} = a_{ij} + b_{ij}\\). For matrix addition \\(\\symbfit A\\) and \\(\\symbfit B\\) must have the same dimensions, i.e. the same number of rows and columns.\nThe dot product, or scalar product, of two vectors \\(\\symbfit a\\) and \\(\\symbfit b\\) is a scalar given by \\(\\symbfit a\\cdot \\symbfit b= \\langle \\symbfit a, \\symbfit b\\rangle  = \\symbfit a^T \\symbfit b= \\symbfit b^T \\symbfit a= \\sum_{i=1}^d a_{i} b_{i}\\).\nMatrix multiplication \\(\\symbfit C= \\symbfit A\\symbfit B\\) is obtained by setting \\(c_{ij} = \\sum_{k=1}^m a_{ik} b_{kj}\\) where \\(m\\) is the number of columns of \\(\\symbfit A\\) and the number of rows in \\(\\symbfit B\\). Thus, \\(\\symbfit C\\) contains all possible dot products of the row vectors in \\(\\symbfit A\\) with the column vectors in \\(\\symbfit B\\). For matrix multiplication the number of columns in \\(\\symbfit A\\) must match the number of rows in \\(\\symbfit B\\). Note that matrix multiplication in general (for \\(m &gt; 1\\)) does not commute, i.e. \\(\\symbfit A\\symbfit B\\neq \\symbfit B\\symbfit A\\).\n\n\n1.3.2 Matrix transpose\nThe matrix transpose \\(\\symbfit A^T\\) indicate by the superscript \\(T\\) interchanges rows and columns of a matrix. The transpose is a linear operator \\((\\symbfit A+ \\symbfit B)^T = \\symbfit A^T + \\symbfit B^T\\) and applied to a matrix product it reverses the ordering, i.e. \\((\\symbfit A\\symbfit B)^T =\\symbfit B^T \\symbfit A^T\\).\nIf \\(\\symbfit A= \\symbfit A^T\\) then \\(\\symbfit A\\) is symmetric (and square).\nBy construction given a rectangular \\(\\symbfit A\\) the matrices \\(\\symbfit A^T \\symbfit A\\) and \\(\\symbfit A\\symbfit A^T\\) are symmetric with non-negative diagonal.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#matrix-summaries",
    "href": "01-matrix-essentials.html#matrix-summaries",
    "title": "1  Matrix essentials",
    "section": "1.4 Matrix summaries",
    "text": "1.4 Matrix summaries\n\n1.4.1 Row, column and grand sum\nAssume a matrix \\(\\symbfit A\\) of size \\(n \\times m\\).\nThe sum over the \\(m\\) entries of row \\(i\\) is \\(\\sum_{j=1}^m a_{ij}\\). In matrix notation the \\(n\\) row sums are given by \\(\\symbfit A\\, \\symbfup 1_m\\).\nThe sum over the \\(n\\) entries of column \\(j\\) is \\(\\sum_{i=1}^n a_{ij}\\). In matrix notation the \\(m\\) column sums are \\(\\symbfit A^T \\symbfup 1_n\\).\nThe grand sum of all matrix entries of \\(\\symbfit A\\) is obtained by \\[\n\\sum_{i=1}^n \\sum_{j=1}^m a_{ij} = \\symbfup 1_n^T \\, \\symbfit A\\, \\symbfup 1_m\n\\]\n\n\n1.4.2 Matrix trace\nThe trace of the matrix is the sum of the diagonal elements \\(\\text{Tr}(\\symbfit A) = \\sum a_{ii}\\).\nThe trace is invariant against transposition, i.e. \\[\n\\text{Tr}(\\symbfit A) = \\text{Tr}(\\symbfit A^T )\n\\]\nA useful identity for the matrix trace of the product of two matrices is \\[\n\\text{Tr}(\\symbfit A\\symbfit B) = \\text{Tr}( \\symbfit B\\symbfit A)  \n\\]\nIntriguingly, the trace of a matrix equals the sum of the eigenvalues of the matrix (see further below).\n\n\n1.4.3 Row, column and grand sum of squares\nThe sum over the \\(m\\) squared entries of row \\(i\\) is \\(\\sum_{j=1}^m a_{ij}^2\\). In matrix notation the \\(n\\) row sums of squares are given by \\(\\text{Diag}( \\symbfit A\\symbfit A^T)\\).\nThe sum over the \\(n\\) squared entries of column \\(j\\) is \\(\\sum_{i=1}^n a_{ij}^2\\). In matrix notation the \\(m\\) column sums of squares are \\(\\text{Diag}( \\symbfit A^T \\symbfit A)\\).\nThe grand sum of all squared elements of \\(\\symbfit A\\) is obtained by \\[\n\\sum_{i=1}^n \\sum_{j=1}^m a_{ij}^2 = \\text{Tr}(\\symbfit A^T \\symbfit A) = \\text{Tr}(\\symbfit A\\symbfit A^T)\n\\] This is also known as the squared Frobenius norm of \\(\\symbfit A\\) (see below).\n\n\n1.4.4 Sum of squared diagonal entries\nThe sum of the squared entries on the diagonal is in matrix notation \\[\n\\text{Diag}(\\symbfit A)^T \\text{Diag}(\\symbfit A) = \\sum_{i=1}^{\\min(n,m)} a_{ii}^2\n\\]\n\n\n1.4.5 Frobenius inner product\nThe Frobenius inner product between two rectangular matrices of the same dimension is the scalar \\[\n\\begin{split}\n\\langle \\symbfit A, \\symbfit B\\rangle &=  \\text{Tr}(\\symbfit A\\symbfit B^T) = \\text{Tr}(\\symbfit B\\symbfit A^T)\\\\\n&=  \\text{Tr}(\\symbfit A^T \\symbfit B) = \\text{Tr}(\\symbfit B^T \\symbfit A)\\\\\n&= \\sum_{i,j} a_{ij} b_{ij} \\,.\n\\end{split}\n\\] This generalises the dot product between two vectors. Note that the dot product can therefore also be written as the trace of a matrix \\[\n\\langle \\symbfit a, \\symbfit b\\rangle = \\text{Tr}( \\symbfit a\\symbfit b^T ) = \\text{Tr}( \\symbfit b\\symbfit a^T) \\,.\n\\]\n\n\n1.4.6 Euclidean norm\nThe squared Euclidean norm or the squared length of the vector \\(\\symbfit a\\) is the dot product \\(||\\symbfit a||^2_2 = \\symbfit a\\cdot \\symbfit a=  \\langle \\symbfit a, \\symbfit a\\rangle =   \\symbfit a^T \\symbfit a= \\symbfit a\\symbfit a^T = \\sum_{i=1}^d a_i^2\\).\nThe squared Frobenius norm is a generalisation of the squared Euclidean vector norm to a rectangular matrix and is the sum of the squares of all its elements. Using the trace it can be written as \\[\n\\begin{split}\n||\\symbfit A||_F^2 &= \\langle \\symbfit A, \\symbfit A\\rangle \\\\\n&= \\text{Tr}(\\symbfit A^T \\symbfit A) = \\text{Tr}(\\symbfit A\\symbfit A^T) \\\\\n&= \\sum_{i,j} a_{ij}^2 \\,.\n\\end{split}\n\\]\nA useful identity for the squared Frobenius norm of the difference of two matrices is \\[\n\\begin{split}\n||\\symbfit A- \\symbfit B||_F^2 &= ||\\symbfit A||_F^2 + ||\\symbfit B||_F^2  - 2 \\langle \\symbfit A, \\symbfit B\\rangle \\\\\n&= \\text{Tr}(\\symbfit A^T \\symbfit A) +\\text{Tr}(\\symbfit B^T \\symbfit B) - 2 \\text{Tr}(\\symbfit A^T \\symbfit B) \\\\\n&= \\sum_{i,j} (a_{ij}-b_{ij})^2 \\,.\n\\end{split}\n\\]\nThe Frobenius norm of a matrix \\(||\\symbfit A||_F\\) is not to be confused with the induced \\(2\\)-norm of a matrix \\(||\\symbfit A||_2\\). The latter equals the maximum absolute eigenvalue of the matrix, with \\(||\\symbfit A||_2 \\leq ||\\symbfit A||_F\\).\n\n\n1.4.7 Determinant of a matrix\nIf \\(\\symbfit A\\) is a square matrix the determinant \\(\\det(\\symbfit A)\\) is a scalar measuring the volume spanned by the column vectors in \\(\\symbfit A\\) with the sign determined by the orientation of the vectors.\nIf \\(\\det(\\symbfit A) \\neq 0\\) the matrix \\(\\symbfit A\\) is non-singular or non-degenerate. Conversely, if \\(\\det(\\symbfit A) =0\\) the matrix \\(\\symbfit A\\) is singular or degenerate.\nIntriguingly, the determinant of \\(\\symbfit A\\) is the product of the eigenvalues of \\(\\symbfit A\\) (see further below).\nOne way to compute the determinant of a matrix \\(\\symbfit A\\) is the Laplace cofactor expansion approach that proceeds recursively based on the determinants of the submatrices \\(\\symbfit A_{-i,-j}\\) obtained by deleting row \\(i\\) and column \\(j\\) from \\(\\symbfit A\\). Specifically, at each level we compute the\n\ncofactor expansion either\n\nalong the \\(i\\)-th row — pick any row \\(i\\): \\[\\det(\\symbfit A) = \\sum_{j=1}^d a_{ij} (-1)^{i+j} \\det(\\symbfit A_{-i,-j})  \\text{ , or}\\]\nalong the \\(j\\)-th column — pick any \\(j\\): \\[\\det(\\symbfit A) = \\sum_{i=1}^d a_{ij} (-1)^{i+j} \\det(\\symbfit A_{-i,-j})\\].\n\nThen repeat until the submatrix is a scalar \\(a\\) and \\(\\det(a)=a \\,.\\)\n\nThe recursive nature of this algorithm leads to a complexity of order \\(O(d!)\\) so it is not practical except for very small \\(d\\). Therefore, in practice other more efficient algorithms for computing determinants are used but these still have algorithmic complexity in the order of \\(O(d^3)\\) so for large dimensions obtaining determinants is very expensive.\nHowever, some specially structured matrices do allow for very fast calculation.\nThe determinant of a triangular matrix (and thus also of a diagonal matrix) \\[\n\\symbfit A= \\begin{pmatrix}\na_{11} & 0       & \\cdots & 0\\\\\na_{21} & a_{22}  & \\cdots & 0\\\\\n\\vdots  & \\vdots & \\ddots & 0 \\\\\na_{d1} & a_{d2} & \\cdots & a_{dd} \\\\\n\\end{pmatrix}\n\\] is the product of its diagonal elements, i.e. \\(\\det(\\symbfit A) = \\prod_{i=1}^d a_{ii}\\).\nFor a two-dimensional matrix \\(\\symbfit A= \\begin{pmatrix} a_{11} & a_{12} \\\\  a_{21} & a_{22} \\\\\\end{pmatrix}\\) the determinant is \\(\\det(A) = a_{11} a_{22} - a_{12} a_{21}\\).\nFor a block-structured square matrix \\[\n\\symbfit A= \\begin{pmatrix} \\symbfit A_{11} & \\symbfit A_{12} \\\\ \\symbfit A_{21} & \\symbfit A_{22} \\\\ \\end{pmatrix} \\, ,\n\\] where the matrices on the diagonal \\(\\symbfit A_{11}\\) and \\(\\symbfit A_{22}\\) are themselves square but \\(\\symbfit A_{12}\\) and \\(\\symbfit A_{21}\\) may be rectangular, the determinant is \\[\n\\det(\\symbfit A) = \\det(\\symbfit A_{22}) \\det(\\symbfit C_1) = \\det(\\symbfit A_{11}) \\det(\\symbfit C_2)\n\\] with the (Schur complement of \\(\\symbfit A_{22}\\)) \\[\n\\symbfit C_1 = \\symbfit A_{11} -  \\symbfit A_{12}  \\symbfit A_{22}^{-1}  \\symbfit A_{21}\n\\] and (Schur complement of \\(\\symbfit A_{11}\\)) \\[\n\\symbfit C_2 = \\symbfit A_{22} -  \\symbfit A_{21}  \\symbfit A_{11}^{-1}  \\symbfit A_{12}\n\\] Note that \\(\\symbfit C_1\\) and \\(\\symbfit C_2\\) are square matrices.\nFor a block-diagonal matrix \\(\\symbfit A\\) with \\(\\symbfit A_{12} = 0\\) and \\(\\symbfit A_{21} = 0\\) the determinant is \\(\\det(\\symbfit A) =  \\det(\\symbfit A_{11})  \\det(\\symbfit A_{22})\\), i.e. the product of the determinants of the submatrices along the diagonal.\nDeterminants have a multiplicative property, \\[\\det(\\symbfit A\\symbfit B) = \\det(\\symbfit B\\symbfit A) = \\det(\\symbfit A) \\det(\\symbfit B) \\,.\\] In the above \\(\\symbfit A\\) and \\(\\symbfit B\\) are both square and of the same dimension.\nFor rectangular \\(\\symbfit A\\) (\\(n \\times m\\)) and rectangular \\(\\symbfit B\\) (\\(m \\times n\\)) with \\(m \\geq n\\) this generalises to the Cauchy-Binet formula \\[\n\\det(\\symbfit A\\symbfit B) = \\sum_{w} \\det(\\symbfit A_{,w}) \\det(\\symbfit B_{w,})\n\\] where the summation is over all \\(\\binom{m}{n}\\) index subsets \\(w\\) of size \\(n\\) taken from \\(\\{1, \\ldots, m\\}\\) keeping the ordering and \\(\\symbfit A_{,w}\\) and \\(\\symbfit B_{w,}\\) are the corresponding square \\(n \\times n\\) submatrices. If \\(m &lt; n\\) then \\(\\det(\\symbfit A\\symbfit B) = 0\\).\nFor scalar \\(a\\) \\(\\det(a \\symbfit B) = a^d \\det(\\symbfit B)\\) where \\(d\\) is the dimension of \\(\\symbfit B\\).\nAnother important identity is \\[\\det(\\symbfit I_n + \\symbfit A\\symbfit B) = \\det(\\symbfit I_m + \\symbfit B\\symbfit A)\\] where \\(\\symbfit A\\) and \\(\\symbfit B\\) are rectangular matrices. This is called the Weinstein-Aronszajn determinant identity (also credited to Sylvester).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#matrix-inverse",
    "href": "01-matrix-essentials.html#matrix-inverse",
    "title": "1  Matrix essentials",
    "section": "1.5 Matrix inverse",
    "text": "1.5 Matrix inverse\n\n1.5.1 Inversion of square matrix\nIf \\(\\symbfit A\\) is a square matrix then the inverse matrix \\(\\symbfit A^{-1}\\) is a matrix such that \\[\\symbfit A^{-1} \\symbfit A= \\symbfit A\\symbfit A^{-1}=  \\symbfit I\\, .\\] Only non-singular matrices with \\(\\det(\\symbfit A) \\neq 0\\) are invertible.\nAs \\(\\det(\\symbfit A^{-1} \\symbfit A) = \\det(\\symbfit I) = 1\\) the determinant of the inverse matrix equals the inverse determinant, \\[\\det(\\symbfit A^{-1}) = \\det(\\symbfit A)^{-1} \\,.\\]\nThe transpose of the inverse is the inverse of the transpose as \\[\n\\begin{split}\n(\\symbfit A^{-1})^T &= (\\symbfit A^{-1})^T \\,  \\symbfit A^T (\\symbfit A^{T})^{-1}   \\\\\n&= (\\symbfit A\\symbfit A^{-1})^T \\, (\\symbfit A^{T})^{-1} = (\\symbfit A^{T})^{-1} \\,. \\\\\n\\end{split}\n\\]\nThe inverse of a matrix product \\((\\symbfit A\\symbfit B)^{-1} = \\symbfit B^{-1} \\symbfit A^{-1}\\) is the product of the indivdual matrix inverses in reverse order.\nThere are many different algorithms to compute the inverse of a matrix (which is essentially a problem of solving a system of equations). The computational complexity of matrix inversion is of the order \\(O(d^3)\\) where \\(d\\) is the dimension of \\(\\symbfit A\\). Therefore matrix inversion is very costly in higher dimensions.\n\nInversion of a \\(2 \\times 2\\) matrix:\nThe inverse of the matrix \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\) is \\(A^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\\)\n\n\n\n1.5.2 Inversion of structured matrices\nHowever, for specially structured matrices inversion can be done effectively:\n\nThe inverse of a diagonal matrix is another diagonal matrix obtained by inverting the diagonal elements.\nMore generally, the inverse of a block-diagonal matrix is obtained by individually inverting the blocks along the diagonal.\n\nThe Woodbury matrix identity simplifies the inversion of matrices that can be written as \\(\\symbfit A+ \\symbfit U\\symbfit B\\symbfit V\\) where \\(\\symbfit A\\) and \\(\\symbfit B\\) are both square and \\(\\symbfit U\\) and \\(\\symbfit V\\) are suitable rectangular matrices: \\[\n(\\symbfit A+ \\symbfit U\\symbfit B\\symbfit V)^{-1} = \\symbfit A^{-1} - \\symbfit A^{-1} \\symbfit U(\\symbfit B^{-1} + \\symbfit V\\symbfit A^{-1} \\symbfit U)^{-1} \\symbfit V\\symbfit A^{-1}\n\\] Typically, the inverse \\(\\symbfit A^{-1}\\) is either already known or can be easily obtained and the dimension of \\(\\symbfit B\\) is much lower than that of \\(\\symbfit A\\).\nThe class of matrices that can be most easily inverted are orthogonal matrices whose inverse is obtained simply by transposing the matrix.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#orthogonal-matrices",
    "href": "01-matrix-essentials.html#orthogonal-matrices",
    "title": "1  Matrix essentials",
    "section": "1.6 Orthogonal matrices",
    "text": "1.6 Orthogonal matrices\n\n1.6.1 Properties\nAn orthogonal matrix \\(\\symbfit Q\\) is a square matrix with the property that \\(\\symbfit Q^T = \\symbfit Q^{-1}\\), i.e. the transpose is also the inverse. This implies that \\(\\symbfit Q\\symbfit Q^T = \\symbfit Q^T \\symbfit Q= \\symbfit I\\).\nBoth the column and the row vectors in \\(\\symbfit Q\\) all have length 1. This implies that each element \\(q_{ij}\\) of \\(\\symbfit Q\\) can only take a value in the interval \\([-1, 1]\\).\nThe identity matrix \\(\\symbfit I\\) is the simplest example of an orthogonal matrix.\nThe squared Euclidean and Frobenius norm is preserved when a vector \\(\\symbfit a\\) or matrix \\(\\symbfit A\\) is multiplied with an orthogonal matrix \\(\\symbfit Q\\): \\[\n|| \\symbfit Q\\symbfit a||^2_2 = (\\symbfit Q\\symbfit a)^T \\symbfit Q\\symbfit a= \\symbfit a^T \\symbfit a= || \\symbfit a||^2_2\n\\] and \\[\n|| \\symbfit Q\\symbfit A||^2_F = \\text{Tr}\\left((\\symbfit Q\\symbfit A)^T \\symbfit Q\\symbfit A\\right) = \\text{Tr}\\left(\\symbfit A^T \\symbfit A\\right) = || \\symbfit A||^2_F\n\\]\nMultiplication of \\(\\symbfit Q\\) with a vector results in a new vector of the same length but with a change in direction (unless \\(\\symbfit Q=\\symbfit I\\)). An orthogonal matrix \\(\\symbfit Q\\) can thus be interpreted geometrically as an operator performing rotation, reflection and/or permutation.\nThe product \\(\\symbfit Q_3 = \\symbfit Q_1 \\symbfit Q_2\\) of two orthogonal matrices \\(\\symbfit Q_1\\) and \\(\\symbfit Q_2\\) yields another orthogonal matrix as \\(\\symbfit Q_3 \\symbfit Q_3^T = \\symbfit Q_1 \\symbfit Q_2  (\\symbfit Q_1 \\symbfit Q_2)^T = \\symbfit Q_1 \\symbfit Q_2 \\symbfit Q_2^T \\symbfit Q_1^T = \\symbfit I\\).\nThe determinant \\(\\det(\\symbfit Q)\\) of an orthogonal matrix is either +1 or -1, because \\(\\symbfit Q\\symbfit Q^T = \\symbfit I\\) and thus \\(\\det(\\symbfit Q)\\det(\\symbfit Q^T) = \\det(\\symbfit Q)^2 = \\det(\\symbfit I) = 1\\).\nThe set of all orthogonal matrices of dimension \\(d\\) together with multiplication form a group called the orthogonal group \\(O(d)\\). The subset of orthogonal matrices with \\(\\det(\\symbfit Q)=1\\) are called rotation matrices and form with multiplication the special orthogonal group \\(SO(d)\\). Orthogonal matrices with \\(\\det(\\symbfit Q)=-1\\) are rotation-reflection matrices.\n\n\n1.6.2 Semi-orthogonal matrices\nA rectangular \\(d \\times k\\) matrix \\(\\symbfit Q\\) is semi-orthogonal if for \\(k &lt; d\\) the \\(k\\) column vectors are orthonormal and hence \\(\\symbfit Q^T \\symbfit Q= \\symbfit I_k\\), or if for \\(k &gt; d\\) the \\(d\\) row vectors are orthonormal with \\(\\symbfit Q\\symbfit Q^T = \\symbfit I_d\\).\nThe set of all (semi)-orthogonal matrices \\(\\symbfit Q\\) with \\(k \\leq d\\) column vectors is known as the Stiefel manifold \\(\\text{St}(d, k)\\).\n\n\n1.6.3 Generating orthogonal matrices\nIn two dimensions \\((d=2)\\) all orthogonal matrices \\(\\symbfit R\\) representing rotations with \\(\\det(\\symbfit R)=1\\) are given by \\[\n\\symbfit R(\\theta) =\n\\begin{pmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{pmatrix}\n\\] and those representing rotation-reflections \\(\\symbfit G\\) with \\(\\det(\\symbfit G)=-1\\) by \\[\n\\symbfit G(\\theta) =\n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix}\\,.\n\\] Every orthogonal matrix of dimension \\(d=2\\) can be represented as the product of at most two rotation-reflection matrices because \\[\n\\symbfit R(\\theta) = \\symbfit G(\\theta)\\, \\symbfit G(0) =  \n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}\\,.\n\\] Thus, the matrix \\(\\symbfit G\\) is a generator of two-dimensional orthogonal matrices. Note that \\(\\symbfit G(\\theta)\\) is symmetric, orthogonal and has determinant -1.\nMore generally, and applicable in arbitrary dimension, the role of generator is taken by the Householder reflection matrix \\[\n\\symbfit Q_{HH}(\\symbfit v) = \\symbfit I- 2 \\symbfit v\\symbfit v^T\n\\] where \\(\\symbfit v\\) is a vector of unit length (with \\(\\symbfit v^T \\symbfit v=1\\)) orthogonal to the reflection hyperplane. Note that \\(\\symbfit Q_{HH}(\\symbfit v) = \\symbfit Q_{HH}(-\\symbfit v)\\). By construction the matrix \\(\\symbfit Q_{HH}(\\symbfit v)\\) is symmetric, orthogonal and has determinant -1.\nIt can be shown that any \\(d\\)-dimensional orthogonal matrix \\(\\symbfit Q\\) can be represented as the product of at most \\(d\\) Householder reflection matrices. The two-dimensional generator \\(\\symbfit G(\\theta)\\) is recovered as the Householder matrix \\(\\symbfit Q_{HH}(\\symbfit v)\\) with \\(\\symbfit v= \\begin{pmatrix} -\\sin  \\frac{\\theta}{2} \\\\ \\cos \\frac{\\theta}{2} \\end{pmatrix}\\) or \\(\\symbfit v= \\begin{pmatrix} \\sin  \\frac{\\theta}{2} \\\\ -\\cos \\frac{\\theta}{2} \\end{pmatrix}\\).\n\n\n1.6.4 Permutation matrix\nA special type of an orthogonal matrix is a permutation matrix \\(\\symbfit P\\) created by permuting rows and/or columns of the identity matrix \\(\\symbfit I\\). Thus, each row and column of \\(\\symbfit P\\) contains exactly one entry of 1, but not necessarily on the diagonal.\nIf a permutation matrix \\(\\symbfit P\\) is multiplied with a matrix \\(\\symbfit A\\) it acts as an operator permuting the columns (\\(\\symbfit A\\symbfit P\\)) or the rows (\\(\\symbfit P\\symbfit A\\)). For a set of \\(d\\) elements there exist \\(d!\\) permutations. Thus, for dimension \\(d\\) there are \\(d!\\) possible permutation matrices (including the identity matrix).\nThe determinant of a permutation matrix is either +1 or -1. The product of two permutation matrices yields another permutation matrix.\nSymmetric permutation matrices correspond to self-inverse permutations (i.e. the permutation matrix is its own inverse), and are also called permutation involutions. They can have determinant +1 and -1.\nA transposition is a permutation where only two elements are exchanged. Thus, in a transposition matrix \\(\\symbfit T\\) exactly two rows and/or columns are exchanged compared to identity matrix \\(\\symbfit I\\). Transpositions are self-inverse, and transposition matrices are symmetric. There are \\(\\frac{d (d-1)}{2}\\) different transposition matrices. The determinant of a transposition matrix is \\(\\det(\\symbfit T)= -1\\).\nNote that the transposition matrix is an instance of a Householder matrix \\(\\symbfit Q_{HH}(\\symbfit v)\\) with vector \\(\\symbfit v\\) filled with zeros except for two elements that have value \\(\\frac{\\sqrt{2}}{2}\\) and \\(-\\frac{\\sqrt{2}}{2}\\).\nAny permutation of \\(d\\) elements can be generated by a series of at most \\(d-1\\) transpositions. Correspondingly, any permutation matrix \\(\\symbfit P\\) can be constructed by multiplication of the identity matrix with at most \\(d-1\\) transposition matrices. If the number of transpositions is even then \\(\\det(\\symbfit P) = 1\\) otherwise for an uneven number \\(\\det(\\symbfit P) = -1\\). This is called the sign or signature of the permutation.\nThe set of all permutations form the symmetric group \\(S_d\\), the subset of even permutations (with positive sign and \\(\\det(\\symbfit P)=1\\)) the alternating group \\(A_d\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#eigenvalues-and-eigenvectors",
    "href": "01-matrix-essentials.html#eigenvalues-and-eigenvectors",
    "title": "1  Matrix essentials",
    "section": "1.7 Eigenvalues and eigenvectors",
    "text": "1.7 Eigenvalues and eigenvectors\n\n1.7.1 Definition\nAssume a square matrix \\(\\symbfit A\\) of size \\(d \\times d\\). A vector \\(\\symbfit u\\neq 0\\) is called an eigenvector of the matrix \\(\\symbfit A\\) and \\(\\lambda\\) the corresponding eigenvalue if\n\\[\\symbfit A\\symbfit u= \\symbfit u\\lambda \\, .\\] This is called eigenvalue equation or eigenequation.\n\n\n1.7.2 Finding eigenvalues and vectors\nTo find the eigenvalues and eigenvectors the eigenequation is rewritten as \\[(\\symbfit A-\\symbfit I\\lambda ) \\; \\symbfit u= \\symbfup 0\\,.\\] For this equation to hold for an eigenvector \\(\\symbfit u\\neq 0\\) with eigenvalue \\(\\lambda\\) implies that the matrix \\(\\symbfit A-\\symbfit I\\lambda\\) is singular. Correspondingly, its determinant must vanish \\[\\det(\\symbfit A-\\symbfit I\\lambda ) =0 \\,.\\] This is called the characteristic equation of the matrix \\(\\symbfit A\\), and its solution yields the \\(d\\) eigenvalues \\(\\lambda_1, \\ldots, \\lambda_d\\). Note the eigenvalues need not be distinct and they may be complex even if the matrix \\(\\symbfit A\\) is real.\nIf there are complex eigenvalues, for a real matrix those eigenvalues come in conjugate pairs. Hence, for a complex \\(\\lambda_1 = r e^{i \\phi}\\) there will also be a corresponding complex eigenvalue \\(\\lambda_2 = r e^{-i \\phi}\\).\nGiven the eigenvalues we then solve the eigenequation for the corresponding non-zero eigenvectors \\(\\symbfit u_1, \\ldots, \\symbfit u_d\\). Note that eigenvectors of real matrices can have complex components. Also the eigenvector is only defined by the eigenequation up to a scalar. By convention eigenvectors are therefore typically standardised to unit length but this still leaves a sign ambiguity for real eigenvectors and implies that complex eigenvectors are defined only up to a factor with modulus 1.\n\n\n1.7.3 Eigenequation in matrix notation\nWith the matrix \\[\\symbfit U= (\\symbfit u_1, \\ldots, \\symbfit u_d)\\] containing the standardised eigenvectors in the columns and the diagonal matrix \\[\\symbfit \\Lambda= \\begin{pmatrix}\n    \\lambda_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}\n\\end{pmatrix}\\] containing the eigenvalues (typically sorted in order of magnitude) the eigenvalue equation can be written as \\[\\symbfit A\\symbfit U= \\symbfit U\\symbfit \\Lambda\\,.\\]\n\n\n1.7.4 Permutation of eigenvalues\nIf eigenvalues are not in order, we may apply a permutation matrix \\(\\symbfit P\\) to arrange them in order. With \\(\\symbfit \\Lambda^{\\text{sort}} = \\symbfit P^T \\symbfit \\Lambda\\symbfit P\\) as the sorted eigenvalues and \\(\\symbfit U^{\\text{sort}} = \\symbfit U\\symbfit P\\) as the corresponding eigenvectors the eigenequation becomes \\[\\symbfit A\\symbfit U^{\\text{sort}} =  \\symbfit A\\symbfit U\\symbfit P= \\symbfit U\\symbfit \\Lambda\\symbfit P=  \\symbfit U\\symbfit P\\symbfit P^T \\symbfit \\Lambda\\symbfit P=  \\symbfit U^{\\text{sort}} \\symbfit \\Lambda^{\\text{sort}} \\,.\\]\n\n\n1.7.5 Similar matrices\nTwo matrices \\(\\symbfit A\\) and \\(\\symbfit B\\) are called similar if they share the same eigenvalues.\nFrom \\(\\symbfit A\\) with eigenvalues \\(\\symbfit \\Lambda\\) and eigenvectors \\(\\symbfit U\\) we can construct a similar \\(\\symbfit B\\) via the similarity transformation \\(\\symbfit B= \\symbfit M\\symbfit A\\symbfit M^{-1}\\) where \\(\\symbfit M\\) is an invertible matrix.\nThen \\(\\symbfit \\Lambda\\) are the eigenvalues of \\(\\symbfit B\\) and \\(\\symbfit V= \\symbfit M\\symbfit U\\) its eigenvectors as \\[\n\\symbfit B\\symbfit V= \\symbfit M\\symbfit A\\symbfit M^{-1} \\symbfit M\\symbfit U= \\symbfit M\\symbfit A\\symbfit U= \\symbfit M\\symbfit U\\symbfit \\Lambda= \\symbfit V\\symbfit \\Lambda\\,.\n\\]\n\n\n1.7.6 Defective matrix\nIn most cases the eigenvectors \\(\\symbfit u_i\\) will be linearly independent so that they form a basis to span a \\(d\\) dimensional space.\nHowever, if this is not the case and the matrix \\(\\symbfit A\\) does not have a complete basis of eigenvectors, then the matrix is called defective. In this case the matrix \\(\\symbfit U\\) containing the eigenvectors is singular and \\(\\det(\\symbfit U)=0\\).\nAn example of a defective matrix is \\(\\begin{pmatrix}\n1 &1 \\\\\n0 & 1 \\\\\n\\end{pmatrix}\\) which has determinant 1 so that it can be inverted and its column vectors do form a complete basis but has only one distinct eigenvector \\((1,0)^T\\) so that the eigenvector basis is incomplete.\n\n\n1.7.7 Eigenvalues of a diagonal or triangular matrix\nIn the special case that \\(\\symbfit A\\) is diagonal or a triangular matrix the eigenvalues are easily determined. This follows from the simple form of their determinants as the product of the diagonal elements. Hence for these matrices the characteristic equation becomes \\(\\prod_{i}^d (a_{ii} -\\lambda) = 0\\) and has solution \\(\\lambda_i=a_{ii}\\), i.e. the eigenvalues are equal to the diagonal elements.\n\n\n1.7.8 Eigenvalues and vectors of a symmetric matrix\nIf \\(\\symbfit A\\) is symmetric, i.e. \\(\\symbfit A= \\symbfit A^T\\), then its eigenvalues and eigenvectors have special properties:\n\nall eigenvalues of \\(\\symbfit A\\) are real,\nthe eigenvectors are orthogonal, i.e \\(\\symbfit u_i^T \\symbfit u_j = 0\\) for \\(i \\neq j\\), and real. Thus, the matrix \\(\\symbfit U\\) containing the standardised orthonormal eigenvectors is orthogonal.\n\\(\\symbfit A\\) is never defective as \\(\\symbfit U\\) forms a complete basis.\n\nFurthermore, for a symmetric matrix \\(\\symbfit A\\) with diagonal elements \\(p_1 \\geq \\ldots \\geq p_d\\) and eigenvalues \\(\\lambda_1 \\geq \\ldots \\geq \\lambda_d\\) (note both written in decreasing order) the sum of the largest \\(k\\) eigenvalues forms an upper bound of the sum of the largest \\(k\\) diagonal elements: \\[\n\\sum_{i}^k \\lambda_i \\geq \\sum_{i}^k p_i\n\\] This theorem is due to Schur (1923) 2. The equality holds for \\(k=d\\) (as the trace of \\(\\symbfit A\\) equals the sum of its eigenvalues) and for any \\(k\\) if \\(\\symbfit A\\) is diagonal (as in this case of the diagonal elements equal the eigenvalues).\n\n\n1.7.9 Eigenvalues of orthogonal matrices\nThe eigenvalues of an orthogonal matrix \\(\\symbfit Q\\) are not necessarily real but they all have modulus 1 and lie on the unit circle . Thus, the eigenvalues of \\(\\symbfit Q\\) all have the form \\(\\lambda = e^{i \\phi} = \\cos \\phi + i \\sin \\phi\\).\nIn any real matrix complex eigenvalues come in conjugate pairs. Hence if an orthogonal matrix \\(\\symbfit Q\\) has the complex eigenvalue \\(e^{i \\phi}\\) it also has an complex eigenvalue \\(e^{-i \\phi} =\\cos \\phi - i \\sin \\phi\\). The product of these two conjugate eigenvalues is 1. Thus, an orthogonal matrix of uneven dimension has at least one real eigenvalue (+1 or -1).\nThe eigenvalues of a Hausholder matrix \\(\\symbfit Q_{HH}(\\symbfit v)\\) are all real (recall that it is symmetric!). In fact, in dimension \\(d\\) its eigenvalues are -1 (one time) and 1 ( \\(d-1\\) times). Since a transposition matrix \\(\\symbfit T\\) is a special Householder matrix they have the same eigenvalues.\n\n\n1.7.10 Positive definite matrices\nIf all eigenvalues of a square matrix \\(\\symbfit A\\) are real and \\(\\lambda_i \\geq 0\\) then \\(\\symbfit A\\) is called positive semi-definite. If all eigenvalues are strictly positive \\(\\lambda_i &gt; 0\\) then \\(\\symbfit A\\) is called positive definite.\nNote that a matrix does not need to be symmetric to be positive definite, e.g. \\(\\begin{pmatrix}\n2 & 3 \\\\\n1 & 4 \\\\\n\\end{pmatrix}\\) has positive eigenvalues 5 and 1. It also has a complete set of eigenvectors and is diagonisable.\nA symmetric matrix \\(\\symbfit A\\) is positive definite if the quadratic form \\(\\symbfit x^T \\symbfit A\\symbfit x&gt; 0\\) for any non-zero \\(\\symbfit x\\), and it is positive semi-definite if \\(\\symbfit x^T \\symbfit A\\symbfit x\\geq 0\\). This holds also the other way around: a symmetric positive definite matrix (with positive eigenvalues) has a positive quadratic form, and a symmetric positive semi-definite matrix (with non-negative eigenvalues) a non-negative quadratic form.\nA symmetric positive definite matrix always has a positive diagonal (this can be seen by setting \\(\\symbfit x\\) above to a unit vector with 1 at a single position, and 0 at all other elements). However, just requiring a positive diagonal is too weak to ensure positive definiteness of a symmetric matrix, for example \\(\\begin{pmatrix}\n1 &10 \\\\\n10 & 1 \\\\\n\\end{pmatrix}\\) has a negative eigenvalue of -9. On the other hand, a symmetric matrix is indeed positive definite if it is strictly diagonally dominant, i.e. if all its diagonal elements are positive and are larger than the absolute value of any of the corresponding row or column elements. However, diagonal dominance is too restrictive as criterion to characterise all symmetric positive definite matrices, since there are many symmetric matrices that are positive definite but not diagonally dominant, such as \\(\\begin{pmatrix}\n1 & 2 \\\\\n2 & 5 \\\\\n\\end{pmatrix}\\).\nFinally, the sum of a symmetric positive semi-definite matrix \\(\\symbfit A\\) and a symmetric positive definite matrix \\(\\symbfit B\\) is itself symmetric positive definite because the corresponding quadratic form \\(\\symbfit x^T ( \\symbfit A+\\symbfit B) \\symbfit x=\n\\symbfit x^T \\symbfit A\\symbfit x+ \\symbfit x^T \\symbfit B\\symbfit x&gt; 0\\) is positive. Similarly, the sum of two symmetric positive (semi)-definite matrices is itself symmetric positive (semi)-definite.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#matrix-decompositions",
    "href": "01-matrix-essentials.html#matrix-decompositions",
    "title": "1  Matrix essentials",
    "section": "1.8 Matrix decompositions",
    "text": "1.8 Matrix decompositions\n\n1.8.1 Diagonalisation and eigenvalue decomposition\nIf \\(\\symbfit A\\) is a square non-defective matrix then the eigensystem \\(\\symbfit U\\) is invertible and we can rewrite the eigenvalue equation to \\[\\symbfit A= \\symbfit U\\symbfit \\Lambda\\symbfit U^{-1} \\,.\\] This is called the eigendecomposition, or spectral decomposition, of \\(\\symbfit A\\) and equivalently \\[\\symbfit \\Lambda= \\symbfit U^{-1} \\symbfit A\\symbfit U\\] is the diagonalisation of \\(\\symbfit A\\).\nIf \\(\\symbfit A\\) is defective (i.e. \\(\\symbfit U\\) is singular) one can still approximately diagonalise \\(\\symbfit A\\) as there always exists a similarity transformation to \\(\\symbfit J= \\symbfit M\\symbfit A\\symbfit M^{-1}\\) where \\(\\symbfit M\\) is a invertible matrix and \\(\\symbfit J\\) has Jordan canonical form, i.e. \\(\\symbfit J\\) is upper triangular with the (potentially complex) eigenvalues on the diagonal and some non-zero entries equal to 1 immediately above the main diagonal.\n\n\n1.8.2 Orthogonal eigenvalue decomposition\nFor symmetric \\(\\symbfit A\\) with real eigenvalues and orthogonal matrix \\(\\symbfit U\\) the spectral decomposition becomes \\[\\symbfit A= \\symbfit U\\symbfit \\Lambda\\symbfit U^T\\] and \\[\\symbfit \\Lambda= \\symbfit U^T \\symbfit A\\symbfit U\\,.\\] This special case is known as the orthogonal diagonalisation of \\(\\symbfit A\\).\nThe orthogonal decomposition for symmetric \\(\\symbfit A\\) is unique apart from the signs of the eigenvectors (columns of \\(\\symbfit U\\)). Thus, in a computer application depending on the specific implementation of a numerical algorithm for eigenvalue decomposition the signs may vary.\n\n\n1.8.3 Singular value decomposition\nThe singular value decomposition (SVD) is a generalisation of the orthogonal eigenvalue decomposition for symmetric matrices.\nAny (!) rectangular matrix \\(\\symbfit A\\) of size \\(n\\times d\\) can be factored into the product \\[\\symbfit A= \\symbfit U\\symbfit D\\symbfit V^T\\] where \\(\\symbfit U\\) is a \\(n \\times n\\) orthogonal matrix, \\(\\symbfit V\\) is a second \\(d \\times d\\) orthogonal matrix and \\(\\symbfit D\\) is a diagonal but rectangular matrix of size \\(n\\times d\\) with \\(m=min(n,d)\\) real diagonal elements \\(d_1, \\ldots\nd_m\\). The \\(d_i\\) are called singular values, and appear along the diagonal in \\(\\symbfit D\\) by order of magnitude.\nThe SVD is unique apart from the signs of the columns vectors in \\(\\symbfit U\\), \\(\\symbfit V\\) and \\(\\symbfit D\\) (you can freely specify the column signs of any two of the three matrices). By convention the signs are chosen such that the singular values in \\(\\symbfit D\\) are all non-negative, which leaves ambiguity in columns signs of \\(\\symbfit U\\) and \\(\\symbfit V\\). Alternatively, one may fix the columns signs of \\(\\symbfit U\\) and \\(\\symbfit V\\), e.g. by requiring a positive diagonal, which then determines the sign of the singular values (thus allowing for negative singular values as well).\nIf \\(\\symbfit A\\) is symmetric then the SVD and the orthogonal eigenvalue decomposition coincide (apart from different sign conventions for singular values, eigenvalues and eigenvectors).\nSince \\(\\symbfit A^T \\symbfit A= \\symbfit V\\symbfit D^T \\symbfit D\\symbfit V^T\\) and \\(\\symbfit A\\symbfit A^T = \\symbfit U\\symbfit D\\symbfit D^T \\symbfit U^T\\) the squared singular values correspond to the eigenvalues of \\(\\symbfit A^T \\symbfit A\\) and \\(\\symbfit A\\symbfit A^T\\). It also follows that \\(\\symbfit A^T \\symbfit A\\) and \\(\\symbfit A\\symbfit A^T\\) are both positive semi-definite symmetric matrices, and that \\(\\symbfit V\\) and \\(\\symbfit U\\) contain the respective sets of eigenvectors.\n\n\n1.8.4 Polar decomposition\nAny square matrix \\(\\symbfit A\\) can be factored into the product \\[\n\\symbfit A= \\symbfit Q\\symbfit B\n\\] of an orthogonal matrix \\(\\symbfit Q\\) and a symmetric positive semi-definite matrix \\(\\symbfit B\\).\nThis follows from the SVD of \\(\\symbfit A\\) given as \\[\n\\begin{split}\n\\symbfit A&= \\symbfit U\\symbfit D\\symbfit V^T \\\\\n    &= ( \\symbfit U\\symbfit V^T ) ( \\symbfit V\\symbfit D\\symbfit V^T ) \\\\\n    &= \\symbfit Q\\symbfit B\\\\\n\\end{split}\n\\] with non-negative \\(\\symbfit D\\). Note that this decomposition is unique as the sign ambiguities in the columns of \\(\\symbfit U\\) and \\(\\symbfit V\\) cancel out in \\(\\symbfit Q\\) and \\(\\symbfit B\\).\n\n\n1.8.5 Cholesky decomposition\nA symmetric positive definite matrix \\(\\symbfit A\\) can be decomposed into a product of a triangular matrix \\(\\symbfit L\\) with its transpose \\[\n\\symbfit A= \\symbfit L\\symbfit L^T \\,.\n\\] Here, \\(\\symbfit L\\) is a lower triangular matrix with positive diagonal elements.\nThis decomposition is unique and is called Cholesky factorisation. It is often used to check whether a symmetric matrix is positive definite as it is algorithmically less demanding than eigenvalue decomposition.\nNote that some implementations of the Cholesky decomposition (e.g. in R) use upper triangular matrices \\(\\symbfit K\\) with positive diagonal so that \\(\\symbfit A= \\symbfit K^T \\symbfit K\\) and \\(\\symbfit L= \\symbfit K^T\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#matrix-summaries-based-on-eigenvalues-and-singular-values",
    "href": "01-matrix-essentials.html#matrix-summaries-based-on-eigenvalues-and-singular-values",
    "title": "1  Matrix essentials",
    "section": "1.9 Matrix summaries based on eigenvalues and singular values",
    "text": "1.9 Matrix summaries based on eigenvalues and singular values\n\n1.9.1 Trace and determinant computed from eigenvalues\nThe eigendecomposition \\(\\symbfit A=\\symbfit U\\symbfit \\Lambda\\symbfit U^{-1}\\) allows to establish a link between the trace and the determinant and the eigenvalues of a matrix.\nSpecifically, \\[\n\\begin{split}\n\\text{Tr}(\\symbfit A) & = \\text{Tr}(\\symbfit U\\symbfit \\Lambda\\symbfit U^{-1}  ) =\n\\text{Tr}( \\symbfit \\Lambda\\symbfit U^{-1} \\symbfit U) \\\\\n&= \\text{Tr}( \\symbfit \\Lambda) = \\sum_{i=1}^d \\lambda_i \\\\\n\\end{split}\n\\] thus the trace of a square matrix \\(\\symbfit A\\) is equal to the sum of its eigenvalues. Likewise, \\[\n\\begin{split}\n\\det(\\symbfit A) & = \\det(\\symbfit U) \\det(\\symbfit \\Lambda) \\det(\\symbfit U^{-1}  ) \\\\\n&=\\det( \\symbfit \\Lambda) = \\prod_{i=1}^d \\lambda_i \\\\\n\\end{split}\n\\] therefore the determinant of \\(\\symbfit A\\) is the product of the eigenvalues.\nThe relationship between the eigenvalues of a square matrix and the trace and the determinant of that matrix is shown above for diagonisable matrices. However, it holds more generally for any square matrix, i.e. also for defective matrices. For the latter the Jordan canonical form \\(\\symbfit J\\) replaces \\(\\symbfit \\Lambda\\) (in both cases the eigenvalues are simply the entries on the diagonal).\nIf any of the eigenvalues are equal to zero then \\(\\det(\\symbfit A) = 0\\) and as hence \\(\\symbfit A\\) is singular and not invertible.\nThe trace and determinant of a real matrix are always real even though the individual eigenvalues may be complex.\n\n\n1.9.2 Eigenvalues of a squared matrix\nFrom the eigendecomposition \\(\\symbfit A=\\symbfit U\\symbfit \\Lambda\\symbfit U^{-1}\\) it is easy to see that the eigenvalues of \\(\\symbfit A^2\\) are simply the squared eigenvalues of \\(\\symbfit A\\) as \\[\n\\symbfit A^2 = \\symbfit U\\symbfit \\Lambda\\symbfit U^{-1} \\symbfit U\\symbfit \\Lambda\\symbfit U^{-1} = \\symbfit U\\symbfit \\Lambda^2 \\symbfit U^{-1}\n\\] As a result we can compute the trace of \\(\\symbfit A^2\\) as the sum of the squared eigenvalues of \\(\\symbfit A\\), i.e. \\(\\text{Tr}(\\symbfit A^2) =  \\sum_{i=1}^d \\lambda_i^2\\), and the determinant as the product of squared eigenvalues, i.e \\(\\det(\\symbfit A^2) = \\prod_{i=1}^d \\lambda_i^2\\).\nIf \\(\\symbfit A\\) is symmetric then \\(\\text{Tr}(\\symbfit A^2) = \\text{Tr}(\\symbfit A\\symbfit A^T) = || A ||^2_F = \\sum_{i=1}^d \\sum_{j=1}^d a_{ij}^2\\). This leads to the identity \\[\n\\sum_{i=1}^d \\lambda_i^2 =  \\sum_{i=1}^d \\sum_{j=1}^d a_{ij}^2\n\\] between the sum of the squared eigenvalues and the sum of all squared entries of a symmetric matrix \\(\\symbfit A\\).\n\n\n1.9.3 Rank and condition number\nThe rank is the dimension of the space spanned by both the column and row vectors. A rectangular matrix of dimension \\(n \\times d\\) will have rank of at most \\(m = \\min(n, d)\\), and if the maximum is indeed achieved then it has full rank.\nThe condition number describes how well- or ill-conditioned a full rank matrix is. For example, for a square matrix a large condition number implies that the matrix is close to being singular and thus ill-conditioned. If the condition number is infinite then the matrix is not full rank.\nThe rank and condition of a matrix can both be determined from the \\(m\\) singular values \\(d_1, \\ldots, d_m\\) of a matrix obtained by SVD:\n\nThe rank is number of non-zero singular values.\nThe condition number is the ratio of the largest singular value divided by the smallest singular value (absolute values if signs are allowed).\n\nIf a square matrix \\(\\symbfit A\\) is singular then the condition number is infinite, and it will not have full rank. On the other hand, a non-singular square matrix, such as a positive definite matrix, has full rank.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#functions-of-symmetric-matrices",
    "href": "01-matrix-essentials.html#functions-of-symmetric-matrices",
    "title": "1  Matrix essentials",
    "section": "1.10 Functions of symmetric matrices",
    "text": "1.10 Functions of symmetric matrices\nWe focus on symmetric square matrices \\(\\symbfit A=\\symbfit U\\symbfit \\Lambda\\symbfit U^T\\) which are always diagonisable with real eigenvalues \\(\\symbfit \\Lambda\\) and orthogonal eigenvectors \\(\\symbfit U\\).\n\n1.10.1 Definition of a matrix function\nAssume a real-valued function \\(f(a)\\) of a real number \\(a\\). Then the corresponding matrix function \\(f(\\symbfit A)\\) is defined as \\[\nf(\\symbfit A) =  \\symbfit Uf(\\symbfit \\Lambda) \\symbfit U^T =  \\symbfit U\\begin{pmatrix}\n    f(\\lambda_{1}) & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & f(\\lambda_{d})\n\\end{pmatrix} \\symbfit U^T\n\\] where the function \\(f(a)\\) is applied to the eigenvalues of \\(\\symbfit A\\). By construction \\(f(\\symbfit A)\\) is real, symmetric and has real eigenvalues \\(f(\\lambda_i)\\).\nExamples:\n\nMatrix power: \\(f(a) = a^p\\) (with \\(p\\) a real number)\n\nSpecial cases of matrix power include :\n\nMatrix inversion: \\(f(a) = a^{-1}\\)\nNote that if the matrix \\(\\symbfit A\\) is singular, i.e. contains one or more eigenvalues \\(\\lambda_i=0\\), then \\(\\symbfit A^{-1}\\) is not defined and therefore \\(\\symbfit A\\) is not invertible.\n\nHowever, a so-called pseudoinverse can still be computed, by inverting the non-zero eigenvalues, and keeping the zero eigenvalues as zero.\n\nMatrix square root: \\(f(a) = a^{1/2}\\)\nSince there are multiple solutions to the square root there are also multiple matrix square roots. The principal matrix square root is obtained by using the positive square roots of all the eigenvalues. Thus the principal matrix square root of a positive semi-definite matrix is also positive semi-definite and it is unique.\n\n\nMatrix exponential: \\(f(a) = \\exp(a)\\)\nNote that because \\(\\exp(a) \\geq 0\\) for all real \\(a\\) the matrix \\(\\exp(\\symbfit A)\\) is positive semi-definite. Thus, the matrix exponential can be used to generate positive semi-definite matrices.\nIf \\(\\symbfit A\\) and \\(\\symbfit B\\) commute, i.e. if \\(\\symbfit A\\symbfit B= \\symbfit B\\symbfit A\\), then \\(\\exp(\\symbfit A+\\symbfit B) = \\exp(\\symbfit A) \\exp(\\symbfit B)\\). However, this is not the case otherwise!\n\n\nMatrix logarithm: \\(f(a) = \\log(a)\\)\nAs the logarithm requires \\(a &gt;0\\) the matrix \\(\\symbfit A\\) needs to be positive definite for \\(\\log(\\symbfit A)\\) to be defined.\n\n\n\n1.10.2 Identities for the matrix exponential and logarithm\nThe above give rise to useful identities:\n\nFor any symmetric matrix \\(\\symbfit A\\) we have \\[\n\\det(\\exp(\\symbfit A)) = \\exp(\\text{Tr}(\\symbfit A))\n\\] because \\(\\prod_i \\exp(\\lambda_i) = \\exp( \\sum_i \\lambda_i)\\) where \\(\\lambda_i\\) are the eigenvalues of \\(\\symbfit A\\).\nIf we take the logarithm on both sides and replace \\(\\exp(\\symbfit A)=\\symbfit B\\) we get another identity for a symmetric positive definite matrix \\(\\symbfit B\\): \\[\n\\log \\det(\\symbfit B) = \\text{Tr}(\\log(\\symbfit B))\n\\] because \\(\\log( \\prod_i \\lambda_i)  = \\sum_i \\log(\\lambda_i)\\) where \\(\\lambda_i\\) are the eigenvalues of \\(\\symbfit B\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "01-matrix-essentials.html#footnotes",
    "href": "01-matrix-essentials.html#footnotes",
    "title": "1  Matrix essentials",
    "section": "",
    "text": "Mardia, K. V., J. T. Kent and J. M. Bibby. 1979. Multivariate Analysis. Academic Press.↩︎\nSchur, I. 1923. Über eine Klasse von Mittelbildungen mit Anwendungen auf die Determinantentheorie. Sitzungsber. Berl. Math. Ges. 22:9–29.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Matrix essentials</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html",
    "href": "02-vector-calculus.html",
    "title": "2  Vector and matrix calculus",
    "section": "",
    "text": "2.1 First order vector derivatives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#first-order-vector-derivatives",
    "href": "02-vector-calculus.html#first-order-vector-derivatives",
    "title": "2  Vector and matrix calculus",
    "section": "",
    "text": "2.1.1 Derivative and gradient\nThe derivative of a scalar-valued function \\(h(\\symbfit x)\\) with regard to its vector argument \\(\\symbfit x= (x_1, \\ldots, x_d)^T\\) is the row vector \\[\nD h(\\symbfit x) =  \\frac{\\partial h(\\symbfit x)}{\\partial \\symbfit x}\n= \\begin{pmatrix}\n\\frac{\\partial h(\\symbfit x)}{\\partial x_1} &\n\\cdots &\n\\frac{\\partial h(\\symbfit x)}{\\partial x_d} \\\\\n\\end{pmatrix}\n\\]\nThe above notation follows the numerator layout convention, where the dimension of the numerator (here 1) determines the number of rows and the dimension of the denominator (here \\(d\\)) the number of columns of the resulting matrix (see https://en.wikipedia.org/wiki/Matrix_calculus for details). For a scalar function this results in a vector of the same dimension as \\(\\symbfit x^T\\).\nThe gradient of \\(h(\\symbfit x)\\) is a column vector and the transpose of the derivative \\[\n\\text{grad } h(\\symbfit x) = \\left( \\frac{\\partial h(\\symbfit x)}{\\partial \\symbfit x} \\right)^T\n\\] It is often written using the nabla operator \\(\\nabla\\) as \\[\n\\nabla h(\\symbfit x) = \\begin{pmatrix}\n\\frac{\\partial h(\\symbfit x)}{\\partial x_1} \\\\\n\\vdots\\\\\n\\frac{\\partial h(\\symbfit x)}{\\partial x_d}\n\\end{pmatrix}\n\\] with \\[\n\\nabla = \\begin{pmatrix}\n\\frac{\\partial }{\\partial x_1} \\\\\n\\vdots\\\\\n\\frac{\\partial }{\\partial x_d}\n\\end{pmatrix}\n\\]\nNote that \\[\n(\\nabla h(\\symbfit x))^T = \\nabla^T h(\\symbfit x) = D h(\\symbfit x) = \\frac{\\partial h(\\symbfit x)}{\\partial \\symbfit x}\n\\]\n\nExamples for the gradient and derivative:\n\n\\(h(\\symbfit x)=\\symbfit a^T \\symbfit x+ b\\).\nThen \\(\\nabla h(\\symbfit x) = \\symbfit a\\) and \\(D h(\\symbfit x) = \\frac{\\partial h(\\symbfit x)}{\\partial \\symbfit x} = \\symbfit a^T\\).\n\\(h(\\symbfit x)=\\symbfit x^T \\symbfit x\\).\nThen \\(\\nabla h(\\symbfit x) = 2 \\symbfit x\\) and \\(D h(\\symbfit x) = \\frac{\\partial h(\\symbfit x)}{\\partial \\symbfit x} = 2 \\symbfit x^T\\).\n\\(h(\\symbfit x)=\\symbfit x^T \\symbfit A\\symbfit x\\).\nThen \\(\\nabla h(\\symbfit x) = (\\symbfit A+ \\symbfit A^T) \\symbfit x\\) and \\(D h(\\symbfit x) =  \\frac{\\partial h(\\symbfit x)}{\\partial \\symbfit x} = \\symbfit x^T (\\symbfit A+ \\symbfit A^T)\\).\n\n\n\n\n2.1.2 Jacobian matrix\nSimilarly, we can also compute the derivative of a vector-valued function \\[\n\\symbfit h(\\symbfit x) = ( h_1(\\symbfit x), \\ldots, h_m(\\symbfit x) )^T\n\\] with regard to \\(\\symbfit x\\). This yields (again in numerator layout convention) a matrix of size \\(m\\) rows and \\(d\\) columns whose rows contain the derivatives of the components of \\(\\symbfit h(\\symbfit x)\\). \\[\n\\begin{split}\nD\\symbfit h(\\symbfit x) &= \\frac{\\partial \\symbfit h(\\symbfit x)}{\\partial \\symbfit x}  = \\left(\\frac{\\partial h_i(\\symbfit x)}{\\partial x_j}\\right) \\\\\n&=\\begin{pmatrix}\n\\frac{\\partial h_1(\\symbfit x)}{\\partial x_1} & \\cdots  & \\frac{\\partial h_1(\\symbfit x)}{\\partial x_d} \\\\\n\\vdots &\\ddots & \\vdots \\\\\n\\frac{\\partial h_m(\\symbfit x)}{\\partial x_1} & \\cdots & \\frac{\\partial h_m(\\symbfit x)}{\\partial x_d} \\\\\n\\end{pmatrix} \\\\\n&=\n\\left( {\\begin{array}{c}\n\\nabla^T h_1(\\symbfit x)   \\\\\n\\vdots   \\\\\n\\nabla^T h_m(\\symbfit x)  \\\\\n\\end{array} } \\right) \\\\\n&= \\symbfit J_{\\symbfit h}(\\symbfit x)\n\\end{split}\n\\]\nThis matrix is also called the Jacobian matrix\n\n\\(\\symbfit h(\\symbfit x)=\\symbfit A^T \\symbfit x+ \\symbfit b\\). Then \\(D \\symbfit h(\\symbfit x) = \\frac{\\partial \\symbfit h(\\symbfit x)}{\\partial \\symbfit x} = \\symbfit J_{\\symbfit h}(\\symbfit x) =\\symbfit A^T\\).\n\nIf \\(m=d\\) then the Jacobian matrix is a square and this allows to compute the determinant of the Jacobian matrix. Both the Jacobian matrix and the Jacobian determinant are often called “the Jacobian” so one needs to determine from the context whether this refers to the matrix or the determinant.\nIf \\(\\symbfit y= \\symbfit h(\\symbfit x)\\) is an invertible function with \\(\\symbfit x= \\symbfit h^{-1}(\\symbfit y)\\) then the Jacobian matrix is invertible and the inverted matrix is the Jacobian matrix of the inverse function: \\[\nD\\symbfit x(\\symbfit y) = \\left( D\\symbfit y(\\symbfit x)\\right)^{-1} \\rvert_{\\symbfit x= \\symbfit x(\\symbfit y)}\n\\] or in alternative notation \\[\n\\frac{\\partial \\symbfit x(\\symbfit y)}{\\partial \\symbfit y} = \\left. \\left( \\frac{\\partial \\symbfit y(\\symbfit x)}{\\partial \\symbfit x}  \\right)^{-1} \\right\\rvert_{\\symbfit x= \\symbfit x(\\symbfit y)}\n\\]\nIn this case the Jacobian determinant of the backtransformation can be computed as the inverse of the Jacobian determinant of the original function: \\[\n\\det D\\symbfit x(\\symbfit y) = \\det \\left( D\\symbfit y(\\symbfit x)\\right)^{-1} \\rvert_{\\symbfit x= \\symbfit x(\\symbfit y)}\n\\] and \\[\n\\det \\left(\\frac{\\partial \\symbfit x(\\symbfit y)}{\\partial \\symbfit y}\\right) = \\left. \\det \\left( \\frac{\\partial \\symbfit y(\\symbfit x)}{\\partial \\symbfit x}  \\right)^{-1} \\right\\rvert_{\\symbfit x= \\symbfit x(\\symbfit y)}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#second-order-vector-derivatives",
    "href": "02-vector-calculus.html#second-order-vector-derivatives",
    "title": "2  Vector and matrix calculus",
    "section": "2.2 Second order vector derivatives",
    "text": "2.2 Second order vector derivatives\nThe matrix of all second order partial derivates of scalar-valued function with vector-valued argument is called the Hessian matrix: \\[\n\\begin{split}\n\\nabla \\nabla^T h(\\symbfit x) &=\n\\begin{pmatrix}\n  \\frac{\\partial^2 h(\\symbfit x)}{\\partial x_1^2}\n     & \\frac{\\partial^2 h(\\symbfit x)}{\\partial x_1 \\partial x_2}\n     & \\cdots\n     & \\frac{\\partial^2 h(\\symbfit x)}{\\partial x_1 \\partial x_d} \\\\\n  \\frac{\\partial^2 h(\\symbfit x)}{\\partial x_2 \\partial x_1}\n     & \\frac{\\partial^2 h(\\symbfit x)}{\\partial x_2^2}\n     & \\cdots\n     & \\frac{\\partial^2 h(\\symbfit x)}{\\partial x_2 \\partial x_d} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\frac{\\partial^2 h(\\symbfit x)}{\\partial x_d \\partial x_1}\n     & \\frac{\\partial^2 h(\\symbfit x)}{\\partial x_d \\partial x_2}  \n     & \\cdots\n     & \\frac{\\partial^2 h(\\symbfit x)}{\\partial x_d^2}\n\\end{pmatrix} \\\\\n&= \\left(\\frac{\\partial h(\\symbfit x)}{\\partial x_i \\partial x_j}\\right) \\\\\n& =   \\frac{\\partial }{\\partial \\symbfit x} \\left( \\frac{\\partial h(\\symbfit x)}{\\partial \\symbfit x}\\right)^T \\\\\n& = D (Dh(\\symbfit x))^T \\\\\n\\end{split}\n\\] By construction the Hessian matrix is square and symmetric.\n\n\\(h(\\symbfit x)=\\symbfit x^T \\symbfit A\\symbfit x\\). Then \\(\\nabla \\nabla^T h(\\symbfit x)  =  (\\symbfit A+ \\symbfit A^T)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#chain-rules-for-gradient-vector-and-hessian-matrix",
    "href": "02-vector-calculus.html#chain-rules-for-gradient-vector-and-hessian-matrix",
    "title": "2  Vector and matrix calculus",
    "section": "2.3 Chain rules for gradient vector and Hessian matrix",
    "text": "2.3 Chain rules for gradient vector and Hessian matrix\nSuppose \\(h(\\symbfit x)\\) is a scalar-valued function and \\(g(\\symbfit y) = h(\\symbfit x(\\symbfit y))\\) is a composite scalar-valued function where \\(\\symbfit x(\\symbfit y)\\) a map from \\(\\symbfit y\\) to \\(\\symbfit x\\).\nThe gradient of the composite function \\(g(\\symbfit y) = h(\\symbfit x(\\symbfit y))\\) can be computed from the gradient of \\(h(\\symbfit x)\\) and the Jacobian matrix for \\(\\symbfit x(\\symbfit y)\\) as follows: \\[\n\\nabla g(\\symbfit y)  =   (D\\symbfit x(\\symbfit y))^T\\,  \\nabla h(\\symbfit x)   \\rvert_{\\symbfit x= \\symbfit x(\\symbfit y)}\n\\] and \\[\n\\nabla g(\\symbfit y)  =   \\left(\\frac{\\partial \\symbfit x(\\symbfit y)}{\\partial \\symbfit y}\\right)^T\\,  \\nabla h(\\symbfit x)   \\rvert_{\\symbfit x= \\symbfit x(\\symbfit y)}\n\\]\nSimilarly, the Hessian matrix of \\(g(\\symbfit y)\\) can be computed from the Hessian of \\(h(\\symbfit x)\\) and the Jacobian matrix for \\(\\symbfit x(\\symbfit y)\\): \\[\n\\nabla \\nabla^T g(\\symbfit y) = (D \\symbfit x(\\symbfit y))^T\\, \\nabla \\nabla^T h(\\symbfit x)\\rvert_{\\symbfit x= \\symbfit x(\\symbfit y)}  \\,   D\\symbfit x(\\symbfit y)\n\\] and \\[\n\\nabla \\nabla^T g(\\symbfit y) = \\left(\\frac{\\partial \\symbfit x(\\symbfit y)}{\\partial \\symbfit y}\\right)^T\\,\n\\nabla \\nabla^T h(\\symbfit x)\\rvert_{\\symbfit x= \\symbfit x(\\symbfit y)}  \\,  \\frac{\\partial \\symbfit x(\\symbfit y)}{\\partial \\symbfit y}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#first-order-matrix-derivatives",
    "href": "02-vector-calculus.html#first-order-matrix-derivatives",
    "title": "2  Vector and matrix calculus",
    "section": "2.4 First order matrix derivatives",
    "text": "2.4 First order matrix derivatives\nThe derivative of a scalar-valued function \\(h(\\symbfit X)\\) with regard to a matrix argument \\(\\symbfit X= (x_{ij})\\) is defined as below and and results (in numerator layout convention) in a matrix of the same dimension as \\(\\symbfit X^T\\): \\[\nD h(\\symbfit X) = \\frac{\\partial h(\\symbfit X)}{\\partial \\symbfit X}  = \\left(\\frac{\\partial h(\\symbfit X)}{\\partial x_{ji}}\\right)\n\\]\n\n\\(\\frac{\\partial \\text{Tr}(\\symbfit A^T \\symbfit X)}{\\partial \\symbfit X} = \\symbfit A^T\\)\n\n\n\\(\\frac{\\partial \\text{Tr}(\\symbfit A^T \\symbfit X\\symbfit B)}{\\partial \\symbfit X} = \\symbfit B\\symbfit A^T\\)\n\n\n\\(\\frac{\\partial \\text{Tr}(\\symbfit X^T \\symbfit A\\symbfit X)}{\\partial \\symbfit X} = \\symbfit X^T (\\symbfit A+ \\symbfit A^T)\\)\n\n\n\\(\\frac{\\partial \\log \\det(\\symbfit X)}{\\partial \\symbfit X} = \\frac{\\partial \\text{Tr}(\\log \\symbfit X)}{\\partial \\symbfit X} = \\symbfit X^{-1}\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#linear-and-quadratic-approximation",
    "href": "02-vector-calculus.html#linear-and-quadratic-approximation",
    "title": "2  Vector and matrix calculus",
    "section": "2.5 Linear and quadratic approximation",
    "text": "2.5 Linear and quadratic approximation\nA linear and quadratic approximation of a differentiable function is given by a Taylor series of first and second order, respectively.\n\nLinear and quadratic approximation of a scalar-valued function of a scalar: \\[\nh(x) \\approx h(x_0) + h'(x_0) \\, (x-x_0) + \\frac{1}{2} h''(x_0) \\, (x-x_0)^2\n\\] Note that \\(h'(x_0) = h'(x) \\,|\\, x_0\\) is first derivative of \\(h(x)\\) evaluated at \\(x_0\\) and \\(h''(x_0) = h''(x) \\,|\\, x_0\\) is the second derivative of \\(h(x)\\) evaluated \\(x_0\\).\nWith \\(x = x_0+ \\varepsilon\\) the approximation can also be written as \\[\nh(x_0+ \\varepsilon) \\approx h(x_0) + h'(x_0) \\, \\varepsilon + \\frac{1}{2} h''(x_0)\\, \\varepsilon^2\n\\] The first two terms on the right comprise the linear approximation, all three terms the quadratic approximation.\nLinear and quadratic approximation of a scalar-valued function of a vector: \\[\nh(\\symbfit x) \\approx h(\\symbfit x_0) + \\nabla^T h(\\symbfit x_0)\\, (\\symbfit x-\\symbfit x_0) + \\frac{1}{2}\n(\\symbfit x-\\symbfit x_0)^T \\, \\nabla \\nabla^T h(\\symbfit x_0) \\, (\\symbfit x-\\symbfit x_0)\n\\] Note that \\(\\nabla^T h(\\symbfit x_0)\\) is the transposed gradient (i.e the vector derivative) of \\(h(\\symbfit x)\\) evaluated at \\(\\symbfit x_0\\) and \\(\\nabla \\nabla^T h(\\symbfit x_0)\\) the Hessian matrix of \\(h(\\symbfit x)\\) evaluated at \\(\\symbfit x_0\\). With \\(\\symbfit x= \\symbfit x_0+ \\symbfit \\varepsilon\\) this approximation can also be written as \\[\nh(\\symbfit x_0+ \\symbfit \\varepsilon) \\approx h(\\symbfit x_0) + \\nabla^T h(\\symbfit x_0)\\, \\symbfit \\varepsilon+ \\frac{1}{2} \\symbfit \\varepsilon^T \\, \\nabla \\nabla^T h(\\symbfit x_0) \\,\\symbfit \\varepsilon\n\\] The first two terms on the right comprise the linear approximation, all three terms the quadratic approximation.\nLinear approximation of a vector-valued function of a vector: \\[\n\\symbfit h(\\symbfit x) \\approx \\symbfit h(\\symbfit x_0) + D \\symbfit h(\\symbfit x_0) \\, (\\symbfit x-\\symbfit x_0)\n\\] Note that \\(D \\symbfit h(\\symbfit x_0)\\) is Jacobian matrix (i.e the vector derivative) of \\(\\symbfit h(\\symbfit x)\\) evaluated at \\(\\symbfit x_0\\). With \\(\\symbfit x= \\symbfit x_0+ \\symbfit \\varepsilon\\) this approximation can also be written as \\[\n\\symbfit h(\\symbfit x_0+ \\symbfit \\varepsilon) \\approx h(\\symbfit x_0) + D \\symbfit h(\\symbfit x_0) \\, \\symbfit \\varepsilon\n\\]\n\n\nExamples of Taylor series approximations of second order:\n\n\\(\\log(x_0+\\varepsilon) \\approx \\log(x_0) + \\frac{\\varepsilon}{x_0} - \\frac{\\varepsilon^2}{2 x_0^2}\\)\n\\(\\frac{x_0}{x_0+\\varepsilon} \\approx 1 - \\frac{\\varepsilon}{x_0} + \\frac{\\varepsilon^2}{ x_0^2}\\)\n\n\n\nAround a local extremum \\(\\symbfit x_0\\) (maximum or minimum) where the gradient vanishes (\\(h(\\symbfit x_0) = 0\\)) the quadratic approximation of the function \\(h(\\symbfit x)\\) simplifies to\n\\[\nh(\\symbfit x_0+ \\symbfit \\varepsilon) \\approx h(\\symbfit x_0) +  \\frac{1}{2} \\symbfit \\varepsilon^T \\nabla \\nabla^T h(\\symbfit x_0) \\symbfit \\varepsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#conditions-for-a-local-extremum-of-a-function",
    "href": "02-vector-calculus.html#conditions-for-a-local-extremum-of-a-function",
    "title": "2  Vector and matrix calculus",
    "section": "2.6 Conditions for a local extremum of a function",
    "text": "2.6 Conditions for a local extremum of a function\nTo check if \\(x_0\\) or \\(\\symbfit x_0\\) is a local extremum, i.e. a local maximum or a local minimum, of a differentiable function \\(h(x)\\) or \\(h(\\symbfit x)\\) we can use the following conditions:\nFor a function of a single variable:\n\nFirst derivative is zero at the extremum: \\(h'(x_0) = 0\\).\nIf the second derivative \\(h''(x_0) &lt; 0\\) at the extremum is negative then it is a maximum.\nIf the second derivative \\(h''(x_0) &gt; 0\\) at the extremum is positive it is a minimum.\n\nNote that conditions ii) and iii) are sufficient but not necessary. For a minimum, it is necessary that the second derivative is non-negative, and for a maximum that the second derivative is non-positive.\nFor a function of several variables:\n\nGradient vanishes at extremum: \\(\\nabla h(\\symbfit x_0)=0\\).\nIf the Hessian \\(\\nabla \\nabla^T h(\\symbfit x_0)\\) is negative definite (= all eigenvalues of Hessian matrix are negative) then the extremum is a maximum.\nIf the Hessian is positive definite (= all eigenvalues of Hessian matrix are positive) then the extremum is a minimum.\n\nAgain, conditions ii) and iii) are sufficient but not necessary. For a minimum it is necessary that the Hessian is positive semi-definite, and for a maximum that the Hessian is negative semi-definite.\n\nMinimum with vanishing second derivative:\n\\(x^4\\) clearly has a minimum at \\(x_0=0\\). As required the first derivative \\(4 x^3\\) vanishes at \\(x_0=0\\). However, the second derivative \\(12 x^2\\) also vanishes at \\(x_0=0\\), showing that a positive second derivative is not necessary for a minimum.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  },
  {
    "objectID": "02-vector-calculus.html#convex-and-concave-functions",
    "href": "02-vector-calculus.html#convex-and-concave-functions",
    "title": "2  Vector and matrix calculus",
    "section": "2.7 Convex and concave functions",
    "text": "2.7 Convex and concave functions\nA function \\(h(\\symbfit x)\\) is convex if for all \\(\\symbfit x_1\\) and \\(\\symbfit x_2\\) the line segment from point \\((\\symbfit x_1, h(\\symbfit x_1))\\) to point \\((\\symbfit x_2, h(\\symbfit x_2))\\) never lies below the function. Moreover, the function is strictly convex if the line segment always lies above the curve, apart from the two end points:\n\\[\n\\lambda h(\\symbfit x_1) + (1-\\lambda) h(\\symbfit x_2) \\geq h(\\lambda \\symbfit x_1 + (1-\\lambda) \\symbfit x_2)\n\\] for all \\(\\lambda \\in [0, 1]\\).\nEquivalently, a differentiable function \\(h(\\symbfit x)\\) is convex (strictly convex) if for all \\(\\symbfit x_0\\) the function \\(h(\\symbfit x)\\) never lies below (always lies above, except at \\(\\symbfit x_0\\)) the linear approximation through the point \\((\\symbfit x_0, h(\\symbfit x_0))\\): \\[\nh(\\symbfit x) \\geq h(\\symbfit x_0) + \\nabla^T h(\\symbfit x_0)\\, (\\symbfit x-\\symbfit x_0)\n\\]\nFor a convex function a vanishing gradient at \\(\\symbfit x_0\\) indicates a minimum at \\(\\symbfit x_0\\). Furthermore, any local minimum must also be a global minimum (for a differentiable function this follows directly from the last inequality). For a strictly convex function the minimum is unique so there is at most one local/global minimum in that case.\nIf \\(h(\\symbfit x)\\) is convex, then \\(-h(\\symbfit x)\\) is concave, and the criteria above can be adapted accordingly to check for concavity and strict concavity, as well as to identify local/global maxima.\n(Strictly) convex and concave functions are convenient objective functions in optimisation as it is straightforward to find their local/global extrema, both analytically and numerically.\nAs the shape of a convex function resembles that of a valley, one way to memorise that fact is that a valley is convex.\n\nConvex functions:\nThis is a convex function but not a strictly convex function:\n\n\\(\\max(x^2, | x |  )\\)\n\nThe following are strictly convex functions:\n\n\\(x^2\\),\n\\(x^4\\),\n\\(e^x\\),\n\\(x \\log(x)\\) for \\(x&gt;0\\).\n\nOn the other hand, this is not a convex function:\n\n\\(\\frac{1}{x^2}\\) for all \\(x \\neq 0\\).\n\nHowever, the function in last example is strictly convex if the domain is restricted to either \\(x &gt;0\\) or \\(x&lt;0\\).\n\n\nConcave functions:\nThe following are strictly concave functions:\n\n\\(-x^2\\),\n\\(\\log(x)\\) for \\(x&gt;0\\),\n\\(\\sqrt{x}\\) for \\(x&gt;0\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector and matrix calculus</span>"
    ]
  }
]