[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"Matrix Calculus Refresher notes written Korbinian Strimmer 2018–2023. version 7 January 2024.questions, comments, corrections please email korbinian.strimmer@manchester.ac.uk.","code":""},{"path":"index.html","id":"updates","chapter":"Welcome","heading":"Updates","text":"notes updated time time. view current\nversion visit \nonline version Matrix Calculus Refresher notes.may also wish download Matrix Calculus Refresher notes PDF A4 format printing (double page layout) 6x9 inch PDF use tablets (single page layout).","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"notes licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"about-the-author","chapter":"Preface","heading":"About the author","text":"Hello! name Korbinian Strimmer Professor Statistics.\nmember Statistics group Department Mathematics\nUniversity Manchester. can find information home page.","code":""},{"path":"preface.html","id":"about-the-notes","chapter":"Preface","heading":"About the notes","text":"statistics machine learning make frequent use matrix notation, matrix algebra, matrix decompositions also vector matrix calculus.\naim supplementary notes provide refresher students quickly gain working knowledge matrices calculus functions several variables.notes supporting information \nnumber lecture notes statistical courses teaching Department Mathematics University Manchester.includes current modules:MATH27720 Statistics 2: Likelihood Bayes andMATH38161 Multivariate Statistics,well retired module (offered ):MATH20802 Statistical Methods.","code":""},{"path":"matrix-essentials.html","id":"matrix-essentials","chapter":"1 Matrix essentials","heading":"1 Matrix essentials","text":"","code":""},{"path":"matrix-essentials.html","id":"overview","chapter":"1 Matrix essentials","heading":"1.1 Overview","text":"statistics frequently make use matrix calculations matrix notation.Throughout mostly work real matrices, .e. assume matrix elements \nreal numbers. However, one important matrix decomposition — eigenvalue decomposition — can yield complex-valued matrices even applied real matrices. Thus occasionally also need deal also complex numbers.details matrix theory please consult lecture notes related modules (e.g. linear algebra).","code":""},{"path":"matrix-essentials.html","id":"matrix-basics","chapter":"1 Matrix essentials","heading":"1.2 Matrix basics","text":"","code":""},{"path":"matrix-essentials.html","id":"matrix-notation","chapter":"1 Matrix essentials","heading":"1.2.1 Matrix notation","text":"matrix notation distinguish scalars, vectors, matrices:Scalar: \\(x\\), \\(X\\), lower upper case, plain type.Vector: \\(\\boldsymbol x\\), lower case, bold type. handwriting arrow \\(\\vec{x}\\) indicates vector.component notation write \\(\\boldsymbol x= \\begin{pmatrix} x_1 \\\\ \\vdots\\\\ x_d\\end{pmatrix}\\). default, vector \ncolumn vector, .e. elements arranged column index components \\(x_i\\) refers row.transpose vector (indicated superscript \\(T\\)) turns row vector. save space can write column vector \\(\\boldsymbol x\\)\n\\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\) \\(\\boldsymbol x^T\\) row vector.Matrix: \\(\\boldsymbol X\\), upper case, bold type. handwriting underscore\n\\(\\underline{X}\\) indicates matrix.component notation write \\(\\boldsymbol X= (x_{ij})\\). convention, first index (\\(\\))\nscalar elements \\(x_{ij}\\) denotes row second index (\\(j\\)) column matrix.\n\\(n\\) number rows \\(d\\) number columns\ncan view matrix\n\\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_d)\\)\neither composed \\(d\\) column vectors\n\\(\\boldsymbol x_j = \\begin{pmatrix} x_{1j} \\\\ \\vdots \\\\ x_{nj}\\\\ \\end{pmatrix}\\)\n\\(\\boldsymbol X= \\begin{pmatrix} \\boldsymbol z_1^T \\\\ \\vdots \\\\ \\boldsymbol z_n^T\\end{pmatrix}\\)\n\ncomposed \\(n\\) row vectors \\(\\boldsymbol z_i^T = (x_{i1}, \\ldots, x_{id})\\).(column) vector dimension \\(d\\) matrix size \\(d\\times 1\\). row vector dimension \\(d\\) matrix size \\(1\\times d\\).\nscalar dimension \\(1\\) matrix size \\(1 \\times 1\\).","code":""},{"path":"matrix-essentials.html","id":"notation-for-random-vectors-and-matrices","chapter":"1 Matrix essentials","heading":"1.2.2 Notation for random vectors and matrices","text":"random matrix (vector) matrix (vector) whose elements random variables.common practise univariate statistics distinguish\nrandom variables fixed realisations using upper case versus lower case.\nHowever, notation breaks matrices vectors.\nTherefore, using multivariate statistics best always state explicitly whether \nmatrix, vector scalar random variable, especially obvious context.","code":""},{"path":"matrix-essentials.html","id":"special-matrices","chapter":"1 Matrix essentials","heading":"1.2.3 Special matrices","text":"\\(\\boldsymbol I_d\\) identity matrix. square matrix size\n\\(d \\times d\\) diagonal\nfilled 1 -diagonals filled 0.\n\\[\\boldsymbol I_d =\n\\begin{pmatrix}\n    1 & 0 & 0 & \\dots & 0\\\\\n    0 & 1 & 0 & \\dots & 0\\\\\n    0 & 0 & 1 &   & 0\\\\\n    \\vdots & \\vdots & & \\ddots &  \\\\\n    0 & 0 & 0 &  & 1 \\\\\n\\end{pmatrix}\\]\\(\\boldsymbol 1\\) matrix contains ones. often\nused form column vector \\(d\\) rows:\n\\[\\boldsymbol 1_d =\n\\begin{pmatrix}\n    1 \\\\\n    1 \\\\\n    1 \\\\\n    \\vdots   \\\\\n    1  \\\\\n\\end{pmatrix}\\]Similarly, \\(\\boldsymbol 0\\) matrix contains zeros. often\nused form column vector \\(d\\) rows:\n\\[\\boldsymbol 0_d =\n\\begin{pmatrix}\n    0 \\\\\n    0 \\\\\n    0 \\\\\n    \\vdots   \\\\\n    0  \\\\\n\\end{pmatrix}\\]diagonal matrix matrix -diagonal elements zero.\n\\(\\text{Diag}(\\boldsymbol )\\) access diagonal elements matrix vector \n\\(\\text{Diag}(a_1, \\ldots, a_d)\\) specify diagonal matrix listing diagonal elements.matrix can partitioned blocks submatrices.\nblock-structured matrix block matrix partioning rows columns two groups form\n\\[\n\\boldsymbol = \\begin{pmatrix} \\boldsymbol A_{11} & \\boldsymbol A_{12} \\\\ \\boldsymbol A_{21} & \\boldsymbol A_{22} \\\\ \\end{pmatrix} \\, ,\n\\]\n\\(\\boldsymbol A_{11}\\), \\(\\boldsymbol A_{22}\\), \\(\\boldsymbol A_{12}\\) \\(\\boldsymbol A_{21}\\) matrices.\n\\(\\boldsymbol \\) symmetric (hence square) \\(\\boldsymbol A_{11}\\) \\(\\boldsymbol A_{22}\\) must also symmetric\n\\(\\boldsymbol A_{21} = \\boldsymbol A_{12}^T\\).block diagonal matrix symmetric block matrix vanishing -diagonal blocks\nsymmetric submatrices along diagonal.triangular matrix square matrix whose elements either diagonal\nzero (upper vs. lower triangular matrix).","code":""},{"path":"matrix-essentials.html","id":"simple-matrix-operations","chapter":"1 Matrix essentials","heading":"1.3 Simple matrix operations","text":"","code":""},{"path":"matrix-essentials.html","id":"matrix-addition-and-multiplication","chapter":"1 Matrix essentials","heading":"1.3.1 Matrix addition and multiplication","text":"Matrices behave much like common numbers. example, can add matrices\n\\(\\boldsymbol C= \\boldsymbol + \\boldsymbol B\\)\nmultiply matrices \\(\\boldsymbol C= \\boldsymbol \\boldsymbol B\\).matrix addition \\(\\boldsymbol C= \\boldsymbol + \\boldsymbol B\\) add corresponding elements \\(c_{ij} = a_{ij} + b_{ij}\\). matrix addition \\(\\boldsymbol \\) \\(\\boldsymbol B\\) must dimensions, .e.\nnumber rows columns.dot product, scalar product, two vectors \\(\\boldsymbol \\) \\(\\boldsymbol b\\) scalar given \\(\\boldsymbol \\cdot \\boldsymbol b= \\langle \\boldsymbol , \\boldsymbol b\\rangle = \\boldsymbol ^T \\boldsymbol b= \\boldsymbol b^T \\boldsymbol = \\sum_{=1}^d a_{} b_{}\\).Matrix multiplication \\(\\boldsymbol C= \\boldsymbol \\boldsymbol B\\) obtained setting \\(c_{ij} = \\sum_{k=1}^m a_{ik} b_{kj}\\) \\(m\\) \nnumber columns \\(\\boldsymbol \\) number rows \\(\\boldsymbol B\\). Thus, \\(\\boldsymbol C\\) contains possible\ndot products row vectors \\(\\boldsymbol \\) column vectors \\(\\boldsymbol B\\).\nmatrix multiplication number columns \\(\\boldsymbol \\) must match number rows \\(\\boldsymbol B\\).\nNote matrix multiplication general (\\(m > 1\\)) commute, .e. \\(\\boldsymbol \\boldsymbol B\\neq \\boldsymbol B\\boldsymbol \\).","code":""},{"path":"matrix-essentials.html","id":"matrix-transpose","chapter":"1 Matrix essentials","heading":"1.3.2 Matrix transpose","text":"matrix transpose \\(\\boldsymbol ^T\\) indicate superscript \\(T\\) interchanges rows columns matrix. transpose\nlinear operator \\((\\boldsymbol + \\boldsymbol B)^T = \\boldsymbol ^T + \\boldsymbol B^T\\) \napplied matrix\nproduct reverses ordering, .e. \\((\\boldsymbol \\boldsymbol B)^T =\\boldsymbol B^T \\boldsymbol ^T\\).\\(\\boldsymbol = \\boldsymbol ^T\\) \\(\\boldsymbol \\) symmetric (square).construction given rectangular \\(\\boldsymbol \\) matrices\n\\(\\boldsymbol ^T \\boldsymbol \\) \\(\\boldsymbol \\boldsymbol ^T\\) symmetric non-negative diagonal.","code":""},{"path":"matrix-essentials.html","id":"matrix-summaries","chapter":"1 Matrix essentials","heading":"1.4 Matrix summaries","text":"","code":""},{"path":"matrix-essentials.html","id":"row-column-and-grand-sum","chapter":"1 Matrix essentials","heading":"1.4.1 Row, column and grand sum","text":"Assume matrix \\(\\boldsymbol \\) size \\(n \\times m\\).sum \\(m\\) entries row \\(\\) \\(\\sum_{j=1}^m a_{ij}\\).\nmatrix notation \\(n\\) row sums given \n\\(\\boldsymbol \\, \\boldsymbol 1_m\\).sum \\(n\\) entries column \\(j\\) \\(\\sum_{=1}^n a_{ij}\\).\nmatrix notation \\(m\\) column sums \\(\\boldsymbol ^T \\boldsymbol 1_n\\).grand sum matrix entries \\(\\boldsymbol \\) obtained \n\\[\n\\sum_{=1}^n \\sum_{j=1}^m a_{ij} = \\boldsymbol 1_n^T \\, \\boldsymbol \\, \\boldsymbol 1_m\n\\]","code":""},{"path":"matrix-essentials.html","id":"matrix-trace","chapter":"1 Matrix essentials","heading":"1.4.2 Matrix trace","text":"trace matrix sum diagonal elements \\(\\text{Tr}(\\boldsymbol ) = \\sum a_{ii}\\).trace invariant transposition, .e.\n\\[\n\\text{Tr}(\\boldsymbol ) = \\text{Tr}(\\boldsymbol ^T )\n\\]useful identity matrix trace product two matrices \n\\[\n\\text{Tr}(\\boldsymbol \\boldsymbol B) = \\text{Tr}( \\boldsymbol B\\boldsymbol )  \n\\]Intriguingly, trace matrix equals sum eigenvalues matrix (see ).","code":""},{"path":"matrix-essentials.html","id":"row-column-and-grand-sum-of-squares","chapter":"1 Matrix essentials","heading":"1.4.3 Row, column and grand sum of squares","text":"sum \\(m\\) squared entries row \\(\\) \\(\\sum_{j=1}^m a_{ij}^2\\).\nmatrix notation \\(n\\) row sums squares given \n\\(\\text{Diag}( \\boldsymbol \\boldsymbol ^T)\\).sum \\(n\\) squared entries column \\(j\\) \\(\\sum_{=1}^n a_{ij}^2\\).\nmatrix notation \\(m\\) column sums squares \\(\\text{Diag}( \\boldsymbol ^T \\boldsymbol )\\).grand sum squared elements \\(\\boldsymbol \\) obtained \n\\[\n\\sum_{=1}^n \\sum_{j=1}^m a_{ij}^2 = \\text{Tr}(\\boldsymbol ^T \\boldsymbol ) = \\text{Tr}(\\boldsymbol \\boldsymbol ^T)\n\\]\nalso known squared Frobenius norm \\(\\boldsymbol \\) (see ).","code":""},{"path":"matrix-essentials.html","id":"sum-of-squared-diagonal-entries","chapter":"1 Matrix essentials","heading":"1.4.4 Sum of squared diagonal entries","text":"sum squared entries diagonal matrix notation\n\\[\n\\text{Diag}(\\boldsymbol )^T \\text{Diag}(\\boldsymbol ) = \\sum_{=1}^{\\min(n,m)} a_{ii}^2\n\\]","code":""},{"path":"matrix-essentials.html","id":"frobenius-inner-product","chapter":"1 Matrix essentials","heading":"1.4.5 Frobenius inner product","text":"Frobenius inner product two rectangular matrices dimension scalar\n\\[\n\\begin{split}\n\\langle \\boldsymbol , \\boldsymbol B\\rangle &=  \\text{Tr}(\\boldsymbol \\boldsymbol B^T) = \\text{Tr}(\\boldsymbol B\\boldsymbol ^T)\\\\\n&=  \\text{Tr}(\\boldsymbol ^T \\boldsymbol B) = \\text{Tr}(\\boldsymbol B^T \\boldsymbol )\\\\\n&= \\sum_{,j} a_{ij} b_{ij} \\,.\n\\end{split}\n\\]\ngeneralises dot product two vectors.\nNote dot product can therefore also written trace matrix\n\\[\n\\langle \\boldsymbol , \\boldsymbol b\\rangle = \\text{Tr}( \\boldsymbol \\boldsymbol b^T ) = \\text{Tr}( \\boldsymbol b\\boldsymbol ^T) \\,.\n\\]","code":""},{"path":"matrix-essentials.html","id":"euclidean-norm","chapter":"1 Matrix essentials","heading":"1.4.6 Euclidean norm","text":"squared Euclidean norm squared length vector \\(\\boldsymbol \\)\n\ndot product \\(||\\boldsymbol ||^2_2 = \\boldsymbol \\cdot \\boldsymbol = \\langle \\boldsymbol , \\boldsymbol \\rangle = \\boldsymbol ^T \\boldsymbol = \\boldsymbol \\boldsymbol ^T = \\sum_{=1}^d a_i^2\\).squared Frobenius norm generalisation squared Euclidean vector norm rectangular matrix \nsum squares elements.\nUsing trace can written \n\\[\n\\begin{split}\n||\\boldsymbol ||_F^2 &= \\langle \\boldsymbol , \\boldsymbol \\rangle \\\\\n&= \\text{Tr}(\\boldsymbol ^T \\boldsymbol ) = \\text{Tr}(\\boldsymbol \\boldsymbol ^T) \\\\\n&= \\sum_{,j} a_{ij}^2 \\,.\n\\end{split}\n\\]useful identity squared Frobenius norm difference two matrices\n\n\\[\n\\begin{split}\n||\\boldsymbol - \\boldsymbol B||_F^2 &= ||\\boldsymbol ||_F^2 + ||\\boldsymbol B||_F^2  - 2 \\langle \\boldsymbol , \\boldsymbol B\\rangle \\\\\n&= \\text{Tr}(\\boldsymbol ^T \\boldsymbol ) +\\text{Tr}(\\boldsymbol B^T \\boldsymbol B) - 2 \\text{Tr}(\\boldsymbol ^T \\boldsymbol B) \\\\\n&= \\sum_{,j} (a_{ij}-b_{ij})^2 \\,.\n\\end{split}\n\\]Frobenius norm matrix \\(||\\boldsymbol ||_F\\) confused induced \\(2\\)-norm matrix \\(||\\boldsymbol ||_2\\). latter equals maximum\nabsolute eigenvalue matrix, \\(||\\boldsymbol ||_2 \\leq ||\\boldsymbol ||_F\\).","code":""},{"path":"matrix-essentials.html","id":"determinant-of-a-matrix","chapter":"1 Matrix essentials","heading":"1.4.7 Determinant of a matrix","text":"\\(\\boldsymbol \\) square matrix determinant \\(\\det(\\boldsymbol )\\) scalar measuring volume spanned column vectors \\(\\boldsymbol \\) sign determined orientation vectors.\\(\\det(\\boldsymbol ) \\neq 0\\) matrix \\(\\boldsymbol \\) non-singular non-degenerate. Conversely, \n\\(\\det(\\boldsymbol ) =0\\) matrix \\(\\boldsymbol \\) singular degenerate.Intriguingly, determinant \\(\\boldsymbol \\) product eigenvalues \\(\\boldsymbol \\) (see ).One way compute determinant matrix \\(\\boldsymbol \\) Laplace cofactor\nexpansion approach proceeds recursively based determinants submatrices \\(\\boldsymbol A_{-,-j}\\) obtained deleting row \\(\\) column \\(j\\) \\(\\boldsymbol \\). Specifically, \nlevel compute thecofactor expansion either\nalong \\(\\)-th row — pick row \\(\\):\n\\[\\det(\\boldsymbol ) = \\sum_{j=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})  \\text{ , }\\]\nalong \\(j\\)-th column — pick \\(j\\):\n\\[\\det(\\boldsymbol ) = \\sum_{=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})\\].\nalong \\(\\)-th row — pick row \\(\\):\n\\[\\det(\\boldsymbol ) = \\sum_{j=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})  \\text{ , }\\]along \\(j\\)-th column — pick \\(j\\):\n\\[\\det(\\boldsymbol ) = \\sum_{=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})\\].repeat submatrix scalar \\(\\) \\(\\det()=\\,.\\)recursive nature algorithm leads complexity order \\(O(d!)\\) practical except small \\(d\\).\nTherefore, practice efficient algorithms computing determinants used still algorithmic complexity order \\(O(d^3)\\) large dimensions obtaining determinants \nexpensive.However, specially structured matrices allow fast calculation.determinant triangular matrix (thus also diagonal matrix)\n\\[\n\\boldsymbol = \\begin{pmatrix}\na_{11} & 0       & \\cdots & 0\\\\\na_{21} & a_{22}  & \\cdots & 0\\\\\n\\vdots  & \\vdots & \\ddots & 0 \\\\\na_{d1} & a_{d2} & \\cdots & a_{dd} \\\\\n\\end{pmatrix}\n\\]\nproduct diagonal elements, .e. \\(\\det(\\boldsymbol ) = \\prod_{=1}^d a_{ii}\\).two-dimensional matrix \\(\\boldsymbol = \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\\\end{pmatrix}\\)\ndeterminant \\(\\det() = a_{11} a_{22} - a_{12} a_{21}\\).block-structured square matrix\n\\[\n\\boldsymbol = \\begin{pmatrix} \\boldsymbol A_{11} & \\boldsymbol A_{12} \\\\ \\boldsymbol A_{21} & \\boldsymbol A_{22} \\\\ \\end{pmatrix} \\, ,\n\\]\nmatrices diagonal \\(\\boldsymbol A_{11}\\) \\(\\boldsymbol A_{22}\\) square \n\\(\\boldsymbol A_{12}\\) \\(\\boldsymbol A_{21}\\) may rectangular,\ndeterminant \n\\[\n\\det(\\boldsymbol ) = \\det(\\boldsymbol A_{22}) \\det(\\boldsymbol C_1) = \\det(\\boldsymbol A_{11}) \\det(\\boldsymbol C_2)\n\\]\n(Schur complement \\(\\boldsymbol A_{22}\\))\n\\[\n\\boldsymbol C_1 = \\boldsymbol A_{11} -  \\boldsymbol A_{12}  \\boldsymbol A_{22}^{-1}  \\boldsymbol A_{21}\n\\]\n(Schur complement \\(\\boldsymbol A_{11}\\))\n\\[\n\\boldsymbol C_2 = \\boldsymbol A_{22} -  \\boldsymbol A_{21}  \\boldsymbol A_{11}^{-1}  \\boldsymbol A_{12}\n\\]\nNote \\(\\boldsymbol C_1\\) \\(\\boldsymbol C_2\\) square matrices.block-diagonal matrix \\(\\boldsymbol \\) \\(\\boldsymbol A_{12} = 0\\) \\(\\boldsymbol A_{21} = 0\\)\ndeterminant \\(\\det(\\boldsymbol ) = \\det(\\boldsymbol A_{11}) \\det(\\boldsymbol A_{22})\\),\n.e. product determinants submatrices along diagonal.Determinants multiplicative property,\n\\[\\det(\\boldsymbol \\boldsymbol B) = \\det(\\boldsymbol B\\boldsymbol ) = \\det(\\boldsymbol ) \\det(\\boldsymbol B) \\,.\\]\n\\(\\boldsymbol \\) \\(\\boldsymbol B\\) square dimension.rectangular \\(\\boldsymbol \\) (\\(n \\times m\\)) rectangular \\(\\boldsymbol B\\) (\\(m \\times n\\))\n\\(m \\geq n\\)\ngeneralises Cauchy-Binet formula\n\\[\n\\det(\\boldsymbol \\boldsymbol B) = \\sum_{w} \\det(\\boldsymbol A_{,w}) \\det(\\boldsymbol B_{w,})\n\\]\nsummation \\(\\binom{m}{n}\\) index subsets \\(w\\) size \\(n\\) taken \\(\\{1, \\ldots, m\\}\\) keeping ordering\n\\(\\boldsymbol A_{,w}\\) \\(\\boldsymbol B_{w,}\\) corresponding square \\(n \\times n\\) submatrices. \\(m < n\\) \\(\\det(\\boldsymbol \\boldsymbol B) = 0\\).scalar \\(\\)\n\\(\\det(\\boldsymbol B) = ^d \\det(\\boldsymbol B)\\) \\(d\\) dimension \\(\\boldsymbol B\\).Another important identity \n\\[\\det(\\boldsymbol I_n + \\boldsymbol \\boldsymbol B) = \\det(\\boldsymbol I_m + \\boldsymbol B\\boldsymbol )\\]\n\\(\\boldsymbol \\) \\(\\boldsymbol B\\) rectangular matrices. called Weinstein-Aronszajn determinant identity (also credited Sylvester).","code":""},{"path":"matrix-essentials.html","id":"matrix-inverse","chapter":"1 Matrix essentials","heading":"1.5 Matrix inverse","text":"","code":""},{"path":"matrix-essentials.html","id":"inversion-of-square-matrix","chapter":"1 Matrix essentials","heading":"1.5.1 Inversion of square matrix","text":"\\(\\boldsymbol \\) square matrix inverse matrix \\(\\boldsymbol ^{-1}\\) matrix\n\n\\[\\boldsymbol ^{-1} \\boldsymbol = \\boldsymbol \\boldsymbol ^{-1}=  \\boldsymbol \\, .\\]\nnon-singular matrices \\(\\det(\\boldsymbol ) \\neq 0\\) invertible.\\(\\det(\\boldsymbol ^{-1} \\boldsymbol ) = \\det(\\boldsymbol ) = 1\\) \ndeterminant inverse matrix equals\ninverse determinant,\n\\[\\det(\\boldsymbol ^{-1}) = \\det(\\boldsymbol )^{-1} \\,.\\]transpose inverse inverse transpose\n\n\\[\n\\begin{split}\n(\\boldsymbol ^{-1})^T &= (\\boldsymbol ^{-1})^T \\,  \\boldsymbol ^T (\\boldsymbol ^{T})^{-1}   \\\\\n&= (\\boldsymbol \\boldsymbol ^{-1})^T \\, (\\boldsymbol ^{T})^{-1} = (\\boldsymbol ^{T})^{-1} \\,. \\\\\n\\end{split}\n\\]inverse matrix product \\((\\boldsymbol \\boldsymbol B)^{-1} = \\boldsymbol B^{-1} \\boldsymbol ^{-1}\\)\nproduct indivdual matrix inverses reverse order.many different algorithms compute inverse matrix\n(essentially problem solving system equations).\ncomputational complexity matrix inversion order \\(O(d^3)\\)\n\\(d\\) dimension \\(\\boldsymbol \\). Therefore matrix inversion costly higher dimensions.Example 1.1  Inversion \\(2 \\times 2\\) matrix:inverse matrix \\(= \\begin{pmatrix} & b \\\\ c & d \\end{pmatrix}\\) \n\\(^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & \\end{pmatrix}\\)","code":""},{"path":"matrix-essentials.html","id":"inversion-of-structured-matrices","chapter":"1 Matrix essentials","heading":"1.5.2 Inversion of structured matrices","text":"However, specially structured matrices inversion can done effectively:inverse diagonal matrix another diagonal matrix obtained inverting diagonal elements.generally, inverse block-diagonal matrix obtained individually inverting blocks along diagonal.Woodbury matrix identity simplifies inversion matrices can \nwritten \\(\\boldsymbol + \\boldsymbol U\\boldsymbol B\\boldsymbol V\\) \\(\\boldsymbol \\) \\(\\boldsymbol B\\) square \n\\(\\boldsymbol U\\) \\(\\boldsymbol V\\) suitable rectangular matrices:\n\\[\n(\\boldsymbol + \\boldsymbol U\\boldsymbol B\\boldsymbol V)^{-1} = \\boldsymbol ^{-1} - \\boldsymbol ^{-1} \\boldsymbol U(\\boldsymbol B^{-1} + \\boldsymbol V\\boldsymbol ^{-1} \\boldsymbol U)^{-1} \\boldsymbol V\\boldsymbol ^{-1}\n\\]\nTypically, inverse \\(\\boldsymbol ^{-1}\\) either already known can easily obtained \ndimension \\(\\boldsymbol B\\) much lower \\(\\boldsymbol \\).class matrices can easily inverted orthogonal matrices whose inverse \nobtained simply transposing matrix.","code":""},{"path":"matrix-essentials.html","id":"orthogonal-matrices","chapter":"1 Matrix essentials","heading":"1.6 Orthogonal matrices","text":"","code":""},{"path":"matrix-essentials.html","id":"properties","chapter":"1 Matrix essentials","heading":"1.6.1 Properties","text":"orthogonal matrix \\(\\boldsymbol Q\\) square matrix property \\(\\boldsymbol Q^T = \\boldsymbol Q^{-1}\\), .e.\ntranspose also inverse. implies \\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol \\).column row vectors \\(\\boldsymbol Q\\) length 1. implies \nelement \\(q_{ij}\\) \\(\\boldsymbol Q\\) can take value interval \\([-1, 1]\\).identity matrix \\(\\boldsymbol \\) simplest example orthogonal matrix.squared Euclidean Frobenius norm preserved vector \\(\\boldsymbol \\) matrix \\(\\boldsymbol \\) multiplied orthogonal matrix \\(\\boldsymbol Q\\):\n\\[\n|| \\boldsymbol Q\\boldsymbol ||^2_2 = (\\boldsymbol Q\\boldsymbol )^T \\boldsymbol Q\\boldsymbol = \\boldsymbol ^T \\boldsymbol = || \\boldsymbol ||^2_2\n\\]\n\n\\[\n|| \\boldsymbol Q\\boldsymbol ||^2_F = \\text{Tr}\\left((\\boldsymbol Q\\boldsymbol )^T \\boldsymbol Q\\boldsymbol \\right) = \\text{Tr}\\left(\\boldsymbol ^T \\boldsymbol \\right) = || \\boldsymbol ||^2_F\n\\]Multiplication \\(\\boldsymbol Q\\) vector results \nnew vector length change direction (unless \\(\\boldsymbol Q=\\boldsymbol \\)).\northogonal matrix\n\\(\\boldsymbol Q\\) can thus interpreted geometrically operator performing\nrotation, reflection /permutation.product \\(\\boldsymbol Q_3 = \\boldsymbol Q_1 \\boldsymbol Q_2\\) two orthogonal matrices \\(\\boldsymbol Q_1\\) \\(\\boldsymbol Q_2\\) yields another orthogonal matrix \\(\\boldsymbol Q_3 \\boldsymbol Q_3^T = \\boldsymbol Q_1 \\boldsymbol Q_2 (\\boldsymbol Q_1 \\boldsymbol Q_2)^T = \\boldsymbol Q_1 \\boldsymbol Q_2 \\boldsymbol Q_2^T \\boldsymbol Q_1^T = \\boldsymbol \\).determinant \\(\\det(\\boldsymbol Q)\\) orthogonal matrix either +1 -1,\n\\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol \\) thus \\(\\det(\\boldsymbol Q)\\det(\\boldsymbol Q^T) = \\det(\\boldsymbol Q)^2 = \\det(\\boldsymbol ) = 1\\).set orthogonal matrices dimension \\(d\\) together multiplication\nform group called orthogonal group \\(O(d)\\).\nsubset orthogonal matrices \\(\\det(\\boldsymbol Q)=1\\) called rotation matrices form multiplication special orthogonal group \\((d)\\).\nOrthogonal matrices \\(\\det(\\boldsymbol Q)=-1\\) rotation-reflection matrices.","code":""},{"path":"matrix-essentials.html","id":"semi-orthogonal-matrices","chapter":"1 Matrix essentials","heading":"1.6.2 Semi-orthogonal matrices","text":"rectangular \\(d \\times k\\) matrix \\(\\boldsymbol Q\\) semi-orthogonal\n\\(k < d\\) \\(k\\) column vectors orthonormal hence \\(\\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol I_k\\), \\(k > d\\) \\(d\\) row vectors orthonormal \n\\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol I_d\\).set (semi)-orthogonal matrices \\(\\boldsymbol Q\\) \\(k \\leq d\\) column vectors known Stiefel manifold \\(\\text{St}(d, k)\\).","code":""},{"path":"matrix-essentials.html","id":"generating-orthogonal-matrices","chapter":"1 Matrix essentials","heading":"1.6.3 Generating orthogonal matrices","text":"two dimensions \\((d=2)\\) orthogonal matrices \\(\\boldsymbol R\\) representing rotations \\(\\det(\\boldsymbol R)=1\\) \ngiven \n\\[\n\\boldsymbol R(\\theta) =\n\\begin{pmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{pmatrix}\n\\]\nrepresenting rotation-reflections \\(\\boldsymbol G\\) \\(\\det(\\boldsymbol G)=-1\\) \n\\[\n\\boldsymbol G(\\theta) =\n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix}\\,.\n\\]\nEvery orthogonal matrix dimension \\(d=2\\)\ncan represented product two rotation-reflection\nmatrices \n\\[\n\\boldsymbol R(\\theta) = \\boldsymbol G(\\theta)\\, \\boldsymbol G(0) =  \n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}\\,.\n\\]\nThus, matrix \\(\\boldsymbol G\\) generator two-dimensional orthogonal matrices.\nNote \\(\\boldsymbol G(\\theta)\\) symmetric, orthogonal determinant -1.generally, applicable arbitrary dimension, role generator taken Householder reflection matrix\n\\[\n\\boldsymbol Q_{HH}(\\boldsymbol v) = \\boldsymbol - 2 \\boldsymbol v\\boldsymbol v^T\n\\]\n\\(\\boldsymbol v\\) vector unit length (\\(\\boldsymbol v^T \\boldsymbol v=1\\)) orthogonal \nreflection hyperplane. Note \\(\\boldsymbol Q_{HH}(\\boldsymbol v) = \\boldsymbol Q_{HH}(-\\boldsymbol v)\\).\nconstruction matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\) symmetric, orthogonal determinant -1.can shown \\(d\\)-dimensional orthogonal matrix \\(\\boldsymbol Q\\) can represented product \\(d\\) Householder reflection matrices.\ntwo-dimensional generator \\(\\boldsymbol G(\\theta)\\) recovered Householder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\)\n\\(\\boldsymbol v= \\begin{pmatrix} -\\sin \\frac{\\theta}{2} \\\\ \\cos \\frac{\\theta}{2} \\end{pmatrix}\\)\n\\(\\boldsymbol v= \\begin{pmatrix} \\sin \\frac{\\theta}{2} \\\\ -\\cos \\frac{\\theta}{2} \\end{pmatrix}\\).","code":""},{"path":"matrix-essentials.html","id":"permutation-matrix","chapter":"1 Matrix essentials","heading":"1.6.4 Permutation matrix","text":"special type orthogonal matrix permutation matrix \\(\\boldsymbol P\\) created \npermuting rows /columns identity matrix \\(\\boldsymbol \\). Thus, row column\n\\(\\boldsymbol P\\) contains exactly one entry 1, necessarily diagonal.permutation matrix \\(\\boldsymbol P\\) multiplied matrix \\(\\boldsymbol \\) acts operator\npermuting columns (\\(\\boldsymbol \\boldsymbol P\\)) rows (\\(\\boldsymbol P\\boldsymbol \\)).\nset \\(d\\) elements exist \\(d!\\) permutations. Thus, dimension \\(d\\) \n\\(d!\\) possible permutation matrices (including identity matrix).determinant permutation matrix either +1 -1.\nproduct two permutation matrices yields another permutation matrix.Symmetric permutation matrices correspond self-inverse permutations\n(.e. permutation matrix inverse), also called permutation involutions.\ncan determinant +1 -1.transposition permutation two elements exchanged.\nThus, transposition matrix \\(\\boldsymbol T\\)\nexactly two rows /columns exchanged compared identity matrix \\(\\boldsymbol \\).\nTranspositions self-inverse, transposition matrices symmetric.\n\\(\\frac{d (d-1)}{2}\\) different transposition matrices.\ndeterminant transposition matrix \\(\\det(\\boldsymbol T)= -1\\).Note transposition matrix instance Householder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\)\nvector \\(\\boldsymbol v\\) filled zeros except two elements value\n\\(\\frac{\\sqrt{2}}{2}\\) \\(-\\frac{\\sqrt{2}}{2}\\).permutation \\(d\\) elements can generated series \\(d-1\\) transpositions.\nCorrespondingly, permutation matrix \\(\\boldsymbol P\\) can constructed multiplication identity\nmatrix \\(d-1\\) transposition matrices. number transpositions even \\(\\det(\\boldsymbol P) = 1\\) otherwise\nuneven number \\(\\det(\\boldsymbol P) = -1\\). called sign signature permutation.set permutations form symmetric group \\(S_d\\), subset even permutations (positive sign \\(\\det(\\boldsymbol P)=1\\)) alternating group \\(A_d\\).","code":""},{"path":"matrix-essentials.html","id":"eigenvalues-and-eigenvectors","chapter":"1 Matrix essentials","heading":"1.7 Eigenvalues and eigenvectors","text":"","code":""},{"path":"matrix-essentials.html","id":"definition","chapter":"1 Matrix essentials","heading":"1.7.1 Definition","text":"Assume square matrix \\(\\boldsymbol \\) size \\(d \\times d\\).\nvector \\(\\boldsymbol u\\neq 0\\) called eigenvector matrix \\(\\boldsymbol \\) \\(\\lambda\\) corresponding\neigenvalue \\[\\boldsymbol \\boldsymbol u= \\boldsymbol u\\lambda \\, .\\]\ncalled eigenvalue equation eigenequation.","code":""},{"path":"matrix-essentials.html","id":"finding-eigenvalues-and-vectors","chapter":"1 Matrix essentials","heading":"1.7.2 Finding eigenvalues and vectors","text":"find eigenvalues eigenvectors eigenequation rewritten \n\\[(\\boldsymbol -\\boldsymbol \\lambda ) \\; \\boldsymbol u= \\boldsymbol 0\\,.\\]\nequation hold eigenvector \\(\\boldsymbol u\\neq 0\\) eigenvalue\n\\(\\lambda\\) implies matrix \\(\\boldsymbol -\\boldsymbol \\lambda\\) singular.\nCorrespondingly, determinant must vanish\n\\[\\det(\\boldsymbol -\\boldsymbol \\lambda ) =0 \\,.\\]\ncalled characteristic equation matrix \\(\\boldsymbol \\), solution yields \\(d\\)\neigenvalues \\(\\lambda_1, \\ldots, \\lambda_d\\). Note eigenvalues need \ndistinct may complex even matrix \\(\\boldsymbol \\) real.complex eigenvalues, real matrix eigenvalues come conjugate pairs.\nHence, complex \\(\\lambda_1 = r e^{\\phi}\\) also corresponding complex eigenvalue \\(\\lambda_2 = r e^{-\\phi}\\).Given eigenvalues solve eigenequation corresponding non-zero eigenvectors\n\\(\\boldsymbol u_1, \\ldots, \\boldsymbol u_d\\). Note eigenvectors real matrices can complex components.\nAlso eigenvector defined eigenequation scalar.\nconvention eigenvectors therefore typically standardised unit length still leaves\nsign ambiguity real eigenvectors implies complex eigenvectors defined factor modulus 1.","code":""},{"path":"matrix-essentials.html","id":"eigenequation-in-matrix-notation","chapter":"1 Matrix essentials","heading":"1.7.3 Eigenequation in matrix notation","text":"matrix\n\\[\\boldsymbol U= (\\boldsymbol u_1, \\ldots, \\boldsymbol u_d)\\] containing standardised eigenvectors columns diagonal matrix\n\\[\\boldsymbol \\Lambda= \\begin{pmatrix}\n    \\lambda_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}\n\\end{pmatrix}\\]\ncontaining eigenvalues (typically sorted order magnitude) eigenvalue equation can written \n\\[\\boldsymbol \\boldsymbol U= \\boldsymbol U\\boldsymbol \\Lambda\\,.\\]","code":""},{"path":"matrix-essentials.html","id":"permutation-of-eigenvalues","chapter":"1 Matrix essentials","heading":"1.7.4 Permutation of eigenvalues","text":"eigenvalues order, may apply permutation matrix \\(\\boldsymbol P\\) arrange order.\n\\(\\boldsymbol \\Lambda^{\\text{sort}} = \\boldsymbol P^T \\boldsymbol \\Lambda\\boldsymbol P\\) sorted eigenvalues\n\\(\\boldsymbol U^{\\text{sort}} = \\boldsymbol U\\boldsymbol P\\) corresponding eigenvectors eigenequation becomes\n\\[\\boldsymbol \\boldsymbol U^{\\text{sort}} =  \\boldsymbol \\boldsymbol U\\boldsymbol P= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol P=  \\boldsymbol U\\boldsymbol P\\boldsymbol P^T \\boldsymbol \\Lambda\\boldsymbol P=  \\boldsymbol U^{\\text{sort}} \\boldsymbol \\Lambda^{\\text{sort}} \\,.\\]","code":""},{"path":"matrix-essentials.html","id":"similar-matrices","chapter":"1 Matrix essentials","heading":"1.7.5 Similar matrices","text":"Two matrices \\(\\boldsymbol \\) \\(\\boldsymbol B\\) called similar share eigenvalues.\\(\\boldsymbol \\) eigenvalues \\(\\boldsymbol \\Lambda\\) eigenvectors \\(\\boldsymbol U\\) can construct similar \\(\\boldsymbol B\\) via similarity transformation \\(\\boldsymbol B= \\boldsymbol M\\boldsymbol \\boldsymbol M^{-1}\\) \\(\\boldsymbol M\\) invertible matrix.\\(\\boldsymbol \\Lambda\\) eigenvalues \\(\\boldsymbol B\\) \\(\\boldsymbol V= \\boldsymbol M\\boldsymbol U\\) eigenvectors \n\\[\n\\boldsymbol B\\boldsymbol V= \\boldsymbol M\\boldsymbol \\boldsymbol M^{-1} \\boldsymbol M\\boldsymbol U= \\boldsymbol M\\boldsymbol \\boldsymbol U= \\boldsymbol M\\boldsymbol U\\boldsymbol \\Lambda= \\boldsymbol V\\boldsymbol \\Lambda\\,.\n\\]","code":""},{"path":"matrix-essentials.html","id":"defective-matrix","chapter":"1 Matrix essentials","heading":"1.7.6 Defective matrix","text":"cases eigenvectors \\(\\boldsymbol u_i\\) linearly independent form basis span \\(d\\) dimensional space.However, case \nmatrix \\(\\boldsymbol \\) complete basis eigenvectors, matrix called defective. case\nmatrix \\(\\boldsymbol U\\) containing eigenvectors singular \\(\\det(\\boldsymbol U)=0\\).example defective matrix \n\\(\\begin{pmatrix} 1 &1 \\\\ 0 & 1 \\\\ \\end{pmatrix}\\)\ndeterminant 1 can inverted column vectors form complete basis\none distinct eigenvector \\((1,0)^T\\) eigenvector basis incomplete.","code":""},{"path":"matrix-essentials.html","id":"eigenvalues-of-a-diagonal-or-triangular-matrix","chapter":"1 Matrix essentials","heading":"1.7.7 Eigenvalues of a diagonal or triangular matrix","text":"special case \\(\\boldsymbol \\) diagonal triangular matrix eigenvalues easily determined.\nfollows simple form determinants product diagonal elements.\nHence matrices characteristic equation becomes \\(\\prod_{}^d (a_{ii} -\\lambda) = 0\\) solution\n\\(\\lambda_i=a_{ii}\\), .e. eigenvalues equal diagonal elements.","code":""},{"path":"matrix-essentials.html","id":"eigenvalues-and-vectors-of-a-symmetric-matrix","chapter":"1 Matrix essentials","heading":"1.7.8 Eigenvalues and vectors of a symmetric matrix","text":"\\(\\boldsymbol \\) symmetric, .e. \\(\\boldsymbol = \\boldsymbol ^T\\), eigenvalues eigenvectors special properties:eigenvalues \\(\\boldsymbol \\) real,eigenvectors orthogonal, .e \\(\\boldsymbol u_i^T \\boldsymbol u_j = 0\\) \\(\\neq j\\), real. Thus, matrix \\(\\boldsymbol U\\) containing standardised orthonormal eigenvectors orthogonal.\\(\\boldsymbol \\) never defective \\(\\boldsymbol U\\) forms complete basis.Furthermore, symmetric matrix \\(\\boldsymbol \\) diagonal elements \\(p_1 \\geq \\ldots \\geq p_d\\)\neigenvalues \\(\\lambda_1 \\geq \\ldots \\geq \\lambda_d\\) (note written decreasing order) sum \nlargest \\(k\\) eigenvalues forms upper bound sum largest \\(k\\) diagonal elements:\n\\[\n\\sum_{}^k \\lambda_i \\geq \\sum_{}^k p_i\n\\]\ntheorem due Schur (1923) 1. equality holds \\(k=d\\) (trace \\(\\boldsymbol \\) equals sum\neigenvalues) \\(k\\) \\(\\boldsymbol \\) diagonal (case diagonal elements equal eigenvalues).","code":""},{"path":"matrix-essentials.html","id":"eigenvalues-of-orthogonal-matrices","chapter":"1 Matrix essentials","heading":"1.7.9 Eigenvalues of orthogonal matrices","text":"eigenvalues orthogonal matrix \\(\\boldsymbol Q\\) necessarily real \nmodulus 1 lie unit circle . Thus, eigenvalues \\(\\boldsymbol Q\\)\nform \\(\\lambda = e^{\\phi} = \\cos \\phi + \\sin \\phi\\).real matrix complex eigenvalues come conjugate\npairs. Hence orthogonal matrix \\(\\boldsymbol Q\\) complex eigenvalue \\(e^{\\phi}\\) also \ncomplex eigenvalue \\(e^{-\\phi} =\\cos \\phi - \\sin \\phi\\). product two conjugate\neigenvalues 1. Thus, orthogonal matrix uneven dimension least one\nreal eigenvalue (+1 -1).eigenvalues Hausholder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\) real (recall symmetric!).\nfact, dimension \\(d\\) eigenvalues -1 (one time) 1 ( \\(d-1\\) times).\nSince transposition matrix \\(\\boldsymbol T\\) special Householder matrix eigenvalues.","code":""},{"path":"matrix-essentials.html","id":"positive-definite-matrices","chapter":"1 Matrix essentials","heading":"1.7.10 Positive definite matrices","text":"eigenvalues square matrix \\(\\boldsymbol \\) real \\(\\lambda_i \\geq 0\\) \\(\\boldsymbol \\) called positive semi-definite.\neigenvalues strictly positive\n\\(\\lambda_i > 0\\) \\(\\boldsymbol \\) called positive definite.Note matrix need symmetric positive\ndefinite, e.g.\n\\(\\begin{pmatrix} 2 & 3 \\\\ 1 & 4 \\\\ \\end{pmatrix}\\)\npositive eigenvalues 5 1. also complete\nset eigenvectors diagonisable.symmetric matrix \\(\\boldsymbol \\) positive definite\nquadratic form \\(\\boldsymbol x^T \\boldsymbol \\boldsymbol x> 0\\) non-zero \\(\\boldsymbol x\\),\npositive semi-definite \\(\\boldsymbol x^T \\boldsymbol \\boldsymbol x\\geq 0\\).\nholds also way around:\nsymmetric positive definite matrix (positive eigenvalues) \npositive quadratic form, symmetric positive semi-definite matrix (non-negative eigenvalues) non-negative quadratic form.symmetric positive definite matrix always positive diagonal\n(can seen setting \\(\\boldsymbol x\\) unit vector 1 \nsingle position, 0 elements).\nHowever, just requiring positive diagonal weak ensure positive definiteness symmetric matrix, example \\(\\begin{pmatrix} 1 &10 \\\\ 10 & 1 \\\\ \\end{pmatrix}\\) negative eigenvalue -9.\nhand, symmetric matrix indeed positive definite strictly\ndiagonally dominant, .e. diagonal elements positive larger absolute value corresponding row column elements.\nHowever, diagonal dominance restrictive criterion \ncharacterise \nsymmetric positive definite matrices, since\nmany symmetric matrices positive definite diagonally dominant, \n\\(\\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\\\ \\end{pmatrix}\\).Finally, sum symmetric positive semi-definite matrix \\(\\boldsymbol \\)\nsymmetric positive definite matrix \\(\\boldsymbol B\\) symmetric positive definite corresponding\nquadratic form \\(\\boldsymbol x^T ( \\boldsymbol +\\boldsymbol B) \\boldsymbol x= \\boldsymbol x^T \\boldsymbol \\boldsymbol x+ \\boldsymbol x^T \\boldsymbol B\\boldsymbol x> 0\\) positive. Similarly, sum\ntwo symmetric positive (semi)-definite matrices symmetric positive (semi)-definite.","code":""},{"path":"matrix-essentials.html","id":"matrix-decompositions","chapter":"1 Matrix essentials","heading":"1.8 Matrix decompositions","text":"","code":""},{"path":"matrix-essentials.html","id":"diagonalisation-and-eigenvalue-decomposition","chapter":"1 Matrix essentials","heading":"1.8.1 Diagonalisation and eigenvalue decomposition","text":"\\(\\boldsymbol \\) square non-defective matrix eigensystem \\(\\boldsymbol U\\) invertible \ncan rewrite eigenvalue equation \n\\[\\boldsymbol = \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1} \\,.\\]\ncalled eigendecomposition, spectral decomposition, \\(\\boldsymbol \\) equivalently\n\\[\\boldsymbol \\Lambda= \\boldsymbol U^{-1} \\boldsymbol \\boldsymbol U\\]\ndiagonalisation \\(\\boldsymbol \\).\\(\\boldsymbol \\) defective (.e. \\(\\boldsymbol U\\) singular) one can still approximately diagonalise \\(\\boldsymbol \\) always exists similarity transformation \n\\(\\boldsymbol J= \\boldsymbol M\\boldsymbol \\boldsymbol M^{-1}\\) \\(\\boldsymbol M\\) invertible matrix\n\\(\\boldsymbol J\\) Jordan canonical form, .e. \\(\\boldsymbol J\\) upper triangular \n(potentially complex) eigenvalues diagonal \nnon-zero entries equal 1 immediately main diagonal.","code":""},{"path":"matrix-essentials.html","id":"orthogonal-eigenvalue-decomposition","chapter":"1 Matrix essentials","heading":"1.8.2 Orthogonal eigenvalue decomposition","text":"symmetric \\(\\boldsymbol \\) real eigenvalues orthogonal matrix \\(\\boldsymbol U\\) spectral decomposition\nbecomes\n\\[\\boldsymbol = \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\]\n\n\\[\\boldsymbol \\Lambda= \\boldsymbol U^T \\boldsymbol \\boldsymbol U\\,.\\]\nspecial case known orthogonal diagonalisation\n\\(\\boldsymbol \\).orthogonal decomposition symmetric \\(\\boldsymbol \\) \nunique apart signs\neigenvectors (columns \\(\\boldsymbol U\\)). Thus, computer application\ndepending specific implementation numerical algorithm eigenvalue\ndecomposition\nsigns may vary.","code":""},{"path":"matrix-essentials.html","id":"singular-value-decomposition","chapter":"1 Matrix essentials","heading":"1.8.3 Singular value decomposition","text":"singular value decomposition (SVD) \ngeneralisation orthogonal eigenvalue decomposition\nsymmetric matrices.(!) rectangular matrix \\(\\boldsymbol \\) size \\(n\\times d\\) can factored\nproduct\n\\[\\boldsymbol = \\boldsymbol U\\boldsymbol D\\boldsymbol V^T\\]\n\\(\\boldsymbol U\\) \\(n \\times n\\) orthogonal matrix, \\(\\boldsymbol V\\) second \\(d \\times d\\) orthogonal matrix \\(\\boldsymbol D\\) diagonal rectangular matrix\nsize \\(n\\times d\\) \\(m=min(n,d)\\) real diagonal elements \\(d_1, \\ldots d_m\\). \\(d_i\\) called singular values, appear\nalong diagonal \\(\\boldsymbol D\\) order magnitude.SVD unique apart \nsigns columns vectors \\(\\boldsymbol U\\), \\(\\boldsymbol V\\) \\(\\boldsymbol D\\) (can freely specify column signs two \nthree matrices). convention \nsigns chosen singular values \\(\\boldsymbol D\\) non-negative, leaves ambiguity\ncolumns signs \\(\\boldsymbol U\\) \\(\\boldsymbol V\\). Alternatively, one may\nfix columns signs \\(\\boldsymbol U\\) \\(\\boldsymbol V\\), e.g. requiring positive diagonal, determines sign singular values (thus allowing negative singular values well).\\(\\boldsymbol \\) symmetric SVD orthogonal eigenvalue decomposition coincide (apart different sign conventions singular values, eigenvalues eigenvectors).Since \\(\\boldsymbol ^T \\boldsymbol = \\boldsymbol V\\boldsymbol D^T \\boldsymbol D\\boldsymbol V^T\\) \\(\\boldsymbol \\boldsymbol ^T = \\boldsymbol U\\boldsymbol D\\boldsymbol D^T \\boldsymbol U^T\\) squared singular values correspond eigenvalues \\(\\boldsymbol ^T \\boldsymbol \\) \\(\\boldsymbol \\boldsymbol ^T\\).\nalso follows \\(\\boldsymbol ^T \\boldsymbol \\) \\(\\boldsymbol \\boldsymbol ^T\\) positive\nsemi-definite symmetric matrices, \\(\\boldsymbol V\\) \\(\\boldsymbol U\\) contain respective sets eigenvectors.","code":""},{"path":"matrix-essentials.html","id":"polar-decomposition","chapter":"1 Matrix essentials","heading":"1.8.4 Polar decomposition","text":"square matrix \\(\\boldsymbol \\) can factored product\n\\[\n\\boldsymbol = \\boldsymbol Q\\boldsymbol B\n\\]\northogonal matrix \\(\\boldsymbol Q\\) symmetric positive semi-definite matrix \\(\\boldsymbol B\\).follows SVD \\(\\boldsymbol \\) given \n\\[\n\\begin{split}\n\\boldsymbol &= \\boldsymbol U\\boldsymbol D\\boldsymbol V^T \\\\\n    &= ( \\boldsymbol U\\boldsymbol V^T ) ( \\boldsymbol V\\boldsymbol D\\boldsymbol V^T ) \\\\\n    &= \\boldsymbol Q\\boldsymbol B\\\\\n\\end{split}\n\\]\nnon-negative \\(\\boldsymbol D\\). Note decomposition unique sign ambiguities columns \\(\\boldsymbol U\\) \\(\\boldsymbol V\\) cancel \\(\\boldsymbol Q\\) \\(\\boldsymbol B\\).","code":""},{"path":"matrix-essentials.html","id":"cholesky-decomposition","chapter":"1 Matrix essentials","heading":"1.8.5 Cholesky decomposition","text":"symmetric positive definite matrix \\(\\boldsymbol \\) can decomposed product\ntriangular matrix \\(\\boldsymbol L\\) transpose\n\\[\n\\boldsymbol = \\boldsymbol L\\boldsymbol L^T \\,.\n\\]\n, \\(\\boldsymbol L\\) lower triangular matrix positive diagonal elements.decomposition unique called Cholesky factorisation. \noften used check whether symmetric matrix positive definite algorithmically\nless demanding eigenvalue decomposition.Note implementations Cholesky decomposition (e.g. R) use\nupper triangular matrices \\(\\boldsymbol K\\) positive diagonal \n\\(\\boldsymbol = \\boldsymbol K^T \\boldsymbol K\\) \\(\\boldsymbol L= \\boldsymbol K^T\\).","code":""},{"path":"matrix-essentials.html","id":"matrix-summaries-based-on-eigenvalues-and-singular-values","chapter":"1 Matrix essentials","heading":"1.9 Matrix summaries based on eigenvalues and singular values","text":"","code":""},{"path":"matrix-essentials.html","id":"trace-and-determinant-computed-from-eigenvalues","chapter":"1 Matrix essentials","heading":"1.9.1 Trace and determinant computed from eigenvalues","text":"eigendecomposition \\(\\boldsymbol =\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1}\\)\nallows establish link trace determinant eigenvalues\nmatrix.Specifically,\n\\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol ) & = \\text{Tr}(\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1}  ) =\n\\text{Tr}( \\boldsymbol \\Lambda\\boldsymbol U^{-1} \\boldsymbol U) \\\\\n&= \\text{Tr}( \\boldsymbol \\Lambda) = \\sum_{=1}^d \\lambda_i \\\\\n\\end{split}\n\\]\nthus trace square matrix \\(\\boldsymbol \\) equal sum eigenvalues. Likewise,\n\\[\n\\begin{split}\n\\det(\\boldsymbol ) & = \\det(\\boldsymbol U) \\det(\\boldsymbol \\Lambda) \\det(\\boldsymbol U^{-1}  ) \\\\\n&=\\det( \\boldsymbol \\Lambda) = \\prod_{=1}^d \\lambda_i \\\\\n\\end{split}\n\\]\ntherefore determinant \\(\\boldsymbol \\) product eigenvalues.relationship eigenvalues square matrix trace determinant\nmatrix shown diagonisable matrices.\nHowever, holds generally square matrix, .e. also defective matrices.\nlatter Jordan canonical form \\(\\boldsymbol J\\) replaces \\(\\boldsymbol \\Lambda\\) (cases eigenvalues simply entries diagonal).eigenvalues equal zero \\(\\det(\\boldsymbol ) = 0\\) hence \\(\\boldsymbol \\) singular invertible.trace determinant real matrix always real even though individual eigenvalues may complex.","code":""},{"path":"matrix-essentials.html","id":"eigenvalues-of-a-squared-matrix","chapter":"1 Matrix essentials","heading":"1.9.2 Eigenvalues of a squared matrix","text":"eigendecomposition \\(\\boldsymbol =\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1}\\)\neasy see eigenvalues \\(\\boldsymbol ^2\\) simply \nsquared eigenvalues \\(\\boldsymbol \\) \n\\[\n\\boldsymbol ^2 = \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1} \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1} = \\boldsymbol U\\boldsymbol \\Lambda^2 \\boldsymbol U^{-1}\n\\]\nresult can compute trace \\(\\boldsymbol ^2\\) sum squared\neigenvalues \\(\\boldsymbol \\), .e. \\(\\text{Tr}(\\boldsymbol ^2) = \\sum_{=1}^d \\lambda_i^2\\),\ndeterminant product squared eigenvalues, .e\n\\(\\det(\\boldsymbol ^2) = \\prod_{=1}^d \\lambda_i^2\\).\\(\\boldsymbol \\) symmetric \\(\\text{Tr}(\\boldsymbol ^2) = \\text{Tr}(\\boldsymbol \\boldsymbol ^T) = || ||^2_F = \\sum_{=1}^d \\sum_{j=1}^d a_{ij}^2\\).\nleads identity\n\\[\n\\sum_{=1}^d \\lambda_i^2 =  \\sum_{=1}^d \\sum_{j=1}^d a_{ij}^2\n\\]\nsum squared eigenvalues sum squared entries symmetric matrix \\(\\boldsymbol \\).","code":""},{"path":"matrix-essentials.html","id":"rank-and-condition-number","chapter":"1 Matrix essentials","heading":"1.9.3 Rank and condition number","text":"rank dimension space spanned column row vectors. rectangular matrix dimension \\(n \\times d\\) \nrank \\(m = \\min(n, d)\\), maximum indeed achieved full rank.condition number describes well- ill-conditioned\nfull rank matrix . example, square matrix large condition number implies matrix close singular\nthus ill-conditioned.\ncondition number infinite matrix full rank.rank condition matrix can determined \\(m\\) singular values \\(d_1, \\ldots, d_m\\) matrix obtained SVD:rank number non-zero singular values.condition number ratio largest singular value\ndivided smallest singular value (absolute values signs allowed).square matrix \\(\\boldsymbol \\) singular condition number infinite, full rank.\nhand, non-singular square matrix, \npositive definite matrix, full rank.","code":""},{"path":"matrix-essentials.html","id":"functions-of-symmetric-matrices","chapter":"1 Matrix essentials","heading":"1.10 Functions of symmetric matrices","text":"focus symmetric square matrices \\(\\boldsymbol =\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) always diagonisable real eigenvalues \\(\\boldsymbol \\Lambda\\) orthogonal eigenvectors \\(\\boldsymbol U\\).","code":""},{"path":"matrix-essentials.html","id":"definition-of-a-matrix-function","chapter":"1 Matrix essentials","heading":"1.10.1 Definition of a matrix function","text":"Assume real-valued function \\(f()\\) real number \\(\\). corresponding\nmatrix function \\(f(\\boldsymbol )\\)\ndefined \n\\[\nf(\\boldsymbol ) =  \\boldsymbol Uf(\\boldsymbol \\Lambda) \\boldsymbol U^T =  \\boldsymbol U\\begin{pmatrix}\n    f(\\lambda_{1}) & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & f(\\lambda_{d})\n\\end{pmatrix} \\boldsymbol U^T\n\\]\nfunction \\(f()\\) applied eigenvalues \\(\\boldsymbol \\).\nconstruction \\(f(\\boldsymbol )\\) real, symmetric \nreal eigenvalues \\(f(\\lambda_i)\\).Examples:Example 1.2  Matrix power: \\(f() = ^p\\) (\\(p\\) real number)Special cases matrix power include :Matrix inversion: \\(f() = ^{-1}\\)\nNote matrix \\(\\boldsymbol \\) singular, .e. contains one eigenvalues \\(\\lambda_i=0\\),\n\\(\\boldsymbol ^{-1}\\) defined therefore \\(\\boldsymbol \\) invertible.However, -called pseudoinverse can still computed, inverting non-zero eigenvalues, \nkeeping zero eigenvalues zero.Matrix square root: \\(f() = ^{1/2}\\)\nSince multiple solutions square root also multiple\nmatrix square roots. principal matrix square root obtained using\npositive square roots eigenvalues. Thus principal matrix square root\npositive semi-definite matrix also positive semi-definite unique.Example 1.3  Matrix exponential: \\(f() = \\exp()\\)\nNote \\(\\exp() \\geq 0\\) real \\(\\) matrix \\(\\exp(\\boldsymbol )\\) positive\nsemi-definite. Thus, matrix exponential can used generate positive semi-definite\nmatrices.\\(\\boldsymbol \\) \\(\\boldsymbol B\\) commute, .e. \\(\\boldsymbol \\boldsymbol B= \\boldsymbol B\\boldsymbol \\), \n\\(\\exp(\\boldsymbol +\\boldsymbol B) = \\exp(\\boldsymbol ) \\exp(\\boldsymbol B)\\). However, case\notherwise!Example 1.4  Matrix logarithm: \\(f() = \\log()\\)\nlogarithm requires \\(>0\\) matrix \\(\\boldsymbol \\) needs positive definite\n\\(\\log(\\boldsymbol )\\) defined.","code":""},{"path":"matrix-essentials.html","id":"identities-for-the-matrix-exponential-and-logarithm","chapter":"1 Matrix essentials","heading":"1.10.2 Identities for the matrix exponential and logarithm","text":"give rise useful identities:symmetric matrix \\(\\boldsymbol \\) \n\\[\n\\det(\\exp(\\boldsymbol )) = \\exp(\\text{Tr}(\\boldsymbol ))\n\\]\n\n\\(\\prod_i \\exp(\\lambda_i) = \\exp( \\sum_i \\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol \\).symmetric matrix \\(\\boldsymbol \\) \n\\[\n\\det(\\exp(\\boldsymbol )) = \\exp(\\text{Tr}(\\boldsymbol ))\n\\]\n\n\\(\\prod_i \\exp(\\lambda_i) = \\exp( \\sum_i \\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol \\).take logarithm sides replace \\(\\exp(\\boldsymbol )=\\boldsymbol B\\) get another\nidentity symmetric positive definite matrix \\(\\boldsymbol B\\):\n\\[\n\\log \\det(\\boldsymbol B) = \\text{Tr}(\\log(\\boldsymbol B))\n\\]\n\n\\(\\log( \\prod_i \\lambda_i) = \\sum_i \\log(\\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol B\\).take logarithm sides replace \\(\\exp(\\boldsymbol )=\\boldsymbol B\\) get another\nidentity symmetric positive definite matrix \\(\\boldsymbol B\\):\n\\[\n\\log \\det(\\boldsymbol B) = \\text{Tr}(\\log(\\boldsymbol B))\n\\]\n\n\\(\\log( \\prod_i \\lambda_i) = \\sum_i \\log(\\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol B\\).","code":""},{"path":"vector-and-matrix-calculus.html","id":"vector-and-matrix-calculus","chapter":"2 Vector and matrix calculus","heading":"2 Vector and matrix calculus","text":"","code":""},{"path":"vector-and-matrix-calculus.html","id":"first-order-vector-derivatives","chapter":"2 Vector and matrix calculus","heading":"2.1 First order vector derivatives","text":"","code":""},{"path":"vector-and-matrix-calculus.html","id":"gradient","chapter":"2 Vector and matrix calculus","heading":"2.1.1 Gradient","text":"gradient scalar-valued function\n\\(h(\\boldsymbol x)\\) vector argument \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\)\nvector containing first order partial derivatives\n\\(h(\\boldsymbol x)\\) regard \\(x_1, \\ldots, x_d\\):\n\\[\n\\begin{split}\n\\nabla h(\\boldsymbol x) &= \\begin{pmatrix}\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_1} \\\\\n\\vdots\\\\\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_d}\n\\end{pmatrix}\\\\\n&=  \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} \\\\\n& = \\text{grad } h(\\boldsymbol x) \\\\\n\\end{split}\n\\]\nsymbol \\(\\nabla\\) called nabla operator (also known del operator).Note write gradient column vector. called \ndenominator layout convention, see https://en.wikipedia.org/wiki/Matrix_calculus details.\ncontrast, many textbooks (also earlier versions lecture notes) assume gradients row vectors, following -called numerator layout convention.Example 2.1  Examples gradient:\\(h(\\boldsymbol x)=\\boldsymbol ^T \\boldsymbol x+ b\\). \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol \\).\\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol x\\). \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = 2 \\boldsymbol x\\).\\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol \\boldsymbol x\\). \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = (\\boldsymbol + \\boldsymbol ^T) \\boldsymbol x\\).","code":""},{"path":"vector-and-matrix-calculus.html","id":"jacobian-matrix","chapter":"2 Vector and matrix calculus","heading":"2.1.2 Jacobian matrix","text":"vector-valued function\n\\[\n\\boldsymbol h(\\boldsymbol x) = ( h_1(\\boldsymbol x), \\ldots, h_m(\\boldsymbol x) )^T\n\\]\ncan also compute vector derivative\n\\[\n\\begin{split}\n\\frac{\\partial \\boldsymbol h(\\boldsymbol x)}{\\partial \\boldsymbol x}  &=\n\\begin{pmatrix}\n\\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_1} & \\cdots  & \\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_1} \\\\\n\\vdots &\\ddots & \\vdots \\\\\n\\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_d} & \\cdots & \\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_d} \\\\\n\\end{pmatrix} \\\\\n&=\\left(\\frac{\\partial h_j(\\boldsymbol x)}{\\partial x_i}\\right) \\\\\n& = \\left(\\nabla h_1(\\boldsymbol x), \\ldots, \\nabla h_m(\\boldsymbol x)  \\right)\n\\end{split}\n\\]\nyields matrix whose columns contain \ngradient vectors component \\(\\boldsymbol h(\\boldsymbol x)\\).transpose matrix called Jacobian matrix (size \\(m\\) rows \\(d\\) columns):\n\\[\n\\begin{split}\nJ_{\\boldsymbol h}(\\boldsymbol x) &=\n\\left( {\\begin{array}{c}\n\\nabla h_1(\\boldsymbol x)^T   \\\\\n\\vdots   \\\\\n\\nabla h_m(\\boldsymbol x)^T  \\\\\n\\end{array} } \\right) \\\\\n& = \\left(\\frac{\\partial h_i(\\boldsymbol x)}{\\partial x_j}\\right) \\\\\n& = \\left( \\frac{\\partial \\boldsymbol h(\\boldsymbol x)}{\\partial \\boldsymbol x}   \\right)^T\n\\end{split}\n\\]\nNote convention Jacobian matrix contains gradients rows.Example 2.2  \\(\\boldsymbol h(\\boldsymbol x)=\\boldsymbol ^T \\boldsymbol x+ \\boldsymbol b\\). \\(\\frac{\\partial \\boldsymbol h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol \\)\n\\(J_{\\boldsymbol h}(\\boldsymbol x) =\\boldsymbol ^T\\).\\(m=d\\) Jacobian matrix square matrix allows compute \nJacobian determinant \\(\\det J_{\\boldsymbol h}(\\boldsymbol x)\\).Jacobian matrix Jacobian determinant often called simply “Jacobian”.\\(\\boldsymbol y= \\boldsymbol h(\\boldsymbol x)\\) invertible function \\(\\boldsymbol x= \\boldsymbol h^{-1}(\\boldsymbol y)\\)\nJacobian matrix invertible inverted matrix fact \nJacobian inverse function!allows compute Jacobian determinant backtransformation \ninverse Jacobian determinant original function:\n\\[\\det  J_{\\boldsymbol h^{-1}}(\\boldsymbol y) = ( \\det  J_{\\boldsymbol h}(\\boldsymbol x) )^{-1}\\]\nalternative notation\n\\[\\det  J_{\\boldsymbol x}(\\boldsymbol y) = \\frac{1}{ \\det J_{\\boldsymbol y}(\\boldsymbol x) }\\]","code":""},{"path":"vector-and-matrix-calculus.html","id":"second-order-vector-derivatives","chapter":"2 Vector and matrix calculus","heading":"2.2 Second order vector derivatives","text":"matrix second order partial derivates scalar-valued\nfunction vector-valued argument called Hessian matrix:\n\\[\n\\begin{split}\n\\nabla \\nabla^T h(\\boldsymbol x) &=\n\\begin{pmatrix}\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1^2}\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1 \\partial x_2}\n     & \\cdots\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1 \\partial x_d} \\\\\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2 \\partial x_1}\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2^2}\n     & \\cdots\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2 \\partial x_d} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d \\partial x_1}\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d \\partial x_2}  \n     & \\cdots\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d^2}\n\\end{pmatrix} \\\\\n&= \\left(\\frac{\\partial h(\\boldsymbol x)}{\\partial x_i \\partial x_j}\\right) \\\\\n& = \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial \\boldsymbol x\\partial \\boldsymbol x^T} \\\\\n\\end{split}\n\\]\nconstruction Hessian matrix square symmetric.Example 2.3  \\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol \\boldsymbol x\\). \\(\\nabla \\nabla^T h(\\boldsymbol x) = \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial \\boldsymbol x\\partial \\boldsymbol x^T} = (\\boldsymbol + \\boldsymbol ^T)\\).","code":""},{"path":"vector-and-matrix-calculus.html","id":"first-order-matrix-derivatives","chapter":"2 Vector and matrix calculus","heading":"2.3 First order matrix derivatives","text":"derivative scalar-valued function \\(f(\\boldsymbol X)\\) regard matrix argument \\(\\boldsymbol X\\)\ncan also defined results matrix. find matrix calculus rules (written \ndenominator layout convention). See\nhttps://en.wikipedia.org/wiki/Matrix_calculus examples.Example 2.4  \\(\\frac{\\partial \\text{Tr}(\\boldsymbol ^T \\boldsymbol X)}{\\partial \\boldsymbol X} = \\boldsymbol \\)Example 2.5  \\(\\frac{\\partial \\text{Tr}(\\boldsymbol ^T \\boldsymbol X\\boldsymbol B)}{\\partial \\boldsymbol X} = \\boldsymbol \\boldsymbol B^T\\)Example 2.6  \\(\\frac{\\partial \\text{Tr}(\\boldsymbol X^T \\boldsymbol \\boldsymbol X)}{\\partial \\boldsymbol X} = (\\boldsymbol + \\boldsymbol ^T) \\boldsymbol X\\)Example 2.7  \\(\\frac{\\partial \\log \\det(\\boldsymbol X)}{\\partial \\boldsymbol X} = \\frac{\\partial \\text{Tr}(\\log \\boldsymbol X)}{\\partial \\boldsymbol X} = (\\boldsymbol X^{-1})^T\\)","code":""},{"path":"vector-and-matrix-calculus.html","id":"conditions-for-a-local-extremum-of-a-function","chapter":"2 Vector and matrix calculus","heading":"2.4 Conditions for a local extremum of a function","text":"check \\(x_0\\) \\(\\boldsymbol x_0\\) local extremum, .e. local maximum local minimum,\ndifferentiable function \\(h(x)\\) \\(h(\\boldsymbol x)\\) can use following conditions:function single variable:First derivative zero extremum: \\(h'(x_0) = 0\\).second derivative \\(h''(x_0) < 0\\) extremum negative maximum.second derivative \\(h''(x_0) > 0\\) extremum positive minimum.Note conditions ii) iii) sufficient necessary. minimum, necessary second derivative non-negative, maximum second derivative non-positive.function several variables:Gradient vanishes extremum: \\(\\nabla h(\\boldsymbol x_0)=0\\).Hessian \\(\\nabla \\nabla^T h(\\boldsymbol x_0)\\) negative definite (= eigenvalues Hessian matrix negative) extremum maximum.Hessian positive definite (= eigenvalues Hessian matrix positive) extremum minimum., conditions ii) iii) sufficient necessary. minimum\nnecessary Hessian positive semi-definite, maximum\nHessian negative semi-definite.Example 2.8  Minimum vanishing second derivative:\\(x^4\\) clearly minimum \\(x_0=0\\). required first derivative \\(4 x^3\\) vanishes\n\\(x_0=0\\). However, second derivative \\(12 x^2\\) also vanishes \\(x_0=0\\), showing\npositive second derivative necessary minimum.","code":""},{"path":"vector-and-matrix-calculus.html","id":"convex-and-concave-functions","chapter":"2 Vector and matrix calculus","heading":"2.5 Convex and concave functions","text":"function \\(h(\\boldsymbol x)\\) convex \\(\\boldsymbol x_1\\) \\(\\boldsymbol x_2\\) \nline segment point \\((\\boldsymbol x_1, h(\\boldsymbol x_1))\\)\npoint \\((\\boldsymbol x_2, h(\\boldsymbol x_2))\\) never lies function. Moreover, function strictly convex line segment always lies curve, apart two end points:\\[\n\\lambda h(\\boldsymbol x_1) + (1-\\lambda) h(\\boldsymbol x_2) \\geq h(\\lambda \\boldsymbol x_1 + (1-\\lambda) \\boldsymbol x_2)\n\\]\n\\(\\lambda \\[0, 1]\\).Equivalently, differentiable function \\(h(\\boldsymbol x)\\) convex (strictly convex) \\(\\boldsymbol x_0\\) function \\(h(\\boldsymbol x)\\) never lies (always lies , except \\(\\boldsymbol x_0\\)) linear approximation point \\((\\boldsymbol x_0, h(\\boldsymbol x_0))\\):\n\\[\nh(\\boldsymbol x) \\geq h(\\boldsymbol x_0) + \\nabla h(\\boldsymbol x_0)^T (\\boldsymbol x-\\boldsymbol x_0)\n\\]convex function vanishing gradient \\(\\boldsymbol x_0\\) indicates minimum \\(\\boldsymbol x_0\\).\nFurthermore, local minimum must also global minimum (differentiable function follows directly last inequality).\nstrictly convex function minimum unique one local/global minimum case.\\(h(\\boldsymbol x)\\) convex, \\(-h(\\boldsymbol x)\\) concave, criteria can \nadapted accordingly check concavity strict concavity, well identify\nlocal/global maxima.(Strictly) convex concave functions convenient objective functions optimisation straightforward find local/global extrema, analytically numerically.shape convex function resembles valley, one way memorise fact valley convex.Example 2.9  Convex functions:convex function strictly convex function:\\(\\max(x^2, | x | )\\)following strictly convex functions:\\(x^2\\),\\(x^4\\),\\(e^x\\),\\(x \\log(x)\\) \\(x>0\\).hand, convex function:\\(\\frac{1}{x^2}\\) \\(x \\neq 0\\).However, function last example strictly convex domain restricted\neither \\(x >0\\) \\(x<0\\).Example 2.10  Concave functions:following strictly concave function:\\(-x^2\\),\\(\\log(x)\\) \\(x>0\\),\\(\\sqrt{x}\\) \\(x>0\\).","code":""},{"path":"vector-and-matrix-calculus.html","id":"linear-and-quadratic-approximation","chapter":"2 Vector and matrix calculus","heading":"2.6 Linear and quadratic approximation","text":"linear quadratic approximation differentiable function given Taylor series first second order, respectively.Applied scalar-valued function scalar:\n\\[\nh(x) \\approx h(x_0) + h'(x_0) (x-x_0) + \\frac{1}{2} h''(x_0) (x-x_0)^2\n\\]\nNote \\(h'(x_0) = h'(x) \\,|\\, x_0\\) first derivative \\(h(x)\\) evaluated \\(x_0\\) \n\\(h''(x_0) = h''(x) \\,|\\, x_0\\) second derivative \\(h(x)\\) evaluated \\(x_0\\).\\(x = x_0+ \\varepsilon\\) approximation can also written \n\\[\nh(x_0+ \\varepsilon) \\approx h(x_0) + h'(x_0) \\, \\varepsilon + \\frac{1}{2} h''(x_0)\\, \\varepsilon^2\n\\]Applied scalar-valued function vector:\n\\[\nh(\\boldsymbol x) \\approx h(\\boldsymbol x_0) + \\nabla h(\\boldsymbol x_0)^T (\\boldsymbol x-\\boldsymbol x_0) + \\frac{1}{2}\n(\\boldsymbol x-\\boldsymbol x_0)^T \\, \\nabla \\nabla^T h(\\boldsymbol x_0) \\, (\\boldsymbol x-\\boldsymbol x_0)\n\\]\nNote \\(\\nabla h(\\boldsymbol x_0)\\) gradient \\(h(\\boldsymbol x)\\) evaluated \\(\\boldsymbol x_0\\)\n\\(\\nabla \\nabla^T h(\\boldsymbol x_0)\\) Hessian matrix \\(h(\\boldsymbol x)\\) evaluated \\(\\boldsymbol x_0\\).\\(\\boldsymbol x= \\boldsymbol x_0+ \\boldsymbol \\varepsilon\\) approximation can also written \n\\[\nh(\\boldsymbol x_0+ \\boldsymbol \\varepsilon) \\approx h(\\boldsymbol x_0) + \\nabla h(\\boldsymbol x_0)^T\\boldsymbol \\varepsilon+ \\frac{1}{2} \\boldsymbol \\varepsilon^T \\, \\nabla \\nabla^T h(\\boldsymbol x_0) \\,\\boldsymbol \\varepsilon\n\\]Example 2.11  Commonly occurring Taylor series approximations second order example\n\\[\n\\log(x_0+\\varepsilon) \\approx \\log(x_0) + \\frac{\\varepsilon}{x_0} - \\frac{\\varepsilon^2}{2 x_0^2}\n\\]\n\n\\[\n\\frac{x_0}{x_0+\\varepsilon} \\approx 1 - \\frac{\\varepsilon}{x_0} + \\frac{\\varepsilon^2}{ x_0^2}\n\\]Example 2.12  Around local extremum \\(\\boldsymbol x_0\\) (maximum minimum) can approximate function using\n\\[\nh(\\boldsymbol x_0+ \\boldsymbol \\varepsilon) \\approx h(\\boldsymbol x_0) +  \\frac{1}{2} \\boldsymbol \\varepsilon^T \\nabla \\nabla^T h(\\boldsymbol x_0) \\boldsymbol \\varepsilon\n\\]\nNote quadratic approximation linear term missing due gradient zero \\(\\boldsymbol x_0\\).","code":""}]
