<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>A Statistics refresher | Statistics 2: Likelihood and Bayes</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="A Statistics refresher | Statistics 2: Likelihood and Bayes">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Statistics refresher | Statistics 2: Likelihood and Bayes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<meta name="description" content="Below you find a brief overview over some relevant concepts in statistics that you should be familiar with from earlier modules.  A.1 Statistical learning The aim in statistics — data science —...">
<meta property="og:description" content="Below you find a brief overview over some relevant concepts in statistics that you should be familiar with from earlier modules.  A.1 Statistical learning The aim in statistics — data science —...">
<meta name="twitter:description" content="Below you find a brief overview over some relevant concepts in statistics that you should be familiar with from earlier modules.  A.1 Statistical learning The aim in statistics — data science —...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistics 2: Likelihood and Bayes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="prerequisites.html">Prerequisites</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="entropy-and-fisher-information.html"><span class="header-section-number">2</span> Entropy and Fisher information</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="conditioning-and-bayes-rule.html"><span class="header-section-number">7</span> Conditioning and Bayes rule</a></li>
<li><a class="" href="models-with-latent-variables-and-missing-data.html"><span class="header-section-number">8</span> Models with latent variables and missing data</a></li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li><a class="" href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li><a class="" href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li class="book-part">Appendix</li>
<li><a class="active" href="statistics-refresher.html"><span class="header-section-number">A</span> Statistics refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="statistics-refresher" class="section level1" number="14">
<h1>
<span class="header-section-number">A</span> Statistics refresher<a class="anchor" aria-label="anchor" href="#statistics-refresher"><i class="fas fa-link"></i></a>
</h1>
<p>Below you find a brief overview over some relevant concepts in statistics
that you should be familiar with from earlier modules.</p>
<div id="statistical-learning" class="section level2" number="14.1">
<h2>
<span class="header-section-number">A.1</span> Statistical learning<a class="anchor" aria-label="anchor" href="#statistical-learning"><i class="fas fa-link"></i></a>
</h2>
<p>The aim in statistics — data science — machine learning is to use data
(from experiments, observations, measurements) to learn about and understand the world
using models. In statistics we employ probabilistic models.</p>
<p>Let denote data by <span class="math inline">\(D =\{x_1, \ldots, x_n\}\)</span> and models by <span class="math inline">\(p(x| \theta)\)</span> where <span class="math inline">\(\theta\)</span> represents
the parameters of the model. Often (but not always) <span class="math inline">\(\theta\)</span> can be interpreted as and/or is
associated with some manifest property of the model.
If there is only a single parameter we write <span class="math inline">\(\theta\)</span> (scalar parameter).
If we wish to highlight that there are multiple parameters we write <span class="math inline">\(\boldsymbol \theta\)</span> (in bold type).</p>
<p>Specifically, our aim is to identify the best model(s) for the data in order to both</p>
<ul>
<li>explain the current data, and</li>
<li>to enable good prediction of future data.</li>
</ul>
<p>Note that it is generally easy to find one or several models that explain the data but these
then often do not predict well.</p>
<p>Therefore, one would like to avoid <strong>overfitting</strong> the data and identify models that are
appropriate for the data at hand (i.e. not too simple but also not too complex).</p>
<p>Typically, we focus the analysis to a specific model family with a some parameter <span class="math inline">\(\theta\)</span>.<br>
An <strong>estimator for <span class="math inline">\(\theta\)</span></strong> is a function <span class="math inline">\(\hat{\theta}(D)\)</span> of the data
that maps the data (input) to an informed guess (output) about <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li>A <strong>point estimator</strong> provides a single number for each parameter</li>
<li>An <strong>interval estimator</strong> provides a set of possible values for each parameter.</li>
</ul>
<p>Interval estimators can be linked to the concept of testing specified values
for a parameter. Specifically a confidence interval contains all parameter values that are not
significantly different from the best parameter.</p>
</div>
<div id="sampling-properties-of-a-point-estimator-hatboldsymbol-theta" class="section level2" number="14.2">
<h2>
<span class="header-section-number">A.2</span> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span><a class="anchor" aria-label="anchor" href="#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fas fa-link"></i></a>
</h2>
<p>A point estimator <span class="math inline">\(\hat\theta\)</span> depends on the data, hence it exibits <strong>sampling variation</strong>, i.e. estimate will be different for a new set of observations.</p>
<p>Thus <span class="math inline">\(\hat\theta\)</span> can be seen as a random variable, and its distribution is called <strong>sampling distribution</strong> (across different experiments).</p>
<p>Properties of this distribution can be used to evaluate how far the estimator
deviates (on average across different experiments) from the true value:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{rr}
\text{Bias:}\\
\text{Variance:}\\
\text{Mean squared error:}\\
\\
\end{array}
\begin{array}{rr}
\text{Bias}(\hat{\theta})\\
\text{Var}(\hat{\theta})\\
\text{MSE}(\hat{\theta})\\
\\
\end{array}
\begin{array}{ll}
=\text{E}(\hat{\theta})-\theta\\
=\text{E}\left((\hat{\theta}-\text{E}(\hat{\theta}))^2\right)\\
=\text{E}((\hat{\theta}-\theta)^2)\\
=\text{Var}(\hat{\theta})+\text{Bias}(\hat{\theta})^2\\
\end{array}
\end{align*}\]</span></p>
<p>The last identity about MSE follows from <span class="math inline">\(\text{E}(x^2)=\text{Var}(x)+\text{E}(x)^2\)</span>.</p>
<p>At first sight it seems desirable to focus on unbiased (for finite <span class="math inline">\(n\)</span>) estimators.
However, requiring strict unbiasedness is not always a good idea.
In many situations it is better to allow for some small bias and in order to achieve a smaller variance
and an overall total smaller MSE. This is called <strong>bias-variance tradeoff</strong> — as more bias
is traded for smaller variance (or, conversely, less bias is traded for higher variance)</p>
</div>
<div id="efficiency-and-consistency-of-an-estimator" class="section level2" number="14.3">
<h2>
<span class="header-section-number">A.3</span> Efficiency and consistency of an estimator<a class="anchor" aria-label="anchor" href="#efficiency-and-consistency-of-an-estimator"><i class="fas fa-link"></i></a>
</h2>
<p>Typically, <span class="math inline">\(\text{Bias}\)</span>, <span class="math inline">\(\text{Var}\)</span> and <span class="math inline">\(\text{MSE}\)</span> all decrease with increasing sample size
so that with more data <span class="math inline">\(n \to \infty\)</span> the errors become smaller and smaller.</p>
<p><strong>Efficiency</strong>: An estimator <span class="math inline">\(\hat\theta_A\)</span> is said to more efficient than estimator
<span class="math inline">\(\hat\theta_B\)</span> if for same sample size <span class="math inline">\(n\)</span> it has smaller error (e.g. MSE) than
the competing estimator.</p>
<p>The typical rate of decrease in variance of a good estimator is <span class="math inline">\(\frac{1}{n}\)</span>
and the rate of decrease in the standard deviation is <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span>.
Note that this implies that to get one digit more accuracy in an estimate (standard deviation
decreasing by factor of 10) we need 100 times more data!</p>
<p><strong>Consistency</strong>: <span class="math inline">\(\hat{\theta}\)</span> is called consistent if
<span class="math display">\[
\text{MSE}(\hat{\theta}) \longrightarrow 0 \text{ with $n\rightarrow \infty$ }
\]</span></p>
<p>Consistency is an essential but rather weak requirement for any reasonable estimator.
Of all consistent
estimators we typically select the estimators that are most <strong>efficient</strong> (i.e. with fasted decrease in MSE)
and that therefore have smallest variance and/or MSE for given finite <span class="math inline">\(n\)</span>.</p>
<p>Consistency implies we recover the true model in the limit of infinite data if the
model class contains the true data generating model.
If the model class does not contain the true model then strict consistency
cannot be achieved but we still wish to get as close as possible
to the true model when choosing model parameters.</p>
</div>
<div id="empirical-distribution-function" class="section level2" number="14.4">
<h2>
<span class="header-section-number">A.4</span> Empirical distribution function<a class="anchor" aria-label="anchor" href="#empirical-distribution-function"><i class="fas fa-link"></i></a>
</h2>
<p>Suppose we observe data <span class="math inline">\(D=\{x_1, \ldots, x_n\}\)</span> with each <span class="math inline">\(x_i \sim F\)</span>
sampled independently and identically. The empirical cumulative distribution function
<span class="math inline">\(\hat{F}_n(x)\)</span> based on data <span class="math inline">\(D\)</span> is then given by
<span class="math display">\[
\hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n 1_{x_i \leq x}
\]</span></p>
<p>The empirical distribution function is monotonically non-decreasing from 0 to 1 in discrete steps.</p>
<p>In R the empirical distribution function is computed by <code><a href="https://rdrr.io/r/stats/ecdf.html">ecdf()</a></code>.</p>
<p>Crucially, the empirical distribution
<span class="math inline">\(\hat{F}_n\)</span> converges strongly (almost surely) to the
underlying distribution <span class="math inline">\(F\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>:
<span class="math display">\[
\hat{F}_n\overset{a. s.}{\to} F
\]</span>
The <a href="https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem">Glivenko–Cantelli theorem</a> additionally asserts that the convergence is uniform.</p>
<p>Note this is effectively a variant of the <strong>law of large numbers</strong> applied
to the whole distribution, rather than just the mean (see below).</p>
<p>As a result, we may use the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> based on data <span class="math inline">\(D\)</span> as
an estimate of the underlying unknown true distribution <span class="math inline">\(F\)</span>. From the convergence
theorems we know that <span class="math inline">\(\hat{F}_n\)</span> is consistent.</p>
<p>However, for <span class="math inline">\(\hat{F}_n\)</span> to work well as an estimate of <span class="math inline">\(F\)</span> the number of observations <span class="math inline">\(n\)</span> must be
sufficiently large
so that the approximation provided by <span class="math inline">\(\hat{F}_n\)</span> is adequate.</p>
</div>
<div id="empirical-estimators" class="section level2" number="14.5">
<h2>
<span class="header-section-number">A.5</span> Empirical estimators<a class="anchor" aria-label="anchor" href="#empirical-estimators"><i class="fas fa-link"></i></a>
</h2>
<p>The fact that for large sample size <span class="math inline">\(n\)</span> the empirical distribution <span class="math inline">\(\hat{F}_n\)</span>
may be used as a substitute for the unknown <span class="math inline">\(F\)</span> allows us to easily construct
empirical estimators.</p>
<p>Specifically, parameters of a model can typically be expressed as a
functional of the distribution
<span class="math inline">\(\theta = g(F)\)</span>. An <strong>empirical estimator</strong> <span class="math inline">\(\hat{\theta}\)</span> is constructed by substituting the true distribution by the empirical distribution <span class="math inline">\(\hat{\theta}= g( \hat{F}_n )\)</span>.</p>
<p>An example is the mean <span class="math inline">\(\text{E}_F(x)\)</span> with regard to <span class="math inline">\(F\)</span>. The <strong>empirical mean</strong> is the expectation with regard to the empirical distribution
which equals the <strong>average of the samples</strong>:
<span class="math display">\[
\hat{\text{E}}(x) = \hat{\mu} =  \text{E}_{\hat{F}_n}(x) = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x}
\]</span></p>
<p>Similarly, other empirical estimators can be constructed simply by replacing
the expectation in the definition of the quantity of interest by the sample average.
For example, the <strong>empirical variance</strong> with unknown mean is given by
<span class="math display">\[
\widehat{\text{Var}}(x) = \widehat{\sigma^2} =
\text{E}_{\hat{F}_n}((x - \hat{\mu})^2) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2
\]</span>
Note the factor <span class="math inline">\(1/n\)</span> before the summation sign. We can also write the empirical
variance in terms of
<span class="math inline">\(\overline{x^2} =\frac{1}{n}\sum^{n}_{k=1} x^2\)</span> as
<span class="math display">\[
\widehat{\text{Var}}(x) = \overline{x^2} - \bar{x}^2
\]</span></p>
<p>By construction, as a result of the strong convergence
of <span class="math inline">\(\hat{F}_n\)</span> to <span class="math inline">\(F\)</span> empirical estimators are consistent, with their MSE, variance
and bias all decreasing to zero with large sample size <span class="math inline">\(n\)</span>. However, for
finite sample size they do have a finite variance and may also be biased.</p>
<p>For example, the empirical variance given above is biased with
<span class="math inline">\(\text{Bias}(\widehat{\sigma^2}) = -\sigma^2/n\)</span>. Note this bias decreases with <span class="math inline">\(n\)</span>.
An unbiased estimator
can be obtained by rescaling the empirical estimator by the factor
<span class="math inline">\(n/(n-1)\)</span>:
<span class="math display">\[
\widehat{\sigma^2}_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\bar{x})^2
\]</span></p>
<p>The empirical estimators for the mean and variance can also be obtained
for random vectors <span class="math inline">\(\boldsymbol x\)</span>. In this case the data <span class="math inline">\(D = \{\boldsymbol x_1, \ldots, \boldsymbol x_n \}\)</span>
is comprised of <span class="math inline">\(n\)</span> vector-valued observations.</p>
<p>For the mean get
<span class="math display">\[
\hat{\boldsymbol \mu} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k = \bar{\boldsymbol x}
\]</span>
and for the covariance
<span class="math display">\[\widehat{\boldsymbol \Sigma} = \frac{1}{n}\sum^{n}_{k=1} \left(\boldsymbol x_k-\bar{\boldsymbol x}\right) \; \left(\boldsymbol x_k-\bar{\boldsymbol x}\right)^T\]</span>
Note the factor <span class="math inline">\(\frac{1}{n}\)</span> in the estimator of the covariance matrix.</p>
<p>With <span class="math inline">\(\overline{\boldsymbol x\boldsymbol x^T} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k \boldsymbol x_k^T\)</span>
we can also write
<span class="math display">\[
\widehat{\boldsymbol \Sigma} = \overline{\boldsymbol x\boldsymbol x^T} - \bar{\boldsymbol x} \bar{\boldsymbol x}^T
\]</span></p>
</div>
<div id="law-of-large-numbers" class="section level2" number="14.6">
<h2>
<span class="header-section-number">A.6</span> Law of large numbers<a class="anchor" aria-label="anchor" href="#law-of-large-numbers"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>law of large numbers</strong> was discovered by <a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Jacob Bernoulli (1655-1705)</a> and states that the average
converges to the mean.</p>
<p>Since <span class="math inline">\(\hat{F}_n\)</span> convergences strongly to <span class="math inline">\(F\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>
there is corresponding convergence of
the average
<span class="math inline">\(\text{E}_{\hat{F}_n}(h(x)) = \frac{1}{n} \sum_{i=1}^n h(x_i)\)</span> to the expectation <span class="math inline">\(\text{E}_{F}(h(x))\)</span>.</p>
<p>In other words, if the mean exists then for sufficiently large <span class="math inline">\(n\)</span> it can
be substituted by the empirical mean.</p>
<p>Likewise, the law of large numbers can be applied to empirical estimators
to show that they will converge to the corresponding true quantities for sufficiently large <span class="math inline">\(n\)</span>.</p>
<p>Furthermore, one may use the law of large numbers as a <strong>justification to interpret large-sample limits of frequencies as probabilities</strong>. However, <strong>the converse</strong> , namely requesting that all probabilities must have a frequentist interpretation, <strong>does not follow</strong> from the law of large numbers or from the axioms of probability.</p>
<p>Finally, it is worth pointing out that the law of large number says nothing
about the finite sample properties of estimators.</p>
</div>
<div id="sampling-distribution-of-mean-and-variance-estimators-for-normal-data" class="section level2" number="14.7">
<h2>
<span class="header-section-number">A.7</span> Sampling distribution of mean and variance estimators for normal data<a class="anchor" aria-label="anchor" href="#sampling-distribution-of-mean-and-variance-estimators-for-normal-data"><i class="fas fa-link"></i></a>
</h2>
<p>If the underlying distribution family of <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> is
known we can often obtain the exact distribution of an estimator.</p>
<p>For example, assuming normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span> we can derive
the sampling distribution for the empirical mean and variance:</p>
<ul>
<li><p>The empirical estimator of the mean parameter <span class="math inline">\(\mu\)</span> is given by <span class="math inline">\(\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i\)</span>. Under the normal assumption the distribution of <span class="math inline">\(\hat{\mu}\)</span> is
<span class="math display">\[
\hat{\mu} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
\]</span>
Thus <span class="math inline">\(\text{E}(\hat{\mu}) = \mu\)</span> and <span class="math inline">\(\text{Var}(\hat{\mu}) = \frac{\sigma^2}{n}\)</span>.
The estimate <span class="math inline">\(\hat{\mu}\)</span> is unbiased as <span class="math inline">\(\text{E}(\hat{\mu})-\mu = 0\)</span>. The mean
squared error of <span class="math inline">\(\hat{\mu}\)</span> is <span class="math inline">\(\text{MSE}(\hat{\mu}) = \frac{\sigma^2}{n}\)</span>.</p></li>
<li><p>The empirical variance <span class="math inline">\(\widehat{\sigma^2} = \frac{1}{n} \sum_{i=1}^n (x_i -\bar{x})^2\)</span> for normal data follows a one-dimensional Wishart distribution
<span class="math display">\[
\widehat{\sigma^2} \sim
W_1\left(s^2 = \frac{\sigma^2}{n}, k=n-1\right)
\]</span>
Thus, <span class="math inline">\(\text{E}( \widehat{\sigma^2} ) = \frac{n-1}{n}\sigma^2\)</span> and
<span class="math inline">\(\text{Var}( \widehat{\sigma^2}_{\text{ML}} ) = \frac{2(n-1)}{n^2}\sigma^4\)</span>.
The estimate <span class="math inline">\(\widehat{\sigma^2}\)</span> is biased since
<span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{ML}} )-\sigma^2 = -\frac{1}{n}\sigma^2\)</span>.
The mean squared error is <span class="math inline">\(\text{MSE}( \widehat{\sigma^2}) = \frac{2(n-1)}{n^2}\sigma^4 +\frac{1}{n^2}\sigma^4 =\frac{2 n-1}{n^2}\sigma^4\)</span>.</p></li>
<li>
<p>The unbiased variance estimate <span class="math inline">\(\widehat{\sigma^2}_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\bar{x})^2\)</span> for normal data follows a one-dimensional Wishart distribution
<span class="math display">\[
\widehat{\sigma^2}_{\text{UB}} \sim
W_1\left(s^2 = \frac{\sigma^2}{n-1}, k = n-1 \right)
\]</span>
Thus, <span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{UB}} ) = \sigma^2\)</span> and
<span class="math inline">\(\text{Var}( \widehat{\sigma^2}_{\text{UB}} ) = \frac{2}{n-1}\sigma^4\)</span>.
The estimate <span class="math inline">\(\widehat{\sigma^2}_{\text{ML}}\)</span> is unbiased since
<span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{UB}} )-\sigma^2 =0\)</span>.
The mean squared error is <span class="math inline">\(\text{MSE}( \widehat{\sigma^2}_{\text{UB}} ) =\frac{2}{n-1}\sigma^4\)</span>.</p>
<p>Interestingly, for any <span class="math inline">\(n&gt;1\)</span> we find that <span class="math inline">\(\text{Var}( \widehat{\sigma^2}_{\text{UB}} ) &gt; \text{Var}( \widehat{\sigma^2}_{\text{ML}} )\)</span> and <span class="math inline">\(\text{MSE}( \widehat{\sigma^2}_{\text{UB}} ) &gt; \text{MSE}( \widehat{\sigma^2}_{\text{ML}} )\)</span> so that the biased empirical estimator has both lower variance and lower mean squared error than the unbiased estimator.</p>
</li>
</ul>
</div>
<div id="t-statistics" class="section level2" number="14.8">
<h2>
<span class="header-section-number">A.8</span> <span class="math inline">\(t\)</span>-statistics<a class="anchor" aria-label="anchor" href="#t-statistics"><i class="fas fa-link"></i></a>
</h2>
<div id="one-sample-t-statistic" class="section level3" number="14.8.1">
<h3>
<span class="header-section-number">A.8.1</span> One sample <span class="math inline">\(t\)</span>-statistic<a class="anchor" aria-label="anchor" href="#one-sample-t-statistic"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we observe <span class="math inline">\(n\)</span> independent data points <span class="math inline">\(x_1, \ldots, x_n \sim N(\mu, \sigma^2)\)</span>.
Then the average <span class="math inline">\(\bar{x} = \sum_{i=1}^n x_i\)</span> is distributed as
<span class="math inline">\(\bar{x} \sim N(\mu, \sigma^2/n)\)</span> and correspondingly
<span class="math display">\[
z = \frac{\bar{x}-\mu}{\sqrt{\sigma^2/n}} \sim N(0, 1)
\]</span></p>
<p>Note that <span class="math inline">\(z\)</span> uses the known variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>If the variance is unknown and is estimated
by the unbiased <span class="math inline">\(s^2_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\bar{x})^2\)</span>
then one arrives at the one sample <span class="math inline">\(t\)</span>-statistic
<span class="math display">\[
t_{\text{UB}} = \frac{\bar{x}-\mu}{\sqrt{s^2_{\text{UB}}/n}} \sim t_{n-1} \,.
\]</span>
It is distributed according to a Student’s <span class="math inline">\(t\)</span>-distribution
with <span class="math inline">\(n-1\)</span> degrees of freedom, with mean 0 for <span class="math inline">\(n&gt;2\)</span> and variance
<span class="math inline">\((n-1)/(n-3)\)</span> for <span class="math inline">\(n&gt;3\)</span>.</p>
<p>If instead of the unbiased estimate the empirical (ML) estimate of the variance <span class="math inline">\(s^2_{\text{ML}} = \frac{1}{n} \sum_{i=1}^n (x_i -\bar{x})^2 = \frac{n-1}{n} s^2_{\text{UB}}\)</span> is used then this leads to a slightly different statistic
<span class="math display">\[
t_{\text{ML}} = \frac{\bar{x}-\mu}{ \sqrt{ s^2_{\text{ML}}/n}}  = \sqrt{\frac{n}{n-1}} t_{\text{UB}}
\]</span>
with
<span class="math display">\[
t_{\text{ML}} \sim \text{lst}\left(0, \tau^2=\frac{n}{n-1}, n-1\right)
\]</span>
Thus, <span class="math inline">\(t_{\text{ML}}\)</span> follows a location-scale <span class="math inline">\(t\)</span>-distribution, with mean 0 for <span class="math inline">\(n&gt;2\)</span> and variance
<span class="math inline">\(n/(n-3)\)</span> for <span class="math inline">\(n&gt;3\)</span>.</p>
</div>
<div id="two-sample-t-statistic-with-common-variance" class="section level3" number="14.8.2">
<h3>
<span class="header-section-number">A.8.2</span> Two sample <span class="math inline">\(t\)</span>-statistic with common variance<a class="anchor" aria-label="anchor" href="#two-sample-t-statistic-with-common-variance"><i class="fas fa-link"></i></a>
</h3>
<p>Now suppose we observe normal data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> from two groups
with sample size <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> (and <span class="math inline">\(n=n_1+n_2\)</span>) with two different means <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> and common variance <span class="math inline">\(\sigma^2\)</span>:
<span class="math display">\[x_1,\dots,x_{n_1} \sim N(\mu_1, \sigma^2)\]</span>
and
<span class="math display">\[x_{n_1+1},\dots,x_{n} \sim N(\mu_2, \sigma^2)\]</span>
Then <span class="math inline">\(\hat{\mu}_1 = \frac{1}{n_1}\sum^{n_1}_{i=1}x_i\)</span> and
<span class="math inline">\(\hat{\mu}_2 = \frac{1}{n_2}\sum^{n}_{i=n_1+1}x_i\)</span> are the sample averages within each group.</p>
<p>The common variance <span class="math inline">\(\sigma^2\)</span> may be estimated either by
the unbiased estimate <span class="math inline">\(s^2_{\text{UB}} = \frac{1}{n-2} \left(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)^2+\sum^n_{i=n_1+1}(x_i-\hat{\mu}_2)^2\right)\)</span>
(note the factor <span class="math inline">\(n-2\)</span>) or by the empirical (ML) estimate <span class="math inline">\(s^2_{\text{ML}} = \frac{1}{n} \left(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)^2+\sum^n_{i=n_1+1}(x_i-\hat{\mu}_2)^2\right) =\frac{n-2}{n} s^2_{\text{UB}}\)</span>. The estimator
for the common variance is a often referred to as <em>pooled variance estimate</em> as information is pooled from two groups to obtain the estimate.</p>
<p>Using the unbiased pooled variance estimate the two sample <span class="math inline">\(t\)</span>-statistic is given by
<span class="math display">\[
t_{\text{UB}} = \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{1}{n_1}+\frac{1}{n_2}\right)  s^2_{\text{UB}}}  }
= \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{n}{n_1 n_2} \right) s^2_{\text{UB}} }  }
\]</span>
In terms of empirical frequencies <span class="math inline">\(\hat{\pi}_1 = \frac{n_1}{n}\)</span> and <span class="math inline">\(\hat{\pi}_2 = \frac{n_2}{n}\)</span>
it can also be written as
<span class="math display">\[
t_{\text{UB}} = \sqrt{n} \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{  \left(\frac{1}{\hat{\pi}_1}+\frac{1}{\hat{\pi}_2}\right) s^2_{\text{UB}} }}
= \sqrt{n\hat{\pi}_1 \hat{\pi}_2} \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ s^2_{\text{UB}}}}
\]</span>
The two sample <span class="math inline">\(t\)</span>-statistic is distributed as
<span class="math display">\[
t_{\text{UB}} \sim t_{n-2}
\]</span>
i.e. according to a Student’s <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, with mean 0 for <span class="math inline">\(n&gt;3\)</span> and variance <span class="math inline">\((n-2)/(n-4)\)</span> for <span class="math inline">\(n&gt;4\)</span>.
Large values of the two sample <span class="math inline">\(t\)</span>-statistic indicates that there are indeed two groups
rather than just one.</p>
<p>The two sample <span class="math inline">\(t\)</span>-statistic using the empirical (ML) pooled estimate of the variance is
<span class="math display">\[
\begin{split}
t_{\text{ML}} &amp; = \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{1}{n_1}+\frac{1}{n_2}\right)  s^2_{\text{ML}}  }   }
= \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{n}{n_1 n_2}\right) s^2_{\text{ML}}  }   }\\
&amp; =\sqrt{n} \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{1}{\hat{\pi}_1}+\frac{1}{\hat{\pi}_2}\right) s^2_{\text{ML}} }}
= \sqrt{n \hat{\pi}_1 \hat{\pi}_2 } \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ s^2_{\text{ML}}}}\\
&amp; = \sqrt{\frac{n}{n-2}} t_{\text{UB}}
\end{split}
\]</span>
with
<span class="math display">\[
t_{\text{ML}} \sim \text{lst}\left(0, \tau^2=\frac{n}{n-2}, n-2\right)
\]</span>
Thus, <span class="math inline">\(t_{\text{ML}}\)</span> follows a location-scale <span class="math inline">\(t\)</span>-distribution, with mean 0 for <span class="math inline">\(n&gt;3\)</span> and variance
<span class="math inline">\(n/(n-4)\)</span> for <span class="math inline">\(n&gt;4\)</span>.</p>
</div>
</div>
<div id="confidence-intervals" class="section level2" number="14.9">
<h2>
<span class="header-section-number">A.9</span> Confidence intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals"><i class="fas fa-link"></i></a>
</h2>
<div id="general-concept" class="section level3" number="14.9.1">
<h3>
<span class="header-section-number">A.9.1</span> General concept<a class="anchor" aria-label="anchor" href="#general-concept"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>A <strong>confidence</strong> interval (CI) is an <strong>interval estimate</strong> with a <strong>frequentist</strong> interpretation.</li>
<li>Definition of <strong>coverage</strong> <span class="math inline">\(\kappa\)</span> of a CI: how often (in repeated identical experiment) does the estimated CI overlap the true parameter value <span class="math inline">\(\theta\)</span>
<ul>
<li>E.g.: Coverage <span class="math inline">\(\kappa=0.95\)</span> (95%) means that in 95 out of 100 case the estimated CI will contain the (unknown) true value (i.e. it will “cover” <span class="math inline">\(\theta\)</span>).</li>
</ul>
</li>
</ul>
<p>Illustration of the repeated construction of a CI for <span class="math inline">\(\theta\)</span>:</p>
<div class="inline-figure"><img src="fig/refresher_p1.PNG" width="40%" style="display: block; margin: auto;"></div>
<ul>
<li>Note that a CI is actually an <strong>estimate</strong>: <span class="math inline">\(\widehat{\text{CI}}(x_1, \ldots, x_n)\)</span>, i.e. it depends on data and has a random (sampling) variation.<br>
</li>
<li>A good CI has high coverage and is compact.</li>
</ul>
<p><strong>Note:</strong> the coverage probability is <strong>not</strong> the probability that the true value is contained in a given estimated interval (that would be the Bayesian <em>credible</em> interval).</p>
<ul>
<li>There is a direct relationship between confidence intervals and statistical testing procedures. Specifically, a confidence interval can be interpreted as the set of parameter values that cannot be rejected.</li>
</ul>
</div>
<div id="symmetric-normal-confidence-interval" class="section level3" number="14.9.2">
<h3>
<span class="header-section-number">A.9.2</span> Symmetric normal confidence interval<a class="anchor" aria-label="anchor" href="#symmetric-normal-confidence-interval"><i class="fas fa-link"></i></a>
</h3>
<p>For a normally distributed univariate random variable
it is straightforward to construct a symmetric two-sided CI with a given desired coverage <span class="math inline">\(\kappa\)</span>.</p>
<div class="inline-figure"><img src="fig/refresher_p2_1.PNG" width="80%" style="display: block; margin: auto;"></div>
<p>For a normal random variable <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> and density function <span class="math inline">\(f(x)\)</span> we can compute the probability</p>
<p><span class="math display">\[\text{Pr}(x \leq \mu + c \sigma) =  \int_{-\infty}^{\mu+c\sigma} f(x) dx  = \Phi (c) = \frac{1+\kappa}{2}\]</span>
Note <span class="math inline">\(\Phi(c)\)</span> is the cumulative distribution function (CDF) of the standard normal <span class="math inline">\(N(0,1)\)</span>:</p>
<p>From the above we obtain the critical point <span class="math inline">\(c\)</span> from the quantile function, i.e. by inversion of <span class="math inline">\(\Phi\)</span>:</p>
<p><span class="math display">\[c=\Phi^{-1}\left(\frac{1+\kappa}{2}\right)\]</span></p>
<p>The following table lists <span class="math inline">\(c\)</span> for the three most commonly used values of <span class="math inline">\(\kappa\)</span> - it is useful to memorise these values!</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span>
</th>
<th>Critical value <span class="math inline">\(c\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>1.64</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>1.96</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>2.58</td>
</tr>
</tbody>
</table></div>
<p>A <strong>symmetric standard normal CI</strong> with nominal coverage <span class="math inline">\(\kappa\)</span> for</p>
<ul>
<li>a scalar parameter <span class="math inline">\(\theta\)</span>
</li>
<li>with normally distributed estimate <span class="math inline">\(\hat{\theta}\)</span> and</li>
<li>with estimated standard deviation <span class="math inline">\(\hat{\text{SD}}(\hat{\theta}) = \hat{\sigma}\)</span>
</li>
</ul>
<p>is then given by
<span class="math display">\[
\widehat{\text{CI}}=[\hat{\theta} \pm c \hat{\sigma}]
\]</span>
where <span class="math inline">\(c\)</span> is chosen for desired coverage level <span class="math inline">\(\kappa\)</span>.</p>
</div>
<div id="confidence-interval-based-on-the-chi-squared-distribution" class="section level3" number="14.9.3">
<h3>
<span class="header-section-number">A.9.3</span> Confidence interval based on the chi-squared distribution<a class="anchor" aria-label="anchor" href="#confidence-interval-based-on-the-chi-squared-distribution"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-figure"><img src="fig/refresher_p4_2.PNG" width="80%" style="display: block; margin: auto;"></div>
<p>As for the normal CI we can compute critical values but for the
chi-squared distribution we use a one-sided interval:
<span class="math display">\[
\text{Pr}(x \leq c) = \kappa
\]</span>
As before we get <span class="math inline">\(c\)</span> by the quantile function, i.e. by inverting the CDF of the chi-squared distribution.</p>
<p>The following list the critical values for the three most common choice of <span class="math inline">\(\kappa\)</span>
for <span class="math inline">\(m=1\)</span> (one degree of freedom):</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span>
</th>
<th>Critical value <span class="math inline">\(c\)</span> (<span class="math inline">\(m=1\)</span>)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>2.71</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>3.84</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>6.63</td>
</tr>
</tbody>
</table></div>
<p>A one-sided CI with nominal coverage <span class="math inline">\(\kappa\)</span> is then given by <span class="math inline">\([0, c ]\)</span>.</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></div>
<div class="next"><a href="further-study.html"><span class="header-section-number">B</span> Further study</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#statistics-refresher"><span class="header-section-number">A</span> Statistics refresher</a></li>
<li><a class="nav-link" href="#statistical-learning"><span class="header-section-number">A.1</span> Statistical learning</a></li>
<li><a class="nav-link" href="#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><span class="header-section-number">A.2</span> Sampling properties of a point estimator \(\hat{\boldsymbol \theta}\)</a></li>
<li><a class="nav-link" href="#efficiency-and-consistency-of-an-estimator"><span class="header-section-number">A.3</span> Efficiency and consistency of an estimator</a></li>
<li><a class="nav-link" href="#empirical-distribution-function"><span class="header-section-number">A.4</span> Empirical distribution function</a></li>
<li><a class="nav-link" href="#empirical-estimators"><span class="header-section-number">A.5</span> Empirical estimators</a></li>
<li><a class="nav-link" href="#law-of-large-numbers"><span class="header-section-number">A.6</span> Law of large numbers</a></li>
<li><a class="nav-link" href="#sampling-distribution-of-mean-and-variance-estimators-for-normal-data"><span class="header-section-number">A.7</span> Sampling distribution of mean and variance estimators for normal data</a></li>
<li>
<a class="nav-link" href="#t-statistics"><span class="header-section-number">A.8</span> \(t\)-statistics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#one-sample-t-statistic"><span class="header-section-number">A.8.1</span> One sample \(t\)-statistic</a></li>
<li><a class="nav-link" href="#two-sample-t-statistic-with-common-variance"><span class="header-section-number">A.8.2</span> Two sample \(t\)-statistic with common variance</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#confidence-intervals"><span class="header-section-number">A.9</span> Confidence intervals</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#general-concept"><span class="header-section-number">A.9.1</span> General concept</a></li>
<li><a class="nav-link" href="#symmetric-normal-confidence-interval"><span class="header-section-number">A.9.2</span> Symmetric normal confidence interval</a></li>
<li><a class="nav-link" href="#confidence-interval-based-on-the-chi-squared-distribution"><span class="header-section-number">A.9.3</span> Confidence interval based on the chi-squared distribution</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistics 2: Likelihood and Bayes</strong>" was written by Korbinian Strimmer. It was last built on 12 January 2024.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
