[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"lecture notes MATH27720 Statistics 2, course second year mathematics students Department Mathematics University Manchester.course text written Korbinian Strimmer 2023–. version 13 December 2023.notes updated time time. view current version visit \nonline MATH27720 Statistics 2 lecture notes web browser.may also wish download MATH27720 Statistics 2 lecture notes PDF A4 format printing (double page layout) 6x9 inch PDF use tablets (single page layout).","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"notes licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"about-the-author","chapter":"Preface","heading":"About the author","text":"Hello! name Korbinian Strimmer Professor Statistics. member Statistics group\nDepartment Mathematics University Manchester. can find information home page.notes version MATH27720 Statistics 2 taught spring 2024 University Manchester.hope enjoy course find notes useful! questions, comments, corrections please email korbinian.strimmer@manchester.ac.uk.","code":""},{"path":"preface.html","id":"about-the-module","chapter":"Preface","heading":"About the module","text":"","code":""},{"path":"preface.html","id":"topics-covered","chapter":"Preface","heading":"Topics covered","text":"MATH27720 Statistics 2 module designed run course 11 weeks.\nfollowing structure:Overview refresher (W1)Likelihood estimation inference (W2–W6)Bayesian learning inference (W7–W11)module focuses conceptual understanding methods, theory. Specifically, learn foundations statistical learning using likelihood Bayesian approaches also underpinned entropy., presentation course non-technical.\naim offer insights diverse statistical approaches linked demonstrate statistics offers concise coherent theory information rather adhoc collection “recipes” data analysis (common wrong perception statistics).","code":""},{"path":"preface.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"module important refresh knowledge :Introduction statisticsProbabilityR data analysis programmingIn addition need elements matrix algebra compute gradient curvature function several variables.Check Appendix notes brief refresher essential material.","code":""},{"path":"preface.html","id":"additional-support-material","chapter":"Preface","heading":"Additional support material","text":"University Manchester student enrolled module\nfind Blackboard:weekly learning plan 11 week study period (plus one additional week revision),weekly worksheets examples solutions R code, andexam papers previous years.Furthermore, also MATH27720 Statistics 2 online reading list hosted University Manchester library.","code":""},{"path":"preface.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"notes part based earlier notes MATH20802 Statistical Methods last run Spring 2023.\nMany thanks Beatriz Costa Gomes help creating 2019 version lecture notes teaching MATH20802 module first time Kristijonas Raudys extensive feedback 2020 version.","code":""},{"path":"overview-of-statistical-learning.html","id":"overview-of-statistical-learning","chapter":"1 Overview of statistical learning","heading":"1 Overview of statistical learning","text":"","code":""},{"path":"overview-of-statistical-learning.html","id":"how-to-learn-from-data","chapter":"1 Overview of statistical learning","heading":"1.1 How to learn from data?","text":"fundamental question extract information data optimal way, make predictions based information.purpose, number competing theories information developed. Statistics oldest science information concerned offering principled ways learn data extract process information using probabilistic models.\nHowever, theories information (e.g. Vapnik-Chernov theory learning, computational learning) algorithmic analytic sometimes even based probability theory.Furthermore, disciplines, computer science machine learning closely linked also substantial overlap statistics. field “data science” today comprises statistics machine learning brings together mathematics, statistics computer science. Also growing field -called “artificial intelligence” makes substantial use statistical machine learning techniques.recent popular science book “Master Algorithm” Domingos (2015) provides accessible informal overview \nvarious schools science information. discusses main algorithms used machine learning statistics:Starting early 1763, Bayesian school learning started later turned closely linked likelihood inference established 1922 Ronald . Fisher (1890–1962) generalised 1951 entropy learning Kullback Leibler.Starting early 1763, Bayesian school learning started later turned closely linked likelihood inference established 1922 Ronald . Fisher (1890–1962) generalised 1951 entropy learning Kullback Leibler.also 1950s concept artificial neural network arises, essentially nonlinear input-output map underlying probabilistic modelling. field saw another leap 1980s progressed 2010 onwards development deep dearning. now one popular (effective) methods analysing imaging data. Even mobile phone likely now dedicated computer chip special neural network hardware. Despite non-probabilistic origins, modern interpretations neural networks view high-dimensional nonlinear\nstatistical models.also 1950s concept artificial neural network arises, essentially nonlinear input-output map underlying probabilistic modelling. field saw another leap 1980s progressed 2010 onwards development deep dearning. now one popular (effective) methods analysing imaging data. Even mobile phone likely now dedicated computer chip special neural network hardware. Despite non-probabilistic origins, modern interpretations neural networks view high-dimensional nonlinear\nstatistical models.advanced theories information developed 1960 term \ncomputational learning, notably Vapnik-Chernov theory, prominent example “support vector machine” (another non-probabilistic model) devised 1990s. important\nadvances include “ensemble learning” corresponding algorithmic approaches classification \n“random Forests”.advanced theories information developed 1960 term \ncomputational learning, notably Vapnik-Chernov theory, prominent example “support vector machine” (another non-probabilistic model) devised 1990s. important\nadvances include “ensemble learning” corresponding algorithmic approaches classification \n“random Forests”.advent large-scale genomic high-dimensional data surge new exciting developments field high-dimensional (large dimension) also big data (large dimension large sample size), statistics machine learning.advent large-scale genomic high-dimensional data surge new exciting developments field high-dimensional (large dimension) also big data (large dimension large sample size), statistics machine learning.connections various fields information still perfectly understood, clear overarching theory need based probabilistic learning.","code":""},{"path":"overview-of-statistical-learning.html","id":"probability-theory-versus-statistical-learning","chapter":"1 Overview of statistical learning","heading":"1.2 Probability theory versus statistical learning","text":"study statistics (information theory) need aware fundamental difference probability theory\nstatistics, relates distinction \n“randomness” “uncertainty”.Probability theory studies randomness, developing mathematical models randomness (probability distributions), studying corresponding mathematical properties (including asymptotics etc). Probability theory may fact viewed branch measure theory, belongs domain pure mathematics.Probability theory provides probabilistic generative models data, simulation\nnew data well explaining observed data.\nMethods theory best identify probabilistic models observations use\npredict future observations belongst domain applied mathematics, specifically statistics related areas machine learning data science.Note statistics, contrast probability, therefore concerned randomness. Instead, focus measuring elucidating uncertainty events, predictions, outcomes, model parameters uncertainty measures state knowledge quantities. soon new data information becomes available, state knowledge thus uncertainty changes! Thus, uncertainty epistemological property.uncertainty often due ignorance true underlying processes (purpose ), underlying process actually random. success statistics based fact can mathematically model uncertainty without knowing detailed specifics underlying processes, yet still can furnish procedures optimal inference despite uncertainty.short, statistics describing state knowledge world, \nmay uncertain incomplete, make decisions predictions face uncertainty, uncertaintly sometimes derives randomness often ignorance (sometimes ignorance even helps create simple yet effective model).","code":""},{"path":"overview-of-statistical-learning.html","id":"cartoon-of-statistical-learning","chapter":"1 Overview of statistical learning","heading":"1.3 Cartoon of statistical learning","text":"aim statistical learning use observed data optimal way learn \nunderlying mechanism data-generating process. Since data typically finite models\ncan principle arbitrarily complex may issues overfitting (enough data\ncomplexity) also underfitting (model simplistic).observe data \\(D = \\{x_1, \\ldots, x_n\\}\\) assumed result underlying\ntrue data-generating model \\(F_{\\text{true}}\\), distribution \\(x\\).explain observed data, also predict future data, make hypotheses\nform candidate models \\(F_{1}, F_{2}, \\ldots\\). Often candidate models\nform model family \\(F_{\\boldsymbol \\theta}\\) indexed parameter vector \\(\\boldsymbol \\theta\\), specific values\nmodel\ncan also write \\(F_{\\boldsymbol \\theta_1}, F_{\\boldsymbol \\theta_2}, \\ldots\\) various models.\nparameters may, may , direct interpretation. Ideally, \nmodels also identifiable within family, .e. distinct model identified unique parameter\n\\(F_{\\boldsymbol \\theta_1} = F_{\\boldsymbol \\theta_2}\\) implies \\(\\boldsymbol \\theta_1 = \\boldsymbol \\theta_2\\),\nhence models corresponding parameters must also .true underlying model unknown observed.\nHowever, can observe data \\(D\\) true model \\(F_{\\text{true}}\\) measuring properties interest (observations experiments). Sometimes can also perturb model see effect (interventional study).various candidate models \\(F_1, F_2, \\ldots\\) model world best good approximations true underlying data generating model \\(F_{\\text{true}}\\).\ncases true model part model family, .e.\nexists parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) \\(F_{\\text{true}} = F_{\\boldsymbol \\theta_{\\text{true}}}\\).\nHowever, typically assume true underlying model contained family. Nonetheless, even imperfect candidate model often provide useful mathematical approximation capture important characteristics true model thus help interpret observed data.\\[\n\\begin{array}{cc}\n\\textbf{Hypothesis} \\\\\n\\text{world works} \\\\\n\\end{array}\n\\longrightarrow\n\\begin{array}{cc}\n\\textbf{Model world} \\\\\nF_1,  \\boldsymbol \\theta_1  \\\\\nF_2, \\boldsymbol \\theta_2  \\\\\n\\vdots\\\\\n\\end{array}\n\\]\n\\[\n\\longrightarrow\n\\begin{array}{cc}\n\\textbf{Real world,} \\\\\n\\textbf{unknown true model} \\\\\nF_{\\text{true}}, \\boldsymbol \\theta_{\\text{true}} \\\\\n\\end{array}\n\\longrightarrow \\textbf{Data } x_1, \\ldots, x_n\n\\]aim statistical learning identify model(s) explain current data also predict future data (.e. predict outcome experiments conducted yet).Thus good model provides good fit current data (.e. explains current observations well) also future data (.e. generalises well).large proportion statistical theory devoted finding “good” models\navoid overfitting (models complex don’t generalise well) \nunderfitting (models simplistic hence also don’t predict well).Typically aim find model whose model complexity well matched \ncomplexity unknown true model also complexity observed data.","code":""},{"path":"overview-of-statistical-learning.html","id":"common-distributions-used-in-statistical-models","chapter":"1 Overview of statistical learning","heading":"1.4 Common distributions used in statistical models","text":"Models employed statistical analysis typically multivariate comprising many\nrandom variables. models can complex, hierarchical network-like structures\nlinking observed latent variables, possibly exhibiting nonlinear functional relationships.However, nonetheless even complex models normally composed \nelementary building blocks. example, following parametric distributions\nfrequently occur statistical analysis:Bernoulli distribution \\(\\text{Ber}(\\theta)\\) categorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\): used\nmodel frequencies (domain \\([0,1]\\)). Repeated application yields \nbinomial distribution \\(\\text{Bin}(n, \\theta)\\) multinomial distribution \\(\\text{Mult}(n, \\boldsymbol \\pi)\\).Bernoulli distribution \\(\\text{Ber}(\\theta)\\) categorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\): used\nmodel frequencies (domain \\([0,1]\\)). Repeated application yields \nbinomial distribution \\(\\text{Bin}(n, \\theta)\\) multinomial distribution \\(\\text{Mult}(n, \\boldsymbol \\pi)\\).Normal distribution univariate \\(N(\\mu, \\sigma^2)\\) \nmultivariate \\(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) version: commonly used model mean values\n(domain \\([-\\infty, \\infty]\\)).Normal distribution univariate \\(N(\\mu, \\sigma^2)\\) \nmultivariate \\(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) version: commonly used model mean values\n(domain \\([-\\infty, \\infty]\\)).Gamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\): used model scale factors\n(domain \\([0, \\infty]\\)). also known (different parameterisation) univariate\nWishart distribution \\(W_1\\left(s^2, k \\right)\\) scaled chi-squared distribution\n\\(s^2 \\text{$\\chi^2_{k}$}\\). Special cases include chi-squared distribution \\(\\text{$\\chi^2_{k}$}\\) \nexponential distribution \\(\\text{Exp}(\\theta)\\).Gamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\): used model scale factors\n(domain \\([0, \\infty]\\)). also known (different parameterisation) univariate\nWishart distribution \\(W_1\\left(s^2, k \\right)\\) scaled chi-squared distribution\n\\(s^2 \\text{$\\chi^2_{k}$}\\). Special cases include chi-squared distribution \\(\\text{$\\chi^2_{k}$}\\) \nexponential distribution \\(\\text{Exp}(\\theta)\\).Note parametric distributions exponential families\n(see Example 1.1) lend particularly well statistical\nlearning, see module.Another commonly used parametric model generalisation normal distribution:Location-scale \\(t\\)-distribution \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\): similar \nnormal distibution \\(N(\\mu, \\sigma^2)\\) heavier tails. emerges \nsampling distribution \\(t\\)-statistic compound distribution Bayesian learning.\nSpecial cases include Student’s \\(t_\\nu\\) distribution Cauchy distribution\n\\(\\text{Cau}(\\mu, \\tau)\\). contrast models mentioned far exponential\nfamily due heavy tails,\ndepending choice degrees freedom \\(\\nu\\), moments distribution may exist.Finally, nonparametric models also often used describe analyse observed data.\nRather specifying parametric model \\(F\\) one focuses using whole distribution define\nmeaningful statistical functionals \\(\\theta = g(F)\\), mean variance.models used throughout Statistics 2 module — refresher technical details refer Appendix. second part module (Bayesian statistics) encounter distributions beta distribution inverse gamma distribution.\nSubsequent modules later study years (year 3 4) introduce complex models, related temporal spatial modelling, regression analysis generalised linear models, multivariate statistics machine learning.Example 1.1  Definition exponential family:distribution family \\(P_{\\boldsymbol \\eta}\\) exponential family canonical parameters\n\\(\\boldsymbol \\eta\\) family results exponential\ntilting base distribution \\(B\\). case\ndensity probability mass function can written \n\\[\np(x|\\boldsymbol \\eta) =  b(x) \\, e^{ \\boldsymbol \\eta^T \\boldsymbol u(x)} / e^{ \\psi(\\boldsymbol \\eta)}\n\\]\n\\(b(x)\\) base density \\(\\boldsymbol u(x)\\) canonical statistics\n\n\\[\n\\psi(\\boldsymbol \\eta) = \\log \\int_x b(x) \\, e^{ \\boldsymbol \\eta^T \\boldsymbol u(x)} \\, dx\n\\]\nlog-normaliser log-partition function ensures \n\\(p(x|\\boldsymbol \\eta)\\) integrates one. Exponential families many favourable properties facilitate estimation inference, existence moments, easy optimise, analytic solutions parameter estimates, among many others.Exponential families studied detail module generalised linear models\ncourse also highlight general results apply specifically .","code":""},{"path":"overview-of-statistical-learning.html","id":"finding-the-best-models","chapter":"1 Overview of statistical learning","heading":"1.5 Finding the best models","text":"core task statistical learning identify distributions explain existing data well also generalise well future yet unseen observations.nonparametric setting may simply rely law large numbers implies \nempirical distribution \\(\\hat{F}_n\\) constructed\nobserved data \\(D\\) converges true distribution \\(F\\) sample size large.\ncan therefore obtain empirical estimator \\(\\hat{\\theta}\\) functional \\(\\theta = g(F)\\) \n\\(\\hat{\\theta}= g( \\hat{F}_n )\\), .e. substituting true distribution empirical distribution.\nallows us, e.g., get empirical estimate mean\n\\[\n\\hat{\\text{E}}(x) = \\hat{\\mu} =  \\text{E}_{\\hat{F}_n}(x) = \\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\n\\]\nvariance\n\\[\n\\widehat{\\text{Var}}(x) = \\widehat{\\sigma^2} =\n\\text{E}_{\\hat{F}_n}((x - \\hat{\\mu})^2) = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\n\\]\nsimply replacing expectation sample average.parametric models need find estimates parameters correspond \ndistributions best approximate unknown true data generating model.\nOne approach provided method maximum likelihood. precisely,\ngiven probability distribution \\(P_{\\boldsymbol \\theta}\\) density mass function \\(p(x|\\boldsymbol \\theta)\\) \\(\\boldsymbol \\theta\\) parameter vector, \\(D = \\{x_1,\\dots,x_n\\}\\) observed iid data (.e. independent identically distributed), likelihood function defined \n\\[\nL_n(\\boldsymbol \\theta| D ) =\\prod_{=1}^{n} p(x_i|\\boldsymbol \\theta)\n\\]\nparameter maximises likelihood maximum likelihood estimate.first part module devoted exploring method maximum likelihood practically\ntheoretically. start considering justification method\nmaximum likelihood. Historically, likelihood function introduced (still often interpreted) probability observe data given model specified parameters \\(\\boldsymbol \\theta\\). However, view incorrect \nbreaks continuous random variables due use densities even discrete random variables additional factor accounting possible permutations samples needed obtain actual probability data. Instead, true foundation \nmaximum likelihood lies information theory, specifically close link relative entropy \nunknown true distribution \\(F\\) model \\(P_{\\boldsymbol \\theta}\\). result see maximum\nlikelihood extends empirical estimation parametric models.\ninsight allows shed light optimality properties well limitations maximum\nlikelihood inference.second part introduce Bayesian approach statistical estimation inference can viewed natural extension likelihood-based statistical analysis overcomes \nlimitations maximum likelihood.aim module thereforeto provide principled introduction maximum likelihood Bayesian\nstatistical analysis andto demonstrate statistics offers well founded coherent theory information, rather just seemingly unrelated collections “recipes” data analysis (still widespread wrong perception statistics).","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"from-entropy-to-maximum-likelihood","chapter":"2 From entropy to maximum likelihood","heading":"2 From entropy to maximum likelihood","text":"","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"overview","chapter":"2 From entropy to maximum likelihood","heading":"2.1 Overview","text":"purpose chapter introduce various\nimportant information criteria used statistics machine learning.\nbased entropy provide foundation method maximum likelihood.\nThus, also enable better understanding properties maximum likelihood.modern definition (relative) entropy first discovered 1870s physicist Ludwig Boltzmann (1844–1906)\ncontext thermodynamics. probabilistic interpretation statistical mechanics entropy developed Josiah W. Gibbs (1839–1903).essence, physics entropy describes \nspread concentration energy.1940–1950’s notion entropy turned \ncentral also information theory, field pioneered mathematicians \nRalph Hartley (1888–1970),\nSolomon Kullback (1907–1994),\nAlan Turing (1912–1954),\nRichard Leibler (1914–2003),\nIrving J. Good (1916–2009),\nClaude Shannon (1916–2001), \nEdwin T. Jaynes (1922–1998),\nlater explored \nShun’ichi Amari (1936–),\nImre Ciszár (1938–),\nBradley Efron (1938–),\nPhilip Dawid (1946–) \nmany others., Turing Good affiliated University Manchester.\\[\\begin{align*}\n\\left.\n\\begin{array}{cc}\n\\\\\n\\textbf{Entropy} \\\\\n\\\\\n\\end{array}\n\\right.\n\\left.\n\\begin{array}{cc}\n\\\\\n\\nearrow  \\\\\n\\searrow  \\\\\n\\\\\n\\end{array}\n\\right.\n\\begin{array}{ll}\n\\text{Shannon Entropy} \\\\\n\\\\\n\\text{Relative Entropy}  \\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{(Shannon 1948)} \\\\\n\\\\\n\\text{(Kullback-Leibler 1951)}  \\\\\n\\end{array}\n\\end{align*}\\]\\[\\begin{align*}\n\\left.\n\\begin{array}{ll}\n\\text{Fisher information} \\\\\n\\\\\n\\text{Mutual Information} \\\\\n\\end{array}\n\\right.\n\\begin{array}{ll}\n\\rightarrow\\text{ Likelihood theory} \\\\\n\\\\\n\\rightarrow\\text{ Information theory} \\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{(Fisher 1922)} \\\\\n\\\\\n\\text{(Shannon 1948, Lindley 1953)}  \\\\\n\\end{array}\n\\end{align*}\\]","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"shannon-entropy-and-differential-entropy","chapter":"2 From entropy to maximum likelihood","heading":"2.2 Shannon entropy and differential entropy","text":"","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"surprise-surprisal-or-shannon-information","chapter":"2 From entropy to maximum likelihood","heading":"2.2.1 Surprise, surprisal or Shannon information","text":"surprise observe event probability \\(p\\) defined \\(-\\log(p)\\).\nalso called surprisal Shannon information.Thus, surprise observe certain event (\\(p=1\\)) zero,\nconversely surprise observe event certain happen\n(\\(p=0\\)) infinite.log-odds ratio can viewed difference surprise event surprise complementary event:\n\\[\n\\log\\left( \\frac{p}{1-p} \\right) =  -\\log(1-p) - ( -\\log(p))\n\\]module always use natural logarithm default, explicitly write \\(\\log_2\\) \\(\\log_{10}\\) logarithms respect base 2 10, respectively.Surprise entropy computed natural logarithm (\\(\\log\\)) given “nats” (=natural information units ). Using \\(\\log_2\\) leads “bits” using \\(\\log_{10}\\) “ban” “Hartley”.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"shannon-entropy","chapter":"2 From entropy to maximum likelihood","heading":"2.2.2 Shannon entropy","text":"Assume categorical distribution \\(P\\) \\(K\\) classes/categories. corresponding\nclass probabilities \\(p_1, \\ldots, p_K\\) \\(\\text{Pr}(\\text{\"class k\"}) = p_k\\)\n\\(\\sum_{k=1}^K p_k= 1\\). probability mass function (PMF)\n\\(p(x = \\text{\"class k\"})= p_k\\).random variable \\(x\\) discrete categorical distribution\n\\(P\\) discrete distribution. However, \\(P\\) generally also known discrete distribution.Shannon entropy (1948) 1 distribution \\(P\\) defined \nexpected surprise expected Shannon information, .e. negative expected log-probability\n\\[\n\\begin{split}\nH(P) &=-\\text{E}_P\\left(\\log p(x)\\right) \\\\\n     &= - \\sum_{k=1}^{K}p_k \\log(p_k) \\\\\n\\end{split}\n\\]\n\\(p_k \\[0,1]\\) construction Shannon entropy must larger equal 0.Furthermore, bounded \\(\\log K\\). can seen maximising Shannon entropy\nfunction regard \\(p_k\\) constraint \\(\\sum_{k=1}^K p_k= 1\\), e.g., constrained optimisation using Lagrange\nmultipliers. maximum achieved \\(P\\) discrete uniform - see Example 2.3.Hence categorical distribution \\(P\\) \\(K\\) categories \n\\[\\log K \\geq  H(P) \\geq 0\\]statistical physics, Shannon entropy known Gibbs entropy (1878).","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"combinatorial-derivation-of-shannon-entropy","chapter":"2 From entropy to maximum likelihood","heading":"2.2.3 Combinatorial derivation of Shannon entropy","text":"close link Shannon entropy multinomial coefficients counting permutations \\(n\\) items (samples) \\(K\\) distinct types (classes):Example 2.1  Large sample asymptotics log-multinomial coefficient link Shannon entropy:number possible permutation \\(n\\) items \\(K\\) distinct types, \\(n_1\\) type 1, \\(n_2\\) type 2 , given multinomial coefficient\n\\[\nW = \\binom{n}{n_1, \\ldots, n_K} = \\frac {n!}{n_1! \\times n_2! \\times\\ldots \\times n_K! }\n\\]\n\\(\\sum_{k=1}^K n_k = n\\) \\(K \\leq n\\).Now recall Moivre-Sterling formula large \\(n\\) allow approximate\nfactorial \n\\[\n\\log n! \\approx  n \\log n  -n\n\\]\n\\[\n\\begin{split}\n\\log W &= \\log \\binom{n}{n_1, \\ldots, n_K} \\\\\n       &= \\log n! - \\sum_{k=1}^K \\log n_k!\\\\\n& \\approx    n \\log n  -n - \\sum_{k=1}^K (n_k \\log n_k  -n_k) \\\\\n& = n \\log n - \\sum_{k=1}^K n_k \\log n_k\\\\\n& = \\sum_{k=1}^K n_k \\log n - \\sum_{k=1}^K n_k \\log n_k\\\\\n& = - n \\sum_{k=1}^K \\frac{n_k}{n} \\log\\left( \\frac{n_k}{n} \\right)\\\\\n\\end{split}\n\\]\nthus\n\\[\n\\begin{split}\n\\frac{1}{n}\\log \\binom{n}{n_1, \\ldots, n_K} &\\approx - \\sum_{k=1}^K \\hat{p}_k \\log \\hat{p}_k \\\\\n&=H(\\hat{P})\n\\end{split}\n\\]\n\\(\\hat{P}\\) empirical categorical distribution \\(\\hat{p}_k = \\frac{n_k}{n}\\).combinatorial derivation Shannon entropy credited statistics Wallis (1962)\nalready used century earlier Boltzmann (1877) discovered work statistical mechanics (recall \\(S = k_b \\log W\\) \nBoltzmann entropy ).","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"differential-entropy","chapter":"2 From entropy to maximum likelihood","heading":"2.2.4 Differential entropy","text":"Shannon entropy defined discrete random variables.Differential Entropy results applying \ndefinition Shannon entropy continuous random variable \\(x\\) density \\(p(x)\\):\n\\[\nH(P) = -\\text{E}_P(\\log p(x)) = - \\int_x p(x) \\log p(x) \\, dx\n\\]\nDespite essentially formula different name justified differential entropy exhibits different properties compared Shannon entropy, logarithm taken density\ncontrast probability can assume values larger one.\nconsequence, differential entropy bounded zero can negative.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"entropy-examples","chapter":"2 From entropy to maximum likelihood","heading":"2.2.5 Entropy examples","text":"Models single parameterExample 2.2  Consider uniform distribution \\(U(0, )\\) \\(>0\\), support \\(0\\) \\(\\) density \\(p(x) = 1/\\). \\(-\\int_0^p(x) \\log p(x) dx =- \\int_0^\\frac{1}{} \\log(\\frac{1}{}) dx = \\log \\)\ndifferential entropy \n\\[H( U(0, ) ) =  \\log \\,.\\]\nNote \\(< 1\\) differential entropy negative.Models multiple parametersExample 2.3  Discrete uniform distribution \\(U_K\\): let \\(p_1=p_2= \\ldots = p_K = \\frac{1}{K}\\).\n\\[H(U_K) = - \\sum_{k=1}^{K}\\frac{1}{K} \\log\\left(\\frac{1}{K}\\right) = \\log K\\]Note \\(\\log K\\) largest value Shannon entropy can assume\n\\(K\\) classes indicates maximum spread.Example 2.4  Concentrated probability mass: let\n\\(p_1=1\\) \\(p_2=p_3=\\ldots=p_K=0\\).\nUsing \\(0\\times\\log(0)=0\\) obtain Shannon entropy\n\\[H(P) = 1\\times\\log(1) + 0\\times\\log(0) + \\dots = 0\\]Note 0 smallest value Shannon entropy can assume \ncorresponds maximum concentration.Example 2.5  log density univariate normal \\(N(\\mu, \\sigma^2)\\) distribution\n\\(\\log p(x |\\mu, \\sigma^2) = -\\frac{1}{2} \\left( \\log(2\\pi\\sigma^2) + \\frac{(x-\\mu)^2}{\\sigma^2} \\right)\\) \\(\\sigma^2 > 0\\).\ncorresponding differential entropy \\(\\text{E}((x-\\mu)^2) = \\sigma^2\\)\n\\[\n\\begin{split}\nH(P) & = -\\text{E}\\left( \\log p(x |\\mu, \\sigma^2) \\right)\\\\\n& = \\frac{1}{2} \\left( \\log(2 \\pi \\sigma^2)+1\\right) \\,. \\\\\n\\end{split}\n\\]\nInterestingly, \\(H(P)\\) depends variance mean, entropy grows variance. Note \\(\\sigma^2 < 1/(2 \\pi e) \\approx 0.0585\\) differential entropy negative.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"maximum-entropy-principle-to-characterise-distributions","chapter":"2 From entropy to maximum likelihood","heading":"2.2.6 Maximum entropy principle to characterise distributions","text":"maximum Shannon entropy differential entropy useful characterise distributions:seen examples , large entropy implies distribution spread whereas small entropy means distribution concentrated.Correspondingly, maximum entropy distributions can considered minimally informative random variable. higher entropy spread (hence uninformative) distribution.discrete\nuniform distribution maximum entropy distribution among discrete distributions.discrete\nuniform distribution maximum entropy distribution among discrete distributions.maximum entropy distribution continuous random variable support \\([-\\infty, \\infty]\\) specific mean variance normal distributionthe maximum entropy distribution continuous random variable support \\([-\\infty, \\infty]\\) specific mean variance normal distributionthe maximum entropy distribution among continuous distributions supported \\([0, \\infty]\\) specified mean exponential distribution.maximum entropy distribution among continuous distributions supported \\([0, \\infty]\\) specified mean exponential distribution.Using maximum entropy characterise maximally uninformative distributions advocated E.T. Jaynes (also proposed use maximum entropy context finding Bayesian priors). maximum entropy principle statistical physics goes back Boltzmann.list maximum entropy distribution given :\nhttps://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution .Many distributions commonly used statistical modelling exponential families.\nIntriguingly, distribution maximum entropy distributions, close link\nprinciple maximum entropy common model choices statistics machine learning.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"cross-entropy-and-kullback-leibler-divergence","chapter":"2 From entropy to maximum likelihood","heading":"2.3 Cross-entropy and Kullback-Leibler divergence","text":"","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"definition-of-cross-entropy","chapter":"2 From entropy to maximum likelihood","heading":"2.3.1 Definition of cross-entropy","text":"modify definition Shannon entropy (differential entropy) \nexpectation \nlog-density (say \\(g(x)\\) distribution \\(G\\)) taken regard different distribution \\(F\\)\narrive cross-entropy\n\\[\nH(F, G) =-\\text{E}_F\\left( \\log g(x)  \\right)\n\\]\nThus, cross-entropy functional two distributions \\(F\\) \\(G\\).discrete distributions \\(F\\) \\(G\\) class probabilities\n\\(f_1, \\ldots, f_K\\) \\(g_1, \\ldots, g_K\\) cross-entropy computed weighted sum\n\\[H(F, G) = - \\sum_{k=1}^{K} f_k \\log g_k\\]\ncontinuous distributions \\(F\\) \\(G\\) densities \\(f(x)\\) \\(g(x)\\) compute integral\n\\[H(F, G) =- \\int_x f(x) \\log g(x) \\, dx\\]Note thatCross-entropy symmetric regard \\(F\\) \\(G\\), \nexpectation taken reference \\(F\\).construction distributions identical cross-entropy reduces Shannon differential entropy, respectively: \\(H(F, F) = H(F)\\).crucial property cross-entropy \\(H(F, G)\\) bounded entropy \\(F\\), therefore\n\\[\nH(F, G) \\geq H(F)\n\\]\nequality \\(F=G\\). known Gibbs’ inequality.Equivalently can write\n\\[\n\\underbrace{H(F, G)-H(F)}_{\\text{relative entropy}} \\geq 0\n\\]\nfact, recalibrated cross-entropy (known KL divergence relative entropy) turns fundamental cross-entropy entropy. studied detail next section.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"definition-of-kl-divergence","chapter":"2 From entropy to maximum likelihood","heading":"2.3.2 Definition of KL divergence","text":"Also known relative entropy discrimination information.relative entropy measures divergence\ndistribution \\(G\\) distribution \\(F\\) defined \n\\[\n\\begin{split}\nD_{\\text{KL}}(F,G) &= \\text{E}_F\\log\\left(\\frac{dF}{dG}\\right) \\\\\n& = \\text{E}_F\\log\\left(\\frac{f(x)}{g(x)}\\right) \\\\\n& =\n\\underbrace{-\\text{E}_F(\\log g(x))}_{\\text{cross-entropy}} -\n(\\underbrace{-\\text{E}_F (\\log f(x))  }_\\text{(differential) entropy})  \\\\\n& = H(F, G)-H(F) \\\\\n\\end{split}\n\\]\\(D_{\\text{KL}}(F, G)\\) measures amount information lost \\(G\\) used approximate \\(F\\).\\(F\\) \\(G\\) identical (information lost) \\(D_{\\text{KL}}(F,G)=0\\).(Note: “divergence” measures dissimilarity probability distributions. type divergence related confused divergence (div) used vector analysis.)use term “divergence” rather “distance” serves emphasise distributions \\(F\\) \\(G\\) interchangeable \\(D_{\\text{KL}}(F, G)\\).exist various notations KL divergence literature.\nuse \\(D_{\\text{KL}}(F, G)\\) often find well \\(\\text{KL}(F || G)\\)\n\\(^{KL}(F; G)\\).authors (e.g. Efron) call twice KL divergence \\(2 D_{\\text{KL}}(F, G) = D(F, G)\\) deviance \\(G\\) \\(F\\).","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"properties-of-kl-divergence","chapter":"2 From entropy to maximum likelihood","heading":"2.3.3 Properties of KL divergence","text":"\\(D_{\\text{KL}}(F, G) \\neq D_{\\text{KL}}(G, F)\\), .e. KL divergence symmetric, \\(F\\) \\(G\\) interchanged.\\(D_{\\text{KL}}(F, G) = 0\\) \\(F=G\\), .e., KL divergence zero \\(F\\) \\(G\\) identical.\\(D_{\\text{KL}}(F, G)\\geq 0\\), proof via Jensen’s inequality.\\(D_{\\text{KL}}(F, G)\\) remains invariant coordinate transformations,\n.e. invariant geometric quantity.Note KL divergence expectation taken ratio densities (ratio probabilities discrete random variables). creates transformation invariance.details proofs properties 3 4 see Worksheet E1.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"origin-of-kl-divergence-and-application-in-statistics","chapter":"2 From entropy to maximum likelihood","heading":"2.3.4 Origin of KL divergence and application in statistics","text":"Historically, physics (negative) relative entropy discovered Boltzmann (1878). 2\nstatistics information theory introduced Kullback Leibler (1951). 3In statistics typical roles distribution \\(F\\) \\(G\\) \\(D_{\\text{KL}}(F, G)\\) :\\(F\\) (unknown) underlying true model data generating process\\(G\\) approximating model (typically distribution family indexed parameters)Optimising (.e. minimising) KL divergence regard \\(G\\) amounts approximation optimising regard \\(F\\) imputation. Later see leads method maximum likelihood Bayesian learning, respectively.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"cross-entropy-and-kl-divergence-examples","chapter":"2 From entropy to maximum likelihood","heading":"2.3.5 Cross-entropy and KL divergence examples","text":"Models single parameterExample 2.6  KL divergence two Bernoulli distributions \\(\\text{Ber}(\\theta_1)\\) \\(\\text{Ber}(\\theta_2)\\):“success” probabilities two distributions \\(\\theta_1\\) \\(\\theta_2\\),\nrespectively, complementary “failure” probabilities \\(1-\\theta_1\\) \\(1-\\theta_2\\).\nget KL divergence\n\\[\nD_{\\text{KL}}(\\text{Ber}(\\theta_1), \\text{Ber}(\\theta_2))=\\theta_1 \\log\\left( \\frac{\\theta_1}{\\theta_2}\\right) + (1-\\theta_1) \\log\\left(\\frac{1-\\theta_1}{1-\\theta_2}\\right)\n\\]Example 2.7  KL divergence two univariate normals different means common variance:Assume \\(F_{\\text{ref}}=N(\\mu_{\\text{ref}},\\sigma^2)\\) \\(F=N(\\mu,\\sigma^2)\\).get\n\\[D_{\\text{KL}}(N(\\mu_{\\text{ref}}, \\sigma^2),   N(\\mu, \\sigma^2)  )=\\frac{1}{2} \\left(\\frac{(\\mu-\\mu_{\\text{ref}})^2}{\\sigma^2}\\right)\\]Models multiple parametersExample 2.8  KL divergence two categorical distributions \\(K\\) classes:\\(P=\\text{Cat}(\\boldsymbol p)\\) \\(Q=\\text{Cat}(\\boldsymbol q)\\) corresponding\nprobabilities \\(p_1,\\dots,p_K\\) \\(q_1,\\dots,q_K\\) satisfying \\(\\sum_{=1}^K p_i =1\\) \\(\\sum_{=1}^K q_i = 1\\) get:\\[\\begin{equation*}\nD_{\\text{KL}}(P, Q)=\\sum_{=1}^K p_i\\log\\left(\\frac{p_i}{q_i}\\right)\n\\end{equation*}\\]explicit \\(K-1\\) parameters categorical distribution can also write\n\\[\\begin{equation*}\nD_{\\text{KL}}(P, Q)=\\sum_{=1}^{K-1} p_i\\log\\left(\\frac{p_i}{q_i}\\right)  + p_K\\log\\left(\\frac{p_K}{q_K}\\right)\n\\end{equation*}\\]\n\\(p_K=\\left(1- \\sum_{=1}^{K-1} p_i\\right)\\) \n\\(q_K=\\left(1- \\sum_{=1}^{K-1} q_i\\right)\\).Example 2.9  Cross-entropy two normals:Assume \\(F_{\\text{ref}}=N(\\mu_{\\text{ref}},\\sigma^2_{\\text{ref}})\\) \\(F=N(\\mu,\\sigma^2)\\).\ncross-entropy \\(H(F_{\\text{ref}}, F)\\) \n\\[\n\\begin{split}\nH(F_{\\text{ref}}, F) &=  -\\text{E}_{F_{\\text{ref}}} \\left( \\log p(x |\\mu, \\sigma^2) \\right)\\\\\n&=  \\frac{1}{2}  \\text{E}_{F_{\\text{ref}}} \\left(  \\log(2\\pi\\sigma^2)  + \\frac{(x-\\mu)^2}{\\sigma^2} \\right) \\\\\n&= \\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 }\n+\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2}  +\\log(2 \\pi \\sigma^2) \\right)  \\\\\n\\end{split}\n\\]\nusing \\(\\text{E}_{F_{\\text{ref}}} ((x-\\mu)^2) = (\\mu_{\\text{ref}}-\\mu)^2 + \\sigma^2_{\\text{ref}}\\).Example 2.10  \\(\\mu_{\\text{ref}} = \\mu\\) \\(\\sigma^2_{\\text{ref}} = \\sigma^2\\)\ncross-entropy \\(H(F_{\\text{ref}},F)\\) Example 2.9\ndegenerates differential entropy\n\\(H(F_{\\text{ref}}) = \\frac{1}{2} \\left(\\log( 2 \\pi \\sigma^2_{\\text{ref}}) +1 \\right)\\).Example 2.11  KL divergence two univariate normals different means variances:Assume \\(F_{\\text{ref}}=N(\\mu_{\\text{ref}},\\sigma^2_{\\text{ref}})\\) \\(F=N(\\mu,\\sigma^2)\\).\n\n\\[\n\\begin{split}\nD_{\\text{KL}}(F_{\\text{ref}},F) &= H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) \\\\\n&= \\frac{1}{2} \\left(   \\frac{(\\mu-\\mu_{\\text{ref}})^2}{\\sigma^2}  + \\frac{\\sigma_{\\text{ref}}^2}{\\sigma^2}\n-\\log\\left(\\frac{\\sigma_{\\text{ref}}^2}{\\sigma^2}\\right)-1  \n   \\right) \\\\\n\\end{split}\n\\]variances equal recover previous Example 2.7\nspecial case.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"local-quadratic-approximation-and-expected-fisher-information","chapter":"2 From entropy to maximum likelihood","heading":"2.4 Local quadratic approximation and expected Fisher information","text":"","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"definition-of-expected-fisher-information","chapter":"2 From entropy to maximum likelihood","heading":"2.4.1 Definition of expected Fisher information","text":"KL information measures divergence two distributions.\nmay thus use relative entropy measure divergence two distributions family, separated parameter space small \\(\\boldsymbol \\varepsilon\\).First, consider \\(D_{\\text{KL}}(F_{\\boldsymbol \\theta}, F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon}) = \\text{E}_{F_{\\boldsymbol \\theta}}\\left( \\log f(\\boldsymbol x| \\boldsymbol \\theta) - \\log f(\\boldsymbol x| \\boldsymbol \\theta+\\boldsymbol \\varepsilon) \\right) = h(\\boldsymbol \\varepsilon)\\)\n\\(\\boldsymbol \\theta\\) kept constant \\(\\boldsymbol \\varepsilon\\) varying. properties\nKL divergence know \\(D_{\\text{KL}}(F_{\\boldsymbol \\theta}, F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon})\\geq 0\\)\nbecomes zero \\(\\boldsymbol \\varepsilon=0\\). Thus, construction function\n\\(h(\\boldsymbol \\varepsilon)\\) achieves true minimum \\(h(0)=0\\) \\(\\boldsymbol \\varepsilon=0\\),\nvanishing gradient \\(\\nabla h(0)=0\\) positive definite Hessian matrix \\(\\nabla \\nabla^T h(0)\\). Therfore can approximate quadratic function around \\(\\boldsymbol \\varepsilon=0\\):\n\\[\nh(\\boldsymbol \\varepsilon) \\approx \\frac{1}{2} \\boldsymbol \\varepsilon^T \\, \\nabla \\nabla^T h(0) \\, \\boldsymbol \\varepsilon\n\\]\nHessian matrix \\(\\nabla \\nabla^T h(0)\\) computed \\(\\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta) = -\\text{E}_{F_{\\boldsymbol \\theta}} \\nabla \\nabla^T \\log f(\\boldsymbol x| \\boldsymbol \\theta)\\) negative expected Hessian matrix log-density \\(\\boldsymbol \\theta\\). called expected Fisher information \\(\\boldsymbol \\theta\\).\nKL divergence can thus locally approximated \n\\[\nD_{\\text{KL}}(F_{\\boldsymbol \\theta}, F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon})\\approx \\frac{1}{2} \\boldsymbol \\varepsilon^T  \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta) \\boldsymbol \\varepsilon\n\\]second possibility may also vary first argument KL divergence.\nstraightforward show leads approximation second order\n\\(\\boldsymbol \\varepsilon\\):\n\\[\n\\begin{split}\nD_{\\text{KL}}(F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon}, F_{\\boldsymbol \\theta})\n&\\approx \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)\\, \\boldsymbol \\varepsilon\\\\\n\\end{split}\n\\]Computing expected Fisher information involves observed data,\npurely property model, precisely \nmodel family indexed \\(\\boldsymbol \\theta\\).\nnext Chapter study related quantity,\nobserved Fisher information contrast function observed data.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"expected-fisher-information-for-a-set-of-independent-and-identically-distributed-random-variables","chapter":"2 From entropy to maximum likelihood","heading":"2.4.2 Expected Fisher information for a set of independent and identically distributed random variables","text":"Furthermore, can also compute expected Fisher information set \niid random variables.Assume random variable \\(x \\sim F_{\\boldsymbol \\theta}\\) log-density \\(\\log f(x| \\boldsymbol \\theta)\\)\nexpected Fisher information \\(\\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)\\).\nexpected Fisher information \\(\\boldsymbol I_{x_1, \\ldots, x_n}^{\\text{Fisher}}(\\boldsymbol \\theta)\\)\nset iid random variables \\(x_1, \\ldots, x_n \\sim F_{\\boldsymbol \\theta}\\)\ncomputed joint log-density \\(\\log f(x_1, \\ldots, x_n) = \\sum_{}^n \\log f(x_i| \\boldsymbol \\theta)\\).\nyields\n\\[\n\\begin{split}\n\\boldsymbol I_{x_1, \\ldots, x_n}^{\\text{Fisher}}(\\boldsymbol \\theta) &= -\\text{E}_{F_{\\boldsymbol \\theta}} \\nabla \\nabla^T  \\sum_{}^n \\log f(x_i| \\boldsymbol \\theta)\\\\\n&= \\sum_{}^n  \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta) = n  \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)\\\\\n\\end{split}\n\\]\nHence, expected Fisher information set \\(n\\) iid random variable \nexpected Fisher information single variable times \\(n\\).","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"expected-fisher-information-examples","chapter":"2 From entropy to maximum likelihood","heading":"2.5 Expected Fisher information examples","text":"","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"models-with-a-single-parameter","chapter":"2 From entropy to maximum likelihood","heading":"2.5.1 Models with a single parameter","text":"Example 2.12  Expected Fisher information Bernoulli distribution:log-probability mass function Bernoulli \\(\\text{Ber}(\\theta)\\) distribution \n\\[\n\\log p(x | \\theta) = x \\log(\\theta) + (1-x) \\log(1-\\theta)\n\\]\n\\(\\theta\\) probability “success”.\nsecond derivative regard parameter \\(\\theta\\) \n\\[\n\\frac{d^2}{d\\theta^2} \\log p(x | \\theta)  =  -\\frac{x}{\\theta^2}-  \\frac{1-x}{(1-\\theta)^2}\n\\]\nSince \\(\\text{E}(x) = \\theta\\) get Fisher information\n\\[\n\\begin{split}\n^{\\text{Fisher}}(\\theta) & = -\\text{E}\\left(\\frac{d^2}{d\\theta^2} \\log p(x | \\theta)  \\right)\\\\\n                           &= \\frac{\\theta}{\\theta^2}+  \\frac{1-\\theta}{(1-\\theta)^2} \\\\\n                            &= \\frac{1}{\\theta(1-\\theta)}\\\\\n\\end{split}\n\\]Example 2.13  Quadratic approximations KL divergence two Bernoulli distributions:Example 2.6 KL divergence\n\\[\nD_{\\text{KL}}\\left (\\text{Ber}(\\theta_1), \\text{Ber}(\\theta_2) \\right)=\\theta_1 \\log\\left( \\frac{\\theta_1}{\\theta_2}\\right) + (1-\\theta_1) \\log\\left(\\frac{1-\\theta_1}{1-\\theta_2}\\right)\n\\]\n\nExample 2.12 corresponding expected Fisher information.quadratic approximation implies \n\\[\nD_{\\text{KL}}\\left( \\text{Ber}(\\theta), \\text{Ber}(\\theta + \\varepsilon) \\right) \\approx \\frac{\\varepsilon^2}{2}  ^{\\text{Fisher}}(\\theta) =  \\frac{\\varepsilon^2}{2 \\theta (1-\\theta)}\n\\]\nalso \n\\[\nD_{\\text{KL}}\\left( \\text{Ber}(\\theta+\\varepsilon), \\text{Ber}(\\theta) \\right) \\approx \\frac{\\varepsilon^2}{2} ^{\\text{Fisher}}(\\theta) =  \\frac{\\varepsilon^2}{2 \\theta (1-\\theta)}\n\\]Worksheet E1 verified using second order Taylor series applied KL divergence.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"models-with-multiple-parameters","chapter":"2 From entropy to maximum likelihood","heading":"2.5.2 Models with multiple parameters","text":"Example 2.14  Expected Fisher information normal distribution \\(N(\\mu, \\sigma^2)\\).log-density \n\\[\n\\log f(x | \\mu, \\sigma^2) = -\\frac{1}{2} \\log(\\sigma^2)\n-\\frac{1}{2 \\sigma^2} (x-\\mu)^2 - \\frac{1}{2}\\log(2 \\pi)\n\\]\ngradient respect \\(\\mu\\) \\(\\sigma^2\\) (!) vector\n\\[\n\\nabla \\log f(x | \\mu, \\sigma^2) =\n\\begin{pmatrix}\n\\frac{1}{\\sigma^2} (x-\\mu) \\\\\n- \\frac{1}{2 \\sigma^2} + \\frac{1}{2 \\sigma^4} (x- \\mu)^2 \\\\\n\\end{pmatrix}\n\\]\nHint calculating gradient: replace \\(\\sigma^2\\) \\(v\\) take partial derivative regard \\(v\\), substitute back.corresponding Hessian matrix \n\\[\n\\nabla \\nabla^T \\log f(x | \\mu, \\sigma^2) =\n\\begin{pmatrix}\n-\\frac{1}{\\sigma^2} & -\\frac{1}{\\sigma^4} (x-\\mu)\\\\\n-\\frac{1}{\\sigma^4} (x-\\mu) &  \\frac{1}{2\\sigma^4} - \\frac{1}{\\sigma^6}(x- \\mu)^2 \\\\\n\\end{pmatrix}\n\\]\n\\(\\text{E}(x) = \\mu\\) \\(\\text{E}(x-\\mu) =0\\).\nFurthermore, \\(\\text{E}( (x-\\mu)^2 ) =\\sigma^2\\) see \n\\(\\text{E}\\left(\\frac{1}{\\sigma^6}(x- \\mu)^2\\right) = \\frac{1}{\\sigma^4}\\). Therefore\nexpected Fisher information matrix negative expected Hessian matrix \n\\[\n\\boldsymbol ^{\\text{Fisher}}\\left(\\mu,\\sigma^2\\right) = \\begin{pmatrix} \\frac{1}{\\sigma^2} & 0 \\\\ 0 & \\frac{1}{2\\sigma^4} \\end{pmatrix}\n\\]Example 2.15  Expected Fisher information categorical distribution:log-probability mass function categorical distribution\n\\(K\\) classes \\(K-1\\) free parameters \\(\\pi_1, \\ldots, \\pi_{K-1}\\) \n\\[\n\\begin{split}\n\\log p(\\boldsymbol x| \\pi_1, \\ldots, \\pi_{K-1}  ) & =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + x_K \\log \\pi_K \\\\\n& =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_k  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\\\\n\\end{split}\n\\]log-probability\nmass function compute Hessian matrix second order partial derivatives\n\\(\\nabla \\nabla^T \\log p(\\boldsymbol x| \\pi_1, \\ldots, \\pi_{K-1} )\\) regard\n\\(\\pi_1, \\ldots, \\pi_{K-1}\\):diagonal entries Hessian matrix (\\(=1, \\ldots, K-1\\)) \n\\[\n\\frac{\\partial^2}{\\partial \\pi_i^2} \\log p(\\boldsymbol x|\\pi_1, \\ldots, \\pi_{K-1}) =\n-\\frac{x_i}{\\pi_i^2}-\\frac{x_K}{\\pi_K^2}\n\\]diagonal entries Hessian matrix (\\(=1, \\ldots, K-1\\)) \n\\[\n\\frac{\\partial^2}{\\partial \\pi_i^2} \\log p(\\boldsymbol x|\\pi_1, \\ldots, \\pi_{K-1}) =\n-\\frac{x_i}{\\pi_i^2}-\\frac{x_K}{\\pi_K^2}\n\\]-diagonal entries (\\(j=1, \\ldots, K-1\\) \\(j \\neq \\))\n\\[\n\\frac{\\partial^2}{\\partial \\pi_i \\partial \\pi_j} \\log p(\\boldsymbol x|\\pi_1, \\ldots, \\pi_{K-1}) =\n-\\frac{ x_K}{\\pi_K^2}\n\\]-diagonal entries (\\(j=1, \\ldots, K-1\\) \\(j \\neq \\))\n\\[\n\\frac{\\partial^2}{\\partial \\pi_i \\partial \\pi_j} \\log p(\\boldsymbol x|\\pi_1, \\ldots, \\pi_{K-1}) =\n-\\frac{ x_K}{\\pi_K^2}\n\\]Recalling \\(\\text{E}(x_i) = \\pi_i\\) obtain expected Fisher information\nmatrix categorical distribution \n\\[\n\\begin{split}\n\\boldsymbol ^{\\text{Fisher}}\\left( \\pi_1, \\ldots, \\pi_{K-1}  \\right) &=\n-\\text{E}\\left( \\nabla \\nabla^T \\log p(\\boldsymbol x| \\pi_1, \\ldots, \\pi_{K-1}) \\right) \\\\\n& =\n\\begin{pmatrix}\n\\frac{1}{\\pi_1} + \\frac{1}{\\pi_K} & \\cdots & \\frac{1}{\\pi_K} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{\\pi_K} & \\cdots & \\frac{1}{\\pi_{K-1}} + \\frac{1}{\\pi_K} \\\\\n\\end{pmatrix}\\\\\n& = \\text{Diag}\\left( \\frac{1}{\\pi_1} , \\ldots,  \\frac{1}{\\pi_{K-1}}   \\right) + \\frac{1}{\\pi_K} \\boldsymbol 1\\\\\n\\end{split}\n\\]\\(K=2\\) \\(\\pi_1=\\theta\\) reduces expected Fisher information \nBernoulli variable, see Example 2.12.\n\\[\n\\begin{split}\n^{\\text{Fisher}}(\\theta) & =  \\left(\\frac{1}{\\theta} + \\frac{1}{1-\\theta} \\right) \\\\\n  &= \\frac{1}{\\theta (1-\\theta)} \\\\\n\\end{split}\n\\]Example 2.16  Quadratic approximation KL divergence categorical distribution \nNeyman Pearson divergence:now consider local approximation KL divergence \\(D_{\\text{KL}}(P, Q)\\) \ncategorical distribution \\(P=\\text{Cat}(\\boldsymbol p)\\) probabilities \\(\\boldsymbol p=(p_1, \\ldots, p_K)^T\\) categorical distribution \\(Q=\\text{Cat}(\\boldsymbol q)\\) probabilities \\(\\boldsymbol q= (q_1, \\ldots, q_K)^T\\).Example 2.8 already know KL divergence\n\nExample 2.15 corresponding expected Fisher information.First, keep \\(P\\) fixed assume \\(Q\\) perturbed version \\(P\\) \\(\\boldsymbol q= \\boldsymbol p+\\boldsymbol \\varepsilon\\).\nNote perturbations \\(\\boldsymbol \\varepsilon=(\\varepsilon_1, \\ldots, \\varepsilon_K)^T\\) satisfy\n\\(\\sum_{k=1}^K \\varepsilon_k = 0\\) \\(\\sum_{k=1}^K p_i=1\\) \\(\\sum_{k=1}^K q_i=1\\).\nThus \\(\\varepsilon_K = -\\sum_{k=1}^{K-1} \\varepsilon_k\\). \n\\[\n\\begin{split}\nD_{\\text{KL}}(P, Q=P+\\varepsilon) &  = D_{\\text{KL}}(\\text{Cat}(\\boldsymbol p), \\text{Cat}(\\boldsymbol p+\\boldsymbol \\varepsilon)) \\\\\n&  \\approx \\frac{1}{2} (\\varepsilon_1, \\ldots,  \\varepsilon_{K-1}) \\,\n\\boldsymbol ^{\\text{Fisher}}\\left( p_1, \\ldots, p_{K-1}  \\right)\n\\begin{pmatrix} \\varepsilon_1 \\\\ \\vdots \\\\  \\varepsilon_{K-1}\\\\\n\\end{pmatrix} \\\\\n&= \\frac{1}{2} \\left( \\sum_{k=1}^{K-1} \\frac{\\varepsilon_k^2}{p_k}   + \\frac{ \\left(\\sum_{k=1}^{K-1} \\varepsilon_k\\right)^2}{p_K} \\right)  \\\\\n&= \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{\\varepsilon_k^2}{p_k}\\\\\n&= \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{(p_k-q_k)^2}{p_k}\\\\\n& = \\frac{1}{2} D_{\\text{Neyman}}(P, Q)\\\\\n\\end{split}\n\\]\nSimilarly, keep \\(Q\\) fixed consider \\(P\\) disturbed version \\(Q\\) get\n\\[\n\\begin{split}\nD_{\\text{KL}}(P=Q+\\varepsilon, Q) &  =D_{\\text{KL}}(\\text{Cat}(\\boldsymbol q+\\boldsymbol \\varepsilon), \\text{Cat}(\\boldsymbol q)) \\\\\n&\\approx \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{(p_k-q_k)^2}{q_k}\\\\\n&= \\frac{1}{2} D_{\\text{Pearson}}(P, Q)\n\\end{split}\n\\]\nNote approximations divide probabilities distribution \nkept fixed.Note appearance Pearson \\(\\chi^2\\) divergence Neyman \\(\\chi^2\\) divergence . , like KL divergence, part family \\(f\\)-divergences. Neyman \\(\\chi^2\\)\ndivergence also known reverse Pearson divergence \\(D_{\\text{Neyman}}(P, Q) = D_{\\text{Pearson}}(Q, P)\\).","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"from-entropy-learning-to-maximum-likelihood","chapter":"2 From entropy to maximum likelihood","heading":"2.6 From entropy learning to maximum likelihood","text":"","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"the-relative-entropy-between-true-model-and-approximating-model","chapter":"2 From entropy to maximum likelihood","heading":"2.6.1 The relative entropy between true model and approximating model","text":"Assume observations \\(D = \\{x_1, \\ldots, x_n\\}\\). data sampled \\(F\\), true unknown data generating distribution. also specify family distributions \\(G_{\\boldsymbol \\theta}\\)\nindexed \\(\\boldsymbol \\theta\\) approximate \\(F\\).relative entropy \\(D_{\\text{KL}}(F,G_{\\boldsymbol \\theta})\\) measures divergence approximation \\(G_{\\boldsymbol \\theta}\\)\nunknown true model \\(F\\). can written :\n\\[\n\\begin{split}\nD_{\\text{KL}}(F,G_{\\boldsymbol \\theta}) &= H(F,G_{\\boldsymbol \\theta}) - H(F) \\\\\n&= \\underbrace{- \\text{E}_{F}\\log g_{\\boldsymbol \\theta}(x)}_{\\text{cross-entropy}}\n-(\\underbrace{-\\text{E}_{F}\\log f(x)}_{\\text{entropy $F$, depend $\\boldsymbol \\theta$}})\\\\\n\\end{split}\n\\]However, since know \\(F\\) actually compute divergence. Nonetheless, may use\nempirical distribution \\(\\hat{F}_n\\) — function observed data — approximation \\(F\\), way arrive approximation \\(D_{\\text{KL}}(F,G_{\\boldsymbol \\theta})\\) becomes accurate growing sample size.Recall “Law Large Numbers” :empirical distribution \\(\\hat{F}_n\\) based observed data \\(D=\\{x_1, \\ldots, x_n\\}\\) converges\nstrongly (almost surely) true underlying distribution \\(F\\) \\(n \\rightarrow \\infty\\):\n\\[\n\\hat{F}_n\\overset{. s.}{\\} F\n\\]empirical distribution \\(\\hat{F}_n\\) based observed data \\(D=\\{x_1, \\ldots, x_n\\}\\) converges\nstrongly (almost surely) true underlying distribution \\(F\\) \\(n \\rightarrow \\infty\\):\n\\[\n\\hat{F}_n\\overset{. s.}{\\} F\n\\]Correspondingly, \\(n \\rightarrow \\infty\\) average \\(\\text{E}_{\\hat{F}_n}(h(x)) = \\frac{1}{n} \\sum_{=1}^n h(x_i)\\) converges expectation \\(\\text{E}_{F}(h(x))\\).Correspondingly, \\(n \\rightarrow \\infty\\) average \\(\\text{E}_{\\hat{F}_n}(h(x)) = \\frac{1}{n} \\sum_{=1}^n h(x_i)\\) converges expectation \\(\\text{E}_{F}(h(x))\\).Hence, large sample size \\(n\\) can approximate cross-entropy result KL divergence. cross-entropy \\(H(F, G_{\\boldsymbol \\theta})\\) approximated empirical cross-entropy expectation taken regard \\(\\hat{F}_n\\) rather \\(F\\):\n\\[\n\\begin{split}\nH(F, G_{\\boldsymbol \\theta}) & \\approx H(\\hat{F}_n, G_{\\boldsymbol \\theta}) \\\\\n                  & = - \\text{E}_{\\hat{F}_n} (\\log g(x|\\boldsymbol \\theta))  \\\\\n                  & = -\\frac{1}{n} \\sum_{=1}^n \\log g(x_i | \\boldsymbol \\theta) \\\\\n                  & = -\\frac{1}{n} l_n ({\\boldsymbol \\theta}| D)\n\\end{split}\n\\]\nturns equal negative log-likelihood standardised sample size \\(n\\)! words, log-likelihood negative empirical cross-entropy multiplied sample size \\(n\\).link multinomial coefficient Shannon entropy (Example 2.1) already know large sample size\n\\[\nH(\\hat{F}) \\approx \\frac{1}{n} \\log \\binom{n}{n_1, \\ldots, n_K}\n\\]KL divergence \\(D_{\\text{KL}}(F,G_{\\boldsymbol \\theta})\\) can therefore approximated \n\\[\nD_{\\text{KL}}(F,G_{\\boldsymbol \\theta}) \\approx -\\frac{1}{n} \\left( \\log \\binom{n}{n_1, \\ldots, n_K} + l_n ({\\boldsymbol \\theta}| D)  \\right)\n\\]\nThus, KL divergence obtain just log-likelihood (cross-entropy part) also multiplicity factor taking account possible orderings data (entropy part).","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"minimum-kl-divergence-and-maximum-likelihood","chapter":"2 From entropy to maximum likelihood","heading":"2.6.2 Minimum KL divergence and maximum likelihood","text":"knew \\(F\\) simply minimise \\(D_{\\text{KL}}(F, G_{\\boldsymbol \\theta})\\) find particular model \\(G_{\\boldsymbol \\theta}\\) closest true model. Equivalently, minimise cross-entropy \\(H(F, G_{\\boldsymbol \\theta})\\).\nHowever, since actually don’t know \\(F\\) possible.However, large sample size \\(n\\) empirical distribution \\(\\hat{F}_n\\)\ngood approximation \\(F\\), can use results previous section.\nThus, instead minimising KL divergence\n\\(D_{\\text{KL}}(F, G_{\\boldsymbol \\theta})\\) simply minimise \\(H(\\hat{F}_n, G_{\\boldsymbol \\theta})\\) \nmaximising log-likelihood \\(l_n ({\\boldsymbol \\theta}| D)\\).Conversely, implies maximising likelihood regard \\(\\boldsymbol \\theta\\) equivalent ( asymptotically large \\(n\\)) minimising KL divergence approximating model unknown true model!\\[\n\\begin{split}\n\\hat{\\boldsymbol \\theta}^{ML} &= \\underset{\\boldsymbol \\theta}{\\arg \\max}\\,\\, l_n(\\boldsymbol \\theta| D) \\\\\n&= \\underset{\\boldsymbol \\theta}{\\arg \\min}\\,\\, H(\\hat{F}_n, G_{\\boldsymbol \\theta}) \\\\\n&\\approx \\underset{\\boldsymbol \\theta}{\\arg \\min}\\,\\, D_{\\text{KL}}(F, G_{\\boldsymbol \\theta}) \\\\\n\\end{split}\n\\]Therefore, reasoning behind method maximum likelihood minimises large sample approximation KL divergence candidate model \\(G_{\\boldsymbol \\theta}\\) unkown true model \\(F\\).\nwords, maximum likelihood estimators minimum empirical relative entropy estimators.relative entropy functional true distribution \\(F\\)\nmaximum likelihood provides empirical estimators \nparametric models.consequence close link maximum likelihood relative entropy\nmaximum likelihood inherits large \\(n\\) (!) optimality properties KL divergence. discussed detail later course.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"further-connections","chapter":"2 From entropy to maximum likelihood","heading":"2.6.3 Further connections","text":"Since minimising KL divergence contains ML estimation special case may wonder whether broader justification relative entropy context statistical data analysis?Indeed, KL divergence strong geometrical interpretation forms basis information geometry.\nfield manifold distributions\nstudied using tools differential geometry. expected Fisher information\nplays important role metric tensor space distributions.Furthermore, also linked probabilistic forecasting.\nframework -called scoring rules.\nlocal proper scoring rule negative log-probability (“surprise”).\nexpected “surprise” cross-entropy\nrelative entropy corresponding natural divergence connected log scoring rule.Furthermore, another intriguing property KL divergence relative entropy \\(D_{\\text{KL}}(F, G)\\) divergence measure Bregman \\(f\\)-divergence.\nNote \\(f\\)-divergences Bregman-divergences (turn related proper scoring rules) two large classes measures similarity divergence two probability distributions.Finally, likelihood estimation also Bayesian update rule (discussed later module) another special case entropy learning.","code":""},{"path":"maximum-likelihood-estimation.html","id":"maximum-likelihood-estimation","chapter":"3 Maximum likelihood estimation","heading":"3 Maximum likelihood estimation","text":"","code":""},{"path":"maximum-likelihood-estimation.html","id":"outline-of-maximum-likelihood-estimation","chapter":"3 Maximum likelihood estimation","heading":"3.1 Outline of maximum likelihood estimation","text":"","code":""},{"path":"maximum-likelihood-estimation.html","id":"general-procedure","chapter":"3 Maximum likelihood estimation","heading":"3.1.1 General procedure","text":"starting points ML analysis arethe observed data \\(D = \\{x_1,\\ldots,x_n\\}\\) \\(n\\) independent identically distributed (iid) samples, ordering irrelevant, amodel \\(F_{\\boldsymbol \\theta}\\) corresponding probability density probability mass function \\(f(x|\\boldsymbol \\theta)\\) parameters \\(\\boldsymbol \\theta\\)construct likelihood function:\\(L_n(\\boldsymbol \\theta|D)=\\prod_{=1}^{n} f(x_i|\\boldsymbol \\theta)\\)Historically, likelihood also often interpreted probability data given model. However, strictly correct. First, interpretation applies discrete random variables. Second, since samples iid even case one still need add factor accounting multiplicity possible orderings samples obtain correct probability data. Third, interpretation likelihood probability data completely breaks continuous random variables \\(f(x |\\boldsymbol \\theta)\\) density, probability.seen previous chapter origin likelihood function\nlies connection relative entropy. Specifically, \nlog-likelihood function\\(l_n(\\boldsymbol \\theta|D)=\\sum_{=1}^n \\log f(x_i|\\boldsymbol \\theta)\\)divided sample size \\(n\\) large sample approximation cross-entropy unknown true data generating model approximating model \\(F_{\\boldsymbol \\theta}\\).\nNote log-likelihood additive samples \\(x_i\\).maximum likelihood point estimate \\(\\hat{\\boldsymbol \\theta}^{ML}\\) \ngiven maximising (log)-likelihood\\[\\hat{\\boldsymbol \\theta}^{ML} = \\text{arg max}\\, l_n(\\boldsymbol \\theta|D)\\]Thus, finding MLE optimisation problem practise often solved numerically computer, using approaches gradient ascent (negative log-likelihood gradient descent) related algorithms. numerical optimisation usually done log-likelihood rather \nlikelihood function avoid problems computer representation small\nfloating point numbers.Depending complexity likelihood function finding maximum can indeed\ndifficult. hand, likelihood functions constructed \nexponential families maximum likelihood estimation straightforward.","code":""},{"path":"maximum-likelihood-estimation.html","id":"obtaining-mles-for-a-regular-model","chapter":"3 Maximum likelihood estimation","heading":"3.1.2 Obtaining MLEs for a regular model","text":"regular situations, .e. whenthe log-likelihood function twice differentiable regard parameters,maximum (peak) likelihood function lies inside parameter space\nboundary,parameters model identifiable (particular model overparameterised), andthe second derivative log-likelihood maximum negative zero (\none parameter: Hessian matrix maximum negative definite singular)order maximise \\(l_n(\\boldsymbol \\theta|D)\\) one may use score function \\(\\boldsymbol S(\\boldsymbol \\theta)\\)\nfirst derivative log-likelihood function regard parameter:\\[\\begin{align*}\n\\begin{array}{cc}\nS_n(\\theta) = \\frac{d l_n(\\theta|D)}{d \\theta}\\\\\n\\\\\n\\\\\n\\boldsymbol S_n(\\boldsymbol \\theta)=\\nabla l_n(\\boldsymbol \\theta|D)\\\\\n\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{scalar parameter $\\theta$: first derivative}\\\\\n\\text{log-likelihood function}\\\\\n\\\\\n\\text{gradient } \\boldsymbol \\theta\\text{ vector}\\\\\n\\text{(.e. one parameter)}\\\\\n\\end{array}\n\\end{align*}\\]necessary (sufficient) condition MLE \n\\[\n\\boldsymbol S_n(\\hat{\\boldsymbol \\theta}_{ML}) = 0\n\\]demonstrate log-likelihood function actually achieves \nmaximum \\(\\hat{\\boldsymbol \\theta}_{ML}\\) curvature\nMLE must negative, .e. log-likelihood must locally concave MLE.case single parameter (scalar \\(\\theta\\)) requires check\nsecond derivative log-likelihood function regard parameter negative:\n\\[\n\\frac{d^2 l_n(\\hat{\\theta}_{ML}| D)}{d \\theta^2} <0\n\\]\ncase parameter vector (multivariate \\(\\boldsymbol \\theta\\)) need compute\nHessian matrix (matrix second order derivatives)\nMLE:\n\\[\n\\nabla \\nabla^T l_n(\\hat{\\boldsymbol \\theta}_{ML}| D)\n\\]\nverify matrix negative definite (.e. eigenvalues must negative).see later second order derivatives log-likelihood function also play important role assessing uncertainty MLE.","code":""},{"path":"maximum-likelihood-estimation.html","id":"maximum-likelihood-estimation-in-practise","chapter":"3 Maximum likelihood estimation","heading":"3.2 Maximum likelihood estimation in practise","text":"","code":""},{"path":"maximum-likelihood-estimation.html","id":"likelihood-estimation-for-a-single-parameter","chapter":"3 Maximum likelihood estimation","heading":"3.2.1 Likelihood estimation for a single parameter","text":"following illustrate likelihood estimation\nmodels single parameter. case score\nfunction second derivative log-likelihood scalar-valued\nlike log-likelihood function .Example 3.1  Estimation proportion – maximum likelihood Bernoulli model:aim estimate true proportion \\(\\theta\\) Bernoulli experiment binary\noutcomes, say proportion “successes” vs. “failures” “heads” vs. “tails” coin tossing experiment.Bernoulli model \\(\\text{Ber}(\\theta)\\): \\(\\text{Pr}(\\text{\"success\"}) = \\theta\\) \\(\\text{Pr}(\\text{\"failure\"}) = 1-\\theta\\).“success” indicated outcome \\(x=1\\) “failure” \\(x=0\\).conduct \\(n\\) trials record \\(n_1\\) successes \\(n-n_1\\) failures.Parameter: \\(\\theta\\) probability “success”.MLE \\(\\theta\\)?observations \\(D=\\{x_1, \\ldots, x_n\\}\\) take values 0 1.observations \\(D=\\{x_1, \\ldots, x_n\\}\\) take values 0 1.average data points \\(\\bar{x} = \\frac{1}{n} \\sum_{=1}^n x_i = \\frac{n_1}{n}\\).average data points \\(\\bar{x} = \\frac{1}{n} \\sum_{=1}^n x_i = \\frac{n_1}{n}\\).probability mass function (PMF) Bernoulli distribution \\(\\text{Ber}(\\theta)\\) :\n\\[\np(x| \\theta) = \\theta^x (1-\\theta)^{1-x} =\n\\begin{cases}\n\\theta &  \\text{$x=1$ }\\\\\n1-\\theta & \\text{$x=0$} \\\\\n\\end{cases}\n\\]probability mass function (PMF) Bernoulli distribution \\(\\text{Ber}(\\theta)\\) :\n\\[\np(x| \\theta) = \\theta^x (1-\\theta)^{1-x} =\n\\begin{cases}\n\\theta &  \\text{$x=1$ }\\\\\n1-\\theta & \\text{$x=0$} \\\\\n\\end{cases}\n\\]log-PMF:\n\\[\n\\log p(x| \\theta) =  x \\log(\\theta) + (1-x) \\log(1 - \\theta)\n\\]log-PMF:\n\\[\n\\log p(x| \\theta) =  x \\log(\\theta) + (1-x) \\log(1 - \\theta)\n\\]log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\theta| D) & = \\sum_{=1}^n \\log f(x_i| \\theta) \\\\\n    & = n_1 \\log \\theta + (n-n_1) \\log(1-\\theta) \\\\\n    & = n \\left( \\bar{x} \\log \\theta + (1-\\bar{x}) \\log(1-\\theta) \\right) \\\\\n\\end{split}\n\\]\nNote log-likelihood depends data \\(\\bar{x}\\)! \nexample sufficient statistic parameter \\(\\theta\\) (fact also minimally sufficient statistic). discussed detail later.log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\theta| D) & = \\sum_{=1}^n \\log f(x_i| \\theta) \\\\\n    & = n_1 \\log \\theta + (n-n_1) \\log(1-\\theta) \\\\\n    & = n \\left( \\bar{x} \\log \\theta + (1-\\bar{x}) \\log(1-\\theta) \\right) \\\\\n\\end{split}\n\\]\nNote log-likelihood depends data \\(\\bar{x}\\)! \nexample sufficient statistic parameter \\(\\theta\\) (fact also minimally sufficient statistic). discussed detail later.Score function:\n\\[\nS_n(\\theta)=  \\frac{dl_n(\\theta| D)}{d\\theta}= n \\left( \\frac{\\bar{x}}{\\theta}-\\frac{1-\\bar{x}}{1-\\theta} \\right)\n\\]Score function:\n\\[\nS_n(\\theta)=  \\frac{dl_n(\\theta| D)}{d\\theta}= n \\left( \\frac{\\bar{x}}{\\theta}-\\frac{1-\\bar{x}}{1-\\theta} \\right)\n\\]Maximum likelihood estimate: Setting \\(S_n(\\hat{\\theta}_{ML})=0\\) yields solution\n\\[\n\\hat{\\theta}_{ML} = \\bar{x} = \\frac{n_1}{n}\n\\]\n\\(\\frac{dS_n(\\theta)}{d\\theta} = -n \\left( \\frac{\\bar{x}}{\\theta^2} + \\frac{1-\\bar{x}}{(1-\\theta)^2} \\right) <0\\) optimum corresponds indeed maximum (log-)likelihood function negative \\(\\hat{\\theta}_{ML}\\) (indeed \\(\\theta\\)).\nmaximum likelihood estimator \\(\\theta\\) therefore identical frequency\nsuccesses among observations.Maximum likelihood estimate: Setting \\(S_n(\\hat{\\theta}_{ML})=0\\) yields solution\n\\[\n\\hat{\\theta}_{ML} = \\bar{x} = \\frac{n_1}{n}\n\\]\\(\\frac{dS_n(\\theta)}{d\\theta} = -n \\left( \\frac{\\bar{x}}{\\theta^2} + \\frac{1-\\bar{x}}{(1-\\theta)^2} \\right) <0\\) optimum corresponds indeed maximum (log-)likelihood function negative \\(\\hat{\\theta}_{ML}\\) (indeed \\(\\theta\\)).maximum likelihood estimator \\(\\theta\\) therefore identical frequency\nsuccesses among observations.Note analyse coin tossing experiment estimate \\(\\theta\\) may equally well use binomial distribution \\(\\text{Bin}(n, \\theta)\\) model number successes. results MLE \\(\\theta\\) likelihood function based binomial PMF includes binomial coefficient. However, depend \\(\\theta\\) disappears score function influence derivation MLE.Example 3.2  Normal distribution unknown mean known variance:\\(x \\sim N(\\mu,\\sigma^2)\\) \\(\\text{E}(x)=\\mu\\) \\(\\text{Var}(x) = \\sigma^2\\)parameter estimated \\(\\mu\\) whereas \\(\\sigma^2\\) known.’s MLE parameter \\(\\mu\\)?data \\(D= \\{x_1, \\ldots, x_n\\}\\) real range \\(x_i \\[-\\infty, \\infty]\\).data \\(D= \\{x_1, \\ldots, x_n\\}\\) real range \\(x_i \\[-\\infty, \\infty]\\).average \\(\\bar{x} = \\frac{1}{n} \\sum_{=1}^n x_i\\) real well.average \\(\\bar{x} = \\frac{1}{n} \\sum_{=1}^n x_i\\) real well.Density: \\[ f(x| \\mu)=\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]Density: \\[ f(x| \\mu)=\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]Log-Density:\n\\[\\log f(x| \\mu) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\]Log-Density:\n\\[\\log f(x| \\mu) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\]Log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\mu| D) &= \\sum_{=1}^n \\log f(x_i| \\mu)\\\\\n&=-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2}\\log(2 \\pi \\sigma^2) }_{\\text{constant term, depend } \\mu \\text{, can removed}}\\\\\n&=-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i^2 - 2 x_i \\mu+\\mu^2)  + C\\\\\n&=\\frac{n}{\\sigma^2}  ( \\bar{x} \\mu  - \\frac{1}{2}\\mu^2)  \\underbrace{ - \\frac{1}{2\\sigma^2}\\sum_{=1}^n   x_i^2 }_{\\text{another constant term}}   + C\\\\\n\\end{split}\n\\]\nNote non-constant terms log-likelihood depend data \\(\\bar{x}\\)!Log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\mu| D) &= \\sum_{=1}^n \\log f(x_i| \\mu)\\\\\n&=-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2}\\log(2 \\pi \\sigma^2) }_{\\text{constant term, depend } \\mu \\text{, can removed}}\\\\\n&=-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i^2 - 2 x_i \\mu+\\mu^2)  + C\\\\\n&=\\frac{n}{\\sigma^2}  ( \\bar{x} \\mu  - \\frac{1}{2}\\mu^2)  \\underbrace{ - \\frac{1}{2\\sigma^2}\\sum_{=1}^n   x_i^2 }_{\\text{another constant term}}   + C\\\\\n\\end{split}\n\\]\nNote non-constant terms log-likelihood depend data \\(\\bar{x}\\)!Score function:\n\\[\nS_n(\\mu) =\n\\frac{n}{\\sigma^2} ( \\bar{x}- \\mu)\n\\]Score function:\n\\[\nS_n(\\mu) =\n\\frac{n}{\\sigma^2} ( \\bar{x}- \\mu)\n\\]Maximum likelihood estimate:\n\\[S_n(\\hat{\\mu}_{ML})=0 \\Rightarrow \\hat{\\mu}_{ML} = \\bar{x}\\]Maximum likelihood estimate:\n\\[S_n(\\hat{\\mu}_{ML})=0 \\Rightarrow \\hat{\\mu}_{ML} = \\bar{x}\\]\\(\\frac{dS_n(\\mu)}{d\\mu} = -\\frac{n}{\\sigma^2}<0\\) optimum indeed maximumWith \\(\\frac{dS_n(\\mu)}{d\\mu} = -\\frac{n}{\\sigma^2}<0\\) optimum indeed maximumThe constant term \\(C\\) log-likelihood function collects terms depend parameter. taking first derivative regard parameter term disappears thus \\(C\\) relevant finding MLE parameter.\nfuture often omit constant terms log-likelihood function without mention.Example 3.3  Normal distribution known mean unknown variance:\\(x \\sim N(\\mu,\\sigma^2)\\) \\(\\text{E}(x)=\\mu\\) \\(\\text{Var}(x) = \\sigma^2\\)\\(\\sigma^2\\) needs estimated whereas mean \\(\\mu\\) knownWhat’s MLE \\(\\sigma^2\\)?data \\(D= \\{x_1, \\ldots, x_n\\}\\) real range \\(x_i \\[-\\infty, \\infty]\\).data \\(D= \\{x_1, \\ldots, x_n\\}\\) real range \\(x_i \\[-\\infty, \\infty]\\).average squared centred data \\(\\overline{(x-\\mu)^2} = \\frac{1}{n} \\sum_{=1}^n (x_i-\\mu)^2 \\geq 0\\) non-negative.average squared centred data \\(\\overline{(x-\\mu)^2} = \\frac{1}{n} \\sum_{=1}^n (x_i-\\mu)^2 \\geq 0\\) non-negative.Density: \\[ f(x| \\sigma^2)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]Density: \\[ f(x| \\sigma^2)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]Log-Density:\n\\[\\log f(x | \\sigma^2) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\]Log-Density:\n\\[\\log f(x | \\sigma^2) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\]Log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\sigma | D) & = l_n(\\mu, \\sigma^2 | D) = \\sum_{=1}^n \\log f(x_i| \\sigma^2)\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2} \\log(2 \\pi) }_{\\text{constant depending } \\sigma^2}\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{n}{2\\sigma^2}  \\overline{(x-\\mu)^2}  + C\\\\\n\\end{split}\n\\]\nNote log-likelihood function depends data \\(\\overline{(x-\\mu)^2}\\)!Log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\sigma | D) & = l_n(\\mu, \\sigma^2 | D) = \\sum_{=1}^n \\log f(x_i| \\sigma^2)\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2} \\log(2 \\pi) }_{\\text{constant depending } \\sigma^2}\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{n}{2\\sigma^2}  \\overline{(x-\\mu)^2}  + C\\\\\n\\end{split}\n\\]\nNote log-likelihood function depends data \\(\\overline{(x-\\mu)^2}\\)!Score function:\n\\[\nS_n(\\sigma^2) =\n-\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4}    \\overline{(x-\\mu)^2}\n\\]\nNote obtain score function derivative needs taken regard variance parameter \\(\\sigma^2\\) — regard \\(\\sigma\\)! trick, relabel \\(\\sigma^2 = v\\) log-likelihood function, take derivative regard \\(v\\), backsubstitute \\(v=\\sigma^2\\) final result.Score function:\n\\[\nS_n(\\sigma^2) =\n-\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4}    \\overline{(x-\\mu)^2}\n\\]Note obtain score function derivative needs taken regard variance parameter \\(\\sigma^2\\) — regard \\(\\sigma\\)! trick, relabel \\(\\sigma^2 = v\\) log-likelihood function, take derivative regard \\(v\\), backsubstitute \\(v=\\sigma^2\\) final result.Maximum likelihood estimate:\n\\[\nS_n(\\widehat{\\sigma^2}_{ML})=0 \\Rightarrow\n\\]\n\\[\n\\widehat{\\sigma^2}_{ML}\n=\\overline{(x-\\mu)^2} = \\frac{1}{n}\\sum_{=1}^n (x_i-\\mu)^2\n\\]Maximum likelihood estimate:\n\\[\nS_n(\\widehat{\\sigma^2}_{ML})=0 \\Rightarrow\n\\]\n\\[\n\\widehat{\\sigma^2}_{ML}\n=\\overline{(x-\\mu)^2} = \\frac{1}{n}\\sum_{=1}^n (x_i-\\mu)^2\n\\]confirm actually maximum need verify \nsecond derivative log-likelihood optimum negative. \\(\\frac{dS_n(\\sigma^2)}{d\\sigma^2} = -\\frac{n}{2\\sigma^4} \\left(\\frac{2}{\\sigma^2} \\overline{(x-\\mu)^2} -1\\right)\\)\nhence \\(\\frac{dS_n( \\widehat{\\sigma^2}_{ML} )}{d\\sigma^2} = -\\frac{n}{2} \\left(\\widehat{\\sigma^2}_{ML} \\right)^{-2}<0\\)\noptimum indeed maximum.confirm actually maximum need verify \nsecond derivative log-likelihood optimum negative. \\(\\frac{dS_n(\\sigma^2)}{d\\sigma^2} = -\\frac{n}{2\\sigma^4} \\left(\\frac{2}{\\sigma^2} \\overline{(x-\\mu)^2} -1\\right)\\)\nhence \\(\\frac{dS_n( \\widehat{\\sigma^2}_{ML} )}{d\\sigma^2} = -\\frac{n}{2} \\left(\\widehat{\\sigma^2}_{ML} \\right)^{-2}<0\\)\noptimum indeed maximum.","code":""},{"path":"maximum-likelihood-estimation.html","id":"likelihood-estimation-for-multiple-parameters","chapter":"3 Maximum likelihood estimation","heading":"3.2.2 Likelihood estimation for multiple parameters","text":"several parameters likelihood estimation conceptually\ndifferent case single parameter. However, \nscore function now vector-valued second derivative log-likelihood matrix-valued function.Example 3.4  Normal distribution mean variance unknown:\\(x \\sim N(\\mu,\\sigma^2)\\) \\(\\text{E}(x)=\\mu\\) \\(\\text{Var}(x) = \\sigma^2\\)\\(\\mu\\) \\(\\sigma^2\\) need estimated.’s MLE parameter vector \\(\\boldsymbol \\theta= (\\mu,\\sigma^2)^T\\)?data \\(D= \\{x_1, \\ldots, x_n\\}\\) real range \\(x_i \\[-\\infty, \\infty]\\).data \\(D= \\{x_1, \\ldots, x_n\\}\\) real range \\(x_i \\[-\\infty, \\infty]\\).average \\(\\bar{x} = \\frac{1}{n} \\sum_{=1}^n x_i\\) real well.average \\(\\bar{x} = \\frac{1}{n} \\sum_{=1}^n x_i\\) real well.average squared data \\(\\overline{x^2} = \\frac{1}{n} \\sum_{=1}^n x_i^2 \\geq 0\\) non-negative.average squared data \\(\\overline{x^2} = \\frac{1}{n} \\sum_{=1}^n x_i^2 \\geq 0\\) non-negative.Density: \\[ f(x| \\mu, \\sigma^2)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]Density: \\[ f(x| \\mu, \\sigma^2)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]Log-Density:\n\\[\\log f(x | \\mu, \\sigma^2) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\]Log-Density:\n\\[\\log f(x | \\mu, \\sigma^2) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\]Log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\boldsymbol \\theta| D) & = l_n(\\mu, \\sigma^2 | D) = \\sum_{=1}^n \\log f(x_i| \\mu, \\sigma^2)\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2} \\log(2 \\pi) }_{\\text{constant depending }\\mu \\text{ } \\sigma^2}\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{n}{2\\sigma^2}  ( \\overline{x^2} -2 \\bar{x} \\mu + \\mu^2)  + C\\\\\n\\end{split}\n\\]\nNote log-likelihood function depends data \\(\\bar{x}\\)\n\\(\\overline{x^2}\\)!Log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\boldsymbol \\theta| D) & = l_n(\\mu, \\sigma^2 | D) = \\sum_{=1}^n \\log f(x_i| \\mu, \\sigma^2)\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2} \\log(2 \\pi) }_{\\text{constant depending }\\mu \\text{ } \\sigma^2}\\\\\n&= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{n}{2\\sigma^2}  ( \\overline{x^2} -2 \\bar{x} \\mu + \\mu^2)  + C\\\\\n\\end{split}\n\\]\nNote log-likelihood function depends data \\(\\bar{x}\\)\n\\(\\overline{x^2}\\)!Score function \\(\\boldsymbol S_n\\), gradient \\(l_n(\\boldsymbol \\theta| D)\\):\n\\[\n\\begin{split}\n\\boldsymbol S_n(\\boldsymbol \\theta) &= \\nabla l_n(\\boldsymbol \\theta| D) \\\\\n&=\n\\begin{pmatrix}\n\\frac{n}{\\sigma^2} (\\bar{x}-\\mu) \\\\\n-\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4}   \\left( \\overline{x^2} - 2\\bar{x} \\mu +\\mu^2 \\right)  \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\]Score function \\(\\boldsymbol S_n\\), gradient \\(l_n(\\boldsymbol \\theta| D)\\):\n\\[\n\\begin{split}\n\\boldsymbol S_n(\\boldsymbol \\theta) &= \\nabla l_n(\\boldsymbol \\theta| D) \\\\\n&=\n\\begin{pmatrix}\n\\frac{n}{\\sigma^2} (\\bar{x}-\\mu) \\\\\n-\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4}   \\left( \\overline{x^2} - 2\\bar{x} \\mu +\\mu^2 \\right)  \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\]Maximum likelihood estimate:\n\\[\n\\boldsymbol S_n(\\hat{\\boldsymbol \\theta}_{ML})=0 \\Rightarrow\n\\]\n\\[\n\\hat{\\boldsymbol \\theta}_{ML}=\n\\begin{pmatrix}\n\\hat{\\mu}_{ML}  \\\\\n\\widehat{\\sigma^2}_{ML} \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\bar{x} \\\\\n\\overline{x^2} -\\bar{x}^2\\\\\n\\end{pmatrix}\n\\]\nML estimate variance can also written\n\\(\\widehat{\\sigma^2}_{ML} = \\overline{x^2} -\\bar{x}^2 =\\overline{(x-\\bar{x})^2} =  \\frac{1}{n}\\sum_{=1}^n (x_i-\\bar{x})^2\\).Maximum likelihood estimate:\n\\[\n\\boldsymbol S_n(\\hat{\\boldsymbol \\theta}_{ML})=0 \\Rightarrow\n\\]\n\\[\n\\hat{\\boldsymbol \\theta}_{ML}=\n\\begin{pmatrix}\n\\hat{\\mu}_{ML}  \\\\\n\\widehat{\\sigma^2}_{ML} \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\bar{x} \\\\\n\\overline{x^2} -\\bar{x}^2\\\\\n\\end{pmatrix}\n\\]\nML estimate variance can also written\n\\(\\widehat{\\sigma^2}_{ML} = \\overline{x^2} -\\bar{x}^2 =\\overline{(x-\\bar{x})^2} =  \\frac{1}{n}\\sum_{=1}^n (x_i-\\bar{x})^2\\).confirm actually maximum need verify eigenvalues\nHessian matrix optimum negative. indeed case, \ndetails see Example 3.9.confirm actually maximum need verify eigenvalues\nHessian matrix optimum negative. indeed case, \ndetails see Example 3.9.Example 3.5  Maximum likelihood estimates parameters multivariate normal distribution:results Example 3.4 can generalised \nmultivariate normal distribution:\\(\\boldsymbol x\\sim N(\\boldsymbol \\mu,\\boldsymbol \\Sigma)\\) \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu\\) \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\)\\(\\boldsymbol \\mu\\) \\(\\boldsymbol \\Sigma\\) need estimated.Withthe data \\(D= \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) containing real vector-valued observations,maximum likelihood can written follows:MLE mean:\n\\[\n\\hat{\\boldsymbol \\mu}_{ML} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k = \\bar{\\boldsymbol x}\n\\]MLE covariance:\n\\[\n\\underbrace{\\widehat{\\boldsymbol \\Sigma}_{ML}}_{d \\times d} = \\frac{1}{n}\\sum^{n}_{k=1} \\underbrace{\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)}_{d \\times 1} \\; \\underbrace{\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)^T}_{1 \\times d}\\]\nNote factor \\(\\frac{1}{n}\\) estimator covariance matrix.\\(\\overline{\\boldsymbol x\\boldsymbol x^T} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k \\boldsymbol x_k^T\\)\ncan also write\n\\[\n\\widehat{\\boldsymbol \\Sigma}_{ML} = \\overline{\\boldsymbol x\\boldsymbol x^T} - \\bar{\\boldsymbol x} \\bar{\\boldsymbol x}^T\n\\]Hence, MLEs correspond well-known empirical estimates.derivation MLEs discussed detail module MATH38161 Multivariate Statistics Machine Learning.Example 3.6  Maximum likelihood estimation parameters categorical distribution:Maximum likelihood estimation seems trivial first sight fact bit complicated since \\(K-1\\) free parameters, \\(K\\). either need optimise regard specific set \\(K-1\\) parameters () use constrained optimisation procedure enforce \\(\\sum_{k=1}^K \\pi_k = 1\\) (example using Lagrange multiplier).data: observe \\(n\\) samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).\ndata matrix dimension \\(n \\times K\\) \n\\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)^T = (x_{ik})\\).\ncontains \\(\\boldsymbol x_i = (x_{i1}, \\ldots, x_{iK})^T\\). corresponding summary\n(minimal sufficient) statistics \n\\(\\bar{\\boldsymbol x} = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = (\\bar{x}_1, \\ldots, \\bar{x}_K)^T\\)\n\n\\(\\bar{x}_k = \\frac{1}{n} \\sum_{=1}^n x_{ik}\\). can also write\n\\(\\bar{x}_{K} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k}\\).\nnumber samples class \\(k\\) \\(n_k = n \\bar{x}_k\\) \n\\(\\sum_{k=1}^K n_k = n\\).data: observe \\(n\\) samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).\ndata matrix dimension \\(n \\times K\\) \n\\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)^T = (x_{ik})\\).\ncontains \\(\\boldsymbol x_i = (x_{i1}, \\ldots, x_{iK})^T\\). corresponding summary\n(minimal sufficient) statistics \n\\(\\bar{\\boldsymbol x} = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = (\\bar{x}_1, \\ldots, \\bar{x}_K)^T\\)\n\n\\(\\bar{x}_k = \\frac{1}{n} \\sum_{=1}^n x_{ik}\\). can also write\n\\(\\bar{x}_{K} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k}\\).\nnumber samples class \\(k\\) \\(n_k = n \\bar{x}_k\\) \n\\(\\sum_{k=1}^K n_k = n\\).log-likelihood \n\\[\n\\begin{split}\nl_n(\\pi_1, \\ldots, \\pi_{K-1}) & = \\sum_{=1}^n \\log f(\\boldsymbol x_i) \\\\\n& =\\sum_{=1}^n \\left( \\sum_{k=1}^{K-1}  x_{ik} \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_{ik}  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\right)\\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right) \\log\\left(1 - \\sum_{k=1}^{K-1} \\pi_k\\right) \\right) \\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\bar{x}_K \\log \\pi_K \\right) \\\\  \n\\end{split}\n\\]log-likelihood \n\\[\n\\begin{split}\nl_n(\\pi_1, \\ldots, \\pi_{K-1}) & = \\sum_{=1}^n \\log f(\\boldsymbol x_i) \\\\\n& =\\sum_{=1}^n \\left( \\sum_{k=1}^{K-1}  x_{ik} \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_{ik}  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\right)\\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right) \\log\\left(1 - \\sum_{k=1}^{K-1} \\pi_k\\right) \\right) \\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\bar{x}_K \\log \\pi_K \\right) \\\\  \n\\end{split}\n\\]Score function (gradient)\n\\[\n\\begin{split}\n\\boldsymbol S_n(\\pi_1, \\ldots, \\pi_{K-1}) &=  \\nabla l_n(\\pi_1, \\ldots, \\pi_{K-1} ) \\\\\n& =\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial \\pi_1} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\vdots\\\\\n\\frac{\\partial}{\\partial \\pi_{K-1}} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\end{pmatrix}\\\\\n& = n\n\\begin{pmatrix}\n\\frac{\\bar{x}_1}{\\pi_1}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\vdots\\\\\n\\frac{\\bar{x}_{K-1}}{\\pi_{K-1}}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\]\nNote particular need second term arises \\(\\pi_K\\) depends \n\\(\\pi_1, \\ldots, \\pi_{K-1}\\).Score function (gradient)\n\\[\n\\begin{split}\n\\boldsymbol S_n(\\pi_1, \\ldots, \\pi_{K-1}) &=  \\nabla l_n(\\pi_1, \\ldots, \\pi_{K-1} ) \\\\\n& =\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial \\pi_1} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\vdots\\\\\n\\frac{\\partial}{\\partial \\pi_{K-1}} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\end{pmatrix}\\\\\n& = n\n\\begin{pmatrix}\n\\frac{\\bar{x}_1}{\\pi_1}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\vdots\\\\\n\\frac{\\bar{x}_{K-1}}{\\pi_{K-1}}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\]\nNote particular need second term arises \\(\\pi_K\\) depends \n\\(\\pi_1, \\ldots, \\pi_{K-1}\\).Maximum likelihood estimate: Setting \\(\\boldsymbol S_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML})=0\\) yields\n\\(K-1\\) equations\n\\[\n\\bar{x}_i \\left(1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML}\\right)  = \\hat{\\pi}_i^{ML} \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right)\n\\]\n\\(=1, \\ldots, K-1\\) solution\n\\[\n\\hat{\\pi}_i^{ML} = \\bar{x}_i\n\\]\nalso follows \n\\[\n\\hat{\\pi}_K^{ML} = 1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} = \\bar{x}_K\n\\]\nmaximum likelihood estimator therefore frequency\noccurance class among \\(n\\) samples.Maximum likelihood estimate: Setting \\(\\boldsymbol S_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML})=0\\) yields\n\\(K-1\\) equations\n\\[\n\\bar{x}_i \\left(1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML}\\right)  = \\hat{\\pi}_i^{ML} \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right)\n\\]\n\\(=1, \\ldots, K-1\\) solution\n\\[\n\\hat{\\pi}_i^{ML} = \\bar{x}_i\n\\]\nalso follows \n\\[\n\\hat{\\pi}_K^{ML} = 1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} = \\bar{x}_K\n\\]\nmaximum likelihood estimator therefore frequency\noccurance class among \\(n\\) samples.","code":""},{"path":"maximum-likelihood-estimation.html","id":"properties-of-maximum-likelihood-estimation","chapter":"3 Maximum likelihood estimation","heading":"3.3 Properties of maximum likelihood estimation","text":"","code":""},{"path":"maximum-likelihood-estimation.html","id":"invariance-property-of-the-maximum-likelihood","chapter":"3 Maximum likelihood estimation","heading":"3.3.1 Invariance property of the maximum likelihood","text":"invariance principle states maximum likelihood invariant reparameterisation.Assume transform parameter \\(\\theta\\) another parameter \\(\\omega\\) using invertible function \\(g()\\)\n\\(\\omega= g(\\theta)\\).\nmaximum likelihood estimate \\(\\hat{\\omega}_{ML}\\) new parameter \\(\\omega\\) simply\ntransformation maximum likelihood estimate \\(\\hat{\\theta}_{ML}\\) original parameter \\(\\theta\\)\n\\(\\hat{\\omega}_{ML}= g( \\hat{\\theta}_{ML})\\). achieved\nmaximum likelihood cases.reason works maximisation procedure invariant transformations argument\nfunction maximised. Consider function \\(h(x)\\) maximum \\(x_{\\max} = \\text{arg max } h(x)\\). Now relabel argument using\n\\(y = g(x)\\) \\(g\\) invertible function. function terms \\(y\\) \\(h( g^{-1}(y))\\).\nclearly function maximum \\(y_{\\max} = g(x_{\\max})\\) since\n\\(h(g^{-1}(y_{\\max} ) ) = h( x_{\\max} )\\).invariance property can useful practise often easier (sometimes numerically stable) maximise likelihood different set parameters.See Worksheet L1 example application invariance principle.","code":""},{"path":"maximum-likelihood-estimation.html","id":"consistency-of-maximum-likelihood-estimates","chapter":"3 Maximum likelihood estimation","heading":"3.3.2 Consistency of maximum likelihood estimates","text":"One important property method maximum likelihood general produces consistent estimates.Specifically, true underlying model \\(F_{\\text{true}}\\) parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) contained set specified candidates models \\(F_{\\boldsymbol \\theta}\\)\n\\[\\underbrace{F_{\\text{true}}}_{\\text{true model}} \\subset \\underbrace{F_{\\boldsymbol \\theta}}_{\\text{specified models}}\\] \\[\\hat{\\boldsymbol \\theta}_{ML} \\overset{\\text{large }n}{\\longrightarrow} \\boldsymbol \\theta_{\\text{true}}\\]consequence \\(D_{\\text{KL}}(F_{\\text{true}},F_{\\boldsymbol \\theta})\\rightarrow 0\\) \\(F_{\\boldsymbol \\theta} \\rightarrow F_{\\text{true}}\\), maximisation likelihood function large \\(n\\) equivalent minimising relative entropy.Thus given sufficient data MLE converge true value. consequence, MLEs asympotically unbiased. see examples can still biased finite samples.Note even candidate model \\(F_{\\boldsymbol \\theta}\\) misspecified (.e. contain actual true model) MLE still optimal sense find closest possible model.possible find inconsistent MLEs, occurs situations dimension model / number parameters increases sample size, MLE boundary singularities likelihood function.","code":""},{"path":"maximum-likelihood-estimation.html","id":"relationship-of-maximum-likelihood-with-least-squares-estimation","chapter":"3 Maximum likelihood estimation","heading":"3.3.3 Relationship of maximum likelihood with least squares estimation","text":"Example 3.2\nform log-likelihood function\nfunction sum squared differences. Maximising \\(l_n(\\mu| D) =-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2\\) equivalent minimising \\(\\sum_{=1}^n(x_i-\\mu)^2\\). Hence, finding mean maximum likelihood assuming normal model equivalent least-squares estimation!Note least-squares estimation use least since early 1800s 4 thus predates maximum likelihood (1922). Due simplicity still popular particular regression link maximum likelihood normality allows understand usually works well!","code":""},{"path":"maximum-likelihood-estimation.html","id":"bias-of-maximum-likelihood-estimates","chapter":"3 Maximum likelihood estimation","heading":"3.3.4 Bias of maximum likelihood estimates","text":"Example 3.4 interesting shows maximum likelihood can result biased well unbiased estimators.Recall \\(x \\sim N(\\mu, \\sigma^2)\\). result\n\\[\\hat{\\mu}_{ML}=\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\n\\(\\text{E}( \\hat{\\mu}_{ML} ) = \\mu\\)\n\n\\[\n\\widehat{\\sigma^2}_{\\text{ML}} \\sim\nW_1\\left(s^2 = \\frac{\\sigma^2}{n}, k=n-1\\right)\n\\]\n(see Appendix) mean \\(\\text{E}(\\widehat{\\sigma^2}_{ML}) = \\frac{n-1}{n} \\, \\sigma^2\\).Therefore, MLE \\(\\mu\\) unbiased \\[\n\\text{Bias}(\\hat{\\mu}_{ML}) = \\text{E}( \\hat{\\mu}_{ML} ) - \\mu = 0\n\\]\ncontrast, however, MLE \\(\\sigma^2\\) negatively biased \n\\[\n\\text{Bias}(\\widehat{\\sigma^2}_{ML}) = \\text{E}( \\widehat{\\sigma^2}_{ML} ) - \\sigma^2 = -\\frac{1}{n} \\, \\sigma^2\n\\]Thus, case variance parameter normal distribution MLE recovering well-known unbiased estimator variance\\[\n\\widehat{\\sigma^2}_{UB} = \\frac{1}{n-1}\\sum_{=1}^n(x_i-\\bar{x})^2 = \\frac{n}{n-1} \\widehat{\\sigma^2}_{ML}\n\\]\nwords, unbiased variance estimate maximum likelihood estimate!Therefore worth keeping mind maximum likelihood can result biased estimates finite \\(n\\).\nlarge \\(n\\), however, bias disappears MLEs consistent.","code":""},{"path":"maximum-likelihood-estimation.html","id":"observed-fisher-information","chapter":"3 Maximum likelihood estimation","heading":"3.4 Observed Fisher information","text":"","code":""},{"path":"maximum-likelihood-estimation.html","id":"motivation-and-definition-of-observed-fisher-information","chapter":"3 Maximum likelihood estimation","heading":"3.4.1 Motivation and definition of observed Fisher information","text":"inspection log-likelihood curves apparent log-likelihood function contains information parameter \\(\\boldsymbol \\theta\\) just maximum point \\(\\hat{\\boldsymbol \\theta}_{ML}\\).particular curvature log-likelihood function MLE must somehow related accuracy \\(\\hat{\\boldsymbol \\theta}_{ML}\\): likelihood surface flat near maximum\n(low curvature) difficult find optimal parameter (also numerically!). Conversely, likelihood surface peaked (strong curvature) maximum point clearly defined.curvature described second-order derivatives (Hessian matrix) log-likelihood function.univariate \\(\\theta\\) Hessian scalar:\n\\[\\frac{d^2 l_n(\\theta|D)}{d\\theta^2}\\]multivariate parameter vector \\(\\boldsymbol \\theta\\) dimension \\(d\\) Hessian matrix size \\(d \\times d\\):\n\\[\\nabla \\nabla^T l_n(\\boldsymbol \\theta| D)\\]construction Hessian negative definite MLE (.e. eigenvalues negative) ensure function concave MLE (.e. peak shaped).observed Fisher information (matrix) defined \nnegative curvature MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\):\n\\[\n{\\boldsymbol J_n}(\\hat{\\boldsymbol \\theta}_{ML}) = -\\nabla \\nabla^T l_n(\\hat{\\boldsymbol \\theta}_{ML}| D)\n\\]Sometimes simply called “observed information”.\navoid confusion expected Fisher information introduced earlier\\[\n\\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta) = -\\text{E}_{F_{\\boldsymbol \\theta}} \\left( \\nabla \\nabla^T \\log f(x|\\boldsymbol \\theta)\\right)\n\\]\nnecessary always use qualifier “observed” referring \\({\\boldsymbol J_n}(\\hat{\\boldsymbol \\theta}_{ML})\\).","code":""},{"path":"maximum-likelihood-estimation.html","id":"relationship-between-observed-and-expected-fisher-information","chapter":"3 Maximum likelihood estimation","heading":"3.4.2 Relationship between observed and expected Fisher information","text":"observed Fisher information \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) expected Fisher information\n\\(\\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)\\) related also two clearly different entities:types Fisher information based computing second order derivatives\n(Hessian matrix), thus based curvature function.types Fisher information based computing second order derivatives\n(Hessian matrix), thus based curvature function.observed Fisher information computed log-likelihood function.\nTherefore takes observed data \\(D\\) account explicitly depends sample size \\(n\\). contains estimates parameters parameters . curvature log-likelihood function may computed point log-likelihood function observed Fisher information specifically refers curvature MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\). linked (asymptotic) variance MLE (see examples discuss detail later).observed Fisher information computed log-likelihood function.\nTherefore takes observed data \\(D\\) account explicitly depends sample size \\(n\\). contains estimates parameters parameters . curvature log-likelihood function may computed point log-likelihood function observed Fisher information specifically refers curvature MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\). linked (asymptotic) variance MLE (see examples discuss detail later).contrast, expected Fisher information derived directly log-density. depend observed data, thus depend sample size. can computed value parameters. describes geometry space models, local approximation relative entropy.contrast, expected Fisher information derived directly log-density. depend observed data, thus depend sample size. can computed value parameters. describes geometry space models, local approximation relative entropy.Assume large sample size \\(n\\) MLE converges \\(\\hat{\\boldsymbol \\theta}_{ML} \\rightarrow \\boldsymbol \\theta_0\\).\nfollows construction \nobserved Fisher information law large numbers asymptotically large sample size \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\rightarrow n \\boldsymbol ^{\\text{Fisher}}( \\boldsymbol \\theta_0 )\\) (.e. expected Fisher information set iid random variables, see Chapter 2).Assume large sample size \\(n\\) MLE converges \\(\\hat{\\boldsymbol \\theta}_{ML} \\rightarrow \\boldsymbol \\theta_0\\).\nfollows construction \nobserved Fisher information law large numbers asymptotically large sample size \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\rightarrow n \\boldsymbol ^{\\text{Fisher}}( \\boldsymbol \\theta_0 )\\) (.e. expected Fisher information set iid random variables, see Chapter 2).important class models, namely exponential family model, find \n\\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) = n \\boldsymbol ^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) valid also finite sample size \\(n\\). fact case examples discussed (e.g. see\nExamples 3.7 2.12\nBernoulli distribution Examples 3.9 2.14\nnormal distribution).important class models, namely exponential family model, find \n\\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) = n \\boldsymbol ^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) valid also finite sample size \\(n\\). fact case examples discussed (e.g. see\nExamples 3.7 2.12\nBernoulli distribution Examples 3.9 2.14\nnormal distribution).However, exception. general model \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\neq n \\boldsymbol ^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\)\nfinite sample size \\(n\\). example provided Cauchy distribution median parameter \\(\\theta\\). exponential family model expected Fisher information \\(^{\\text{Fisher}}(\\theta )=\\frac{1}{2}\\) regardless choice\nmedian parameter whereas observed Fisher information \\(J_n(\\hat{\\theta}_{ML})\\) depends \nMLE \\(\\hat{\\theta}_{ML}\\) median parameter simply \\(\\frac{n}{2}\\).However, exception. general model \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\neq n \\boldsymbol ^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\)\nfinite sample size \\(n\\). example provided Cauchy distribution median parameter \\(\\theta\\). exponential family model expected Fisher information \\(^{\\text{Fisher}}(\\theta )=\\frac{1}{2}\\) regardless choice\nmedian parameter whereas observed Fisher information \\(J_n(\\hat{\\theta}_{ML})\\) depends \nMLE \\(\\hat{\\theta}_{ML}\\) median parameter simply \\(\\frac{n}{2}\\).","code":""},{"path":"maximum-likelihood-estimation.html","id":"observed-fisher-information-examples","chapter":"3 Maximum likelihood estimation","heading":"3.5 Observed Fisher information examples","text":"","code":""},{"path":"maximum-likelihood-estimation.html","id":"models-with-a-single-parameter-1","chapter":"3 Maximum likelihood estimation","heading":"3.5.1 Models with a single parameter","text":"Example 3.7  Bernoulli model \\(\\text{Ber}(\\theta)\\):continue Example 3.1. Recall \n\\(\\hat{\\theta}_{ML} = \\bar{x}=\\frac{n_1}{n}\\) score function\n\\(S_n(\\theta)=n \\left( \\frac{\\bar{x} }{\\theta} - \\frac{1-\\bar{x}}{1-\\theta} \\right)\\). negative second derivative log-likelihood function \n\\[\n-\\frac{d S_n(\\theta)}{d\\theta}=n \\left( \\frac{ \\bar{x} }{\\theta^2} + \\frac{1 - \\bar{x} }{(1-\\theta)^2} \\right)\n\\]\nobserved Fisher information therefore\n\\[\n\\begin{split}\nJ_n(\\hat{\\theta}_{ML}) & = n \\left(\\frac{ \\bar{x} }{\\hat{\\theta}_{ML}^2} + \\frac{ 1 - \\bar{x} }{  (1-\\hat{\\theta}_{ML})^2  } \\right) \\\\\n  & = n \\left(\\frac{1}{\\hat{\\theta}_{ML}} + \\frac{1}{1-\\hat{\\theta}_{ML}} \\right) \\\\\n  &= \\frac{n}{\\hat{\\theta}_{ML} (1-\\hat{\\theta}_{ML})} \\\\\n\\end{split}\n\\]inverse observed Fisher information :\n\\[J_n(\\hat{\\theta}_{ML})^{-1}=\\frac{\\hat{\\theta}_{ML}(1-\\hat{\\theta}_{ML})}{n}\\]Compare \\(\\text{Var}\\left(\\frac{x}{n}\\right) = \\frac{\\theta(1-\\theta)}{n}\\) \n\\(x \\sim \\text{Bin}(n, \\theta)\\).Example 3.8  Normal distribution unknown mean known variance:continuation Example 3.2.\nRecall MLE mean\n\\(\\hat{\\mu}_{ML}=\\frac{1}{n}\\sum_{=1}^n x_i=\\bar{x}\\)\nscore function\n\\(\\boldsymbol S_n(\\mu) = \\frac{n}{\\sigma^2} (\\bar{x} -\\mu)\\).\nnegative second derivative score function \n\\[\n-\\frac{d S_n(\\mu)}{d\\mu}= \\frac{n}{\\sigma^2}\n\\]\nobserved Fisher information MLE therefore\n\\[\nJ_n(\\hat{\\mu}_{ML}) = \\frac{n}{\\sigma^2}\n\\]\ninverse observed Fisher information \n\\[\nJ_n(\\hat{\\mu}_{ML})^{-1} = \\frac{\\sigma^2}{n}\n\\]\\(x_i \\sim N(\\mu, \\sigma^2)\\) \\(\\text{Var}(x_i) = \\sigma^2\\)\nhence \\(\\text{Var}(\\bar{x}) = \\frac{\\sigma^2}{n}\\),\nequal inverse observed Fisher information.","code":""},{"path":"maximum-likelihood-estimation.html","id":"models-with-multiple-parameters-1","chapter":"3 Maximum likelihood estimation","heading":"3.5.2 Models with multiple parameters","text":"Example 3.9  Normal distribution mean variance parameter:continuation Example 3.4.\nRecall MLE mean variance:\n\\[\\hat{\\mu}_{ML}=\\frac{1}{n}\\sum_{=1}^n x_i=\\bar{x}\\]\n\\[\\widehat{\\sigma^2}_{ML} = \\frac{1}{n}\\sum_{=1}^n(x_i-\\bar{x})^2 =  \\overline{x^2} - \\bar{x}^2\\]\nscore function\n\\[\\boldsymbol S_n(\\mu,\\sigma^2)=\\nabla l_n(\\mu, \\sigma^2| D) =\n\\begin{pmatrix}\n\\frac{n}{\\sigma^2}   (\\bar{x}-\\mu) \\\\\n-\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4} \\left(\\overline{x^2} - 2 \\mu \\bar{x} + \\mu^2\\right) \\\\\n\\end{pmatrix}\n\\]\nHessian matrix log-likelihood function \n\\[\\nabla \\nabla^T l_n(\\mu,\\sigma^2| D) =\n\\begin{pmatrix}\n    - \\frac{n}{\\sigma^2}&  -\\frac{n}{\\sigma^4} (\\bar{x} -\\mu)\\\\\n    - \\frac{n}{\\sigma^4} (\\bar{x} -\\mu) & \\frac{n}{2\\sigma^4}-\\frac{n}{\\sigma^6} \\left(\\overline{x^2} - 2 \\mu \\bar{x} + \\mu^2\\right) \\\\\n    \\end{pmatrix}\n\\]\nnegative Hessian MLE, .e. \\(\\hat{\\mu}_{ML} = \\bar{x}\\)\n\\(\\widehat{\\sigma^2}_{ML} = \\overline{x^2} -\\bar{x}^2\\)\nyields observed Fisher information matrix:\n\\[\n\\boldsymbol J_n(\\hat{\\mu}_{ML},\\widehat{\\sigma^2}_{ML}) = \\begin{pmatrix}\n    \\frac{n}{\\widehat{\\sigma^2}_{ML}}&0 \\\\\n    0 & \\frac{n}{2(\\widehat{\\sigma^2}_{ML})^2}\n    \\end{pmatrix}\n\\]\nNote observed Fisher information matrix diagonal\npositive entries. Therefore \neigenvalues positive required maximum, diagonal matrix eigenvalues simply \nentries diagonal.inverse observed Fisher information matrix \n\\[\n\\boldsymbol J_n(\\hat{\\mu}_{ML},\\widehat{\\sigma^2}_{ML})^{-1} = \\begin{pmatrix}\n    \\frac{\\widehat{\\sigma^2}_{ML}}{n}& 0\\\\\n    0 & \\frac{2(\\widehat{\\sigma^2}_{ML})^2}{n}\n    \\end{pmatrix}\n\\]Recall \\(x \\sim N(\\mu, \\sigma^2)\\) therefore\n\\[\\hat{\\mu}_{ML}=\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\nHence \\(\\text{Var}(\\hat{\\mu}_{ML}) = \\frac{\\sigma^2}{n}\\). compare \n\nfirst diagonal entry inverse observed Fisher information matrix see essentially expression (apart “hat”).empirical variance \\(\\widehat{\\sigma^2}_{ML}\\) follows one-dimensional Wishart distribution\n\\[\n\\widehat{\\sigma^2}_{\\text{ML}} \\sim\nW_1\\left(s^2 = \\frac{\\sigma^2}{n}, k=n-1\\right)\n\\]\n(see Appendix) variance\n\\(\\text{Var}(\\widehat{\\sigma^2}_{ML}) = \\frac{n-1}{n} \\, \\frac{2 \\sigma ^4}{n}\\). large \\(n\\) becomes \\(\\text{Var}(\\widehat{\\sigma^2}_{ML})\\overset{}{=} \\frac{2 \\sigma ^4}{n}\\) essentially (apart “hat”) second diagonal entry inverse observed Fisher information matrix.Example 3.10  Observed Fisher information categorical distribution:continue Example 3.6.\nfirst need compute negative Hessian matrix log likelihood function\n\\(- \\nabla \\nabla^T l_n(\\pi_1, \\ldots, \\pi_{K-1} )\\) evaluate \nMLEs \\(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}\\).diagonal entries Hessian matrix (\\(=1, \\ldots, K-1\\)) \n\\[\n\\frac{\\partial^2}{\\partial \\pi_i^2} l_n(\\pi_1, \\ldots, \\pi_{K-1} ) =\n-n \\left( \\frac{\\bar{x}_i}{\\pi_i^2} +\\frac{\\bar{x}_K}{\\pi_K^2}\\right)\n\\]\n-diagonal entries (\\(j=1, \\ldots, K-1\\))\n\\[\n\\frac{\\partial^2}{\\partial \\pi_i \\partial \\pi_j} l_n(\\pi_1, \\ldots, \\pi_{K-1} ) =\n-\\frac{n \\bar{x}_K}{\\pi_K^2}\n\\]\nThus, observed Fisher information matrix MLE categorical distribution \n\\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) =\nn\n\\begin{pmatrix}\n\\frac{1}{\\hat{\\pi}_1^{ML}} + \\frac{1}{\\hat{\\pi}_K^{ML}} & \\cdots & \\frac{1}{\\hat{\\pi}_K^{ML}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{\\hat{\\pi}_K^{ML}} & \\cdots & \\frac{1}{\\hat{\\pi}_{K-1}^{ML}} + \\frac{1}{\\hat{\\pi}_K^{ML}} \\\\\n\\end{pmatrix}\n\\]\\(K=2\\) (cf. Example 3.7) reduces observed Fisher information Bernoulli variable\n\\[\n\\begin{split}\nJ_n(\\hat{\\theta}_{ML}) & = n \\left(\\frac{1}{\\hat{\\theta}_{ML}} + \\frac{1}{1-\\hat{\\theta}_{ML}} \\right) \\\\\n  &= \\frac{n}{\\hat{\\theta}_{ML} (1-\\hat{\\theta}_{ML})} \\\\\n\\end{split}\n\\]inverse observed Fisher information :\n\\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  )^{-1} =\n\\frac{1}{n}\n\\begin{pmatrix}\n\\hat{\\pi}_1^{ML} (1- \\hat{\\pi}_1^{ML} )  & \\cdots & -  \\hat{\\pi}_{1}^{ML} \\hat{\\pi}_{K-1}^{ML}   \\\\\n\\vdots & \\ddots & \\vdots \\\\\n-  \\hat{\\pi}_{K-1}^{ML} \\hat{\\pi}_{1}^{ML} & \\cdots & \\hat{\\pi}_{K-1}^{ML} (1- \\hat{\\pi}_{K-1}^{ML} )  \\\\\n\\end{pmatrix}\n\\]show indeed inverse use \nWoodbury matrix identity\\[\n(\\boldsymbol + \\boldsymbol U\\boldsymbol B\\boldsymbol V)^{-1} = \\boldsymbol ^{-1} - \\boldsymbol ^{-1} \\boldsymbol U(\\boldsymbol B^{-1} + \\boldsymbol V\\boldsymbol ^{-1} \\boldsymbol U)^{-1} \\boldsymbol V\\boldsymbol ^{-1}\n\\]\n\\(B=1\\),\\(\\boldsymbol u= (\\pi_1, \\ldots, \\pi_{K-1})^T\\),\\(\\boldsymbol v=-\\boldsymbol u^T\\),\\(\\boldsymbol = \\text{Diag}(\\boldsymbol u)\\) inverse \\(\\boldsymbol ^{-1} = \\text{Diag}(\\pi_1^{-1}, \\ldots, \\pi_{K-1}^{-1})\\).\\(\\boldsymbol ^{-1} \\boldsymbol u= \\boldsymbol 1_{K-1}\\) \\(1-\\boldsymbol u^T \\boldsymbol ^{-1} \\boldsymbol u= \\pi_K\\).\n\n\\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  )^{-1} = \\frac{1}{n}\n\\left( \\boldsymbol - \\boldsymbol u\\boldsymbol u^T \\right)\n\\]\n\n\\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) = n \\left( \\boldsymbol ^{-1} + \\frac{1}{\\pi_K} \\boldsymbol 1_{K-1 \\times K-1}  \\right)\n\\]","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"quadratic-approximation-and-normal-asymptotics","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4 Quadratic approximation and normal asymptotics","text":"","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"approximate-distribution-of-maximum-likelihood-estimates","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.1 Approximate distribution of maximum likelihood estimates","text":"","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"quadratic-log-likelihood-of-the-multivariate-normal-model","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.1.1 Quadratic log-likelihood of the multivariate normal model","text":"Assume observe single sample \\(\\boldsymbol x\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) known covariance.\nNoting multivariate normal density \n\\[\nf(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) = (2\\pi)^{-\\frac{d}{2}} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}}\n\\exp\\left(-\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu) \\right)\n\\]\ncorresponding log-likelihood \\(\\boldsymbol \\mu\\) \n\\[\nl_1(\\boldsymbol \\mu| \\boldsymbol x) = C - \\frac{1}{2}(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)\n\\]\n\\(C\\) constant depend \\(\\boldsymbol \\mu\\).\nNote log-likelihood quadratic function (\\(\\boldsymbol x\\) \\(\\boldsymbol \\mu\\))\nmaximum function lies point \\((\\boldsymbol x, C)\\).","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"quadratic-approximation-of-a-log-likelihood-function","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.1.2 Quadratic approximation of a log-likelihood function","text":"Now consider quadratic approximation general log-likelihood function \\(l_n(\\boldsymbol \\theta| D)\\) \\(\\boldsymbol \\theta\\)\naround MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\).assume underlying model regular \n\\(\\nabla l_n(\\hat{\\boldsymbol \\theta}_{ML} | D) = 0\\).Taylor series approximation scalar-valued function \\(f(\\boldsymbol x)\\) around \\(\\boldsymbol x_0\\) \n\\[\nf(\\boldsymbol x) = f(\\boldsymbol x_0) + \\nabla f(\\boldsymbol x_0)^T (\\boldsymbol x-\\boldsymbol x_0) + \\frac{1}{2}\n(\\boldsymbol x-\\boldsymbol x_0)^T \\nabla \\nabla^T f(\\boldsymbol x_0) (\\boldsymbol x-\\boldsymbol x_0) + \\ldots\n\\]\nApplied log-likelihood function yields\\[l_n(\\boldsymbol \\theta| D) \\approx l_n(\\hat{\\boldsymbol \\theta}_{ML} | D)- \\frac{1}{2}(\\hat{\\boldsymbol \\theta}_{ML}- \\boldsymbol \\theta)^T J_n(\\hat{\\boldsymbol \\theta}_{ML})(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta)\\]quadratic function maximum \\(( \\hat{\\boldsymbol \\theta}_{ML}, l_n(\\hat{\\boldsymbol \\theta}_{ML} | D) )\\).\nNote appearance\nobserved Fisher information \\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) quadratic term.\nlinear term vanishing gradient MLE.Crucially, approximated log-likelihood takes form \\(\\hat{\\boldsymbol \\theta}_{ML}\\) sampled\nmultivariate normal distribution mean \\(\\boldsymbol \\theta\\) covariance given inverse\nobserved Fisher information.Note requires positive definite observed\nFisher information matrix \\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) actually invertible!Example 4.1  Quadratic approximation log-likelihood proportion:Example 3.1 log-likelihood\n\\[\nl_n(p | D) = n \\left( \\bar{x} \\log p + (1-\\bar{x}) \\log(1-p) \\right)\n\\]\nMLE\n\\[\n\\hat{p}_{ML} = \\bar{x}\n\\]\nExample 3.7 observed Fisher information\n\\[\n\\begin{split}\nJ_n(\\hat{p}_{ML}) = \\frac{n}{\\bar{x} (1-\\bar{x})}\n\\end{split}\n\\]\nlog-likelihood MLE \n\\[\nl_n(\\hat{p}_{ML} | D) = n \\left( \\bar{x} \\log \\bar{x} + (1-\\bar{x}) \\log(1-\\bar{x}) \\right)\n\\]\nallows us construct quadratic approximation log-likelihood\naround MLE \n\\[\n\\begin{split}\nl_n(p| D) & \\approx  l_n(\\hat{p}_{ML} | D) - \\frac{1}{2} J_n(\\hat{p}_{ML}) (p-\\hat{p}_{ML})^2 \\\\\n   &= n \\left( \\bar{x} \\log \\bar{x} + (1-\\bar{x}) \\log(1-\\bar{x}) - \\frac{(p-\\bar{x})^2}{2 \\bar{x} (1-\\bar{x})}  \\right) \\\\\n&=  C + \\frac{ \\bar{x} p -\\frac{1}{2} p^2}{ \\bar{x} (1-\\bar{x})/n} \\\\\n\\end{split}\n\\]\nconstant \\(C\\) depend \\(p\\), function match approximate log-likelihood MLE corresponding original log-likelihood. \napproximate log-likelihood takes form normal log-likelihood\n(Example 3.2) one observation\n\\(\\hat{p}_{ML}=\\bar{x}\\) \\(N\\left(p, \\frac{\\bar{x} (1-\\bar{x})}{n} \\right)\\).following figure shows log-likelihood function quadratic approximation\nexample data \\(n = 30\\) \\(\\bar{x} = 0.7\\):","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"asymptotic-normality-of-maximum-likelihood-estimates","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.1.3 Asymptotic normality of maximum likelihood estimates","text":"Intuitively, makes sense associate large amount curvature log-likelihood MLE low variance MLE (conversely, low amount curvature high variance).see thatnormality implies quadratic log-likelihood,conversely, taking quadratic approximation log-likelihood implies\napproximate normality, andin quadratic approximation inverse observed Fisher information plays role covariance MLE.suggests following theorem: Asymptotically, MLE normally distributed around true parameter covariance equal inverse observed Fisher information:\\[\\hat{\\boldsymbol \\theta}_{ML} \\overset{}{\\sim}\\underbrace{N_d}_{\\text{multivariate normal}}\\left(\\underbrace{\\boldsymbol \\theta}_{\\text{mean vector}},\\underbrace{\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1}}_{\\text{ covariance matrix}}\\right)\\]theorem distributional properties MLEs greatly enhances usefulness method maximum likelihood. implies regular settings maximum likelihood just method obtaining point estimates also also provides estimates uncertainty.However, need clarify “asymptotic” actually means context theorem:Primarily, means sufficient sample size log-likelihood \\(l_n(\\boldsymbol \\theta)\\)\nsufficiently well approximated quadratic function around \\(\\hat{\\boldsymbol \\theta}_{ML}\\).\nbetter local quadratic approximation better normal approximation!Primarily, means sufficient sample size log-likelihood \\(l_n(\\boldsymbol \\theta)\\)\nsufficiently well approximated quadratic function around \\(\\hat{\\boldsymbol \\theta}_{ML}\\).\nbetter local quadratic approximation better normal approximation!regular model positive definite observed Fisher information matrix guaranteed large sample size \\(n \\rightarrow \\infty\\) thanks central limit theorem).regular model positive definite observed Fisher information matrix guaranteed large sample size \\(n \\rightarrow \\infty\\) thanks central limit theorem).However, \\(n\\) going infinity fact always required normal approximation hold!\nDepending particular model good local fit quadratic log-likelihood\nmay available also finite \\(n\\). trivial example, normal log-likelihood valid \\(n\\).However, \\(n\\) going infinity fact always required normal approximation hold!\nDepending particular model good local fit quadratic log-likelihood\nmay available also finite \\(n\\). trivial example, normal log-likelihood valid \\(n\\).hand, non-regular models (nondifferentiable log-likelihood MLE /singular Fisher information matrix) amount data, even \\(n\\rightarrow \\infty\\), make quadratic approximation work.hand, non-regular models (nondifferentiable log-likelihood MLE /singular Fisher information matrix) amount data, even \\(n\\rightarrow \\infty\\), make quadratic approximation work.Remarks:asymptotic normality MLEs first discussed Fisher (1925)\n5The technical details considerations worked theory locally asymptotically normal (LAN) models pioneered 1960 Lucien LeCam (1924–2000).technical details considerations worked theory locally asymptotically normal (LAN) models pioneered 1960 Lucien LeCam (1924–2000).also methods obtain higher-order (higher quadratic thus non-normal) asymptotic approximations. relate -called saddle point approximations.also methods obtain higher-order (higher quadratic thus non-normal) asymptotic approximations. relate -called saddle point approximations.","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"asymptotic-optimal-efficiency","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.1.4 Asymptotic optimal efficiency","text":"Assume now \\(\\hat{\\boldsymbol \\theta}\\) arbitrary unbiased estimator \\(\\boldsymbol \\theta\\) \nunderlying data generating model regular density \\(f(\\boldsymbol x| \\boldsymbol \\theta)\\).H. Cramér (1893–1985),\nC. R. Rao (1920–)\nothers demonstrated 1945 -called information inequality,\n\\[\n\\text{Var}(\\hat{\\boldsymbol \\theta}) \\geq \\frac{1}{n} \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)^{-1}\n\\]\nputs lower bound variance estimator \\(\\boldsymbol \\theta\\).\n(Note \\(d>1\\) matrix inequality, meaning difference matrix positive semidefinite).large sample size \\(n \\rightarrow \\infty\\) \\(\\hat{\\boldsymbol \\theta}_{ML} \\rightarrow \\boldsymbol \\theta\\) observed\nFisher information becomes\n\\(J_n(\\hat{\\boldsymbol \\theta}) \\rightarrow n \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)\\)\ntherefore can write asymptotic distribution \\(\\hat{\\boldsymbol \\theta}_{ML}\\) \n\\[\n\\hat{\\boldsymbol \\theta}_{ML} \\overset{}{\\sim} N_d\\left(  \\boldsymbol \\theta,  \\frac{1}{n} \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)^{-1}  \\right)\n\\]\nmeans large \\(n\\) regular models \\(\\hat{\\boldsymbol \\theta}_{ML}\\) achieves lowest variance possible according Cramér-Rao information inequality. words, large sample size maximum likelihood optimally efficient thus best available estimator fact MLE!However, see later hold small sample size indeed possible (necessary) improve MLE (e.g. via Bayesian estimation regularisation).","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"quantifying-the-uncertainty-of-maximum-likelihood-estimates","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2 Quantifying the uncertainty of maximum likelihood estimates","text":"","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"estimating-the-variance-of-mles","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2.1 Estimating the variance of MLEs","text":"previous section saw MLEs asymptotically normally distributed,\ninverse Fisher information (expected observed) linked asymptotic variance.leads question whether use observed Fisher information\n\\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) expected Fisher information MLE\n\\(n \\boldsymbol ^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) estimate variance MLE?Clearly, \\(n\\rightarrow \\infty\\) can used interchangeably.However, can different finite \\(n\\)\nparticular models exponential families.Also normality may occur well \\(n\\) goes \\(\\infty\\).Therefore one needs choose two, considering also thatthe expected Fisher information MLE average curvature MLE,\nwhereas observed Fisher information actual observed curvature, andthe observed Fisher information naturally occurs quadratic approximation log-likelihood., observed Fisher information estimator variance appropriate\nbased actual observed data also works large \\(n\\) (case yields\nresult using expected Fisher information):\n\\[\n\\widehat{\\text{Var}}(\\hat{\\boldsymbol \\theta}_{ML}) = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1}\n\\]\nsquare-root estimate standard deviation\n\\[\n\\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML}) = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1/2}\n\\]\nNote use matrix inversion (inverse) matrix square root.reasons preferring observed Fisher information made mathematically precise classic paper \nEfron Hinkley (1978) 6 .","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"examples-for-the-estimated-variance-and-asymptotic-normal-distribution","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2.2 Examples for the estimated variance and asymptotic normal distribution","text":"Example 4.2  Estimated variance distribution MLE proportion:Examples 3.1 3.7\nknow MLE\n\\[\n\\hat{p}_{ML} = \\bar{x} = \\frac{k}{n}\n\\]\ncorresponding observed Fisher information\n\\[\nJ_n(\\hat{p}_{ML})=\\frac{n}{\\hat{p}_{ML}(1-\\hat{p}_{ML})}\n\\]\nestimated variance MLE therefore\n\\[\n\\widehat{\\text{Var}}(   \\hat{p}_{ML}  ) = \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\n\\]\ncorresponding asymptotic normal distribution \n\\[\n\\hat{p}_{ML} \\overset{}{\\sim} N\\left(p,   \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}   \\right)\n\\]Example 4.3  Estimated variance distribution MLE mean parameter normal distribution known variance:Examples 3.2 3.8 know \n\\[\\hat{\\mu}_{ML} =\\bar{x}\\]\ncorresponding observed Fisher information \\(\\hat{\\mu}_{ML}\\) \n\\[J_n(\\hat{\\mu}_{ML})=\\frac{n}{\\sigma^2}\\]estimated variance MLE therefore\n\\[\n\\widehat{\\text{Var}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma^2}{n}\n\\]\ncorresponding asymptotic normal distribution \n\\[\n\\hat{\\mu}_{ML} \\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right)\n\\]Note case distribution asymptotic exact, .e. valid\nalso small \\(n\\) (long data \\(x_i\\) actually \\(N(\\mu, \\sigma^2)\\)!).","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"wald-statistic","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2.3 Wald statistic","text":"Centering MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\) \\(\\boldsymbol \\theta_0\\) followed \nstandardising \\(\\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML})\\) yields Wald statistic\n(named Abraham Wald, 1902–1950):\n\\[\n\\begin{split}\n\\boldsymbol t(\\boldsymbol \\theta_0) & = \\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML})^{-1}(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\\n& = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{1/2}(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\\n\\end{split}\n\\]\nsquared Wald statistic scalar defined \n\\[\n\\begin{split}\nt(\\boldsymbol \\theta_0)^2 &= \\boldsymbol t(\\boldsymbol \\theta_0)^T \\boldsymbol t(\\boldsymbol \\theta_0) \\\\\n&=\n(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)^T\n\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})\n(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\\n\\end{split}\n\\]\nNote literature \\(\\boldsymbol t(\\boldsymbol \\theta_0)\\) \\(t(\\boldsymbol \\theta_0)^2\\) commonly referred Wald statistics. text use qualifier “squared” refer latter.now assume true underlying parameter \\(\\boldsymbol \\theta_0\\). Since MLE asymptotically normal Wald statistic\nasymptotically standard normal distributed:\n\\[\\begin{align*}\n\\begin{array}{cc}\n\\boldsymbol t(\\boldsymbol \\theta_0) \\overset{}{\\sim}\\\\\nt(\\theta_0) \\overset{}{\\sim}\\\\\n\\end{array}\n\\begin{array}{ll}\nN_d(\\boldsymbol 0_d,\\boldsymbol I_d)\\\\\nN(0,1)\\\\\n\\end{array}\n\\begin{array}{ll}\n  \\text{vector } \\boldsymbol \\theta\\\\\n  \\text{scalar } \\theta\\\\\n\\end{array}\n\\end{align*}\\]\nCorrespondingly, squared Wald statistic chi-squared distributed:\n\\[\\begin{align*}\n\\begin{array}{cc}\nt(\\boldsymbol \\theta_0)^2 \\\\\nt(\\theta_0)^2\\\\\n\\end{array}\n\\begin{array}{ll}\n\\overset{}{\\sim}\\chi^2_d\\\\\n\\overset{}{\\sim}\\chi^2_1\\\\\n\\end{array}\n\\begin{array}{ll}\n  \\text{vector } \\boldsymbol \\theta\\\\\n  \\text{scalar } \\theta\\\\\n\\end{array}\n\\end{align*}\\]\ndegree freedom chi-squared distribution dimension \\(d\\)\nparameter vector \\(\\boldsymbol \\theta\\).","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"examples-of-the-squared-wald-statistic","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2.4 Examples of the (squared) Wald statistic","text":"Example 4.4  Wald statistic proportion:continue Example 4.2.\n\\(\\hat{p}_{ML} = \\bar{x}\\)\n\n\\(\\widehat{\\text{Var}}( \\hat{p}_{ML} ) = \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\\)\nthus \\(\\widehat{\\text{SD}}( \\hat{p}_{ML} ) =\\sqrt{ \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n} }\\)\nget Wald statistic:\\[\nt(p_0) = \\frac{\\bar{x}-p_0}{ \\sqrt{\\bar{x}(1-\\bar{x}) / n }  }\\overset{}{\\sim} N(0,1)\n\\]squared Wald statistic :\n\\[t(p_0)^2 = n \\frac{(\\bar{x}-p_0)^2}{ \\bar{x}(1-\\bar{x})   }\\overset{}{\\sim} \\chi^2_1 \\]Example 4.5  Wald statistic mean parameter normal distribution known variance:continue Example 4.3.\n\\(\\hat{\\mu}_{ML} =\\bar{x}\\) \n\\(\\widehat{\\text{Var}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma^2}{n}\\)\nthus \\(\\widehat{\\text{SD}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma}{\\sqrt{n}}\\)\nget Wald statistic:\\[t(\\mu_0) = \\frac{\\bar{x}-\\mu_0}{\\sigma / \\sqrt{n}}\\sim N(0,1)\\]\nNote one sample \\(t\\)-statistic given \\(\\sigma\\).\nsquared Wald statistic :\n\\[t(\\mu_0)^2 = \\frac{(\\bar{x}-\\mu_0)^2}{\\sigma^2 / n}\\sim \\chi^2_1 \\], instance exact distribution, just asymptotic one.Using Wald statistic squared Wald statistic can test whether particular\n\\(\\mu_0\\) can rejected underlying true parameter, can also\nconstruct corresponding confidence intervals.Example 4.6  Wald statistic categorical distribution:squared Wald statistic \n\\[\n\\begin{split}\nt(\\boldsymbol p_0)^2 &=\n(\\hat{\\pi}_{1}^{ML}-p_1^0, \\ldots,  \\hat{\\pi}_{K-1}^{ML}-p_{K-1}^0)   \\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML} ) \\begin{pmatrix} \\hat{\\pi}_{1}^{ML}-p_1^0 \\\\\n\\vdots \\\\\n\\hat{\\pi}_{K-1}^{ML}-p_{K-1}^0\\\\\n\\end{pmatrix}\\\\\n&= n  \\left( \\sum_{k=1}^{K-1} \\frac{(\\hat{\\pi}_{k}^{ML}-p_{k}^0)^2}{\\hat{\\pi}_{k}^{ML}}   + \\frac{ \\left(\\sum_{k=1}^{K-1} (\\hat{\\pi}_{k}^{ML}-p_{k}^0)\\right)^2}{\\hat{\\pi}_{K}^{ML}} \\right)  \\\\\n&= n  \\left( \\sum_{k=1}^{K} \\frac{(\\hat{\\pi}_{k}^{ML}-p_{k}^0)^2}{\\hat{\\pi}_{k}^{ML}}    \\right)  \\\\\n& = n D_{\\text{Neyman}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) )\n\\end{split}\n\\]\\(n_1, \\ldots, n_K\\) observed counts \\(n = \\sum_{k=1}^K n_k\\)\n\\(\\hat{\\pi}_k^{ML} = \\frac{n_k}{n} = \\bar{x}_k\\),\n\\(n_1^{\\text{expect}}, \\ldots, n_K^{\\text{expect}}\\) \nexpected counts \\(n_k^{\\text{expect}} = n p_k^{0}\\) \\(\\boldsymbol p_0\\)\ncan write squared Wald statistic\nfollows:\n\\[\nt(\\boldsymbol p_0)^2 = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}} )^2}{n_k} =  \\chi^2_{\\text{Neyman}}\n\\]\nknown Neyman chi-squared statistic (note observed counts denominator) asymptotically distributed \\(\\chi^2_{K-1}\\) \n\\(K-1\\) free parameters \\(\\boldsymbol p_0\\).","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"normal-confidence-intervals-using-the-wald-statistic","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2.5 Normal confidence intervals using the Wald statistic","text":"asymptotic normality MLEs derived regular models enables us construct \ncorresponding normal confidence interval (CI):example, construct asymptotic normal CI MLE \nscalar parameter \\(\\theta\\) use MLE \\(\\hat{\\theta}_{ML}\\) estimate mean\nstandard deviation \\(\\widehat{\\text{SD}}(\\hat{\\theta}_{ML})\\) computed observed Fisher information:\\[\\text{CI}=[\\hat{\\theta}_{ML} \\pm c_{\\text{normal}} \\widehat{\\text{SD}}(\\hat{\\theta}_{ML})]\\]\\(c_{normal}\\) critical value standard-normal symmetric confidence interval\nchosen achieve desired nominal coverage-\ncritical values computed using inverse standard normal distribution function via\n\\(c_{\\text{normal}}=\\Phi^{-1}\\left(\\frac{1+\\kappa}{2}\\right)\\)\n(cf. refresher section Appendix).example, CI 95% coverage one uses factor 1.96 \n\\[\\text{CI}=[\\hat{\\theta}_{ML} \\pm 1.96\\, \\widehat{\\text{SD}}(\\hat{\\theta}_{ML}) ]\\]normal CI can expressed using Wald statistic follows:\\[\\text{CI}=\\{\\theta_0:  | t(\\theta_0)| < c_{\\text{normal}} \\}\\]Similary, can also expressed using squared Wald statistic:\\[\\text{CI}=\\{\\theta_0:   t(\\boldsymbol \\theta_0)^2 < c_{\\text{chisq}} \\}\\]\nNote form facilitates construction normal confidence intervals\nparameter vector \\(\\boldsymbol \\theta_0\\).following lists containst critical values resulting chi-squared distribution\ndegree freedom \\(m=1\\) three common choices \ncoverage \\(\\kappa\\) normal CI univariate parameter:","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"normal-tests-using-the-wald-statistic","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2.6 Normal tests using the Wald statistic","text":"Finally, recall duality confidence intervals statistical tests. Specifically,\nconfidence interval coverage \\(\\kappa\\) can also used testing follows:every \\(\\theta_0\\) inside CI data allow reject hypothesis \\(\\theta_0\\) true parameter significance level \\(1-\\kappa\\).Conversely, values \\(\\theta_0\\) outside CI can rejected true parameter significance level \\(1-\\kappa\\) .Hence, order test whether \\(\\boldsymbol \\theta_0\\) true underlying parameter value can\ncompute corresponding (squared) Wald statistic, find desired critical\nvalue decide rejection.","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"examples-for-normal-ci-and-tests","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2.7 Examples for normal CI and tests","text":"Example 4.7  Asymptotic normal confidence interval proportion:continue Examples 4.2 4.4.\nAssume observe \\(n=30\\) measurements average \\(\\bar{x} = 0.7\\).\n\\(\\hat{p}_{ML} = \\bar{x} = 0.7\\) \n\\(\\widehat{\\text{SD}}(\\hat{p}_{ML}) = \\sqrt{ \\frac{ \\bar{x}(1-\\bar{x})}{n} } \\approx 0.084\\).symmetric asymptotic normal CI \\(p\\) 95% coverage given \n\\(\\hat{p}_{ML} \\pm 1.96 \\, \\widehat{\\text{SD}}(\\hat{p}_{ML})\\) present data results interval \\([0.536, 0.864]\\).Example 4.8  Asymptotic normal test proportion:continue Example 4.7.now consider two possible values (\\(p_0=0.5\\) \\(p_0=0.8\\)) potentially true underlying proportion.value \\(p_0=0.8\\) lies inside 95% confidence interval \\([0.536, 0.864]\\). implies reject hypthesis true underlying parameter 5% significance\nlevel. contrast, \\(p_0=0.5\\) outside \nconfidence interval can indeed reject value. words, data plus model\nexclude value statistically implausible.can verified directly computing corresponding (squared) Wald statistics\n(see Example 4.4) comparing relevant critical value (3.84 chi-squared distribution 5% significance level):\\(t(0.5)^2 = \\frac{(0.7-0.5)^2}{0.084^2} = 5.71 > 3.84\\) hence \\(p_0=0.5\\) can rejected.\\(t(0.8)^2 = \\frac{(0.7-0.8)^2}{0.084^2} = 1.43 < 3.84\\) hence \\(p_0=0.8\\) rejected.Note squared Wald statistic boundaries normal confidence interval\nequal critical value.Example 4.9  Normal confidence interval mean:continue Examples 4.3 4.5.\nAssume observe \\(n=25\\) measurements average \\(\\bar{x} = 10\\), normal\nunknown mean variance \\(\\sigma^2=4\\).\\(\\hat{\\mu}_{ML} = \\bar{x} = 10\\) \n\\(\\widehat{\\text{SD}}(\\hat{\\mu}_{ML}) = \\sqrt{ \\frac{ \\sigma^2}{n} } = \\frac{2}{5}\\).symmetric asymptotic normal CI \\(p\\) 95% coverage given \n\\(\\hat{\\mu}_{ML} \\pm 1.96 \\, \\widehat{\\text{SD}}(\\hat{\\mu}_{ML})\\) present data results interval \\([9.216, 10.784]\\).Example 4.10  Normal test mean:continue Example 4.9.now consider two possible values (\\(\\mu_0=9.5\\) \\(\\mu_0=11\\)) potentially true underlying mean parameter.value \\(\\mu_0=9.5\\) lies inside 95% confidence interval \\([9.216, 10.784]\\). implies reject hypthesis true underlying parameter 5% significance\nlevel. contrast, \\(\\mu_0=11\\) outside \nconfidence interval can indeed reject value. words, data plus model\nexclude value statistically implausible.can verified directly computing corresponding (squared) Wald statistics\n(see Example 4.5) comparing relevant critical values:\\(t(9.5)^2 = \\frac{(10-9.5)^2}{4/25}= 1.56 < 3.84\\) hence \\(\\mu_0=9.5\\) rejected.\\(t(11)^2 = \\frac{(10-11)^2}{4/25} = 6.25 > 3.84\\) hence \\(\\mu_0=11\\) can rejected.squared Wald statistic boundaries confidence interval\nequals critical value.Note standard one-sample test mean, exact,\napproximation.","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"example-of-a-non-regular-model","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.3 Example of a non-regular model","text":"models allow quadratic approximation log-likelihood function around MLE. case log-likelihood function differentiable MLE. models called non-regular models normal approximation available.Example 4.11  Uniform distribution upper bound \\(\\theta\\):\n\\[x_1,\\dots,x_n \\sim U(0,\\theta)\\]\n\\(x_{[]}\\) denote ordered observations \n\\(0 \\leq x_{[1]} < x_{[2]} < \\ldots < x_{[n]} \\leq \\theta\\) \n\\(x_{[n]} = \\max(x_1,\\dots,x_n)\\).like obtain maximum likelihood estimator\n\\(\\hat{\\theta}_{ML}\\) distribution.probability density function \\(U(0,\\theta)\\) \n\\[f(x|\\theta) =\\begin{cases}\n    \\frac{1}{\\theta} &\\text{} x \\[0,\\theta] \\\\\n    0              & \\text{otherwise.}\n\\end{cases}\n\\]\n\nlog-scale\n\\[\n\\log f(x|\\theta) =\\begin{cases}\n    - \\log \\theta &\\text{} x \\[0,\\theta] \\\\\n    - \\infty              & \\text{otherwise.}\n\\end{cases}\n\\]Since observed data \\(D =\\{x_1, \\ldots, x_n\\}\\) lie interval \\([0,\\theta]\\)\nget log-likelihood function\n\\[\nl_n(\\theta| D) =\\begin{cases}\n    -n\\log \\theta  &\\text{} x_{[n]} \\leq \\theta \\\\\n    - \\infty              & \\text{otherwise}\n\\end{cases}\n\\]Obtaining MLE \\(\\theta\\) straightforward: \\(-n\\log \\theta\\) monotonically decreasing \\(\\theta\\)\n\\(\\theta \\geq x_{[n]}\\) hence log-likelihood function maximum \\(\\hat{\\theta}_{ML}=x_{[n]}\\).However, discontinuity \\(l_n(\\theta| D)\\) \\(x_{[n]}\\) therefore\n\\(l_n(\\theta| D)\\) differentiable \\(\\hat{\\theta}_{ML}\\).\nThus, quadratic approximation around \\(\\hat{\\theta}_{ML}\\)\nobserved Fisher information computed.\nHence, normal approximation distribution \\(\\hat{\\theta}_{ML}\\) valid regardless sample size, .e. even asymptotically \\(n \\rightarrow \\infty\\).Nonetheless, can fact still obtain sampling distribution \\(\\hat{\\theta}_{ML}=x_{[n]}\\). However, via asymptotic arguments instead understanding \\(x_{[n]}\\) order statistic (see https://en.wikipedia.org/wiki/Order_statistic ) following properties:\n\\[\\begin{align*}\n\\begin{array}{cc}\nx_{[n]}\\sim \\theta \\, \\text{Beta}(n,1)\\\\\n\\\\\n\\text{E}(x_{[n]})=\\frac{n}{n+1} \\theta\\\\\n\\\\\n\\text{Var}(x_{[n]})=\\frac{n}{(n+1)^2(n+2)}\\theta^2\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{\"n-th order statistic\" }\\\\\n\\\\\n\\\\\n\\\\\n\\approx \\frac{\\theta^2}{n^2}\\\\\n\\end{array}\n\\end{align*}\\]Note variance decreases \\(\\frac{1}{n^2}\\) much faster usual \\(\\frac{1}{n}\\) “efficient” estimator. Correspondingly,\n\\(\\hat{\\theta}_{ML}\\) -called “super efficient” estimator.","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"likelihood-based-confidence-interval-and-likelihood-ratio","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5 Likelihood-based confidence interval and likelihood ratio","text":"","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"likelihood-based-confidence-intervals-and-wilks-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1 Likelihood-based confidence intervals and Wilks statistic","text":"","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"general-idea-and-definition-of-wilks-log-likelihood-ratio-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.1 General idea and definition of Wilks log-likelihood ratio statistic","text":"Instead relying normal / quadratic approximation, can also use log-likelihood directly find called likelihood confidence intervals:Idea: find \\(\\boldsymbol \\theta_0\\) log-likelihood almost good \\(l_n(\\hat{\\boldsymbol \\theta}_{ML} | D)\\).\n\\[\\text{CI}= \\{\\boldsymbol \\theta_0: l_n(\\hat{\\boldsymbol \\theta}_{ML}| D) - l_n(\\boldsymbol \\theta_0 | D) \\leq \\Delta\\}\\]\n\\(\\Delta\\) tolerated deviation maximum log-likelihood.\nsee determine suitable \\(\\Delta\\).leads naturally Wilks log likelihood ratio statistic \\(W(\\boldsymbol \\theta_0)\\) defined :\n\\[\n\\begin{split}\nW(\\boldsymbol \\theta_0) & = 2 \\log \\left(\\frac{L(\\hat{\\boldsymbol \\theta}_{ML}| D)}{L(\\boldsymbol \\theta_0| D)}\\right) \\\\\n& =2(l_n(\\hat{\\boldsymbol \\theta}_{ML}| D)-l_n(\\boldsymbol \\theta_0 |D))\\\\\n\\end{split}\n\\]\nhelp can write likelihood CI follows:\n\\[\\text{CI}= \\{\\boldsymbol \\theta_0: W(\\boldsymbol \\theta_0) \\leq 2 \\Delta\\}\\]Wilks statistic named Samuel S. Wilks (1906–1964).Advantages using likelihood-based CI:restricted symmetricenables construct multivariate CIs parameter vector easily even non-normal casescontains normal CI special case","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"examples-of-the-wilks-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.2 Examples of the Wilks statistic","text":"Example 5.1  Wilks statistic proportion:log-likelihood parameter \\(\\theta\\) (cf. Example 3.1)\n\\[\nl_n(\\theta| D) = n ( \\bar{x} \\log \\theta + (1-\\bar{x}) \\log(1-\\theta) )\n\\]\nHence Wilks statistic \n\\[\n\\begin{split}\nW(\\theta_0) & = 2 ( l_n( \\hat{\\theta}_{ML} | D)  -l_n( \\theta_0 | D ) )\\\\\n& = 2 n \\left(  \\bar{x} \\log \\left( \\frac{  \\bar{x}  }{\\theta_0}  \\right)  \n                + (1-\\bar{x}) \\log \\left( \\frac{1-\\bar{x} }{1-\\theta_0}  \\right)  \n    \\right) \\\\\n\\end{split}\n\\]Comparing Example 2.6 see case Wilks\nstatistic essentially (apart scale factor \\(2n\\)) KL divergence two\nBernoulli distributions:\n\\[\nW(\\theta_0) =2 n D_{\\text{KL}}( \\text{Ber}( \\hat{\\theta}_{ML} ), \\text{Ber}(\\theta_0)  )\n\\]Example 5.2  Wilks statistic mean parameter normal model:Wilks statistic \n\\[\nW(\\mu_0)^2 = \\frac{(\\bar{x}-\\mu_0)^2}{\\sigma^2 / n}\n\\]See Worksheet L3 derivation\nWilks statistic directly log-likelihood function.Note squared Wald statistic discussed \nExample 4.5.Comparing Example 2.7 see case Wilks\nstatistic essentially (apart scale factor \\(2n\\)) KL divergence two\nnormal distributions different means variance equal \\(\\sigma^2\\):\n\\[\nW(p_0) =2 n D_{\\text{KL}}( N( \\hat{\\mu}_{ML}, \\sigma^2 ), N(\\mu_0, \\sigma^2)  )\n\\]Example 5.3  Wilks log-likelihood ratio statistic categorical distribution:Wilks log-likelihood ratio \n\\[\nW(\\boldsymbol p_0) = 2 (l_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) - l_n(p_1^{0}, \\ldots, p_{K-1}^{0}    ))\n\\]\n\\(\\boldsymbol p_0 = c(p_1^{0}, \\ldots, p_{K}^{0} )^T\\).\nprobabilities sum 1 \\(K-1\\) free parameters.log-likelihood MLE \n\\[\nl_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log \\hat{\\pi}_k^{ML}  =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log \\bar{x}_k\n\\]\n\\(\\hat{\\pi}_k^{ML} = \\frac{n_k}{n} = \\bar{x}_k\\).\nNote following sums run \\(1\\) \\(K\\) \\(K\\)-th component always computed components \\(1\\) \\(K-1\\), previous section.\nlog-likelihood \\(\\boldsymbol p_0\\) \n\\[l_n( p_1^{0}, \\ldots, p_{K-1}^{0}    ) =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log p_k^{0}\n\\]\nWilks statistic becomes\n\\[\nW(\\boldsymbol p_0) = 2 n   \\sum_{k=1}^{K}  \\bar{x}_k \\log\\left(\\frac{\\bar{x}_k}{ p_k^{0}} \\right)\n\\]\nasymptotically chi-squared distributed \\(K-1\\) degrees freedom.Note model Wilks statistic equal KL Divergence\n\\[\nW(\\boldsymbol p_0) = 2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) )\n\\]Wilks log-likelihood ratio statistic categorical distribution also known \\(G\\) test statistic \\(\\hat{\\boldsymbol \\pi}_{ML}\\) corresponds observed frequencies (observed data) \\(\\boldsymbol p_0\\) expected frequencies (.e. hypothesised true frequencies).Using observed counts \\(n_k\\) expected counts \\(n_k^{\\text{expect}} = n p_k^{0}\\)\ncan write Wilks statistic respectively \\(G\\)-statistic\nfollows:\n\\[\nW(\\boldsymbol p_0) = 2   \\sum_{k=1}^{K}  n_k \\log\\left(\\frac{  n_k }{  n_k^{\\text{expect}}   } \\right)\n\\]","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"quadratic-approximation-of-the-wilks-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.3 Quadratic approximation of the Wilks statistic","text":"Recall quadratic approximation log-likelihood function \\(l_n(\\boldsymbol \\theta_0| D)\\) (= second order Taylor series around MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\)):\\[l_n(\\boldsymbol \\theta_0| D)\\approx l_n(\\hat{\\boldsymbol \\theta}_{ML}| D)-\\frac{1}{2}(\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})^T \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) (\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})\\]can approximate Wilks statistic:\n\\[\n\\begin{split}\nW(\\boldsymbol \\theta_0) & = 2(l_n(\\hat{\\boldsymbol \\theta}_{ML}| D)-l_n(\\boldsymbol \\theta_0| D))\\\\\n& \\approx (\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})^T \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})(\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})\\\\\n& =t(\\boldsymbol \\theta_0)^2 \\\\\n\\end{split}\n\\]Thus quadratic approximation Wilks statistic yields squared Wald statistic.Conversely, Wilks statistic can understood generalisation squared Wald statistic.","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"examples-of-quadratic-approximations","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.4 Examples of quadratic approximations","text":"Example 5.4  Quadratic approximation Wilks statistic proportion (continued Example 5.1):Taylor series second order (\\(p_0\\) around \\(\\bar{x}\\)) yields\n\\[\n\\log \\left( \\frac{  \\bar{x}  }{p_0} \\right) \\approx -\\frac{p_0-\\bar{x}}{\\bar{x}} + \\frac{ ( p_0-\\bar{x} )^2    }{2  \\bar{x}^2   }\n\\]\n\n\\[\n\\log \\left( \\frac{ 1- \\bar{x}  }{1- p_0} \\right) \\approx \\frac{p_0-\\bar{x}}{1-\\bar{x}} + \\frac{ ( p_0-\\bar{x} )^2    }{2  (1-\\bar{x})^2   }\n\\]\ncan approximate Wilks statistic proportion \n\\[\n\\begin{split}\nW(p_0) & \\approx  2 n \\left(  - (p_0-\\bar{x})  +\\frac{ ( p_0-\\bar{x} )^2    }{2  \\bar{x}  }\n+ (p_0-\\bar{x}) + \\frac{ ( p_0-\\bar{x} )^2    }{2  (1-\\bar{x}) } \\right)   \\\\\n& = n \\left(    \\frac{ ( p_0-\\bar{x} )^2    }{  \\bar{x}  } + \\frac{ ( p_0-\\bar{x} )^2    }{  (1-\\bar{x}) } \\right)  \\\\\n& = n \\left(    \\frac{ ( p_0-\\bar{x} )^2    }{  \\bar{x} (1-\\bar{x})  } \\right)   \\\\\n&= t(p_0)^2 \\,.\n\\end{split}\n\\]\nverifies quadratic approximation Wilks statistic leads\nback squared Wald statistic Example 4.4.Example 5.5  Quadratic approximation Wilks statistic mean parameter normal model\n(continued Example 5.2):normal log-likelihood already quadratic mean parameter (cf. Example 3.2).\nCorrespondingly, Wilks statistic quadratic mean parameter well.\nHence particular case quadratic “approximation” fact exact\nWilks statistic squared Wald statistic identical!Correspondingly, confidence intervals tests based Wilks statistic\nidentical obtained using Wald statistic.Example 5.6  Quadratic approximation Wilks log-likelihood ratio statistic categorical distribution:Developing Wilks statistic \\(W(\\boldsymbol p_0)\\) around MLE \\(\\hat{\\boldsymbol \\pi}_{ML}\\) yields squared Wald statistic categorical distribution Neyman chi-squared statistic:\n\\[\n\\begin{split}\nW(\\boldsymbol p_0)& = 2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) \\\\\n& \\approx n D_{\\text{Neyman}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) \\\\\n& = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}} )^2}{n_k} \\\\\n& =  \\chi^2_{\\text{Neyman}}\\\\\n\\end{split}\n\\]instead approximate KL divergence assuming \\(\\boldsymbol p_0\\) fixed arrive \n\\[\n\\begin{split}\n2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) &\\approx n D_{\\text{Pearson}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}),  \\text{Cat}(\\boldsymbol p_0 ) )\\\\\n& = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}})^2}{n_k^{\\text{expect}}} \\\\\n& = \\chi^2_{\\text{Pearson}}\n\\end{split}\n\\]\nwell-known Pearson chi-squared statistic (note expected counts denominator).","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"distribution-of-the-wilks-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.5 Distribution of the Wilks statistic","text":"connection squared Wald statistic quadratic approximation\nWilks log-likelihood ratio statistic implies asympotically \ndistribution.Hence, \\(\\boldsymbol \\theta_0\\) Wilks statistic distributed asymptotically \n\\[W(\\boldsymbol \\theta_0) \\overset{}{\\sim} \\chi^2_d\\]\n\\(d\\) number parameters \\(\\boldsymbol \\theta\\), .e. dimension model.scalar \\(\\theta\\) (.e. single parameter \\(d=1\\)) becomes\n\\[\nW(\\theta_0) \\overset{}{\\sim} \\chi^2_1\n\\]fact known Wilks’ theorem.","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"cutoff-values-for-the-likelihood-ci","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.6 Cutoff values for the likelihood CI","text":"asymptotic distribution \\(W\\) useful choose suitable \\(\\Delta\\) likelihood\nCI — note \\(2 \\Delta = c_{\\text{chisq}}\\) \\(c_{\\text{chisq}}\\) critical value specified coverage \\(\\kappa\\). yields table scalar parameterEssentially, means calibrate interval\nsimply compare normal CI.Example 5.7  Likelihood confidence interval proportion:continue Example 5.1, Example 4.7 asssume data \\(n = 30\\) \\(\\bar{x} = 0.7\\).yields (via numerical root finding) 95% likelihood confidence interval\ninterval \\([0.524, 0.843]\\).\nsimilar identical corresponding\nasymptotic normal interval \\([0.536, 0.864]\\) obtained Example 4.7.following figure illustrate relationship normal CI, likelihood\nCI also shows role quadratic approximation (see also Example 4.1). Note :normal CI symmetric around MLE whereas likelihood CI symmetricthe normal CI identical likelihood CI using quadratic approximation!","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"likelihood-ratio-test-lrt-using-wilks-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.7 Likelihood ratio test (LRT) using Wilks statistic","text":"normal case (Wald statistic normal CIs) one can also construct\ntest using Wilks statistic:\\[\\begin{align*}\n\\begin{array}{ll}\nH_0: \\boldsymbol \\theta= \\boldsymbol \\theta_0\\\\\nH_1: \\boldsymbol \\theta\\neq \\boldsymbol \\theta_0\\\\\n\\end{array}\n\\begin{array}{ll}\n  \\text{ True model } \\boldsymbol \\theta_0\\\\\n  \\text{ True model } \\textbf{} \\boldsymbol \\theta_0\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{  Null hypothesis}\\\\\n\\text{  Alternative hypothesis}\\\\\n\\end{array}\n\\end{align*}\\]test statistic use Wilks log likelihood ratio \\(W(\\boldsymbol \\theta_0)\\).\nExtreme values test statistic imply evidence \\(H_0\\).Note null model “simple” (= single parameter value)\nwhereas alternative model “composite” (= set parameter values).Remarks:composite alternative \\(H_1\\) represented single point (MLE).Reject \\(H_0\\) large values \\(W(\\boldsymbol \\theta_0)\\)\\(H_0\\) large \\(n\\) statistic \\(W(\\boldsymbol \\theta_0)\\) chi-squared distributed, .e. \\(W(\\boldsymbol \\theta_0) \\overset{}{\\sim} \\chi^2_d\\). allows compute\ncritical values (.e tresholds declared rejection given significance level) also \\(p\\)-values corresponding observed test statistics.Models outside CI rejectedModels inside CI rejected, .e. can’t statistically distinguished best alternative model.statistic equivalent \\(W(\\boldsymbol \\theta_0)\\) likelihood ratio\n\\[\n\\Lambda(\\boldsymbol \\theta_0)  = \\frac{L(\\boldsymbol \\theta_0| D)}{L(\\hat{\\boldsymbol \\theta}_{ML}| D)}\n\\]\ntwo statistics can transformed \\(W(\\boldsymbol \\theta_0) = -2\\log \\Lambda(\\boldsymbol \\theta_0)\\)\n\\(\\Lambda(\\boldsymbol \\theta_0) = e^{ - W(\\boldsymbol \\theta_0) / 2 }\\).\nreject \\(H_0\\) small values \\(\\Lambda\\).can shown likelihood ratio test compare two simple models optimal sense given specified type error (=probability wrongly rejecting \\(H_0\\), .e.\nsigificance level) maximise power (=1- type II error, probability correctly\naccepting \\(H_1\\)). known Neyman-Pearson theorem.Example 5.8  Likelihood test proportion:continue Example 5.7 95% likelihood confidence\ninterval \\([0.524, 0.843]\\).value \\(p_0=0.5\\) outside CI hence can rejected whereas \\(p_0=0.8\\)\ninsided CI hence rejected 5% significance level.Wilks statistic \\(p_0=0.5\\) \\(p_0=0.8\\) takes following values:\\(W(0.5) = 4.94 > 3.84\\) hence \\(p_0=0.5\\) can rejected.\\(W(0.8) = 1.69 < 3.84\\) hence \\(p_0=0.8\\) rejected.Note Wilks statistic boundaries likelihood confidence interval\nequal critical value (3.84 corresponding 5% significance level chi-squared\ndistribution 1 degree freedom).Compare also normal test proportion Example 4.8.","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"origin-of-likelihood-ratio-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.8 Origin of likelihood ratio statistic","text":"likelihood ratio statistic asymptotically linked differences KL divergences two compared models underlying true model.Assume \\(F\\) true (unknown) data generating model \n\\(G_{\\boldsymbol \\theta}\\) family models\nlike compare two candidate models \\(G_A\\) \\(G_B\\) corresponding\nparameters \\(\\boldsymbol \\theta_A\\) \\(\\boldsymbol \\theta_B\\) \nbasis observed data \\(D = \\{x_1, \\ldots, x_n\\}\\).\nKL divergences \\(D_{\\text{KL}}(F, G_A)\\) \\(D_{\\text{KL}}(F, G_B)\\) indicate close\nmodels \\(G_A\\) \\(G_B\\) fit true \\(F\\).\ndifference two divergences way measure relative fit two models,\ncan computed \n\\[\nD_{\\text{KL}}(F, G_B)-D_{\\text{KL}}(F, G_A) = \\text{E}_{F} \\log \\frac{g(x|\\boldsymbol \\theta_A )}{g(x| \\boldsymbol \\theta_B)}\n\\]\nReplacing \\(F\\) empirical distribution \\(\\hat{F}_n\\) leads \nlarge sample approximation\n\\[\n2 n (D_{\\text{KL}}(F, G_B)-D_{\\text{KL}}(F, G_A))  \\approx 2 (l_n(\\boldsymbol \\theta_A| D) - l_n(\\boldsymbol \\theta_B| D))\n\\]\nHence, difference log-likelihoods provides estimate difference\nKL divergence two models involved.Wilks log likelihood ratio statistic\n\\[\nW(\\boldsymbol \\theta_0) = 2 ( l_n( \\hat{\\boldsymbol \\theta}_{ML}| D ) - l_n(\\boldsymbol \\theta_0| D) )\n\\]\nthus compares best-fit distribution \\(\\hat{\\boldsymbol \\theta}_{ML}\\)\nparameter distribution parameter \\(\\boldsymbol \\theta_0\\).specific models Wilks statistic\ncan also written form KL divergence:\n\\[\nW(\\boldsymbol \\theta_0) = 2n D_{\\text{KL}}( F_{\\hat{\\boldsymbol \\theta}_{ML}}, F_{\\boldsymbol \\theta_0})\n\\]\ncase examples 5.1 5.2 also generally exponential\nfamily models, true general.","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"generalised-likelihood-ratio-test-glrt","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.2 Generalised likelihood ratio test (GLRT)","text":"Also known maximum likelihood ratio test (MLRT). Generalised Likelihood Ratio Test (GLRT) works just like standard likelihood ratio test difference now null model \\(H_0\\) also composite model.\\[\\begin{align*}\n\\begin{array}{ll}\nH_0: \\boldsymbol \\theta\\\\omega_0 \\subset \\Omega \\\\\nH_1: \\boldsymbol \\theta\\\\omega_1  = \\Omega \\setminus \\omega_0\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{ True model lies restricted model space }\\\\\n\\text{ True model restricted model space } \\\\\n\\end{array}\n\\end{align*}\\]\\(H_0\\) \\(H_1\\) now composite hypotheses.\n\\(\\Omega\\) represents unrestricted model space dimension\n(=number free parameters)\n\\(d = |\\Omega|\\). constrained space \\(\\omega_0\\) degree freedom\n\\(d_0 = |\\omega_0|\\) \\(d_0 < d\\).\nNote standard LRT set \\(\\omega_0\\) simple point\n\\(d_0=0\\)\nnull model simple distribution. Thus, LRT contained GLRT\nspecial case!corresponding generalised (log) likelihood ratio statistic given \\[\nW = 2\\log\\left(\\frac{L(\\hat{\\theta}_{ML} |D )}{L(\\hat{\\theta}_{ML}^0 | D )}\\right)\n\\text{ }\n\\Lambda = \\frac{\\underset{\\theta \\\\omega_0}{\\max}\\, L(\\theta| D)}{\\underset{\\theta \\\\Omega}{\\max}\\, L(\\theta | D)}\n\\]\\(L(\\hat{\\theta}_{ML}| D)\\) maximised likelihood assuming full model\n(parameter space \\(\\Omega\\)) \\(L(\\hat{\\theta}_{ML}^0| D)\\) maximised likelihood restricted model (parameter space \\(\\omega_0\\)).\nHence, compute GRLT test statistic need perform two optimisations, one full\nanother restricted model.Remarks:MLE restricted model space \\(\\omega_0\\) taken representative \\(H_0\\).likelihood maximised numerator denominator.restricted model special case full model (.e. two models nested).asymptotic distribution \\(W\\) chi-squared degree freedom depending \\(d\\) \\(d_0\\):\\[W \\overset{}{\\sim} \\text{$\\chi^2_{d-d_0}$}\\]result due Wilks (1938) 7. Note \nassumes true model contained among investigated models.result due Wilks (1938) 7. Note \nassumes true model contained among investigated models.\\(H_0\\) simple hypothesis (.e. \\(d_0=0\\)) standard LRT (corresponding CI) recovered special case GLRT.\\(H_0\\) simple hypothesis (.e. \\(d_0=0\\)) standard LRT (corresponding CI) recovered special case GLRT.Example 5.9  GLRT example:Case-control study: (e.g. “healthy” vs. “disease”)\nobserve normal data \\(D = \\{x_1, \\ldots, x_n\\}\\) two groups sample size \\(n_1\\) \\(n_2\\)\n(\\(n=n_1+n_2\\)), two different means \\(\\mu_1\\) \\(\\mu_2\\) common variance \\(\\sigma^2\\):\\[x_1,\\dots,x_{n_1} \\sim N(\\mu_1, \\sigma^2)\\]\n\n\\[x_{n_1+1},\\dots,x_{n} \\sim N(\\mu_2, \\sigma^2)\\]Question: two means \\(\\mu_1\\) \\(\\mu_2\\) two groups?\\[\\begin{align*}\n\\begin{array}{ll}\nH_0: \\mu_1=\\mu_2  \\text{ (variance unknown, .e. treated nuisance parameter)}\n\\\\\nH_1: \\mu_1\\neq\\mu_2\\\\\n\\end{array}\n\\end{align*}\\]Restricted full models:\\(\\omega_0\\): restricted model two parameters \\(\\mu_0\\) \\(\\sigma^2_0\\)\n(\\(x_{1},\\dots,x_{n} \\sim N(\\mu_0, \\sigma_0^2)\\) ).\\(\\Omega\\): full model three parameters \\(\\mu_1, \\mu_2, \\sigma^2\\).Corresponding log-likelihood functions:Restricted model \\(\\omega_0\\):\n\\[\n\\log L(\\mu_0, \\sigma_0^2 | D) = -\\frac{n}{2} \\log(\\sigma_0^2)\n- \\frac{1}{2\\sigma_0^2} \\sum_{=1}^n (x_i-\\mu_0)^2\n\\]Full model \\(\\Omega\\):\n\\[\n\\begin{split}\n\\log L(\\mu_1, \\mu_2, \\sigma^2 | D) & =\n\\left(-\\frac{n_1}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_{=1}^{n_1} (x_i-\\mu_1)^2   \\right) + \\\\\n& \\phantom{==}\n\\left(-\\frac{n_2}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_{=n_1+1}^{n} (x_i-\\mu_2)^2   \\right)\n\\\\\n&= -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\left( \\sum_{=1}^{n_1} (x_i-\\mu_1)^2 + \\sum_{=n_1+1}^n (x_i-\\mu_2)^2 \\right) \\\\\n\\end{split}\n\\]Corresponding MLEs:\\[\\begin{align*}\n\\begin{array}{ll}\n\\omega_0:\\\\\n\\\\\n\\Omega:\\\\\n\\\\\n\\end{array}\n\\begin{array}{ll}\n\\hat{\\mu}_0 = \\frac{1}{n}\\sum^n_{=1}x_i\\\\\n\\\\\n\\hat{\\mu}_1 = \\frac{1}{n_1}\\sum^{n_1}_{=1}x_i\\\\\n\\hat{\\mu}_2 = \\frac{1}{n_2}\\sum^{n}_{=n_1+1}x_i\\\\\n\\end{array}\n\\begin{array}{ll}\n\\widehat{\\sigma^2_0} = \\frac{1}{n}\\sum^n_{=1}(x_i-\\hat{\\mu}_0)^2\\\\\n\\\\\n\\widehat{\\sigma^2} = \\frac{1}{n}\\left\\{\\sum^{n_1}_{=1}(x_i-\\hat{\\mu}_1)^2+\\sum^n_{=n_1+1}(x_i-\\hat{\\mu}_2)^2\\right\\}\\\\\n\\\\\n\\end{array}\n\\end{align*}\\]Note three estimated means related \n\\[\n\\begin{split}\n\\hat{\\mu}_0  & = \\frac{n_1}{n} \\hat{\\mu}_1 + \\frac{n_2}{n} \\hat{\\mu}_2 \\\\\n             & = \\hat{\\pi_1} \\hat{\\mu}_1 +\\hat{\\pi_2} \\hat{\\mu}_2 \\\\\n\\end{split}\n\\]\noverall mean weighted average two individual group means.Moreover, two estimated variances related \n\\[\n\\widehat{\\sigma^2_0} =  \\hat{\\pi_1} \\hat{\\pi_2} (\\hat{\\mu}_1 - \\hat{\\mu}_2)^2 + \\widehat{\\sigma^2}\n\\]\nNote example variance decomposition, \\(\\widehat{\\sigma^2_0}\\) estimated total variance,\\(\\hat{\\pi_1} \\hat{\\pi_2} (\\hat{\\mu}_1 - \\hat{\\mu}_2)^2\\) estimated -group (explained) variance, \\(\\widehat{\\sigma^2}\\) estimated average within-group (unexplained) variance.following also note \n\\[\n\\begin{split}\n\\frac{ \\widehat{\\sigma^2_0} }{\\widehat{\\sigma^2}}\n& =  \\hat{\\pi_1} \\hat{\\pi_2} \\frac{ (\\hat{\\mu}_1 - \\hat{\\mu}_2)^2}{\\widehat{\\sigma^2}} + 1\\\\\n& =  \\frac{t^2_{\\text{ML}}}{n} +1 \\\\\n& =   \\frac{t^2_{\\text{UB}}}{n-2} +1 \\\\\n\\end{split}\n\\]\n\n\\[\nt_{\\text{ML}} = \\sqrt{n \\hat{\\pi_1} \\hat{\\pi_2} }  \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\widehat{\\sigma^2}}\n\\]\ntwo sample \\(t\\)-statistic based ML variance estimate\n\\(\\widehat{\\sigma^2}\\)\n\n\\(t_{\\text{UB}} = t_{ML} \\sqrt{\\frac{n-2}{n}}\\)\nconventional two sample \\(t\\)-statistic based unbiased variance estimate\n\\(\\widehat{\\sigma^2}_{\\text{UB}}=\\frac{n}{n-2} \\widehat{\\sigma^2}\\) (see Appendix).Corresponding maximised log-likelihood:Restricted model:\\[\\log L(\\hat{\\mu}_0,\\widehat{\\sigma^2_0}| D) = -\\frac{n}{2} \\log(\\widehat{\\sigma^2_0}) -\\frac{n}{2} \\]Full model:\\[\n\\log L(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2}| D) = -\\frac{n}{2} \\log(\\widehat{\\sigma^2}) -\\frac{n}{2}\n\\]Likelihood ratio statistic:\\[\n\\begin{split}\nW & = 2\\log\\left(\\frac{L(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2} | D)}{L(\\hat{\\mu}_0,\\widehat{\\sigma^2_0} | D)}\\right)\\\\\n& = 2 \\log L\\left(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2}| D\\right) - 2 \\log L\\left(\\hat{\\mu}_0,\\widehat{\\sigma^2_0}| D\\right) \\\\\n& = n\\log\\left(\\frac{\\widehat{\\sigma^2_0}}{\\widehat{\\sigma^2}} \\right) \\\\\n& = n\\log\\left(\\frac{t^2_{\\text{ML}}}{n}+1\\right) \\\\\n& = n\\log\\left(\\frac{t^2_{\\text{UB}}}{n-2}+1\\right) \\\\\n\\end{split}\n\\]Thus, log-likelihood ratio statistic \\(W\\) monotonic function (one--one transformation!) (squared) two sample \\(t\\)-statistic!Asymptotic distribution:degree freedom full model \\(d=3\\) constrained model \\(d_0=2\\) \ngeneralised log likelihood ratio statistic \\(W\\) distributed asymptotically \\(\\text{$\\chi^2_{1}$}\\).\nHence, reject null model 5% significance level \\(W > 3.84\\).application GLRTsAs shown , two sample \\(t\\) statistic\ncan derived likelihood ratio statistic.generally, turns many commonly used familiar statistical tests test statistics can interpreted GLRTs. shows wide applicability procedure.","code":""},{"path":"optimality-properties-and-conclusion.html","id":"optimality-properties-and-conclusion","chapter":"6 Optimality properties and conclusion","heading":"6 Optimality properties and conclusion","text":"","code":""},{"path":"optimality-properties-and-conclusion.html","id":"properties-of-maximum-likelihood-encountered-so-far","chapter":"6 Optimality properties and conclusion","heading":"6.1 Properties of maximum likelihood encountered so far","text":"MLE special case relative entropy minimisation valid large samples.MLE can seen generalisation least squares (conversely, least squares special case ML).\\[\\begin{align*}\n\\begin{array}{cc}\n\\text{Kullback-Leibler 1951}\\\\\n\\textbf{Entropy learning: minimise  } D_{\\text{KL}}(F_{\\text{true}},F_{\\boldsymbol \\theta})\\\\\n\\downarrow\\\\\n\\text{large } n\\\\\n\\downarrow\\\\\n\\text{Fisher 1922}\\\\\n\\textbf{Maximise Likelihood  } L(\\boldsymbol \\theta|D)\\\\\n\\downarrow\\\\\n\\text{normal model}\\\\\n\\downarrow\\\\\n\\text{Gauss 1805}\\\\\n\\textbf{Minimise squared error  } \\sum_i (x_i-\\theta)^2\\\\\n\\end{array}\n\\end{align*}\\]Given model, derivation MLE basically automatic (optimisation required)!Given model, derivation MLE basically automatic (optimisation required)!MLEs consistent, .e. true underlying model \\(F_{\\text{true}}\\) parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) contained set specified candidates models \\(F_{\\boldsymbol \\theta}\\)\nMLE converge true model.MLEs consistent, .e. true underlying model \\(F_{\\text{true}}\\) parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) contained set specified candidates models \\(F_{\\boldsymbol \\theta}\\)\nMLE converge true model.Correspondingly, MLEs asympotically unbiased.Correspondingly, MLEs asympotically unbiased.However, MLEs necessarily unbiased finite samples\n(e.g. MLE variance parameter normal distribution).However, MLEs necessarily unbiased finite samples\n(e.g. MLE variance parameter normal distribution).maximum likelihood invariant parameter transformations.maximum likelihood invariant parameter transformations.regular situations (local quadratic approximation possible)\nMLEs asympotically normally distributed, asymptotic variance determined \nobserved Fisher information.regular situations (local quadratic approximation possible)\nMLEs asympotically normally distributed, asymptotic variance determined \nobserved Fisher information.regular situations large sample size MLEs asympotically optimally efficient (Cramer-Rao theorem): large samples MLE achieves lowest possible variance possible estimator — -called Cramer-Rao lower bound. variance decreases zero \\(n \\rightarrow \\infty\\) typically rate \\(1/n\\).regular situations large sample size MLEs asympotically optimally efficient (Cramer-Rao theorem): large samples MLE achieves lowest possible variance possible estimator — -called Cramer-Rao lower bound. variance decreases zero \\(n \\rightarrow \\infty\\) typically rate \\(1/n\\).likelihood ratio can used construct optimal tests (sense Neyman-Pearson theorem).likelihood ratio can used construct optimal tests (sense Neyman-Pearson theorem).","code":""},{"path":"optimality-properties-and-conclusion.html","id":"summarising-data-and-the-concept-of-minimal-sufficiency","chapter":"6 Optimality properties and conclusion","heading":"6.2 Summarising data and the concept of (minimal) sufficiency","text":"","code":""},{"path":"optimality-properties-and-conclusion.html","id":"sufficient-statistic","chapter":"6 Optimality properties and conclusion","heading":"6.2.1 Sufficient statistic","text":"Another important concept statistics likelihood theory -called sufficient statistics summarise information available data parameter model.Generally, statistic \\(T(D)\\) function observed data \\(D=\\{x_1, \\ldots, x_n\\}\\).\nstatistic \\(T(D)\\) can type value (scalar, vector, matrix etc. — even function). \\(T(D)\\) called summary statistic describes important aspects data location (e.g. average \\(\\text{avg}(D) =\\bar{x}\\), median) scale (e.g. standard deviation, interquartile range).statistic \\(T(D)\\) said sufficient \nparameter \\(\\boldsymbol \\theta\\) model corresponding likelihood function can written using \\(T(D)\\)\nterms involve \\(\\boldsymbol \\theta\\) \n\\[\nL(\\boldsymbol \\theta| D) = h( T(D) , \\boldsymbol \\theta) \\, k(D) \\,,\n\\]\n\\(h()\\) \\(k()\\) positive-valued functions, equivalently log-scale\n\\[\nl_n(\\boldsymbol \\theta) = \\log h( T(D) , \\boldsymbol \\theta) + \\log k(D) \\,.\n\\]\nknown Fisher-Pearson factorisation.construction, estimation inference \\(\\boldsymbol \\theta\\) based factorised likelihood \\(L(\\boldsymbol \\theta)\\) mediated sufficient statistic \\(T(D)\\) require original data \\(D\\). Instead, sufficient statistic \\(T(D)\\) contains information \\(D\\) required learn parameter \\(\\boldsymbol \\theta\\).Therefore, MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\) \\(\\boldsymbol \\theta\\) exists unique MLE unique function sufficient statistic \\(T(D)\\). MLE unique can chosen function \\(T(D)\\).\nNote sufficient statistic always exists since data \\(D\\)\nsufficient statistics, \\(T(D) = D\\). Furthermore, sufficient statistics unique since applying one--one transformation \n\\(T(D)\\) yields another sufficient statistic.","code":""},{"path":"optimality-properties-and-conclusion.html","id":"induced-partioning-of-data-space-and-likelihood-equivalence","chapter":"6 Optimality properties and conclusion","heading":"6.2.2 Induced partioning of data space and likelihood equivalence","text":"Every sufficient statistic \\(T(D)\\) induces partitioning space data sets\nclustering hypothetical outcomes statistic \\(T(D)\\) assumes value \\(t\\):\n\\[\\mathcal{X}_t = \\{D: T(D) = t\\}\\]\ndata sets \\(\\mathcal{X}_t\\) equivalent terms sufficient statistic \\(T(D)\\). Note implies \\(T(D)\\)\n1:1 transformation \\(D\\). Instead \\(n\\) data points \\(x_1, \\ldots, x_n\\) one two summaries (mean variance) may sufficient fully convey information data model parameters.\nThus, transforming data \\(D\\) using sufficient statistic \\(T(D)\\) may result substantial data reduction.Two data sets \\(D_1\\) \\(D_2\\) ratio corresponding\nlikelihoods\n\\(L(\\boldsymbol \\theta| D_1 )/L(\\boldsymbol \\theta| D_2)\\) depend \\(\\boldsymbol \\theta\\) (two likelihoods\nproportional constant)\ncalled likelihood equivalent likelihood-based procedure\nlearn \\(\\boldsymbol \\theta\\) draw identical conclusions \\(D_1\\) \\(D_2\\).\ndata sets \\(D_1, D_2 \\\\mathcal{X}_t\\) equivalent respect\nsufficient statistic \\(T\\)\nfollows directly Fisher-Pearson factorisation \nratio\n\\[L(\\boldsymbol \\theta| D_1 )/L(\\boldsymbol \\theta| D_2) = k(D_1)/ k(D_2)\\]\nthus constant regard \\(\\boldsymbol \\theta\\). result, data sets \\(\\mathcal{X}_t\\) likelihood equivalent.\nHowever, converse true: depending sufficient statistics usually many likelihood equivalent data\nsets part set \\(\\mathcal{X}_t\\).","code":""},{"path":"optimality-properties-and-conclusion.html","id":"minimal-sufficient-statistics","chapter":"6 Optimality properties and conclusion","heading":"6.2.3 Minimal sufficient statistics","text":"particular interest therefore find sufficient statistics achieve coarsest partitioning sample space thus may allow highest data reduction.\nSpecifically, minimal sufficient statistic sufficient statistic\nlikelihood equivalent data sets also equivalent statistic.Therefore, check whether sufficient statistic \\(T(D)\\) minimally sufficient need verify whether two likelihood equivalent data sets \\(D_1\\) \\(D_2\\)\nalso follows \\(T(D_1) = T(D_2)\\). holds true\n\\(T\\) minimally sufficient statistic.equivalent non-operational definition minimal sufficient statistic \\(T(D)\\) sufficient statistic can computed sufficient statistic \\(S(D)\\). follows directly: assume sufficient statistic \\(S(D)\\), defines\ncorresponding set \\(\\mathcal{X}_s\\) likelihood equivalent data sets. implication\n\\(D_1, D_2 \\\\mathcal{X}_s\\) necessarily also \\(\\mathcal{X}_t\\), thus\nwhenever \\(S(D_1)=S(D_2)\\) also \\(T(D_1)=T(D_2)\\), therefore\n\\(T(D_1)\\) function \\(S(D_1)\\).trivial important example \nminimal sufficient statistic likelihood function \nsince definition can computed set sufficient statistics. Thus likelihood function \\(L(\\boldsymbol \\theta)\\) captures information \\(\\boldsymbol \\theta\\) available data. words, provides optimal summary observed data regard model. Note Bayesian statistics (discussed Part 2 module) likelihood function used proxy/summary data.","code":""},{"path":"optimality-properties-and-conclusion.html","id":"example-normal-distribution","chapter":"6 Optimality properties and conclusion","heading":"6.2.4 Example: normal distribution","text":"Example 6.1  Sufficient statistics parameters normal distribution:normal model \\(N(\\mu, \\sigma^2)\\)\nparameter vector \\(\\boldsymbol \\theta= (\\mu, \\sigma^2)^T\\) log-likelihood\n\\[\nl_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2)  - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i-\\mu)^2\n\\]\nOne possible set minimal sufficient statistics \\(\\boldsymbol \\theta\\) \\(\\bar{x}\\)\n\\(\\overline{x^2}\\), can rewrite log-likelihood function\nwithout reference original data \\(x_1, \\ldots, x_n\\) follows\n\\[\nl_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2)\n-\\frac{n}{2 \\sigma^2} (\\overline{x^2} - 2 \\bar{x} \\mu + \\mu^2)\n\\]\nalternative set minimal sufficient statistics \\(\\boldsymbol \\theta\\)\nconsists \\(s^2 = \\overline{x^2} - \\bar{x}^2 = \\widehat{\\sigma^2}_{ML}\\) \n\\(\\bar{x} = \\hat{\\mu}_{ML}\\). log-likelihood written terms \\(s^2\\) \\(\\bar{x}\\) \n\\[\nl_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2)\n-\\frac{n}{2 \\sigma^2} (s^2 + (\\bar{x} - \\mu)^2 )\n\\]Note example dimension parameter\nvector \\(\\boldsymbol \\theta\\) equals dimension minimal sufficient statistic,\nfurthermore, MLEs parameters fact minimal sufficient!","code":""},{"path":"optimality-properties-and-conclusion.html","id":"mles-of-parameters-of-an-exponential-family-are-minimal-sufficient-statistics","chapter":"6 Optimality properties and conclusion","heading":"6.2.5 MLEs of parameters of an exponential family are minimal sufficient statistics","text":"conclusion Example 6.1\nholds true generally: exponential family model (normal distribution particular important case) MLEs parameters minimal sufficient statistics.\nThus, typically substantial dimension reduction raw data sufficient statistics.However, outside exponential families\nMLE necessarily minimal sufficient statistic, may even sufficient statistic.\n(minimal) sufficient statistic dimension \nparameters always exist. classic example Cauchy distribution \nminimal sufficient statistics ordered observations,\nthus MLE parameters constitute sufficient statistics, let alone minimal sufficient statistics.\nHowever, MLE course still function minimal sufficient statistic.summary, likelihood function acts perfect data summariser\n(.e. minimally sufficient statistic),\nexponential families (e.g. normal distribution) \nMLEs parameters \\(\\hat{\\boldsymbol \\theta}_{ML}\\) minimal sufficient.Finally, sufficiency clearly useful concept data reduction one needs keep mind always reference specific model. Therefore, unless one strongly believes certain model generally good idea keep (discard!) original data.","code":""},{"path":"optimality-properties-and-conclusion.html","id":"concluding-remarks-on-maximum-likelihood","chapter":"6 Optimality properties and conclusion","heading":"6.3 Concluding remarks on maximum likelihood","text":"","code":""},{"path":"optimality-properties-and-conclusion.html","id":"remark-on-kl-divergence","chapter":"6 Optimality properties and conclusion","heading":"6.3.1 Remark on KL divergence","text":"Finding model \\(F_{\\boldsymbol \\theta}\\) best approximates underlying true model \\(F_0\\)\ndone minimising relative entropy \\(D_{\\text{KL}}(F_0,F_{\\boldsymbol \\theta})\\). large sample size \\(n\\)\nmay approximate \\(F_0\\) empirical distribution \\(\\hat{F}_0\\),\nminimising \\(D_{\\text{KL}}(\\hat{F}_0,F_{\\boldsymbol \\theta})\\) yields method maximum likelihood, discussed earlier.However, since KL divergence symmetric fact two ways minimise divergence\nfixed \\(F_0\\) family \\(F_{\\boldsymbol \\theta}\\), different properties:forward KL, approximation KL: \\(\\min_{\\boldsymbol \\theta} D_{\\text{KL}}(F_0,F_{\\boldsymbol \\theta})\\)\nNote keep first argument fixed minimise KL changing second argument.\nalso called “M (Moment) projection”. zero avoiding property:\n\\(f_{\\boldsymbol \\theta}(x)>0 \\text{ whenever } f_0(x)>0\\).\nprocedure mean-seeking inclusive, .e. multiple modes density \\(F_0\\) fitted unimodal density \\(F_{\\hat{\\boldsymbol \\theta}}\\) seek cover modes.forward KL, approximation KL: \\(\\min_{\\boldsymbol \\theta} D_{\\text{KL}}(F_0,F_{\\boldsymbol \\theta})\\)Note keep first argument fixed minimise KL changing second argument.also called “M (Moment) projection”. zero avoiding property:\n\\(f_{\\boldsymbol \\theta}(x)>0 \\text{ whenever } f_0(x)>0\\).procedure mean-seeking inclusive, .e. multiple modes density \\(F_0\\) fitted unimodal density \\(F_{\\hat{\\boldsymbol \\theta}}\\) seek cover modes.reverse KL, inference KL: \\(\\min_{\\boldsymbol \\theta} D_{\\text{KL}}(F_{\\boldsymbol \\theta},F_0)\\)\nNote keep second argument fixed minimise KL changing first argument.\nalso called “(Information) projection”. zero forcing property:\n\\(f_{\\boldsymbol \\theta}(x)=0 \\text{ whenever } f_0(x)=0\\).\nprocedure mode-seeking exclusive, .e. multiple modes density \\(F_0\\) fitted unimodal density \\(F_{\\hat{\\boldsymbol \\theta}}\\) seek one mode exclusion others.reverse KL, inference KL: \\(\\min_{\\boldsymbol \\theta} D_{\\text{KL}}(F_{\\boldsymbol \\theta},F_0)\\)Note keep second argument fixed minimise KL changing first argument.also called “(Information) projection”. zero forcing property:\n\\(f_{\\boldsymbol \\theta}(x)=0 \\text{ whenever } f_0(x)=0\\).procedure mode-seeking exclusive, .e. multiple modes density \\(F_0\\) fitted unimodal density \\(F_{\\hat{\\boldsymbol \\theta}}\\) seek one mode exclusion others.Maximum likelihood based “forward KL”, whereas Bayesian updating Variational Bayes\napproximations use “reverse KL”.","code":""},{"path":"optimality-properties-and-conclusion.html","id":"what-happens-if-n-is-small","chapter":"6 Optimality properties and conclusion","heading":"6.3.2 What happens if \\(n\\) is small?","text":"long list optimality properties ML\nclear large sample size \\(n\\) best estimator typically MLE.However, small sample size indeed possible (necessary) improve MLE (e.g. via Bayesian estimation regularisation). ideas discussed Part II.Likelihood overfit!Alternative methods need used:regularised/penalised likelihoodBayesian methodswhich essentially two sides coin.Classic example simple non-ML estimator better MLE:\nStein’s example / Stein paradox (C. Stein, 1955):Problem setting: estimation mean multivariate caseProblem setting: estimation mean multivariate caseMaximum likelihood estimation breaks ! \\(\\rightarrow\\) average (=MLE) worse terms MSE Stein estimator.Maximum likelihood estimation breaks ! \\(\\rightarrow\\) average (=MLE) worse terms MSE Stein estimator.small \\(n\\) asymptotic distributions MLE LRT accurate, inference situations distributions may need obtained simulation\n(e.g. parametric nonparametric bootstrap).small \\(n\\) asymptotic distributions MLE LRT accurate, inference situations distributions may need obtained simulation\n(e.g. parametric nonparametric bootstrap).","code":""},{"path":"optimality-properties-and-conclusion.html","id":"model-selection","chapter":"6 Optimality properties and conclusion","heading":"6.3.3 Model selection","text":"CI sets models statistically distinguishable best ML modelin doubt, choose simplest model compatible databetter prediction, avoids overfittingUseful model exploration model building.Note , construction, model parameters always higher likelihood, implying likelihood favours complex modelsNote , construction, model parameters always higher likelihood, implying likelihood favours complex modelsComplex model may overfit!Complex model may overfit!comparison models penalised likelihood Bayesian approaches may necessaryFor comparison models penalised likelihood Bayesian approaches may necessaryModel selection small samples high dimension challengingModel selection small samples high dimension challengingRecall aim statistics rejecting models (easy large sample size model rejected!)Recall aim statistics rejecting models (easy large sample size model rejected!)Instead, aim model building, .e. find model explains data well predicts well!Instead, aim model building, .e. find model explains data well predicts well!Typically, best-fit ML model, rather simpler model close enough best / complex model.Typically, best-fit ML model, rather simpler model close enough best / complex model.","code":""},{"path":"conditioning-and-bayes-rule.html","id":"conditioning-and-bayes-rule","chapter":"7 Conditioning and Bayes rule","heading":"7 Conditioning and Bayes rule","text":"chapter review conditional probabilities. Conditional probability essential Bayesian statistical modelling.","code":""},{"path":"conditioning-and-bayes-rule.html","id":"conditional-probability","chapter":"7 Conditioning and Bayes rule","heading":"7.1 Conditional probability","text":"Assume two random variables \\(x\\) \\(y\\) joint density (joint PMF) \\(p(x,y)\\).\ndefinition \\(\\int_{x,y} p(x,y) dx dy = 1\\).marginal densities individual \\(x\\) \\(y\\) given \\(p(x) = \\int_y p(x,y) dy\\)\n\\(p(y) = \\int_x f(x,y) dx\\). Thus, computing marginal densities variable removed\njoint density integrating possible states variable.\nfollows also \\(\\int_x p(x) dx = 1\\) \\(\\int_y p(y) dy = 1\\), .e. \nmarginal densities also integrate 1.alternative integrating random variable joint density \\(p(x,y)\\)\nmay wish keep fixed value, say keep \\(y\\) fixed \\(y_0\\).\ncase \\(p(x, y=y_0)\\) proportional conditional density (PMF)\ngiven ratio\n\\[\np(x | y=y_0) = \\frac{p(x, y=y_0)}{p(y=y_0)}\n\\]\ndenominator \\(p(y=y_0) = \\int_x p(x, y=y_0) dx\\) \nneeded ensure \\(\\int_x p(x | y=y_0) dx = 1\\), thus renormalises\n\\(p(x, y=y_0)\\) proper density.simplify notation, specific value variable conditioned often left \njust write \\(p(x | y)\\).","code":""},{"path":"conditioning-and-bayes-rule.html","id":"bayes-theorem","chapter":"7 Conditioning and Bayes rule","heading":"7.2 Bayes’ theorem","text":"Thomas Bayes (1701-1761) \nfirst state Bayes’ theorem\nconditional probabilities.Using definition conditional probabilities\nsee joint density can written \nproduct marginal conditional density two different ways:\n\\[\np(x,y) = p(x| y) p(y) = p(y | x) p(x)\n\\]directly leads Bayes’ theorem:\n\\[\np(x | y) = p(y | x) \\frac{ p(x) }{ p(y)}\n\\]\nrule relates two possible conditional densities (conditional probability mass functions) two random variables \\(x\\) \\(y\\). thus allows reverse ordering conditioning.Bayes’s theorem published 1763 death Richard Price (1723-1791):Pierre-Simon Laplace independently published Bayes’ theorem 1774 fact first routinely apply statistical calculations.","code":""},{"path":"conditioning-and-bayes-rule.html","id":"conditional-mean-and-variance","chapter":"7 Conditioning and Bayes rule","heading":"7.3 Conditional mean and variance","text":"mean \\(\\text{E}(x| y)\\) variance \\(\\text{Var}(x|y)\\) conditional distribution \ndensity \\(p(x|y)\\) called conditional mean conditional variance.law total expectation states \n\\[\n\\text{E}(x) = \\text{E}( \\text{E}(x| y) )\n\\]law total variance states \n\\[\n\\text{Var}(x) = \\text{Var}(\\text{E}(x| y)) + \\text{E}(\\text{Var}(x|y))\n\\]\nfirst term “explained” “-group” variance, second “unexplained”\n“mean within group” variance.Example 7.1  Mean variance mixture model:Assume \\(K\\) groups indicated discrete variable \\(y = 1, 2, \\ldots, K\\) probability \\(p(y) = \\pi_y\\). group\nobservations \\(x\\) follow density \\(p(x|y)\\) conditional mean \\(E(x|y) = \\mu_y\\) conditional variance \\(\\text{Var}(x| y)= \\sigma^2_y\\). joint density \\(x\\) \\(y\\) \n\\(p(x, y) = \\pi_y p(x|y)\\).\nmarginal density \\(x\\) \n\\(\\sum_{y=1}^K \\pi_y p(x|y)\\). called mixture model.total mean \\(\\text{E}(x) = \\mu_0\\) equal \\(\\sum_{y=1}^K \\pi_y \\mu_y\\).total variance \\(\\text{Var}(x) = \\sigma^2_0\\) equal \n\\[\n\\sum_{y=1}^K \\pi_y (\\mu_y - \\mu_0)^2 + \\sum_{y=1}^K \\pi_y \\sigma^2_y\n\\]","code":""},{"path":"conditioning-and-bayes-rule.html","id":"conditional-entropy-and-entropy-chain-rules","chapter":"7 Conditioning and Bayes rule","heading":"7.4 Conditional entropy and entropy chain rules","text":"entropy joint distribution find \n\\[\n\\begin{split}\nH( P_{x,y}) &= -\\text{E}_{P_{x,y}} \\log p(x, y) \\\\\n&= -\\text{E}_{P_x} \\text{E}_{P_{y| x}} (\\log p(x) + \\log p(y| x)\\\\\n&= -\\text{E}_{P_x} \\log p(x) - \\text{E}_{P_x} \\text{E}_{P_{y| x}} \\log p(y| x)\\\\\n&= H(P_{x}) + H(P_{y| x} ) \\\\\n\\end{split}\n\\]\nthus decomposes entropy marginal distribution \nconditional entropy defined \n\\[\nH(P_{y| x} ) = - \\text{E}_{P_x} \\text{E}_{P_{y| x}} \\log p(y| x)\n\\]\nNote simplify notation convention expectation \\(\\text{E}_{P_{x}}\\) variable \\(x\\) condition (\\(x\\)) implicitly assumed.Similarly, cross-entropy get\n\\[\n\\begin{split}\nH(Q_{x,y} , P_{x, y}) &= -\\text{E}_{Q_{x,y}} \\log  p(x, y) \\\\\n&= -\\text{E}_{Q_x} \\text{E}_{Q_{y| x}} \\log \\left(\\, p(x)\\, p(y| x)\\, \\right)\\\\\n  &= -\\text{E}_{Q_x} \\log p(x)    -\\text{E}_{Q_x} \\text{E}_{Q_{y| x}}  \\log  p(y| x)         \\\\\n&= H(Q_x, P_x)  +  H(Q_{y|x}, P_{y|x})\n\\end{split}\n\\]\nconditional cross-entropy defined \n\\[\nH(Q_{y|x}, P_{y|x})= -\\text{E}_{Q_x} \\text{E}_{Q_{y| x}}  \\log  p(y| x)\n\\]\nNote implicit expectation \\(\\text{E}_{Q_x}\\) \\(x\\) implied notation.KL divergence joint distributions can decomposed follows:\n\\[\n\\begin{split}\nD_{\\text{KL}}(Q_{x,y} , P_{x, y}) &= \\text{E}_{Q_{x,y}} \\log \\left(\\frac{ q(x, y) }{ p(x, y) }\\right)\\\\\n&= \\text{E}_{Q_x} \\text{E}_{Q_{y| x}} \\log \\left(\\frac{ q(x) q(y| x) }{ p(x) p(y| x) }\\right)\\\\\n&= \\text{E}_{Q_x} \\log \\left(\\frac{ q(x) }{ p(x) }\\right)      + \\text{E}_{Q_x} \\text{E}_{Q_{y| x}}  \\log \\left(\\frac{  q(y| x) }{ p(y| x) }\\right)          \\\\\n&= D_{\\text{KL}}(Q_{x} , P_{x}) +   D_{\\text{KL}}(Q_{y| x} , P_{y|x}) \\\\\n\\end{split}\n\\]\nconditional KL divergence conditional relative entropy defined \n\\[\nD_{\\text{KL}}(Q_{y| x} , P_{y|x})  = \\text{E}_{Q_x} \\text{E}_{Q_{y| x}}  \\log \\left(\\frac{  q(y| x) }{ p(y| x) }\\right)\n\\]\n(expectation \\(\\text{E}_{Q_{x}}\\) usually dropped convenience).\nconditional relative entropy can also computed conditional (cross-)entropies \n\\[\nD_{\\text{KL}}(Q_{y| x} , P_{y|x}) = H(Q_{y|x}, P_{y|x})\n-  H(Q_{y| x})\n\\]decompositions entropy, cross-entropy relative entropy known entropy chain rules.","code":""},{"path":"conditioning-and-bayes-rule.html","id":"entropy-bounds-for-the-marginal-variables","chapter":"7 Conditioning and Bayes rule","heading":"7.5 Entropy bounds for the marginal variables","text":"chain rule KL divergence\ndirectly shows \n\\[\n\\begin{split}\n\\underbrace{D_{\\text{KL}}(Q_{x,y} , P_{x, y})}_{\\text{upper bound}} &= D_{\\text{KL}}(Q_{x} , P_{x}) + \\underbrace{  D_{\\text{KL}}(Q_{y| x} , P_{y|x})   }_{\\geq 0}\\\\\n&\\geq D_{\\text{KL}}(Q_{x} , P_{x})\n\\end{split}\n\\]\nmeans KL divergence joint distributions\nforms upper bound KL divergence marginal distributions, difference given conditional KL divergence \\(D_{\\text{KL}}(Q_{y| x} , P_{y|x})\\).Equivalently, can state upper bound marginal cross-entropy:\n\\[\n\\begin{split}\n\\underbrace{H(Q_{x,y} , P_{x, y}) - H(Q_{y| x} )}_{\\text{upper bound}} &= H(Q_{x}, P_{x})  + \\underbrace{ D_{\\text{KL}}(Q_{y| x} , P_{y|x}) }_{\\geq 0}\\\\\n& \\geq H(Q_{x}, P_{x}) \\\\\n\\end{split}\n\\]\nInstead upper bound may well express lower bound negative marginal cross-entropy\n\\[.\n\\begin{split}\n- H(Q_{x}, P_{x}) &= \\underbrace{ - H(Q_{x} Q_{y| x} , P_{x, y})  + H(Q_{y| x} )}_{\\text{lower bound}}  + \\underbrace{ D_{\\text{KL}}(Q_{y| x} , P_{y|x})}_{\\geq 0}\\\\\n& \\geq F\\left( Q_{x}, Q_{y| x},  P_{x, y}\\right)\\\\\n\\end{split}\n\\]Since entropy KL divergence closedly linked maximum likelihood bounds play major role statistical learning\nmodels unobserved latent variables (\\(y\\)).\nform basis important methods EM algorithm well variational Bayes.","code":""},{"path":"models-with-latent-variables-and-missing-data.html","id":"models-with-latent-variables-and-missing-data","chapter":"8 Models with latent variables and missing data","heading":"8 Models with latent variables and missing data","text":"","code":""},{"path":"models-with-latent-variables-and-missing-data.html","id":"complete-data-log-likelihood-versus-observed-data-log-likelihood","chapter":"8 Models with latent variables and missing data","heading":"8.1 Complete data log-likelihood versus observed data log-likelihood","text":"frequently case need employ models \nvariables observable corresponding data missing.example consider two random variables \\(x\\) \\(y\\) joint density\n\\[\np(x, y| \\boldsymbol \\theta)\n\\]\nparameters \\(\\boldsymbol \\theta\\). observe data \\(D_x = \\{ x_1, \\ldots, x_n\\}\\)\n\\(D_y = \\{ y_1, \\ldots, y_n\\}\\) \\(n\\) samples can use complete data log-likelihood\n\\[\nl_n(\\boldsymbol \\theta| D_x, D_y) = \\sum_{=1}^n  \\log p(x_i, y_i| \\boldsymbol \\theta)\n\\]\nestimate \\(\\boldsymbol \\theta\\). Recall \n\\[\nl_n(\\boldsymbol \\theta| D_x, D_y) =-n H(\\hat{Q}_{x,y}, P_{x, y|\\boldsymbol \\theta})\n\\]\n\\(\\hat{Q}_{x,y}\\) empirical joint distribution based \\(D_x\\) \\(D_y\\) \n\\(P_{x, y|\\boldsymbol \\theta}\\) joint model, maximising complete data log-likelihood minimises\ncross-entropy \\(H(\\hat{Q}_{x,y}, P_{x, y|\\boldsymbol \\theta})\\).Now assume \\(y\\) observable hence -called latent variable. don’t observations \\(D_y\\) therefore use complete data likelihood.\nInstead, maximum likelihood estimation missing data\nneed use observed data log-likelihood.joint density obtain marginal density \\(x\\) integrating unobserved variable \\(y\\):\n\\[\np(x | \\boldsymbol \\theta) = \\int_y  p(x, y| \\boldsymbol \\theta) dy\n\\]\nUsing marginal model compute observed data log-likelihood\n\\[\nl_n(\\boldsymbol \\theta| D_x) = \\sum_{=1}^n  \\log p(x_i| \\boldsymbol \\theta) =\\sum_{=1}^n \\log \\int_y  p(x_i, y| \\boldsymbol \\theta) dy\n\\]\nNote data \\(D_x\\) used.Maximum likelihood estimation based marginal model proceeds usual maximising corresponding\nobserved data likelihood function \n\\[\nl_n(\\boldsymbol \\theta| D_x) = -n H(\\hat{Q}_{x}, P_{x|\\boldsymbol \\theta})\n\\]\n\\(\\hat{Q}_{x}\\) empirical distribution based \\(D_x\\) \\(P_{x|\\boldsymbol \\theta}\\)\nmodel family. Hence, maximising observed data log-likelihood minimises cross-entropy\n\\(H(\\hat{Q}_{x}, P_{x|\\boldsymbol \\theta})\\).Example 8.1  Two group normal mixture model:Assume two groups labelled \\(y=1\\) \\(y=2\\) (thus variable \\(y\\) discrete). data \\(x\\) observed group normal means \\(\\mu_1\\) \\(\\mu_2\\) variances\n\\(\\sigma^2_1\\) \\(\\sigma^2_2\\), respectively.\nprobability group \\(1\\) \\(\\pi_1 = p\\) probability group \\(2\\) \\(\\pi_2=1-p\\).\ndensity joint model \\(x\\) \\(y\\) \n\\[\np(x, y| \\boldsymbol \\theta)  = \\pi_y N(x| \\mu_y, \\sigma_y)\n\\]\nmodel parameters \\(\\boldsymbol \\theta= (p, \\mu_1, \\mu_2, \\sigma^2_1, \\sigma^2_2)^T\\)\ncan inferred complete data comprised \\(D_x = \\{x_1, \\ldots, x_n\\}\\) group allocations \\(D_y=\\{y_1, \\ldots, y_n\\}\\) sample using complete data log-likelihood\n\\[\nl_n(\\boldsymbol \\theta| D_x, D_y  ) =\\sum_{=1}^n  \\log \\pi_{y_i} + \\sum_{=1}^n \\log  N(x_i| \\mu_{y_i}, \\sigma_{y_i})\n\\]However, typically know class allocation \\(y\\) thus need use marginal\nmodel \\(x\\) alone density\n\\[\n\\begin{split}\np(x| \\boldsymbol \\theta) &= \\sum_{y=1}^2 \\pi_y N(\\mu_y, \\sigma^2_y) \\\\\n&= p N(x| \\mu_1, \\sigma^2_1) + (1-p)  N(x | \\mu_2, \\sigma^2_2)\\\\\n\\end{split}\n\\]\nexample two-component mixture model.\ncorresponding observed data log-likelihood \n\\[\nl_n(\\boldsymbol \\theta| D_x ) = \\sum_{=1}^n  \\log \\sum_{y=1}^2 \\pi_y N(x |\\mu_y, \\sigma^2_y)\n\\]\nNote form observed data log-likelihood complex \ncomplete data log-likelihood contains logarithm sum simplified.\nused estimate model parameters \\(\\boldsymbol \\theta\\) \\(D_x\\) without\nrequiring knowledge class allocations \\(D_y\\).Example 8.2  Alternative computation observed data likelihood:alternative way arrive observed data likelihood marginalise complete data likelihood.\n\\[\nL_n(\\boldsymbol \\theta| D_x, D_y) = \\prod_{=1}^n p(x_i, y_i| \\boldsymbol \\theta)\n\\]\n\n\\[\nL_n(\\boldsymbol \\theta| D_x) = \\int_{y_1, \\ldots, y_n} \\prod_{=1}^n p(x_i, y_i| \\boldsymbol \\theta) dy_1 \\ldots dy_n\n\\]\nintegration (sum) multiplication can interchanged per Generalised Distributive Law leading \n\\[\nL_n(\\boldsymbol \\theta| D_x) =  \\prod_{=1}^n \\int_{y} p(x_i, y| \\boldsymbol \\theta) dy\n\\]\nconstructing likelihood marginal density.","code":""},{"path":"models-with-latent-variables-and-missing-data.html","id":"estimation-of-the-unobservable-latent-states-using-bayes-theorem","chapter":"8 Models with latent variables and missing data","heading":"8.2 Estimation of the unobservable latent states using Bayes theorem","text":"estimating marginal model straightforward obtain \nprobabilistic prediction state latent variables \\(y_1, \\ldots, y_n\\).\nSince\n\\[\np(x, y | \\boldsymbol \\theta) = p( x|\\boldsymbol \\theta) \\, p(y | x, \\boldsymbol \\theta) =  p( y|\\boldsymbol \\theta) \\, p(x | y, \\boldsymbol \\theta)\n\\]\ngiven estimate \\(\\hat{\\boldsymbol \\theta}\\) able compute observation \\(x_i\\)\n\\[\np(y_i | x_i , \\hat{\\boldsymbol \\theta}) = \\frac{p(x_i, y_i | \\hat{\\boldsymbol \\theta} ) }{p(x_i|\\hat{\\boldsymbol \\theta})}\n=\\frac{  p( y_i|\\hat{\\boldsymbol \\theta}) \\, p(x_i | y_i, \\hat{\\boldsymbol \\theta})     }{p(x_i|\\hat{\\boldsymbol \\theta})}\n\\]\nprobabilities / densities states \\(y_i\\)\n(note application Bayes’ theorem).Example 8.3  Latent states two group normal mixture model:Continuing Example 8.1 assume marginal model fitted \nparameter values \\(\\hat{\\boldsymbol \\theta} = (\\hat{p},\\hat{\\mu}_1, \\hat{\\mu}_2, \\widehat{\\sigma^2_1}, \\widehat{\\sigma^2_2} )^T\\).\nsample \\(x_i\\) can get\nprobabilistic prediction group assocation sample \n\\[\np(y_i | x_i, \\hat{\\boldsymbol \\theta}) = \\frac{\\hat{\\pi}_{y_i} N(x_i| \\hat{\\mu}_{y_i}, \\widehat{\\sigma^2_{y_i}})}{\\hat{p} N(x_i| \\hat{\\mu}_1, \\widehat{\\sigma^2_1}) + (1-\\hat{p})  N(x_i | \\hat{\\mu}_2,  \\widehat{\\sigma^2_2})}\n\\]","code":""},{"path":"models-with-latent-variables-and-missing-data.html","id":"em-algorithm","chapter":"8 Models with latent variables and missing data","heading":"8.3 EM Algorithm","text":"Computing maximising observed data log-likelihood can difficult\nintegration unobserved variable (summation case discrete latent variable). contrast, complete data log-likelihood function may easier compute.widely used EM algorithm, formally described Dempster others (1977) also used , addresses problem maximises observed data log-likelihood indirectly iterative procedure comprising two steps:First (“E” step), missing data \\(D_y\\) imputed using Bayes’ theorem. provides probabilities (“soft allocations”) possible state latent variable.Subsequently (“M” step), expected complete data log-likelihood function computed,\nexpectation taken regard distribution latent states, \nmaximised regard \\(\\boldsymbol \\theta\\) estimate model parameters.EM algorithm leads exact estimates observed data log-likelihood optimised directly. Therefore EM algorithm fact approximation, just different way find MLEs.EM algorithm application clustering discussed detail module MATH38161 Multivariate Statistics Machine Learning.nutshell, justication EM algorithm follows entropy chain rules corresponding bounds, \\(D_{\\text{KL}}(Q_{x,y} , P_{x, y}) \\geq D_{\\text{KL}}(Q_{x} , P_{x})\\) (see previous chapter). Given observed data \\(x\\) know empirical distribution \\(\\hat{Q}_x\\).\nHence, minimising \\(D_{\\text{KL}}( \\hat{Q}_{x} Q_{y| x}, P_{x, y}^{\\boldsymbol \\theta})\\) iterativelywith regard \\(Q_{y| x}\\) (“E” step) andwith regard parameters \\(\\boldsymbol \\theta\\) \\(P_{x, y}^{\\boldsymbol \\theta}\\) (“M” step”)one minimises \\(D_{\\text{KL}}(\\hat{Q}_{x} , P_{x}^{\\boldsymbol \\theta})\\) regard parameters \\(P_{x}^{\\boldsymbol \\theta}\\).Interestingly, “E” step first argument KL divergence optimised (“” projection) “M” step second argument (“M” projection).Alternatively, instead bounding marginal KL divergence one can also either minimise upper bound cross-entropy maximise lower bound negative cross-entropy.\nthree procedures yield EM algorithm.Note optimisation entropy bound “E” step\nrequires variational calculus since argument distribution!\nEM algorithm therefore fact special case variational Bayes algorithm\nsince provides estimates \\(\\boldsymbol \\theta\\) also yields distribution latent states means calculus variations.Finally, see can learn unobservable states means Bayes theorem. extending principle learning parameters models arrive Bayesian learning.","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"essentials-of-bayesian-statistics","chapter":"9 Essentials of Bayesian statistics","heading":"9 Essentials of Bayesian statistics","text":"","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"principle-of-bayesian-learning","chapter":"9 Essentials of Bayesian statistics","heading":"9.1 Principle of Bayesian learning","text":"","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"from-prior-to-posterior-distribution","chapter":"9 Essentials of Bayesian statistics","heading":"9.1.1 From prior to posterior distribution","text":"Bayesian statistical learning applies Bayes’ theorem update state knowledge parameter light data.Ingredients:\\(\\boldsymbol \\theta\\) parameter(s) interest, unknown fixed.prior distribution density \\(p(\\boldsymbol \\theta)\\) describing uncertainty (randomness!) \\(\\boldsymbol \\theta\\)data generating process \\(p(x | \\boldsymbol \\theta)\\)Note model underlying Bayesian approach joint distribution\n\\[\np(\\boldsymbol \\theta, x) = p(\\boldsymbol \\theta) p(x | \\boldsymbol \\theta)\n\\]\nprior distribution parameters well data generating process specified.Question: new information form new observation \\(x\\) arrives - uncertainty \\(\\boldsymbol \\theta\\) change?Answer: use Bayes’ theorem update prior density posterior density.\\[\n\\underbrace{p(\\boldsymbol \\theta| x)}_{\\text{posterior} } = \\underbrace{p(\\boldsymbol \\theta)}_{\\text{prior}} \\frac{p(x | \\boldsymbol \\theta) }{ p(x)}\n\\]denominator Bayes formula need compute \\(p(x)\\).\nobtained \\[\n\\begin{split}\np(x) &= \\int_{\\boldsymbol \\theta} p(x , \\boldsymbol \\theta) d\\boldsymbol \\theta\\\\\n&= \\int_{\\boldsymbol \\theta} p(x | \\boldsymbol \\theta) p(\\boldsymbol \\theta) d\\boldsymbol \\theta\\\\\n\\end{split}\n\\]\n.e. marginalisation parameter \\(\\boldsymbol \\theta\\) joint\ndistribution \\(\\boldsymbol \\theta\\) \\(x\\).\n(discrete \\(\\boldsymbol \\theta\\) replace integral sum).\nDepending context quantity either called thenormalisation constant ensures posterior density \\(p(\\boldsymbol \\theta| x)\\)\nintegrates one.prior predictive density data \\(x\\) given model \\(M\\) seeing data. emphasise implicit conditioning\nmodel may write \\(p(x| M)\\). Since parameters integrated \\(M\\) fact refers model class.marginal likelihood underlying model (class) \\(M\\) given data \\(x\\). emphasise may write \\(L(M| x)\\). Sometimes also called model likelihood.","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"zero-forcing-property","chapter":"9 Essentials of Bayesian statistics","heading":"9.1.2 Zero forcing property","text":"easy see Bayes rule prior density/probability zero parameter value \\(\\boldsymbol \\theta\\) posterior density/probability remain zero \\(\\boldsymbol \\theta\\), regardless data collected. zero-forcing property Bayes update rule called Cromwell’s rule Dennis Lindley (1923–2013). Therefore, assigning prior density/probability 0 event avoided.Note implies assigning prior probability 1 avoided, .","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"bayesian-update-and-likelihood","chapter":"9 Essentials of Bayesian statistics","heading":"9.1.3 Bayesian update and likelihood","text":"independent identically distributed data \\(D = \\{x_1, \\ldots, x_n\\}\\) observed Bayesian posterior computed \\[\n\\underbrace{p(\\boldsymbol \\theta| D) }_{\\text{posterior} } = \\underbrace{p(\\boldsymbol \\theta)}_{\\text{prior}} \\frac{ L(\\boldsymbol \\theta| D) }{ p(D)}\n\\]\ninvolving likelihood \\(L(\\boldsymbol \\theta| D) = \\prod_{=1}^n p(x_i | \\boldsymbol \\theta)\\)\nmarginal likelihoood \\(p(D) = \\int_{\\boldsymbol \\theta} p(\\boldsymbol \\theta) L(\\boldsymbol \\theta| D) d\\boldsymbol \\theta\\) \\(\\boldsymbol \\theta\\) integrated .marginal likelihood serves standardising factor posterior density \\(\\boldsymbol \\theta\\) integrates 1:\n\\[\n\\int_{\\boldsymbol \\theta} p(\\boldsymbol \\theta| D) d\\boldsymbol \\theta= \\frac{1}{p(D)} \\int_{\\boldsymbol \\theta} p(\\boldsymbol \\theta) L(\\boldsymbol \\theta| D) d\\boldsymbol \\theta= 1\n\\]\nUnfortunately, integral compute marginal likelihood typically analytically intractable requires\nnumerical integration /approximation.Comparing likelihood Bayes procedures note thatconducting Bayesian statistical analysis requires integration respectively averaging (compute marginal likelihood)contrast likelihood analysis requires optimisation (find maximum likelihood).","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"sequential-updates","chapter":"9 Essentials of Bayesian statistics","heading":"9.1.4 Sequential updates","text":"Note Bayesian update procedure can repeated : can use posterior new prior update data. Thus, may also update posterior density sequentially, data points \\(x_1, \\ldots, x_n\\) arriving one , computing first \\(p(\\boldsymbol \\theta| x_1)\\), \\(p(\\boldsymbol \\theta| x_1, x_2)\\) reach \\(p(\\boldsymbol \\theta| x_1, \\ldots, x_n) = p(\\boldsymbol \\theta| D)\\).example, first update \n\\[\np(\\boldsymbol \\theta| x_1) =  p(\\boldsymbol \\theta)   \\frac{p(x_1 | \\boldsymbol \\theta)  }{p(x_1)}\n\\]\n\\(p(x_1) =\\int_{\\boldsymbol \\theta} p(x_1 | \\boldsymbol \\theta) p(\\boldsymbol \\theta) d\\boldsymbol \\theta\\).\nsecond update yields\n\\[\n\\begin{split}\np(\\boldsymbol \\theta| x_1, x_2) &=  p(\\boldsymbol \\theta| x_1)   \\frac{p(x_2 | \\boldsymbol \\theta, x_1)  }{p(x_2| x_1)}\\\\\n&= p(\\boldsymbol \\theta| x_1)   \\frac{p(x_2 | \\boldsymbol \\theta)  }{p(x_2| x_1)}\\\\\n&=  p(\\boldsymbol \\theta) \\frac{  p(x_1 | \\boldsymbol \\theta)    p(x_2 | \\boldsymbol \\theta)  }{p(x_1) p(x_2| x_1)}\\\\\n\\end{split}\n\\]\n\\(p(x_2| x_1) = \\int_{\\boldsymbol \\theta} p(x_2 | \\boldsymbol \\theta) p(\\boldsymbol \\theta| x_1) d\\boldsymbol \\theta\\).\nfinal step \n\\[\n\\begin{split}\np(\\boldsymbol \\theta| D)  = p(\\boldsymbol \\theta| x_1, \\ldots, x_n) &=   p(\\boldsymbol \\theta) \\frac{ \\prod_{=1}^n p(x_i | \\boldsymbol \\theta)  }{ p(D)  }\\\\\n\\end{split}\n\\]\nmarginal likelihood factorising \n\\[\np(D) = \\prod_{=1}^n p(x_i| x_{<})\n\\]\n\n\\[\np(x_i| x_{<}) = \\int_{\\boldsymbol \\theta} p(x_i | \\boldsymbol \\theta) p(\\boldsymbol \\theta| x_{<}) d\\boldsymbol \\theta\n\\]\nlast factor posterior predictive density new data \\(x_i\\) seeing data \\(x_1, \\ldots, x_{-1}\\) (given model class \\(M\\)).\nstraightforward understand probability new \\(x_i\\) depends previously observed data points — uncertainty model parameter \\(\\boldsymbol \\theta\\) depends much data already observed. Therefore marginal likelihood \\(p(D)\\) simply product\nmarginal densities \\(p(x_i)\\) \\(x_i\\) instead product conditional densities \\(p(x_i| x_{<})\\).parameter fully known uncertainty \\(\\boldsymbol \\theta\\) observations\n\\(x_i\\) independent. leads back standard likelihood condition particular \\(\\boldsymbol \\theta\\) likelihood product \\(p(D| \\boldsymbol \\theta) = \\prod_{=1}^n p(x_i| \\boldsymbol \\theta)\\).","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"summaries-of-posterior-distributions-and-credible-intervals","chapter":"9 Essentials of Bayesian statistics","heading":"9.1.5 Summaries of posterior distributions and credible intervals","text":"Bayesian estimate full complete posterior distribution!However, useful summarise aspects posterior distribution:Posterior mean \\(\\text{E}(\\boldsymbol \\theta| D)\\)Posterior variance \\(\\text{Var}(\\boldsymbol \\theta| D)\\)Posterior mode\netc.particular mean posterior distribution often taken Bayesian point estimate.posterior distribution also allows define credible regions credible intervals.\nBayesian equivalent confidence intervals constructed \nfinding areas highest probability mass (say 95%) posterior distribution.Bayesian credible intervals (unlike frequentist confidence counterparts) thus easy interpret - simply correspond area parameter space can find parameter given specified probability.\ncontrast, frequentist statistics make sense assign \nprobability parameter value!Note typically many credible intervals given specified coverage \\(\\alpha\\) (say 95%). Therefore, may need criteria\nconstruct intervals.univariate parameter \\(\\theta\\) two-sided equal-tail credible interval obtained finding corresponding lower \\(1-\\alpha/2\\)\nupper \\(\\alpha/2\\) quantiles. Typically type credible interval easy compute. However, note density values left right boundary points interval typically different.\nAlso generalise well multivariate parameter \\(\\boldsymbol \\theta\\).alternative, highest posterior density (HPD) credible interval coverage \\(\\alpha\\) found identifying shortest interval (.e. smallest support) given \\(\\alpha\\)\nprobability mass. point within HDP credible interval higher density point outside HDP credible interval. Correspondingly, density\nboundary HPD credible interval constant taking value everywhere along boundary.Bayesian HPD credible interval constructed similar fashion likelihood-based confidence interval, starting mode posterior density looking common threshold value density define boundary credible interval. posterior density multiple modes HPD interval may disjoint. HPD intervals also well defined multivariate \\(\\boldsymbol \\theta\\) boundaries given contour lines posterior density resulting threshold value.Worksheet B1 examples types credible intervals given compared visually.","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"practical-application-of-bayes-statistics-on-the-computer","chapter":"9 Essentials of Bayesian statistics","heading":"9.1.6 Practical application of Bayes statistics on the computer","text":"seen Bayesian learning conceptually straightforward:Specify prior uncertainty \\(p(\\boldsymbol \\theta\\)) parameters interest \\(\\boldsymbol \\theta\\).Specify data generating process specified parameter: \\(p(x | \\boldsymbol \\theta)\\).Apply Bayes’ theorem update prior uncertainty light\nnew data.practise, however, computing posterior distribution can computationally demanding, especially\ncomplex models.reason specialised software packages developed computational Bayesian modelling, example:Bayesian statistics R: https://cran.r-project.org/web/views/Bayesian.htmlBayesian statistics R: https://cran.r-project.org/web/views/Bayesian.htmlStan probabilistic programming language (interfaces R, Python, Julia languages) — https://mc-stan.org/Stan probabilistic programming language (interfaces R, Python, Julia languages) — https://mc-stan.org/Bayesian statistics Python:\nPyMC using Aesara/JAX backend,\nNumPyro using JAX backend,\nTensorFlow Probability JAX using JAX backend,\nPyMC3 using Theano backend,\nPyro using PyTorch backend,\nTensorFlow Probability using Tensorflow backend.Bayesian statistics Python:\nPyMC using Aesara/JAX backend,\nNumPyro using JAX backend,\nTensorFlow Probability JAX using JAX backend,\nPyMC3 using Theano backend,\nPyro using PyTorch backend,\nTensorFlow Probability using Tensorflow backend.Bayesian statistics Julia: Turing.jlBayesian statistics Julia: Turing.jlBayesian hierarchical modelling BUGS, JAGS NIMBLE.Bayesian hierarchical modelling BUGS, JAGS NIMBLE.addition numerical procedures sample posterior distribution also many procedures aiming approximate Bayesian posterior, employing Laplace approximation, integrated nested Laplace approximation (INLA), variational Bayes etc.","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"some-background-on-bayesian-statistics","chapter":"9 Essentials of Bayesian statistics","heading":"9.2 Some background on Bayesian statistics","text":"","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"bayesian-interpretation-of-probability","chapter":"9 Essentials of Bayesian statistics","heading":"9.2.1 Bayesian interpretation of probability","text":"","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"what-makes-you-bayesian","chapter":"9 Essentials of Bayesian statistics","heading":"9.2.1.1 What makes you “Bayesian”?","text":"use Bayes’ theorem therefore automatically Bayesian? !!Bayes’ theorem mathematical fact probability theory.\nHence, Bayes’ theorem valid everyone, whichever form \nstatistical learning subscribing (frequentist ideas,\nlikelihood methods, entropy learning, Bayesian learning).discuss now key difference Bayesian frequentist\nstatistical learning lies differences interpretation probability,\nmathematical formalism probability (includes Bayes’ theorem).","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"mathematics-of-probability","chapter":"9 Essentials of Bayesian statistics","heading":"9.2.1.2 Mathematics of probability","text":"mathematics probability modern foundation developed Andrey Kolmogorov (1903–1987). book Foundations Theory Probability (1933) establishes probability terms set theory/ measure theory. theory provides coherent mathematical framework work probabilities.However, Kolmogorov’s theory provide interpretation probability!\\(\\rightarrow\\) Kolmogorov framework basis frequentist Bayesian interpretation probability.","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"interpretations-of-probability","chapter":"9 Essentials of Bayesian statistics","heading":"9.2.1.3 Interpretations of probability","text":"Essentially, two major commonly used interpretation probability statistics - frequentist interpretation Bayesian interpretation.: Frequentist interpretationprobability = frequency (event long-running series identically repeated experiments)ontological view probability (.e. probability “exists” identical something can observed.).also restrictive view probability. example, frequentist probability\nused describe events occur single time.\nFrequentist probability thus can applied asymptotically, large samples!B: Bayesian probability“Probability exist” — famous quote Bruno de Finetti (1906–1985), Bayesian statistician.mean?Probability description state knowledge uncertainty.Probability thus epistemological quantity assigned changes rather something inherent property object.Note require repeated experiments.\nBayesian interpretation probability valid regardless sample size number repetitions experiment.Hence, key difference frequentist Bayesian approaches use Bayes’ theorem.\nRather whether consider probability ontological (frequentist) epistemological entity (Bayesian).","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"historical-developments","chapter":"9 Essentials of Bayesian statistics","heading":"9.2.2 Historical developments","text":"Bayesian statistics named Thomas Bayes (1701-1761). paper 8 introducing famous theorem published death (1763).Pierre-Simon Laplace (1749-1827) first practically use Bayes’ theorem statistical calculations, also independently discovered Bayes’ theorem 1774 9This activity called “inverse probability” “Bayesian statistics”.activity called “inverse probability” “Bayesian statistics”.1900 1940 classical mathematical statistics developed field heavily influenced dominated R.. Fisher (invented likelihood theory ANOVA, among things - also working biology professor genetics). Fisher much opposed Bayesian statistics.1900 1940 classical mathematical statistics developed field heavily influenced dominated R.. Fisher (invented likelihood theory ANOVA, among things - also working biology professor genetics). Fisher much opposed Bayesian statistics.1931 Bruno de Finetti publishes “representation theorem”. shows joint distribution sequence exchangeable events (.e. ordering can permuted) can represented mixture distribution can constructed via Bayes’ theorem. (Note exchangeability weaker condition ..d.)\ntheorem often used justification Bayesian statistics (along -called Dutch book argument, also de Finetti).1931 Bruno de Finetti publishes “representation theorem”. shows joint distribution sequence exchangeable events (.e. ordering can permuted) can represented mixture distribution can constructed via Bayes’ theorem. (Note exchangeability weaker condition ..d.)\ntheorem often used justification Bayesian statistics (along -called Dutch book argument, also de Finetti).1933 publication Andrey Kolmogorov’s book probability theory.1933 publication Andrey Kolmogorov’s book probability theory.1946 Cox theorem Richard T. Cox (1898–1991): aim generalise classical logic TRUE/FALSE statements continuous measures uncertainty inevitably leads probability theory Bayesian learning! justification Bayesian statistics later popularised Edwin T. Jaynes (1922–1998) various books (1959, 2003).1946 Cox theorem Richard T. Cox (1898–1991): aim generalise classical logic TRUE/FALSE statements continuous measures uncertainty inevitably leads probability theory Bayesian learning! justification Bayesian statistics later popularised Edwin T. Jaynes (1922–1998) various books (1959, 2003).1955 Stein Paradox - Charles M. Stein (1920–2016) publishes paper Stein estimator — estimator mean dominates ML estimator (.e. sample average). Stein estimator better terms MSE ML estimator, puzzling time easy understand Bayesian perspective.1955 Stein Paradox - Charles M. Stein (1920–2016) publishes paper Stein estimator — estimator mean dominates ML estimator (.e. sample average). Stein estimator better terms MSE ML estimator, puzzling time easy understand Bayesian perspective.1950s use term “Bayesian statistics” became prevalent —\nsee Fienberg (2006) 10Only 1950s use term “Bayesian statistics” became prevalent —\nsee Fienberg (2006) 10Due advances personal computing 1970 onwards Bayesian learning become pervasive!Computers allow complex (numerical) calculations needed Bayesian statistics .Metropolis-Hastings algorithm published 1970 (allows sample posterior distribution without explicitly computing marginal likelihood).Development regularised estimation techniques penalised likelihood regression (e.g. ridge regression 1970).penalised likelihood via KL divergence model selection (Akaike 1973).lot work interpreting Stein estimators empirical Bayes estimators (Efron Morris 1975)regularisation originally meant make singular systems/matrices invertible, turned regularisation also Bayesian interpretation.Reference priors (Bernardo 1979) proposed default priors models multiple parameters.EM algorithm (published 1977) uses Bayes theorem imputing distribution latent variables.Another boost 1990/2000s science (e.g. genomics) many complex high-dimensional data set becoming norm, exception.Classical statistical methods used setting (overfitting!) new methods developed high-dimensional data analysis, many direct link Bayesian statistics1996 lasso (L1 regularised) regression invented Robert Tibshirani.Machine learning methods non-parametric extremely highly parametric models (neural network) require either explicit implicit regularisation.Many Bayesians field, many using variational Bayes techniques may viewed generalisation EM algorithm also linked methods used statistical physics.","code":""},{"path":"bayesian-learning-in-practise.html","id":"bayesian-learning-in-practise","chapter":"10 Bayesian learning in practise","heading":"10 Bayesian learning in practise","text":"chapter discuss three basic problems, namely estimate proportion, mean variance Bayesian framework.","code":""},{"path":"bayesian-learning-in-practise.html","id":"estimating-a-proportion-using-the-beta-binomial-model","chapter":"10 Bayesian learning in practise","heading":"10.1 Estimating a proportion using the beta-binomial model","text":"","code":""},{"path":"bayesian-learning-in-practise.html","id":"binomial-likelihood","chapter":"10 Bayesian learning in practise","heading":"10.1.1 Binomial likelihood","text":"order apply Bayes’ theorem first need find suitable\nlikelihood. use Bernoulli model Example 3.1:Repeated Bernoulli experiment (binomial model):Bernoulli data generating process:\n\\[\nx  \\sim \\text{Ber}(\\theta)\n\\]\\(x \\\\{0, 1\\}\\) (e.g. “success” vs. “failure”)“success” indicated outcome \\(x=1\\) “failure” \\(x=0\\)Parameter: \\(\\theta\\) probability “success”probability mass function (PMF): \\(\\text{Pr}(x=1) = \\theta\\), \\(\\text{Pr}(x=0) = 1-\\theta\\)Mean: \\(\\text{E}(x) = \\theta\\)Variance \\(\\text{Var}(x) = \\theta (1-\\theta)\\)Binomial model \\(\\text{Bin}(n,\\theta)\\) (sum \\(n\\) Bernoulli experiments):\\(y \\\\{0, 1, \\ldots, n\\} = \\sum_{=1}^n x_i\\)Mean: \\(\\text{E}(y) = n \\theta\\)Variance: \\(\\text{Var}(y) = n \\theta (1-\\theta)\\)Mean standardised \\(y\\): \\(\\text{E}(y/n) = \\theta\\)Variance standardised \\(y\\): \\(\\text{Var}(y/n) = \\frac{\\theta (1-\\theta)}{n}\\)Maximum likelihood estimate \\(\\theta\\):conduct \\(n\\) Bernoulli trials observe data \\(D = \\{x_1, \\ldots, x_n\\}\\)\naverage \\(\\bar{x}\\) \\(n_1\\) successes \\(n_2 = n-n_1\\) failures.Binomial likelihood:\n\\[\nL(\\theta|D) = \\begin{pmatrix} n \\\\ n_1 \\end{pmatrix} \\theta^{n_1} (1-\\theta)^{n_2}\n\\]\nNote binomial coefficient arises ordering \\(x_i\\) \nirrelevant may discarded contain parameter \\(\\theta\\).Example 3.1 know maximum likelihood estimate \nproportion \\(\\theta\\) frequency\n\\[\\hat{\\theta}_{ML} = \\frac{n_1}{n} = \\bar{x}\\]\nThus, MLE \\(\\hat{\\theta}_{ML}\\) can expressed average\n(individual data points). seemingly trivial\nfact important Bayesian estimation \\(\\theta\\) using linear shrinkage, \nbecome evident .","code":""},{"path":"bayesian-learning-in-practise.html","id":"beta-prior-distribution","chapter":"10 Bayesian learning in practise","heading":"10.1.2 Beta prior distribution","text":"Bayesian statistics need specify data generating process\nalso prior distribution parameters likelihood function.Therefore, need explicitly specify prior uncertainty \\(\\theta\\).parameter \\(\\theta\\) support \\([0,1]\\). Therefore may use beta distribution\n\\(\\text{Beta}(\\alpha_1, \\alpha_2)\\) prior \\(\\theta\\) (see Appendix properties distribution). see beta distribution \nnatural choice prior conjunction binomial likelihood.parameters prior\n(\\(\\alpha_1 \\geq 0\\) \\(\\alpha_2 \\geq 0\\)) also known hyperparameters\nmodel distinguish parameters likelihood function (\\(\\theta\\)).write prior distribution\n\\[\n\\theta \\sim \\text{Beta}(\\alpha_1, \\alpha_2)\n\\]\ndensity\n\\[\np(\\theta) = \\frac{1}{B(\\alpha_1, \\alpha_2)} \\theta^{\\alpha_1-1} (1-\\theta)^{\\alpha_2-1}\n\\]terms mean parameterisation \\(\\text{Beta}(\\mu_0, k_0)\\) corresponds :prior concentration parameter set \\(k_0 = \\alpha_1 + \\alpha_2\\)prior mean parameter set \\(\\mu_0 = \\alpha_1 / k_0\\).prior mean therefore\n\\[\n\\text{E}(\\theta) = \\mu_0\n\\]\nprior variance\n\\[\n\\text{Var}(\\theta)  = \\frac{\\mu_0 (1-\\mu_0)}{k_0 + 1}\n\\]important actually mean \\(\\theta\\) random.\nmeans model uncertainty \\(\\theta\\) using beta-distributed random variable. flexibility beta distribution allows accommodate large variety possible scenarios prior knowledge using just two parameters.Note mean variance beta prior mean variance standardised\nbinomial variable \\(y/n\\) form. indication binomial\nlikelihood beta prior well matched — see discussion “conjugate priors”.","code":""},{"path":"bayesian-learning-in-practise.html","id":"computing-the-posterior-distribution","chapter":"10 Bayesian learning in practise","heading":"10.1.3 Computing the posterior distribution","text":"observing data \\(D = \\{x_1, \\ldots, x_n\\}\\) \\(n_1\\) “successes”\n\\(n_2 = n-n_1\\) “failures”\ncan compute posterior\ndensity \\(\\theta\\) using Bayes’ theorem:\n\\[\np(\\theta| D) = \\frac{p(\\theta) L(\\theta | D) }{p(D)}\n\\]Applying Bayes’ theorem results posterior distribution:\n\\[\n\\theta| D \\sim \\text{Beta}(\\alpha_1+n_2, \\alpha_2+n_2)\n\\]\ndensity\n\\[\np(\\theta| D) = \\frac{1}{B(\\alpha_1+n_1, \\alpha_2+n_2)} \\theta^{\\alpha_1+n_1-1} (1-\\theta)^{\\alpha_2+n_2-1}\n\\]\n(proof see Worksheet B1.)corresponding mean parameterisation \\(\\text{Beta}(\\mu_1, k_1)\\) results following updates:concentration parameter updated \\(k_1 = k_0+n\\)mean parameter updated \n\\[\n\\mu_1 = \\frac{\\alpha_1 + n_1}{k_1}\n\\]\ncan written \n\\[\n\\begin{split}\n\\mu_1 & =  \\frac{\\alpha_1}{k_1}  + \\frac{n_1}{k_1}\\\\\n    & =  \\frac{k_0}{k_1} \\frac{\\alpha_1}{k_0}   + \\frac{n}{k_1} \\frac{n_1}{n}\\\\\n    & = \\lambda \\mu_0 + (1-\\lambda) \\hat{\\theta}_{ML}\\\\\n\\end{split}\n\\]\n\\(\\lambda = \\frac{k_0}{k_1}\\). Hence, \\(\\mu_1\\) \nconvex combination prior mean MLE.Therefore, posterior mean \n\\[\n\\text{E}(\\theta | D) = \\mu_1\n\\]\nposterior variance \n\\[\n\\text{Var}(\\theta | D)\n= \\frac{\\mu_1 (1-\\mu_1)}{k_1+1 }\n\\]","code":""},{"path":"bayesian-learning-in-practise.html","id":"properties-of-bayesian-learning","chapter":"10 Bayesian learning in practise","heading":"10.2 Properties of Bayesian learning","text":"beta-binomial model, even though one simplest possible models, already allows observe number important features properties Bayesian learning. Many apply also models see later.","code":""},{"path":"bayesian-learning-in-practise.html","id":"prior-acting-as-pseudodata","chapter":"10 Bayesian learning in practise","heading":"10.2.1 Prior acting as pseudodata","text":"expression mean variance can see \nconcentration parameter \\(k_0=\\alpha_1 + \\alpha_2\\) behaves like \nimplicit sample size connected prior information \\(\\theta\\).Specifically, \\(\\alpha_1\\) \\(\\alpha_2\\) act pseudocounts influence\nposterior mean posterior variance, exactly way conventional\nobservations.example, larger \\(k_0\\) (thus larger \\(\\alpha_1\\) \\(\\alpha_2\\)) smaller posterior variance, variance decreasing proportional inverse \\(k_0\\). prior highly concentrated, .e. low variance large precision (=inverse variance) implicit data size \\(k_0\\) large. Conversely, prior large variance, prior vague implicit data size \\(k_0\\) small.Hence, prior effect one add data — without actually adding data! precisely prior acts regulariser prevents overfitting, increases effective sample size.Another interpretation prior summarises data\nmay available previously observations.","code":""},{"path":"bayesian-learning-in-practise.html","id":"linear-shrinkage-of-mean","chapter":"10 Bayesian learning in practise","heading":"10.2.2 Linear shrinkage of mean","text":"beta-binomial model posterior mean convex combination (.e. weighted average) ML estimate prior mean can seen update formula\n\\[\n\\mu_1 = \\lambda \\mu_0 + (1-\\lambda) \\hat{\\theta}_{ML}\n\\]\nweight \\(\\lambda \\[0,1]\\)\n\\[\n\\lambda = \\frac{k_0}{k_1} \\,.\n\\]\nThus, posterior mean \\(\\mu_1\\) linearly adjusted \\(\\hat{\\theta}_{ML}\\). factor \\(\\lambda\\) called shrinkage intensity — note ratio “prior sample size” (\\(k_0\\)) “effective total sample size” (\\(k_1\\)).adjustment MLE called shrinkage, \\(\\hat{\\theta}_{ML}\\) “shrunk” towards prior mean \\(\\mu_0\\) (often called “target”, sometimes target zero, terminology “shrinking” makes sense).adjustment MLE called shrinkage, \\(\\hat{\\theta}_{ML}\\) “shrunk” towards prior mean \\(\\mu_0\\) (often called “target”, sometimes target zero, terminology “shrinking” makes sense).shrinkage intensity zero (\\(\\lambda = 0\\)) ML point estimator recovered. happens \\(\\alpha_1=0\\) \\(\\alpha_2=0\\) \\(n \\rightarrow \\infty\\).\nRemark: using maximum likelihood estimate \\(\\theta\\) (moderate small \\(n\\)) Bayesian posterior mean estimation using beta-binomial model prior \\(\\alpha_1=0\\) \\(\\alpha_2=0\\). prior extremely “u-shaped” implicit prior ML estimation. use prior intentionally?shrinkage intensity zero (\\(\\lambda = 0\\)) ML point estimator recovered. happens \\(\\alpha_1=0\\) \\(\\alpha_2=0\\) \\(n \\rightarrow \\infty\\).Remark: using maximum likelihood estimate \\(\\theta\\) (moderate small \\(n\\)) Bayesian posterior mean estimation using beta-binomial model prior \\(\\alpha_1=0\\) \\(\\alpha_2=0\\). prior extremely “u-shaped” implicit prior ML estimation. use prior intentionally?shrinkage intensity large (\\(\\lambda \\rightarrow 1\\)) posterior mean corresponds prior.\nhappens \\(n=0\\) \\(k_0\\) large (implying prior sharply concentrated around prior mean).shrinkage intensity large (\\(\\lambda \\rightarrow 1\\)) posterior mean corresponds prior.\nhappens \\(n=0\\) \\(k_0\\) large (implying prior sharply concentrated around prior mean).Since ML estimate \\(\\hat{\\theta}_{ML}\\) unbiased Bayesian point estimate biased (finite \\(n\\)!). bias induced prior mean deviating true mean. also true generally Bayesian learning typically produces biased estimators (asymptotically unbiased like ML).Since ML estimate \\(\\hat{\\theta}_{ML}\\) unbiased Bayesian point estimate biased (finite \\(n\\)!). bias induced prior mean deviating true mean. also true generally Bayesian learning typically produces biased estimators (asymptotically unbiased like ML).fact posterior mean linear combination MLE prior mean coincidence. fact, true distributions exponential families, see e.g. Diaconis Ylvisaker (1979)11.\nCrucially, exponential families can always parameterised corresponding MLEs expressed averages functions data (technically: MLE mean parameter EF average canonical statistic). conjunction particular type prior (conjugate priors, always existing exponential families, see ) allows write update prior posterior mean linear adjustment MLE.fact posterior mean linear combination MLE prior mean coincidence. fact, true distributions exponential families, see e.g. Diaconis Ylvisaker (1979)11.\nCrucially, exponential families can always parameterised corresponding MLEs expressed averages functions data (technically: MLE mean parameter EF average canonical statistic). conjunction particular type prior (conjugate priors, always existing exponential families, see ) allows write update prior posterior mean linear adjustment MLE.Furthermore, possible (indeed quite useful computational reasons!) formulate Bayes learning assuming first second moments (.e. without full distributions) terms linear shrinkage, see e.g. Hartigan (1969)12. resulting theory called “Bayes linear statistics” (Goldstein Wooff, 2007)13.Furthermore, possible (indeed quite useful computational reasons!) formulate Bayes learning assuming first second moments (.e. without full distributions) terms linear shrinkage, see e.g. Hartigan (1969)12. resulting theory called “Bayes linear statistics” (Goldstein Wooff, 2007)13.","code":""},{"path":"bayesian-learning-in-practise.html","id":"conjugacy-of-prior-and-posterior-distribution","chapter":"10 Bayesian learning in practise","heading":"10.2.3 Conjugacy of prior and posterior distribution","text":"beta-binomial model estimating proportion \\(\\theta\\) choice beta distribution prior distribution along binomial likelihood resulted beta distribution posterior distribution well.prior posterior belong distributional family prior called conjugate prior. case prior functional form likelihood. Therefore one also says prior conjugate likelihood.can shown conjugate priors exist likelihood functions \nbased data generating models exponential families.beta-binomial model likelihood based binomial distribution following form\n(terms depending parameter \\(\\theta\\) shown):\n\\[\n\\theta^{n_1} (1-\\theta)^{n_2}\n\\]\nform beta prior (, showing terms depending \\(\\theta\\)):\n\\[\n\\theta^{\\alpha_1-1} (1-\\theta)^{\\alpha_2-1}\n\\]\nSince posterior proportional product prior\nlikelihood posterior exactly form \nprior:\n\\[\n\\theta^{\\alpha_1+n_1-1} (1-\\theta)^{\\alpha_2+n_2-1}\n\\]\nChoosing prior distribution family conjugate likelihood\ngreatly simplifies Bayesian analysis since Bayes formula can written form update formula parameters beta distribution:\n\\[\n\\alpha_1 \\rightarrow \\alpha_1 + n_1  = \\alpha_1 + n \\hat{\\theta}_{ML}\n\\]\n\\[\n\\alpha_2 \\rightarrow \\alpha_2 + n_2 = \\alpha_2 + n (1-\\hat{\\theta}_{ML})\n\\]Thus, conjugate prior distributions convenient choices. However, application must ensured prior distribution flexible enough encapsulate prior information may available. cases case alternative priors used (likely require compute posterior distribution numerically rather analytically).","code":""},{"path":"bayesian-learning-in-practise.html","id":"large-sample-limits-of-mean-and-variance","chapter":"10 Bayesian learning in practise","heading":"10.2.4 Large sample limits of mean and variance","text":"\\(n\\) large \\(n >> \\alpha, \\beta\\) \\(\\lambda \\rightarrow 0\\)\nhence posterior mean variance become asympotically\\[\n\\text{E}(\\theta| D)  \\overset{}{=} \\frac{n_1 }{n} = \\hat{\\theta}_{ML}\n\\]\n\n\\[\n\\text{Var}(\\theta| D) \\overset{}{=}   \\frac{\\hat{\\theta}_{ML} (1-\\hat{\\theta}_{ML})}{n}\n\\]Thus, sample size large Bayes’ estimator turns ML estimator! Specifically,\nposterior mean becomes ML point estimate, posterior variance equal asymptotic variance computed via observed Fisher information.Thus, large \\(n\\) data dominate details prior (settings hyperparameters \\(\\alpha_1\\) \\(\\alpha_2\\)) become irrelevant!","code":""},{"path":"bayesian-learning-in-practise.html","id":"asymptotic-normality-of-the-posterior-distribution","chapter":"10 Bayesian learning in practise","heading":"10.2.5 Asymptotic normality of the posterior distribution","text":"Also known Bayesian Central Limit Theorem (CLT).regularity conditions (regular likelihood positive prior probability \nparameter values, finite number parameters, etc.) large sample size Bayesian posterior distribution converges normal distribution\ncentred around MLE variance MLE:\\[\n\\text{large $n$:  }  p(\\boldsymbol \\theta| D) \\N(\\hat{\\boldsymbol \\theta}_{ML}, \\text{Var}(\\hat{\\boldsymbol \\theta}_{ML}) )\n\\]posterior mean variance converging MLE variance MLE\nlarge sample size, also posterior distribution converges sampling distribution!holds generally many regular cases, just simple case .Bayesian CLT generally known\nBernstein-von Mises theorem (discovered around 1920–30), special cases already known Laplace.Worksheet B1 asymptotic convergence posterior distribution normal distribution demonstrated graphically.","code":""},{"path":"bayesian-learning-in-practise.html","id":"posterior-variance-for-finite-n","chapter":"10 Bayesian learning in practise","heading":"10.2.6 Posterior variance for finite \\(n\\)","text":"Bayesian posterior can obtain Bayesian point estimate\nproportion \\(\\theta\\) computing posterior mean\n\\[\n\\text{E}(\\theta | D) = \\frac{\\alpha_1+n_1}{k_1} = \\hat{\\theta}_{\\text{Bayes}}\n\\]\nalong posterior variance\n\\[\n\\text{Var}(\\theta | D) = \\frac{\\hat{\\theta}_{\\text{Bayes}} (1-\\hat{\\theta}_{\\text{Bayes}})}{k_1+1}\n\\]Asymptotically large \\(n\\) posterior mean becomes maximum likelihood estimate (MLE), \nposterior variance becomes asymptotic variance MLE.\nThus, large \\(n\\) Bayesian point estimate indistinguishable MLE\nshares favourable properties.addition, finite sample size posterior variance typically smaller asymptotic\nposterior variance (large \\(n\\)) prior variance, showing combining information\navailable prior data leads efficient estimate.","code":""},{"path":"bayesian-learning-in-practise.html","id":"estimating-the-mean-using-the-normal-normal-model","chapter":"10 Bayesian learning in practise","heading":"10.3 Estimating the mean using the normal-normal model","text":"","code":""},{"path":"bayesian-learning-in-practise.html","id":"normal-likelihood","chapter":"10 Bayesian learning in practise","heading":"10.3.1 Normal likelihood","text":"Example 3.2 estimated mean parameter \nmaximum likelihood assume data-generating model normal distribution\nunknown mean \\(\\mu\\) known variance \\(\\sigma^2\\):\n\\[\nx \\sim N(\\mu, \\sigma^2)\n\\]\nobserve \\(n\\) samples \\(D = \\{x_1, \\ldots x_n\\}\\).\nyields using maximum likelihood estimate \\(\\hat{\\mu}_{ML} = \\bar{x}\\).note MLE \\(\\hat\\mu_{ML}\\) expressed average data points,\nenables linear shrinkage seen .","code":""},{"path":"bayesian-learning-in-practise.html","id":"normal-prior-distribution","chapter":"10 Bayesian learning in practise","heading":"10.3.2 Normal prior distribution","text":"normal distribution conjugate distribution mean parameter \nnormal likelihood, use normal prior posterior \\(\\mu\\) normal well.model uncertainty \\(\\mu\\) use normal distribution form\n\\(N(\\mu, \\sigma^2/k)\\) mean parameter \\(\\mu\\) concentration\nparameter \\(k > 0\\) (remember \\(\\sigma^2\\) given also used likelihood).Specifically, use normal prior distribution mean\n\\[\n\\mu \\sim N\\left(\\mu_0, \\frac{\\sigma^2}{k_0}\\right)\n\\]prior concentration parameter set \\(k_0\\)prior mean parameter set \\(\\mu_0\\)Hence prior mean \n\\[\n\\text{E}(\\mu) =  \\mu_0\n\\]\nprior variance\n\\[\n\\text{Var}(\\mu)  = \\frac{\\sigma^2}{k_0}\n\\]\nconcentration parameter \\(k_0\\) corresponds implied sample size prior.\nNote \\(k_0\\) need integer value.","code":""},{"path":"bayesian-learning-in-practise.html","id":"normal-posterior-distribution","chapter":"10 Bayesian learning in practise","heading":"10.3.3 Normal posterior distribution","text":"observing data \\(D\\) posterior distribution\nalso normal updated parameters \\(\\mu=\\mu_1\\) \\(k_1\\)\n\\[\n\\mu | D \\sim N\\left(\\mu_1, \\frac{\\sigma^2}{k_1}\\right)\n\\]posterior concentration parameter updated \\(k_1 = k_0 +n\\)posterior mean parameter updated \n\\[\n\\mu_1 = \\lambda \\mu_0 + (1-\\lambda) \\hat\\mu_{ML}\n\\]\n\\(\\lambda = \\frac{k_0}{k_1}\\).\ncan seen linear shrinkage \n\\(\\hat\\mu_{ML}\\) towards prior mean \\(\\mu_0\\).(proof see Worksheet B2.)posterior mean \n\\[\n\\text{E}(\\mu | D) = \\mu_1\n\\]\nposterior variance \n\\[\n\\text{Var}(\\mu | D)  = \\frac{\\sigma^2}{k_1}\n\\]","code":""},{"path":"bayesian-learning-in-practise.html","id":"large-sample-asymptotics","chapter":"10 Bayesian learning in practise","heading":"10.3.4 Large sample asymptotics","text":"\\(n\\) large \\(n >> k_0\\) shrinkage intensity \\(\\lambda \\rightarrow 0\\)\n\\(k_1 \\rightarrow n\\). result\n\\[\n\\text{E}(\\mu |  D) \\overset{}{=}  \\hat\\mu_{ML}\n\\]\n\\[\n\\text{Var}(\\mu |  D) \\overset{}{=} \\frac{\\sigma^2}{n}\n\\]\n.e. recover MLE asymptotic variance!Note finite \\(n\\) posterior variance \\(\\frac{\\sigma^2}{n+k_0}\\) smaller\nasymptotic variance \\(\\frac{\\sigma^2}{n}\\) MLE prior variance \\(\\frac{\\sigma^2}{k_0}\\).","code":""},{"path":"bayesian-learning-in-practise.html","id":"estimating-the-variance-using-the-inverse-gamma-normal-model","chapter":"10 Bayesian learning in practise","heading":"10.4 Estimating the variance using the inverse-gamma-normal model","text":"","code":""},{"path":"bayesian-learning-in-practise.html","id":"normal-likelihood-1","chapter":"10 Bayesian learning in practise","heading":"10.4.1 Normal likelihood","text":"data generating model use\nnormal distribution\n\\[\nx  \\sim N(\\mu, \\sigma^2)\n\\]\nunknown variance \\(\\sigma^2\\) known mean \\(\\mu\\). yields maximum likelihood estimate variance\n\\[\n\\widehat{\\sigma^2}_{ML}= \\frac{1}{n}\\sum_{=1}^n (x_i-\\mu)^2\n\\]Note , , MLE average (quadratic function \nindividual data points). enables linear shrinkage MLE seen .","code":""},{"path":"bayesian-learning-in-practise.html","id":"ig-prior-distribution","chapter":"10 Bayesian learning in practise","heading":"10.4.2 IG prior distribution","text":"model uncertainty variance use inverse-gamma (IG) distribution, also known\ninverse Wishart (IW) distribution (see Appendix details distribution).\nIG distribution conjugate variance parameter normal likelihood, hence prior posterior distribution IG.\nuse Wishart parameterisation may equally well call \ninverse Wishart (IW) prior, whole model IW-normal model.Specifically, prior distribution \\(\\sigma^2\\) assume using\nmean parameter \\(\\mu\\) concentration parameter \\(\\kappa\\):\n\\[\n\\sigma^2 \\sim  W^{-1}_1(\\psi=\\kappa_0 \\sigma^2_0, \\nu=\\kappa_0+2)\n\\]prior concentration parameter set \\(\\kappa_0\\)prior mean parameter set \\(\\sigma^2_0\\)corresponding prior mean \n\\[\n\\text{E}(\\sigma^2) = \\sigma^2_0\n\\]\nprior variance \n\\[\n\\text{Var}(\\sigma^2) = \\frac{2 \\sigma_0^4}{\\kappa_0-2}\n\\]\n(note \\(\\kappa_0 > 2\\) variance exist)","code":""},{"path":"bayesian-learning-in-practise.html","id":"ig-posterior-distribution","chapter":"10 Bayesian learning in practise","heading":"10.4.3 IG posterior distribution","text":"observing \\(D = \\{ x_1 \\ldots, x_n\\}\\) posterior distribution \nalso IG updated parameters:\n\\[\n\\sigma^2| D \\sim W^{-1}_1(\\psi=\\kappa_1 \\sigma^2_1, \\nu=\\kappa_1+2)\n\\]posterior concentration parameter updated \\(\\kappa_1 = \\kappa_0+n\\)posterior mean parameter update follows standard linear shrinkage rule:\n\\[\n\\sigma^2_1 =  \\lambda \\sigma^2_0 + (1-\\lambda) \\widehat{\\sigma^2}_{ML}\n\\]\n\\(\\lambda=\\frac{\\kappa_0}{\\kappa_1}\\).posterior mean \n\\[\n\\text{E}(\\sigma^2 | D) = \\sigma^2_1\n\\]\nposterior variance\n\\[\n\\text{Var}(\\sigma^2 | D) = \\frac{ 2 \\sigma^4_1}{\\kappa_1-2}\n\\]","code":""},{"path":"bayesian-learning-in-practise.html","id":"large-sample-asymptotics-1","chapter":"10 Bayesian learning in practise","heading":"10.4.4 Large sample asymptotics","text":"large sample size \\(n\\) \\(n >> \\kappa_0\\)\nshrinkage intensity vanishes\n(\\(\\lambda \\rightarrow 0\\)) therefore \\(\\sigma^2_1 \\rightarrow \\widehat{\\sigma^2}_{ML}\\). also find \\(\\kappa_1-2 \\rightarrow n\\).results asymptotic posterior mean\n\\[\n\\text{E}(\\sigma^2 |  D) \\overset{}{=}  \\widehat{\\sigma^2}_{ML}\n\\]\nasymptotic\nposterior variance\n\\[\n\\text{Var}(\\sigma^2 |  D) \\overset{}{=} \\frac{2 (\\widehat{\\sigma^2}_{ML})^2}{n}\n\\]\nThus recover MLE \\(\\sigma^2\\) asymptotic variance.","code":""},{"path":"bayesian-learning-in-practise.html","id":"other-equivalent-update-rules","chapter":"10 Bayesian learning in practise","heading":"10.4.5 Other equivalent update rules","text":"update rule prior posterior inverse gamma distribution \nstated mean parameterisation:\\(\\kappa_0 \\rightarrow \\kappa_1 = \\kappa_0+n\\)\\(\\sigma^2_0 \\rightarrow \\sigma^2_1 = \\lambda \\sigma^2_0 + (1-\\lambda) \\widehat{\\sigma^2}_{ML}\\) \n\\(\\lambda=\\frac{\\kappa_0}{\\kappa_1}\\)advantage mean inverse gamma distribution\nupdated directly, prior posterior variance also\nstraightforward compute.update rule can also expressed terms parameterisations.\nterms conventional parameters \\(\\alpha\\) \\(\\beta\\) inverse gamma\ndistribution update rule \\(\\alpha_0 \\rightarrow \\alpha_1 = \\alpha_0 +\\frac{n}{2}\\)\\(\\beta_0 \\rightarrow \\beta_1 = \\beta_0 + \\frac{n}{2} \\widehat{\\sigma^2}_{ML} = \\beta_0 + \\frac{1}{2} \\sum_{=1}^n (x_i-\\mu)^2\\)parameters \\(\\psi\\) \\(\\nu\\) univariate inverse Wishart distribution\nupdate rule \\(\\nu_0 \\rightarrow \\nu_1 = \\nu_0 +n\\)\\(\\psi_0 \\rightarrow \\psi_1 = \\psi_0 + n \\widehat{\\sigma^2}_{ML} = \\psi_0 + \\sum_{=1}^n (x_i-\\mu)^2\\)parameters \\(\\tau^2\\) \\(\\nu\\) scaled inverse chi-squared\ndistribution update rule \\(\\nu_0 \\rightarrow \\nu_1 = \\nu_0 +n\\)\\(\\tau^2_0 \\rightarrow \\tau^2_1 = \\frac{\\nu_0}{\\nu_1} \\tau^2_0 + \\frac{n}{\\nu_1} \\widehat{\\sigma^2}_{ML}\\)(See Worksheet B3 proof equivalence update rules.)","code":""},{"path":"bayesian-learning-in-practise.html","id":"estimating-the-precision-using-the-gamma-normal-model","chapter":"10 Bayesian learning in practise","heading":"10.5 Estimating the precision using the gamma-normal model","text":"","code":""},{"path":"bayesian-learning-in-practise.html","id":"mle-of-the-precision","chapter":"10 Bayesian learning in practise","heading":"10.5.1 MLE of the precision","text":"Instead estimating variance \\(\\sigma^2\\) may wish estimate precision \\(w1/\\sigma^2\\), .e. inverse variance.data generating model \nnormal distribution\n\\[\nx  \\sim N(\\mu, 1/w)\n\\]\nunknown precision \\(w\\) known mean \\(\\mu\\). yields maximum likelihood estimate\n(easily derived thanks invariance principle)\n\\[\n\\hat{w}_{ML} =  \\frac{ 1}{\\widehat{\\sigma^2}_{ML} } = \\frac{1}{\\frac{1}{n}\\sum_{=1}^n (x_i-\\mu)^2}\n\\]\nCrucially, MLE precision \\(w\\) average (instead, function average).\nconsequence, seen , posterior mean \\(w\\) written linear adjustment MLE.","code":""},{"path":"bayesian-learning-in-practise.html","id":"gamma-wishart-prior","chapter":"10 Bayesian learning in practise","heading":"10.5.2 Gamma (Wishart) prior","text":"modelling variance used inverse gamma (inverse Wishart) distribution prior posterior distributions. Thus, order model precision therefore now use gamma (Wishart) distribution.Specifically, use Wishart distribution mean parameterisation (see Appendix):\n\\[\nw \\sim  W_1(s^2 = w_0/k_0, k=k_0)\n\\]prior concentration parameter set \\(k_0\\)prior mean parameter set \\(w_0\\)corresponding prior mean \n\\[\n\\text{E}(w) = w_0\n\\]\nprior variance \n\\[\n\\text{Var}(\\sigma^2) = 2 w_0^2/ k_0\n\\]","code":""},{"path":"bayesian-learning-in-practise.html","id":"gamma-wishart-posterior","chapter":"10 Bayesian learning in practise","heading":"10.5.3 Gamma / Wishart posterior","text":"observing \\(D = \\{ x_1 \\ldots, x_n\\}\\) posterior distribution \nalso gamma resp. Wishart updated parameters:\\[\nw | D \\sim   W_1(s^2 = w_1/k_1, k=k_1)\n\\]posterior concentration parameter updated \\(k_1 = k_0+n\\)posterior mean parameter update follows update:\n\\[\n\\frac{1}{w_1} =  \\lambda \\frac{1}{w_0}  + (1-\\lambda)  \\frac{1}{\\hat{w}_{ML}}\n\\]\n\\(\\lambda = \\frac{k_0}{k_1}\\).\nCrucially, linear update applied inverse precision\nprecision . MLE precision parameter expressed average.Equivalent update rules inverse scale parameter \\(s^2\\)\n\\[\n\\frac{1}{s^2_1} =  \\frac{1}{s^2_0}  + n \\widehat{\\sigma^2}_{ML}\n\\]\nrate parameter \\(\\beta = 1/(2 s^2)\\) gamma distribution\n\\[\n\\beta_1 =  \\beta_0 + \\frac{n}{2} \\widehat{\\sigma^2}_{ML}\n\\]\nform find often textbooks.posterior mean \n\\[\n\\text{E}(w | D) = w_1\n\\]\nposterior variance\n\\[\n\\text{Var}(w | D) = 2 w_1^2/ k_1\n\\]","code":""},{"path":"bayesian-model-comparison.html","id":"bayesian-model-comparison","chapter":"11 Bayesian model comparison","heading":"11 Bayesian model comparison","text":"","code":""},{"path":"bayesian-model-comparison.html","id":"marginal-likelihood-as-model-likelihood","chapter":"11 Bayesian model comparison","heading":"11.1 Marginal likelihood as model likelihood","text":"","code":""},{"path":"bayesian-model-comparison.html","id":"simple-and-composite-models","chapter":"11 Bayesian model comparison","heading":"11.1.1 Simple and composite models","text":"introduction Bayesian learning already encountered marginal likelihood \\(p(D | M)\\) model class \\(M\\) denominator Bayes’ rule:\n\\[\np(\\boldsymbol \\theta| D, M) =  \\frac{p(\\boldsymbol \\theta| M)  p(D | \\boldsymbol \\theta, M) }{p(D | M)}\n\\]\nComputing marginal likelihood different simple composite models.model called “simple” directly corresponds specific distribution,\nsay, normal distribution fixed mean variance, binomial distribution given probability two classes. Thus, simple model point model space described parameters distribution family (e.g.\n\\(\\mu\\) \\(\\sigma^2\\) normal family \\(N(\\mu, \\sigma^2\\)). simple model \\(M\\) density\n\\(p(D | M)\\) corresponds standard likelihood \\(M\\) free parameters.hand, model “composite” composed simple models. can finite set, can comprised infinite number simpple models. Thus composite model\nrepresent model class.\nexample, normal distribution given mean unspecified variance, binomial model unspecified class probability, composite model.\\(M\\) composite model, underlying simple models indexed \nparameter \\(\\boldsymbol \\theta\\), likelihood model \nobtained marginalisation \\(\\boldsymbol \\theta\\):\n\\[\n\\begin{split}\np(D | M) &= \\int_{\\boldsymbol \\theta} p(D | \\boldsymbol \\theta, M) p(\\boldsymbol \\theta| M) d\\boldsymbol \\theta\\\\\n             &= \\int_{\\boldsymbol \\theta} p(D , \\boldsymbol \\theta| M) d\\boldsymbol \\theta\\\\\n\\end{split}\n\\]\n.e. integrate parameter values \\(\\boldsymbol \\theta\\).distribution parameter \\(\\boldsymbol \\theta\\) model strongly concentrated around specific value \\(\\boldsymbol \\theta_0\\) composite model degenerates simple point model, marginal likelihood becomes\nlikelihood parameter \\(\\boldsymbol \\theta_0\\) model.Example 11.1  Beta-binomial distribution:Assume likelihood binomial mean parameter \\(\\theta\\). \\(\\theta\\) follows\nBeta distribution marginal likelihood \\(\\theta\\) integrated \nbeta-binomial distribution (see also Worksheet B3).\nexample compound probability distribution.","code":""},{"path":"bayesian-model-comparison.html","id":"log-marginal-likelihood-as-penalised-maximum-log-likelihood","chapter":"11 Bayesian model comparison","heading":"11.1.2 Log-marginal likelihood as penalised maximum log-likelihood","text":"rearranging Bayes’ rule see \n\\[\n\\log p(D | M) =  \\log p(D | \\boldsymbol \\theta, M) - \\log  \\frac{ p(\\boldsymbol \\theta| D, M) }{p(\\boldsymbol \\theta| M)  }\n\\]\nvalid \\(\\boldsymbol \\theta\\).Assuming concentration posterior around MLE \\(\\hat{\\boldsymbol \\theta}_{\\text{ML}}\\) \\(p(\\hat{\\boldsymbol \\theta}_{\\text{ML}} | D, M)> p(\\hat{\\boldsymbol \\theta}_{\\text{ML}}| M)\\)\nthus\n\\[\n\\log p(D | M) =  \\underbrace{\\log p(D | \\hat{\\boldsymbol \\theta}_{\\text{ML}}, M)}_{\\text{maximum log-likelihood}}\n- \\underbrace{ \\log  \\frac{ p( \\hat{\\boldsymbol \\theta}_{\\text{ML}} | D, M) }{p( \\hat{\\boldsymbol \\theta}_{\\text{ML}}| M)  } }_{\\text{penalty > 0}}\n\\]\nTherefore, log-marginal likelihood essentially penalised version maximum log-likelihood, penalty depends concentration posterior\naround MLE","code":""},{"path":"bayesian-model-comparison.html","id":"model-complexity-and-occams-razor","chapter":"11 Bayesian model comparison","heading":"11.1.3 Model complexity and Occams razor","text":"Intriguingly, penality implicit log-marginal likelihood linked complexity model, particular number parameters \\(M\\).\nsee directly Schwarz approximation \nlog-marginal likelihood discussed .Thus, averaging \\(\\boldsymbol \\theta\\) marginal likelihood effect automatically penalising complex models.\nTherefore, comparing models using marginal likelihood complex model may ranked simpler models.\ncontrast, selecting model comparing maximum likelihood directly model highest number parameters always wins simpler models.\nHence, penalisation implicit marginal likelihood prevents overfitting\noccurs maximum likelihood.principle preferring less complex model called Occam’s razor\nlaw parsimony.choosing models simpler model often preferable complex model, simpler model typically better suited explaining currently observed data well future data, whereas complex model typically excel fitting current data perform poorly prediction.","code":""},{"path":"bayesian-model-comparison.html","id":"the-bayes-factor-for-comparing-two-models","chapter":"11 Bayesian model comparison","heading":"11.2 The Bayes factor for comparing two models","text":"","code":""},{"path":"bayesian-model-comparison.html","id":"definition-of-the-bayes-factor","chapter":"11 Bayesian model comparison","heading":"11.2.1 Definition of the Bayes factor","text":"Bayes factor ratio likelihoods\ntwo models:\n\\[\nB_{12} = \\frac{p(D | M_1)}{p(D | M_2)}\n\\]log-Bayes factor\n\\(\\log B_{12}\\)\nalso called weight evidence \\(M_1\\) \\(M_2\\).","code":""},{"path":"bayesian-model-comparison.html","id":"bayes-theorem-in-terms-of-the-bayes-factor","chapter":"11 Bayesian model comparison","heading":"11.2.2 Bayes theorem in terms of the Bayes factor","text":"like compare two models \\(M_1\\) \\(M_2\\). seeing data \\(D\\) can check Prior odds (= ratio prior probabilities models \\(M_1\\) \\(M_2\\)):\\[\\frac{\\text{Pr}(M_1)}{\\text{Pr}(M_2)}\\]seeing data \\(D = \\{x_1, \\ldots, x_n\\}\\) arrive Posterior odds (= ratio posterior probabilities):\n\\[\\frac{\\text{Pr}(M_1 | D)}{\\text{Pr}(M_2  | D)}\\]Using Bayes Theorem \\(\\text{Pr}(M_i | D) = \\text{Pr}(M_i) \\frac{p(D | M_i) }{p(D)}\\) can rewrite \nposterior odds \n\\[\n\\underbrace{\\frac{\\text{Pr}(M_1 | D)}{\\text{Pr}(M_2 | D)}}_{\\text{posterior odds}} = \\underbrace{\\frac{p(D | M_1)}{p(D | M_2)}}_{\\text{Bayes factor $B_{12}$}} \\,\n\\underbrace{\\frac{\\text{Pr}(M_1)}{\\text{Pr}(M_2)}}_{\\text{prior odds}}\n\\]Bayes factor multiplicative factor updates prior odds posterior odds.log scale see \\[\n\\text{log-posterior odds = weight evidence + log-prior odds}\n\\]","code":""},{"path":"bayesian-model-comparison.html","id":"scale-for-the-bayes-factor","chapter":"11 Bayesian model comparison","heading":"11.2.3 Scale for the Bayes factor","text":"Following Harold Jeffreys (1961) 14 one may interpret strength Bayes factor follows:recently, Kass Raftery (1995) 15 proposed use following slightly modified scale:","code":""},{"path":"bayesian-model-comparison.html","id":"bayes-factor-versus-likelihood-ratio","chapter":"11 Bayesian model comparison","heading":"11.2.4 Bayes factor versus likelihood ratio","text":"\\(M_1\\) \\(M_2\\) simple models Bayes factor identical likelihood ratio two models.However, one two models composite Bayes factor \ngeneralised likelihood ratio differ:\nBayes factor representative composite model \nmodel average simple models indexed \\(\\boldsymbol \\theta\\), weights\ntaken prior distribution simple models contained \\(M\\). contrast, generalised likelihood ratio statistic representative composite model chosen maximisation.Thus, composite models, Bayes factor equal corresponding generalised likelihood ratio statistic. fact, key difference Bayes factor penalised version likelihood ratio, penality\ndepending difference complexity (number parameters) two models","code":""},{"path":"bayesian-model-comparison.html","id":"approximate-computations","chapter":"11 Bayesian model comparison","heading":"11.3 Approximate computations","text":"marginal likelihood Bayes factor can difficult compute\npractise. Therefore, number approximations developed.\nimportant -called Schwarz (1978) approximation log-marginal likelihood. used approximate log-Bayes factor also yields\nBIC (Bayesian information criterion) can interpreted penalised maximum\nlikelihood.","code":""},{"path":"bayesian-model-comparison.html","id":"schwarz-1978-approximation-of-log-marginal-likelihood","chapter":"11 Bayesian model comparison","heading":"11.3.1 Schwarz (1978) approximation of log-marginal likelihood","text":"logarithm marginal likelihood model can approximated\nfollowing Schwarz (1978) 16 follow:\n\\[\n\\log p(D | M) \\approx l_n^M(\\hat{\\boldsymbol \\theta}_{ML}^{M}) - \\frac{1}{2} d_M \\log n  \n\\]\n\\(d_M\\) dimension model \\(M\\) (number parameters \\(\\boldsymbol \\theta\\) belonging \\(M\\)) \\(n\\) sample size\n\\(\\hat{\\boldsymbol \\theta}_{ML}^{M}\\) MLE.\nsimple model \\(d_M=0\\) \napproximation case marginal likelihood equals likelihood.formula can obtained quadratic approximation likelihood assuming large \\(n\\) assuming prior locally uniform around MLE. Schwarz (1978) approximation therefore special case Laplace approximation.Note approximation maximum log-likelihood minus penalty depends model complexity (measured dimension \\(d\\)), hence example penalised ML! Also note distribution parameter \\(\\boldsymbol \\theta\\) required approximation.","code":""},{"path":"bayesian-model-comparison.html","id":"bayesian-information-criterion-bic","chapter":"11 Bayesian model comparison","heading":"11.3.2 Bayesian information criterion (BIC)","text":"BIC (Bayesian information criterion) model \\(M\\) \napproximated log-marginal likelihood times factor -2:\\[\nBIC(M) = -2 l_n^M(\\hat{\\boldsymbol \\theta}_{ML}^{M}) + d_M \\log n\n\\]Thus, comparing models one aimes maximise marginal likelihood , approximation, minimise BIC.reason factor “-2” simply quantity \nscale Wilks log likelihood ratio. people / software packages also use factor “2”.","code":""},{"path":"bayesian-model-comparison.html","id":"approximating-the-weight-of-evidence-log-bayes-factor-with-bic","chapter":"11 Bayesian model comparison","heading":"11.3.3 Approximating the weight of evidence (log-Bayes factor) with BIC","text":"Using BIC (twice) log-Bayes factor can approximated \n\\[\n\\begin{split}\n2 \\log B_{12} &\\approx -BIC(M_1) + BIC(M_2) \\\\\n&=2 \\left( l_n^{M_{1}}(\\hat{\\boldsymbol \\theta}_{ML}^{M_{1}}) - l_n^{M_{2}}(\\hat{\\boldsymbol \\theta}_{ML}^{M_{2}}) \\right) - \\log(n) (d_{M_{1}}-d_{M_{2}}) \\\\\n\\end{split}\n\\]\n.e. penalised log-likelihood ratio model \\(M_1\\) vs. \\(M_2\\).","code":""},{"path":"bayesian-model-comparison.html","id":"bayesian-testing-using-false-discovery-rates","chapter":"11 Bayesian model comparison","heading":"11.4 Bayesian testing using false discovery rates","text":"introduce False Discovery Rates (FDR) Bayesian method \ndistinguish null model alternative model. closely linked classical\nfrequentist multiple testing procedures.","code":""},{"path":"bayesian-model-comparison.html","id":"setup-for-testing-a-null-model-h_0-versus-an-alternative-model-h_a","chapter":"11 Bayesian model comparison","heading":"11.4.1 Setup for testing a null model \\(H_0\\) versus an alternative model \\(H_A\\)","text":"consider two models:\\(H_0:\\) null model, density \\(f_0(x)\\) distribution \\(F_0(x)\\)\\(H_A:\\) alternative model, density \\(f_A(x)\\) distribution \\(F_A(x)\\)Aim: given observations \\(x_1, \\ldots, x_n\\) like decide \\(x_i\\) whether\nbelongs \\(H_0\\) \\(H_A\\).done critical decision threshold \\(x_c\\): \\(x_i > x_c\\) \\(x_i\\) called “significant” otherwise called “significant”.classical statistics one widely used approach find decision threshold computing \\(p\\)-values \\(x_i\\)\n(uses null model alternative model), thresholding \\(p\\)-values certain level (say 5%). \\(n\\) large often test modified adjusting \\(p\\)-values threshold (e.g. Bonferroni correction).Note procedure ignores information may alternative model!","code":""},{"path":"bayesian-model-comparison.html","id":"test-errors","chapter":"11 Bayesian model comparison","heading":"11.4.2 Test errors","text":"","code":""},{"path":"bayesian-model-comparison.html","id":"true-and-false-positives-and-negatives","chapter":"11 Bayesian model comparison","heading":"11.4.2.1 True and false positives and negatives","text":"decision threshold \\(x_c\\) can distinguish following errors:False positives (FP), “false alarm”, type error: \\(x_i\\) belongs null called “significant”False negative (FN), “miss”, type II error: \\(x_i\\) belongs alternative, called “significant”addition :True positives (TP), “hits”: belongs alternative called “significant”True negatives (TN), “correct rejections”: belongs null called “significant”","code":""},{"path":"bayesian-model-comparison.html","id":"specificity-and-sensitivity","chapter":"11 Bayesian model comparison","heading":"11.4.2.2 Specificity and Sensitivity","text":"counts TP, TN, FN, FP can derive quantities:True Negative Rate TNR, specificity: \\(TNR= \\frac{TN}{TN+FP} = 1- FPR\\) FPR=False Positive Rate = \\(1-\\alpha_I\\)True Negative Rate TNR, specificity: \\(TNR= \\frac{TN}{TN+FP} = 1- FPR\\) FPR=False Positive Rate = \\(1-\\alpha_I\\)True Positive Rate TPR, sensitivity, power, recall: \\(TPR= \\frac{TP}{TP+FN} = 1- FNR\\) FNR=False negative rate = \\(1-\\alpha_{II}\\)True Positive Rate TPR, sensitivity, power, recall: \\(TPR= \\frac{TP}{TP+FN} = 1- FNR\\) FNR=False negative rate = \\(1-\\alpha_{II}\\)Accuracy: \\(ACC = \\frac{TP+TN}{TP+TN+FP+FN}\\)Accuracy: \\(ACC = \\frac{TP+TN}{TP+TN+FP+FN}\\)Another common way choose decision threshold \\(x_d\\) classical statistics balance sensitivity/power vs. specificity (maximising power specificity, equivalently, minimising false positive false negative rates). ROC curves plot TPR/sensitivity vs. FPR = 1-specificity.","code":""},{"path":"bayesian-model-comparison.html","id":"fdr-and-fndr","chapter":"11 Bayesian model comparison","heading":"11.4.2.3 FDR and FNDR","text":"possible link observed counts TP, FP, TN, FN:False Discovery Rate (FDR): \\(FDR = \\frac{FP}{FP+TP}\\)False Nondiscovery Rate (FNDR): \\(FNDR = \\frac{FN}{TN+FN}\\)Positive predictive value (PPV), True Discovery Rate (TDR), precision: \\(PPV = \\frac{TP}{FP+TP} = 1-FDR\\)Negative predictive value (NPV): \\(NPV = \\frac{TN}{TN+FN} = 1-FNDR\\)order choose decision threshold natural balance FDR FDNR (PPV NPV), minimising FDR FNDR maximising PPV NPV.machine learning common use “precision-recall plots” plot precision (=PPV, TDR)\nvs. recall (=power, sensitivity).","code":""},{"path":"bayesian-model-comparison.html","id":"bayesian-perspective","chapter":"11 Bayesian model comparison","heading":"11.4.3 Bayesian perspective","text":"","code":""},{"path":"bayesian-model-comparison.html","id":"two-component-mixture-model","chapter":"11 Bayesian model comparison","heading":"11.4.3.1 Two component mixture model","text":"Bayesian perspective problem choosing decision threshold related computing posterior probability\n\\[\\text{Pr}(H_0 | x_i) , \\]\n.e. probability null model given observation \\(x_i\\), equivalently\ncomputing\n\\[\\text{Pr}(H_A | x_i) = 1- \\text{Pr}(H_0 | x_i)\\]\nprobability alternative model given observation \\(x_i\\).done assuming mixture model\n\\[\nf(x) = \\pi_0 f_0(x) + (1-\\pi_0) f_A(x)\n\\]\n\\(\\pi_0 = \\text{Pr}(H_0)\\) prior probability \\(H_0\\) .\n\\(\\pi_A = 1- \\pi_0 = \\text{Pr}(H_A)\\) prior probabiltiy \\(H_A\\).Note weights \\(\\pi_0\\) can fact estimated observations fitting mixture distribution\nobservations \\(x_1, \\ldots, x_n\\) (effectively empirical Bayes method prior informed data).","code":""},{"path":"bayesian-model-comparison.html","id":"local-fdr","chapter":"11 Bayesian model comparison","heading":"11.4.3.2 Local FDR","text":"posterior probability null model given data point given \n\\[\\text{Pr}(H_0 | x_i) = \\frac{\\pi_0 f_0(x_i)}{f(x_i)} = LFDR(x_i)\\]\nquantity also known local FDR local False Discovery Rate.given one-sided setup local FDR large (close 1) small \\(x\\), \nbecome close 0 large \\(x\\). common decision rule given thresholding\nlocal false discovery rates: \\(LFDR(x_i) < 0.1\\) \\(x_i\\) called significant.","code":""},{"path":"bayesian-model-comparison.html","id":"q-values","chapter":"11 Bayesian model comparison","heading":"11.4.3.3 q-values","text":"correspondence \\(p\\)-values one can also define tail-area based false discovery rates:\n\\[\nFdr(x_i) = \\text{Pr}(H_0 | X > x_i) = \\frac{\\pi_0 F_0(x_i)}{F(x_i)}\n\\]called q-values, simply False Discovery Rates (FDR). Intriguingly, also frequentist\ninterpretation adjusted p-values (using Benjamini-Hochberg adjustment procedure).","code":""},{"path":"bayesian-model-comparison.html","id":"software","chapter":"11 Bayesian model comparison","heading":"11.4.4 Software","text":"number R packages compute (local) FDR values:example:locfdrqvaluefdrtooland many .Using FDR values screening especially useful high-dimensional settings\n(e.g. analysing genomic high-throughput data).FDR values Bayesian well frequentist interpretation, providing evidence \ngood classical statistical methods Bayesian interpretation.","code":""},{"path":"choosing-priors-in-bayesian-analysis.html","id":"choosing-priors-in-bayesian-analysis","chapter":"12 Choosing priors in Bayesian analysis","heading":"12 Choosing priors in Bayesian analysis","text":"","code":""},{"path":"choosing-priors-in-bayesian-analysis.html","id":"choosing-a-prior","chapter":"12 Choosing priors in Bayesian analysis","heading":"12.1 Choosing a prior","text":"","code":""},{"path":"choosing-priors-in-bayesian-analysis.html","id":"prior-as-part-of-the-model","chapter":"12 Choosing priors in Bayesian analysis","heading":"12.1.1 Prior as part of the model","text":"essential Bayesian analysis specify prior\nuncertainty model parameters. Note simply part modelling process! Thus Bayesian approach data analyst needs explicit modelling assumptions.Typically, choosing suitable prior distribution consider overall form (shape domain) distribution well key characteristics mean variance. learned precision (inverse variance)\nprior may often viewed implied sample size.large sample size \\(n\\) posterior mean converges maximum likelihood estimate (posterior distribution normal distribution centered around MLE), large \\(n\\) may ignore specifying prior.However, small \\(n\\) essential prior specified. non-Bayesian approaches prior still either implicit (maximum likelihood estimation) specified via penality (penalised maximum likelihood estimation).","code":""},{"path":"choosing-priors-in-bayesian-analysis.html","id":"some-guidelines","chapter":"12 Choosing priors in Bayesian analysis","heading":"12.1.2 Some guidelines","text":"question remains good ways choose prior? Two useful ways :Use weakly informative prior. means idea (even vague) suitable values parameter interest, use corresponding prior (example moderate variance) model uncertainty. acknowledges uninformative priors also aims prior dominate likelihood (.e. data). result weakly regularised estimator. Note often desirable prior adds information (little) can act regulariser.Use weakly informative prior. means idea (even vague) suitable values parameter interest, use corresponding prior (example moderate variance) model uncertainty. acknowledges uninformative priors also aims prior dominate likelihood (.e. data). result weakly regularised estimator. Note often desirable prior adds information (little) can act regulariser.Empirical Bayes methods can often used determine one hyperparameters (.e. parameters prior) observed data. several ways , one tune shrinkage parameter \\(\\lambda\\) achieve minimum MSE. discuss .Empirical Bayes methods can often used determine one hyperparameters (.e. parameters prior) observed data. several ways , one tune shrinkage parameter \\(\\lambda\\) achieve minimum MSE. discuss .Furthermore, also exist many proposals advocating -called “uninformative priors” “objective priors”.\nHowever, actually unformative priors, since prior distribution looks uninformative (.e. “flat”) one coordinate system can informative another — simple consequence rule transformation \nprobability densities. result, often suggested objective priors fact improper, .e. actually probability distributions!","code":""},{"path":"choosing-priors-in-bayesian-analysis.html","id":"default-priors-or-uninformative-priors","chapter":"12 Choosing priors in Bayesian analysis","heading":"12.2 Default priors or uninformative priors","text":"Objective default priors attempts 1) automatise specification prior 2) find uniformative priors.","code":""},{"path":"choosing-priors-in-bayesian-analysis.html","id":"jeffreys-prior","chapter":"12 Choosing priors in Bayesian analysis","heading":"12.2.1 Jeffreys prior","text":"well-known non-informative prior given proposal \nHarold Jeffreys (1891–1989) \n1946 17.Specifically, prior constructed expected Fisher information thus promises automatic construction objective uninformative priors using likelihood:\n\\[\np(\\boldsymbol \\theta) \\propto \\sqrt{\\det \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)}\n\\]reasoning underlying prior invariance transformation coordinate system parameters.Beta-Binomial model Jeffreys prior corresponds \\(\\text{Beta}(\\frac{1}{2}, \\frac{1}{2})\\). Note uniform distribution U-shaped prior.normal-normal model corresponds flat improper prior \\(p(\\mu) =1\\).IG-normal model Jeffreys prior improper prior \\(p(\\sigma^2) = \\frac{1}{\\sigma^2}\\).already illustrates main problem type prior – namely often improper, .e. prior distribution actually probability distribution (.e. density integrate 1).Another issue Jeffreys priors usually conjugate complicates update prior posterior.Furthermore, multiple parameters (\\(\\boldsymbol \\theta\\) vector) Jeffreys priors usually lead sensible priors.","code":""},{"path":"choosing-priors-in-bayesian-analysis.html","id":"reference-priors","chapter":"12 Choosing priors in Bayesian analysis","heading":"12.2.2 Reference priors","text":"alternative Jeffreys priors -called reference priors developed Bernardo (1979) 18.\ntype priors aims choose prior maximal “correlation” data parameter. precisely, mutual information \\(\\theta\\) \\(x\\) maximised (.e. expected KL divergence posterior prior distribution). underlying motivation data parameters maximally linked (thereby minimising influence prior).univariate settings reference priors identical Jeffreys priors. However, reference prior also provide reasonable priors multivariate settings.Jeffreys’ reference prior approach choice prior expectation data, .e. specific data set hand (can seen positive negative!).","code":""},{"path":"choosing-priors-in-bayesian-analysis.html","id":"empirical-bayes","chapter":"12 Choosing priors in Bayesian analysis","heading":"12.3 Empirical Bayes","text":"empirical Bayes data analysist specifies family prior distribution\n(say Beta distribution free parameters), data hand used find optimal choise hyper-parameters (hence name “empirical”). Thus hyper-parameters specified estimated.","code":""},{"path":"choosing-priors-in-bayesian-analysis.html","id":"type-ii-maximum-likelihood","chapter":"12 Choosing priors in Bayesian analysis","heading":"12.3.1 Type II maximum likelihood","text":"particular, assuming data \\(D\\), likelihood \\(p(D|\\boldsymbol \\theta)\\) model parameters \\(\\boldsymbol \\theta\\) well prior\n\\(p(\\boldsymbol \\theta| \\lambda)\\) \\(\\boldsymbol \\theta\\) hyper-parameter \\(\\lambda\\) marginal likelihood now depends \\(\\lambda\\):\n\\[\np(D | \\lambda) = \\int_{\\boldsymbol \\theta}  p(D|\\boldsymbol \\theta) p(\\boldsymbol \\theta| \\lambda) d\\boldsymbol \\theta\n\\]\ncan therefore use maximum (marginal) likelihood find optimal values \\(\\lambda\\) given data.Since maximum-likelihood used second level step (hyper-parameters) type empirical Bayes also often called “type II maximum likelihood”.","code":""},{"path":"choosing-priors-in-bayesian-analysis.html","id":"shrinkage-estimation-using-empirical-risk-minimisation","chapter":"12 Choosing priors in Bayesian analysis","heading":"12.3.2 Shrinkage estimation using empirical risk minimisation","text":"alternative (related) way estimate hyper-parameters minimising empirical risk.examples Bayesian estimation considered far\nposterior mean parameter interest obtained\nlinear shrinkage\n\\[\n\\hat\\theta_{\\text{shrink}} = \\text{E}( \\theta | D) = \\lambda \\theta_0 + (1-\\lambda) \\hat\\theta_{\\text{ML}}\n\\]\nMLE \\(\\hat\\theta_{\\text{ML}}\\) towards \nprior mean \\(\\theta_0\\), shrinkage intensity \\(\\lambda=\\frac{k_0}{k_0}\\)\ndetermined ration prior posterior concentration parameters \\(k_0\\) \\(k_1\\).resulting point estimate \\(\\hat\\theta_{\\text{shrink}}\\) called shrinkage estimate\nconvex combination \\(\\theta_0\\) \\(\\hat\\theta_{\\text{ML}}\\). prior mean \\(\\theta_0\\) also called “target”.hyperparameter setting \\(k_0\\) (linked precision prior) equivalently \nshrinkage intensity \\(\\lambda\\).optimal value \\(\\lambda\\) can obtained minimising mean squared error \nestimator \\(\\hat\\theta_{\\text{shrink}}\\).particular, construction, target \\(\\theta_0\\) low even zero variance\nnon-vanishing potentially large bias, whereas MLE \\(\\hat\\theta_{\\text{ML}}\\) low zero bias substantial variance. combinining two estimators opposite properties aim achieve\nbias-variance tradeoff resulting estimator \\(\\hat\\theta_{\\text{shrink}}\\) lower MSE either\n\\(\\theta_0\\) \\(\\hat\\theta_{\\text{ML}}\\).Specifically, aim find\n\\[\n\\lambda^{\\star} = \\underset{\\lambda}{\\arg \\min \\ }  \n\\text{E}\\left( ( \\theta - \\hat\\theta_{\\text{shrink}} )^2\\right)\n\\]turns can minimised without knowing actual true value \\(\\theta\\)\nresult unbiased \\(\\hat\\theta_{\\text{ML}}\\) \n\\[\n\\lambda^{\\star} = \\frac{\\text{Var}(\\hat\\theta_{\\text{ML}})}{\\text{E}( (\\hat\\theta_{\\text{ML}} - \\theta_0)^2 )}\n\\]\nHence, shrinkage intensity small variance MLE small /target\nMLE differ substantially. hand, variance MLE large /target close MLE shrinkage intensity large.Choosing shrinkage parameter optimising expected risk (mean squared error) also form empirical Bayes.Example 12.1  James-Stein estimator:Empirical risk minimisation estimate shrinkage parameter normal-normal model\nsingle observation yields James-Stein estimator (1955).Specifically, James Stein propose following estimate \nmultivariate mean \\(\\boldsymbol \\mu\\) using single sample \\(\\boldsymbol x\\)\ndrawn multivariate normal \\(N_d(\\boldsymbol \\mu, \\boldsymbol )\\):\n\\[\n\\hat{\\boldsymbol \\mu}_{JS} = \\left(1-\\frac{d-2}{||\\boldsymbol x||^2}\\right) \\boldsymbol x\n\\]\n, recognise \\(\\hat{\\boldsymbol \\mu}_{ML} = \\boldsymbol x\\), \\(\\boldsymbol \\mu_0=0\\) shrinkage intensity \\(\\lambda^{\\star}=\\frac{d-2}{||\\boldsymbol x||^2}\\).Efron Morris (1972) Lindley Smith (1972)\nlater generalised James-Stein estimator case\nmultiple observations \\(\\boldsymbol x_1, \\ldots \\boldsymbol x_n\\)\ntarget \\(\\boldsymbol \\mu_0\\), yielding empirical Bayes estimate \\(\\mu\\) based normal-normal model.","code":""},{"path":"optimality-properties-and-summary.html","id":"optimality-properties-and-summary","chapter":"13 Optimality properties and summary","heading":"13 Optimality properties and summary","text":"","code":""},{"path":"optimality-properties-and-summary.html","id":"bayesian-statistics-in-a-nutshell","chapter":"13 Optimality properties and summary","heading":"13.1 Bayesian statistics in a nutshell","text":"Bayesian statistics explicitly models uncertainty parameters\ninterest probabilityIn light new evidence (observed data) uncertainty updated, .e. prior distribution combined via Bayes rule likelihood form posterior distributionIf posterior distribution family prior \\(\\rightarrow\\) conjugate prior.exponential family Bayesian update mean always expressible\nlinear shrinkage MLE.large sample size posterior mean becomes maximum likelihood estimator prior playes role.Conversely, small sample size data available posterior stays close prior..","code":""},{"path":"optimality-properties-and-summary.html","id":"advantages","chapter":"13 Optimality properties and summary","heading":"13.1.1 Advantages","text":"Adding prior information regularisation properties. important complex models many parameters, e.g., estimation covariance matrix (avoid singularity).Improves small-sample accuracy (e.g. MSE)Bayesian estimators tend perform better MLEs - surprising use\nobserved data plus extra information available prior.Bayesian credible intervals conceptually much simple frequentist\nconfidence intervals.","code":""},{"path":"optimality-properties-and-summary.html","id":"frequentist-properties-of-bayesian-estimators","chapter":"13 Optimality properties and summary","heading":"13.1.2 Frequentist properties of Bayesian estimators","text":"Bayesian point estimator (e.g. posterior mean) can also assessed frequentist properties.First, construction due introducing prior Bayesian estimator biased \nfinite \\(n\\) even MLE unbiased.Second, intriguingly turns sampling variance Bayes point estimator (confused posterior variance!) can smaller variance MLE. depends choice shrinkage parameter \\(\\lambda\\) also determines posterior variance.result, Bayesian estimators may smaller MSE (=squared bias + variance) ML estimator finite \\(n\\).statistical decision theory called theorem admissibility Bayes rules.\nstates mild conditions every admissible estimation rule (.e. one dominates \nestimators regard expected loss, MSE) fact Bayes estimator prior.Unfortunately, theorem tell prior needed achive optimality, however optimal estimator can often found tuning hyperparameters.","code":""},{"path":"optimality-properties-and-summary.html","id":"specifying-the-prior-problem-or-advantage","chapter":"13 Optimality properties and summary","heading":"13.1.3 Specifying the prior — problem or advantage?","text":"Bayesian statistics data analyst needs explicit modelling assumptions:Model = data generating process (likelihood) + prior uncertainty (prior distribution)Note alternative statistical methods can often interpreted Bayesian methods assuming specific implicit prior!example, likelihood estimation binomial model equivalent Bayes estimation using Beta-Binomial model \\(\\text{Beta}(0,0)\\) prior (=Haldane prior).\nHowever, choosing prior explicitly model, interestingly analysts rather use \nflat prior \\(\\text{Beta}(1,1)\\) (=Laplace prior) implicit sample size \\(k_0=2\\) transformation-invariant prior \\(\\text{Beta}(1/2, 1/2)\\) (=Jeffreys prior) implicit sample size \\(k_0=1\\) rather Haldane prior!\\(\\rightarrow\\) aware implicit priors!!Better acknowledge prior used (even implicit!)\nspecific assumptions enforced Bayesian approach.Specifying prior thus best understood intrinsic part model specification.\nhelps improve inference may ignored lots data.","code":""},{"path":"optimality-properties-and-summary.html","id":"optimality-of-bayesian-inference","chapter":"13 Optimality properties and summary","heading":"13.2 Optimality of Bayesian inference","text":"optimality Bayesian model making use full model specification (likelihood plus prior) can shown number different perspectives. Correspondingly,\nmany theorems prove (least indicate) optimality:Richard Cox’s theorem: generalising classical logic invariably leads Bayesian inference.Richard Cox’s theorem: generalising classical logic invariably leads Bayesian inference.de Finetti’s representation theorem: joint distribution exchangeable observations can always expressed weighted mixture prior distribution parameter model. implies existence prior distribution requirement Bayesian approach.de Finetti’s representation theorem: joint distribution exchangeable observations can always expressed weighted mixture prior distribution parameter model. implies existence prior distribution requirement Bayesian approach.Frequentist decision theory: admissible decision rules Bayes rules!Frequentist decision theory: admissible decision rules Bayes rules!Entropy perspective: posterior density (function!) obtained result optimising entropy criterion. Bayesian updating may thus viewed variational optimisation problem. Specifically, Bayes theorem minimal update new information arrives form observations\n(see ).Entropy perspective: posterior density (function!) obtained result optimising entropy criterion. Bayesian updating may thus viewed variational optimisation problem. Specifically, Bayes theorem minimal update new information arrives form observations\n(see ).Remark: exist number (often somewhat esoteric) suggestions propagating uncertainty “fuzzy logic”, imprecise probabilities, etc. contradict Bayesian learning thus direct violation theorems.","code":""},{"path":"optimality-properties-and-summary.html","id":"connection-with-entropy-learning","chapter":"13 Optimality properties and summary","heading":"13.3 Connection with entropy learning","text":"Bayesian update rule general form learning new information arrives form data. actually even general principle Bayesian update rule just special case: principle minimal information update (e.g. Jaynes 1959, 2003) principle minimum information discrimination (MDI) (Kullback 1959).can summarised follows: Change beliefs much necessary coherent new evidence!principle “inertia beliefs” new information arrives uncertainty parameter minimally adjusted, much needed account new information.\nimplement principle KL divergence natural measure quantify \nchange underlying beliefs. known entropy learning.Bayes rule emerges special case entropy learning:KL divergence joint posterior \\(Q_{x,\\boldsymbol \\theta}\\) joint prior distribution \\(P_{x,\\boldsymbol \\theta}\\) computed, posterior\ndistribution \\(Q_{\\boldsymbol \\theta|x}\\) free parameter.conditional distribution \\(Q_{\\boldsymbol \\theta|x}\\) found minimising KL divergence \\(D_{\\text{KL}}(Q_{x,\\boldsymbol \\theta}, P_{x,\\boldsymbol \\theta})\\).optimal solution variational optimisation problem given Bayes’ rule!application KL divergence example reverse KL optimisation (aka \\(\\)-projection, see Part notes). Intringuingly, explains zero forcing property Bayes’ rule (general property \\(\\)-projection).Applying entropy learning therefore includes Bayesian learning special case:information arrives form data \\(\\rightarrow\\) update prior Bayes’ theorem (Bayesian learning).Interestingly, entropy learning lead update rules types information:information arrives form another distribution \\(\\rightarrow\\) update using R. Jeffrey’s rule conditioning (1965).information arrives form another distribution \\(\\rightarrow\\) update using R. Jeffrey’s rule conditioning (1965).information presented form constraints \\(\\rightarrow\\) Kullback’s principle minimum MDI (1959), E. T. Jaynes maximum entropy (MaxEnt) principle (1957).information presented form constraints \\(\\rightarrow\\) Kullback’s principle minimum MDI (1959), E. T. Jaynes maximum entropy (MaxEnt) principle (1957).shows () fundamentally important KL divergence statistics. leads likelihood inference (via forward KL) also Bayesian learning, well forms information updating (via reverse KL).Furthermore, Bayesian statistics relative entropy useful choose priors (e.g. reference priors) also helps (Bayesian) experimental design quantify information provided experiment.","code":""},{"path":"optimality-properties-and-summary.html","id":"conclusion","chapter":"13 Optimality properties and summary","heading":"13.4 Conclusion","text":"Bayesian statistics offers coherent framework statistical learning data, methods forestimationtestingmodel buildingThere number theorems show “optimal” estimators (defined various ways) Bayesian.conceptually simple — can computationally involved!provides coherent generalisation classical TRUE/FALSE logic (therefore suffer inconsistencies prevalent frequentist statistics).Bayesian statistics non-asymptotic theory, works sample size.\nAsympotically (large \\(n\\)) consistent converges true model (like ML!).\nBayesian reasoning can also applied events take place — assumption hypothetical infinitely many repetitions frequentist statistics needed.Moreover, many classical (frequentist) procedures may viewed approximations Bayesian methods estimators, using classical approaches correct application domain perfectly line Bayesian framework.Bayesian estimation inference also automatically regularises (via prior) important complex models problem overfitting.","code":""},{"path":"matrix-and-calculus-refresher.html","id":"matrix-and-calculus-refresher","chapter":"A Matrix and calculus refresher","heading":"A Matrix and calculus refresher","text":"Statistics mathematical science requires practical working knowledge\nvector matrices calculus functions several variables.refresher essentials matrix algebra calculus please refer supplementary\nMatrix Calculus Refresher notes.details please consult lecture notes related relevant modules\n(e.g. linear algebra, matrix analysis vector calculus).find list topics particularly relevant \nmodule MATH27720 Statistics 2.","code":""},{"path":"matrix-and-calculus-refresher.html","id":"vectors-and-matrices","chapter":"A Matrix and calculus refresher","heading":"A.1 Vectors and matrices","text":"Vector matrix notationVector algebraEigenvectors eigenvalues real symmetric matrixPositive negative definiteness real symmetric matrix (containing positive negative eigenvalues)Matrix inverse","code":""},{"path":"matrix-and-calculus-refresher.html","id":"functions","chapter":"A Matrix and calculus refresher","heading":"A.2 Functions","text":"Gradient vectorHessian matrixConditions local extremum functionConvex concave functionsLinear quadratic approximation","code":""},{"path":"probability-and-statistics-refresher.html","id":"probability-and-statistics-refresher","chapter":"B Probability and statistics refresher","heading":"B Probability and statistics refresher","text":"find brief overview relevant concepts probability statistics\nfamiliar earlier modules (Probability 1 2 well Statistics 2)","code":""},{"path":"probability-and-statistics-refresher.html","id":"basic-mathematical-notation","chapter":"B Probability and statistics refresher","heading":"B.1 Basic mathematical notation","text":"Summation:\n\\[\n\\sum_{=1}^n x_i = x_1 + x_2 + \\ldots + x_n\n\\]Multiplication:\n\\[\n\\prod_{=1}^n x_i = x_1 \\times x_2 \\times \\ldots \\times x_n\n\\]Indicator function:\n\\[\n1_{} =\n\\begin{cases}\n1 & \\text{$$ true}\\\\\n0 & \\text{$$ true}\\\\\n\\end{cases}\n\\]","code":""},{"path":"probability-and-statistics-refresher.html","id":"combinatorics","chapter":"B Probability and statistics refresher","heading":"B.2 Combinatorics","text":"","code":""},{"path":"probability-and-statistics-refresher.html","id":"number-of-permutations","chapter":"B Probability and statistics refresher","heading":"B.2.1 Number of permutations","text":"number possible orderings, permutations, \\(n\\) distinct items \nnumber ways put \\(n\\) items \\(n\\) bins exactly one item bin. given \nfactorial\n\\[\nn! = \\prod_{=1}^n = 1 \\times 2 \\times \\ldots \\times n\n\\]\n\\(n\\) positive integer.\n\\(n=0\\) factorial defined \n\\[\n0! = 1\n\\]\nexactly one permutation zero objects.factorial can also obtained using \ngamma function\n\\[\n\\Gamma(x) = \\int_0^\\infty t^{x-1} e^{-t} dt\n\\]\ncan viewed continuous version factorial\n\n\\(\\Gamma(x) = (x-1)!\\) positive integer \\(x\\).","code":""},{"path":"probability-and-statistics-refresher.html","id":"multinomial-and-binomial-coefficient","chapter":"B Probability and statistics refresher","heading":"B.2.2 Multinomial and binomial coefficient","text":"number possible permutation \\(n\\) items \\(K\\) distinct types, \\(n_1\\) type 1, \\(n_2\\) type 2 , equals number ways\nput \\(n\\) items \\(K\\) bins \\(n_1\\) items first bin, \\(n_2\\) second .\ngiven multinomial coefficient\n\\[\n\\binom{n}{n_1, \\ldots, n_K} = \\frac {n!}{n_1! \\times n_2! \\times\\ldots \\times n_K! }\n\\]\n\\(\\sum_{k=1}^K n_k = n\\) \\(K \\leq n\\).\nNote equals number permutation items divided number permutations items bin (type).\\(n_k=1\\) hence \\(K=n\\) multinomial coefficient reduces factorial.two bins / types (\\(K=2\\)) multinomial coefficients becomes \nbinomial coefficient\n\\[\n\\binom{n}{n_1} = \\binom{n}{n_1, n-n_1}    =  \\frac {n!}{n_1! (n - n_1)!}\n\\]\ncounts number ways choose \\(n_1\\) elements set \\(n\\) elements.","code":""},{"path":"probability-and-statistics-refresher.html","id":"de-moivre-sterling-approximation-of-the-factorial","chapter":"B Probability and statistics refresher","heading":"B.2.3 De Moivre-Sterling approximation of the factorial","text":"factorial frequently approximated following formula derived Abraham de Moivre (1667–1754) James Stirling (1692-1770)\n\\[\nn! \\approx \\sqrt{2 \\pi} n^{n+\\frac{1}{2}} e^{-n}\n\\]\nequivalently logarithmic scale\n\\[\n\\log n!  \\approx \\left(n+\\frac{1}{2}\\right) \\log n  -n + \\frac{1}{2}\\log \\left( 2 \\pi\\right)\n\\]\napproximation good small \\(n\\) (fails \\(n=0\\)) becomes\naccurate increasing \\(n\\). large \\(n\\) approximation can simplified \n\\[\n\\log n! \\approx  n \\log n  -n\n\\]","code":""},{"path":"probability-and-statistics-refresher.html","id":"probability","chapter":"B Probability and statistics refresher","heading":"B.3 Probability","text":"","code":""},{"path":"probability-and-statistics-refresher.html","id":"random-variables","chapter":"B Probability and statistics refresher","heading":"B.3.1 Random variables","text":"random variable describes random experiment. set possible outcomes\nsample space state space random variable denoted \n\\(\\Omega = \\{\\omega_1, \\omega_2, \\ldots\\}\\). outcomes \\(\\omega_i\\) elementary events.\nsample space \\(\\Omega\\) can finite infinite. Depending type outcomes\nrandom variable discrete continuous.event \\(\\subseteq \\Omega\\) subset \\(\\Omega\\) thus set composed elementary events: \\(= \\{a_1, a_2, \\ldots\\}\\).\nincludes special cases full set \\(= \\Omega\\), empty set \\(= \\emptyset\\), elementary\nevents \\(=\\omega_i\\). complementary event \\(^C\\) complement set \\(\\) set \\(\\Omega\\)\n\\(^C = \\Omega \\setminus = \\{\\omega_i \\\\Omega: \\omega_i \\notin \\}\\).probability event \\(\\) denoted \\(\\text{Pr}()\\).\nEssentially, obtain probability need count \nelementary elements corresponding \\(\\). \nassume axioms probability \\(\\text{Pr}() \\geq 0\\), probabilities positive,\\(\\text{Pr}(\\Omega) = 1\\), certain event probability 1, \\(\\text{Pr}() = \\sum_{a_i \\} \\text{Pr}(a_i)\\), probability \nevent equals sum constituting elementary events \\(a_i\\).\nsum taken finite countable infinite number elements.implies\\(\\text{Pr}() \\leq 1\\), .e. probabilities lie interval \\([0,1]\\)\\(\\text{Pr}(^C) = 1 - \\text{Pr}()\\), \\(\\text{Pr}(\\emptyset) = 0\\)Assume now two events \\(\\) \\(B\\).\nprobability event “\\(\\) \\(B\\)” given probability set intersection\n\\(\\text{Pr}(\\cap B)\\).\nLikewise probability event “\\(\\) \\(B\\)” given probability set union\n\\(\\text{Pr}(\\cup B)\\).clear definition theory probability closely linked set theory, particular measure theory. Indeed, viewing probability special type measure allows elegant treatment discrete continuous random variables\n(one scope module).","code":""},{"path":"probability-and-statistics-refresher.html","id":"probability-mass-and-density-function-distribution-function-and-quantile-function","chapter":"B Probability and statistics refresher","heading":"B.3.2 Probability mass and density function, distribution function and quantile function","text":"describe random variable \\(x\\) state space \\(\\Omega\\) need way effectively store probabilities corresponding elementary outcomes \\(x \\\\Omega\\). Note convenience use symbol denote random variable elementary outcomes.discrete random variable define \nevent \\(= \\{x: x=\\} = \\{\\}\\) get probability\n\\[\n\\text{Pr}() = \\text{Pr}(x=) = f()\n\\]\ndirectly probability mass function (PMF), denoted lower case \\(f\\)\n(frequently also use \\(p\\) \\(q\\)).\nPMF property \\(\\sum_{x \\\\Omega} f(x) = 1\\) \n\\(f(x) \\[0,1]\\).continuous random variables need use probability density function (PDF) instead. define event\n\\(= \\{x: < x \\leq + da\\}\\) infinitesimal interval\nassign probability\n\\[\n\\text{Pr}() = \\text{Pr}( < x \\leq + da) = f() da \\,.\n\\]\nPDF property \\(\\int_{x \\\\Omega} f(x) dx = 1\\)\ncontrast PMF density \\(f(x)\\geq 0\\) may take values larger 1.alternative using PMF/PDFs may also use distribution function describe random variable. assumes ordering exist among elementary events can define event \\(= \\{x: x \\leq \\}\\) compute \nprobability \n\\[\nF() = \\text{Pr}() = \\text{Pr}( x \\leq ) =\n\\begin{cases}\n\\sum_{x \\} f(x) & \\text{discrete case} \\\\\n\\int_{x \\} f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\]\nalso known cumulative distribution function (CDF)\ndenoted upper case \\(F\\) (\\(P\\) \\(Q\\)).\nconstruction distribution function monotonically non-decreasing value ranges 0 1.\nhelp can compute probability interval set\n\n\\[\n\\text{Pr}( < x \\leq b ) = F(b)-F() \\,.\n\\]inverse distribution function \\(y=F(x)\\) quantile function \\(x=F^{-1}(y)\\).\n50% quantile \\(F^{-1}\\left(\\frac{1}{2}\\right)\\) median.random variable \\(x\\) distribution function \\(F\\) write \\(x \\sim F\\).","code":""},{"path":"probability-and-statistics-refresher.html","id":"expectation-of-a-random-variable","chapter":"B Probability and statistics refresher","heading":"B.3.3 Expectation of a random variable","text":"expected value \\(\\text{E}(x)\\) random variable defined \nweighted average possible outcomes, weight given PMF / PDF \\(f(x)\\):\n\\[\n\\text{E}_{F}(x) =\n\\begin{cases}\n\\sum_{x \\\\Omega} f(x) x & \\text{discrete case} \\\\\n\\int_{x \\\\Omega} f(x) x dx & \\text{continuous case} \\\\\n\\end{cases}\n\\]\nNote notation emphasise expectation taken regard distribution \\(F\\). subscript \\(F\\) usually left \nambiguities.\nFurthermore, sum integral may diverge\nexpectation necessarily always defined (contrast quantiles).expected value function random variable \\(h(x)\\) \nobtained similarly:\n\\[\n\\text{E}_{F}(h(x)) =\n\\begin{cases}\n\\sum_{x \\\\Omega} f(x) h(x) & \\text{discrete case} \\\\\n\\int_{x \\\\Omega} f(x) h(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\]\ncalled “law unconscious statistician”, short LOTUS.\n, highlight random variable \\(x\\) distribution \\(F\\) \nwrite \\(\\text{E}_F(h(x))\\).","code":""},{"path":"probability-and-statistics-refresher.html","id":"jensens-inequality-for-the-expectation","chapter":"B Probability and statistics refresher","heading":"B.3.4 Jensen’s inequality for the expectation","text":"\\(h(\\boldsymbol x)\\) convex function following\ninequality holds:\\[\n\\text{E}(h(\\boldsymbol x)) \\geq h(\\text{E}(\\boldsymbol x))\n\\]Recall: convex function (\\(x^2\\)) shape “valley”.","code":""},{"path":"probability-and-statistics-refresher.html","id":"probability-as-expectation","chapter":"B Probability and statistics refresher","heading":"B.3.5 Probability as expectation","text":"Probability can also understood expectation.\nevent \\(\\) can define corresponding indicator function\n\\(1_{ x \\}\\) elementary element \\(x\\) part \\(\\).\nfollows\n\\[\n\\text{E}( 1_{x \\} ) = \\text{Pr}() \\, ,\n\\]Interestingly, one can develop whole theory probability perspective. 19","code":""},{"path":"probability-and-statistics-refresher.html","id":"moments-and-variance-of-a-random-variable","chapter":"B Probability and statistics refresher","heading":"B.3.6 Moments and variance of a random variable","text":"moments random variable defined follows:Zeroth moment: \\(\\text{E}(x^0) = 1\\) construction PDF PMF,First moment: \\(\\text{E}(x^1) = \\text{E}(x) = \\mu\\) , mean,Second moment: \\(\\text{E}(x^2)\\)variance second moment centred mean \\(\\mu\\):\n\\[\\text{Var}(x) = \\text{E}( (x - \\mu)^2 ) = \\sigma^2\\]variance can also computed \\(\\text{Var}(x) = \\text{E}(x^2)-\\text{E}(x)^2\\). Note example Jensen’s inequality,\n\\(\\text{E}(x^2) =\\text{E}(x)^2 + \\text{Var}(x) \\geq \\text{E}(x)^2\\).distribution necessarily need finite first higher moments.\nexample Cauchy distribution mean variance (higher moment).","code":""},{"path":"probability-and-statistics-refresher.html","id":"distribution-of-sums-of-random-variables","chapter":"B Probability and statistics refresher","heading":"B.3.7 Distribution of sums of random variables","text":"sum two normal random variables also normal (appropriate mean variance).central limit theorem, first postulated Abraham de Moivre (1667–1754), asserts\nmany cases distribution sum identically distributed random variables converges normal distribution, even individual random variables normal.example, result binomial distribution (sum Bernoulli random variables) can approximated normal distribution.","code":""},{"path":"probability-and-statistics-refresher.html","id":"transformation-of-random-variables","chapter":"B Probability and statistics refresher","heading":"B.3.8 Transformation of random variables","text":"Linear transformation random variables: \\(\\) \\(b\\) constants \\(x\\) random variable, random variable \\(y= + b x\\) mean \\(\\text{E}(y) = + b \\text{E}(x)\\) variance \\(\\text{Var}(y) = b^2 \\text{Var}(x)\\).general invertible coordinate transformation \\(y = h(x) = y(x)\\) backtransformation \\(x = h^{-1}(y) = x(y)\\).transformation infinitesimal volume element \\(dy = \\left|\\frac{dy}{dx}\\right| dx\\).transformation density \\(f_y(y) =\\left|\\frac{dx}{dy}\\right| f_x(x(y))\\).Note \\(\\left|\\frac{dx}{dy}\\right| = \\left|\\frac{dy}{dx}\\right|^{-1}\\).","code":""},{"path":"probability-and-statistics-refresher.html","id":"random-vectors-and-covariance-matrix","chapter":"B Probability and statistics refresher","heading":"B.3.9 Random vectors and covariance matrix","text":"Instead scalar random variables one often also considers random vectors also random matrices.random vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T\\) mean \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\) simply comprised means components, .e. \\(\\boldsymbol \\mu= (\\mu_1, \\ldots, \\mu_d)^T\\). Thus, mean random vector dimension vector length.variance random vector length \\(d\\), however, vector matrix size \\(d\\times d\\). matrix called covariance matrix:\n\\[\n\\begin{split}\n\\text{Var}(\\boldsymbol x) &= \\underbrace{\\boldsymbol \\Sigma}_{d\\times d} = (\\sigma_{ij}) = \\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix} \\\\\n  &=\\text{E}\\left(\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d\\times 1} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1\\times d}\\right) \\\\\n  & = \\text{E}(\\boldsymbol x\\boldsymbol x^T)-\\boldsymbol \\mu\\boldsymbol \\mu^T \\\\\n\\end{split}\n\\]\nentries covariance matrix \\(\\sigma_{ij} =\\text{Cov}(x_i, x_j)\\) describe covariance random variables \\(x_i\\) \\(x_j\\). covariance matrix symmetric, hence \\(\\sigma_{ij}=\\sigma_{ji}\\). diagonal entries \\(\\sigma_{ii} = \\text{Cov}(x_i, x_i) = \\text{Var}(x_i) = \\sigma_i^2\\) correspond variances components \\(\\boldsymbol x\\). covariance matrix construction positive semi-definite, .e. eigenvalues \\(\\boldsymbol \\Sigma\\) positive equal zero.However, wherever possible one aim use models non-singular covariance matrices, eigenvalues positive, covariance matrix invertible.univariate \\(x\\) scalar constant \\(\\) variance \\(x\\) equals \\(\\text{Var}(x) = ^2 \\text{Var}(x)\\). random vector \\(\\boldsymbol x\\) dimension \\(d\\) constant matrix \\(\\boldsymbol \\) dimension \\(m \\times d\\) generalises \\(\\text{Var}(\\boldsymbol Ax) = \\boldsymbol \\text{Var}(\\boldsymbol x) \\boldsymbol ^T\\).","code":""},{"path":"probability-and-statistics-refresher.html","id":"correlation-matrix","chapter":"B Probability and statistics refresher","heading":"B.3.10 Correlation matrix","text":"covariance matrix can factorised product\n\\[\n\\boldsymbol \\Sigma= \\boldsymbol V^{\\frac{1}{2}} \\boldsymbol P\\boldsymbol V^{\\frac{1}{2}}\n\\]\n\\(\\boldsymbol V\\) diagonal matrix containing variances\n\\[\n\\boldsymbol V= \\begin{pmatrix}\n    \\sigma_{11} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma_{dd}\n\\end{pmatrix}\n\\]\nmatrix \\(\\boldsymbol P\\) (“upper case rho”) symmetric correlation matrix\n\\[\n\\boldsymbol P= (\\rho_{ij}) = \\begin{pmatrix}\n    1 & \\dots & \\rho_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{d1} & \\dots & 1\n\\end{pmatrix}   = \\boldsymbol V^{-\\frac{1}{2}} \\boldsymbol \\Sigma\\boldsymbol V^{-\\frac{1}{2}}\n\\]\nThus, correlation \\(x_i\\) \\(x_j\\) defined \n\\[\n\\rho_{ij} = \\text{Cor}(x_i,x_j) = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}\n\\]","code":""},{"path":"probability-and-statistics-refresher.html","id":"statistics","chapter":"B Probability and statistics refresher","heading":"B.4 Statistics","text":"","code":""},{"path":"probability-and-statistics-refresher.html","id":"statistical-learning","chapter":"B Probability and statistics refresher","heading":"B.4.1 Statistical learning","text":"aim statistics — data science — machine learning use data\n(experiments, observations, measurements) learn understand world\nusing models. statistics employ probabilistic models.Let denote data \\(D =\\{x_1, \\ldots, x_n\\}\\) models \\(p(x| \\theta)\\) \\(\\theta\\) represents\nparameters model. Often (always) \\(\\theta\\) can interpreted /\nassociated manifest property model.\nsingle parameter write \\(\\theta\\) (scalar parameter).\nwish highlight multiple parameters write \\(\\boldsymbol \\theta\\) (bold type).Specifically, aim identify best model(s) data order bothexplain current data, andto enable good prediction future data.Note generally easy find one several models explain data \noften predict well.Therefore, one like avoid overfitting data identify models \nappropriate data hand (.e. simple also complex).Typically, focus analysis specific model family parameter \\(\\theta\\).\nestimator \\(\\theta\\) function \\(\\hat{\\theta}(D)\\) data\nmaps data (input) informed guess (output) \\(\\theta\\).point estimator provides single number parameterAn interval estimator provides set possible values parameter.Interval estimators can linked concept testing specified values\nparameter. Specfically confidence interval contains parameter values \nsignificantly different best parameter.","code":""},{"path":"probability-and-statistics-refresher.html","id":"sampling-properties-of-a-point-estimator-hatboldsymbol-theta","chapter":"B Probability and statistics refresher","heading":"B.4.2 Sampling properties of a point estimator \\(\\hat{\\boldsymbol \\theta}\\)","text":"point estimator \\(\\hat\\theta\\) depends data, hence exibits sampling variation, .e. estimate different new set observations.Thus \\(\\hat\\theta\\) can seen random variable, distribution called sampling distribution (across different experiments).Properties distribution can used evaluate far estimator\ndeviates (average across different experiments) true value:\\[\\begin{align*}\n\\begin{array}{rr}\n\\text{Bias:}\\\\\n\\text{Variance:}\\\\\n\\text{Mean squared error:}\\\\\n\\\\\n\\end{array}\n\\begin{array}{rr}\n\\text{Bias}(\\hat{\\theta})\\\\\n\\text{Var}(\\hat{\\theta})\\\\\n\\text{MSE}(\\hat{\\theta})\\\\\n\\\\\n\\end{array}\n\\begin{array}{ll}\n=\\text{E}(\\hat{\\theta})-\\theta\\\\\n=\\text{E}\\left((\\hat{\\theta}-\\text{E}(\\hat{\\theta}))^2\\right)\\\\\n=\\text{E}((\\hat{\\theta}-\\theta)^2)\\\\\n=\\text{Var}(\\hat{\\theta})+\\text{Bias}(\\hat{\\theta})^2\\\\\n\\end{array}\n\\end{align*}\\]last identity MSE follows \\(\\text{E}(x^2)=\\text{Var}(x)+\\text{E}(x)^2\\).first sight seems desirable focus unbiased (finite \\(n\\)) estimators.\nHowever, requiring strict unbiasedness always good idea.\nmany situations better allow small bias order achieve smaller variance\noverall total smaller MSE. called bias-variance tradeoff — bias\ntraded smaller variance (, conversely, less bias traded higher variance)","code":""},{"path":"probability-and-statistics-refresher.html","id":"efficiency-and-consistency-of-an-estimator","chapter":"B Probability and statistics refresher","heading":"B.4.3 Efficiency and consistency of an estimator","text":"Typically, \\(\\text{Bias}\\), \\(\\text{Var}\\) \\(\\text{MSE}\\) decrease increasing sample size\ndata \\(n \\\\infty\\) errors become smaller smaller.Efficiency: estimator \\(\\hat\\theta_A\\) said efficient estimator\n\\(\\hat\\theta_B\\) sample size \\(n\\) smaller error (e.g. MSE) \ncompeting estimator.typical rate decrease variance good estimator \\(\\frac{1}{n}\\)\nrate decrease standard deviation \\(\\frac{1}{\\sqrt{n}}\\).\nNote implies get one digit accuracy estimate (standard deviation\ndecreasing factor 10) need 100 times data!Consistency: \\(\\hat{\\theta}\\) called consistent \n\\[\n\\text{MSE}(\\hat{\\theta}) \\longrightarrow 0 \\text{ $n\\rightarrow \\infty$ }\n\\]Consistency essential rather weak requirement reasonable estimator.\nconsistent\nestimators typically select estimators efficient (.e. fasted decrease MSE)\ntherefore smallest variance /MSE given finite \\(n\\).Consistency implies recover true model limit infinite data \nmodel class contains true data generating model.\nmodel class contain true model strict consistency\nachieved still wish get close possible\ntrue model choosing model parameters.","code":""},{"path":"probability-and-statistics-refresher.html","id":"empirical-distribution-function","chapter":"B Probability and statistics refresher","heading":"B.4.4 Empirical distribution function","text":"Suppose observe data \\(D=\\{x_1, \\ldots, x_n\\}\\) \\(x_i \\sim F\\)\nsampled independently identically. empirical cumulative distribution function\n\\(\\hat{F}_n(x)\\) based data \\(D\\) given \n\\[\n\\hat{F}_n(x) = \\frac{1}{n} \\sum_{=1}^n 1_{x_i \\leq x}\n\\]empirical distribution function monotonically non-decreasing 0 1 discrete steps.R empirical distribution function computed ecdf().Crucially, empirical distribution\n\\(\\hat{F}_n\\) converges strongly (almost surely) \nunderlying distribution \\(F\\) \\(n \\rightarrow \\infty\\):\n\\[\n\\hat{F}_n\\overset{. s.}{\\} F\n\\]\nGlivenko–Cantelli theorem additionally asserts convergence uniform.Note effectively variant law large numbers applied\nwhole distribution, rather just mean (see ).result, may use empirical distribution \\(\\hat{F}_n\\) based data \\(D\\) \nestimate underlying unknown true distribution \\(F\\). convergence\ntheorems know \\(\\hat{F}_n\\) consistent.However, \\(\\hat{F}_n\\) work well estimate \\(F\\) number observations \\(n\\) must \nsufficiently large\napproximation provided \\(\\hat{F}_n\\) adequate.","code":""},{"path":"probability-and-statistics-refresher.html","id":"empirical-estimators","chapter":"B Probability and statistics refresher","heading":"B.4.5 Empirical estimators","text":"fact large sample size \\(n\\) empirical distribution \\(\\hat{F}_n\\)\nmay used substitute unknown \\(F\\) allows us easily construct\nempirical estimators.Specifically, parameters model can typically expressed \nfunctional distribution\n\\(\\theta = g(F)\\). empirical estimator \\(\\hat{\\theta}\\) constructed substituting true distribution empirical distribution \\(\\hat{\\theta}= g( \\hat{F}_n )\\).example mean \\(\\text{E}_F(x)\\) regard \\(F\\). empirical mean expectation regard empirical distribution\nequals average samples:\n\\[\n\\hat{\\text{E}}(x) = \\hat{\\mu} =  \\text{E}_{\\hat{F}_n}(x) = \\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\n\\]Similarly, empirical estimators can constructed simply replacing\nexpectation definition quantity interest sample average.\nexample, empirical variance unknown mean given \n\\[\n\\widehat{\\text{Var}}(x) = \\widehat{\\sigma^2} =\n\\text{E}_{\\hat{F}_n}((x - \\hat{\\mu})^2) = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\n\\]\nNote factor \\(1/n\\) summation sign. can also write empirical\nvariance terms \n\\(\\overline{x^2} =\\frac{1}{n}\\sum^{n}_{k=1} x^2\\) \n\\[\n\\widehat{\\text{Var}}(x) = \\overline{x^2} - \\bar{x}^2\n\\]construction, result strong convergence\n\\(\\hat{F}_n\\) \\(F\\) empirical estimators consistent, MSE, variance\nbias decreasing zero large sample size \\(n\\). However, \nfinite sample size finite variance may also biased.example, empirical variance given biased \n\\(\\text{Bias}(\\widehat{\\sigma^2}) = -\\sigma^2/n\\). Note bias decreases \\(n\\).\nunbiased estimator\ncan obtained rescaling empirical estimator factor\n\\(n/(n-1)\\):\n\\[\n\\widehat{\\sigma^2}_{\\text{UB}} = \\frac{1}{n-1} \\sum_{=1}^n (x_i -\\bar{x})^2\n\\]empirical estimators mean variance can also obtained\nrandom vectors \\(\\boldsymbol x\\). case data \\(D = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n \\}\\)\ncomprised \\(n\\) vector-valued observations.mean get\n\\[\n\\hat{\\boldsymbol \\mu} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k = \\bar{\\boldsymbol x}\n\\]\ncovariance\n\\[\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\sum^{n}_{k=1} \\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right) \\; \\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)^T\\]\nNote factor \\(\\frac{1}{n}\\) estimator covariance matrix.\\(\\overline{\\boldsymbol x\\boldsymbol x^T} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k \\boldsymbol x_k^T\\)\ncan also write\n\\[\n\\widehat{\\boldsymbol \\Sigma} = \\overline{\\boldsymbol x\\boldsymbol x^T} - \\bar{\\boldsymbol x} \\bar{\\boldsymbol x}^T\n\\]","code":""},{"path":"probability-and-statistics-refresher.html","id":"law-of-large-numbers","chapter":"B Probability and statistics refresher","heading":"B.4.6 Law of large numbers","text":"law large numbers discovered Jacob Bernoulli (1655-1705) states average\nconverges mean.Since \\(\\hat{F}_n\\) convergences strongly \\(F\\) \\(n \\rightarrow \\infty\\)\ncorresponding convergence \naverage\n\\(\\text{E}_{\\hat{F}_n}(h(x)) = \\frac{1}{n} \\sum_{=1}^n h(x_i)\\) expectation \\(\\text{E}_{F}(h(x))\\).words, mean exists sufficiently large \\(n\\) can\nsubstituted empirical mean.Likewise, law large numbers can applied empirical estimators\nshow converge corresponding true quantities sufficiently large \\(n\\).Furthermore, one may use law large numbers justification interpret large-sample limits frequencies probabilities. However, converse , namely requesting probabilities must frequentist interpretation, follow law large numbers axioms probability.Finally, worth pointing law large number says nothing\nfinite sample properties estimators.","code":""},{"path":"probability-and-statistics-refresher.html","id":"sampling-distribution-of-mean-and-variance-estimators-for-normal-data","chapter":"B Probability and statistics refresher","heading":"B.4.7 Sampling distribution of mean and variance estimators for normal data","text":"underlying distribution family \\(D = \\{x_1, \\ldots, x_n\\}\\) \nknown can often obtain exact distribution estimator.example, assuming normal distribution \\(N(\\mu, \\sigma^2)\\) can derive\nsampling distribution empirical mean variance:empirical estimator mean parameter \\(\\mu\\) given \\(\\hat{\\mu} = \\frac{1}{n} \\sum_{=1}^n x_i\\). normal assumption distribution \\(\\hat{\\mu}\\) \n\\[\n\\hat{\\mu} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]\nThus \\(\\text{E}(\\hat{\\mu}) = \\mu\\) \\(\\text{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n}\\).\nestimate \\(\\hat{\\mu}\\) unbiased \\(\\text{E}(\\hat{\\mu})-\\mu = 0\\). mean\nsquared error \\(\\hat{\\mu}\\) \\(\\text{MSE}(\\hat{\\mu}) = \\frac{\\sigma^2}{n}\\).empirical estimator mean parameter \\(\\mu\\) given \\(\\hat{\\mu} = \\frac{1}{n} \\sum_{=1}^n x_i\\). normal assumption distribution \\(\\hat{\\mu}\\) \n\\[\n\\hat{\\mu} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]\nThus \\(\\text{E}(\\hat{\\mu}) = \\mu\\) \\(\\text{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n}\\).\nestimate \\(\\hat{\\mu}\\) unbiased \\(\\text{E}(\\hat{\\mu})-\\mu = 0\\). mean\nsquared error \\(\\hat{\\mu}\\) \\(\\text{MSE}(\\hat{\\mu}) = \\frac{\\sigma^2}{n}\\).empirical variance \\(\\widehat{\\sigma^2} = \\frac{1}{n} \\sum_{=1}^n (x_i -\\bar{x})^2\\) normal data follows one-dimensional Wishart distribution\n\\[\n\\widehat{\\sigma^2} \\sim\nW_1\\left(s^2 = \\frac{\\sigma^2}{n}, k=n-1\\right)\n\\]\nThus, \\(\\text{E}( \\widehat{\\sigma^2} ) = \\frac{n-1}{n}\\sigma^2\\) \n\\(\\text{Var}( \\widehat{\\sigma^2}_{\\text{ML}} ) = \\frac{2(n-1)}{n^2}\\sigma^4\\).\nestimate \\(\\widehat{\\sigma^2}\\) biased since\n\\(\\text{E}( \\widehat{\\sigma^2}_{\\text{ML}} )-\\sigma^2 = -\\frac{1}{n}\\sigma^2\\).\nmean squared error \\(\\text{MSE}( \\widehat{\\sigma^2}) = \\frac{2(n-1)}{n^2}\\sigma^4 +\\frac{1}{n^2}\\sigma^4 =\\frac{2 n-1}{n^2}\\sigma^4\\).empirical variance \\(\\widehat{\\sigma^2} = \\frac{1}{n} \\sum_{=1}^n (x_i -\\bar{x})^2\\) normal data follows one-dimensional Wishart distribution\n\\[\n\\widehat{\\sigma^2} \\sim\nW_1\\left(s^2 = \\frac{\\sigma^2}{n}, k=n-1\\right)\n\\]\nThus, \\(\\text{E}( \\widehat{\\sigma^2} ) = \\frac{n-1}{n}\\sigma^2\\) \n\\(\\text{Var}( \\widehat{\\sigma^2}_{\\text{ML}} ) = \\frac{2(n-1)}{n^2}\\sigma^4\\).\nestimate \\(\\widehat{\\sigma^2}\\) biased since\n\\(\\text{E}( \\widehat{\\sigma^2}_{\\text{ML}} )-\\sigma^2 = -\\frac{1}{n}\\sigma^2\\).\nmean squared error \\(\\text{MSE}( \\widehat{\\sigma^2}) = \\frac{2(n-1)}{n^2}\\sigma^4 +\\frac{1}{n^2}\\sigma^4 =\\frac{2 n-1}{n^2}\\sigma^4\\).unbiased variance estimate \\(\\widehat{\\sigma^2}_{\\text{UB}} = \\frac{1}{n-1} \\sum_{=1}^n (x_i -\\bar{x})^2\\) normal data follows one-dimensional Wishart distribution\n\\[\n\\widehat{\\sigma^2}_{\\text{UB}} \\sim\nW_1\\left(s^2 = \\frac{\\sigma^2}{n-1}, k = n-1 \\right)\n\\]\nThus, \\(\\text{E}( \\widehat{\\sigma^2}_{\\text{UB}} ) = \\sigma^2\\) \n\\(\\text{Var}( \\widehat{\\sigma^2}_{\\text{UB}} ) = \\frac{2}{n-1}\\sigma^4\\).\nestimate \\(\\widehat{\\sigma^2}_{\\text{ML}}\\) unbiased since\n\\(\\text{E}( \\widehat{\\sigma^2}_{\\text{UB}} )-\\sigma^2 =0\\).\nmean squared error \\(\\text{MSE}( \\widehat{\\sigma^2}_{\\text{UB}} ) =\\frac{2}{n-1}\\sigma^4\\).\nInterestingly, \\(n>1\\) find \\(\\text{Var}( \\widehat{\\sigma^2}_{\\text{UB}} ) > \\text{Var}( \\widehat{\\sigma^2}_{\\text{ML}} )\\) \\(\\text{MSE}( \\widehat{\\sigma^2}_{\\text{UB}} ) > \\text{MSE}( \\widehat{\\sigma^2}_{\\text{ML}} )\\) biased empirical estimator lower variance lower mean squared error unbiased estimator.unbiased variance estimate \\(\\widehat{\\sigma^2}_{\\text{UB}} = \\frac{1}{n-1} \\sum_{=1}^n (x_i -\\bar{x})^2\\) normal data follows one-dimensional Wishart distribution\n\\[\n\\widehat{\\sigma^2}_{\\text{UB}} \\sim\nW_1\\left(s^2 = \\frac{\\sigma^2}{n-1}, k = n-1 \\right)\n\\]\nThus, \\(\\text{E}( \\widehat{\\sigma^2}_{\\text{UB}} ) = \\sigma^2\\) \n\\(\\text{Var}( \\widehat{\\sigma^2}_{\\text{UB}} ) = \\frac{2}{n-1}\\sigma^4\\).\nestimate \\(\\widehat{\\sigma^2}_{\\text{ML}}\\) unbiased since\n\\(\\text{E}( \\widehat{\\sigma^2}_{\\text{UB}} )-\\sigma^2 =0\\).\nmean squared error \\(\\text{MSE}( \\widehat{\\sigma^2}_{\\text{UB}} ) =\\frac{2}{n-1}\\sigma^4\\).Interestingly, \\(n>1\\) find \\(\\text{Var}( \\widehat{\\sigma^2}_{\\text{UB}} ) > \\text{Var}( \\widehat{\\sigma^2}_{\\text{ML}} )\\) \\(\\text{MSE}( \\widehat{\\sigma^2}_{\\text{UB}} ) > \\text{MSE}( \\widehat{\\sigma^2}_{\\text{ML}} )\\) biased empirical estimator lower variance lower mean squared error unbiased estimator.","code":""},{"path":"probability-and-statistics-refresher.html","id":"one-sample-t-statistic","chapter":"B Probability and statistics refresher","heading":"B.4.8 One sample \\(t\\)-statistic","text":"Suppose observe \\(n\\) independent data points \\(x_1, \\ldots, x_n \\sim N(\\mu, \\sigma^2)\\).\naverage \\(\\bar{x} = \\sum_{=1}^n x_i\\) distributed \n\\(\\bar{x} \\sim N(\\mu, \\sigma^2/n)\\) correspondingly\n\\[\nz = \\frac{\\bar{x}-\\mu}{\\sqrt{\\sigma^2/n}} \\sim N(0, 1)\n\\]Note \\(z\\) uses known variance \\(\\sigma^2\\).variance unknown estimated\nunbiased \\(s^2_{\\text{UB}} = \\frac{1}{n-1} \\sum_{=1}^n (x_i -\\bar{x})^2\\)\none arrives one sample \\(t\\)-statistic\n\\[\nt_{\\text{UB}} = \\frac{\\bar{x}-\\mu}{\\sqrt{s^2_{\\text{UB}}/n}} \\sim t_{n-1} \\,.\n\\]\ndistributed according Student’s \\(t\\)-distribution\n\\(n-1\\) degrees freedom, mean 0 \\(n>2\\) variance\n\\((n-1)/(n-3)\\) \\(n>3\\).instead unbiased estimate empirical (ML) estimate variance \\(s^2_{\\text{ML}} = \\frac{1}{n} \\sum_{=1}^n (x_i -\\bar{x})^2 = \\frac{n-1}{n} s^2_{\\text{UB}}\\) used leads slightly different statistic\n\\[\nt_{\\text{ML}} = \\frac{\\bar{x}-\\mu}{ \\sqrt{ s^2_{\\text{ML}}/n}}  = \\sqrt{\\frac{n}{n-1}} t_{\\text{UB}}\n\\]\n\n\\[\nt_{\\text{ML}} \\sim \\text{lst}\\left(0, \\tau^2=\\frac{n}{n-1}, n-1\\right)\n\\]\nThus, \\(t_{\\text{ML}}\\) follows location-scale \\(t\\)-distribution, mean 0 \\(n>2\\) variance\n\\(n/(n-3)\\) \\(n>3\\).","code":""},{"path":"probability-and-statistics-refresher.html","id":"two-sample-t-statistic-with-common-variance","chapter":"B Probability and statistics refresher","heading":"B.4.9 Two sample \\(t\\)-statistic with common variance","text":"Now suppose observe normal data \\(D = \\{x_1, \\ldots, x_n\\}\\) two groups\nsample size \\(n_1\\) \\(n_2\\) (\\(n=n_1+n_2\\)) two different means \\(\\mu_1\\) \\(\\mu_2\\) common variance \\(\\sigma^2\\):\n\\[x_1,\\dots,x_{n_1} \\sim N(\\mu_1, \\sigma^2)\\]\n\n\\[x_{n_1+1},\\dots,x_{n} \\sim N(\\mu_2, \\sigma^2)\\]\n\\(\\hat{\\mu}_1 = \\frac{1}{n_1}\\sum^{n_1}_{=1}x_i\\) \n\\(\\hat{\\mu}_2 = \\frac{1}{n_2}\\sum^{n}_{=n_1+1}x_i\\) sample averages within group.common variance \\(\\sigma^2\\) may estimated either \nunbiased estimate \\(s^2_{\\text{UB}} = \\frac{1}{n-2} \\left(\\sum^{n_1}_{=1}(x_i-\\hat{\\mu}_1)^2+\\sum^n_{=n_1+1}(x_i-\\hat{\\mu}_2)^2\\right)\\)\n(note factor \\(n-2\\)) empirical (ML) estimate \\(s^2_{\\text{ML}} = \\frac{1}{n} \\left(\\sum^{n_1}_{=1}(x_i-\\hat{\\mu}_1)^2+\\sum^n_{=n_1+1}(x_i-\\hat{\\mu}_2)^2\\right) =\\frac{n-2}{n} s^2_{\\text{UB}}\\). estimator\ncommon variance often referred pooled variance estimate information pooled two groups obtain estimate.Using unbiased pooled variance estimate two sample \\(t\\)-statistic given \n\\[\nt_{\\text{UB}} = \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ \\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)  s^2_{\\text{UB}}}  }\n= \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ \\left(\\frac{n}{n_1 n_2} \\right) s^2_{\\text{UB}} }  }\n\\]\nterms empirical frequencies \\(\\hat{\\pi}_1 = \\frac{n_1}{n}\\) \\(\\hat{\\pi}_2 = \\frac{n_2}{n}\\)\ncan also written \n\\[\nt_{\\text{UB}} = \\sqrt{n} \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{  \\left(\\frac{1}{\\hat{\\pi}_1}+\\frac{1}{\\hat{\\pi}_2}\\right) s^2_{\\text{UB}} }}\n= \\sqrt{n\\hat{\\pi}_1 \\hat{\\pi}_2} \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ s^2_{\\text{UB}}}}\n\\]\ntwo sample \\(t\\)-statistic distributed \n\\[\nt_{\\text{UB}} \\sim t_{n-2}\n\\]\n.e. according Student’s \\(t\\)-distribution \\(n-2\\) degrees freedom, mean 0 \\(n>3\\) variance \\((n-2)/(n-4)\\) \\(n>4\\).\nLarge values two sample \\(t\\)-statistic indicates indeed two groups\nrather just one.two sample \\(t\\)-statistic using empirical (ML) pooled estimate variance \n\\[\n\\begin{split}\nt_{\\text{ML}} & = \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ \\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)  s^2_{\\text{ML}}  }   }\n= \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ \\left(\\frac{n}{n_1 n_2}\\right) s^2_{\\text{ML}}  }   }\\\\\n& =\\sqrt{n} \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ \\left(\\frac{1}{\\hat{\\pi}_1}+\\frac{1}{\\hat{\\pi}_2}\\right) s^2_{\\text{ML}} }}\n= \\sqrt{n \\hat{\\pi}_1 \\hat{\\pi}_2 } \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{ \\sqrt{ s^2_{\\text{ML}}}}\\\\\n& = \\sqrt{\\frac{n}{n-2}} t_{\\text{UB}}\n\\end{split}\n\\]\n\n\\[\nt_{\\text{ML}} \\sim \\text{lst}\\left(0, \\tau^2=\\frac{n}{n-2}, n-2\\right)\n\\]\nThus, \\(t_{\\text{ML}}\\) follows location-scale \\(t\\)-distribution, mean 0 \\(n>3\\) variance\n\\(n/(n-4)\\) \\(n>4\\).","code":""},{"path":"probability-and-statistics-refresher.html","id":"confidence-intervals","chapter":"B Probability and statistics refresher","heading":"B.4.10 Confidence intervals","text":"confidence interval (CI) interval estimate frequentist interpretation.Definition coverage \\(\\kappa\\) CI: often (repeated identical experiment) estimated CI overlap true parameter value \\(\\theta\\)\nEg.: Coverage \\(\\kappa=0.95\\) (95%) means 95 100 case estimated CI contain (unknown) true value (.e. “cover” \\(\\theta\\)).\nEg.: Coverage \\(\\kappa=0.95\\) (95%) means 95 100 case estimated CI contain (unknown) true value (.e. “cover” \\(\\theta\\)).Illustration repeated construction CI \\(\\theta\\):Note CI actually estimate: \\(\\widehat{\\text{CI}}(x_1, \\ldots, x_n)\\), .e. depends data random (sampling) variation.good CI high coverage compact.Note: coverage probability probability true value contained given estimated interval (Bayesian credible interval).","code":""},{"path":"probability-and-statistics-refresher.html","id":"symmetric-normal-confidence-interval","chapter":"B Probability and statistics refresher","heading":"B.4.11 Symmetric normal confidence interval","text":"normally distributed univariate random variable\nstraightforward construct symmetric two-sided CI given desired coverage \\(\\kappa\\).normal random variable \\(X \\sim N(\\mu, \\sigma^2)\\) mean \\(\\mu\\) variance \\(\\sigma^2\\) density function \\(f(x)\\) can compute probability\\[\\text{Pr}(x \\leq \\mu + c \\sigma) =  \\int_{-\\infty}^{\\mu+c\\sigma} f(x) dx  = \\Phi (c) = \\frac{1+\\kappa}{2}\\]\nNote \\(\\Phi(c)\\) cumulative distribution function (CDF) standard normal \\(N(0,1)\\):obtain critical point \\(c\\) quantile function, .e. inversion \\(\\Phi\\):\\[c=\\Phi^{-1}\\left(\\frac{1+\\kappa}{2}\\right)\\]following table lists \\(c\\) three commonly used values \\(\\kappa\\) - useful memorise values!symmetric standard normal CI nominal coverage \\(\\kappa\\) fora scalar parameter \\(\\theta\\)normally distributed estimate \\(\\hat{\\theta}\\) andwith estimated standard deviation \\(\\hat{\\text{SD}}(\\hat{\\theta}) = \\hat{\\sigma}\\)given \n\\[\n\\widehat{\\text{CI}}=[\\hat{\\theta} \\pm c \\hat{\\sigma}]\n\\]\n\\(c\\) chosen desired coverage level \\(\\kappa\\).","code":""},{"path":"probability-and-statistics-refresher.html","id":"confidence-interval-based-on-the-chi-squared-distribution","chapter":"B Probability and statistics refresher","heading":"B.4.12 Confidence interval based on the chi-squared distribution","text":"normal CI can compute critical values \nchi-squared distribution use one-sided interval:\n\\[\n\\text{Pr}(x \\leq c) = \\kappa\n\\]\nget \\(c\\) quantile function, .e. inverting CDF chi-squared distribution.following list critical values three common choice \\(\\kappa\\)\n\\(m=1\\) (one degree freedom):one-sided CI nominal coverage \\(\\kappa\\) given \\([0, c ]\\).","code":""},{"path":"distribution-refresher.html","id":"distribution-refresher","chapter":"C Distribution refresher","heading":"C Distribution refresher","text":"overview distributions frequently employed statistical analysis please refer\nsupplementary\nDistribution Refresher notes.Specifically, module make use distributions listed .\nalready familiar \nearlier modules (Probability 1 2, Statistics 1).","code":""},{"path":"distribution-refresher.html","id":"univariate-distributions","chapter":"C Distribution refresher","heading":"C.1 Univariate distributions:","text":"Bernoulli distribution \\(\\text{Ber}(\\theta)\\)Bernoulli distribution \\(\\text{Ber}(\\theta)\\)Binomial distribution \\(\\text{Bin}(n, \\theta)\\)Binomial distribution \\(\\text{Bin}(n, \\theta)\\)Normal distribution \\(N(\\mu, \\sigma^2)\\)Normal distribution \\(N(\\mu, \\sigma^2)\\)Gamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\)\nnames parameterisations gamma distribution :\nunivariate Wishart distribution \\(W_1\\left(s^2, k \\right)\\)\nscaled chi-squared distribution \\(s^2 \\text{$\\chi^2_{k}$}\\)\nSpecial cases gamma distribution :\nchi-squared distribution \\(\\text{$\\chi^2_{k}$}\\)\nexponential distribution \\(\\text{Exp}(\\theta)\\)\nGamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\)\nnames parameterisations gamma distribution :univariate Wishart distribution \\(W_1\\left(s^2, k \\right)\\)scaled chi-squared distribution \\(s^2 \\text{$\\chi^2_{k}$}\\)Special cases gamma distribution :chi-squared distribution \\(\\text{$\\chi^2_{k}$}\\)exponential distribution \\(\\text{Exp}(\\theta)\\)Location-scale \\(t\\)-distribution \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\)\nSpecial case location-scale \\(t\\)-distribution :\nStudent’s \\(t\\)-distribution \\(t_\\nu\\)\nCauchy distribution \\(\\text{Cau}(\\mu, \\tau)\\)\nLocation-scale \\(t\\)-distribution \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\)\nSpecial case location-scale \\(t\\)-distribution :Student’s \\(t\\)-distribution \\(t_\\nu\\)Cauchy distribution \\(\\text{Cau}(\\mu, \\tau)\\)","code":""},{"path":"distribution-refresher.html","id":"multivariate-distribution","chapter":"C Distribution refresher","heading":"C.2 Multivariate distribution:","text":"Categorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\)Multinomial distribution \\(\\text{Mult}(n, \\boldsymbol \\pi)\\)Multivariate normal distribution \\(N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\)","code":""},{"path":"further-distributions-used-in-bayesian-analysis.html","id":"further-distributions-used-in-bayesian-analysis","chapter":"D Further distributions used in Bayesian analysis","heading":"D Further distributions used in Bayesian analysis","text":"appendix introduces number distributions\nessential Bayesian analysis.See particular Chapter “Bayesian learning practise”.","code":""},{"path":"further-distributions-used-in-bayesian-analysis.html","id":"beta-distribution","chapter":"D Further distributions used in Bayesian analysis","heading":"D.1 Beta distribution","text":"","code":""},{"path":"further-distributions-used-in-bayesian-analysis.html","id":"standard-parameterisation","chapter":"D Further distributions used in Bayesian analysis","heading":"D.1.1 Standard parameterisation","text":"density beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\) \n\\[\np(x | \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}\n\\]\n\\(x \\[0,1]\\) \\(\\alpha>0\\) \\(\\beta>0\\).\ndensity depends beta function \\(B(z_1, z_1) = \\frac{ \\Gamma(z_1) \\Gamma(z_2)}{\\Gamma(z_1 + z_2)}\\)\nturn defined via Euler’s gamma function \\(\\Gamma(x)\\).\nNote \\(\\Gamma(x) = (x-1)!\\) positive integer \\(x\\).mean beta distribution \n\\[\n\\text{E}(x) = \\frac{\\alpha}{\\alpha+\\beta}\n\\]\nvariance \n\\[\n\\text{Var}(x)=\\frac{\\alpha \\beta}{(\\alpha+\\beta)^2 (\\alpha+\\beta+1)}\n\\]beta distribution flexible can assume number different shapes, depending value \\(\\alpha\\) \\(\\beta\\):","code":""},{"path":"further-distributions-used-in-bayesian-analysis.html","id":"mean-parameterisation","chapter":"D Further distributions used in Bayesian analysis","heading":"D.1.2 Mean parameterisation","text":"useful reparameterisation \\(\\text{Beta}(\\mu, k)\\) beta distribution terms mean parameter\n\\(\\mu \\[0,1]\\) concentration parameter \\(k > 0\\). given \n\\[\nk=\\alpha+\\beta\n\\]\n\n\\[\\mu = \\frac{\\alpha}{\\alpha+\\beta}\n\\]\noriginal parameters can recovered \n\\[\\alpha= \\mu k\\] \\[\\beta=(1-\\mu) k\\]mean variance beta distribution expressed terms \\(\\mu\\) \\(k\\) \n\\[\n\\text{E}(x) = \\mu\n\\]\n\n\\[\n\\text{Var}(x)=\\frac{\\mu (1-\\mu)}{k+1}\n\\]\nincreasing concentration parameter \\(k\\) variance decreases thus probability mass becomes concentrated around mean.","code":""},{"path":"further-distributions-used-in-bayesian-analysis.html","id":"inverse-gamma-inverse-wishart-distribution","chapter":"D Further distributions used in Bayesian analysis","heading":"D.2 Inverse gamma (inverse Wishart) distribution","text":"","code":""},{"path":"further-distributions-used-in-bayesian-analysis.html","id":"standard-parameterisation-1","chapter":"D Further distributions used in Bayesian analysis","heading":"D.2.1 Standard parameterisation","text":"inverse gamma (IG) distribution \\(\\text{Inv-Gam}(\\alpha, \\beta)\\)\ndensity\n\\[\n\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} (1/x)^{\\alpha+1} e^{-\\beta/x}\n\\]\ntwo parameters \\(\\alpha >0\\) (shape parameter) \\(\\beta >0\\) (scale parameter) support \\(x >0\\).mean inverse gamma distribution \n\\[\\text{E}(x) = \\frac{\\beta}{\\alpha-1}\\]\nvariance\n\\[\\text{Var}(x) = \\frac{\\beta^2}{(\\alpha-1)^2 (\\alpha-2)}\\]Thus, mean exist restriction\n\\(\\alpha>1\\) variance exist \\(\\alpha>2\\).IG distribution closely linked gamma distribution.\n\\(x \\sim \\text{Inv-Gam}(\\alpha, \\beta)\\) IG-distributed inverse \\(x\\) gamma distributed:\n\\[\\frac{1}{x} \\sim \\text{Gam}(\\alpha, \\theta=\\beta^{-1})\\]\n\\(\\alpha\\) shared shape parameter \\(\\theta\\) scale parameter gamma distribution.","code":""},{"path":"further-distributions-used-in-bayesian-analysis.html","id":"wishart-parameterisation","chapter":"D Further distributions used in Bayesian analysis","heading":"D.2.2 Wishart parameterisation","text":"inverse gamma distribution frequently used different\nset parameters\n\\(\\psi = 2\\beta\\) (scale parameter) \\(\\nu = 2\\alpha\\) (shape parameter), \nconversely \\(\\alpha=\\nu/2\\) \\(\\beta=\\psi/2\\).\nform called one-dimensional inverse Wishart distribution\n\\(W^{-1}_1(\\psi, \\nu)\\) mean variance given \n\\[\n\\text{E}(x) = \\frac{\\psi}{\\nu-2} = \\mu\n\\]\n\\(\\nu>2\\) \n\\[\n\\text{Var}(x) =\\frac{2 \\psi^2}{(\\nu-4) (\\nu-2)^2}  = \\frac{2 \\mu^2}{\\nu-4}\n\\]\n\\(\\nu >4\\).Instead \\(\\psi\\) \\(\\nu\\) may also equivalently use \\(\\mu\\) \\(\\kappa=\\nu-2\\) parameters inverse\nWishart distribution, \\(W^{-1}_1(\\psi=\\kappa \\mu, \\nu=\\kappa+2)\\)\nmean\n\\[\\text{E}(x) = \\mu\\]\n\\(\\kappa>0\\) variance \n\\[\\text{Var}(x) = \\frac{2 \\mu^2}{\\kappa-2}\\] \n\\(\\kappa>2\\).\nmean parameterisation useful employing IG distribution\nprior posterior.Finally, \\(W^{-1}_1(\\psi=\\nu \\tau^2, \\nu)\\), \n\\(\\tau^2 = \\mu \\frac{ \\kappa}{\\kappa+2} = \\frac{\\psi}{\\nu}\\) \nbiased mean parameter, get\nscaled inverse chi-squared distribution \\(\\tau^2 \\text{Inv-$\\chi^2_{\\nu}$}\\)\n\n\\[\n\\text{E}(x) = \\tau^2 \\frac{ \\nu}{\\nu-2}\n\\]\n\\(\\nu>2\\) \n\\[\n\\text{Var}(x) =\\frac{2 \\tau^4}{\\nu-4} \\frac{\\nu^2}{(\\nu-2)^2}\n\\]\n\\(\\nu >4\\).inverse Wishart Wishart distributions linked.\n\\(x \\sim W^{-1}_1(\\psi, \\nu)\\) inverse-Wishart distributed inverse \\(x\\) Wishart distributed inverted scale parameter:\n\\[\\frac{1}{x} \\sim W_1(s^2=\\psi^{-1}, k=\\nu)\\]\n\\(k\\) shape parameter \\(s^2\\) scale parameter Wishart distribution.","code":""},{"path":"further-distributions-used-in-bayesian-analysis.html","id":"location-scale-t-distribution-as-compound-distribution","chapter":"D Further distributions used in Bayesian analysis","heading":"D.3 Location-scale \\(t\\)-distribution as compound distribution","text":"Suppose \n\\[\nx | s^2 \\sim N(\\mu,s^2)\n\\] corresponding density \\(p(x | s^2)\\)\nmean \\(\\text{E}(x | s^2) = \\mu\\) variance \\(\\text{Var}(x|s^2) = s^2\\).Now let variance \\(s^2\\) distributed inverse gamma / inverse Wishart\n\\[\ns^2 \\sim  W^{-1}(\\psi=\\kappa \\sigma^2, \\nu=\\kappa+2) = W^{-1}(\\psi=\\tau^2\\nu, \\nu)\n\\]\ncorresponding density \\(p(s^2)\\) mean \\(\\text{E}(s^2) = \\sigma^2 = \\tau^2 \\nu/(\\nu-2)\\).\nNote use mean parameterisation (\\(\\sigma^2, \\kappa\\))\ninverse chi-squared parameterisation (\\(\\tau^2, \\nu\\)).joint density \\(x\\) \\(s^2\\) \\(p(x, s^2) = p(x | s^2) p(s^2)\\).\ninterested marginal density \\(x\\):\n\\[\np(x) = \\int p(x, s^2) ds^2  = \\int p(s^2)  p(x | s^2) ds^2\n\\]\ncompound distribution normal fixed mean \\(\\mu\\)\nvariance \\(s^2\\) varying according inverse gamma distribution.\nCalculating integral results \nlocation-scale \\(t\\)-distribution parameters\n\\[\nx \\sim  \\text{lst}\\left(\\mu, \\sigma^2 \\frac{\\kappa}{\\kappa+2}, \\kappa+2\\right) = \\text{lst}\\left(\\mu, \\tau^2, \\nu\\right)\n\\]\nmean\n\\[\n\\text{E}(x) = \\mu\n\\]\n\nvariance\n\\[\n\\text{Var}(x) = \\sigma^2 =\\tau^2 \\frac{\\nu}{\\nu-2}\n\\]law total expectation variance can also directly verify \n\\[\n\\text{E}(x) = \\text{E}( \\text{E}(x| s^2) ) =\\mu\n\\]\n\n\\[\n\\text{Var}(x) = \\text{E}(\\text{Var}(x|s^2))+ \\text{Var}(\\text{E}(x|s^2)) = \\text{E}(s^2) = \\sigma^2 =\\tau^2 \\frac{\\nu}{\\nu-2}\n\\]","code":""},{"path":"further-study.html","id":"further-study","chapter":"E Further study","heading":"E Further study","text":"module can touch surface likelihood Bayes inference.\nstarting point reading following text books recommended.","code":""},{"path":"further-study.html","id":"recommended-reading","chapter":"E Further study","heading":"E.1 Recommended reading","text":"Held Bové (2020) Applied Statistical Inference: Likelihood Bayes (2nd edition). Springer.Held Bové (2020) Applied Statistical Inference: Likelihood Bayes (2nd edition). Springer.Agresti Kateri (2022) Foundations Statistics Data Scientists. Chapman Hall/CRC.Agresti Kateri (2022) Foundations Statistics Data Scientists. Chapman Hall/CRC.","code":""},{"path":"further-study.html","id":"additional-references","chapter":"E Further study","heading":"E.2 Additional references","text":"Heard (2021) Introduction Bayesian Inference, Methods Computation. Springer.Heard (2021) Introduction Bayesian Inference, Methods Computation. Springer.Gelman et al. (2014) Bayesian data analysis (3rd edition). CRC Press.Gelman et al. (2014) Bayesian data analysis (3rd edition). CRC Press.","code":""},{"path":"bibliography.html","id":"bibliography","chapter":"Bibliography","heading":"Bibliography","text":"","code":""}]
