<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>A Refresher | Statistics 2: Likelihood and Bayes</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.34 with bs4_book()">
<meta property="og:title" content="A Refresher | Statistics 2: Likelihood and Bayes">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Refresher | Statistics 2: Likelihood and Bayes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="Statistics is a mathematical science that requires practical use of tools from probability, vector and matrices, analysis etc. Here we briefly list some essentials that are needed for MATH27720...">
<meta property="og:description" content="Statistics is a mathematical science that requires practical use of tools from probability, vector and matrices, analysis etc. Here we briefly list some essentials that are needed for MATH27720...">
<meta name="twitter:description" content="Statistics is a mathematical science that requires practical use of tools from probability, vector and matrices, analysis etc. Here we briefly list some essentials that are needed for MATH27720...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistics 2: Likelihood and Bayes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="conditioning-and-bayes-rule.html"><span class="header-section-number">7</span> Conditioning and Bayes rule</a></li>
<li><a class="" href="models-with-latent-variables-and-missing-data.html"><span class="header-section-number">8</span> Models with latent variables and missing data</a></li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li><a class="" href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li><a class="" href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li class="book-part">Appendix</li>
<li><a class="active" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="distributions-used-in-bayesian-analysis.html"><span class="header-section-number">B</span> Distributions used in Bayesian analysis</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">C</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="refresher" class="section level1" number="14">
<h1>
<span class="header-section-number">A</span> Refresher<a class="anchor" aria-label="anchor" href="#refresher"><i class="fas fa-link"></i></a>
</h1>
<p>Statistics is a mathematical science that requires practical use of tools from probability,
vector and matrices, analysis etc.</p>
<p>Here we briefly list some essentials that are needed for MATH27720 Statistics 2.
Please familiarise yourself (again) with these topics.</p>
<div id="basic-mathematical-notation" class="section level2" number="14.1">
<h2>
<span class="header-section-number">A.1</span> Basic mathematical notation<a class="anchor" aria-label="anchor" href="#basic-mathematical-notation"><i class="fas fa-link"></i></a>
</h2>
<p>Summation:
<span class="math display">\[
\sum_{i=1}^n x_i = x_1 + x_2 + \ldots + x_n
\]</span></p>
<p>Multiplication:
<span class="math display">\[
\prod_{i=1}^n x_i = x_1 \times x_2 \times \ldots \times x_n
\]</span></p>
</div>
<div id="vectors-and-matrices" class="section level2" number="14.2">
<h2>
<span class="header-section-number">A.2</span> Vectors and matrices<a class="anchor" aria-label="anchor" href="#vectors-and-matrices"><i class="fas fa-link"></i></a>
</h2>
<p>Vector and matrix notation.</p>
<p>Vector algebra.</p>
<p>Eigenvectors and eigenvalues for a real symmetric matrix.</p>
<p>Eigenvalue (spectral) decomposition of a real symmetric matrix.</p>
<p>Positive and negative definiteness of a real symmetric matrix (containing only positive or only negative eigenvalues).</p>
<p>Singularity of a real symmetric matrix (containing one or more eigenvalues identical to zero).</p>
<p>Singular value decomposition of a real matrix.</p>
<p>Inverse of a matrix.</p>
<p>Trace and determinant of a square matrix.</p>
<p>Connection with eigenvalues (trace = sum of eigenvalues, determinant = product of eigenvalues).</p>
</div>
<div id="functions" class="section level2" number="14.3">
<h2>
<span class="header-section-number">A.3</span> Functions<a class="anchor" aria-label="anchor" href="#functions"><i class="fas fa-link"></i></a>
</h2>
<div id="gradient" class="section level3" number="14.3.1">
<h3>
<span class="header-section-number">A.3.1</span> Gradient<a class="anchor" aria-label="anchor" href="#gradient"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>gradient</strong> of a scalar-valued function
<span class="math inline">\(h(\boldsymbol x)\)</span> with vector argument <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_d)^T\)</span>
is the vector containing the first order partial derivatives
of <span class="math inline">\(h(\boldsymbol x)\)</span> with regard to each <span class="math inline">\(x_1, \ldots, x_d\)</span>:
<span class="math display">\[
\begin{split}
\nabla h(\boldsymbol x) &amp;= \begin{pmatrix}
\frac{\partial h(\boldsymbol x)}{\partial x_1} \\
\vdots\\
\frac{\partial h(\boldsymbol x)}{\partial x_d}
\end{pmatrix}\\
 &amp;=  \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} \\
&amp; = \text{grad } h(\boldsymbol x) \\
\end{split}
\]</span>
The symbol <span class="math inline">\(\nabla\)</span> is called the <strong>nabla operator</strong> (also known as <strong>del operator</strong>).</p>
<p>Note that we write the gradient as a <strong>column vector</strong>. This is called the
<strong>denominator layout</strong> convention, see <a href="https://en.wikipedia.org/wiki/Matrix_calculus" class="uri">https://en.wikipedia.org/wiki/Matrix_calculus</a> for details.
In contrast, many textbooks (and also earlier versions of these lecture notes) assume that gradients are row vectors, following the so-called numerator layout convention.</p>
<div class="example">
<p><span id="exm:gradientexamples" class="example"><strong>Example A.1  </strong></span>Examples for the gradient:</p>
<ul>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol a^T \boldsymbol x+ b\)</span>. Then <span class="math inline">\(\nabla h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol a\)</span>.</li>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol x^T \boldsymbol x\)</span>. Then <span class="math inline">\(\nabla h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = 2 \boldsymbol x\)</span>.</li>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol x^T \boldsymbol A\boldsymbol x\)</span>. Then <span class="math inline">\(\nabla h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = (\boldsymbol A+ \boldsymbol A^T) \boldsymbol x\)</span>.</li>
</ul>
</div>
</div>
<div id="hessian-matrix" class="section level3" number="14.3.2">
<h3>
<span class="header-section-number">A.3.2</span> Hessian matrix<a class="anchor" aria-label="anchor" href="#hessian-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>The matrix of all second order partial derivates of scalar-valued
function with vector-valued argument is called the <strong>Hessian matrix</strong>:
<span class="math display">\[
\begin{split}
\nabla \nabla^T h(\boldsymbol x) &amp;=
\begin{pmatrix}
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_1^2}
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_1 \partial x_2} 
     &amp; \cdots 
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_1 \partial x_d} \\
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_2 \partial x_1} 
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_2^2}
     &amp; \cdots 
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_2 \partial x_d} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_d \partial x_1} 
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_d \partial x_2}  
     &amp; \cdots 
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_d^2}
 \end{pmatrix} \\
&amp;= \left(\frac{\partial h(\boldsymbol x)}{\partial x_i \partial x_j}\right)  \\
&amp; = \frac{\partial^2 h(\boldsymbol x)}{\partial \boldsymbol x\partial \boldsymbol x^T} \\
\end{split}
\]</span>
By construction the Hessian matrix is square and symmetric.</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example A.2  </strong></span><span class="math inline">\(h(\boldsymbol x)=\boldsymbol x^T \boldsymbol A\boldsymbol x\)</span>. Then <span class="math inline">\(\nabla \nabla^T h(\boldsymbol x) = \frac{\partial^2 h(\boldsymbol x)}{\partial \boldsymbol x\partial \boldsymbol x^T} = (\boldsymbol A+ \boldsymbol A^T)\)</span>.</p>
</div>
</div>
<div id="convex-and-concave-functions" class="section level3" number="14.3.3">
<h3>
<span class="header-section-number">A.3.3</span> Convex and concave functions<a class="anchor" aria-label="anchor" href="#convex-and-concave-functions"><i class="fas fa-link"></i></a>
</h3>
<p>A function <span class="math inline">\(h(x)\)</span> is convex if the second derivative <span class="math inline">\(h''(x) \geq 0\)</span> for all <span class="math inline">\(x\)</span>.
More generally, a function <span class="math inline">\(h(\boldsymbol x)\)</span>, where <span class="math inline">\(\boldsymbol x\)</span> is a vector, is convex if the Hessian matrix <span class="math inline">\(\nabla \nabla^T h(\boldsymbol x)\)</span>
is positive definite, i.e. if the eigenvalues of the Hessian matrix are all positive.</p>
<p>If <span class="math inline">\(h(\boldsymbol x)\)</span> is convex, then <span class="math inline">\(-h(\boldsymbol x)\)</span> is <em>concave</em>. A function is concave if the Hessian matrix is negative definite, i.e. if the eigenvalues of the Hessian matrix are all negative.</p>
<div class="example">
<p><span id="exm:convexconcave" class="example"><strong>Example A.3  </strong></span>The logarithm <span class="math inline">\(\log(x)\)</span> is an example of a concave function whereas <span class="math inline">\(x^2\)</span> is a convex function.</p>
<p>To memorise, a <strong>v</strong>alley is con<strong>v</strong>ex.</p>
</div>
</div>
<div id="linear-and-quadratic-approximation" class="section level3" number="14.3.4">
<h3>
<span class="header-section-number">A.3.4</span> Linear and quadratic approximation<a class="anchor" aria-label="anchor" href="#linear-and-quadratic-approximation"><i class="fas fa-link"></i></a>
</h3>
<p>A linear and quadratic approximation of a function is given by a Taylor series of first and second order, respectively.</p>
<p>Applied to scalar-valued function of a scalar:
<span class="math display">\[
h(x) \approx h(x_0) + h'(x_0) (x-x_0) + \frac{1}{2} h''(x_0) (x-x_0)^2
\]</span>
Note that <span class="math inline">\(h'(x_0) = h'(x) \,|\, x_0\)</span> is first derivative of <span class="math inline">\(h(x)\)</span> evaluated at <span class="math inline">\(x_0\)</span> and
<span class="math inline">\(h''(x_0) = h''(x) \,|\, x_0\)</span> is the second derivative of <span class="math inline">\(h(x)\)</span> evaluated <span class="math inline">\(x_0\)</span>.</p>
<p>With <span class="math inline">\(x = x_0+ \varepsilon\)</span> the approximation can also be written as
<span class="math display">\[
h(x_0+ \varepsilon) \approx h(x_0) + h'(x_0) \, \varepsilon + \frac{1}{2} h''(x_0)\, \varepsilon^2
\]</span></p>
<p>Applied to scalar-valued function of a vector:
<span class="math display">\[
h(\boldsymbol x) \approx h(\boldsymbol x_0) + \nabla h(\boldsymbol x_0)^T (\boldsymbol x-\boldsymbol x_0) + \frac{1}{2} 
(\boldsymbol x-\boldsymbol x_0)^T \, \nabla \nabla^T h(\boldsymbol x_0) \, (\boldsymbol x-\boldsymbol x_0)
\]</span>
Note that <span class="math inline">\(\nabla h(\boldsymbol x_0)\)</span> is the gradient of <span class="math inline">\(h(\boldsymbol x)\)</span> evaluated at <span class="math inline">\(\boldsymbol x_0\)</span>
and <span class="math inline">\(\nabla \nabla^T h(\boldsymbol x_0)\)</span> the Hessian matrix of <span class="math inline">\(h(\boldsymbol x)\)</span> evaluated at <span class="math inline">\(\boldsymbol x_0\)</span>.</p>
<p>With <span class="math inline">\(\boldsymbol x= \boldsymbol x_0+ \boldsymbol \varepsilon\)</span> this approximation can also be written as
<span class="math display">\[
h(\boldsymbol x_0+ \boldsymbol \varepsilon) \approx h(\boldsymbol x_0) + \nabla h(\boldsymbol x_0)^T\boldsymbol \varepsilon+ \frac{1}{2} \boldsymbol \varepsilon^T \, \nabla \nabla^T h(\boldsymbol x_0) \,\boldsymbol \varepsilon
\]</span></p>
<div class="example">
<p><span id="exm:taylorexamples" class="example"><strong>Example A.4  </strong></span>Commonly occurring Taylor series approximations of second order are for example
<span class="math display">\[
\log(x_0+\varepsilon) \approx \log(x_0) + \frac{\varepsilon}{x_0} - \frac{\varepsilon^2}{2 x_0^2}
\]</span>
and
<span class="math display">\[
\frac{x_0}{x_0+\varepsilon} \approx 1 - \frac{\varepsilon}{x_0} + \frac{\varepsilon^2}{ x_0^2}
\]</span></p>
</div>
</div>
<div id="conditions-for-local-optimum-of-a-function" class="section level3" number="14.3.5">
<h3>
<span class="header-section-number">A.3.5</span> Conditions for local optimum of a function<a class="anchor" aria-label="anchor" href="#conditions-for-local-optimum-of-a-function"><i class="fas fa-link"></i></a>
</h3>
<p>To check if <span class="math inline">\(x_0\)</span> or <span class="math inline">\(\boldsymbol x_0\)</span> is a local maximum or minimum we can use the following conditions:</p>
<p>For a function of a single variable:</p>
<ol style="list-style-type: lower-roman">
<li>First derivative is zero at optimum <span class="math inline">\(h'(x_0) = 0\)</span>.</li>
<li>If the second derivative <span class="math inline">\(h''(x_0) &lt; 0\)</span> at the optimum is negative the function is locally concave and the optimum is a maximum.</li>
<li>If the second derivative <span class="math inline">\(h''(x_0) &gt; 0\)</span> is positive at the optimum the function is locally convex and the optimum is a minimum.</li>
</ol>
<p>For a function of several variables:</p>
<ol style="list-style-type: lower-roman">
<li>Gradient vanishes at maximum, <span class="math inline">\(\nabla h(\boldsymbol x_0)=0\)</span>.</li>
<li>If the Hessian <span class="math inline">\(\nabla \nabla^T h(\boldsymbol x_0)\)</span> is negative definite (= all eigenvalues of Hessian matrix are negative) then the function is locally concave and the optimum is a maximum.</li>
<li>If the Hessian is positive definite (= all eigenvalues of Hessian matrix are positive) then the function is locally convex and the optimum is a minimum.</li>
</ol>
<p>Around the local optimum <span class="math inline">\(\boldsymbol x_0\)</span> we can approximate the function quadratically using
<span class="math display">\[
h(\boldsymbol x_0+ \boldsymbol \varepsilon) \approx h(\boldsymbol x_0) +  \frac{1}{2} \boldsymbol \varepsilon^T \nabla \nabla^T h(\boldsymbol x_0) \boldsymbol \varepsilon
\]</span>
Note the linear term is missing due to the gradient being zero at <span class="math inline">\(\boldsymbol x_0\)</span>.</p>
</div>
</div>
<div id="combinatorics" class="section level2" number="14.4">
<h2>
<span class="header-section-number">A.4</span> Combinatorics<a class="anchor" aria-label="anchor" href="#combinatorics"><i class="fas fa-link"></i></a>
</h2>
<div id="number-of-permutations" class="section level3" number="14.4.1">
<h3>
<span class="header-section-number">A.4.1</span> Number of permutations<a class="anchor" aria-label="anchor" href="#number-of-permutations"><i class="fas fa-link"></i></a>
</h3>
<p>The number of possible orderings, or permutations, of <span class="math inline">\(n\)</span> distinct items is
the number of ways to put <span class="math inline">\(n\)</span> items in <span class="math inline">\(n\)</span> bins with exactly one item in each bin. It is given by
the factorial
<span class="math display">\[
n! = \prod_{i=1}^n i = 1 \times 2 \times \ldots \times n
\]</span>
where <span class="math inline">\(n\)</span> is a positive integer.
For <span class="math inline">\(n=0\)</span> the factorial is defined as
<span class="math display">\[
0! = 1 
\]</span>
as there is exactly one permutation of zero objects.</p>
<p>The factorial can also be obtained using the
<a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>
<span class="math display">\[
\Gamma(x) = \int_0^\infty t^{x-1} e^{-t} dt
\]</span>
which can be viewed as continuous version of the factorial
with
<span class="math inline">\(\Gamma(x) = (x-1)!\)</span> for any positive integer <span class="math inline">\(x\)</span>.</p>
</div>
<div id="multinomial-and-binomial-coefficient" class="section level3" number="14.4.2">
<h3>
<span class="header-section-number">A.4.2</span> Multinomial and binomial coefficient<a class="anchor" aria-label="anchor" href="#multinomial-and-binomial-coefficient"><i class="fas fa-link"></i></a>
</h3>
<p>The number of possible permutation of <span class="math inline">\(n\)</span> items of <span class="math inline">\(K\)</span> distinct types, with <span class="math inline">\(n_1\)</span> of type 1, <span class="math inline">\(n_2\)</span> of type 2 and so on, equals the number of ways
to put <span class="math inline">\(n\)</span> items into <span class="math inline">\(K\)</span> bins with <span class="math inline">\(n_1\)</span> items in the first bin, <span class="math inline">\(n_2\)</span> in the second and so on.
It is given by the <strong>multinomial</strong> coefficient
<span class="math display">\[
\binom{n}{n_1, \ldots, n_K} = \frac {n!}{n_1! \times n_2! \times\ldots \times n_K! } 
\]</span>
with <span class="math inline">\(\sum_{k=1}^K n_k = n\)</span> and <span class="math inline">\(K \leq n\)</span>.
Note that it equals the number of permutation of all items divided by the number of permutations of the items in each bin (or of each type).</p>
<p>If all <span class="math inline">\(n_k=1\)</span> and hence <span class="math inline">\(K=n\)</span> the multinomial coefficient reduces to the factorial.</p>
<p>If there are only two bins / types (<span class="math inline">\(K=2\)</span>) the multinomial coefficients becomes the
<strong>binomial coefficient</strong>
<span class="math display">\[
 \binom{n}{n_1} = \binom{n}{n_1, n-n_1}    =  \frac {n!}{n_1! (n - n_1)!} 
\]</span>
which counts the number of ways to choose <span class="math inline">\(n_1\)</span> elements from a set of <span class="math inline">\(n\)</span> elements.</p>
</div>
<div id="de-moivre-sterling-approximation-of-the-factorial" class="section level3" number="14.4.3">
<h3>
<span class="header-section-number">A.4.3</span> De Moivre-Sterling approximation of the factorial<a class="anchor" aria-label="anchor" href="#de-moivre-sterling-approximation-of-the-factorial"><i class="fas fa-link"></i></a>
</h3>
<p>The factorial is frequently approximated by the following formula derived by <a href="https://en.wikipedia.org/wiki/Abraham_de_Moivre">Abraham de Moivre (1667–1754)</a> and <a href="https://en.wikipedia.org/wiki/James_Stirling_(mathematician)">James Stirling (1692-1770)</a>
<span class="math display">\[
n! \approx \sqrt{2 \pi} n^{n+\frac{1}{2}} e^{-n}
\]</span>
or equivalently on logarithmic scale
<span class="math display">\[
\log n!  \approx \left(n+\frac{1}{2}\right) \log n  -n + \frac{1}{2}\log \left( 2 \pi\right)
\]</span>
The approximation is good for small <span class="math inline">\(n\)</span> (but fails for <span class="math inline">\(n=0\)</span>) and becomes
more and more accurate with increasing <span class="math inline">\(n\)</span>. For large <span class="math inline">\(n\)</span> the approximation can be simplified to
<span class="math display">\[
\log n! \approx  n \log n  -n 
\]</span></p>
</div>
</div>
<div id="probability" class="section level2" number="14.5">
<h2>
<span class="header-section-number">A.5</span> Probability<a class="anchor" aria-label="anchor" href="#probability"><i class="fas fa-link"></i></a>
</h2>
<div id="random-variables" class="section level3" number="14.5.1">
<h3>
<span class="header-section-number">A.5.1</span> Random variables<a class="anchor" aria-label="anchor" href="#random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>A <strong>random variable</strong> describes a random experiment. The set of possible outcomes
is the <strong>sample space</strong> or <strong>state space</strong> and is denoted by
<span class="math inline">\(\Omega = \{\omega_1, \omega_2, \ldots\}\)</span>. The outcomes <span class="math inline">\(\omega_i\)</span> are the <strong>elementary events</strong>.
The sample space <span class="math inline">\(\Omega\)</span> can be finite or infinite. Depending on type of outcomes
the random variable is <strong>discrete</strong> or <strong>continuous</strong>.</p>
<p>An event <span class="math inline">\(A \subseteq \Omega\)</span> is subset of <span class="math inline">\(\Omega\)</span> and thus itself a set of elementary events <span class="math inline">\(A = \{a_1, a_2, \ldots\}\)</span>.
This includes as special cases the full set <span class="math inline">\(A = \Omega\)</span>, the empty set <span class="math inline">\(A = \emptyset\)</span>, and the elementary
events <span class="math inline">\(A=\omega_i\)</span>. The complementary event <span class="math inline">\(A^C\)</span> is the complement of the set <span class="math inline">\(A\)</span> in the set <span class="math inline">\(\Omega\)</span>
so that <span class="math inline">\(A^C = \Omega \setminus A = \{\omega_i \in \Omega: \omega_i \notin A\}\)</span>.</p>
<p>The probability of an event is denoted by <span class="math inline">\(\text{Pr}(A)\)</span>.
We assume that</p>
<ul>
<li>
<span class="math inline">\(\text{Pr}(A) \geq 0\)</span>, probabilities are positive,</li>
<li>
<span class="math inline">\(\text{Pr}(\Omega) = 1\)</span>,
the certain event has probability 1, and</li>
<li>
<span class="math inline">\(\text{Pr}(A) = \sum_{a_i \in A} \text{Pr}(a_i)\)</span>, the probability of
an event equals the sum of its constituting elementary events <span class="math inline">\(a_i\)</span>.</li>
</ul>
<p>This implies</p>
<ul>
<li>
<span class="math inline">\(\text{Pr}(A) \leq 1\)</span>, i.e. probabilities all lie in the interval <span class="math inline">\([0,1]\)</span>
</li>
<li>
<span class="math inline">\(\text{Pr}(A^C) = 1 - \text{Pr}(A)\)</span>, and</li>
<li><span class="math inline">\(\text{Pr}(\emptyset) = 0\)</span></li>
</ul>
<p>Assume now we have two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.
The probability of the event “<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>” is then given by the probability of the set intersection
<span class="math inline">\(\text{Pr}(A \cap B)\)</span>.
Likewise the probability of the event “<span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>” is given by the probability of the set union
<span class="math inline">\(\text{Pr}(A \cup B)\)</span>.</p>
<p>From the above it is clear that probability theory is closely linked to set theory,
and in particular to measure theory. This allows for an unified treatment of discrete
and continuous random variables (an elegant framework but not needed for this module).</p>
</div>
<div id="probability-mass-and-density-function-and-distribution-and-quantile-function" class="section level3" number="14.5.2">
<h3>
<span class="header-section-number">A.5.2</span> Probability mass and density function and distribution and quantile function<a class="anchor" aria-label="anchor" href="#probability-mass-and-density-function-and-distribution-and-quantile-function"><i class="fas fa-link"></i></a>
</h3>
<p>To describe a random variable <span class="math inline">\(x\)</span> we need to assign probabilities to the corresponding elementary outcomes
<span class="math inline">\(x \in \Omega\)</span>. For convenience we use the same name to denote the random variable and the elementary outcomes.</p>
<p>For a discrete random variable we employ a probability mass function (PMF).
We denote the it by a lower case <span class="math inline">\(f\)</span> but occasionally we also use
<span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span>. In the discrete case we can define the
event <span class="math inline">\(A = \{x: x=a\} = \{a\}\)</span> and obtain the probability directly from the PMF:
<span class="math display">\[\text{Pr}(A) = \text{Pr}(x=a) =f(a) \,.\]</span>
The PMF has the property that <span class="math inline">\(\sum_{x \in \Omega} f(x) = 1\)</span> and that
<span class="math inline">\(f(x) \in [0,1]\)</span>.</p>
<p>For continuous random variables we need to use a probability density function (PDF)
instead. We define the event
<span class="math inline">\(A = \{x: a &lt; x \leq a + da\}\)</span> as an infinitesimal interval
and then assign the probability
<span class="math display">\[
\text{Pr}(A) = \text{Pr}( a &lt; x \leq a + da) = f(a) da \,.
\]</span>
The PDF has the property that <span class="math inline">\(\int_{x \in \Omega} f(x) dx = 1\)</span>
but in contrast to a PMF the density <span class="math inline">\(f(x)\geq 0\)</span> may take on values larger
than 1.</p>
<p>Assuming an ordering
we can define the event <span class="math inline">\(A = \{x: x \leq a \}\)</span> and compute its
probability
<span class="math display">\[
F(a) = \text{Pr}(A) = \text{Pr}( x \leq a ) =
\begin{cases}
 \sum_{x \in A} f(x) &amp; \text{discrete case} \\
\int_{x \in A} f(x) dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
This is known as the <strong>distribution function</strong>, or <strong>cumulative distribution function</strong> (CDF)
and is denoted by upper case <span class="math inline">\(F\)</span> if the corresponding PDF/PMF is <span class="math inline">\(f\)</span>
(or <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> if the corresponding PDF/PMF are <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>).
By construction the distribution function is monotonically increasing and its value ranges from 0 to 1.
With its help we can compute the probability of general interval sets such as
<span class="math display">\[
\text{Pr}( a &lt; x \leq b ) = F(b)-F(a) \,.
\]</span></p>
<p>The inverse of the distribution function <span class="math inline">\(y=F(x)\)</span> is the <strong>quantile function</strong> <span class="math inline">\(x=F^{-1}(y)\)</span>.
The 50% quantile <span class="math inline">\(F^{-1}\left(\frac{1}{2}\right)\)</span> is the <strong>median</strong>.</p>
<p>If the random variable <span class="math inline">\(x\)</span> has distribution function <span class="math inline">\(F\)</span> we write <span class="math inline">\(x \sim F\)</span>.</p>
<div class="inline-figure"><img src="fig/refresher_dens-dist.png" width="100%" style="display: block; margin: auto;"></div>
</div>
<div id="expection-and-variance-of-a-random-variable" class="section level3" number="14.5.3">
<h3>
<span class="header-section-number">A.5.3</span> Expection and variance of a random variable<a class="anchor" aria-label="anchor" href="#expection-and-variance-of-a-random-variable"><i class="fas fa-link"></i></a>
</h3>
<p>The expected value <span class="math inline">\(\text{E}(x)\)</span> of a random variable is defined as
the weighted average over all possible outcomes, with the weight given by the PMF / PDF <span class="math inline">\(f(x)\)</span>:
<span class="math display">\[
\text{E}(x) = 
\begin{cases}
 \sum_{x \in \Omega} f(x) x &amp; \text{discrete case} \\
\int_{x \in \Omega} f(x) x dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
To emphasise that the expecation is taken with regard to the distribution
<span class="math inline">\(F\)</span> we write <span class="math inline">\(\text{E}_F(x)\)</span> with the distribution <span class="math inline">\(F\)</span> as subscript. The expectation is not necessarily always defined for a continuous
random variable as the integral may diverge.</p>
<p>The expected value of a function of a random variable <span class="math inline">\(h(x)\)</span> is
obtained similarly:
<span class="math display">\[
\text{E}(h(x)) = 
\begin{cases}
 \sum_{x \in \Omega} f(x) h(x) &amp; \text{discrete case} \\
\int_{x \in \Omega} f(x) h(x) dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
This is called the <a href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician">“law of the unconscious statistician”</a>, or short LOTUS.
Again, to highlight that the random variable <span class="math inline">\(x\)</span> has distribution <span class="math inline">\(F\)</span> we
write <span class="math inline">\(\text{E}_F(h(x))\)</span>.</p>
<p>For an event <span class="math inline">\(A\)</span> we can define a corresponding <strong>indicator function</strong>
<span class="math display">\[
1_A(x) =
\begin{cases}
1 &amp; x \in A\\
0 &amp; x \notin A\\
\end{cases}
\]</span>
Intriguingly,
<span class="math display">\[
\text{E}(1_A(x) ) = \text{Pr}(A)
\]</span>
i.e. the expectation of the indicator variable for <span class="math inline">\(A\)</span> is the probability
of <span class="math inline">\(A\)</span>.</p>
<p>The moments of random variables are also defined by expectation:</p>
<ul>
<li>Zeroth moment: <span class="math inline">\(\text{E}(x^0) = 1\)</span> by definition of PDF and PMF,</li>
<li>First moment: <span class="math inline">\(\text{E}(x^1) = \text{E}(x) = \mu\)</span> , the mean,</li>
<li>Second moment: <span class="math inline">\(\text{E}(x^2)\)</span>
</li>
<li>The variance is the second momented centered about the mean <span class="math inline">\(\mu\)</span>:
<span class="math display">\[\text{Var}(x) = \text{E}( (x - \mu)^2 ) = \sigma^2\]</span>
</li>
<li>The variance can also be computed by <span class="math inline">\(\text{Var}(x) = \text{E}(x^2)-\text{E}(x)^2\)</span>.</li>
</ul>
<p>A distribution does not necessarily need to have any finite first or higher moments.
An example is the <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a> that does not have a mean or variance (or any other higher moment).</p>
</div>
<div id="transformation-of-random-variables" class="section level3" number="14.5.4">
<h3>
<span class="header-section-number">A.5.4</span> Transformation of random variables<a class="anchor" aria-label="anchor" href="#transformation-of-random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>Linear transformation of random variables: if <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants and <span class="math inline">\(x\)</span> is a random variable, then the random variable <span class="math inline">\(y= a + b x\)</span> has mean <span class="math inline">\(\text{E}(y) = a + b \text{E}(x)\)</span> and variance <span class="math inline">\(\text{Var}(y) = b^2 \text{Var}(x)\)</span>.</p>
<p>For a general invertible coordinate transformation <span class="math inline">\(y = h(x) = y(x)\)</span> the backtransformation is <span class="math inline">\(x = h^{-1}(y) = x(y)\)</span>.</p>
<p>The transformation of the infinitesimal volume element is <span class="math inline">\(dy = |\frac{dy}{dx}| dx\)</span>.</p>
<p>The transformation of the density is <span class="math inline">\(f_y(y) =\left|\frac{dx}{dy}\right| f_x(x(y))\)</span>.</p>
<p>Note that <span class="math inline">\(\left|\frac{dx}{dy}\right| = \left|\frac{dy}{dx}\right|^{-1}\)</span>.</p>
</div>
<div id="law-of-large-numbers" class="section level3" number="14.5.5">
<h3>
<span class="header-section-number">A.5.5</span> Law of large numbers:<a class="anchor" aria-label="anchor" href="#law-of-large-numbers"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we observe data <span class="math inline">\(D=\{x_1, \ldots, x_n\}\)</span> with each <span class="math inline">\(x_i \sim F\)</span>.</p>
<ul>
<li><p>By the strong law of large numbers the empirical distribution
<span class="math inline">\(\hat{F}_n\)</span> based on data <span class="math inline">\(D=\{x_1, \ldots, x_n\}\)</span> converges to the true underlying distribution <span class="math inline">\(F\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> almost surely:
<span class="math display">\[
\hat{F}_n\overset{a. s.}{\to} F
\]</span>
The <a href="https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem">Glivenko–Cantelli theorem</a> asserts that the convergence is uniform. Since the strong law implies the weak law we also have convergence in probability:
<span class="math display">\[
\hat{F}_n\overset{P}{\to} F
\]</span></p></li>
<li><p>Correspondingly, for <span class="math inline">\(n \rightarrow \infty\)</span> the average <span class="math inline">\(\text{E}_{\hat{F}_n}(h(x)) = \frac{1}{n} \sum_{i=1}^n h(x_i)\)</span> converges to the expectation <span class="math inline">\(\text{E}_{F}(h(x))\)</span>.</p></li>
</ul>
</div>
<div id="jensens-inequality" class="section level3" number="14.5.6">
<h3>
<span class="header-section-number">A.5.6</span> Jensen’s inequality<a class="anchor" aria-label="anchor" href="#jensens-inequality"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[\text{E}(h(\boldsymbol x)) \geq h(\text{E}(\boldsymbol x))\]</span>
for a <em>convex</em> function <span class="math inline">\(h(\boldsymbol x)\)</span>.</p>
<p>Recall: a con<strong>v</strong>ex function (such as <span class="math inline">\(x^2\)</span>) has the shape of a “<strong>v</strong>alley”.</p>
</div>
</div>
<div id="distributions" class="section level2" number="14.6">
<h2>
<span class="header-section-number">A.6</span> Distributions<a class="anchor" aria-label="anchor" href="#distributions"><i class="fas fa-link"></i></a>
</h2>
<div id="bernoulli-distribution-and-binomial-distribution" class="section level3" number="14.6.1">
<h3>
<span class="header-section-number">A.6.1</span> Bernoulli distribution and binomial distribution<a class="anchor" aria-label="anchor" href="#bernoulli-distribution-and-binomial-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The Bernoulli distribution <span class="math inline">\(\text{Ber}(\theta)\)</span> is simplest distribution possible.
It is named after <a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Jacob Bernoulli (1655-1705)</a>
who also invented the law of large numbers.</p>
<p>It describes a discrete binary random variable
with two states <span class="math inline">\(x=0\)</span> (“failure”) and <span class="math inline">\(x=1\)</span> (“success”),
where the parameter <span class="math inline">\(\theta \in [0,1]\)</span> is the probability of “success”.
Often the Bernoulli distribution is also referred to as “coin tossing” model with
the two outcomes “heads” and “tails”.</p>
<p>Correspondingly, the probability mass function of <span class="math inline">\(\text{Ber}(\theta)\)</span> is
<span class="math display">\[
p(x=0) = \text{Pr}(\text{"failure"}) = 1-\theta  
\]</span>
and
<span class="math display">\[
p(x=1) = \text{Pr}(\text{"success"}) = \theta 
\]</span>
A compact way to write the PMF of the Bernoulli distribution is
<span class="math display">\[
p(x | \theta ) = \theta^{x} (1-\theta)^{1-x}
\]</span></p>
<p>If a random variable <span class="math inline">\(x\)</span> follows the Bernoulli distribution we
write
<span class="math display">\[
x \sim \text{Ber}(\theta) \,.
\]</span>
The expected value is <span class="math inline">\(\text{E}(x) = \theta\)</span> and the variance is <span class="math inline">\(\text{Var}(x) = \theta (1 - \theta)\)</span>.</p>
<p>Closely related to the Bernoulli distribution is the binomial distribution
<span class="math inline">\(\text{Bin}(n, \theta)\)</span> which results from repeating a
Bernoulli experiment <span class="math inline">\(n\)</span> times and counting the number of successes among
the <span class="math inline">\(n\)</span> trials (without keeping track of the ordering of the experiments).
Thus, if <span class="math inline">\(x_1, \ldots, x_n\)</span> are <span class="math inline">\(n\)</span> independent <span class="math inline">\(\text{Ber}(\theta)\)</span> random variables
then <span class="math inline">\(y = \sum_{i=1}^n\)</span> is distributed as <span class="math inline">\(\text{Bin}(n, \theta)\)</span>.</p>
<p>Its probability mass function is:
<span class="math display">\[
p(y | n, \theta) = \binom{n}{y} \theta^y (1 - \theta)^{n - y}
\]</span>
for <span class="math inline">\(y \in \{ 0, 1, 2, \ldots, n\}\)</span>.
The binomial coefficient <span class="math inline">\(\binom{n}{x}\)</span> is needed to account for the multiplicity
of ways (orderings of samples) in which we can observe <span class="math inline">\(y\)</span> sucesses.</p>
<p>The expected value is <span class="math inline">\(\text{E}(y) = n \theta\)</span> and the variance is <span class="math inline">\(\text{Var}(y) = n \theta (1 - \theta)\)</span>.</p>
<p>If a random variable <span class="math inline">\(y\)</span> follows the binomial distribution we
write
<span class="math display">\[
y \sim \text{Bin}(n, \theta)\,
\]</span>
For <span class="math inline">\(n=1\)</span> it reduces to the Bernoulli distribution <span class="math inline">\(\text{Ber}(\theta)\)</span>.</p>
<p>In R the PMF of the binomial distribution is called <code><a href="https://rdrr.io/r/stats/Binomial.html">dbinom()</a></code>. The binomial coefficient itself is computed by <code><a href="https://rdrr.io/r/base/Special.html">choose()</a></code>.</p>
</div>
<div id="normal-distribution" class="section level3" number="14.6.2">
<h3>
<span class="header-section-number">A.6.2</span> Normal distribution<a class="anchor" aria-label="anchor" href="#normal-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The normal distribution is the most important continuous probability distribution.
It is also called Gaussian distribution named after <a href="https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss">Carl Friedrich Gauss (1777–1855)</a>.</p>
<p>The univariate normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span> has two parameters <span class="math inline">\(\mu\)</span> (location) and <span class="math inline">\(\sigma^2\)</span> (scale):</p>
<p><span class="math display">\[
x \sim N(\mu,\sigma^2)
\]</span>
with mean
<span class="math display">\[
\text{E}(x)=\mu
\]</span>
and variance
<span class="math display">\[
\text{Var}(x) = \sigma^2
\]</span></p>
<p>Probability density function (PDF):
<span class="math display">\[
p(x| \mu, \sigma^2)=(2\pi\sigma^2)^{-\frac{1}{2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\]</span></p>
<p>In R the density function is called <code><a href="https://rdrr.io/r/stats/Normal.html">dnorm()</a></code>.</p>
<p>The standard normal distribution is <span class="math inline">\(N(0, 1)\)</span> with mean 0 and variance 1.</p>
<p>Plot of the PDF of the standard normal:</p>
<div class="inline-figure"><img src="19-refresher_files/figure-html/unnamed-chunk-2-1.png" width="384"></div>
<p>The cumulative distribution function (CDF) of the standard normal <span class="math inline">\(N(0,1)\)</span>
is
<span class="math display">\[
\Phi (x ) = \int_{-\infty}^{x} f(x'| \mu=0, \sigma^2=1) dx' 
\]</span>
There is no analytic expression for <span class="math inline">\(\Phi(x)\)</span>. In R the function is called <code><a href="https://rdrr.io/r/stats/Normal.html">pnorm()</a></code>.</p>
<p>Plot of the CDF of the standard normal:</p>
<div class="inline-figure"><img src="19-refresher_files/figure-html/unnamed-chunk-3-1.png" width="384"></div>
<p>The inverse <span class="math inline">\(\Phi^{-1}(p)\)</span> is called the quantile function of the standard normal.
In R the function is called <code><a href="https://rdrr.io/r/stats/Normal.html">qnorm()</a></code>.</p>
<div class="inline-figure"><img src="19-refresher_files/figure-html/unnamed-chunk-4-1.png" width="384"></div>
<p>The sum of two normal random variables is also normal (with the appropriate mean and variance).</p>
<p>The <strong>central limit theorem</strong> (first postulated by <a href="https://en.wikipedia.org/wiki/Abraham_de_Moivre">Abraham de Moivre (1667–1754)</a>) asserts
that in many cases the distribution of the mean of identically distributed independent random variables converges to a normal distribution, even if the individual random variables are not normal.</p>
</div>
<div id="gamma-distribution" class="section level3" number="14.6.3">
<h3>
<span class="header-section-number">A.6.3</span> Gamma distribution<a class="anchor" aria-label="anchor" href="#gamma-distribution"><i class="fas fa-link"></i></a>
</h3>
<div id="standard-parameterisation" class="section level4" number="14.6.3.1">
<h4>
<span class="header-section-number">A.6.3.1</span> Standard parameterisation<a class="anchor" aria-label="anchor" href="#standard-parameterisation"><i class="fas fa-link"></i></a>
</h4>
<p>Another important continous distribution is the gamma distribution
<span class="math inline">\(\text{Gam}(\alpha, \theta)\)</span>.
It has two parameters <span class="math inline">\(\alpha&gt;0\)</span> (shape) and <span class="math inline">\(\theta&gt;0\)</span> (scale):
<span class="math display">\[
x \sim\text{Gam}(\alpha, \theta)
\]</span>
with mean
<span class="math display">\[\text{E}(x)=\alpha \theta\]</span>
and variance
<span class="math display">\[\text{Var}(x) = \alpha \theta^2\]</span></p>
<p>The gamma distribution is also often used with a rate
parameter <span class="math inline">\(\beta=1/\theta\)</span> (so one needs to pay attention which parameterisation is used).</p>
<p>Probability density function (PDF):
<span class="math display">\[
p(x| \alpha, \theta)=\frac{1}{\Gamma(\alpha) \theta^{\alpha} } x^{\alpha-1} e^{-x/\theta}
\]</span>
The density of the gamma distribution is available in the R function <code><a href="https://rdrr.io/r/stats/GammaDist.html">dgamma()</a></code>. The cumulative density function is <code><a href="https://rdrr.io/r/stats/GammaDist.html">pgamma()</a></code> and the quantile function is <code><a href="https://rdrr.io/r/stats/GammaDist.html">qgamma()</a></code>.</p>
</div>
<div id="wishart-parameterisation-and-scaled-chi-squared-distribution" class="section level4" number="14.6.3.2">
<h4>
<span class="header-section-number">A.6.3.2</span> Wishart parameterisation and scaled chi-squared distribution<a class="anchor" aria-label="anchor" href="#wishart-parameterisation-and-scaled-chi-squared-distribution"><i class="fas fa-link"></i></a>
</h4>
<p>The gamma distribution is often used with a different set of parameters
<span class="math inline">\(k=2 \alpha\)</span> and <span class="math inline">\(s^2 =\theta/2\)</span> (hence conversely <span class="math inline">\(\alpha = k/2\)</span> and <span class="math inline">\(\theta=2 s^2\)</span>).
In this form it is known as <strong>one-dimensional Wishart distribution</strong>
<span class="math display">\[
W_1\left(s^2, k \right)
\]</span>
named after <a href="https://en.wikipedia.org/wiki/John_Wishart_(statistician)">John Wishart (1898–1954)</a>.
In the Wishart parameterisation the mean is
<span class="math display">\[
\text{E}(x) = k s^2
\]</span>
and the variance
<span class="math display">\[
\text{Var}(x) = 2 k s^4
\]</span></p>
<p>Another name for the one-dimensional Wishart distribution with exactly the same
parameterisation is <strong>scaled chi-squared distribution</strong> denoted as
<span class="math display">\[
s^2 \text{$\chi^2_{k}$} 
\]</span></p>
<p>Finally, note we often employ the Wishart distribution in <strong>mean parameterisation</strong>
<span class="math inline">\(W_1\left(s^2= \mu / k, k \right)\)</span>
with <span class="math inline">\(\mu = k s^2\)</span> and <span class="math inline">\(k\)</span> (and thus <span class="math inline">\(\theta = 2 \mu /k\)</span>). It has
mean
<span class="math display">\[
\text{E}(x) = \mu
\]</span>
and variance
<span class="math display">\[
\text{Var}(x) = \frac{2 \mu^2}{k}
\]</span></p>
</div>
<div id="construction-as-sum-of-squared-normals" class="section level4" number="14.6.3.3">
<h4>
<span class="header-section-number">A.6.3.3</span> Construction as sum of squared normals<a class="anchor" aria-label="anchor" href="#construction-as-sum-of-squared-normals"><i class="fas fa-link"></i></a>
</h4>
<p>A gamma distributed variable can be constructed as follows.
Assume <span class="math inline">\(k\)</span> independent normal random variables with mean 0
and variance <span class="math inline">\(s^2\)</span>:
<span class="math display">\[z_1,z_2,\dots,z_k\sim N(0,s^2)\]</span>
Then the sum of the squares
<span class="math display">\[
x = \sum_{i=1}^{k} z_i^2
\]</span>
follows
<span class="math display">\[
x \sim \sigma^2 \text{$\chi^2_{k}$} =  W_1\left( s^2, k \right)
\]</span>
or equivalently
<span class="math display">\[
x \sim \text{Gam}\left(\alpha=\frac{k}{2}, \theta = 2 s^2\right)
\]</span></p>
</div>
</div>
<div id="special-cases-of-the-gamma-distribution" class="section level3" number="14.6.4">
<h3>
<span class="header-section-number">A.6.4</span> Special cases of the gamma distribution<a class="anchor" aria-label="anchor" href="#special-cases-of-the-gamma-distribution"><i class="fas fa-link"></i></a>
</h3>
<div id="chi-squared-distribution" class="section level4" number="14.6.4.1">
<h4>
<span class="header-section-number">A.6.4.1</span> Chi-squared distribution<a class="anchor" aria-label="anchor" href="#chi-squared-distribution"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>chi-squared distribution</strong>
<span class="math inline">\(\text{$\chi^2_{k}$}\)</span> is a special one-parameter restriction of the
gamma resp. Wishart distribution obtained when setting
<span class="math inline">\(s^2=1\)</span> or, equivalently, <span class="math inline">\(\theta = 2\)</span> or <span class="math inline">\(\mu = k\)</span>.</p>
<p>It has mean <span class="math inline">\(\text{E}(x)=k\)</span> and variance <span class="math inline">\(\text{Var}(x)=2k\)</span>. The chi-squared distribution <span class="math inline">\(\text{$\chi^2_{k}$}\)</span> equals <span class="math inline">\(\text{Gam}(\alpha=k/2, \theta=2) = W_1\left(1, k \right)\)</span>.</p>
<p>Here is a plot of the density of the chi-squared distribution
for degrees of freedom <span class="math inline">\(k=1\)</span> and <span class="math inline">\(k=3\)</span>:</p>
<div class="inline-figure"><img src="19-refresher_files/figure-html/unnamed-chunk-5-1.png" width="768"></div>
<p>In R the density of the chi-squared distribution is given by <code><a href="https://rdrr.io/r/stats/Chisquare.html">dchisq()</a></code>. The cumulative density function is <code><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq()</a></code> and
the quantile function is <code><a href="https://rdrr.io/r/stats/Chisquare.html">qchisq()</a></code>.</p>
</div>
<div id="exponential-distribution" class="section level4" number="14.6.4.2">
<h4>
<span class="header-section-number">A.6.4.2</span> Exponential distribution<a class="anchor" aria-label="anchor" href="#exponential-distribution"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>exponential distribution</strong> <span class="math inline">\(\text{Exp}(\theta)\)</span> with scale parameter <span class="math inline">\(\theta\)</span>
is another special one-parameter restriction of the gamma distribution with shape parameter set to
<span class="math inline">\(\alpha=1\)</span> (or equivalently <span class="math inline">\(k=2\)</span>).</p>
<p>It thus equals
<span class="math inline">\(\text{Gam}(\alpha=1, \theta) = W_1(s^2=\theta/2, k=2)\)</span>. It has mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(\theta^2\)</span>.</p>
<p>Just like the gamma distribution the exponential distribution is also often specified using a rate parameter <span class="math inline">\(\beta= 1/\theta\)</span> instead of a scale parameter <span class="math inline">\(\theta\)</span>.</p>
<p>In R the command <code><a href="https://rdrr.io/r/stats/Exponential.html">dexp()</a></code> returns the
density of the exponential distribution,
<code><a href="https://rdrr.io/r/stats/Exponential.html">pexp()</a></code> is the corresponding cumulative density function and <code><a href="https://rdrr.io/r/stats/Exponential.html">qexp()</a></code> is the quantile function.</p>
</div>
</div>
<div id="location-scale-t-distribution-students-t-distribution-and-cauchy-distribution" class="section level3" number="14.6.5">
<h3>
<span class="header-section-number">A.6.5</span> Location-scale <span class="math inline">\(t\)</span>-distribution, Student’s <span class="math inline">\(t\)</span>-distribution and Cauchy distribution<a class="anchor" aria-label="anchor" href="#location-scale-t-distribution-students-t-distribution-and-cauchy-distribution"><i class="fas fa-link"></i></a>
</h3>
<div id="location-scale-t-distribution" class="section level4" number="14.6.5.1">
<h4>
<span class="header-section-number">A.6.5.1</span> Location-scale <span class="math inline">\(t\)</span>-distribution<a class="anchor" aria-label="anchor" href="#location-scale-t-distribution"><i class="fas fa-link"></i></a>
</h4>
<p>The location-scale <span class="math inline">\(t\)</span>-distribution <span class="math inline">\(\text{lst}(\mu, \tau^2, \nu)\)</span> is a generalisation of the normal distribution.
It has an additional parameter <span class="math inline">\(\nu &gt; 0\)</span> (degrees of freedom) that controls the probability mass in the tails. For small values of <span class="math inline">\(\nu\)</span> the distribution is heavy-tailed — indeed so heavy that for <span class="math inline">\(\nu \leq 1\)</span> even the mean is not defined
and for <span class="math inline">\(\nu \leq 2\)</span> the variance is undefined.</p>
<p>The probability density of <span class="math inline">\(\text{lst}(\mu, \tau^2, \nu)\)</span> is
<span class="math display">\[
p(x | \mu, \tau^2, \nu) = \frac{\Gamma(\frac{\nu+1}{2})} {\sqrt{\pi \nu \tau^2}  \,\Gamma(\frac{\nu}{2})} \left(1+\frac{(x-\mu)^2}{\nu \tau^2} \right)^{-(\nu+1)/2}
\]</span>
The mean is (for <span class="math inline">\(\nu&gt;1\)</span>)
<span class="math display">\[
\text{E}(x) = \mu
\]</span>
and the variance (for <span class="math inline">\(\nu&gt;2\)</span>)
<span class="math display">\[
\text{Var}(x) = \tau^2 \frac{\nu}{\nu-2}
\]</span></p>
<p>For <span class="math inline">\(\nu \rightarrow \infty\)</span> the location-scale <span class="math inline">\(t\)</span>-distribution <span class="math inline">\(\text{lst}(\mu, \tau^2, \nu)\)</span> becomes
the normal distribution <span class="math inline">\(N(\mu, \tau^2)\)</span>.</p>
<p>For <span class="math inline">\(\nu=1\)</span> the location-scale <span class="math inline">\(t\)</span>-distribution becomes the <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a> <span class="math inline">\(\text{Cau}(\mu, \tau)\)</span>
with density <span class="math inline">\(p(x| \mu, \tau) = \frac{\tau}{\pi (\tau^2+(x-\mu)^2)}\)</span>.</p>
<p>In the R <code>extraDistr</code> package the command <code>dlst()</code> returns the
density of the location-scale <span class="math inline">\(t\)</span>-distribution,
<code>plst()</code> is the corresponding cumulative density function and <code>qlst()</code> is the quantile function.</p>
</div>
<div id="students-t-distribution" class="section level4" number="14.6.5.2">
<h4>
<span class="header-section-number">A.6.5.2</span> Student’s <span class="math inline">\(t\)</span>-distribution<a class="anchor" aria-label="anchor" href="#students-t-distribution"><i class="fas fa-link"></i></a>
</h4>
<p>For <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\tau^2=1\)</span> the location-scale <span class="math inline">\(t\)</span>-distribution becomes the
<a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student’s <span class="math inline">\(t\)</span>-distribution</a> <span class="math inline">\(t_\nu\)</span>
with mean 0 (for <span class="math inline">\(\nu&gt;1\)</span>) and variance <span class="math inline">\(\frac{\nu}{\nu-2}\)</span> (for <span class="math inline">\(\nu&gt;2\)</span>).</p>
<p>If <span class="math inline">\(y \sim t_\nu\)</span> then <span class="math inline">\(x = \mu + \tau y\)</span> is distributed as <span class="math inline">\(x \sim \text{lst}(\mu, \tau^2, \nu)\)</span>.</p>
<p>For <span class="math inline">\(\nu \rightarrow \infty\)</span> the <span class="math inline">\(t\)</span>-distribution becomes equal to <span class="math inline">\(N(0,1)\)</span>.</p>
<p>For <span class="math inline">\(\nu=1\)</span> the <span class="math inline">\(t\)</span>-distribution becomes the standard Cauchy distribution
<span class="math inline">\(\text{Cau}(0, 1)\)</span> with density <span class="math inline">\(p(x) = \frac{1}{\pi (1+x^2)}\)</span>.</p>
<p>In R the command <code><a href="https://rdrr.io/r/stats/TDist.html">dt()</a></code> returns the
density of the <span class="math inline">\(t\)</span>-distribution,
<code><a href="https://rdrr.io/r/stats/TDist.html">pt()</a></code> is the corresponding cumulative density function and <code><a href="https://rdrr.io/r/stats/TDist.html">qt()</a></code> is the quantile function.</p>
</div>
</div>
</div>
<div id="statistics" class="section level2" number="14.7">
<h2>
<span class="header-section-number">A.7</span> Statistics<a class="anchor" aria-label="anchor" href="#statistics"><i class="fas fa-link"></i></a>
</h2>
<div id="statistical-learning" class="section level3" number="14.7.1">
<h3>
<span class="header-section-number">A.7.1</span> Statistical learning<a class="anchor" aria-label="anchor" href="#statistical-learning"><i class="fas fa-link"></i></a>
</h3>
<p>The aim in statistics — data science — machine learning is to use data
(from experiments, observations, measurements) to learn about and understand the world.</p>
<p>Specifically, the aim is to identify the best model(s) for the data in order to</p>
<ul>
<li>to explain the current data, and</li>
<li>to enable good prediction of future data</li>
</ul>
<p>Note that it is easy to find models that explain the data but do not predict well!</p>
<p>Typically, one would like to avoid <em>overfitting</em> the data and prefers models that are
appropriate for the data at hand (i.e. not too simple but also not too complex).</p>
<p>Specifically, data are denoted <span class="math inline">\(D =\{x_1, \ldots, x_n\}\)</span> and models <span class="math inline">\(p(x| \theta)\)</span> that are indexed the parameter <span class="math inline">\(\theta\)</span>.</p>
<p>Often (but not always) <span class="math inline">\(\theta\)</span> can be interpreted and/or is associated with some property of the model.</p>
<p>If there is only a single parameter we write <span class="math inline">\(\theta\)</span> (scalar parameter). For a parameter vector we write <span class="math inline">\(\boldsymbol \theta\)</span> (in bold type).</p>
</div>
<div id="point-and-interval-estimation" class="section level3" number="14.7.2">
<h3>
<span class="header-section-number">A.7.2</span> Point and interval estimation<a class="anchor" aria-label="anchor" href="#point-and-interval-estimation"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>There is a parameter <span class="math inline">\(\theta\)</span> of interest in a model</li>
<li>we are uncertain about this parameter (i.e. we don’t know the exact value)</li>
<li>we would like to learn about this parameter by observing data <span class="math inline">\(x_1, \ldots, x_n\)</span> from the model</li>
</ul>
<p>Often the parameter(s) of interest are related to moments (such as mean and variance) or
to quantiles of the distribution representing the model.</p>
<p>Estimation:</p>
<ul>
<li>An <strong>estimator for <span class="math inline">\(\theta\)</span></strong> is a function <span class="math inline">\(\hat{\theta}(x_1, \ldots, x_n)\)</span> that maps the data (input) to a “guess” (output) about <span class="math inline">\(\theta\)</span>.</li>
<li>A <strong>point estimator</strong> provides a single number for each parameter</li>
<li>An <strong>interval estimator</strong> provides a set of possible values for each parameter.</li>
</ul>
<p>Simple estimators of mean and variance:</p>
<p>Suppose we have data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> all sampled independently from a distribution <span class="math inline">\(F\)</span>.</p>
<ul>
<li>The average (also known as empirical mean) <span class="math inline">\(\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i\)</span> is an estimate of the mean of <span class="math inline">\(F\)</span>.</li>
<li>The empirical variance <span class="math inline">\(\widehat{\sigma^2}_{\text{ML}} = \frac{1}{n} \sum_{i=1}^n (x_i -\hat{\mu})^2\)</span> is an estimate of the variance of <span class="math inline">\(F\)</span>. Note the factor <span class="math inline">\(1/n\)</span>. It is the maximum likelihood estimate assuming a normal model.</li>
<li>The unbiased sample variance <span class="math inline">\(\widehat{\sigma^2}_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\hat{\mu})^2\)</span> is another estimate of the variance of <span class="math inline">\(F\)</span>. Note the factor <span class="math inline">\(1/(n-1)\)</span> therefore <span class="math inline">\(n\geq 2\)</span> is required for this estimator.</li>
</ul>
</div>
<div id="sampling-properties-of-a-point-estimator-hatboldsymbol-theta" class="section level3" number="14.7.3">
<h3>
<span class="header-section-number">A.7.3</span> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span><a class="anchor" aria-label="anchor" href="#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fas fa-link"></i></a>
</h3>
<p>A point estimator <span class="math inline">\(\hat\theta\)</span> depends on the data, hence it has sampling variation (i.e. estimate will be different for a new set of observations)</p>
<p>Thus <span class="math inline">\(\hat\theta\)</span> can be seen as a random variable, and its distribution is called sampling distribution (across different experiments).</p>
<p>Properties of this distribution can be used to evaluate how far the estimator
deviates (on average across different experiments) from the true value:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{rr}
\text{Bias:}\\
\text{Variance:}\\
\text{Mean squared error:}\\
\\
\end{array}
\begin{array}{rr}
\text{Bias}(\hat{\theta})\\
\text{Var}(\hat{\theta})\\
\text{MSE}(\hat{\theta})\\
\\
\end{array}
\begin{array}{ll}
=\text{E}(\hat{\theta})-\theta\\
=\text{E}\left((\hat{\theta}-\text{E}(\hat{\theta}))^2\right)\\
=\text{E}((\hat{\theta}-\theta)^2)\\
=\text{Var}(\hat{\theta})+\text{Bias}(\hat{\theta})^2\\
\end{array}
\end{align*}\]</span></p>
<p>The last identity about MSE follows from <span class="math inline">\(\text{E}(x^2)=\text{Var}(x)+\text{E}(x)^2\)</span>.</p>
<p>At first sight it seems desirable to focus on unbiased (for finite <span class="math inline">\(n\)</span>) estimators.
However, requiring strict unbiasedness is not always a good idea!</p>
<p>In many situations it is better to allow for some small bias and in order to achieve a smaller variance and an overall total smaller MSE. This is called <em>bias-variance tradeoff</em> — as more bias
is traded for smaller variance (or, conversely, less bias is traded for higher variance)</p>
</div>
<div id="consistency" class="section level3" number="14.7.4">
<h3>
<span class="header-section-number">A.7.4</span> Consistency<a class="anchor" aria-label="anchor" href="#consistency"><i class="fas fa-link"></i></a>
</h3>
<p>Typically, <span class="math inline">\(\text{Bias}\)</span>, <span class="math inline">\(\text{Var}\)</span> and <span class="math inline">\(\text{MSE}\)</span> all decrease with increasing sample size
so that with more data <span class="math inline">\(n \to \infty\)</span> the errors become smaller and smaller.</p>
<p>The typical rate of decrease of variance of a good estimator is <span class="math inline">\(\frac{1}{n}\)</span>.
Thus, when sample size is doubled the variance is divided by 2
(and the standard deviation is divided by <span class="math inline">\(\sqrt{2}\)</span>).</p>
<p>Consistency: <span class="math inline">\(\hat{\theta}\)</span> is called consistent if
<span class="math display">\[
\text{MSE}(\hat{\theta}) \longrightarrow 0 \text{ with $n\rightarrow \infty$ }
\]</span>
The three estimators discussed above (empirical mean, empirical variance, unbiased variance) are all consistent as their MSE goes to zero with large sample size <span class="math inline">\(n\)</span>.</p>
<p>Consistency is a <em>minimum</em> essential requirement for any reasonable estimator! Of all consistent
estimators we typically prefer the estimator that is most efficient (i.e. with fasted decrease in MSE) and that therefore has smallest variance and/or MSE for given finite <span class="math inline">\(n\)</span>.</p>
<p>Consistency implies we recover the true model in the limit of infinite data if the model class contains the true data generating model.</p>
<p>If the model class does not contain the true model then strict consistency
cannot be achived but we still wish to get as close as possible
to the true model when choosing model parameters.</p>
</div>
<div id="sampling-distribution-of-mean-and-variance-estimators-for-normal-data" class="section level3" number="14.7.5">
<h3>
<span class="header-section-number">A.7.5</span> Sampling distribution of mean and variance estimators for normal data<a class="anchor" aria-label="anchor" href="#sampling-distribution-of-mean-and-variance-estimators-for-normal-data"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we have data <span class="math inline">\(x_1, \ldots, x_n\)</span> all sampled from a normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<ul>
<li><p>The empirical estimator of the mean parameter <span class="math inline">\(\mu\)</span> is given by <span class="math inline">\(\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i\)</span>. Under the normal assumption the distribution of <span class="math inline">\(\hat{\mu}\)</span> is
<span class="math display">\[
\hat{\mu} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
\]</span>
Thus <span class="math inline">\(\text{E}(\hat{\mu}) = \mu\)</span> and <span class="math inline">\(\text{Var}(\hat{\mu}) = \frac{\sigma^2}{n}\)</span>.
The estimate <span class="math inline">\(\hat{\mu}\)</span> is unbiased since <span class="math inline">\(\text{E}(\hat{\mu})-\mu = 0\)</span> The mean
squared error of <span class="math inline">\(\hat{\mu}\)</span> is <span class="math inline">\(\text{MSE}(\hat{\mu}) = \frac{\sigma^2}{n}\)</span>.</p></li>
<li><p>The empirical variance <span class="math inline">\(\widehat{\sigma^2}_{\text{ML}} = \frac{1}{n} \sum_{i=1}^n (x_i -\hat{\mu})^2\)</span> for normal data follows a one-dimensional Wishart distribution
<span class="math display">\[
\widehat{\sigma^2}_{\text{ML}} \sim 
W_1\left(s^2 = \frac{\sigma^2}{n}, k=n-1\right)
\]</span>
Thus, <span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{ML}} ) = \frac{n-1}{n}\sigma^2\)</span> and
<span class="math inline">\(\text{Var}( \widehat{\sigma^2}_{\text{ML}} ) = \frac{2(n-1)}{n^2}\sigma^4\)</span>.
The estimate <span class="math inline">\(\widehat{\sigma^2}_{\text{ML}}\)</span> is biased since
<span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{ML}} )-\sigma^2 = -\frac{1}{n}\sigma^2\)</span>.
The mean squared error is <span class="math inline">\(\text{MSE}( \widehat{\sigma^2}_{\text{ML}} ) = \frac{2(n-1)}{n^2}\sigma^4 +\frac{1}{n^2}\sigma^4 =\frac{2 n-1}{n^2}\sigma^4\)</span>.</p></li>
<li>
<p>The unbiased variance estimate <span class="math inline">\(\widehat{\sigma^2}_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\hat{\mu})^2\)</span> for normal data follows a one-dimensional Wishart distribution
<span class="math display">\[
\widehat{\sigma^2}_{\text{UB}} \sim 
W_1\left(s^2 = \frac{\sigma^2}{n-1}, k = n-1 \right)
\]</span>
Thus, <span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{UB}} ) = \sigma^2\)</span> and
<span class="math inline">\(\text{Var}( \widehat{\sigma^2}_{\text{UB}} ) = \frac{2}{n-1}\sigma^4\)</span>.
The estimate <span class="math inline">\(\widehat{\sigma^2}_{\text{ML}}\)</span> is unbiased since
<span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{UB}} )-\sigma^2 =0\)</span>.
The mean squared error is <span class="math inline">\(\text{MSE}( \widehat{\sigma^2}_{\text{UB}} ) =\frac{2}{n-1}\sigma^4\)</span>.</p>
<p>Interestingly, for any <span class="math inline">\(n&gt;1\)</span> we find that <span class="math inline">\(\text{Var}( \widehat{\sigma^2}_{\text{UB}} ) &gt; \text{Var}( \widehat{\sigma^2}_{\text{ML}} )\)</span> and <span class="math inline">\(\text{MSE}( \widehat{\sigma^2}_{\text{UB}} ) &gt; \text{MSE}( \widehat{\sigma^2}_{\text{ML}} )\)</span> so that the biased empirical estimator has both lower variance and lower mean squared error than the unbiased estimator.</p>
</li>
</ul>
</div>
<div id="one-sample-t-statistic" class="section level3" number="14.7.6">
<h3>
<span class="header-section-number">A.7.6</span> One sample <span class="math inline">\(t\)</span>-statistic<a class="anchor" aria-label="anchor" href="#one-sample-t-statistic"><i class="fas fa-link"></i></a>
</h3>
<p>Supppose we observe <span class="math inline">\(n\)</span> independent data points <span class="math inline">\(x_1, \ldots, x_n \sim N(\mu, \sigma^2)\)</span>.
Then the average <span class="math inline">\(\bar{x} = \sum_{i=1}^n x_i\)</span> is distributed as
<span class="math inline">\(\bar{x} \sim N(\mu, \sigma^2/n)\)</span> and correspondingly
<span class="math display">\[
z = \frac{\bar{x}-\mu}{\sqrt{\sigma^2/n}} \sim N(0, 1)
\]</span></p>
<p>Note that <span class="math inline">\(z\)</span> uses the known variance <span class="math inline">\(\sigma^2\)</span>. If instead the variance is estimated
by <span class="math inline">\(s^2_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\bar{x})^2\)</span>
then the one sample <span class="math inline">\(t\)</span>-statistic
<span class="math display">\[
t = \frac{\bar{x}-\mu}{\sqrt{s^2_{\text{UB}}/n}} \sim t_{n-1}
\]</span>
is obtained. It is distributed according to a Student’s <span class="math inline">\(t\)</span>-distribution
with <span class="math inline">\(n-1\)</span> degrees of freedom, with mean 0 for <span class="math inline">\(n&gt;2\)</span> and variance
<span class="math inline">\((n-1)/(n-3)\)</span> for <span class="math inline">\(n&gt;3\)</span>.</p>
<p>If instead of the unbiased estimate the empirical (ML) estimate of the variance <span class="math inline">\(s^2_{\text{ML}} = \frac{1}{n} \sum_{i=1}^n (x_i -\bar{x})^2 = \frac{n-1}{n} s^2_{\text{UB}}\)</span> is used then this leads to a slightly different statistic
<span class="math display">\[
t_{\text{ML}} = \frac{\bar{x}-\mu}{ \sqrt{ s^2_{\text{ML}}/n}}  = \sqrt{\frac{n}{n-1}} t 
\]</span>
with
<span class="math display">\[
t_{\text{ML}} \sim \text{lst}\left(0, \tau^2=\frac{n}{n-1}, n-1\right)
\]</span>
Thus, <span class="math inline">\(t_{\text{ML}}\)</span> follows a location-scale <span class="math inline">\(t\)</span>-distribution, with mean 0 for <span class="math inline">\(n&gt;2\)</span> and variance
<span class="math inline">\(n/(n-3)\)</span> for <span class="math inline">\(n&gt;3\)</span>.</p>
</div>
<div id="two-sample-t-statistic-with-common-variance" class="section level3" number="14.7.7">
<h3>
<span class="header-section-number">A.7.7</span> Two sample <span class="math inline">\(t\)</span>-statistic with common variance<a class="anchor" aria-label="anchor" href="#two-sample-t-statistic-with-common-variance"><i class="fas fa-link"></i></a>
</h3>
<p>Now suppose we observe normal data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> from two groups
with sample size <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> (and <span class="math inline">\(n=n_1+n_2\)</span>) with two different means <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> and common variance <span class="math inline">\(\sigma^2\)</span>:
<span class="math display">\[x_1,\dots,x_{n_1} \sim N(\mu_1, \sigma^2)\]</span>
and
<span class="math display">\[x_{n_1+1},\dots,x_{n} \sim N(\mu_2, \sigma^2)\]</span>
Then <span class="math inline">\(\hat{\mu}_1 = \frac{1}{n_1}\sum^{n_1}_{i=1}x_i\)</span> and
<span class="math inline">\(\hat{\mu}_2 = \frac{1}{n_2}\sum^{n}_{i=n_1+1}x_i\)</span> are the sample averages with each group.</p>
<p>The common variance <span class="math inline">\(\sigma^2\)</span> may be estimated either by
the unbiased estimate <span class="math inline">\(s^2_{\text{UB}} = \frac{1}{n-2} \left(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)^2+\sum^n_{i=n_1+1}(x_i-\hat{\mu}_2)^2\right)\)</span>
(note the factor <span class="math inline">\(n-2\)</span>) or the empirical (ML) estimate <span class="math inline">\(s^2_{\text{ML}} = \frac{1}{n} \left(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)^2+\sum^n_{i=n_1+1}(x_i-\hat{\mu}_2)^2\right) =\frac{n-2}{n} s^2_{\text{UB}}\)</span>. These two estimators
for the common variance are a often referred to as <em>pooled variance estimate</em> as information is pooled from two groups to obtain the estimate.</p>
<p>This gives rise to the two sample <span class="math inline">\(t\)</span>-statistic
<span class="math display">\[
t = \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ s^2_{\text{UB}} \left(\frac{1}{n_1}+\frac{1}{n_2}\right)}   } \sim t_{n-2}
\]</span>
that is distributed according to a Student’s <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, with mean 0 for <span class="math inline">\(n&gt;3\)</span> and variance <span class="math inline">\((n-2)/(n-4)\)</span> for <span class="math inline">\(n&gt;4\)</span>.
Large values of the two sample <span class="math inline">\(t\)</span>-statistic indicates that there are indeed two groups
rather than just one.</p>
<p>The two sample <span class="math inline">\(t\)</span>-statistic using the empirical (ML) estimate of the common variance is
<span class="math display">\[
t_{\text{ML}} = \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ s^2_{\text{ML}} \left(\frac{1}{n_1}+\frac{1}{n_2}\right)}   } = \sqrt{\frac{n}{n-2}} t
\]</span>
with
<span class="math display">\[
t_{\text{ML}} \sim \text{lst}\left(0, \tau^2=\frac{n}{n-2}, n-2\right)
\]</span>
Thus, <span class="math inline">\(t_{\text{ML}}\)</span> follows a location-scale <span class="math inline">\(t\)</span>-distribution, with mean 0 for <span class="math inline">\(n&gt;3\)</span> and variance
<span class="math inline">\(n/(n-4)\)</span> for <span class="math inline">\(n&gt;4\)</span>.</p>
</div>
<div id="confidence-intervals" class="section level3" number="14.7.8">
<h3>
<span class="header-section-number">A.7.8</span> Confidence intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>A <strong>confidence</strong> interval (CI) is an <strong>interval estimate</strong> with a <strong>frequentist</strong> interpretation.</li>
<li>Definition of <strong>coverage</strong> <span class="math inline">\(\kappa\)</span> of a CI: how often (in repeated identical experiment) does the estimated CI overlap the true parameter value <span class="math inline">\(\theta\)</span>
<ul>
<li>Eg.: Coverage <span class="math inline">\(\kappa=0.95\)</span> (95%) means that in 95 out of 100 case the estimated CI will contain the (unknown) true value (i.e. it will “cover” <span class="math inline">\(\theta\)</span>).</li>
</ul>
</li>
</ul>
<p>Illustration of the repeated construction of a CI for <span class="math inline">\(\theta\)</span>:</p>
<div class="inline-figure"><img src="fig/refresher_p1.PNG" width="40%" style="display: block; margin: auto;"></div>
<ul>
<li>Note that a CI is actually an <strong>estimate</strong>: <span class="math inline">\(\widehat{\text{CI}}(x_1, \ldots, x_n)\)</span>, i.e. it depends on data and has a random (sampling) variation.<br>
</li>
<li>A good CI has high coverage and is compact.</li>
</ul>
<p><strong>Note:</strong> the coverage probability is <strong>not</strong> the probability that the true value is contained in a given estimated interval (that would be the Bayesian <em>credible</em> interval).</p>
</div>
<div id="symmetric-normal-confidence-interval" class="section level3" number="14.7.9">
<h3>
<span class="header-section-number">A.7.9</span> Symmetric normal confidence interval<a class="anchor" aria-label="anchor" href="#symmetric-normal-confidence-interval"><i class="fas fa-link"></i></a>
</h3>
<p>For a normally distributed univariate random variable
it is straightforward to construct a symmetric two-sided CI with a given desired coverage <span class="math inline">\(\kappa\)</span>.</p>
<div class="inline-figure"><img src="fig/refresher_p2_1.PNG" width="80%" style="display: block; margin: auto;"></div>
<p>For a normal random variable <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> and density function <span class="math inline">\(f(x)\)</span> we can compute the probability</p>
<p><span class="math display">\[\text{Pr}(x \leq \mu + c \sigma) =  \int_{-\infty}^{\mu+c\sigma} f(x) dx  = \Phi (c) = \frac{1+\kappa}{2}\]</span>
Note <span class="math inline">\(\Phi(c)\)</span> is the cumulative distribution function (CDF) of the standard normal <span class="math inline">\(N(0,1)\)</span>:</p>
<p>From the above we obtain the critical point <span class="math inline">\(c\)</span> from the quantile function, i.e. by inversion of <span class="math inline">\(\Phi\)</span>:</p>
<p><span class="math display">\[c=\Phi^{-1}\left(\frac{1+\kappa}{2}\right)\]</span></p>
<p>The following table lists <span class="math inline">\(c\)</span> for the three most commonly used values of <span class="math inline">\(\kappa\)</span> - it is useful to memorise these values!</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span>
</th>
<th>Critical value <span class="math inline">\(c\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>1.64</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>1.96</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>2.58</td>
</tr>
</tbody>
</table></div>
<p>A <strong>symmetric standard normal CI</strong> with nominal coverage <span class="math inline">\(\kappa\)</span> for</p>
<ul>
<li>a scalar parameter <span class="math inline">\(\theta\)</span>
</li>
<li>with normally distributed estimate <span class="math inline">\(\hat{\theta}\)</span> and</li>
<li>with estimated standard deviation <span class="math inline">\(\hat{\text{SD}}(\hat{\theta}) = \hat{\sigma}\)</span>
</li>
</ul>
<p>is then given by
<span class="math display">\[
\widehat{\text{CI}}=[\hat{\theta} \pm c \hat{\sigma}]
\]</span>
where <span class="math inline">\(c\)</span> is chosen for desired coverage level <span class="math inline">\(\kappa\)</span>.</p>
</div>
<div id="confidence-interval-based-on-the-chi-squared-distribution" class="section level3" number="14.7.10">
<h3>
<span class="header-section-number">A.7.10</span> Confidence interval based on the chi-squared distribution<a class="anchor" aria-label="anchor" href="#confidence-interval-based-on-the-chi-squared-distribution"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-figure"><img src="fig/refresher_p4_2.PNG" width="80%" style="display: block; margin: auto;"></div>
<p>As for the normal CI we can compute critical values but for the
chi-squared distribution we use a one-sided interval:
<span class="math display">\[
\text{Pr}(x \leq c) = \kappa
\]</span>
As before we get <span class="math inline">\(c\)</span> by the quantile function, i.e. by inverting the CDF of the chi-squared distribution.</p>
<p>The following list the critical values for the three most common choice of <span class="math inline">\(\kappa\)</span>
for <span class="math inline">\(m=1\)</span> (one degree of freedom):</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span>
</th>
<th>Critical value <span class="math inline">\(c\)</span> (<span class="math inline">\(m=1\)</span>)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>2.71</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>3.84</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>6.63</td>
</tr>
</tbody>
</table></div>
<p>A one-sided CI with nominal coverage <span class="math inline">\(\kappa\)</span> is then given by <span class="math inline">\([0, c ]\)</span>.</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></div>
<div class="next"><a href="distributions-used-in-bayesian-analysis.html"><span class="header-section-number">B</span> Distributions used in Bayesian analysis</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#refresher"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="nav-link" href="#basic-mathematical-notation"><span class="header-section-number">A.1</span> Basic mathematical notation</a></li>
<li><a class="nav-link" href="#vectors-and-matrices"><span class="header-section-number">A.2</span> Vectors and matrices</a></li>
<li>
<a class="nav-link" href="#functions"><span class="header-section-number">A.3</span> Functions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#gradient"><span class="header-section-number">A.3.1</span> Gradient</a></li>
<li><a class="nav-link" href="#hessian-matrix"><span class="header-section-number">A.3.2</span> Hessian matrix</a></li>
<li><a class="nav-link" href="#convex-and-concave-functions"><span class="header-section-number">A.3.3</span> Convex and concave functions</a></li>
<li><a class="nav-link" href="#linear-and-quadratic-approximation"><span class="header-section-number">A.3.4</span> Linear and quadratic approximation</a></li>
<li><a class="nav-link" href="#conditions-for-local-optimum-of-a-function"><span class="header-section-number">A.3.5</span> Conditions for local optimum of a function</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#combinatorics"><span class="header-section-number">A.4</span> Combinatorics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#number-of-permutations"><span class="header-section-number">A.4.1</span> Number of permutations</a></li>
<li><a class="nav-link" href="#multinomial-and-binomial-coefficient"><span class="header-section-number">A.4.2</span> Multinomial and binomial coefficient</a></li>
<li><a class="nav-link" href="#de-moivre-sterling-approximation-of-the-factorial"><span class="header-section-number">A.4.3</span> De Moivre-Sterling approximation of the factorial</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#probability"><span class="header-section-number">A.5</span> Probability</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#random-variables"><span class="header-section-number">A.5.1</span> Random variables</a></li>
<li><a class="nav-link" href="#probability-mass-and-density-function-and-distribution-and-quantile-function"><span class="header-section-number">A.5.2</span> Probability mass and density function and distribution and quantile function</a></li>
<li><a class="nav-link" href="#expection-and-variance-of-a-random-variable"><span class="header-section-number">A.5.3</span> Expection and variance of a random variable</a></li>
<li><a class="nav-link" href="#transformation-of-random-variables"><span class="header-section-number">A.5.4</span> Transformation of random variables</a></li>
<li><a class="nav-link" href="#law-of-large-numbers"><span class="header-section-number">A.5.5</span> Law of large numbers:</a></li>
<li><a class="nav-link" href="#jensens-inequality"><span class="header-section-number">A.5.6</span> Jensen’s inequality</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#distributions"><span class="header-section-number">A.6</span> Distributions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#bernoulli-distribution-and-binomial-distribution"><span class="header-section-number">A.6.1</span> Bernoulli distribution and binomial distribution</a></li>
<li><a class="nav-link" href="#normal-distribution"><span class="header-section-number">A.6.2</span> Normal distribution</a></li>
<li><a class="nav-link" href="#gamma-distribution"><span class="header-section-number">A.6.3</span> Gamma distribution</a></li>
<li><a class="nav-link" href="#special-cases-of-the-gamma-distribution"><span class="header-section-number">A.6.4</span> Special cases of the gamma distribution</a></li>
<li><a class="nav-link" href="#location-scale-t-distribution-students-t-distribution-and-cauchy-distribution"><span class="header-section-number">A.6.5</span> Location-scale \(t\)-distribution, Student’s \(t\)-distribution and Cauchy distribution</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#statistics"><span class="header-section-number">A.7</span> Statistics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#statistical-learning"><span class="header-section-number">A.7.1</span> Statistical learning</a></li>
<li><a class="nav-link" href="#point-and-interval-estimation"><span class="header-section-number">A.7.2</span> Point and interval estimation</a></li>
<li><a class="nav-link" href="#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><span class="header-section-number">A.7.3</span> Sampling properties of a point estimator \(\hat{\boldsymbol \theta}\)</a></li>
<li><a class="nav-link" href="#consistency"><span class="header-section-number">A.7.4</span> Consistency</a></li>
<li><a class="nav-link" href="#sampling-distribution-of-mean-and-variance-estimators-for-normal-data"><span class="header-section-number">A.7.5</span> Sampling distribution of mean and variance estimators for normal data</a></li>
<li><a class="nav-link" href="#one-sample-t-statistic"><span class="header-section-number">A.7.6</span> One sample \(t\)-statistic</a></li>
<li><a class="nav-link" href="#two-sample-t-statistic-with-common-variance"><span class="header-section-number">A.7.7</span> Two sample \(t\)-statistic with common variance</a></li>
<li><a class="nav-link" href="#confidence-intervals"><span class="header-section-number">A.7.8</span> Confidence intervals</a></li>
<li><a class="nav-link" href="#symmetric-normal-confidence-interval"><span class="header-section-number">A.7.9</span> Symmetric normal confidence interval</a></li>
<li><a class="nav-link" href="#confidence-interval-based-on-the-chi-squared-distribution"><span class="header-section-number">A.7.10</span> Confidence interval based on the chi-squared distribution</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistics 2: Likelihood and Bayes</strong>" was written by Korbinian Strimmer. It was last built on 5 June 2023.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
