<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>10 Bayesian learning in practise | Statistics 2: Likelihood and Bayes</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="10 Bayesian learning in practise | Statistics 2: Likelihood and Bayes">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="10 Bayesian learning in practise | Statistics 2: Likelihood and Bayes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<meta name="description" content="In this chapter we discuss how three basic problems, namely how to estimate a proportion, the mean and the variance in a Bayesian framework.  10.1 Estimating a proportion using the beta-binomial...">
<meta property="og:description" content="In this chapter we discuss how three basic problems, namely how to estimate a proportion, the mean and the variance in a Bayesian framework.  10.1 Estimating a proportion using the beta-binomial...">
<meta name="twitter:description" content="In this chapter we discuss how three basic problems, namely how to estimate a proportion, the mean and the variance in a Bayesian framework.  10.1 Estimating a proportion using the beta-binomial...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistics 2: Likelihood and Bayes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="conditioning-and-bayes-rule.html"><span class="header-section-number">7</span> Conditioning and Bayes rule</a></li>
<li><a class="" href="models-with-latent-variables-and-missing-data.html"><span class="header-section-number">8</span> Models with latent variables and missing data</a></li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></li>
<li><a class="active" href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li><a class="" href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li><a class="" href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="matrix-and-calculus-refresher.html"><span class="header-section-number">A</span> Matrix and calculus refresher</a></li>
<li><a class="" href="probability-and-statistics-refresher.html"><span class="header-section-number">B</span> Probability and statistics refresher</a></li>
<li><a class="" href="distribution-refresher.html"><span class="header-section-number">C</span> Distribution refresher</a></li>
<li><a class="" href="further-distributions-used-in-bayesian-analysis.html"><span class="header-section-number">D</span> Further distributions used in Bayesian analysis</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">E</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bayesian-learning-in-practise" class="section level1" number="10">
<h1>
<span class="header-section-number">10</span> Bayesian learning in practise<a class="anchor" aria-label="anchor" href="#bayesian-learning-in-practise"><i class="fas fa-link"></i></a>
</h1>
<p>In this chapter we discuss how three basic problems, namely how to estimate a proportion, the mean and the variance in a Bayesian framework.</p>
<div id="estimating-a-proportion-using-the-beta-binomial-model" class="section level2" number="10.1">
<h2>
<span class="header-section-number">10.1</span> Estimating a proportion using the beta-binomial model<a class="anchor" aria-label="anchor" href="#estimating-a-proportion-using-the-beta-binomial-model"><i class="fas fa-link"></i></a>
</h2>
<div id="binomial-likelihood" class="section level3" number="10.1.1">
<h3>
<span class="header-section-number">10.1.1</span> Binomial likelihood<a class="anchor" aria-label="anchor" href="#binomial-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>In order to apply Bayes’ theorem we first need to find a suitable
likelihood. We use the Bernoulli model as in Example <a href="maximum-likelihood-estimation.html#exm:mleproportion">3.1</a>:</p>
<p>Repeated Bernoulli experiment (binomial model):</p>
<p>Bernoulli data generating process:
<span class="math display">\[
x  \sim \text{Ber}(\theta)
\]</span></p>
<ul>
<li>
<span class="math inline">\(x \in \{0, 1\}\)</span> (e.g. “success” vs. “failure”)</li>
<li>The “success” is indicated by outcome <span class="math inline">\(x=1\)</span> and the “failure” by <span class="math inline">\(x=0\)</span>
</li>
<li>Parameter: <span class="math inline">\(\theta\)</span> is the probability of “success”</li>
<li>probability mass function (PMF): <span class="math inline">\(\text{Pr}(x=1) = \theta\)</span>, <span class="math inline">\(\text{Pr}(x=0) = 1-\theta\)</span>
</li>
<li>Mean: <span class="math inline">\(\text{E}(x) = \theta\)</span>
</li>
<li>Variance <span class="math inline">\(\text{Var}(x) = \theta (1-\theta)\)</span>
</li>
</ul>
<p>Binomial model <span class="math inline">\(\text{Bin}(n,\theta)\)</span> (sum of <span class="math inline">\(n\)</span> Bernoulli experiments):</p>
<ul>
<li><span class="math inline">\(y \in \{0, 1, \ldots, n\} = \sum_{i=1}^n x_i\)</span></li>
<li>Mean: <span class="math inline">\(\text{E}(y) = n \theta\)</span>
</li>
<li>Variance: <span class="math inline">\(\text{Var}(y) = n \theta (1-\theta)\)</span>
</li>
<li>Mean of standardised <span class="math inline">\(y\)</span>: <span class="math inline">\(\text{E}(y/n) = \theta\)</span>
</li>
<li>Variance of standardised <span class="math inline">\(y\)</span>: <span class="math inline">\(\text{Var}(y/n) = \frac{\theta (1-\theta)}{n}\)</span>
</li>
</ul>
<p>Maximum likelihood estimate of <span class="math inline">\(\theta\)</span>:</p>
<ul>
<li>We conduct <span class="math inline">\(n\)</span> Bernoulli trials and observe data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span>
with average <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(n_1\)</span> successes and <span class="math inline">\(n_2 = n-n_1\)</span> failures.</li>
<li>Binomial likelihood:
<span class="math display">\[
L(\theta|D) = \begin{pmatrix} n \\ n_1 \end{pmatrix} \theta^{n_1} (1-\theta)^{n_2}
\]</span>
Note that the binomial coefficient arises as the ordering of the <span class="math inline">\(x_i\)</span> is
irrelevant but it may be discarded as is does not contain the parameter <span class="math inline">\(\theta\)</span>.</li>
<li>From Example <a href="maximum-likelihood-estimation.html#exm:mleproportion">3.1</a> we know that the maximum likelihood estimate of the
proportion <span class="math inline">\(\theta\)</span> is the frequency
<span class="math display">\[\hat{\theta}_{ML} = \frac{n_1}{n} = \bar{x}\]</span>
Thus, the MLE <span class="math inline">\(\hat{\theta}_{ML}\)</span> can be expressed as an average
(of the individual data points). This seemingly trivial
fact is important for Bayesian estimation of <span class="math inline">\(\theta\)</span> using linear shrinkage, as
will become evident below.</li>
</ul>
</div>
<div id="beta-prior-distribution" class="section level3" number="10.1.2">
<h3>
<span class="header-section-number">10.1.2</span> Beta prior distribution<a class="anchor" aria-label="anchor" href="#beta-prior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>In Bayesian statistics we need not only to specify the data generating process
but also a prior distribution over the parameters of the likelihood function.</p>
<p>Therefore, we need to <strong>explicitly specify our prior uncertainty about <span class="math inline">\(\theta\)</span>.</strong></p>
<p>The parameter <span class="math inline">\(\theta\)</span> has support <span class="math inline">\([0,1]\)</span>. Therefore we may use a <strong>beta distribution
<span class="math inline">\(\text{Beta}(\alpha_1, \alpha_2)\)</span> as prior for <span class="math inline">\(\theta\)</span></strong> (see the Appendix for properties of this distribution). We will see below that the beta distribution is
a natural choice as a prior in conjunction with a binomial likelihood.</p>
<p>The parameters of a prior
(here <span class="math inline">\(\alpha_1 \geq 0\)</span> and <span class="math inline">\(\alpha_2 \geq 0\)</span>) are also known as the <strong>hyperparameters</strong>
of the model to distinguish them from the parameters of the likelihood function (here <span class="math inline">\(\theta\)</span>).</p>
<p>We write for the prior distribution
<span class="math display">\[
\theta \sim \text{Beta}(\alpha_1, \alpha_2)
\]</span>
with density
<span class="math display">\[
p(\theta) = \frac{1}{B(\alpha_1, \alpha_2)} \theta^{\alpha_1-1} (1-\theta)^{\alpha_2-1}
\]</span></p>
<p>In terms of mean parameterisation <span class="math inline">\(\text{Beta}(\mu_0, k_0)\)</span> this corresponds to:</p>
<ul>
<li>The prior concentration parameter is set to <span class="math inline">\(k_0 = \alpha_1 + \alpha_2\)</span>
</li>
<li>The prior mean parameter is set to <span class="math inline">\(\mu_0 = \alpha_1 / k_0\)</span>.</li>
</ul>
<p>The prior mean is therefore
<span class="math display">\[
\text{E}(\theta) = \mu_0
\]</span>
and the prior variance
<span class="math display">\[
\text{Var}(\theta)  = \frac{\mu_0 (1-\mu_0)}{k_0 + 1}
\]</span></p>
<p>It is important that this does not actually mean that <span class="math inline">\(\theta\)</span> is random.
It only means that we model the uncertainty about <span class="math inline">\(\theta\)</span> using a beta-distributed random variable. The flexibility of the beta distribution allows to accommodate a large variety of possible scenarios for our prior knowledge using just two parameters.</p>
<p>Note the mean and variance of the beta prior and the mean and variance of the standardised
binomial variable <span class="math inline">\(y/n\)</span> have the same form. This is further indication that the binomial
likelihood and the beta prior are well matched — see the discussion below about “conjugate priors”.</p>
</div>
<div id="computing-the-posterior-distribution" class="section level3" number="10.1.3">
<h3>
<span class="header-section-number">10.1.3</span> Computing the posterior distribution<a class="anchor" aria-label="anchor" href="#computing-the-posterior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>After observing data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> with <span class="math inline">\(n_1\)</span> “successes”
and <span class="math inline">\(n_2 = n-n_1\)</span> “failures”
we can compute the posterior
density over <span class="math inline">\(\theta\)</span> using Bayes’ theorem:
<span class="math display">\[
p(\theta| D) = \frac{p(\theta) L(\theta | D) }{p(D)}
\]</span></p>
<p>Applying Bayes’ theorem results in the posterior distribution:
<span class="math display">\[
\theta| D \sim \text{Beta}(\alpha_1+n_2, \alpha_2+n_2)
\]</span>
with density
<span class="math display">\[
p(\theta| D) = \frac{1}{B(\alpha_1+n_1, \alpha_2+n_2)} \theta^{\alpha_1+n_1-1} (1-\theta)^{\alpha_2+n_2-1}
\]</span>
(For a proof see Worksheet B1.)</p>
<p>In the corresponding mean parameterisation <span class="math inline">\(\text{Beta}(\mu_1, k_1)\)</span> this results in the following updates:</p>
<ul>
<li>The concentration parameter is updated to <span class="math inline">\(k_1 = k_0+n\)</span>
</li>
<li>The mean parameter is updated to
<span class="math display">\[
\mu_1 = \frac{\alpha_1 + n_1}{k_1}
\]</span>
This can be written as
<span class="math display">\[
\begin{split}
\mu_1 &amp; =  \frac{\alpha_1}{k_1}  + \frac{n_1}{k_1}\\
    &amp; =  \frac{k_0}{k_1} \frac{\alpha_1}{k_0}   + \frac{n}{k_1} \frac{n_1}{n}\\
    &amp; = \lambda \mu_0 + (1-\lambda) \hat{\theta}_{ML}\\
\end{split}
\]</span>
with <span class="math inline">\(\lambda = \frac{k_0}{k_1}\)</span>. Hence, <span class="math inline">\(\mu_1\)</span> is a
convex combination of the prior mean and the MLE.</li>
</ul>
<p>Therefore, the posterior mean is
<span class="math display">\[
\text{E}(\theta | D) = \mu_1
\]</span>
and the posterior variance is
<span class="math display">\[
\text{Var}(\theta | D)
= \frac{\mu_1 (1-\mu_1)}{k_1+1 }
\]</span></p>
</div>
</div>
<div id="properties-of-bayesian-learning" class="section level2" number="10.2">
<h2>
<span class="header-section-number">10.2</span> Properties of Bayesian learning<a class="anchor" aria-label="anchor" href="#properties-of-bayesian-learning"><i class="fas fa-link"></i></a>
</h2>
<p>The beta-binomial model, even though it is one of the simplest possible models, already allows to observe a number of important features and properties of Bayesian learning. Many of these apply also to other models as we will see later.</p>
<div id="prior-acting-as-pseudodata" class="section level3" number="10.2.1">
<h3>
<span class="header-section-number">10.2.1</span> Prior acting as pseudodata<a class="anchor" aria-label="anchor" href="#prior-acting-as-pseudodata"><i class="fas fa-link"></i></a>
</h3>
<p>In the expression for the mean and variance you can see that
the concentration parameter <span class="math inline">\(k_0=\alpha_1 + \alpha_2\)</span> behaves like an
implicit sample size connected with the prior information about <span class="math inline">\(\theta\)</span>.</p>
<p>Specifically, <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\alpha_2\)</span> act as <strong>pseudocounts</strong> that influence
both the posterior mean and the posterior variance, exactly in the same way as conventional
observations.</p>
<p>For example, the larger <span class="math inline">\(k_0\)</span> (and thus the larger <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\alpha_2\)</span>) the smaller is the posterior variance, with variance decreasing proportional to the inverse of <span class="math inline">\(k_0\)</span>. If the prior is highly concentrated, i.e. if it has low variance and large precision (=inverse variance) then the implicit data size <span class="math inline">\(k_0\)</span> is large. Conversely, if the prior has large variance, then the prior is vague and the implicit data size <span class="math inline">\(k_0\)</span> is small.</p>
<p>Hence, a prior has the same effect as if one would add data — but without actually adding data! This is precisely this why a prior acts as a regulariser and prevents overfitting, because it increases the effective sample size.</p>
<p>Another interpretation is that a prior summarises data
that may have been available previously as observations.</p>
</div>
<div id="linear-shrinkage-of-mean" class="section level3" number="10.2.2">
<h3>
<span class="header-section-number">10.2.2</span> Linear shrinkage of mean<a class="anchor" aria-label="anchor" href="#linear-shrinkage-of-mean"><i class="fas fa-link"></i></a>
</h3>
<p>In the beta-binomial model the <strong>posterior mean is a convex combination (i.e. the weighted average) of the ML estimate and the prior mean</strong> as can be seen from the update formula
<span class="math display">\[
\mu_1 = \lambda \mu_0 + (1-\lambda) \hat{\theta}_{ML}
\]</span>
with weight <span class="math inline">\(\lambda \in [0,1]\)</span>
<span class="math display">\[
\lambda = \frac{k_0}{k_1} \,.
\]</span>
Thus, the posterior mean <span class="math inline">\(\mu_1\)</span> is a linearly adjusted <span class="math inline">\(\hat{\theta}_{ML}\)</span>. The factor <span class="math inline">\(\lambda\)</span> is called the <strong>shrinkage intensity</strong> — note that this is the ratio of the “prior sample size” (<span class="math inline">\(k_0\)</span>) and the “effective total sample size” (<span class="math inline">\(k_1\)</span>).</p>
<ol style="list-style-type: decimal">
<li><p>This adjustment of the MLE is called <em>shrinkage</em>, because the <span class="math inline">\(\hat{\theta}_{ML}\)</span> is “shrunk” towards the prior mean <span class="math inline">\(\mu_0\)</span> (which is often called the “target”, and sometimes the target is zero, and then the terminology “shrinking” makes most sense).</p></li>
<li>
<p>If the shrinkage intensity is zero (<span class="math inline">\(\lambda = 0\)</span>) then the ML point estimator is recovered. This happens when <span class="math inline">\(\alpha_1=0\)</span> and <span class="math inline">\(\alpha_2=0\)</span> or for <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>Remark: using maximum likelihood to estimate <span class="math inline">\(\theta\)</span> (for moderate or small <span class="math inline">\(n\)</span>) is the same as Bayesian posterior mean estimation using the beta-binomial model with prior <span class="math inline">\(\alpha_1=0\)</span> and <span class="math inline">\(\alpha_2=0\)</span>. This prior is extremely “u-shaped” and the implicit prior for the ML estimation. Would you use such a prior intentionally?</p>
</li>
<li><p>If the shrinkage intensity is large (<span class="math inline">\(\lambda \rightarrow 1\)</span>) then the posterior mean corresponds to the prior.
This happens if <span class="math inline">\(n=0\)</span> or if <span class="math inline">\(k_0\)</span> is very large (implying that the prior is sharply concentrated around the prior mean).</p></li>
<li><p>Since the ML estimate <span class="math inline">\(\hat{\theta}_{ML}\)</span> is unbiased the Bayesian point estimate is biased (for finite <span class="math inline">\(n\)</span>!). And the bias is induced by the prior mean deviating from the true mean. This is also true more generally as Bayesian learning typically produces biased estimators (but asymptotically they will be unbiased like in ML).</p></li>
<li><p>The fact that the posterior mean is a linear combination of the MLE and the prior mean is not a coincidence. In fact, this is true for all distributions that are exponential families, see e.g. Diaconis and Ylvisaker (1979)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Diaconis, P., and D Ylvisaker. 1979. &lt;em&gt;Conjugate Priors for Exponential Families.&lt;/em&gt;
Ann. Statist. &lt;strong&gt;7&lt;/strong&gt;:269–281. &lt;a href="https://doi.org/10.1214/aos/1176344611" class="uri"&gt;https://doi.org/10.1214/aos/1176344611&lt;/a&gt;&lt;/p&gt;'><sup>11</sup></a>.
Crucially, exponential families can always be parameterised such that the corresponding MLEs are expressed as averages of functions of the data (more technically: the MLE of the mean parameter in an EF is the average of the canonical statistic). In conjunction with a particular type of prior (conjugate priors, always existing for exponential families, see below) this allows to write the update from the prior to posterior mean as a linear adjustment of the MLE.</p></li>
<li><p>Furthermore, it is possible (and indeed quite useful for computational reasons!) to formulate Bayes learning assuming only first and second moments (i.e. without full distributions) and in terms of linear shrinkage, see e.g. Hartigan (1969)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Hartigan, J. A. 1969. &lt;em&gt;Linear Bayesian methods.&lt;/em&gt; J. Roy. Statist. Soc. B &lt;strong&gt;31&lt;/strong&gt;:446-454
&lt;a href="https://doi.org/10.1111/j.2517-6161.1969.tb00804.x" class="uri"&gt;https://doi.org/10.1111/j.2517-6161.1969.tb00804.x&lt;/a&gt;&lt;/p&gt;'><sup>12</sup></a>. The resulting theory is called “Bayes linear statistics” (Goldstein and Wooff, 2007)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Goldstein, M., and D. Wooff. 2007. &lt;em&gt;Bayes Linear Statistics: Theory and Methods.&lt;/em&gt; Wiley.
&lt;a href="https://doi.org/10.1002/9780470065662" class="uri"&gt;https://doi.org/10.1002/9780470065662&lt;/a&gt;&lt;/p&gt;'><sup>13</sup></a>.</p></li>
</ol>
</div>
<div id="conjugacy-of-prior-and-posterior-distribution" class="section level3" number="10.2.3">
<h3>
<span class="header-section-number">10.2.3</span> Conjugacy of prior and posterior distribution<a class="anchor" aria-label="anchor" href="#conjugacy-of-prior-and-posterior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>In the beta-binomial model for estimating the proportion <span class="math inline">\(\theta\)</span> the choice of the <strong>beta distribution as prior distribution</strong> along with the binomial likelihood resulted in having the <strong>beta distribution as posterior distribution</strong> as well.</p>
<p>If the prior and posterior belong to the same distributional family the prior is called a <strong>conjugate prior</strong>. This will be the case if the prior has the same functional form as the likelihood. Therefore one also says that the prior is conjugate for the likelihood.</p>
<p>It can be shown that conjugate priors exist for all likelihood functions that are
based on data generating models that are exponential families.</p>
<p>In the beta-binomial model the likelihood is based on the binomial distribution and has the following form
(only terms depending on the parameter <span class="math inline">\(\theta\)</span> are shown):
<span class="math display">\[
\theta^{n_1} (1-\theta)^{n_2}
\]</span>
The form of the beta prior is (again, only showing terms depending on <span class="math inline">\(\theta\)</span>):
<span class="math display">\[
\theta^{\alpha_1-1} (1-\theta)^{\alpha_2-1}
\]</span>
Since the posterior is proportional to the product of prior
and likelihood the posterior will have exactly the same form as
the prior:
<span class="math display">\[
\theta^{\alpha_1+n_1-1} (1-\theta)^{\alpha_2+n_2-1}
\]</span>
Choosing the prior distribution from a family conjugate for the likelihood
greatly simplifies Bayesian analysis since the Bayes formula can then be written in form of an update formula for the parameters of the beta distribution:
<span class="math display">\[
\alpha_1 \rightarrow \alpha_1 + n_1  = \alpha_1 + n \hat{\theta}_{ML}
\]</span>
<span class="math display">\[
\alpha_2 \rightarrow \alpha_2 + n_2 = \alpha_2 + n (1-\hat{\theta}_{ML})
\]</span></p>
<p>Thus, conjugate prior distributions are very convenient choices. However, in their application it must be ensured that the prior distribution is flexible enough to encapsulate all prior information that may be available. In cases where this is not the case alternative priors should be used (and most likely this will then require to compute the posterior distribution numerically rather than analytically).</p>
</div>
<div id="large-sample-limits-of-mean-and-variance" class="section level3" number="10.2.4">
<h3>
<span class="header-section-number">10.2.4</span> Large sample limits of mean and variance<a class="anchor" aria-label="anchor" href="#large-sample-limits-of-mean-and-variance"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(n\)</span> is large and <span class="math inline">\(n &gt;&gt; \alpha, \beta\)</span> then <span class="math inline">\(\lambda \rightarrow 0\)</span>
and hence the posterior mean and variance become asympotically</p>
<p><span class="math display">\[
\text{E}(\theta| D)  \overset{a}{=} \frac{n_1 }{n} = \hat{\theta}_{ML}
\]</span>
and
<span class="math display">\[
\text{Var}(\theta| D) \overset{a}{=}   \frac{\hat{\theta}_{ML} (1-\hat{\theta}_{ML})}{n}
\]</span></p>
<p>Thus, if the sample size is large then the Bayes’ estimator turns into the ML estimator! Specifically,
the posterior mean becomes the ML point estimate, and the posterior variance is equal to the asymptotic variance computed via the observed Fisher information.</p>
<p>Thus, for large <span class="math inline">\(n\)</span> the data dominate and any details about the prior (such as the settings of the hyperparameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\alpha_2\)</span>) become irrelevant!</p>
</div>
<div id="asymptotic-normality-of-the-posterior-distribution" class="section level3" number="10.2.5">
<h3>
<span class="header-section-number">10.2.5</span> Asymptotic normality of the posterior distribution<a class="anchor" aria-label="anchor" href="#asymptotic-normality-of-the-posterior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Also known as <strong>Bayesian Central Limit Theorem (CLT)</strong>.</p>
<p>Under some regularity conditions (such as regular likelihood and positive prior probability for all
parameter values, finite number of parameters, etc.) for large sample size the Bayesian posterior distribution converges to a normal distribution
centred around the MLE and with the variance of the MLE:</p>
<p><span class="math display">\[
\text{for large $n$:  }  p(\boldsymbol \theta| D) \to N(\hat{\boldsymbol \theta}_{ML}, \text{Var}(\hat{\boldsymbol \theta}_{ML}) )
\]</span></p>
<p>So not only are the posterior mean and variance converging to the MLE and the variance of the MLE
for large sample size, but also the posterior distribution itself converges to the sampling distribution!</p>
<p>This holds generally in many regular cases, not just in the simple case above.</p>
<p>The Bayesian CLT is generally known
as the <strong><a href="https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem">Bernstein-von Mises theorem</a></strong> (who discovered it at around 1920–30), but special cases were already known as by Laplace.</p>
<p>In the Worksheet B1 the asymptotic convergence of the posterior distribution to a normal distribution is demonstrated graphically.</p>
</div>
<div id="posterior-variance-for-finite-n" class="section level3" number="10.2.6">
<h3>
<span class="header-section-number">10.2.6</span> Posterior variance for finite <span class="math inline">\(n\)</span><a class="anchor" aria-label="anchor" href="#posterior-variance-for-finite-n"><i class="fas fa-link"></i></a>
</h3>
<p>From the Bayesian posterior we can obtain a Bayesian point estimate
for the proportion <span class="math inline">\(\theta\)</span> by computing the posterior mean
<span class="math display">\[
\text{E}(\theta | D) = \frac{\alpha_1+n_1}{k_1} = \hat{\theta}_{\text{Bayes}}
\]</span>
along with the posterior variance
<span class="math display">\[
\text{Var}(\theta | D) = \frac{\hat{\theta}_{\text{Bayes}} (1-\hat{\theta}_{\text{Bayes}})}{k_1+1}
\]</span></p>
<p>Asymptotically for large <span class="math inline">\(n\)</span> the posterior mean becomes the maximum likelihood estimate (MLE), and the
posterior variance becomes the asymptotic variance of the MLE.
Thus, for large <span class="math inline">\(n\)</span> the Bayesian point estimate will be indistinguishable from the MLE
and shares its favourable properties.</p>
<p>In addition, for finite sample size the posterior variance will typically be <em>smaller</em> than both the asymptotic
posterior variance (for large <span class="math inline">\(n\)</span>) and the prior variance, showing that combining the information
available in the prior and in the data leads to a more efficient estimate.</p>
</div>
</div>
<div id="estimating-the-mean-using-the-normal-normal-model" class="section level2" number="10.3">
<h2>
<span class="header-section-number">10.3</span> Estimating the mean using the normal-normal model<a class="anchor" aria-label="anchor" href="#estimating-the-mean-using-the-normal-normal-model"><i class="fas fa-link"></i></a>
</h2>
<div id="normal-likelihood" class="section level3" number="10.3.1">
<h3>
<span class="header-section-number">10.3.1</span> Normal likelihood<a class="anchor" aria-label="anchor" href="#normal-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>As in Example <a href="maximum-likelihood-estimation.html#exm:mlenormalmean">3.2</a> where we estimated the mean parameter by
maximum likelihood we assume as data-generating model the normal distribution
with unknown mean <span class="math inline">\(\mu\)</span> and known variance <span class="math inline">\(\sigma^2\)</span>:
<span class="math display">\[
x \sim N(\mu, \sigma^2)
\]</span>
We observe <span class="math inline">\(n\)</span> samples <span class="math inline">\(D = \{x_1, \ldots x_n\}\)</span>.
This yields using maximum likelihood the estimate <span class="math inline">\(\hat{\mu}_{ML} = \bar{x}\)</span>.</p>
<p>We note that the MLE <span class="math inline">\(\hat\mu_{ML}\)</span> is expressed as an average of the data points,
which is what enables the linear shrinkage seen below.</p>
</div>
<div id="normal-prior-distribution" class="section level3" number="10.3.2">
<h3>
<span class="header-section-number">10.3.2</span> Normal prior distribution<a class="anchor" aria-label="anchor" href="#normal-prior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>normal distribution is the conjugate distribution for the mean parameter of a
normal likelihood</strong>, so if we use a normal prior then posterior for <span class="math inline">\(\mu\)</span> is normal as well.</p>
<p>To model the uncertainty about <span class="math inline">\(\mu\)</span> we use the normal distribution in the form
<span class="math inline">\(N(\mu, \sigma^2/k)\)</span> with a mean parameter <span class="math inline">\(\mu\)</span> and a concentration
parameter <span class="math inline">\(k &gt; 0\)</span> (remember that <span class="math inline">\(\sigma^2\)</span> is given and is also used in the likelihood).</p>
<p>Specifically, we use as normal <strong>prior distribution</strong> for the mean
<span class="math display">\[
\mu \sim N\left(\mu_0, \frac{\sigma^2}{k_0}\right)
\]</span></p>
<ul>
<li>The prior concentration parameter is set to <span class="math inline">\(k_0\)</span>
</li>
<li>The prior mean parameter is set to <span class="math inline">\(\mu_0\)</span>
</li>
</ul>
<p>Hence the prior mean is
<span class="math display">\[
\text{E}(\mu) =  \mu_0
\]</span>
and the prior variance
<span class="math display">\[
\text{Var}(\mu)  = \frac{\sigma^2}{k_0}
\]</span>
where the concentration parameter <span class="math inline">\(k_0\)</span> corresponds the implied sample size of the prior.
Note that <span class="math inline">\(k_0\)</span> does not need to be an integer value.</p>
</div>
<div id="normal-posterior-distribution" class="section level3" number="10.3.3">
<h3>
<span class="header-section-number">10.3.3</span> Normal posterior distribution<a class="anchor" aria-label="anchor" href="#normal-posterior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>After observing data <span class="math inline">\(D\)</span> the <strong>posterior distribution</strong>
is also normal with updated parameters <span class="math inline">\(\mu=\mu_1\)</span> and <span class="math inline">\(k_1\)</span>
<span class="math display">\[
\mu | D \sim N\left(\mu_1, \frac{\sigma^2}{k_1}\right)
\]</span></p>
<ul>
<li>The posterior concentration parameter is updated to <span class="math inline">\(k_1 = k_0 +n\)</span>
</li>
<li>The posterior mean parameter is updated to
<span class="math display">\[
\mu_1 = \lambda \mu_0 + (1-\lambda) \hat\mu_{ML}
\]</span>
with <span class="math inline">\(\lambda = \frac{k_0}{k_1}\)</span>.
This can be seen as linear shrinkage of
<span class="math inline">\(\hat\mu_{ML}\)</span> towards the prior mean <span class="math inline">\(\mu_0\)</span>.</li>
</ul>
<p>(For a proof see Worksheet B2.)</p>
<p>The posterior mean is
<span class="math display">\[
\text{E}(\mu | D) = \mu_1
\]</span>
and the posterior variance is
<span class="math display">\[
\text{Var}(\mu | D)  = \frac{\sigma^2}{k_1}
\]</span></p>
</div>
<div id="large-sample-asymptotics" class="section level3" number="10.3.4">
<h3>
<span class="header-section-number">10.3.4</span> Large sample asymptotics<a class="anchor" aria-label="anchor" href="#large-sample-asymptotics"><i class="fas fa-link"></i></a>
</h3>
<p>For <span class="math inline">\(n\)</span> large and <span class="math inline">\(n &gt;&gt; k_0\)</span> the shrinkage intensity <span class="math inline">\(\lambda \rightarrow 0\)</span>
and and <span class="math inline">\(k_1 \rightarrow n\)</span>. As a result
<span class="math display">\[
\text{E}(\mu |  D) \overset{a}{=}  \hat\mu_{ML}
\]</span>
<span class="math display">\[
\text{Var}(\mu |  D) \overset{a}{=} \frac{\sigma^2}{n}
\]</span>
i.e. we recover the MLE and its asymptotic variance!</p>
<p>Note that for finite <span class="math inline">\(n\)</span> the posterior variance <span class="math inline">\(\frac{\sigma^2}{n+k_0}\)</span> is smaller
than both the asymptotic variance <span class="math inline">\(\frac{\sigma^2}{n}\)</span> of the MLE and the prior variance <span class="math inline">\(\frac{\sigma^2}{k_0}\)</span>.</p>
</div>
</div>
<div id="estimating-the-variance-using-the-inverse-gamma-normal-model" class="section level2" number="10.4">
<h2>
<span class="header-section-number">10.4</span> Estimating the variance using the inverse-gamma-normal model<a class="anchor" aria-label="anchor" href="#estimating-the-variance-using-the-inverse-gamma-normal-model"><i class="fas fa-link"></i></a>
</h2>
<div id="normal-likelihood-1" class="section level3" number="10.4.1">
<h3>
<span class="header-section-number">10.4.1</span> Normal likelihood<a class="anchor" aria-label="anchor" href="#normal-likelihood-1"><i class="fas fa-link"></i></a>
</h3>
<p>As data generating model we use
normal distribution
<span class="math display">\[
x  \sim N(\mu, \sigma^2)
\]</span>
with unknown variance <span class="math inline">\(\sigma^2\)</span> and known mean <span class="math inline">\(\mu\)</span>. This yields as maximum likelihood estimate for the variance
<span class="math display">\[
\widehat{\sigma^2}_{ML}= \frac{1}{n}\sum_{i=1}^n (x_i-\mu)^2
\]</span></p>
<p>Note that, again, the MLE is an average (of a quadratic function of the
individual data points). This enables linear shrinkage of the MLE as seen below.</p>
</div>
<div id="ig-prior-distribution" class="section level3" number="10.4.2">
<h3>
<span class="header-section-number">10.4.2</span> IG prior distribution<a class="anchor" aria-label="anchor" href="#ig-prior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>To model the uncertainty about the variance we use the inverse-gamma (IG) distribution, also known
as inverse Wishart (IW) distribution (see Appendix for details of this distribution).
The IG distribution is conjugate for the variance parameter in the normal likelihood, hence both the prior and the posterior distribution are IG.<br>
As we use the Wishart parameterisation we may equally well call this
an inverse Wishart (IW) prior, and the whole model IW-normal model.</p>
<p>Specifically, as prior distribution for <span class="math inline">\(\sigma^2\)</span> we assume using
the mean parameter <span class="math inline">\(\mu\)</span> and concentration parameter <span class="math inline">\(\kappa\)</span>:
<span class="math display">\[
\sigma^2 \sim  W^{-1}_1(\psi=\kappa_0 \sigma^2_0, \nu=\kappa_0+2)
\]</span></p>
<ul>
<li>The prior concentration parameter is set to <span class="math inline">\(\kappa_0\)</span>
</li>
<li>The prior mean parameter is set to <span class="math inline">\(\sigma^2_0\)</span>
</li>
</ul>
<p>The corresponding prior mean is
<span class="math display">\[
\text{E}(\sigma^2) = \sigma^2_0
\]</span>
and the prior variance is
<span class="math display">\[
\text{Var}(\sigma^2) = \frac{2 \sigma_0^4}{\kappa_0-2}
\]</span>
(note that <span class="math inline">\(\kappa_0 &gt; 2\)</span> for the variance to exist)</p>
</div>
<div id="ig-posterior-distribution" class="section level3" number="10.4.3">
<h3>
<span class="header-section-number">10.4.3</span> IG posterior distribution<a class="anchor" aria-label="anchor" href="#ig-posterior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>After observing <span class="math inline">\(D = \{ x_1 \ldots, x_n\}\)</span> the posterior distribution is
also IG with updated parameters:
<span class="math display">\[
\sigma^2| D \sim W^{-1}_1(\psi=\kappa_1 \sigma^2_1, \nu=\kappa_1+2)
\]</span></p>
<ul>
<li>The posterior concentration parameter is updated to <span class="math inline">\(\kappa_1 = \kappa_0+n\)</span>
</li>
<li>The posterior mean parameter update follows the standard linear shrinkage rule:
<span class="math display">\[
\sigma^2_1 =  \lambda \sigma^2_0 + (1-\lambda) \widehat{\sigma^2}_{ML}
\]</span>
with <span class="math inline">\(\lambda=\frac{\kappa_0}{\kappa_1}\)</span>.</li>
</ul>
<p>The posterior mean is
<span class="math display">\[
\text{E}(\sigma^2 | D) = \sigma^2_1
\]</span>
and the posterior variance
<span class="math display">\[
\text{Var}(\sigma^2 | D) = \frac{ 2 \sigma^4_1}{\kappa_1-2}
\]</span></p>
</div>
<div id="large-sample-asymptotics-1" class="section level3" number="10.4.4">
<h3>
<span class="header-section-number">10.4.4</span> Large sample asymptotics<a class="anchor" aria-label="anchor" href="#large-sample-asymptotics-1"><i class="fas fa-link"></i></a>
</h3>
<p>For large sample size <span class="math inline">\(n\)</span> with <span class="math inline">\(n &gt;&gt; \kappa_0\)</span>
the shrinkage intensity vanishes
(<span class="math inline">\(\lambda \rightarrow 0\)</span>) and therefore <span class="math inline">\(\sigma^2_1 \rightarrow \widehat{\sigma^2}_{ML}\)</span>. We also find that <span class="math inline">\(\kappa_1-2 \rightarrow n\)</span>.</p>
<p>This results in the asymptotic posterior mean
<span class="math display">\[
\text{E}(\sigma^2 |  D) \overset{a}{=}  \widehat{\sigma^2}_{ML}
\]</span>
and the asymptotic
posterior variance
<span class="math display">\[
\text{Var}(\sigma^2 |  D) \overset{a}{=} \frac{2 (\widehat{\sigma^2}_{ML})^2}{n}
\]</span>
Thus we recover the MLE of <span class="math inline">\(\sigma^2\)</span> and its asymptotic variance.</p>
</div>
<div id="other-equivalent-update-rules" class="section level3" number="10.4.5">
<h3>
<span class="header-section-number">10.4.5</span> Other equivalent update rules<a class="anchor" aria-label="anchor" href="#other-equivalent-update-rules"><i class="fas fa-link"></i></a>
</h3>
<p>Above the update rule from prior to posterior inverse gamma distribution is
stated for the mean parameterisation:</p>
<ul>
<li><span class="math inline">\(\kappa_0 \rightarrow \kappa_1 = \kappa_0+n\)</span></li>
<li>
<span class="math inline">\(\sigma^2_0 \rightarrow \sigma^2_1 = \lambda \sigma^2_0 + (1-\lambda) \widehat{\sigma^2}_{ML}\)</span> with
<span class="math inline">\(\lambda=\frac{\kappa_0}{\kappa_1}\)</span>
</li>
</ul>
<p>This has the advantage that the mean of the inverse gamma distribution
is updated directly, and that the prior and posterior variance is also
straightforward to compute.</p>
<p>The same update rule can also be expressed in terms of the other parameterisations.
In terms of the conventional parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> of the inverse gamma
distribution the update rule is</p>
<ul>
<li><span class="math inline">\(\alpha_0 \rightarrow \alpha_1 = \alpha_0 +\frac{n}{2}\)</span></li>
<li><span class="math inline">\(\beta_0 \rightarrow \beta_1 = \beta_0 + \frac{n}{2} \widehat{\sigma^2}_{ML} = \beta_0 + \frac{1}{2} \sum_{i=1}^n (x_i-\mu)^2\)</span></li>
</ul>
<p>For the parameters <span class="math inline">\(\psi\)</span> and <span class="math inline">\(\nu\)</span> of the univariate inverse Wishart distribution
the update rule is</p>
<ul>
<li><span class="math inline">\(\nu_0 \rightarrow \nu_1 = \nu_0 +n\)</span></li>
<li><span class="math inline">\(\psi_0 \rightarrow \psi_1 = \psi_0 + n \widehat{\sigma^2}_{ML} = \psi_0 + \sum_{i=1}^n (x_i-\mu)^2\)</span></li>
</ul>
<p>For the parameters <span class="math inline">\(\tau^2\)</span> and <span class="math inline">\(\nu\)</span> of the scaled inverse chi-squared
distribution the update rule is</p>
<ul>
<li><span class="math inline">\(\nu_0 \rightarrow \nu_1 = \nu_0 +n\)</span></li>
<li><span class="math inline">\(\tau^2_0 \rightarrow \tau^2_1 = \frac{\nu_0}{\nu_1} \tau^2_0 + \frac{n}{\nu_1} \widehat{\sigma^2}_{ML}\)</span></li>
</ul>
<p>(See Worksheet B3 for proof of equivalence of all above update rules.)</p>
</div>
</div>
<div id="estimating-the-precision-using-the-gamma-normal-model" class="section level2" number="10.5">
<h2>
<span class="header-section-number">10.5</span> Estimating the precision using the gamma-normal model<a class="anchor" aria-label="anchor" href="#estimating-the-precision-using-the-gamma-normal-model"><i class="fas fa-link"></i></a>
</h2>
<div id="mle-of-the-precision" class="section level3" number="10.5.1">
<h3>
<span class="header-section-number">10.5.1</span> MLE of the precision<a class="anchor" aria-label="anchor" href="#mle-of-the-precision"><i class="fas fa-link"></i></a>
</h3>
<p>Instead of estimating the variance <span class="math inline">\(\sigma^2\)</span> we may wish to estimate the precision <span class="math inline">\(w1/\sigma^2\)</span>, i.e. the inverse of variance.</p>
<p>As above the data generating model is a
normal distribution
<span class="math display">\[
x  \sim N(\mu, 1/w)
\]</span>
with unknown precision <span class="math inline">\(w\)</span> and known mean <span class="math inline">\(\mu\)</span>. This yields as maximum likelihood estimate
(easily derived thanks to the invariance principle)
<span class="math display">\[
\hat{w}_{ML} =  \frac{ 1}{\widehat{\sigma^2}_{ML} } = \frac{1}{\frac{1}{n}\sum_{i=1}^n (x_i-\mu)^2}
\]</span>
Crucially, the MLE of the precision <span class="math inline">\(w\)</span> is not an average itself (instead, it is a function of an average).
As a consequence, as seen below, the posterior mean of <span class="math inline">\(w\)</span> cannot be written as linear adjustment of the MLE.</p>
</div>
<div id="gamma-wishart-prior" class="section level3" number="10.5.2">
<h3>
<span class="header-section-number">10.5.2</span> Gamma (Wishart) prior<a class="anchor" aria-label="anchor" href="#gamma-wishart-prior"><i class="fas fa-link"></i></a>
</h3>
<p>For modelling the variance we have used an inverse gamma (inverse Wishart) distribution for the prior and posterior distributions. Thus, in order to model the precision we therefore now use a gamma (Wishart) distribution.</p>
<p>Specifically, we use the Wishart distribution in the mean parameterisation (see Appendix):
<span class="math display">\[
w \sim  W_1(s^2 = w_0/k_0, k=k_0)
\]</span></p>
<ul>
<li>The prior concentration parameter is set to <span class="math inline">\(k_0\)</span>
</li>
<li>The prior mean parameter is set to <span class="math inline">\(w_0\)</span>
</li>
</ul>
<p>The corresponding prior mean is
<span class="math display">\[
\text{E}(w) = w_0
\]</span>
and the prior variance is
<span class="math display">\[
\text{Var}(\sigma^2) = 2 w_0^2/ k_0
\]</span></p>
</div>
<div id="gamma-wishart-posterior" class="section level3" number="10.5.3">
<h3>
<span class="header-section-number">10.5.3</span> Gamma / Wishart posterior<a class="anchor" aria-label="anchor" href="#gamma-wishart-posterior"><i class="fas fa-link"></i></a>
</h3>
<p>After observing <span class="math inline">\(D = \{ x_1 \ldots, x_n\}\)</span> the posterior distribution is
also gamma resp. Wishart with updated parameters:</p>
<p><span class="math display">\[
w | D \sim   W_1(s^2 = w_1/k_1, k=k_1)
\]</span></p>
<ul>
<li>The posterior concentration parameter is updated to <span class="math inline">\(k_1 = k_0+n\)</span>
</li>
<li>The posterior mean parameter update follows the update:
<span class="math display">\[
\frac{1}{w_1} =  \lambda \frac{1}{w_0}  + (1-\lambda)  \frac{1}{\hat{w}_{ML}}
\]</span>
with <span class="math inline">\(\lambda = \frac{k_0}{k_1}\)</span>.
Crucially, the linear update is applied to the inverse of the precision
but <strong>not</strong> to the precision itself. This is because the MLE of the precision parameter cannot be expressed as an average.</li>
<li>Equivalent update rules are for the inverse scale parameter <span class="math inline">\(s^2\)</span>
<span class="math display">\[
\frac{1}{s^2_1} =  \frac{1}{s^2_0}  + n \widehat{\sigma^2}_{ML}
\]</span>
and for the rate parameter <span class="math inline">\(\beta = 1/(2 s^2)\)</span> of the gamma distribution
<span class="math display">\[
\beta_1 =  \beta_0 + \frac{n}{2} \widehat{\sigma^2}_{ML}
\]</span>
This is the form you will find most often in textbooks.</li>
</ul>
<p>The posterior mean is
<span class="math display">\[
\text{E}(w | D) = w_1
\]</span>
and the posterior variance
<span class="math display">\[
\text{Var}(w | D) = 2 w_1^2/ k_1
\]</span></p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></div>
<div class="next"><a href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#bayesian-learning-in-practise"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li>
<a class="nav-link" href="#estimating-a-proportion-using-the-beta-binomial-model"><span class="header-section-number">10.1</span> Estimating a proportion using the beta-binomial model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#binomial-likelihood"><span class="header-section-number">10.1.1</span> Binomial likelihood</a></li>
<li><a class="nav-link" href="#beta-prior-distribution"><span class="header-section-number">10.1.2</span> Beta prior distribution</a></li>
<li><a class="nav-link" href="#computing-the-posterior-distribution"><span class="header-section-number">10.1.3</span> Computing the posterior distribution</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#properties-of-bayesian-learning"><span class="header-section-number">10.2</span> Properties of Bayesian learning</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#prior-acting-as-pseudodata"><span class="header-section-number">10.2.1</span> Prior acting as pseudodata</a></li>
<li><a class="nav-link" href="#linear-shrinkage-of-mean"><span class="header-section-number">10.2.2</span> Linear shrinkage of mean</a></li>
<li><a class="nav-link" href="#conjugacy-of-prior-and-posterior-distribution"><span class="header-section-number">10.2.3</span> Conjugacy of prior and posterior distribution</a></li>
<li><a class="nav-link" href="#large-sample-limits-of-mean-and-variance"><span class="header-section-number">10.2.4</span> Large sample limits of mean and variance</a></li>
<li><a class="nav-link" href="#asymptotic-normality-of-the-posterior-distribution"><span class="header-section-number">10.2.5</span> Asymptotic normality of the posterior distribution</a></li>
<li><a class="nav-link" href="#posterior-variance-for-finite-n"><span class="header-section-number">10.2.6</span> Posterior variance for finite \(n\)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#estimating-the-mean-using-the-normal-normal-model"><span class="header-section-number">10.3</span> Estimating the mean using the normal-normal model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#normal-likelihood"><span class="header-section-number">10.3.1</span> Normal likelihood</a></li>
<li><a class="nav-link" href="#normal-prior-distribution"><span class="header-section-number">10.3.2</span> Normal prior distribution</a></li>
<li><a class="nav-link" href="#normal-posterior-distribution"><span class="header-section-number">10.3.3</span> Normal posterior distribution</a></li>
<li><a class="nav-link" href="#large-sample-asymptotics"><span class="header-section-number">10.3.4</span> Large sample asymptotics</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#estimating-the-variance-using-the-inverse-gamma-normal-model"><span class="header-section-number">10.4</span> Estimating the variance using the inverse-gamma-normal model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#normal-likelihood-1"><span class="header-section-number">10.4.1</span> Normal likelihood</a></li>
<li><a class="nav-link" href="#ig-prior-distribution"><span class="header-section-number">10.4.2</span> IG prior distribution</a></li>
<li><a class="nav-link" href="#ig-posterior-distribution"><span class="header-section-number">10.4.3</span> IG posterior distribution</a></li>
<li><a class="nav-link" href="#large-sample-asymptotics-1"><span class="header-section-number">10.4.4</span> Large sample asymptotics</a></li>
<li><a class="nav-link" href="#other-equivalent-update-rules"><span class="header-section-number">10.4.5</span> Other equivalent update rules</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#estimating-the-precision-using-the-gamma-normal-model"><span class="header-section-number">10.5</span> Estimating the precision using the gamma-normal model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#mle-of-the-precision"><span class="header-section-number">10.5.1</span> MLE of the precision</a></li>
<li><a class="nav-link" href="#gamma-wishart-prior"><span class="header-section-number">10.5.2</span> Gamma (Wishart) prior</a></li>
<li><a class="nav-link" href="#gamma-wishart-posterior"><span class="header-section-number">10.5.3</span> Gamma / Wishart posterior</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistics 2: Likelihood and Bayes</strong>" was written by Korbinian Strimmer. It was last built on 13 December 2023.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
