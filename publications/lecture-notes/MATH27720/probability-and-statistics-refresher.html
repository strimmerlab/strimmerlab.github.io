<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>B Probability and statistics refresher | Statistics 2: Likelihood and Bayes</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="B Probability and statistics refresher | Statistics 2: Likelihood and Bayes">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="B Probability and statistics refresher | Statistics 2: Likelihood and Bayes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<meta name="description" content="Below you find a brief overview over some relevant concepts in probability and statistics that you should be familiar with from earlier modules (Probability 1 and 2 as well as Statistics 2)  B.1...">
<meta property="og:description" content="Below you find a brief overview over some relevant concepts in probability and statistics that you should be familiar with from earlier modules (Probability 1 and 2 as well as Statistics 2)  B.1...">
<meta name="twitter:description" content="Below you find a brief overview over some relevant concepts in probability and statistics that you should be familiar with from earlier modules (Probability 1 and 2 as well as Statistics 2)  B.1...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistics 2: Likelihood and Bayes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="conditioning-and-bayes-rule.html"><span class="header-section-number">7</span> Conditioning and Bayes rule</a></li>
<li><a class="" href="models-with-latent-variables-and-missing-data.html"><span class="header-section-number">8</span> Models with latent variables and missing data</a></li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li><a class="" href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li><a class="" href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="matrix-and-calculus-refresher.html"><span class="header-section-number">A</span> Matrix and calculus refresher</a></li>
<li><a class="active" href="probability-and-statistics-refresher.html"><span class="header-section-number">B</span> Probability and statistics refresher</a></li>
<li><a class="" href="distribution-refresher.html"><span class="header-section-number">C</span> Distribution refresher</a></li>
<li><a class="" href="further-distributions-used-in-bayesian-analysis.html"><span class="header-section-number">D</span> Further distributions used in Bayesian analysis</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">E</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="probability-and-statistics-refresher" class="section level1" number="15">
<h1>
<span class="header-section-number">B</span> Probability and statistics refresher<a class="anchor" aria-label="anchor" href="#probability-and-statistics-refresher"><i class="fas fa-link"></i></a>
</h1>
<p>Below you find a brief overview over some relevant concepts in probability and statistics
that you should be familiar with from earlier modules (Probability 1 and 2 as well as Statistics 2)</p>
<div id="basic-mathematical-notation" class="section level2" number="15.1">
<h2>
<span class="header-section-number">B.1</span> Basic mathematical notation<a class="anchor" aria-label="anchor" href="#basic-mathematical-notation"><i class="fas fa-link"></i></a>
</h2>
<p>Summation:
<span class="math display">\[
\sum_{i=1}^n x_i = x_1 + x_2 + \ldots + x_n
\]</span></p>
<p>Multiplication:
<span class="math display">\[
\prod_{i=1}^n x_i = x_1 \times x_2 \times \ldots \times x_n
\]</span></p>
<p>Indicator function:
<span class="math display">\[
1_{A} =
\begin{cases}
1 &amp; \text{if $A$ is true}\\
0 &amp; \text{if $A$ is not true}\\
\end{cases}
\]</span></p>
</div>
<div id="combinatorics" class="section level2" number="15.2">
<h2>
<span class="header-section-number">B.2</span> Combinatorics<a class="anchor" aria-label="anchor" href="#combinatorics"><i class="fas fa-link"></i></a>
</h2>
<div id="number-of-permutations" class="section level3" number="15.2.1">
<h3>
<span class="header-section-number">B.2.1</span> Number of permutations<a class="anchor" aria-label="anchor" href="#number-of-permutations"><i class="fas fa-link"></i></a>
</h3>
<p>The number of possible orderings, or permutations, of <span class="math inline">\(n\)</span> distinct items is
the number of ways to put <span class="math inline">\(n\)</span> items in <span class="math inline">\(n\)</span> bins with exactly one item in each bin. It is given by
the factorial
<span class="math display">\[
n! = \prod_{i=1}^n i = 1 \times 2 \times \ldots \times n
\]</span>
where <span class="math inline">\(n\)</span> is a positive integer.
For <span class="math inline">\(n=0\)</span> the factorial is defined as
<span class="math display">\[
0! = 1
\]</span>
as there is exactly one permutation of zero objects.</p>
<p>The factorial can also be obtained using the
<a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>
<span class="math display">\[
\Gamma(x) = \int_0^\infty t^{x-1} e^{-t} dt
\]</span>
which can be viewed as continuous version of the factorial
with
<span class="math inline">\(\Gamma(x) = (x-1)!\)</span> for any positive integer <span class="math inline">\(x\)</span>.</p>
</div>
<div id="multinomial-and-binomial-coefficient" class="section level3" number="15.2.2">
<h3>
<span class="header-section-number">B.2.2</span> Multinomial and binomial coefficient<a class="anchor" aria-label="anchor" href="#multinomial-and-binomial-coefficient"><i class="fas fa-link"></i></a>
</h3>
<p>The number of possible permutation of <span class="math inline">\(n\)</span> items of <span class="math inline">\(K\)</span> distinct types, with <span class="math inline">\(n_1\)</span> of type 1, <span class="math inline">\(n_2\)</span> of type 2 and so on, equals the number of ways
to put <span class="math inline">\(n\)</span> items into <span class="math inline">\(K\)</span> bins with <span class="math inline">\(n_1\)</span> items in the first bin, <span class="math inline">\(n_2\)</span> in the second and so on.
It is given by the <strong>multinomial</strong> coefficient
<span class="math display">\[
\binom{n}{n_1, \ldots, n_K} = \frac {n!}{n_1! \times n_2! \times\ldots \times n_K! }
\]</span>
with <span class="math inline">\(\sum_{k=1}^K n_k = n\)</span> and <span class="math inline">\(K \leq n\)</span>.
Note that it equals the number of permutation of all items divided by the number of permutations of the items in each bin (or of each type).</p>
<p>If all <span class="math inline">\(n_k=1\)</span> and hence <span class="math inline">\(K=n\)</span> the multinomial coefficient reduces to the factorial.</p>
<p>If there are only two bins / types (<span class="math inline">\(K=2\)</span>) the multinomial coefficients becomes the
<strong>binomial coefficient</strong>
<span class="math display">\[
\binom{n}{n_1} = \binom{n}{n_1, n-n_1}    =  \frac {n!}{n_1! (n - n_1)!}
\]</span>
which counts the number of ways to choose <span class="math inline">\(n_1\)</span> elements from a set of <span class="math inline">\(n\)</span> elements.</p>
</div>
<div id="de-moivre-sterling-approximation-of-the-factorial" class="section level3" number="15.2.3">
<h3>
<span class="header-section-number">B.2.3</span> De Moivre-Sterling approximation of the factorial<a class="anchor" aria-label="anchor" href="#de-moivre-sterling-approximation-of-the-factorial"><i class="fas fa-link"></i></a>
</h3>
<p>The factorial is frequently approximated by the following formula derived by <a href="https://en.wikipedia.org/wiki/Abraham_de_Moivre">Abraham de Moivre (1667–1754)</a> and <a href="https://en.wikipedia.org/wiki/James_Stirling_(mathematician)">James Stirling (1692-1770)</a>
<span class="math display">\[
n! \approx \sqrt{2 \pi} n^{n+\frac{1}{2}} e^{-n}
\]</span>
or equivalently on logarithmic scale
<span class="math display">\[
\log n!  \approx \left(n+\frac{1}{2}\right) \log n  -n + \frac{1}{2}\log \left( 2 \pi\right)
\]</span>
The approximation is good for small <span class="math inline">\(n\)</span> (but fails for <span class="math inline">\(n=0\)</span>) and becomes
more and more accurate with increasing <span class="math inline">\(n\)</span>. For large <span class="math inline">\(n\)</span> the approximation can be simplified to
<span class="math display">\[
\log n! \approx  n \log n  -n
\]</span></p>
</div>
</div>
<div id="probability" class="section level2" number="15.3">
<h2>
<span class="header-section-number">B.3</span> Probability<a class="anchor" aria-label="anchor" href="#probability"><i class="fas fa-link"></i></a>
</h2>
<div id="random-variables" class="section level3" number="15.3.1">
<h3>
<span class="header-section-number">B.3.1</span> Random variables<a class="anchor" aria-label="anchor" href="#random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>A <strong>random variable</strong> describes a random experiment. The set of all possible outcomes
is the <strong>sample space</strong> or <strong>state space</strong> of the random variable and is denoted by
<span class="math inline">\(\Omega = \{\omega_1, \omega_2, \ldots\}\)</span>. The outcomes <span class="math inline">\(\omega_i\)</span> are the <strong>elementary events</strong>.
The sample space <span class="math inline">\(\Omega\)</span> can be finite or infinite. Depending on type of outcomes
the random variable is <strong>discrete</strong> or <strong>continuous</strong>.</p>
<p>An event <span class="math inline">\(A \subseteq \Omega\)</span> is a subset of <span class="math inline">\(\Omega\)</span> and thus itself a set composed of elementary events: <span class="math inline">\(A = \{a_1, a_2, \ldots\}\)</span>.
This includes as special cases the full set <span class="math inline">\(A = \Omega\)</span>, the empty set <span class="math inline">\(A = \emptyset\)</span>, and the elementary
events <span class="math inline">\(A=\omega_i\)</span>. The complementary event <span class="math inline">\(A^C\)</span> is the complement of the set <span class="math inline">\(A\)</span> in the set <span class="math inline">\(\Omega\)</span>
so that <span class="math inline">\(A^C = \Omega \setminus A = \{\omega_i \in \Omega: \omega_i \notin A\}\)</span>.</p>
<p>The probability of an event <span class="math inline">\(A\)</span> is denoted by <span class="math inline">\(\text{Pr}(A)\)</span>.
Essentially, to obtain this probability we need to count the
elementary elements corresponding to <span class="math inline">\(A\)</span>. To do this
we assume as <a href="https://en.wikipedia.org/wiki/Probability_axioms">axioms of probability</a> that</p>
<ul>
<li>
<span class="math inline">\(\text{Pr}(A) \geq 0\)</span>, probabilities are positive,</li>
<li>
<span class="math inline">\(\text{Pr}(\Omega) = 1\)</span>, the certain event has probability 1, and</li>
<li>
<span class="math inline">\(\text{Pr}(A) = \sum_{a_i \in A} \text{Pr}(a_i)\)</span>, the probability of
an event equals the sum of its constituting elementary events <span class="math inline">\(a_i\)</span>.
This sum is taken over a finite or countable infinite number of elements.</li>
</ul>
<p>This implies</p>
<ul>
<li>
<span class="math inline">\(\text{Pr}(A) \leq 1\)</span>, i.e. probabilities all lie in the interval <span class="math inline">\([0,1]\)</span>
</li>
<li>
<span class="math inline">\(\text{Pr}(A^C) = 1 - \text{Pr}(A)\)</span>, and</li>
<li><span class="math inline">\(\text{Pr}(\emptyset) = 0\)</span></li>
</ul>
<p>Assume now that we have two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.
The probability of the event “<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>” is then given by the probability of the set intersection
<span class="math inline">\(\text{Pr}(A \cap B)\)</span>.
Likewise the probability of the event “<span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>” is given by the probability of the set union
<span class="math inline">\(\text{Pr}(A \cup B)\)</span>.</p>
<p>From the above it is clear that the definition and theory of probability is closely linked to set theory, and in particular to measure theory. Indeed, viewing probability as a special type of measure allows for an elegant treatment of both discrete and continuous random variables
(but one which is out of scope of this module).</p>
</div>
<div id="probability-mass-and-density-function-distribution-function-and-quantile-function" class="section level3" number="15.3.2">
<h3>
<span class="header-section-number">B.3.2</span> Probability mass and density function, distribution function and quantile function<a class="anchor" aria-label="anchor" href="#probability-mass-and-density-function-distribution-function-and-quantile-function"><i class="fas fa-link"></i></a>
</h3>
<p>To describe a random variable <span class="math inline">\(x\)</span> with state space <span class="math inline">\(\Omega\)</span> we need a way to effectively store the probabilities of the corresponding elementary outcomes <span class="math inline">\(x \in \Omega\)</span>. Note that for convenience we use the same symbol to denote the random variable and its elementary outcomes.</p>
<p>For a discrete random variable we define the
event <span class="math inline">\(A = \{x: x=a\} = \{a\}\)</span> and get the probability
<span class="math display">\[
\text{Pr}(A) = \text{Pr}(x=a) = f(a)
\]</span>
directly from the <strong>probability mass function</strong> (PMF), here denoted by lower case <span class="math inline">\(f\)</span>
(but we frequently also use <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span>).
The PMF has the property that <span class="math inline">\(\sum_{x \in \Omega} f(x) = 1\)</span> and that
<span class="math inline">\(f(x) \in [0,1]\)</span>.</p>
<p>For continuous random variables we need to use a <strong>probability density function</strong> (PDF) instead. We define the event
<span class="math inline">\(A = \{x: a &lt; x \leq a + da\}\)</span> as an infinitesimal interval
and then assign the probability
<span class="math display">\[
\text{Pr}(A) = \text{Pr}( a &lt; x \leq a + da) = f(a) da \,.
\]</span>
The PDF has the property that <span class="math inline">\(\int_{x \in \Omega} f(x) dx = 1\)</span>
but in contrast to a PMF the density <span class="math inline">\(f(x)\geq 0\)</span> may take on values larger than 1.</p>
<p>As alternative to using PMF/PDFs we may also use a <strong>distribution function</strong> to describe the random variable. This assumes an ordering exist among the elementary events so that we can define the event <span class="math inline">\(A = \{x: x \leq a \}\)</span> and compute its
probability as
<span class="math display">\[
F(a) = \text{Pr}(A) = \text{Pr}( x \leq a ) =
\begin{cases}
\sum_{x \in A} f(x) &amp; \text{discrete case} \\
\int_{x \in A} f(x) dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
This is also known <strong>cumulative distribution function</strong> (CDF)
and is denoted by upper case <span class="math inline">\(F\)</span> (or <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>).
By construction the distribution function is monotonically non-decreasing and its value ranges from 0 to 1.
With its help we can compute the probability of an interval set
such as
<span class="math display">\[
\text{Pr}( a &lt; x \leq b ) = F(b)-F(a) \,.
\]</span></p>
<p>The inverse of the distribution function <span class="math inline">\(y=F(x)\)</span> is the <strong>quantile function</strong> <span class="math inline">\(x=F^{-1}(y)\)</span>.
The 50% quantile <span class="math inline">\(F^{-1}\left(\frac{1}{2}\right)\)</span> is the <strong>median</strong>.</p>
<p>If the random variable <span class="math inline">\(x\)</span> has distribution function <span class="math inline">\(F\)</span> we write <span class="math inline">\(x \sim F\)</span>.</p>
<div class="inline-figure"><img src="fig/refresher_dens-dist.png" width="100%" style="display: block; margin: auto;"></div>
</div>
<div id="expectation-of-a-random-variable" class="section level3" number="15.3.3">
<h3>
<span class="header-section-number">B.3.3</span> Expectation of a random variable<a class="anchor" aria-label="anchor" href="#expectation-of-a-random-variable"><i class="fas fa-link"></i></a>
</h3>
<p>The expected value <span class="math inline">\(\text{E}(x)\)</span> of a random variable is defined as
the weighted average over all possible outcomes, with the weight given by the PMF / PDF <span class="math inline">\(f(x)\)</span>:
<span class="math display">\[
\text{E}_{F}(x) =
\begin{cases}
\sum_{x \in \Omega} f(x) x &amp; \text{discrete case} \\
\int_{x \in \Omega} f(x) x dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
Note the notation to emphasise that the expectation is taken with regard to the distribution <span class="math inline">\(F\)</span>. The subscript <span class="math inline">\(F\)</span> is usually left out if
there are no ambiguities.
Furthermore, because the sum or integral may diverge
the expectation is not necessarily always defined (in contrast to quantiles).</p>
<p>The expected value of a function of a random variable <span class="math inline">\(h(x)\)</span> is
obtained similarly:
<span class="math display">\[
\text{E}_{F}(h(x)) =
\begin{cases}
\sum_{x \in \Omega} f(x) h(x) &amp; \text{discrete case} \\
\int_{x \in \Omega} f(x) h(x) dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
This is called the <a href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician">“law of the unconscious statistician”</a>, or short LOTUS.
Again, to highlight that the random variable <span class="math inline">\(x\)</span> has distribution <span class="math inline">\(F\)</span> we
write <span class="math inline">\(\text{E}_F(h(x))\)</span>.</p>
</div>
<div id="jensens-inequality-for-the-expectation" class="section level3" number="15.3.4">
<h3>
<span class="header-section-number">B.3.4</span> Jensen’s inequality for the expectation<a class="anchor" aria-label="anchor" href="#jensens-inequality-for-the-expectation"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(h(\boldsymbol x)\)</span> is a <em>convex</em> function then the following
inequality holds:</p>
<p><span class="math display">\[
\text{E}(h(\boldsymbol x)) \geq h(\text{E}(\boldsymbol x))
\]</span></p>
<p>Recall: a con<strong>ve</strong>x function (such as <span class="math inline">\(x^2\)</span>) has the shape of a “<strong>va</strong>lley”.</p>
</div>
<div id="probability-as-expectation" class="section level3" number="15.3.5">
<h3>
<span class="header-section-number">B.3.5</span> Probability as expectation<a class="anchor" aria-label="anchor" href="#probability-as-expectation"><i class="fas fa-link"></i></a>
</h3>
<p>Probability itself can also be understood as an expectation.
For an event <span class="math inline">\(A\)</span> we can define a corresponding indicator function
<span class="math inline">\(1_{ x \in A}\)</span> for an elementary element <span class="math inline">\(x\)</span> to be part of <span class="math inline">\(A\)</span>.
From the above it then follows
<span class="math display">\[
\text{E}( 1_{x \in A} ) = \text{Pr}(A) \, ,
\]</span></p>
<p>Interestingly, one can develop the whole theory of probability from this perspective. <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Whittle, P. 2000. Probability via Expectation (3rd ed.). Springer. &lt;a href="https://doi.org/10.1007/978-1-4612-0509-8" class="uri"&gt;https://doi.org/10.1007/978-1-4612-0509-8&lt;/a&gt;&lt;/p&gt;'><sup>19</sup></a></p>
</div>
<div id="moments-and-variance-of-a-random-variable" class="section level3" number="15.3.6">
<h3>
<span class="header-section-number">B.3.6</span> Moments and variance of a random variable<a class="anchor" aria-label="anchor" href="#moments-and-variance-of-a-random-variable"><i class="fas fa-link"></i></a>
</h3>
<p>The moments of a random variable are defined as follows:</p>
<ul>
<li>Zeroth moment: <span class="math inline">\(\text{E}(x^0) = 1\)</span> by construction of PDF and PMF,</li>
<li>First moment: <span class="math inline">\(\text{E}(x^1) = \text{E}(x) = \mu\)</span> , the mean,</li>
<li>Second moment: <span class="math inline">\(\text{E}(x^2)\)</span>
</li>
<li>The variance is the second moment centred about the mean <span class="math inline">\(\mu\)</span>:
<span class="math display">\[\text{Var}(x) = \text{E}( (x - \mu)^2 ) = \sigma^2\]</span>
</li>
<li>The variance can also be computed by <span class="math inline">\(\text{Var}(x) = \text{E}(x^2)-\text{E}(x)^2\)</span>. Note this is an example of Jensen’s inequality,
with <span class="math inline">\(\text{E}(x^2) =\text{E}(x)^2 + \text{Var}(x) \geq \text{E}(x)^2\)</span>.</li>
</ul>
<p>A distribution does not necessarily need to have any finite first or higher moments.
An example is the <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a> that does not have a mean or variance (or any other higher moment).</p>
</div>
<div id="distribution-of-sums-of-random-variables" class="section level3" number="15.3.7">
<h3>
<span class="header-section-number">B.3.7</span> Distribution of sums of random variables<a class="anchor" aria-label="anchor" href="#distribution-of-sums-of-random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>The sum of two normal random variables is also normal (with the appropriate mean and variance).</p>
<p>The <strong>central limit theorem</strong>, first postulated by <a href="https://en.wikipedia.org/wiki/Abraham_de_Moivre">Abraham de Moivre (1667–1754)</a>, asserts
that in many cases the distribution of the sum of identically distributed random variables converges to a normal distribution, even if the individual random variables are not normal.</p>
<p>For example, as a result the binomial distribution (as sum of Bernoulli random variables) can be approximated by a normal distribution.</p>
</div>
<div id="transformation-of-random-variables" class="section level3" number="15.3.8">
<h3>
<span class="header-section-number">B.3.8</span> Transformation of random variables<a class="anchor" aria-label="anchor" href="#transformation-of-random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>Linear transformation of random variables: if <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants and <span class="math inline">\(x\)</span> is a random variable, then the random variable <span class="math inline">\(y= a + b x\)</span> has mean <span class="math inline">\(\text{E}(y) = a + b \text{E}(x)\)</span> and variance <span class="math inline">\(\text{Var}(y) = b^2 \text{Var}(x)\)</span>.</p>
<p>For a general invertible coordinate transformation <span class="math inline">\(y = h(x) = y(x)\)</span> the backtransformation is <span class="math inline">\(x = h^{-1}(y) = x(y)\)</span>.</p>
<p>The transformation of the infinitesimal volume element is <span class="math inline">\(dy = \left|\frac{dy}{dx}\right| dx\)</span>.</p>
<p>The transformation of the density is <span class="math inline">\(f_y(y) =\left|\frac{dx}{dy}\right| f_x(x(y))\)</span>.</p>
<p>Note that <span class="math inline">\(\left|\frac{dx}{dy}\right| = \left|\frac{dy}{dx}\right|^{-1}\)</span>.</p>
</div>
<div id="random-vectors-and-covariance-matrix" class="section level3" number="15.3.9">
<h3>
<span class="header-section-number">B.3.9</span> Random vectors and covariance matrix<a class="anchor" aria-label="anchor" href="#random-vectors-and-covariance-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>Instead of scalar random variables one often also considers random vectors and also random matrices.</p>
<p>For a random vector <span class="math inline">\(\boldsymbol x= (x_1, x_2,...,x_d)^T\)</span> the mean <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \mu\)</span> is simply comprised of the means of its components, i.e. <span class="math inline">\(\boldsymbol \mu= (\mu_1, \ldots, \mu_d)^T\)</span>. Thus, the mean of a random vector of dimension is a vector of of the same length.</p>
<p>The variance of a random vector of length <span class="math inline">\(d\)</span>, however, is not a vector but a matrix of size <span class="math inline">\(d\times d\)</span>. This matrix is called the <strong>covariance matrix</strong>:
<span class="math display">\[
\begin{split}
\text{Var}(\boldsymbol x) &amp;= \underbrace{\boldsymbol \Sigma}_{d\times d} = (\sigma_{ij}) = \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix} \\
  &amp;=\text{E}\left(\underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d\times 1} \underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1\times d}\right) \\
  &amp; = \text{E}(\boldsymbol x\boldsymbol x^T)-\boldsymbol \mu\boldsymbol \mu^T \\
\end{split}
\]</span>
The entries of the covariance matrix <span class="math inline">\(\sigma_{ij} =\text{Cov}(x_i, x_j)\)</span> describe the covariance between the random variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>. The covariance matrix is symmetric, hence <span class="math inline">\(\sigma_{ij}=\sigma_{ji}\)</span>. The diagonal entries <span class="math inline">\(\sigma_{ii} = \text{Cov}(x_i, x_i) = \text{Var}(x_i) = \sigma_i^2\)</span> correspond to the variances of the components of <span class="math inline">\(\boldsymbol x\)</span>. The covariance matrix is by construction <strong>positive semi-definite</strong>, i.e. the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span> are all positive or equal to zero.</p>
<p>However, wherever possible one will aim to use models with non-singular covariance matrices, with all eigenvalues positive, so that the covariance matrix is invertible.</p>
<p>For univariate <span class="math inline">\(x\)</span> and scalar constant <span class="math inline">\(a\)</span> the variance of <span class="math inline">\(a x\)</span> equals <span class="math inline">\(\text{Var}(a x) = a^2 \text{Var}(x)\)</span>. For a random vector <span class="math inline">\(\boldsymbol x\)</span> of dimension <span class="math inline">\(d\)</span> and constant matrix <span class="math inline">\(\boldsymbol A\)</span> of dimension <span class="math inline">\(m \times d\)</span> this generalises to <span class="math inline">\(\text{Var}(\boldsymbol Ax) = \boldsymbol A\text{Var}(\boldsymbol x) \boldsymbol A^T\)</span>.</p>
</div>
<div id="correlation-matrix" class="section level3" number="15.3.10">
<h3>
<span class="header-section-number">B.3.10</span> Correlation matrix<a class="anchor" aria-label="anchor" href="#correlation-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>A covariance matrix can be factorised into the product
<span class="math display">\[
\boldsymbol \Sigma= \boldsymbol V^{\frac{1}{2}} \boldsymbol P\boldsymbol V^{\frac{1}{2}}
\]</span>
where <span class="math inline">\(\boldsymbol V\)</span> is a diagonal matrix containing the variances
<span class="math display">\[
\boldsymbol V= \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}
\]</span>
and the matrix <span class="math inline">\(\boldsymbol P\)</span> (“upper case rho”) is the symmetric <strong>correlation matrix</strong>
<span class="math display">\[
\boldsymbol P= (\rho_{ij}) = \begin{pmatrix}
    1 &amp; \dots &amp; \rho_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \rho_{d1} &amp; \dots &amp; 1
\end{pmatrix}   = \boldsymbol V^{-\frac{1}{2}} \boldsymbol \Sigma\boldsymbol V^{-\frac{1}{2}}
\]</span>
Thus, the correlation between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> is defined as
<span class="math display">\[
\rho_{ij} = \text{Cor}(x_i,x_j) = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}
\]</span></p>
</div>
</div>
<div id="statistics" class="section level2" number="15.4">
<h2>
<span class="header-section-number">B.4</span> Statistics<a class="anchor" aria-label="anchor" href="#statistics"><i class="fas fa-link"></i></a>
</h2>
<div id="statistical-learning" class="section level3" number="15.4.1">
<h3>
<span class="header-section-number">B.4.1</span> Statistical learning<a class="anchor" aria-label="anchor" href="#statistical-learning"><i class="fas fa-link"></i></a>
</h3>
<p>The aim in statistics — data science — machine learning is to use data
(from experiments, observations, measurements) to learn about and understand the world
using models. In statistics we employ probabilistic models.</p>
<p>Let denote data by <span class="math inline">\(D =\{x_1, \ldots, x_n\}\)</span> and models by <span class="math inline">\(p(x| \theta)\)</span> where <span class="math inline">\(\theta\)</span> represents
the parameters of the model. Often (but not always) <span class="math inline">\(\theta\)</span> can be interpreted as and/or is
associated with some manifest property of the model.
If there is only a single parameter we write <span class="math inline">\(\theta\)</span> (scalar parameter).
If we wish to highlight that there are multiple parameters we write <span class="math inline">\(\boldsymbol \theta\)</span> (in bold type).</p>
<p>Specifically, our aim is to identify the best model(s) for the data in order to both</p>
<ul>
<li>explain the current data, and</li>
<li>to enable good prediction of future data.</li>
</ul>
<p>Note that it is generally easy to find one or several models that explain the data but these
then often do not predict well.</p>
<p>Therefore, one would like to avoid <strong>overfitting</strong> the data and identify models that are
appropriate for the data at hand (i.e. not too simple but also not too complex).</p>
<p>Typically, we focus the analysis to a specific model family with a some parameter <span class="math inline">\(\theta\)</span>.<br>
An <strong>estimator for <span class="math inline">\(\theta\)</span></strong> is a function <span class="math inline">\(\hat{\theta}(D)\)</span> of the data
that maps the data (input) to an informed guess (output) about <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li>A <strong>point estimator</strong> provides a single number for each parameter</li>
<li>An <strong>interval estimator</strong> provides a set of possible values for each parameter.</li>
</ul>
<p>Interval estimators can be linked to the concept of testing specified values
for a parameter. Specfically a confidence interval contains all parameter values that are not
significantly different from the best parameter.</p>
</div>
<div id="sampling-properties-of-a-point-estimator-hatboldsymbol-theta" class="section level3" number="15.4.2">
<h3>
<span class="header-section-number">B.4.2</span> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span><a class="anchor" aria-label="anchor" href="#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fas fa-link"></i></a>
</h3>
<p>A point estimator <span class="math inline">\(\hat\theta\)</span> depends on the data, hence it exibits <strong>sampling variation</strong>, i.e. estimate will be different for a new set of observations.</p>
<p>Thus <span class="math inline">\(\hat\theta\)</span> can be seen as a random variable, and its distribution is called <strong>sampling distribution</strong> (across different experiments).</p>
<p>Properties of this distribution can be used to evaluate how far the estimator
deviates (on average across different experiments) from the true value:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{rr}
\text{Bias:}\\
\text{Variance:}\\
\text{Mean squared error:}\\
\\
\end{array}
\begin{array}{rr}
\text{Bias}(\hat{\theta})\\
\text{Var}(\hat{\theta})\\
\text{MSE}(\hat{\theta})\\
\\
\end{array}
\begin{array}{ll}
=\text{E}(\hat{\theta})-\theta\\
=\text{E}\left((\hat{\theta}-\text{E}(\hat{\theta}))^2\right)\\
=\text{E}((\hat{\theta}-\theta)^2)\\
=\text{Var}(\hat{\theta})+\text{Bias}(\hat{\theta})^2\\
\end{array}
\end{align*}\]</span></p>
<p>The last identity about MSE follows from <span class="math inline">\(\text{E}(x^2)=\text{Var}(x)+\text{E}(x)^2\)</span>.</p>
<p>At first sight it seems desirable to focus on unbiased (for finite <span class="math inline">\(n\)</span>) estimators.
However, requiring strict unbiasedness is not always a good idea.
In many situations it is better to allow for some small bias and in order to achieve a smaller variance
and an overall total smaller MSE. This is called <strong>bias-variance tradeoff</strong> — as more bias
is traded for smaller variance (or, conversely, less bias is traded for higher variance)</p>
</div>
<div id="efficiency-and-consistency-of-an-estimator" class="section level3" number="15.4.3">
<h3>
<span class="header-section-number">B.4.3</span> Efficiency and consistency of an estimator<a class="anchor" aria-label="anchor" href="#efficiency-and-consistency-of-an-estimator"><i class="fas fa-link"></i></a>
</h3>
<p>Typically, <span class="math inline">\(\text{Bias}\)</span>, <span class="math inline">\(\text{Var}\)</span> and <span class="math inline">\(\text{MSE}\)</span> all decrease with increasing sample size
so that with more data <span class="math inline">\(n \to \infty\)</span> the errors become smaller and smaller.</p>
<p><strong>Efficiency</strong>: An estimator <span class="math inline">\(\hat\theta_A\)</span> is said to more efficient than estimator
<span class="math inline">\(\hat\theta_B\)</span> if for same sample size <span class="math inline">\(n\)</span> it has smaller error (e.g. MSE) than
the competing estimator.</p>
<p>The typical rate of decrease in variance of a good estimator is <span class="math inline">\(\frac{1}{n}\)</span>
and the rate of decrease in the standard deviation is <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span>.
Note that this implies that to get one digit more accuracy in an estimate (standard deviation
decreasing by factor of 10) we need 100 times more data!</p>
<p><strong>Consistency</strong>: <span class="math inline">\(\hat{\theta}\)</span> is called consistent if
<span class="math display">\[
\text{MSE}(\hat{\theta}) \longrightarrow 0 \text{ with $n\rightarrow \infty$ }
\]</span></p>
<p>Consistency is an essential but rather weak requirement for any reasonable estimator.
Of all consistent
estimators we typically select the estimators that are most <strong>efficient</strong> (i.e. with fasted decrease in MSE)
and that therefore have smallest variance and/or MSE for given finite <span class="math inline">\(n\)</span>.</p>
<p>Consistency implies we recover the true model in the limit of infinite data if the
model class contains the true data generating model.
If the model class does not contain the true model then strict consistency
cannot be achieved but we still wish to get as close as possible
to the true model when choosing model parameters.</p>
</div>
<div id="empirical-distribution-function" class="section level3" number="15.4.4">
<h3>
<span class="header-section-number">B.4.4</span> Empirical distribution function<a class="anchor" aria-label="anchor" href="#empirical-distribution-function"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we observe data <span class="math inline">\(D=\{x_1, \ldots, x_n\}\)</span> with each <span class="math inline">\(x_i \sim F\)</span>
sampled independently and identically. The empirical cumulative distribution function
<span class="math inline">\(\hat{F}_n(x)\)</span> based on data <span class="math inline">\(D\)</span> is then given by
<span class="math display">\[
\hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n 1_{x_i \leq x}
\]</span></p>
<p>The empirical distribution function is monotonically non-decreasing from 0 to 1 in discrete steps.</p>
<p>In R the empirical distribution function is computed by <code><a href="https://rdrr.io/r/stats/ecdf.html">ecdf()</a></code>.</p>
<p>Crucially, the empirical distribution
<span class="math inline">\(\hat{F}_n\)</span> converges strongly (almost surely) to the
underlying distribution <span class="math inline">\(F\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>:
<span class="math display">\[
\hat{F}_n\overset{a. s.}{\to} F
\]</span>
The <a href="https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem">Glivenko–Cantelli theorem</a> additionally asserts that the convergence is uniform.</p>
<p>Note this is effectively a variant of the <strong>law of large numbers</strong> applied
to the whole distribution, rather than just the mean (see below).</p>
<p>As a result, we may use the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> based on data <span class="math inline">\(D\)</span> as
an estimate of the underlying unknown true distribution <span class="math inline">\(F\)</span>. From the convergence
theorems we know that <span class="math inline">\(\hat{F}_n\)</span> is consistent.</p>
<p>However, for <span class="math inline">\(\hat{F}_n\)</span> to work well as an estimate of <span class="math inline">\(F\)</span> the number of observations <span class="math inline">\(n\)</span> must be
sufficiently large
so that the approximation provided by <span class="math inline">\(\hat{F}_n\)</span> is adequate.</p>
</div>
<div id="empirical-estimators" class="section level3" number="15.4.5">
<h3>
<span class="header-section-number">B.4.5</span> Empirical estimators<a class="anchor" aria-label="anchor" href="#empirical-estimators"><i class="fas fa-link"></i></a>
</h3>
<p>The fact that for large sample size <span class="math inline">\(n\)</span> the empirical distribution <span class="math inline">\(\hat{F}_n\)</span>
may be used as a substitute for the unknown <span class="math inline">\(F\)</span> allows us to easily construct
empirical estimators.</p>
<p>Specifically, parameters of a model can typically be expressed as a
functional of the distribution
<span class="math inline">\(\theta = g(F)\)</span>. An <strong>empirical estimator</strong> <span class="math inline">\(\hat{\theta}\)</span> is constructed by substituting the true distribution by the empirical distribution <span class="math inline">\(\hat{\theta}= g( \hat{F}_n )\)</span>.</p>
<p>An example is the mean <span class="math inline">\(\text{E}_F(x)\)</span> with regard to <span class="math inline">\(F\)</span>. The <strong>empirical mean</strong> is the expectation with regard to the empirical distribution
which equals the <strong>average of the samples</strong>:
<span class="math display">\[
\hat{\text{E}}(x) = \hat{\mu} =  \text{E}_{\hat{F}_n}(x) = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x}
\]</span></p>
<p>Similarly, other empirical estimators can be constructed simply by replacing
the expectation in the definition of the quantity of interest by the sample average.
For example, the <strong>empirical variance</strong> with unknown mean is given by
<span class="math display">\[
\widehat{\text{Var}}(x) = \widehat{\sigma^2} =
\text{E}_{\hat{F}_n}((x - \hat{\mu})^2) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2
\]</span>
Note the factor <span class="math inline">\(1/n\)</span> before the summation sign. We can also write the empirical
variance in terms of
<span class="math inline">\(\overline{x^2} =\frac{1}{n}\sum^{n}_{k=1} x^2\)</span> as
<span class="math display">\[
\widehat{\text{Var}}(x) = \overline{x^2} - \bar{x}^2
\]</span></p>
<p>By construction, as a result of the strong convergence
of <span class="math inline">\(\hat{F}_n\)</span> to <span class="math inline">\(F\)</span> empirical estimators are consistent, with their MSE, variance
and bias all decreasing to zero with large sample size <span class="math inline">\(n\)</span>. However, for
finite sample size they do have a finite variance and may also be biased.</p>
<p>For example, the empirical variance given above is biased with
<span class="math inline">\(\text{Bias}(\widehat{\sigma^2}) = -\sigma^2/n\)</span>. Note this bias decreases with <span class="math inline">\(n\)</span>.
An unbiased estimator
can be obtained by rescaling the empirical estimator by the factor
<span class="math inline">\(n/(n-1)\)</span>:
<span class="math display">\[
\widehat{\sigma^2}_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\bar{x})^2
\]</span></p>
<p>The empirical estimators for the mean and variance can also be obtained
for random vectors <span class="math inline">\(\boldsymbol x\)</span>. In this case the data <span class="math inline">\(D = \{\boldsymbol x_1, \ldots, \boldsymbol x_n \}\)</span>
is comprised of <span class="math inline">\(n\)</span> vector-valued observations.</p>
<p>For the mean get
<span class="math display">\[
\hat{\boldsymbol \mu} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k = \bar{\boldsymbol x}
\]</span>
and for the covariance
<span class="math display">\[\widehat{\boldsymbol \Sigma} = \frac{1}{n}\sum^{n}_{k=1} \left(\boldsymbol x_k-\bar{\boldsymbol x}\right) \; \left(\boldsymbol x_k-\bar{\boldsymbol x}\right)^T\]</span>
Note the factor <span class="math inline">\(\frac{1}{n}\)</span> in the estimator of the covariance matrix.</p>
<p>With <span class="math inline">\(\overline{\boldsymbol x\boldsymbol x^T} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k \boldsymbol x_k^T\)</span>
we can also write
<span class="math display">\[
\widehat{\boldsymbol \Sigma} = \overline{\boldsymbol x\boldsymbol x^T} - \bar{\boldsymbol x} \bar{\boldsymbol x}^T
\]</span></p>
</div>
<div id="law-of-large-numbers" class="section level3" number="15.4.6">
<h3>
<span class="header-section-number">B.4.6</span> Law of large numbers<a class="anchor" aria-label="anchor" href="#law-of-large-numbers"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>law of large numbers</strong> was discovered by <a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Jacob Bernoulli (1655-1705)</a> and states that the average
converges to the mean.</p>
<p>Since <span class="math inline">\(\hat{F}_n\)</span> convergences strongly to <span class="math inline">\(F\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>
there is corresponding convergence of
the average
<span class="math inline">\(\text{E}_{\hat{F}_n}(h(x)) = \frac{1}{n} \sum_{i=1}^n h(x_i)\)</span> to the expectation <span class="math inline">\(\text{E}_{F}(h(x))\)</span>.</p>
<p>In other words, if the mean exists then for sufficiently large <span class="math inline">\(n\)</span> it can
be substituted by the empirical mean.</p>
<p>Likewise, the law of large numbers can be applied to empirical estimators
to show that they will converge to the corresponding true quantities for sufficiently large <span class="math inline">\(n\)</span>.</p>
<p>Furthermore, one may use the law of large numbers as a <strong>justification to interpret large-sample limits of frequencies as probabilities</strong>. However, <strong>the converse</strong> , namely requesting that all probabilities must have a frequentist interpretation, <strong>does not follow</strong> from the law of large numbers or from the axioms of probability.</p>
<p>Finally, it is worth pointing out that the law of large number says nothing
about the finite sample properties of estimators.</p>
</div>
<div id="sampling-distribution-of-mean-and-variance-estimators-for-normal-data" class="section level3" number="15.4.7">
<h3>
<span class="header-section-number">B.4.7</span> Sampling distribution of mean and variance estimators for normal data<a class="anchor" aria-label="anchor" href="#sampling-distribution-of-mean-and-variance-estimators-for-normal-data"><i class="fas fa-link"></i></a>
</h3>
<p>If the underlying distribution family of <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> is
known we can often obtain the exact distribution of an estimator.</p>
<p>For example, assuming normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span> we can derive
the sampling distribution for the empirical mean and variance:</p>
<ul>
<li><p>The empirical estimator of the mean parameter <span class="math inline">\(\mu\)</span> is given by <span class="math inline">\(\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i\)</span>. Under the normal assumption the distribution of <span class="math inline">\(\hat{\mu}\)</span> is
<span class="math display">\[
\hat{\mu} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
\]</span>
Thus <span class="math inline">\(\text{E}(\hat{\mu}) = \mu\)</span> and <span class="math inline">\(\text{Var}(\hat{\mu}) = \frac{\sigma^2}{n}\)</span>.
The estimate <span class="math inline">\(\hat{\mu}\)</span> is unbiased as <span class="math inline">\(\text{E}(\hat{\mu})-\mu = 0\)</span>. The mean
squared error of <span class="math inline">\(\hat{\mu}\)</span> is <span class="math inline">\(\text{MSE}(\hat{\mu}) = \frac{\sigma^2}{n}\)</span>.</p></li>
<li><p>The empirical variance <span class="math inline">\(\widehat{\sigma^2} = \frac{1}{n} \sum_{i=1}^n (x_i -\bar{x})^2\)</span> for normal data follows a one-dimensional Wishart distribution
<span class="math display">\[
\widehat{\sigma^2} \sim
W_1\left(s^2 = \frac{\sigma^2}{n}, k=n-1\right)
\]</span>
Thus, <span class="math inline">\(\text{E}( \widehat{\sigma^2} ) = \frac{n-1}{n}\sigma^2\)</span> and
<span class="math inline">\(\text{Var}( \widehat{\sigma^2}_{\text{ML}} ) = \frac{2(n-1)}{n^2}\sigma^4\)</span>.
The estimate <span class="math inline">\(\widehat{\sigma^2}\)</span> is biased since
<span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{ML}} )-\sigma^2 = -\frac{1}{n}\sigma^2\)</span>.
The mean squared error is <span class="math inline">\(\text{MSE}( \widehat{\sigma^2}) = \frac{2(n-1)}{n^2}\sigma^4 +\frac{1}{n^2}\sigma^4 =\frac{2 n-1}{n^2}\sigma^4\)</span>.</p></li>
<li>
<p>The unbiased variance estimate <span class="math inline">\(\widehat{\sigma^2}_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\bar{x})^2\)</span> for normal data follows a one-dimensional Wishart distribution
<span class="math display">\[
\widehat{\sigma^2}_{\text{UB}} \sim
W_1\left(s^2 = \frac{\sigma^2}{n-1}, k = n-1 \right)
\]</span>
Thus, <span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{UB}} ) = \sigma^2\)</span> and
<span class="math inline">\(\text{Var}( \widehat{\sigma^2}_{\text{UB}} ) = \frac{2}{n-1}\sigma^4\)</span>.
The estimate <span class="math inline">\(\widehat{\sigma^2}_{\text{ML}}\)</span> is unbiased since
<span class="math inline">\(\text{E}( \widehat{\sigma^2}_{\text{UB}} )-\sigma^2 =0\)</span>.
The mean squared error is <span class="math inline">\(\text{MSE}( \widehat{\sigma^2}_{\text{UB}} ) =\frac{2}{n-1}\sigma^4\)</span>.</p>
<p>Interestingly, for any <span class="math inline">\(n&gt;1\)</span> we find that <span class="math inline">\(\text{Var}( \widehat{\sigma^2}_{\text{UB}} ) &gt; \text{Var}( \widehat{\sigma^2}_{\text{ML}} )\)</span> and <span class="math inline">\(\text{MSE}( \widehat{\sigma^2}_{\text{UB}} ) &gt; \text{MSE}( \widehat{\sigma^2}_{\text{ML}} )\)</span> so that the biased empirical estimator has both lower variance and lower mean squared error than the unbiased estimator.</p>
</li>
</ul>
</div>
<div id="one-sample-t-statistic" class="section level3" number="15.4.8">
<h3>
<span class="header-section-number">B.4.8</span> One sample <span class="math inline">\(t\)</span>-statistic<a class="anchor" aria-label="anchor" href="#one-sample-t-statistic"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we observe <span class="math inline">\(n\)</span> independent data points <span class="math inline">\(x_1, \ldots, x_n \sim N(\mu, \sigma^2)\)</span>.
Then the average <span class="math inline">\(\bar{x} = \sum_{i=1}^n x_i\)</span> is distributed as
<span class="math inline">\(\bar{x} \sim N(\mu, \sigma^2/n)\)</span> and correspondingly
<span class="math display">\[
z = \frac{\bar{x}-\mu}{\sqrt{\sigma^2/n}} \sim N(0, 1)
\]</span></p>
<p>Note that <span class="math inline">\(z\)</span> uses the known variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>If the variance is unknown and is estimated
by the unbiased <span class="math inline">\(s^2_{\text{UB}} = \frac{1}{n-1} \sum_{i=1}^n (x_i -\bar{x})^2\)</span>
then one arrives at the one sample <span class="math inline">\(t\)</span>-statistic
<span class="math display">\[
t_{\text{UB}} = \frac{\bar{x}-\mu}{\sqrt{s^2_{\text{UB}}/n}} \sim t_{n-1} \,.
\]</span>
It is distributed according to a Student’s <span class="math inline">\(t\)</span>-distribution
with <span class="math inline">\(n-1\)</span> degrees of freedom, with mean 0 for <span class="math inline">\(n&gt;2\)</span> and variance
<span class="math inline">\((n-1)/(n-3)\)</span> for <span class="math inline">\(n&gt;3\)</span>.</p>
<p>If instead of the unbiased estimate the empirical (ML) estimate of the variance <span class="math inline">\(s^2_{\text{ML}} = \frac{1}{n} \sum_{i=1}^n (x_i -\bar{x})^2 = \frac{n-1}{n} s^2_{\text{UB}}\)</span> is used then this leads to a slightly different statistic
<span class="math display">\[
t_{\text{ML}} = \frac{\bar{x}-\mu}{ \sqrt{ s^2_{\text{ML}}/n}}  = \sqrt{\frac{n}{n-1}} t_{\text{UB}}
\]</span>
with
<span class="math display">\[
t_{\text{ML}} \sim \text{lst}\left(0, \tau^2=\frac{n}{n-1}, n-1\right)
\]</span>
Thus, <span class="math inline">\(t_{\text{ML}}\)</span> follows a location-scale <span class="math inline">\(t\)</span>-distribution, with mean 0 for <span class="math inline">\(n&gt;2\)</span> and variance
<span class="math inline">\(n/(n-3)\)</span> for <span class="math inline">\(n&gt;3\)</span>.</p>
</div>
<div id="two-sample-t-statistic-with-common-variance" class="section level3" number="15.4.9">
<h3>
<span class="header-section-number">B.4.9</span> Two sample <span class="math inline">\(t\)</span>-statistic with common variance<a class="anchor" aria-label="anchor" href="#two-sample-t-statistic-with-common-variance"><i class="fas fa-link"></i></a>
</h3>
<p>Now suppose we observe normal data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> from two groups
with sample size <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> (and <span class="math inline">\(n=n_1+n_2\)</span>) with two different means <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> and common variance <span class="math inline">\(\sigma^2\)</span>:
<span class="math display">\[x_1,\dots,x_{n_1} \sim N(\mu_1, \sigma^2)\]</span>
and
<span class="math display">\[x_{n_1+1},\dots,x_{n} \sim N(\mu_2, \sigma^2)\]</span>
Then <span class="math inline">\(\hat{\mu}_1 = \frac{1}{n_1}\sum^{n_1}_{i=1}x_i\)</span> and
<span class="math inline">\(\hat{\mu}_2 = \frac{1}{n_2}\sum^{n}_{i=n_1+1}x_i\)</span> are the sample averages within each group.</p>
<p>The common variance <span class="math inline">\(\sigma^2\)</span> may be estimated either by
the unbiased estimate <span class="math inline">\(s^2_{\text{UB}} = \frac{1}{n-2} \left(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)^2+\sum^n_{i=n_1+1}(x_i-\hat{\mu}_2)^2\right)\)</span>
(note the factor <span class="math inline">\(n-2\)</span>) or by the empirical (ML) estimate <span class="math inline">\(s^2_{\text{ML}} = \frac{1}{n} \left(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)^2+\sum^n_{i=n_1+1}(x_i-\hat{\mu}_2)^2\right) =\frac{n-2}{n} s^2_{\text{UB}}\)</span>. The estimator
for the common variance is a often referred to as <em>pooled variance estimate</em> as information is pooled from two groups to obtain the estimate.</p>
<p>Using the unbiased pooled variance estimate the two sample <span class="math inline">\(t\)</span>-statistic is given by
<span class="math display">\[
t_{\text{UB}} = \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{1}{n_1}+\frac{1}{n_2}\right)  s^2_{\text{UB}}}  }
= \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{n}{n_1 n_2} \right) s^2_{\text{UB}} }  }
\]</span>
In terms of empirical frequencies <span class="math inline">\(\hat{\pi}_1 = \frac{n_1}{n}\)</span> and <span class="math inline">\(\hat{\pi}_2 = \frac{n_2}{n}\)</span>
it can also be written as
<span class="math display">\[
t_{\text{UB}} = \sqrt{n} \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{  \left(\frac{1}{\hat{\pi}_1}+\frac{1}{\hat{\pi}_2}\right) s^2_{\text{UB}} }}
= \sqrt{n\hat{\pi}_1 \hat{\pi}_2} \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ s^2_{\text{UB}}}}
\]</span>
The two sample <span class="math inline">\(t\)</span>-statistic is distributed as
<span class="math display">\[
t_{\text{UB}} \sim t_{n-2}
\]</span>
i.e. according to a Student’s <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, with mean 0 for <span class="math inline">\(n&gt;3\)</span> and variance <span class="math inline">\((n-2)/(n-4)\)</span> for <span class="math inline">\(n&gt;4\)</span>.
Large values of the two sample <span class="math inline">\(t\)</span>-statistic indicates that there are indeed two groups
rather than just one.</p>
<p>The two sample <span class="math inline">\(t\)</span>-statistic using the empirical (ML) pooled estimate of the variance is
<span class="math display">\[
\begin{split}
t_{\text{ML}} &amp; = \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{1}{n_1}+\frac{1}{n_2}\right)  s^2_{\text{ML}}  }   }
= \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{n}{n_1 n_2}\right) s^2_{\text{ML}}  }   }\\
&amp; =\sqrt{n} \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ \left(\frac{1}{\hat{\pi}_1}+\frac{1}{\hat{\pi}_2}\right) s^2_{\text{ML}} }}
= \sqrt{n \hat{\pi}_1 \hat{\pi}_2 } \frac{\hat{\mu}_1-\hat{\mu}_2}{ \sqrt{ s^2_{\text{ML}}}}\\
&amp; = \sqrt{\frac{n}{n-2}} t_{\text{UB}}
\end{split}
\]</span>
with
<span class="math display">\[
t_{\text{ML}} \sim \text{lst}\left(0, \tau^2=\frac{n}{n-2}, n-2\right)
\]</span>
Thus, <span class="math inline">\(t_{\text{ML}}\)</span> follows a location-scale <span class="math inline">\(t\)</span>-distribution, with mean 0 for <span class="math inline">\(n&gt;3\)</span> and variance
<span class="math inline">\(n/(n-4)\)</span> for <span class="math inline">\(n&gt;4\)</span>.</p>
</div>
<div id="confidence-intervals" class="section level3" number="15.4.10">
<h3>
<span class="header-section-number">B.4.10</span> Confidence intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>A <strong>confidence</strong> interval (CI) is an <strong>interval estimate</strong> with a <strong>frequentist</strong> interpretation.</li>
<li>Definition of <strong>coverage</strong> <span class="math inline">\(\kappa\)</span> of a CI: how often (in repeated identical experiment) does the estimated CI overlap the true parameter value <span class="math inline">\(\theta\)</span>
<ul>
<li>Eg.: Coverage <span class="math inline">\(\kappa=0.95\)</span> (95%) means that in 95 out of 100 case the estimated CI will contain the (unknown) true value (i.e. it will “cover” <span class="math inline">\(\theta\)</span>).</li>
</ul>
</li>
</ul>
<p>Illustration of the repeated construction of a CI for <span class="math inline">\(\theta\)</span>:</p>
<div class="inline-figure"><img src="fig/refresher_p1.PNG" width="40%" style="display: block; margin: auto;"></div>
<ul>
<li>Note that a CI is actually an <strong>estimate</strong>: <span class="math inline">\(\widehat{\text{CI}}(x_1, \ldots, x_n)\)</span>, i.e. it depends on data and has a random (sampling) variation.<br>
</li>
<li>A good CI has high coverage and is compact.</li>
</ul>
<p><strong>Note:</strong> the coverage probability is <strong>not</strong> the probability that the true value is contained in a given estimated interval (that would be the Bayesian <em>credible</em> interval).</p>
</div>
<div id="symmetric-normal-confidence-interval" class="section level3" number="15.4.11">
<h3>
<span class="header-section-number">B.4.11</span> Symmetric normal confidence interval<a class="anchor" aria-label="anchor" href="#symmetric-normal-confidence-interval"><i class="fas fa-link"></i></a>
</h3>
<p>For a normally distributed univariate random variable
it is straightforward to construct a symmetric two-sided CI with a given desired coverage <span class="math inline">\(\kappa\)</span>.</p>
<div class="inline-figure"><img src="fig/refresher_p2_1.PNG" width="80%" style="display: block; margin: auto;"></div>
<p>For a normal random variable <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> and density function <span class="math inline">\(f(x)\)</span> we can compute the probability</p>
<p><span class="math display">\[\text{Pr}(x \leq \mu + c \sigma) =  \int_{-\infty}^{\mu+c\sigma} f(x) dx  = \Phi (c) = \frac{1+\kappa}{2}\]</span>
Note <span class="math inline">\(\Phi(c)\)</span> is the cumulative distribution function (CDF) of the standard normal <span class="math inline">\(N(0,1)\)</span>:</p>
<p>From the above we obtain the critical point <span class="math inline">\(c\)</span> from the quantile function, i.e. by inversion of <span class="math inline">\(\Phi\)</span>:</p>
<p><span class="math display">\[c=\Phi^{-1}\left(\frac{1+\kappa}{2}\right)\]</span></p>
<p>The following table lists <span class="math inline">\(c\)</span> for the three most commonly used values of <span class="math inline">\(\kappa\)</span> - it is useful to memorise these values!</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span>
</th>
<th>Critical value <span class="math inline">\(c\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>1.64</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>1.96</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>2.58</td>
</tr>
</tbody>
</table></div>
<p>A <strong>symmetric standard normal CI</strong> with nominal coverage <span class="math inline">\(\kappa\)</span> for</p>
<ul>
<li>a scalar parameter <span class="math inline">\(\theta\)</span>
</li>
<li>with normally distributed estimate <span class="math inline">\(\hat{\theta}\)</span> and</li>
<li>with estimated standard deviation <span class="math inline">\(\hat{\text{SD}}(\hat{\theta}) = \hat{\sigma}\)</span>
</li>
</ul>
<p>is then given by
<span class="math display">\[
\widehat{\text{CI}}=[\hat{\theta} \pm c \hat{\sigma}]
\]</span>
where <span class="math inline">\(c\)</span> is chosen for desired coverage level <span class="math inline">\(\kappa\)</span>.</p>
</div>
<div id="confidence-interval-based-on-the-chi-squared-distribution" class="section level3" number="15.4.12">
<h3>
<span class="header-section-number">B.4.12</span> Confidence interval based on the chi-squared distribution<a class="anchor" aria-label="anchor" href="#confidence-interval-based-on-the-chi-squared-distribution"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-figure"><img src="fig/refresher_p4_2.PNG" width="80%" style="display: block; margin: auto;"></div>
<p>As for the normal CI we can compute critical values but for the
chi-squared distribution we use a one-sided interval:
<span class="math display">\[
\text{Pr}(x \leq c) = \kappa
\]</span>
As before we get <span class="math inline">\(c\)</span> by the quantile function, i.e. by inverting the CDF of the chi-squared distribution.</p>
<p>The following list the critical values for the three most common choice of <span class="math inline">\(\kappa\)</span>
for <span class="math inline">\(m=1\)</span> (one degree of freedom):</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span>
</th>
<th>Critical value <span class="math inline">\(c\)</span> (<span class="math inline">\(m=1\)</span>)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>2.71</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>3.84</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>6.63</td>
</tr>
</tbody>
</table></div>
<p>A one-sided CI with nominal coverage <span class="math inline">\(\kappa\)</span> is then given by <span class="math inline">\([0, c ]\)</span>.</p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="matrix-and-calculus-refresher.html"><span class="header-section-number">A</span> Matrix and calculus refresher</a></div>
<div class="next"><a href="distribution-refresher.html"><span class="header-section-number">C</span> Distribution refresher</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#probability-and-statistics-refresher"><span class="header-section-number">B</span> Probability and statistics refresher</a></li>
<li><a class="nav-link" href="#basic-mathematical-notation"><span class="header-section-number">B.1</span> Basic mathematical notation</a></li>
<li>
<a class="nav-link" href="#combinatorics"><span class="header-section-number">B.2</span> Combinatorics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#number-of-permutations"><span class="header-section-number">B.2.1</span> Number of permutations</a></li>
<li><a class="nav-link" href="#multinomial-and-binomial-coefficient"><span class="header-section-number">B.2.2</span> Multinomial and binomial coefficient</a></li>
<li><a class="nav-link" href="#de-moivre-sterling-approximation-of-the-factorial"><span class="header-section-number">B.2.3</span> De Moivre-Sterling approximation of the factorial</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#probability"><span class="header-section-number">B.3</span> Probability</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#random-variables"><span class="header-section-number">B.3.1</span> Random variables</a></li>
<li><a class="nav-link" href="#probability-mass-and-density-function-distribution-function-and-quantile-function"><span class="header-section-number">B.3.2</span> Probability mass and density function, distribution function and quantile function</a></li>
<li><a class="nav-link" href="#expectation-of-a-random-variable"><span class="header-section-number">B.3.3</span> Expectation of a random variable</a></li>
<li><a class="nav-link" href="#jensens-inequality-for-the-expectation"><span class="header-section-number">B.3.4</span> Jensen’s inequality for the expectation</a></li>
<li><a class="nav-link" href="#probability-as-expectation"><span class="header-section-number">B.3.5</span> Probability as expectation</a></li>
<li><a class="nav-link" href="#moments-and-variance-of-a-random-variable"><span class="header-section-number">B.3.6</span> Moments and variance of a random variable</a></li>
<li><a class="nav-link" href="#distribution-of-sums-of-random-variables"><span class="header-section-number">B.3.7</span> Distribution of sums of random variables</a></li>
<li><a class="nav-link" href="#transformation-of-random-variables"><span class="header-section-number">B.3.8</span> Transformation of random variables</a></li>
<li><a class="nav-link" href="#random-vectors-and-covariance-matrix"><span class="header-section-number">B.3.9</span> Random vectors and covariance matrix</a></li>
<li><a class="nav-link" href="#correlation-matrix"><span class="header-section-number">B.3.10</span> Correlation matrix</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#statistics"><span class="header-section-number">B.4</span> Statistics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#statistical-learning"><span class="header-section-number">B.4.1</span> Statistical learning</a></li>
<li><a class="nav-link" href="#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><span class="header-section-number">B.4.2</span> Sampling properties of a point estimator \(\hat{\boldsymbol \theta}\)</a></li>
<li><a class="nav-link" href="#efficiency-and-consistency-of-an-estimator"><span class="header-section-number">B.4.3</span> Efficiency and consistency of an estimator</a></li>
<li><a class="nav-link" href="#empirical-distribution-function"><span class="header-section-number">B.4.4</span> Empirical distribution function</a></li>
<li><a class="nav-link" href="#empirical-estimators"><span class="header-section-number">B.4.5</span> Empirical estimators</a></li>
<li><a class="nav-link" href="#law-of-large-numbers"><span class="header-section-number">B.4.6</span> Law of large numbers</a></li>
<li><a class="nav-link" href="#sampling-distribution-of-mean-and-variance-estimators-for-normal-data"><span class="header-section-number">B.4.7</span> Sampling distribution of mean and variance estimators for normal data</a></li>
<li><a class="nav-link" href="#one-sample-t-statistic"><span class="header-section-number">B.4.8</span> One sample \(t\)-statistic</a></li>
<li><a class="nav-link" href="#two-sample-t-statistic-with-common-variance"><span class="header-section-number">B.4.9</span> Two sample \(t\)-statistic with common variance</a></li>
<li><a class="nav-link" href="#confidence-intervals"><span class="header-section-number">B.4.10</span> Confidence intervals</a></li>
<li><a class="nav-link" href="#symmetric-normal-confidence-interval"><span class="header-section-number">B.4.11</span> Symmetric normal confidence interval</a></li>
<li><a class="nav-link" href="#confidence-interval-based-on-the-chi-squared-distribution"><span class="header-section-number">B.4.12</span> Confidence interval based on the chi-squared distribution</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistics 2: Likelihood and Bayes</strong>" was written by Korbinian Strimmer. It was last built on 13 December 2023.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
