<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>1 Overview of statistical learning | Statistics 2: Likelihood and Bayes</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="1 Overview of statistical learning | Statistics 2: Likelihood and Bayes">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Overview of statistical learning | Statistics 2: Likelihood and Bayes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<meta name="description" content="1.1 How to learn from data? A fundamental question is how to extract information from data in an optimal way, and to make predictions based on this information. For this purpose, a number of...">
<meta property="og:description" content="1.1 How to learn from data? A fundamental question is how to extract information from data in an optimal way, and to make predictions based on this information. For this purpose, a number of...">
<meta name="twitter:description" content="1.1 How to learn from data? A fundamental question is how to extract information from data in an optimal way, and to make predictions based on this information. For this purpose, a number of...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistics 2: Likelihood and Bayes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="active" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="conditioning-and-bayes-rule.html"><span class="header-section-number">7</span> Conditioning and Bayes rule</a></li>
<li><a class="" href="models-with-latent-variables-and-missing-data.html"><span class="header-section-number">8</span> Models with latent variables and missing data</a></li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li><a class="" href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li><a class="" href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="matrix-and-calculus-refresher.html"><span class="header-section-number">A</span> Matrix and calculus refresher</a></li>
<li><a class="" href="probability-and-distribution-refresher.html"><span class="header-section-number">B</span> Probability and distribution refresher</a></li>
<li><a class="" href="statistics-refresher.html"><span class="header-section-number">C</span> Statistics refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">D</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="overview-of-statistical-learning" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Overview of statistical learning<a class="anchor" aria-label="anchor" href="#overview-of-statistical-learning"><i class="fas fa-link"></i></a>
</h1>
<div id="how-to-learn-from-data" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> How to learn from data?<a class="anchor" aria-label="anchor" href="#how-to-learn-from-data"><i class="fas fa-link"></i></a>
</h2>
<p>A fundamental question is how to extract information from data in an optimal way, and to make predictions based on this information.</p>
<p>For this purpose, a number of competing <strong>theories of information</strong> have been developed. <strong>Statistics</strong> is the oldest science of information and is concerned with offering principled ways to learn from data and to extract and process information using probabilistic models.
However, there are other theories of information (e.g. Vapnik-Chernov theory of learning, computational learning) that are more algorithmic than analytic and sometimes not even based on probability theory.</p>
<p>Furthermore, there are other disciplines, such computer science and machine learning that are closely linked with and also have substantial overlap with statistics. The field of “data science” today comprises of both statistics and machine learning and brings together mathematics, statistics and computer science. Also the growing field of so-called “artificial intelligence” makes substantial use of statistical and machine learning techniques.</p>
<p>The recent popular science book “The Master Algorithm” by <span class="citation">Domingos (<a href="bibliography.html#ref-Domingos2015" role="doc-biblioref">2015</a>)</span> provides an accessible informal overview over
the various schools of science of information. It discusses the main algorithms used in machine learning and statistics:</p>
<ul>
<li><p>Starting as early as 1763, the <strong>Bayesian school</strong> of learning was started which later turned out to be closely linked with <em>likelihood inference</em> established in 1922 by <a href="https://de.wikipedia.org/wiki/Ronald_Aylmer_Fisher">Ronald A. Fisher (1890–1962)</a> and generalised in 1951 to <strong>entropy learning</strong> by Kullback and Leibler.</p></li>
<li><p>It was also in the 1950s that the concept of artificial <strong>neural network</strong> arises, essentially a nonlinear input-output map with no underlying probabilistic modelling. This field saw another leap in the 1980s and further progressed from 2010 onwards with the development of <em>deep dearning</em>. It is now one of the most popular (and most effective) methods for analysing imaging data. Even your mobile phone most likely now has a dedicated computer chip with special neural network hardware. Despite their non-probabilistic origins, modern interpretations of neural networks view them as high-dimensional nonlinear
statistical models.</p></li>
<li><p>Further advanced theories of information were developed in the 1960 under the term of
<strong>computational learning</strong>, most notably the Vapnik-Chernov theory, with the most prominent example of the “support vector machine” (another non-probabilistic model) devised in the 1990s. Other important
advances include “ensemble learning” and corresponding algorithmic approaches to classification such
as “random Forests”.</p></li>
<li><p>With the advent of large-scale genomic and other high-dimensional data there has been a surge of new and exciting developments in the field of high-dimensional (large dimension) and also big data (large dimension and large sample size), both in statistics and in machine learning.</p></li>
</ul>
<p><strong>The connections between various fields of information is still not perfectly understood, but it is clear that an overarching theory will need to be based on probabilistic learning.</strong></p>
</div>
<div id="probability-theory-versus-statistical-learning" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Probability theory versus statistical learning<a class="anchor" aria-label="anchor" href="#probability-theory-versus-statistical-learning"><i class="fas fa-link"></i></a>
</h2>
<p>When you study statistics (or any other information theory) you need to be aware that there is a fundamental difference between probability theory
and statistics, and that relates to the <strong>distinction between
“randomness” and “uncertainty”</strong>.</p>
<p>Probability theory studies <strong>randomness</strong>, by developing mathematical models for randomness (such as probability distributions), and studying corresponding mathematical properties (including asymptotics etc). Probability theory may in fact be viewed as a branch of measure theory, and as such it belongs to the domain of pure mathematics.</p>
<p>Probability theory provides probabilistic generative models for data, both for simulation
of new data and as well as for explaining observed data.
Methods and theory how to best identify probabilistic models from observations and to use
them to predict future observations belongst to the domain of applied mathematics, specifically statistics and the related areas of machine learning and data science.</p>
<p>Note that statistics, in contrast to probability, is therefore not at all concerned with randomness. Instead, the focus is about measuring and elucidating the <strong>uncertainty</strong> of events, predictions, outcomes, model parameters and this uncertainty measures the <strong>state of knowledge</strong> about these quantities. As soon as new data or information becomes available, the state of knowledge and thus the uncertainty changes! Thus, <strong>uncertainty is an epistemological property</strong>.</p>
<p>The uncertainty most often is due to our ignorance of the true underlying processes (on purpose or not), but not because the underlying process is actually random. The success of statistics is based on the fact that we can mathematically model the uncertainty without knowing any detailed specifics of the underlying processes, and yet we still can furnish procedures for optimal inference despite the uncertainty.</p>
<p>In short, statistics is about describing the state of knowledge of the world, which
may be uncertain and incomplete, and to make decisions and predictions in the face of uncertainty, and this uncertaintly sometimes derives from randomness but most often from our ignorance (and sometimes this ignorance even helps to create a simple yet effective model).</p>
</div>
<div id="cartoon-of-statistical-learning" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Cartoon of statistical learning<a class="anchor" aria-label="anchor" href="#cartoon-of-statistical-learning"><i class="fas fa-link"></i></a>
</h2>
<p>The aim of statistical learning is to use observed data in an optimal way to learn about the
underlying mechanism of the data-generating process. Since data is typically finite but models
can be in principle arbitrarily complex there may be issues of overfitting (not enough data
for the complexity) but also underfitting (model is too simplistic).</p>
<p>We observe data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> assumed to result from an underlying
true data-generating model <span class="math inline">\(F_{\text{true}}\)</span>, the distribution for <span class="math inline">\(x\)</span>.</p>
<p>To explain the observed data, and also to predict future data, we will make hypotheses
in the form of candidate models <span class="math inline">\(F_{1}, F_{2}, \ldots\)</span>. Often these candidate models
form a model family <span class="math inline">\(F_{\boldsymbol \theta}\)</span> indexed by a parameter vector <span class="math inline">\(\boldsymbol \theta\)</span>, with specific values
for each model
so that we can also write <span class="math inline">\(F_{\boldsymbol \theta_1}, F_{\boldsymbol \theta_2}, \ldots\)</span> for the various models.
These parameters may, or may not, have some direct interpretation. Ideally, the
models are also identifiable within a family, i.e. each distinct model is identified by a unique parameter
so that <span class="math inline">\(F_{\boldsymbol \theta_1} = F_{\boldsymbol \theta_2}\)</span> implies <span class="math inline">\(\boldsymbol \theta_1 = \boldsymbol \theta_2\)</span>,
hence if models are the same then the corresponding parameters must also be the same.</p>
<p>The true underlying model itself is unknown and cannot be observed.
However, what we can observe is data <span class="math inline">\(D\)</span> from the true model <span class="math inline">\(F_{\text{true}}\)</span> by measuring properties of interest (our observations from experiments). Sometimes we can also perturb the model and see what the effect is (interventional study).</p>
<p>The various candidate models <span class="math inline">\(F_1, F_2, \ldots\)</span> in the <strong>model world</strong> will at best be good approximations to the true underlying data generating model <span class="math inline">\(F_{\text{true}}\)</span>.
In some cases the true model will be part of the model family, i.e.<br>
there exists a parameter <span class="math inline">\(\boldsymbol \theta_{\text{true}}\)</span> so that <span class="math inline">\(F_{\text{true}} = F_{\boldsymbol \theta_{\text{true}}}\)</span>.
However, more typically we cannot assume that the true underlying model is contained in the family. Nonetheless, even an imperfect candidate model will often provide a useful mathematical approximation and capture some important characteristics of the true model and thus will help to interpret the observed data.</p>
<p><span class="math display">\[
\begin{array}{cc}
\textbf{Hypothesis} \\
\text{How the world works} \\
\end{array}
\longrightarrow
\begin{array}{cc}
\textbf{Model world} \\
F_1,  \boldsymbol \theta_1  \\
F_2, \boldsymbol \theta_2  \\
\vdots\\
\end{array}
\]</span>
<span class="math display">\[
\longrightarrow
\begin{array}{cc}
\textbf{Real world,} \\
\textbf{unknown true model} \\
F_{\text{true}}, \boldsymbol \theta_{\text{true}} \\
\end{array}
\longrightarrow \textbf{Data } x_1, \ldots, x_n
\]</span></p>
<p><strong>The aim of statistical learning is to identify the model(s) that explain the current data and also predict future data (i.e. predict outcome of experiments that have not been conducted yet).</strong></p>
<p>Thus a good model provides a good fit to the current data (i.e. it explains current observations well) and also to the future data (i.e. it generalises well).</p>
<p>A large proportion of statistical theory is devoted to finding these “good” models
that avoid both <em>overfitting</em> (models being too complex and don’t generalise well) or
<em>underfitting</em> (models being too simplistic and hence also don’t predict well).</p>
<p>Typically the aim is to find a model whose <strong>model complexity</strong> is well matched with the
complexity of the unknown true model and also with the complexity of the observed data.</p>
</div>
<div id="common-distributions-used-in-statistical-models" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> Common distributions used in statistical models<a class="anchor" aria-label="anchor" href="#common-distributions-used-in-statistical-models"><i class="fas fa-link"></i></a>
</h2>
<p>Models employed in statistical analysis are typically multivariate comprising many
random variables. As such these models can be very complex, with hierarchical or network-like structures
linking observed and latent variables, and possibly exhibiting nonlinear functional relationships.</p>
<p>However, nonetheless even the most complex models will normally be composed of
more elementary building blocks. For example, the following parametric distributions
frequently occur in statistical analysis:</p>
<ul>
<li><p><strong>Bernoulli distribution</strong> <span class="math inline">\(\text{Ber}(\theta)\)</span> and <strong>categorical distribution</strong> <span class="math inline">\(\text{Cat}(\boldsymbol \pi)\)</span>: used
to model frequencies (on the domain <span class="math inline">\([0,1]\)</span>). Repeated application yields the
<strong>binomial distribution</strong> <span class="math inline">\(\text{Bin}(n, \theta)\)</span> and <strong>multinomial distribution</strong> <span class="math inline">\(\text{Mult}(n, \boldsymbol \pi)\)</span>.</p></li>
<li><p><strong>Normal distribution</strong> in both the univariate <span class="math inline">\(N(\mu, \sigma^2)\)</span> and
multivariate <span class="math inline">\(N(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> version: commonly used to model mean values
(on the domain <span class="math inline">\([-\infty, \infty]\)</span>).</p></li>
<li><p><strong>Gamma distribution</strong> <span class="math inline">\(\text{Gam}(\alpha, \theta)\)</span>: used to model scale factors
(on the domain <span class="math inline">\([0, \infty]\)</span>). It is also known (with different parameterisation) as univariate
<strong>Wishart distribution</strong> <span class="math inline">\(W_1\left(s^2, k \right)\)</span> or as <strong>scaled chi-squared distribution</strong>
<span class="math inline">\(s^2 \text{$\chi^2_{k}$}\)</span>. Special cases include the <strong>chi-squared distribution</strong> <span class="math inline">\(\text{$\chi^2_{k}$}\)</span> and
the <strong>exponential distribution</strong> <span class="math inline">\(\text{Exp}(\theta)\)</span>.</p></li>
</ul>
<p>Note that all of the above parametric distributions are <strong>exponential families</strong>
(see Example <a href="overview-of-statistical-learning.html#exm:expfamdef">1.1</a>) and as such they lend themselves particularly well to statistical
learning, as we will see in this module.</p>
<p>Another commonly used parametric model is a generalisation of the normal distribution:</p>
<ul>
<li>
<strong>Location-scale <span class="math inline">\(t\)</span>-distribution</strong> <span class="math inline">\(\text{lst}(\mu, \tau^2, \nu)\)</span>: similar to
the normal distibution <span class="math inline">\(N(\mu, \sigma^2)\)</span> but with heavier tails. It emerges as
the sampling distribution for the <span class="math inline">\(t\)</span>-statistic and as compound distribution in Bayesian learning.
Special cases include the <strong>Student’s <span class="math inline">\(t_\nu\)</span> distribution</strong> and <strong>Cauchy distribution</strong>
<span class="math inline">\(\text{Cau}(\mu, \tau)\)</span>. In contrast to the other models mentioned so far this is not an exponential
family and due to the heavy tails,
depending on the choice of the degrees of freedom <span class="math inline">\(\nu\)</span>, not all moments of the distribution may exist.</li>
</ul>
<p>Finally, <strong>nonparametric models</strong> are also often used to describe and analyse the observed data.
Rather than specifying a parametric model for <span class="math inline">\(F\)</span> one focuses on using the whole distribution to define
meaningful <strong>statistical functionals</strong> <span class="math inline">\(\theta = g(F)\)</span>, such as the mean and the variance.</p>
<p>The above models will be used throughout the Statistics 2 module — for a refresher of the technical details we refer to the Appendix. In the second part of the module (Bayesian statistics) we will encounter further distributions such as the <strong>beta distribution</strong> or <strong>inverse gamma distribution</strong>.
Subsequent modules in later study years (year 3 and 4) will introduce more complex models, related to temporal and spatial modelling, regression analysis and generalised linear models, and multivariate statistics and machine learning.</p>
<div class="example">
<p><span id="exm:expfamdef" class="example"><strong>Example 1.1  </strong></span>Definition of an exponential family:</p>
<p>A distribution family <span class="math inline">\(P_{\boldsymbol \eta}\)</span> is an exponential family with canonical parameters
<span class="math inline">\(\boldsymbol \eta\)</span> if the family results from <strong>exponential
tilting</strong> of a <strong>base distribution</strong> <span class="math inline">\(B\)</span>. In this case
its density or probability mass function can be written as
<span class="math display">\[
p(x|\boldsymbol \eta) =  b(x) \, e^{ \boldsymbol \eta^T \boldsymbol u(x)} / e^{ \psi(\boldsymbol \eta)}
\]</span>
where <span class="math inline">\(b(x)\)</span> is the base density and <span class="math inline">\(\boldsymbol u(x)\)</span> are the canonical statistics
and
<span class="math display">\[
\psi(\boldsymbol \eta) = \log \int_x b(x) \, e^{ \boldsymbol \eta^T \boldsymbol u(x)} \, dx
\]</span>
is the <strong>log-normaliser</strong> or <strong>log-partition function</strong> that ensures that
<span class="math inline">\(p(x|\boldsymbol \eta)\)</span> integrates to one. Exponential families have many favourable properties that facilitate estimation and inference, such as existence of all moments, easy to optimise, analytic solutions for parameter estimates, among many others.</p>
<p>Exponential families are studied in more detail the module about generalised linear models
but in this course we will also highlight if there are general results that apply specifically to them.</p>
</div>
</div>
<div id="finding-the-best-models" class="section level2" number="1.5">
<h2>
<span class="header-section-number">1.5</span> Finding the best models<a class="anchor" aria-label="anchor" href="#finding-the-best-models"><i class="fas fa-link"></i></a>
</h2>
<p>A core task in statistical learning is to identify those distributions that explain the existing data well and that also generalise well to future yet unseen observations.</p>
<p>In a <strong>nonparametric setting</strong> we may simply rely on the law of large numbers that implies that
the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> constructed
from the observed data <span class="math inline">\(D\)</span> converges to the true distribution <span class="math inline">\(F\)</span> if the sample size is large.
We can therefore obtain an <strong>empirical estimator</strong> <span class="math inline">\(\hat{\theta}\)</span> of the functional <span class="math inline">\(\theta = g(F)\)</span> by
<span class="math inline">\(\hat{\theta}= g( \hat{F}_n )\)</span>, i.e. by substituting the true distribution with the empirical distribution.
This allows us, e.g., to get the empirical estimate of the mean
<span class="math display">\[
\hat{\text{E}}(x) = \hat{\mu} =  \text{E}_{\hat{F}_n}(x) = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x}
\]</span>
and of the variance
<span class="math display">\[
\widehat{\text{Var}}(x) = \widehat{\sigma^2} =
\text{E}_{\hat{F}_n}((x - \hat{\mu})^2) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2
\]</span>
simply by replacing the expectation with the sample average.</p>
<p>For parametric models we need to find estimates of the parameters that correspond to the
distributions that best approximate the unknown true data generating model.
One such approach is provided by the <strong>method of maximum likelihood</strong>. More precisely,
given a probability distribution <span class="math inline">\(P_{\boldsymbol \theta}\)</span> with density or mass function <span class="math inline">\(p(x|\boldsymbol \theta)\)</span> where <span class="math inline">\(\boldsymbol \theta\)</span> is a parameter vector, and <span class="math inline">\(D = \{x_1,\dots,x_n\}\)</span> are the observed iid data (i.e. independent and identically distributed), the <strong>likelihood function</strong> is then defined as
<span class="math display">\[
L_n(\boldsymbol \theta| D ) =\prod_{i=1}^{n} p(x_i|\boldsymbol \theta)
\]</span>
The parameter that maximises the likelihood is the <strong>maximum likelihood estimate</strong>.</p>
<p>The first part of this module is devoted to exploring the method of maximum likelihood both practically
and more theoretically. We start by considering the justification of the method
of maximum likelihood. Historically, the likelihood function was introduced (and still often is interpreted) as the probability to observe the data given the model with specified parameters <span class="math inline">\(\boldsymbol \theta\)</span>. However, this view is incorrect as it not only
breaks down for continuous random variables due to the use of densities and even for discrete random variables an additional factor accounting for the possible permutations of samples is needed to obtain the actual probability of the data. Instead, the true foundation of
maximum likelihood lies in information theory, specifically in its close link with the relative entropy between
the unknown true distribution <span class="math inline">\(F\)</span> and the model <span class="math inline">\(P_{\boldsymbol \theta}\)</span>. As a result we will see that <strong>maximum
likelihood extends empirical estimation to parametric models</strong>.
This insight allows to shed light both on the optimality properties as well as the limitations of maximum
likelihood inference.</p>
<p>In the second part we then introduce the Bayesian approach to statistical estimation and inference that can be viewed as a natural extension of likelihood-based statistical analysis that overcomes some of the
limitations of maximum likelihood.</p>
<p>The aim of this module is therefore</p>
<ol style="list-style-type: lower-roman">
<li>to provide a principled introduction to maximum likelihood and Bayesian
statistical analysis and</li>
<li>to demonstrate that statistics offers a well founded and coherent theory of information, rather than just seemingly unrelated collections of “recipes” for data analysis (a still widespread but wrong perception of statistics).</li>
</ol>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="preface.html">Preface</a></div>
<div class="next"><a href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#overview-of-statistical-learning"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="nav-link" href="#how-to-learn-from-data"><span class="header-section-number">1.1</span> How to learn from data?</a></li>
<li><a class="nav-link" href="#probability-theory-versus-statistical-learning"><span class="header-section-number">1.2</span> Probability theory versus statistical learning</a></li>
<li><a class="nav-link" href="#cartoon-of-statistical-learning"><span class="header-section-number">1.3</span> Cartoon of statistical learning</a></li>
<li><a class="nav-link" href="#common-distributions-used-in-statistical-models"><span class="header-section-number">1.4</span> Common distributions used in statistical models</a></li>
<li><a class="nav-link" href="#finding-the-best-models"><span class="header-section-number">1.5</span> Finding the best models</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistics 2: Likelihood and Bayes</strong>" was written by Korbinian Strimmer. It was last built on 3 January 2024.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
