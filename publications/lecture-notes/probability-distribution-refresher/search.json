[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"supplementary notes aim provide quick\nrefresher essentials combinatorics probability\nwell offer overview selected\nunivariate multivariate distributions.notes supporting information \nlecture notes number statistical courses teaching Department Mathematics University Manchester.includes current modules:MATH27720 Statistics 2: Likelihood Bayes andMATH38161 Multivariate Statistics,well retired module (offered ):MATH20802 Statistical Methods.Probability Distribution Refresher notes written Korbinian Strimmer 2018–2024. version 6 January 2024.notes updated time time. view current\nversion visit \nonline version Probability Distribution Refresher notes.may also wish download Probability Distribution Refresher notes PDF A4 format printing (double page layout) 6x9 inch PDF use tablets (single page layout).","code":""},{"path":"index.html","id":"about-the-author","chapter":"Welcome","heading":"About the author","text":"Hello! name Korbinian Strimmer Professor Statistics.\nmember Statistics group Department Mathematics\nUniversity Manchester. can find information home page.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"notes licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.","code":""},{"path":"combinatorics.html","id":"combinatorics","chapter":"1 Combinatorics","heading":"1 Combinatorics","text":"","code":""},{"path":"combinatorics.html","id":"some-basic-mathematical-notation","chapter":"1 Combinatorics","heading":"1.1 Some basic mathematical notation","text":"Summation:\n\\[\n\\sum_{=1}^n x_i = x_1 + x_2 + \\ldots + x_n\n\\]Multiplication:\n\\[\n\\prod_{=1}^n x_i = x_1 \\times x_2 \\times \\ldots \\times x_n\n\\]Indicator function:\n\\[\n1_{} =\n\\begin{cases}\n1 & \\text{$$ true}\\\\\n0 & \\text{$$ true}\\\\\n\\end{cases}\n\\]","code":""},{"path":"combinatorics.html","id":"number-of-permutations","chapter":"1 Combinatorics","heading":"1.2 Number of permutations","text":"number possible orderings, permutations, \\(n\\) distinct items \nnumber ways put \\(n\\) items \\(n\\) bins exactly one item bin. given \nfactorial\n\\[\nn! = \\prod_{=1}^n = 1 \\times 2 \\times \\ldots \\times n\n\\]\n\\(n\\) positive integer.\n\\(n=0\\) factorial defined \n\\[\n0! = 1\n\\]\nexactly one permutation zero objects.factorial can also obtained using \ngamma function\n\\[\n\\Gamma(x) = \\int_0^\\infty t^{x-1} e^{-t} dt\n\\]\ncan viewed continuous version factorial\n\n\\(\\Gamma(x) = (x-1)!\\) positive integer \\(x\\).","code":""},{"path":"combinatorics.html","id":"multinomial-and-binomial-coefficient","chapter":"1 Combinatorics","heading":"1.3 Multinomial and binomial coefficient","text":"number possible permutation \\(n\\) items \\(K\\) distinct types, \\(n_1\\) type 1, \\(n_2\\) type 2 , equals number ways\nput \\(n\\) items \\(K\\) bins \\(n_1\\) items first bin, \\(n_2\\) second .\ngiven multinomial coefficient\n\\[\n\\binom{n}{n_1, \\ldots, n_K} = \\frac {n!}{n_1! \\times n_2! \\times\\ldots \\times n_K! }\n\\]\n\\(\\sum_{k=1}^K n_k = n\\) \\(K \\leq n\\).\nNote equals number permutation items divided number permutations items bin (type).\\(n_k=1\\) hence \\(K=n\\) multinomial coefficient reduces factorial.two bins / types (\\(K=2\\)) multinomial coefficients becomes \nbinomial coefficient\n\\[\n\\binom{n}{n_1} = \\binom{n}{n_1, n-n_1}    =  \\frac {n!}{n_1! (n - n_1)!}\n\\]\ncounts number ways choose \\(n_1\\) elements set \\(n\\) elements.","code":""},{"path":"combinatorics.html","id":"de-moivre-sterling-approximation-of-the-factorial","chapter":"1 Combinatorics","heading":"1.4 De Moivre-Sterling approximation of the factorial","text":"factorial frequently approximated following formula derived Abraham de Moivre (1667–1754) James Stirling (1692-1770)\n\\[\nn! \\approx \\sqrt{2 \\pi} n^{n+\\frac{1}{2}} e^{-n}\n\\]\nequivalently logarithmic scale\n\\[\n\\log n!  \\approx \\left(n+\\frac{1}{2}\\right) \\log n  -n + \\frac{1}{2}\\log \\left( 2 \\pi\\right)\n\\]\napproximation good small \\(n\\) (fails \\(n=0\\)) becomes\naccurate increasing \\(n\\). large \\(n\\) approximation can simplified \n\\[\n\\log n! \\approx  n \\log n  -n\n\\]","code":""},{"path":"probability.html","id":"probability","chapter":"2 Probability","heading":"2 Probability","text":"","code":""},{"path":"probability.html","id":"random-variables","chapter":"2 Probability","heading":"2.1 Random variables","text":"random variable describes random experiment. set possible outcomes\nsample space state space random variable denoted \n\\(\\Omega = \\{\\omega_1, \\omega_2, \\ldots\\}\\). outcomes \\(\\omega_i\\) elementary events.\nsample space \\(\\Omega\\) can finite infinite. Depending type outcomes\nrandom variable discrete continuous.event \\(\\subseteq \\Omega\\) subset \\(\\Omega\\) thus set composed elementary events: \\(= \\{a_1, a_2, \\ldots\\}\\).\nincludes special cases full set \\(= \\Omega\\), empty set \\(= \\emptyset\\), elementary\nevents \\(=\\omega_i\\). complementary event \\(^C\\) complement set \\(\\) set \\(\\Omega\\)\n\\(^C = \\Omega \\setminus = \\{\\omega_i \\\\Omega: \\omega_i \\notin \\}\\).probability event \\(\\) denoted \\(\\text{Pr}()\\).\nEssentially, obtain probability need count \nelementary elements corresponding \\(\\). \nassume axioms probability \\(\\text{Pr}() \\geq 0\\), probabilities positive,\\(\\text{Pr}(\\Omega) = 1\\), certain event probability 1, \\(\\text{Pr}() = \\sum_{a_i \\} \\text{Pr}(a_i)\\), probability \nevent equals sum constituting elementary events \\(a_i\\).\nsum taken finite countable infinite number elements.implies\\(\\text{Pr}() \\leq 1\\), .e. probabilities lie interval \\([0,1]\\)\\(\\text{Pr}(^C) = 1 - \\text{Pr}()\\), \\(\\text{Pr}(\\emptyset) = 0\\)Assume now two events \\(\\) \\(B\\).\nprobability event “\\(\\) \\(B\\)” given probability set intersection\n\\(\\text{Pr}(\\cap B)\\).\nLikewise probability event “\\(\\) \\(B\\)” given probability set union\n\\(\\text{Pr}(\\cup B)\\).clear definition theory probability closely linked set theory, particular measure theory. Indeed, viewing probability special type measure allows elegant treatment discrete continuous random variables.","code":""},{"path":"probability.html","id":"probability-mass-and-density-function","chapter":"2 Probability","heading":"2.2 Probability mass and density function","text":"describe random variable \\(x\\) state space \\(\\Omega\\) need way effectively store probabilities corresponding elementary outcomes \\(x \\\\Omega\\). Note convenience use symbol denote random variable elementary outcomes.discrete random variable define \nevent \\(= \\{x: x=\\} = \\{\\}\\) get probability\n\\[\n\\text{Pr}() = \\text{Pr}(x=) = f()\n\\]\ndirectly probability mass function (PMF), denoted lower case \\(f\\)\n(frequently also use \\(p\\) \\(q\\)).\nPMF property \\(\\sum_{x \\\\Omega} f(x) = 1\\) \n\\(f(x) \\[0,1]\\).continuous random variables need use probability density function (PDF) instead. define event\n\\(= \\{x: < x \\leq + da\\}\\) infinitesimal interval\nassign probability\n\\[\n\\text{Pr}() = \\text{Pr}( < x \\leq + da) = f() da \\,.\n\\]\nPDF property \\(\\int_{x \\\\Omega} f(x) dx = 1\\)\ncontrast PMF density \\(f(x)\\geq 0\\) may take values larger 1.set \\(x\\) \\(f(x)\\) positive called support PDF/PMF.","code":""},{"path":"probability.html","id":"distribution-function-and-quantile-function","chapter":"2 Probability","heading":"2.3 Distribution function and quantile function","text":"alternative using PMF/PDFs may also use distribution function describe random variable. assumes ordering exist among elementary events can define event \\(= \\{x: x \\leq \\}\\) compute \nprobability \n\\[\nF() = \\text{Pr}() = \\text{Pr}( x \\leq ) =\n\\begin{cases}\n\\sum_{x \\} f(x) & \\text{discrete case} \\\\\n\\int_{x \\} f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\]\nalso known cumulative distribution function (CDF)\ndenoted upper case \\(F\\) (\\(P\\) \\(Q\\)).\nconstruction distribution function monotonically non-decreasing value ranges 0 1.\nhelp can compute probability interval set\n\n\\[\n\\text{Pr}( < x \\leq b ) = F(b)-F() \\,.\n\\]inverse distribution function \\(y=F(x)\\) quantile function \\(x=F^{-1}(y)\\).\n50% quantile \\(F^{-1}\\left(\\frac{1}{2}\\right)\\) called median.random variable \\(x\\) distribution function \\(F\\) write \\(x \\sim F\\).","code":""},{"path":"probability.html","id":"families-of-distributions","chapter":"2 Probability","heading":"2.4 Families of distributions","text":"distribution \\(F_{\\theta}\\) parameter \\(\\theta\\) constitutes distribution family collecting distributions corresponding\nparticular instances parameter. parameter \\(\\theta\\)\ntherefore acts index distributions contained family.corresponding density (PDF) probability mass function (PMF) \nwritten either \\(f_{\\theta}(x)\\), \\(f(x; \\theta)\\) \\(f(x | \\theta)\\).\nlatter form general suggests parameter \\(\\theta\\) may potentially also distribution, joint density formed \\(f(x, \\theta) = f(x | \\theta) f(\\theta)\\).Note parameterisation generally unique, one--one transformation \\(\\theta\\) yield another equivalent index distribution family. Typically, commonly used distribution families several standard parameterisations. Often use parameterisations parameters can interpreted easily (e.g. terms moments).pair different parameter values \\(\\theta_1 \\neq \\theta_2\\) get distinct distributions \\(F_{\\theta_1} \\neq F_{\\theta_2}\\) distribution family \\(F_{\\theta}\\) said identifiable parameter \\(\\theta\\).","code":""},{"path":"probability.html","id":"expectation-of-a-random-variable","chapter":"2 Probability","heading":"2.5 Expectation of a random variable","text":"expected value \\(\\text{E}(x)\\) random variable defined \nweighted average possible outcomes, weight given PMF / PDF \\(f(x)\\):\n\\[\n\\text{E}_{F}(x) =\n\\begin{cases}\n\\sum_{x \\\\Omega} f(x) x & \\text{discrete case} \\\\\n\\int_{x \\\\Omega} f(x) x dx & \\text{continuous case} \\\\\n\\end{cases}\n\\]\nNote notation emphasise expectation taken regard distribution \\(F\\). subscript \\(F\\) usually left \nambiguities.\nFurthermore, sum integral may diverge\nexpectation necessarily always defined (contrast quantiles).expected value function random variable \\(h(x)\\) \nobtained similarly:\n\\[\n\\text{E}_{F}(h(x)) =\n\\begin{cases}\n\\sum_{x \\\\Omega} f(x) h(x) & \\text{discrete case} \\\\\n\\int_{x \\\\Omega} f(x) h(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\]\ncalled “law unconscious statistician”, short LOTUS.\n, highlight random variable \\(x\\) distribution \\(F\\) \nwrite \\(\\text{E}_F(h(x))\\).","code":""},{"path":"probability.html","id":"jensens-inequality-for-the-expectation","chapter":"2 Probability","heading":"2.6 Jensen’s inequality for the expectation","text":"\\(h(\\boldsymbol x)\\) convex function following\ninequality holds:\\[\n\\text{E}(h(\\boldsymbol x)) \\geq h(\\text{E}(\\boldsymbol x))\n\\]Recall: convex function (\\(x^2\\)) shape “valley”.","code":""},{"path":"probability.html","id":"probability-as-expectation","chapter":"2 Probability","heading":"2.7 Probability as expectation","text":"Probability can also understood expectation.\nevent \\(\\) can define corresponding indicator function\n\\(1_{ x \\}\\) elementary element \\(x\\) part \\(\\).\nfollows\n\\[\n\\text{E}( 1_{x \\} ) = \\text{Pr}() \\, ,\n\\]Interestingly, one can develop whole theory probability perspective. 1","code":""},{"path":"probability.html","id":"moments-and-variance-of-a-random-variable","chapter":"2 Probability","heading":"2.8 Moments and variance of a random variable","text":"moments random variable defined follows:Zeroth moment: \\(\\text{E}(x^0) = 1\\) construction PDF PMF,First moment: \\(\\text{E}(x^1) = \\text{E}(x) = \\mu\\) , mean,Second moment: \\(\\text{E}(x^2)\\)variance second moment centred mean \\(\\mu\\):\n\\[\\text{Var}(x) = \\text{E}( (x - \\mu)^2 ) = \\sigma^2\\]variance can also computed \\(\\text{Var}(x) = \\text{E}(x^2)-\\text{E}(x)^2\\). provides example Jensen’s inequality,\n\\(\\text{E}(x^2) =\\text{E}(x)^2 + \\text{Var}(x) \\geq \\text{E}(x)^2\\).distribution necessarily need finite first higher moments.\nexample Cauchy distribution mean variance (higher moment).","code":""},{"path":"probability.html","id":"distribution-of-sums-of-random-variables","chapter":"2 Probability","heading":"2.9 Distribution of sums of random variables","text":"sum two normal random variables also normal (appropriate mean variance).central limit theorem, first postulated Abraham de Moivre (1667–1754), asserts\nmany cases distribution sum identically distributed random variables converges normal distribution, even individual random variables normal.example, result binomial distribution (sum Bernoulli random variables) can approximated normal distribution.","code":""},{"path":"probability.html","id":"transformation-of-random-variables","chapter":"2 Probability","heading":"2.10 Transformation of random variables","text":"Linear transformation random variables: \\(\\) \\(b\\) constants \\(x\\) random variable, random variable \\(y= + b x\\) mean \\(\\text{E}(y) = + b \\text{E}(x)\\) variance \\(\\text{Var}(y) = b^2 \\text{Var}(x)\\).general invertible coordinate transformation \\(y = h(x) = y(x)\\) backtransformation \\(x = h^{-1}(y) = x(y)\\).transformation infinitesimal volume element \\(dy = \\left|\\frac{dy}{dx}\\right| dx\\).transformation density \\(f_y(y) =\\left|\\frac{dx}{dy}\\right| f_x(x(y))\\).Note \\(\\left|\\frac{dx}{dy}\\right| = \\left|\\frac{dy}{dx}\\right|^{-1}\\).","code":""},{"path":"probability.html","id":"random-vectors-and-their-mean-and-variance","chapter":"2 Probability","heading":"2.11 Random vectors and their mean and variance","text":"Instead scalar random variables one often also considers random vectors also random matrices.random vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T\\) mean \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\) simply comprised means components, .e. \\(\\boldsymbol \\mu= (\\mu_1, \\ldots, \\mu_d)^T\\). Thus, mean random vector dimension vector length.variance random vector length \\(d\\), however, vector matrix size \\(d\\times d\\). matrix called covariance matrix:\n\\[\n\\begin{split}\n\\text{Var}(\\boldsymbol x) &= \\underbrace{\\boldsymbol \\Sigma}_{d\\times d} = (\\sigma_{ij}) = \\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix} \\\\\n  &=\\text{E}\\left(\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d\\times 1} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1\\times d}\\right) \\\\\n  & = \\text{E}(\\boldsymbol x\\boldsymbol x^T)-\\boldsymbol \\mu\\boldsymbol \\mu^T \\\\\n\\end{split}\n\\]\nentries covariance matrix \\(\\sigma_{ij} =\\text{Cov}(x_i, x_j)\\) describe covariance random variables \\(x_i\\) \\(x_j\\). covariance matrix symmetric, hence \\(\\sigma_{ij}=\\sigma_{ji}\\). diagonal entries \\(\\sigma_{ii} = \\text{Cov}(x_i, x_i) = \\text{Var}(x_i) = \\sigma_i^2\\) correspond variances components \\(\\boldsymbol x\\). covariance matrix construction positive semi-definite, .e. eigenvalues \\(\\boldsymbol \\Sigma\\) positive equal zero.However, wherever possible one aim use models non-singular covariance matrices, eigenvalues positive, covariance matrix invertible.univariate \\(x\\) scalar constant \\(\\) variance \\(x\\) equals \\(\\text{Var}(x) = ^2 \\text{Var}(x)\\). random vector \\(\\boldsymbol x\\) dimension \\(d\\) constant matrix \\(\\boldsymbol \\) dimension \\(m \\times d\\) generalises \\(\\text{Var}(\\boldsymbol Ax) = \\boldsymbol \\text{Var}(\\boldsymbol x) \\boldsymbol ^T\\).","code":""},{"path":"probability.html","id":"correlation-matrix","chapter":"2 Probability","heading":"2.12 Correlation matrix","text":"covariance matrix can factorised product\n\\[\n\\boldsymbol \\Sigma= \\boldsymbol V^{\\frac{1}{2}} \\boldsymbol P\\boldsymbol V^{\\frac{1}{2}}\n\\]\n\\(\\boldsymbol V\\) diagonal matrix containing variances\n\\[\n\\boldsymbol V= \\begin{pmatrix}\n    \\sigma_{11} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma_{dd}\n\\end{pmatrix}\n\\]\nmatrix \\(\\boldsymbol P\\) (“upper case rho”) symmetric correlation matrix\n\\[\n\\boldsymbol P= (\\rho_{ij}) = \\begin{pmatrix}\n    1 & \\dots & \\rho_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{d1} & \\dots & 1\n\\end{pmatrix}   = \\boldsymbol V^{-\\frac{1}{2}} \\boldsymbol \\Sigma\\boldsymbol V^{-\\frac{1}{2}}\n\\]\nThus, correlation \\(x_i\\) \\(x_j\\) defined \n\\[\n\\rho_{ij} = \\text{Cor}(x_i,x_j) = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}\n\\]","code":""},{"path":"univariate-distributions.html","id":"univariate-distributions","chapter":"3 Univariate distributions","heading":"3 Univariate distributions","text":"","code":""},{"path":"univariate-distributions.html","id":"bernoulli-distribution","chapter":"3 Univariate distributions","heading":"3.1 Bernoulli distribution","text":"Bernoulli distribution \\(\\text{Ber}(\\theta)\\) simplest distribution families.\nnamed Jacob Bernoulli (1655-1705)\nalso discovered law large numbers.describes discrete binary random variable\ntwo states \\(x=0\\) (“failure”) \\(x=1\\) (“success”),\nparameter \\(\\theta \\[0,1]\\) probability “success”.\nOften Bernoulli distribution also referred “coin tossing” model \ntwo outcomes “heads” “tails”.Correspondingly, probability mass function \\(\\text{Ber}(\\theta)\\) \n\\[\np(x=0 | \\theta) = \\text{Pr}(\\text{\"failure\"}| \\theta) = 1-\\theta  \n\\]\n\n\\[\np(x=1| \\theta) = \\text{Pr}(\\text{\"success\"}| \\theta) = \\theta\n\\]\ncompact way write PMF Bernoulli distribution \n\\[\np(x | \\theta ) = \\theta^{x} (1-\\theta)^{1-x}\n\\]\nlog PMF \n\\[\n\\log p(x | \\theta ) = x \\log \\theta + (1-x)\\log (1-\\theta)\n\\]random variable \\(x\\) follows Bernoulli distribution \nwrite\n\\[\nx \\sim \\text{Ber}(\\theta) \\,.\n\\]\nexpected value \\(\\text{E}(x) = \\theta\\) variance \\(\\text{Var}(x) = \\theta (1 - \\theta)\\).","code":""},{"path":"univariate-distributions.html","id":"binomial-distribution","chapter":"3 Univariate distributions","heading":"3.2 Binomial distribution","text":"Closely related Bernoulli distribution binomial distribution\n\\(\\text{Bin}(n, \\theta)\\) results repeating \nBernoulli experiment \\(n\\) times counting number successes among\n\\(n\\) trials (without keeping track ordering experiments).\nThus, \\(x_1, \\ldots, x_n\\) \\(n\\) independent \\(\\text{Ber}(\\theta)\\) random variables\n\\(y = \\sum_{=1}^n x_i\\) distributed \\(\\text{Bin}(n, \\theta)\\).random variable \\(y\\) follows binomial distribution \nwrite\n\\[\ny \\sim \\text{Bin}(n, \\theta)\\,\n\\]corresponding probability mass function :\n\\[\np(y | n, \\theta) = \\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}\n\\]\nsupport \\(y \\\\{ 0, 1, 2, \\ldots, n\\}\\).\nbinomial coefficient \\(\\binom{n}{y}\\) needed account multiplicity\nways (orderings samples) can observe \\(y\\) successes.expected value \\(\\text{E}(y) = n \\theta\\) variance \\(\\text{Var}(y) = n \\theta (1 - \\theta)\\).standardise support binomial variable unit interval\n\\(\\frac{y}{n} \\\\left\\{0,\\frac{1}{n},...,1\\right\\}\\) mean\n\\(\\text{E}\\left(\\frac{y}{n}\\right) = \\theta\\) variance \n\\(\\text{Var}\\left(\\frac{y}{n}\\right)=\\frac{\\theta (1-\\theta)}{n}\\).binomial distribution may illustrated urn model\ndistributing \\(n\\) balls \\(2\\) bins:\\(n=1\\) binomial distribution reduces Bernoulli distribution.R PMF binomial distribution called dbinom(). binomial coefficient computed choose().","code":""},{"path":"univariate-distributions.html","id":"beta-distribution","chapter":"3 Univariate distributions","heading":"3.3 Beta distribution","text":"","code":""},{"path":"univariate-distributions.html","id":"standard-parameterisation","chapter":"3 Univariate distributions","heading":"3.3.1 Standard parameterisation","text":"beta-distributed random variable denoted \n\\[\nx \\sim \\text{Beta}(\\alpha, \\beta)\n\\]\nsupport \\(x \\[0,1]\\) \\(\\alpha>0\\) \\(\\beta>0\\).beta random variable can visualised \nbreaking unit stick length one\ntwo pieces length \\(x\\) \\(1-x\\):density beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\) \n\\[\np(x | \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}\n\\]\ndepends beta function\ndefined \n\\[\nB(z_1, z_1) = \\frac{ \\Gamma(z_1) \\Gamma(z_2)}{\\Gamma(z_1 + z_2)}\n\\]beta distribution flexible can assume number different shapes, depending value \\(\\alpha\\) \\(\\beta\\). example, \\(\\alpha=\\beta=1\\) becomes uniform distribution unit interval:","code":""},{"path":"univariate-distributions.html","id":"mean-parameterisation","chapter":"3 Univariate distributions","heading":"3.3.2 Mean parameterisation","text":"Instead employing \\(\\alpha\\) \\(\\beta\\) parameters another useful reparameterisation \\(\\text{Beta}(\\mu, k)\\) beta distribution terms mean parameter\n\\(\\mu \\[0,1]\\) concentration parameter \\(k > 0\\). given \n\\[\nk=\\alpha+\\beta\n\\]\n\n\\[\\mu = \\frac{\\alpha}{\\alpha+\\beta}\n\\]\noriginal parameters can recovered \n\\(\\alpha= \\mu k\\) \\(\\beta=(1-\\mu) k\\).mean variance beta distribution expressed terms \\(\\mu\\) \\(k\\) \n\\[\n\\text{E}(x) = \\mu\n\\]\n\n\\[\n\\text{Var}(x)=\\frac{\\mu (1-\\mu)}{k+1}\n\\]\nincreasing concentration parameter \\(k\\) variance decreases thus probability mass becomes concentrated around mean.uniform distribution (\\(\\alpha=\\beta=1\\)) corresponds \\(\\mu=1/2\\) \\(k=2\\).Finally, note mean variance continuous beta distribution\nclosely match unit-standardised discrete binomial distribution .","code":""},{"path":"univariate-distributions.html","id":"normal-distribution","chapter":"3 Univariate distributions","heading":"3.4 Normal distribution","text":"normal distribution important continuous probability distribution.\nalso called Gaussian distribution named Carl Friedrich Gauss (1777–1855).univariate normal distribution \\(N(\\mu, \\sigma^2)\\) two parameters \\(\\mu\\) (location) \\(\\sigma^2\\) (scale) support \\(x \\]-\\infty, \\infty[\\).\\[\nx \\sim N(\\mu,\\sigma^2)\n\\]\nmean\n\\[\n\\text{E}(x)=\\mu\n\\]\nvariance\n\\[\n\\text{Var}(x) = \\sigma^2\n\\]Probability density function (PDF):\n\\[\np(x| \\mu, \\sigma^2)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]R density function called dnorm().standard normal distribution \\(N(0, 1)\\) mean 0 variance 1.Plot PDF standard normal:cumulative distribution function (CDF) standard normal \\(N(0,1)\\)\n\n\\[\n\\Phi (x ) = \\int_{-\\infty}^{x} p(x'| \\mu=0, \\sigma^2=1) dx'\n\\]\nanalytic expression \\(\\Phi(x)\\). R function called pnorm().Plot CDF standard normal:inverse \\(\\Phi^{-1}(p)\\) called quantile function standard normal.\nR function called qnorm().","code":""},{"path":"univariate-distributions.html","id":"gamma-distribution-and-special-cases","chapter":"3 Univariate distributions","heading":"3.5 Gamma distribution and special cases","text":"gamma distribution widely used statistics, also appears various\nparameterisations names, univariate Wishart \nscaled chi-squared distribution","code":""},{"path":"univariate-distributions.html","id":"standard-parameterisation-1","chapter":"3 Univariate distributions","heading":"3.5.1 Standard parameterisation","text":"gamma distribution\n\\(\\text{Gam}(\\alpha, \\theta)\\) continuous distribution \ntwo parameters \\(\\alpha>0\\) (shape) \\(\\theta>0\\) (scale):\n\\[\nx \\sim\\text{Gam}(\\alpha, \\theta)\n\\]\nsupport \\(x \\[0, \\infty[\\)\nmean\n\\[\\text{E}(x)=\\alpha \\theta\\]\nvariance\n\\[\\text{Var}(x) = \\alpha \\theta^2\\]gamma distribution also often used rate\nparameter \\(\\beta=1/\\theta\\) (one needs pay attention parameterisation used).probability density function (PDF) :\n\\[\np(x| \\alpha, \\theta)=\\frac{1}{\\Gamma(\\alpha) \\theta^{\\alpha} } x^{\\alpha-1} e^{-x/\\theta}\n\\]\ndensity gamma distribution available R function dgamma(). cumulative density function pgamma() quantile function qgamma().","code":""},{"path":"univariate-distributions.html","id":"wishart-parameterisation-and-scaled-chi-squared-distribution","chapter":"3 Univariate distributions","heading":"3.5.2 Wishart parameterisation and scaled chi-squared distribution","text":"gamma distribution often used different set parameters\n\\(k=2 \\alpha\\) \\(s^2 =\\theta/2\\) (hence conversely \\(\\alpha = k/2\\) \\(\\theta=2 s^2\\)).\nform known univariate one-dimensional Wishart distribution\n\\[\n\\text{W}_1\\left(s^2, k \\right)\n\\]\nnamed John Wishart (1898–1954).\nWishart parameterisation mean \n\\[\n\\text{E}(x) = k s^2\n\\]\nvariance\n\\[\n\\text{Var}(x) = 2 k s^4\n\\]Another name one-dimensional Wishart distribution exactly \nparameterisation scaled chi-squared distribution denoted \n\\[\ns^2 \\text{$\\chi^2_{k}$}\n\\]Finally, also often employ Wishart distribution mean parameterisation\n\\[\n\\text{W}_1\\left(s^2= \\frac{\\mu}{k}, k \\right)\n\\]\nparameters \\(\\mu = k s^2\\) \\(k\\) (thus \\(\\theta = 2 \\mu /k\\)). parameterisation mean \n\\[\n\\text{E}(x) = \\mu\n\\]\nvariance\n\\[\n\\text{Var}(x) = \\frac{2 \\mu^2}{k}\n\\]","code":""},{"path":"univariate-distributions.html","id":"construction-as-sum-of-squared-normals","chapter":"3 Univariate distributions","heading":"3.5.3 Construction as sum of squared normals","text":"gamma distributed variable can constructed follows.\nAssume \\(k\\) independent normal random variables mean 0\nvariance \\(s^2\\):\n\\[z_1,z_2,\\dots,z_k\\sim N(0,s^2)\\]\nsum squares\n\\[\nx = \\sum_{=1}^{k} z_i^2\n\\]\nfollows\n\\[\n\\begin{split}\nx \\sim & \\phantom{=} s^2 \\text{$\\chi^2_{k}$} \\\\\n       & = \\text{W}_1\\left( s^2, k \\right)\\\\\n       & =\\text{Gam}\\left(\\alpha=\\frac{k}{2}, \\theta = 2 s^2\\right)\n\\end{split}\n\\]","code":""},{"path":"univariate-distributions.html","id":"chi-squared-distribution","chapter":"3 Univariate distributions","heading":"3.5.4 Chi-squared distribution","text":"chi-squared distribution\n\\(\\text{$\\chi^2_{k}$}\\) special one-parameter restriction \ngamma resp. Wishart distribution obtained setting\n\\(s^2=1\\) , equivalently, \\(\\theta = 2\\) \\(\\mu = k\\).mean \\(\\text{E}(x)=k\\) variance \\(\\text{Var}(x)=2k\\). chi-squared distribution \\(\\text{$\\chi^2_{k}$}\\) equals \\(\\text{Gam}(\\alpha=k/2, \\theta=2) = \\text{W}_1\\left(1, k \\right)\\).plot density chi-squared distribution\ndegrees freedom \\(k=1\\) \\(k=3\\):R density chi-squared distribution given dchisq(). cumulative density function pchisq() \nquantile function qchisq().","code":""},{"path":"univariate-distributions.html","id":"exponential-distribution","chapter":"3 Univariate distributions","heading":"3.5.5 Exponential distribution","text":"exponential distribution \\(\\text{Exp}(\\theta)\\) scale parameter \\(\\theta\\)\nanother special one-parameter restriction gamma distribution shape parameter set \n\\(\\alpha=1\\) (equivalently \\(k=2\\)).thus equals\n\\(\\text{Gam}(\\alpha=1, \\theta) = \\text{W}_1(s^2=\\theta/2, k=2)\\). mean \\(\\theta\\) variance \\(\\theta^2\\).Just like gamma distribution exponential distribution also often specified using rate parameter \\(\\beta= 1/\\theta\\) instead scale parameter \\(\\theta\\).R command dexp() returns \ndensity exponential distribution,\npexp() corresponding cumulative density function qexp() quantile function.","code":""},{"path":"univariate-distributions.html","id":"inverse-gamma-distribution","chapter":"3 Univariate distributions","heading":"3.6 Inverse gamma distribution","text":"Also know inverse univariate Wishart distribution.","code":""},{"path":"univariate-distributions.html","id":"standard-parameterisation-2","chapter":"3 Univariate distributions","heading":"3.6.1 Standard parameterisation","text":"random variable \\(x\\) following inverse gamma distribution denoted \n\\[\nx \\sim \\text{Inv-Gam}(\\alpha, \\beta)\n\\]\ntwo parameters \\(\\alpha >0\\) (shape parameter) \\(\\beta >0\\) (scale parameter) support \\(x >0\\).inverse \\(x\\) gamma distributed\n\\[\n\\frac{1}{x} \\sim \\text{Gam}(\\alpha, \\theta=\\beta^{-1})\n\\]\n\\(\\alpha\\) shared shape parameter \\(\\theta\\) scale parameter gamma distribution.inverse gamma distribution \\(\\text{Inv-Gam}(\\alpha, \\beta)\\)\ndensity\n\\[\np(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} (1/x)^{\\alpha+1} e^{-\\beta/x}\n\\]mean inverse gamma distribution \n\\[\\text{E}(x) = \\frac{\\beta}{\\alpha-1}\\]\nvariance\n\\[\\text{Var}(x) = \\frac{\\beta^2}{(\\alpha-1)^2 (\\alpha-2)}\\]Thus, mean exist restriction\n\\(\\alpha>1\\) variance exist \\(\\alpha>2\\).","code":""},{"path":"univariate-distributions.html","id":"wishart-parameterisation","chapter":"3 Univariate distributions","heading":"3.6.2 Wishart parameterisation","text":"inverse gamma distribution frequently used different\nset parameters\n\\(\\psi = 2\\beta\\) (scale parameter) \\(\\nu = 2\\alpha\\) (shape parameter), \nconversely \\(\\alpha=\\nu/2\\) \\(\\beta=\\psi/2\\).\nform called one-dimensional inverse Wishart distribution\n\\[\n\\text{W}^{-1}_1(\\psi, \\nu)\n\\]\nmean given \n\\[\n\\text{E}(x) = \\frac{\\psi}{\\nu-2}\n\\]\n\\(\\nu>2\\) variance\n\\[\n\\text{Var}(x) =\\frac{2 \\psi^2}{(\\nu-2)^2 (\\nu-4) }  \n\\]\n\\(\\nu >4\\).inverse univariate Wishart univariate Wishart distributions linked.\nrandom variable \\(x\\) inverse Wishart distributed\n\\[\nx \\sim \\text{W}^{-1}_1(\\psi, \\nu)\n\\]\ninverse \\(x\\) Wishart distributed inverted scale parameter:\n\\[\\frac{1}{x} \\sim \\text{W}_1(s^2=\\psi^{-1}, k=\\nu)\\]\n\\(k\\) shape parameter \\(s^2\\) scale parameter Wishart distribution.Instead \\(\\psi\\) \\(\\nu\\) may also equivalently use\n\\(\\kappa=\\nu-2\\) \n\\(\\mu = \\psi/(\\nu-2)\\)\nparameters inverse\nWishart distribution, \n\\[\n\\text{W}^{-1}_1(\\psi=\\kappa \\mu, \\nu=\\kappa+2)\n\\]\nmean\n\\[\\text{E}(x) = \\mu\\]\n\\(\\kappa>0\\) variance \n\\[\\text{Var}(x) = \\frac{2 \\mu^2}{\\kappa-2}\\] \n\\(\\kappa>2\\).\nmean parameterisation useful employing inverse gamma distribution\nprior posterior.Finally, \\(\\text{W}^{-1}_1(\\psi=\\nu \\tau^2, \\nu)\\), \n\\(\\tau^2 = \\mu \\frac{ \\kappa}{\\kappa+2} = \\frac{\\psi}{\\nu}\\) \nbiased mean parameter, get\nscaled inverse chi-squared distribution \\(\\tau^2 \\text{Inv-$\\chi^2_{\\nu}$}\\)\n\n\\[\n\\text{E}(x) = \\tau^2 \\frac{ \\nu}{\\nu-2}\n\\]\n\\(\\nu>2\\) \n\\[\n\\text{Var}(x) =\\frac{2 \\tau^4}{\\nu-4} \\frac{\\nu^2}{(\\nu-2)^2}\n\\]\n\\(\\nu >4\\).","code":""},{"path":"univariate-distributions.html","id":"location-scale-t-distribution-and-special-cases","chapter":"3 Univariate distributions","heading":"3.7 Location-scale \\(t\\)-distribution and special cases","text":"","code":""},{"path":"univariate-distributions.html","id":"location-scale-t-distribution","chapter":"3 Univariate distributions","heading":"3.7.1 Location-scale \\(t\\)-distribution","text":"location-scale \\(t\\)-distribution \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\) generalisation normal distribution.\nadditional parameter \\(\\nu > 0\\) (degrees freedom) controls probability mass tails. small values \\(\\nu\\) distribution heavy-tailed — indeed heavy \\(\\nu \\leq 1\\) even mean defined\n\\(\\nu \\leq 2\\) variance undefined.probability density \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\) \n\\[\np(x | \\mu, \\tau^2, \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {\\sqrt{\\pi \\nu \\tau^2}  \\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{(x-\\mu)^2}{\\nu \\tau^2} \\right)^{-(\\nu+1)/2}\n\\]\nsupport \\(x \\]-\\infty, \\infty[\\).\nmean (\\(\\nu>1\\))\n\\[\n\\text{E}(x) = \\mu\n\\]\nvariance (\\(\\nu>2\\))\n\\[\n\\text{Var}(x) = \\tau^2 \\frac{\\nu}{\\nu-2}\n\\]\\(\\nu \\rightarrow \\infty\\) location-scale \\(t\\)-distribution \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\) becomes\nnormal distribution \\(N(\\mu, \\tau^2)\\).R extraDistr package command dlst() returns \ndensity location-scale \\(t\\)-distribution,\nplst() corresponding cumulative density function qlst() quantile function.","code":""},{"path":"univariate-distributions.html","id":"location-scale-t-distribution-as-compound-distribution","chapter":"3 Univariate distributions","heading":"3.7.2 Location-scale \\(t\\)-distribution as compound distribution","text":"Suppose \n\\[\nx | s^2 \\sim N(\\mu,s^2)\n\\] corresponding density \\(p(x | s^2)\\)\nmean \\(\\text{E}(x | s^2) = \\mu\\) variance \\(\\text{Var}(x|s^2) = s^2\\).Now let variance \\(s^2\\) distributed inverse gamma / inverse Wishart\n\\[\ns^2 \\sim  \\text{W}^{-1}(\\psi=\\kappa \\sigma^2, \\nu=\\kappa+2) = \\text{W}^{-1}(\\psi=\\tau^2\\nu, \\nu)\n\\]\ncorresponding density \\(p(s^2)\\) mean \\(\\text{E}(s^2) = \\sigma^2 = \\tau^2 \\nu/(\\nu-2)\\).\nNote use mean parameterisation (\\(\\sigma^2, \\kappa\\))\ninverse chi-squared parameterisation (\\(\\tau^2, \\nu\\)).joint density \\(x\\) \\(s^2\\) \\(p(x, s^2) = p(x | s^2) p(s^2)\\).\ninterested marginal density \\(x\\):\n\\[\np(x) = \\int p(x, s^2) ds^2  = \\int p(s^2)  p(x | s^2) ds^2\n\\]\ncompound distribution normal fixed mean \\(\\mu\\)\nvariance \\(s^2\\) varying according inverse gamma distribution.\nCalculating integral results \nlocation-scale \\(t\\)-distribution parameters\n\\[\nx \\sim  \\text{lst}\\left(\\mu, \\sigma^2 \\frac{\\kappa}{\\kappa+2}, \\kappa+2\\right) = \\text{lst}\\left(\\mu, \\tau^2, \\nu\\right)\n\\]\nmean\n\\[\n\\text{E}(x) = \\mu\n\\]\n\nvariance\n\\[\n\\text{Var}(x) = \\sigma^2 =\\tau^2 \\frac{\\nu}{\\nu-2}\n\\]law total expectation variance can also directly verify \n\\[\n\\text{E}(x) = \\text{E}( \\text{E}(x| s^2) ) =\\mu\n\\]\n\n\\[\n\\text{Var}(x) = \\text{E}(\\text{Var}(x|s^2))+ \\text{Var}(\\text{E}(x|s^2)) = \\text{E}(s^2) = \\sigma^2 =\\tau^2 \\frac{\\nu}{\\nu-2}\n\\]","code":""},{"path":"univariate-distributions.html","id":"students-t-distribution","chapter":"3 Univariate distributions","heading":"3.7.3 Student’s \\(t\\)-distribution","text":"\\(\\mu=0\\) \\(\\tau^2=1\\) location-scale \\(t\\)-distribution becomes \nStudent’s \\(t\\)-distribution \\(t_\\nu\\)\nmean 0 (\\(\\nu>1\\)) variance \\(\\frac{\\nu}{\\nu-2}\\) (\\(\\nu>2\\)).can thus viewed generalisation standard normal distribution \\(N(0,1)\\).\\(y \\sim t_\\nu\\) \\(x = \\mu + \\tau y\\) distributed \\(x \\sim \\text{lst}(\\mu, \\tau^2, \\nu)\\).\\(\\nu \\rightarrow \\infty\\) \\(t\\)-distribution becomes equal \\(N(0,1)\\).R command dt() returns \ndensity \\(t\\)-distribution,\npt() corresponding cumulative density function qt() quantile function.","code":""},{"path":"univariate-distributions.html","id":"cauchy-and-standard-cauchy-distribution","chapter":"3 Univariate distributions","heading":"3.7.4 Cauchy and standard Cauchy distribution","text":"\\(\\nu=1\\) location-scale \\(t\\)-distribution becomes Cauchy distribution \\(\\text{Cau}(\\mu, \\tau)\\)\ndensity \\(p(x| \\mu, \\tau) = \\frac{\\tau}{\\pi (\\tau^2+(x-\\mu)^2)}\\).\\(\\nu=1\\) \\(t\\)-distribution becomes standard Cauchy distribution\n\\(\\text{Cau}(0, 1)\\) density \\(p(x) = \\frac{1}{\\pi (1+x^2)}\\).","code":""},{"path":"multivariate-distributions.html","id":"multivariate-distributions","chapter":"4 Multivariate distributions","heading":"4 Multivariate distributions","text":"","code":""},{"path":"multivariate-distributions.html","id":"categorical-distribution","chapter":"4 Multivariate distributions","heading":"4.1 Categorical distribution","text":"categorical distribution generalisation Bernoulli distribution \ntwo classes \\(K\\) classes.categorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\) describes\ndiscrete random variable \\(K\\) states (“categories”, “classes”, “bins”) \nparameter vector\n\\(\\boldsymbol \\pi= (\\pi_1, \\ldots, \\pi_K)^T\\) specifies\nprobability class \n\\(\\text{Pr}(\\text{\"class k\"}) = \\pi_k\\).\nparameters satisfy \\(\\pi_k \\[0,1]\\) \n\\(\\sum_{k=1}^K \\pi_k = 1\\), hence \\(K-1\\) independent parameters categorical distribution (\\(K\\)).two main ways numerically represent “class k”:“integer encoding”, .e. corresponding integer \\(k\\).“one hot encoding”, .e. indicator vector\n\\(\\boldsymbol x= (x_1, \\ldots, x_K)^T = (0, 0, \\ldots, 1, \\ldots, 0)^T\\) containing zeros everywhere except element \\(x_k=1\\) position \\(k\\). Thus \\(x_k \\\\{ 0, 1\\}\\) \\(\\sum_{k=1}^K x_k = 1\\).following use “one hot encoding”. Therefore sampling categorical distribution parameters \\(\\boldsymbol \\pi\\)\n\\[\n\\boldsymbol x\\sim \\text{Cat}(\\boldsymbol \\pi)\n\\]\nyields random index vector \\(\\boldsymbol x\\).corresponding probability mass function (PMF)\ncan written conveniently terms \\(x_k\\) \n\\[\np(\\boldsymbol x| \\boldsymbol \\pi) = \\prod_{k=1}^K \\pi_k^{x_k} =\n\\begin{cases}\n   \\pi_k  & \\text{} x_k = 1 \\\\\n\\end{cases}\n\\]\nlog PMF \n\\[\n\\log p(\\boldsymbol x| \\boldsymbol \\pi) = \\sum_{k=1}^K x_k \\log \\pi_k   =\n\\begin{cases}\n   \\log \\pi_k  & \\text{} x_k = 1 \\\\\n\\end{cases}\n\\]order explicit categorical distribution \\(K-1\\) \\(K\\) parameters\nrewrite log-density \n\\(\\pi_K = 1 - \\sum_{k=1}^{K-1} \\pi_k\\) \\(x_K = 1 - \\sum_{k=1}^{K-1} x_k\\) \n\\[\n\\begin{split}\n\\log p(\\boldsymbol x| \\boldsymbol \\pi) & =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + x_K \\log \\pi_K \\\\\n& =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_k  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\\\\n\\end{split}\n\\]\nNote particular reason choose \\(\\pi_K\\) \ndependent probabilities classes,\nplace \\(\\pi_k\\) may selected.expected value \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\pi\\), component notation\n\\(\\text{E}(x_k) = \\pi_k\\).\ncovariance matrix \\(\\text{Var}(\\boldsymbol x) = \\text{Diag}(\\boldsymbol \\pi) - \\boldsymbol \\pi\\boldsymbol \\pi^T\\), \ncomponent notation \\(\\text{Var}(x_i) = \\pi_i (1-\\pi_i)\\) \\(\\text{Cov}(x_i, x_j) = -\\pi_i \\pi_j\\).form categorical covariance matrix follows directly definition \nvariance \\(\\text{Var}(\\boldsymbol x) = \\text{E}( \\boldsymbol x\\boldsymbol x^T) - \\text{E}( \\boldsymbol x) \\text{E}( \\boldsymbol x)^T\\)\nnoting \\(x_i^2 = x_i\\) \\(x_i x_j = 0\\) \\(\\neq j\\).\nFurthermore, categorical covariance matrix singular construction, \\(K\\) random variables\n\\(x_1, \\ldots, x_K\\) dependent constraint \\(\\sum_{k=1}^K x_k = 1\\).\\(K=2\\) categorical distribution reduces Bernoulli \\(\\text{Ber}(\\theta)\\) distribution,\n\\(\\pi_1=\\theta\\) \\(\\pi_2=1-\\theta\\).","code":""},{"path":"multivariate-distributions.html","id":"multinomial-distribution","chapter":"4 Multivariate distributions","heading":"4.2 Multinomial distribution","text":"multinomial distribution \\(\\text{Mult}(n, \\boldsymbol \\pi)\\) arises repeated categorical sampling,\nfashion binomial distribution arises repeated Bernoulli sampling. Thus, \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) \\(n\\) independent \\(\\text{Cat}(\\boldsymbol \\pi)\\) random categorical variables\n\\(\\boldsymbol y= \\sum_{=1}^n \\boldsymbol x_i\\) distributed \\(\\text{Mult}(n, \\boldsymbol \\pi)\\).corresponding PMF describes probability pattern \\(y_1, \\ldots, y_K\\) \nsamples distributed across \\(K\\) classes (\\(n= \\sum_{k=1}^K y_k\\)):\n\\[\np(\\boldsymbol y| n, \\theta) = \\binom{n}{y_1, \\ldots, y_n} \\prod_{k=1}^K \\pi_k^{y_k}\n\\]\n\\(\\binom{n}{y_1, \\ldots, y_n}\\) multinomial coefficient.expected value \\(\\text{E}(\\boldsymbol y) = n \\boldsymbol \\pi\\), component notation\n\\(\\text{E}(y_k) = n \\pi_k\\).\ncovariance matrix \\(\\text{Var}(\\boldsymbol y) = n \\text{Diag}(\\boldsymbol \\pi) - n \\boldsymbol \\pi\\boldsymbol \\pi^T\\), \ncomponent notation \\(\\text{Var}(x_i) = n \\pi_i (1-\\pi_i)\\) \\(\\text{Cov}(x_i, x_j) = -n \\pi_i \\pi_j\\).Standardised unit interval:\n\\[\\frac{y_i}{n} \\\\left\\{0,\\frac{1}{n},\\frac{2}{n},...,1\\right\\}\\]\n\\[\\text{E}\\left(\\frac{\\boldsymbol y}{n}\\right) = \\boldsymbol \\pi\\]\n\\[\\text{Var}\\left(\\frac{\\boldsymbol y}{n}\\right) = \\frac{ \\text{Diag}(\\boldsymbol \\pi)}{n} - \\frac{ \\boldsymbol \\pi\\boldsymbol \\pi^T}{n}\\]\n\\[\\text{Var}\\left(\\frac{y_i}{n}\\right)=\\frac{\\pi_i(1-\\pi_i)}{n}\\]\n\\[\\text{Cov}\\left(\\frac{y_i}{n},\\frac{y_j}{n}\\right)=-\\frac{\\pi_i\\pi_j}{n} \\]multinomial distribution may illustrated urn model\ndistributing \\(n\\) balls \\(K\\) bins:\\(n=1\\) multinomial distribution reduces categorical distribution.\\(K=2\\) multinomial distribution reduced Binomial distribution.","code":""},{"path":"multivariate-distributions.html","id":"dirichlet-distribution","chapter":"4 Multivariate distributions","heading":"4.3 Dirichlet distribution","text":"","code":""},{"path":"multivariate-distributions.html","id":"standard-parameterisation-3","chapter":"4 Multivariate distributions","heading":"4.3.1 Standard parameterisation","text":"Dirichlet distribution multivariate generalisation beta distribution.Dirichlet distributed random vector denoted \n\\[\n\\boldsymbol x\\sim \\text{Dir}(\\boldsymbol \\alpha)\n\\]\nparameter \\(\\boldsymbol \\alpha= (\\alpha_1,...,\\alpha_K)^T >0\\)\n\\(K\\geq 2\\) \nsupport \\(\\boldsymbol x\\) \\(K-1\\) dimensional simplex\n\\(x_i \\[0,1]\\) \\(\\sum^{K}_{=1} x_i = 1\\).Dirichlet random variable can visualised \nbreaking unit stick length one\n\\(K\\) pieces length \\(x_1\\) \\(x_K\\):density Dirichlet distribution \\(\\text{Dir}(\\boldsymbol \\alpha)\\) \n\\[\np(\\boldsymbol x| \\boldsymbol \\alpha) = \\frac{1}{B(\\boldsymbol \\alpha)}  \\prod_{k=1}^K x_k^{\\alpha_k-1}\n\\]\ndepends beta function multivariate argument\ndefined \n\\[\nB(\\boldsymbol z) = \\frac{ \\prod_{k=1}^K \\Gamma(z_k) }{\\Gamma\\left( \\sum_{k=1}^K   z_k\\right)}\n\\]\\(K=2\\) Dirichlet distribution reduces beta distribution.","code":""},{"path":"multivariate-distributions.html","id":"mean-parameterisation-1","chapter":"4 Multivariate distributions","heading":"4.3.2 Mean parameterisation","text":"Instead employing \\(\\boldsymbol \\alpha\\) parameter vector another useful reparameterisation \\(\\text{Dir}(\\boldsymbol \\pi, k)\\) Dirichlet\ndistribution terms mean parameter\n\\(\\boldsymbol \\pi\\), \\(\\pi_i \\[0,1]\\) \\(\\sum^{K}_{=1}\\pi_i = 1\\),\nconcentration parameter \\(k > 0\\). given \n\\[\nk = \\sum^{K}_{=1}\\alpha_i\n\\]\n\n\\[\n\\boldsymbol \\pi= \\frac{\\boldsymbol \\alpha}{k}\n\\]\noriginal parameters can recovered \\(\\alpha= \\boldsymbol \\pi k\\).mean variance Dirichlet distribution expressed terms \\(\\boldsymbol \\pi\\) \\(k\\) \n\\[\n\\text{E}(\\boldsymbol x) = \\boldsymbol \\pi\n\\]\n\n\\[\\text{Var}(x_i)=\\frac{\\pi_i(1-\\pi_i)}{k+1}\\]\n\\[\\text{Cov}(x_i,x_j)=-\\frac{\\pi_i \\pi_j}{k+1}\\]Finally, note mean variance continuous Dirichlet distribution\nclosely match unit-standardised discrete multinomial distribution .","code":""},{"path":"multivariate-distributions.html","id":"multivariate-normal-distribution","chapter":"4 Multivariate distributions","heading":"4.4 Multivariate normal distribution","text":"univariate normal distribution random scalar \\(x\\) generalises multivariate normal distribution random vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T\\).\\(\\boldsymbol x\\) follows multivariate normal distribution write\n\\[\n\\boldsymbol x\\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\n\\]\n\\(\\boldsymbol \\mu\\) mean (location) parameter \\(\\boldsymbol \\Sigma\\) variance (scale) parameter.corresponding density \\[p(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) = \\det(2 \\pi \\boldsymbol \\Sigma)^{-\\frac{1}{2}} \\exp\\left({{-\\frac{1}{2}} \\underbrace{\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1 \\times d} \\underbrace{\\boldsymbol \\Sigma^{-1}}_{d \\times d} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d \\times 1} }_{1 \\times 1 = \\text{scalar!}}}\\right)\\]\\(\\det(2 \\pi \\boldsymbol \\Sigma)^{-\\frac{1}{2}} = \\det(2 \\pi \\boldsymbol I_d)^{-\\frac{1}{2}} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}} = (2 \\pi)^{-d/2} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}}\\)\ndensity can also written \n\\[\np(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) = (2\\pi)^{-\\frac{d}{2}} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)   \\right)\n\\]\nexplicit occurence dimension \\(d\\).expectation \\(\\boldsymbol x\\) \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\) variance \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\).\\(d=1\\) random vector \\(\\boldsymbol x=x\\) scalar \\(\\boldsymbol \\mu= \\mu\\) \\(\\boldsymbol \\Sigma= \\sigma^2\\) multivariate normal density reduces univariate normal density.","code":""},{"path":"multivariate-distributions.html","id":"wishart-distribution","chapter":"4 Multivariate distributions","heading":"4.5 Wishart distribution","text":"Wishart distribution multivariate generalisation gamma distribution.Recall gamma distribution can motivated distribution \nsums squared normal random variables. Likewise, Wishart distribution\ncan understood sum squared multivariate normal variables:\n\\[\n\\boldsymbol z_1,\\boldsymbol z_2,\\ldots,\\boldsymbol z_k \\stackrel{\\text{iid}}\\sim N_d(0,\\boldsymbol S)\n\\]\n\\(\\boldsymbol S=(s_{ij})\\) specified covariance matrix.\nrandom variable\n\\[\\underbrace{\\boldsymbol X}_{d\\times d}=\\sum^{k}_{=1}\\underbrace{\\boldsymbol z_i\\boldsymbol z_i^T}_{d\\times d}\\]\nrandom matrix distributed \n\\[\n\\boldsymbol X\\sim  \\text{W}_d\\left(\\boldsymbol S, k\\right)\n\\]\nmean\n\\[\n\\text{E}(\\boldsymbol X) = k \\boldsymbol S\n\\]\nvariances\n\\[\n\\text{Var}(x_{ij})  = k \\left(s^2_{ij}+s_{ii} s_{jj}  \\right)\n\\]often also employ Wishart distribution mean parameterisation\n\\[\n\\text{W}_d\\left(\\boldsymbol S= \\frac{\\boldsymbol M}{k}, k \\right)\n\\]\nparameters \\(\\boldsymbol M= k \\boldsymbol S\\) \\(k\\). parameterisation \nmean \n\\[\n\\text{E}(\\boldsymbol X) = \\boldsymbol M= (\\mu_{ij})\n\\]\nvariances \n\\[\n\\text{Var}(x_{ij})  = \\frac{ \\mu^2_{ij}+\\mu_{ii}\\mu_{jj} }{k}\n\\]\\(\\boldsymbol S\\) \\(\\boldsymbol M\\) scalar rather matrix multivariate Wishart distribution reduces univariate Wishart aka gamma distribution.","code":""},{"path":"multivariate-distributions.html","id":"inverse-wishart-distribution","chapter":"4 Multivariate distributions","heading":"4.6 Inverse Wishart distribution","text":"inverse Wishart distribution multivariate generalisation inverse gamma distribution linked Wishart distribution.random matrix \\(\\boldsymbol X\\) following inverse Wishart distribution\ndenoted \n\\[\n\\boldsymbol X\\sim \\text{W}^{-1}_d\\left(\\boldsymbol \\Psi, \\nu\\right)\n\\]\n\\(\\boldsymbol \\Psi\\) scale parameter \\(\\nu\\) shape parameter.\ncorresponding mean given \n\\[\n\\text{E}(\\boldsymbol X) = \\frac{\\boldsymbol \\Psi}{\\nu-d-1}\n\\]\n\nvariances \n\\[\n\\text{Var}(x_{ij})= \\frac{(\\nu-d+1) \\psi_{ij}^2 + (\\nu-d-1) \\, \\psi_{ii} \\psi_{jj} }{(\\nu-d)(\\nu-d-1)^2(\\nu-d-3)}\n\\]inverse \\(\\boldsymbol X\\) Wishart distributed:\n\\[\n\\boldsymbol X^{-1} \\sim \\text{W}_d\\left( \\boldsymbol S= \\boldsymbol \\Psi^{-1}   \\, , k = \\nu \\right)\n\\]Instead \\(\\boldsymbol \\Psi\\) \\(\\nu\\) may use mean parameterisation \nparameters\n\\(\\kappa = \\nu-d-1\\) \n\\(\\boldsymbol M= \\boldsymbol \\Psi/(\\nu-d-1)\\):\n\\[\n\\boldsymbol X\\sim \\text{W}^{-1}_d\\left(\\boldsymbol \\Psi=  \\kappa  \\boldsymbol M\\, , \\, \\nu= \\kappa+d+1\\right)\n\\]\nmean\n\\[\n\\text{E}(\\boldsymbol X) =\\boldsymbol M\n\\]\nvariances\n\\[\n\\text{Var}(x_{ij})= \\frac{(\\kappa+2) \\mu_{ij}^2 + \\kappa \\, \\mu_{ii} \\mu_{jj} }{(\\kappa + 1)(\\kappa-2)}\n\\]\\(\\boldsymbol \\Psi\\) \\(\\boldsymbol M\\) scalar rather matrix multivariate inverse Wishart distribution reduces univariate inverse Wishart aka inverse gamma distribution.","code":""}]
