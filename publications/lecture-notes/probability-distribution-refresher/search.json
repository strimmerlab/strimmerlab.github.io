[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability and Distribution Refresher",
    "section": "",
    "text": "Welcome\nThe Probability and Distribution Refresher notes were written by Korbinian Strimmer from 2018–2025. This version is from 11 October 2025.\nIf you have any questions, comments, or corrections please get in touch! 1",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Probability and Distribution Refresher",
    "section": "Updates",
    "text": "Updates\nThe lecture notes will be updated from time to time.\nThe most current version is found at the web page for the\n\nonline version of the Probability and Distribution Refresher notes.\n\nThere you can also download the Probability and Distribution Refresher notes as\n\nPDF in A4 format for printing (double page layout), or as\n6x9 inch PDF for use on tablets (single page layout).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Probability and Distribution Refresher",
    "section": "License",
    "text": "License\nThese notes are licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Probability and Distribution Refresher",
    "section": "",
    "text": "Email address: korbinian.strimmer@manchester.ac.uk↩︎",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "00-preface.html",
    "href": "00-preface.html",
    "title": "Preface",
    "section": "",
    "text": "About the author\nHello! My name is Korbinian Strimmer and I am a Professor in Statistics. I am a member of the Statistics group at the Department of Mathematics of the University of Manchester. You can find more information about me on my home page.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#about-the-notes",
    "href": "00-preface.html#about-the-notes",
    "title": "Preface",
    "section": "About the notes",
    "text": "About the notes\nThese supplementary notes aim to provide a quick refresher of some essentials in combinatorics and probability as well as to offer an overview over selected univariate and multivariate distributions.\nThe notes are supporting information for a number of lecture notes of statistical courses I am or have been teaching at the Department of Mathematics of the University of Manchester.\nThis includes the currently offered modules:\n\nMATH27720 Statistics 2: Likelihood and Bayes and\nMATH38161 Multivariate Statistics\n\nas well as the retired module (not offered any more):\n\nMATH20802 Statistical Methods.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-combinatorics.html",
    "href": "01-combinatorics.html",
    "title": "1  Combinatorics",
    "section": "",
    "text": "1.1 Some basic mathematical notation\nScalar quantity: plain font, typically lower case (\\(x\\), \\(\\theta\\), n), sometimes upper case (\\(K\\), \\(R^2\\), distribution functions \\(F\\), \\(P\\), \\(Q\\)).\nSets: plain font, upper case (\\(\\Omega, \\mathcal{F}\\))\nVector quantity: bold font, lower case (\\(\\boldsymbol x\\), \\(\\boldsymbol \\theta\\)).\nMatrix quantity: bold font, upper case (\\(\\boldsymbol X\\), \\(\\boldsymbol \\Sigma\\)).\nSummation: \\[\n\\sum_{i=1}^n x_i = x_1 + x_2 + \\ldots + x_n\n\\]\nProduct: \\[\n\\begin{split}\n\\prod_{i=1}^n x_i &= x_1 \\times x_2 \\times \\ldots \\times x_n\\\\\n&= x_1  x_2  \\ldots  x_n\n\\end{split}\n\\] The multiplication sign \\(\\times\\) between the factors is usually omitted unless it is needed for clarity.\nIndicator function (in Iverson bracket notation): \\[\n[A] =\n\\begin{cases}\n1 & \\text{if $A$ is true}\\\\\n0 & \\text{if $A$ is not true}\\\\\n\\end{cases}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "01-combinatorics.html#number-of-permutations",
    "href": "01-combinatorics.html#number-of-permutations",
    "title": "1  Combinatorics",
    "section": "1.2 Number of permutations",
    "text": "1.2 Number of permutations\nThe number of possible orderings, or permutations, of \\(n\\) distinct items is the number of ways to put \\(n\\) items in \\(n\\) bins with exactly one item in each bin. It is given by the factorial \\[\nn! = \\prod_{i=1}^n i = 1 \\times 2 \\times \\ldots \\times n\n\\] where \\(n\\) is a positive integer. For \\(n=0\\) the factorial is defined as \\[\n0! = 1\n\\] as there is exactly one permutation of zero objects.\nThe factorial can also be obtained using the gamma function \\[\n\\Gamma(x) = \\int_0^\\infty t^{x-1} e^{-t} dt\n\\] which can be viewed as continuous version of the factorial with \\(\\Gamma(x) = (x-1)!\\) for any positive integer \\(x\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "01-combinatorics.html#de-moivre-sterling-approximation-of-the-factorial",
    "href": "01-combinatorics.html#de-moivre-sterling-approximation-of-the-factorial",
    "title": "1  Combinatorics",
    "section": "1.3 De Moivre-Sterling approximation of the factorial",
    "text": "1.3 De Moivre-Sterling approximation of the factorial\nThe factorial is frequently approximated by the following formula derived by Abraham de Moivre (1667–1754) and James Stirling (1692-1770) \\[\nn! \\approx \\sqrt{2 \\pi} n^{n+\\frac{1}{2}} e^{-n}\n\\] or equivalently on logarithmic scale \\[\n\\log n!  \\approx \\left(n+\\frac{1}{2}\\right) \\log n  -n + \\frac{1}{2}\\log \\left( 2 \\pi\\right)\n\\] The approximation is good for small \\(n\\) (but fails for \\(n=0\\)) and becomes more and more accurate with increasing \\(n\\). For large \\(n\\) the approximation can be simplified to \\[\n\\log n! \\approx  n \\log n  -n\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "01-combinatorics.html#multinomial-and-binomial-coefficient",
    "href": "01-combinatorics.html#multinomial-and-binomial-coefficient",
    "title": "1  Combinatorics",
    "section": "1.4 Multinomial and binomial coefficient",
    "text": "1.4 Multinomial and binomial coefficient\nThe number of possible permutation of \\(n\\) items of \\(K\\) distinct types, with \\(n_1\\) of type 1, \\(n_2\\) of type 2 and so on, equals the number of ways to put \\(n\\) items into \\(K\\) bins with \\(n_1\\) items in the first bin, \\(n_2\\) in the second and so on. It is given by the multinomial coefficient \\[\n\\binom{n}{n_1, \\ldots, n_K} = \\frac {n!}{n_1! \\, n_2! \\, \\ldots \\, n_K! }\n\\] with \\(\\sum_{k=1}^K n_k = n\\) and \\(K \\leq n\\). Note that it equals the number of permutation of all items divided by the number of permutations of the items in each bin (or of each type).\nIf all \\(n_k=1\\) and hence \\(K=n\\) the multinomial coefficient reduces to the factorial.\nIf there are only two bins / types (\\(K=2\\)) the multinomial coefficients becomes the binomial coefficient \\[\n\\binom{n}{n_1} = \\binom{n}{n_1, n-n_1}    =  \\frac {n!}{n_1! \\, (n - n_1)!}\n\\] which counts the number of ways to choose \\(n_1\\) elements from a set of \\(n\\) elements.\nFor large \\(n\\) and \\(n_k\\) we can apply the De Moivre-Sterling approximation to the multinomial coefficient, yielding \\[\n\\log\\binom{n}{n_1, \\ldots, n_K} = - n \\sum_{k=1}^K \\frac{n_k}{n} \\log\\left( \\frac{n_k}{n} \\right)\n\\] Note this is \\(n\\) times the Shannon-Gibbs entropy of a categorical distribution with \\(n_k/n\\) as class probabilities.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "02-probability.html",
    "href": "02-probability.html",
    "title": "2  Probability",
    "section": "",
    "text": "2.1 Random variables\nA random variable describes a random experiment. The set of all possible outcomes is the sample space of the random variable and is denoted by \\(\\Omega\\). If \\(\\Omega\\) is countable then the random variable is discrete, otherwise it is continuous. For a discrete random variable the sample space \\(\\Omega = \\{\\omega_1, \\omega_2, \\ldots\\}\\) is composed of a finite or infinite number of elementary outcomes \\(\\omega_i\\).\nAn event \\(A \\subseteq \\Omega\\) is a subset of \\(\\Omega\\). This includes as special cases the complete set \\(\\Omega\\) (“certain event”) and the empty set \\(\\emptyset\\) (“impossible event”). The set of all possible events is denoted by \\(\\mathcal{F}\\). The complementary event \\(A^C = \\Omega \\setminus A\\) is the complement of the set \\(A\\) in the sample space \\(\\Omega\\). Two events \\(A_1\\) and \\(A_2\\) are mutually exclusive if the sets are disjoint with \\(A_1 \\cap A_2 = \\emptyset\\).\nFor a discrete random variable, the elementary outcomes \\(\\omega_i\\) are referred to as elementary events, and they are all mutually exclusive. An event \\(A\\) consists of a number of elementary events \\(\\omega_i \\in A\\) and the complementary event is given by \\(A^C = \\{\\omega_i \\in \\Omega:  \\omega_i \\notin A\\}\\).\nThe probability of an event \\(A\\) is denoted by \\(\\text{Pr}(A)\\). Broadly, \\(\\text{Pr}(A)\\) provides a measure of the size of the set \\(A\\) relative to the set \\(\\Omega\\). The probability measure \\(\\text{Pr}(A)\\) satisfies the three axioms of probability:\nThis implies\nFrom the above it is evident that probability is closely linked to set theory, in particular to measure theory which serves as the theoretical foundations of probability and generalisations. For instance, if \\(\\text{Pr}(\\emptyset) = 0\\) is assumed instead of \\(\\text{Pr}(\\Omega) = 1\\), this leads to the axioms for a positive measure (of which probability is a special case).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#random-variables",
    "href": "02-probability.html#random-variables",
    "title": "2  Probability",
    "section": "",
    "text": "\\(\\text{Pr}(A) \\geq 0\\), probabilities are non-negative,\n\\(\\text{Pr}(\\Omega) = 1\\), the certain event has probability 1, and\n\\(\\text{Pr}(A_1 \\cup A_2 \\cup \\ldots) = \\sum_i \\text{Pr}(A_i)\\), the probability of countable mutually exclusive events \\(A_i\\) is additive.\n\n\n\n\\(\\text{Pr}(A) \\leq 1\\), probability values lie within the range \\([0,1]\\),\n\\(\\text{Pr}(A^C) = 1 - \\text{Pr}(A)\\), the probability of the complement, and\n\\(\\text{Pr}(\\emptyset) = 0\\), the impossible event has probability 0.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#conditional-probability",
    "href": "02-probability.html#conditional-probability",
    "title": "2  Probability",
    "section": "2.2 Conditional probability",
    "text": "2.2 Conditional probability\nConsider two events \\(A\\) and \\(B\\), which may not be be mutually exclusive. The probability of the event “\\(A\\) and \\(B\\)” is given by the probability of the set intersection \\(\\text{Pr}(A \\cap B)\\). The probability of the event “\\(A\\) or \\(B\\)” is given by the probability of the set union \\[\n\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\cap B)\\,.\n\\] This identity follows from the axioms.\nThe conditional probability of event \\(A\\) assuming event \\(B\\) has occurred is given by \\[\n\\text{Pr}(A | B) = {\\text{Pr}( A \\cap B) \\over \\text{Pr}(B)}\n\\] Essentially, now \\(B\\) acts as the new sample space relative to which \\(A\\) is measured, restricting it from \\(\\Omega\\). Note that \\(\\text{Pr}(A | B)\\) is generally not the same as \\(\\text{Pr}(B | A)\\), see Bayes’ theorem below.\nImportantly, it can be seen that any probability may be viewed as conditional, namely relative to \\(\\Omega\\) as \\(\\text{Pr}(A) = \\text{Pr}(A| \\Omega)\\).\nFrom the definition of conditional probability we derive the product rule \\[\n\\begin{split}\n\\text{Pr}( A \\cap B) &= \\text{Pr}(A | B)\\, \\text{Pr}(B) \\\\\n&= \\text{Pr}(B | A)\\, \\text{Pr}(A)\n\\end{split}\n\\] which in turn yields Bayes’ theorem \\[\n\\text{Pr}(A | B ) = \\text{Pr}(B | A) { \\text{Pr}(A) \\over \\text{Pr}(B)}\n\\] This theorem is useful for changing the order of conditioning and it plays a key role in Bayesian statistics.\nIf \\(\\text{Pr}( A \\cap B) =  \\text{Pr}(A) \\, \\text{Pr}(B)\\) then the two events \\(A\\) and \\(B\\) are independent with \\(\\text{Pr}(A | B ) = \\text{Pr}(A)\\) and \\(\\text{Pr}(B | A ) = \\text{Pr}(B)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#probability-mass-and-density-function",
    "href": "02-probability.html#probability-mass-and-density-function",
    "title": "2  Probability",
    "section": "2.3 Probability mass and density function",
    "text": "2.3 Probability mass and density function\nThe distribution (or law) of a random variable \\(x\\) with sample space \\(\\Omega\\) gives the probability for each value or a range of values of \\(x\\) according to the underlying probability measure. This is done in practise by employing probability mass functions (for discrete random variables) or probability density functions (for continuous random variables).\nHere \\(x\\) is a scalar random variable, denoted by lower case and plain font. We also write \\(x\\) for the outcomes of the random variable. Thus, we use the same symbol to denote a random variable and its realisations.1\nFor a discrete random variable we define the event \\(A = \\{x: x=a\\} = \\{a\\}\\) (corresponding to a single elementary event) and get the probability \\[\n\\text{Pr}(A) = \\text{Pr}(x=a) = f(a)\n\\] directly from the probability mass function (pmf). The pmf has the property that \\(\\sum_{x \\in \\Omega} f(x) = 1\\) and that \\(f(x) \\in [0,1]\\).\nFor continuous random variables employ a probability density function (pdf) instead. We define the event \\(A = \\{x: a &lt; x \\leq a + da\\}\\) (corresponding to an infinitesimal interval) and then assign the probability \\[\n\\text{Pr}(A) = \\text{Pr}( a &lt; x \\leq a + da) = f(a) da \\,.\n\\] Similarly, the probability of the event \\(A = \\{x:a_1 &lt; x \\leq a_2  \\}\\) is given by \\[\n\\text{Pr}(A) = \\text{Pr}( a_1 &lt; x \\leq a_2) = \\int_{a_1}^{a_2} f(a) da \\,.\n\\] The pdf has the property that \\(\\int_{x \\in \\Omega} f(x) dx = 1\\) but in contrast to a pmf the density \\(f(x)\\geq 0\\) may take on values larger than 1.\nIt is sometimes convenient to refer to a pdf or a pmf collectively as probability density mass function (pdmf) without specifying whether \\(x\\) is continuous or discrete.\nThe set of all \\(x\\) for which \\(f(x)\\) is positive is called the support of the pdmf.\nUsing the pdmf, the probability of general event \\(A  \\subseteq \\Omega\\) is given by \\[\n\\text{Pr}(A) =\n\\begin{cases}\n\\sum_{x \\in A} f(x) & \\text{discrete case} \\\\\n\\int_{x \\in A} f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\]\n\n\n\n\n\n\nFigure 2.1: Illustration of i) pdmf, ii) distribution function and iii) quantile function for a continuous (first column) and a discrete random variable (second column).\n\n\n\nFigure 2.1 (first row) illustrates the pdmf for a continuous and discrete random variable.\nIn the above we denoted the pdmf by the lower case letter \\(f\\) though we also often use \\(p\\) or \\(q\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#cumulative-distribution-function",
    "href": "02-probability.html#cumulative-distribution-function",
    "title": "2  Probability",
    "section": "2.4 Cumulative distribution function",
    "text": "2.4 Cumulative distribution function\nAs alternative to the pdmf we can describe the random variable using a cumulative distribution function (cdf). This requires an ordering so that we can define the event \\(A = \\{x: x \\leq a \\}\\) and compute its probability as \\[\nF(a) = \\text{Pr}(A) = \\text{Pr}( x \\leq a ) =\n\\begin{cases}\n\\sum_{x \\in A} f(x) & \\text{discrete case} \\\\\n\\int_{x \\in A} f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\] Th cdf is denoted by the same letter as the pdmf but in upper case (usually \\(F\\), \\(P\\) and \\(Q\\)). By construction the cumulative distribution function is monotonically non-decreasing and its value ranges from 0 to 1. For a discrete random variable \\(F(a)\\) is a step function with jumps of size \\(f(\\omega_i)\\) at the elementary outcomes \\(\\omega_i\\).\nWith the help of the cdf we can compute the probability of the event \\(A = \\{x:a_1 &lt; x \\leq a_2  \\}\\) simply as \\[\n\\text{Pr}( A ) = F(a_2)-F(a_1) \\,.\n\\] This works both for discrete and continuous random variables.\nFigure 2.1 (second row) illustrates the distribution function for a continuous and discrete random variable.\nIt is common to use the same upper case letter as the cdf to name the distribution. Thus, if a random variable \\(x\\) has distribution \\(F\\) we write \\(x \\sim F\\), and this implies it has a pdmf \\(f(x)\\) and cdf \\(F(x)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#quantile-function-and-quantiles",
    "href": "02-probability.html#quantile-function-and-quantiles",
    "title": "2  Probability",
    "section": "2.5 Quantile function and quantiles",
    "text": "2.5 Quantile function and quantiles\nThe quantile function is defined as \\(q_F(b) = \\min\\{ x: F(x) \\geq b \\}\\). For a continuous random variable the quantile function simplifies to \\(q_F(b) = F^{-1}(b)\\), i.e. it is the ordinary inverse \\(F^{-1}(b)\\) of the distribution function.\nFigure 2.1 (third row) illustrates the quantile function for a continuous and discrete random variable.\nThe quantile \\(x\\) of order \\(b\\) of the distribution \\(F\\) is often denoted by \\(x_b= q_F(b)\\).\nThe 25% quantile \\(x_{1/4} = x_{25\\%} =  q_F(1/4)\\) is called the first quartile or lower quartile.\nThe 50% quantile \\(x_{1/2} = x_{50\\%} = q_F(1/2)\\) is called the second quartile or median.\nThe 75% quantile \\(x_{3/4} = x_{75\\%} = q_F(3/4)\\) is called the third quartile or upper quartile.\nThe interquartile range is the difference between the upper and lower quartiles and equals \\(\\text{IQR}(F) =\nq_F(3/4) - q_F(1/4)\\).\nThe quantile function is also useful for generating general random variates from uniform random variates. If \\(y\\sim \\text{Unif}(0,1)\\) then \\(x=q_F(y) \\sim F\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#expectation-or-mean",
    "href": "02-probability.html#expectation-or-mean",
    "title": "2  Probability",
    "section": "2.6 Expectation or mean",
    "text": "2.6 Expectation or mean\nThe expected value of a random variable \\(x\\sim F\\) is defined as the weighted average over all possible outcomes, with the weight given by the pdmf \\(f(x)\\): \\[\n\\begin{split}\n\\text{E}(F) &= \\text{E}(x)\\\\\n& = \\mu =\n\\begin{cases}\n\\sum_{x \\in \\Omega} f(x) \\, x & \\text{discrete case} \\\\\n\\int_{x \\in \\Omega} f(x) \\, x \\, dx  & \\text{continuous case} \\\\\n\\end{cases}\\\\\n\\end{split}\n\\] We may can also write \\(\\text{E}_{F}(x)\\) as a reminder that the expectation is taken with regard to the distribution \\(F\\). Usually, the subscript \\(F\\) is left out if there are no ambiguities. A further variant is to write the expectation as \\(\\text{E}(F)\\) to indicate that the mean is a functional of the distribution \\(F\\).\nBecause the sum or integral may diverge, not all distributions have finite means so the mean does not always exist (in contrast to the median, or quantiles in general). For example, the location-scale \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mu, \\tau^2)\\) does not have a mean for a degree of freedom in the range \\(0 &lt; \\nu \\leq 1\\) (see Section 4.6).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#variance",
    "href": "02-probability.html#variance",
    "title": "2  Probability",
    "section": "2.7 Variance",
    "text": "2.7 Variance\nThe variance of a random variable \\(x\\sim F\\) is the expected value of the squared deviation around the mean \\(\\mu = \\text{E}(x)\\): \\[\n\\begin{split}\n\\text{Var}(F)& = \\text{Var}(x) \\\\\n       &= \\text{E}\\left( (x - \\mu))^2 \\right) \\\\\n       &=  \\text{E}(x^2)-\\mu^2\n\\end{split}\n\\] By construction, \\(\\text{Var}(x) \\geq 0\\).\nThe notation \\(\\text{Var}(F)\\) highlights that the variance is a functional of the distribution \\(F\\). Occasionally, we write \\(\\text{Var}_F(x)\\) indicate that the expectation is taken with regard to the distribution \\(F\\).\nLike the mean, the variance may diverge and hence not necessarily exists for all distribution. For example, the location-scale \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mu, \\tau^2)\\) does not have a variance for the degree of freedom in the range \\(0 &lt; \\nu \\leq 2\\) (see Section 4.6).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#moments-of-a-distribution",
    "href": "02-probability.html#moments-of-a-distribution",
    "title": "2  Probability",
    "section": "2.8 Moments of a distribution",
    "text": "2.8 Moments of a distribution\nThe \\(n\\)-th moment of a distribution \\(F\\) for a random variable \\(x\\) is defined as follows: \\[\n\\mu_n(F) = \\text{E}(x^n)\n\\]\nSpecial important cases are the\n\nZeroth moment: \\(\\mu_0(F) = \\text{E}(x^0) = 1\\) (since the pdmf integrates to one)\nFirst moment: \\(\\mu_1(F) = \\text{E}(x^1) = \\text{E}(x) = \\mu\\) (=the mean)\nSecond moment: \\(\\mu_2(F) = \\text{E}(x^2)\\)\n\nThe \\(n\\)-th central moment centred around the mean \\(\\text{E}(x) = \\mu\\) is given by \\[\nm_n(F) = \\text{E}((x-\\mu)^n)\n\\]\nThe first few central moments are the\n\nZeroth central moment: \\(m_0(F) = \\text{E}((x-\\mu)^0) = 1\\)\nFirst central moment: \\(m_1(F) = \\text{E}((x-\\mu)^1) = 0\\)\nSecond central moment: \\(m_2(F) = \\text{E}\\left( (x - \\mu)^2 \\right)\\) (=the variance)\n\nThe moments of a distribution are not necessarily all finite, i.e. some moments may not exist. For example, the location-scale \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mu, \\tau^2)\\) only has finite moments of degree smaller than the degree of freedom \\(\\nu\\) (see Section 4.6).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#expectation-of-a-transformed-random-variable",
    "href": "02-probability.html#expectation-of-a-transformed-random-variable",
    "title": "2  Probability",
    "section": "2.9 Expectation of a transformed random variable",
    "text": "2.9 Expectation of a transformed random variable\nOften, one needs to find the mean of a transformed random variable. If \\(x\\sim F_x\\) and \\(y= h(x)\\) with \\(y \\sim F_y\\) then one can directly apply the above definition to obtain \\(\\text{E}(y) = \\text{E}(F_y)\\). However, this requires knowledge of the transformed pdmf \\(f_y(y)\\) (see Chapter 3 for more details about variable transformations).\nAs an alternative, the “law of the unconscious statistician”(LOTUS) provides a convenient shortcut to compute the mean of the transformed random variable \\(y=h(x)\\) using only the pdmf of the original variable \\(x\\): \\[\n\\text{E}(h(x)) =\n\\begin{cases}\n\\sum_{x \\in \\Omega} f(x) \\, h(x) & \\text{discrete case} \\\\\n\\int_{x \\in \\Omega}  f(x) \\, h(x) \\, dx & \\text{continuous case} \\\\\n\\end{cases}\n\\] Note this is not an approximation but equivalent to obtaining the mean using the transformed pdmf.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#probability-as-expectation",
    "href": "02-probability.html#probability-as-expectation",
    "title": "2  Probability",
    "section": "2.10 Probability as expectation",
    "text": "2.10 Probability as expectation\nProbability itself can also be understood as an expectation.\nFor an event \\(A  \\subseteq \\Omega\\) we define a corresponding indicator function \\([x \\in A]\\). From LOTUS it then follows immediately that \\[\n\\begin{split}\n\\text{E}\\left( \\left[x \\in A\\right] \\right) &=\n\\begin{cases}\n\\sum_{x \\in A} f(x)  & \\text{discrete case} \\\\\n\\int_{x \\in A}  f(x)  \\, dx & \\text{continuous case} \\\\\n\\end{cases}\\\\\n& =\\text{Pr}(A)\n\\end{split}\n\\]\nThis relation is called the “fundamental bridge” between probability and expectation. Interestingly, one can develop the whole theory of probability from this perspective (e.g., Whittle 2000).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#jensens-inequality-for-the-expectation",
    "href": "02-probability.html#jensens-inequality-for-the-expectation",
    "title": "2  Probability",
    "section": "2.11 Jensen’s inequality for the expectation",
    "text": "2.11 Jensen’s inequality for the expectation\nIf \\(h(\\boldsymbol x)\\) is a convex function then the following inequality holds:\n\\[\n\\text{E}(h(\\boldsymbol x)) \\geq h(\\text{E}(\\boldsymbol x))\n\\]\nRecall: a convex function (such as \\(x^2\\)) has the shape of a “valley”.\nAn example of Jensen’s inequality is \\(\\text{E}(x^2)\\geq \\text{E}(x)^2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#random-vectors-and-their-mean-and-variance",
    "href": "02-probability.html#random-vectors-and-their-mean-and-variance",
    "title": "2  Probability",
    "section": "2.12 Random vectors and their mean and variance",
    "text": "2.12 Random vectors and their mean and variance\nIn addition to scalar random variables we often make use of random vectors and random matrices.2\nThe mean of a random vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T \\sim F\\) is given by \\[\n\\begin{split}\n\\text{E}(F) &= \\text{E}(\\boldsymbol x) \\\\\n           &= \\underbrace{\\boldsymbol \\mu}_{d \\times 1} = (\\mu_1, \\ldots, \\mu_d)^T\\\\\n\\end{split}\n\\] and thus is a vector of the same dimension as \\(\\boldsymbol x\\), where \\(\\mu_i = \\text{E}(x_i)\\) are the means of the individual components \\(x_i\\).\nThe variance of a random vector \\(\\boldsymbol x\\) of length \\(d\\), however, is not a vector but a matrix of size \\(d\\times d\\). This matrix is called the covariance matrix: \\[\n\\begin{split}\n\\text{Var}(F) &= \\text{Var}(\\boldsymbol x) \\\\\n          &= \\underbrace{\\boldsymbol \\Sigma}_{d\\times d} = (\\sigma_{ij})\n          = \\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix} \\\\\n  &=\\text{E}\\left(\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d\\times 1} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1\\times d}\\right) \\\\\n  & = \\text{E}(\\boldsymbol x\\boldsymbol x^T)-\\boldsymbol \\mu\\boldsymbol \\mu^T \\\\\n\\end{split}\n\\] The elements \\(\\text{Cov}(x_i, x_j)=\\sigma_{ij}\\) describe the covariance between the random variables \\(x_i\\) and \\(x_j\\). The covariance matrix is symmetric, hence \\(\\sigma_{ij}=\\sigma_{ji}\\). The diagonal elements \\(\\text{Cov}(x_i, x_i)=\\sigma_{ii}\\) correspond to the individual variances \\(\\text{Var}(x_i) = \\sigma_i^2\\). By construction, the covariance matrix \\(\\boldsymbol \\Sigma\\) is positive semi-definite, i.e. the eigenvalues of \\(\\boldsymbol \\Sigma\\) are all positive or equal to zero.\nHowever, wherever possible one will aim to use models with non-singular covariance matrices, with all eigenvalues positive, so that the covariance matrix is invertible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#correlation-matrix",
    "href": "02-probability.html#correlation-matrix",
    "title": "2  Probability",
    "section": "2.13 Correlation matrix",
    "text": "2.13 Correlation matrix\nThe correlation matrix \\(\\boldsymbol P\\) (“upper case rho”, not “upper case p”) is the variance standardised version of the covariance matrix \\(\\boldsymbol \\Sigma\\).\nSpecifically, denote by \\(\\boldsymbol V\\) the diagonal matrix containing the variances \\[\n\\boldsymbol V= \\begin{pmatrix}\n    \\sigma_{11} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma_{dd}\n\\end{pmatrix}\n\\] then the correlation matrix \\(\\boldsymbol P\\) is given by \\[\n\\boldsymbol P= (\\rho_{ij}) = \\begin{pmatrix}\n    1 & \\dots & \\rho_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{d1} & \\dots & 1\n\\end{pmatrix}   = \\boldsymbol V^{-1/2} \\, \\boldsymbol \\Sigma\\, \\boldsymbol V^{-1/2}\n\\] Like the covariance matrix the correlation matrix is symmetric. The elements of the diagonal of \\(\\boldsymbol P\\) are all set to 1.\nEquivalently, in component notation the correlation between \\(x_i\\) and \\(x_j\\) is given by \\[\n\\rho_{ij} = \\text{Cor}(x_i,x_j) = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}\n\\]\nFollowing from the definition above, a covariance matrix \\(\\boldsymbol \\Sigma\\) can be factorised into the product of standard deviations \\(\\boldsymbol V^{1/2}\\) and the correlation matrix \\(\\boldsymbol P\\) as follows: \\[\n\\boldsymbol \\Sigma= \\boldsymbol V^{1/2}\\, \\boldsymbol P\\,\\boldsymbol V^{1/2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#parameters-and-families-of-distributions",
    "href": "02-probability.html#parameters-and-families-of-distributions",
    "title": "2  Probability",
    "section": "2.14 Parameters and families of distributions",
    "text": "2.14 Parameters and families of distributions\nA distribution family \\(F(\\theta)\\) is a collection of distributions obtained by varying a parameter \\(\\theta\\). Each specific value of the parameter \\(\\theta\\) indexes one distribution in that family.\nCommon distribution families are usually denoted by familiar abbreviation such as \\(N(\\mu, \\sigma^2)\\) for the normal family. We also call these simply “distributions” with parameters and omit the word “family”.\nIf a random variable \\(x\\) has distribution \\(F(\\theta)\\) we write \\(x \\sim F(\\theta)\\) or simply \\(x \\sim N(\\mu, \\sigma^2)\\) in case of a named normal distribution.\nThe associated pdmf is written \\(f(x; \\theta)\\) or \\(f(x | \\theta)\\). The conditional notation is more general because it implies the parameter \\(\\theta\\) may have its own distribution, yielding a joint density \\(f(x, \\theta) = f(x | \\theta) f(\\theta)\\). Similarly, the corresponding cumulative distribution function is written \\(F(x; \\theta)\\) or \\(F(x | \\theta)\\).\nNote that parametrisations are generally not unique, as any one-to-one transformation of \\(\\theta\\) yields an equivalent index of the same distribution family. For most commonly used distribution families there exist several standard parametrisations. We usually prefer those whose parameters that can be interpreted easily (e.g. in terms of moments) or that help to simplify calculations.\nIf for any pair of different parameter values \\(\\theta_1 \\neq \\theta_2\\) we get distinct distributions with \\(F(\\theta_1) \\neq F(\\theta_2)\\) then the distribution family \\(F(\\theta)\\) is said to be identifiable by the parameter \\(\\theta\\).\n\n\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Academic Press.\n\n\nWhittle, P. 2000. Probability via Expectation. 3rd ed. Springer. https://doi.org/10.1007/978-1-4612-0509-8.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#footnotes",
    "href": "02-probability.html#footnotes",
    "title": "2  Probability",
    "section": "",
    "text": "This notation is common in statistical machine learning and multivariate statistics, see for example Mardia, Kent, and Bibby (1979). In alternative notation, random variables are often written in upper case and the outcomes in lower case. However, that convention doesn’t work well for multivariate random quantities (i.e. random vectors and random matrices) and is also ill-suited in Bayesian statistics where the uncertainty of parameters are modelled by random variables.↩︎\nIn our notational conventions, a scalar \\(x\\) is written in lower case plain font, a vector \\(\\boldsymbol x\\) is written in lower case bold font, a matrix \\(\\boldsymbol X\\) in upper case bold font.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "03-transformations.html",
    "href": "03-transformations.html",
    "title": "3  Transformations",
    "section": "",
    "text": "3.1 Affine or location-scale transformation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#sec-affinetrans",
    "href": "03-transformations.html#sec-affinetrans",
    "title": "3  Transformations",
    "section": "",
    "text": "Transformation rule\nSuppose \\(x \\sim F_x\\) is a scalar random variable. The random variable \\[y= a + b x\\] is a location-scale transformation or affine transformation of \\(x\\), where \\(a\\) plays the role of the location parameter and \\(b\\) is the scale parameter. For \\(a=0\\) this is a linear transformation. If \\(b\\neq 0\\) then the transformation is invertible, with back-transformation \\[x = (y-a)/b\\] Invertible transformations provide a one-to-one map between \\(x\\) and \\(y\\).\nFor a random vector \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) of dimension \\(d\\) the location-scale transformation is \\[\n\\boldsymbol y= \\boldsymbol a+ \\boldsymbol B\\boldsymbol x\n\\] where \\(\\boldsymbol a\\) (a \\(m \\times 1\\) vector) is the location parameter and \\(\\boldsymbol B\\) (a \\(m \\times d\\) matrix) the scale parameter For \\(m=d\\) (square \\(\\boldsymbol B\\)) and \\(\\det(\\boldsymbol B) \\neq 0\\) the affine transformation is invertible with back-transformation \\[\\boldsymbol x= \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol a)\\]\n\n\nDensity\nIf \\(x\\) is a continuous random variable with density \\(f_{x}(x)\\) and assuming an invertible transformation the density for \\(y\\) is given by \\[\nf_{y}(y)=|b|^{-1} f_{x} \\left( \\frac{y-a}{b}\\right)\n\\] where \\(|b|\\) is the absolute value of \\(b\\). Likewise, assuming an invertible transformation for a continuous random vector \\(\\boldsymbol x\\) with density \\(f_{\\boldsymbol x}(\\boldsymbol x)\\) the density for \\(\\boldsymbol y\\) is given by \\[\nf_{\\boldsymbol y}(\\boldsymbol y)=|\\det(\\boldsymbol B)|^{-1} f_{\\boldsymbol x} \\left( \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol a)\\right)\n\\] where \\(|\\det(\\boldsymbol B)|\\) is the absolute value of the determinant \\(\\det(\\boldsymbol B)\\).\n\n\nMoments\nThe transformed random variable \\(y \\sim F_y\\) has mean \\[\\text{E}(y) = a + b \\mu_x\\] and variance \\[\\text{Var}(y) = b^2 \\sigma^2_x\\] where \\(\\text{E}(x) = \\mu_x\\) and \\(\\text{Var}(x) = \\sigma^2_x\\) are the mean and variance of the original variable \\(x\\).\nThe mean and variance of the transformed random vector \\(\\boldsymbol y\\sim F_{\\boldsymbol y}\\) is \\[\\text{E}(\\boldsymbol y)=\\boldsymbol a+ \\boldsymbol B\\,\\boldsymbol \\mu_{\\boldsymbol x}\\] and \\[\\text{Var}(\\boldsymbol y)= \\boldsymbol B\\,\\boldsymbol \\Sigma_{\\boldsymbol x} \\,\\boldsymbol B^T\\] where \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\) and \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\) are the mean and variance of the original random vector \\(\\boldsymbol x\\).\n\n\nImportance of affine transformations\nThe constants \\(\\boldsymbol a\\) and \\(\\boldsymbol B\\) (or \\(a\\) and \\(b\\) in the univariate case) are the parameters of the location-scale family \\(F_{\\boldsymbol y}\\) created from \\(F_{\\boldsymbol x}\\). Many important distributions are location-scale families such as the normal distribution (cf. Section 4.3 and Section 5.3) and the location-scale \\(t\\)-distribution (Section 4.6 and Section 5.6). Furthermore, key procedures in multivariate statistics such as orthogonal transformations (including PCA) or whitening transformations (e.g. the Mahalanobis transformation) are affine transformations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#general-invertible-transformation",
    "href": "03-transformations.html#general-invertible-transformation",
    "title": "3  Transformations",
    "section": "3.2 General invertible transformation",
    "text": "3.2 General invertible transformation\n\nTransformation rule\nAs above we assume \\(x \\sim F_x\\) is a scalar random variable and \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) is a random vector.\nAs a generalisation of invertible affine transformations we now consider general invertible transformations. For a scalar random variable we assume the transformation is specified by \\(y(x) = h(x)\\) and the back-transformation by \\(x(y) = h^{-1}(y)\\) For a random vector we assume \\(\\boldsymbol y(\\boldsymbol x) = \\boldsymbol h(\\boldsymbol x)\\) is invertible with back-transformation \\(\\boldsymbol x(\\boldsymbol y) = \\boldsymbol h^{-1}(\\boldsymbol y)\\).\n\n\nDensity\nIf \\(x\\) is a continuous random variable with density \\(f_{x}(x)\\) the density of the transformed variable \\(y\\) can be computed exactly and is given by \\[\nf_y(y) =\\left| D x(y) \\right|\\, f_x(x(y))\n\\] where \\(D x(y)\\) is the derivative of the inverse transformation \\(x(y)\\).\nLikewise, for a continuous random vector \\(\\boldsymbol x\\) with density \\(f_{\\boldsymbol x}(\\boldsymbol x)\\) the density for \\(\\boldsymbol y\\) is obtained by \\[\nf_{\\boldsymbol y}(\\boldsymbol y) = |\\det\\left( D\\boldsymbol x(\\boldsymbol y) \\right)| \\,\\,  f_{\\boldsymbol x}\\left( \\boldsymbol x(\\boldsymbol y) \\right)\n\\] where \\(D\\boldsymbol x(\\boldsymbol y)\\) is the Jacobian matrix of the inverse transformation \\(\\boldsymbol x(\\boldsymbol y)\\).\n\n\nMoments\nThe mean and variance of the transformed random variable can typically only be approximated. Assume that \\(\\text{E}(x) = \\mu_x\\) and \\(\\text{Var}(x) = \\sigma^2_x\\) are the mean and variance of the original random variable \\(x\\) and \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\) and \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\) are the mean and variance of the original random vector \\(\\boldsymbol x\\). In the delta method the transformation \\(y(x)\\) resp. \\(\\boldsymbol y(\\boldsymbol x)\\) is linearised around the mean \\(\\mu_x\\) respectively \\(\\boldsymbol \\mu_{\\boldsymbol x}\\) and the mean and variance resulting from the linear transformation is reported.\nSpecifically, the linear approximation for the scalar-valued function is \\[\ny(x) \\approx y\\left(\\mu_x\\right) + D y\\left(\\mu_x\\right)\\, \\left(x-\\mu_x\\right)\n\\] where \\(D y(x) = y'(x)\\) is the first derivative of the transformation \\(y(x)\\) and \\(D y\\left(\\mu_x\\right)\\) is the first derivative evaluated at the mean \\(\\mu_x\\), and for the vector-valued function \\[\n\\boldsymbol y(\\boldsymbol x) \\approx \\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right) + D \\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right) \\, \\left(\\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x}\\right)\n\\] where \\(D \\boldsymbol y(\\boldsymbol x)\\) is the Jacobian matrix (vector derivative) for the transformation \\(\\boldsymbol y(\\boldsymbol x)\\) and \\(D \\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right)\\) is the Jacobian matrix evaluated at the mean \\(\\boldsymbol \\mu_{\\boldsymbol x}\\).\nIn the univariate case the delta method yields as approximation for the mean and variance of the transformed random variable \\(y\\) \\[\n\\text{E}(y) \\approx y\\left(\\mu_x\\right)\n\\] and \\[\n\\text{Var}(y)\\approx \\left(D y\\left(\\mu_x\\right)\\right)^2 \\, \\sigma^2_x  \n\\]\nFor the vector random variable \\(\\boldsymbol y\\) the delta method yields \\[\\text{E}(\\boldsymbol y)\\approx\\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right)\\] and \\[\n\\text{Var}(\\boldsymbol y)\\approx D \\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right) \\, \\boldsymbol \\Sigma_{\\boldsymbol x} \\, D\\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right)^T\n\\]\nAssuming \\(y(x) = a + b x\\), with \\(x(y) = (y-a)/b\\), \\(D y(x) = b\\) and \\(D x(y) = b^{-1}\\), recovers the univariate location-scale transformation. Likewise, assuming \\(\\boldsymbol y(\\boldsymbol x) = \\boldsymbol a+ \\boldsymbol B\\boldsymbol x\\), with \\(\\boldsymbol x(\\boldsymbol y) = \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol a)\\), \\(D\\boldsymbol y(\\boldsymbol x) = \\boldsymbol B\\) and \\(D\\boldsymbol x(\\boldsymbol y) = \\boldsymbol B^{-1}\\), recovers the multivariate location-scale transformation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#exponential-tilting-and-exponential-families",
    "href": "03-transformations.html#exponential-tilting-and-exponential-families",
    "title": "3  Transformations",
    "section": "3.3 Exponential tilting and exponential families",
    "text": "3.3 Exponential tilting and exponential families\nAnother way to change the distribution of a random variable is by exponential tilting.\nSuppose there is a vector valued function \\(\\boldsymbol t(x)\\) where each component is a transformation of \\(x\\), usually a simple function such the identity \\(x\\), the square \\(x^2\\), the logarithm \\(\\log(x)\\) etc. These are called the canonical statistics. Typically, the dimension of \\(\\boldsymbol t(x)\\) is small.\nThe exponential tilt of a base distribution \\(B\\) with base function \\(h(x)\\) (possibly unnormalised) toward the linear combination \\(\\boldsymbol \\eta^T \\boldsymbol t(x)\\) of the canonical statistics \\(\\boldsymbol t(x)\\) and the canonical parameters \\(\\boldsymbol \\eta\\) yields the distribution family \\(P(\\boldsymbol \\eta)\\) with pdmf \\[\np(x|\\boldsymbol \\eta) =    \\underbrace{e^{ \\boldsymbol \\eta^T \\boldsymbol t(x)}}_{\\text{exponential tilt}}\\, h(x) \\, /\\, z(\\boldsymbol \\eta)\n\\] The normaliser or partition function \\(z(\\boldsymbol \\eta)\\) ensures that \\(p(x|\\boldsymbol \\eta)\\) integrates to one, with \\[\nz(\\boldsymbol \\eta) = \\int_x \\, e^{ \\boldsymbol \\eta^T \\boldsymbol t(x)}\\, h(x) \\, dx\n\\] In particular, \\(z(\\mathbf 0)=\\int_x  h(x) \\, dx\\) ensures that \\[\np(x|\\mathbf 0) = h(x)/z(\\mathbf 0) = b(x)\n\\] is a valid base pdmf. If \\(h(x)\\) is a pdmf then \\(z(\\mathbf 0)=1\\) and \\(b(x)=h(x)\\).\nA distribution family \\(P(\\boldsymbol \\eta)\\) obtained by exponential tiling is called an exponential family. The set of values of \\(\\boldsymbol \\eta\\) for which \\(z(\\boldsymbol \\eta)  &lt; \\infty\\), and hence for which \\(p(x|\\boldsymbol \\eta)\\) is well defined, comprises the parameter space of the exponential family. Some choices of \\(h(x)\\) and \\(\\boldsymbol t(x)\\) do not yield a finite normalising factor for any \\(\\boldsymbol \\eta\\) and hence these cannot be used to form an exponential family.\nThe log-normaliser or log-partition function \\(a(\\boldsymbol \\eta) = \\log z(\\boldsymbol \\eta)\\) is the cumulant generating function for the canonical statistics. Its gradient yields the mean \\[\n\\text{E}( \\boldsymbol t(x) )  = \\boldsymbol \\mu_{\\boldsymbol t}= \\nabla a(\\boldsymbol \\eta)\n\\] and the Hessian matrix the variance \\[\n\\text{Var}( \\boldsymbol t(x) )  = \\boldsymbol \\Sigma_{\\boldsymbol t}= \\nabla \\nabla^T a(\\boldsymbol \\eta)\n\\]\nMany common distributions are exponential families, such as the normal distribution and the Bernoulli distribution. Exponential families are central in probability and statistics. They support effective statistical learning using likelihood and Bayesian approaches, enable data reduction via minimal sufficiency and provide the basis for generalised linear models. Furthermore, exponential families often allow to generalise results established for specific cases, such as the normal distribution, to a broader domain.\nSee also (Wikipedia): exponential family — table of distributions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#sec-convolution",
    "href": "03-transformations.html#sec-convolution",
    "title": "3  Transformations",
    "section": "3.4 Sums of random variables and convolution",
    "text": "3.4 Sums of random variables and convolution\n\nMoments\nSuppose we have a sum of \\(n\\) independent random variables. \\[\ny = x_1 + x_2 + \\ldots + x_n\n\\] where each \\(x_i \\sim F_{x_i}\\) has its own distribution and corresponding probability density mass function \\(f_{x_i}(x)\\).\nWith \\(\\boldsymbol x= (x_1, \\ldots, x_n)^T\\) and \\(\\mathbf 1_n = (1, 1, \\ldots, 1)^T\\) the relationship between \\(y\\) and \\(\\boldsymbol x\\) can be written as affine transformation \\(y= \\mathbf 1_n^T \\boldsymbol x\\). Assuming \\(\\text{E}(x_i) = \\mu_i\\), \\(\\text{Var}(x_i) = \\sigma^2_i\\) and \\(\\text{Cov}(x_i, x_j)=0\\) for \\(i\\neq j\\) the mean and variance of the random variable \\(y\\) equals (cf. Section 3.1) \\[\n\\text{E}(y) = \\mathbf 1_n^T \\boldsymbol \\mu= \\sum_{i=1}^n \\mu_i\n\\] and \\[\n\\text{Var}(y) = \\mathbf 1_n^T \\, \\text{Var}(\\boldsymbol x) \\,  \\mathbf 1_n  =   \\sum_{i=1}^n \\sigma^2_i\n\\]\nThus both the means and variance are additive (but note that for the variance this is only true because of the independence assumption).\n\n\nConvolution\nThe pdmf for \\(y\\) is obtained by repeatedly convolving (denoted by the asterisk \\(\\ast\\) operator) the pdmfs of the \\(x_i\\): \\[\nf_y(y) = \\left(f_{x_1} \\ast f_{x_2} \\ast \\ldots f_{x_n}\\right)(y)\n\\]\nThe convolution of two functions is defined as (continuous case) \\[\n\\left(f_{x_1}\\ast f_{x_2}\\right)(y)=\\int_x f_{x_1}(x)\\, f_{x_2}(y-x) dx\n\\] and (discrete case) \\[\n\\left(f_{x_1}\\ast f_{x_2}\\right)(y)=\\sum_x f_{x_1}(x)\\, f_{x_2}(y-x)\n\\] Convolution is commutative and associative so so you may convolve multiple pdmfs in any order or grouping. Furthermore, the convolution of pdmfs yields another pdmf, i.e. the resulting function integrates to one.\nMany commonly used random variables can be viewed as the outcome of convolutions. For example, the sum of Bernoulli variables yields a binomial random variable and the sum of normal variables yields another normal random variable.\nSee also (Wikipedia): list of convolutions of probability distributions.\n\n\nCentral limit theorem\nThe central limit theorem, first postulated by Abraham de Moivre (1667–1754) and later proved by Pierre-Simon Laplace (1749–1827) asserts that the distribution of the sum of \\(n\\) independent and identically distributed random variables with finite mean and finite variance converges in the limit of large \\(n\\) to a normal distribution (Section 4.3), even if the individual random variables are not themselves normal. In other words, it asserts that for large \\(n\\) the convolution of \\(n\\) identical distributions with finite first two moments converges to the normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#sec-lossfunc",
    "href": "03-transformations.html#sec-lossfunc",
    "title": "3  Transformations",
    "section": "3.5 Loss functions and scoring rules",
    "text": "3.5 Loss functions and scoring rules\n\nLoss function\nA loss or cost function \\(L(x, a)\\) evaluates a prediction \\(a\\) (for example a parameter or a probability distribution) on the basis of an observed outcome \\(x\\), and returns a numerical score.\nA loss function measures, informally, the error between \\(x\\) and \\(a\\). During optimisation the prediction \\(a\\) is varied and the aim is minimisation of the error (hence a loss function has negative orientation, smaller is better).\nAdding a constant or a positive scaling factor to the loss function will not change the location of its minimum, so such loss functions are considered equivalent.\nA utility or reward function is a loss function with a reversed sign (hence it has positive orientation, larger is better).\n\n\nRisk function\nThe risk of \\(a\\) under the distribution \\(Q\\) for \\(x\\) is defined as the expected loss \\[\nR_Q(a) = \\text{E}_Q(L(x, a))\n\\] If there is no ambiguity we drop the reference to \\(Q\\) and write \\[\nR(a) = \\text{E}(L(x, a))\n\\]\nThe risk of \\(a\\) under the empirical distribution \\(\\hat{Q}_n\\) obtained from observations \\(x_1, \\ldots, x_n\\) is the empirical risk \\[\n\\hat{R}(a) = R_{\\hat{Q}_n}(a) =\n\\frac{1}{n} \\sum_{i=1}^{n} L(x_i, a)\n\\] where the expectation is replaced by the sample average.\nMinimising \\(R(a)\\) finds optimal predictions\n\\[\na^{\\ast} = \\underset{a}{\\arg \\min}\\, R(a)\n\\] Depending on the choice of underlying loss \\(L(x, a)\\) minimising the risk provides a very general optimisation-based way to identify distributional features of the distribution \\(Q\\) and to obtain parameter estimates.\n\n\nScoring rules\nA scoring rule \\(S(x, P)\\) is special type of loss function1 that assesses the probabilistic forecast \\(P\\) by assigning a numerical score based on \\(P\\) and the observed outcome \\(x\\).\nThe associated risk of \\(P\\) under \\(Q\\) is \\[\nR_Q(P) = \\text{E}_{Q}\\left(S(x, P)\\right)\n\\] For a proper scoring rule the risk \\(R_Q(P)\\) is minimised at \\(P = Q\\), hence \\[\nR_Q(P) \\geq R_Q(Q)\n\\] For a strictly proper scoring rule the minimum is achieved only at the true distribution \\(Q\\), so equality holds exclusively for \\(P = Q\\).\nA proper scoring rule induces a divergence between the distributions \\(Q\\) and \\(P\\), as the difference between the risk and the minimum risk : \\[\nD(Q, P) = R_Q(P) - R_Q(Q) \\geq 0\n\\] By construction, the divergence \\(D(Q, P)\\) is always non-negative and equals zero if \\(P=Q\\). For a stricly proper scoring rule the divergence vanishes exclusively for \\(P=Q\\).\nProper scoring rules are very useful as they allow to identify the underlying distribution and their parameters by risk minimisation or minimisation of the associated divergences.\nProper scoring rules also have a number of further useful properties. For example, various decompositions exist for their risk, and the divergence satisfies a generalised Pythagorean theorem. Furthermore, there is a correspondence of proper scoring rules and their associated divergences with Bregman divergences.\n\n\nCommon loss functions\nThe squared loss or squared error is one of the most commonly used loss functions: \\[\nL(x,a) = (x-a)^2\n\\] The corresponding risk is the mean squared loss or mean squared error (MSE) \\[\nR(a) = \\text{E}((x-a)^2)\n\\] From \\(R(a) = \\text{E}((x-a)^2) = \\text{E}(x^2) - 2 a \\text{E}(x) + a^2\\) it follows \\(dR(a)/da = - 2 \\text{E}(x) + 2 a\\) and thus that the MSE is minimised at the mean \\(a^{\\ast} = \\text{E}(x)\\). The achieved minimum risk \\(R(a^{\\ast}) = \\text{Var}(x)\\) is the variance.\nThe 0-1 loss function can be written as \\[\nL(x, a) =\n\\begin{cases}\n-[x = a] & \\text{discrete case} \\\\\n-\\delta(x-a) & \\text{continuous case} \\\\\n\\end{cases}\n\\] employing the indicator function and Dirac delta function, respectively. The corresponding risk assuming \\(x \\sim Q\\) and pdmf \\(q(x)\\) is \\[\nR_Q(a) = -q(a)\n\\] which is minimised at the mode of the pdmf.\nThe asymmetric loss can be defined as \\[\nL(x, a; \\tau) =\n\\begin{cases}\n2 \\tau (x-a) & \\text{for $x\\geq a$} \\\\\n2 (1-\\tau) (a-x)  & \\text{for $x &lt; a$} \\\\\n\\end{cases}\n\\] and the corresponding risk is minimised at the quantile \\(x_{\\tau}\\).\nFor \\(\\tau=1/2\\) it reduces to the absolute loss \\[\nL(x, a) = | x - a|\n\\] whose corresponding risk is minimised at the median \\(x_{1/2}\\).\n\n\nLogarithmic scoring rule\nThe most important scoring rule is the logarithmic scoring rule or log-loss \\[\nS(x, P) = - \\log p(x)\n\\]\nThe risk of \\(P\\) under \\(Q\\) based on the log-loss is the mean log-loss or cross-entropy \\[\nR_Q(P) = - \\text{E}_{Q} \\log p(x) = H(Q,P)\n\\] which is uniquely minimised for \\(P=Q\\). Thus, the log-loss is strictly proper. Furthermore, the log-loss is notably the only local strictly proper scoring rule, as it solely depends on the value of the pdmf at the observed outcome \\(x\\), and not on any other features of the distribution \\(P\\). The minimum risk is the Shannon-Gibbs entropy of \\(Q\\): \\[\nR_Q(Q) = -\\text{E}_{Q} \\log q(x) = H(Q)\n\\] The relationship \\(H(Q, P) \\geq H(Q)\\), with equality exclusively for \\(P=Q\\), is known as Gibbs’ inequality.\nThe divergence induced by the log-loss is the Kullback-Leibler (KL) divergence \\[\n\\begin{split}\nD_{\\text{KL}}(Q,P) &=  H(Q,P) -H(Q) \\\\\n          &= \\text{E}_{Q} \\log\\left(\\frac{q(x)}{p(x)}\\right)\\\\\n\\end{split}\n\\] The KL divergence obeys the data processing inequality, i.e. applying a transformation to the underlying random variables cannot increase the KL divergence \\(D_{\\text{KL}}(Q,P)\\) between \\(Q\\) and \\(P\\). This property also holds for all \\(f\\)-divergences (of which the KL divergence is a principal example), but is notably not satisfied by divergences of other proper scoring rules (and thus other Bregman divergences).\nFurthermore, the KL divergence is the only divergence induced by proper scoring rules (and thus the only Bregman divergence), as well as the only \\(f\\)-divergence, that is invariant against general coordinate transformations. Coordinate transformations can be viewed as a special case of data processing, and for \\(D_{\\text{KL}}(Q,P)\\) the data-processing inequality under general invertible transformations becomes an identity.\nThe empirical risk of a distribution family \\(P(\\theta)\\) based on the log-loss is proportional to the log-likelihood function \\[\n\\begin{split}\n\\hat{R}(\\theta) &= H(\\hat{Q}_n, P(\\theta)) \\\\\n                &= - \\frac{1}{n} \\sum_{i=1}^n \\log p(x_i | \\theta) \\\\\n                &= - \\frac{1}{n} \\ell_n(\\theta)\\\\\n\\end{split}\n\\] Minimising the empirical risk \\(\\hat{R}(\\theta)\\) is equivalent to maximising the log-likelihood function \\(\\ell_n(\\theta)\\).\nSimilarly, minimising the KL divergence \\(D_{\\text{KL}}(\\hat{Q}_n,P(\\theta))\\) with regard to \\(\\theta\\) is equivalent to minimising the empirical risk \\(\\hat{R}(\\theta)\\) and hence to maximum likelihood.\n\n\nBrier or quadratic scoring rule\nThe Brier scoring rule, also known as quadratic scoring rule, evaluates a probabilistic categorical forecast \\(P\\) with corresponding class probabilities \\(p_1, \\ldots, p_K\\) given a realisation \\(\\boldsymbol x\\) from the categorical distribution \\(Q\\) with class probabilities \\(q_1, \\ldots, q_K\\). It can be written as \\[\n\\begin{split}\nS(\\boldsymbol x, P) &= \\sum_{y=1}^K \\left(x_y -p_y\\right)^2 \\\\\n&= 1 -  2 \\sum_{y=1}^K x_y p_y +   \\sum_{y=1}^K p_y^2\\\\\n&= 1 -  2  p_k +   \\sum_{y=1}^K p_y^2\\\\\n\\end{split}\n\\] The indicator vector \\(\\boldsymbol x= (x_1, \\ldots, x_K)^T = (0, 0, \\ldots, 1, \\ldots, 0)^T\\) contains zeros everywhere except for a single element \\(x_k=1\\). Unlike the log-loss, the Brier score is not local as the pmf for \\(P\\) is evaluated across all \\(K\\) classes, not just at the realised class \\(k\\).\nThe corresponding risk is \\[\n\\begin{split}\nR_Q(P) &= \\text{E}_Q(S(\\boldsymbol x, P)) \\\\\n       &= 1 -2 \\sum_{y=1}^K q_y p_y +\\sum_{y=1}^K p_y^2\\\\\n\\end{split}\n\\] which is uniquely minimised for \\(P=Q\\). Thus, the Brier score is strictly proper. The minimum risk is \\[\nR_Q(Q) = 1 -  \\sum_{y=1}^K q_y^2\n\\]\nThe divergence induced by the Brier score is the squared Euclidean distance between the two pmfs: \\[\n\\begin{split}\nD(Q, P) &= R_Q(P) - R_Q(Q) \\\\\n        & = \\sum_{y=1}^K  \\left(q_y - p_y\\right)^2\\\\\n\\end{split}\n\\]\n\n\nProper but not strictly proper scoring rules\nAn example of a proper, but not strictly proper, scoring rule is the squared error relative to the mean of the quoted model \\(P\\): \\[\nS(x, P) = (x- \\text{E}(P))^2\n\\]\nThe corresponding risk is \\[\n\\begin{split}\nR_Q(P) &= \\text{E}_Q\\left( (x- \\text{E}(P))^2 \\right)\\\\\n       & = (\\text{E}(Q)-\\text{E}(P))^2 + \\text{Var}(Q)\\\\\n\\end{split}\n\\] which is minimised at \\(P=Q\\) but also at any distribution \\(P\\) with the same mean as \\(Q\\). The minimum risk is the variance of \\(Q\\): \\[\nR_Q(Q) = \\text{Var}(Q)\n\\]\nThe associated divergence is the squared distance between the two means \\[\n\\begin{split}\nD(Q, P) &= R_Q(P) - \\text{Var}(Q) \\\\\n        &= (\\text{E}(Q)-\\text{E}(P))^2\\\\\n\\end{split}\n\\] which vanishes at \\(P=Q\\) but also at any \\(P\\) with \\(\\text{E}(P)=\\text{E}(Q)\\).\nThe Dawid-Sebastiani scoring rule is a related scoring rule given by \\[\nS\\left(x, P\\right) =  \\log \\text{Var}(P) + \\frac{(x-\\text{E}(P))^2}{\\text{Var}(P)}\n\\] It is equivalent to the log-loss applied to a normal model \\(P\\).\nThe corresponding risk is \\[\n\\begin{split}\nR_Q(P) &= \\log \\text{Var}(P)  + \\frac{(\\text{E}(Q)-\\text{E}(P))^2}{\\text{Var}(P)} + \\frac{\\text{Var}(Q)}{\\text{Var}(P)}\\\\\n\\end{split}\n\\] which is minimised at \\(P=Q\\) but also at any distribution \\(P\\) with \\(\\text{E}(P)=\\text{E}(Q)\\) and \\(\\text{Var}(P)=\\text{Var}(Q)\\).\nThe minimum risk is \\[\nR_Q(Q) = \\log \\text{Var}(Q) +1\n\\]\nThe associated divergence is \\[\n\\begin{split}\nD(Q, P) &= R_Q(P) - \\text{Var}(Q) \\\\\n        &= \\frac{(\\text{E}(Q)-\\text{E}(P))^2}{\\text{Var}(P)} +\\frac{\\text{Var}(Q)}{\\text{Var}(P)}  - \\log\\left( \\frac{\\text{Var}(Q}{\\text{Var}(P} \\right)  -1 \\\\\n\\end{split}\n\\] which vanishes at \\(P=Q\\) but also at any \\(P\\) for which \\(\\text{E}(P)=\\text{E}(Q)\\) and \\(\\text{Var}(P)=\\text{Var}(Q)\\).\n\n\nOther strictly proper scoring rules\nOther useful strictly proper scoring rules include:\n\nthe continuous ranked probability score (CRPS),\nthe energy score, and\nthe Hyvärinen scoring rule.\n\nSee also (Wikipedia): scoring rule.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#footnotes",
    "href": "03-transformations.html#footnotes",
    "title": "3  Transformations",
    "section": "",
    "text": "As a loss function, scoring rules are negatively oriented. However, some authors consider them as utility functions with positive orientation.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "04-univariate.html",
    "href": "04-univariate.html",
    "title": "4  Univariate distributions",
    "section": "",
    "text": "4.1 Binomial distribution\nThe binomial distribution \\(\\text{Bin}(n, \\theta)\\) is a discrete distribution counting binary outcomes.\nThe Bernoulli distribution \\(\\text{Ber}(\\theta)\\) is a special case of the binomial distribution.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#sec-binomdist",
    "href": "04-univariate.html#sec-binomdist",
    "title": "4  Univariate distributions",
    "section": "",
    "text": "Standard parametrisation\nA binomial random variable \\(x\\) describes the number of successful outcomes in \\(n\\) identical and independent trials. We write \\[\nx \\sim \\text{Bin}(n, \\theta)\\,\n\\] where \\(\\theta \\in [0,1]\\) is the probability of a positive outcome (“success”) in a single trial. Conversely, \\(1-\\theta \\in [0,1]\\) is the complementary probability (“failure”). The support is \\(x \\in \\{ 0, 1, 2, \\ldots, n\\}\\) which notably depends on \\(n\\).\n\n\n\n\n\n\nFigure 4.1: Binomial urn model.\n\n\n\nThe binomial distribution is often motivated by a coin tossing experiment where \\(\\theta\\) is the probability of “head” when flipping the coin and \\(x\\) is the number of observed “heads” among \\(n\\) throws. Another common interpretation is that of an urn model where \\(n\\) items are distributed into two bins (Figure 4.1). Here \\(\\theta\\) is the probability to put an item into one urn (representing “success”, “head”) and \\(1-\\theta\\) the probability to put it in the other urn (representing “failure”, “tail”).\nThe expected value is \\[\n\\text{E}(x) = n \\theta\n\\] and the variance is \\[\n\\text{Var}(x) = n \\theta (1 - \\theta)\n\\]\nThe corresponding pmf is \\[\np(x | n, \\theta) = \\binom{n}{x} \\theta^x (1 - \\theta)^{n - x}\n\\] The binomial coefficient \\(\\binom{n}{x}\\) in the pdf accounts for the multiplicity of ways in which we can observe \\(x\\) successes in \\(n\\) trials.\n\n\n\n\n\n\nTipR code\n\n\n\nThe pmf of the binomial distribution is given by dbinom(), the distribution function is pbinom() and the quantile function is qbinom(). The corresponding random number generator is rbinom().\n\n\n\n\nMean parametrisation\nInstead of \\(\\theta\\) one may also use a mean parameter \\(\\mu \\in [0,n]\\) so that \\[\nx \\sim \\text{Bin}\\left(n, \\theta= \\frac{\\mu}{n}\\right)\n\\] The mean parameter \\(\\mu\\) can be obtained from \\(\\theta\\) and \\(n\\) by \\(\\mu = n \\theta\\).\nThe mean and variance of the binomial distribution expressed in terms of \\(\\mu\\) and \\(n\\) are \\[\n\\text{E}(x) = \\mu\n\\] and \\[\n\\text{Var}(x) = \\mu - \\frac{\\mu^2}{n}\n\\]\n\n\nSpecial case: Bernoulli distribution\nFor \\(n=1\\) the binomial distribution reduces to the Bernoulli distribution \\(\\text{Ber}(\\theta)\\). This is the simplest of all distribution families and is named after Jacob Bernoulli (1655-1705) who also discovered the law of large numbers.\nIf a random variable \\(x\\) follows the Bernoulli distribution we write \\[\nx \\sim \\text{Ber}(\\theta)\n\\] with “success” probability \\(\\theta \\in [0,1]\\). Conversely, the complementary “failure” probability is \\(1-\\theta \\in [0,1]\\). The support is \\(x \\in \\{0, 1\\}\\). The variable \\(x\\) acts as an indicator variable, with “success” indicated by \\(x=1\\) and “failure” indicated by \\(x=0\\).\nOften the Bernoulli distribution is referred to as “coin flipping” model. Then \\(\\theta\\) is the probability of “head” and \\(1-\\theta\\) the complementary probability of “tail” and \\(x=1\\) corresponds to the outcome “head” and \\(x=0\\) to the outcome “tail”.\nThe expected value is \\[\n\\text{E}(x) = \\theta\n\\] and the variance is \\[\n\\text{Var}(x) = \\theta (1 - \\theta)\n\\]\nThe pmf of \\(\\text{Ber}(\\theta)\\) is \\[\np(x | \\theta ) = \\theta^{x} (1-\\theta)^{1-x}  =\n\\begin{cases}\n   \\theta  & \\text{if } x = 1 \\\\\n   1-\\theta  & \\text{if } x = 0 \\\\\n\\end{cases}\n\\]\n\n\nConvolution property and normal approximation\nThe convolution of \\(n\\) binomial distributions, each with identical success probability \\(\\theta\\) but possibly different number of trials \\(n_i\\), yields another binomial distribution with the same parameter \\(\\theta\\): \\[\n\\sum_{i=1}^n \\text{Bin}(n_i, \\theta) \\sim \\text{Bin}\\left(\\sum_{i=1}^n n_i, \\theta\\right)\n\\]\nIt follows that the binomial distribution with \\(n\\) trials is the result of the convolution of \\(n\\) Bernoulli distributions: \\[\n\\sum_{i=1}^n \\text{Ber}(\\theta) \\sim \\text{Bin}(n, \\theta)\n\\] Thus, repeating the same Bernoulli trial \\(n\\) times and counting the total number of successes yields a binomial random variable.\nAs a consequence, following the central limit theorem (Section 3.4), for large \\(n\\) the binomial distribution can be well approximated by a normal distribution (Section 4.3) with the same mean and variance. This is known as the De Moivre–Laplace theorem.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#sec-betadist",
    "href": "04-univariate.html#sec-betadist",
    "title": "4  Univariate distributions",
    "section": "4.2 Beta distribution",
    "text": "4.2 Beta distribution\nThe beta distribution \\(\\text{Beta}(\\alpha_1, \\alpha_2)\\) is a continuous distribution that is useful to model proportions or probabilities for \\(K=2\\) classes.\nIt includes the uniform distribution over the unit interval as a special case.\n\nStandard parametrisation\nA beta-distributed random variable is denoted by \\[\nx \\sim \\text{Beta}(\\alpha_1, \\alpha_2)\n\\] with shape parameters \\(\\alpha_1&gt;0\\) and \\(\\alpha_2&gt;0\\). Let \\(m = \\alpha_1 +\\alpha_2\\). The support of \\(x\\) is the unit interval given by \\(x \\in [0,1]\\). Thus, the beta distribution is defined over a one-dimensional space.\n\n\n\n\n\n\nFigure 4.2: Stick breaking visualisation of a beta random variable.\n\n\n\nA beta random variable can be visualised as breaking a unit stick of length one into two pieces of length \\(x_1=x\\) and \\(x_2 = 1-x\\) (Figure 4.2). Thus, the \\(x_i\\) may be used as the exclusive proportions or probabilities for \\(K=2\\) classes.\nThe mean is \\[\n\\text{E}(x) = \\text{E}(x_1) = \\frac{\\alpha_1}{m}\n\\] and hence \\[\n\\text{E}(1-x) = \\text{E}(x_2) = \\frac{\\alpha_2}{m}\n\\]\nThe variance is \\[\n\\text{Var}(x) = \\text{Var}(x_1)  = \\text{Var}(x_2) = \\frac{\\alpha_1 \\alpha_2}{m^2 (m+1) }\n\\]\nThe pdf of the beta distribution \\(\\text{Beta}(\\alpha_1, \\alpha_2)\\) is \\[\np(x | \\alpha_1, \\alpha_2) = \\frac{1}{B(\\alpha_1, \\alpha_2)} x^{\\alpha_1-1} (1-x)^{\\alpha_2-1}\n\\] This depends on the beta function with arguments \\(\\alpha_1\\) and \\(\\alpha_2\\) defined as \\[\nB(\\alpha_1, \\alpha_1) = \\frac{ \\Gamma(\\alpha_1) \\Gamma(\\alpha_2)}{\\Gamma(m)}\n\\]\n\n\n\n\n\n\nFigure 4.3: Shapes of the pdf of the beta distribution.\n\n\n\nThe beta distribution can assume a number of different shapes, depending on the values of \\(\\alpha_1\\) and \\(\\alpha_2\\) (see Figure 4.3).\n\n\n\n\n\n\nTipR code\n\n\n\nThe pdf of the beta distribution is given by dbeta(), the distribution function is pbeta() and the quantile function is qbeta(). The corresponding random number generator is rbeta().\n\n\n\n\nMean parametrisation\nInstead of employing \\(\\alpha_1\\) and \\(\\alpha_2\\) as parameters another useful reparametrisation of the beta distribution is in terms of a mean parameter \\(\\mu \\in [0,1]\\) and a concentration parameter \\(m &gt; 0\\) so that \\[\nx \\sim \\text{Beta}(\\alpha_1 = m \\mu, \\alpha_2= m (1-\\mu))\n\\] The concentration and mean parameters can be obtained from \\(\\alpha_1\\) and \\(\\alpha_2\\) by \\(m = \\alpha_1+\\alpha_2\\) and \\(\\mu = \\alpha_1/m\\).\nThe mean and variance of the beta distribution expressed in terms of \\(\\mu\\) and \\(m\\) are \\[\n\\text{E}(x) = \\mu\n\\] and \\[\n\\text{Var}(x)=\\frac{\\mu (1-\\mu)}{m+1}\n\\] With increasing concentration parameter \\(m\\) the variance decreases and thus the probability mass becomes more concentrated around the mean.\n\n\nSpecial case: symmetric beta distribution\nFor \\(\\alpha_1=\\alpha_2=\\alpha\\) the beta distribution becomes the symmetric beta distribution with a single shape parameter \\(\\alpha&gt;0\\). In mean parametrisation the symmetric beta distribution corresponds to \\(\\mu=1/2\\) and \\(m=2 \\alpha\\).\n\n\nSpecial case: uniform distribution\nFor \\(\\alpha_1=\\alpha_2=1\\) the beta distribution becomes the uniform distribution over the unit interval with pdf \\(p(x)=1\\). In mean parametrisation the uniform distribution corresponds to \\(\\mu=1/2\\) and \\(m=2\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#sec-normdist",
    "href": "04-univariate.html#sec-normdist",
    "title": "4  Univariate distributions",
    "section": "4.3 Normal distribution",
    "text": "4.3 Normal distribution\nThe normal distribution \\(N(\\mu, \\sigma^2)\\) is the most important continuous probability distribution. It is also called Gaussian distribution named after Carl Friedrich Gauss (1777–1855).\nSpecial cases are the standard normal distribution \\(N(0, 1)\\) and the delta distribution \\(\\delta\\).\n\nStandard parametrisation\nThe univariate normal distribution \\(N(\\mu, \\sigma^2)\\) has two parameters \\(\\mu\\) (location) and \\(\\sigma^2 &gt; 0\\) (variance) and support \\(x \\in \\mathbb{R}\\).\nIf a random variable \\(x\\) is normally distributed we write \\[\nx \\sim N(\\mu,\\sigma^2)\n\\] with mean \\[\n\\text{E}(x)=\\mu\n\\] and variance \\[\n\\text{Var}(x) = \\sigma^2\n\\]\nThe pdf is given by \\[\n\\begin{split}\np(x| \\mu, \\sigma^2) &=(2\\pi\\sigma^2)^{-1/2} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\\\\n                    &=(\\sigma^2)^{-1/2} (2\\pi)^{-1/2} e^{-\\Delta^2/2}\\\\\n\\end{split}\n\\] Here \\(\\Delta^2 = (x-\\mu)^2/\\sigma^2\\) is the squared distance between \\(x\\) and \\(\\mu\\) weighted by the variance \\(\\sigma^2\\), also known as squared Mahalanobis distance.\nThe normal distribution is sometimes also used by specifying the precision \\(1/\\sigma^2\\) instead of the variance \\(\\sigma^2\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe normal pdf is given by dnorm(), the distribution function is pnorm() and the quantile function is qnorm(). The corresponding random number generator is rnorm().\n\n\n\n\nScale parametrisation\nInstead of the variance parameter \\(\\sigma^2\\) it is often also convenient to use the standard deviation \\(\\sigma=\\sqrt{\\sigma^2} &gt; 0\\) as scale parameter. Similarly, instead of the precision \\(1/\\sigma^2\\) one may wish to use the inverse standard deviation \\(w = 1/\\sigma\\).\nThe scale parametrisation is central for location-scale transformations (see below).\n\n\nSpecial case: standard normal distribution\n\n\n\n\n\n\n\n\nFigure 4.4: Probability density function (left) and cumulative density function (right) of the standard normal distribution.\n\n\n\n\n\nThe standard normal distribution \\(N(0, 1)\\) has mean \\(\\mu=0\\) and variance \\(\\sigma^2=1\\). The corresponding pdf is \\[\np(x)=(2\\pi)^{-1/2} e^{-x^2/2}\n\\] with the squared Mahalanobis distance reduced to \\(\\Delta^2=x^2\\).\nThe cumulative distribution function (cdf) of the standard normal \\(N(0,1)\\) is \\[\n\\Phi (x ) = \\int_{-\\infty}^{x} p(x'| \\mu=0, \\sigma^2=1) dx'\n\\] There is no analytic expression for \\(\\Phi(x)\\). The inverse \\(\\Phi^{-1}(p)\\) is called the quantile function of the standard normal distribution.\nFigure 4.4 shows the pdf and cdf of the standard normal distribution.\n\n\nSpecial case: delta distribution\nThe delta distribution \\(\\delta\\) is obtained as the limit of \\(N(0, \\varepsilon \\sigma^2)\\) for \\(\\varepsilon \\rightarrow 0\\) and where \\(\\sigma^2\\) is a positive number (e.g. \\(\\sigma^2=1\\)). Thus \\(\\delta\\) is a distribution that behaves like an infinite spike at zero.\nThe corresponding pdf \\(\\delta(x)\\) is called the Dirac delta function, even though it is not an ordinary function. It satisfies \\(\\delta(x)=0\\) for all \\(x\\neq 0\\) and integrates to one, thus representing a point mass at zero.\n\n\nLocation-scale transformation\nLet \\(\\sigma &gt; 0\\) be the positive square root of the variance \\(\\sigma^2\\) and \\(w=1/\\sigma\\).\nIf \\(x \\sim N(\\mu, \\sigma^2)\\) then \\(y=w(x-\\mu) \\sim N(0, 1)\\). This location-scale transformation corresponds to centring and standardisation of a normal random variable, reducing it to a standard normal random variable.\nConversely, if \\(y \\sim N(0, 1)\\) then \\(x = \\mu + \\sigma y \\sim N(\\mu, \\sigma^2)\\). This location-scale transformation generates the normal distribution from the standard normal distribution.\n\n\nConvolution property\nThe convolution of \\(n\\) independent, but not necessarily identical, normal distributions results in another normal distribution with corresponding mean and variance: \\[\n\\sum_{i=1}^n N(\\mu_i, \\sigma^2_i) \\sim N\\left( \\sum_{i=1}^n \\mu_i,  \\sum_{i=1}^n \\sigma^2_i \\right)\n\\] Hence, any normal random variable can be constructed as the sum of \\(n\\) suitable independent normal random variables.\nSince \\(n\\) is an arbitrary positive integer the normal distribution is said to be infinitely divisible.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#sec-gamdist",
    "href": "04-univariate.html#sec-gamdist",
    "title": "4  Univariate distributions",
    "section": "4.4 Gamma distribution",
    "text": "4.4 Gamma distribution\n\n\n\n\n\n\nFigure 4.5: The gamma and the univariate Wishart distribution and its relatives.\n\n\n\nThe gamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\) is another widely used continuous distribution and is also known as univariate Wishart distribution \\(\\text{Wis}\\left(s^2, k \\right)\\) using a different parametrisation.\nIt contains as special cases the scaled chi-squared distribution \\(s^2 \\text{$\\chi^2_{k}$}\\) (two parameter restrictions) as well as the univariate standard Wishart distribution \\(\\text{Wis}\\left(1, k \\right)\\), the chi-squared distribution \\(\\text{$\\chi^2_{k}$}\\) and the exponential distribution \\(\\text{Exp}(\\theta)\\) (one parameter restrictions). Figure 4.5 illustrates the relationship of the gamma and the univariate Wishart distribution with these related distributions.\n\nStandard parametrisation\nThe gamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\) is a continuous distribution with two parameters \\(\\alpha&gt;0\\) (shape) and \\(\\theta&gt;0\\) (scale): \\[\nx \\sim\\text{Gam}(\\alpha, \\theta)\n\\] and support \\(x \\in [0, \\infty[\\) with mean \\[\\text{E}(x)=\\alpha \\theta\\] and variance \\[\\text{Var}(x) = \\alpha \\theta^2\\]\nThe gamma distribution is also often used with a rate parameter \\(\\beta=1/\\theta\\). Therefore one needs to pay attention which parametrisation is used.\nThe pdf is \\[\np(x| \\alpha, \\theta)=\\frac{1}{\\Gamma(\\alpha) \\theta^{\\alpha} } x^{\\alpha-1} e^{-x/\\theta}\n\\]\n\n\n\n\n\n\nTipR code\n\n\n\nThe pdf of the gamma distribution is available in the function dgamma(), the distribution function is pgamma() and the quantile function is qgamma(). The corresponding random number generator is rgamma().\n\n\n\n\nWishart parametrisation\nThe gamma distribution is often used with a different set of parameters \\(s^2 =\\theta/2 &gt; 0\\) (scale) and \\(k=2 \\alpha &gt; 0\\) (shape or concentration). In this form it is known as univariate or one-dimensional Wishart distribution \\[\nx \\sim \\text{Wis}\\left(s^2, k \\right) = \\text{Gam}\\left(\\alpha=\\frac{k}{2}, \\theta=2 s^2\\right)\n\\] named after John Wishart (1898–1954).\nIn the above the scale parameter \\(s^2\\) is scalar and hence the resulting Wishart distribution is univariate. If instead a matrix-valued scale parameter \\(\\boldsymbol S\\) is used this yields the multivariate or \\(d\\)-dimensional Wishart distribution, see Section 5.4.\nIn the Wishart parametrisation the mean is \\[\n\\text{E}(x) = k s^2\n\\] and the variance \\[\n\\text{Var}(x) = 2 k s^4\n\\] The pdf in terms of \\(s^2\\) and \\(k\\) is \\[\np(x| s^2, k)=\\frac{1}{\\Gamma(k/2) (2 s^2)^{k/2} } x^{(k-2)/2} e^{-s^{-2}x/2}\n\\]\n\n\nMean parametrisation\nFinally, we also often employ the Wishart resp. gamma distribution in mean parametrisation \\[\nx \\sim \\text{Wis}\\left(s^2= \\frac{\\mu}{k}, k \\right) = \\text{Gam}\\left(\\alpha=\\frac{k}{2}, \\theta=\\frac{2 \\mu}{k}\\right)\n\\] with parameters \\(\\mu = k s^2 &gt;0\\) and \\(k &gt; 0\\). In this parametrisation the mean is \\[\n\\text{E}(x) = \\mu\n\\] and the variance \\[\n\\text{Var}(x) = \\frac{2 \\mu^2}{k}\n\\]\n\n\nSpecial case: univariate standard Wishart distribution\nFor \\(s^2=1\\) the univariate Wishart distribution reduces to the univariate standard Wishart distribution \\[\nx \\sim \\text{Wis}\\left(1, k \\right) = \\text{Gam}\\left(\\alpha=\\frac{k}{2}, \\theta=2\\right)\n\\] with mean \\[\n\\text{E}(x) = k\n\\] and variance \\[\n\\text{Var}(x) = 2 k\n\\]\nThe pdf is \\[\np(x| k)=\\frac{1}{\\Gamma(k/2) 2^{k/2} } x^{(k-2)/2} e^{-x/2}\n\\]\n\n\nSpecial case: scaled chi-squared distribution\nIf the shape parameter \\(k\\) of the Wishart distribution \\(\\text{Wis}\\left(s^2, k \\right)\\) is restricted to the positive integers \\(k \\in \\{1, 2, \\ldots\\}\\) the Wishart distribution becomes the scaled chi-squared distribution \\(s^2 \\text{$\\chi^2_{k}$}\\) where \\(k\\) is called the degree of freedom.\nThis is equivalent to restricting the shape parameter \\(\\alpha\\) of the gamma distribution \\(\\text{Gam}(\\alpha=k/2, \\theta=2 s^2)\\) to \\(\\alpha \\in \\{1/2, 1, 3/2, 2, \\ldots\\}\\).\nThe scaled chi-squared distribution with \\(k=1\\) is the distribution of a squared normal random variable with mean zero. Specifically, if \\(z \\sim N(0, s^2)\\) then \\(z^2 \\sim s^2 \\text{$\\chi^2_{1}$}= \\text{Wis}(s^2, 1)=N(0, s^2)^2\\).\n\n\nSpecial case: chi-squared distribution\n\n\n\n\n\n\n\n\nFigure 4.6: Probability density function of the chi-squared distribution.\n\n\n\n\n\nIf \\(k\\) is restricted to the positive integers \\(k \\in\\{1, 2, \\ldots\\}\\) the univariate standard Wishart distribution reduces to the chi-squared distribution \\[\nx \\sim \\text{$\\chi^2_{k}$}=\\text{Wis}\\left(s^2=1, k \\right)=\\text{Gam}\\left(\\alpha=\\frac{k}{2}, \\theta=2\\right)\n\\] where \\(k\\) is called the degree of freedom.\nThe chi-squared distribution has mean \\[\n\\text{E}(x)=k\n\\] and variance \\[\n\\text{Var}(x)=2k\n\\]\nFigure 4.6 shows the pdf of the chi-squared distribution for degrees of freedom \\(k=1\\) and \\(k=3\\).\nThe chi-squared distribution with \\(k=1\\) is the distribution of a squared standard normal random variable. Specifically, if \\(z \\sim N(0, 1)\\) then \\(z^2 \\sim  \\text{$\\chi^2_{1}$} = \\text{Wis}(1,1)=N(0,1)^2\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe pdf of the chi-squared distribution is given by dchisq(). The distribution function is pchisq() and the quantile function is qchisq(). The corresponding random number generator is rchisq().\n\n\n\n\nSpecial case: exponential distribution\nIf the shape parameter \\(\\alpha\\) of the gamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\) is set to \\(\\alpha=1\\), or if the shape parameter \\(k\\) of the Wishart distribution \\(\\text{Wis}(s^2, k)\\) is set to \\(k=2\\), we obtain the exponential distribution \\[\nx \\sim \\text{Exp}(\\theta) = \\text{Gam}(\\alpha=1, \\theta) = \\text{Wis}(s^2=\\theta/2, k=2)\n\\] with scale parameter \\(\\theta\\).\nIt has mean \\[\n\\text{E}(x)=\\theta\n\\] and variance \\[\n\\text{Var}(x) = \\theta^2\n\\] and the pdf is \\[\np(x|  \\theta)=\\theta^{-1}  e^{-x/\\theta}\n\\]\nJust like the gamma distribution the exponential distribution is also often specified using a rate parameter \\(\\beta= 1/\\theta\\) instead of a scale parameter \\(\\theta\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe command dexp() returns the pdf of the exponential distribution, pexp() is the distribution function and qexp() is the quantile function. The corresponding random number generator is rexp().\n\n\n\n\nScale transformation\nIf \\(x \\sim \\text{Gam}(\\alpha, \\theta)\\) then the scaled random variable \\(b x\\) with \\(b&gt;0\\) is also gamma distributed with \\(b x \\sim \\text{Gam}(\\alpha, b \\theta)\\).\nHence,\n\n\\(\\theta \\, \\text{Gam}(\\alpha, 1) = \\text{Gam}(\\alpha, \\theta)\\),\n\\(\\theta \\, \\text{Exp}(1) = \\text{Exp}(\\theta)\\),\n\\((\\mu / k) \\, \\text{Wis}(1, k) =  \\text{Wis}(s^2= \\mu/k, k)\\) and\n\\(s^2 \\, \\text{Wis}(1, k) = \\text{Wis}(s^2, k)\\).\n\nAs \\(\\text{$\\chi^2_{k}$}\\) equals \\(\\text{Wis}(1, k)\\) the last example demonstrates that the scaled chi-squared distribution \\(s^2 \\text{$\\chi^2_{k}$}\\) equals the univariate Wishart distribution \\(\\text{Wis}(s^2, k)\\).\n\n\nConvolution property\nThe convolution of \\(n\\) gamma distributions with the same scale parameter \\(\\theta\\) but possible different shape parameters \\(\\alpha_i\\) yields another gamma distribution: \\[\n\\sum_{i=1}^n \\text{Gam}(\\alpha_i, \\theta) \\sim \\text{Gam}\\left( \\sum_{i=1}^n \\alpha_i,  \\theta \\right)\n\\] Thus, any gamma random variable can be obtained as the sum of \\(n\\) suitable independent gamma random variables.\nIn Wishart parametrisation this becomes \\[\n\\sum_{i=1}^n \\text{Wis}(s^2, k_i) \\sim \\text{Wis}\\left(s^2, \\sum_{i=1}^n k_i\\right)\n\\]\nAs a result, since \\(n\\) is an arbitrary positive integer, the gamma resp. univariate Wishart distribution is infinitely divisible.\nThe above includes the following two specific constructions:\n\nIf \\(x_1, \\ldots, x_n \\sim \\text{Exp}(\\theta)\\) are independent samples from \\(\\text{Exp}(\\theta)\\) then the sum \\(y = \\sum_{i=1}^n x_i \\sim \\text{Gam}(\\alpha=n, \\theta)\\) is gamma distributed with the same scale parameter.\nThe sum of \\(k\\) independent scaled chi-squared random variables \\(s^2 \\text{$\\chi^2_{1}$}\\) with one degree of freedom and identical scale parameter \\(s^2\\) yields a scaled chi-squared random variable \\(s^2 \\text{$\\chi^2_{k}$}\\) with degree of freedom \\(k\\) and the same scale parameter. Thus, if \\(z_1,z_2,\\dots,z_k\\sim N(0,1)\\) are \\(k\\) independent samples from \\(N(0,1)\\) then \\(\\sum_{i=1}^{k} z_i^2 \\sim \\text{$\\chi^2_{k}$}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#sec-invgamdist",
    "href": "04-univariate.html#sec-invgamdist",
    "title": "4  Univariate distributions",
    "section": "4.5 Inverse gamma distribution",
    "text": "4.5 Inverse gamma distribution\nThe inverse gamma distribution \\(\\text{IG}(\\alpha, \\beta)\\) is a continuous distribution and is also known as univariate inverse Wishart distribution \\(\\text{IW}(\\psi, k)\\) using a different parametrisation. It is linked to the gamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\) aka univariate Wishart distribution \\(\\text{Wis}\\left(s^2, k \\right)\\) (Section 4.4).\nSpecial cases include the inverse chi-squared distribution \\(\\text{Inv-$\\chi^2_{k}$}\\) and the scaled inverse chi-squared distribution \\(s^2 \\text{Inv-$\\chi^2_{k}$}\\).\n\nStandard parametrisation\nA random variable \\(x\\) following an inverse gamma distribution is denoted by \\[\nx \\sim \\text{IG}(\\alpha, \\beta)\n\\] with two parameters \\(\\alpha &gt;0\\) (shape parameter) and \\(\\beta &gt;0\\) (scale parameter) and support \\(x &gt;0\\).\nThe mean of the inverse gamma distribution is (for \\(\\alpha&gt;1\\)) \\[\n\\text{E}(x) = \\frac{\\beta}{\\alpha-1}\n\\] and the variance (for \\(\\alpha&gt;2\\)) \\[\n\\text{Var}(x) = \\frac{\\beta^2}{(\\alpha-1)^2 (\\alpha-2)}\n\\]\nThe inverse gamma distribution \\(\\text{IG}(\\alpha, \\beta)\\) has pdf \\[\np(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} (1/x)^{\\alpha+1} e^{-\\beta/x}\n\\]\nThe inverse gamma distribution and the gamma distribution are directly linked. If \\(x \\sim \\text{IG}(\\alpha, \\beta)\\) then the inverse of \\(x\\) is gamma distributed with inverted scale parameter \\[\n\\frac{1}{x} \\sim \\text{Gam}(\\alpha, \\theta=\\beta^{-1})\n\\] where \\(\\alpha\\) is the shape parameter and \\(\\theta\\) the scale parameter of the gamma distribution.\n\n\n\n\n\n\nTipR code\n\n\n\nThe extraDistr package implements the inverse gamma distribution. The function extraDistr::dinvgamma() provides the pdf, extraDistr::pinvgamma() the distribution function and extraDistr::qinvgamma() is the quantile function. The corresponding random number generator is extraDistr::rinvgamma().\n\n\n\n\nWishart parametrisation\nThe inverse gamma distribution is frequently used with a different set of parameters \\(\\psi = 2\\beta\\) (scale parameter) and \\(k = 2\\alpha\\) (shape parameter). In this form it is called univariate inverse Wishart distribution \\[\nx \\sim \\text{IW}(\\psi, k) =\n\\text{IG}\\left(\\alpha=\\frac{k}{2}, \\beta=\\frac{\\psi}{2}\\right)\n\\]\nIn the above the scale parameter \\(\\psi\\) is scalar and hence the resulting inverse Wishart distribution is univariate. If instead a matrix-valued scale parameter \\(\\boldsymbol \\Psi\\) is used this yields the multivariate or \\(d\\)-dimensional inverse Wishart distribution, see Section 5.5.\nIn the Wishart parametrisation the mean is (for \\(k&gt;2\\)) \\[\n\\text{E}(x) = \\frac{\\psi}{k-2}\n\\] and the variance is (for \\(k &gt;4\\)) \\[\n\\text{Var}(x) =\\frac{2 \\psi^2}{(k-4) (k-2)^2 }  \n\\]\nThe pdf in terms of \\(\\psi\\) and \\(k\\) is \\[\np(x | \\psi, k) = \\frac{(\\psi/2)^{(k/2)}}{\\Gamma(k/2)} \\, x^{-(k+2)/2}\\, e^{-\\psi x^{-1}/2}\n\\]\nThe univariate inverse Wishart and the univariate Wishart distributions are linked. If \\(x \\sim \\text{IW}(\\psi, k)\\) then the inverse of \\(x\\) is Wishart distributed with inverted scale parameter: \\[\n\\frac{1}{x} \\sim \\text{Wis}(s^2=\\psi^{-1}, k)\n\\] where \\(k\\) is shape parameter and \\(s^2\\) the scale parameter of the Wishart distribution.\n\n\nMean parametrisation\nInstead of \\(\\psi\\) and \\(k\\) we may also equivalently use \\(\\mu = \\psi/(\\nu-2)\\) and \\(\\kappa=\\nu-2\\) as parameters for the univariate inverse Wishart distribution, so that \\[\nx \\sim \\text{IW}(\\psi=\\kappa \\mu, k=\\kappa+2) =\n\\text{IG}\\left(\\alpha=\\frac{\\kappa+2}{2}, \\beta=\\frac{\\mu \\kappa}{2}\\right)\n\\] has mean (for \\(\\kappa&gt;0\\)) \\[\\text{E}(x) = \\mu\\] and the variance (for \\(\\kappa&gt;2\\)) \\[\\text{Var}(x) = \\frac{2 \\mu^2}{\\kappa-2}\\]\nThe mean parametrisation is useful in Bayesian analysis when employing the inverse gamma aka univariate inverse Wishart distribution as prior and posterior distribution.\n\n\nBiased mean parametrisation\nUsing \\(\\tau^2= \\frac{\\psi}{k}\\) as biased mean parameter together with \\(\\nu=k\\) we arrive at the biased mean parametrisation \\[\nx \\sim \\text{IW}(\\psi= \\nu \\tau^2, k=\\nu ) =\n\\text{IG}\\left(\\alpha=\\frac{\\nu}{2}, \\beta=\\frac{\\nu \\tau^2}{2}\\right)\n\\] with mean (for \\(\\nu&gt;2\\)) \\[\n\\text{E}(x) = \\frac{ \\nu}{\\nu-2}  \\tau^2 = \\mu\n\\] and variance (\\(\\nu &gt;4\\)) \\[\n\\text{Var}(x) = \\left(\\frac{\\nu}{\\nu-2}\\right)^2  \\frac{2 \\tau^4}{\\nu-4}\n\\] As \\(\\tau^2 = \\mu (\\nu-2)/\\nu\\) for large \\(\\nu\\) the parameter \\(\\tau^2\\) will become identical to the true mean \\(\\mu\\).\nThis parametrisation is useful to derive the location-scale \\(t\\)-distribution with its matching parameters (see Section 4.6). It is also common in Bayesian analysis.\n\n\nSpecial case: inverse chi-squared distribution\nIf the scale parameter in \\(\\text{IW}(\\psi, k)\\) is set to \\(\\psi=1\\) and \\(k\\) is restricted to the positive integers \\(\\{1, 2, \\ldots\\}\\) then the univariate inverse Wishart distribution reduces to the inverse chi-squared distribution \\[\nx \\sim \\text{Inv-$\\chi^2_{k}$}=\\text{IW}(\\psi=1, k)=\n\\text{IG}\\left(\\alpha=\\frac{k}{2}, \\beta=\\frac{1}{2}\\right)\n\\] where \\(k\\) is called the degree of freedom.\nThe inverse chi-squared distribution has mean (for \\(k&gt;2\\)) \\[\n\\text{E}(x) = \\frac{1}{k-2}\n\\] and the variance is (for \\(k &gt;4\\)) \\[\n\\text{Var}(x) =\\frac{2}{(k-2)^2 (k-4) }  \n\\]\nThe inverse chi-squared distribution and the chi-squared distribution are linked. If \\(x \\sim \\text{Inv-$\\chi^2_{k}$}\\) then the inverse of \\(x\\) is chi-squared distributed: \\[\n\\frac{1}{x} \\sim \\text{$\\chi^2_{k}$}\n\\] where \\(k\\) is the degree of freedom.\n\n\nScale transformation\nIf \\(x \\sim \\text{IG}(\\alpha, \\beta)\\) then the scaled random variable \\(b x\\) with \\(b&gt;0\\) is also inverse gamma distributed with \\(b x \\sim \\text{IG}(\\alpha, b \\beta)\\).\nHence,\n\n\\(\\beta \\, \\text{IG}(\\alpha, 1) = \\text{IG}(\\alpha, \\beta)\\),\n\\(\\psi \\, \\text{IW}(1, k) = \\text{IW}(\\psi, k)\\),\n\\(\\kappa \\mu \\,\\text{IW}(1, k=\\kappa+2) = \\text{IW}(\\psi=\\kappa \\mu, k=\\kappa+2)\\) and\n\\(\\nu \\tau^2 \\, \\text{IW}(1, k=\\nu) = \\text{IW}(\\psi=\\nu \\tau^2, k=\\nu)\\)\n\nAs \\(\\text{Inv-$\\chi^2_{k}$}\\) equals \\(\\text{IW}(\\psi=1, k)\\) the scaled inverse chi-squared distribution \\(\\psi \\text{Inv-$\\chi^2_{k}$}\\) is thus equivalent to the univariate inverse Wishart distribution \\(\\text{IW}(\\psi, k)\\). If a random variable follows the scaled inverse chi-squared distribution its inverse follows the corresponding scaled chi-squared distribution. Specifically, if \\(x \\sim \\psi \\, \\text{Inv-$\\chi^2_{k}$}\\) then \\(1/x \\sim \\psi^{-1}\\, \\text{$\\chi^2_{k}$}\\).\nThe scaled inverse chi-squared distribution is frequently used in the biased mean parametrisation with \\(\\tau = \\psi/\\nu\\) and \\(\\nu=k\\). Then \\(\\psi \\, \\text{Inv-$\\chi^2_{k}$}\\) is equal to \\(\\nu \\tau^2 \\, \\text{Inv-$\\chi^2_{\\nu}$} = \\text{IW}(\\psi=\\nu \\tau^2, k=\\nu)\\) which is sometimes also written as \\(\\text{Inv-$\\chi^2_{}$}(\\nu, \\tau^2)\\). If \\(x \\sim \\nu \\tau^2 \\, \\text{Inv-$\\chi^2_{\\nu}$}\\) then \\(1/x \\sim 1/(\\nu \\tau^2)\\, \\text{$\\chi^2_{\\nu}$}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#sec-lstdist",
    "href": "04-univariate.html#sec-lstdist",
    "title": "4  Univariate distributions",
    "section": "4.6 Location-scale \\(t\\)-distribution",
    "text": "4.6 Location-scale \\(t\\)-distribution\n\n\n\n\n\n\nFigure 4.7: The location-scale \\(t\\)-distribution and its relatives.\n\n\n\nThe location-scale \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mu, \\tau^2)\\) is a continuous distribution and is a generalisation of the normal distribution \\(N(\\mu, \\tau^2)\\) (Section 4.3) with an additional parameter \\(\\nu &gt; 0\\) (degrees of freedom) controlling the probability mass in the tails.\nSpecial cases include the Student’s \\(t\\)-distribution \\(\\text{$t_{\\nu}$}\\), the normal distribution \\(N(\\mu, \\tau^2)\\) and the Cauchy distribution \\(\\text{Cau}(\\mu, \\tau^2)\\). Figure 4.7 illustrates the relationship of the location-scale \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mu, \\tau^2)\\) with these related distributions.\n\nStandard parametrisation\nIf a random variable \\(x \\in \\mathbb{R}\\) follows the location-scale \\(t\\)-distribution we write \\[\nx \\sim \\text{$t_{\\nu}$}(\\mu, \\tau^2)\n\\] where \\(\\mu\\) is the location and \\(\\tau^2\\) the dispersion parameter. The parameter \\(\\nu &gt; 0\\) prescribes the degrees of freedom. For small values of \\(\\nu\\) the distribution is heavy-tailed and as a result only moments of order smaller than \\(\\nu\\) are finite and defined.\nThe mean is (for \\(\\nu&gt;1\\)) \\[\n\\text{E}(x) = \\mu\n\\] and the variance (for \\(\\nu&gt;2\\)) \\[\n\\text{Var}(x) = \\frac{\\nu}{\\nu-2} \\tau^2\n\\]\nThe pdf of \\(\\text{$t_{\\nu}$}(\\mu, \\tau^2)\\) is \\[\np(x | \\mu, \\tau^2, \\nu) = ( \\tau^2)^{-1/2}\n\\frac{\\Gamma(\\frac{\\nu+1}{2})} { (\\pi \\nu)^{1/2}  \\,\\Gamma(\\frac{\\nu}{2})}\n\\left(1+ \\frac{\\Delta^2}{\\nu}  \\right)^{-(\\nu+1)/2}\n\\] with \\(\\Delta^2 = (x-\\mu)^2/\\tau^2\\) the squared Mahalanobis distance between \\(x\\) and \\(\\mu\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe package extraDistr implements the location-scale \\(t\\)-distribution. The function extraDistr::dlst() returns the pdf, extraDistr::plst() the distribution function function and extraDistr::qlst() is the quantile function. The corresponding random number generator is extraDistr::rlst().\n\n\n\n\nScale parametrisation\nInstead of the dispersion parameter \\(\\tau^2\\) it is often also convenient to use the scale parameter \\(\\tau=\\sqrt{\\tau^2} &gt; 0\\). Similarly, instead of the inverse dispersion \\(1/\\tau^2\\) one may wish to use the inverse scale \\(w = 1/\\tau\\).\nThe scale parametrisation is central for location-scale transformations (see below).\n\n\nSpecial case: Student’s \\(t\\)-distribution\nWith \\(\\mu=0\\) and \\(\\tau^2=1\\) the location-scale \\(t\\)-distribution reduces to the standard \\(t\\)-distribution \\(\\text{$t_{\\nu}$}=\\text{$t_{\\nu}$}(0,1)\\). It is commonly known Student’s \\(t\\)-distribution named after “Student” which was the pseudonym of William Sealy Gosset (1876–1937). It is a generalisation of the standard normal distribution \\(N(0,1)\\) to allow for heavy tails.\nThe distribution has mean \\(\\text{E}(x)=0\\) (for \\(\\nu&gt;1\\)) and variance \\(\\text{Var}(x)=\\frac{\\nu}{\\nu-2}\\) (for \\(\\nu&gt;2\\)).\nThe pdf of \\(\\text{$t_{\\nu}$}\\) is \\[\np(x | \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {(\\pi \\nu)^{1/2}  \\,\\Gamma(\\frac{\\nu}{2})}\n\\left(1+\\frac{x^2}{\\nu} \\right)^{-(\\nu+1)/2}\n\\] with the squared Mahalanobis distance reducing to \\(\\Delta^2=x^2\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe command dt() returns the pdf of the \\(t\\)-distribution, pt() is distribution function and qt() the quantile function. The corresponding random number generator is rt().\n\n\n\n\nSpecial case: normal distribution\nFor \\(\\nu \\rightarrow \\infty\\) the location-scale \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mu, \\tau^2)\\) reduces to the normal distribution \\(N(\\mu, \\tau^2)\\) (Section 4.3). Correspondingly, for \\(\\nu \\rightarrow \\infty\\) the Student’s \\(t\\)-distribution becomes equal to the standard normal distribution \\(N(0,1)\\).\nSee Section 5.6 for further details.\n\n\nSpecial case: Cauchy distribution\nFor \\(\\nu=1\\) the location-scale \\(t\\)-distribution becomes the Cauchy distribution \\(\\text{Cau}(\\mu, \\tau^2)=\\text{$t_{1}$}(\\mu, \\tau^2)\\) named after Augustin-Louis Cauchy (1789–1857).\nIts mean, variance and other higher moments are all undefined.\nIt has pdf \\[\n\\begin{split}\np(x| \\mu, \\tau^2) &= (\\tau^2)^{-1/2} (\\pi (1+\\Delta^2))^{-1}\\\\\n&= \\frac{\\tau}{\\pi (\\tau^2+(x-\\mu)^2)}\n\\end{split}\n\\] with \\(\\tau=\\sqrt{\\tau^2}&gt;0\\).\nNote that in the above we employ \\(\\tau^2\\) as dispersion parameter as this parallels the location-scale \\(t\\)-distribution and the normal distribution but very often the Cauchy distribution is used with \\(\\tau&gt; 0\\) as scale parameter.\n\n\n\n\n\n\nTipR code\n\n\n\nThe command dcauchy() returns the pdf of the Cauchy distribution, pcauchy() is the distribution function and qcauchy() the quantile function. The corresponding random number generator is rcauchy().\n\n\n\n\nSpecial case: standard Cauchy distribution\nThe standard Cauchy distribution \\(\\text{Cau}(0, 1)=\\text{$t_{1}$}(0, 1) =\\text{$t_{1}$}\\) is obtained by setting \\(\\mu=0\\) and \\(\\tau^2=1\\) (Cauchy distribution) or, equivalently, by setting \\(\\nu=1\\) (Student’s \\(t\\)-distribution).\nIt has pdf \\[\np(x) = \\frac{1}{\\pi (1+x^2)}\n\\]\n\n\nLocation-scale transformation\nLet \\(\\tau &gt; 0\\) be the positive square root of \\(\\tau^2\\) and \\(w=1/\\tau\\).\nIf \\(x \\sim \\text{$t_{\\nu}$}(\\mu, \\tau^2)\\) then \\(y=w(x-\\mu) \\sim \\text{$t_{\\nu}$}\\). This location-scale transformation reduces a location-scale \\(t\\)-distributed random variable to a Student’s \\(t\\)-distributed random variable.\nConversely, if \\(y \\sim \\text{$t_{\\nu}$}\\) then \\(x = \\mu + \\tau y \\sim \\text{$t_{\\nu}$}(\\mu, \\tau^2)\\). This location-scale transformation generates the location-scale \\(t\\)-distribution from the Student’s \\(t\\) -distribution.\nFor the special case of the Cauchy distribution (corresponding to \\(\\nu=1\\)) similar relations hold between it and the standard Cauchy distribution. If \\(x \\sim \\text{Cau}(\\mu, \\tau^2)\\) then \\(y=w(x-\\mu) \\sim \\text{Cau}(0, 1)\\). Conversely, if \\(y \\sim \\text{Cau}(0, 1)\\) then \\(x = \\mu + \\tau y \\sim \\text{Cau}(\\mu, \\tau^2)\\).\n\n\nConvolution property\nThe location-scale \\(t\\)-distribution is not generally closed under convolution, with the exception of two special cases, the normal distribution (\\(\\nu=\\infty\\)), see Section 4.3, and the Cauchy distribution (\\(\\nu=1\\)).\nFor the Cauchy distribution with \\(\\tau_i^2= a_i^2 \\tau^2\\), where \\(a_i&gt;0\\) are positive scalars, \\[\n\\sum_{i=1}^n  \\text{Cau}(\\mu_i, a_i^2 \\tau^2)  \\sim\n  \\text{Cau}\\left( \\sum_{i=1}^n \\mu_i, \\left(\\sum_{i=1}^n a_i\\right)^2  \\tau^2\\right)\n\\]\n\n\nLocation-scale \\(t\\)-distribution as compound distribution\nThe location-scale \\(t\\)-distribution can be obtained as mixture of normal distributions with identical mean and varying variance. Specifically, let \\(z\\) be a univariate inverse Wishart random variable \\[\nz \\sim  \\text{IW}(\\psi=\\nu, k=\\nu) =\n        \\text{IG}\\left(\\alpha=\\frac{\\nu}{2}, \\beta=\\frac{\\nu}{2}\\right)\n\\] and let \\(x| z\\) be normal \\[\nx | z \\sim N(\\mu,\\sigma^2 = z \\tau^2)\n\\]\nThen the resulting marginal (scale mixture) distribution for \\(x\\) is the location-scale \\(t\\)-distribution \\[\nx \\sim \\text{$t_{\\nu}$}\\left(\\mu, \\tau^2\\right)\n\\]\nAn alternative way to arrive at \\(\\text{$t_{\\nu}$}\\left(\\mu, \\tau^2\\right)\\) is to include \\(\\tau^2\\) as parameter in the inverse Wishart distribution \\[\nz \\sim  \\tau^2 \\text{IW}(\\psi=\\nu, k=\\nu) = \\text{IW}(\\psi=\\nu \\tau^2, k=\\nu)\n        \\] and let \\[\nx | z \\sim N(\\mu,\\sigma^2 = z)\n\\] Note that \\(\\tau^2\\) is now the biased mean parameter of the univariate inverse Wishart distribution. This characterisation is useful in Bayesian analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html",
    "href": "05-multivariate.html",
    "title": "5  Multivariate distributions",
    "section": "",
    "text": "5.1 Multinomial distribution\nThe multinomial distribution \\(\\text{Mult}(n, \\boldsymbol \\theta)\\) is the multivariate generalisation of the binomial distribution \\(\\text{Bin}(n, \\theta)\\) (Section 4.1) from two classes to \\(K\\) classes.\nA special case is the categorical distribution \\(\\text{Cat}(\\boldsymbol \\theta)\\) that generalises the Bernoulli distribution \\(\\text{Ber}(\\theta)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#multinomial-distribution",
    "href": "05-multivariate.html#multinomial-distribution",
    "title": "5  Multivariate distributions",
    "section": "",
    "text": "Standard parametrisation\nA multinomial random variable \\(\\boldsymbol x\\) describes describes the allocation of \\(n\\) items to \\(K\\) classes. We write \\[\n\\boldsymbol x\\sim \\text{Mult}(n, \\boldsymbol \\theta)\n\\] where the parameter vector \\(\\boldsymbol \\theta=(\\theta_1, \\ldots, \\theta_K)^T\\) specifies the probability of each of \\(K\\) classes, with \\(\\theta_i \\in [0,1]\\) and \\(\\boldsymbol \\theta^T \\mathbf 1_K = \\sum_{i=1}^K \\theta_i = 1\\). Thus there are \\(K-1\\) independent elements in \\(\\boldsymbol \\theta\\). The number of classes \\(K\\) is implicitly given by the dimension of the vector \\(\\boldsymbol \\theta\\). Each element of the vector \\(\\boldsymbol x= (x_1, \\ldots, x_K)^T\\) is an integer \\(x_i \\in \\{0, 1, \\ldots, n\\}\\) and \\(\\boldsymbol x\\) satisfies the constraint \\(\\boldsymbol x^T \\mathbf 1_K = \\sum_{i=1}^K x_i = n\\). Therefore the support of \\(\\boldsymbol x\\) is a \\(K-1\\) dimensional space and it notably depends on \\(n\\).\n\n\n\n\n\n\nFigure 5.1: Multinomial urn model.\n\n\n\nThe multinomial distribution is best illustrated by an urn model distributing \\(n\\) items into \\(K\\) bins where \\(\\boldsymbol \\theta\\) contains the corresponding bin probabilities (Figure 5.1).\nThe expected value is \\[\n\\text{E}(\\boldsymbol x) = n \\boldsymbol \\theta\n\\] The covariance matrix is \\[\n\\text{Var}(\\boldsymbol x) = n (\\text{Diag}(\\boldsymbol \\theta) - \\boldsymbol \\theta\\boldsymbol \\theta^T)\n\\] The covariance matrix is singular by construction because of the dependencies among the elements of \\(\\boldsymbol x\\).\nThe corresponding pmf is \\[\np(\\boldsymbol x| \\boldsymbol \\theta) = \\binom{n}{x_1, \\ldots, x_n} \\prod_{i=1}^K \\theta_i^{x_i}\n\\] where \\(\\binom{n}{x_1, \\ldots, x_n}\\) is the multinomial coefficient. While all \\(K\\) elements of \\(\\boldsymbol x\\) appear in the pmf recall that due the dependencies among the \\(x_i\\) the pmf is defined over a \\(K-1\\) dimensional support.\nFor \\(K=2\\) the multinomial distribution reduces to the binomial distribution (Section 4.1).\n\n\n\n\n\n\nTipR code\n\n\n\nThe pmf of the multinomial distribution is given by dmultinom(). The corresponding random number generator is rmultinom().\n\n\n\n\nMean parametrisation\nInstead of \\(\\boldsymbol \\theta\\) one may also use a mean parameter \\(\\boldsymbol \\mu\\), with elements \\(\\mu_i \\in [0,n]\\) and \\(\\boldsymbol \\mu^T \\mathbf 1_K = \\sum^{K}_{i=1}\\mu_i = n\\), so that \\[\n\\boldsymbol x\\sim \\text{Mult}\\left(n, \\boldsymbol \\theta= \\frac{\\boldsymbol \\mu}{n}\\right)\n\\] The mean parameter \\(\\boldsymbol \\mu\\) can be obtained from \\(\\boldsymbol \\theta\\) and \\(n\\) by \\(\\boldsymbol \\mu= n \\boldsymbol \\theta\\). Note that the parameter space for \\(\\boldsymbol \\mu\\) and the support of \\(\\boldsymbol x\\) are both of dimension \\(K-1\\).\nThe mean and variance of the multinomial distribution expressed in terms of \\(\\boldsymbol \\mu\\) and \\(n\\) are \\[\n\\text{E}(x) = \\boldsymbol \\mu\n\\] and \\[\n\\text{Var}(x) = \\text{Diag}(\\boldsymbol \\mu) - \\frac{\\boldsymbol \\mu\\boldsymbol \\mu^T}{n}\n\\] The covariance matrix is singular by construction because of the dependencies among the elements of \\(\\boldsymbol x\\).\n\n\nSpecial case: categorical distribution\nFor \\(n=1\\) the multinomial distribution reduces to the categorical distribution \\(\\text{Cat}(\\boldsymbol \\theta)\\) which in turn is the multivariate generalisation of the Bernoulli distribution \\(\\text{Ber}(\\theta)\\) from two classes to \\(K\\) classes.\nIf a random variable \\(\\boldsymbol x\\) follows the categorical distribution we write \\[\n\\boldsymbol x\\sim \\text{Cat}(\\boldsymbol \\theta)\n\\] with class probabilities \\(\\boldsymbol \\theta\\) and \\(\\boldsymbol \\theta^T \\mathbf 1_K = 1\\). The support is \\(x_i \\in \\{0, 1\\}\\) and \\(\\boldsymbol x^T \\mathbf 1_K =1\\) and is a \\(K-1\\) dimensional space.\nThe random vector \\(\\boldsymbol x\\) takes the form of an indicator vector \\(\\boldsymbol x= (x_1, \\ldots, x_K)^T = (0, 0, \\ldots, 1, \\ldots, 0)^T\\) containing zeros everywhere except for a single element \\(x_k=1\\) indicating the class \\(k\\) to which the item has been allocated. This is called “one hot encoding”, as opposed to “integer encoding”, i.e. stating the class number \\(k\\).\nThe expected value is \\[\n\\text{E}(\\boldsymbol x) = \\boldsymbol \\theta\n\\] The covariance matrix is \\[\n\\text{Var}(\\boldsymbol x) = \\text{Diag}(\\boldsymbol \\theta) - \\boldsymbol \\theta\\boldsymbol \\theta^T\n\\] The covariance matrix is singular by construction because of the dependencies among the elements of \\(\\boldsymbol x\\). This follows directly from the definition of the variance \\(\\text{Var}(\\boldsymbol x) = \\text{E}( \\boldsymbol x\\boldsymbol x^T) - \\text{E}( \\boldsymbol x) \\text{E}( \\boldsymbol x)^T\\) and noting that \\(x_i^2 = x_i\\) and \\(x_i x_j = 0\\) if \\(i \\neq j\\).\nThe corresponding pmf is \\[\np(\\boldsymbol x| \\boldsymbol \\theta) = \\prod_{k=1}^K \\theta_k^{x_k} =\n\\begin{cases}\n   \\theta_k  & \\text{if } x_k = 1 \\\\\n\\end{cases}\n\\] Recall that the pmf is defined over the \\(K-1\\) dimensional support of \\(\\boldsymbol x\\).\nFor \\(K=2\\) the categorical distribution reduces to the Bernoulli \\(\\text{Ber}(\\theta)\\) distribution, with \\(\\theta_1=\\theta\\), \\(\\theta_2=1-\\theta\\) and \\(x_1=x\\) and \\(x_2=1-x\\).\n\n\nConvolution property\nThe convolution of \\(n\\) multinomial distributions, each with identical bin probabilities \\(\\boldsymbol \\theta\\) but possibly different number of items \\(n_i\\), yields another multinomial distribution with the same parameter \\(\\boldsymbol \\theta\\): \\[\n\\sum_{i=1}^n \\text{Mult}(n_i, \\boldsymbol \\theta) \\sim \\text{Mult}\\left(\\sum_{i=1}^n n_i, \\boldsymbol \\theta\\right)\n\\]\nIt follows that the multinomial distribution with \\(n\\) items is the result of the convolution of \\(n\\) categorical distributions: \\[\n\\sum_{i=1}^n \\text{Cat}(\\boldsymbol \\theta) \\sim \\text{Mult}(n, \\boldsymbol \\theta)\n\\] Thus, repeating the same categorical trial \\(n\\) times and counting the total number of allocations in each bin yields a multinomial random variable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#dirichlet-distribution",
    "href": "05-multivariate.html#dirichlet-distribution",
    "title": "5  Multivariate distributions",
    "section": "5.2 Dirichlet distribution",
    "text": "5.2 Dirichlet distribution\nThe Dirichlet distribution \\(\\text{Dir}(\\boldsymbol \\alpha)\\) is the multivariate generalisation of the beta distribution \\(\\text{Beta}(\\alpha_1, \\alpha_2)\\) (Section 4.2) that is useful to model proportions or probabilities for \\(K\\geq 2\\) classes. It is named after Peter Gustav Lejeune Dirichlet (1805–1859).\nIt includes the uniform distribution over the \\(K-1\\) unit simplex as special case.\n\nStandard parametrisation\nA Dirichlet distributed random vector is denoted by \\[\n\\boldsymbol x\\sim \\text{Dir}(\\boldsymbol \\alpha)\n\\] with shape parameter \\(\\boldsymbol \\alpha= (\\alpha_1,...,\\alpha_K)^T &gt;0\\) and \\(K\\geq 2\\). Let \\(m = \\boldsymbol \\alpha^T \\mathbf 1_K = \\sum^{K}_{i=1}\\alpha_i\\). The support of \\(\\boldsymbol x\\) is the \\(K-1\\) dimensional unit simplex given by \\(x_i \\in [0,1]\\) and \\(\\boldsymbol x^T \\mathbf 1_K = \\sum^{K}_{i=1} x_i = 1\\). Thus, the Dirichlet distribution is defined over a \\(K-1\\) dimensional space.\n\n\n\n\n\n\nFigure 5.2: Stick breaking visualisation of a Dirichlet random variable.\n\n\n\nA Dirichlet random variable can be visualised as breaking a unit stick into \\(K\\) individual pieces of lengths \\(x_1, x_2, \\ldots, x_K\\) adding up to one (Figure 5.2). Thus, the \\(x_i\\) may be used as the exclusive proportions or probabilities for \\(K\\) classes.\nThe mean is \\[\n\\text{E}(\\boldsymbol x) = \\frac{\\boldsymbol \\alpha}{m }\n\\] and the variance is \\[\n\\text{Var}\\left(\\boldsymbol x\\right) = \\frac{ m \\text{Diag}(\\boldsymbol \\alpha)-\\boldsymbol \\alpha\\boldsymbol \\alpha^T}{m^2(m+1)}\n\\] The covariance matrix is singular by construction because of the dependencies among the elements of \\(\\boldsymbol x\\). In component notation it is \\[\n\\text{Cov}(x_i,x_j) =  \\frac{[i=j] m \\alpha_i -  \\alpha_i \\alpha_j}{m^2(m+1)}\n\\] where the indicator function \\([i=j]\\) equals 1 if \\(i=j\\) and 0 otherwise.\nThe pdf of the Dirichlet distribution \\(\\text{Dir}(\\boldsymbol \\alpha)\\) is \\[\np(\\boldsymbol x| \\boldsymbol \\alpha) = \\frac{1}{B(\\boldsymbol \\alpha)}  \\prod_{k=1}^K x_k^{\\alpha_k-1}\n\\] This depends on the beta function with multivariate argument \\(\\boldsymbol \\alpha\\) defined as \\[\nB(\\boldsymbol \\alpha) = \\frac{ \\prod_{k=1}^K \\Gamma(\\alpha_k) }{\\Gamma(m )}\n\\] While all \\(K\\) elements of \\(\\boldsymbol x\\) appear in the pdf recall that due the dependencies among the \\(x_i\\) the pdf is defined over a \\(K-1\\) dimensional support.\nFor \\(K=2\\) the Dirichlet distribution reduces to the beta distribution (Section 4.2).\n\n\n\n\n\n\nTipR code\n\n\n\nThe extraDistr package implements the Dirichlet distribution. The pmf of the Dirichlet distribution is given by extraDistr::ddirichlet(). The corresponding random number generator is extraDistr::rdirichlet().\n\n\n\n\nMean parametrisation\nInstead of employing \\(\\boldsymbol \\alpha\\) as parameter vector another useful reparametrisation of the Dirichlet distribution is in terms of a mean parameter \\(\\boldsymbol \\mu\\), with elements \\(\\mu_i \\in [0,1]\\) and \\(\\boldsymbol \\mu^T \\mathbf 1_K = \\sum^{K}_{i=1}\\mu_i = 1\\), and a concentration parameter \\(m &gt; 0\\) so that \\[\n\\boldsymbol x\\sim \\text{Dir}(\\boldsymbol \\alpha=m \\boldsymbol \\mu)\n\\] The concentration and mean parameters can be obtained from \\(\\boldsymbol \\alpha\\) by \\(m = \\boldsymbol \\alpha^T \\mathbf 1_K\\) and \\(\\boldsymbol \\mu= \\boldsymbol \\alpha/m\\). The space of possible values for the mean parameter \\(\\boldsymbol \\mu\\) and the support of \\(\\boldsymbol x\\) are both of dimension \\(K-1\\).\nThe mean and variance of the Dirichlet distribution expressed in terms of \\(\\boldsymbol \\mu\\) and \\(m\\) are \\[\n\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\n\\] and \\[\\text{Var}\\left(\\boldsymbol x\\right) = \\frac{ \\text{Diag}(\\boldsymbol \\mu)-\\boldsymbol \\mu\\boldsymbol \\mu^T}{m+1}\\] The covariance matrix is singular by construction because of the dependencies among the elements of \\(\\boldsymbol x\\). In component notation it is \\[\n\\begin{split}\n\\text{Cov}(x_i,x_j) &=  \\frac{[i=j]\\mu_i -  \\mu_i \\mu_j}{m+1} \\\\\n&=\n\\begin{cases}\n\\mu_i(1-\\mu_i)/(m+1) & \\text{if $i=j$}\\\\\n-\\mu_i \\mu_j / (m+1) & \\text{if $i\\neq j$}\\\\\n\\end{cases}\\\\\n\\end{split}\n\\]\n\n\nSpecial case: symmetric Dirichlet distribution\nFor \\(\\boldsymbol \\alpha= \\alpha \\mathbf 1_K\\) the Dirichlet distribution becomes the symmetric beta distribution with a single shape parameters \\(\\alpha&gt;0\\). In mean parametrisation the symmetric Dirichlet distribution corresponds to \\(\\boldsymbol \\mu=\\mathbf 1_K/K\\) and \\(m=\\alpha K\\).\n\n\nSpecial case: uniform distribution\nFor \\(\\boldsymbol \\alpha= \\mathbf 1_K\\) the Dirichlet distribution becomes the uniform distribution over the \\(K-1\\) unit simplex with pdf \\(p(\\boldsymbol x)=1/\\Gamma(K)\\). In mean parametrisation the symmetric Dirichlet distribution corresponds to \\(\\boldsymbol \\mu=\\mathbf 1_K/K\\) and \\(m=K\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#sec-mvndist",
    "href": "05-multivariate.html#sec-mvndist",
    "title": "5  Multivariate distributions",
    "section": "5.3 Multivariate normal distribution",
    "text": "5.3 Multivariate normal distribution\nThe multivariate normal distribution \\(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) generalises the univariate normal distribution \\(N(\\mu, \\sigma^2)\\) (Section 4.3) from one to \\(d\\) dimensions.\nSpecial cases are the multivariate standard normal distribution \\(N(\\mathbf 0, \\boldsymbol I)\\) and the multivariate delta distribution \\(\\delta\\).\n\nStandard parametrisation\nThe multivariate normal distribution \\(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) has a mean or location parameter \\(\\boldsymbol \\mu\\) (a \\(d\\) dimensional vector), a variance parameter \\(\\boldsymbol \\Sigma\\) (a \\(d \\times d\\) positive definite symmetric matrix) and support \\(\\boldsymbol x\\in \\mathbb{R}^d\\).\nIf a random vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T\\) follows a multivariate normal distribution we write \\[\n\\boldsymbol x\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\n\\] with mean \\[\n\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\n\\] and variance \\[\n\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\n\\] In the above notation the dimension \\(d\\) is implicitly given by the dimensions of \\(\\boldsymbol \\mu\\) and \\(\\boldsymbol \\Sigma\\) but for clarity one often also writes \\(N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) to explicitly indicate the dimension.\nThe pdf is given by \\[\n\\begin{split}\np(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) &= \\det(2 \\pi \\boldsymbol \\Sigma)^{-1/2} \\exp\\left(-\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)   \\right)\\\\\n&= \\det(\\boldsymbol \\Sigma)^{-1/2} (2\\pi)^{-d/2}  e^{-\\Delta^2 /2}\\\\\n\\end{split}\n\\] Here \\(\\Delta^2 = (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)\\) is the squared Mahalanobis distance between \\(\\boldsymbol x\\) and \\(\\boldsymbol \\mu\\) taking into account the variance \\(\\boldsymbol \\Sigma\\). Note that this pdf is a joint pdf over the \\(d\\) elements \\(x_1, \\ldots, x_d\\) of the random vector \\(\\boldsymbol x\\).\nThe multivariate normal distribution is sometimes also used by specifying the precision matrix \\(\\boldsymbol \\Sigma^{-1}\\) instead of the variance \\(\\boldsymbol \\Sigma\\).\nFor \\(d=1\\) the random vector \\(\\boldsymbol x=x\\) is a scalar, \\(\\boldsymbol \\mu= \\mu\\), \\(\\boldsymbol \\Sigma= \\sigma^2\\) and thus the multivariate normal distribution reduces to the univariate normal distribution (Section 4.3).\n\n\n\n\n\n\nTipR code\n\n\n\nThe mnormt package implements the multivariate normal distribution. The function mnormt::dmnorm() provides the pdf and mnormt::pmnorm() returns the distribution function. The function mnormt::rmnorm() is the corresponding random number generator.\nThe mniw package also implements the multivariate normal distribution. The pdf of the Wishart distribution is given by mniw::dmNorm(). The corresponding random number generator is mniw::rmNorm().\n\n\n\n\nScale parametrisation\nIn the univariate case it is straightforward to use the standard deviation \\(\\sigma\\) as scale parameter instead of the variance \\(\\sigma^2\\), and similarly the inverse standard deviation \\(w=1/\\sigma\\) instead of the precision \\(\\sigma^{-2}\\). However, in the multivariate setting with a matrix variance parameter \\(\\boldsymbol \\Sigma\\) it is less obvious how to define a suitable matrix scale parameter.\nLet \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) be the eigendecomposition of the positive definite matrix \\(\\boldsymbol \\Sigma\\). Then \\(\\boldsymbol \\Sigma^{1/2} = \\boldsymbol U\\boldsymbol \\Lambda^{1/2} \\boldsymbol U^T\\) is the principal matrix square root and \\(\\boldsymbol \\Sigma^{-1/2} = \\boldsymbol U\\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T\\) the inverse principal matrix square root. Furthermore, let \\(\\boldsymbol Q\\) be an arbitrary orthogonal matrix with \\(\\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol I\\).\nThen \\(\\boldsymbol W= \\boldsymbol Q\\boldsymbol \\Sigma^{-1/2}\\) is called a whitening matrix based on \\(\\boldsymbol \\Sigma\\) and \\(\\boldsymbol L= \\boldsymbol W^{-1}= \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q^T\\) is the corresponding inverse whitening matrix. By construction, the matrix \\(\\boldsymbol L\\) provides a factorisation of the covariance matrix by \\(\\boldsymbol L\\boldsymbol L^T = \\boldsymbol \\Sigma\\). Similarly, \\(\\boldsymbol W\\) factorises the precision matrix by \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1}\\). The two matrices thus provide the basis for the scale parametrisation of the multivariate normal distribution.\nSpecifically, the matrix \\(\\boldsymbol L\\) is used in place of \\(\\boldsymbol \\Sigma\\) and plays the role of the matrix scale parameter (corresponding to \\(\\sigma\\) in the univariate setting) and \\(\\boldsymbol W\\) is used in place of the precision matrix \\(\\boldsymbol \\Sigma^{-1}\\) and plays the role of the inverse matrix scale parameter (corresponding to \\(1/\\sigma\\) in the univariate case). The determinants occurring in the multivariate normal pdf can be rewritten in terms of \\(\\boldsymbol L\\) and \\(\\boldsymbol W\\) using the identities \\(|\\det(\\boldsymbol W)|=\\det(\\boldsymbol \\Sigma)^{-1/2}\\) and \\(|\\det(\\boldsymbol L)|=\\det(\\boldsymbol \\Sigma)^{1/2}\\) as \\(\\det(\\boldsymbol Q) = \\pm 1\\).\nSince \\(\\boldsymbol Q\\) can be freely chosen the matrices \\(\\boldsymbol W\\) and \\(\\boldsymbol L\\) are not fully determined by \\(\\boldsymbol \\Sigma\\) alone but there is rotational freedom due to \\(\\boldsymbol Q\\). Standard choices are\n\n\\(\\boldsymbol Q^{\\text{ZCA}}=\\boldsymbol I\\) for ZCA-type factorisation with \\(\\boldsymbol W^{\\text{ZCA}}=\\boldsymbol \\Sigma^{-1/2}\\) and\n\\(\\boldsymbol Q^{\\text{PCA}}=\\boldsymbol U^T\\) for PCA-type factorisation with \\(\\boldsymbol W^{\\text{PCA}}=\\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T\\). Note that the matrix \\(\\boldsymbol U\\) is not unique because its columns (eigenvectors) can have different signs (directions), hence \\(\\boldsymbol W^{\\text{PCA}}\\) and \\(\\boldsymbol L^{\\text{PCA}}\\) are also not unique without further constraints, such as positive diagonal elements of the (inverse) whitening matrix.\nA third common choice is to compute \\(\\boldsymbol L\\) directly by Cholesky decomposition of \\(\\boldsymbol \\Sigma\\), which yields an \\(\\boldsymbol L^{\\text{Chol}}\\) (and also a \\(\\boldsymbol W^{\\text{Chol}}\\)) in the form of a lower-triangular matrix with a positive diagonal, and a corresponding underlying \\(\\boldsymbol Q^{\\text{Chol}}=(\\boldsymbol L^{\\text{Chol}})^T \\boldsymbol \\Sigma^{-1/2}\\).\n\nFinally, the whitening matrix \\(\\boldsymbol W\\) and its inverse may also be constructed from the correlation matrix \\(\\boldsymbol P\\) and the diagonal matrix containing the variances \\(\\boldsymbol V\\) (with \\(\\boldsymbol \\Sigma= \\boldsymbol V^{1/2} \\boldsymbol P\\boldsymbol V^{1/2}\\)) in the form \\(\\boldsymbol W= \\boldsymbol Q\\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2}\\) and \\(\\boldsymbol L= \\boldsymbol V^{1/2}  \\boldsymbol P^{1/2} \\boldsymbol Q^T\\).\n\n\nSpecial case: multivariate standard normal distribution\nThe multivariate standard normal distribution \\(N(\\mathbf 0, \\boldsymbol I)\\) has mean \\(\\boldsymbol \\mu=\\mathbf 0\\) and variance \\(\\boldsymbol \\Sigma=\\boldsymbol I\\). The corresponding pdf is \\[\np(\\boldsymbol x) = (2\\pi)^{-d/2} e^{-\\boldsymbol x^T \\boldsymbol x/2 }\n\\] with the squared Mahalanobis distance reduced to \\(\\Delta^2=\\boldsymbol x^T \\boldsymbol x= \\sum_{i=1}^d x_i^2\\).\nThe density of the multivariate standard normal distribution is the product of the corresponding univariate standard normal densities \\[\np(\\boldsymbol x) = \\prod_{i=1}^d \\, (2\\pi)^{-1/2} e^{-x_i^2/2 }\n\\] and therefore the elements \\(x_i\\) of \\(\\boldsymbol x=(x_1, \\ldots, x_d)^T\\) are independent of each other.\n\n\nSpecial case: multivariate delta distribution\nThe multivariate delta distribution \\(\\delta\\) is obtained as the limit of \\(N(\\mathbf 0, \\varepsilon \\boldsymbol A)\\) for \\(\\varepsilon \\rightarrow 0\\) and where \\(\\boldsymbol A\\) is a positive definite matrix (e.g. \\(\\boldsymbol A=\\boldsymbol I\\)). Thus \\(\\delta\\) is a distribution that behaves like an infinite spike at zero.\nThe corresponding pdf \\(\\delta(\\boldsymbol x)\\) is called the multivariate Dirac delta function, even though it is not an ordinary function. It satisfies \\(\\delta(\\boldsymbol x)=0\\) for all \\(\\boldsymbol x\\neq \\mathbf 0\\) and integrates to one, thus representing a point mass at zero.\n\n\nLocation-scale transformation\nLet \\(\\boldsymbol W\\) be a whitening matrix for \\(\\boldsymbol \\Sigma\\) and \\(\\boldsymbol L\\) the corresponding inverse whitening matrix.\nIf \\(\\boldsymbol x\\sim  N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) then \\(\\boldsymbol y= \\boldsymbol W(\\boldsymbol x-\\boldsymbol \\mu)  \\sim N(\\mathbf 0, \\boldsymbol I)\\). This location-scale transformation corresponds to centring and whitening (i.e. standardisation and decorrelation) of a multivariate normal random variable.\nConversely, if \\(\\boldsymbol y\\sim N(\\mathbf 0, \\boldsymbol I)\\) then \\(\\boldsymbol x= \\boldsymbol \\mu+ \\boldsymbol L\\boldsymbol y\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\). This location-scale transformation generates the multivariate normal distribution from the multivariate standard normal distribution.\nNote that under the location-scale transformation \\(\\boldsymbol x= \\boldsymbol \\mu+ \\boldsymbol L\\boldsymbol y\\) with \\(\\text{Var}(\\boldsymbol y)=\\boldsymbol I\\) we get \\(\\text{Cov}(\\boldsymbol x, \\boldsymbol y) = \\boldsymbol L\\). This provides a means to choose between different (inverse) whitening transformation and the corresponding factorisations of \\(\\boldsymbol \\Sigma\\) and \\(\\boldsymbol \\Sigma^{-1}\\). For example, if positive correlation between corresponding elements in \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) is desired then the diagonal elements in \\(\\boldsymbol L\\) must be positive.\n\n\nConvolution property\nThe convolution of \\(n\\) independent, but not necessarily identical, multivariate normal distributions of the same dimension \\(d\\) results in another \\(d\\)-dimensional multivariate normal distribution with corresponding mean and variance: \\[\n\\sum_{i=1}^n N(\\boldsymbol \\mu_i, \\boldsymbol \\Sigma_i) \\sim N\\left( \\sum_{i=1}^n \\boldsymbol \\mu_i,  \\sum_{i=1}^n \\boldsymbol \\Sigma_i \\right)\n\\] Hence, any multivariate normal random variable can be constructed as the sum of \\(n\\) suitable independent multivariate normal random variables.\nSince \\(n\\) is an arbitrary positive integer the multivariate normal distribution is said to be infinitely divisible.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#sec-wisdist",
    "href": "05-multivariate.html#sec-wisdist",
    "title": "5  Multivariate distributions",
    "section": "5.4 Wishart distribution",
    "text": "5.4 Wishart distribution\nThe Wishart distribution \\(\\text{Wis}\\left(\\boldsymbol S, k\\right)\\) is a multivariate generalisation of the gamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\) (Section 4.4) from one to \\(d\\) dimensions.\n\nStandard parametrisation\nIf the symmetric random matrix \\(\\boldsymbol X\\) of dimension \\(d \\times d\\) is Wishart distributed we write \\[\n\\boldsymbol X\\sim  \\text{Wis}\\left(\\boldsymbol S, k\\right)\n\\] where \\(\\boldsymbol S=(s_{ij})\\) is the scale parameter (a symmetric \\(d \\times d\\) positive definite matrix with elements \\(s_{ij}\\)). The dimension \\(d\\) is implicit in the scale parameter \\(\\boldsymbol S\\).\nThe shape parameter \\(k\\) takes on real values in the range \\(k &gt; d-1\\) and integer values in the range \\(k \\in {1, \\ldots, d-1}\\) for \\(d&gt;1\\). For \\(k&gt;d-1\\) the matrix \\(\\boldsymbol X\\) is positive definite and invertible (see also Section 5.5), otherwise \\(\\boldsymbol X\\) is singular and positive semi-definite.\nThe distribution has mean \\[\n\\text{E}(\\boldsymbol X) = k \\boldsymbol S\n\\] and variances of the elements of \\(\\boldsymbol X\\) are \\[\n\\text{Var}(x_{ij})  = k \\left(s^2_{ij}+s_{ii} s_{jj}  \\right)\n\\]\nThe pdf is (for \\(k&gt;d-1\\)) \\[\np(\\boldsymbol X| \\boldsymbol S, k) = \\frac{1}{\\Gamma_d(k/2) \\det(2 \\boldsymbol S)^{k/2}} \\det(\\boldsymbol X)^{(k-d-1)/2}\n\\exp\\left(-\\text{Tr}(\\boldsymbol S^{-1}\\boldsymbol X)/2\\right)\n\\] with the multivariate gamma function defined as \\[\n\\Gamma_d(k/2) = \\pi^{d (d-1)/4} \\prod_{j=1}^d  \\Gamma((k - j+1)/2)\n\\] Note that this pdf is a joint pdf over the \\(d\\) diagonal elements \\(x_{ii}\\) and the \\(d(d-1)/2\\) off-diagonal elements \\(x_{ij}\\) of the symmetric random matrix \\(\\boldsymbol X\\).\nIf \\(\\boldsymbol S\\) is a scalar rather than a matrix (and hence \\(d=1\\)) then the multivariate Wishart distribution reduces to the univariate Wishart aka gamma distribution (Section 4.4).\nThe Wishart distribution is closely related to the multivariate normal distribution with mean zero. Specifically, if \\(\\boldsymbol z\\sim N(\\mathbf 0, \\boldsymbol S)\\) then \\(\\boldsymbol z\\boldsymbol z^T \\sim  \\text{Wis}(\\boldsymbol S, 1)\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe mniw package implements the Wishart distribution. The pdf of the Wishart distribution is given by mniw::dwish(). The corresponding random number generator is mniw::rwish().\n\n\n\n\nMean parametrisation\nIt is useful to employ the Wishart distribution in mean parametrisation \\[\n\\text{Wis}\\left(\\boldsymbol S= \\frac{\\boldsymbol M}{k}, k \\right)\n\\] with parameters \\(\\boldsymbol M= k \\boldsymbol S\\) and \\(k\\). In this parametrisation the mean is \\[\n\\text{E}(\\boldsymbol X) = \\boldsymbol M= (\\mu_{ij})\n\\] and variances of the elements of \\(\\boldsymbol X\\) are \\[\n\\text{Var}(x_{ij})  = \\frac{ \\mu^2_{ij}+\\mu_{ii}\\mu_{jj} }{k}\n\\]\n\n\nSpecial case: standard Wishart distribution\nFor \\(\\boldsymbol S=\\boldsymbol I\\) the Wishart distribution reduces to the standard Wishart distribution \\[\n\\boldsymbol X\\sim  \\text{Wis}\\left(\\boldsymbol I, k\\right)\n\\] with a single shape parameter \\(k\\). The mean is \\[\n\\text{E}(\\boldsymbol X) = k \\boldsymbol I\n\\] and variances of the elements of \\(\\boldsymbol X\\) are \\[\n\\text{Var}(x_{ij})  = \\begin{cases}\n2k & \\text{if $i=j$}\\\\\nk & \\text{if $i\\neq j$}\\\\\n\\end{cases}\n\\] The pdf is (for \\(k&gt; d-1\\)) \\[\np(\\boldsymbol X| k) = \\frac{1}{\\Gamma_d(k/2) 2^{d k/2}} \\det(\\boldsymbol X)^{(k-d-1)/2}\n\\exp\\left(-\\text{Tr}(\\boldsymbol X)/2\\right)\n\\]\nThe standard Wishart distribution is closely related to the standard multivariate normal distribution with mean zero. Specifically, if \\(\\boldsymbol z\\sim N(\\mathbf 0, \\boldsymbol I)\\) then \\(\\boldsymbol z\\boldsymbol z^T \\sim  \\text{Wis}(\\boldsymbol I, 1)\\).\nThe Bartlett decomposition of the standard multivariate Wishart \\(\\text{Wis}(\\boldsymbol I, k)\\) distribution for any real \\(k &gt; d-1\\) is obtained by Cholesky factorisation of the random matrix \\(\\boldsymbol X= \\boldsymbol Z\\boldsymbol Z^T\\). By construction \\(\\boldsymbol Z\\) is a lower-triangular matrix with positive diagonal elements \\(z_{ii}\\) and lower off-diagonal elements \\(z_{ij}\\) with \\(i&gt;j\\) and \\(i,j \\in \\{1, \\ldots, d\\}\\). The corresponding upper off-diagonal elements are set to zero (\\(z_{ji}=0\\)).\nThe \\(d(d+1)/2\\) elements of \\(\\boldsymbol Z\\) are independent and allow to generate a standard Wishart variate as follows:\n\nthe squared diagonal elements follow a univariate standard Wishart distribution \\(z_{ii}^2 \\sim \\text{Wis}(1, k-i+1)\\) and\nthe off-diagonal elements follow the univariate standard normal distribution \\(z_{ij}\\sim N(0,1)\\).\nThen \\(\\boldsymbol X= \\boldsymbol Z\\boldsymbol Z^T \\sim \\text{Wis}(\\boldsymbol I, k)\\).\n\n\n\nScale transformation\nIf \\(\\boldsymbol X\\sim \\text{Wis}(\\boldsymbol S, k)\\) then the scaled symmetric random matrix \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T\\) is also Wishart distributed with \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T \\sim \\text{Wis}(\\boldsymbol A\\boldsymbol S\\boldsymbol A^T, k)\\) where the matrix \\(\\boldsymbol A\\) must be full rank and \\(\\boldsymbol A\\boldsymbol S\\boldsymbol A^T\\) remains positive definite. The matrix \\(\\boldsymbol A\\) may be rectangular, hence the size of \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T\\) and \\(\\boldsymbol A\\boldsymbol S\\boldsymbol A^T\\) may be smaller compared to \\(\\boldsymbol X\\) and \\(\\boldsymbol S\\).\nThe transformations between the Wishart distribution and the standard Wishart distribution are two important special cases:\n\nWith \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol S^{-1}\\) and \\(\\boldsymbol X\\sim \\text{Wis}(\\boldsymbol S, k)\\) then \\(\\boldsymbol Y= \\boldsymbol W\\boldsymbol X\\boldsymbol W^T  \\sim \\text{Wis}(\\boldsymbol I, k)\\) as \\(\\boldsymbol W\\boldsymbol S\\boldsymbol W^T=\\boldsymbol I\\). This transformation reduces the Wishart distribution to the standard Wishart distribution.\nConversely, with \\(\\boldsymbol L\\boldsymbol L^T = \\boldsymbol S\\) and \\(\\boldsymbol Y\\sim \\text{Wis}(\\boldsymbol I, k)\\) then \\(\\boldsymbol X= \\boldsymbol L\\boldsymbol Y\\boldsymbol L^T \\sim \\text{Wis}(\\boldsymbol S, k)\\) as \\(\\boldsymbol L\\boldsymbol I\\boldsymbol L^T=\\boldsymbol S\\). This transformation generates the Wishart distribution from the standard Wishart distribution.\n\n\n\nConvolution property\nThe convolution of \\(n\\) Wishart distributions with the same scale parameter \\(\\boldsymbol S\\) but possible different shape parameters \\(k_i\\) yields another Wishart distribution: \\[\n\\sum_{i=1}^n \\text{Wis}(\\boldsymbol S, k_i) \\sim \\text{Wis}\\left(\\boldsymbol S, \\sum_{i=1}^n k_i\\right)\n\\] Note that the shape parameter \\(k\\) is restricted to be an integer in the range \\(1, \\ldots, d-1\\) for \\(d&gt;1\\) but is a real number in the range \\(k&gt; d-1\\). Thus, if the \\(k_i\\) are all valid shape parameters (for dimension \\(d\\)) then \\(\\sum_{i=1}^n k_i\\) is also a valid shape parameter.\nDue the partial restriction of the shape parameter \\(k\\) to integer values the multivariate Wishart distribution is not infinitely divisible for \\(d&gt;1\\).\nThe above includes the following construction of the multivariate Wishart distribution \\(\\text{Wis}(S, k)\\) for integer-valued \\(k\\). The sum of \\(k\\) independent Wishart random variables \\(\\text{Wis}(\\boldsymbol S, 1)\\) with one degree of freedom and identical scale parameter yields a Wishart random variable \\(\\text{Wis}(\\boldsymbol S, k)\\) with degree of freedom \\(k\\) and the same scale parameter. Thus, if \\(\\boldsymbol z_1,\\boldsymbol z_2,\\dots,\\boldsymbol z_k\\sim N(\\mathbf 0,\\boldsymbol S)\\) are \\(k\\) independent samples from \\(N(\\mathbf 0,\\boldsymbol S)\\) then \\(\\sum_{i_1}^k \\boldsymbol z_i \\boldsymbol z_i^T \\sim  \\text{Wis}(\\boldsymbol S, k)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#sec-iwdist",
    "href": "05-multivariate.html#sec-iwdist",
    "title": "5  Multivariate distributions",
    "section": "5.5 Inverse Wishart distribution",
    "text": "5.5 Inverse Wishart distribution\nThe inverse Wishart distribution \\(\\text{IW}\\left(\\boldsymbol \\Psi, k\\right)\\) is a multivariate generalisation of the inverse gamma distribution \\(\\text{IG}(\\alpha, \\beta)\\) (Section 4.5) from one to \\(d\\) dimensions. It is linked to the Wishart distribution \\(\\text{Wis}(\\boldsymbol S, k)\\) (Section 5.4).\n\nStandard parametrisation\nA symmetric positive definite random matrix \\(\\boldsymbol X\\) of dimension \\(d \\times d\\) following an inverse Wishart distribution is denoted by \\[\n\\boldsymbol X\\sim \\text{IW}\\left(\\boldsymbol \\Psi, k\\right)\n\\] where \\(\\boldsymbol \\Psi= (\\psi_{ij})\\) is the scale parameter (a \\(d \\times d\\) positive definite symmetric matrix) and \\(k&gt; d-1\\) is the shape parameter. The dimension \\(d\\) is implicit in the scale parameter \\(\\boldsymbol \\Psi\\).\nThe mean is (for \\(k &gt; d+1\\)) \\[\n\\text{E}(\\boldsymbol X) = \\frac{\\boldsymbol \\Psi}{k-d-1}\n\\] and the variances of elements of \\(\\boldsymbol X\\) are (for \\(k &gt; d+3\\)) \\[\n\\text{Var}(x_{ij})= \\frac{ (k-d-1) \\, \\psi_{ii} \\psi_{jj} + (k-d+1)\\, \\psi_{ij}^2 }{ (k-d)  (k-d-3) (k-d-1)^2  }\n\\]\nThe inverse Wishart distribution \\(\\text{IW}\\left(\\boldsymbol \\Psi, k\\right)\\) has pdf \\[\np(\\boldsymbol X| \\boldsymbol \\Psi, k) = \\frac{ \\det(\\boldsymbol \\Psi/2)^{k/2}  }{\\Gamma_d(k/2) } \\det(\\boldsymbol X)^{-(k+d+1)/2}\n\\exp\\left(-\\text{Tr}(\\boldsymbol \\Psi\\boldsymbol X^{-1})/2\\right)\n\\] As with the Wishart distribution his pdf is a joint pdf over the \\(d\\) diagonal elements \\(x_{ii}\\) and the \\(d(d-1)/2\\) off-diagonal elements \\(x_{ij}\\) of the symmetric random matrix \\(\\boldsymbol X\\).\nThe inverse Wishart and the Wishart distributions are linked. If \\(\\boldsymbol X\\sim \\text{IW}\\left(\\boldsymbol \\Psi, k\\right)\\) then the inverse of \\(\\boldsymbol X\\) is Wishart distributed with inverted scale parameter: \\[\n\\boldsymbol X^{-1} \\sim \\text{Wis}\\left( \\boldsymbol S= \\boldsymbol \\Psi^{-1}   \\, , k \\right)\n\\] where \\(k\\) is the shape parameter and \\(\\boldsymbol S\\) the scale parameter of the Wishart distribution.\nIf \\(\\boldsymbol \\Psi\\) is a scalar \\(\\psi\\) (and \\(d=1\\)) then the multivariate inverse Wishart distribution reduces to the univariate inverse Wishart distribution (Section 4.5).\n\n\n\n\n\n\nTipR code\n\n\n\nThe mniw package implements the Wishart distribution. The pdf of the Wishart distribution is given by mniw::diwish(). The corresponding random number generator is mniw::riwish().\n\n\n\n\nMean parametrisation\nInstead of \\(\\boldsymbol \\Psi\\) and \\(k\\) we may also equivalently use \\(\\boldsymbol M= \\boldsymbol \\Psi/(k-d-1)\\) and \\(\\kappa = k-d-1\\) as parameters for the inverse Wishart distribution, so that \\[\n\\boldsymbol X\\sim \\text{IW}\\left(\\boldsymbol \\Psi=  \\kappa  \\boldsymbol M\\, , \\, k= \\kappa+d+1\\right)\n\\] with mean (for \\(\\kappa &gt; 0\\)) \\[\n\\text{E}(\\boldsymbol X) =\\boldsymbol M\n\\] and variances (for \\(\\kappa &gt;2)\\) \\[\n\\text{Var}(x_{ij})= \\frac{\\kappa \\, \\mu_{ii} \\mu_{jj} +(\\kappa+2) \\, \\mu_{ij}^2   }{(\\kappa + 1)(\\kappa-2)}\n\\]\nFor \\(\\boldsymbol M\\) equal to scalar \\(\\mu\\) with \\(d=1\\) the above reduces to the univariate inverse Wishart distribution in mean parametrisation.\n\n\nBiased mean parametrisation\nUsing \\(\\boldsymbol T= (t_{ij}) = \\boldsymbol \\Psi/(k-d+1) = \\boldsymbol \\Psi/\\nu\\) as biased mean parameter together with \\(\\nu=k-d+1\\) we arrive at the biased mean parametrisation \\[\n\\boldsymbol X\\sim \\text{IW}\\left(\\boldsymbol \\Psi=  \\nu \\boldsymbol T\\, , \\, k=\\nu+d-1\\right)\n\\]\nThe corresponding mean is (for \\(\\nu &gt; 2\\)) \\[\n\\text{E}(\\boldsymbol X) = \\frac{\\nu}{\\nu-2} \\boldsymbol T= \\boldsymbol M\n\\] and the variances of elements of \\(\\boldsymbol X\\) are (for \\(\\nu &gt; 4\\)) \\[\n\\text{Var}(x_{ij})= \\left(\\frac{\\nu}{\\nu-2}\\right)^2 \\, \\frac{    (\\nu-2) \\, t_{ii} t_{jj} + \\nu \\, t_{ij}^2  }{(\\nu-1)(\\nu-4)}\n\\] As \\(\\boldsymbol T= \\boldsymbol M(\\nu-2)/\\nu\\) for large \\(\\nu\\) the parameter \\(\\boldsymbol T\\) will become identical to the true mean \\(\\boldsymbol M\\).\nFor \\(\\boldsymbol T\\) equal to scalar \\(\\tau^2\\) with \\(d=1\\) the above reduces to the univariate inverse Wishart distribution in biased mean parametrisation.\n\n\nScale transformation\nIf \\(\\boldsymbol X\\sim \\text{IW}(\\boldsymbol \\Psi, k)\\) then the scaled symmetric random matrix \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T\\) is also inverse Wishart distributed with \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T \\sim \\text{IW}(\\boldsymbol A\\boldsymbol \\Psi\\boldsymbol A^T, k)\\) where the matrix \\(\\boldsymbol A\\) has full rank and both \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T\\) and \\(\\boldsymbol A\\boldsymbol \\Psi\\boldsymbol A^T\\) remain positive definite. The matrix \\(\\boldsymbol A\\) may be rectangular, hence the size of \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T\\) and \\(\\boldsymbol A\\boldsymbol \\Psi\\boldsymbol A^T\\) may be smaller compared to \\(\\boldsymbol X\\) and \\(\\boldsymbol \\Psi\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#sec-mvtdist",
    "href": "05-multivariate.html#sec-mvtdist",
    "title": "5  Multivariate distributions",
    "section": "5.6 Multivariate \\(t\\)-distribution",
    "text": "5.6 Multivariate \\(t\\)-distribution\nThe multivariate \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\boldsymbol \\mu, \\boldsymbol T)\\) is a multivariate generalisation of the location-scale \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mu, \\tau^2)\\) (Section 4.6) from one to \\(d\\) dimensions. It is a generalisation of the multivariate normal distribution \\(N(\\boldsymbol \\mu, \\boldsymbol T)\\) (Section 5.3) with an additional parameter \\(\\nu &gt; 0\\) (degrees of freedom) controlling the probability mass in the tails.\nSpecial cases include the multivariate standard \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mathbf 0, \\boldsymbol I)\\), the multivariate normal distribution \\(N(\\boldsymbol \\mu, \\boldsymbol T)\\) and the multivariate Cauchy distribution \\(\\text{Cau}(\\boldsymbol \\mu, \\boldsymbol T)\\).\n\nStandard parametrisation\nIf \\(\\boldsymbol x\\in \\mathbb{R}^d\\) is a multivariate \\(t\\)-distributed random variable we write \\[\n\\boldsymbol x\\sim \\text{$t_{\\nu}$}(\\boldsymbol \\mu, \\boldsymbol T)\n\\] where the vector \\(\\boldsymbol \\mu\\) is the location parameter (a \\(d\\) dimensional vector) and the dispersion parameter \\(\\boldsymbol T\\) is a symmetric positive definite matrix of dimension \\(d \\times d\\). The dimension \\(d\\) is implicit in both parameters. The parameter \\(\\nu &gt; 0\\) prescribes the degrees of freedom. For small values of \\(\\nu\\) the distribution is heavy-tailed and as a result only moments of order smaller than \\(\\nu\\) are finite and defined.\nThe mean is (for \\(\\nu&gt;1\\)) \\[\n\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\n\\] and the variance (for \\(\\nu&gt;2\\)) \\[\n\\text{Var}(\\boldsymbol x) = \\frac{\\nu}{\\nu-2} \\boldsymbol T\n\\]\nThe pdf of \\(\\text{$t_{\\nu}$}(\\boldsymbol \\mu, \\boldsymbol T)\\) is \\[\np(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol T, \\nu) = \\det(\\boldsymbol T)^{-1/2}\n\\frac{\\Gamma(\\frac{\\nu+d}{2})} { (\\pi\\nu)^{d/2}   \\,\\Gamma(\\frac{\\nu}{2})}\n\\left(1+ \\frac{\\Delta^2}{\\nu}  \\right)^{-(\\nu+d)/2}\n\\] with \\(\\Delta^2 = (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol T^{-1} (\\boldsymbol x-\\boldsymbol \\mu)\\) the squared Mahalanobis distance between \\(\\boldsymbol x\\) and \\(\\boldsymbol \\mu\\). Note that this pdf is a joint pdf over the \\(d\\) elements \\(x_1, \\ldots, x_d\\) of the random vector \\(\\boldsymbol x\\).\nFor \\(d=1\\) the random vector \\(\\boldsymbol x=x\\) is a scalar, \\(\\boldsymbol \\mu= \\mu\\), \\(\\boldsymbol T= \\tau^2\\) and thus the multivariate \\(t\\)-distribution reduces to the location-scale \\(t\\)-distribution (Section 4.6).\n\n\n\n\n\n\nTipR code\n\n\n\nThe mnormt package implements the multivariate \\(t\\)-distribution. The function mnormt::dmt() provides the pdf and mnormt::pmt() returns the distribution function. The function mnormt::rmt() is the corresponding random number generator.\n\n\n\n\nScale parametrisation\nThe multivariate \\(t\\)-distribution, like the multivariate distribution, can also be represented with a matrix scale parameter \\(\\boldsymbol L\\) in place of a matrix dispersion parameter \\(\\boldsymbol T\\).\nLet \\(\\boldsymbol L\\) be a matrix scale parameter such that \\(\\boldsymbol L\\boldsymbol L^T = \\boldsymbol T\\) and \\(\\boldsymbol W=\\boldsymbol L^{-1}\\) be the corresponding inverse matrix scale parameter with \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol T^{-1}\\). By construction \\(|\\det(\\boldsymbol W)|=\\det(\\boldsymbol T)^{-1/2}\\) and \\(|\\det(\\boldsymbol L)|=\\det(\\boldsymbol T)^{1/2}\\).\nNote that \\(\\boldsymbol T\\) alone does not fully determine \\(\\boldsymbol L\\) and \\(\\boldsymbol W\\) due to rotational freedom, see the discussion in Section 5.3 for details.\n\n\nSpecial case: multivariate standard \\(t\\)-distribution\nWith \\(\\boldsymbol \\mu=\\mathbf 0\\) and \\(\\boldsymbol T=\\boldsymbol I\\) the multivariate \\(t\\)-distribution reduces to the multivariate standard \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mathbf 0,\\boldsymbol I)\\). It is a generalisation of the multivariate standard normal distribution \\(N(\\mathbf 0,\\boldsymbol I)\\) to allow for heavy tails.\nThe distribution has mean \\(\\text{E}(\\boldsymbol x)=\\mathbf 0\\) (for \\(\\nu&gt;1\\)) and variance \\(\\text{Var}(\\boldsymbol x)=\\frac{\\nu}{\\nu-2}\\boldsymbol I\\) (for \\(\\nu&gt;2\\)).\nThe pdf of \\(\\text{$t_{\\nu}$}(\\mathbf 0,\\boldsymbol I)\\) is \\[\np(\\boldsymbol x| \\nu) =\n\\frac{\\Gamma(\\frac{\\nu+d}{2})} { (\\pi \\nu)^{d/2}   \\,\\Gamma(\\frac{\\nu}{2})}\n\\left(1+ \\frac{  \\boldsymbol x^T \\boldsymbol x}{\\nu}  \\right)^{-(\\nu+d)/2}\n\\] with the squared Mahalanobis distance reducing to \\(\\Delta^2=\\boldsymbol x^T \\boldsymbol x\\).\nFor scalar \\(x\\) (and hence \\(d=1\\)) the multivariate standard \\(t\\)-distribution reduces to the Student’s \\(t\\)-distribution \\(\\text{$t_{\\nu}$}=\\text{$t_{\\nu}$}(0,1)\\).\nUnlike the multivariate standard normal distribution, the density of the multivariate standard \\(t\\)-distribution cannot be written as product of corresponding univariate standard densities.\n\n\nSpecial case: multivariate normal distribution\nFor \\(\\nu \\rightarrow \\infty\\) the multivariate \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\boldsymbol \\mu, \\boldsymbol T)\\) reduces to the multivariate normal distribution \\(N(\\boldsymbol \\mu, \\boldsymbol T)\\) (Section 5.3). Correspondingly, for \\(\\nu \\rightarrow \\infty\\) the multivariate standard \\(t\\)-distribution \\(\\text{$t_{\\nu}$}(\\mathbf 0,\\boldsymbol I)\\) becomes equal to the multivariate standard normal distribution \\(N(\\mathbf 0,\\boldsymbol I)\\).\nThis can be seen from the corresponding limits of the two factors in the pdf of the multivariate \\(t\\)-distribution that depend on \\(\\nu\\):\n\nFollowing Sterling’s approximation for large \\(x\\) we can approximate \\(\\log \\Gamma(x) \\approx (x-1) \\log(x-1)\\). For large \\(\\nu\\) this implies that \\[\\frac{\\Gamma((\\nu+d)/2)} {(\\pi\\nu)^{d/2}  \\,\\Gamma(\\nu/2)} \\rightarrow  (2\\pi)^{-d/2}\\]\nFor small \\(x\\) we can approximate \\(\\log(1+x) \\approx x\\). Thus for large \\(\\nu \\gg d\\) (and hence small \\(\\Delta^2 / \\nu\\)) this yields \\((\\nu+d) \\log(1+ \\Delta^2 / \\nu) \\rightarrow \\Delta^2\\) and hence \\(\\left(1+ \\Delta^2 / \\nu \\right)^{-(\\nu+d)/2} \\rightarrow e^{-\\Delta^2/2}\\).\n\nHence, the pdf of \\(\\text{$t_{\\infty}$}(\\boldsymbol \\mu, \\boldsymbol T)\\) is the multivariate normal pdf \\[\np(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol T, \\nu=\\infty) =\n\\det(\\boldsymbol T)^{-1/2}\n(2\\pi)^{-d/2}\ne^{-\\Delta^2/2}\n\\]\n\n\nSpecial case: multivariate Cauchy distribution\nFor \\(\\nu=1\\) the multivariate \\(t\\)-distribution becomes the multivariate Cauchy distribution \\(\\text{Cau}(\\boldsymbol \\mu, \\boldsymbol T)=\\text{$t_{1}$}(\\boldsymbol \\mu, \\boldsymbol T)\\).\nIts mean, variance and other higher moments are all undefined.\nIt has pdf \\[\np(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol T) =\n\\det(\\boldsymbol T)^{-1/2}\n\\Gamma\\left(\\frac{d+1}{2}\\right)\n\\left(  \\pi (1+ \\Delta^2 ) \\right)^{-(d+1)/2}\n\\]\nFor scalar \\(x\\) (and hence \\(d=1\\)) the multivariate Cauchy distribution \\(\\text{Cau}(\\boldsymbol \\mu, \\boldsymbol T)\\) reduces to the univariate Cauchy distribution \\(\\text{Cau}(\\mu, \\tau^2)\\).\n\n\nSpecial case: multivariate standard Cauchy distribution\nThe multivariate standard Cauchy distribution \\(\\text{Cau}(\\mathbf 0, \\boldsymbol I)=\\text{$t_{1}$}(\\mathbf 0, \\boldsymbol I)\\) is obtained by setting \\(\\boldsymbol \\mu=\\mathbf 0\\) and \\(\\boldsymbol T=\\boldsymbol I\\) in the multivariate Cauchy distribution or, equivalently, by setting \\(\\nu=1\\) in the multivariate standard \\(t\\)-distribution.\nIt has pdf \\[\np(\\boldsymbol x) =\n\\Gamma\\left(\\frac{d+1}{2}\\right)\n\\left(  \\pi (1+ \\boldsymbol x^T \\boldsymbol x) \\right)^{-(d+1)/2}\n\\]\nFor scalar \\(x\\) (and hence \\(d=1\\)) the multivariate standard Cauchy distribution \\(\\text{Cau}(\\mathbf 0, \\boldsymbol I)\\) reduces to the standard univariate Cauchy distribution \\(\\text{Cau}(0, 1)\\).\n\n\nLocation-scale transformation\nLet \\(\\boldsymbol L\\) be a scale matrix for \\(\\boldsymbol T\\) and \\(\\boldsymbol W\\) the corresponding inverse scale matrix.\nIf \\(\\boldsymbol x\\sim  \\text{$t_{\\nu}$}(\\boldsymbol \\mu, \\boldsymbol T)\\) then \\(\\boldsymbol y= \\boldsymbol W(\\boldsymbol x-\\boldsymbol \\mu)  \\sim \\text{$t_{\\nu}$}(\\mathbf 0, \\boldsymbol I)\\). This location-scale transformation reduces a multivariate \\(t\\)-distributed random variable to a standard multivariate \\(t\\)-distributed random variable.\nConversely, if \\(\\boldsymbol y\\sim \\text{$t_{\\nu}$}(\\mathbf 0, \\boldsymbol I)\\) then \\(\\boldsymbol x= \\boldsymbol \\mu+ \\boldsymbol L\\boldsymbol y\\sim \\text{$t_{\\nu}$}(\\boldsymbol \\mu, \\boldsymbol T)\\). This location-scale transformation generates the multivariate \\(t\\)-distribution from the multivariate standard \\(t\\)-distribution.\nNote that for \\(\\nu &gt; 2\\) under the location-scale transformation \\(\\boldsymbol x= \\boldsymbol \\mu+ \\boldsymbol L\\boldsymbol y\\) with \\(\\text{Var}(\\boldsymbol y)=\\nu/(\\nu-2) \\boldsymbol I\\) we get \\(\\text{Cov}(\\boldsymbol x, \\boldsymbol y) = \\nu/(\\nu-2)\\boldsymbol L\\). This provides a means to choose between different factorisations of \\(\\boldsymbol T\\) and \\(\\boldsymbol T^{-1}\\). For example, if positive correlation between corresponding elements in \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) is desired then the diagonal elements in \\(\\boldsymbol L\\) must be positive.\nFor the special case of the multivariate Cauchy distribution (corresponding to \\(\\nu=1\\)) similar relations hold between it and the multivariate standard Cauchy distribution. If \\(\\boldsymbol x\\sim  \\text{Cau}(\\boldsymbol \\mu, \\boldsymbol T)\\) then \\(\\boldsymbol y= \\boldsymbol W(\\boldsymbol x-\\boldsymbol \\mu)  \\sim \\text{Cau}(\\mathbf 0, \\boldsymbol I)\\). Conversely, if \\(\\boldsymbol y\\sim \\text{Cau}(\\mathbf 0, \\boldsymbol I)\\) then \\(\\boldsymbol x= \\boldsymbol \\mu+ \\boldsymbol L\\boldsymbol y\\sim \\text{Cau}(\\boldsymbol \\mu, \\boldsymbol T)\\).\n\n\nConvolution property\nThe multivariate \\(t\\)-distribution is not generally closed under convolution, with the exception of two special cases, the multivariate normal distribution (\\(\\nu=\\infty\\)), see Section 5.3, and the multivariate Cauchy distribution (\\(\\nu=1\\)) with the additional restriction that the dispersion parameters are proportional.\nFor the Cauchy distribution with \\(\\boldsymbol T_i= a_i^2 \\boldsymbol T\\), where \\(a_i&gt;0\\) are positive scalars, \\[\n\\sum_{i=1}^n  \\text{Cau}(\\boldsymbol \\mu_i, a_i^2 \\boldsymbol T)  \\sim\n  \\text{Cau}\\left( \\sum_{i=1}^n \\boldsymbol \\mu_i, \\left(\\sum_{i=1}^n a_i\\right)^2  \\boldsymbol T\\right)\n\\]\n\n\nMultivariate \\(t\\)-distribution as compound distribution\nThe multivariate \\(t\\)-distribution can be obtained as mixture of multivariate normal distributions with identical mean and varying covariance matrix. Specifically, let \\(z\\) be a univariate inverse Wishart random variable \\[\nz \\sim  \\text{IW}(\\psi=\\nu, k=\\nu) =\n        \\text{IG}\\left(\\alpha=\\frac{\\nu}{2}, \\beta=\\frac{\\nu}{2}\\right)\n\\] and let \\(\\boldsymbol x| z\\) be multivariate normal \\[\n\\boldsymbol x| z \\sim N(\\boldsymbol \\mu,\\boldsymbol \\Sigma= z \\boldsymbol T)\n\\] The resulting marginal (scale mixture) distribution for \\(\\boldsymbol x\\) is the multivariate \\(t\\)-distribution \\[\n\\boldsymbol x\\sim \\text{$t_{\\nu}$}\\left(\\boldsymbol \\mu, \\boldsymbol T\\right)\n\\]\nAn alternative way to arrive at \\(\\text{$t_{\\nu}$}\\left(\\boldsymbol \\mu, \\boldsymbol T\\right)\\) is to include \\(\\boldsymbol T\\) as parameter in the inverse Wishart distribution \\[\n\\boldsymbol Z\\sim \\text{IW}(\\boldsymbol \\Psi=\\nu \\boldsymbol T, k=\\nu+d-1)\n\\] and let \\[\n\\boldsymbol x| \\boldsymbol Z\\sim N(\\boldsymbol \\mu,\\boldsymbol \\Sigma= \\boldsymbol Z)\n\\] Note that \\(\\boldsymbol T\\) is now the biased mean parameter of the multivariate inverse Wishart distribution. This characterisation is useful in Bayesian analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Mardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate\nAnalysis. Academic Press.\n\n\nWhittle, P. 2000. Probability via Expectation. 3rd ed.\nSpringer. https://doi.org/10.1007/978-1-4612-0509-8.",
    "crumbs": [
      "Bibliography"
    ]
  }
]