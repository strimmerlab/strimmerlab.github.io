[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability and Distribution Refresher",
    "section": "",
    "text": "Welcome\nThe Probability and Distribution Refresher notes were written by Korbinian Strimmer from 2018–2025. This version is from 11 December 2025.\nIf you have any questions, comments, or corrections please get in touch! 1",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Probability and Distribution Refresher",
    "section": "Updates",
    "text": "Updates\nThe lecture notes will be updated from time to time.\nThe most current version is found at the web page for the\n\nonline version of the Probability and Distribution Refresher notes.\n\nThere you can also download the Probability and Distribution Refresher notes as\n\nPDF in A4 format for printing (double page layout), or as\n6x9 inch PDF for use on tablets (single page layout).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Probability and Distribution Refresher",
    "section": "License",
    "text": "License\nThese notes are licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Probability and Distribution Refresher",
    "section": "",
    "text": "Email address: korbinian.strimmer@manchester.ac.uk↩︎",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "00-preface.html",
    "href": "00-preface.html",
    "title": "Preface",
    "section": "",
    "text": "About the author\nHello! My name is Korbinian Strimmer and I am a Professor in Statistics. I am a member of the Statistics group at the Department of Mathematics of the University of Manchester. You can find more information about me on my home page.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#about-the-notes",
    "href": "00-preface.html#about-the-notes",
    "title": "Preface",
    "section": "About the notes",
    "text": "About the notes\nThese supplementary notes provide a quick refresher on some essential combinatorics and probability concepts, and present an overview of selected univariate and multivariate distributions, along with an introduction to exponential families.\nThe notes are supporting information for a number of lecture courses in statistics that I teach or have taught at the Department of Mathematics of the University of Manchester.\nThis includes the currently offered modules:\n\nMATH27720 Statistics 2: Likelihood and Bayes and\nMATH38161 Multivariate Statistics\n\nas well as the retired module (not offered any more):\n\nMATH20802 Statistical Methods.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-combinatorics.html",
    "href": "01-combinatorics.html",
    "title": "1  Combinatorics",
    "section": "",
    "text": "1.1 Some basic mathematical notation\nScalar quantity: plain font, typically lower case (\\(x\\), \\(\\theta\\), n), sometimes upper case (\\(K\\), \\(R^2\\), distribution functions \\(F\\), \\(P\\), \\(Q\\)).\nSets: plain font, upper case (\\(\\Omega, \\mathcal{F}\\))\nVector quantity: bold font, lower case (\\(\\boldsymbol x\\), \\(\\boldsymbol \\theta\\)).\nMatrix quantity: bold font, upper case (\\(\\boldsymbol X\\), \\(\\boldsymbol \\Sigma\\)).\nSummation: \\[\n\\sum_{i=1}^n x_i = x_1 + x_2 + \\ldots + x_n\n\\]\nProduct: \\[\n\\begin{split}\n\\prod_{i=1}^n x_i &= x_1 \\times x_2 \\times \\ldots \\times x_n\\\\\n&= x_1  x_2  \\ldots  x_n\n\\end{split}\n\\] The multiplication sign \\(\\times\\) between the factors is usually omitted unless it is needed for clarity.\nIndicator function (in Iverson bracket notation): \\[\n[A] =\n\\begin{cases}\n1 & \\text{if $A$ is true}\\\\\n0 & \\text{if $A$ is not true}\\\\\n\\end{cases}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "01-combinatorics.html#number-of-permutations",
    "href": "01-combinatorics.html#number-of-permutations",
    "title": "1  Combinatorics",
    "section": "1.2 Number of permutations",
    "text": "1.2 Number of permutations\nA permutation or ordering is a specific arrangement of items in a sequence, or equivalently, a specific assignment to labelled positions.\nThe factorial \\[\nn! = \\prod_{i=1}^n i = 1 \\times 2 \\times \\ldots \\times n\n\\] is the number of permutations of \\(n\\) distinct items, where \\(n\\) is a positive integer.\nFor \\(n=0\\) the factorial is defined as \\[\n0! = 1\n\\]\nThus, the factorial \\(n!\\) equals the number of ways to place \\(n\\) distinct items into \\(n\\) labelled boxes so that each box contains exactly one item.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "01-combinatorics.html#multinomial-and-binomial-coefficient",
    "href": "01-combinatorics.html#multinomial-and-binomial-coefficient",
    "title": "1  Combinatorics",
    "section": "1.3 Multinomial and binomial coefficient",
    "text": "1.3 Multinomial and binomial coefficient\nThe multinomial coefficient for \\(K\\) groups \\[\n\\begin{split}\nW_K &= \\binom{n}{n_1, \\ldots, n_K} \\\\\n    &= \\frac {n!}{n_1! \\, n_2! \\, \\ldots \\, n_K! }\n\\end{split}\n\\] is the number of permutations of \\(n\\) distinct items allocated to \\(K\\leq n\\) groups, with \\(n_k\\) unordered items in group \\(k\\) and \\(\\sum_{k=1}^K n_k=n\\).\nThus, the multinomial coefficient \\(W_K\\) equals the number of ways to place \\(n\\) distinct items into \\(K\\) labelled boxes so that box \\(k\\) contains exactly \\(n_k\\) unordered items, with \\(\\sum_{k=1}^K n_k=n\\).\nFor \\(n_k=1\\) (and thus \\(K=n\\)) the multinomial coefficient reduces to the factorial.\nFor two groups (\\(K=2\\)) the multinomial coefficient becomes the binomial coefficient \\[\n\\begin{split}\nW_2 &= \\binom{n}{n_1, n_2} = \\binom{n}{n_1, n- n_1}\\\\\n    &=  \\frac {n!}{n_1! \\, (n - n_1)!}\\\\\n    & = \\binom{n}{n_1}\n\\end{split}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "01-combinatorics.html#de-moivre-sterling-approximation",
    "href": "01-combinatorics.html#de-moivre-sterling-approximation",
    "title": "1  Combinatorics",
    "section": "1.4 De Moivre-Sterling approximation",
    "text": "1.4 De Moivre-Sterling approximation\nThe factorial is frequently approximated by the following formula derived by Abraham de Moivre (1667–1754) and James Stirling (1692-1770) \\[\nn! \\approx \\sqrt{2 \\pi} n^{n+\\frac{1}{2}} e^{-n}\n\\] or equivalently on logarithmic scale \\[\n\\log n!  \\approx \\left(n+\\frac{1}{2}\\right) \\log n  -n + \\frac{1}{2}\\log \\left( 2 \\pi\\right)\n\\] The approximation is good for small \\(n\\) (but fails for \\(n=0\\)) and becomes more and more accurate with increasing \\(n\\). For large \\(n\\) the approximation can be simplified to \\[\n\\log n! \\approx  n \\log n  -n\n\\]\nThe de Moivre-Sterling approximation applied to the multinomial coefficient yields \\[\n\\begin{split}\n\\log W_K & \\approx - n \\sum_{k=1}^K \\frac{n_k}{n} \\log\\left( \\frac{n_k}{n} \\right)\\\\\n& = - n \\sum_{k=1}^K q_k \\log q_k  = n H(\\hat{Q})\\\\\n\\end{split}\n\\] Hence, for large \\(n\\) and large \\(n_k\\) the logarithm of the multinomial coefficient equals \\(n\\) times the information entropy \\(H(\\hat{Q})\\) of the empirical categorical distribution \\(\\hat{Q}\\) with class frequencies \\(\\hat{q}_k = n_k/n\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "02-probability.html",
    "href": "02-probability.html",
    "title": "2  Probability",
    "section": "",
    "text": "2.1 Random variables\nA random variable describes a random experiment. The set of all possible outcomes is the sample space of the random variable and is denoted by \\(\\Omega\\). If \\(\\Omega\\) is countable then the random variable is discrete, otherwise it is continuous. For a discrete random variable the sample space \\(\\Omega = \\{\\omega_1, \\omega_2, \\ldots\\}\\) is composed of a finite or infinite number of elementary outcomes \\(\\omega_i\\).\nAn event \\(A \\subseteq \\Omega\\) is a subset of \\(\\Omega\\). This includes as special cases the complete set \\(\\Omega\\) (“certain event”) and the empty set \\(\\emptyset\\) (“impossible event”). The set of all possible events is denoted by \\(\\mathcal{F}\\). The complementary event \\(A^C = \\Omega \\setminus A\\) is the complement of the set \\(A\\) in the sample space \\(\\Omega\\). Two events \\(A_1\\) and \\(A_2\\) are mutually exclusive if the sets are disjoint with \\(A_1 \\cap A_2 = \\emptyset\\).\nFor a discrete random variable, the elementary outcomes \\(\\omega_i\\) are referred to as elementary events, and they are all mutually exclusive. An event \\(A\\) consists of a number of elementary events \\(\\omega_i \\in A\\) and the complementary event is given by \\(A^C = \\{\\omega_i \\in \\Omega:  \\omega_i \\notin A\\}\\).\nThe probability of an event \\(A\\) is denoted by \\(\\operatorname{Pr}(A)\\). Broadly, \\(\\operatorname{Pr}(A)\\) provides a measure of the size of the set \\(A\\) relative to the set \\(\\Omega\\). The probability measure \\(\\operatorname{Pr}(A)\\) satisfies the three axioms of probability:\nThis implies\nFrom the above it is evident that probability is closely linked to set theory, in particular to measure theory which serves as the theoretical foundations of probability and generalisations. For instance, if \\(\\operatorname{Pr}(\\emptyset) = 0\\) is assumed instead of \\(\\operatorname{Pr}(\\Omega) = 1\\), this leads to the axioms for a positive measure (of which probability is a special case).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#random-variables",
    "href": "02-probability.html#random-variables",
    "title": "2  Probability",
    "section": "",
    "text": "\\(\\operatorname{Pr}(A) \\geq 0\\), probabilities are non-negative,\n\\(\\operatorname{Pr}(\\Omega) = 1\\), the certain event has probability 1, and\n\\(\\operatorname{Pr}(A_1 \\cup A_2 \\cup \\ldots) = \\sum_i \\operatorname{Pr}(A_i)\\), the probability of countable mutually exclusive events \\(A_i\\) is additive.\n\n\n\n\\(\\operatorname{Pr}(A) \\leq 1\\), probability values lie within the range \\([0,1]\\),\n\\(\\operatorname{Pr}(A^C) = 1 - \\operatorname{Pr}(A)\\), the probability of the complement, and\n\\(\\operatorname{Pr}(\\emptyset) = 0\\), the impossible event has probability 0.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#conditional-probability",
    "href": "02-probability.html#conditional-probability",
    "title": "2  Probability",
    "section": "2.2 Conditional probability",
    "text": "2.2 Conditional probability\nConsider two events \\(A\\) and \\(B\\), which may not be be mutually exclusive. The probability of the event “\\(A\\) and \\(B\\)” is given by the probability of the set intersection \\(\\operatorname{Pr}(A \\cap B)\\). The probability of the event “\\(A\\) or \\(B\\)” is given by the probability of the set union \\[\n\\operatorname{Pr}(A \\cup B) = \\operatorname{Pr}(A) + \\operatorname{Pr}(B) - \\operatorname{Pr}(A \\cap B)\\,.\n\\] This identity follows from the axioms.\nThe conditional probability of event \\(A\\) assuming event \\(B\\) has occurred is given by \\[\n\\operatorname{Pr}(A | B) = {\\operatorname{Pr}( A \\cap B) \\over \\operatorname{Pr}(B)}\n\\] Essentially, now \\(B\\) acts as the new sample space relative to which \\(A\\) is measured, restricting it from \\(\\Omega\\). Note that \\(\\operatorname{Pr}(A | B)\\) is generally not the same as \\(\\operatorname{Pr}(B | A)\\), see Bayes’ theorem below.\nImportantly, it can be seen that any probability may be viewed as conditional, namely relative to \\(\\Omega\\) as \\(\\operatorname{Pr}(A) = \\operatorname{Pr}(A| \\Omega)\\).\nFrom the definition of conditional probability we derive the product rule \\[\n\\begin{split}\n\\operatorname{Pr}( A \\cap B) &= \\operatorname{Pr}(A | B)\\, \\operatorname{Pr}(B) \\\\\n&= \\operatorname{Pr}(B | A)\\, \\operatorname{Pr}(A)\n\\end{split}\n\\] which in turn yields Bayes’ theorem \\[\n\\operatorname{Pr}(A | B ) = \\operatorname{Pr}(B | A) { \\operatorname{Pr}(A) \\over \\operatorname{Pr}(B)}\n\\] This theorem is useful for changing the order of conditioning and it plays a key role in Bayesian statistics.\nIf \\(\\operatorname{Pr}( A \\cap B) =  \\operatorname{Pr}(A) \\, \\operatorname{Pr}(B)\\) then the two events \\(A\\) and \\(B\\) are independent with \\(\\operatorname{Pr}(A | B ) = \\operatorname{Pr}(A)\\) and \\(\\operatorname{Pr}(B | A ) = \\operatorname{Pr}(B)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#probability-mass-and-density-function",
    "href": "02-probability.html#probability-mass-and-density-function",
    "title": "2  Probability",
    "section": "2.3 Probability mass and density function",
    "text": "2.3 Probability mass and density function\nThe distribution (or law) of a random variable \\(x\\) with sample space \\(\\Omega\\) is the probability measure that assigns probabilities to values or ranges of \\(x\\). This is done in practise by employing probability mass functions (pmf, for discrete random variables) or probability density functions (pdf, for continuous random variables).\nThe scalar random variable \\(x\\) is written in lowercase plain font. We use the same symbol \\(x\\) for both the random variable and its realisations.1\nFor a discrete random variable we define the event \\(A = \\{x: x=a\\} = \\{a\\}\\) (corresponding to a single elementary event) and get the probability \\[\n\\operatorname{Pr}(A) = \\operatorname{Pr}(x=a) = f(a)\n\\] directly from the probability mass function (pmf). The pmf has the property that \\(\\sum_{x \\in \\Omega} f(x) = 1\\) and that \\(f(x) \\in [0,1]\\).\nFor continuous random variables employ a probability density function (pdf) instead. We define the event \\(A = \\{x: a &lt; x \\leq a + da\\}\\) (corresponding to an infinitesimal interval) and then assign the probability \\[\n\\operatorname{Pr}(A) = \\operatorname{Pr}( a &lt; x \\leq a + da) = f(a) da \\,.\n\\] Similarly, the probability of the event \\(A = \\{x:a_1 &lt; x \\leq a_2  \\}\\) is given by \\[\n\\operatorname{Pr}(A) = \\operatorname{Pr}( a_1 &lt; x \\leq a_2) = \\int_{a_1}^{a_2} f(a) da \\,.\n\\] The pdf has the property that \\(\\int_{x \\in \\Omega} f(x) dx = 1\\) but in contrast to a pmf the density \\(f(x)\\geq 0\\) may take on values larger than 1.\nIt is sometimes convenient to refer to a pdf or a pmf collectively as probability density mass function (pdmf) without specifying whether \\(x\\) is continuous or discrete.\nThe set of all \\(x\\) for which \\(f(x)\\) is positive is called the support of the pdmf.\nUsing the pdmf, the probability of general event \\(A  \\subseteq \\Omega\\) is given by \\[\n\\operatorname{Pr}(A) =\n\\begin{cases}\n\\sum_{x \\in A} f(x) & \\text{discrete case} \\\\\n\\int_{x \\in A} f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\]\n\n\n\n\n\n\nFigure 2.1: Illustration of i) pdmf, ii) distribution function and iii) quantile function for a continuous (first column) and a discrete random variable (second column).\n\n\n\nFigure 2.1 (first row) illustrates the pdmf for a continuous and discrete random variable.\nIn the above we denoted the pdmf by the lower case letter \\(f\\) though we also often use \\(p\\) or \\(q\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#cumulative-distribution-function",
    "href": "02-probability.html#cumulative-distribution-function",
    "title": "2  Probability",
    "section": "2.4 Cumulative distribution function",
    "text": "2.4 Cumulative distribution function\nAs alternative to the pdmf we can describe the random variable using a cumulative distribution function (cdf). This requires an ordering so that we can define the event \\(A = \\{x: x \\leq a \\}\\) and compute its probability as \\[\nF(a) = \\operatorname{Pr}(A) = \\operatorname{Pr}( x \\leq a ) =\n\\begin{cases}\n\\sum_{x \\in A} f(x) & \\text{discrete case} \\\\\n\\int_{x \\in A} f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\] Th cdf is denoted by the same letter as the pdmf but in upper case (usually \\(F\\), \\(P\\) and \\(Q\\)). By construction the cumulative distribution function is monotonically non-decreasing and its value ranges from 0 to 1. For a discrete random variable \\(F(a)\\) is a step function with jumps of size \\(f(\\omega_i)\\) at the elementary outcomes \\(\\omega_i\\).\nWith the help of the cdf we can compute the probability of the event \\(A = \\{x:a_1 &lt; x \\leq a_2  \\}\\) simply as \\[\n\\operatorname{Pr}( A ) = F(a_2)-F(a_1) \\,.\n\\] This works both for discrete and continuous random variables.\nFigure 2.1 (second row) illustrates the distribution function for a continuous and discrete random variable.\nIt is common to use the same upper case letter as the cdf to name the distribution. Thus, if a random variable \\(x\\) has distribution \\(F\\) we write \\(x \\sim F\\), and this implies it has a pdmf \\(f(x)\\) and cdf \\(F(x)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#quantile-function-and-quantiles",
    "href": "02-probability.html#quantile-function-and-quantiles",
    "title": "2  Probability",
    "section": "2.5 Quantile function and quantiles",
    "text": "2.5 Quantile function and quantiles\nThe quantile function is defined as \\(q_F(b) = \\min\\{ x: F(x) \\geq b \\}\\). For a continuous random variable the quantile function simplifies to \\(q_F(b) = F^{-1}(b)\\), i.e. it is the ordinary inverse \\(F^{-1}(b)\\) of the distribution function.\nFigure 2.1 (third row) illustrates the quantile function for a continuous and discrete random variable.\nThe quantile \\(x\\) of order \\(b\\) of the distribution \\(F\\) is often denoted by \\(x_b= q_F(b)\\).\nThe 25% quantile \\(x_{1/4} = x_{25\\%} =  q_F(1/4)\\) is called the first quartile or lower quartile.\nThe 50% quantile \\(x_{1/2} = x_{50\\%} = q_F(1/2)\\) is called the second quartile or median.\nThe 75% quantile \\(x_{3/4} = x_{75\\%} = q_F(3/4)\\) is called the third quartile or upper quartile.\nThe interquartile range is the difference between the upper and lower quartiles and equals \\(\\operatorname{IQR}(F) =\nq_F(3/4) - q_F(1/4)\\).\nThe quantile function is also useful for generating general random variates from uniform random variates. If \\(y\\sim \\operatorname{Unif}(0,1)\\) then \\(x=q_F(y) \\sim F\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#expectation-or-mean",
    "href": "02-probability.html#expectation-or-mean",
    "title": "2  Probability",
    "section": "2.6 Expectation or mean",
    "text": "2.6 Expectation or mean\nThe expected value of a random variable \\(x\\sim F\\) is defined as the weighted average over all possible outcomes, with the weight given by the pdmf \\(f(x)\\): \\[\n\\begin{split}\n\\operatorname{E}(x) &= \\operatorname{E}_F(x) = \\operatorname{E}(F)\\\\\n& =\n\\begin{cases}\n\\sum_{x \\in \\Omega} f(x) \\, x & \\text{discrete case} \\\\\n\\int_{x \\in \\Omega} f(x) \\, x \\, dx  & \\text{continuous case} \\\\\n\\end{cases}\\\\\n\\end{split}\n\\] The subscript \\(F\\) in \\(\\operatorname{E}_{F}(x)\\) indicates that the expectation is taken with regard to the distribution \\(F\\), but is usually left out if there are no ambiguities. The notation \\(\\operatorname{E}(F)\\) emphasises that the mean is a functional of the distribution \\(F\\).\nBecause the sum or integral may diverge, not all distributions have finite means so the mean does not always exist (in contrast to the median, or quantiles in general). For example, the location-scale \\(t\\)-distribution \\(t_{\\nu}(\\mu, \\tau^2)\\) does not have a mean for a degree of freedom in the range \\(0 &lt; \\nu \\leq 1\\) (see Section 5.6).\nExpectation is a linear operator, meaning that \\[\n\\operatorname{E}(a_1 x_1 + a_2 x_2) = a_1  \\operatorname{E}(x_1) +  a_2  \\operatorname{E}(x_2)\n\\] for random variables \\(x_1\\sim F_1\\) and \\(x_2 \\sim F_2\\) and constants \\(a_1\\) and \\(a_2\\).\nConsequently, expectation is mixture preserving so that \\[\n\\operatorname{E}(Q_{\\lambda}) = (1-\\lambda) \\operatorname{E}(Q_0) + \\lambda \\operatorname{E}(Q_1)\n\\] for the mixture \\(Q_{\\lambda}=(1-\\lambda) Q_0 + \\lambda Q_1\\) with \\(0 &lt; \\lambda &lt; 1\\) and \\(Q_0 \\neq Q_1\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#variance",
    "href": "02-probability.html#variance",
    "title": "2  Probability",
    "section": "2.7 Variance",
    "text": "2.7 Variance\nThe variance of a random variable \\(x\\sim F\\) is the expected value of the squared deviation around the mean \\(\\mu = \\operatorname{E}(x)\\): \\[\n\\begin{split}\n\\operatorname{Var}(x) &= \\operatorname{Var}_F(x) =  \\operatorname{Var}(F)\\\\\n       &= \\operatorname{E}\\left( (x - \\mu))^2 \\right) \\\\\n       &=  \\operatorname{E}(x^2)-\\mu^2\n\\end{split}\n\\] By construction, \\(\\operatorname{Var}(x) \\geq 0\\).\nThe notation \\(\\operatorname{Var}(F)\\) highlights that the variance is a functional of the distribution \\(F\\). Occasionally, we write \\(\\operatorname{Var}_F(x)\\) indicate that the expectation is taken with regard to the distribution \\(F\\).\nLike the mean, the variance may diverge and hence not necessarily exists for all distribution. For example, the location-scale \\(t\\)-distribution \\(t_{\\nu}(\\mu, \\tau^2)\\) does not have a variance for the degree of freedom in the range \\(0 &lt; \\nu \\leq 2\\) (see Section 5.6).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#moments-of-a-distribution",
    "href": "02-probability.html#moments-of-a-distribution",
    "title": "2  Probability",
    "section": "2.8 Moments of a distribution",
    "text": "2.8 Moments of a distribution\nThe \\(n\\)-th moment of a distribution \\(F\\) for a random variable \\(x\\) is defined as follows: \\[\n\\mu_n(F) = \\operatorname{E}(x^n)\n\\]\nSpecial important cases are the\n\nZeroth moment: \\(\\mu_0(F) = \\operatorname{E}(x^0) = 1\\) (since the pdmf integrates to one)\nFirst moment: \\(\\mu_1(F) = \\operatorname{E}(x^1) = \\operatorname{E}(x) = \\mu\\) (=the mean)\nSecond moment: \\(\\mu_2(F) = \\operatorname{E}(x^2)\\)\n\nThe \\(n\\)-th central moment centred around the mean \\(\\operatorname{E}(x) = \\mu\\) is given by \\[\nm_n(F) = \\operatorname{E}((x-\\mu)^n)\n\\]\nThe first few central moments are the\n\nZeroth central moment: \\(m_0(F) = \\operatorname{E}((x-\\mu)^0) = 1\\)\nFirst central moment: \\(m_1(F) = \\operatorname{E}((x-\\mu)^1) = 0\\)\nSecond central moment: \\(m_2(F) = \\operatorname{E}\\left( (x - \\mu)^2 \\right)\\) (=the variance)\n\nThe moments of a distribution are not necessarily all finite, i.e. some moments may not exist. For example, the location-scale \\(t\\)-distribution \\(t_{\\nu}(\\mu, \\tau^2)\\) only has finite moments of degree smaller than the degree of freedom \\(\\nu\\) (see Section 5.6).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#expectation-of-a-transformed-random-variable",
    "href": "02-probability.html#expectation-of-a-transformed-random-variable",
    "title": "2  Probability",
    "section": "2.9 Expectation of a transformed random variable",
    "text": "2.9 Expectation of a transformed random variable\nOften, one needs to find the mean of a transformed random variable. If \\(x\\sim F_x\\) and \\(y= h(x)\\) with \\(y \\sim F_y\\) then one can directly apply the above definition to obtain \\(\\operatorname{E}(y) = \\operatorname{E}(F_y)\\). However, this requires knowledge of the transformed pdmf \\(f_y(y)\\) (see Chapter 3 for more details about variable transformations).\nAs an alternative, the “law of the unconscious statistician”(LOTUS) provides a convenient shortcut to compute the mean of the transformed random variable \\(y=h(x)\\) using only the pdmf of the original variable \\(x\\): \\[\n\\operatorname{E}(h(x)) =\n\\begin{cases}\n\\sum_{x \\in \\Omega} f(x) \\, h(x) & \\text{discrete case} \\\\\n\\int_{x \\in \\Omega}  f(x) \\, h(x) \\, dx & \\text{continuous case} \\\\\n\\end{cases}\n\\] Note this is not an approximation but equivalent to obtaining the mean using the transformed pdmf.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#probability-as-expectation",
    "href": "02-probability.html#probability-as-expectation",
    "title": "2  Probability",
    "section": "2.10 Probability as expectation",
    "text": "2.10 Probability as expectation\nProbability itself can also be understood as an expectation.\nFor an event \\(A  \\subseteq \\Omega\\) we define a corresponding indicator function \\([x \\in A]\\). From LOTUS it then follows immediately that \\[\n\\begin{split}\n\\operatorname{E}\\left( \\left[x \\in A\\right] \\right) &=\n\\begin{cases}\n\\sum_{x \\in A} f(x)  & \\text{discrete case} \\\\\n\\int_{x \\in A}  f(x)  \\, dx & \\text{continuous case} \\\\\n\\end{cases}\\\\\n& =\\operatorname{Pr}(A)\n\\end{split}\n\\]\nThis relation is called the “fundamental bridge” between probability and expectation. Interestingly, one can develop the whole theory of probability from this perspective (e.g., Whittle 2000).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#jensens-inequality-for-the-expectation",
    "href": "02-probability.html#jensens-inequality-for-the-expectation",
    "title": "2  Probability",
    "section": "2.11 Jensen’s inequality for the expectation",
    "text": "2.11 Jensen’s inequality for the expectation\nIf \\(h(\\boldsymbol x)\\) is a convex function then the following inequality holds:\n\\[\n\\operatorname{E}(h(\\boldsymbol x)) \\geq h(\\operatorname{E}(\\boldsymbol x))\n\\]\nRecall: a convex function (such as \\(x^2\\)) has the shape of a “valley”.\nAn example of Jensen’s inequality is \\(\\operatorname{E}(x^2)\\geq \\operatorname{E}(x)^2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#random-vectors-and-their-mean-and-variance",
    "href": "02-probability.html#random-vectors-and-their-mean-and-variance",
    "title": "2  Probability",
    "section": "2.12 Random vectors and their mean and variance",
    "text": "2.12 Random vectors and their mean and variance\nIn addition to scalar random variables we often make use of random vectors and random matrices.2\nThe mean of a random vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T \\sim F\\) is given by \\[\n\\begin{split}\n\\operatorname{E}(\\boldsymbol x) &= \\operatorname{E}(F) \\\\\n           &= \\underbrace{\\boldsymbol \\mu}_{d \\times 1} = (\\mu_1, \\ldots, \\mu_d)^T\\\\\n\\end{split}\n\\] and thus is a vector of the same dimension as \\(\\boldsymbol x\\), where \\(\\mu_i = \\operatorname{E}(x_i)\\) are the means of the individual components \\(x_i\\).\nThe variance of a random vector \\(\\boldsymbol x\\) of length \\(d\\), however, is not a vector but a matrix of size \\(d\\times d\\). This matrix is called the covariance matrix: \\[\n\\begin{split}\n\\operatorname{Var}(\\boldsymbol x) &= \\operatorname{Var}(F) \\\\\n          &= \\underbrace{\\boldsymbol \\Sigma}_{d\\times d} = (\\sigma_{ij})\n          = \\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix} \\\\\n  &=\\operatorname{E}\\left(\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d\\times 1} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1\\times d}\\right) \\\\\n  & = \\operatorname{E}(\\boldsymbol x\\boldsymbol x^T)-\\boldsymbol \\mu\\boldsymbol \\mu^T \\\\\n\\end{split}\n\\] The elements \\(\\operatorname{Cov}(x_i, x_j)=\\sigma_{ij}\\) describe the covariance between the random variables \\(x_i\\) and \\(x_j\\). The covariance matrix is symmetric, hence \\(\\sigma_{ij}=\\sigma_{ji}\\). The diagonal elements \\(\\operatorname{Cov}(x_i, x_i)=\\sigma_{ii}\\) correspond to the individual variances \\(\\operatorname{Var}(x_i) = \\sigma_i^2\\). By construction, the covariance matrix \\(\\boldsymbol \\Sigma\\) is positive semi-definite, i.e. the eigenvalues of \\(\\boldsymbol \\Sigma\\) are all positive or equal to zero.\nHowever, wherever possible one will aim to use models with non-singular covariance matrices, with all eigenvalues positive, so that the covariance matrix is invertible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#correlation-matrix",
    "href": "02-probability.html#correlation-matrix",
    "title": "2  Probability",
    "section": "2.13 Correlation matrix",
    "text": "2.13 Correlation matrix\nThe correlation matrix \\(\\boldsymbol P\\) (“upper case rho”, not “upper case p”) is the variance standardised version of the covariance matrix \\(\\boldsymbol \\Sigma\\).\nSpecifically, denote by \\(\\boldsymbol V\\) the diagonal matrix containing the variances \\[\n\\boldsymbol V= \\begin{pmatrix}\n    \\sigma_{11} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma_{dd}\n\\end{pmatrix}\n\\] then the correlation matrix \\(\\boldsymbol P\\) is given by \\[\n\\boldsymbol P= (\\rho_{ij}) = \\begin{pmatrix}\n    1 & \\dots & \\rho_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{d1} & \\dots & 1\n\\end{pmatrix}   = \\boldsymbol V^{-1/2} \\, \\boldsymbol \\Sigma\\, \\boldsymbol V^{-1/2}\n\\] Like the covariance matrix the correlation matrix is symmetric. The elements of the diagonal of \\(\\boldsymbol P\\) are all set to 1.\nEquivalently, in component notation the correlation between \\(x_i\\) and \\(x_j\\) is given by \\[\n\\rho_{ij} = \\operatorname{Cor}(x_i,x_j) = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}\n\\]\nFollowing from the definition above, a covariance matrix \\(\\boldsymbol \\Sigma\\) can be factorised into the product of standard deviations \\(\\boldsymbol V^{1/2}\\) and the correlation matrix \\(\\boldsymbol P\\) as follows: \\[\n\\boldsymbol \\Sigma= \\boldsymbol V^{1/2}\\, \\boldsymbol P\\,\\boldsymbol V^{1/2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#parameters-and-families-of-distributions",
    "href": "02-probability.html#parameters-and-families-of-distributions",
    "title": "2  Probability",
    "section": "2.14 Parameters and families of distributions",
    "text": "2.14 Parameters and families of distributions\nA distribution family \\(F(\\theta)\\) is a collection of distributions obtained by varying a parameter \\(\\theta\\). Each specific value of the parameter \\(\\theta\\) indexes one distribution in that family.\nCommon distribution families are usually denoted by familiar abbreviation such as \\(N(\\mu, \\sigma^2)\\) for the normal family. We also call these simply “distributions” with parameters and omit the word “family”.\nIf a random variable \\(x\\) has distribution \\(F(\\theta)\\) we write \\(x \\sim F(\\theta)\\). In case of named distributions, we use the corresponding abbreviation such \\(x \\sim N(\\mu, \\sigma^2)\\) (normal distribution) or \\(x \\sim \\operatorname{Beta}(\\alpha_1, \\alpha_2)\\).\nThe associated pdmf is written \\(f(x; \\theta)\\) or \\(f(x | \\theta)\\). The conditional notation is more general because it implies the parameter \\(\\theta\\) may have its own distribution, yielding a joint density \\(f(x, \\theta) = f(x | \\theta) f(\\theta)\\). Similarly, the corresponding cumulative distribution function is written \\(F(x; \\theta)\\) or \\(F(x | \\theta)\\).\nNote that parametrisations are generally not unique, as any one-to-one transformation of \\(\\theta\\) yields an equivalent index of the same distribution family. For most commonly used distribution families there exist several standard parametrisations. We usually prefer those whose parameters that can be interpreted easily (e.g. in terms of moments) or that help to simplify calculations.\nIf distinct parameters correspond to distinct distributions they are called identifiable. This is an important property as it allows parameters to be estimated from data. Specifically, if parameters are identifiable then \\(P(\\theta_1) = P(\\theta_2)\\) implies \\(\\theta_1 = \\theta_2\\), and conversely if \\(P(\\theta_1) \\neq P(\\theta_2)\\) then \\(\\theta_1 \\neq \\theta_2\\). If parameters and distributions are unique within a neighbourhood \\(\\theta_0+\\epsilon\\) relative to a reference value \\(\\theta_0\\), rather than globally, they are locally identifiable at \\(\\theta_0\\).\nAn important class of distribution families are exponential families discussed in Chapter 7.\n\n\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Academic Press.\n\n\nWhittle, P. 2000. Probability via Expectation. 3rd ed. Springer. https://doi.org/10.1007/978-1-4612-0509-8.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#footnotes",
    "href": "02-probability.html#footnotes",
    "title": "2  Probability",
    "section": "",
    "text": "This notation is common in statistical machine learning and multivariate statistics, see for example Mardia, Kent, and Bibby (1979). An alternative convention uses uppercase letters for random variables and lowercase for outcomes, but that convention is problematic for multivariate objects (random vectors and random matrices) and is also ill-suited in Bayesian statistics where parameters are modelled as random variables.↩︎\nIn our notational conventions, a scalar \\(x\\) is written in lower case plain font, a vector \\(\\boldsymbol x\\) is written in lower case bold font, a matrix \\(\\boldsymbol X\\) in upper case bold font.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "03-transformations.html",
    "href": "03-transformations.html",
    "title": "3  Transformations and convolution",
    "section": "",
    "text": "3.1 Affine or location-scale transformation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations and convolution</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#sec-affinetrans",
    "href": "03-transformations.html#sec-affinetrans",
    "title": "3  Transformations and convolution",
    "section": "",
    "text": "Transformation rule\nSuppose \\(x\\) is a scalar. The variable \\[\ny= a + b x\n\\] is a location-scale transformation or affine transformation of \\(x\\), where \\(a\\) plays the role of the location parameter and \\(b\\) is the scale parameter. For \\(a=0\\) this is a linear transformation.\nIf \\(b\\neq 0\\) then the transformation is invertible, with back-transformation \\[x = (y-a)/b\\] Invertible transformations provide a one-to-one map between \\(x\\) and \\(y\\).\nFor a vector \\(\\boldsymbol x\\) of dimension \\(d\\) the location-scale transformation is \\[\n\\boldsymbol y= \\boldsymbol a+ \\boldsymbol B\\boldsymbol x\n\\] where \\(\\boldsymbol a\\) (a \\(m \\times 1\\) vector) is the location parameter and \\(\\boldsymbol B\\) (a \\(m \\times d\\) matrix) the scale parameter. For \\(\\boldsymbol a=0\\) this is a linear transformation.\nFor \\(m=d\\) (square \\(\\boldsymbol B\\)) and \\(\\det(\\boldsymbol B) \\neq 0\\) the affine transformation is invertible with back-transformation \\[\\boldsymbol x= \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol a)\\]\n\n\nProbability mass function\nIf \\(x \\sim F_x\\) is a discrete scalar random variable with pmf \\(f_{x}(x)\\) and assuming an invertible transformation \\(y(x)= a + b x\\) the pmf \\(f_{y}(y)\\) for the discrete scalar random variable \\(y\\) is given by \\[\nf_{y}(y)= f_{x} \\left( \\frac{y-a}{b}\\right)\n\\]\nLikewise, if \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) is a discrete random vector with pmf \\(f_{\\boldsymbol x}(\\boldsymbol x)\\) and assuming an invertible transformation \\(\\boldsymbol y(\\boldsymbol x) = \\boldsymbol a+ \\boldsymbol B\\boldsymbol x\\) the pmf \\(f_{\\boldsymbol y}(\\boldsymbol y)\\) for the discrete random vector \\(\\boldsymbol y\\) is given by \\[\nf_{\\boldsymbol y}(\\boldsymbol y)= f_{\\boldsymbol x} \\left( \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol a)\\right)\n\\]\n\n\nDensity\nIf \\(x \\sim F_x\\) is a continuous scalar random variable with pdf \\(f_{x}(x)\\) and assuming an invertible transformation \\(y(x)= a + b x\\) the pdf \\(f_{y}(y)\\) for the continuous random scalar \\(y\\) is given by \\[\nf_{y}(y)=|b|^{-1} f_{x} \\left( \\frac{y-a}{b}\\right)\n\\] where \\(|b|\\) is the absolute value of \\(b\\). The transformation of the corresponding differential element is \\[\ndy = |b| \\, dx\n\\]\nLikewise, if \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) is a continuous random vector with pdf \\(f_{\\boldsymbol x}(\\boldsymbol x)\\) and assuming an invertible transformation \\(\\boldsymbol y(\\boldsymbol x) = \\boldsymbol a+ \\boldsymbol B\\boldsymbol x\\) the pdf \\(f_{\\boldsymbol y}(\\boldsymbol y)\\) for the continuous random vector \\(\\boldsymbol y\\) is given by \\[\nf_{\\boldsymbol y}(\\boldsymbol y)=|\\det\\left(\\boldsymbol B\\right)|^{-1} f_{\\boldsymbol x} \\left( \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol a)\\right)\n\\] where \\(|\\det(\\boldsymbol B)|\\) is the absolute value of the determinant \\(\\det(\\boldsymbol B)\\). The transformation of the corresponding infinitesimal volume element is \\[\nd{\\boldsymbol y} = |\\det\\left(\\boldsymbol B\\right)|\\, d{\\boldsymbol x}\n\\]\n\n\nMoments\nThe transformed random variable \\(y \\sim F_y\\) has mean \\[\\operatorname{E}(y) = a + b \\mu_x\\] and variance \\[\\operatorname{Var}(y) = b^2 \\sigma^2_x\\] where \\(\\operatorname{E}(x) = \\mu_x\\) and \\(\\operatorname{Var}(x) = \\sigma^2_x\\) are the mean and variance of the original variable \\(x\\).\nThe mean and variance of the transformed random vector \\(\\boldsymbol y\\sim F_{\\boldsymbol y}\\) is \\[\\operatorname{E}(\\boldsymbol y)=\\boldsymbol a+ \\boldsymbol B\\,\\boldsymbol \\mu_{\\boldsymbol x}\\] and \\[\\operatorname{Var}(\\boldsymbol y)= \\boldsymbol B\\,\\boldsymbol \\Sigma_{\\boldsymbol x} \\,\\boldsymbol B^T\\] where \\(\\operatorname{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\) and \\(\\operatorname{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\) are the mean and variance of the original random vector \\(\\boldsymbol x\\).\n\n\nImportance of affine transformations\nThe constants \\(\\boldsymbol a\\) and \\(\\boldsymbol B\\) (or \\(a\\) and \\(b\\) in the univariate case) are the parameters of the location-scale family \\(F_{\\boldsymbol y}(\\boldsymbol a, \\boldsymbol B)\\) created from \\(F_{\\boldsymbol x}\\). Many important distributions are location-scale families such as the normal distribution (cf. Section 5.3 and Section 6.3) and the location-scale \\(t\\)-distribution (Section 5.6 and Section 6.6). Furthermore, key procedures in multivariate statistics such as orthogonal transformations (including PCA) or whitening transformations (e.g. the Mahalanobis transformation) are affine transformations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations and convolution</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#sec-geninvtrans",
    "href": "03-transformations.html#sec-geninvtrans",
    "title": "3  Transformations and convolution",
    "section": "3.2 General invertible transformation",
    "text": "3.2 General invertible transformation\n\nTransformation rule\nAs above we assume \\(x\\) is a scalar and \\(\\boldsymbol x\\) is a vector and consider the general invertible transformation.\nFor a scalar variable the transformation is specified by \\(y(x) = h(x)\\) and the back-transformation by \\(x(y) = h^{-1}(y)\\). For a vector this becomes \\(\\boldsymbol y(\\boldsymbol x) = \\boldsymbol h(\\boldsymbol x)\\) with back-transformation \\(\\boldsymbol x(\\boldsymbol y) = \\boldsymbol h^{-1}(\\boldsymbol y)\\). The functions \\(h(x)\\) and \\(\\boldsymbol h(\\boldsymbol x)\\) are assumed to be invertible.\n\n\nProbability mass function\nIf \\(x \\sim F_x\\) is a discrete scalar random variable with pmf \\(f_{x}(x)\\) then the pmf \\(f_y(y)\\) of the transformed discrete scalar random variable \\(y(x)\\) is given by \\[\nf_y(y) = f_x(x(y))\n\\]\nLikewise, for a discrete random vector \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) with pmf \\(f_{\\boldsymbol x}(\\boldsymbol x)\\) the pmf \\(f_{\\boldsymbol y}(\\boldsymbol y)\\) for the discrete random vector \\(\\boldsymbol y(\\boldsymbol x)\\) is obtained by \\[\nf_{\\boldsymbol y}(\\boldsymbol y) =  f_{\\boldsymbol x}\\left( \\boldsymbol x(\\boldsymbol y) \\right)\n\\]\n\n\nDensity\nIf \\(x \\sim F_x\\) is a continuous scalar random variable with pdf \\(f_{x}(x)\\) the pdf \\(f_y(y)\\) of the transformed continuous scalar random variable \\(y(x)\\) is given by \\[\nf_y(y) =\\left| D x(y) \\right|\\, f_x(x(y))\n\\] where \\(D x(y)\\) is the derivative of the inverse transformation \\(x(y)\\). The transformation of the differential element is \\[\ndy = \\left| D y(x) \\right| \\, dx\n\\] Note that \\(| D x(y)| = | D y(x)|^{-1}\\rvert_{x = x(y)}\\).\nLikewise, for a continuous random vector \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) with pdf \\(f_{\\boldsymbol x}(\\boldsymbol x)\\) the pdf \\(f_{\\boldsymbol y}(\\boldsymbol y)\\) for the continuous random vector \\(\\boldsymbol y(\\boldsymbol x)\\) is obtained by \\[\nf_{\\boldsymbol y}(\\boldsymbol y) = |\\det\\left( D\\boldsymbol x(\\boldsymbol y) \\right)| \\,\\,  f_{\\boldsymbol x}\\left( \\boldsymbol x(\\boldsymbol y) \\right)\n\\] where \\(D\\boldsymbol x(\\boldsymbol y)\\) is the Jacobian matrix of the inverse transformation \\(\\boldsymbol x(\\boldsymbol y)\\). The transformation of the infinitesimal volume element is \\[\nd{\\boldsymbol y} = |\\det\\left( D\\boldsymbol y(\\boldsymbol x) \\right)|\\, d{\\boldsymbol x}\n\\] Note that \\(|\\det\\left( D\\boldsymbol x(\\boldsymbol y) \\right)| = |\\det\\left( D\\boldsymbol y(\\boldsymbol x) \\right)|^{-1} \\rvert_{\\boldsymbol x= \\boldsymbol x(\\boldsymbol y)}\\).\n\n\nMoments\nThe mean and variance of the transformed random variable can typically only be approximated. Assume that \\(\\operatorname{E}(x) = \\mu_x\\) and \\(\\operatorname{Var}(x) = \\sigma^2_x\\) are the mean and variance of the original random variable \\(x\\) and \\(\\operatorname{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\) and \\(\\operatorname{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\) are the mean and variance of the original random vector \\(\\boldsymbol x\\). In the delta method the transformation \\(y(x)\\) resp. \\(\\boldsymbol y(\\boldsymbol x)\\) is linearised around the mean \\(\\mu_x\\) respectively \\(\\boldsymbol \\mu_{\\boldsymbol x}\\) and the mean and variance resulting from the linear transformation is reported.\nSpecifically, the linear approximation for the scalar-valued function is \\[\ny(x) \\approx y\\left(\\mu_x\\right) + D y\\left(\\mu_x\\right)\\, \\left(x-\\mu_x\\right)\n\\] where \\(D y(x) = y'(x)\\) is the first derivative of the transformation \\(y(x)\\) and \\(D y\\left(\\mu_x\\right)\\) is the first derivative evaluated at the mean \\(\\mu_x\\), and for the vector-valued function \\[\n\\boldsymbol y(\\boldsymbol x) \\approx \\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right) + D \\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right) \\, \\left(\\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x}\\right)\n\\] where \\(D \\boldsymbol y(\\boldsymbol x)\\) is the Jacobian matrix (vector derivative) for the transformation \\(\\boldsymbol y(\\boldsymbol x)\\) and \\(D \\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right)\\) is the Jacobian matrix evaluated at the mean \\(\\boldsymbol \\mu_{\\boldsymbol x}\\).\nIn the univariate case the delta method yields as approximation for the mean and variance of the transformed random variable \\(y\\) \\[\n\\operatorname{E}(y) \\approx y\\left(\\mu_x\\right)\n\\] and \\[\n\\operatorname{Var}(y)\\approx \\left(D y\\left(\\mu_x\\right)\\right)^2 \\, \\sigma^2_x  \n\\]\nFor the vector random variable \\(\\boldsymbol y\\) the delta method yields \\[\\operatorname{E}(\\boldsymbol y)\\approx\\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right)\\] and \\[\n\\operatorname{Var}(\\boldsymbol y)\\approx D \\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right) \\, \\boldsymbol \\Sigma_{\\boldsymbol x} \\, D\\boldsymbol y\\left(\\boldsymbol \\mu_{\\boldsymbol x}\\right)^T\n\\]\n\n\nInvertible affine transformation as special case\nThe invertible affine transformation (Section 3.1) is special case of the general invertible transformation.\nAssuming \\(y(x) = a + b x\\), with \\(x(y) = (y-a)/b\\), \\(D y(x) = b\\) and \\(D x(y) = b^{-1}\\), recovers the univariate location-scale transformation.\nLikewise, assuming \\(\\boldsymbol y(\\boldsymbol x) = \\boldsymbol a+ \\boldsymbol B\\boldsymbol x\\), with \\(\\boldsymbol x(\\boldsymbol y) = \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol a)\\), \\(D\\boldsymbol y(\\boldsymbol x) = \\boldsymbol B\\) and \\(D\\boldsymbol x(\\boldsymbol y) = \\boldsymbol B^{-1}\\), recovers the multivariate location-scale transformation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations and convolution</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#sec-convolution",
    "href": "03-transformations.html#sec-convolution",
    "title": "3  Transformations and convolution",
    "section": "3.3 Convolution of random variables",
    "text": "3.3 Convolution of random variables\n\nSum of independent random variables\nSuppose we have a sum of \\(n\\) independent scalar random variables. \\[\ny = x_1 + x_2 + \\ldots + x_n\n\\] where each \\(x_i \\sim F_{x_i}\\) has its own distribution and corresponding pdmf \\(f_{x_i}(x)\\). The corresponding means are \\(\\operatorname{E}(x_i) = \\mu_i\\) and the variances are \\(\\operatorname{Var}(x_i) = \\sigma^2_i\\). As the \\(x_i\\) are independent, and therefore uncorrelated, the covariances \\(\\operatorname{Cov}(x_i, x_j)=0\\) vanish for \\(i \\neq j\\).\nWith \\(\\boldsymbol x= (x_1, \\ldots, x_n)^T\\) and \\(\\mathbf 1_n = (1, 1, \\ldots, 1)^T\\) the relationship between \\(y\\) and \\(\\boldsymbol x\\) can be written as the linear transformation \\[\ny= \\mathbf 1_n^T \\boldsymbol x\n\\] As \\(y\\) is a scalar and \\(\\boldsymbol x\\) a vector the transformation from \\(\\boldsymbol x\\) to \\(y\\) is not invertible.\n\n\nMoments\nWith \\(\\operatorname{E}(\\boldsymbol x) = \\boldsymbol \\mu\\) and \\(\\operatorname{Var}(\\boldsymbol x) = \\operatorname{Diag}(\\sigma^2_1, \\ldots, \\sigma^2_n)\\) the mean of the random variable \\(y\\) equals \\[\n\\operatorname{E}(y) = \\mathbf 1_n^T \\boldsymbol \\mu= \\sum_{i=1}^n \\mu_i\n\\] and the variance of \\(y\\) is \\[\n\\operatorname{Var}(y) = \\mathbf 1_n^T \\, \\operatorname{Var}(\\boldsymbol x) \\,  \\mathbf 1_n  =   \\sum_{i=1}^n \\sigma^2_i\n\\]\n(cf. Section 3.1). Thus both the mean and variance of \\(y\\) are simply the sums of the individual means and variances (note that for the variance this only holds because the individual variables are uncorrelated).\n\n\nConvolution\nThe pdmf \\(f_y(y)\\) for \\(y\\) is obtained by repeatedly convolving (denoted by the asterisk \\(\\ast\\) operator) the pdmfs of the \\(x_i\\): \\[\nf_y(y) = \\left(f_{x_1} \\ast f_{x_2} \\ast \\ldots f_{x_n}\\right)(y)\n\\]\nThe convolution of two functions is defined as (continuous case) \\[\n\\left(f_{x_1}\\ast f_{x_2}\\right)(y)=\\int_x f_{x_1}(x)\\, f_{x_2}(y-x) dx\n\\] and (discrete case) \\[\n\\left(f_{x_1}\\ast f_{x_2}\\right)(y)=\\sum_x f_{x_1}(x)\\, f_{x_2}(y-x)\n\\] Convolution is commutative and associative so so you may convolve multiple pdmfs in any order or grouping. Furthermore, the convolution of pdmfs yields another pdmf, i.e. the resulting function integrates to one.\nMany commonly used random variables can be viewed as the outcome of convolutions. For example, the sum of Bernoulli variables yields a binomial random variable and the sum of normal variables yields another normal random variable.\nSee also (Wikipedia): list of convolutions of probability distributions.\n\n\nCentral limit theorem\nThe central limit theorem, first postulated by Abraham de Moivre (1667–1754) and later proved by Pierre-Simon Laplace (1749–1827) asserts that the distribution of the sum of \\(n\\) independent and identically distributed random variables with finite mean and finite variance converges in the limit of large \\(n\\) to a normal distribution (Section 5.3), even if the individual random variables are not themselves normal. In other words, it asserts that for large \\(n\\) the convolution of \\(n\\) identical distributions with finite first two moments converges to a normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations and convolution</span>"
    ]
  },
  {
    "objectID": "04-evaluation.html",
    "href": "04-evaluation.html",
    "title": "4  Evaluation",
    "section": "",
    "text": "4.1 Loss functions",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "04-evaluation.html#sec-lossfunc",
    "href": "04-evaluation.html#sec-lossfunc",
    "title": "4  Evaluation",
    "section": "",
    "text": "Loss function\nA loss or cost function \\(L(x, a)\\) evaluates a prediction \\(a\\), for example a parameter or a probability distribution, on the basis of an observed outcome \\(x\\), and returns a numerical score.\nA loss function measures, informally, the error between \\(x\\) and \\(a\\). During optimisation the prediction \\(a\\) is varied and the aim is minimisation of the error (hence a loss function has negative orientation, smaller is better).\nA utility or reward function is a loss function with a reversed sign (hence it has positive orientation, larger is better).\n\n\nRisk function\nThe risk of \\(a\\) under the distribution \\(Q\\) for \\(x\\) is defined as the expected loss \\[\nR(Q, a) = \\operatorname{E}_Q(L(x, a))\n\\]\nThe risk is mixture preserving in Q meaning that \\[\nR( Q_{\\lambda}, a ) = (1-\\lambda) R(Q_0, a) + \\lambda R(Q_1, a)\n\\] for the mixture \\(Q_{\\lambda}=(1-\\lambda) Q_0 + \\lambda Q_1\\) with \\(0 &lt; \\lambda &lt; 1\\) and \\(Q_0 \\neq Q_1\\). This follows from the linearity of expectation.\nThe risk of \\(a\\) under the empirical distribution \\(\\hat{Q}_n\\) obtained from observations \\(x_1, \\ldots, x_n\\) is the empirical risk \\[\nR(\\hat{Q}_n, a) =\n\\frac{1}{n} \\sum_{i=1}^{n} L(x_i, a)\n\\] where the expectation is replaced by the sample average.\n\n\nMinimising risk\nMinimising \\(R(Q, a)\\) with regard to \\(a\\) finds optimal predictions\n\\[\na^{\\ast} = \\underset{a}{\\arg \\min}\\, R(Q, a)\n\\] with associated minimum risk \\(R(Q, a^{\\ast})\\).\nDepending on the choice of underlying loss \\(L(x, a)\\) minimising the risk provides a very general optimisation-based way to identify distributional features of the distribution \\(Q\\) and to obtain parameter estimates.\n\n\nEquivalent loss functions\nAdding a positive scaling factor \\(c &gt; 0\\) or an additive term \\(k(x)\\) to a loss function generates a family of equivalent loss functions \\[\nL^{\\text{equiv}}(x, a) = c L(x, a) + k(x)\n\\] with associated risk \\[\nR^{\\text{equiv}}(Q, a) = c R(Q, a) + \\operatorname{E}_Q(k(x))\n\\] Equivalent losses yield the same risk minimiser \\({\\arg \\min}_a\\, R(Q, a)\\) and the same loss minimiser \\({\\arg \\min}_a\\, L(x, a)\\) for fixed \\(x\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "04-evaluation.html#common-loss-functions",
    "href": "04-evaluation.html#common-loss-functions",
    "title": "4  Evaluation",
    "section": "4.2 Common loss functions",
    "text": "4.2 Common loss functions\n\nSquared loss\nThe squared loss or squared error \\[\nL_\\text{sq}(x,a) = (x-a)^2\n\\] is one of the most commonly used loss functions. The corresponding risk is the mean squared loss or mean squared error (MSE) \\[\nR_{\\text{sq}}(Q, a) = \\operatorname{E}_Q((x-a)^2)\n\\] which is minimised at the mean \\(a^{\\ast} = \\operatorname{E}(Q)\\). This follows from \\(R_{\\text{sq}}(Q, a) = \\operatorname{E}_Q(x^2) - 2 a \\operatorname{E}_Q(x) + a^2\\) and \\(dR_{\\text{sq}}(Q, a)/da = - 2 \\operatorname{E}_Q(x) + 2 a\\). The minimum risk \\(R_{\\text{sq}}(a^{\\ast}) = \\operatorname{Var}(Q)\\) equals the variance.\n\n\n0-1 loss\nThe 0-1 loss function can be written as \\[\nL_{\\text{01}}(x, a) =\n\\begin{cases}\n-[x = a] & \\text{discrete case} \\\\\n-\\delta(x-a) & \\text{continuous case} \\\\\n\\end{cases}\n\\] employing the indicator function and Dirac delta function, respectively. The corresponding risk assuming \\(x \\sim Q\\) and pdmf \\(q(x)\\) is \\[\nR_{\\text{01}}(Q, a) = -q(a)\n\\] which is minimised at the mode of the pdmf.\n\n\nAsymmetric loss\nThe asymmetric loss can be defined as \\[\nL_{\\text{asym}}(x, a; \\tau) =\n\\begin{cases}\n2 \\tau (x-a) & \\text{for $x\\geq a$} \\\\\n2 (1-\\tau) (a-x)  & \\text{for $x &lt; a$} \\\\\n\\end{cases}\n\\] and the corresponding risk is minimised at the quantile \\(x_{\\tau}\\).\n\n\nAbsolute loss\nFor \\(\\tau=1/2\\) it reduces to the absolute loss \\[\nL_{\\text{abs}}(x, a) = | x - a|\n\\] whose corresponding risk is minimised at the median \\(x_{1/2}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "04-evaluation.html#sec-scoringrules",
    "href": "04-evaluation.html#sec-scoringrules",
    "title": "4  Evaluation",
    "section": "4.3 Scoring rules",
    "text": "4.3 Scoring rules\n\nProper scoring rules\nA scoring rule \\(S(x, P)\\) is special type of loss function1 that assesses the probabilistic forecast \\(P\\) by assigning a numerical score based on \\(P\\) and the observed outcome \\(x\\).\nThe risk of \\(P\\) under \\(Q\\) is the expected score \\[\nR(Q, P) = \\operatorname{E}_{Q}\\left(S(x, P)\\right)\n\\]\nFor a proper scoring rule, the risk \\(R(Q, P)\\) is smallest when the quoted model \\(P\\) matches the true model \\(Q\\). The minimal risk, achieved for \\(P=Q\\), leads to the properness inequality \\[\nR(Q, P) \\geq R(Q, Q)\n\\] For a strictly proper scoring rule, the minimum risk is realised only for the true model, so equality holds exclusively for \\(P = Q\\).\n\n\nScore entropy\nThe minimum risk associated with a proper scoring rule is called the score entropy \\(R(Q) = R(Q,Q)\\). With it the properness inequality becomes \\[\nR(Q, P) \\geq R(Q)\n\\]\nFor a proper scoring rule, the score entropy \\(R(Q)\\) is concave in \\(Q\\). For a strictly proper scoring rule, the score entropy \\(R(Q)\\) is strictly concave. This means that \\[\nR( Q_{\\lambda}) \\geq (1-\\lambda) R(Q_0) + \\lambda R(Q_1)\n\\] for the mixture \\(Q_{\\lambda}=(1-\\lambda) Q_0 + \\lambda Q_1\\) with \\(0 &lt; \\lambda &lt; 1\\) and \\(Q_0 \\neq Q_1\\) (for strict concavity replace \\(\\geq\\) by \\(&gt;\\)).\nThis follows from the fact that risk \\(R(Q, P)\\) is mixture-preserving in \\(Q\\). Hence, \\(R(Q_{\\lambda}, Q_{\\lambda} ) = (1-\\lambda) R(Q_0, Q_{\\lambda} ) + \\lambda R(Q_1, Q_{\\lambda} )\\). Applying properness \\(R(Q_i, Q_{\\lambda} ) \\geq R(Q_i)\\) with \\(i \\in \\{0,1\\}\\) yields concavity.\n\n\nScore divergence\nThe score divergence between the distributions \\(Q\\) and \\(P\\) equals the excess risk given by \\[\nD(Q, P) = R(Q, P) - R(Q)\n\\] For a proper scoring rule, the divergence \\(D(Q, P)  \\geq 0\\) is always non-negative and with \\(D(Q, P)=0\\) if \\(P=Q\\). For a strictly proper scoring rule \\(D(Q, P)=0\\) only when \\(P=Q\\).\nThe score divergence \\(D(Q, P)\\) is convex in \\(Q\\) for fixed \\(P\\) for a proper scoring rule. It is strictly convex in \\(Q\\) for a strictly proper scoring rule. The convexity of \\(D(Q,P)\\) in \\(Q\\) derives from the concavity of \\(R(Q)\\) and the fact that \\(R(Q, P)\\) is mixture-preserving in \\(Q\\).\n\n\nEquivalent scoring rules\nEquivalent scoring rules \\[\nS^{\\text{equiv}}(x, P) = c S(x, P) + k(x)\n\\] have associated equivalent score divergences \\[\nD^{\\text{equiv}}(Q, P) = R^{\\text{equiv}}(Q, P) - R^{\\text{equiv}}(Q) = c D(Q, P)\n\\] Thus, (strictly) proper scoring rules remain (strictly) proper under equivalence transformations. Furthermore, for \\(c=1\\) equivalent scoring rules are strongly equivalent as their divergences are identical.\n\n\nCorrespondence with Bregman divergences\nProper scoring rules and their associated divergences correspond to Bregman divergences well-known in optimisation and machine learning.\nSpecifically, the negative score entropy acts as the convex potential \\(\\Phi(Q) = -R(Q)\\) generating the score (Bregman) divergence \\(D(Q,P)\\) via \\[\nD(Q,P) = \\Phi(Q) -\\Phi(P) - \\langle \\nabla \\Phi(P), Q-P \\rangle\n\\]\n\n\nFurther properties\nProper scoring rules are highly useful because they enable identification of an underlying distribution and its parameters via risk minimisation or minimisation of the associated divergences. These approaches generalise conventional likelihood and Bayesian methods that are based on the logarithmic scoring rule (Section 4.4.1).\nProper scoring rules also enjoy several additional properties not mentioned above. For example, various decompositions exist for their risk, and the score divergence satisfies a generalised Pythagorean theorem.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "04-evaluation.html#common-scoring-rules",
    "href": "04-evaluation.html#common-scoring-rules",
    "title": "4  Evaluation",
    "section": "4.4 Common scoring rules",
    "text": "4.4 Common scoring rules\n\nLogarithmic scoring rule\nThe most important scoring rule is the logarithmic scoring rule or log-loss \\[\nS_{\\text{log}}(x, P) = - \\log p(x)\n\\]\nThe risk of \\(P\\) under \\(Q\\) based on the log-loss is the mean log-loss \\[\nR_{\\text{log}}(Q, P) = - \\operatorname{E}_{Q} \\log p(x) = H(Q, P)\n\\] which is uniquely minimised for \\(P=Q\\). Thus, the log-loss is strictly proper. Moreover, the log-loss is noted as the only local strictly proper scoring rule, as it solely depends on the value of the pdmf at the observed outcome \\(x\\), and not on any other features of the distribution \\(P\\).\nThe mean log-loss is also known as cross-entropy denoted by \\(H(Q, P)\\).\nThe minimum risk (score entropy) equals the information entropy denoted by \\(H(Q):\\) \\[\nR_{\\text{log}}(Q) = -\\operatorname{E}_{Q} \\log q(x) = H(Q)\n\\] The properness inequality \\(H(Q, P) \\geq H(Q)\\), with equality exclusively for \\(P=Q\\) and relating cross-entropy and information entropy, is known as Gibbs’ inequality.\nThe score divergence induced by the log-loss is the Kullback-Leibler (KL) divergence \\[\n\\begin{split}\nD_{\\text{KL}}(Q,P) &=  R_{\\text{log}}(Q,P) -R_{\\text{log}}(Q)) \\\\\n          &= H(Q, P) - H(Q) \\\\\n          &= \\operatorname{E}_{Q} \\log\\left(\\frac{q(x)}{p(x)}\\right)\\\\\n\\end{split}\n\\] The KL divergence obeys the data processing inequality, i.e. applying a transformation to the underlying random variables cannot increase the KL divergence \\(D_{\\text{KL}}(Q,P)\\) between \\(Q\\) and \\(P\\). This property also holds for all \\(f\\)-divergences (of which the KL divergence is a principal example), but is notably not satisfied by divergences of other proper scoring rules (and thus other Bregman divergences).\nFurthermore, the KL divergence is the only divergence induced by proper scoring rules (and thus the only Bregman divergence), as well as the only \\(f\\)-divergence, that is invariant against general coordinate transformations. Coordinate transformations can be viewed as a special case of data processing, and for \\(D_{\\text{KL}}(Q,P)\\) the data-processing inequality under general invertible transformations becomes an identity.\nThe empirical risk of a distribution family \\(P(\\theta)\\) based on the log-loss is proportional to the log-likelihood function \\[\n\\begin{split}\nR_{\\text{log}}(\\hat{Q}_n, P(\\theta)) &= - \\frac{1}{n} \\sum_{i=1}^n \\log p(x_i | \\theta) \\\\\n                &= - \\frac{1}{n} \\ell_n(\\theta)\\\\\n\\end{split}\n\\] Minimising the empirical risk is thus equivalent to maximising the log-likelihood \\(\\ell_n(\\theta)\\).\nSimilarly, minimising the KL divergence \\(D_{\\text{KL}}(\\hat{Q}_n,P(\\theta))\\) with regard to \\(\\theta\\) is equivalent to minimising the empirical risk and hence to maximum likelihood.\n\n\nBrier or quadratic scoring rule\nThe Brier scoring rule, also known as quadratic scoring rule, evaluates a probabilistic categorical forecast \\(P\\) with corresponding class probabilities \\(p_1, \\ldots, p_K\\) given a realisation \\(\\boldsymbol x\\) from the categorical distribution \\(Q\\) with class probabilities \\(q_1, \\ldots, q_K\\). It can be written as \\[\n\\begin{split}\nS_{\\text{Brier}}(\\boldsymbol x, P) &= \\sum_{y=1}^K \\left(x_y -p_y\\right)^2 \\\\\n&= 1 -  2 \\sum_{y=1}^K x_y p_y +   \\sum_{y=1}^K p_y^2\\\\\n&= 1 -  2  p_k +   \\sum_{y=1}^K p_y^2\\\\\n\\end{split}\n\\] The indicator vector \\(\\boldsymbol x= (x_1, \\ldots, x_K)^T = (0, 0, \\ldots, 1, \\ldots, 0)^T\\) contains zeros everywhere except for a single element \\(x_k=1\\). Unlike the log-loss, the Brier score is not local as the pmf for \\(P\\) is evaluated across all \\(K\\) classes, not just at the realised class \\(k\\).\nThe corresponding risk is \\[\n\\begin{split}\nR_{\\text{Brier}}(Q,P) &= \\operatorname{E}_Q(S(\\boldsymbol x, P)) \\\\\n       &= 1 -2 \\sum_{y=1}^K q_y p_y +\\sum_{y=1}^K p_y^2\\\\\n\\end{split}\n\\] which is uniquely minimised for \\(P=Q\\). Thus, the Brier score is strictly proper.\nThe minimum risk (score entropy) is \\[\nR_{\\text{Brier}}(Q) = 1 -  \\sum_{y=1}^K q_y^2\n\\]\nThe divergence induced by the Brier score is the squared Euclidean distance between the two pmfs: \\[\n\\begin{split}\nD_{\\text{Brier}}(Q, P) &= R_{\\text{Brier}}(Q, P) - R_{\\text{Brier}}(Q) \\\\\n        & = \\sum_{y=1}^K  \\left(q_y - p_y\\right)^2\\\\\n\\end{split}\n\\]\n\n\nProper but not strictly proper scoring rules\nAn example of a proper, but not strictly proper, scoring rule is the squared error relative to the mean of the quoted model \\(P\\): \\[\nS_{\\text{sq}}(x, P) = (x- \\operatorname{E}(P))^2\n\\]\nThe corresponding risk is \\[\n\\begin{split}\nR_{\\text{sq}}(Q, P) &= \\operatorname{E}_Q\\left( (x- \\operatorname{E}(P))^2 \\right)\\\\\n       & = (\\operatorname{E}(Q)-\\operatorname{E}(P))^2 + \\operatorname{Var}(Q)\\\\\n\\end{split}\n\\] which is minimised at \\(P=Q\\) but also at any distribution \\(P\\) with the same mean as \\(Q\\).\nThe minimum risk (score entropy) is the variance \\[\nR_{\\text{sq}}(Q) = \\operatorname{Var}(Q)\n\\]\nThe score divergence is the squared distance between the two means \\[\n\\begin{split}\nD_{\\text{sq}}(Q, P) &= R_{\\text{sq}}(Q, P) -R_{\\text{sq}}(Q)) \\\\\n        &= (\\operatorname{E}(Q)-\\operatorname{E}(P))^2\\\\\n\\end{split}\n\\] which vanishes at \\(P=Q\\) but also at any \\(P\\) with \\(\\operatorname{E}(P)=\\operatorname{E}(Q)\\).\nThe Dawid-Sebastiani scoring rule is a related scoring rule given by \\[\nS_{\\text{DS}}\\left(x, P\\right) =  \\log \\operatorname{Var}(P) + \\frac{(x-\\operatorname{E}(P))^2}{\\operatorname{Var}(P)}\n\\] It is equivalent to the log-loss applied to a normal model \\(P\\).\nThe corresponding risk is \\[\n\\begin{split}\nR_{\\text{DS}}(Q, P) &= \\log \\operatorname{Var}(P)  + \\frac{(\\operatorname{E}(Q)-\\operatorname{E}(P))^2}{\\operatorname{Var}(P)} + \\frac{\\operatorname{Var}(Q)}{\\operatorname{Var}(P)}\\\\\n\\end{split}\n\\] which is minimised at \\(P=Q\\) but also at any distribution \\(P\\) with \\(\\operatorname{E}(P)=\\operatorname{E}(Q)\\) and \\(\\operatorname{Var}(P)=\\operatorname{Var}(Q)\\).\nThe minimum risk (score entropy) is \\[\nR_{\\text{DS}}(Q) = \\log \\operatorname{Var}(Q) +1\n\\]\nThe score divergence is \\[\n\\begin{split}\nD_{\\text{DS}}(Q, P) &= R_{\\text{DS}}(Q, P) - R_{\\text{DS}}(Q) \\\\\n        &= \\frac{(\\operatorname{E}(Q)-\\operatorname{E}(P))^2}{\\operatorname{Var}(P)} +\\frac{\\operatorname{Var}(Q)}{\\operatorname{Var}(P)}  - \\log\\left( \\frac{\\operatorname{Var}(Q}{\\operatorname{Var}(P} \\right)  -1 \\\\\n\\end{split}\n\\] which vanishes at \\(P=Q\\) but also at any \\(P\\) for which \\(\\operatorname{E}(P)=\\operatorname{E}(Q)\\) and \\(\\operatorname{Var}(P)=\\operatorname{Var}(Q)\\).\n\n\nOther strictly proper scoring rules\nOther useful strictly proper scoring rules include:\n\nthe continuous ranked probability score (CRPS),\nthe energy score (multivariate CRPS), and\nthe Hyvärinen scoring rule.\n\nSee also (Wikipedia): scoring rule.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "04-evaluation.html#footnotes",
    "href": "04-evaluation.html#footnotes",
    "title": "4  Evaluation",
    "section": "",
    "text": "Treating scoring rules as loss functions implies a negative orientation. However, some authors adopt the opposite convention and treat scoring rules as positively oriented utility functions.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "05-univariate.html",
    "href": "05-univariate.html",
    "title": "5  Univariate distributions",
    "section": "",
    "text": "5.1 Binomial distribution\nThe binomial distribution \\(\\operatorname{Bin}(n, \\theta)\\) is a discrete distribution counting binary outcomes.\nThe Bernoulli distribution \\(\\operatorname{Ber}(\\theta)\\) is a special case of the binomial distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "05-univariate.html#sec-binomdist",
    "href": "05-univariate.html#sec-binomdist",
    "title": "5  Univariate distributions",
    "section": "",
    "text": "Standard parametrisation\nA binomial random variable \\(x\\) describes the number of successful outcomes in \\(n\\) identical and independent trials. We write \\[\nx \\sim \\operatorname{Bin}(n, \\theta)\\,\n\\] where \\(\\theta \\in [0,1]\\) is the probability of a positive outcome (“success”) in a single trial. Conversely, \\(1-\\theta \\in [0,1]\\) is the complementary probability (“failure”). The support is \\(x \\in \\{ 0, 1, 2, \\ldots, n\\}\\) which notably depends on \\(n\\).\n\n\n\n\n\n\nFigure 5.1: Binomial urn model.\n\n\n\nThe binomial distribution is often motivated by a coin tossing experiment where \\(\\theta\\) is the probability of “head” when flipping the coin and \\(x\\) is the number of observed “heads” among \\(n\\) throws. Another common interpretation is that of an urn model where \\(n\\) items are distributed into two bins (Figure 5.1). Here \\(\\theta\\) is the probability to put an item into one urn (representing “success”, “head”) and \\(1-\\theta\\) the probability to put it in the other urn (representing “failure”, “tail”).\nThe expected value is \\[\n\\operatorname{E}(x) = n \\theta\n\\] and the variance is \\[\n\\operatorname{Var}(x) = n \\theta (1 - \\theta)\n\\]\nThe corresponding pmf is \\[\np(x | n, \\theta) = W_2\\, \\theta^x (1 - \\theta)^{n - x}\n\\] The binomial coefficient \\[\nW_2 = \\binom{n}{x}\n\\] in the pmf accounts for the number of possible permutations of \\(n\\) items of two distinct types (“success” and “failure”). Note that the binomial coefficient \\(W_2\\) does not depend on \\(\\theta\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe pmf of the binomial distribution is given by dbinom(), the distribution function is pbinom() and the quantile function is qbinom(). The corresponding random number generator is rbinom().\n\n\n\n\nMean parametrisation\nInstead of \\(\\theta\\) one may also use a mean parameter \\(\\mu \\in [0,n]\\) so that \\[\nx \\sim \\operatorname{Bin}\\left(n, \\theta= \\frac{\\mu}{n}\\right)\n\\] The mean parameter \\(\\mu\\) can be obtained from \\(\\theta\\) and \\(n\\) by \\(\\mu = n \\theta\\).\nThe mean and variance of the binomial distribution expressed in terms of \\(\\mu\\) and \\(n\\) are \\[\n\\operatorname{E}(x) = \\mu\n\\] and \\[\n\\operatorname{Var}(x) = \\mu - \\frac{\\mu^2}{n}\n\\]\n\n\nSpecial case: Bernoulli distribution\nFor \\(n=1\\) the binomial distribution reduces to the Bernoulli distribution \\(\\operatorname{Ber}(\\theta)\\). This is the simplest of all distribution families and is named after Jacob Bernoulli (1655-1705) who also discovered the law of large numbers.\nIf a random variable \\(x\\) follows the Bernoulli distribution we write \\[\nx \\sim \\operatorname{Ber}(\\theta)\n\\] with “success” probability \\(\\theta \\in [0,1]\\). Conversely, the complementary “failure” probability is \\(1-\\theta \\in [0,1]\\). The support is \\(x \\in \\{0, 1\\}\\). The variable \\(x\\) acts as an indicator variable, with “success” indicated by \\(x=1\\) and “failure” indicated by \\(x=0\\).\nOften the Bernoulli distribution is referred to as “coin flipping” model. Then \\(\\theta\\) is the probability of “head” and \\(1-\\theta\\) the complementary probability of “tail” and \\(x=1\\) corresponds to the outcome “head” and \\(x=0\\) to the outcome “tail”.\nThe expected value is \\[\n\\operatorname{E}(x) = \\theta\n\\] and the variance is \\[\n\\operatorname{Var}(x) = \\theta (1 - \\theta)\n\\]\nThe pmf of \\(\\operatorname{Ber}(\\theta)\\) is \\[\np(x | \\theta ) = \\theta^{x} (1-\\theta)^{1-x}  =\n\\begin{cases}\n   \\theta  & \\text{if } x = 1 \\\\\n   1-\\theta  & \\text{if } x = 0 \\\\\n\\end{cases}\n\\]\n\n\nConvolution property and normal approximation\nThe convolution of \\(n\\) binomial distributions, each with identical success probability \\(\\theta\\) but possibly different number of trials \\(n_i\\), yields another binomial distribution with the same parameter \\(\\theta\\): \\[\n\\sum_{i=1}^n \\operatorname{Bin}(n_i, \\theta) \\sim \\operatorname{Bin}\\left(\\sum_{i=1}^n n_i, \\theta\\right)\n\\]\nIt follows that the binomial distribution with \\(n\\) trials is the result of the convolution of \\(n\\) Bernoulli distributions: \\[\n\\sum_{i=1}^n \\operatorname{Ber}(\\theta) \\sim \\operatorname{Bin}(n, \\theta)\n\\] Thus, repeating the same Bernoulli trial \\(n\\) times and counting the total number of successes yields a binomial random variable.\nAs a consequence, following the central limit theorem (Section 3.3), for large \\(n\\) the binomial distribution can be well approximated by a normal distribution (Section 5.3) with the same mean and variance. This is known as the De Moivre–Laplace theorem.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "05-univariate.html#sec-betadist",
    "href": "05-univariate.html#sec-betadist",
    "title": "5  Univariate distributions",
    "section": "5.2 Beta distribution",
    "text": "5.2 Beta distribution\nThe beta distribution \\(\\operatorname{Beta}(\\alpha_1, \\alpha_2)\\) is a continuous distribution that is useful to model proportions or probabilities for \\(K=2\\) classes.\nIt includes the uniform distribution over the unit interval as a special case.\n\nStandard parametrisation\nA beta-distributed random variable is denoted by \\[\nx \\sim \\operatorname{Beta}(\\alpha_1, \\alpha_2)\n\\] with shape parameters \\(\\alpha_1&gt;0\\) and \\(\\alpha_2&gt;0\\). Let \\(m = \\alpha_1 +\\alpha_2\\). The support of \\(x\\) is the unit interval given by \\(x \\in [0,1]\\). Thus, the beta distribution is defined over a one-dimensional space.\n\n\n\n\n\n\nFigure 5.2: Stick breaking visualisation of a beta random variable.\n\n\n\nA beta random variable can be visualised as breaking a unit stick of length one into two pieces of length \\(x_1=x\\) and \\(x_2 = 1-x\\) (Figure 5.2). Thus, the \\(x_i\\) may be used as the exclusive proportions or probabilities for \\(K=2\\) classes.\nThe mean is \\[\n\\operatorname{E}(x) = \\operatorname{E}(x_1) = \\frac{\\alpha_1}{m}\n\\] and hence \\[\n\\operatorname{E}(1-x) = \\operatorname{E}(x_2) = \\frac{\\alpha_2}{m}\n\\]\nThe variance is \\[\n\\operatorname{Var}(x) = \\operatorname{Var}(x_1)  = \\operatorname{Var}(x_2) = \\frac{\\alpha_1 \\alpha_2}{m^2 (m+1) }\n\\]\nThe pdf of the beta distribution \\(\\operatorname{Beta}(\\alpha_1, \\alpha_2)\\) is \\[\np(x | \\alpha_1, \\alpha_2) = \\frac{1}{B(\\alpha_1, \\alpha_2)} x^{\\alpha_1-1} (1-x)^{\\alpha_2-1}\n\\] In this density the beta function\n\\[\nB(\\alpha_1, \\alpha_1) = \\frac{ \\Gamma(\\alpha_1) \\Gamma(\\alpha_2)}{\\Gamma(\\alpha_1+\\alpha_2)}\n\\] serves as normalisation factor.\n\n\n\n\n\n\nFigure 5.3: Shapes of the pdf of the beta distribution.\n\n\n\nThe beta distribution can assume a number of different shapes, depending on the values of \\(\\alpha_1\\) and \\(\\alpha_2\\) (see Figure 5.3).\n\n\n\n\n\n\nTipR code\n\n\n\nThe pdf of the beta distribution is given by dbeta(), the distribution function is pbeta() and the quantile function is qbeta(). The corresponding random number generator is rbeta().\n\n\n\n\nMean parametrisation\nInstead of employing \\(\\alpha_1\\) and \\(\\alpha_2\\) as parameters another useful reparametrisation of the beta distribution is in terms of a mean parameter \\(\\mu \\in [0,1]\\) and a concentration parameter \\(m &gt; 0\\) so that \\[\nx \\sim \\operatorname{Beta}(\\alpha_1 = m \\mu, \\alpha_2= m (1-\\mu))\n\\] The concentration and mean parameters can be obtained from \\(\\alpha_1\\) and \\(\\alpha_2\\) by \\(m = \\alpha_1+\\alpha_2\\) and \\(\\mu = \\alpha_1/m\\).\nThe mean and variance of the beta distribution expressed in terms of \\(\\mu\\) and \\(m\\) are \\[\n\\operatorname{E}(x) = \\mu\n\\] and \\[\n\\operatorname{Var}(x)=\\frac{\\mu (1-\\mu)}{m+1}\n\\] With increasing concentration parameter \\(m\\) the variance decreases and thus the probability mass becomes more concentrated around the mean.\n\n\nSpecial case: symmetric beta distribution\nFor \\(\\alpha_1=\\alpha_2=\\alpha\\) the beta distribution becomes the symmetric beta distribution with a single shape parameter \\(\\alpha&gt;0\\). In mean parametrisation the symmetric beta distribution corresponds to \\(\\mu=1/2\\) and \\(m=2 \\alpha\\).\n\n\nSpecial case: uniform distribution\nFor \\(\\alpha_1=\\alpha_2=1\\) the beta distribution becomes the uniform distribution over the unit interval with pdf \\(p(x)=1\\). In mean parametrisation the uniform distribution corresponds to \\(\\mu=1/2\\) and \\(m=2\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "05-univariate.html#sec-normdist",
    "href": "05-univariate.html#sec-normdist",
    "title": "5  Univariate distributions",
    "section": "5.3 Normal distribution",
    "text": "5.3 Normal distribution\nThe normal distribution \\(N(\\mu, \\sigma^2)\\) is the most important continuous probability distribution. It is also called Gaussian distribution named after Carl Friedrich Gauss (1777–1855).\nSpecial cases are the standard normal distribution \\(N(0, 1)\\) and the delta distribution \\(\\delta(\\mu)\\).\n\nStandard parametrisation\nThe univariate normal distribution \\(N(\\mu, \\sigma^2)\\) has two parameters \\(\\mu\\) (location) and \\(\\sigma^2 &gt; 0\\) (variance) and support \\(x \\in \\mathbb{R}\\).\nIf a random variable \\(x\\) is normally distributed we write \\[\nx \\sim N(\\mu,\\sigma^2)\n\\] with mean \\[\n\\operatorname{E}(x)=\\mu\n\\] and variance \\[\n\\operatorname{Var}(x) = \\sigma^2\n\\]\nThe pdf is given by \\[\n\\begin{split}\np(x| \\mu, \\sigma^2) &=(2\\pi\\sigma^2)^{-1/2} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\\\\n                    &=(\\sigma^2)^{-1/2} (2\\pi)^{-1/2} e^{-\\Delta^2/2}\\\\\n\\end{split}\n\\] Here \\(\\Delta^2 = (x-\\mu)^2/\\sigma^2\\) is the squared distance between \\(x\\) and \\(\\mu\\) weighted by the variance \\(\\sigma^2\\), also known as squared Mahalanobis distance.\nThe normal distribution is sometimes also used by specifying the precision \\(1/\\sigma^2\\) instead of the variance \\(\\sigma^2\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe normal pdf is given by dnorm(), the distribution function is pnorm() and the quantile function is qnorm(). The corresponding random number generator is rnorm().\n\n\n\n\nScale parametrisation\nInstead of the variance parameter \\(\\sigma^2\\) it is often also convenient to use the standard deviation \\(\\sigma=\\sqrt{\\sigma^2} &gt; 0\\) as scale parameter. Similarly, instead of the precision \\(1/\\sigma^2\\) one may wish to use the inverse standard deviation \\(w = 1/\\sigma\\).\nThe scale parametrisation is central for location-scale transformations (see below).\n\n\nSpecial case: standard normal distribution\n\n\n\n\n\n\n\n\nFigure 5.4: Probability density function (left) and cumulative density function (right) of the standard normal distribution.\n\n\n\n\n\nThe standard normal distribution \\(N(0, 1)\\) has mean \\(\\mu=0\\) and variance \\(\\sigma^2=1\\). The corresponding pdf is \\[\np(x)=(2\\pi)^{-1/2} e^{-x^2/2}\n\\] with the squared Mahalanobis distance reduced to \\(\\Delta^2=x^2\\).\nThe cumulative distribution function (cdf) of the standard normal \\(N(0,1)\\) is \\[\n\\Phi (x ) = \\int_{-\\infty}^{x} p(x'| \\mu=0, \\sigma^2=1) dx'\n\\] There is no analytic expression for \\(\\Phi(x)\\). The inverse \\(\\Phi^{-1}(p)\\) is called the quantile function of the standard normal distribution.\nFigure 5.4 shows the pdf and cdf of the standard normal distribution.\n\n\nSpecial case: delta distribution\nThe delta distribution \\(\\delta(\\mu)\\) is obtained as the limit of \\(N(\\mu, \\varepsilon \\sigma^2)\\) for \\(\\varepsilon \\rightarrow 0\\) and where \\(\\sigma^2\\) is a positive number (e.g. \\(\\sigma^2=1\\)). Thus \\(\\delta(\\mu)\\) is a continuous distribution representing a point mass at \\(\\mu\\).\nThe corresponding pdf \\(\\delta(x| \\mu)\\) is called the Dirac delta function, even though it is not an ordinary function. It satisfies \\(\\delta(x | \\mu)=0\\) for all \\(x\\neq \\mu\\) with an infinite spike at \\(\\mu\\) but still integrates to one.\n\n\nLocation-scale transformation\nLet \\(\\sigma &gt; 0\\) be the positive square root of the variance \\(\\sigma^2\\) and \\(w=1/\\sigma\\).\nIf \\(x \\sim N(\\mu, \\sigma^2)\\) then \\(y=w(x-\\mu) \\sim N(0, 1)\\). This location-scale transformation corresponds to centring and standardisation of a normal random variable, reducing it to a standard normal random variable.\nConversely, if \\(y \\sim N(0, 1)\\) then \\(x = \\mu + \\sigma y \\sim N(\\mu, \\sigma^2)\\). This location-scale transformation generates the normal distribution from the standard normal distribution.\n\n\nConvolution property\nThe convolution of \\(n\\) independent, but not necessarily identical, normal distributions results in another normal distribution with corresponding mean and variance: \\[\n\\sum_{i=1}^n N(\\mu_i, \\sigma^2_i) \\sim N\\left( \\sum_{i=1}^n \\mu_i,  \\sum_{i=1}^n \\sigma^2_i \\right)\n\\] Hence, any normal random variable can be constructed as the sum of \\(n\\) suitable independent normal random variables.\nSince \\(n\\) is an arbitrary positive integer the normal distribution is said to be infinitely divisible.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "05-univariate.html#sec-gamdist",
    "href": "05-univariate.html#sec-gamdist",
    "title": "5  Univariate distributions",
    "section": "5.4 Gamma distribution",
    "text": "5.4 Gamma distribution\n\n\n\n\n\n\nFigure 5.5: The gamma and the univariate Wishart distribution and their relatives.\n\n\n\nThe gamma distribution \\(\\operatorname{Gam}(\\alpha, \\theta)\\) is another widely used continuous distribution and is also known as univariate Wishart distribution \\(\\operatorname{Wis}\\left(s^2, k \\right)\\) using a different parametrisation.\nIt contains as special cases the scaled chi-squared distribution \\(s^2 \\chi^2_{k}\\) (two parameter restrictions) as well as the univariate standard Wishart distribution \\(\\operatorname{Wis}\\left(1, k \\right)\\), the chi-squared distribution \\(\\chi^2_{k}\\) and the exponential distribution \\(\\operatorname{Exp}(\\theta)\\) (one parameter restrictions). Figure 5.5 illustrates the relationship of the gamma and the univariate Wishart distribution with these related distributions.\n\nStandard parametrisation\nThe gamma distribution \\(\\operatorname{Gam}(\\alpha, \\theta)\\) is a continuous distribution with two parameters \\(\\alpha&gt;0\\) (shape) and \\(\\theta&gt;0\\) (scale): \\[\nx \\sim\\operatorname{Gam}(\\alpha, \\theta)\n\\] and support \\(x \\in [0, \\infty[\\) with mean \\[\\operatorname{E}(x)=\\alpha \\theta\\] and variance \\[\\operatorname{Var}(x) = \\alpha \\theta^2\\]\nThe gamma distribution is also often used with a rate parameter \\(\\beta=1/\\theta\\). Therefore one needs to pay attention which parametrisation is used.\nThe pdf is \\[\np(x| \\alpha, \\theta)=\\frac{1}{\\Gamma(\\alpha) \\theta^{\\alpha} } x^{\\alpha-1} e^{-x/\\theta}\n\\] In this density the gamma function \\[\n\\Gamma(x) = \\int_0^\\infty t^{x-1} e^{-t} dt\n\\] is part of the normalisation factor.\nThe gamma function can also be used as continuous version of the factorial as \\(\\Gamma(x) = (x-1)!\\) for any positive integer \\(x\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe pdf of the gamma distribution is available in the function dgamma(), the distribution function is pgamma() and the quantile function is qgamma(). The corresponding random number generator is rgamma().\n\n\n\n\nWishart parametrisation\nThe gamma distribution is often used with a different set of parameters \\(s^2 =\\theta/2 &gt; 0\\) (scale) and \\(k=2 \\alpha &gt; 0\\) (shape or concentration). In this form it is known as univariate or one-dimensional Wishart distribution \\[\nx \\sim \\operatorname{Wis}\\left(s^2, k \\right) = \\operatorname{Gam}\\left(\\alpha=\\frac{k}{2}, \\theta=2 s^2\\right)\n\\] named after John Wishart (1898–1954).\nIn the above the scale parameter \\(s^2\\) is a scalar and hence the resulting Wishart distribution is univariate. If instead a matrix-valued scale parameter \\(\\boldsymbol S\\) is used this yields the multivariate or \\(d\\)-dimensional Wishart distribution, see Section 6.4.\nIn the Wishart parametrisation the mean is \\[\n\\operatorname{E}(x) = k s^2\n\\] and the variance \\[\n\\operatorname{Var}(x) = 2 k s^4\n\\] The pdf in terms of \\(s^2\\) and \\(k\\) is \\[\np(x| s^2, k)=\\frac{1}{\\Gamma(k/2) (2 s^2)^{k/2} } x^{(k-2)/2} e^{-s^{-2}x/2}\n\\]\n\n\nMean parametrisation\nFinally, we also often employ the Wishart resp. gamma distribution in mean parametrisation \\[\nx \\sim \\operatorname{Wis}\\left(s^2= \\frac{\\mu}{k}, k \\right) = \\operatorname{Gam}\\left(\\alpha=\\frac{k}{2}, \\theta=\\frac{2 \\mu}{k}\\right)\n\\] with parameters \\(\\mu = k s^2 &gt;0\\) and \\(k &gt; 0\\). In this parametrisation the mean is \\[\n\\operatorname{E}(x) = \\mu\n\\] and the variance \\[\n\\operatorname{Var}(x) = \\frac{2 \\mu^2}{k}\n\\]\n\n\nSpecial case: univariate standard Wishart distribution\nFor \\(s^2=1\\) the univariate Wishart distribution reduces to the univariate standard Wishart distribution \\[\nx \\sim \\operatorname{Wis}\\left(1, k \\right) = \\operatorname{Gam}\\left(\\alpha=\\frac{k}{2}, \\theta=2\\right)\n\\] with mean \\[\n\\operatorname{E}(x) = k\n\\] and variance \\[\n\\operatorname{Var}(x) = 2 k\n\\]\nThe pdf is \\[\np(x| k)=\\frac{1}{\\Gamma(k/2) 2^{k/2} } x^{(k-2)/2} e^{-x/2}\n\\]\n\n\nSpecial case: scaled chi-squared distribution\nIf the shape parameter of the Wishart distribution \\(\\operatorname{Wis}\\left(s^2, k \\right)\\) is restricted to the positive integers \\(k \\in \\{ 1, 2, \\ldots \\}\\) the Wishart distribution becomes the scaled chi-squared distribution \\(s^2 \\chi^2_{k}\\) where \\(k\\) is called the degree of freedom.\nThis is equivalent to restricting the shape parameter \\(\\alpha\\) of the gamma distribution \\(\\operatorname{Gam}(\\alpha=k/2, \\theta=2 s^2)\\) to \\(\\alpha \\in \\{1/2, 1, 3/2, 2, \\ldots\\}\\).\nThe scaled chi-squared distribution with \\(k=1\\) is the distribution of a squared normal random variable with mean zero. Specifically, if \\(z \\sim N(0, s^2)\\) then \\(z^2 \\sim s^2 \\chi^2_{1}= \\operatorname{Wis}(s^2, 1)=N(0, s^2)^2\\).\n\n\nSpecial case: chi-squared distribution\n\n\n\n\n\n\n\n\nFigure 5.6: Probability density function of the chi-squared distribution.\n\n\n\n\n\nIf \\(k \\in \\{ 1, 2, \\ldots \\}\\) is restricted to the positive integers the univariate standard Wishart distribution reduces to the chi-squared distribution \\[\nx \\sim \\chi^2_{k}=\\operatorname{Wis}\\left(s^2=1, k \\right)=\\operatorname{Gam}\\left(\\alpha=\\frac{k}{2}, \\theta=2\\right)\n\\] where \\(k\\) is called the degree of freedom.\nThe chi-squared distribution has mean \\[\n\\operatorname{E}(x)=k\n\\] and variance \\[\n\\operatorname{Var}(x)=2k\n\\]\nFigure 5.6 shows the pdf of the chi-squared distribution for degrees of freedom \\(k=1\\) and \\(k=3\\).\nThe chi-squared distribution with \\(k=1\\) is the distribution of a squared standard normal random variable. Specifically, if \\(z \\sim N(0, 1)\\) then \\(z^2 \\sim  \\chi^2_{1} = \\operatorname{Wis}(1,1)=N(0,1)^2\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe pdf of the chi-squared distribution is given by dchisq(). The distribution function is pchisq() and the quantile function is qchisq(). The corresponding random number generator is rchisq().\n\n\n\n\nSpecial case: exponential distribution\nIf the shape parameter \\(\\alpha\\) of the gamma distribution \\(\\operatorname{Gam}(\\alpha, \\theta)\\) is set to \\(\\alpha=1\\), or if the shape parameter \\(k\\) of the Wishart distribution \\(\\operatorname{Wis}(s^2, k)\\) is set to \\(k=2\\), we obtain the exponential distribution \\[\nx \\sim \\operatorname{Exp}(\\theta) = \\operatorname{Gam}(\\alpha=1, \\theta) = \\operatorname{Wis}(s^2=\\theta/2, k=2)\n\\] with scale parameter \\(\\theta\\).\nIt has mean \\[\n\\operatorname{E}(x)=\\theta\n\\] and variance \\[\n\\operatorname{Var}(x) = \\theta^2\n\\] and the pdf is \\[\np(x|  \\theta)=\\theta^{-1}  e^{-x/\\theta}\n\\]\nJust like the gamma distribution the exponential distribution is also often specified using a rate parameter \\(\\beta= 1/\\theta\\) instead of a scale parameter \\(\\theta\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe command dexp() returns the pdf of the exponential distribution, pexp() is the distribution function and qexp() is the quantile function. The corresponding random number generator is rexp().\n\n\n\n\nScale transformation\nIf \\(x \\sim \\operatorname{Gam}(\\alpha, \\theta)\\) then the scaled random variable \\(b x\\) with \\(b&gt;0\\) is also gamma distributed with \\(b x \\sim \\operatorname{Gam}(\\alpha, b \\theta)\\).\nHence,\n\n\\(\\theta \\, \\operatorname{Gam}(\\alpha, 1) = \\operatorname{Gam}(\\alpha, \\theta)\\),\n\\(\\theta \\, \\operatorname{Exp}(1) = \\operatorname{Exp}(\\theta)\\),\n\\((\\mu / k) \\, \\operatorname{Wis}(1, k) =  \\operatorname{Wis}(s^2= \\mu/k, k)\\) and\n\\(s^2 \\, \\operatorname{Wis}(1, k) = \\operatorname{Wis}(s^2, k)\\).\n\nAs \\(\\chi^2_{k}\\) equals \\(\\operatorname{Wis}(1, k)\\) the last example demonstrates that the scaled chi-squared distribution \\(s^2 \\chi^2_{k}\\) equals the univariate Wishart distribution \\(\\operatorname{Wis}(s^2, k)\\).\n\n\nConvolution property\nThe convolution of \\(n\\) gamma distributions with the same scale parameter \\(\\theta\\) but possible different shape parameters \\(\\alpha_i\\) yields another gamma distribution: \\[\n\\sum_{i=1}^n \\operatorname{Gam}(\\alpha_i, \\theta) \\sim \\operatorname{Gam}\\left( \\sum_{i=1}^n \\alpha_i,  \\theta \\right)\n\\] Thus, any gamma random variable can be obtained as the sum of \\(n\\) suitable independent gamma random variables.\nIn Wishart parametrisation this becomes \\[\n\\sum_{i=1}^n \\operatorname{Wis}(s^2, k_i) \\sim \\operatorname{Wis}\\left(s^2, \\sum_{i=1}^n k_i\\right)\n\\]\nAs a result, since \\(n\\) is an arbitrary positive integer, the gamma resp. univariate Wishart distribution is infinitely divisible.\nThe above includes the following two specific constructions:\n\nIf \\(x_1, \\ldots, x_n \\sim \\operatorname{Exp}(\\theta)\\) are independent samples from \\(\\operatorname{Exp}(\\theta)\\) then the sum \\(y = \\sum_{i=1}^n x_i \\sim \\operatorname{Gam}(\\alpha=n, \\theta)\\) is gamma distributed with the same scale parameter.\nThe sum of \\(k\\) independent scaled chi-squared random variables \\(s^2 \\chi^2_{1}\\) with one degree of freedom and identical scale parameter \\(s^2\\) yields a scaled chi-squared random variable \\(s^2 \\chi^2_{k}\\) with degree of freedom \\(k\\) and the same scale parameter. Thus, if \\(z_1,z_2,\\dots,z_k\\sim N(0,1)\\) are \\(k\\) independent samples from \\(N(0,1)\\) then \\(\\sum_{i=1}^{k} z_i^2 \\sim \\chi^2_{k}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "05-univariate.html#sec-invgamdist",
    "href": "05-univariate.html#sec-invgamdist",
    "title": "5  Univariate distributions",
    "section": "5.5 Inverse gamma distribution",
    "text": "5.5 Inverse gamma distribution\nThe inverse gamma distribution \\(\\operatorname{IGam}(\\alpha, \\beta)\\) is a continuous distribution and is also known as univariate inverse Wishart distribution \\(\\operatorname{IWis}(\\psi, k)\\) using a different parametrisation. It is linked to the gamma distribution \\(\\operatorname{Gam}(\\alpha, \\theta)\\) aka univariate Wishart distribution \\(\\operatorname{Wis}\\left(s^2, k \\right)\\) (Section 5.4).\nSpecial cases include the inverse chi-squared distribution \\(\\chi^{-2}_{k}\\) and the scaled inverse chi-squared distribution \\(s^2 \\chi^{-2}_{k}\\).\n\nStandard parametrisation\nA random variable \\(x\\) following an inverse gamma distribution is denoted by \\[\nx \\sim \\operatorname{IGam}(\\alpha, \\beta)\n\\] with two parameters \\(\\alpha &gt;0\\) (shape parameter) and \\(\\beta &gt;0\\) (scale parameter) and support \\(x &gt;0\\).\nThe mean of the inverse gamma distribution is (for \\(\\alpha&gt;1\\)) \\[\n\\operatorname{E}(x) = \\frac{\\beta}{\\alpha-1}\n\\] and the variance (for \\(\\alpha&gt;2\\)) \\[\n\\operatorname{Var}(x) = \\frac{\\beta^2}{(\\alpha-1)^2 (\\alpha-2)}\n\\]\nThe inverse gamma distribution \\(\\operatorname{IGam}(\\alpha, \\beta)\\) has pdf \\[\np(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{-\\alpha-1} e^{-\\beta/x}\n\\]\n\n\n\n\n\n\nTipR code\n\n\n\nThe extraDistr package implements the inverse gamma distribution. The function extraDistr::dinvgamma() provides the pdf, extraDistr::pinvgamma() the distribution function and extraDistr::qinvgamma() is the quantile function. The corresponding random number generator is extraDistr::rinvgamma().\n\n\n\nRelation to the gamma distribution\nThe inverse gamma distribution is closely linked to the gamma distribution. Assume that the random variable \\(y\\) follows a gamma distribution with \\[\ny \\sim \\operatorname{Gam}(\\alpha, \\theta)\n\\] then the inverse random variable \\(x = 1/y\\) follows an inverse gamma distribution with inverted scale parameter \\[\nx = \\frac{1}{y} \\sim \\operatorname{IGam}\\left(\\alpha, \\beta=\\frac{1}{\\theta}\\right)\n\\] where \\(\\alpha\\) is the shared shape parameter, \\(\\theta\\) the scale parameter of the gamma distribution and \\(\\beta\\) the scale parameter of the inverse gamma distribution.\nCorrespondingly, the density \\(p_x(x| \\alpha, \\beta )\\) of the inverse gamma distribution is obtained from the density \\(p_y(y |\\alpha, \\theta )\\) of the gamma distribution via \\[\np_x(x| \\alpha, \\beta) = \\frac{1}{x^2}\\, p_y\\left(\\left.\\frac{1}{x} \\right| \\alpha, \\theta=\\frac{1}{\\beta}  \\right)\n\\]\n\n\n\nWishart parametrisation\nThe inverse gamma distribution is frequently used with a different set of parameters \\(\\psi = 2\\beta\\) (scale parameter) and \\(k = 2\\alpha\\) (shape parameter). In this form it is called univariate inverse Wishart distribution \\[\nx \\sim \\operatorname{IWis}(\\psi, k) =\n\\operatorname{IGam}\\left(\\alpha=\\frac{k}{2}, \\beta=\\frac{\\psi}{2}\\right)\n\\]\nIn the above the scale parameter \\(\\psi\\) is scalar and hence the resulting inverse Wishart distribution is univariate. If instead a matrix-valued scale parameter \\(\\boldsymbol \\Psi\\) is used this yields the multivariate or \\(d\\)-dimensional inverse Wishart distribution, see Section 6.5.\nIn the Wishart parametrisation the mean is (for \\(k&gt;2\\)) \\[\n\\operatorname{E}(x) = \\frac{\\psi}{k-2}\n\\] and the variance is (for \\(k &gt;4\\)) \\[\n\\operatorname{Var}(x) =\\frac{2 \\psi^2}{(k-4) (k-2)^2 }  \n\\]\nThe pdf in terms of \\(\\psi\\) and \\(k\\) is \\[\np(x | \\psi, k) = \\frac{(\\psi/2)^{(k/2)}}{\\Gamma(k/2)} \\, x^{-(k+2)/2}\\, e^{-\\psi x^{-1}/2}\n\\]\n\nRelation to the univariate Wishart distribution\nThe univariate inverse Wishart distribution is closely linked to the univariate Wishart distribution. Assume that the random variable \\(y\\) follows an univariate Wishart distribution with \\[\ny \\sim \\operatorname{Wis}(s^2, k)\n\\] then the inverse random variable \\(x = 1/y\\) follows a univariate inverse Wishart distribution with inverted scale parameter \\[\nx = \\frac{1}{y} \\sim \\operatorname{IWis}\\left(\\psi = \\frac{1}{s^2}, k\\right)\n\\] where \\(k\\) is the shared shape parameter, \\(s^2\\) the scale parameter of the univariate Wishart distribution and \\(\\psi\\) the scale parameter of the univariate inverse Wishart distribution.\nCorrespondingly, the density \\(p_x(x|\\psi, k)\\) of the univariate inverse Wishart distribution is obtained from the density \\(p_y(y| s^2, k)\\) of the univariate Wishart distribution via \\[\np_x(x|\\psi, k) = \\frac{1}{x^2}\\, p_y\\left(\\left.\\frac{1}{x}\\right| s^2=\\frac{1}{\\psi}, k \\right)\n\\]\n\n\n\nMean parametrisation\nInstead of \\(\\psi\\) and \\(k\\) we may also equivalently use \\(\\mu = \\psi/(\\nu-2)\\) and \\(\\kappa=\\nu-2\\) as parameters for the univariate inverse Wishart distribution, so that \\[\nx \\sim \\operatorname{IWis}(\\psi=\\kappa \\mu, k=\\kappa+2) =\n\\operatorname{IGam}\\left(\\alpha=\\frac{\\kappa+2}{2}, \\beta=\\frac{\\mu \\kappa}{2}\\right)\n\\] has mean (for \\(\\kappa&gt;0\\)) \\[\\operatorname{E}(x) = \\mu\\] and the variance (for \\(\\kappa&gt;2\\)) \\[\\operatorname{Var}(x) = \\frac{2 \\mu^2}{\\kappa-2}\\]\nThe mean parametrisation is useful in Bayesian analysis when employing the inverse gamma aka univariate inverse Wishart distribution as prior and posterior distribution.\n\n\nBiased mean parametrisation\nUsing \\(\\tau^2= \\frac{\\psi}{k}\\) as biased mean parameter together with \\(\\nu=k\\) we arrive at the biased mean parametrisation \\[\nx \\sim \\operatorname{IWis}(\\psi= \\nu \\tau^2, k=\\nu ) =\n\\operatorname{IGam}\\left(\\alpha=\\frac{\\nu}{2}, \\beta=\\frac{\\nu \\tau^2}{2}\\right)\n\\] with mean (for \\(\\nu&gt;2\\)) \\[\n\\operatorname{E}(x) = \\frac{ \\nu}{\\nu-2}  \\tau^2 = \\mu\n\\] and variance (\\(\\nu &gt;4\\)) \\[\n\\operatorname{Var}(x) = \\left(\\frac{\\nu}{\\nu-2}\\right)^2  \\frac{2 \\tau^4}{\\nu-4}\n\\] As \\(\\tau^2 = \\mu (\\nu-2)/\\nu\\) for large \\(\\nu\\) the parameter \\(\\tau^2\\) will become identical to the true mean \\(\\mu\\).\nThis parametrisation is useful to derive the location-scale \\(t\\)-distribution with its matching parameters (see Section 5.6). It is also common in Bayesian analysis.\n\n\nSpecial case: inverse chi-squared distribution\nIf the scale parameter in \\(\\operatorname{IWis}(\\psi, k)\\) is set to \\(\\psi=1\\) and \\(k \\in \\{ 1, 2, \\ldots \\}\\) is restricted to the positive integers the univariate inverse Wishart distribution reduces to the inverse chi-squared distribution \\[\nx \\sim \\chi^{-2}_{k}=\\operatorname{IWis}(\\psi=1, k)=\n\\operatorname{IGam}\\left(\\alpha=\\frac{k}{2}, \\beta=\\frac{1}{2}\\right)\n\\] where \\(k\\) is called the degree of freedom.\nThe inverse chi-squared distribution has mean (for \\(k&gt;2\\)) \\[\n\\operatorname{E}(x) = \\frac{1}{k-2}\n\\] and the variance is (for \\(k &gt;4\\)) \\[\n\\operatorname{Var}(x) =\\frac{2}{(k-2)^2 (k-4) }  \n\\]\n\nRelation to the chi-squared distribution\nThe inverse chi-squared distribution is closely linked to the chi-squared distribution. Assume that the random variable \\(y\\) follows a chi-squared distribution with \\[\ny \\sim \\chi^2_{k}\n\\] then the inverse random variable \\(x = 1/y\\) follows an inverse chi-squared distribution \\[\nx = \\frac{1}{y} \\sim \\chi^{-2}_{k}\n\\] where \\(k\\) is the shared degree of freedom (shape parameter).\n\n\n\nScale transformation\nIf \\(x \\sim \\operatorname{IGam}(\\alpha, \\beta)\\) then the scaled random variable \\(b x\\) with \\(b&gt;0\\) is also inverse gamma distributed with \\(b x \\sim \\operatorname{IGam}(\\alpha, b \\beta)\\).\nHence,\n\n\\(\\beta \\, \\operatorname{IGam}(\\alpha, 1) = \\operatorname{IGam}(\\alpha, \\beta)\\),\n\\(\\psi \\, \\operatorname{IWis}(1, k) = \\operatorname{IWis}(\\psi, k)\\),\n\\(\\kappa \\mu \\,\\operatorname{IWis}(1, k=\\kappa+2) = \\operatorname{IWis}(\\psi=\\kappa \\mu, k=\\kappa+2)\\) and\n\\(\\nu \\tau^2 \\, \\operatorname{IWis}(1, k=\\nu) = \\operatorname{IWis}(\\psi=\\nu \\tau^2, k=\\nu)\\)\n\nAs \\(\\chi^{-2}_{k}\\) equals \\(\\operatorname{IWis}(\\psi=1, k)\\) the scaled inverse chi-squared distribution \\(\\psi \\chi^{-2}_{k}\\) is thus equivalent to the univariate inverse Wishart distribution \\(\\operatorname{IWis}(\\psi, k)\\). If a random variable \\(y\\) follows the scaled chi-squared distribution its inverse \\(y = 1/y\\) follows the corresponding scaled inverse chi-squared distribution. Specifically, if \\(y \\sim s^2 \\, \\chi^2_{k}\\) then \\(x=1/y \\sim s^{-2} \\, \\chi^{-2}_{k}\\).\nThe scaled inverse chi-squared distribution is frequently used in the biased mean parametrisation with \\(\\tau = \\psi/\\nu\\) and \\(\\nu=k\\). Then \\(\\psi \\, \\chi^{-2}_{k}\\) is equal to \\(\\nu \\tau^2 \\, \\chi^{-2}_{\\nu} = \\operatorname{IWis}(\\psi=\\nu \\tau^2, k=\\nu)\\) which is sometimes also written as \\(\\chi^{-2}_{}(\\nu, \\tau^2)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "05-univariate.html#sec-lstdist",
    "href": "05-univariate.html#sec-lstdist",
    "title": "5  Univariate distributions",
    "section": "5.6 Location-scale \\(t\\)-distribution",
    "text": "5.6 Location-scale \\(t\\)-distribution\n\n\n\n\n\n\nFigure 5.7: The location-scale \\(t\\)-distribution and its relatives.\n\n\n\nThe location-scale \\(t\\)-distribution \\(t_{\\nu}(\\mu, \\tau^2)\\) is a continuous distribution and is a generalisation of the normal distribution \\(N(\\mu, \\tau^2)\\) (Section 5.3) with an additional parameter \\(\\nu &gt; 0\\) (degrees of freedom) controlling the probability mass in the tails.\nSpecial cases include the Student’s \\(t\\)-distribution \\(t_{\\nu}\\), the normal distribution \\(N(\\mu, \\tau^2)\\) and the Cauchy distribution \\(\\operatorname{Cau}(\\mu, \\tau^2)\\). Figure 5.7 illustrates the relationship of the location-scale \\(t\\)-distribution \\(t_{\\nu}(\\mu, \\tau^2)\\) with these related distributions.\n\nStandard parametrisation\nIf a random variable \\(x \\in \\mathbb{R}\\) follows the location-scale \\(t\\)-distribution we write \\[\nx \\sim t_{\\nu}(\\mu, \\tau^2)\n\\] where \\(\\mu\\) is the location and \\(\\tau^2\\) the dispersion parameter. The parameter \\(\\nu &gt; 0\\) prescribes the degrees of freedom. For small values of \\(\\nu\\) the distribution is heavy-tailed and as a result only moments of order smaller than \\(\\nu\\) are finite and defined.\nThe mean is (for \\(\\nu&gt;1\\)) \\[\n\\operatorname{E}(x) = \\mu\n\\] and the variance (for \\(\\nu&gt;2\\)) \\[\n\\operatorname{Var}(x) = \\frac{\\nu}{\\nu-2} \\tau^2\n\\]\nThe pdf of \\(t_{\\nu}(\\mu, \\tau^2)\\) is \\[\np(x | \\mu, \\tau^2, \\nu) = ( \\tau^2)^{-1/2}\n\\frac{\\Gamma(\\frac{\\nu+1}{2})} { (\\pi \\nu)^{1/2}  \\,\\Gamma(\\frac{\\nu}{2})}\n\\left(1+ \\frac{\\Delta^2}{\\nu}  \\right)^{-(\\nu+1)/2}\n\\] with \\(\\Delta^2 = (x-\\mu)^2/\\tau^2\\) the squared Mahalanobis distance between \\(x\\) and \\(\\mu\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe package extraDistr implements the location-scale \\(t\\)-distribution. The function extraDistr::dlst() returns the pdf, extraDistr::plst() the distribution function function and extraDistr::qlst() is the quantile function. The corresponding random number generator is extraDistr::rlst().\n\n\n\n\nScale parametrisation\nInstead of the dispersion parameter \\(\\tau^2\\) it is often also convenient to use the scale parameter \\(\\tau=\\sqrt{\\tau^2} &gt; 0\\). Similarly, instead of the inverse dispersion \\(1/\\tau^2\\) one may wish to use the inverse scale \\(w = 1/\\tau\\).\nThe scale parametrisation is central for location-scale transformations (see below).\n\n\nSpecial case: Student’s \\(t\\)-distribution\nWith \\(\\mu=0\\) and \\(\\tau^2=1\\) the location-scale \\(t\\)-distribution reduces to the standard \\(t\\)-distribution \\(t_{\\nu}=t_{\\nu}(0,1)\\). It is commonly known Student’s \\(t\\)-distribution named after “Student” which was the pseudonym of William Sealy Gosset (1876–1937). It is a generalisation of the standard normal distribution \\(N(0,1)\\) to allow for heavy tails.\nThe distribution has mean \\(\\operatorname{E}(x)=0\\) (for \\(\\nu&gt;1\\)) and variance \\(\\operatorname{Var}(x)=\\frac{\\nu}{\\nu-2}\\) (for \\(\\nu&gt;2\\)).\nThe pdf of \\(t_{\\nu}\\) is \\[\np(x | \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {(\\pi \\nu)^{1/2}  \\,\\Gamma(\\frac{\\nu}{2})}\n\\left(1+\\frac{x^2}{\\nu} \\right)^{-(\\nu+1)/2}\n\\] with the squared Mahalanobis distance reducing to \\(\\Delta^2=x^2\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe command dt() returns the pdf of the \\(t\\)-distribution, pt() is distribution function and qt() the quantile function. The corresponding random number generator is rt().\n\n\n\n\nSpecial case: normal distribution\nFor \\(\\nu \\rightarrow \\infty\\) the location-scale \\(t\\)-distribution \\(t_{\\nu}(\\mu, \\tau^2)\\) reduces to the normal distribution \\(N(\\mu, \\tau^2)\\) (Section 5.3). Correspondingly, for \\(\\nu \\rightarrow \\infty\\) the Student’s \\(t\\)-distribution becomes equal to the standard normal distribution \\(N(0,1)\\).\nSee Section 6.6 for further details.\n\n\nSpecial case: Cauchy distribution\nFor \\(\\nu=1\\) the location-scale \\(t\\)-distribution becomes the Cauchy distribution \\(\\operatorname{Cau}(\\mu, \\tau^2)=t_{1}(\\mu, \\tau^2)\\) named after Augustin-Louis Cauchy (1789–1857).\nIts mean, variance and other higher moments are all undefined.\nIt has pdf \\[\n\\begin{split}\np(x| \\mu, \\tau^2) &= (\\tau^2)^{-1/2} (\\pi (1+\\Delta^2))^{-1}\\\\\n&= \\frac{\\tau}{\\pi (\\tau^2+(x-\\mu)^2)}\n\\end{split}\n\\] with \\(\\tau=\\sqrt{\\tau^2}&gt;0\\).\nNote that in the above we employ \\(\\tau^2\\) as dispersion parameter as this parallels the location-scale \\(t\\)-distribution and the normal distribution but very often the Cauchy distribution is used with \\(\\tau&gt; 0\\) as scale parameter.\n\n\n\n\n\n\nTipR code\n\n\n\nThe command dcauchy() returns the pdf of the Cauchy distribution, pcauchy() is the distribution function and qcauchy() the quantile function. The corresponding random number generator is rcauchy().\n\n\n\n\nSpecial case: standard Cauchy distribution\nThe standard Cauchy distribution \\(\\operatorname{Cau}(0, 1)=t_{1}(0, 1) =t_{1}\\) is obtained by setting \\(\\mu=0\\) and \\(\\tau^2=1\\) (Cauchy distribution) or, equivalently, by setting \\(\\nu=1\\) (Student’s \\(t\\)-distribution).\nIt has pdf \\[\np(x) = \\frac{1}{\\pi (1+x^2)}\n\\]\n\n\nLocation-scale transformation\nLet \\(\\tau &gt; 0\\) be the positive square root of \\(\\tau^2\\) and \\(w=1/\\tau\\).\nIf \\(x \\sim t_{\\nu}(\\mu, \\tau^2)\\) then \\(y=w(x-\\mu) \\sim t_{\\nu}\\). This location-scale transformation reduces a location-scale \\(t\\)-distributed random variable to a Student’s \\(t\\)-distributed random variable.\nConversely, if \\(y \\sim t_{\\nu}\\) then \\(x = \\mu + \\tau y \\sim t_{\\nu}(\\mu, \\tau^2)\\). This location-scale transformation generates the location-scale \\(t\\)-distribution from the Student’s \\(t\\) -distribution.\nFor the special case of the Cauchy distribution (corresponding to \\(\\nu=1\\)) similar relations hold between it and the standard Cauchy distribution. If \\(x \\sim \\operatorname{Cau}(\\mu, \\tau^2)\\) then \\(y=w(x-\\mu) \\sim \\operatorname{Cau}(0, 1)\\). Conversely, if \\(y \\sim \\operatorname{Cau}(0, 1)\\) then \\(x = \\mu + \\tau y \\sim \\operatorname{Cau}(\\mu, \\tau^2)\\).\n\n\nConvolution property\nThe location-scale \\(t\\)-distribution is not generally closed under convolution, with the exception of two special cases, the normal distribution (\\(\\nu=\\infty\\)), see Section 5.3, and the Cauchy distribution (\\(\\nu=1\\)).\nFor the Cauchy distribution with \\(\\tau_i^2= a_i^2 \\tau^2\\), where \\(a_i&gt;0\\) are positive scalars, \\[\n\\sum_{i=1}^n  \\operatorname{Cau}(\\mu_i, a_i^2 \\tau^2)  \\sim\n  \\operatorname{Cau}\\left( \\sum_{i=1}^n \\mu_i, \\left(\\sum_{i=1}^n a_i\\right)^2  \\tau^2\\right)\n\\]\n\n\nLocation-scale \\(t\\)-distribution as compound distribution\nThe location-scale \\(t\\)-distribution can be obtained as mixture of normal distributions with identical mean and varying variance. Specifically, let \\(z\\) be a univariate inverse Wishart random variable \\[\nz \\sim  \\operatorname{IWis}(\\psi=\\nu, k=\\nu) =\n        \\operatorname{IGam}\\left(\\alpha=\\frac{\\nu}{2}, \\beta=\\frac{\\nu}{2}\\right)\n\\] and let \\(x| z\\) be normal \\[\nx | z \\sim N(\\mu,\\sigma^2 = z \\tau^2)\n\\]\nThen the resulting marginal (scale mixture) distribution for \\(x\\) is the location-scale \\(t\\)-distribution \\[\nx \\sim t_{\\nu}\\left(\\mu, \\tau^2\\right)\n\\]\nAn alternative way to arrive at \\(t_{\\nu}\\left(\\mu, \\tau^2\\right)\\) is to include \\(\\tau^2\\) as parameter in the inverse Wishart distribution \\[\nz \\sim  \\tau^2 \\operatorname{IWis}(\\psi=\\nu, k=\\nu) = \\operatorname{IWis}(\\psi=\\nu \\tau^2, k=\\nu)\n        \\] and let \\[\nx | z \\sim N(\\mu,\\sigma^2 = z)\n\\] Note that \\(\\tau^2\\) is now the biased mean parameter of the univariate inverse Wishart distribution. This characterisation is useful in Bayesian analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "06-multivariate.html",
    "href": "06-multivariate.html",
    "title": "6  Multivariate distributions",
    "section": "",
    "text": "6.1 Multinomial distribution\nThe multinomial distribution \\(\\operatorname{Mult}(n, \\boldsymbol \\theta)\\) is the multivariate generalisation of the binomial distribution \\(\\operatorname{Bin}(n, \\theta)\\) (Section 5.1) from two classes to \\(K\\) classes.\nA special case is the categorical distribution \\(\\operatorname{Cat}(\\boldsymbol \\theta)\\) that generalises the Bernoulli distribution \\(\\operatorname{Ber}(\\theta)\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "06-multivariate.html#multinomial-distribution",
    "href": "06-multivariate.html#multinomial-distribution",
    "title": "6  Multivariate distributions",
    "section": "",
    "text": "Standard parametrisation\nA multinomial random variable \\(\\boldsymbol x\\) describes describes the allocation of \\(n\\) items to \\(K\\) classes. We write \\[\n\\boldsymbol x\\sim \\operatorname{Mult}(n, \\boldsymbol \\theta)\n\\] where the parameter vector \\(\\boldsymbol \\theta=(\\theta_1, \\ldots, \\theta_K)^T\\) specifies the probability of each of \\(K\\) classes, with \\(\\theta_k \\in [0,1]\\) and \\(\\boldsymbol \\theta^T \\mathbf 1_K = \\sum_{k=1}^K \\theta_k = 1\\). Thus there are \\(K-1\\) independent elements in \\(\\boldsymbol \\theta\\). The number of classes \\(K\\) is implicitly given by the dimension of the vector \\(\\boldsymbol \\theta\\). Each element of the vector \\(\\boldsymbol x= (x_1, \\ldots, x_K)^T\\) is an integer \\(x_k \\in \\{0, 1, \\ldots, n\\}\\) and \\(\\boldsymbol x\\) satisfies the constraint \\(\\boldsymbol x^T \\mathbf 1_K = \\sum_{k=1}^K x_k = n\\). Therefore the support of \\(\\boldsymbol x\\) is a \\(K-1\\) dimensional space and it notably depends on \\(n\\).\n\n\n\n\n\n\nFigure 6.1: Multinomial urn model.\n\n\n\nThe multinomial distribution is best illustrated by an urn model distributing \\(n\\) items into \\(K\\) bins where \\(\\boldsymbol \\theta\\) contains the corresponding bin probabilities (Figure 6.1).\nThe expected value is \\[\n\\operatorname{E}(\\boldsymbol x) = n \\boldsymbol \\theta\n\\] The covariance matrix is \\[\n\\operatorname{Var}(\\boldsymbol x) = n (\\operatorname{Diag}(\\boldsymbol \\theta) - \\boldsymbol \\theta\\boldsymbol \\theta^T)\n\\] The covariance matrix is singular by construction because of the dependencies among the elements of \\(\\boldsymbol x\\).\nThe corresponding pmf is \\[\np(\\boldsymbol x| \\boldsymbol \\theta) = W_K \\prod_{k=1}^K \\theta_k^{x_k}\n\\] The multinomial coefficient \\[\nW_K = \\binom{n}{x_1, \\ldots, x_K}\n\\] in the pmf accounts for the number of possible permutation of \\(n\\) items of \\(K\\) distinct types. Note that the multinomial coefficient \\(W_K\\) does not depend on \\(\\boldsymbol \\theta\\).\nWhile all \\(K\\) elements of \\(\\boldsymbol x\\) appear in the pmf recall that due the dependencies among the \\(x_k\\) the pmf is defined over a \\(K-1\\) dimensional support.\nFor \\(K=2\\) the multinomial distribution reduces to the binomial distribution (Section 5.1).\n\n\n\n\n\n\nTipR code\n\n\n\nThe pmf of the multinomial distribution is given by dmultinom(). The corresponding random number generator is rmultinom().\n\n\n\n\nMean parametrisation\nInstead of \\(\\boldsymbol \\theta\\) one may also use a mean parameter \\(\\boldsymbol \\mu\\), with elements \\(\\mu_k \\in [0,n]\\) and \\(\\boldsymbol \\mu^T \\mathbf 1_K = \\sum^{K}_{k=1}\\mu_k = n\\), so that \\[\n\\boldsymbol x\\sim \\operatorname{Mult}\\left(n, \\boldsymbol \\theta= \\frac{\\boldsymbol \\mu}{n}\\right)\n\\] The mean parameter \\(\\boldsymbol \\mu\\) can be obtained from \\(\\boldsymbol \\theta\\) and \\(n\\) by \\(\\boldsymbol \\mu= n \\boldsymbol \\theta\\). Note that the parameter space for \\(\\boldsymbol \\mu\\) and the support of \\(\\boldsymbol x\\) are both of dimension \\(K-1\\).\nThe mean and variance of the multinomial distribution expressed in terms of \\(\\boldsymbol \\mu\\) and \\(n\\) are \\[\n\\operatorname{E}(x) = \\boldsymbol \\mu\n\\] and \\[\n\\operatorname{Var}(x) = \\operatorname{Diag}(\\boldsymbol \\mu) - \\frac{\\boldsymbol \\mu\\boldsymbol \\mu^T}{n}\n\\] The covariance matrix is singular by construction because of the dependencies among the elements of \\(\\boldsymbol x\\).\n\n\nSpecial case: categorical distribution\nFor \\(n=1\\) the multinomial distribution reduces to the categorical distribution \\(\\operatorname{Cat}(\\boldsymbol \\theta)\\) which in turn is the multivariate generalisation of the Bernoulli distribution \\(\\operatorname{Ber}(\\theta)\\) from two classes to \\(K\\) classes.\nIf a random variable \\(\\boldsymbol x\\) follows the categorical distribution we write \\[\n\\boldsymbol x\\sim \\operatorname{Cat}(\\boldsymbol \\theta)\n\\] with class probabilities \\(\\boldsymbol \\theta\\) and \\(\\boldsymbol \\theta^T \\mathbf 1_K = 1\\). The support is \\(x_k \\in \\{0, 1\\}\\) and \\(\\boldsymbol x^T \\mathbf 1_K =1\\) and is a \\(K-1\\) dimensional space.\nThe random vector \\(\\boldsymbol x\\) takes the form of an indicator vector \\(\\boldsymbol x= (x_1, \\ldots, x_K)^T = (0, 0, \\ldots, 1, \\ldots, 0)^T\\) containing zeros everywhere except for a single element indicating the class to which the item has been allocated. This is called “one hot encoding”, as opposed to “integer encoding” (i.e. stating the class number directly).\nThe expected value is \\[\n\\operatorname{E}(\\boldsymbol x) = \\boldsymbol \\theta\n\\] The covariance matrix is \\[\n\\operatorname{Var}(\\boldsymbol x) = \\operatorname{Diag}(\\boldsymbol \\theta) - \\boldsymbol \\theta\\boldsymbol \\theta^T\n\\] The covariance matrix is singular because of the dependencies among the elements of \\(\\boldsymbol x\\).\nThe corresponding pmf is \\[\np(\\boldsymbol x| \\boldsymbol \\theta) = \\prod_{k=1}^K \\theta_k^{x_k} =\n\\begin{cases}\n   \\theta_k  & \\text{if } x_k = 1 \\\\\n\\end{cases}\n\\] Recall that the pmf is defined over the \\(K-1\\) dimensional support of \\(\\boldsymbol x\\).\nFor \\(K=2\\) the categorical distribution reduces to the Bernoulli \\(\\operatorname{Ber}(\\theta)\\) distribution, with \\(\\theta_1=\\theta\\), \\(\\theta_2=1-\\theta\\) and \\(x_1=x\\) and \\(x_2=1-x\\).\n\n\nConvolution property\nThe convolution of \\(n\\) multinomial distributions, each with identical bin probabilities \\(\\boldsymbol \\theta\\) but possibly different number of items \\(n_i\\), yields another multinomial distribution with the same parameter \\(\\boldsymbol \\theta\\): \\[\n\\sum_{i=1}^n \\operatorname{Mult}(n_i, \\boldsymbol \\theta) \\sim \\operatorname{Mult}\\left(\\sum_{i=1}^n n_i, \\boldsymbol \\theta\\right)\n\\]\nIt follows that the multinomial distribution with \\(n\\) items is the result of the convolution of \\(n\\) categorical distributions: \\[\n\\sum_{i=1}^n \\operatorname{Cat}(\\boldsymbol \\theta) \\sim \\operatorname{Mult}(n, \\boldsymbol \\theta)\n\\] Thus, repeating the same categorical trial \\(n\\) times and counting the total number of allocations in each bin yields a multinomial random variable.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "06-multivariate.html#dirichlet-distribution",
    "href": "06-multivariate.html#dirichlet-distribution",
    "title": "6  Multivariate distributions",
    "section": "6.2 Dirichlet distribution",
    "text": "6.2 Dirichlet distribution\nThe Dirichlet distribution \\(\\operatorname{Dir}(\\boldsymbol \\alpha)\\) is the multivariate generalisation of the beta distribution \\(\\operatorname{Beta}(\\alpha_1, \\alpha_2)\\) (Section 5.2) that is useful to model proportions or probabilities for \\(K\\geq 2\\) classes. It is named after Peter Gustav Lejeune Dirichlet (1805–1859).\nIt includes the uniform distribution over the \\(K-1\\) unit simplex as special case.\n\nStandard parametrisation\nA Dirichlet distributed random vector is denoted by \\[\n\\boldsymbol x\\sim \\operatorname{Dir}(\\boldsymbol \\alpha)\n\\] with shape parameter \\(\\boldsymbol \\alpha= (\\alpha_1,...,\\alpha_K)^T &gt;0\\) and \\(K\\geq 2\\). Let \\(m = \\boldsymbol \\alpha^T \\mathbf 1_K = \\sum^{K}_{k=1}\\alpha_k\\). The support of \\(\\boldsymbol x\\) is the \\(K-1\\) dimensional unit simplex given by \\(x_k \\in [0,1]\\) and \\(\\boldsymbol x^T \\mathbf 1_K = \\sum^{K}_{k=1} x_k = 1\\). Thus, the Dirichlet distribution is defined over a \\(K-1\\) dimensional space.\n\n\n\n\n\n\nFigure 6.2: Stick breaking visualisation of a Dirichlet random variable.\n\n\n\nA Dirichlet random variable can be visualised as breaking a unit stick into \\(K\\) individual pieces of lengths \\(x_1, x_2, \\ldots, x_K\\) adding up to one (Figure 6.2). Thus, the \\(x_k\\) may be used as the exclusive proportions or probabilities for \\(K\\) classes.\nThe mean is \\[\n\\operatorname{E}(\\boldsymbol x) = \\frac{\\boldsymbol \\alpha}{m }\n\\] and the variance is \\[\n\\operatorname{Var}\\left(\\boldsymbol x\\right) = \\frac{ m \\operatorname{Diag}(\\boldsymbol \\alpha)-\\boldsymbol \\alpha\\boldsymbol \\alpha^T}{m^2(m+1)}\n\\] The covariance matrix is singular by construction because of the dependencies among the elements of \\(\\boldsymbol x\\). In component notation it is \\[\n\\operatorname{Cov}(x_i,x_j) =  \\frac{[i=j] m \\alpha_i -  \\alpha_i \\alpha_j}{m^2(m+1)}\n\\] where the indicator function \\([i=j]\\) equals 1 if \\(i=j\\) and 0 otherwise.\nThe pdf of the Dirichlet distribution \\(\\operatorname{Dir}(\\boldsymbol \\alpha)\\) is \\[\np(\\boldsymbol x| \\boldsymbol \\alpha) = \\frac{1}{B(\\boldsymbol \\alpha)}  \\prod_{k=1}^K x_k^{\\alpha_k-1}\n\\] In this density the normalisation factor is given by the multivariate beta function \\[\nB(\\boldsymbol \\alpha) = \\frac{ \\prod_{k=1}^K \\Gamma(\\alpha_k) }{\\Gamma( \\sum_{k=1}^K \\alpha_k )}\n\\] For \\(K=2\\) it reduces to the conventional beta function \\(B(\\alpha_1, \\alpha_2)\\).\nWhile all \\(K\\) elements of \\(\\boldsymbol x\\) appear in the pdf recall that due the dependencies among the \\(x_k\\) the pdf is defined over a \\(K-1\\) dimensional support.\nFor \\(K=2\\) the Dirichlet distribution reduces to the beta distribution (Section 5.2).\n\n\n\n\n\n\nTipR code\n\n\n\nThe extraDistr package implements the Dirichlet distribution. The pmf of the Dirichlet distribution is given by extraDistr::ddirichlet(). The corresponding random number generator is extraDistr::rdirichlet().\n\n\n\n\nMean parametrisation\nInstead of employing \\(\\boldsymbol \\alpha\\) as parameter vector another useful reparametrisation of the Dirichlet distribution is in terms of a mean parameter \\(\\boldsymbol \\mu\\), with elements \\(\\mu_k \\in [0,1]\\) and \\(\\boldsymbol \\mu^T \\mathbf 1_K = \\sum^{K}_{k=1}\\mu_k = 1\\), and a concentration parameter \\(m &gt; 0\\) so that \\[\n\\boldsymbol x\\sim \\operatorname{Dir}(\\boldsymbol \\alpha=m \\boldsymbol \\mu)\n\\] The concentration and mean parameters can be obtained from \\(\\boldsymbol \\alpha\\) by \\(m = \\boldsymbol \\alpha^T \\mathbf 1_K\\) and \\(\\boldsymbol \\mu= \\boldsymbol \\alpha/m\\). The space of possible values for the mean parameter \\(\\boldsymbol \\mu\\) and the support of \\(\\boldsymbol x\\) are both of dimension \\(K-1\\).\nThe mean and variance of the Dirichlet distribution expressed in terms of \\(\\boldsymbol \\mu\\) and \\(m\\) are \\[\n\\operatorname{E}(\\boldsymbol x) = \\boldsymbol \\mu\n\\] and \\[\\operatorname{Var}\\left(\\boldsymbol x\\right) = \\frac{ \\operatorname{Diag}(\\boldsymbol \\mu)-\\boldsymbol \\mu\\boldsymbol \\mu^T}{m+1}\\] The covariance matrix is singular by construction because of the dependencies among the elements of \\(\\boldsymbol x\\). In component notation it is \\[\n\\begin{split}\n\\operatorname{Cov}(x_i,x_j) &=  \\frac{[i=j]\\mu_i -  \\mu_i \\mu_j}{m+1} \\\\\n&=\n\\begin{cases}\n\\mu_i(1-\\mu_i)/(m+1) & \\text{if $i=j$}\\\\\n-\\mu_i \\mu_j / (m+1) & \\text{if $i\\neq j$}\\\\\n\\end{cases}\\\\\n\\end{split}\n\\]\n\n\nSpecial case: symmetric Dirichlet distribution\nFor \\(\\boldsymbol \\alpha= \\alpha \\mathbf 1_K\\) the Dirichlet distribution becomes the symmetric beta distribution with a single shape parameters \\(\\alpha&gt;0\\). In mean parametrisation the symmetric Dirichlet distribution corresponds to \\(\\boldsymbol \\mu=\\mathbf 1_K/K\\) and \\(m=\\alpha K\\).\n\n\nSpecial case: uniform distribution\nFor \\(\\boldsymbol \\alpha= \\mathbf 1_K\\) the Dirichlet distribution becomes the uniform distribution over the \\(K-1\\) unit simplex with pdf \\(p(\\boldsymbol x)=1/\\Gamma(K)\\). In mean parametrisation the uniform distribution corresponds to \\(\\boldsymbol \\mu=\\mathbf 1_K/K\\) and \\(m=K\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "06-multivariate.html#sec-mvndist",
    "href": "06-multivariate.html#sec-mvndist",
    "title": "6  Multivariate distributions",
    "section": "6.3 Multivariate normal distribution",
    "text": "6.3 Multivariate normal distribution\nThe multivariate normal distribution \\(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) generalises the univariate normal distribution \\(N(\\mu, \\sigma^2)\\) (Section 5.3) from one to \\(d\\) dimensions.\nSpecial cases are the multivariate standard normal distribution \\(N(0, \\boldsymbol I)\\) and the multivariate delta distribution \\(\\delta(\\boldsymbol \\mu)\\).\n\nStandard parametrisation\nThe multivariate normal distribution \\(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) has a mean or location parameter \\(\\boldsymbol \\mu\\) (a \\(d\\) dimensional vector), a variance parameter \\(\\boldsymbol \\Sigma\\) (a \\(d \\times d\\) positive definite symmetric matrix) and support \\(\\boldsymbol x\\in \\mathbb{R}^d\\).\nIf a random vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T\\) follows a multivariate normal distribution we write \\[\n\\boldsymbol x\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\n\\] with mean \\[\n\\operatorname{E}(\\boldsymbol x) = \\boldsymbol \\mu\n\\] and variance \\[\n\\operatorname{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\n\\] In the above notation the dimension \\(d\\) is implicitly given by the dimensions of \\(\\boldsymbol \\mu\\) and \\(\\boldsymbol \\Sigma\\) but for clarity one often also writes \\(N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) to explicitly indicate the dimension.\nThe pdf is given by \\[\n\\begin{split}\np(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) &= \\det(2 \\pi \\boldsymbol \\Sigma)^{-1/2} \\exp\\left(-\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)   \\right)\\\\\n&= \\det(\\boldsymbol \\Sigma)^{-1/2} (2\\pi)^{-d/2}  e^{-\\Delta^2 /2}\\\\\n\\end{split}\n\\] Here \\(\\Delta^2 = (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)\\) is the squared Mahalanobis distance between \\(\\boldsymbol x\\) and \\(\\boldsymbol \\mu\\) taking into account the variance \\(\\boldsymbol \\Sigma\\). Note that this pdf is a joint pdf over the \\(d\\) elements \\(x_1, \\ldots, x_d\\) of the random vector \\(\\boldsymbol x\\).\nThe multivariate normal distribution is sometimes also used by specifying the precision matrix \\(\\boldsymbol \\Sigma^{-1}\\) instead of the variance \\(\\boldsymbol \\Sigma\\).\nFor \\(d=1\\) the random vector \\(\\boldsymbol x=x\\) is a scalar, \\(\\boldsymbol \\mu= \\mu\\), \\(\\boldsymbol \\Sigma= \\sigma^2\\) and thus the multivariate normal distribution reduces to the univariate normal distribution (Section 5.3).\n\n\n\n\n\n\nTipR code\n\n\n\nThe mnormt package implements the multivariate normal distribution. The function mnormt::dmnorm() provides the pdf and mnormt::pmnorm() returns the distribution function. The function mnormt::rmnorm() is the corresponding random number generator.\nThe mniw package also implements the multivariate normal distribution. The pdf of the Wishart distribution is given by mniw::dmNorm(). The corresponding random number generator is mniw::rmNorm().\n\n\n\n\nScale parametrisation\nIn the univariate case it is straightforward to use the standard deviation \\(\\sigma\\) as scale parameter instead of the variance \\(\\sigma^2\\), and similarly the inverse standard deviation \\(w=1/\\sigma\\) instead of the precision \\(\\sigma^{-2}\\). However, in the multivariate setting with a matrix variance parameter \\(\\boldsymbol \\Sigma\\) it is less obvious how to define a suitable matrix scale parameter.\nLet \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) be the eigendecomposition of the positive definite matrix \\(\\boldsymbol \\Sigma\\). Then \\(\\boldsymbol \\Sigma^{1/2} = \\boldsymbol U\\boldsymbol \\Lambda^{1/2} \\boldsymbol U^T\\) is the principal matrix square root and \\(\\boldsymbol \\Sigma^{-1/2} = \\boldsymbol U\\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T\\) the inverse principal matrix square root. Furthermore, let \\(\\boldsymbol Q\\) be an arbitrary orthogonal matrix with \\(\\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol I\\).\nThen \\(\\boldsymbol W= \\boldsymbol Q\\boldsymbol \\Sigma^{-1/2}\\) is called a whitening matrix based on \\(\\boldsymbol \\Sigma\\) and \\(\\boldsymbol L= \\boldsymbol W^{-1}= \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q^T\\) is the corresponding inverse whitening matrix. By construction, the matrix \\(\\boldsymbol L\\) provides a factorisation of the covariance matrix by \\(\\boldsymbol L\\boldsymbol L^T = \\boldsymbol \\Sigma\\). Similarly, \\(\\boldsymbol W\\) factorises the precision matrix by \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1}\\). The two matrices thus provide the basis for the scale parametrisation of the multivariate normal distribution.\nSpecifically, the matrix \\(\\boldsymbol L\\) is used in place of \\(\\boldsymbol \\Sigma\\) and plays the role of the matrix scale parameter (corresponding to \\(\\sigma\\) in the univariate setting) and \\(\\boldsymbol W\\) is used in place of the precision matrix \\(\\boldsymbol \\Sigma^{-1}\\) and plays the role of the inverse matrix scale parameter (corresponding to \\(1/\\sigma\\) in the univariate case). The determinants occurring in the multivariate normal pdf can be rewritten in terms of \\(\\boldsymbol L\\) and \\(\\boldsymbol W\\) using the identities \\(|\\det(\\boldsymbol W)|=\\det(\\boldsymbol \\Sigma)^{-1/2}\\) and \\(|\\det(\\boldsymbol L)|=\\det(\\boldsymbol \\Sigma)^{1/2}\\) as \\(\\det(\\boldsymbol Q) = \\pm 1\\).\nSince \\(\\boldsymbol Q\\) can be freely chosen the matrices \\(\\boldsymbol W\\) and \\(\\boldsymbol L\\) are not fully determined by \\(\\boldsymbol \\Sigma\\) alone but there is rotational freedom due to \\(\\boldsymbol Q\\). Standard choices are\n\n\\(\\boldsymbol Q^{\\text{ZCA}}=\\boldsymbol I\\) for ZCA-type factorisation with \\(\\boldsymbol W^{\\text{ZCA}}=\\boldsymbol \\Sigma^{-1/2}\\) and\n\\(\\boldsymbol Q^{\\text{PCA}}=\\boldsymbol U^T\\) for PCA-type factorisation with \\(\\boldsymbol W^{\\text{PCA}}=\\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T\\). Note that the matrix \\(\\boldsymbol U\\) is not unique because its columns (eigenvectors) can have different signs (directions), hence \\(\\boldsymbol W^{\\text{PCA}}\\) and \\(\\boldsymbol L^{\\text{PCA}}\\) are also not unique without further constraints, such as positive diagonal elements of the (inverse) whitening matrix.\nA third common choice is to compute \\(\\boldsymbol L\\) directly by Cholesky decomposition of \\(\\boldsymbol \\Sigma\\), which yields an \\(\\boldsymbol L^{\\text{Chol}}\\) (and also a \\(\\boldsymbol W^{\\text{Chol}}\\)) in the form of a lower-triangular matrix with a positive diagonal, and a corresponding underlying \\(\\boldsymbol Q^{\\text{Chol}}=(\\boldsymbol L^{\\text{Chol}})^T \\boldsymbol \\Sigma^{-1/2}\\).\n\nFinally, the whitening matrix \\(\\boldsymbol W\\) and its inverse may also be constructed from the correlation matrix \\(\\boldsymbol P\\) and the diagonal matrix containing the variances \\(\\boldsymbol V\\) (with \\(\\boldsymbol \\Sigma= \\boldsymbol V^{1/2} \\boldsymbol P\\boldsymbol V^{1/2}\\)) in the form \\(\\boldsymbol W= \\boldsymbol Q\\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2}\\) and \\(\\boldsymbol L= \\boldsymbol V^{1/2}  \\boldsymbol P^{1/2} \\boldsymbol Q^T\\).\n\n\nSpecial case: multivariate standard normal distribution\nThe multivariate standard normal distribution \\(N(0, \\boldsymbol I)\\) has mean \\(\\boldsymbol \\mu=0\\) and variance \\(\\boldsymbol \\Sigma=\\boldsymbol I\\). The corresponding pdf is \\[\np(\\boldsymbol x) = (2\\pi)^{-d/2} e^{-\\boldsymbol x^T \\boldsymbol x/2 }\n\\] with the squared Mahalanobis distance reduced to \\(\\Delta^2=\\boldsymbol x^T \\boldsymbol x= \\sum_{i=1}^d x_i^2\\).\nThe density of the multivariate standard normal distribution is the product of the corresponding univariate standard normal densities \\[\np(\\boldsymbol x) = \\prod_{i=1}^d \\, (2\\pi)^{-1/2} e^{-x_i^2/2 }\n\\] and therefore the elements \\(x_i\\) of \\(\\boldsymbol x=(x_1, \\ldots, x_d)^T\\) are independent of each other.\n\n\nSpecial case: multivariate delta distribution\nThe multivariate delta distribution \\(\\delta(\\boldsymbol \\mu)\\) is obtained as the limit of \\(N(\\boldsymbol \\mu, \\varepsilon \\boldsymbol A)\\) for \\(\\varepsilon \\rightarrow 0\\) and where \\(\\boldsymbol A\\) is a positive definite matrix (e.g. \\(\\boldsymbol A=\\boldsymbol I\\)). Thus \\(\\delta(\\boldsymbol \\mu)\\) is a continuous distribution representing a point mass at \\(\\boldsymbol \\mu\\).\nThe corresponding pdf \\(\\delta(\\boldsymbol x| \\boldsymbol \\mu)\\) is called the multivariate Dirac delta function, even though it is not an ordinary function. It satisfies \\(\\delta(\\boldsymbol x| \\boldsymbol \\mu)=0\\) for all \\(\\boldsymbol x\\neq \\boldsymbol \\mu\\) with an infinite spike at \\(\\boldsymbol \\mu\\) but still integrates to one.\n\n\nLocation-scale transformation\nLet \\(\\boldsymbol W\\) be a whitening matrix for \\(\\boldsymbol \\Sigma\\) and \\(\\boldsymbol L\\) the corresponding inverse whitening matrix.\nIf \\(\\boldsymbol x\\sim  N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) then \\(\\boldsymbol y= \\boldsymbol W(\\boldsymbol x-\\boldsymbol \\mu)  \\sim N(0, \\boldsymbol I)\\). This location-scale transformation corresponds to centring and whitening (i.e. standardisation and decorrelation) of a multivariate normal random variable.\nConversely, if \\(\\boldsymbol y\\sim N(0, \\boldsymbol I)\\) then \\(\\boldsymbol x= \\boldsymbol \\mu+ \\boldsymbol L\\boldsymbol y\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\). This location-scale transformation generates the multivariate normal distribution from the multivariate standard normal distribution.\nNote that under the location-scale transformation \\(\\boldsymbol x= \\boldsymbol \\mu+ \\boldsymbol L\\boldsymbol y\\) with \\(\\operatorname{Var}(\\boldsymbol y)=\\boldsymbol I\\) we get \\(\\operatorname{Cov}(\\boldsymbol x, \\boldsymbol y) = \\boldsymbol L\\). This provides a means to choose between different (inverse) whitening transformation and the corresponding factorisations of \\(\\boldsymbol \\Sigma\\) and \\(\\boldsymbol \\Sigma^{-1}\\). For example, if positive correlation between corresponding elements in \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) is desired then the diagonal elements in \\(\\boldsymbol L\\) must be positive.\n\n\nConvolution property\nThe convolution of \\(n\\) independent, but not necessarily identical, multivariate normal distributions of the same dimension \\(d\\) results in another \\(d\\)-dimensional multivariate normal distribution with corresponding mean and variance: \\[\n\\sum_{i=1}^n N(\\boldsymbol \\mu_i, \\boldsymbol \\Sigma_i) \\sim N\\left( \\sum_{i=1}^n \\boldsymbol \\mu_i,  \\sum_{i=1}^n \\boldsymbol \\Sigma_i \\right)\n\\] Hence, any multivariate normal random variable can be constructed as the sum of \\(n\\) suitable independent multivariate normal random variables.\nSince \\(n\\) is an arbitrary positive integer the multivariate normal distribution is said to be infinitely divisible.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "06-multivariate.html#sec-wisdist",
    "href": "06-multivariate.html#sec-wisdist",
    "title": "6  Multivariate distributions",
    "section": "6.4 Wishart distribution",
    "text": "6.4 Wishart distribution\nThe Wishart distribution \\(\\operatorname{Wis}\\left(\\boldsymbol S, k\\right)\\) is a multivariate generalisation of the gamma distribution \\(\\operatorname{Gam}(\\alpha, \\theta)\\) (Section 5.4) from one to \\(d\\) dimensions.\n\nStandard parametrisation\nIf the symmetric random matrix \\(\\boldsymbol X\\) of dimension \\(d \\times d\\) is Wishart distributed we write \\[\n\\boldsymbol X\\sim  \\operatorname{Wis}\\left(\\boldsymbol S, k\\right)\n\\] where \\(\\boldsymbol S=(s_{ij})\\) is the scale parameter (a symmetric \\(d \\times d\\) positive definite matrix with elements \\(s_{ij}\\)). The dimension \\(d\\) is implicit in the scale parameter \\(\\boldsymbol S\\).\nThe shape parameter \\(k\\) takes on real values in the range \\(k &gt; d-1\\) and integer values in the range \\(k \\in {1, \\ldots, d-1}\\) for \\(d&gt;1\\). For \\(k&gt;d-1\\) the matrix \\(\\boldsymbol X\\) is positive definite and invertible (see also Section 6.5), otherwise \\(\\boldsymbol X\\) is singular and positive semi-definite.\nThe distribution has mean \\[\n\\operatorname{E}(\\boldsymbol X) = k \\boldsymbol S\n\\] and variances of the elements of \\(\\boldsymbol X\\) are \\[\n\\operatorname{Var}(x_{ij})  = k \\left(s^2_{ij}+s_{ii} s_{jj}  \\right)\n\\]\nThe pdf is (for \\(k&gt;d-1\\)) \\[\np(\\boldsymbol X| \\boldsymbol S, k) = \\frac{1}{\\Gamma_d(k/2) \\det(2 \\boldsymbol S)^{k/2}} \\det(\\boldsymbol X)^{(k-d-1)/2}\n\\exp\\left(-\\operatorname{Tr}(\\boldsymbol S^{-1}\\boldsymbol X)/2\\right)\n\\] This pdf is a joint pdf over the \\(d\\) diagonal elements \\(x_{ii}\\) and the \\(d(d-1)/2\\) off-diagonal elements \\(x_{ij}\\) of the symmetric random matrix \\(\\boldsymbol X\\).\nPart of the normalisation factor in the density is the multivariate gamma function \\[\n\\Gamma_d(x) = \\pi^{d (d-1)/4} \\prod_{j=1}^d  \\Gamma(x - (j-1)/2)\n\\] For \\(d=1\\) it reduces to the standard gamma function. The multivariate gamma function also occurs in densities of other matrix-variate distributions.\nIf \\(\\boldsymbol S\\) is a scalar rather than a matrix (and hence \\(d=1\\)) then the multivariate Wishart distribution reduces to the univariate Wishart aka gamma distribution (Section 5.4).\nThe Wishart distribution is closely related to the multivariate normal distribution with mean zero. Specifically, if \\(\\boldsymbol z\\sim N(0, \\boldsymbol S)\\) then \\(\\boldsymbol z\\boldsymbol z^T \\sim  \\operatorname{Wis}(\\boldsymbol S, 1)\\).\n\n\n\n\n\n\nTipR code\n\n\n\nThe mniw package implements the Wishart distribution. The pdf of the Wishart distribution is given by mniw::dwish(). The corresponding random number generator is mniw::rwish().\n\n\n\n\nMean parametrisation\nIt is useful to employ the Wishart distribution in mean parametrisation \\[\n\\operatorname{Wis}\\left(\\boldsymbol S= \\frac{\\boldsymbol M}{k}, k \\right)\n\\] with parameters \\(\\boldsymbol M= k \\boldsymbol S\\) and \\(k\\). In this parametrisation the mean is \\[\n\\operatorname{E}(\\boldsymbol X) = \\boldsymbol M= (\\mu_{ij})\n\\] and variances of the elements of \\(\\boldsymbol X\\) are \\[\n\\operatorname{Var}(x_{ij})  = \\frac{ \\mu^2_{ij}+\\mu_{ii}\\mu_{jj} }{k}\n\\]\n\n\nSpecial case: standard Wishart distribution\nFor \\(\\boldsymbol S=\\boldsymbol I\\) the Wishart distribution reduces to the standard Wishart distribution \\[\n\\boldsymbol X\\sim  \\operatorname{Wis}\\left(\\boldsymbol I, k\\right)\n\\] with a single shape parameter \\(k\\). The mean is \\[\n\\operatorname{E}(\\boldsymbol X) = k \\boldsymbol I\n\\] and variances of the elements of \\(\\boldsymbol X\\) are \\[\n\\operatorname{Var}(x_{ij})  = \\begin{cases}\n2k & \\text{if $i=j$}\\\\\nk & \\text{if $i\\neq j$}\\\\\n\\end{cases}\n\\] The pdf is (for \\(k&gt; d-1\\)) \\[\np(\\boldsymbol X| k) = \\frac{1}{\\Gamma_d(k/2) 2^{d k/2}} \\det(\\boldsymbol X)^{(k-d-1)/2}\n\\exp\\left(-\\operatorname{Tr}(\\boldsymbol X)/2\\right)\n\\]\nThe standard Wishart distribution is closely related to the standard multivariate normal distribution with mean zero. Specifically, if \\(\\boldsymbol z\\sim N(0, \\boldsymbol I)\\) then \\(\\boldsymbol z\\boldsymbol z^T \\sim  \\operatorname{Wis}(\\boldsymbol I, 1)\\).\nThe Bartlett decomposition of the standard multivariate Wishart \\(\\operatorname{Wis}(\\boldsymbol I, k)\\) distribution for any real \\(k &gt; d-1\\) is obtained by Cholesky factorisation of the random matrix \\(\\boldsymbol X= \\boldsymbol Z\\boldsymbol Z^T\\). By construction \\(\\boldsymbol Z\\) is a lower-triangular matrix with positive diagonal elements \\(z_{ii}\\) and lower off-diagonal elements \\(z_{ij}\\) with \\(i&gt;j\\) and \\(i,j \\in \\{1, \\ldots, d\\}\\). The corresponding upper off-diagonal elements are set to zero (\\(z_{ji}=0\\)).\nThe \\(d(d+1)/2\\) elements of \\(\\boldsymbol Z\\) are independent and allow to generate a standard Wishart variate as follows:\n\nthe squared diagonal elements follow a univariate standard Wishart distribution \\(z_{ii}^2 \\sim \\operatorname{Wis}(1, k-i+1)\\) and\nthe off-diagonal elements follow the univariate standard normal distribution \\(z_{ij}\\sim N(0,1)\\).\nThen \\(\\boldsymbol X= \\boldsymbol Z\\boldsymbol Z^T \\sim \\operatorname{Wis}(\\boldsymbol I, k)\\).\n\n\n\nScale transformation\nIf \\(\\boldsymbol X\\sim \\operatorname{Wis}(\\boldsymbol S, k)\\) then the scaled symmetric random matrix \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T\\) is also Wishart distributed with \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T \\sim \\operatorname{Wis}(\\boldsymbol A\\boldsymbol S\\boldsymbol A^T, k)\\) where the matrix \\(\\boldsymbol A\\) must be full rank and \\(\\boldsymbol A\\boldsymbol S\\boldsymbol A^T\\) remains positive definite. The matrix \\(\\boldsymbol A\\) may be rectangular, hence the size of \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T\\) and \\(\\boldsymbol A\\boldsymbol S\\boldsymbol A^T\\) may be smaller compared to \\(\\boldsymbol X\\) and \\(\\boldsymbol S\\).\nThe transformations between the Wishart distribution and the standard Wishart distribution are two important special cases:\n\nWith \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol S^{-1}\\) and \\(\\boldsymbol X\\sim \\operatorname{Wis}(\\boldsymbol S, k)\\) then \\(\\boldsymbol Y= \\boldsymbol W\\boldsymbol X\\boldsymbol W^T  \\sim \\operatorname{Wis}(\\boldsymbol I, k)\\) as \\(\\boldsymbol W\\boldsymbol S\\boldsymbol W^T=\\boldsymbol I\\). This transformation reduces the Wishart distribution to the standard Wishart distribution.\nConversely, with \\(\\boldsymbol L\\boldsymbol L^T = \\boldsymbol S\\) and \\(\\boldsymbol Y\\sim \\operatorname{Wis}(\\boldsymbol I, k)\\) then \\(\\boldsymbol X= \\boldsymbol L\\boldsymbol Y\\boldsymbol L^T \\sim \\operatorname{Wis}(\\boldsymbol S, k)\\) as \\(\\boldsymbol L\\boldsymbol I\\boldsymbol L^T=\\boldsymbol S\\). This transformation generates the Wishart distribution from the standard Wishart distribution.\n\n\n\nConvolution property\nThe convolution of \\(n\\) Wishart distributions with the same scale parameter \\(\\boldsymbol S\\) but possible different shape parameters \\(k_i\\) yields another Wishart distribution: \\[\n\\sum_{i=1}^n \\operatorname{Wis}(\\boldsymbol S, k_i) \\sim \\operatorname{Wis}\\left(\\boldsymbol S, \\sum_{i=1}^n k_i\\right)\n\\] Note that the shape parameter \\(k\\) is restricted to be an integer in the range \\(1, \\ldots, d-1\\) for \\(d&gt;1\\) but is a real number in the range \\(k&gt; d-1\\). Thus, if the \\(k_i\\) are all valid shape parameters (for dimension \\(d\\)) then \\(\\sum_{i=1}^n k_i\\) is also a valid shape parameter.\nDue the partial restriction of the shape parameter \\(k\\) to integer values the multivariate Wishart distribution is not infinitely divisible for \\(d&gt;1\\).\nThe above includes the following construction of the multivariate Wishart distribution \\(\\operatorname{Wis}(S, k)\\) for integer-valued \\(k\\). The sum of \\(k\\) independent Wishart random variables \\(\\operatorname{Wis}(\\boldsymbol S, 1)\\) with one degree of freedom and identical scale parameter yields a Wishart random variable \\(\\operatorname{Wis}(\\boldsymbol S, k)\\) with degree of freedom \\(k\\) and the same scale parameter. Thus, if \\(\\boldsymbol z_1,\\boldsymbol z_2,\\dots,\\boldsymbol z_k\\sim N(0,\\boldsymbol S)\\) are \\(k\\) independent samples from \\(N(0,\\boldsymbol S)\\) then \\(\\sum_{i_1}^k \\boldsymbol z_i \\boldsymbol z_i^T \\sim  \\operatorname{Wis}(\\boldsymbol S, k)\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "06-multivariate.html#sec-iwdist",
    "href": "06-multivariate.html#sec-iwdist",
    "title": "6  Multivariate distributions",
    "section": "6.5 Inverse Wishart distribution",
    "text": "6.5 Inverse Wishart distribution\nThe inverse Wishart distribution \\(\\operatorname{IWis}\\left(\\boldsymbol \\Psi, k\\right)\\) is a multivariate generalisation of the inverse gamma distribution \\(\\operatorname{IGam}(\\alpha, \\beta)\\) (Section 5.5) from one to \\(d\\) dimensions. It is linked to the Wishart distribution \\(\\operatorname{Wis}(\\boldsymbol S, k)\\) (Section 6.4).\n\nStandard parametrisation\nA symmetric positive definite random matrix \\(\\boldsymbol X\\) of dimension \\(d \\times d\\) following an inverse Wishart distribution is denoted by \\[\n\\boldsymbol X\\sim \\operatorname{IWis}\\left(\\boldsymbol \\Psi, k\\right)\n\\] where \\(\\boldsymbol \\Psi= (\\psi_{ij})\\) is the scale parameter (a \\(d \\times d\\) positive definite symmetric matrix) and \\(k&gt; d-1\\) is the shape parameter. The dimension \\(d\\) is implicit in the scale parameter \\(\\boldsymbol \\Psi\\).\nThe mean is (for \\(k &gt; d+1\\)) \\[\n\\operatorname{E}(\\boldsymbol X) = \\frac{\\boldsymbol \\Psi}{k-d-1}\n\\] and the variances of elements of \\(\\boldsymbol X\\) are (for \\(k &gt; d+3\\)) \\[\n\\operatorname{Var}(x_{ij})= \\frac{ (k-d-1) \\, \\psi_{ii} \\psi_{jj} + (k-d+1)\\, \\psi_{ij}^2 }{ (k-d)  (k-d-3) (k-d-1)^2  }\n\\]\nThe inverse Wishart distribution \\(\\operatorname{IWis}\\left(\\boldsymbol \\Psi, k\\right)\\) has pdf \\[\np(\\boldsymbol X| \\boldsymbol \\Psi, k) = \\frac{ \\det(\\boldsymbol \\Psi/2)^{k/2}  }{\\Gamma_d(k/2) } \\det(\\boldsymbol X)^{-(k+d+1)/2}\n\\exp\\left(-\\operatorname{Tr}(\\boldsymbol \\Psi\\boldsymbol X^{-1})/2\\right)\n\\] As with the Wishart distribution his pdf is a joint pdf over the \\(d\\) diagonal elements \\(x_{ii}\\) and the \\(d(d-1)/2\\) off-diagonal elements \\(x_{ij}\\) of the symmetric random matrix \\(\\boldsymbol X\\).\nIf \\(\\boldsymbol \\Psi\\) is a scalar \\(\\psi\\) (and \\(d=1\\)) then the multivariate inverse Wishart distribution reduces to the univariate inverse Wishart distribution (Section 5.5).\n\n\n\n\n\n\nTipR code\n\n\n\nThe mniw package implements the inverse Wishart distribution. The pdf of the inverse Wishart distribution is given by mniw::diwish(). The corresponding random number generator is mniw::riwish().\n\n\n\nRelation to the Wishart distribution\nThe inverse Wishart distribution is closely linked to the Wishart distribution. Assume that the random variable \\(\\boldsymbol Y\\) (positive definite and symmetric) follows a Wishart distribution with \\[\n\\boldsymbol Y\\sim \\operatorname{Wis}\\left(\\boldsymbol S, k \\right)\n\\] then the inverse random variable \\(\\boldsymbol X= \\boldsymbol Y^{-1}\\) (positive definite and symmetric) follows an inverse Wishart distribution with inverted scale parameter \\[\n\\boldsymbol X= \\boldsymbol Y^{-1} \\sim \\operatorname{IWis}\\left(\\boldsymbol \\Psi= \\boldsymbol S^{-1}, k\\right)\n\\] where \\(k\\) is the shared shape parameter, \\(\\boldsymbol S\\) the scale parameter of the Wishart distribution and \\(\\boldsymbol \\Psi\\) the scale parameter of the inverse Wishart distribution.\nCorrespondingly, the density \\(p_{\\boldsymbol X}(\\boldsymbol X| \\boldsymbol \\Psi, k)\\) of the inverse Wishart distribution is obtained from the density \\(p_{\\boldsymbol Y}(\\boldsymbol Y|\\boldsymbol S, k)\\) of the Wishart distribution via \\[\np_{\\boldsymbol X}(\\boldsymbol X| \\boldsymbol \\Psi, k) = \\det(\\boldsymbol X)^{-d-1} p_{\\boldsymbol Y}\\left( \\boldsymbol X^{-1} | \\boldsymbol S=\\boldsymbol \\Psi^{-1}, k \\right)\n\\] The exponent \\(-d-1\\) in the Jacobian determinant for matrix inversion arises because \\(\\boldsymbol X\\) and \\(\\boldsymbol Y\\) are symmetric (for non-symmetric matrices the exponent is \\(-2d\\)).\n\n\n\nMean parametrisation\nInstead of \\(\\boldsymbol \\Psi\\) and \\(k\\) we may also equivalently use \\(\\boldsymbol M= \\boldsymbol \\Psi/(k-d-1)\\) and \\(\\kappa = k-d-1\\) as parameters for the inverse Wishart distribution, so that \\[\n\\boldsymbol X\\sim \\operatorname{IWis}\\left(\\boldsymbol \\Psi=  \\kappa  \\boldsymbol M\\, , \\, k= \\kappa+d+1\\right)\n\\] with mean (for \\(\\kappa &gt; 0\\)) \\[\n\\operatorname{E}(\\boldsymbol X) =\\boldsymbol M\n\\] and variances (for \\(\\kappa &gt;2)\\) \\[\n\\operatorname{Var}(x_{ij})= \\frac{\\kappa \\, \\mu_{ii} \\mu_{jj} +(\\kappa+2) \\, \\mu_{ij}^2   }{(\\kappa + 1)(\\kappa-2)}\n\\]\nFor \\(\\boldsymbol M\\) equal to scalar \\(\\mu\\) with \\(d=1\\) the above reduces to the univariate inverse Wishart distribution in mean parametrisation.\n\n\nBiased mean parametrisation\nUsing \\(\\boldsymbol T= (t_{ij}) = \\boldsymbol \\Psi/(k-d+1) = \\boldsymbol \\Psi/\\nu\\) as biased mean parameter together with \\(\\nu=k-d+1\\) we arrive at the biased mean parametrisation \\[\n\\boldsymbol X\\sim \\operatorname{IWis}\\left(\\boldsymbol \\Psi=  \\nu \\boldsymbol T\\, , \\, k=\\nu+d-1\\right)\n\\]\nThe corresponding mean is (for \\(\\nu &gt; 2\\)) \\[\n\\operatorname{E}(\\boldsymbol X) = \\frac{\\nu}{\\nu-2} \\boldsymbol T= \\boldsymbol M\n\\] and the variances of elements of \\(\\boldsymbol X\\) are (for \\(\\nu &gt; 4\\)) \\[\n\\operatorname{Var}(x_{ij})= \\left(\\frac{\\nu}{\\nu-2}\\right)^2 \\, \\frac{    (\\nu-2) \\, t_{ii} t_{jj} + \\nu \\, t_{ij}^2  }{(\\nu-1)(\\nu-4)}\n\\] As \\(\\boldsymbol T= \\boldsymbol M(\\nu-2)/\\nu\\) for large \\(\\nu\\) the parameter \\(\\boldsymbol T\\) will become identical to the true mean \\(\\boldsymbol M\\).\nFor \\(\\boldsymbol T\\) equal to scalar \\(\\tau^2\\) with \\(d=1\\) the above reduces to the univariate inverse Wishart distribution in biased mean parametrisation.\n\n\nScale transformation\nIf \\(\\boldsymbol X\\sim \\operatorname{IWis}(\\boldsymbol \\Psi, k)\\) then the scaled symmetric random matrix \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T\\) is also inverse Wishart distributed with \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T \\sim \\operatorname{IWis}(\\boldsymbol A\\boldsymbol \\Psi\\boldsymbol A^T, k)\\) where the matrix \\(\\boldsymbol A\\) has full rank and both \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T\\) and \\(\\boldsymbol A\\boldsymbol \\Psi\\boldsymbol A^T\\) remain positive definite. The matrix \\(\\boldsymbol A\\) may be rectangular, hence the size of \\(\\boldsymbol A\\boldsymbol X\\boldsymbol A^T\\) and \\(\\boldsymbol A\\boldsymbol \\Psi\\boldsymbol A^T\\) may be smaller compared to \\(\\boldsymbol X\\) and \\(\\boldsymbol \\Psi\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "06-multivariate.html#sec-mvtdist",
    "href": "06-multivariate.html#sec-mvtdist",
    "title": "6  Multivariate distributions",
    "section": "6.6 Multivariate \\(t\\)-distribution",
    "text": "6.6 Multivariate \\(t\\)-distribution\nThe multivariate \\(t\\)-distribution \\(t_{\\nu}(\\boldsymbol \\mu, \\boldsymbol T)\\) is a multivariate generalisation of the location-scale \\(t\\)-distribution \\(t_{\\nu}(\\mu, \\tau^2)\\) (Section 5.6) from one to \\(d\\) dimensions. It is a generalisation of the multivariate normal distribution \\(N(\\boldsymbol \\mu, \\boldsymbol T)\\) (Section 6.3) with an additional parameter \\(\\nu &gt; 0\\) (degrees of freedom) controlling the probability mass in the tails.\nSpecial cases include the multivariate standard \\(t\\)-distribution \\(t_{\\nu}(0, \\boldsymbol I)\\), the multivariate normal distribution \\(N(\\boldsymbol \\mu, \\boldsymbol T)\\) and the multivariate Cauchy distribution \\(\\operatorname{Cau}(\\boldsymbol \\mu, \\boldsymbol T)\\).\n\nStandard parametrisation\nIf \\(\\boldsymbol x\\in \\mathbb{R}^d\\) is a multivariate \\(t\\)-distributed random variable we write \\[\n\\boldsymbol x\\sim t_{\\nu}(\\boldsymbol \\mu, \\boldsymbol T)\n\\] where the vector \\(\\boldsymbol \\mu\\) is the location parameter (a \\(d\\) dimensional vector) and the dispersion parameter \\(\\boldsymbol T\\) is a symmetric positive definite matrix of dimension \\(d \\times d\\). The dimension \\(d\\) is implicit in both parameters. The parameter \\(\\nu &gt; 0\\) prescribes the degrees of freedom. For small values of \\(\\nu\\) the distribution is heavy-tailed and as a result only moments of order smaller than \\(\\nu\\) are finite and defined.\nThe mean is (for \\(\\nu&gt;1\\)) \\[\n\\operatorname{E}(\\boldsymbol x) = \\boldsymbol \\mu\n\\] and the variance (for \\(\\nu&gt;2\\)) \\[\n\\operatorname{Var}(\\boldsymbol x) = \\frac{\\nu}{\\nu-2} \\boldsymbol T\n\\]\nThe pdf of \\(t_{\\nu}(\\boldsymbol \\mu, \\boldsymbol T)\\) is \\[\np(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol T, \\nu) = \\det(\\boldsymbol T)^{-1/2}\n\\frac{\\Gamma(\\frac{\\nu+d}{2})} { (\\pi\\nu)^{d/2}   \\,\\Gamma(\\frac{\\nu}{2})}\n\\left(1+ \\frac{\\Delta^2}{\\nu}  \\right)^{-(\\nu+d)/2}\n\\] with \\(\\Delta^2 = (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol T^{-1} (\\boldsymbol x-\\boldsymbol \\mu)\\) the squared Mahalanobis distance between \\(\\boldsymbol x\\) and \\(\\boldsymbol \\mu\\). Note that this pdf is a joint pdf over the \\(d\\) elements \\(x_1, \\ldots, x_d\\) of the random vector \\(\\boldsymbol x\\).\nFor \\(d=1\\) the random vector \\(\\boldsymbol x=x\\) is a scalar, \\(\\boldsymbol \\mu= \\mu\\), \\(\\boldsymbol T= \\tau^2\\) and thus the multivariate \\(t\\)-distribution reduces to the location-scale \\(t\\)-distribution (Section 5.6).\n\n\n\n\n\n\nTipR code\n\n\n\nThe mnormt package implements the multivariate \\(t\\)-distribution. The function mnormt::dmt() provides the pdf and mnormt::pmt() returns the distribution function. The function mnormt::rmt() is the corresponding random number generator.\n\n\n\n\nScale parametrisation\nThe multivariate \\(t\\)-distribution, like the multivariate distribution, can also be represented with a matrix scale parameter \\(\\boldsymbol L\\) in place of a matrix dispersion parameter \\(\\boldsymbol T\\).\nLet \\(\\boldsymbol L\\) be a matrix scale parameter such that \\(\\boldsymbol L\\boldsymbol L^T = \\boldsymbol T\\) and \\(\\boldsymbol W=\\boldsymbol L^{-1}\\) be the corresponding inverse matrix scale parameter with \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol T^{-1}\\). By construction \\(|\\det(\\boldsymbol W)|=\\det(\\boldsymbol T)^{-1/2}\\) and \\(|\\det(\\boldsymbol L)|=\\det(\\boldsymbol T)^{1/2}\\).\nNote that \\(\\boldsymbol T\\) alone does not fully determine \\(\\boldsymbol L\\) and \\(\\boldsymbol W\\) due to rotational freedom, see the discussion in Section 6.3 for details.\n\n\nSpecial case: multivariate standard \\(t\\)-distribution\nWith \\(\\boldsymbol \\mu=0\\) and \\(\\boldsymbol T=\\boldsymbol I\\) the multivariate \\(t\\)-distribution reduces to the multivariate standard \\(t\\)-distribution \\(t_{\\nu}(0,\\boldsymbol I)\\). It is a generalisation of the multivariate standard normal distribution \\(N(0,\\boldsymbol I)\\) to allow for heavy tails.\nThe distribution has mean \\(\\operatorname{E}(\\boldsymbol x)=0\\) (for \\(\\nu&gt;1\\)) and variance \\(\\operatorname{Var}(\\boldsymbol x)=\\frac{\\nu}{\\nu-2}\\boldsymbol I\\) (for \\(\\nu&gt;2\\)).\nThe pdf of \\(t_{\\nu}(0,\\boldsymbol I)\\) is \\[\np(\\boldsymbol x| \\nu) =\n\\frac{\\Gamma(\\frac{\\nu+d}{2})} { (\\pi \\nu)^{d/2}   \\,\\Gamma(\\frac{\\nu}{2})}\n\\left(1+ \\frac{  \\boldsymbol x^T \\boldsymbol x}{\\nu}  \\right)^{-(\\nu+d)/2}\n\\] with the squared Mahalanobis distance reducing to \\(\\Delta^2=\\boldsymbol x^T \\boldsymbol x\\).\nFor scalar \\(x\\) (and hence \\(d=1\\)) the multivariate standard \\(t\\)-distribution reduces to the Student’s \\(t\\)-distribution \\(t_{\\nu}=t_{\\nu}(0,1)\\).\nUnlike the multivariate standard normal distribution, the density of the multivariate standard \\(t\\)-distribution cannot be written as product of corresponding univariate standard densities.\n\n\nSpecial case: multivariate normal distribution\nFor \\(\\nu \\rightarrow \\infty\\) the multivariate \\(t\\)-distribution \\(t_{\\nu}(\\boldsymbol \\mu, \\boldsymbol T)\\) reduces to the multivariate normal distribution \\(N(\\boldsymbol \\mu, \\boldsymbol T)\\) (Section 6.3). Correspondingly, for \\(\\nu \\rightarrow \\infty\\) the multivariate standard \\(t\\)-distribution \\(t_{\\nu}(0,\\boldsymbol I)\\) becomes equal to the multivariate standard normal distribution \\(N(0,\\boldsymbol I)\\).\nThis can be seen from the corresponding limits of the two factors in the pdf of the multivariate \\(t\\)-distribution that depend on \\(\\nu\\):\n\nFollowing Sterling’s approximation for large \\(x\\) we can approximate \\(\\log \\Gamma(x) \\approx (x-1) \\log(x-1)\\). For large \\(\\nu\\) this implies that \\[\\frac{\\Gamma((\\nu+d)/2)} {(\\pi\\nu)^{d/2}  \\,\\Gamma(\\nu/2)} \\rightarrow  (2\\pi)^{-d/2}\\]\nFor small \\(x\\) we can approximate \\(\\log(1+x) \\approx x\\). Thus for large \\(\\nu \\gg d\\) (and hence small \\(\\Delta^2 / \\nu\\)) this yields \\((\\nu+d) \\log(1+ \\Delta^2 / \\nu) \\rightarrow \\Delta^2\\) and hence \\(\\left(1+ \\Delta^2 / \\nu \\right)^{-(\\nu+d)/2} \\rightarrow e^{-\\Delta^2/2}\\).\n\nHence, the pdf of \\(t_{\\infty}(\\boldsymbol \\mu, \\boldsymbol T)\\) is the multivariate normal pdf \\[\np(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol T, \\nu=\\infty) =\n\\det(\\boldsymbol T)^{-1/2}\n(2\\pi)^{-d/2}\ne^{-\\Delta^2/2}\n\\]\n\n\nSpecial case: multivariate Cauchy distribution\nFor \\(\\nu=1\\) the multivariate \\(t\\)-distribution becomes the multivariate Cauchy distribution \\(\\operatorname{Cau}(\\boldsymbol \\mu, \\boldsymbol T)=t_{1}(\\boldsymbol \\mu, \\boldsymbol T)\\).\nIts mean, variance and other higher moments are all undefined.\nIt has pdf \\[\np(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol T) =\n\\det(\\boldsymbol T)^{-1/2}\n\\Gamma\\left(\\frac{d+1}{2}\\right)\n\\left(  \\pi (1+ \\Delta^2 ) \\right)^{-(d+1)/2}\n\\]\nFor scalar \\(x\\) (and hence \\(d=1\\)) the multivariate Cauchy distribution \\(\\operatorname{Cau}(\\boldsymbol \\mu, \\boldsymbol T)\\) reduces to the univariate Cauchy distribution \\(\\operatorname{Cau}(\\mu, \\tau^2)\\).\n\n\nSpecial case: multivariate standard Cauchy distribution\nThe multivariate standard Cauchy distribution \\(\\operatorname{Cau}(0, \\boldsymbol I)=t_{1}(0, \\boldsymbol I)\\) is obtained by setting \\(\\boldsymbol \\mu=0\\) and \\(\\boldsymbol T=\\boldsymbol I\\) in the multivariate Cauchy distribution or, equivalently, by setting \\(\\nu=1\\) in the multivariate standard \\(t\\)-distribution.\nIt has pdf \\[\np(\\boldsymbol x) =\n\\Gamma\\left(\\frac{d+1}{2}\\right)\n\\left(  \\pi (1+ \\boldsymbol x^T \\boldsymbol x) \\right)^{-(d+1)/2}\n\\]\nFor scalar \\(x\\) (and hence \\(d=1\\)) the multivariate standard Cauchy distribution \\(\\operatorname{Cau}(0, \\boldsymbol I)\\) reduces to the standard univariate Cauchy distribution \\(\\operatorname{Cau}(0, 1)\\).\n\n\nLocation-scale transformation\nLet \\(\\boldsymbol L\\) be a scale matrix for \\(\\boldsymbol T\\) and \\(\\boldsymbol W\\) the corresponding inverse scale matrix.\nIf \\(\\boldsymbol x\\sim  t_{\\nu}(\\boldsymbol \\mu, \\boldsymbol T)\\) then \\(\\boldsymbol y= \\boldsymbol W(\\boldsymbol x-\\boldsymbol \\mu)  \\sim t_{\\nu}(0, \\boldsymbol I)\\). This location-scale transformation reduces a multivariate \\(t\\)-distributed random variable to a standard multivariate \\(t\\)-distributed random variable.\nConversely, if \\(\\boldsymbol y\\sim t_{\\nu}(0, \\boldsymbol I)\\) then \\(\\boldsymbol x= \\boldsymbol \\mu+ \\boldsymbol L\\boldsymbol y\\sim t_{\\nu}(\\boldsymbol \\mu, \\boldsymbol T)\\). This location-scale transformation generates the multivariate \\(t\\)-distribution from the multivariate standard \\(t\\)-distribution.\nNote that for \\(\\nu &gt; 2\\) under the location-scale transformation \\(\\boldsymbol x= \\boldsymbol \\mu+ \\boldsymbol L\\boldsymbol y\\) with \\(\\operatorname{Var}(\\boldsymbol y)=\\nu/(\\nu-2) \\boldsymbol I\\) we get \\(\\operatorname{Cov}(\\boldsymbol x, \\boldsymbol y) = \\nu/(\\nu-2)\\boldsymbol L\\). This provides a means to choose between different factorisations of \\(\\boldsymbol T\\) and \\(\\boldsymbol T^{-1}\\). For example, if positive correlation between corresponding elements in \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) is desired then the diagonal elements in \\(\\boldsymbol L\\) must be positive.\nFor the special case of the multivariate Cauchy distribution (corresponding to \\(\\nu=1\\)) similar relations hold between it and the multivariate standard Cauchy distribution. If \\(\\boldsymbol x\\sim  \\operatorname{Cau}(\\boldsymbol \\mu, \\boldsymbol T)\\) then \\(\\boldsymbol y= \\boldsymbol W(\\boldsymbol x-\\boldsymbol \\mu)  \\sim \\operatorname{Cau}(0, \\boldsymbol I)\\). Conversely, if \\(\\boldsymbol y\\sim \\operatorname{Cau}(0, \\boldsymbol I)\\) then \\(\\boldsymbol x= \\boldsymbol \\mu+ \\boldsymbol L\\boldsymbol y\\sim \\operatorname{Cau}(\\boldsymbol \\mu, \\boldsymbol T)\\).\n\n\nConvolution property\nThe multivariate \\(t\\)-distribution is not generally closed under convolution, with the exception of two special cases, the multivariate normal distribution (\\(\\nu=\\infty\\)), see Section 6.3, and the multivariate Cauchy distribution (\\(\\nu=1\\)) with the additional restriction that the dispersion parameters are proportional.\nFor the Cauchy distribution with \\(\\boldsymbol T_i= a_i^2 \\boldsymbol T\\), where \\(a_i&gt;0\\) are positive scalars, \\[\n\\sum_{i=1}^n  \\operatorname{Cau}(\\boldsymbol \\mu_i, a_i^2 \\boldsymbol T)  \\sim\n  \\operatorname{Cau}\\left( \\sum_{i=1}^n \\boldsymbol \\mu_i, \\left(\\sum_{i=1}^n a_i\\right)^2  \\boldsymbol T\\right)\n\\]\n\n\nMultivariate \\(t\\)-distribution as compound distribution\nThe multivariate \\(t\\)-distribution can be obtained as mixture of multivariate normal distributions with identical mean and varying covariance matrix. Specifically, let \\(z\\) be a univariate inverse Wishart random variable \\[\nz \\sim  \\operatorname{IWis}(\\psi=\\nu, k=\\nu) =\n        \\operatorname{IGam}\\left(\\alpha=\\frac{\\nu}{2}, \\beta=\\frac{\\nu}{2}\\right)\n\\] and let \\(\\boldsymbol x| z\\) be multivariate normal \\[\n\\boldsymbol x| z \\sim N(\\boldsymbol \\mu,\\boldsymbol \\Sigma= z \\boldsymbol T)\n\\] The resulting marginal (scale mixture) distribution for \\(\\boldsymbol x\\) is the multivariate \\(t\\)-distribution \\[\n\\boldsymbol x\\sim t_{\\nu}\\left(\\boldsymbol \\mu, \\boldsymbol T\\right)\n\\]\nAn alternative way to arrive at \\(t_{\\nu}\\left(\\boldsymbol \\mu, \\boldsymbol T\\right)\\) is to include \\(\\boldsymbol T\\) as parameter in the inverse Wishart distribution \\[\n\\boldsymbol Z\\sim \\operatorname{IWis}(\\boldsymbol \\Psi=\\nu \\boldsymbol T, k=\\nu+d-1)\n\\] and let \\[\n\\boldsymbol x| \\boldsymbol Z\\sim N(\\boldsymbol \\mu,\\boldsymbol \\Sigma= \\boldsymbol Z)\n\\] Note that \\(\\boldsymbol T\\) is now the biased mean parameter of the multivariate inverse Wishart distribution. This characterisation is useful in Bayesian analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "07-expfam.html",
    "href": "07-expfam.html",
    "title": "7  Exponential families",
    "section": "",
    "text": "7.1 Definition of an exponential family",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exponential families</span>"
    ]
  },
  {
    "objectID": "07-expfam.html#definition-of-an-exponential-family",
    "href": "07-expfam.html#definition-of-an-exponential-family",
    "title": "7  Exponential families",
    "section": "",
    "text": "Exponential tilting\nA distribution family \\(P(\\boldsymbol \\eta)\\) for a random variable \\(x\\) is an exponential family if it is generated by exponential tilting of a base distribution \\(B\\), resulting in a pdmf of the form \\[\n\\begin{split}\np(x|\\boldsymbol \\eta) &=  \\underbrace{\\vphantom{z(\\boldsymbol \\eta)} h(x) }_{\\text{base}}\n                \\underbrace{\\vphantom{z(\\boldsymbol \\eta)} e^{\\langle \\boldsymbol \\eta, \\boldsymbol t(x) \\rangle}}_{\\text{exponential tilt}}\n                / \\underbrace{z(\\boldsymbol \\eta)}_{\\text{normaliser}}\\\\\n       & =  h(x)\\, e^{\\langle \\boldsymbol \\eta, \\boldsymbol t(x) \\rangle -a(\\boldsymbol \\eta)}\\\\\n\\end{split}\n\\] where\n\n\\(\\boldsymbol t(x)\\) are the canonical statistics,\n\\(\\boldsymbol \\eta\\) are the canonical parameters,\n\\(h(x)\\) is a positive base function (typically unnormalised),\n\\(z(\\boldsymbol \\eta)\\) is the partition function and\n\\(a(\\boldsymbol \\eta) = \\log z(\\boldsymbol \\eta)\\) the corresponding log-partition function.\n\nThe base pdmf is obtained at \\(\\boldsymbol \\eta=0\\) yielding \\(b(x) = p(x | \\boldsymbol \\eta=0)  =   h(x) / z(0)\\). If \\(h(x)\\) is already a normalised pdmf then \\(z(0)=1\\) and \\(b(x)=h(x)\\).\nThe above presentation of exponential families assumes a univariate random variable (scalar \\(x\\)) but also applies to multivariate random variables (vector \\(\\boldsymbol x\\) or matrix \\(\\boldsymbol X\\)) .\nLikewise, canonical statistics and parameters are written as vectors but these may also be scalars or matrices (or a combination of both). The use of inner product notation \\(\\langle, \\rangle\\) includes all these cases, recall that for scalars \\(\\langle a, b \\rangle = ab\\), for vectors \\(\\langle \\boldsymbol a, \\boldsymbol b\\rangle = \\boldsymbol a^T \\boldsymbol a\\) and for matrices \\(\\langle \\boldsymbol A, \\boldsymbol B\\rangle = \\operatorname{Tr}( \\boldsymbol A^T \\boldsymbol B) = \\operatorname{Vec}(\\boldsymbol A)^T \\operatorname{Vec}(\\boldsymbol B)\\).\n\n\nCanonical statistics\nThe canonical statistics \\(\\boldsymbol t(x)\\) are transformations of \\(x\\), usually simple functions such as the identity (\\(x\\)), the square (\\(x^2\\)), the inverse (\\(1/x\\)) or the logarithm (\\(\\log x\\)).\nTypically, the number of canonical statistics and hence the dimension of \\(\\boldsymbol t\\) is small.\nThe canonical statistics \\(\\boldsymbol t(x)\\) may be affinely dependent. If this is the case there is a vector \\(\\boldsymbol \\eta_0\\) for which\n\\[\n\\langle \\boldsymbol \\eta_0, \\boldsymbol t(x) \\rangle  = \\text{const.}\n\\] A common example is when \\(\\boldsymbol x\\) is a vector of counts \\((n_1, \\ldots, n_K)^T\\) for \\(K\\) classes with a fixed total count \\(n = \\sum_{i=k}^K n_k =  \\boldsymbol x^T \\mathbf 1_K\\) and the canonical statistics are \\(\\boldsymbol t(\\boldsymbol x) = \\boldsymbol x\\) and thus include all \\(K\\) counts.\nIf the elements in \\(\\boldsymbol t(x)\\) are affinely independent the representation of the exponential family is minimal or complete, otherwise the representation is non-minimal or overcomplete.\n\n\nCanonical parameters and identifiability\nFor each canonical statistic there is a corresponding canonical parameter so the dimensions and shape of \\(\\boldsymbol t(x)\\) and \\(\\boldsymbol \\eta\\) match.\nIn a minimal representation the canonical parameters of the exponential family are identifiable and hence distinct parameter settings for \\(\\boldsymbol \\eta\\) yield distinct distributions.\nConversely, in a non-minimal or overcomplete representation there are redundant elements in the canonical parameters \\(\\boldsymbol \\eta\\) and the distributions within the exponential family are not identifiable. Specifically, there will be multiple \\(\\boldsymbol \\eta\\) yielding the same underlying distribution.\nIn the example above, where \\(\\boldsymbol x\\) is a vector of counts with a fixed total count and \\(\\boldsymbol t(\\boldsymbol x) = \\boldsymbol x\\) and corresponding canonical parameters \\(\\boldsymbol \\eta\\), the effective number of parameter is \\(K-1\\) rather than \\(K\\), so there is one redundant parameter in \\(\\boldsymbol \\eta\\).\n\n\nMoment and cumulant generating functions\nThe moment generating function for the canonical statistics \\(\\boldsymbol t(x)\\) is \\[\n\\begin{split}\nM(\\boldsymbol \\tau) & = \\operatorname{E}\\left( e^{\\langle \\boldsymbol \\tau, \\boldsymbol t(x)\\rangle} \\right)\\\\\n         &=  \\int_x    e^{\\langle \\boldsymbol \\tau, \\boldsymbol t(x)\\rangle}  p(x |\\boldsymbol \\eta) dx \\\\\n          &=  \\int_x    e^{\\langle \\boldsymbol \\tau, \\boldsymbol t(x)\\rangle}\\,  e^{\\langle \\boldsymbol \\eta, \\boldsymbol t(x)\\rangle}\\, h(x)\\, / z(\\boldsymbol \\eta) dx \\\\\n         & =  \\left( \\int_x    e^{\\langle \\boldsymbol \\tau+\\boldsymbol \\eta,\\boldsymbol t(x)\\rangle}\\, h(x) \\, dx\\right) /z(\\boldsymbol \\eta)  \\\\\n         & =  z(\\boldsymbol \\tau+\\boldsymbol \\eta)/z(\\boldsymbol \\eta)\\\\\n\\end{split}\n\\] Correspondingly, the cumulant generating function is \\[\n\\begin{split}\nK(\\boldsymbol \\tau) &= \\log M(\\boldsymbol \\tau) \\\\\n         &= a(\\boldsymbol \\tau+\\boldsymbol \\eta)-a(\\boldsymbol \\eta)\\\\\n\\end{split}\n\\] Thus, the moment and cumulant generating functions for the canonical statistics are closely linked to the partition and log-partition functions, respectively.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exponential families</span>"
    ]
  },
  {
    "objectID": "07-expfam.html#roles-of-the-partition-function",
    "href": "07-expfam.html#roles-of-the-partition-function",
    "title": "7  Exponential families",
    "section": "7.2 Roles of the partition function",
    "text": "7.2 Roles of the partition function\n\nNormalising factor\nThe pdmf \\(p(x|\\boldsymbol \\eta)\\) must integrate to one. Therefore, given \\(h(x)\\) and \\(\\boldsymbol t(x)\\) the partition function \\(z(\\boldsymbol \\eta)\\) is obtained by \\[\nz(\\boldsymbol \\eta)= \\int_x  e^{ \\langle \\boldsymbol \\eta, \\boldsymbol t(x) \\rangle } \\, h(x) \\, dx\n\\] For discrete \\(x\\) replace the integral by a sum.\nConsequently partition function \\(z(\\boldsymbol \\eta)\\) is also called the normaliser and and the log-partition function \\(a(\\boldsymbol \\eta)\\) is called the log-normaliser.\nConstructed as weighted integral of exponentials of linear functions, the partition function \\(z(\\boldsymbol \\eta)\\) and the log-partition function \\(a(\\boldsymbol \\eta)\\) are convex with regard to \\(\\boldsymbol \\eta\\).\nFor an exponential family in minimal representation the (log)-partition function(s) are strictly convex.\n\n\nDefinition of parameter space\nThe set of values of \\(\\boldsymbol \\eta\\) for which \\(z(\\boldsymbol \\eta) &lt; \\infty\\), and hence for which the pdmf \\(p(x|\\boldsymbol \\eta)\\) is well defined, comprises the parameter space of the exponential family. Some choices of \\(h(x)\\) and \\(\\boldsymbol t(x)\\) do not yield a finite normalising factor for any \\(\\boldsymbol \\eta\\) and hence these cannot be used to form an exponential family.\n\n\nMoments of canonical statistics\nThe first cumulant (the mean) and second cumulant (the variance) are obtained as the first and second derivatives of the cumulant generating function \\(K(\\boldsymbol \\tau) = a(\\boldsymbol \\tau+\\boldsymbol \\eta)-a(\\boldsymbol \\eta)\\) evaluated at \\(\\boldsymbol \\tau=0\\), respectively. As a result, the log-partition function \\(a(\\boldsymbol \\eta)\\) provides a practical way to obtain the mean and variance of the canonical statistics \\(\\boldsymbol t(x)\\).\nSpecifically, computing its gradient yields the mean \\[\n\\begin{split}\n\\operatorname{E}( \\boldsymbol t(x) )  = \\boldsymbol \\mu_{\\boldsymbol t} & = \\nabla a(\\boldsymbol \\eta)\\\\\n&= \\frac{\\nabla z(\\boldsymbol \\eta)}{z(\\boldsymbol \\eta)}\n\\end{split}\n\\] and computing the Hessian matrix the covariance matrix \\[\n\\begin{split}\n\\operatorname{Var}( \\boldsymbol t(x) )  = \\boldsymbol \\Sigma_{\\boldsymbol t} & = \\nabla \\nabla^T a(\\boldsymbol \\eta)\\\\\n&= \\frac{\\nabla \\nabla^T z(\\boldsymbol \\eta)}{z(\\boldsymbol \\eta)} - \\left(\\frac{\\nabla z(\\boldsymbol \\eta)}{z(\\boldsymbol \\eta)}\\right) \\left(\\frac{\\nabla z(\\boldsymbol \\eta)}{z(\\boldsymbol \\eta)}\\right)^T\n\\end{split}\n\\]\nFor an exponential family with minimal representation the log-partition function is strictly convex. Furthermore, the variance \\(\\boldsymbol \\Sigma_{\\boldsymbol t}\\) is a positive definite matrix and invertible.\nFor overcomplete representations the log-partition function is convex but not strictly convex. Furthermore, the covariance matrix is positive semi-definite and not invertible.\nBy construction, the log-partition function \\(a(\\boldsymbol \\eta)\\) is finite in the interior of its parameter space. Therefore all moments and cumulants of the canonical statistics \\(\\boldsymbol t(x)\\) exist (are finite) and are given by the derivatives of \\(a(\\boldsymbol \\eta)\\) at \\(\\boldsymbol \\eta\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exponential families</span>"
    ]
  },
  {
    "objectID": "07-expfam.html#further-properties",
    "href": "07-expfam.html#further-properties",
    "title": "7  Exponential families",
    "section": "7.3 Further properties",
    "text": "7.3 Further properties\n\nEquivalent representations\nAn exponential family admits many equivalent representations such that different specifications of canonical statistics \\(\\boldsymbol t(x)\\) and base function \\(h(x)\\) describe the same family.\nFirst, any invertible linear transformation of the canonical statistic \\(\\boldsymbol t(x)\\) yields the same distribution family.\nSecond, any member of the family, say \\(P(\\boldsymbol \\eta_{0})\\), can serve as its base distribution. Specifically, with \\(p(x | \\boldsymbol \\eta_{0})\\) used as base the pdmf for the exponential family \\(P(\\boldsymbol \\eta)\\) is \\[\np(x| \\boldsymbol \\eta) = e^{ \\langle \\boldsymbol \\eta- \\boldsymbol \\eta_{0},  \\boldsymbol t(x)\\rangle - (a(\\boldsymbol \\eta) -a(\\boldsymbol \\eta_{0}))}\\, p(x| \\boldsymbol \\eta_{0})\n\\] which is in exponential family form.\nThird, the base function \\(h(x)\\) can be left unnormalised, so there are infinitely many positive base functions \\(h(x)\\) that yield the same base pdmf \\(b(x)\\) after normalisation.\nFourth, any factors in \\(h(x)\\) of the form \\(e^{ \\langle \\boldsymbol \\eta_{0},  \\boldsymbol t(x)\\rangle}\\) for some constant \\(\\boldsymbol \\eta_{0}\\) can be removed from \\(h(x)\\) and absorbed into the exponential by replacing \\(\\boldsymbol \\eta\\) with \\(\\boldsymbol \\eta+ \\boldsymbol \\eta_{0}\\) (thus leading to a different set of canonical parameters).\nAs a result, for many (but not all) commonly used exponential families the base function can be set to \\(h(x)=1\\) or some other constant value, so that all dependence on \\(x\\) enters through the canonical statistics \\(\\boldsymbol t(x)\\) via \\(\\langle \\boldsymbol \\eta,  \\boldsymbol t(x)\\rangle\\).\n\n\nNatural exponential families\nIf the canonical statistic is the identity, \\(t(x) = x\\) or \\(\\boldsymbol t(\\boldsymbol x)=\\boldsymbol x\\), the family is a natural exponential family (NEF). A univariate NEF has a scalar canonical statistic \\(t(x)\\) and a scalar canonical parameter \\(\\eta\\) and is thus a one-parameter family.\n\n\nAlternative parametrisations\nAn exponential family can be parametrised by three different sets of parameters:\n\ncanonical parameters \\(\\boldsymbol \\eta\\),\nexpectation parameters \\(\\boldsymbol \\mu_{\\boldsymbol t} = \\operatorname{E}(\\boldsymbol t(x))\\) (the mean of the canonical statistics \\(\\boldsymbol t(x)\\)), as well as\nconventional parameters \\(\\boldsymbol \\theta\\) (such as mean and variance of \\(x\\)).\n\nIf the exponential family is minimal then there is a one-to-one map between the canonical parameters \\(\\boldsymbol \\eta\\) and the expectation parameters  \\(\\boldsymbol \\mu_{\\boldsymbol t}\\).\nThe canonical and the expectation parameters can be expressed as a function of the conventional parameters \\(\\boldsymbol \\theta\\).\nOften, some expectation parameters \\(\\boldsymbol \\mu_{\\boldsymbol t}\\) correspond to conventional parameters \\(\\boldsymbol \\theta\\) (e.g., if one of the canonical statistics is \\(x\\), then the corresponding expectation parameter is the mean of \\(x\\)).\n\n\nExponential families and change of variables\nAssume that \\(x\\) is a random variable with an exponential-family distribution and \\(y(x)\\) is an invertible transformation (see Section 3.2).\nThe resulting pdmf for \\(y\\) is \\[\np(y|\\boldsymbol \\eta) = h_y(y)\\, e^{ \\langle \\boldsymbol \\eta, \\boldsymbol t_y(y) \\rangle } / z(\\boldsymbol \\eta)\n\\] with transformed canonical statistics \\[\n\\boldsymbol t_y(y) = \\boldsymbol t_x(x(y))\n\\] For a discrete random variable the base function changes to \\[\nh_y(y) = h_x(x(y))\n\\] whereas for a continuous random variable the base function becomes \\[\nh_y(y) = \\left| D x(y) \\right|\\,h_x(x(y))\n\\] where \\(D x(y)\\) is the Jacobian matrix of the inverse transformation \\(x(y)\\).\nThus, for both discrete and continuous random variables the exponential-family form is preserved under a change of variables.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exponential families</span>"
    ]
  },
  {
    "objectID": "07-expfam.html#univariate-exponential-families",
    "href": "07-expfam.html#univariate-exponential-families",
    "title": "7  Exponential families",
    "section": "7.4 Univariate exponential families",
    "text": "7.4 Univariate exponential families\nTable 7.1 lists univariate exponential families, more details about these distributions are found in Chapter 5.\n\\(\\operatorname{Bin}(n, \\theta)\\) is a one-parameter exponential family with \\(n\\) assumed fixed.\nFor \\(\\operatorname{Bin}(n, \\theta)\\) and \\(\\operatorname{Ber}(\\theta)\\) the conventional parameter is \\[\n\\theta = \\operatorname{logit}^{-1}(\\eta)=\\frac{e^\\eta}{1+e^\\eta}\n\\] (logistic function). Conversely, since this is a one-to-one map, the canonical parameter equals \\[\n\\eta = \\operatorname{logit}(\\theta)= \\log\\left(\\frac{\\theta}{1-\\theta}\\right)\n\\]\nApart from \\(\\operatorname{Bin}(n, \\theta)\\) all families listed in Table 7.1 have constant base function (\\(h(x)=1\\)).\nFurthermore, \\(\\operatorname{Bin}(n, \\theta)\\), \\(\\operatorname{Ber}(\\theta)\\) and \\(\\operatorname{Exp}(\\theta)\\) are NEFs. \\(N(\\mu,\\sigma^2)\\) with fixed \\(\\sigma^2\\) (variance), \\(\\operatorname{Gam}(\\alpha, \\theta)\\) with fixed \\(\\alpha\\) (shape) and \\(\\operatorname{Wis}(s^2, k)\\) with fixed \\(k\\) (shape) are also NEFs.\n\n\n\n\nTable 7.1: Common univariate exponential families\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\n\\(h(x)\\)\n\\(z(\\boldsymbol \\eta)\\)\n\\(\\boldsymbol \\eta\\)\n\\(\\boldsymbol t(x)\\)\n\\(\\boldsymbol \\mu_{\\boldsymbol t}\\)\n\n\n\n\n\\(\\operatorname{Bin}(n, \\theta)\\)\n\\(W_2\\)\n\\((1+e^\\eta)^n\\)\n\\(\\operatorname{logit}(\\theta)\\)\n\\(x\\)\n\\(\\theta\\)\n\n\n\\(\\operatorname{Ber}(\\theta)\\)\n\\(1\\)\n\\(1+e^\\eta\\)\n\\(\\operatorname{logit}(\\theta)\\)\n\\(x\\)\n\\(\\theta\\)\n\n\n\\(\\operatorname{Beta}(\\alpha_1, \\alpha_2)\\)\n\\(1\\)\n\\(B(\\eta_1+1, \\eta_2+1)\\)\n\\(\\begin{pmatrix}\\alpha_1-1 \\\\ \\alpha_2-1\\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\log x \\\\ \\log(1-x) \\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\psi^{(0)}(\\alpha_1)-\\psi^{(0)}(m) \\\\ \\psi^{(0)}(\\alpha_2)-\\psi^{(0)}(m)\\end{pmatrix}\\)\n\n\n\\(N(\\mu,\\sigma^2)\\)\n\\(1\\)\n\\((-\\pi \\eta_2^{-1})^{1/2}\\) \\(\\exp(-\\frac{1}{4}\\eta_1^2 \\eta_2^{-1})\\)\n\\(\\begin{pmatrix} \\sigma^{-2} \\mu \\\\ -\\frac{1}{2}\\sigma^{-2}\\end{pmatrix}\\)\n\\(\\begin{pmatrix} x \\\\ x^2\\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\mu \\\\ \\sigma^2 + \\mu^2 \\end{pmatrix}\\)\n\n\n\\(\\operatorname{Gam}(\\alpha, \\theta)\\)\n\\(1\\)\n\\((-\\eta_1)^{-\\eta_2-1}\\) \\(\\Gamma(\\eta_2+1)\\)\n\\(\\begin{pmatrix} -1/\\theta \\\\ \\alpha-1 \\end{pmatrix}\\)\n\\(\\begin{pmatrix}  x \\\\ \\log x \\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\alpha \\theta \\\\ \\psi^{(0)}(\\alpha) +\\log\\theta\\end{pmatrix}\\)\n\n\n\\(\\operatorname{Exp}(\\theta)\\)\n\\(1\\)\n\\(-\\eta^{-1}\\)\n\\(-1/\\theta\\)\n\\(x\\)\n\\(\\theta\\)\n\n\n\\(\\operatorname{Wis}(s^2, k)\\)\n\\(1\\)\n\\((-\\eta_1)^{-\\eta_2-1}\\) \\(\\Gamma(\\eta_2+1)\\)\n\\(\\begin{pmatrix} -\\frac{1}{2} s^{-2} \\\\ \\frac{k}{2} -1 \\end{pmatrix}\\)\n\\(\\begin{pmatrix} x \\\\ \\log x \\end{pmatrix}\\)\n\\(\\begin{pmatrix} k s^2 \\\\ \\psi^{(0)}(\\frac{k}{2}) +\\log(2 s^2)\\end{pmatrix}\\)\n\n\n\\(\\operatorname{IGam}(\\alpha, \\beta)\\)\n\\(1\\)\n\\((-\\eta_1)^{\\eta_2+1}\\) \\(\\Gamma(-\\eta_2-1)\\)\n\\(\\begin{pmatrix} -\\beta \\\\  -\\alpha-1 \\end{pmatrix}\\)\n\\(\\begin{pmatrix} x^{-1} \\\\ \\log x \\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\alpha/\\beta \\\\ -\\psi^{(0)}(\\alpha) +\\log\\beta \\end{pmatrix}\\)\n\n\n\\(\\operatorname{IWis}(\\psi, k)\\)\n\\(1\\)\n\\((-\\eta_1)^{\\eta_2+1}\\) \\(\\Gamma(-\\eta_2-1)\\)\n\\(\\begin{pmatrix} -\\frac{\\psi}{2} \\\\  -\\frac{k}{2}-1 \\end{pmatrix}\\)\n\\(\\begin{pmatrix} x^{-1} \\\\ \\log x \\end{pmatrix}\\)\n\\(\\begin{pmatrix} k / \\psi \\\\-\\psi^{(0)}(\\frac{k}{2}) +\\log(\\frac{\\psi}{2})  \\end{pmatrix}\\)\n\n\n\n\n\n\nNotes:\n\n\\(W_2 = \\binom{n}{x}\\) is the binomial coefficient.\n\\(B(\\alpha_1, \\alpha_2)\\) is the beta function.\n\\(m=\\alpha_1 + \\alpha_2\\).\n\\(\\psi^{(0)}(x) =\\frac{d}{dx} \\log \\Gamma(x)\\) is the digamma function.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exponential families</span>"
    ]
  },
  {
    "objectID": "07-expfam.html#multivariate-exponential-families",
    "href": "07-expfam.html#multivariate-exponential-families",
    "title": "7  Exponential families",
    "section": "7.5 Multivariate exponential families",
    "text": "7.5 Multivariate exponential families\nTable 7.2 lists multvariate exponential families, more details about these distributions are found in Chapter 6.\n\\(\\operatorname{Mult}(n, \\boldsymbol \\theta)\\) is a \\((K-1)\\)-parameter exponential family with \\(n\\) assumed fixed.\nFor \\(\\operatorname{Mult}(n, \\boldsymbol \\theta)\\) and \\(\\operatorname{Cat}(\\boldsymbol \\theta)\\) the conventional parameters are given by \\[\n\\boldsymbol \\theta= \\operatorname{softmax}(\\boldsymbol \\eta) = \\frac{(\\exp \\eta_k)}{\\sum_{i=1}^K \\exp \\eta_i}\n\\] As the softmax function is invariant against translation and is a many-to-one map, its inverse is not unique and the canonical parameters \\[\n\\boldsymbol \\eta= (c + \\log \\theta_k)\n\\] are determined by the conventional parameters \\(\\boldsymbol \\theta\\) only up to a constant \\(c\\). This representation using \\(K\\) canonical parameters \\(\\boldsymbol \\eta\\) is non-minimal, hence \\(\\boldsymbol \\eta\\) is not identifiable and different values of \\(\\boldsymbol \\eta\\) can represent the same distribution.\nA minimal representation with \\(K-1\\) parameters \\(\\eta_1, \\ldots, \\eta_{K-1}\\) and \\(\\eta_K=0\\) corresponds to \\(c=-\\log \\theta_K\\) and \\(\\eta_k = \\log (\\theta_k/ \\theta_K )\\). For \\(K=2\\) this yields the minimal representations of \\(\\operatorname{Bin}(n, \\theta)\\) and \\(\\operatorname{Ber}(\\theta)\\) shown in Table 7.1.\nApart from \\(\\operatorname{Mult}(n, \\boldsymbol \\theta)\\) all families listed in Table 7.2 have constant base function (\\(h(\\boldsymbol x)=1\\)).\nFurthermore, \\(\\operatorname{Mult}(n, \\boldsymbol \\theta)\\) and \\(\\operatorname{Cat}(\\boldsymbol \\theta)\\) are NEFs. \\(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) with fixed \\(\\boldsymbol \\Sigma\\) (variance) and \\(\\operatorname{Wis}(\\boldsymbol S, k)\\) with fixed \\(k\\) (shape) are NEFs as well.\n\n\n\n\nTable 7.2: Common multivariate exponential families\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\n\\(h(\\boldsymbol x)\\)\n\\(z(\\boldsymbol \\eta)\\)\n\\(\\boldsymbol \\eta\\)\n\\(\\boldsymbol t(\\boldsymbol x)\\)\n\\(\\boldsymbol \\mu_{\\boldsymbol t}\\)\n\n\n\n\n\\(\\operatorname{Mult}(n, \\boldsymbol \\theta)\\)\n\\(W_K\\)\n\\((\\sum_{k=1}^K \\exp \\eta_k)^n\\)\n\\((c+\\log \\theta_k)\\)\n\\(\\boldsymbol x\\)\n\\(\\boldsymbol \\theta\\)\n\n\n\\(\\operatorname{Cat}(\\boldsymbol \\theta)\\)\n\\(1\\)\n\\(\\sum_{k=1}^K \\exp \\eta_k\\)\n\\((c+ \\log \\theta_k)\\)\n\\(\\boldsymbol x\\)\n\\(\\boldsymbol \\theta\\)\n\n\n\\(\\operatorname{Dir}(\\boldsymbol \\alpha)\\)\n\\(1\\)\n\\(B(\\boldsymbol \\eta+1)\\)\n\\(\\begin{pmatrix}\\alpha_k-1 \\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\log x_k \\end{pmatrix}\\)\n\\(\\begin{pmatrix}  \\psi^{(0)}(\\alpha_k)-\\psi^{(0)}(m)\\end{pmatrix}\\)\n\n\n\\(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\)\n\\(1\\)\n\\(\\det(- \\pi \\boldsymbol \\eta_2^{-1})^{1/2}\\) \\(\\exp(-\\frac{1}{4} \\boldsymbol \\eta_1^T \\boldsymbol \\eta_2^{-1} \\boldsymbol \\eta_1)\\)\n\\(\\begin{pmatrix}\\boldsymbol \\Sigma^{-1}\\boldsymbol \\mu\\\\ -\\frac{1}{2}\\boldsymbol \\Sigma^{-1}\\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\boldsymbol x\\\\ \\boldsymbol x\\boldsymbol x^T\\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\boldsymbol \\mu\\\\ \\boldsymbol \\Sigma+ \\boldsymbol \\mu\\boldsymbol \\mu^T \\end{pmatrix}\\)\n\n\n\\(\\operatorname{Wis}(\\boldsymbol S, k)\\)\n\\(1\\)\n\\(\\det(-\\boldsymbol \\eta_1)^{-\\eta_2-\\frac{d+1}{2}}\\) \\(\\Gamma_d(\\eta_2+\\frac{d+1}{2})\\)\n\\(\\begin{pmatrix} -\\frac{1}{2}\\boldsymbol S^{-1} \\\\ \\frac{k}{2} - \\frac{d+1}{2} \\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\boldsymbol X\\\\ \\log \\det(\\boldsymbol X) \\end{pmatrix}\\)\n\\(\\begin{pmatrix} k \\boldsymbol S\\\\ \\psi^{(0)}_d(\\frac{k}{2}) + \\log \\det(2 \\boldsymbol S)\\end{pmatrix}\\)\n\n\n\\(\\operatorname{IWis}\\left(\\boldsymbol \\Psi, k\\right)\\)\n\\(1\\)\n\\(\\det(-\\boldsymbol \\eta_1)^{\\eta_2+\\frac{d+1}{2}}\\) \\(\\Gamma_d(-\\eta_2-\\frac{d+1}{2})\\)\n\\(\\begin{pmatrix} -\\frac{1}{2}\\boldsymbol \\Psi\\\\ -\\frac{k}{2} - \\frac{d+1}{2} \\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\boldsymbol X^{-1} \\\\ \\log \\det(\\boldsymbol X) \\end{pmatrix}\\)\n\\(\\begin{pmatrix} k \\boldsymbol \\Psi^{-1} \\\\-\\psi^{(0)}_d(\\frac{k}{2})  +\\log \\det(\\frac{\\boldsymbol \\Psi}{2}) \\end{pmatrix}\\)\n\n\n\n\n\n\nNotes:\n\n\\(W_K = \\binom{n}{x_1, \\ldots, x_K}\\) is the multinomial coefficient for \\(K\\) groups.\n\\(B(\\boldsymbol \\alpha)\\) is the multivariate beta function.\n\\(m=\\sum_{i=k}^K \\alpha_k\\).\n\\(\\psi^{(0)}_d  = \\frac{d}{dx} \\log \\Gamma_d(x) = \\sum_{i=1}^d \\psi^{(0)}(x - (i-1)/2)\\) is the multivariate digamma function.\n\nSee also: Exponential family (Wikipedia) and Efron (2022).\n\n\n\n\nEfron, B. 2022. Exponential Families in Theory and Practise. Cambridge University Press. https://doi.org/10.1017/9781108773157.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exponential families</span>"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Efron, B. 2022. Exponential Families in Theory and Practise.\nCambridge University Press. https://doi.org/10.1017/9781108773157.\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate\nAnalysis. Academic Press.\n\n\nWhittle, P. 2000. Probability via Expectation. 3rd ed.\nSpringer. https://doi.org/10.1007/978-1-4612-0509-8.",
    "crumbs": [
      "Bibliography"
    ]
  }
]