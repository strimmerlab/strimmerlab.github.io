[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability and Distribution Refresher",
    "section": "",
    "text": "Welcome\nThe Probability and Distribution Refresher notes were written by Korbinian Strimmer from 2018–2024. This version is from 21 March 2024.\nIf you have any questions, comments, or corrections please get in touch! 1",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Probability and Distribution Refresher",
    "section": "Updates",
    "text": "Updates\nThe notes will be updated from time to time. To view the current version visit the\n\nonline version of the Probability and Distribution Refresher notes.\n\nYou may also wish to download the Probability and Distribution Refresher notes as\n\nPDF in A4 format for printing (double page layout), or as\n6x9 inch PDF for use on tablets (single page layout).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Probability and Distribution Refresher",
    "section": "License",
    "text": "License\nThese notes are licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Probability and Distribution Refresher",
    "section": "",
    "text": "Email address: korbinian.strimmer@manchester.ac.uk↩︎",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "00-preface.html",
    "href": "00-preface.html",
    "title": "Preface",
    "section": "",
    "text": "About the author\nHello! My name is Korbinian Strimmer and I am a Professor in Statistics. I am a member of the Statistics group at the Department of Mathematics of the University of Manchester. You can find more information about me on my home page.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#about-the-notes",
    "href": "00-preface.html#about-the-notes",
    "title": "Preface",
    "section": "About the notes",
    "text": "About the notes\nThese supplementary notes aim to provide a quick refresher of some essentials in combinatorics and probability as well as to offer an overview over selected univariate and multivariate distributions.\nThe notes are supporting information for a number of lecture notes of statistical courses I am or have been teaching at the Department of Mathematics of the University of Manchester.\nThis includes the currently offered modules:\n\nMATH27720 Statistics 2: Likelihood and Bayes and\nMATH38161 Multivariate Statistics\n\nas well as the retired module (not offered any more):\n\nMATH20802 Statistical Methods.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-combinatorics.html",
    "href": "01-combinatorics.html",
    "title": "1  Combinatorics",
    "section": "",
    "text": "1.1 Some basic mathematical notation\nSummation: \\[\n\\sum_{i=1}^n x_i = x_1 + x_2 + \\ldots + x_n\n\\]\nMultiplication: \\[\n\\prod_{i=1}^n x_i = x_1 \\times x_2 \\times \\ldots \\times x_n\n\\]\nIndicator function: \\[\n1_{A} =\n\\begin{cases}\n1 & \\text{if $A$ is true}\\\\\n0 & \\text{if $A$ is not true}\\\\\n\\end{cases}\n\\]\nScalar: plain type, typically lower case (\\(x\\), \\(\\theta\\)), sometimes upper case (\\(K\\)).\nVector: bold type, lower case (\\(\\symbfit x\\), \\(\\symbfit \\theta\\)).\nMatrix: bold type, upper case (\\(\\symbfit X\\), \\(\\symbfit \\Sigma\\)).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "01-combinatorics.html#number-of-permutations",
    "href": "01-combinatorics.html#number-of-permutations",
    "title": "1  Combinatorics",
    "section": "1.2 Number of permutations",
    "text": "1.2 Number of permutations\nThe number of possible orderings, or permutations, of \\(n\\) distinct items is the number of ways to put \\(n\\) items in \\(n\\) bins with exactly one item in each bin. It is given by the factorial \\[\nn! = \\prod_{i=1}^n i = 1 \\times 2 \\times \\ldots \\times n\n\\] where \\(n\\) is a positive integer. For \\(n=0\\) the factorial is defined as \\[\n0! = 1\n\\] as there is exactly one permutation of zero objects.\nThe factorial can also be obtained using the gamma function \\[\n\\Gamma(x) = \\int_0^\\infty t^{x-1} e^{-t} dt\n\\] which can be viewed as continuous version of the factorial with \\(\\Gamma(x) = (x-1)!\\) for any positive integer \\(x\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "01-combinatorics.html#de-moivre-sterling-approximation-of-the-factorial",
    "href": "01-combinatorics.html#de-moivre-sterling-approximation-of-the-factorial",
    "title": "1  Combinatorics",
    "section": "1.3 De Moivre-Sterling approximation of the factorial",
    "text": "1.3 De Moivre-Sterling approximation of the factorial\nThe factorial is frequently approximated by the following formula derived by Abraham de Moivre (1667–1754) and James Stirling (1692-1770) \\[\nn! \\approx \\sqrt{2 \\pi} n^{n+\\frac{1}{2}} e^{-n}\n\\] or equivalently on logarithmic scale \\[\n\\log n!  \\approx \\left(n+\\frac{1}{2}\\right) \\log n  -n + \\frac{1}{2}\\log \\left( 2 \\pi\\right)\n\\] The approximation is good for small \\(n\\) (but fails for \\(n=0\\)) and becomes more and more accurate with increasing \\(n\\). For large \\(n\\) the approximation can be simplified to \\[\n\\log n! \\approx  n \\log n  -n\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "01-combinatorics.html#multinomial-and-binomial-coefficient",
    "href": "01-combinatorics.html#multinomial-and-binomial-coefficient",
    "title": "1  Combinatorics",
    "section": "1.4 Multinomial and binomial coefficient",
    "text": "1.4 Multinomial and binomial coefficient\nThe number of possible permutation of \\(n\\) items of \\(K\\) distinct types, with \\(n_1\\) of type 1, \\(n_2\\) of type 2 and so on, equals the number of ways to put \\(n\\) items into \\(K\\) bins with \\(n_1\\) items in the first bin, \\(n_2\\) in the second and so on. It is given by the multinomial coefficient \\[\n\\binom{n}{n_1, \\ldots, n_K} = \\frac {n!}{n_1! \\times n_2! \\times\\ldots \\times n_K! }\n\\] with \\(\\sum_{k=1}^K n_k = n\\) and \\(K \\leq n\\). Note that it equals the number of permutation of all items divided by the number of permutations of the items in each bin (or of each type).\nIf all \\(n_k=1\\) and hence \\(K=n\\) the multinomial coefficient reduces to the factorial.\nIf there are only two bins / types (\\(K=2\\)) the multinomial coefficients becomes the binomial coefficient \\[\n\\binom{n}{n_1} = \\binom{n}{n_1, n-n_1}    =  \\frac {n!}{n_1! (n - n_1)!}\n\\] which counts the number of ways to choose \\(n_1\\) elements from a set of \\(n\\) elements.\nFor large \\(n\\) and \\(n_k\\) we can apply the De Moivre-Sterling approximation to the multinomial coefficient, yielding \\[\n\\log\\binom{n}{n_1, \\ldots, n_K} = - n \\sum_{k=1}^K \\frac{n_k}{n} \\log\\left( \\frac{n_k}{n} \\right)\n\\] Note this is \\(n\\) times the Shannon entropy of a categorical distribution with \\(n_k/n\\) as class probabilities.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Combinatorics</span>"
    ]
  },
  {
    "objectID": "02-probability.html",
    "href": "02-probability.html",
    "title": "2  Probability",
    "section": "",
    "text": "2.1 Random variables\nA random variable describes a random experiment. The set of all possible outcomes is the sample space or state space of the random variable and is denoted by \\(\\Omega = \\{\\omega_1, \\omega_2, \\ldots\\}\\). The outcomes \\(\\omega_i\\) are the elementary events. The sample space \\(\\Omega\\) can be finite or infinite. Depending on type of outcomes the random variable is discrete or continuous.\nAn event \\(A \\subseteq \\Omega\\) is a subset of \\(\\Omega\\) and thus itself a set composed of elementary events: \\(A = \\{a_1, a_2, \\ldots\\}\\). This includes as special cases the full set \\(A = \\Omega\\), the empty set \\(A = \\emptyset\\), and the elementary events \\(A=\\omega_i\\). The complementary event \\(A^C\\) is the complement of the set \\(A\\) in the set \\(\\Omega\\) so that \\(A^C = \\Omega \\setminus A =  \\{\\omega_i \\in \\Omega:  \\omega_i \\notin A\\}\\).\nThe probability of an event \\(A\\) is denoted by \\(\\text{Pr}(A)\\). Essentially, to obtain this probability we need to count the elementary elements corresponding to \\(A\\). To do this we assume as axioms of probability that\nThis implies\nAssume now that we have two events \\(A\\) and \\(B\\). The probability of the event “\\(A\\) and \\(B\\)” is then given by the probability of the set intersection \\(\\text{Pr}(A \\cap B)\\). Likewise the probability of the event “\\(A\\) or \\(B\\)” is given by the probability of the set union \\(\\text{Pr}(A \\cup B)\\).\nFrom the above it is clear that the definition and theory of probability is closely linked to set theory, and in particular to measure theory. Indeed, viewing probability as a special type of measure allows for an elegant treatment of both discrete and continuous random variables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#random-variables",
    "href": "02-probability.html#random-variables",
    "title": "2  Probability",
    "section": "",
    "text": "\\(\\text{Pr}(A) \\geq 0\\), probabilities are positive,\n\\(\\text{Pr}(\\Omega) = 1\\), the certain event has probability 1, and\n\\(\\text{Pr}(A) = \\sum_{a_i \\in A} \\text{Pr}(a_i)\\), the probability of an event equals the sum of its constituting elementary events \\(a_i\\). This sum is taken over a finite or countable infinite number of elements.\n\n\n\n\\(\\text{Pr}(A) \\leq 1\\), i.e. probabilities all lie in the interval \\([0,1]\\)\n\\(\\text{Pr}(A^C) = 1 - \\text{Pr}(A)\\), and\n\\(\\text{Pr}(\\emptyset) = 0\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#probability-mass-and-density-function",
    "href": "02-probability.html#probability-mass-and-density-function",
    "title": "2  Probability",
    "section": "2.2 Probability mass and density function",
    "text": "2.2 Probability mass and density function\nTo describe a random variable \\(x\\) with state space \\(\\Omega\\) we need a way to effectively store the probabilities of the corresponding elementary outcomes \\(x \\in \\Omega\\).\nFor simplicity of notation we use the same symbol to denote the random variable and its elementary outcomes.1 This convention greatly facilitates working with random vectors and matrices and follows, e.g., the classic multivariate statistics textbook by Mardia, Kent, and Bibby (1979). If a quantity is random we will always specify this explicitly in the context.\nFor a discrete random variable we define the event \\(A = \\{x: x=a\\} = \\{a\\}\\) and get the probability \\[\n\\text{Pr}(A) = \\text{Pr}(x=a) = f(a)\n\\] directly from the probability mass function (PMF), here denoted by lower case \\(f\\) (but we frequently also use \\(p\\) or \\(q\\)). The PMF has the property that \\(\\sum_{x \\in \\Omega} f(x) = 1\\) and that \\(f(x) \\in [0,1]\\).\nFor continuous random variables we need to use a probability density function (PDF) instead. We define the event \\(A = \\{x: a &lt; x \\leq a + da\\}\\) as an infinitesimal interval and then assign the probability \\[\n\\text{Pr}(A) = \\text{Pr}( a &lt; x \\leq a + da) = f(a) da \\,.\n\\] The PDF has the property that \\(\\int_{x \\in \\Omega} f(x) dx = 1\\) but in contrast to a PMF the density \\(f(x)\\geq 0\\) may take on values larger than 1.\nThe set of all \\(x\\) for which \\(f(x)\\) is positive is called the support of the PDF/PMF.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#distribution-function-and-quantile-function",
    "href": "02-probability.html#distribution-function-and-quantile-function",
    "title": "2  Probability",
    "section": "2.3 Distribution function and quantile function",
    "text": "2.3 Distribution function and quantile function\nAs alternative to using PMF/PDFs we may also use a distribution function to describe the random variable. This assumes an ordering exist among the elementary events so that we can define the event \\(A = \\{x: x \\leq a \\}\\) and compute its probability as \\[\nF(a) = \\text{Pr}(A) = \\text{Pr}( x \\leq a ) =\n\\begin{cases}\n\\sum_{x \\in A} f(x) & \\text{discrete case} \\\\\n\\int_{x \\in A} f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\] This is also known cumulative distribution function (CDF) and is denoted by upper case \\(F\\) (or \\(P\\) and \\(Q\\)). By construction the distribution function is monotonically non-decreasing and its value ranges from 0 to 1. With its help we can compute the probability of an interval set such as \\[\n\\text{Pr}( a &lt; x \\leq b ) = F(b)-F(a) \\,.\n\\]\nThe inverse of the distribution function \\(y=F(x)\\) is the quantile function \\(x=F^{-1}(y)\\). The 50% quantile \\(F^{-1}\\left(\\frac{1}{2}\\right)\\) is called the median.\nIf the random variable \\(x\\) has distribution function \\(F\\) we write \\(x \\sim F\\).\n\n\n\n\n\n\nFigure 2.1: Density function and distribution function.\n\n\n\nFigure 2.1 illustrates a density function \\(f(x)\\) and the corresponding distribution function \\(F(x)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#families-of-distributions",
    "href": "02-probability.html#families-of-distributions",
    "title": "2  Probability",
    "section": "2.4 Families of distributions",
    "text": "2.4 Families of distributions\nA distribution \\(F_{\\theta}\\) with a parameter \\(\\theta\\) constitutes a distribution family collecting all the distributions corresponding to particular instances of the parameter. The parameter \\(\\theta\\) therefore acts as an index of the distributions contained in the family.\nThe corresponding density (PDF) or probability mass function (PMF) is written either as \\(f_{\\theta}(x)\\), \\(f(x; \\theta)\\) or \\(f(x | \\theta)\\). The latter form is the most general is it suggests that the parameter \\(\\theta\\) may potentially also have its own distribution, with a joint density formed by \\(f(x, \\theta) = f(x | \\theta) f(\\theta)\\).\nNote that any parametrisation is generally not unique, as a one-to-one transformation of \\(\\theta\\) will yield another equivalent index to the same distribution family. Typically, for most commonly used distribution families there are several standard parametrisations. Often we use those parametrisations where the parameters can be interpreted easily (e.g. in terms of moments).\nIf for any pair of different parameter values \\(\\theta_1 \\neq \\theta_2\\) we get distinct distributions with \\(F_{\\theta_1} \\neq F_{\\theta_2}\\) then the distribution family \\(F_{\\theta}\\) is said to be identifiable by the parameter \\(\\theta\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#expectation-of-a-random-variable",
    "href": "02-probability.html#expectation-of-a-random-variable",
    "title": "2  Probability",
    "section": "2.5 Expectation of a random variable",
    "text": "2.5 Expectation of a random variable\nThe expected value \\(\\text{E}(x)\\) of a random variable is defined as the weighted average over all possible outcomes, with the weight given by the PMF / PDF \\(f(x)\\): \\[\n\\text{E}_{F}(x) =\n\\begin{cases}\n\\sum_{x \\in \\Omega} x f(x) & \\text{discrete case} \\\\\n\\int_{x \\in \\Omega} x f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\] Note the notation to emphasise that the expectation is taken with regard to the distribution \\(F\\). The subscript \\(F\\) is usually left out if there are no ambiguities. Furthermore, because the sum or integral may diverge the expectation is not necessarily always defined (in contrast to quantiles).\nThe expected value of a function of a random variable \\(h(x)\\) is obtained similarly: \\[\n\\text{E}_{F}(h(x)) =\n\\begin{cases}\n\\sum_{x \\in \\Omega} h(x) f(x)  & \\text{discrete case} \\\\\n\\int_{x \\in \\Omega}  h(x) f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\] This is called the “law of the unconscious statistician”, or short LOTUS. Again, to highlight that the random variable \\(x\\) has distribution \\(F\\) we write \\(\\text{E}_F(h(x))\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#jensens-inequality-for-the-expectation",
    "href": "02-probability.html#jensens-inequality-for-the-expectation",
    "title": "2  Probability",
    "section": "2.6 Jensen’s inequality for the expectation",
    "text": "2.6 Jensen’s inequality for the expectation\nIf \\(h(\\symbfit x)\\) is a convex function then the following inequality holds:\n\\[\n\\text{E}(h(\\symbfit x)) \\geq h(\\text{E}(\\symbfit x))\n\\]\nRecall: a convex function (such as \\(x^2\\)) has the shape of a “valley”.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#probability-as-expectation",
    "href": "02-probability.html#probability-as-expectation",
    "title": "2  Probability",
    "section": "2.7 Probability as expectation",
    "text": "2.7 Probability as expectation\nProbability itself can also be understood as an expectation. For an event \\(A\\) we can define a corresponding indicator function \\(1_{ x \\in A}\\) for an elementary element \\(x\\) to be part of \\(A\\). From the above it then follows \\[\n\\text{E}( 1_{x \\in A} ) = \\text{Pr}(A) \\, ,\n\\]\nInterestingly, one can develop the whole theory of probability from this perspective (e.g., Whittle 2000).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#moments-and-variance-of-a-random-variable",
    "href": "02-probability.html#moments-and-variance-of-a-random-variable",
    "title": "2  Probability",
    "section": "2.8 Moments and variance of a random variable",
    "text": "2.8 Moments and variance of a random variable\nThe moments of a random variable are defined as follows:\n\nZeroth moment: \\(\\text{E}(x^0) = 1\\) by construction of PDF and PMF,\nFirst moment: \\(\\text{E}(x^1) = \\text{E}(x) = \\mu\\) , the mean,\nSecond moment: \\(\\text{E}(x^2)\\)\nThe variance is the second moment centred about the mean \\(\\mu\\): \\[\\text{Var}(x) = \\text{E}\\left( (x - \\mu)^2 \\right) = \\sigma^2\\]\nThe variance can also be computed by \\(\\text{Var}(x) = \\text{E}(x^2)-\\text{E}(x)^2\\). This provides an example of Jensen’s inequality, with \\(\\text{E}(x^2) =\\text{E}(x)^2 + \\text{Var}(x) \\geq \\text{E}(x)^2\\).\n\nA distribution does not necessarily need to have any finite first or higher moments. An example is the location-scale \\(t\\)-distribution (Section 4.7.1) that depending on the value of the parameter \\(\\nu\\) may not have a mean or variance (or other higher moments).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#random-vectors-and-their-mean-and-variance",
    "href": "02-probability.html#random-vectors-and-their-mean-and-variance",
    "title": "2  Probability",
    "section": "2.9 Random vectors and their mean and variance",
    "text": "2.9 Random vectors and their mean and variance\nIn addition to scalar random variables we often make use of random vectors and also random matrices.2\nFor a random vector \\(\\symbfit x= (x_1, x_2,...,x_d)^T \\sim F\\) the mean \\(\\text{E}(\\symbfit x) = \\symbfit \\mu\\) is given by the means of its components, i.e. \\(\\symbfit \\mu= (\\mu_1, \\ldots, \\mu_d)^T\\) with \\(\\mu_i = \\text{E}(x_i)\\). Thus, the mean of a random vector of dimension \\(d\\) is a vector of the same length.\nThe variance of a random vector of length \\(d\\), however, is not a vector but a matrix of size \\(d\\times d\\). This matrix is called the covariance matrix: \\[\n\\begin{split}\n\\text{Var}(\\symbfit x) &= \\underbrace{\\symbfit \\Sigma}_{d\\times d} = (\\sigma_{ij}) = \\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix} \\\\\n  &=\\text{E}\\left(\\underbrace{(\\symbfit x-\\symbfit \\mu)}_{d\\times 1} \\underbrace{(\\symbfit x-\\symbfit \\mu)^T}_{1\\times d}\\right) \\\\\n  & = \\text{E}(\\symbfit x\\symbfit x^T)-\\symbfit \\mu\\symbfit \\mu^T \\\\\n\\end{split}\n\\] The entries of the covariance matrix \\(\\sigma_{ij} =\\text{Cov}(x_i, x_j)\\) describe the covariance between the random variables \\(x_i\\) and \\(x_j\\). The covariance matrix is symmetric, hence \\(\\sigma_{ij}=\\sigma_{ji}\\). The diagonal entries \\(\\sigma_{ii}  = \\text{Cov}(x_i, x_i) = \\text{Var}(x_i) = \\sigma_i^2\\) correspond to the variances of the components of \\(\\symbfit x\\). The covariance matrix is by construction positive semi-definite, i.e. the eigenvalues of \\(\\symbfit \\Sigma\\) are all positive or equal to zero.\nHowever, wherever possible one will aim to use models with non-singular covariance matrices, with all eigenvalues positive, so that the covariance matrix is invertible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#correlation-matrix",
    "href": "02-probability.html#correlation-matrix",
    "title": "2  Probability",
    "section": "2.10 Correlation matrix",
    "text": "2.10 Correlation matrix\nThe correlation matrix \\(\\symbfit P\\) (“upper case rho”, not “upper case p”) is the variance standardised version of the covariance matrix \\(\\symbfit \\Sigma\\).\nSpecifically, denote by \\(\\symbfit V\\) the diagonal matrix containing the variances \\[\n\\symbfit V= \\begin{pmatrix}\n    \\sigma_{11} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma_{dd}\n\\end{pmatrix}\n\\] then the correlation matrix \\(\\symbfit P\\) is given by \\[\n\\symbfit P= (\\rho_{ij}) = \\begin{pmatrix}\n    1 & \\dots & \\rho_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{d1} & \\dots & 1\n\\end{pmatrix}   = \\symbfit V^{-1/2} \\, \\symbfit \\Sigma\\, \\symbfit V^{-1/2}\n\\] Like the covariance matrix the correlation matrix is symmetric. The elements of the diagonal of \\(\\symbfit P\\) are all set to 1.\nEquivalently, in component notation the correlation between \\(x_i\\) and \\(x_j\\) is given by \\[\n\\rho_{ij} = \\text{Cor}(x_i,x_j) = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}\n\\]\nUsing the above, a covariance matrix can be factorised into the product of standard deviations \\(\\symbfit V^{1/2}\\) and the correlation matrix as follows: \\[\n\\symbfit \\Sigma= \\symbfit V^{1/2}\\, \\symbfit P\\,\\symbfit V^{1/2}\n\\]\n\n\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Academic Press.\n\n\nWhittle, P. 2000. Probability via Expectation. 3rd ed. Springer. https://doi.org/10.1007/978-1-4612-0509-8.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#footnotes",
    "href": "02-probability.html#footnotes",
    "title": "2  Probability",
    "section": "",
    "text": "For scalar random variables many texts use upper case to designate the random variable and lower case for its realisations. However, this convention quickly breaks down in multivariate statistics when dealing with random vectors and random matrices. Hence, we use upper case primarily to indicate a matrix quantity (in bold type). Upper case (in plain type) may denote sets and some scalar quantities traditionally written in upper case (e.g. \\(R^2\\), \\(K\\)).↩︎\nIn our notational conventions, a vector \\(\\symbfit x\\) is written in lower case in bold type, a matrix \\(\\symbfit M\\) in upper case in bold type. Hence random vectors and matrices as well as their realisations are indicated in bold type, with vectors given in lower case and matrices in upper case. Hence, as for scalar variables, upper vs. lower case does not indicate randomness vs. realisation.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "03-transformations.html",
    "href": "03-transformations.html",
    "title": "3  Transformations",
    "section": "",
    "text": "3.1 Affine or location-scale transformation of random variables\nSuppose \\(x \\sim F_x\\) is a scalar random variable. The random variable \\[y= a + b x\\] is a location-scale transformation or affine transformation of \\(x\\), where \\(a\\) plays the role of the location parameter and \\(b\\) is the scale parameter. For \\(a=0\\) this is a linear transformation. If \\(b\\neq 0\\) then the transformation is invertible, with back-transformation \\[x = (y-a)/b\\] Invertible transformations provide a one-to-one map between \\(x\\) and \\(y\\).\nFor a random vector \\(\\symbfit x\\sim F_{\\symbfit x}\\) of dimension \\(d\\) the location-scale transformation is \\[\n\\symbfit y= \\symbfit a+ \\symbfit B\\symbfit x\n\\] where \\(\\symbfit a\\) (a \\(m \\times 1\\) vector) is the location parameter and \\(\\symbfit B\\) (a \\(m \\times d\\) matrix) the scale parameter For \\(m=d\\) (square \\(\\symbfit B\\)) and \\(\\det(\\symbfit B) \\neq 0\\) the affine transformation is invertible with back-transformation \\[\\symbfit x= \\symbfit B^{-1}(\\symbfit y-\\symbfit a)\\]\nIf \\(x\\) is a continuous random variable with density \\(f_{x}(x)\\) and assuming an invertible transformation the density for \\(y\\) is given by \\[\nf_{y}(y)=|b|^{-1} f_{x} \\left( \\frac{y-a}{b}\\right)\n\\] where \\(|b|\\) is the absolute value of \\(b\\). Likewise, assuming an invertible transformation for a continous random vector \\(\\symbfit x\\) with density \\(f_{\\symbfit x}(\\symbfit x)\\) the density for \\(\\symbfit y\\) is given by \\[\nf_{\\symbfit y}(\\symbfit y)=|\\det(\\symbfit B)|^{-1} f_{\\symbfit x} \\left( \\symbfit B^{-1}(\\symbfit y-\\symbfit a)\\right)\n\\] where \\(|\\det(\\symbfit B)|\\) is the absolute value of the determinant \\(\\det(\\symbfit B)\\).\nThe transformed random variable \\(y \\sim F_y\\) has mean \\[\\text{E}(y) = a + b \\mu_x\\] and variance \\[\\text{Var}(y) = b^2 \\sigma^2_x\\] where \\(\\text{E}(x) = \\mu_x\\) and \\(\\text{Var}(x) = \\sigma^2_x\\) are the mean and variance of the original variable \\(x\\).\nThe mean and variance of the transformed random vector \\(\\symbfit y\\sim F_{\\symbfit y}\\) is \\[\\text{E}(\\symbfit y)=\\symbfit a+ \\symbfit B\\,\\symbfit \\mu_{\\symbfit x}\\] and \\[\\text{Var}(\\symbfit y)= \\symbfit B\\,\\symbfit \\Sigma_{\\symbfit x} \\,\\symbfit B^T\\] where \\(\\text{E}(\\symbfit x)=\\symbfit \\mu_{\\symbfit x}\\) and \\(\\text{Var}(\\symbfit x)=\\symbfit \\Sigma_{\\symbfit x}\\) are the mean and variance of the original random vector \\(\\symbfit x\\).\nThe constants \\(\\symbfit a\\) and \\(\\symbfit B\\) (or \\(a\\) and \\(b\\) in the univariate case) are the parameters of the location-scale family \\(F_{\\symbfit y}\\) created from \\(F_{\\symbfit x}\\). Many important distributions are location-scale families such as the normal distribution (cf. Section 5.4 and Section 5.4) and the location-scale \\(t\\)-distribution (Section 4.7.1). Furthermore, key procedures in multivariate statistics such as orthogonal transformations (including PCA) or whitening transformations (e.g. the Mahalanobis transformation) are affine transformations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#general-invertible-transformation-of-random-variables",
    "href": "03-transformations.html#general-invertible-transformation-of-random-variables",
    "title": "3  Transformations",
    "section": "3.2 General invertible transformation of random variables",
    "text": "3.2 General invertible transformation of random variables\nAs above we assume \\(x \\sim F_x\\) is a scalar random variable and \\(\\symbfit x\\sim F_{\\symbfit x}\\) is a random vector.\nAs a generalisation of invertible affine transformations we now consider general invertible transformations. For a scalar random variable we assume the transformation is specified by \\(y(x) = h(x)\\) and the back-transformation by \\(x(y) = h^{-1}(y)\\) For a random vector we assume \\(\\symbfit y(\\symbfit x) = \\symbfit h(\\symbfit x)\\) is invertible with backtransformation \\(\\symbfit x(\\symbfit y) = \\symbfit h^{-1}(\\symbfit y)\\).\nIf \\(x\\) is a continuous random variable with density \\(f_{x}(x)\\) the density of the transformed variable \\(y\\) can be computed exactly and is given by \\[\nf_y(y) =\\left| D x(y) \\right|\\, f_x(x(y))\n\\] where \\(D x(y)\\) is the derivative of the inverse transformation \\(x(y)\\).\nLikewise, for a continuous random vector \\(\\symbfit x\\) with density \\(f_{\\symbfit x}(\\symbfit x)\\) the density for \\(\\symbfit y\\) is obtained by \\[\nf_{\\symbfit y}(\\symbfit y) = |\\det\\left( D\\symbfit x(\\symbfit y) \\right)| \\,\\,  f_{\\symbfit x}\\left( \\symbfit x(\\symbfit y) \\right)\n\\] where \\(D\\symbfit x(\\symbfit y)\\) is the Jacobian matrix of the inverse transformation \\(\\symbfit x(\\symbfit y)\\).\nThe mean and variance of the transformed random variable can typically only be approximated. Assume that \\(\\text{E}(x) = \\mu_x\\) and \\(\\text{Var}(x) = \\sigma^2_x\\) are the mean and variance of the original random variable \\(x\\) and \\(\\text{E}(\\symbfit x)=\\symbfit \\mu_{\\symbfit x}\\) and \\(\\text{Var}(\\symbfit x)=\\symbfit \\Sigma_{\\symbfit x}\\) are the mean and variance of the original random vector \\(\\symbfit x\\). In the delta method the transformation \\(y(x)\\) resp. \\(\\symbfit y(\\symbfit x)\\) is linearised around the mean \\(\\mu_x\\) respectively \\(\\symbfit \\mu_{\\symbfit x}\\) and the mean and variance resulting from the linear transformation is reported.\nSpecifically, the linear approximation for the scalar-valued function is \\[\ny(x) \\approx y\\left(\\mu_x\\right) + D y\\left(\\mu_x\\right)\\, \\left(x-\\mu_x\\right)\n\\] where \\(D y(x) = y'(x)\\) is the first derivative of the transformation \\(y(x)\\) and \\(D y\\left(\\mu_x\\right)\\) is the first derivative evaluated at the mean \\(\\mu_x\\), and for the vector-valued function \\[\n\\symbfit y(\\symbfit x) \\approx \\symbfit y\\left(\\symbfit \\mu_{\\symbfit x}\\right) + D \\symbfit y\\left(\\symbfit \\mu_{\\symbfit x}\\right) \\, \\left(\\symbfit x-\\symbfit \\mu_{\\symbfit x}\\right)\n\\] where \\(D \\symbfit y(\\symbfit x)\\) is the Jacobian matrix (vector derivative) for the transformation \\(\\symbfit y(\\symbfit x)\\) and \\(D \\symbfit y\\left(\\symbfit \\mu_{\\symbfit x}\\right)\\) is the Jacobian matrix evaluated at the mean \\(\\symbfit \\mu_{\\symbfit x}\\).\nIn the univariate case the delta method yields as approximation for the mean and variance of the transformed random variable \\(y\\) \\[\n\\text{E}(y) \\approx y\\left(\\mu_x\\right)\n\\] and \\[\n\\text{Var}(y)\\approx \\left(D y\\left(\\mu_x\\right)\\right)^2 \\, \\sigma^2_x  \n\\]\nFor the vector random variable \\(\\symbfit y\\) the delta method yields \\[\\text{E}(\\symbfit y)\\approx\\symbfit y\\left(\\symbfit \\mu_{\\symbfit x}\\right)\\] and \\[\n\\text{Var}(\\symbfit y)\\approx D \\symbfit y\\left(\\symbfit \\mu_{\\symbfit x}\\right) \\, \\symbfit \\Sigma_{\\symbfit x} \\, D\\symbfit y\\left(\\symbfit \\mu_{\\symbfit x}\\right)^T\n\\]\nAssuming \\(y(x) = a + b x\\), with \\(x(y) = (y-a)/b\\), \\(D y(x) = b\\) and \\(D x(y) = b^{-1}\\), recovers the univariate location-scale transformation. Likewise, assuming \\(\\symbfit y(\\symbfit x) = \\symbfit a+ \\symbfit B\\symbfit x\\), with \\(\\symbfit x(\\symbfit y) = \\symbfit B^{-1}(\\symbfit y-\\symbfit a)\\), \\(D\\symbfit y(\\symbfit x) = \\symbfit B\\) and \\(D\\symbfit x(\\symbfit y) = \\symbfit B^{-1}\\), recovers the multivariate location-scale transformation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#exponential-tilting-and-exponential-families",
    "href": "03-transformations.html#exponential-tilting-and-exponential-families",
    "title": "3  Transformations",
    "section": "3.3 Exponential tilting and exponential families",
    "text": "3.3 Exponential tilting and exponential families\nAnother way to change the distribution of a random variable is by exponential tilting.\nSuppose there is a vector valued function \\(\\symbfit u(x)\\) where each component is a transformation of \\(x\\), usually a simple function such the identity \\(x\\), the square \\(x^2\\), the logarithm \\(\\log(x)\\) etc. These are called the canonical statistics. Typically, the dimension of \\(\\symbfit u(x)\\) is small.\nThe exponential tilt of a base distribution \\(B\\) with density or probability mass function \\(b(x)\\) towards the linear combination \\(\\symbfit \\eta^T \\symbfit u(x)\\) of the canonical statistics \\(\\symbfit u(x)\\) yields the distribution family \\(P_{\\symbfit \\eta}\\) with density or probability mass function \\[\np(x|\\symbfit \\eta) =    \\underbrace{e^{ \\symbfit \\eta^T \\symbfit u(x)}}_{\\text{exponential tilt}}\\, b(x) \\, /\\, e^{ \\psi(\\symbfit \\eta)}\n\\] where \\(\\symbfit \\eta\\) are the canonical parameters. The normalising factor \\(e^{ \\psi(\\symbfit \\eta)}\\) ensures that \\(p(x|\\symbfit \\eta)\\) integrates to one following the exponential tilt.\nThe corresponding log-density / log probability mass function is \\[\n\\log p(x|\\symbfit \\eta) =  \\symbfit \\eta^T \\symbfit u(x) + \\log b(x) - \\psi(\\symbfit \\eta)\n\\] The log-normaliser or log-partition function \\(\\psi(\\symbfit \\eta)\\) is obtained by computing \\[\n\\psi(\\symbfit \\eta) = \\log \\int_x \\, e^{ \\symbfit \\eta^T \\symbfit u(x)}\\, b(x) \\, dx\n\\] The set of values of \\(\\symbfit \\eta\\) for which the integral is finite and hence \\(\\psi(\\symbfit \\eta) &lt; \\infty\\) defines the parameter space.\nThe distribution family \\(P_{\\symbfit \\eta}\\) obtained by exponential tiling is called an exponential family.\nMany commonly used distribution families are exponential families (most importantly the normal distribution). Exponential families are extremely important in probability and statistics. They provide highly effective models for statistical learning using entropy, likelihood and Bayesian approaches, allow for substantial data reduction via minimal sufficiency, and provide the basis of generalised linear models. Furthermore, exponential families often enable to generalise probabilistic results valid for the normal distribution to more general settings.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#sums-of-random-variables-and-convolution",
    "href": "03-transformations.html#sums-of-random-variables-and-convolution",
    "title": "3  Transformations",
    "section": "3.4 Sums of random variables and convolution",
    "text": "3.4 Sums of random variables and convolution\nSuppose we have a sum of \\(n\\) independent and identically distributed (iid) random variables. \\[\ny = x_1 + x_2 + \\ldots + x_n\n\\] where each \\(x_i \\sim F_x\\) with density or probability mass function \\(f_x(x)\\). The density or probability mass function for \\(y\\) is obtained by repeated application of convolution (symbolised by the \\(\\ast\\) operator): \\[\nf_y(y) = \\left(f_{x_1} \\ast f_{x_2} \\ast \\ldots f_{x_n}\\right)(y)\n\\]\nThe convolution of two functions is defined as (continuous case) \\[\n\\left(f_{x_1}\\ast f_{x_2}\\right)(y)=\\int_x f_{x_1}(x)\\, f_{x_2}(y-x) dx\n\\] and (discrete case) \\[\n\\left(f_{x_1}\\ast f_{x_2}\\right)(y)=\\sum_x f_{x_1}(x)\\, f_{x_2}(y-x)\n\\] Convolution is commutative and associative so it can be applied in any order to compute the convolution of multiple functions. Furthermore, the convolution of probability densities / mass function yields another probability density / mass function.\nMany commonly used random variables can be viewed as the outcome of convolutions. For example, the sum of Bernoulli variables yields a binomial random variable and the sum of normal variables yields another normal random variable.\nSee also: list of convolutions of probability distributions.\nThe central limit theorem, first postulated by Abraham de Moivre (1667–1754) and later proved by Pierre-Simon Laplace (1749–1827) asserts that, under appropriate conditions, the distribution of the sum of independent and identically distributed random variables converges in the limit of large \\(n\\) to a normal distribution (Section 4.4), even if the individual random variables are not normal. In other words, it asserts that for large \\(n\\) the convolution of \\(n\\) identical distributions typically converges to the normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "04-univariate.html",
    "href": "04-univariate.html",
    "title": "4  Univariate distributions",
    "section": "",
    "text": "4.1 Bernoulli distribution\nThe Bernoulli distribution \\(\\text{Ber}(\\theta)\\) is the simplest of all distribution families. It is named after Jacob Bernoulli (1655-1705) who also discovered the law of large numbers.\nIt describes a discrete binary random variable with two states \\(x=0\\) (“failure”) and \\(x=1\\) (“success”), where the parameter \\(\\theta \\in [0,1]\\) is the probability of “success”. Often the Bernoulli distribution is also referred to as “coin tossing” model with the two outcomes “heads” and “tails”.\nCorrespondingly, the probability mass function of \\(\\text{Ber}(\\theta)\\) is \\[\np(x=0 | \\theta) = \\text{Pr}(\\text{\"failure\"}| \\theta) = 1-\\theta  \n\\] and \\[\np(x=1| \\theta) = \\text{Pr}(\\text{\"success\"}| \\theta) = \\theta\n\\] A compact way to write the PMF of the Bernoulli distribution is \\[\np(x | \\theta ) = \\theta^{x} (1-\\theta)^{1-x}\n\\] The log PMF is \\[\n\\log p(x | \\theta ) = x \\log \\theta + (1-x)\\log (1-\\theta)\n\\]\nIf a random variable \\(x\\) follows the Bernoulli distribution we write \\[\nx \\sim \\text{Ber}(\\theta) \\,.\n\\] The expected value is \\(\\text{E}(x) = \\theta\\) and the variance is \\(\\text{Var}(x) = \\theta (1 - \\theta)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#sec-binomdist",
    "href": "04-univariate.html#sec-binomdist",
    "title": "4  Univariate distributions",
    "section": "4.2 Binomial distribution",
    "text": "4.2 Binomial distribution\nClosely related to the Bernoulli distribution is the binomial distribution \\(\\text{Bin}(n, \\theta)\\) which results from repeating a Bernoulli experiment \\(n\\) times and counting the number of successes among the \\(n\\) trials (without keeping track of the ordering of the experiments). Thus, if \\(x_1, \\ldots, x_n\\) are \\(n\\) independent \\(\\text{Ber}(\\theta)\\) random variables then \\(y = \\sum_{i=1}^n x_i\\) is distributed as \\(\\text{Bin}(n, \\theta)\\).\nIf a random variable \\(y\\) follows the binomial distribution we write \\[\ny \\sim \\text{Bin}(n, \\theta)\\,\n\\]\nThe corresponding probability mass function is: \\[\np(y | n, \\theta) = \\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}\n\\] with support \\(y \\in \\{ 0, 1, 2, \\ldots, n\\}\\). The binomial coefficient \\(\\binom{n}{y}\\) is needed to account for the multiplicity of ways (orderings of samples) in which we can observe \\(y\\) successes.\nThe expected value is \\(\\text{E}(y) = n \\theta\\) and the variance is \\(\\text{Var}(y) = n \\theta (1 - \\theta)\\).\nIf we standardise the support of the binomial variable to the unit interval with \\(\\frac{y}{n} \\in \\left\\{0,\\frac{1}{n},...,1\\right\\}\\) then the mean is \\(\\text{E}\\left(\\frac{y}{n}\\right) = \\theta\\) and the variance is \\(\\text{Var}\\left(\\frac{y}{n}\\right)=\\frac{\\theta (1-\\theta)}{n}\\).\nFor \\(n=1\\) the binomial distribution reduces to the Bernoulli distribution (Section 4.1).\n\n\n\n\n\n\nFigure 4.1: Binomial urn model.\n\n\n\nThe binomial distribution may be illustrated by an urn model distributing \\(n\\) items into two bins (Figure 4.1).\nAs a result of the central limit theorem, the binomial distribution, obtained as the convolution of \\(n\\) Bernoulli distributions, can for large \\(n\\) be well approximated by a normal distribution (this is known as the De Moivre–Laplace theorem).\n\n\n\n\n\n\nR code\n\n\n\nThe probability mass function of the binomial distribution is given by dbinom(), the cumulative distribution function is pbinom() and the quantile function is qbinom(). The binomial coefficient is computed by choose().",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#sec-betadist",
    "href": "04-univariate.html#sec-betadist",
    "title": "4  Univariate distributions",
    "section": "4.3 Beta distribution",
    "text": "4.3 Beta distribution\n\n4.3.1 Standard parameterisation\nA beta-distributed random variable is denoted by \\[\nx \\sim \\text{Beta}(\\alpha, \\beta)\n\\] where the support is \\(x \\in [0,1]\\) and \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\) are two shape parameters.\nThe density of the beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\) is \\[\np(x | \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}\n\\] This depends on the beta function defined as \\[\nB(z_1, z_1) = \\frac{ \\Gamma(z_1) \\Gamma(z_2)}{\\Gamma(z_1 + z_2)}\n\\]\n\n\n\n\n\n\nFigure 4.2: Shapes of the density of the beta distribution.\n\n\n\nThe beta distribution is very flexible and can assume a number of different shapes, depending on the value of \\(\\alpha\\) and \\(\\beta\\). For example, for \\(\\alpha=\\beta=1\\) it becomes the uniform distribution over the unit interval (see Figure 4.2).\n\n\n\n\n\n\nFigure 4.3: Stick breaking visualisation of a beta random variable.\n\n\n\nA beta random variable can be visualised as breaking a unit stick of length one into two pieces of length \\(x\\) and \\(1-x\\) (Figure 4.3).\n\n\n\n\n\n\nR code\n\n\n\nThe probability density function of the beta distribution is given by dbeta(), the cumulative distribution function is pbeta() and the quantile function is qbeta().\n\n\n\n\n4.3.2 Mean parametrisation\nInstead of employing \\(\\alpha\\) and \\(\\beta\\) as parameters another useful reparametrisation \\(\\text{Beta}(\\mu, k)\\) of the beta distribution is in terms of a mean parameter \\(\\mu \\in [0,1]\\) and a concentration parameter \\(k &gt; 0\\). These are given by \\[\nk=\\alpha+\\beta\n\\] and \\[\\mu = \\frac{\\alpha}{\\alpha+\\beta}\n\\] The original parameters can be recovered by \\(\\alpha= \\mu k\\) and \\(\\beta=(1-\\mu) k\\).\nThe mean and variance of the beta distribution expressed in terms of \\(\\mu\\) and \\(k\\) are \\[\n\\text{E}(x) = \\mu\n\\] and \\[\n\\text{Var}(x)=\\frac{\\mu (1-\\mu)}{k+1}\n\\] With increasing concentration parameter \\(k\\) the variance decreases and thus the probability mass becomes more concentrated around the mean.\nThe uniform distribution (with \\(\\alpha=\\beta=1\\)) corresponds to \\(\\mu=1/2\\) and \\(k=2\\).\nFinally, note that the mean and variance of the continuous beta distribution closely match those of the unit-standardised discrete binomial distribution above.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#sec-normaldist",
    "href": "04-univariate.html#sec-normaldist",
    "title": "4  Univariate distributions",
    "section": "4.4 Normal distribution",
    "text": "4.4 Normal distribution\nThe normal distribution is the most important continuous probability distribution. It is also called Gaussian distribution named after Carl Friedrich Gauss (1777–1855).\nThe univariate normal distribution \\(N(\\mu, \\sigma^2)\\) has two parameters \\(\\mu\\) (location) and \\(\\sigma^2\\) (scale) and support \\(x \\in ]-\\infty, \\infty[\\).\n\\[\nx \\sim N(\\mu,\\sigma^2)\n\\] with mean \\[\n\\text{E}(x)=\\mu\n\\] and variance \\[\n\\text{Var}(x) = \\sigma^2\n\\]\nProbability density function (PDF): \\[\np(x| \\mu, \\sigma^2)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\n\n\n\n\n\n\n\n\nFigure 4.4: Probability density function (left) and cumulative density function (right) of the standard normal distribution.\n\n\n\n\n\nThe standard normal distribution is \\(N(0, 1)\\) with mean 0 and variance 1. The cumulative distribution function (CDF) of the standard normal \\(N(0,1)\\) is \\[\n\\Phi (x ) = \\int_{-\\infty}^{x} p(x'| \\mu=0, \\sigma^2=1) dx'\n\\] There is no analytic expression for \\(\\Phi(x)\\). The inverse \\(\\Phi^{-1}(p)\\) is called the quantile function of the standard normal distribution.\nFigure 4.4 shows the PDF and CDF of the standard normal distribution.\n\n\n\n\n\n\nR code\n\n\n\nThe normal probability density function is given by dnorm(), the cumulative distribution function is pnorm() and the quantile function is qnorm().",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#sec-gamdist",
    "href": "04-univariate.html#sec-gamdist",
    "title": "4  Univariate distributions",
    "section": "4.5 Gamma distribution and special cases",
    "text": "4.5 Gamma distribution and special cases\nThe gamma distribution is widely used in statistics, and also appears in various parametrisations and under some other names, such as univariate Wishart and scaled chi-squared distribution\n\n4.5.1 Standard parametrisation\nThe gamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\) is a continuous distribution with two parameters \\(\\alpha&gt;0\\) (shape) and \\(\\theta&gt;0\\) (scale): \\[\nx \\sim\\text{Gam}(\\alpha, \\theta)\n\\] and support \\(x \\in [0, \\infty[\\) with mean \\[\\text{E}(x)=\\alpha \\theta\\] and variance \\[\\text{Var}(x) = \\alpha \\theta^2\\]\nThe gamma distribution is also often used with a rate parameter \\(\\beta=1/\\theta\\). Therefore one needs to pay attention which parametrisation is used.\nThe probability density function (PDF) is: \\[\np(x| \\alpha, \\theta)=\\frac{1}{\\Gamma(\\alpha) \\theta^{\\alpha} } x^{\\alpha-1} e^{-x/\\theta}\n\\]\n\n\n\n\n\n\nR code\n\n\n\nThe density of the gamma distribution is available in the function dgamma(). The cumulative density function is pgamma() and the quantile function is qgamma().\n\n\n\n\n4.5.2 Wishart parametrisation and scaled chi-squared distribution\nThe gamma distribution is often used with a different set of parameters \\(k=2 \\alpha\\) &gt; 0 and \\(s^2 =\\theta/2 &gt; 0\\) (hence conversely \\(\\alpha = k/2\\) and \\(\\theta=2 s^2\\)). In this form it is known as univariate or one-dimensional Wishart distribution \\[\n\\text{W}_1\\left(s^2, k \\right)\n\\] named after John Wishart (1898–1954). In the Wishart parametrisation the mean is \\[\n\\text{E}(x) = k s^2\n\\] and the variance \\[\n\\text{Var}(x) = 2 k s^4\n\\]\nAnother name for the one-dimensional Wishart distribution with exactly the same parametrisation is scaled chi-squared distribution denoted as \\[\ns^2 \\text{$\\chi^2_{k}$}\n\\]\nFinally, we also often employ the Wishart distribution in mean parametrisation \\[\n\\text{W}_1\\left(s^2= \\frac{\\mu}{k}, k \\right)\n\\] with parameters \\(\\mu = k s^2 &gt;0\\) and \\(k &gt; 0\\) (and thus \\(\\theta = 2 \\mu /k\\)). In this parametrisation the mean is \\[\n\\text{E}(x) = \\mu\n\\] and the variance \\[\n\\text{Var}(x) = \\frac{2 \\mu^2}{k}\n\\]\n\n\n4.5.3 Construction as sum of squared normals\nA gamma distributed variable can be constructed as follows. Assume \\(k\\) independent normal random variables with mean 0 and variance \\(s^2\\): \\[z_1,z_2,\\dots,z_k\\sim N(0,s^2)\\] Then the sum of the squares \\[\nx = \\sum_{i=1}^{k} z_i^2\n\\] follows the distribution \\[\n\\begin{split}\nx \\sim & \\phantom{=} s^2 \\text{$\\chi^2_{k}$} \\\\\n       & = \\text{W}_1\\left( s^2, k \\right)\\\\\n       & =\\text{Gam}\\left(\\alpha=\\frac{k}{2}, \\theta = 2 s^2\\right)\n\\end{split}\n\\]\n\n\n4.5.4 Chi-squared distribution\n\n\n\n\n\n\n\n\nFigure 4.5: Density of the chi-squared distribution.\n\n\n\n\n\nThe chi-squared distribution \\(\\text{$\\chi^2_{k}$}\\) is a special one-parameter restriction of the gamma resp. Wishart distribution obtained when setting \\(s^2=1\\) or, equivalently, \\(\\theta = 2\\) or \\(\\mu = k\\).\nIt has mean \\(\\text{E}(x)=k\\) and variance \\(\\text{Var}(x)=2k\\). The chi-squared distribution \\(\\text{$\\chi^2_{k}$}\\) equals \\(\\text{Gam}(\\alpha=k/2, \\theta=2) =  \\text{W}_1\\left(1, k \\right)\\).\nFigure 4.5 shows plots the density of the chi-squared distribution for degrees of freedom \\(k=1\\) and \\(k=3\\).\n\n\n\n\n\n\nR code\n\n\n\nThe density of the chi-squared distribution is given by dchisq(). The cumulative density function is pchisq() and the quantile function is qchisq().\n\n\n\n\n4.5.5 Exponential distribution\nThe exponential distribution \\(\\text{Exp}(\\theta)\\) with scale parameter \\(\\theta\\) is another special one-parameter restriction of the gamma distribution with shape parameter set to \\(\\alpha=1\\) (or equivalently \\(k=2\\)).\n\\(\\text{Exp}(\\theta)\\) equals \\(\\text{Gam}(\\alpha=1, \\theta) = \\text{W}_1(s^2=\\theta/2, k=2)\\).\nThe density of the exponential distribution is \\[\np(x|  \\theta)=\\frac{1}{\\theta } e^{-x/\\theta}\n\\] with mean \\(\\text{E}(x)=\\theta\\) and variance \\(\\text{Var}(x) = \\theta^2\\).\nJust like the gamma distribution the exponential distribution is also often specified using a rate parameter \\(\\beta= 1/\\theta\\) instead of a scale parameter \\(\\theta\\).\n\n\n\n\n\n\nR code\n\n\n\nThe command dexp() returns the density of the exponential distribution, pexp() is the corresponding cumulative density function and qexp() is the quantile function.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#sec-invgamdist",
    "href": "04-univariate.html#sec-invgamdist",
    "title": "4  Univariate distributions",
    "section": "4.6 Inverse gamma distribution",
    "text": "4.6 Inverse gamma distribution\nAlso know as inverse univariate Wishart distribution.\n\n4.6.1 Standard parametrisation\nA random variable \\(x\\) following an inverse gamma distribution is denoted by \\[\nx \\sim \\text{Inv-Gam}(\\alpha, \\beta)\n\\] with two parameters \\(\\alpha &gt;0\\) (shape parameter) and \\(\\beta &gt;0\\) (scale parameter) and support \\(x &gt;0\\).\nThe inverse of \\(x\\) is then gamma distributed \\[\n\\frac{1}{x} \\sim \\text{Gam}(\\alpha, \\theta=\\beta^{-1})\n\\] where \\(\\alpha\\) is the shared shape parameter and \\(\\theta\\) the scale parameter of the gamma distribution.\nThe inverse gamma distribution \\(\\text{Inv-Gam}(\\alpha, \\beta)\\) has density \\[\np(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} (1/x)^{\\alpha+1} e^{-\\beta/x}\n\\]\nThe mean of the inverse gamma distribution is \\[\\text{E}(x) = \\frac{\\beta}{\\alpha-1}\\] and the variance \\[\\text{Var}(x) = \\frac{\\beta^2}{(\\alpha-1)^2 (\\alpha-2)}\\]\nThus, for the mean to exist we have the restriction \\(\\alpha&gt;1\\) and for the variance to exist \\(\\alpha&gt;2\\).\n\n\n\n\n\n\nR code\n\n\n\nThe extraDistr package implements the inverse gamma distribution. The function extraDistr::dinvgamma() provides the density, the function extraDistr::pinvgamma() returns the corresponding cumulative density function and extraDistr::qinvgamma() is the quantile function.\n\n\n\n\n4.6.2 Wishart parametrisation\nThe inverse gamma distribution is frequently used with a different set of parameters \\(\\psi = 2\\beta\\) (scale parameter) and \\(\\nu = 2\\alpha\\) (shape parameter), or conversely \\(\\alpha=\\nu/2\\) and \\(\\beta=\\psi/2\\). In this form it is called one-dimensional inverse Wishart distribution \\[\n\\text{W}^{-1}_1(\\psi, \\nu)\n\\] with mean given by \\[\n\\text{E}(x) = \\frac{\\psi}{\\nu-2}\n\\] for \\(\\nu&gt;2\\) and variance \\[\n\\text{Var}(x) =\\frac{2 \\psi^2}{(\\nu-2)^2 (\\nu-4) }  \n\\] for \\(\\nu &gt;4\\).\nThe inverse univariate Wishart and univariate Wishart distributions are linked. If a random variable \\(x\\) is inverse Wishart distributed \\[\nx \\sim \\text{W}^{-1}_1(\\psi, \\nu)\n\\] then the inverse of \\(x\\) is Wishart distributed with inverted scale parameter: \\[\\frac{1}{x} \\sim \\text{W}_1(s^2=\\psi^{-1}, k=\\nu)\\] where \\(k\\) is the shape parameter and \\(s^2\\) the scale parameter of the Wishart distribution.\nInstead of \\(\\psi\\) and \\(\\nu\\) we may also equivalently use \\(\\kappa=\\nu-2\\) and \\(\\mu = \\psi/(\\nu-2)\\) as parameters for the inverse Wishart distribution, so that \\[\n\\text{W}^{-1}_1(\\psi=\\kappa \\mu, \\nu=\\kappa+2)\n\\] has mean \\[\\text{E}(x) = \\mu\\] with \\(\\kappa&gt;0\\) and the variance is \\[\\text{Var}(x) = \\frac{2 \\mu^2}{\\kappa-2}\\] with \\(\\kappa&gt;2\\). This mean parametrisation is useful when employing the inverse gamma distribution as prior and posterior.\nFinally, with \\(\\text{W}^{-1}_1(\\psi=\\nu \\tau^2, \\nu)\\), where \\(\\tau^2  = \\mu \\frac{ \\kappa}{\\kappa+2} = \\frac{\\psi}{\\nu}\\) is a biased mean parameter, we get the scaled inverse chi-squared distribution \\(\\tau^2 \\text{Inv-$\\chi^2_{\\nu}$}\\) with \\[\n\\text{E}(x) = \\tau^2 \\frac{ \\nu}{\\nu-2}\n\\] for \\(\\nu&gt;2\\) and \\[\n\\text{Var}(x) =\\frac{2 \\tau^4}{\\nu-4} \\frac{\\nu^2}{(\\nu-2)^2}\n\\] for \\(\\nu &gt;4\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "04-univariate.html#location-scale-t-distribution-and-special-cases",
    "href": "04-univariate.html#location-scale-t-distribution-and-special-cases",
    "title": "4  Univariate distributions",
    "section": "4.7 Location-scale \\(t\\)-distribution and special cases",
    "text": "4.7 Location-scale \\(t\\)-distribution and special cases\n\n4.7.1 Location-scale \\(t\\)-distribution\nThe location-scale \\(t\\)-distribution \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\) is a generalisation of the normal distribution. It has an additional parameter \\(\\nu &gt; 0\\) (degrees of freedom) that controls the probability mass in the tails. For small values of \\(\\nu\\) the distribution is heavy-tailed — indeed so heavy that for \\(\\nu \\leq 1\\) even the mean is not defined and for \\(\\nu \\leq 2\\) the variance is undefined.\nThe probability density of \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\) is \\[\np(x | \\mu, \\tau^2, \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {\\sqrt{\\pi \\nu \\tau^2}  \\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{(x-\\mu)^2}{\\nu \\tau^2} \\right)^{-(\\nu+1)/2}\n\\] with support \\(x \\in ]-\\infty, \\infty[\\). The mean is (for \\(\\nu&gt;1\\)) \\[\n\\text{E}(x) = \\mu\n\\] and the variance (for \\(\\nu&gt;2\\)) \\[\n\\text{Var}(x) = \\tau^2 \\frac{\\nu}{\\nu-2}\n\\]\nFor \\(\\nu \\rightarrow \\infty\\) the location-scale \\(t\\)-distribution \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\) becomes the normal distribution \\(N(\\mu, \\tau^2)\\).\n\n\n\n\n\n\nFigure 4.6: The location-scale \\(t\\) distribution and its relatives.\n\n\n\nFigure 4.6 illustrates the relationship of the location-scale \\(t\\) distribution \\(\\text{lst}(\\mu, \\tau^2, \\nu)\\) with related distributions such as the normal distribution \\(N(\\mu, \\tau^2)\\), Student’s \\(t\\)-distribution \\(t_\\nu\\) and the Cauchy distribution \\(\\text{Cau}(\\mu, \\tau)\\) discussed further below.\n\n\n\n\n\n\nR code\n\n\n\nThe package extraDistr implements the location-scale \\(t\\)-distribution. The function extraDistr::dlst() returns the density, extraDistr::plst() is the corresponding cumulative density function and extraDistr::qlst() is the quantile function.\n\n\n\n\n4.7.2 Location-scale \\(t\\)-distribution as compound distribution\nSuppose that \\[\nx | s^2 \\sim N(\\mu,s^2)\n\\] with corresponding density \\(p(x | s^2)\\) and mean \\(\\text{E}(x | s^2) = \\mu\\) and variance \\(\\text{Var}(x|s^2) = s^2\\).\nNow let the variance \\(s^2\\) be distributed as univariate inverse gamma / inverse Wishart \\[\ns^2 \\sim  \\text{W}^{-1}_1(\\psi=\\kappa \\sigma^2, \\nu=\\kappa+2) = \\text{W}^{-1}_1(\\psi=\\tau^2\\nu, \\nu)\n\\] with corresponding density \\(p(s^2)\\) and mean \\(\\text{E}(s^2)  = \\sigma^2 = \\tau^2 \\nu/(\\nu-2)\\). Note we use here both the mean parametrisation (\\(\\sigma^2, \\kappa\\)) and the inverse chi-squared parametrisation (\\(\\tau^2, \\nu\\)).\nThe joint density for \\(x\\) and \\(s^2\\) is \\(p(x, s^2) = p(x | s^2) p(s^2)\\). We are interested in the marginal density for \\(x\\): \\[\np(x) = \\int p(x, s^2) ds^2  = \\int p(s^2)  p(x | s^2) ds^2\n\\] This is a compound distribution of a normal with fixed mean \\(\\mu\\) and variance \\(s^2\\) varying according the inverse gamma distribution. Calculating the integral results in the location-scale \\(t\\)-distribution with parameters \\[\nx \\sim  \\text{lst}\\left(\\mu, \\sigma^2 \\frac{\\kappa}{\\kappa+2}, \\kappa+2\\right) = \\text{lst}\\left(\\mu, \\tau^2, \\nu\\right)\n\\] with mean \\[\n\\text{E}(x) = \\mu\n\\] and variance \\[\n\\text{Var}(x) = \\sigma^2 =\\tau^2 \\frac{\\nu}{\\nu-2}\n\\]\nFrom the law of total expectation and variance we can also directly verify that \\[\n\\text{E}(x) = \\text{E}( \\text{E}(x |  s^2) ) =\\mu\n\\] and \\[\n\\text{Var}(x) = \\text{E}(\\text{Var}(x|s^2))+ \\text{Var}(\\text{E}(x|s^2)) = \\text{E}(s^2) = \\sigma^2 =\\tau^2 \\frac{\\nu}{\\nu-2}\n\\]\n\n\n4.7.3 Student’s \\(t\\)-distribution\nFor \\(\\mu=0\\) and \\(\\tau^2=1\\) the location-scale \\(t\\)-distribution becomes the Student’s \\(t\\)-distribution \\(t_\\nu\\). It is named after “Student” which was the pseudonym of William Sealy Gosset (1876–1937).\nIt has mean 0 (for \\(\\nu&gt;1\\)) and variance \\(\\frac{\\nu}{\\nu-2}\\) (for \\(\\nu&gt;2\\)).\nIt can thus be viewed as a generalisation of the standard normal distribution \\(N(0,1)\\).\nIf \\(y \\sim t_\\nu\\) then \\(x = \\mu + \\tau y\\) is distributed as \\(x \\sim \\text{lst}(\\mu, \\tau^2, \\nu)\\).\nFor \\(\\nu \\rightarrow \\infty\\) the \\(t\\)-distribution becomes equal to \\(N(0,1)\\).\nThe probability density of \\(t_\\nu\\) is \\[\np(x | \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {\\sqrt{\\pi \\nu}  \\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{x^2}{\\nu} \\right)^{-(\\nu+1)/2}\n\\] with support \\(x \\in ]-\\infty, \\infty[\\).\n\n\n\n\n\n\nR code\n\n\n\nThe command dt() returns the density of the \\(t\\)-distribution, pt() is the corresponding cumulative density function and qt() is the quantile function.\n\n\n\n\n4.7.4 Cauchy and standard Cauchy distribution\nFor \\(\\nu=1\\) the location-scale \\(t\\)-distribution becomes the Cauchy distribution \\(\\text{Cau}(\\mu, \\tau)\\) with density \\(p(x| \\mu, \\tau) = \\frac{\\tau}{\\pi (\\tau^2+(x-\\mu)^2)}\\). It is named after Augustin-Louis Cauchy (1789–1857).\nFor \\(\\nu=1\\) the \\(t\\)-distribution becomes the standard Cauchy distribution \\(\\text{Cau}(0, 1)\\) with density \\(p(x) = \\frac{1}{\\pi (1+x^2)}\\).\n\n\n\n\n\n\nR code\n\n\n\nThe command dcauchy() returns the density of the Cauchy distribution, pcauchy() is the corresponding cumulative density function and qcauchy() is the quantile function.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html",
    "href": "05-multivariate.html",
    "title": "5  Multivariate distributions",
    "section": "",
    "text": "5.1 Categorical distribution\nThe categorical distribution is a generalisation of the Bernoulli distribution from two classes to \\(K\\) classes.\nThe categorical distribution \\(\\text{Cat}(\\symbfit \\pi)\\) describes a discrete random variable with \\(K\\) states (“categories”, “classes”, “bins”) where the parameter vector \\(\\symbfit \\pi= (\\pi_1, \\ldots, \\pi_K)^T\\) specifies the probability of each of class so that \\(\\text{Pr}(\\text{\"class k\"}) = \\pi_k\\). The parameters satisfy \\(\\pi_k \\in [0,1]\\) and \\(\\sum_{k=1}^K \\pi_k = 1\\), hence there are \\(K-1\\) independent parameters in a categorical distribution (and not \\(K\\)).\nThere are two main ways to numerically represent “class k”:\nIn the following we use “one hot encoding”. Therefore sampling from a categorical distribution with parameters \\(\\symbfit \\pi\\) \\[\n\\symbfit x\\sim \\text{Cat}(\\symbfit \\pi)\n\\] yields a random index vector \\(\\symbfit x\\).\nThe corresponding probability mass function (PMF) can be written conveniently in terms of \\(x_k\\) as \\[\np(\\symbfit x| \\symbfit \\pi) = \\prod_{k=1}^K \\pi_k^{x_k} =\n\\begin{cases}\n   \\pi_k  & \\text{if } x_k = 1 \\\\\n\\end{cases}\n\\] and the log PMF as \\[\n\\log p(\\symbfit x| \\symbfit \\pi) = \\sum_{k=1}^K x_k \\log \\pi_k   =\n\\begin{cases}\n   \\log \\pi_k  & \\text{if } x_k = 1 \\\\\n\\end{cases}\n\\]\nIn order to be more explicit that the categorical distribution has \\(K-1\\) and not \\(K\\) parameters we rewrite the log-density with \\(\\pi_K = 1 - \\sum_{k=1}^{K-1} \\pi_k\\) and \\(x_K = 1 - \\sum_{k=1}^{K-1} x_k\\) as \\[\n\\begin{split}\n\\log p(\\symbfit x| \\symbfit \\pi) & =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + x_K \\log \\pi_K \\\\\n& =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_k  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\\\\n\\end{split}\n\\] Note that there is no particular reason to choose \\(\\pi_K\\) as dependent of the probabilities of the other classes, in its place any other of the \\(\\pi_k\\) may be selected.\nThe expected value is \\(\\text{E}(\\symbfit x) = \\symbfit \\pi\\), in component notation \\(\\text{E}(x_k) = \\pi_k\\). The covariance matrix is \\(\\text{Var}(\\symbfit x) = \\text{Diag}(\\symbfit \\pi) - \\symbfit \\pi\\symbfit \\pi^T\\), which in component notation is \\(\\text{Var}(x_i) = \\pi_i (1-\\pi_i)\\) and \\(\\text{Cov}(x_i, x_j) = -\\pi_i \\pi_j\\).\nThe form of the categorical covariance matrix follows directly from the definition of the variance \\(\\text{Var}(\\symbfit x) = \\text{E}( \\symbfit x\\symbfit x^T) - \\text{E}( \\symbfit x) \\text{E}( \\symbfit x)^T\\) and noting that \\(x_i^2 = x_i\\) and \\(x_i x_j = 0\\) if \\(i \\neq j\\). Furthermore, the categorical covariance matrix is singular by construction, as the \\(K\\) random variables \\(x_1, \\ldots, x_K\\) are dependent through the constraint \\(\\sum_{k=1}^K x_k = 1\\).\nFor \\(K=2\\) the categorical distribution reduces to the Bernoulli \\(\\text{Ber}(\\theta)\\) distribution, with \\(\\pi_1=\\theta\\) and \\(\\pi_2=1-\\theta\\) (Section 4.1).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#sec-catdist",
    "href": "05-multivariate.html#sec-catdist",
    "title": "5  Multivariate distributions",
    "section": "",
    "text": "by “integer encoding”, i.e. by the corresponding integer \\(k\\).\nby “one hot encoding”, i.e. by an indicator vector \\(\\symbfit x= (x_1, \\ldots, x_K)^T = (0, 0, \\ldots, 1, \\ldots, 0)^T\\) containing zeros everywhere except for the element \\(x_k=1\\) at position \\(k\\). Thus all \\(x_k \\in \\{ 0, 1\\}\\) and \\(\\sum_{k=1}^K x_k = 1\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#multinomial-distribution",
    "href": "05-multivariate.html#multinomial-distribution",
    "title": "5  Multivariate distributions",
    "section": "5.2 Multinomial distribution",
    "text": "5.2 Multinomial distribution\nThe multinomial distribution \\(\\text{Mult}(n, \\symbfit \\pi)\\) arises from repeated categorical sampling, in the same fashion as the binomial distribution arises from repeated Bernoulli sampling. Thus, if \\(\\symbfit x_1, \\ldots, \\symbfit x_n\\) are \\(n\\) independent \\(\\text{Cat}(\\symbfit \\pi)\\) random categorical variables then \\(\\symbfit y= \\sum_{i=1}^n \\symbfit x_i\\) is distributed as \\(\\text{Mult}(n, \\symbfit \\pi)\\).\nThe corresponding PMF describes the probability of a pattern \\(y_1, \\ldots, y_K\\) of samples distributed across \\(K\\) classes (with \\(n= \\sum_{k=1}^K y_k\\)): \\[\np(\\symbfit y| n, \\theta) = \\binom{n}{y_1, \\ldots, y_n} \\prod_{k=1}^K \\pi_k^{y_k}\n\\] where \\(\\binom{n}{y_1, \\ldots, y_n}\\) is the multinomial coefficient.\nThe expected value is \\[\\text{E}(\\symbfit y) = n \\symbfit \\pi\\] which in component notation is \\(\\text{E}(y_k) = n \\pi_k\\). The covariance matrix is \\[\\text{Var}(\\symbfit y) = n (\\text{Diag}(\\symbfit \\pi) - \\symbfit \\pi\\symbfit \\pi^T)\n\\] which in component notation is \\(\\text{Var}(x_i) = n \\pi_i (1-\\pi_i)\\) and \\(\\text{Cov}(x_i, x_j) = -n \\pi_i \\pi_j\\).\nStandardised to unit interval we get: \\[\\frac{y_i}{n} \\in \\left\\{0,\\frac{1}{n},\\frac{2}{n},...,1\\right\\}\\] \\[\\text{E}\\left(\\frac{\\symbfit y}{n}\\right) = \\symbfit \\pi\\] \\[\\text{Var}\\left(\\frac{\\symbfit y}{n}\\right) = \\frac{ \\text{Diag}(\\symbfit \\pi)-\\symbfit \\pi\\symbfit \\pi^T}{n}\\] \\[\\text{Var}\\left(\\frac{y_i}{n}\\right)=\\frac{\\pi_i(1-\\pi_i)}{n}\\] \\[\\text{Cov}\\left(\\frac{y_i}{n},\\frac{y_j}{n}\\right)=-\\frac{\\pi_i\\pi_j}{n} \\]\nFor \\(n=1\\) the multinomial distribution reduces to the categorical distribution (Section 5.1).\nFor \\(K=2\\) the multinomial distribution reduces to the Binomial distribution (Section 4.2).\n\n\n\n\n\n\nFigure 5.1: Multinomial urn model.\n\n\n\nThe multinomial distribution may be illustrated by an urn model distributing \\(n\\) balls into \\(K\\) bins (Figure 5.1).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#dirichlet-distribution",
    "href": "05-multivariate.html#dirichlet-distribution",
    "title": "5  Multivariate distributions",
    "section": "5.3 Dirichlet distribution",
    "text": "5.3 Dirichlet distribution\n\n5.3.1 Standard parametrisation\nThe Dirichlet distribution is the multivariate generalisation of the beta distribution. It is named after Peter Gustav Lejeune Dirichlet (1805–1859).\nA Dirichlet distributed random vector is denoted by \\[\n\\symbfit x\\sim \\text{Dir}(\\symbfit \\alpha)\n\\] with parameter \\(\\symbfit \\alpha= (\\alpha_1,...,\\alpha_K)^T &gt;0\\) and \\(K\\geq 2\\) and where the support of \\(\\symbfit x\\) is the \\(K-1\\) dimensional simplex with \\(x_i \\in [0,1]\\) and \\(\\sum^{K}_{i=1} x_i = 1\\).\nThe density of the Dirichlet distribution \\(\\text{Dir}(\\symbfit \\alpha)\\) is \\[\np(\\symbfit x| \\symbfit \\alpha) = \\frac{1}{B(\\symbfit \\alpha)}  \\prod_{k=1}^K x_k^{\\alpha_k-1}\n\\] This depends on the beta function with multivariate argument defined as \\[\nB(\\symbfit z) = \\frac{ \\prod_{k=1}^K \\Gamma(z_k) }{\\Gamma\\left( \\sum_{k=1}^K   z_k\\right)}\n\\]\n\n\n\n\n\n\nFigure 5.2: Stick breaking visualisation of a Dirichlet random variable.\n\n\n\nA Dirichlet random variable can be visualised as breaking a unit stick into \\(K\\) individual pieces of lengths \\(x_1\\) to \\(x_K\\) adding up to one (Figure 5.2).\nFor \\(K=2\\) the Dirichlet distribution reduces to the beta distribution (Section 4.3).\n\n\n5.3.2 Mean parametrisation\nInstead of employing \\(\\symbfit \\alpha\\) as parameter vector another useful reparametrisation \\(\\text{Dir}(\\symbfit \\pi, k)\\) of the Dirichlet distribution is in terms of a mean parameter \\(\\symbfit \\pi\\), with \\(\\pi_i \\in [0,1]\\) and \\(\\sum^{K}_{i=1}\\pi_i = 1\\), and a concentration parameter \\(k &gt; 0\\). These are given by \\[\nk = \\sum^{K}_{i=1}\\alpha_i\n\\] and \\[\n\\symbfit \\pi= \\frac{\\symbfit \\alpha}{k}\n\\] The original parameters can be recovered by \\(\\alpha= \\symbfit \\pi k\\).\nThe mean and variance of the Dirichlet distribution expressed in terms of \\(\\symbfit \\pi\\) and \\(k\\) are \\[\n\\text{E}(\\symbfit x) = \\symbfit \\pi\n\\] and \\[\\text{Var}\\left(\\symbfit x\\right) = \\frac{ \\text{Diag}(\\symbfit \\pi)-\\symbfit \\pi\\symbfit \\pi^T}{k+1}\\] which in component notation is \\[\\text{Var}(x_i)=\\frac{\\pi_i(1-\\pi_i)}{k+1}\\] and \\[\\text{Cov}(x_i,x_j)=-\\frac{\\pi_i \\pi_j}{k+1}\\]\nFinally, note that the mean and variance of the continuous Dirichlet distribution closely match those of the unit-standardised discrete multinomial distribution above.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#sec-mvnormaldist",
    "href": "05-multivariate.html#sec-mvnormaldist",
    "title": "5  Multivariate distributions",
    "section": "5.4 Multivariate normal distribution",
    "text": "5.4 Multivariate normal distribution\nThe univariate normal distribution for a random scalar \\(x\\) generalises to the multivariate normal distribution for a random vector \\(\\symbfit x= (x_1, x_2,...,x_d)^T\\).\nIf \\(\\symbfit x\\) follows a multivariate normal distribution we write \\[\n\\symbfit x\\sim N_d(\\symbfit \\mu, \\symbfit \\Sigma)\n\\] where \\(\\symbfit \\mu\\) is the mean (location) parameter and \\(\\symbfit \\Sigma\\) the variance (scale) parameter.\nThe corresponding density is\n\\[\np(\\symbfit x| \\symbfit \\mu, \\symbfit \\Sigma) = \\det(2 \\pi \\symbfit \\Sigma)^{-1/2} \\exp\\left(-\\frac{1}{2} (\\symbfit x-\\symbfit \\mu)^T \\symbfit \\Sigma^{-1} (\\symbfit x-\\symbfit \\mu)   \\right)\n\\]\nAs \\(\\det(2 \\pi \\symbfit \\Sigma)^{-1/2} = \\det(2 \\pi \\symbfit I_d)^{-1/2} \\det(\\symbfit \\Sigma)^{-1/2} =  (2 \\pi)^{-d/2} \\det(\\symbfit \\Sigma)^{-1/2}\\) the density can also be written as \\[\np(\\symbfit x| \\symbfit \\mu, \\symbfit \\Sigma) = (2\\pi)^{-d/2} \\det(\\symbfit \\Sigma)^{-1/2} \\exp\\left(-\\frac{1}{2} (\\symbfit x-\\symbfit \\mu)^T \\symbfit \\Sigma^{-1} (\\symbfit x-\\symbfit \\mu)   \\right)\n\\] i.e. with explicit occurrence of the dimension \\(d\\).\nThe expectation of \\(\\symbfit x\\) is \\(\\text{E}(\\symbfit x) = \\symbfit \\mu\\) and the variance is \\(\\text{Var}(\\symbfit x) = \\symbfit \\Sigma\\).\nFor \\(d=1\\) the random vector \\(\\symbfit x=x\\) is a scalar and \\(\\symbfit \\mu= \\mu\\) and \\(\\symbfit \\Sigma= \\sigma^2\\) and the multivariate normal density reduces to the univariate normal density (Section 4.4).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#wishart-distribution",
    "href": "05-multivariate.html#wishart-distribution",
    "title": "5  Multivariate distributions",
    "section": "5.5 Wishart distribution",
    "text": "5.5 Wishart distribution\nThe Wishart distribution is a multivariate generalisation of the gamma distribution.\nRecall that the gamma distribution can be motivated as the distribution of sums of squared normal random variables. Likewise, the Wishart distribution can be understood as the sum of squared multivariate normal variables: \\[\n\\symbfit z_1,\\symbfit z_2,\\ldots,\\symbfit z_k \\stackrel{\\text{iid}}\\sim N_d(0,\\symbfit S)\n\\]\nwith \\(\\symbfit S=(s_{ij})\\) the specified covariance matrix. The random variable \\[\\underbrace{\\symbfit X}_{d\\times d}=\\sum^{k}_{i=1}\\underbrace{\\symbfit z_i\\symbfit z_i^T}_{d\\times d}\\]\nis a random matrix and is distributed as \\[\n\\symbfit X\\sim  \\text{W}_d\\left(\\symbfit S, k\\right)\n\\] with mean \\[\n\\text{E}(\\symbfit X) = k \\symbfit S\n\\] and variances \\[\n\\text{Var}(x_{ij})  = k \\left(s^2_{ij}+s_{ii} s_{jj}  \\right)\n\\]\nWe often also employ the Wishart distribution in mean parametrisation \\[\n\\text{W}_d\\left(\\symbfit S= \\frac{\\symbfit M}{k}, k \\right)\n\\] with parameters \\(\\symbfit M= k \\symbfit S\\) and \\(k\\). In this parametrisation the mean is \\[\n\\text{E}(\\symbfit X) = \\symbfit M= (\\mu_{ij})\n\\] and variances are \\[\n\\text{Var}(x_{ij})  = \\frac{ \\mu^2_{ij}+\\mu_{ii}\\mu_{jj} }{k}\n\\]\nIf \\(\\symbfit S\\) or \\(\\symbfit M\\) is a scalar rather than a matrix then the multivariate Wishart distribution reduces to the univariate Wishart aka gamma distribution (Section 4.5).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "05-multivariate.html#inverse-wishart-distribution",
    "href": "05-multivariate.html#inverse-wishart-distribution",
    "title": "5  Multivariate distributions",
    "section": "5.6 Inverse Wishart distribution",
    "text": "5.6 Inverse Wishart distribution\nThe inverse Wishart distribution is a multivariate generalisation of the inverse gamma distribution and is linked to the Wishart distribution.\nA random matrix \\(\\symbfit X\\) following an inverse Wishart distribution is denoted by \\[\n\\symbfit X\\sim \\text{W}^{-1}_d\\left(\\symbfit \\Psi, \\nu\\right)\n\\] where \\(\\symbfit \\Psi\\) is the scale parameter and \\(\\nu\\) the shape parameter. The corresponding mean is given by \\[\n\\text{E}(\\symbfit X) = \\frac{\\symbfit \\Psi}{\\nu-d-1}\n\\] and the variances are \\[\n\\text{Var}(x_{ij})= \\frac{(\\nu-d+1) \\psi_{ij}^2 + (\\nu-d-1) \\, \\psi_{ii} \\psi_{jj} }{(\\nu-d)(\\nu-d-1)^2(\\nu-d-3)}\n\\]\nThe inverse of \\(\\symbfit X\\) is then Wishart distributed: \\[\n\\symbfit X^{-1} \\sim \\text{W}_d\\left( \\symbfit S= \\symbfit \\Psi^{-1}   \\, , k = \\nu \\right)\n\\]\nInstead of \\(\\symbfit \\Psi\\) and \\(\\nu\\) we may use the mean parametrisation with parameters \\(\\kappa = \\nu-d-1\\) and \\(\\symbfit M= \\symbfit \\Psi/(\\nu-d-1)\\): \\[\n\\symbfit X\\sim \\text{W}^{-1}_d\\left(\\symbfit \\Psi=  \\kappa  \\symbfit M\\, , \\, \\nu= \\kappa+d+1\\right)\n\\] with mean \\[\n\\text{E}(\\symbfit X) =\\symbfit M\n\\] and variances \\[\n\\text{Var}(x_{ij})= \\frac{(\\kappa+2) \\mu_{ij}^2 + \\kappa \\, \\mu_{ii} \\mu_{jj} }{(\\kappa + 1)(\\kappa-2)}\n\\]\nIf \\(\\symbfit \\Psi\\) or \\(\\symbfit M\\) is a scalar rather than a matrix then the multivariate inverse Wishart distribution reduces to the univariate inverse Wishart aka inverse gamma distribution (Section 4.6).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Mardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate\nAnalysis. Academic Press.\n\n\nWhittle, P. 2000. Probability via Expectation. 3rd ed.\nSpringer. https://doi.org/10.1007/978-1-4612-0509-8.",
    "crumbs": [
      "Bibliography"
    ]
  }
]