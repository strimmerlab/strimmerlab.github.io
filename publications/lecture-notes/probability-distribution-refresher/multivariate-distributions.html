<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>4 Multivariate distributions | Probability and Distribution Refresher</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="4 Multivariate distributions | Probability and Distribution Refresher">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="4 Multivariate distributions | Probability and Distribution Refresher">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="4.1 Categorical distribution The categorical distribution is a generalisation of the Bernoulli distribution from two classes to \(K\) classes. The categorical distribution \(\text{Cat}(\boldsymbol...">
<meta property="og:description" content="4.1 Categorical distribution The categorical distribution is a generalisation of the Bernoulli distribution from two classes to \(K\) classes. The categorical distribution \(\text{Cat}(\boldsymbol...">
<meta name="twitter:description" content="4.1 Categorical distribution The categorical distribution is a generalisation of the Bernoulli distribution from two classes to \(K\) classes. The categorical distribution \(\text{Cat}(\boldsymbol...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Probability and Distribution Refresher</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="combinatorics.html"><span class="header-section-number">1</span> Combinatorics</a></li>
<li><a class="" href="probability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="univariate-distributions.html"><span class="header-section-number">3</span> Univariate distributions</a></li>
<li><a class="active" href="multivariate-distributions.html"><span class="header-section-number">4</span> Multivariate distributions</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="multivariate-distributions" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Multivariate distributions<a class="anchor" aria-label="anchor" href="#multivariate-distributions"><i class="fas fa-link"></i></a>
</h1>
<div id="categorical-distribution" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> Categorical distribution<a class="anchor" aria-label="anchor" href="#categorical-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>categorical distribution</strong> is a generalisation of the Bernoulli distribution from
two classes to <span class="math inline">\(K\)</span> classes.</p>
<p>The categorical distribution <span class="math inline">\(\text{Cat}(\boldsymbol \pi)\)</span> describes
a discrete random variable with <span class="math inline">\(K\)</span> states (“categories”, “classes”, “bins”) where
the parameter vector
<span class="math inline">\(\boldsymbol \pi= (\pi_1, \ldots, \pi_K)^T\)</span> specifies
the probability of each of class so that
<span class="math inline">\(\text{Pr}(\text{"class k"}) = \pi_k\)</span>.
The parameters satisfy <span class="math inline">\(\pi_k \in [0,1]\)</span> and
<span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span>, hence there are <span class="math inline">\(K-1\)</span> independent parameters in a categorical distribution (and not <span class="math inline">\(K\)</span>).</p>
<p>There are two main ways to numerically represent “class k”:</p>
<ol style="list-style-type: lower-roman">
<li>by “integer encoding”, i.e. by the corresponding integer <span class="math inline">\(k\)</span>.</li>
<li>by “one hot encoding”, i.e. by an indicator vector
<span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_K)^T = (0, 0, \ldots, 1, \ldots, 0)^T\)</span> containing zeros everywhere except for the element <span class="math inline">\(x_k=1\)</span> at position <span class="math inline">\(k\)</span>. Thus all <span class="math inline">\(x_k \in \{ 0, 1\}\)</span> and <span class="math inline">\(\sum_{k=1}^K x_k = 1\)</span>.</li>
</ol>
<p>In the following we use “one hot encoding”. Therefore sampling from a categorical distribution with parameters <span class="math inline">\(\boldsymbol \pi\)</span>
<span class="math display">\[
\boldsymbol x\sim \text{Cat}(\boldsymbol \pi)
\]</span>
yields a random index vector <span class="math inline">\(\boldsymbol x\)</span>.</p>
<p>The corresponding probability mass function (PMF)
can be written conveniently in terms of <span class="math inline">\(x_k\)</span> as
<span class="math display">\[
p(\boldsymbol x| \boldsymbol \pi) = \prod_{k=1}^K \pi_k^{x_k} =
\begin{cases}
   \pi_k  &amp; \text{if } x_k = 1 \\
\end{cases}
\]</span>
and the log PMF as
<span class="math display">\[
\log p(\boldsymbol x| \boldsymbol \pi) = \sum_{k=1}^K x_k \log \pi_k   =
\begin{cases}
   \log \pi_k  &amp; \text{if } x_k = 1 \\
\end{cases}
\]</span></p>
<p>In order to be more explicit that the categorical distribution has <span class="math inline">\(K-1\)</span> and not <span class="math inline">\(K\)</span> parameters
we rewrite the log-density with
<span class="math inline">\(\pi_K = 1 - \sum_{k=1}^{K-1} \pi_k\)</span> and <span class="math inline">\(x_K = 1 - \sum_{k=1}^{K-1} x_k\)</span> as
<span class="math display">\[
\begin{split}
\log p(\boldsymbol x| \boldsymbol \pi) &amp; =\sum_{k=1}^{K-1}  x_k \log \pi_k    + x_K \log \pi_K \\
&amp; =\sum_{k=1}^{K-1}  x_k \log \pi_k    + \left( 1 - \sum_{k=1}^{K-1} x_k  \right) \log \left( 1 - \sum_{k=1}^{K-1} \pi_k \right) \\
\end{split}
\]</span>
Note that there is no particular reason to choose <span class="math inline">\(\pi_K\)</span> as
dependent of the probabilities of the other classes,
in its place any other of the <span class="math inline">\(\pi_k\)</span> may be selected.</p>
<p>The expected value is <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \pi\)</span>, in component notation
<span class="math inline">\(\text{E}(x_k) = \pi_k\)</span>.
The covariance matrix is <span class="math inline">\(\text{Var}(\boldsymbol x) = \text{Diag}(\boldsymbol \pi) - \boldsymbol \pi\boldsymbol \pi^T\)</span>, which in
component notation is <span class="math inline">\(\text{Var}(x_i) = \pi_i (1-\pi_i)\)</span> and <span class="math inline">\(\text{Cov}(x_i, x_j) = -\pi_i \pi_j\)</span>.</p>
<p>The form of the categorical covariance matrix follows directly from the definition of the
variance <span class="math inline">\(\text{Var}(\boldsymbol x) = \text{E}( \boldsymbol x\boldsymbol x^T) - \text{E}( \boldsymbol x) \text{E}( \boldsymbol x)^T\)</span>
and noting that <span class="math inline">\(x_i^2 = x_i\)</span> and <span class="math inline">\(x_i x_j = 0\)</span> if <span class="math inline">\(i \neq j\)</span>.
Furthermore, the categorical covariance matrix is singular by construction, as the <span class="math inline">\(K\)</span> random variables
<span class="math inline">\(x_1, \ldots, x_K\)</span> are dependent through the constraint <span class="math inline">\(\sum_{k=1}^K x_k = 1\)</span>.</p>
<p>For <span class="math inline">\(K=2\)</span> the categorical distribution reduces to the Bernoulli <span class="math inline">\(\text{Ber}(\theta)\)</span> distribution,
with <span class="math inline">\(\pi_1=\theta\)</span> and <span class="math inline">\(\pi_2=1-\theta\)</span>.</p>
</div>
<div id="multinomial-distribution" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Multinomial distribution<a class="anchor" aria-label="anchor" href="#multinomial-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>multinomial distribution</strong> <span class="math inline">\(\text{Mult}(n, \boldsymbol \pi)\)</span> arises from repeated categorical sampling,
in the same fashion as the binomial distribution arises from repeated Bernoulli sampling. Thus, if <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n\)</span> are <span class="math inline">\(n\)</span> independent <span class="math inline">\(\text{Cat}(\boldsymbol \pi)\)</span> random categorical variables
then <span class="math inline">\(\boldsymbol y= \sum_{i=1}^n \boldsymbol x_i\)</span> is distributed as <span class="math inline">\(\text{Mult}(n, \boldsymbol \pi)\)</span>.</p>
<p>The corresponding PMF describes the probability of a pattern <span class="math inline">\(y_1, \ldots, y_K\)</span> of
samples distributed across <span class="math inline">\(K\)</span> classes (with <span class="math inline">\(n= \sum_{k=1}^K y_k\)</span>):
<span class="math display">\[
p(\boldsymbol y| n, \theta) = \binom{n}{y_1, \ldots, y_n} \prod_{k=1}^K \pi_k^{y_k}
\]</span>
where <span class="math inline">\(\binom{n}{y_1, \ldots, y_n}\)</span> is the multinomial coefficient.</p>
<p>The expected value is
<span class="math display">\[\text{E}(\boldsymbol y) = n \boldsymbol \pi\]</span>
which in component notation is <span class="math inline">\(\text{E}(y_k) = n \pi_k\)</span>.
The covariance matrix is
<span class="math display">\[\text{Var}(\boldsymbol y) = n (\text{Diag}(\boldsymbol \pi) - \boldsymbol \pi\boldsymbol \pi^T)
\]</span>
which in component notation is <span class="math inline">\(\text{Var}(x_i) = n \pi_i (1-\pi_i)\)</span> and <span class="math inline">\(\text{Cov}(x_i, x_j) = -n \pi_i \pi_j\)</span>.</p>
<p>Standardised to unit interval we get:
<span class="math display">\[\frac{y_i}{n} \in \left\{0,\frac{1}{n},\frac{2}{n},...,1\right\}\]</span>
<span class="math display">\[\text{E}\left(\frac{\boldsymbol y}{n}\right) = \boldsymbol \pi\]</span>
<span class="math display">\[\text{Var}\left(\frac{\boldsymbol y}{n}\right) = \frac{ \text{Diag}(\boldsymbol \pi)-\boldsymbol \pi\boldsymbol \pi^T}{n}\]</span>
<span class="math display">\[\text{Var}\left(\frac{y_i}{n}\right)=\frac{\pi_i(1-\pi_i)}{n}\]</span>
<span class="math display">\[\text{Cov}\left(\frac{y_i}{n},\frac{y_j}{n}\right)=-\frac{\pi_i\pi_j}{n} \]</span></p>
<p>The multinomial distribution may be illustrated by an urn model
distributing <span class="math inline">\(n\)</span> balls into <span class="math inline">\(K\)</span> bins:</p>
<div class="inline-figure"><img src="fig/urn-multinom.png" width="60%" style="display: block; margin: auto;"></div>
<p>For <span class="math inline">\(n=1\)</span> the multinomial distribution reduces to the categorical distribution.</p>
<p>For <span class="math inline">\(K=2\)</span> the multinomial distribution reduces to the Binomial distribution.</p>
</div>
<div id="dirichlet-distribution" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> Dirichlet distribution<a class="anchor" aria-label="anchor" href="#dirichlet-distribution"><i class="fas fa-link"></i></a>
</h2>
<div id="standard-parameterisation-3" class="section level3" number="4.3.1">
<h3>
<span class="header-section-number">4.3.1</span> Standard parameterisation<a class="anchor" aria-label="anchor" href="#standard-parameterisation-3"><i class="fas fa-link"></i></a>
</h3>
<p>The Dirichlet distribution is the multivariate generalisation of the beta distribution.</p>
<p>A Dirichlet distributed random vector is denoted by
<span class="math display">\[
\boldsymbol x\sim \text{Dir}(\boldsymbol \alpha)
\]</span>
with parameter <span class="math inline">\(\boldsymbol \alpha= (\alpha_1,...,\alpha_K)^T &gt;0\)</span>
and <span class="math inline">\(K\geq 2\)</span> and
where the support of <span class="math inline">\(\boldsymbol x\)</span> is the <span class="math inline">\(K-1\)</span> dimensional simplex
with <span class="math inline">\(x_i \in [0,1]\)</span> and <span class="math inline">\(\sum^{K}_{i=1} x_i = 1\)</span>.</p>
<p>The Dirichlet random variable can be visualised as
breaking a unit stick of length one
in <span class="math inline">\(K\)</span> pieces of length <span class="math inline">\(x_1\)</span> to <span class="math inline">\(x_K\)</span>:</p>
<div class="inline-figure"><img src="fig/stick-dirichlet.png" width="60%" style="display: block; margin: auto;"></div>
<p>The density of the Dirichlet distribution <span class="math inline">\(\text{Dir}(\boldsymbol \alpha)\)</span> is
<span class="math display">\[
p(\boldsymbol x| \boldsymbol \alpha) = \frac{1}{B(\boldsymbol \alpha)}  \prod_{k=1}^K x_k^{\alpha_k-1}
\]</span>
This depends on the beta function with multivariate argument
defined as
<span class="math display">\[
B(\boldsymbol z) = \frac{ \prod_{k=1}^K \Gamma(z_k) }{\Gamma\left( \sum_{k=1}^K   z_k\right)}
\]</span></p>
<p>For <span class="math inline">\(K=2\)</span> the Dirichlet distribution reduces to the beta distribution.</p>
</div>
<div id="mean-parameterisation-1" class="section level3" number="4.3.2">
<h3>
<span class="header-section-number">4.3.2</span> Mean parameterisation<a class="anchor" aria-label="anchor" href="#mean-parameterisation-1"><i class="fas fa-link"></i></a>
</h3>
<p>Instead of employing <span class="math inline">\(\boldsymbol \alpha\)</span> as parameter vector another useful reparameterisation <span class="math inline">\(\text{Dir}(\boldsymbol \pi, k)\)</span> of the Dirichlet
distribution is in terms of a mean parameter
<span class="math inline">\(\boldsymbol \pi\)</span>, with <span class="math inline">\(\pi_i \in [0,1]\)</span> and <span class="math inline">\(\sum^{K}_{i=1}\pi_i = 1\)</span>,
and a concentration parameter <span class="math inline">\(k &gt; 0\)</span>. These are given by
<span class="math display">\[
k = \sum^{K}_{i=1}\alpha_i
\]</span>
and
<span class="math display">\[
\boldsymbol \pi= \frac{\boldsymbol \alpha}{k}
\]</span>
The original parameters can be recovered by <span class="math inline">\(\alpha= \boldsymbol \pi k\)</span>.</p>
<p>The mean and variance of the Dirichlet distribution expressed in terms of <span class="math inline">\(\boldsymbol \pi\)</span> and <span class="math inline">\(k\)</span> are
<span class="math display">\[
\text{E}(\boldsymbol x) = \boldsymbol \pi
\]</span>
and
<span class="math display">\[\text{Var}\left(\boldsymbol x\right) = \frac{ \text{Diag}(\boldsymbol \pi)-\boldsymbol \pi\boldsymbol \pi^T}{k+1}\]</span>
which in component notation is
<span class="math display">\[\text{Var}(x_i)=\frac{\pi_i(1-\pi_i)}{k+1}\]</span>
and
<span class="math display">\[\text{Cov}(x_i,x_j)=-\frac{\pi_i \pi_j}{k+1}\]</span></p>
<p>Finally, note that the mean and variance of the continuous Dirichlet distribution
closely match those of the unit-standardised discrete multinomial distribution above.</p>
</div>
</div>
<div id="multivariate-normal-distribution" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> Multivariate normal distribution<a class="anchor" aria-label="anchor" href="#multivariate-normal-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>The univariate normal distribution for a random scalar <span class="math inline">\(x\)</span> generalises to the <strong>multivariate normal distribution</strong> for a random vector <span class="math inline">\(\boldsymbol x= (x_1, x_2,...,x_d)^T\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol x\)</span> follows a multivariate normal distribution we write
<span class="math display">\[
\boldsymbol x\sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)
\]</span>
where <span class="math inline">\(\boldsymbol \mu\)</span> is the mean (location) parameter and <span class="math inline">\(\boldsymbol \Sigma\)</span> the variance (scale) parameter.</p>
<p>The corresponding density is</p>
<p><span class="math display">\[p(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma) = \det(2 \pi \boldsymbol \Sigma)^{-\frac{1}{2}} \exp\left({{-\frac{1}{2}} \underbrace{\underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1 \times d} \underbrace{\boldsymbol \Sigma^{-1}}_{d \times d} \underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d \times 1} }_{1 \times 1 = \text{scalar!}}}\right)\]</span></p>
<p>As <span class="math inline">\(\det(2 \pi \boldsymbol \Sigma)^{-\frac{1}{2}} = \det(2 \pi \boldsymbol I_d)^{-\frac{1}{2}} \det(\boldsymbol \Sigma)^{-\frac{1}{2}} = (2 \pi)^{-d/2} \det(\boldsymbol \Sigma)^{-\frac{1}{2}}\)</span>
the density can also be written as
<span class="math display">\[
p(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma) = (2\pi)^{-\frac{d}{2}} \det(\boldsymbol \Sigma)^{-\frac{1}{2}} \exp\left(-\frac{1}{2} (\boldsymbol x-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu)   \right)
\]</span>
with explicit occurence of the dimension <span class="math inline">\(d\)</span>.</p>
<p>The expectation of <span class="math inline">\(\boldsymbol x\)</span> is <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \mu\)</span> and the variance is <span class="math inline">\(\text{Var}(\boldsymbol x) = \boldsymbol \Sigma\)</span>.</p>
<p>For <span class="math inline">\(d=1\)</span> the random vector <span class="math inline">\(\boldsymbol x=x\)</span> is a scalar and <span class="math inline">\(\boldsymbol \mu= \mu\)</span> and <span class="math inline">\(\boldsymbol \Sigma= \sigma^2\)</span> and the multivariate normal density reduces to the univariate normal density.</p>
</div>
<div id="wishart-distribution" class="section level2" number="4.5">
<h2>
<span class="header-section-number">4.5</span> Wishart distribution<a class="anchor" aria-label="anchor" href="#wishart-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>The Wishart distribution is a multivariate generalisation of the gamma distribution.</p>
<p>Recall that the gamma distribution can be motivated as the distribution of
sums of squared normal random variables. Likewise, the Wishart distribution
can be understood as the sum of squared multivariate normal variables:
<span class="math display">\[
\boldsymbol z_1,\boldsymbol z_2,\ldots,\boldsymbol z_k \stackrel{\text{iid}}\sim N_d(0,\boldsymbol S)
\]</span><br>
with <span class="math inline">\(\boldsymbol S=(s_{ij})\)</span> the specified covariance matrix.
The random variable
<span class="math display">\[\underbrace{\boldsymbol X}_{d\times d}=\sum^{k}_{i=1}\underbrace{\boldsymbol z_i\boldsymbol z_i^T}_{d\times d}\]</span><br>
is a random <em>matrix</em> and is distributed as
<span class="math display">\[
\boldsymbol X\sim  \text{W}_d\left(\boldsymbol S, k\right)
\]</span>
with mean
<span class="math display">\[
\text{E}(\boldsymbol X) = k \boldsymbol S
\]</span>
and variances
<span class="math display">\[
\text{Var}(x_{ij})  = k \left(s^2_{ij}+s_{ii} s_{jj}  \right)
\]</span></p>
<p>We often also employ the Wishart distribution in <strong>mean parameterisation</strong>
<span class="math display">\[
\text{W}_d\left(\boldsymbol S= \frac{\boldsymbol M}{k}, k \right)
\]</span>
with parameters <span class="math inline">\(\boldsymbol M= k \boldsymbol S\)</span> and <span class="math inline">\(k\)</span>. In this parameterisation the
mean is
<span class="math display">\[
\text{E}(\boldsymbol X) = \boldsymbol M= (\mu_{ij})
\]</span>
and variances are
<span class="math display">\[
\text{Var}(x_{ij})  = \frac{ \mu^2_{ij}+\mu_{ii}\mu_{jj} }{k}
\]</span></p>
<p>If <span class="math inline">\(\boldsymbol S\)</span> or <span class="math inline">\(\boldsymbol M\)</span> is a scalar rather than a matrix then the multivariate Wishart distribution reduces to the univariate Wishart aka gamma distribution.</p>
</div>
<div id="inverse-wishart-distribution" class="section level2" number="4.6">
<h2>
<span class="header-section-number">4.6</span> Inverse Wishart distribution<a class="anchor" aria-label="anchor" href="#inverse-wishart-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>The inverse Wishart distribution is a multivariate generalisation of the inverse gamma distribution and is linked to the Wishart distribution.</p>
<p>A random matrix <span class="math inline">\(\boldsymbol X\)</span> following an <strong>inverse Wishart distribution</strong>
is denoted by
<span class="math display">\[
\boldsymbol X\sim \text{W}^{-1}_d\left(\boldsymbol \Psi, \nu\right)
\]</span>
where <span class="math inline">\(\boldsymbol \Psi\)</span> is the scale parameter and <span class="math inline">\(\nu\)</span> the shape parameter.
The corresponding mean is given by
<span class="math display">\[
\text{E}(\boldsymbol X) = \frac{\boldsymbol \Psi}{\nu-d-1}
\]</span>
and the
variances are
<span class="math display">\[
\text{Var}(x_{ij})= \frac{(\nu-d+1) \psi_{ij}^2 + (\nu-d-1) \, \psi_{ii} \psi_{jj} }{(\nu-d)(\nu-d-1)^2(\nu-d-3)}
\]</span></p>
<p>The inverse of <span class="math inline">\(\boldsymbol X\)</span> is then Wishart distributed:
<span class="math display">\[
\boldsymbol X^{-1} \sim \text{W}_d\left( \boldsymbol S= \boldsymbol \Psi^{-1}   \, , k = \nu \right)
\]</span></p>
<p>Instead of <span class="math inline">\(\boldsymbol \Psi\)</span> and <span class="math inline">\(\nu\)</span> we may use the mean parameterisation with
parameters
<span class="math inline">\(\kappa = \nu-d-1\)</span> and
<span class="math inline">\(\boldsymbol M= \boldsymbol \Psi/(\nu-d-1)\)</span>:
<span class="math display">\[
\boldsymbol X\sim \text{W}^{-1}_d\left(\boldsymbol \Psi=  \kappa  \boldsymbol M\, , \, \nu= \kappa+d+1\right)
\]</span>
with mean
<span class="math display">\[
\text{E}(\boldsymbol X) =\boldsymbol M
\]</span>
and variances
<span class="math display">\[
\text{Var}(x_{ij})= \frac{(\kappa+2) \mu_{ij}^2 + \kappa \, \mu_{ii} \mu_{jj} }{(\kappa + 1)(\kappa-2)}
\]</span></p>
<p>If <span class="math inline">\(\boldsymbol \Psi\)</span> or <span class="math inline">\(\boldsymbol M\)</span> is a scalar rather than a matrix then the multivariate inverse Wishart distribution reduces to the univariate inverse Wishart aka inverse gamma distribution.</p>

</div>
</div>






  <div class="chapter-nav">
<div class="prev"><a href="univariate-distributions.html"><span class="header-section-number">3</span> Univariate distributions</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#multivariate-distributions"><span class="header-section-number">4</span> Multivariate distributions</a></li>
<li><a class="nav-link" href="#categorical-distribution"><span class="header-section-number">4.1</span> Categorical distribution</a></li>
<li><a class="nav-link" href="#multinomial-distribution"><span class="header-section-number">4.2</span> Multinomial distribution</a></li>
<li>
<a class="nav-link" href="#dirichlet-distribution"><span class="header-section-number">4.3</span> Dirichlet distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#standard-parameterisation-3"><span class="header-section-number">4.3.1</span> Standard parameterisation</a></li>
<li><a class="nav-link" href="#mean-parameterisation-1"><span class="header-section-number">4.3.2</span> Mean parameterisation</a></li>
</ul>
</li>
<li><a class="nav-link" href="#multivariate-normal-distribution"><span class="header-section-number">4.4</span> Multivariate normal distribution</a></li>
<li><a class="nav-link" href="#wishart-distribution"><span class="header-section-number">4.5</span> Wishart distribution</a></li>
<li><a class="nav-link" href="#inverse-wishart-distribution"><span class="header-section-number">4.6</span> Inverse Wishart distribution</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Probability and Distribution Refresher</strong>" was written by Korbinian Strimmer. It was last built on 7 January 2024.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
