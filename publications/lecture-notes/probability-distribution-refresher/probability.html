<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>2 Probability | Probability and Distribution Refresher</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="2 Probability | Probability and Distribution Refresher">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="2 Probability | Probability and Distribution Refresher">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="2.1 Random variables A random variable describes a random experiment. The set of all possible outcomes is the sample space or state space of the random variable and is denoted by \(\Omega =...">
<meta property="og:description" content="2.1 Random variables A random variable describes a random experiment. The set of all possible outcomes is the sample space or state space of the random variable and is denoted by \(\Omega =...">
<meta name="twitter:description" content="2.1 Random variables A random variable describes a random experiment. The set of all possible outcomes is the sample space or state space of the random variable and is denoted by \(\Omega =...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Probability and Distribution Refresher</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="combinatorics.html"><span class="header-section-number">1</span> Combinatorics</a></li>
<li><a class="active" href="probability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="univariate-distributions.html"><span class="header-section-number">3</span> Univariate distributions</a></li>
<li><a class="" href="multivariate-distributions.html"><span class="header-section-number">4</span> Multivariate distributions</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="probability" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Probability<a class="anchor" aria-label="anchor" href="#probability"><i class="fas fa-link"></i></a>
</h1>
<div id="random-variables" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Random variables<a class="anchor" aria-label="anchor" href="#random-variables"><i class="fas fa-link"></i></a>
</h2>
<p>A <strong>random variable</strong> describes a random experiment. The set of all possible outcomes
is the <strong>sample space</strong> or <strong>state space</strong> of the random variable and is denoted by
<span class="math inline">\(\Omega = \{\omega_1, \omega_2, \ldots\}\)</span>. The outcomes <span class="math inline">\(\omega_i\)</span> are the <strong>elementary events</strong>.
The sample space <span class="math inline">\(\Omega\)</span> can be finite or infinite. Depending on type of outcomes
the random variable is <strong>discrete</strong> or <strong>continuous</strong>.</p>
<p>An event <span class="math inline">\(A \subseteq \Omega\)</span> is a subset of <span class="math inline">\(\Omega\)</span> and thus itself a set composed of elementary events: <span class="math inline">\(A = \{a_1, a_2, \ldots\}\)</span>.
This includes as special cases the full set <span class="math inline">\(A = \Omega\)</span>, the empty set <span class="math inline">\(A = \emptyset\)</span>, and the elementary
events <span class="math inline">\(A=\omega_i\)</span>. The complementary event <span class="math inline">\(A^C\)</span> is the complement of the set <span class="math inline">\(A\)</span> in the set <span class="math inline">\(\Omega\)</span>
so that <span class="math inline">\(A^C = \Omega \setminus A = \{\omega_i \in \Omega: \omega_i \notin A\}\)</span>.</p>
<p>The probability of an event <span class="math inline">\(A\)</span> is denoted by <span class="math inline">\(\text{Pr}(A)\)</span>.
Essentially, to obtain this probability we need to count the
elementary elements corresponding to <span class="math inline">\(A\)</span>. To do this
we assume as <a href="https://en.wikipedia.org/wiki/Probability_axioms">axioms of probability</a> that</p>
<ul>
<li>
<span class="math inline">\(\text{Pr}(A) \geq 0\)</span>, probabilities are positive,</li>
<li>
<span class="math inline">\(\text{Pr}(\Omega) = 1\)</span>, the certain event has probability 1, and</li>
<li>
<span class="math inline">\(\text{Pr}(A) = \sum_{a_i \in A} \text{Pr}(a_i)\)</span>, the probability of
an event equals the sum of its constituting elementary events <span class="math inline">\(a_i\)</span>.
This sum is taken over a finite or countable infinite number of elements.</li>
</ul>
<p>This implies</p>
<ul>
<li>
<span class="math inline">\(\text{Pr}(A) \leq 1\)</span>, i.e. probabilities all lie in the interval <span class="math inline">\([0,1]\)</span>
</li>
<li>
<span class="math inline">\(\text{Pr}(A^C) = 1 - \text{Pr}(A)\)</span>, and</li>
<li><span class="math inline">\(\text{Pr}(\emptyset) = 0\)</span></li>
</ul>
<p>Assume now that we have two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.
The probability of the event “<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>” is then given by the probability of the set intersection
<span class="math inline">\(\text{Pr}(A \cap B)\)</span>.
Likewise the probability of the event “<span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>” is given by the probability of the set union
<span class="math inline">\(\text{Pr}(A \cup B)\)</span>.</p>
<p>From the above it is clear that the definition and theory of probability is closely linked to set theory, and in particular to measure theory. Indeed, viewing probability as a special type of measure allows for an elegant treatment of both discrete and continuous random variables.</p>
</div>
<div id="probability-mass-and-density-function" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Probability mass and density function<a class="anchor" aria-label="anchor" href="#probability-mass-and-density-function"><i class="fas fa-link"></i></a>
</h2>
<p>To describe a random variable <span class="math inline">\(x\)</span> with state space <span class="math inline">\(\Omega\)</span> we need a way to effectively store the probabilities of the corresponding elementary outcomes <span class="math inline">\(x \in \Omega\)</span>. Note that for convenience we use the same symbol to denote the random variable and its elementary outcomes.</p>
<p>For a discrete random variable we define the
event <span class="math inline">\(A = \{x: x=a\} = \{a\}\)</span> and get the probability
<span class="math display">\[
\text{Pr}(A) = \text{Pr}(x=a) = f(a)
\]</span>
directly from the <strong>probability mass function</strong> (PMF), here denoted by lower case <span class="math inline">\(f\)</span>
(but we frequently also use <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span>).
The PMF has the property that <span class="math inline">\(\sum_{x \in \Omega} f(x) = 1\)</span> and that
<span class="math inline">\(f(x) \in [0,1]\)</span>.</p>
<p>For continuous random variables we need to use a <strong>probability density function</strong> (PDF) instead. We define the event
<span class="math inline">\(A = \{x: a &lt; x \leq a + da\}\)</span> as an infinitesimal interval
and then assign the probability
<span class="math display">\[
\text{Pr}(A) = \text{Pr}( a &lt; x \leq a + da) = f(a) da \,.
\]</span>
The PDF has the property that <span class="math inline">\(\int_{x \in \Omega} f(x) dx = 1\)</span>
but in contrast to a PMF the density <span class="math inline">\(f(x)\geq 0\)</span> may take on values larger than 1.</p>
<p>The set of all <span class="math inline">\(x\)</span> for which <span class="math inline">\(f(x)\)</span> is positive is called the <strong>support</strong> of the PDF/PMF.</p>
</div>
<div id="distribution-function-and-quantile-function" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Distribution function and quantile function<a class="anchor" aria-label="anchor" href="#distribution-function-and-quantile-function"><i class="fas fa-link"></i></a>
</h2>
<p>As alternative to using PMF/PDFs we may also use a <strong>distribution function</strong> to describe the random variable. This assumes an ordering exist among the elementary events so that we can define the event <span class="math inline">\(A = \{x: x \leq a \}\)</span> and compute its
probability as
<span class="math display">\[
F(a) = \text{Pr}(A) = \text{Pr}( x \leq a ) =
\begin{cases}
\sum_{x \in A} f(x) &amp; \text{discrete case} \\
\int_{x \in A} f(x) dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
This is also known <strong>cumulative distribution function</strong> (CDF)
and is denoted by upper case <span class="math inline">\(F\)</span> (or <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>).
By construction the distribution function is monotonically non-decreasing and its value ranges from 0 to 1.
With its help we can compute the probability of an interval set
such as
<span class="math display">\[
\text{Pr}( a &lt; x \leq b ) = F(b)-F(a) \,.
\]</span></p>
<p>The inverse of the distribution function <span class="math inline">\(y=F(x)\)</span> is the <strong>quantile function</strong> <span class="math inline">\(x=F^{-1}(y)\)</span>.
The 50% quantile <span class="math inline">\(F^{-1}\left(\frac{1}{2}\right)\)</span> is called the <strong>median</strong>.</p>
<p>If the random variable <span class="math inline">\(x\)</span> has distribution function <span class="math inline">\(F\)</span> we write <span class="math inline">\(x \sim F\)</span>.</p>
<div class="inline-figure"><img src="fig/refresher_dens-dist.png" width="100%" style="display: block; margin: auto;"></div>
</div>
<div id="families-of-distributions" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> Families of distributions<a class="anchor" aria-label="anchor" href="#families-of-distributions"><i class="fas fa-link"></i></a>
</h2>
<p>A distribution <span class="math inline">\(F_{\theta}\)</span> with a parameter <span class="math inline">\(\theta\)</span> constitutes a <strong>distribution family</strong> collecting all the distributions corresponding
to particular instances of the parameter. The parameter <span class="math inline">\(\theta\)</span>
therefore acts as an index of the distributions contained in the family.</p>
<p>The corresponding density (PDF) or probability mass function (PMF) is
written either as <span class="math inline">\(f_{\theta}(x)\)</span>, <span class="math inline">\(f(x; \theta)\)</span> or <span class="math inline">\(f(x | \theta)\)</span>.
The latter form is the most general is it suggests that the parameter <span class="math inline">\(\theta\)</span> may potentially also have its own distribution, with a joint density formed by <span class="math inline">\(f(x, \theta) = f(x | \theta) f(\theta)\)</span>.</p>
<p>Note that any parameterisation is generally not unique, as a one-to-one transformation of <span class="math inline">\(\theta\)</span> will yield another equivalent index to the same distribution family. Typically, for most commonly used distribution families there are several standard parameterisations. Often we use those parameterisations where the parameters can be interpreted easily (e.g. in terms of moments).</p>
<p>If for any pair of different parameter values <span class="math inline">\(\theta_1 \neq \theta_2\)</span> we get distinct distributions with <span class="math inline">\(F_{\theta_1} \neq F_{\theta_2}\)</span> then the distribution family <span class="math inline">\(F_{\theta}\)</span> is said to be <strong>identifiable</strong> under the parameter <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="expectation-of-a-random-variable" class="section level2" number="2.5">
<h2>
<span class="header-section-number">2.5</span> Expectation of a random variable<a class="anchor" aria-label="anchor" href="#expectation-of-a-random-variable"><i class="fas fa-link"></i></a>
</h2>
<p>The expected value <span class="math inline">\(\text{E}(x)\)</span> of a random variable is defined as
the weighted average over all possible outcomes, with the weight given by the PMF / PDF <span class="math inline">\(f(x)\)</span>:
<span class="math display">\[
\text{E}_{F}(x) =
\begin{cases}
\sum_{x \in \Omega} f(x) x &amp; \text{discrete case} \\
\int_{x \in \Omega} f(x) x dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
Note the notation to emphasise that the expectation is taken with regard to the distribution <span class="math inline">\(F\)</span>. The subscript <span class="math inline">\(F\)</span> is usually left out if
there are no ambiguities.
Furthermore, because the sum or integral may diverge
the expectation is not necessarily always defined (in contrast to quantiles).</p>
<p>The expected value of a function of a random variable <span class="math inline">\(h(x)\)</span> is
obtained similarly:
<span class="math display">\[
\text{E}_{F}(h(x)) =
\begin{cases}
\sum_{x \in \Omega} f(x) h(x) &amp; \text{discrete case} \\
\int_{x \in \Omega} f(x) h(x) dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
This is called the <a href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician">“law of the unconscious statistician”</a>, or short LOTUS.
Again, to highlight that the random variable <span class="math inline">\(x\)</span> has distribution <span class="math inline">\(F\)</span> we
write <span class="math inline">\(\text{E}_F(h(x))\)</span>.</p>
</div>
<div id="jensens-inequality-for-the-expectation" class="section level2" number="2.6">
<h2>
<span class="header-section-number">2.6</span> Jensen’s inequality for the expectation<a class="anchor" aria-label="anchor" href="#jensens-inequality-for-the-expectation"><i class="fas fa-link"></i></a>
</h2>
<p>If <span class="math inline">\(h(\boldsymbol x)\)</span> is a <em>convex</em> function then the following
inequality holds:</p>
<p><span class="math display">\[
\text{E}(h(\boldsymbol x)) \geq h(\text{E}(\boldsymbol x))
\]</span></p>
<p>Recall: a con<strong>ve</strong>x function (such as <span class="math inline">\(x^2\)</span>) has the shape of a “<strong>va</strong>lley”.</p>
</div>
<div id="probability-as-expectation" class="section level2" number="2.7">
<h2>
<span class="header-section-number">2.7</span> Probability as expectation<a class="anchor" aria-label="anchor" href="#probability-as-expectation"><i class="fas fa-link"></i></a>
</h2>
<p>Probability itself can also be understood as an expectation.
For an event <span class="math inline">\(A\)</span> we can define a corresponding indicator function
<span class="math inline">\(1_{ x \in A}\)</span> for an elementary element <span class="math inline">\(x\)</span> to be part of <span class="math inline">\(A\)</span>.
From the above it then follows
<span class="math display">\[
\text{E}( 1_{x \in A} ) = \text{Pr}(A) \, ,
\]</span></p>
<p>Interestingly, one can develop the whole theory of probability from this perspective. <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Whittle, P. 2000. Probability via Expectation (3rd ed.). Springer. &lt;a href="https://doi.org/10.1007/978-1-4612-0509-8" class="uri"&gt;https://doi.org/10.1007/978-1-4612-0509-8&lt;/a&gt;&lt;/p&gt;'><sup>1</sup></a></p>
</div>
<div id="moments-and-variance-of-a-random-variable" class="section level2" number="2.8">
<h2>
<span class="header-section-number">2.8</span> Moments and variance of a random variable<a class="anchor" aria-label="anchor" href="#moments-and-variance-of-a-random-variable"><i class="fas fa-link"></i></a>
</h2>
<p>The moments of a random variable are defined as follows:</p>
<ul>
<li>Zeroth moment: <span class="math inline">\(\text{E}(x^0) = 1\)</span> by construction of PDF and PMF,</li>
<li>First moment: <span class="math inline">\(\text{E}(x^1) = \text{E}(x) = \mu\)</span> , the mean,</li>
<li>Second moment: <span class="math inline">\(\text{E}(x^2)\)</span>
</li>
<li>The variance is the second moment centred about the mean <span class="math inline">\(\mu\)</span>:
<span class="math display">\[\text{Var}(x) = \text{E}( (x - \mu)^2 ) = \sigma^2\]</span>
</li>
<li>The variance can also be computed by <span class="math inline">\(\text{Var}(x) = \text{E}(x^2)-\text{E}(x)^2\)</span>. This provides an example of Jensen’s inequality,
with <span class="math inline">\(\text{E}(x^2) =\text{E}(x)^2 + \text{Var}(x) \geq \text{E}(x)^2\)</span>.</li>
</ul>
<p>A distribution does not necessarily need to have any finite first or higher moments.
An example is the <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a> that does not have a mean or variance (or any other higher moment).</p>
</div>
<div id="distribution-of-sums-of-random-variables" class="section level2" number="2.9">
<h2>
<span class="header-section-number">2.9</span> Distribution of sums of random variables<a class="anchor" aria-label="anchor" href="#distribution-of-sums-of-random-variables"><i class="fas fa-link"></i></a>
</h2>
<p>The sum of two normal random variables is also normal (with the appropriate mean and variance).</p>
<p>The <strong>central limit theorem</strong>, first postulated by <a href="https://en.wikipedia.org/wiki/Abraham_de_Moivre">Abraham de Moivre (1667–1754)</a>, asserts
that in many cases the distribution of the sum of identically distributed random variables converges to a normal distribution, even if the individual random variables are not normal.</p>
<p>For example, as a result the binomial distribution (as sum of Bernoulli random variables) can be approximated by a normal distribution.</p>
</div>
<div id="transformation-of-random-variables" class="section level2" number="2.10">
<h2>
<span class="header-section-number">2.10</span> Transformation of random variables<a class="anchor" aria-label="anchor" href="#transformation-of-random-variables"><i class="fas fa-link"></i></a>
</h2>
<p>Linear transformation of random variables: if <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants and <span class="math inline">\(x\)</span> is a random variable, then the random variable <span class="math inline">\(y= a + b x\)</span> has mean <span class="math inline">\(\text{E}(y) = a + b \text{E}(x)\)</span> and variance <span class="math inline">\(\text{Var}(y) = b^2 \text{Var}(x)\)</span>.</p>
<p>For a general invertible coordinate transformation <span class="math inline">\(y = h(x) = y(x)\)</span> the backtransformation is <span class="math inline">\(x = h^{-1}(y) = x(y)\)</span>.</p>
<p>The transformation of the infinitesimal volume element is <span class="math inline">\(dy = \left|\frac{dy}{dx}\right| dx\)</span>.</p>
<p>The transformation of the density is <span class="math inline">\(f_y(y) =\left|\frac{dx}{dy}\right| f_x(x(y))\)</span>.</p>
<p>Note that <span class="math inline">\(\left|\frac{dx}{dy}\right| = \left|\frac{dy}{dx}\right|^{-1}\)</span>.</p>
</div>
<div id="random-vectors-and-their-mean-and-variance" class="section level2" number="2.11">
<h2>
<span class="header-section-number">2.11</span> Random vectors and their mean and variance<a class="anchor" aria-label="anchor" href="#random-vectors-and-their-mean-and-variance"><i class="fas fa-link"></i></a>
</h2>
<p>Instead of scalar random variables one often also considers random vectors and also random matrices.</p>
<p>For a random vector <span class="math inline">\(\boldsymbol x= (x_1, x_2,...,x_d)^T\)</span> the mean <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \mu\)</span> is simply comprised of the means of its components, i.e. <span class="math inline">\(\boldsymbol \mu= (\mu_1, \ldots, \mu_d)^T\)</span>. Thus, the mean of a random vector of dimension is a vector of of the same length.</p>
<p>The variance of a random vector of length <span class="math inline">\(d\)</span>, however, is not a vector but a matrix of size <span class="math inline">\(d\times d\)</span>. This matrix is called the <strong>covariance matrix</strong>:
<span class="math display">\[
\begin{split}
\text{Var}(\boldsymbol x) &amp;= \underbrace{\boldsymbol \Sigma}_{d\times d} = (\sigma_{ij}) = \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix} \\
  &amp;=\text{E}\left(\underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d\times 1} \underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1\times d}\right) \\
  &amp; = \text{E}(\boldsymbol x\boldsymbol x^T)-\boldsymbol \mu\boldsymbol \mu^T \\
\end{split}
\]</span>
The entries of the covariance matrix <span class="math inline">\(\sigma_{ij} =\text{Cov}(x_i, x_j)\)</span> describe the covariance between the random variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>. The covariance matrix is symmetric, hence <span class="math inline">\(\sigma_{ij}=\sigma_{ji}\)</span>. The diagonal entries <span class="math inline">\(\sigma_{ii} = \text{Cov}(x_i, x_i) = \text{Var}(x_i) = \sigma_i^2\)</span> correspond to the variances of the components of <span class="math inline">\(\boldsymbol x\)</span>. The covariance matrix is by construction <strong>positive semi-definite</strong>, i.e. the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span> are all positive or equal to zero.</p>
<p>However, wherever possible one will aim to use models with non-singular covariance matrices, with all eigenvalues positive, so that the covariance matrix is invertible.</p>
<p>For univariate <span class="math inline">\(x\)</span> and scalar constant <span class="math inline">\(a\)</span> the variance of <span class="math inline">\(a x\)</span> equals <span class="math inline">\(\text{Var}(a x) = a^2 \text{Var}(x)\)</span>. For a random vector <span class="math inline">\(\boldsymbol x\)</span> of dimension <span class="math inline">\(d\)</span> and constant matrix <span class="math inline">\(\boldsymbol A\)</span> of dimension <span class="math inline">\(m \times d\)</span> this generalises to <span class="math inline">\(\text{Var}(\boldsymbol Ax) = \boldsymbol A\text{Var}(\boldsymbol x) \boldsymbol A^T\)</span>.</p>
</div>
<div id="correlation-matrix" class="section level2" number="2.12">
<h2>
<span class="header-section-number">2.12</span> Correlation matrix<a class="anchor" aria-label="anchor" href="#correlation-matrix"><i class="fas fa-link"></i></a>
</h2>
<p>A covariance matrix can be factorised into the product
<span class="math display">\[
\boldsymbol \Sigma= \boldsymbol V^{\frac{1}{2}} \boldsymbol P\boldsymbol V^{\frac{1}{2}}
\]</span>
where <span class="math inline">\(\boldsymbol V\)</span> is a diagonal matrix containing the variances
<span class="math display">\[
\boldsymbol V= \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}
\]</span>
and the matrix <span class="math inline">\(\boldsymbol P\)</span> (“upper case rho”) is the symmetric <strong>correlation matrix</strong>
<span class="math display">\[
\boldsymbol P= (\rho_{ij}) = \begin{pmatrix}
    1 &amp; \dots &amp; \rho_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \rho_{d1} &amp; \dots &amp; 1
\end{pmatrix}   = \boldsymbol V^{-\frac{1}{2}} \boldsymbol \Sigma\boldsymbol V^{-\frac{1}{2}}
\]</span>
Thus, the correlation between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> is defined as
<span class="math display">\[
\rho_{ij} = \text{Cor}(x_i,x_j) = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}
\]</span></p>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="combinatorics.html"><span class="header-section-number">1</span> Combinatorics</a></div>
<div class="next"><a href="univariate-distributions.html"><span class="header-section-number">3</span> Univariate distributions</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#probability"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="nav-link" href="#random-variables"><span class="header-section-number">2.1</span> Random variables</a></li>
<li><a class="nav-link" href="#probability-mass-and-density-function"><span class="header-section-number">2.2</span> Probability mass and density function</a></li>
<li><a class="nav-link" href="#distribution-function-and-quantile-function"><span class="header-section-number">2.3</span> Distribution function and quantile function</a></li>
<li><a class="nav-link" href="#families-of-distributions"><span class="header-section-number">2.4</span> Families of distributions</a></li>
<li><a class="nav-link" href="#expectation-of-a-random-variable"><span class="header-section-number">2.5</span> Expectation of a random variable</a></li>
<li><a class="nav-link" href="#jensens-inequality-for-the-expectation"><span class="header-section-number">2.6</span> Jensen’s inequality for the expectation</a></li>
<li><a class="nav-link" href="#probability-as-expectation"><span class="header-section-number">2.7</span> Probability as expectation</a></li>
<li><a class="nav-link" href="#moments-and-variance-of-a-random-variable"><span class="header-section-number">2.8</span> Moments and variance of a random variable</a></li>
<li><a class="nav-link" href="#distribution-of-sums-of-random-variables"><span class="header-section-number">2.9</span> Distribution of sums of random variables</a></li>
<li><a class="nav-link" href="#transformation-of-random-variables"><span class="header-section-number">2.10</span> Transformation of random variables</a></li>
<li><a class="nav-link" href="#random-vectors-and-their-mean-and-variance"><span class="header-section-number">2.11</span> Random vectors and their mean and variance</a></li>
<li><a class="nav-link" href="#correlation-matrix"><span class="header-section-number">2.12</span> Correlation matrix</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Probability and Distribution Refresher</strong>" was written by Korbinian Strimmer. It was last built on 6 January 2024.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
