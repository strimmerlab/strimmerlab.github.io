<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Transformations, convolution and loss functions – Probability and Distribution Refresher</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04-univariate.html" rel="next">
<link href="./02-probability.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-9290db0fca16de22067673ab8036b289.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-transformations.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations, convolution and loss functions</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Probability and Distribution Refresher</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Combinatorics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-transformations.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations, convolution and loss functions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-univariate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Univariate distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-multivariate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multivariate distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-expfam.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Exponential families</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-affinetrans" id="toc-sec-affinetrans" class="nav-link active" data-scroll-target="#sec-affinetrans"><span class="header-section-number">3.1</span> Affine or location-scale transformation</a>
  <ul class="collapse">
  <li><a href="#transformation-rule" id="toc-transformation-rule" class="nav-link" data-scroll-target="#transformation-rule">Transformation rule</a></li>
  <li><a href="#probability-mass-function" id="toc-probability-mass-function" class="nav-link" data-scroll-target="#probability-mass-function">Probability mass function</a></li>
  <li><a href="#density" id="toc-density" class="nav-link" data-scroll-target="#density">Density</a></li>
  <li><a href="#moments" id="toc-moments" class="nav-link" data-scroll-target="#moments">Moments</a></li>
  <li><a href="#importance-of-affine-transformations" id="toc-importance-of-affine-transformations" class="nav-link" data-scroll-target="#importance-of-affine-transformations">Importance of affine transformations</a></li>
  </ul></li>
  <li><a href="#sec-geninvtrans" id="toc-sec-geninvtrans" class="nav-link" data-scroll-target="#sec-geninvtrans"><span class="header-section-number">3.2</span> General invertible transformation</a>
  <ul class="collapse">
  <li><a href="#transformation-rule-1" id="toc-transformation-rule-1" class="nav-link" data-scroll-target="#transformation-rule-1">Transformation rule</a></li>
  <li><a href="#probability-mass-function-1" id="toc-probability-mass-function-1" class="nav-link" data-scroll-target="#probability-mass-function-1">Probability mass function</a></li>
  <li><a href="#density-1" id="toc-density-1" class="nav-link" data-scroll-target="#density-1">Density</a></li>
  <li><a href="#moments-1" id="toc-moments-1" class="nav-link" data-scroll-target="#moments-1">Moments</a></li>
  <li><a href="#invertible-affine-transformation-as-special-case" id="toc-invertible-affine-transformation-as-special-case" class="nav-link" data-scroll-target="#invertible-affine-transformation-as-special-case">Invertible affine transformation as special case</a></li>
  </ul></li>
  <li><a href="#sec-convolution" id="toc-sec-convolution" class="nav-link" data-scroll-target="#sec-convolution"><span class="header-section-number">3.3</span> Convolution of random variables</a>
  <ul class="collapse">
  <li><a href="#sum-of-independent-random-variables" id="toc-sum-of-independent-random-variables" class="nav-link" data-scroll-target="#sum-of-independent-random-variables">Sum of independent random variables</a></li>
  <li><a href="#moments-2" id="toc-moments-2" class="nav-link" data-scroll-target="#moments-2">Moments</a></li>
  <li><a href="#convolution" id="toc-convolution" class="nav-link" data-scroll-target="#convolution">Convolution</a></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem">Central limit theorem</a></li>
  </ul></li>
  <li><a href="#sec-lossfunc" id="toc-sec-lossfunc" class="nav-link" data-scroll-target="#sec-lossfunc"><span class="header-section-number">3.4</span> Loss functions and scoring rules</a>
  <ul class="collapse">
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss function</a></li>
  <li><a href="#risk-function" id="toc-risk-function" class="nav-link" data-scroll-target="#risk-function">Risk function</a></li>
  <li><a href="#scoring-rules" id="toc-scoring-rules" class="nav-link" data-scroll-target="#scoring-rules">Scoring rules</a></li>
  <li><a href="#common-loss-functions" id="toc-common-loss-functions" class="nav-link" data-scroll-target="#common-loss-functions">Common loss functions</a></li>
  <li><a href="#logarithmic-scoring-rule" id="toc-logarithmic-scoring-rule" class="nav-link" data-scroll-target="#logarithmic-scoring-rule">Logarithmic scoring rule</a></li>
  <li><a href="#brier-or-quadratic-scoring-rule" id="toc-brier-or-quadratic-scoring-rule" class="nav-link" data-scroll-target="#brier-or-quadratic-scoring-rule">Brier or quadratic scoring rule</a></li>
  <li><a href="#proper-but-not-strictly-proper-scoring-rules" id="toc-proper-but-not-strictly-proper-scoring-rules" class="nav-link" data-scroll-target="#proper-but-not-strictly-proper-scoring-rules">Proper but not strictly proper scoring rules</a></li>
  <li><a href="#other-strictly-proper-scoring-rules" id="toc-other-strictly-proper-scoring-rules" class="nav-link" data-scroll-target="#other-strictly-proper-scoring-rules">Other strictly proper scoring rules</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-transformations" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations, convolution and loss functions</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="sec-affinetrans" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-affinetrans"><span class="header-section-number">3.1</span> Affine or location-scale transformation</h2>
<section id="transformation-rule" class="level3">
<h3 class="anchored" data-anchor-id="transformation-rule">Transformation rule</h3>
<p>Suppose <span class="math inline">\(x\)</span> is a scalar. The variable <span class="math display">\[y= a + b x\]</span> is a <strong>location-scale transformation</strong> or <strong>affine transformation</strong> of <span class="math inline">\(x\)</span>, where <span class="math inline">\(a\)</span> plays the role of the <strong>location parameter</strong> and <span class="math inline">\(b\)</span> is the <strong>scale parameter</strong>. For <span class="math inline">\(a=0\)</span> this is a <strong>linear transformation</strong>.</p>
<p>If <span class="math inline">\(b\neq 0\)</span> then the transformation is <strong>invertible</strong>, with back-transformation <span class="math display">\[x = (y-a)/b\]</span> Invertible transformations provide a one-to-one map between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<p>For a vector <span class="math inline">\(\boldsymbol x\)</span> of dimension <span class="math inline">\(d\)</span> the location-scale transformation is <span class="math display">\[
\boldsymbol y= \boldsymbol a+ \boldsymbol B\boldsymbol x
\]</span> where <span class="math inline">\(\boldsymbol a\)</span> (a <span class="math inline">\(m \times 1\)</span> vector) is the <strong>location parameter</strong> and <span class="math inline">\(\boldsymbol B\)</span> (a <span class="math inline">\(m \times d\)</span> matrix) the <strong>scale parameter</strong>. For <span class="math inline">\(\boldsymbol a=0\)</span> this is a <strong>linear transformation</strong>.</p>
<p>For <span class="math inline">\(m=d\)</span> (square <span class="math inline">\(\boldsymbol B\)</span>) and <span class="math inline">\(\det(\boldsymbol B) \neq 0\)</span> the affine transformation is <strong>invertible</strong> with back-transformation <span class="math display">\[\boldsymbol x= \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\]</span></p>
</section>
<section id="probability-mass-function" class="level3">
<h3 class="anchored" data-anchor-id="probability-mass-function">Probability mass function</h3>
<p>If <span class="math inline">\(x \sim F_x\)</span> is a discrete scalar random variable with pmf <span class="math inline">\(f_{x}(x)\)</span> and assuming an invertible transformation <span class="math inline">\(y(x)= a + b x\)</span> the pmf <span class="math inline">\(f_{y}(y)\)</span> for the discrete scalar random variable <span class="math inline">\(y\)</span> is given by <span class="math display">\[
f_{y}(y)= f_{x} \left( \frac{y-a}{b}\right)
\]</span></p>
<p>Likewise, if <span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> is a discrete random vector with pmf <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span> and assuming an invertible transformation <span class="math inline">\(\boldsymbol y(\boldsymbol x) = \boldsymbol a+ \boldsymbol B\boldsymbol x\)</span> the pmf <span class="math inline">\(f_{\boldsymbol y}(\boldsymbol y)\)</span> for the discrete random vector <span class="math inline">\(\boldsymbol y\)</span> is given by <span class="math display">\[
f_{\boldsymbol y}(\boldsymbol y)= f_{\boldsymbol x} \left( \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\right)
\]</span></p>
</section>
<section id="density" class="level3">
<h3 class="anchored" data-anchor-id="density">Density</h3>
<p>If <span class="math inline">\(x \sim F_x\)</span> is a continuous scalar random variable with pdf <span class="math inline">\(f_{x}(x)\)</span> and assuming an invertible transformation <span class="math inline">\(y(x)= a + b x\)</span> the pdf <span class="math inline">\(f_{y}(y)\)</span> for the continuous random scalar <span class="math inline">\(y\)</span> is given by <span class="math display">\[
f_{y}(y)=|b|^{-1} f_{x} \left( \frac{y-a}{b}\right)
\]</span> where <span class="math inline">\(|b|\)</span> is the absolute value of <span class="math inline">\(b\)</span>. The transformation of the corresponding differential element is <span class="math display">\[
dy = |b| \, dx
\]</span></p>
<p>Likewise, if <span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> is a continuous random vector with pdf <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span> and assuming an invertible transformation <span class="math inline">\(\boldsymbol y(\boldsymbol x) = \boldsymbol a+ \boldsymbol B\boldsymbol x\)</span> the pdf <span class="math inline">\(f_{\boldsymbol y}(\boldsymbol y)\)</span> for the continuous random vector <span class="math inline">\(\boldsymbol y\)</span> is given by <span class="math display">\[
f_{\boldsymbol y}(\boldsymbol y)=|\det\left(\boldsymbol B\right)|^{-1} f_{\boldsymbol x} \left( \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\right)
\]</span> where <span class="math inline">\(|\det(\boldsymbol B)|\)</span> is the absolute value of the determinant <span class="math inline">\(\det(\boldsymbol B)\)</span>. The transformation of the corresponding infinitesimal volume element is <span class="math display">\[
d{\boldsymbol y} = |\det\left(\boldsymbol B\right)|\, d{\boldsymbol x}
\]</span></p>
</section>
<section id="moments" class="level3">
<h3 class="anchored" data-anchor-id="moments">Moments</h3>
<p>The transformed random variable <span class="math inline">\(y \sim F_y\)</span> has mean <span class="math display">\[\operatorname{E}(y) = a + b \mu_x\]</span> and variance <span class="math display">\[\operatorname{Var}(y) = b^2 \sigma^2_x\]</span> where <span class="math inline">\(\operatorname{E}(x) = \mu_x\)</span> and <span class="math inline">\(\operatorname{Var}(x) = \sigma^2_x\)</span> are the mean and variance of the original variable <span class="math inline">\(x\)</span>.</p>
<p>The mean and variance of the transformed random vector <span class="math inline">\(\boldsymbol y\sim F_{\boldsymbol y}\)</span> is <span class="math display">\[\operatorname{E}(\boldsymbol y)=\boldsymbol a+ \boldsymbol B\,\boldsymbol \mu_{\boldsymbol x}\]</span> and <span class="math display">\[\operatorname{Var}(\boldsymbol y)= \boldsymbol B\,\boldsymbol \Sigma_{\boldsymbol x} \,\boldsymbol B^T\]</span> where <span class="math inline">\(\operatorname{E}(\boldsymbol x)=\boldsymbol \mu_{\boldsymbol x}\)</span> and <span class="math inline">\(\operatorname{Var}(\boldsymbol x)=\boldsymbol \Sigma_{\boldsymbol x}\)</span> are the mean and variance of the original random vector <span class="math inline">\(\boldsymbol x\)</span>.</p>
</section>
<section id="importance-of-affine-transformations" class="level3">
<h3 class="anchored" data-anchor-id="importance-of-affine-transformations">Importance of affine transformations</h3>
<p>The constants <span class="math inline">\(\boldsymbol a\)</span> and <span class="math inline">\(\boldsymbol B\)</span> (or <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> in the univariate case) are the parameters of the <strong>location-scale family</strong> <span class="math inline">\(F_{\boldsymbol y}(\boldsymbol a, \boldsymbol B)\)</span> created from <span class="math inline">\(F_{\boldsymbol x}\)</span>. Many important distributions are location-scale families such as the normal distribution (cf. <a href="04-univariate.html#sec-normdist" class="quarto-xref"><span>Section 4.3</span></a> and <a href="05-multivariate.html#sec-mvndist" class="quarto-xref"><span>Section 5.3</span></a>) and the location-scale <span class="math inline">\(t\)</span>-distribution (<a href="04-univariate.html#sec-lstdist" class="quarto-xref"><span>Section 4.6</span></a> and <a href="05-multivariate.html#sec-mvtdist" class="quarto-xref"><span>Section 5.6</span></a>). Furthermore, key procedures in multivariate statistics such as orthogonal transformations (including PCA) or whitening transformations (e.g.&nbsp;the Mahalanobis transformation) are affine transformations.</p>
</section>
</section>
<section id="sec-geninvtrans" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-geninvtrans"><span class="header-section-number">3.2</span> General invertible transformation</h2>
<section id="transformation-rule-1" class="level3">
<h3 class="anchored" data-anchor-id="transformation-rule-1">Transformation rule</h3>
<p>As above we assume <span class="math inline">\(x\)</span> is a scalar and <span class="math inline">\(\boldsymbol x\)</span> is a vector and consider the general invertible transformation.</p>
<p>For a scalar variable the transformation is specified by <span class="math inline">\(y(x) = h(x)\)</span> and the back-transformation by <span class="math inline">\(x(y) = h^{-1}(y)\)</span>. For a vector this becomes <span class="math inline">\(\boldsymbol y(\boldsymbol x) = \boldsymbol h(\boldsymbol x)\)</span> with back-transformation <span class="math inline">\(\boldsymbol x(\boldsymbol y) = \boldsymbol h^{-1}(\boldsymbol y)\)</span>. The functions <span class="math inline">\(h(x)\)</span> and <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> are assumed to be invertible.</p>
</section>
<section id="probability-mass-function-1" class="level3">
<h3 class="anchored" data-anchor-id="probability-mass-function-1">Probability mass function</h3>
<p>If <span class="math inline">\(x \sim F_x\)</span> is a discrete scalar random variable with pmf <span class="math inline">\(f_{x}(x)\)</span> then the pmf <span class="math inline">\(f_y(y)\)</span> of the transformed discrete scalar random variable <span class="math inline">\(y(x)\)</span> is given by <span class="math display">\[
f_y(y) = f_x(x(y))
\]</span></p>
<p>Likewise, for a discrete random vector <span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> with pmf <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span> the pmf <span class="math inline">\(f_{\boldsymbol y}(\boldsymbol y)\)</span> for the discrete random vector <span class="math inline">\(\boldsymbol y(\boldsymbol x)\)</span> is obtained by <span class="math display">\[
f_{\boldsymbol y}(\boldsymbol y) =  f_{\boldsymbol x}\left( \boldsymbol x(\boldsymbol y) \right)
\]</span></p>
</section>
<section id="density-1" class="level3">
<h3 class="anchored" data-anchor-id="density-1">Density</h3>
<p>If <span class="math inline">\(x \sim F_x\)</span> is a continuous scalar random variable with pdf <span class="math inline">\(f_{x}(x)\)</span> the pdf <span class="math inline">\(f_y(y)\)</span> of the transformed continuous scalar random variable <span class="math inline">\(y(x)\)</span> is given by <span class="math display">\[
f_y(y) =\left| D x(y) \right|\, f_x(x(y))
\]</span> where <span class="math inline">\(D x(y)\)</span> is the derivative of the inverse transformation <span class="math inline">\(x(y)\)</span>. The transformation of the differential element is <span class="math display">\[
dy = \left| D y(x) \right| \, dx
\]</span> Note that <span class="math inline">\(| D x(y)| = | D y(x)|^{-1}\rvert_{x = x(y)}\)</span>.</p>
<p>Likewise, for a continuous random vector <span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> with pdf <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span> the pdf <span class="math inline">\(f_{\boldsymbol y}(\boldsymbol y)\)</span> for the continuous random vector <span class="math inline">\(\boldsymbol y(\boldsymbol x)\)</span> is obtained by <span class="math display">\[
f_{\boldsymbol y}(\boldsymbol y) = |\det\left( D\boldsymbol x(\boldsymbol y) \right)| \,\,  f_{\boldsymbol x}\left( \boldsymbol x(\boldsymbol y) \right)
\]</span> where <span class="math inline">\(D\boldsymbol x(\boldsymbol y)\)</span> is the Jacobian matrix of the inverse transformation <span class="math inline">\(\boldsymbol x(\boldsymbol y)\)</span>. The transformation of the infinitesimal volume element is <span class="math display">\[
d{\boldsymbol y} = |\det\left( D\boldsymbol y(\boldsymbol x) \right)|\, d{\boldsymbol x}
\]</span> Note that <span class="math inline">\(|\det\left( D\boldsymbol x(\boldsymbol y) \right)| = |\det\left( D\boldsymbol y(\boldsymbol x) \right)|^{-1} \rvert_{\boldsymbol x= \boldsymbol x(\boldsymbol y)}\)</span>.</p>
</section>
<section id="moments-1" class="level3">
<h3 class="anchored" data-anchor-id="moments-1">Moments</h3>
<p>The mean and variance of the transformed random variable can typically only be approximated. Assume that <span class="math inline">\(\operatorname{E}(x) = \mu_x\)</span> and <span class="math inline">\(\operatorname{Var}(x) = \sigma^2_x\)</span> are the mean and variance of the original random variable <span class="math inline">\(x\)</span> and <span class="math inline">\(\operatorname{E}(\boldsymbol x)=\boldsymbol \mu_{\boldsymbol x}\)</span> and <span class="math inline">\(\operatorname{Var}(\boldsymbol x)=\boldsymbol \Sigma_{\boldsymbol x}\)</span> are the mean and variance of the original random vector <span class="math inline">\(\boldsymbol x\)</span>. In the <strong>delta method</strong> the transformation <span class="math inline">\(y(x)\)</span> resp. <span class="math inline">\(\boldsymbol y(\boldsymbol x)\)</span> is linearised around the mean <span class="math inline">\(\mu_x\)</span> respectively <span class="math inline">\(\boldsymbol \mu_{\boldsymbol x}\)</span> and the mean and variance resulting from the linear transformation is reported.</p>
<p>Specifically, the linear approximation for the scalar-valued function is <span class="math display">\[
y(x) \approx y\left(\mu_x\right) + D y\left(\mu_x\right)\, \left(x-\mu_x\right)
\]</span> where <span class="math inline">\(D y(x) = y'(x)\)</span> is the first derivative of the transformation <span class="math inline">\(y(x)\)</span> and <span class="math inline">\(D y\left(\mu_x\right)\)</span> is the first derivative evaluated at the mean <span class="math inline">\(\mu_x\)</span>, and for the vector-valued function <span class="math display">\[
\boldsymbol y(\boldsymbol x) \approx \boldsymbol y\left(\boldsymbol \mu_{\boldsymbol x}\right) + D \boldsymbol y\left(\boldsymbol \mu_{\boldsymbol x}\right) \, \left(\boldsymbol x-\boldsymbol \mu_{\boldsymbol x}\right)
\]</span> where <span class="math inline">\(D \boldsymbol y(\boldsymbol x)\)</span> is the Jacobian matrix (vector derivative) for the transformation <span class="math inline">\(\boldsymbol y(\boldsymbol x)\)</span> and <span class="math inline">\(D \boldsymbol y\left(\boldsymbol \mu_{\boldsymbol x}\right)\)</span> is the Jacobian matrix evaluated at the mean <span class="math inline">\(\boldsymbol \mu_{\boldsymbol x}\)</span>.</p>
<p>In the univariate case the delta method yields as approximation for the mean and variance of the transformed random variable <span class="math inline">\(y\)</span> <span class="math display">\[
\operatorname{E}(y) \approx y\left(\mu_x\right)
\]</span> and <span class="math display">\[
\operatorname{Var}(y)\approx \left(D y\left(\mu_x\right)\right)^2 \, \sigma^2_x  
\]</span></p>
<p>For the vector random variable <span class="math inline">\(\boldsymbol y\)</span> the delta method yields <span class="math display">\[\operatorname{E}(\boldsymbol y)\approx\boldsymbol y\left(\boldsymbol \mu_{\boldsymbol x}\right)\]</span> and <span class="math display">\[
\operatorname{Var}(\boldsymbol y)\approx D \boldsymbol y\left(\boldsymbol \mu_{\boldsymbol x}\right) \, \boldsymbol \Sigma_{\boldsymbol x} \, D\boldsymbol y\left(\boldsymbol \mu_{\boldsymbol x}\right)^T
\]</span></p>
</section>
<section id="invertible-affine-transformation-as-special-case" class="level3">
<h3 class="anchored" data-anchor-id="invertible-affine-transformation-as-special-case">Invertible affine transformation as special case</h3>
<p>The invertible affine transformation (<a href="#sec-affinetrans" class="quarto-xref"><span>Section 3.1</span></a>) is special case of the general invertible transformation.</p>
<p>Assuming <span class="math inline">\(y(x) = a + b x\)</span>, with <span class="math inline">\(x(y) = (y-a)/b\)</span>, <span class="math inline">\(D y(x) = b\)</span> and <span class="math inline">\(D x(y) = b^{-1}\)</span>, recovers the univariate location-scale transformation.</p>
<p>Likewise, assuming <span class="math inline">\(\boldsymbol y(\boldsymbol x) = \boldsymbol a+ \boldsymbol B\boldsymbol x\)</span>, with <span class="math inline">\(\boldsymbol x(\boldsymbol y) = \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\)</span>, <span class="math inline">\(D\boldsymbol y(\boldsymbol x) = \boldsymbol B\)</span> and <span class="math inline">\(D\boldsymbol x(\boldsymbol y) = \boldsymbol B^{-1}\)</span>, recovers the multivariate location-scale transformation.</p>
</section>
</section>
<section id="sec-convolution" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-convolution"><span class="header-section-number">3.3</span> Convolution of random variables</h2>
<section id="sum-of-independent-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="sum-of-independent-random-variables">Sum of independent random variables</h3>
<p>Suppose we have a sum of <span class="math inline">\(n\)</span> <em>independent</em> scalar random variables. <span class="math display">\[
y = x_1 + x_2 + \ldots + x_n
\]</span> where each <span class="math inline">\(x_i \sim F_{x_i}\)</span> has its own distribution and corresponding pdmf <span class="math inline">\(f_{x_i}(x)\)</span>. The corresponding means are <span class="math inline">\(\operatorname{E}(x_i) = \mu_i\)</span> and the variances are <span class="math inline">\(\operatorname{Var}(x_i) = \sigma^2_i\)</span>. As the <span class="math inline">\(x_i\)</span> are independent, and therefore uncorrelated, the covariances <span class="math inline">\(\operatorname{Cov}(x_i, x_j)=0\)</span> vanish for <span class="math inline">\(i \neq j\)</span>.</p>
<p>With <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_n)^T\)</span> and <span class="math inline">\(\mathbf 1_n = (1, 1, \ldots, 1)^T\)</span> the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> can be written as the linear transformation <span class="math display">\[
y= \mathbf 1_n^T \boldsymbol x
\]</span> As <span class="math inline">\(y\)</span> is a scalar and <span class="math inline">\(\boldsymbol x\)</span> a vector the transformation from <span class="math inline">\(\boldsymbol x\)</span> to <span class="math inline">\(y\)</span> is not invertible.</p>
</section>
<section id="moments-2" class="level3">
<h3 class="anchored" data-anchor-id="moments-2">Moments</h3>
<p>With <span class="math inline">\(\operatorname{E}(\boldsymbol x) = \boldsymbol \mu\)</span> and <span class="math inline">\(\operatorname{Var}(\boldsymbol x) = \operatorname{Diag}(\sigma^2_1, \ldots, \sigma^2_n)\)</span> the mean of the random variable <span class="math inline">\(y\)</span> equals <span class="math display">\[
\operatorname{E}(y) = \mathbf 1_n^T \boldsymbol \mu= \sum_{i=1}^n \mu_i
\]</span> and the variance of <span class="math inline">\(y\)</span> is <span class="math display">\[
\operatorname{Var}(y) = \mathbf 1_n^T \, \operatorname{Var}(\boldsymbol x) \,  \mathbf 1_n  =   \sum_{i=1}^n \sigma^2_i
\]</span><br>
(cf. <a href="#sec-affinetrans" class="quarto-xref"><span>Section 3.1</span></a>). Thus both the mean and variance of <span class="math inline">\(y\)</span> are simply the sums of the individual means and variances (note that for the variance this only holds because the individual variables are uncorrelated).</p>
</section>
<section id="convolution" class="level3">
<h3 class="anchored" data-anchor-id="convolution">Convolution</h3>
<p>The pdmf <span class="math inline">\(f_y(y)\)</span> for <span class="math inline">\(y\)</span> is obtained by repeatedly convolving (denoted by the asterisk <span class="math inline">\(\ast\)</span> operator) the pdmfs of the <span class="math inline">\(x_i\)</span>: <span class="math display">\[
f_y(y) = \left(f_{x_1} \ast f_{x_2} \ast \ldots f_{x_n}\right)(y)
\]</span></p>
<p>The <strong>convolution</strong> of two functions is defined as (continuous case) <span class="math display">\[
\left(f_{x_1}\ast f_{x_2}\right)(y)=\int_x f_{x_1}(x)\, f_{x_2}(y-x) dx
\]</span> and (discrete case) <span class="math display">\[
\left(f_{x_1}\ast f_{x_2}\right)(y)=\sum_x f_{x_1}(x)\, f_{x_2}(y-x)
\]</span> Convolution is commutative and associative so so you may convolve multiple pdmfs in any order or grouping. Furthermore, the convolution of pdmfs yields another pdmf, i.e.&nbsp;the resulting function integrates to one.</p>
<p>Many commonly used random variables can be viewed as the outcome of convolutions. For example, the sum of Bernoulli variables yields a binomial random variable and the sum of normal variables yields another normal random variable.</p>
<p>See also (Wikipedia): <a href="https://en.wikipedia.org/wiki/List_of_convolutions_of_probability_distributions">list of convolutions of probability distributions</a>.</p>
</section>
<section id="central-limit-theorem" class="level3">
<h3 class="anchored" data-anchor-id="central-limit-theorem">Central limit theorem</h3>
<p>The <strong>central limit theorem</strong>, first postulated by <a href="https://en.wikipedia.org/wiki/Abraham_de_Moivre">Abraham de Moivre (1667–1754)</a> and later proved by <a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">Pierre-Simon Laplace (1749–1827)</a> asserts that the distribution of the sum of <span class="math inline">\(n\)</span> independent and identically distributed random variables with finite mean and finite variance converges in the limit of large <span class="math inline">\(n\)</span> to a normal distribution (<a href="04-univariate.html#sec-normdist" class="quarto-xref"><span>Section 4.3</span></a>), even if the individual random variables are not themselves normal. In other words, it asserts that for large <span class="math inline">\(n\)</span> the convolution of <span class="math inline">\(n\)</span> identical distributions with finite first two moments converges to a normal distribution.</p>
</section>
</section>
<section id="sec-lossfunc" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-lossfunc"><span class="header-section-number">3.4</span> Loss functions and scoring rules</h2>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss function</h3>
<p>A <strong>loss or cost function</strong> <span class="math inline">\(L(x, a)\)</span> evaluates a prediction <span class="math inline">\(a\)</span> (for example a parameter or a probability distribution) on the basis of an observed outcome <span class="math inline">\(x\)</span>, and returns a numerical score.</p>
<p>A loss function measures, informally, the error between <span class="math inline">\(x\)</span> and <span class="math inline">\(a\)</span>. During optimisation the prediction <span class="math inline">\(a\)</span> is varied and the aim is minimisation of the error (hence a loss function has <em>negative orientation</em>, smaller is better).</p>
<p>Adding a constant or a positive scaling factor to the loss function will not change the location of its minimum, so such loss functions are considered equivalent.</p>
<p>A <strong>utility or reward function</strong> is a loss function with a reversed sign (hence it has <em>positive orientation</em>, larger is better).</p>
</section>
<section id="risk-function" class="level3">
<h3 class="anchored" data-anchor-id="risk-function">Risk function</h3>
<p>The <strong>risk</strong> of <span class="math inline">\(a\)</span> under the distribution <span class="math inline">\(Q\)</span> for <span class="math inline">\(x\)</span> is defined as the expected loss <span class="math display">\[
R_Q(a) = \operatorname{E}_Q(L(x, a))
\]</span> If there is no ambiguity we drop the reference to <span class="math inline">\(Q\)</span> and write <span class="math display">\[
R(a) = \operatorname{E}(L(x, a))
\]</span></p>
<p>The risk of <span class="math inline">\(a\)</span> under the empirical distribution <span class="math inline">\(\hat{Q}_n\)</span> obtained from observations <span class="math inline">\(x_1, \ldots, x_n\)</span> is the <strong>empirical risk</strong> <span class="math display">\[
\hat{R}(a) = R_{\hat{Q}_n}(a) =
\frac{1}{n} \sum_{i=1}^{n} L(x_i, a)
\]</span> where the expectation is replaced by the sample average.</p>
<p>Minimising <span class="math inline">\(R(a)\)</span> finds optimal predictions<br>
<span class="math display">\[
a^{\ast} = \underset{a}{\arg \min}\, R(a)
\]</span> Depending on the choice of underlying loss <span class="math inline">\(L(x, a)\)</span> minimising the risk provides a very general optimisation-based way to identify distributional features of the distribution <span class="math inline">\(Q\)</span> and to obtain parameter estimates.</p>
</section>
<section id="scoring-rules" class="level3">
<h3 class="anchored" data-anchor-id="scoring-rules">Scoring rules</h3>
<p>A <strong>scoring rule</strong> <span class="math inline">\(S(x, P)\)</span> is special type of loss function<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> that assesses the probabilistic forecast <span class="math inline">\(P\)</span> by assigning a numerical score based on <span class="math inline">\(P\)</span> and the observed outcome <span class="math inline">\(x\)</span>.</p>
<p>The associated <strong>risk</strong> of <span class="math inline">\(P\)</span> under <span class="math inline">\(Q\)</span> is <span class="math display">\[
R_Q(P) = \operatorname{E}_{Q}\left(S(x, P)\right)
\]</span> For a <strong>proper</strong> scoring rule the risk <span class="math inline">\(R_Q(P)\)</span> is minimised at <span class="math inline">\(P = Q\)</span>, hence <span class="math display">\[
R_Q(P) \geq R_Q(Q)
\]</span> For a <strong>strictly proper</strong> scoring rule the minimum is achieved only at the true distribution <span class="math inline">\(Q\)</span>, so equality holds exclusively for <span class="math inline">\(P = Q\)</span>.</p>
<p>A proper scoring rule induces a <strong>divergence</strong> between the distributions <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>, as the difference between the risk and the minimum risk : <span class="math display">\[
D(Q, P) = R_Q(P) - R_Q(Q) \geq 0
\]</span> By construction, the divergence <span class="math inline">\(D(Q, P)\)</span> is always <strong>non-negative</strong> and equals zero if <span class="math inline">\(P=Q\)</span>. For a stricly proper scoring rule the divergence vanishes exclusively for <span class="math inline">\(P=Q\)</span>.</p>
<p>Proper scoring rules are very useful as they allow to identify the underlying distribution and their parameters by risk minimisation or minimisation of the associated divergences.</p>
<p>Proper scoring rules also have a number of further useful properties. For example, various <strong>decompositions</strong> exist for their risk, and the divergence satisfies a <strong>generalised Pythagorean theorem</strong>. Furthermore, there is a <strong>correspondence</strong> of proper scoring rules and their associated divergences with <a href="https://en.wikipedia.org/wiki/Bregman_divergence"><strong>Bregman divergences</strong></a>.</p>
</section>
<section id="common-loss-functions" class="level3">
<h3 class="anchored" data-anchor-id="common-loss-functions">Common loss functions</h3>
<p>The <strong>squared loss</strong> or <strong>squared error</strong> is one of the most commonly used loss functions: <span class="math display">\[
L(x,a) = (x-a)^2
\]</span> The corresponding risk is the <strong>mean squared loss</strong> or <strong>mean squared error</strong> (MSE) <span class="math display">\[
R(a) = \operatorname{E}((x-a)^2)
\]</span> From <span class="math inline">\(R(a) = \operatorname{E}((x-a)^2) = \operatorname{E}(x^2) - 2 a \operatorname{E}(x) + a^2\)</span> it follows <span class="math inline">\(dR(a)/da = - 2 \operatorname{E}(x) + 2 a\)</span> and thus that the MSE is <strong>minimised at the mean</strong> <span class="math inline">\(a^{\ast} = \operatorname{E}(x)\)</span>. The <strong>achieved minimum risk</strong> <span class="math inline">\(R(a^{\ast}) = \operatorname{Var}(x)\)</span> is the <strong>variance</strong>.</p>
<p>The <strong>0-1 loss</strong> function can be written as <span class="math display">\[
L(x, a) =
\begin{cases}
-[x = a] &amp; \text{discrete case} \\
-\delta(x-a) &amp; \text{continuous case} \\
\end{cases}
\]</span> employing the indicator function and Dirac delta function, respectively. The corresponding risk assuming <span class="math inline">\(x \sim Q\)</span> and pdmf <span class="math inline">\(q(x)\)</span> is <span class="math display">\[
R_Q(a) = -q(a)
\]</span> which is <strong>minimised at the mode</strong> of the pdmf.</p>
<p>The <strong>asymmetric loss</strong> can be defined as <span class="math display">\[
L(x, a; \tau) =
\begin{cases}
2 \tau (x-a) &amp; \text{for $x\geq a$} \\
2 (1-\tau) (a-x)  &amp; \text{for $x &lt; a$} \\
\end{cases}
\]</span> and the corresponding risk is <strong>minimised at the quantile</strong> <span class="math inline">\(x_{\tau}\)</span>.</p>
<p>For <span class="math inline">\(\tau=1/2\)</span> it reduces to the <strong>absolute loss</strong> <span class="math display">\[
L(x, a) = | x - a|
\]</span> whose corresponding risk is <strong>minimised at the median</strong> <span class="math inline">\(x_{1/2}\)</span>.</p>
</section>
<section id="logarithmic-scoring-rule" class="level3">
<h3 class="anchored" data-anchor-id="logarithmic-scoring-rule">Logarithmic scoring rule</h3>
<p>The most important scoring rule is the <strong>logarithmic scoring rule</strong> or <strong>log-loss</strong> <span class="math display">\[
S(x, P) = - \log p(x)
\]</span></p>
<p>The risk of <span class="math inline">\(P\)</span> under <span class="math inline">\(Q\)</span> based on the log-loss is the <strong>mean log-loss</strong> or <strong>cross-entropy</strong> <span class="math display">\[
R_Q(P) = - \operatorname{E}_{Q} \log p(x) = H(Q,P)
\]</span> which is uniquely minimised for <span class="math inline">\(P=Q\)</span>. Thus, the log-loss is <strong>strictly proper</strong>. Furthermore, the log-loss is notably the only <strong>local</strong> strictly proper scoring rule, as it solely depends on the value of the pdmf at the observed outcome <span class="math inline">\(x\)</span>, and not on any other features of the distribution&nbsp;<span class="math inline">\(P\)</span>. The minimum risk is the Shannon-Gibbs <strong>entropy</strong> of&nbsp;<span class="math inline">\(Q\)</span>: <span class="math display">\[
R_Q(Q) = -\operatorname{E}_{Q} \log q(x) = H(Q)
\]</span> The relationship <span class="math inline">\(H(Q, P) \geq H(Q)\)</span>, with equality exclusively for <span class="math inline">\(P=Q\)</span>, is known as <strong>Gibbs’ inequality</strong>.</p>
<p>The divergence induced by the log-loss is the Kullback-Leibler (KL) divergence <span class="math display">\[
\begin{split}
D_{\text{KL}}(Q,P) &amp;=  H(Q,P) -H(Q) \\
          &amp;= \operatorname{E}_{Q} \log\left(\frac{q(x)}{p(x)}\right)\\
\end{split}
\]</span> The KL divergence obeys the <strong>data processing inequality</strong>, i.e.&nbsp;applying a transformation to the underlying random variables cannot increase the KL divergence <span class="math inline">\(D_{\text{KL}}(Q,P)\)</span> between <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span>. This property also holds for all <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergences</a> (of which the KL divergence is a principal example), but is notably <em>not</em> satisfied by divergences of other proper scoring rules (and thus other Bregman divergences).</p>
<p>Furthermore, the KL divergence is the only divergence induced by proper scoring rules (and thus the only Bregman divergence), as well as the only <span class="math inline">\(f\)</span>-divergence, that is <strong>invariant against general coordinate transformations</strong>. Coordinate transformations can be viewed as a special case of data processing, and for <span class="math inline">\(D_{\text{KL}}(Q,P)\)</span> the data-processing inequality under general invertible transformations becomes an identity.</p>
<p>The empirical risk of a distribution family <span class="math inline">\(P(\theta)\)</span> based on the log-loss is proportional to the log-likelihood function <span class="math display">\[
\begin{split}
\hat{R}(\theta) &amp;= H(\hat{Q}_n, P(\theta)) \\
                &amp;= - \frac{1}{n} \sum_{i=1}^n \log p(x_i | \theta) \\
                &amp;= - \frac{1}{n} \ell_n(\theta)\\
\end{split}
\]</span> Minimising the empirical risk <span class="math inline">\(\hat{R}(\theta)\)</span> is equivalent to maximising the log-likelihood function <span class="math inline">\(\ell_n(\theta)\)</span>.</p>
<p>Similarly, minimising the KL divergence <span class="math inline">\(D_{\text{KL}}(\hat{Q}_n,P(\theta))\)</span> with regard to&nbsp;<span class="math inline">\(\theta\)</span> is equivalent to minimising the empirical risk <span class="math inline">\(\hat{R}(\theta)\)</span> and hence to maximum likelihood.</p>
</section>
<section id="brier-or-quadratic-scoring-rule" class="level3">
<h3 class="anchored" data-anchor-id="brier-or-quadratic-scoring-rule">Brier or quadratic scoring rule</h3>
<p>The <strong>Brier scoring rule</strong>, also known as <strong>quadratic scoring rule</strong>, evaluates a probabilistic categorical forecast <span class="math inline">\(P\)</span> with corresponding class probabilities <span class="math inline">\(p_1, \ldots, p_K\)</span> given a realisation <span class="math inline">\(\boldsymbol x\)</span> from the categorical distribution <span class="math inline">\(Q\)</span> with class probabilities <span class="math inline">\(q_1, \ldots, q_K\)</span>. It can be written as <span class="math display">\[
\begin{split}
S(\boldsymbol x, P) &amp;= \sum_{y=1}^K \left(x_y -p_y\right)^2 \\
&amp;= 1 -  2 \sum_{y=1}^K x_y p_y +   \sum_{y=1}^K p_y^2\\
&amp;= 1 -  2  p_k +   \sum_{y=1}^K p_y^2\\
\end{split}
\]</span> The indicator vector <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_K)^T = (0, 0, \ldots, 1, \ldots, 0)^T\)</span> contains zeros everywhere except for a single element <span class="math inline">\(x_k=1\)</span>. Unlike the log-loss, the Brier score is <em>not local</em> as the pmf for <span class="math inline">\(P\)</span> is evaluated across all <span class="math inline">\(K\)</span> classes, not just at the realised class <span class="math inline">\(k\)</span>.</p>
<p>The corresponding risk is <span class="math display">\[
\begin{split}
R_Q(P) &amp;= \operatorname{E}_Q(S(\boldsymbol x, P)) \\
       &amp;= 1 -2 \sum_{y=1}^K q_y p_y +\sum_{y=1}^K p_y^2\\
\end{split}
\]</span> which is uniquely minimised for <span class="math inline">\(P=Q\)</span>. Thus, the Brier score is <strong>strictly proper</strong>. The minimum risk is <span class="math display">\[
R_Q(Q) = 1 -  \sum_{y=1}^K q_y^2
\]</span></p>
<p>The divergence induced by the Brier score is the <strong>squared Euclidean distance</strong> between the two pmfs: <span class="math display">\[
\begin{split}
D(Q, P) &amp;= R_Q(P) - R_Q(Q) \\
        &amp; = \sum_{y=1}^K  \left(q_y - p_y\right)^2\\
\end{split}
\]</span></p>
</section>
<section id="proper-but-not-strictly-proper-scoring-rules" class="level3">
<h3 class="anchored" data-anchor-id="proper-but-not-strictly-proper-scoring-rules">Proper but not strictly proper scoring rules</h3>
<p>An example of a proper, but not strictly proper, scoring rule is the <strong>squared error relative to the mean</strong> of the quoted model <span class="math inline">\(P\)</span>: <span class="math display">\[
S(x, P) = (x- \operatorname{E}(P))^2
\]</span></p>
<p>The corresponding risk is <span class="math display">\[
\begin{split}
R_Q(P) &amp;= \operatorname{E}_Q\left( (x- \operatorname{E}(P))^2 \right)\\
       &amp; = (\operatorname{E}(Q)-\operatorname{E}(P))^2 + \operatorname{Var}(Q)\\
\end{split}
\]</span> which is minimised at <span class="math inline">\(P=Q\)</span> but also at any distribution <span class="math inline">\(P\)</span> with the same mean as <span class="math inline">\(Q\)</span>. The minimum risk is the variance of <span class="math inline">\(Q\)</span>: <span class="math display">\[
R_Q(Q) = \operatorname{Var}(Q)
\]</span></p>
<p>The associated divergence is the squared distance between the two means <span class="math display">\[
\begin{split}
D(Q, P) &amp;= R_Q(P) - \operatorname{Var}(Q) \\
        &amp;= (\operatorname{E}(Q)-\operatorname{E}(P))^2\\
\end{split}
\]</span> which vanishes at <span class="math inline">\(P=Q\)</span> but also at any <span class="math inline">\(P\)</span> with <span class="math inline">\(\operatorname{E}(P)=\operatorname{E}(Q)\)</span>.</p>
<p>The <strong>Dawid-Sebastiani scoring rule</strong> is a related scoring rule given by <span class="math display">\[
S\left(x, P\right) =  \log \operatorname{Var}(P) + \frac{(x-\operatorname{E}(P))^2}{\operatorname{Var}(P)}
\]</span> It is equivalent to the log-loss applied to a normal model <span class="math inline">\(P\)</span>.</p>
<p>The corresponding risk is <span class="math display">\[
\begin{split}
R_Q(P) &amp;= \log \operatorname{Var}(P)  + \frac{(\operatorname{E}(Q)-\operatorname{E}(P))^2}{\operatorname{Var}(P)} + \frac{\operatorname{Var}(Q)}{\operatorname{Var}(P)}\\
\end{split}
\]</span> which is minimised at <span class="math inline">\(P=Q\)</span> but also at any distribution <span class="math inline">\(P\)</span> with <span class="math inline">\(\operatorname{E}(P)=\operatorname{E}(Q)\)</span> and <span class="math inline">\(\operatorname{Var}(P)=\operatorname{Var}(Q)\)</span>.</p>
<p>The minimum risk is <span class="math display">\[
R_Q(Q) = \log \operatorname{Var}(Q) +1
\]</span></p>
<p>The associated divergence is <span class="math display">\[
\begin{split}
D(Q, P) &amp;= R_Q(P) - \operatorname{Var}(Q) \\
        &amp;= \frac{(\operatorname{E}(Q)-\operatorname{E}(P))^2}{\operatorname{Var}(P)} +\frac{\operatorname{Var}(Q)}{\operatorname{Var}(P)}  - \log\left( \frac{\operatorname{Var}(Q}{\operatorname{Var}(P} \right)  -1 \\
\end{split}
\]</span> which vanishes at <span class="math inline">\(P=Q\)</span> but also at any <span class="math inline">\(P\)</span> for which <span class="math inline">\(\operatorname{E}(P)=\operatorname{E}(Q)\)</span> and <span class="math inline">\(\operatorname{Var}(P)=\operatorname{Var}(Q)\)</span>.</p>
</section>
<section id="other-strictly-proper-scoring-rules" class="level3">
<h3 class="anchored" data-anchor-id="other-strictly-proper-scoring-rules">Other strictly proper scoring rules</h3>
<p>Other useful strictly proper scoring rules include:</p>
<ul>
<li>the continuous ranked probability score (CRPS),</li>
<li>the energy score, and</li>
<li>the Hyvärinen scoring rule.</li>
</ul>
<p>See also (Wikipedia): <a href="https://en.wikipedia.org/wiki/Scoring_rule">scoring rule</a>.</p>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>As a loss function, scoring rules are negatively oriented. However, some authors consider them as utility functions with positive orientation.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/probability-distribution-refresher\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02-probability.html" class="pagination-link" aria-label="Probability">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04-univariate.html" class="pagination-link" aria-label="Univariate distributions">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Univariate distributions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>