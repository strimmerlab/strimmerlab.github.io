[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Multivariate Statistics and Machine Learning",
    "section": "",
    "text": "Welcome\nThese are the lecture notes for MATH38161, a course in Multivariate Statistics and Machine Learning for third year mathematics students at the Department of Mathematics of the University of Manchester.\nThe course text was written by Korbinian Strimmer from 2018–2024. This version is from 8 April 2024.\nIf you have any questions, comments, or corrections please get in touch! 1",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Multivariate Statistics and Machine Learning",
    "section": "Updates",
    "text": "Updates\nThe notes will be updated from time to time. To view the current version visit the\n\nonline MATH38161 lecture notes.\n\nYou may also wish to download the MATH38161 lecture notes as\n\nPDF in A4 format for printing (double page layout), or as\n6x9 inch PDF for use on tablets (single page layout).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Multivariate Statistics and Machine Learning",
    "section": "License",
    "text": "License\nThese notes are licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Multivariate Statistics and Machine Learning",
    "section": "",
    "text": "Email address: korbinian.strimmer@manchester.ac.uk↩︎",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "00-preface.html",
    "href": "00-preface.html",
    "title": "Preface",
    "section": "",
    "text": "About the author\nHello! My name is Korbinian Strimmer and I am a Professor in Statistics. I am a member of the Statistics group at the Department of Mathematics of the University of Manchester. You can find more information about me on my home page.\nI have first taught this module in the winter semester 2018 at the University of Manchester, and subsequently all the following years until (including) the winter semester 2023.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#about-the-module",
    "href": "00-preface.html#about-the-module",
    "title": "Preface",
    "section": "About the module",
    "text": "About the module\n\nTopics covered\nThe MATH38161 module is designed to run over the course of 11 weeks. It has six parts, each covering a particular aspect of multivariate statistics and machine learning:\n\nMultivariate random variables and estimation in large and small sample settings (W1 and W2)\nTransformations and dimension reduction (W3 and W4)\nUnsupervised learning/clustering (W5 and W6)\nSupervised learning/classification (W7 and W8)\nMeasuring and modelling multivariate dependencies (W9)\nNonlinear and nonparametric models (W10, W11)\n\nThis module focuses on:\n\nConcepts and methods (not on theory)\nImplementation and application in R\nPractical data analysis and interpretation (incl. report writing)\nModern tools in data science and statistics (R markdown, R studio)\n\n\n\nAdditional support material\nIf you are a University of Manchester student and enrolled in this module you will find on Blackboard:\n\na weekly learning plan for an 11 week study period,\nweekly worksheets with with examples (theory and application in R) and solutions in R Markdown, and\nexam papers of previous years.\n\nFurthermore, there is also an MATH38161 online reading list hosted by the University of Manchester library.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#prerequisites",
    "href": "00-preface.html#prerequisites",
    "title": "Preface",
    "section": "Prerequisites",
    "text": "Prerequisites\nMultivariate statistics relies heavily on matrix algebra and vector and matrix calculus. For a refresher of the essentials please refer to the supplementary\n\nMatrix and Calculus Refresher notes.\n\nFurthermore, this module builds on earlier statistics modules, especially on likelihood estimation and Bayesian statistics as discussed, e.g., in the module\n\nMATH27720 Statistics 2.\n\nFor an overview of essential probability distributions see the\n\nProbability and Distributions Refresher notes.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#acknowledgements",
    "href": "00-preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany thanks to Beatriz Costa Gomes for her help to compile the first draft of these course notes in the winter term 2018 while she was a graduate teaching assistant for this course. I also thank the many students who suggested corrections.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-multivariate.html",
    "href": "01-multivariate.html",
    "title": "1  Multivariate random variables",
    "section": "",
    "text": "1.1 Essentials in multivariate statistics",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Multivariate random variables</span>"
    ]
  },
  {
    "objectID": "01-multivariate.html#essentials-in-multivariate-statistics",
    "href": "01-multivariate.html#essentials-in-multivariate-statistics",
    "title": "1  Multivariate random variables",
    "section": "",
    "text": "Why multivariate statistics?\nIn science we use experiments to learn about underlying mechanisms of interest, both deterministic and stochastic, to compare different models and to verify or reject hypotheses about the world. Statistics provides tools to quantify this procedure and offers methods to link data (experiments) with probabilistic models (hypotheses).\nIn univariate statistics with we use relatively simple approaches based on a single random variable or single parameter. However, in practise we often have to consider multiple random variables and multiple parameters, so we need more complex models and also be able to deal with more complex data. Hence, the need for multivariate statistical approaches and models.\nSpecifically, multivariate statistics is concerned with methods and models for random vectors and random matrices, rather than just random univariate (scalar) variables. Therefore, in multivariate statistics we will frequently make use of matrix notation.\nClosely related to multivariate statistics (traditionally a subfield of statistics) is machine learning (ML) which is traditionally a subfield of computer science. ML used to focus more on algorithms rather on probabilistic modelling but nowadays most machine learning methods are fully based on statistical multivariate approaches, so the two fields are converging.\nMultivariate models provide a means to learn dependencies and interactions among the components of the random variables which in turn allow us to draw conclusion about underlying mechanisms of interest (e.g. in biological or medical problems).\nTwo main tasks:\n\nunsupervised learning (finding structure, clustering)\nsupervised learning (training from labelled data, followed by prediction)\n\nChallenges:\n\ncomplexity of model needs to be appropriate for problem and available data\nhigh dimensions make estimation and inference difficult\ncomputational issues\n\n\n\nUnivariate vs. multivariate random variables\nUnivariate random variable (dimension \\(d=1\\)): \\[x \\sim F\\] where \\(x\\) is a scalar and \\(F\\) is the distribution. \\(\\text{E}(x) = \\mu\\) denotes the mean and \\(\\text{Var}(x) = \\sigma^2\\) the variance of \\(x\\).\nMultivariate random vector of dimension \\(d\\): \\[\\boldsymbol x= (x_1, x_2,...,x_d)^T  \\sim F\\]\n\\(\\boldsymbol x\\) is vector valued random variable.\nThe vector \\(\\boldsymbol x\\) is column vector (=matrix of size \\(d \\times 1\\)). Its components \\(x_1, x_2,...,x_d\\) are univariate random variables. The dimension \\(d\\) is also often denoted by \\(p\\) or \\(q\\).\nNote that for simplicity of notation we use the same symbol to denote the random variable and its elementary outcomes (in particular we don’t use capitalisation to indicate a random variable). This convention greatly facilitates working with random vectors and matrices and follows, e.g., the classic multivariate statistics textbook by Mardia, Kent, and Bibby (1979). If a quantity is random we will always specify this explicitly in the context.\n\n\nMultivariate data\nVector notation:\nSamples from a multivariate distribution are vectors (not scalars as for univariate normal): \\[\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_n \\stackrel{\\text{iid}}\\sim F\\]\nMatrix and component notation:\nAll the data points are commonly collected into a matrix \\(\\boldsymbol X\\).\nIn statistics the convention is to store each data vector in the rows of the data matrix \\(\\boldsymbol X\\):\n\\[\\boldsymbol X= (\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_n)^T = \\begin{pmatrix}\n    x_{11}  & x_{12} & \\dots & x_{1d}   \\\\\n    x_{21}  & x_{22} & \\dots & x_{2d}   \\\\\n    \\vdots \\\\\n    x_{n1}  & x_{n2} & \\dots & x_{nd}\n\\end{pmatrix}\\]\nTherefore, \\[\\boldsymbol x_1=\\begin{pmatrix}\n    x_{11}       \\\\\n    \\vdots \\\\\n    x_{1d}\n\\end{pmatrix} , \\space \\boldsymbol x_2=\\begin{pmatrix}\n    x_{21}       \\\\\n    \\vdots \\\\\n    x_{2d}\n\\end{pmatrix} , \\ldots , \\boldsymbol x_n=\\begin{pmatrix}\n    x_{n1}       \\\\\n    \\vdots \\\\\n    x_{nd}\n\\end{pmatrix}\\]\nThus, in statistics the first index runs over \\((1,...,n)\\) and denotes the samples while the second index runs over \\((1,...,d)\\) and refers to the variables.\nThe statistics convention on data matrices is not universal! In fact, in most of the machine learning literature in engineering and computer science the data samples are stored in the columns so that the variables appear in the rows (thus in the engineering convention the data matrix is transposed compared to the statistics convention).\nIn order to avoid confusion and ambiguity it is recommended to prefer vector notation to describe data over matrix or component notation (see also the section below on estimating covariance matrices for examples).\n\n\nMean of a random vector\nThe mean / expectation of a random vector with dimensions \\(d\\) is also a vector with dimensions \\(d\\): \\[\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu= \\begin{pmatrix}\n    \\text{E}(x_1)       \\\\\n    \\text{E}(x_2)       \\\\\n    \\vdots \\\\\n    \\text{E}(x_d)\n\\end{pmatrix} = \\left( \\begin{array}{l}\n    \\mu_1       \\\\\n    \\mu_2       \\\\\n    \\vdots \\\\\n    \\mu_d\n\\end{array}\\right)\\]\n\n\nVariance of a random vector\nRecall the definition of mean and variance for a univariate random variable:\n\\[\\text{E}(x) = \\mu\\]\n\\[\\text{Var}(x) = \\sigma^2 = \\text{E}( (x-\\mu)^2 )=\\text{E}( (x-\\mu)(x-\\mu) ) = \\text{E}(x^2)-\\mu^2\\]\nDefinition of variance of a random vector:\n\\[\\text{Var}(\\boldsymbol x) = \\underbrace{\\boldsymbol \\Sigma}_{d\\times d} =\n\\text{E}\\left(\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d\\times 1} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1\\times d} \\right)\n= \\text{E}(\\boldsymbol x\\boldsymbol x^T)-\\boldsymbol \\mu\\boldsymbol \\mu^T\\]\nThe variance of a random vector is, therefore, not a vector but a matrix!\n\\[\\boldsymbol \\Sigma= (\\sigma_{ij}) = \\underbrace{\\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix}}_{d\\times d}\\]\n\n\nProperties of the covariance matrix\n\n\\(\\boldsymbol \\Sigma\\) is real valued: \\(\\sigma_{ij} \\in \\mathbb{R}\\)\n\\(\\boldsymbol \\Sigma\\) is symmetric: \\(\\sigma_{ij} = \\sigma_{ji}\\)\nThe diagonal of \\(\\boldsymbol \\Sigma\\) contains \\(\\sigma_{ii} = \\text{Var}(x_i) = \\sigma_i^2\\), i.e. the variances of the components of \\(\\boldsymbol x\\).\nOff-diagonal elements \\(\\sigma_{ij} = \\text{Cov}(x_i,x_j)\\) represent linear dependencies among the \\(x_i\\). \\(\\Longrightarrow\\) linear regression, correlation\n\n\n\n\nTable 1.1: Number of distinct elements in a covariance matrix.\n\n\n\n\n\n\\(d\\)\n# entries\n\n\n\n\n1\n1\n\n\n10\n55\n\n\n100\n5050\n\n\n1000\n500500\n\n\n10000\n50005000\n\n\n\n\n\n\nHow many distinct elements does \\(\\boldsymbol \\Sigma\\) have? \\[\n\\frac{d(d+1)}{2}\n\\] This grows with the square of the dimension \\(d\\), i.e. it grows with order \\(O(d^2)\\) (Table 1.1).\nFor large dimension \\(d\\) the covariance matrix has many components!\n–&gt; computationally expensive (both for storage and in handling) –&gt; very challenging to estimate \\(\\boldsymbol \\Sigma\\) in high dimensions \\(d\\).\nNote: matrix inversion requires \\(O(d^3)\\) operations using standard algorithms such as Gauss Jordan elimination. 1 Hence, computing \\(\\boldsymbol \\Sigma^{-1}\\) is computationally expensive for large \\(d\\)!\n\n\nEigenvalue decomposition of \\(\\boldsymbol \\Sigma\\)\nRecall from linear matrix algebra that any real symmetric matrix has real eigenvalues and a complete set of orthogonal eigenvectors. These can be obtained by orthogonal eigendecomposition. 2\nApplying eigenvalue decomposition to the covariance matrix yields \\[\n\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\n\\] where \\(\\boldsymbol U\\) is an orthogonal matrix 3 containing the eigenvectors of the covariance matrix and \\[\\boldsymbol \\Lambda= \\begin{pmatrix}\n    \\lambda_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}\n\\end{pmatrix}\\] contains the corresponding eigenvalues \\(\\lambda_i\\).\nImportantly, the eigenvalues of a covariance matrix are not only real-valued but are by construction further constrained to be non-negative. This can be seen by computing the quadratic form \\(\\boldsymbol z^T  \\boldsymbol \\Sigma\\boldsymbol z\\) where \\(\\boldsymbol z\\) is a non-random vector. For any non-zero \\(\\boldsymbol z\\) \\[\n\\begin{split}\n\\boldsymbol z^T  \\boldsymbol \\Sigma\\boldsymbol z& = \\boldsymbol z^T \\text{E}\\left(  (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T  \\right) \\boldsymbol z\\\\\n& =  \\text{E}\\left( \\boldsymbol z^T (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol z\\right) \\\\\n& =  \\text{E}\\left( \\left( \\boldsymbol z^T (\\boldsymbol x-\\boldsymbol \\mu) \\right)^2 \\right) \\geq 0 \\, .\\\\\n\\end{split}\n\\] Furthermore, with \\(\\boldsymbol y= \\boldsymbol U^T \\boldsymbol z\\) we get \\[\n\\begin{split}\n\\boldsymbol z^T  \\boldsymbol \\Sigma\\boldsymbol z& =  \\boldsymbol z^T\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T \\boldsymbol z\\\\\n                      & =  \\boldsymbol y^T \\boldsymbol \\Lambda\\boldsymbol y= \\sum_{i=1}^d  y_i^2 \\lambda_i \\\\\n\\end{split}\n\\] and hence all the \\(\\lambda_i \\geq 0\\). Therefore the covariance matrix \\(\\boldsymbol \\Sigma\\) is always positive semi-definite.\nIn fact, unless there is collinearity ( i.e. a variable is a linear function the other variables) all eigenvalues will be positive and \\(\\boldsymbol \\Sigma\\) is positive definite.\n\n\nJoint covariance matrix\nAssume we have random vector \\(\\boldsymbol z\\) with mean \\(\\text{E}(\\boldsymbol z) = \\boldsymbol \\mu_{\\boldsymbol z}\\) and covariance matrix \\(\\text{Var}(\\boldsymbol z) = \\boldsymbol \\Sigma_{\\boldsymbol z}\\).\nOften it makes sense to partion the components of \\(\\boldsymbol z\\) into two groups \\[\n\\boldsymbol z= \\begin{pmatrix} \\boldsymbol x\\\\ \\boldsymbol y\\end{pmatrix}\n\\] This induces a corresponding partition in the expectation \\[\n\\boldsymbol \\mu_{\\boldsymbol z} =  \\begin{pmatrix} \\boldsymbol \\mu_{\\boldsymbol x} \\\\ \\boldsymbol \\mu_{\\boldsymbol y} \\end{pmatrix}\n\\] where \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu_{\\boldsymbol x}\\) and \\(\\text{E}(\\boldsymbol y) = \\boldsymbol \\mu_{\\boldsymbol y}\\).\nFurthermore, the joint covariance matrix for \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) can then be written as \\[\n\\boldsymbol \\Sigma_{\\boldsymbol z} =\n\\begin{pmatrix}\n\\boldsymbol \\Sigma_{\\boldsymbol x} &  \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} & \\boldsymbol \\Sigma_{\\boldsymbol y} \\\\\n\\end{pmatrix}\n\\] It contains the within-group group covariance matrices \\(\\boldsymbol \\Sigma_{\\boldsymbol x}\\) and \\(\\boldsymbol \\Sigma_{\\boldsymbol y}\\) as diagonal elements and the cross-covariance matrix \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} = \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x}^T\\) as off-diagonal element.\nNote that the cross-covariance matrix \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}\\) is rectangular and not symmetric. We also write \\(\\text{Cov}(\\boldsymbol x, \\boldsymbol y) = \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}\\) and we can define cross-covariance directly by \\[\n\\text{Cov}(\\boldsymbol x, \\boldsymbol y) = \\text{E}\\left( (\\boldsymbol x- \\boldsymbol \\mu_{\\boldsymbol x}) ( \\boldsymbol y- \\boldsymbol \\mu_{\\boldsymbol y} )^T \\right) = \\text{E}(\\boldsymbol x\\boldsymbol y^T)-\\boldsymbol \\mu_{\\boldsymbol x} \\boldsymbol \\mu_{\\boldsymbol y}^T\n\\]\n\n\nQuantities related to the covariance matrix\n\nCorrelation matrix \\(\\boldsymbol P\\)\nThe correlation matrix \\(\\boldsymbol P\\) (= upper case greek “rho”) is the standardised covariance matrix\n\\[\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}=\\text{Cor}(x_i,x_j)\\]\n\\[\\rho_{ii} = 1 = \\text{Cor}(x_i,x_i)\\]\n\\[ \\boldsymbol P= (\\rho_{ij}) = \\begin{pmatrix}\n    1 & \\dots & \\rho_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{d1} & \\dots & 1\n\\end{pmatrix}\\]\nwhere \\(\\boldsymbol P\\) (“upper case rho”) is a symmetric matrix (\\(\\rho_{ij}=\\rho_{ji}\\)).\nNote the variance-correlation decomposition\n\\[\\boldsymbol \\Sigma= \\boldsymbol V^{\\frac{1}{2}} \\boldsymbol P\\boldsymbol V^{\\frac{1}{2}}\\]\nwhere \\(\\boldsymbol V\\) is a diagonal matrix containing the variances:\n\\[ \\boldsymbol V= \\begin{pmatrix}\n    \\sigma_{11} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma_{dd}\n\\end{pmatrix}\\]\n\\[\\boldsymbol P= \\boldsymbol V^{-\\frac{1}{2}}\\boldsymbol \\Sigma\\boldsymbol V^{-\\frac{1}{2}}\\]\nThis is the definition of correlation written in matrix notation.\nAs with the covariance matrix, in many applications it makes sense to partition a joint correlation matrix \\[\n\\boldsymbol P_{\\boldsymbol z} =\n\\begin{pmatrix}\n\\boldsymbol P_{\\boldsymbol x} &  \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol P_{\\boldsymbol y\\boldsymbol x} & \\boldsymbol P_{\\boldsymbol y} \\\\\n\\end{pmatrix}\n\\] into within-group group correlation matrices \\[\n\\boldsymbol P_{\\boldsymbol x} = \\boldsymbol V_{\\boldsymbol x}^{-\\frac{1}{2}} \\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol V_{\\boldsymbol x}^{-\\frac{1}{2}}\n\\] and \\[\n\\boldsymbol P_{\\boldsymbol y} = \\boldsymbol V_{\\boldsymbol y}^{-\\frac{1}{2}} \\boldsymbol \\Sigma_{\\boldsymbol y} \\boldsymbol V_{\\boldsymbol y}^{-\\frac{1}{2}}\n\\] and the cross-correlation matrix \\[\n\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = \\boldsymbol V_{\\boldsymbol x}^{-\\frac{1}{2}} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\boldsymbol V_{\\boldsymbol y}^{-\\frac{1}{2}}\n\\] with \\[\n\\boldsymbol P_{\\boldsymbol y\\boldsymbol x} = \\boldsymbol P_{\\boldsymbol x\\boldsymbol y}^T\n= \\boldsymbol V_{\\boldsymbol y}^{-\\frac{1}{2}} \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} \\boldsymbol V_{\\boldsymbol x}^{-\\frac{1}{2}} \\,.\n\\]\n\n\nPrecision matrix or concentration matrix\n\\[\\boldsymbol \\Omega= (\\omega_{ij}) = \\boldsymbol \\Sigma^{-1}\\]\n\\(\\boldsymbol \\Omega\\) (“Omega”) is the inverse of the covariance matrix.\nThe inverse of the covariance matrix can be obtained via the spectral decomposition, followed by inverting the eigenvalues \\(\\lambda_i\\): \\[\\boldsymbol \\Sigma^{-1} = \\boldsymbol U\\boldsymbol \\Lambda^{-1} \\boldsymbol U^T =\n\\boldsymbol U\\begin{pmatrix}\n    \\lambda_{1}^{-1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}^{-1}\n\\end{pmatrix} \\boldsymbol U^T \\]\nNote that all eigenvalues \\(\\lambda_i\\) need to be positive so that \\(\\boldsymbol \\Sigma\\) can be inverted. (i.e., \\(\\boldsymbol \\Sigma\\) needs to be positive definite).\nIf any \\(\\lambda_i = 0\\) then \\(\\boldsymbol \\Sigma\\) is singular and not invertible.\nImportance of \\(\\boldsymbol \\Sigma^{-1}\\):\n\nMany expressions in multivariate statistics contain \\(\\boldsymbol \\Sigma^{-1}\\) and not \\(\\boldsymbol \\Sigma\\).\n\\(\\boldsymbol \\Sigma^{-1}\\) has close connection with graphical models (e.g. conditional independence graph, partial correlations).\n\\(\\boldsymbol \\Sigma^{-1}\\) is a natural parameter from an exponential family perspective.\n\n\n\nPartial correlation matrix\nThis is a standardised version of the precision matrix, see later chapter on graphical models.\n\n\nTotal variation and generalised variance\nTo summarise the covariance matrix \\(\\boldsymbol \\Sigma\\) in a single scalar value there are two commonly used measures:\n\ntotal variation: \\(\\text{Tr}(\\boldsymbol \\Sigma) = \\sum_{i=1}^d \\lambda_i\\)\ngeneralised variance: \\(\\det(\\boldsymbol \\Sigma) = \\prod_{i=1}^d \\lambda_i\\)\n\nThe generalised variance \\(\\det(\\boldsymbol \\Sigma)\\) is also known as the volume of \\(\\boldsymbol \\Sigma\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Multivariate random variables</span>"
    ]
  },
  {
    "objectID": "01-multivariate.html#multivariate-distributions",
    "href": "01-multivariate.html#multivariate-distributions",
    "title": "1  Multivariate random variables",
    "section": "1.2 Multivariate distributions",
    "text": "1.2 Multivariate distributions\n\nCommon distributions\nIn multivariate statistics we make use of multivariate distributions. These are typically generalisations of corresponding univariate distribution.\nAmong the most commonly used multivariate distributions are:\n\nThe multivariate normal distribution \\(N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) as a generalisation of univariate normal distribution \\(N(\\mu, \\sigma^2)\\)\nThe categorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\) as a generalisation of the Bernoulli distribution \\(\\text{Ber}(\\theta)\\)\nThe multinomial distribution \\(\\text{Mult}(n, \\boldsymbol \\pi)\\) as a generalisation of binomial distribution \\(\\text{Bin}(n, \\theta)\\)\n\nThe above distribution have already been introduced earlier in MATH27720 Statistics 2.\nConceptually, these multivariate generalisation work behave exactly the same as their univariate counterparts and are employed in the same settings.\n\n\nFurther multivariate distributions\nFor multivariate Bayesian analyis we also need to consider a number of further multivariate distributions:\n\nThe Dirichlet distribution \\(\\text{Dir}(\\boldsymbol \\alpha)\\) as the generalisation of the beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\),\nThe Wishart distribution as the generalisation of the gamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\),\nThe inverse Wishart distribution as the generalisation of the inverse gamma distribution \\(\\text{IG}(\\alpha, \\beta)\\).\n\nFor technical details of the densities etc. of the multivariate distribution families we refer to the supplementary Probability and Distribution refresher notes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Multivariate random variables</span>"
    ]
  },
  {
    "objectID": "01-multivariate.html#multivariate-normal-distribution",
    "href": "01-multivariate.html#multivariate-normal-distribution",
    "title": "1  Multivariate random variables",
    "section": "1.3 Multivariate normal distribution",
    "text": "1.3 Multivariate normal distribution\nThe multivariate normal disribution is ubiquitous in multivariate statistics and hence it is important to discuss it in more detail.\nThe multivariate normal model is a generalisation of the univariate normal distribution from dimension 1 to dimension \\(d\\).\n\nUnivariate normal distribution:\n\\[\\text{Dimension } d = 1\\] \\[x \\sim N(\\mu, \\sigma^2)\\] \\[\\text{E}(x) = \\mu \\space , \\space  \\text{Var}(x) = \\sigma^2\\]\nProbability Density Function:\n\\[f(x |\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right) \\]\nPlot of univariate normal density \n\n\n\n\n\n\n\n\nFigure 1.1: Illustration of the density of the normal distribution.\n\n\n\n\n\nSee Figure 1.1. The density is unimodal with a mode at \\(\\mu\\) and width determined by \\(\\sigma\\) (in this plot: \\(\\mu=2, \\sigma^2=1\\) )\nSpecial case: standard normal with \\(\\mu=0\\) and \\(\\sigma^2=1\\):\n\\[f(x |\\mu=0,\\sigma^2=1)=\\frac{1}{\\sqrt{2\\pi}} \\exp\\left( {-\\frac{x^2}{2}} \\right) \\]\nDifferential entropy:\n\\[\nH(F) = \\frac{1}{2} (\\log(2 \\pi \\sigma^2) + 1)\n\\]\nCross-entropy:\n\\[\nH(F_{\\text{ref}}, F) = \\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 }\n+\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2}  +\\log(2 \\pi \\sigma^2) \\right)\n\\] KL divergence:\n\\[\nD_{\\text{KL}}(F_{\\text{ref}}, F) = H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) =\n\\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 }\n+\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2}  -\\log\\left(\\frac{\\sigma^2_{\\text{ref}}}{ \\sigma^2}\\right) -1\n\\right)\n\\]\nMaximum entropy characterisation: the normal distribution is the unique distribution that has the highest (differential) entropy over all continuous distributions with support from minus infinity to plus infinity with a given mean and variance.\nThis is in fact one of the reasons why the normal distribution is so important (und useful) – if we only know that a random variable has a mean and variance, and not much else, then using the normal distribution will be a reasonable and well justified model.\n\n\nMultivariate normal model\n\\[\\text{Dimension } d\\] \\[\\boldsymbol x\\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\] \\[\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\space , \\space  \\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\]\nDensity:\n\\[f(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) = \\det(2 \\pi \\boldsymbol \\Sigma)^{-\\frac{1}{2}} \\exp\\left({{-\\frac{1}{2}} \\underbrace{\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1 \\times d} \\underbrace{\\boldsymbol \\Sigma^{-1}}_{d \\times d} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d \\times 1} }_{1 \\times 1 = \\text{scalar!}}}\\right)\\]\n\nthe density contains the precision matrix \\(\\boldsymbol \\Sigma^{-1}\\)\nto invert the covariance matrix \\(\\boldsymbol \\Sigma\\) we need to invert its eigenvalues \\(\\lambda_i\\) (hence we require that all \\(\\lambda_i &gt; 0\\))\nthe density also contains \\(\\det(\\boldsymbol \\Sigma) = \\prod\\limits_{i=1}^d \\lambda_i\\) \\(\\equiv\\) product of the eigenvalues of \\(\\boldsymbol \\Sigma\\)\nnote that \\(\\det(2 \\pi \\boldsymbol \\Sigma)^{-\\frac{1}{2}} = \\det(2 \\pi \\boldsymbol I_d)^{-\\frac{1}{2}} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}} =  (2 \\pi)^{-d/2} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}}\\)\n\nSpecial case: standard multivariate normal with \\[\\boldsymbol \\mu=\\mathbf 0, \\boldsymbol \\Sigma=\\boldsymbol I=\\begin{pmatrix}\n    1 & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & 1\n\\end{pmatrix}\\]\n\\[f(\\boldsymbol x| \\boldsymbol \\mu=\\mathbf 0,\\boldsymbol \\Sigma=\\boldsymbol I)=(2\\pi)^{-d/2}\\exp\\left( -\\frac{1}{2} \\boldsymbol x^T \\boldsymbol x\\right) = \\prod\\limits_{i=1}^d \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_i^2}{2}\\right)\\] which is equivalent to the product of \\(d\\) univariate standard normals!\nMisc:\n\nfor \\(d=1\\), the multivariate normal density reduces to the univariate normal density.\nfor \\(\\boldsymbol \\Sigma\\) diagonal (i.e. \\(\\boldsymbol P= \\boldsymbol I\\), no correlation), the multivariate normal density is the product of univariate normal densities (see Worksheet 2).\n\nPlot of the multivariate normal density:\n\n\n\n\n\n\n\n\nFigure 1.2: Illustration of the density of the bivariate normal distribution.\n\n\n\n\n\nFigure 1.2 illustrates the bivariate normal distribution, with location determined by \\(\\boldsymbol \\mu\\), shape determined by \\(\\boldsymbol \\Sigma\\) and a single mode. The support ranges from \\(-\\infty\\) to \\(+\\infty\\) in each dimension.\nAn interactive R Shiny web app of the bivariate normal density plot is available online at https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/.\nDifferential entropy:\n\\[\nH = \\frac{1}{2} (\\log \\det(2 \\pi \\boldsymbol \\Sigma) + d)\n\\]\nCross-entropy:\n\\[\nH(F_{\\text{ref}}, F) = \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    + \\log \\det \\biggl( 2 \\pi \\boldsymbol \\Sigma\\biggr)    \\biggr\\}\n\\] KL divergence:\n\\[\n\\begin{split}\nD_{\\text{KL}}(F_{\\text{ref}}, F) &= H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) \\\\\n&= \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n     - d   \\biggr\\} \\\\\n\\end{split}\n\\]\n\n\nShape of the multivariate normal density\nNow we show that the contour lines of the multivariate normal density always take on the form of an ellipse, and that the radii and orientation of the ellipse is determined by the eigenvalues of \\(\\boldsymbol \\Sigma\\).\nWe start by observing that a circle with radius \\(r\\) around the origin can be described as the set of points \\((x_1,x_2)\\) satisfying \\(x_1^2+x_2^2 = r^2\\), or equivalently, \\(\\frac{x_1^2}{r^2} + \\frac{x_2^2}{r^2} = 1\\). This is generalised to the shape of an ellipse by allowing (in two dimensions) for two radii \\(r_1\\) and \\(r_2\\) with \\(\\frac{x_1^2}{r_1^2} + \\frac{x_2^2}{r_2^2} = 1\\), or in vector notation \\(\\boldsymbol x^T \\text{Diag}(r_1^2, r_2^2)^{-1} \\boldsymbol x= 1\\). Here two axes of the ellipse are parallel to the two coordinate axes.\nIn \\(d\\) dimensions and allowing for rotation of the axes and a shift of the origin from 0 to \\(\\boldsymbol \\mu\\) the condition for an ellipse is \\[(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol Q\\, \\text{Diag}(r_1^2, \\ldots , r_d^2)^{-1} \\boldsymbol Q^T (\\boldsymbol x-\\boldsymbol \\mu) = 1\\] where \\(\\boldsymbol Q\\) is an orthogonal matrix whose column vectors indicate the direction of the axes. These are also called the principal axes of the ellipse, and by construction all \\(d\\) principal axes are perpendicular to each other.\nA contour line of a probability density function is a set of connected points where the density assumes the same constant value. In the case of the multivariate normal distribution keeping the density \\(f(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) at some fixed value implies that \\((\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu) = c\\) where \\(c\\) is a constant. Using the eigenvalue decomposition of \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) we can rewrite this condition as \\[\n(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol U\\boldsymbol \\Lambda^{-1} \\boldsymbol U^T (\\boldsymbol x-\\boldsymbol \\mu) = c \\,.\n\\] This implies that\n\nthe contour lines of the multivariate normal density are indeed ellipses,\nthe direction of the principal axes of the ellipse are given correspond to the colum vectors in \\(\\boldsymbol U\\) (i.e. the eigenvectors of \\(\\boldsymbol \\Sigma\\)), and\nthe squared radii of the ellipse are proportional to the eigenvalues of \\(\\boldsymbol \\Sigma\\) Equivalently, the positive square roots of the eigenvalues are proportional to the radii of the ellipse. Hence, for a singular covariance matrix with one or more \\(\\lambda_i=0\\) the corresponding radii are zero.\n\nAn interactive R Shiny web app to play with the contour lines of the bivariate normal distribution is available online at https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/.\n\n\nThree types of covariances\nFollowing the above we can parameterise a covariance matrix in terms of its i) volume, ii) shape and iii) orientation by writing \\[\n\\boldsymbol \\Sigma= \\kappa \\, \\boldsymbol U\\boldsymbol A\\boldsymbol U^T = \\boldsymbol U\\; \\left(\\kappa \\boldsymbol A\\right) \\; \\boldsymbol U^T\n\\] with \\(\\boldsymbol A=\\text{Diag}(a_1, \\ldots, a_d)\\) and \\(\\det(\\boldsymbol A) = \\prod_{i=1}^d a_i = 1\\). Note that in this parameterisation the eigenvalues of \\(\\boldsymbol \\Sigma\\) are \\(\\lambda_i = \\kappa a_i\\).\n\nThe volume is \\(\\det(\\boldsymbol \\Sigma) = \\kappa^d\\), determined by a single parameter \\(\\kappa\\). This parameter can be interpreted as the length of the side of a \\(d\\)-dimensional hypercube.\nThe shape is determined by the diagonal matrix \\(\\boldsymbol A\\) with \\(d-1\\) free parameters. Note that there are only \\(d-1\\) and not \\(d\\) free parameters because of the constraint \\(\\det(\\boldsymbol A) = 1\\).\nThe orientation is given by the orthogonal matrix \\(\\boldsymbol U\\), with \\(d (d-1)/2\\) free parameters.\n\nThis leads to classification of covariances into three varieties:\nType 1: spherical covariance \\(\\boldsymbol \\Sigma=\\kappa \\boldsymbol I\\), with spherical contour lines, 1 free parameter (\\(\\boldsymbol A=\\boldsymbol I\\), \\(\\boldsymbol U=\\boldsymbol I\\)).\nExample (Figure 1.3): \\(\\boldsymbol \\Sigma= \\begin{pmatrix}\n2 & 0 \\\\\n0 & 2\n\\end{pmatrix}\\) with \\(\\sqrt{\\lambda_1/ \\lambda_2} = 1\\):\n\n\n\n\n\n\n\n\nFigure 1.3: Contor lines for a spherical covariance matrix.\n\n\n\n\n\nType 2: diagonal covariance \\(\\boldsymbol \\Sigma= \\kappa \\boldsymbol A\\), with elliptical contour lines and the principal axes of the ellipse oriented parallel to the coordinate axes, \\(d\\) free parameters (\\(\\boldsymbol U=\\boldsymbol I\\)).\nExample (Figure 1.4): \\(\\boldsymbol \\Sigma= \\begin{pmatrix}\n1 & 0 \\\\\n0 & 2\n\\end{pmatrix}\\) with \\(\\sqrt{\\lambda_1 / \\lambda_2} \\approx 1.41\\):\n\n\n\n\n\n\n\n\nFigure 1.4: Contour lines for a diagonal covariance matrix.\n\n\n\n\n\nType 3: general unrestricted covariance \\(\\boldsymbol \\Sigma\\), with elliptical contour lines, with the principal axes of the ellipse oriented according to the column vectors in \\(\\boldsymbol U\\), \\(d (d+1)/2\\) free parameters.\nExample (Figure 1.5): \\(\\boldsymbol \\Sigma= \\begin{pmatrix}\n2 & 0.6 \\\\\n0.6 & 1\n\\end{pmatrix}\\) with \\(\\sqrt{\\lambda_1 / \\lambda_2} \\approx 2.20\\):\n\n\n\n\n\n\n\n\nFigure 1.5: Contour lines for a general covariance matrix.\n\n\n\n\n\n\n\nConcentration of probability mass for small and large dimension\nThe density of the multivariate normal distribution has a bell shape with a single mode. Intuitively, we may assume that most of the probability mass is always concentrated around this mode, as it is in the univariate case (\\(d=1\\)). While this is still true for small dimensions (small \\(d\\)) we now show that this intuition is incorrect for high dimensions (large \\(d\\)).\nFor simplicity we consider the standard multivariate normal distribution with dimension \\(d\\) \\[\\boldsymbol x\\sim N_d(\\mathbf 0, \\boldsymbol I_d)\\] with a spherical covariance \\(\\boldsymbol I_d\\) and sample \\(\\boldsymbol x\\). The squared Euclidean length of \\(\\boldsymbol x\\) is \\(r^2= || \\boldsymbol x||^2 = \\boldsymbol x^T \\boldsymbol x= \\sum_{i=1}^d x_i^2\\). The corresponding density of the \\(d\\)-dimensional standard multivariate normal distribution is \\[\ng_d(\\boldsymbol x) = (2\\pi)^{-d/2} e^{-\\boldsymbol x^T \\boldsymbol x/2}\n\\] A natural way to define the main part of the “bell” of the standard multivariate normal as the set of all \\(\\boldsymbol x\\) for which the density is larger than a specified fraction \\(\\eta\\) (say 0.001) of the maximum value of the density \\(g_d(0)\\) at the peak at zero. To formalise \\[\nB = \\left\\{ \\boldsymbol x: \\frac{g_d(\\boldsymbol x)}{ g_d(0)} &gt; \\eta    \\right\\}\n\\] which can be equivalently written as the set \\[\nB = \\{ \\boldsymbol x: \\boldsymbol x^T \\boldsymbol x= r^2 &lt; -2 \\log(\\eta) = r^2_{\\max} \\}\n\\]\nEach individual component in the sample \\(\\boldsymbol x\\) is independently distributed as \\(x_i \\sim N(0,1)\\), hence \\(r^2 \\sim \\text{$\\chi^2_{d}$}\\) is chi-squared distributed with degree of freedom \\(d\\). The probability \\(\\text{Pr}(\\boldsymbol x\\in B)\\) can thus be obtained as the value of the cumulative density function of a chi-squared distribution with \\(d\\) degrees of freedom at \\(r^2_{\\max}\\). Computing this probability for fixed \\(\\eta\\) as a function of the dimension \\(d\\) we obtain the curve shown in Figure 1.6. In this plot we have used \\(\\eta=0.001\\). You can see that for dimensions up to around \\(d=10\\) the probability mass is indeed concentrated in the center of the distribution but from \\(d=30\\) onwards it has moved completely to the tails.\n\n\n\n\n\n\n\n\nFigure 1.6: Concentration of probability mass in the center of a multivariate normal distribution.\n\n\n\n\n\n\n\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Academic Press.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Multivariate random variables</span>"
    ]
  },
  {
    "objectID": "01-multivariate.html#footnotes",
    "href": "01-multivariate.html#footnotes",
    "title": "1  Multivariate random variables",
    "section": "",
    "text": "Specialised matrix algorithms improve this to about \\(O(d^{2.373})\\). Matrices with special symmetries (e.g. diagonal and block diagonal matrices) or particular properties (e.g. orthogonal matrix) can also be inverted much easier.↩︎\nA brief summary of eigenvalue decompositon is found in the supplementary Matrix and Calculus Refresher notes.↩︎\nAn orthogonal matrix \\(\\boldsymbol Q\\) satisfies \\(\\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol I\\), \\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol I\\) and \\(\\boldsymbol Q^{-1} = \\boldsymbol Q^T\\) and is also called rotation-reflection matrix. We will make frequent use of orthogonal matrices so this might be a good time to revisit their properties, see e.g. the Matrix and Calculus Refresher notes..↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Multivariate random variables</span>"
    ]
  },
  {
    "objectID": "02-estimation.html",
    "href": "02-estimation.html",
    "title": "2  Multivariate estimation",
    "section": "",
    "text": "2.1 Overview\nIn practical application of multivariate normal model we need to learn its parameters from observed data points:\n\\[\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_n \\stackrel{\\text{iid}}\\sim F_{\\boldsymbol \\theta}\\]\nWe first consider the case when there are many measurements available (\\(n\\) large), and then subsequently the case when the number of data points \\(n\\) is small compared to the dimensions and the number of parameters.\nIn a previous course in year 2 (see MATH27720 Statistics 2) the method of maximum likelihood as well as the essentials of Bayesian statistics were introduced. Below we apply these approaches to the problem of estimating the parameters of the multivariate normal distribution and also show how the main Bayesian modelling strategies extend to the multivariate case.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate estimation</span>"
    ]
  },
  {
    "objectID": "02-estimation.html#empirical-estimates",
    "href": "02-estimation.html#empirical-estimates",
    "title": "2  Multivariate estimation",
    "section": "2.2 Empirical estimates",
    "text": "2.2 Empirical estimates\n\nGeneral principle\nFor large \\(n\\) we have thanks to the law of large numbers: \\[\\underbrace{F}_{\\text{true}} \\approx \\underbrace{\\widehat{F}_n}_{\\text{empirical}}\\]\nWe now would like to estimate \\(A\\) which is a functional \\(A=m(F)\\) of the distribution \\(F\\) — recall that a functional is a function that takes another function as argument. For example all standard distributional summaries such as the mean, the median etc. are derived from \\(F\\) and hence are functionals of \\(F\\).\nThe empirical estimate is obtained by replacing the unknown true distribution \\(F\\) with the observed empirical distribution: \\(\\hat{A} = m(\\widehat{F}_n)\\).\nFor example, the expectation of a random variable is approximated/estimated as the average over the observations: \\[\\text{E}_F(\\boldsymbol x) \\approx \\text{E}_{\\widehat{F}_n}(\\boldsymbol x) = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k\\] \\[\\text{E}_F(g(\\boldsymbol x)) \\approx  \\text{E}_{\\widehat{F}_n}(g(\\boldsymbol x)) = \\frac{1}{n}\\sum^{n}_{k=1} g(\\boldsymbol x_k)\\]\nSimple recipe to obtain an empirical estimator: simply replace the expectation operator by the sample average.\nWhat does this work: the empirical distribution \\(\\widehat{F}_n\\) is the nonparametric maximum likelihood estimate of \\(F\\) (see below for likelihood estimation).\nNote: the approximation of \\(F\\) by \\(\\widehat{F}_n\\) is also the basis other approaches such as Efron’s bootstrap method (1979) 1.\n\n\nEmpirical estimates of mean and covariance\nRecall the definitions: \\[\n\\boldsymbol \\mu= \\text{E}(\\boldsymbol x)\n\\] and \\[\n\\boldsymbol \\Sigma= \\text{E}\\left(   (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T \\right)\n\\]\nFor the empirical estimate we replace the expectations by the corresponding sample averages.\nThese resulting estimators can be written in three different ways:\nVector notation:\n\\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k\\]\n\\[\n\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\sum^{n}_{k=1} (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})  (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})^T\n= \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k  \\boldsymbol x_k^T - \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\n\\]\nComponent notation:\nThe corresponding component notation with \\(\\boldsymbol X= (x_{ki})\\) and following the statistics convention with samples contained in rows of \\(\\boldsymbol X\\) we get:\n\\[\\hat{\\mu}_i = \\frac{1}{n}\\sum^{n}_{k=1} x_{ki}\\]\n\\[\\hat{\\sigma}_{ij} = \\frac{1}{n}\\sum^{n}_{k=1} (x_{ki}-\\hat{\\mu}_i) (\nx_{kj}-\\hat{\\mu}_j )\\]\n\\[\\hat{\\boldsymbol \\mu}=\\begin{pmatrix}\n    \\hat{\\mu}_{1}       \\\\\n    \\vdots \\\\\n    \\hat{\\mu}_{d}\n\\end{pmatrix}, \\widehat{\\boldsymbol \\Sigma} = (\\hat{\\sigma}_{ij})\\]\nVariance estimate:\n\\[\\hat{\\sigma}_{ii} = \\frac{1}{n}\\sum^{n}_{k=1} \\left(x_{ki}-\\hat{\\mu}_i\\right)^2\\] Note the factor \\(\\frac{1}{n}\\) (not \\(\\frac{1}{n-1}\\))\nData matrix notation:\nThe empirical mean and covariance can also be written in terms of the data matrix \\(\\boldsymbol X\\).\nIf the data matrix \\(\\boldsymbol X\\) follows the statistics convention we can write\n\\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n} \\boldsymbol X^T \\mathbf 1_n\\]\n\\[\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\boldsymbol X^T \\boldsymbol X- \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\\]\nOn the other hand, if \\(\\boldsymbol X\\) follows the engineering convention with samples in columns the estimators are written as:\n\\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n} \\boldsymbol X\\mathbf 1_n\\]\n\\[\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\boldsymbol X\\boldsymbol X^T - \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\\]\nTo avoid confusion when using matrix or component notation you need to always state which convention is used! In these notes we exlusively follow the statistics convention.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate estimation</span>"
    ]
  },
  {
    "objectID": "02-estimation.html#maximum-likelihood-estimation",
    "href": "02-estimation.html#maximum-likelihood-estimation",
    "title": "2  Multivariate estimation",
    "section": "2.3 Maximum likelihood estimation",
    "text": "2.3 Maximum likelihood estimation\n\nGeneral principle\nR.A. Fisher (1922) 2: model-based estimators using the density or probability mass function\nLog-likelihood function:\nObserving data \\(D=\\{ \\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) the log-likelihood function is\n\\[\\log L(\\boldsymbol \\theta| D ) = \\sum^{n}_{k=1}  \\underbrace{\\log f}_{\\text{log-density}}(\\boldsymbol x_k |\\boldsymbol \\theta)\\]\nMaximum likelihood estimate: \\[\\hat{\\boldsymbol \\theta}_{\\text{ML}}=\\underset{\\boldsymbol \\theta}{\\arg\\,\\max} \\log L(\\boldsymbol \\theta| D)\\]\nMaximum likelihood (ML) finds the parameters that make the observed data most likely (it does not find the most probable model!)\nRecall from MATH27720 Statistics 2 that maximum likelihood is closely linked to minimising the Kullback-Leibler (KL) divergence \\(D_{\\text{KL}}(F, F_{\\boldsymbol \\theta})\\) between the unknown true model \\(F\\) and the specified model \\(F_{\\boldsymbol \\theta}\\). Specifically, for large sample size \\(n\\) the model \\(F_{\\hat{\\boldsymbol \\theta}}\\) fit by maximum likelihood is indeed the model that is closest to \\(F\\).\nCorrespondingly, the great appeal of maximum likelihood estimates (MLEs) is that they are optimal for large \\(\\mathbf{n}\\), i.e. so that for large sample size no estimator can be constructed that outperforms the MLE (note the emphasis on “for large \\(n\\)”!). A further advantage of the method of maximum likelihood is that it does not only provide a point estimate but also the asymptotic error (via the observed Fisher information which is related to the curvature of the log-likelihood function).\n\n\nMaximum likelihood estimates of the parameters of the multivariate normal distribution\nWe now derive the MLE of the parameters \\(\\boldsymbol \\mu\\) and \\(\\boldsymbol \\Sigma\\) of the multivariate normal distribution. The corresponding log-likelihood function is \\[\n\\begin{split}\n\\log L(\\boldsymbol \\mu, \\boldsymbol \\Sigma| D) & = \\sum_{k=1}^n \\log f( \\boldsymbol x_k | \\boldsymbol \\mu, \\boldsymbol \\Sigma) \\\\\n  & = -\\frac{n d}{2} \\log(2\\pi) -\\frac{n}{2} \\log \\det(\\boldsymbol \\Sigma)  \n   - \\frac{1}{2}  \\sum_{k=1}^n  (\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x_k-\\boldsymbol \\mu) \\,.\\\\\n\\end{split}\n\\] Written in terms of the precision matrix \\(\\boldsymbol \\Omega= \\boldsymbol \\Sigma^{-1}\\) this becomes \\[\n\\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega| D) = -\\frac{n d}{2} \\log(2\\pi) +\\frac{n}{2} \\log \\det(\\boldsymbol \\Omega)  - \\frac{1}{2}  \\sum_{k=1}^n  (\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Omega(\\boldsymbol x_k-\\boldsymbol \\mu) \\,.\n\\] First, to find the MLE for \\(\\boldsymbol \\mu\\) we compute the derivative with regard to the vector \\(\\boldsymbol \\mu\\) \\[ \\frac{\\partial \\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega| D) }{\\partial \\boldsymbol \\mu}= \\sum_{k=1}^n  (\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Omega\\] noting that \\(\\boldsymbol \\Omega\\) is symmetric. Setting this equal to zero we get \\(\\sum_{k=1}^n \\boldsymbol x_k = n \\hat{\\boldsymbol \\mu}_{ML}\\) and thus \\[\\hat{\\boldsymbol \\mu}_{ML} = \\frac{1}{n} \\sum_{k=1}^n \\boldsymbol x_k\\,.\\]\nNext, to obtain the MLE for \\(\\boldsymbol \\Omega\\) we compute the derivative with regard to the matrix \\(\\boldsymbol \\Omega\\) \\[\\frac{\\partial \\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega| D) }{\\partial \\boldsymbol \\Omega}=\\frac{n}{2}\\boldsymbol \\Omega^{-1} - \\frac{1}{2}  \\sum_{k=1}^n (\\boldsymbol x_k-\\boldsymbol \\mu) (\\boldsymbol x_k-\\boldsymbol \\mu)^T\\]. Setting this equal to zero and substituting the MLE for \\(\\boldsymbol \\mu\\) we get \\[\\widehat{\\boldsymbol \\Omega}^{-1}_{ML}=  \\frac{1}{n} \\sum_{k=1}^n  (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu}) (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})^T=\\widehat{\\boldsymbol \\Sigma}_{ML}\\,.\\]\nSee the supplementary Matrix Refresher notes for the relevant formulas in vector and matrix calculus.\nTherefore, the MLEs are identical to the empirical estimates.\nNote the factor \\(\\frac{1}{n}\\) in the MLE of the covariance matrix.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate estimation</span>"
    ]
  },
  {
    "objectID": "02-estimation.html#sampling-distribution-of-the-empirical-maximum-likelihood-estimates",
    "href": "02-estimation.html#sampling-distribution-of-the-empirical-maximum-likelihood-estimates",
    "title": "2  Multivariate estimation",
    "section": "2.4 Sampling distribution of the empirical / maximum likelihood estimates",
    "text": "2.4 Sampling distribution of the empirical / maximum likelihood estimates\nWith \\(\\boldsymbol x_1,...,\\boldsymbol x_n \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) one can find the exact distributions of the estimators. The sample average is denoted by \\(\\bar{\\boldsymbol x}= \\frac{1}{n}\\sum_{i=1}^n \\boldsymbol x_i\\)\n1. Distribution of the estimate of the mean:\nThe empirical estimate of the mean is normally distributed:\n\\[\\hat{\\boldsymbol \\mu}_{ML}=\\bar{\\boldsymbol x} \\sim N_d\\left(\\boldsymbol \\mu, \\frac{\\boldsymbol \\Sigma}{n}\\right)\\] Since \\(\\text{E}(\\hat{\\boldsymbol \\mu}_{ML}) = \\boldsymbol \\mu\\Longrightarrow \\hat{\\boldsymbol \\mu}_{ML}\\) is unbiased.\n2. Distribution of the covariance estimate:\nThe empirical and unbiased estimate of the covariance matrix is Wishart distributed:\nCase of unknown mean \\(\\boldsymbol \\mu\\) (estimated by \\(\\bar{\\boldsymbol x}\\)):\n\\[\\widehat{\\boldsymbol \\Sigma}_{ML} = \\frac{1}{n}\\sum_{i=1}^n (\\boldsymbol x_i -\\bar{\\boldsymbol x})(\\boldsymbol x_i -\\bar{\\boldsymbol x})^T \\sim \\text{Wis}_d\\left(\\frac{\\boldsymbol \\Sigma}{n}, n-1\\right)\\]\nSince \\(\\text{E}(\\widehat{\\boldsymbol \\Sigma}_{ML}) = \\frac{n-1}{n}\\boldsymbol \\Sigma\\) \\(\\Longrightarrow \\widehat{\\boldsymbol \\Sigma}_{ML}\\) is biased, with \\(\\text{Bias}(\\widehat{\\boldsymbol \\Sigma}_{ML} ) = \\boldsymbol \\Sigma- \\text{E}(\\widehat{\\boldsymbol \\Sigma}_{ML}) = -\\frac{\\boldsymbol \\Sigma}{n}\\).\nEasy to make unbiased: \\(\\widehat{\\boldsymbol \\Sigma}_{UB} = \\frac{n}{n-1}\\widehat{\\boldsymbol \\Sigma}_{ML}=\n\\frac{1}{n-1}\\sum^n_{k=1}\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)^T\\) which is distributed as \\[\\widehat{\\boldsymbol \\Sigma}_{UB}  \\sim \\text{Wis}_d\\left(\\frac{\\boldsymbol \\Sigma}{n-1}, n-1\\right)\\]\nHence \\(\\text{E}(\\widehat{\\boldsymbol \\Sigma}_{UB}) = \\boldsymbol \\Sigma\\) \\(\\Longrightarrow \\widehat{\\boldsymbol \\Sigma}_{UB}\\) is unbiased.\nBut unbiasedness of an estimator is not a very relevant criterion in multivariate statistics, especially when the number of samples is small compared to the dimension (see further below).\nCovariance estimator for known mean \\(\\boldsymbol \\mu\\):\n\\[\\frac{1}{n}\\sum_{i=1}^n (\\boldsymbol x_i -\\boldsymbol \\mu)(\\boldsymbol x_i -\\boldsymbol \\mu)^T \\sim \\text{Wis}_d\\left(\\frac{\\boldsymbol \\Sigma}{n}, n\\right)\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate estimation</span>"
    ]
  },
  {
    "objectID": "02-estimation.html#small-sample-estimation",
    "href": "02-estimation.html#small-sample-estimation",
    "title": "2  Multivariate estimation",
    "section": "2.5 Small sample estimation",
    "text": "2.5 Small sample estimation\n\nProblems with maximum likelihood in small sample settings and high dimensions\nModern data is high dimensional!\nData sets with \\(n&lt;d\\), i.e. high dimension \\(d\\) and small sample size \\(n\\) are now common in many fields, e.g., medicine, biology but also finance and business analytics.\n\\[n = 100 \\, \\text{(e.g, patients/samples)}\\] \\[d = 20000 \\, \\text{(e.g., genes/SNPs/proteins/variables)}\\] Reasons:\n\nthe number of measured variables is increasing quickly with technological advances (e.g. genomics)\nbut the number of samples cannot be similary increased (for cost and ethical reasons)\n\nGeneral problems of MLEs:\n\nML estimators are optimal only if sample size is large compared to the number of parameters. However, this optimality is not any more valid if sample size is moderate or smaller than the number of parameters.\nIf there is not enough data the ML estimate overfits. This means ML fits the current data perfectly but the resulting model does not generalise well (i.e. model will perform poorly in prediction)\nIf there is a choice between different models with different complexity ML will always select the model with the largest number of parameters.\n\n-&gt; for high-dimensional data with small sample size maximum likelihood estimation does not work!!!\nHistory of Statistics:\n\n\n\n\n\n\nFigure 2.1: Very brief sketch of the statistics in the 20th and 21st century.\n\n\n\nMuch of modern statistics (from 1960 onwards) is devoted to the development of inference and estimation techniques that work with complex, high-dimensional data (cf. Figure 2.1).\n\nMaximum likelihood is a method from classical statistics (time up to about 1960).\nFrom 1960 modern (computational) statistics emerges, starting with “Stein Paradox” (1956): Charles Stein showed that in a multivariate setting ML estimators are dominated by (= are always worse than) shrinkage estimators!\nFor example, there is a shrinkage estimator for the mean that is better (in terms of MSE) than the average (which is the MLE)!\n\nModern statistics has developed many different (but related) methods for use in high-dimensional, small sample settings:\n\nregularised estimators\nshrinkage estimators\npenalised maximum likelihood estimators\nBayesian estimators\nEmpirical Bayes estimators\nKL / entropy-based estimators\n\nMost of this is out of scope for our class, but will be covered in advanced statistical courses.\nNext, we describe a simple regularised estimator for the estimation of the covariance that we will use later (i.e. in classification).\n\n\nEstimation of covariance matrix in small sample settings\nProblems with ML estimate of \\(\\boldsymbol \\Sigma\\)\n\n\\(\\Sigma\\) has O(\\(d^2\\)) number of parameters! \\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}^{\\text{MLE}}\\) requires a lot of data! \\(n\\gg d \\text{ or } d^2\\)\nif \\(n &lt; d\\) then \\(\\hat{\\boldsymbol \\Sigma}\\) is positive semi-definite (even if the true \\(\\Sigma\\) is positive definite!)\n\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}\\) will have vanishing eigenvalues (some \\(\\lambda_i=0\\)) and thus cannot be inverted and is singular!\n\nNote that in many expression in multivariate statistics we actually need to use the inverse of the covariance matrix, e.g., in the density of the multivariate normal distribution, so it is essential that we obtain a non-singular invertible estimate of the covariance matrix.\nMaking the ML estimate of \\(\\boldsymbol \\Sigma\\) invertible\nThere is a simple numerical trick credited to A. N. Tikhonov to make \\(\\hat{\\boldsymbol \\Sigma}\\) invertible, by adding a small number (say \\(\\varepsilon=10^{-6}\\) to the diagonal elements of \\(\\hat{\\boldsymbol \\Sigma}\\): \\[\n\\boldsymbol S_{\\text{Tik}} = \\hat{\\boldsymbol \\Sigma} + \\varepsilon \\boldsymbol I\n\\]\nThe resulting \\(\\boldsymbol S_{\\text{Tik}}\\) is positive definite because the sum of a symmetric positive definite matrix (\\(\\varepsilon \\boldsymbol I\\)) and a symmetric positive semi-definite matrix (\\(\\hat{\\boldsymbol \\Sigma}\\)) is always positive definite.\nHowever, while this simple regularisation results in an invertible matrix the estimator itself has not improved over the MLE, and the matrix \\(\\boldsymbol S_{\\text{Tik}}\\) will also be poorly conditioned (i.e. large condition number).\nSimple regularised estimate of \\(\\boldsymbol \\Sigma\\)\nRegularised estimator \\(\\boldsymbol S^\\ast\\) = convex combination of \\(\\boldsymbol S= \\hat{\\boldsymbol \\Sigma}^\\text{MLE}\\) and \\(\\boldsymbol I_d\\) (identity matrix) to get\nRegularisation: \\[\n\\underbrace{\\boldsymbol S^\\ast}_{\\text{regularised estimate}} = (1-\\lambda)\\underbrace{\\boldsymbol S}_{\\text{ML estimate}} +\\underbrace{\\lambda}_{\\text{shrinkage intensity}} \\, \\underbrace{\\boldsymbol I_d}_{\\text{target}}\\]\nIdea: choose \\(\\lambda \\in [0,1]\\) such that \\(\\boldsymbol S^\\ast\\) is better (e.g. in terms of MSE) than both \\(\\boldsymbol S\\) and \\(\\boldsymbol I_d\\). Note that \\(\\lambda\\) does not need to be small like \\(\\varepsilon\\).\nThis form of estimator is corresponds to computing the mean of the Bayesian posterior by directly shrinking the MLE towards a prior mean (target): \\[\n\\underbrace{\\boldsymbol S^\\ast}_{\\text{posterior mean}} = \\underbrace{\\lambda \\boldsymbol I_d}_{\\text{prior information}}  + (1-\\lambda)\\underbrace{\\boldsymbol S}_{\\text{data summarised by maximum likelihood}}\n\\]\n\nPrior information helps to infer \\(\\boldsymbol \\Sigma\\) even in small samples.\nalso called shrinkage estimator since the off-diagonal entries are shrunk towards zero.\nthis type of linear shrinkage/regularisation is natural for exponential family models (Diaconis and Ylvisaker, 1979).\nInstead of a diagonal target other options are possible, e.g. block-diagonal or banded covariances.\nIf \\(\\lambda\\) is not prespecified but learned from data (see below) then the resulting estimate is an empirical Bayes estimator.\nThe resulting estimate will typically be biased as mixing in the target will increase the bias.\n\nHow to find optimal shrinkage / regularisation parameter \\(\\lambda\\)?\n\n\n\n\n\n\nFigure 2.2: Bias-variance tradeoff to find optimal shrinkage intensity.\n\n\n\nOne way to do this is to chose \\(\\lambda\\) to minimise \\(\\text{MSE}\\) (Mean Squared Error) — see Figure 2.2. This is also called L2 regularisation or Ridge regularisation.\nBias-variance trade-off: \\(\\text{MSE}\\) is composed of squared bias and variance.\n\\[\\text{MSE}(\\theta) = \\text{E}((\\hat{\\theta}-\\theta)^2) = \\text{Bias}(\\hat{\\theta})^2 + \\text{Var}(\\hat{\\theta})\\] with \\(\\text{Bias}(\\hat{\\theta}) = \\text{E}(\\hat{\\theta})-\\theta\\)\n\\(\\boldsymbol S\\): ML estimate, many parameters, low bias, high variance\n\\(\\boldsymbol I_d\\): “target”, no parameters, high bias, low variance\n\\(\\Longrightarrow\\) reduce high variance of \\(\\boldsymbol S\\) by introducing a bit of bias through \\(\\boldsymbol I_d\\)!\n\\(\\Longrightarrow\\) overall, \\(\\text{MSE}\\) is decreased\nChallenge: since we don’t know the true \\(\\boldsymbol \\Sigma\\) we cannot actually compute the \\(\\text{MSE}\\) directly but have to estimate it! How is this done in practise?\n\nby cross-validation (=resampling procedure)\nby using some analytic approximation (e.g. Stein’s formula)\n\nIn Worksheet 3 the empirical estimator of covariance is compared with the regularised covariance estimator implemented in the R package “corpcor”. This uses a regularisation similar as above (but for the correlation rather than the covariance matrix) and it employs an analytic data-adaptive estimate of the shrinkage intensity \\(\\lambda\\). This estimator is a variant of an empirical Bayes / James-Stein estimator (see MATH27720 Statistics 2).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate estimation</span>"
    ]
  },
  {
    "objectID": "02-estimation.html#full-bayesian-multivariate-modelling",
    "href": "02-estimation.html#full-bayesian-multivariate-modelling",
    "title": "2  Multivariate estimation",
    "section": "2.6 Full Bayesian multivariate modelling",
    "text": "2.6 Full Bayesian multivariate modelling\nSee also the section about multivariate distributions in the Probability and Distribution refresher for details about the distributions used below.\n\nThree main scenarios\nAs discussed in MATH27720 Statistics 2 there are three main Bayesian models in the univariate case that cover a large range of applications:\n\nthe beta-binomial model to estimate proportions\nthe normal-normal model to estimate means\nthe inverse gamma-normal model to estimate variances\n\nBelow we briefly sketch the extensions of these three elementary models to the multivariate case.\n\n\nDirichlet-multinomial model\nThis generalises the univariate beta-binomial model.\nThe Dirichlet distribution is useful as conjugate prior and posterior distribution for the parameters of a categorical distribution.\n\nData: \\(D=\\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) with \\(\\boldsymbol x_i \\sim \\text{Cat}(\\boldsymbol \\pi)\\)\nMLE: \\(\\hat{\\boldsymbol \\pi}_{ML} = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol x_i\\)\nPrior parameters (Dirichlet in mean parameterisation): \\(k_0\\), \\(\\boldsymbol \\pi_0\\),\n\\[\\boldsymbol \\pi\\sim \\text{Dir}(\\boldsymbol \\pi_0, k_0)\\] \\[\\text{E}(\\boldsymbol \\pi) = \\boldsymbol \\pi_0\\]\nPosterior parameters: \\(k_1 = k_0+n\\), \\(\\boldsymbol \\pi_1 = \\lambda \\boldsymbol \\pi_0 + (1-\\lambda) \\, \\hat{\\boldsymbol \\pi}_{ML}\\) with \\(\\lambda = \\frac{k_0}{k_1}\\)\n\\[\\boldsymbol \\pi\\,| \\, D \\sim \\text{Dir}(\\boldsymbol \\pi_1, k_1)\\] \\[\\text{E}(\\boldsymbol \\pi\\,| \\, D) = \\boldsymbol \\pi_1\\]\nEquivalent update rule (for Dirichlet with \\(\\boldsymbol \\alpha\\) parameter): \\(\\boldsymbol \\alpha_0\\) \\(\\rightarrow\\) \\(\\boldsymbol \\alpha_1 = \\boldsymbol \\alpha_0 + \\sum_{i=1}^n \\boldsymbol x_i = \\boldsymbol \\alpha_0 + n \\hat{\\boldsymbol \\pi}_{ML}\\)\n\n\n\nMultivariate normal-normal model\nThis generalises the univariate normal-normal model.\nThe multivariate normal distribution is useful as conjugate prior and posterior distribution of the mean:\n\nData: \\(D =\\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) with \\(\\boldsymbol x_i \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) with known mean \\(\\boldsymbol \\Sigma\\)\nMLE: \\(\\widehat{\\boldsymbol \\mu}_{ML} = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol x_i\\)\nPrior parameters: \\(k_0\\), \\(\\boldsymbol \\mu_0\\) \\[\\boldsymbol \\mu\\sim N_d\\left(\\boldsymbol \\mu_0, \\frac{\\boldsymbol \\Sigma}{k_0}\\right)\\] \\[\\text{E}(\\boldsymbol \\mu) = \\boldsymbol \\mu_0\\]\nPosterior parameters: \\(k_1 = k_0+n\\), \\(\\boldsymbol \\mu_1 = \\lambda \\boldsymbol \\mu_0 + (1-\\lambda) \\widehat{\\boldsymbol \\mu}_{ML}\\) with \\(\\lambda = \\frac{k_0}{k_1}\\)\n\\[\\boldsymbol \\mu\\, |\\, D \\sim N_d\\left( \\boldsymbol \\mu_1, \\frac{\\boldsymbol \\Sigma}{k_1}  \\right)\\] \\[\\text{E}(\\boldsymbol \\mu\\, |\\, D) = \\boldsymbol \\mu_1\\]\n\n\n\nInverse Wishart-normal model\nThis generalises the univariate inverse gamma-normal model for the variance.\nThe inverse Wishart distribution is useful as conjugate prior and posterior distribution of the covariance:\n\nData: \\(D =\\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) with \\(\\boldsymbol x_i \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) with known mean \\(\\boldsymbol \\mu\\)\nMLE: \\(\\widehat{\\boldsymbol \\Sigma}_{ML} = \\frac{1}{n} \\sum_{i=1}^n (\\boldsymbol x_i-\\boldsymbol \\mu)(\\boldsymbol x_i-\\boldsymbol \\mu)^T\\)\nPrior parameters: \\(\\kappa_0\\), \\(\\boldsymbol \\Sigma_0\\) \\[\\boldsymbol \\Sigma\\sim \\text{W}^{-1}_d\\left( \\kappa_0 \\boldsymbol \\Sigma_0 \\, , \\,  \\kappa_0+d+1\\right)\\] \\[\\text{E}(\\boldsymbol \\Sigma) = \\boldsymbol \\Sigma_0\\]\nPosterior parameters: \\(\\kappa_1 = \\kappa_0+n\\), \\(\\boldsymbol \\Sigma_1 = \\lambda \\boldsymbol \\Sigma_0 + (1-\\lambda) \\widehat{\\boldsymbol \\Sigma}_{ML}\\) with \\(\\lambda = \\frac{\\kappa_0}{\\kappa_1}\\)\n\\[\\boldsymbol \\Sigma\\, |\\, D \\sim \\text{W}^{-1}_d\\left( \\kappa_1 \\boldsymbol \\Sigma_1 \\, , \\,  \\kappa_1+d+1\\right)\\] \\[\\text{E}(\\boldsymbol \\Sigma\\, |\\, D) = \\boldsymbol \\Sigma_1\\]\nEquivalent update rule: \\(\\nu_0\\) \\(\\rightarrow\\) \\(\\nu_1 = \\nu_0+n\\), \\(\\boldsymbol \\Psi_0\\) \\(\\rightarrow\\) \\(\\boldsymbol \\Psi_1 = \\boldsymbol \\Psi_0 + \\sum_{i=1}^n (\\boldsymbol x_i-\\boldsymbol \\mu)(\\boldsymbol x_i-\\boldsymbol \\mu)^T\n= \\boldsymbol \\Psi_0 + n \\widehat{\\boldsymbol \\Sigma}_{ML}\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate estimation</span>"
    ]
  },
  {
    "objectID": "02-estimation.html#conclusion",
    "href": "02-estimation.html#conclusion",
    "title": "2  Multivariate estimation",
    "section": "2.7 Conclusion",
    "text": "2.7 Conclusion\n\nMultivariate models are often high-dimensional with large number of parameters but often only a small number of samples are available. In this instance it is useful (and often necessary) to introduce additional information (via priors or by regularisation).\nUnbiased estimation, a highly valued property in classical univariate statistics when sample size is large and number of parameters is small, is typically not a good idea in multivariate settings and often leads to poor estimators.\nRegularisation introduces bias and reduces variance, minimising overall MSE. Likewise, Bayesian estimators also introduce bias and regularise (via the prior) and thus are useful in multivariate settings.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate estimation</span>"
    ]
  },
  {
    "objectID": "02-estimation.html#footnotes",
    "href": "02-estimation.html#footnotes",
    "title": "2  Multivariate estimation",
    "section": "",
    "text": "Efron, B. 1979. Bootstrap methods: Another look at the jackknife. The Annals of Statistics 7:1–26. https://doi.org/10.1214/aos/1176344552↩︎\nFisher, R. A. 1922. On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society A 222:309–368. https://doi.org/10.1098/rsta.1922.0009↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate estimation</span>"
    ]
  },
  {
    "objectID": "03-transformations.html",
    "href": "03-transformations.html",
    "title": "3  Transformations and dimension reduction",
    "section": "",
    "text": "3.1 Linear Transformations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations and dimension reduction</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#linear-transformations",
    "href": "03-transformations.html#linear-transformations",
    "title": "3  Transformations and dimension reduction",
    "section": "",
    "text": "Location-scale transformation\nAlso known as affine transformation.\n\\[\\boldsymbol y= \\underbrace{\\boldsymbol a}_{\\text{location parameter}}+\\underbrace{\\boldsymbol B}_{\\text{scale parameter}} \\boldsymbol x\\space\\] \\[\\boldsymbol y: m \\times 1 \\text{ random vector}\\] \\[\\boldsymbol a: m \\times 1 \\text{ vector, location parameter}\\] \\[\\boldsymbol B: m \\times d \\text{ matrix, scale parameter },  m \\geq 1\\] \\[\\boldsymbol x: d \\times 1 \\text{ random vector}\\]\nMean and variance:\nMean and variance of the original vector \\(\\boldsymbol x\\):\n\\[\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\] \\[\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\]\nMean and variance of the transformed random vector \\(\\boldsymbol y\\):\n\\[\\text{E}(\\boldsymbol y)=\\boldsymbol a+ \\boldsymbol B\\boldsymbol \\mu_{\\boldsymbol x}\\] \\[\\text{Var}(\\boldsymbol y)= \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T\\]\nCross-covariance and cross-correlation:\nCross-covariance \\(\\boldsymbol \\Phi= \\Sigma_{\\boldsymbol x\\boldsymbol y} =  \\text{Cov}(\\boldsymbol x, \\boldsymbol y)\\) between \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\): \\[\n\\boldsymbol \\Phi= \\text{Cov}(\\boldsymbol x, \\boldsymbol B\\boldsymbol x) = \\boldsymbol \\Sigma_{\\boldsymbol x}  \\boldsymbol B^T\n\\] Note that \\(\\boldsymbol \\Phi\\) is a matrix of dimensions \\(d \\times m\\) as the dimension of \\(\\boldsymbol x\\) is \\(d\\) and the dimension of \\(\\boldsymbol y\\) is \\(m\\).\nCross-correlation \\(\\boldsymbol \\Psi= \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = \\text{Cor}(\\boldsymbol x, \\boldsymbol y)\\) between \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\): \\[\n\\boldsymbol \\Psi= \\boldsymbol V_{\\boldsymbol x}^{-1/2}  \\boldsymbol \\Phi\\boldsymbol V_{\\boldsymbol y}^{-1/2}\n\\] where \\(\\boldsymbol V_{\\boldsymbol x} = \\text{Diag}(\\boldsymbol \\Sigma_{\\boldsymbol x})\\) and \\(\\boldsymbol V_{\\boldsymbol y} = \\text{Diag}(\\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x}  \\boldsymbol B^T)\\) are diagonal matrices containing the variances for the components of \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\). The dimensions of the matrix \\(\\boldsymbol \\Psi\\) are also \\(d \\times m\\).\nSpecial cases/examples:\n\nExample 3.1 Univariate case (\\(d=1, m=1\\)): \\(y=a + b x\\)\n\n\\(\\text{E}(y)=a+b\\mu\\)\n\\(\\text{Var}(y)=b^2\\sigma^2\\)\n\\(\\text{Cov}(y, x) = b\\sigma^2\\)\n\\(\\text{Cor}(y, x) = \\frac{b \\sigma^2}{\\sqrt{b^2\\sigma^2} \\sqrt{\\sigma^2}  } =1\\)\n\nNote that \\(y\\) can predicted perfectly from \\(x\\) as \\(\\text{Cor}(y, x)=1\\). This is because there is no error term in the transformation. See also the more general case with multiple correlation further below.\n\n\nExample 3.2 Sum of two random univariate variables: \\(y = x_1 + x_2\\), i.e. \\(a=0\\) and \\(\\boldsymbol B=(1,1)\\)\n\n\\(\\text{E}(y) = \\text{E}(x_1+x_2)=\\mu_1+\\mu_2\\)\n\\(\\text{Var}(y) = \\text{Var}(x_1+x_2) = (1,1)\\begin{pmatrix}\n\\sigma^2_1 & \\sigma_{12}\\\\\n\\sigma_{12} & \\sigma^2_2\n\\end{pmatrix} \\begin{pmatrix}\n1\\\\\n1\n\\end{pmatrix} = \\sigma^2_1+\\sigma^2_2+2\\sigma_{12} = \\text{Var}(x_1)+\\text{Var}(x_2)+2\\,\\text{Cov}(x_1,x_2)\\)\n\n\n\nExample 3.3 \\(y_1=a_1+b_1 x_1\\) and \\(y_2=a_2+b_2 x_2\\), i.e. \\(\\boldsymbol a= \\begin{pmatrix} a_1\\\\ a_2 \\end{pmatrix}\\) and\n\\(\\boldsymbol B= \\begin{pmatrix}b_1 & 0\\\\ 0 & b_2\\end{pmatrix}\\)\n\n\\(\\text{E}(\\boldsymbol y)= \\begin{pmatrix}  a_1\\\\ a_2 \\end{pmatrix} +  \\begin{pmatrix}b_1 & 0\\\\ 0 & b_2\\end{pmatrix}\n\\begin{pmatrix} \\mu_1 \\\\ \\mu_2\\end{pmatrix}\n  = \\begin{pmatrix} a_1+b_1 \\mu_1\\\\ a_2+b_2 \\mu_2 \\end{pmatrix}\\)\n\n\\(\\text{Var}(\\boldsymbol y) = \\begin{pmatrix} b_1 & 0\\\\ 0 & b_2 \\end{pmatrix}\n   \\begin{pmatrix}\n\\sigma^2_1 & \\sigma_{12}\\\\\n\\sigma_{12} & \\sigma^2_2\n\\end{pmatrix}\n\\begin{pmatrix} b_1 & 0\\\\ 0 & b_2 \\end{pmatrix} =\n\\begin{pmatrix}\nb^2_1\\sigma^2_1 & b_1b_2\\sigma_{12}\\\\\nb_1b_2\\sigma_{12} & b^2_2\\sigma^2_2\n\\end{pmatrix}\\)\nnote that \\(\\text{Cov}(y_1, y_2) = b_1 b_2\\text{Cov}(x_1,x_2)\\)\n\n\n\n\nSquared multiple correlation\nSquared multiple correlation \\(\\text{MCor}(y, \\boldsymbol x)^2\\) is a scalar measure summarising the linear association between a scalar response variable \\(y\\) and a set of predictors \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\). It is defined as \\[\n\\begin{split}\n\\text{MCor}(y, \\boldsymbol x)^2 &= \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} / \\sigma^2_y\\\\\n&=\\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{ \\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\\\\\n\\end{split}\n\\] If \\(y\\) can be perfectly linearly predicted by \\(\\boldsymbol x\\) then \\(\\text{MCor}(y, \\boldsymbol x)^2 = 1\\).\nThe empirical estimate of \\(\\text{MCor}(y, \\boldsymbol x)^2\\) is the \\(R^2\\) coefficient that you will find in any software for linear regression.\n\nExample 3.4 Squared multiple correlation for an affine transformation.\nSince we linearly transform \\(\\boldsymbol x\\) into \\(\\boldsymbol y\\) with no additional error involved we expect that for each component \\(y_i\\) in \\(\\boldsymbol y\\) we have \\(\\text{MCor}(y_i, \\boldsymbol x)^2=1\\). This can be shown directly by computing \\[\n\\begin{split}\n\\left(\\text{MCor}(y_1, \\boldsymbol x)^2, \\ldots, \\text{MCor}(y_m, \\boldsymbol x)^2 \\right)^T\n&=\\text{Diag}\\left(\\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}  \\right) / \\text{Diag}\\left( \\boldsymbol \\Sigma_{\\boldsymbol y} \\right) \\\\\n&= \\text{Diag}\\left(\\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) / \\text{Diag}\\left( \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) \\\\\n&= \\text{Diag}\\left(\\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) / \\text{Diag}\\left( \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) \\\\\n&=\\left(1, \\ldots, 1 \\right)^T\\\\\n\\end{split}\n\\]\n\n\n\nInvertible location-scale transformation\nIf \\(m=d\\) (square \\(\\boldsymbol B\\)) and \\(\\det(\\boldsymbol B) \\neq 0\\) then the affine transformation is invertible.\nForward transformation: \\[\\boldsymbol y= \\boldsymbol a+ \\boldsymbol B\\boldsymbol x\\]\nBack transformation: \\[\\boldsymbol x= \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol a)\\]\nInvertible transformations thus provide a one-to-one map between \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\).\n\nExample 3.5 Orthogonal transformation\nSetting \\(\\boldsymbol a=0\\) and \\(\\boldsymbol B=\\boldsymbol Q\\) to an orthogonal matrix \\(\\boldsymbol Q\\) yields an orthogonal transformation. The inverse transformation is given by setting \\(\\boldsymbol B^{-1} = \\boldsymbol Q^T\\).\nAssume that \\(\\boldsymbol x\\) has a positive definite covariance matrix \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\) with eigenvalue decomposition \\(\\boldsymbol \\Sigma_{\\boldsymbol x} = \\boldsymbol U_1 \\boldsymbol \\Lambda\\boldsymbol U_1^T\\). After orthogonal transformation \\(\\boldsymbol y= \\boldsymbol Q\\boldsymbol x\\) the covariance matrix for \\(\\boldsymbol y\\) is \\(\\text{Var}(\\boldsymbol y)= \\boldsymbol \\Sigma_{\\boldsymbol y} = \\boldsymbol Q\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol Q^T = \\boldsymbol Q\\boldsymbol U_1 \\boldsymbol \\Lambda\\boldsymbol U_1^T\\boldsymbol Q^T = \\boldsymbol U_2 \\boldsymbol \\Lambda\\boldsymbol U_2^T\\) where \\(\\boldsymbol U_2 = \\boldsymbol Q\\boldsymbol U_1\\) is another orthogonal matrix. This shows that an orthogonal transformation reorientates the principal axes of the ellipse corresponding to covariance matrix, without changing the shape of the ellipse itself as the eigenvalues stay the same.\nIf you set \\(\\boldsymbol Q=\\boldsymbol U_1^T\\) then \\(\\boldsymbol U_2= \\boldsymbol I\\) and the reoriented principal axes are now parallel to the coordinate axes. This special type of orthogonal transformation is called principal component analysis (PCA) transformation. We revisit PCA in a later chapter.\n\n\nExample 3.6 Whitening transformation\nAssume that \\(\\boldsymbol x\\) has a positive definite covariance matrix \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\). The inverse principal matrix square root is denoted by \\(\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x}\\). This can be obtained by eigendecomposition of \\(\\boldsymbol \\Sigma_{\\boldsymbol x} = \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) so that \\(\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x} =\\boldsymbol U\\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T\\).\nSetting \\(\\boldsymbol a=0\\) and \\(\\boldsymbol B=\\boldsymbol Q\\boldsymbol \\Sigma_{\\boldsymbol x}^{-1/2}\\) where \\(\\boldsymbol Q\\) is an orthogonal matrix yields the covariance-based parameterisation of the general whitening transformation. The matrix \\(\\boldsymbol B\\) is called the whitening matrix and is also often denoted by \\(\\boldsymbol W\\).\nThe inverse transformation is given by setting \\(\\boldsymbol B^{-1} = \\boldsymbol \\Sigma_{\\boldsymbol x}^{1/2} \\boldsymbol Q^T\\).\nAfter transformation \\(\\boldsymbol y= \\boldsymbol Q\\boldsymbol \\Sigma_{\\boldsymbol x}^{-1/2} \\boldsymbol x\\) the covariance matrix for \\(\\boldsymbol y\\) is \\(\\text{Var}(\\boldsymbol y)= \\boldsymbol \\Sigma_{\\boldsymbol y} = \\boldsymbol Q\\boldsymbol \\Sigma_{\\boldsymbol x}^{-1/2} \\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1/2} \\boldsymbol Q^T = \\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol I\\), hence the name of the transformation. Whitening transformations are discussed in detail later.\n\n\nExample 3.7 Mahalanobis transform\n\nWe assume \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\) and a positive definite covariance matrix \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\) with \\(\\det(\\boldsymbol \\Sigma_{\\boldsymbol x}) &gt; 0\\).\nThe Mahalanobis transformation is given by \\[\n\\boldsymbol y=\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x}(\\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x})\n\\] This corresponds to an affine transformation with \\(\\boldsymbol a= - \\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x} \\boldsymbol \\mu_{\\boldsymbol x}\\) and \\(\\boldsymbol B= \\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x}\\).\nThe mean and the variance of \\(\\boldsymbol y\\) becomes \\[\n\\text{E}(\\boldsymbol y) = \\mathbf 0\\] and \\[\\text{Var}(\\boldsymbol y) = \\boldsymbol I_d\\].\nThe Mahalanobis transforms performs three functions:\n\nCentering (\\(-\\boldsymbol \\mu\\))\nStandardisation \\(\\text{Var}(y_i)=1\\)\nDecorrelation \\(\\text{Cor}(y_i,y_j)=0\\) for \\(i \\neq j\\)\n\nIn the univariate case (\\(d=1\\)) the coefficients reduce to \\(a = - \\frac{\\mu_x}{\\sigma_x}\\) and \\(B = \\frac{1}{\\sigma_x}\\) and the Mahalanobis transform becomes \\[y = \\frac{x-\\mu_x}{\\sigma_x}\\] i.e. it applies centering + standardisation.\nThe Mahalanobis transformation appears implicitly in many places in multivariate statistics, e.g. in the multivariate normal density. It is a particular example of a whitening transformation (plus centering).\n\nExample 3.8 Inverse Mahalanobis transformation\n\nThe inverse of the Mahalanobis transform is given by \\[\n\\boldsymbol y= \\boldsymbol \\mu_{\\boldsymbol y}+\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y} \\boldsymbol x\n\\] As the Mahalanobis transform is a whitening transform the inverse Mahalonobis transform is sometimes called the Mahalanobis colouring transformation. The coefficients in the affine transformation are \\(\\boldsymbol a=\\boldsymbol \\mu_{\\boldsymbol y}\\) and \\(\\boldsymbol B=\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y}\\).\nStarting with \\(\\text{E}(\\boldsymbol x)=\\mathbf 0\\) and \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol I_d\\) the mean and variance of the transformed variable are \\[\\text{E}(\\boldsymbol y) = \\boldsymbol \\mu_{\\boldsymbol y}\n\\] and \\[\\text{Var}(\\boldsymbol y) = \\boldsymbol \\Sigma_{\\boldsymbol y}\n\\]\n\n\nTransformation of a density under an invertible location-scale transformation:\nAssume \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) with density \\(f_{\\boldsymbol x}(\\boldsymbol x)\\).\nAfter linear transformation \\(\\boldsymbol y= \\boldsymbol a+ \\boldsymbol B\\boldsymbol x\\) we get \\(\\boldsymbol y\\sim F_{\\boldsymbol y}\\) with density \\[f_{\\boldsymbol y}(\\boldsymbol y)=|\\det(\\boldsymbol B)|^{-1} f_{\\boldsymbol x} \\left( \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol a)\\right)\\]\n\nExample 3.9 Transformation of standard normal with inverse Mahalanobis transform\nAssume \\(\\boldsymbol x\\) is multivariate standard normal \\(\\boldsymbol x\\sim N_d(\\mathbf 0,\\boldsymbol I_d)\\) with density \\[f_{\\boldsymbol x}(\\boldsymbol x) = (2\\pi)^{-d/2}\\exp\\left( -\\frac{1}{2} \\boldsymbol x^T \\boldsymbol x\\right)\\] Then the density after applying the inverse Mahalanobis transform\n\\(\\boldsymbol y= \\boldsymbol \\mu_{\\boldsymbol y}+\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y} \\boldsymbol x\\) is \\[\n\\begin{split}\nf_{\\boldsymbol y}(\\boldsymbol y) &= |\\det(\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y})|^{-1} (2\\pi)^{-d/2} \\exp\\left(-\\frac{1}{2}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})^T\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol y} \\,\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol y}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})\\right)\\\\\n& = (2\\pi)^{-d/2} \\det(\\boldsymbol \\Sigma_{\\boldsymbol y})^{-1/2} \\exp\\left(-\\frac{1}{2}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})^T\\boldsymbol \\Sigma^{-1}_{\\boldsymbol y}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})\\right) \\\\\n\\end{split}\n\\] \\(\\Longrightarrow\\) \\(\\boldsymbol y\\) has multivariate normal density \\(N_d(\\boldsymbol \\mu_{\\boldsymbol y}, \\boldsymbol \\Sigma_{\\boldsymbol y})\\)\nApplication: e.g. random number generation: draw from \\(N_d(\\mathbf 0,\\boldsymbol I_d)\\) (easy!) then convert to multivariate normal by tranformation (see Worksheet 4).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations and dimension reduction</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#nonlinear-transformations",
    "href": "03-transformations.html#nonlinear-transformations",
    "title": "3  Transformations and dimension reduction",
    "section": "3.2 Nonlinear transformations",
    "text": "3.2 Nonlinear transformations\n\nGeneral transformation\n\\[\\boldsymbol y= \\boldsymbol h(\\boldsymbol x)\\] with \\(\\boldsymbol h\\) an arbitrary vector-valued function\n\nlinear case: \\(\\boldsymbol h(\\boldsymbol x) = \\boldsymbol a+\\boldsymbol B\\boldsymbol x\\)\n\n\n\nDelta method\nAssume that we know the mean \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\) and variance \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\) of \\(\\boldsymbol x\\). Is it possible to say something about the mean and variance of the transformed random variable \\(\\boldsymbol y\\)? \\[\n\\text{E}(\\boldsymbol y)= \\text{E}(\\boldsymbol h(\\boldsymbol x))= ?\n\\] \\[\n\\text{Var}(\\boldsymbol y) = \\text{Var}(\\boldsymbol h(\\boldsymbol x))= ? \\\\\n\\]\nIn general, for a transformation \\(\\boldsymbol h(\\boldsymbol x)\\) the exact mean and variance of the transformed variable cannot be obtained analytically.\nHowever, we can find a linear approximation and then compute its mean and variance. This approximation is called the “Delta Method”, or the “law of propagation of errors”, and is credited to Gauss 1.\nLinearisation of \\(\\boldsymbol h(\\boldsymbol x)\\) is achieved by a Taylor series approximation of first order of \\(\\boldsymbol h(\\boldsymbol x)\\) around \\(\\boldsymbol x_0\\): \\[\\boldsymbol h(\\boldsymbol x) \\approx \\boldsymbol h(\\boldsymbol x_0) + \\underbrace{D \\boldsymbol h(\\boldsymbol x_0)}_{\\text{Jacobian matrix}}(\\boldsymbol x-\\boldsymbol x_0)  =\n\\underbrace{\\boldsymbol h(\\boldsymbol x_0) - D \\boldsymbol h(\\boldsymbol x_0)\\, \\boldsymbol x_0}_{\\boldsymbol a} + \\underbrace{D \\boldsymbol h(\\boldsymbol x_0)}_{\\boldsymbol B} \\boldsymbol x\\]\nIf \\(h(\\boldsymbol x)\\) is scalar-valued then gradient \\(\\nabla h(\\boldsymbol x)\\) is given by the vector of partial correlations \\[\n\\nabla h(\\boldsymbol x) =\n\\begin{pmatrix}\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_1}  \\\\\n\\vdots\\\\\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_d} \\\\\n\\end{pmatrix}\n\\] where \\(\\nabla\\) is the nabla operator.\nThe Jacobian matrix is used if \\(\\boldsymbol h(\\boldsymbol x)\\) is vector-valued:\n\\[D \\boldsymbol h(\\boldsymbol x) = \\begin{pmatrix}\\nabla^T h_1(\\boldsymbol x)\\\\ \\nabla h_2(\\boldsymbol x)^T \\\\ \\vdots \\\\ \\nabla^T h_m(\\boldsymbol x) \\end{pmatrix} = \\begin{pmatrix}\n    \\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_1} & \\dots & \\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_d}\\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_1} & \\dots & \\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_d}\n    \\end{pmatrix}\\] Note that in the Jacobian matrix by convention the gradient for each individual component of \\(\\boldsymbol h(\\boldsymbol x)\\) is contained in the row of the matrix so the number of rows corresponds to the dimension of \\(\\boldsymbol h(\\boldsymbol x)\\) and the number of columns to the dimension of \\(\\boldsymbol x\\).\nFirst order approximation of \\(\\boldsymbol h(\\boldsymbol x)\\) around \\(\\boldsymbol x_0=\\boldsymbol \\mu_{\\boldsymbol x}\\) yields \\(\\boldsymbol a= \\boldsymbol h(\\boldsymbol \\mu_{\\boldsymbol x}) - D \\boldsymbol h(\\boldsymbol \\mu_{\\boldsymbol x})\\, \\boldsymbol \\mu_{\\boldsymbol x}\\) and \\(\\boldsymbol B= D \\boldsymbol h(\\boldsymbol \\mu_{\\boldsymbol x})\\) and leads directly to the multivariate Delta method:\n\\[\\text{E}(\\boldsymbol y)\\approx\\boldsymbol h(\\boldsymbol \\mu_{\\boldsymbol x})\\] \\[\\text{Var}(\\boldsymbol y)\\approx D \\boldsymbol h(\\boldsymbol \\mu_{\\boldsymbol x}) \\, \\boldsymbol \\Sigma_{\\boldsymbol x} \\, D \\boldsymbol h(\\boldsymbol \\mu_{\\boldsymbol x})^T\\]\nThe univariate Delta method is a special case: \\[\\text{E}(y) \\approx h(\\mu_x)\\] \\[\\text{Var}(y)\\approx \\sigma^2_x \\, h'(\\mu_x)^2\\]\nNote that the Delta approximation breaks down if \\(\\text{Var}(\\boldsymbol y)\\) is singular, for example if the first derivative (or gradient or Jacobian matrix) at \\(\\boldsymbol \\mu_{\\boldsymbol x}\\) is zero.\n\nExample 3.10 Variance of the odds ratio\nThe proportion \\(\\hat{p} = \\frac{n_1}{n}\\) resulting from \\(n\\) repeats of a Bernoulli experiment has expectation \\(\\text{E}(\\hat{p})=p\\) and variance \\(\\text{Var}(\\hat{p}) = \\frac{p (1-p)}{n}\\). What are the (approximate) mean and the variance of the corresponding odds ratio \\(\\widehat{OR}=\\frac{\\hat{p}}{1-\\hat{p}}\\)?\nWith \\(h(x) = \\frac{x}{1-x}\\), \\(\\widehat{OR} = h(\\hat{p})\\) and \\(h'(x) = \\frac{1}{(1-x)^2}\\) we get using the Delta method \\(\\text{E}( \\widehat{OR} ) \\approx h(p) = \\frac{p}{1-p}\\) and \\(\\text{Var}( \\widehat{OR} )\\approx h'(p)^2 \\text{Var}( \\hat{p} ) = \\frac{p}{n (1-p)^3}\\).\n\n\nExample 3.11 Log-transform as variance stabilisation\nAssume \\(x\\) has some mean \\(\\text{E}(x)=\\mu\\) and variance \\(\\text{Var}(x) = \\sigma^2 \\mu^2\\), i.e. the standard deviation \\(\\text{SD}(x)\\) is proportional to the mean \\(\\mu\\). What are the (approximate) mean and the variance of the log-transformed variable \\(\\log(x)\\)?\nWith \\(h(x) = \\log(x)\\) and \\(h'(x) = \\frac{1}{x}\\) we get using the Delta method \\(\\text{E}( \\log(x) ) \\approx h(\\mu) = \\log(\\mu)\\) and \\(\\text{Var}( \\log(x) )\\approx h'(\\mu)^2 \\text{Var}( x ) = \\left(\\frac{1}{\\mu} \\right)^2 \\sigma^2 \\mu^2 = \\sigma^2\\). Thus, after applying the log-transform the variance does not depend any more on the mean!\n\n\n\nTransformation of a probability density function under a general invertible transformation\nAssume \\(\\boldsymbol y(\\boldsymbol x) = \\boldsymbol h(\\boldsymbol x)\\) is invertible: \\(\\boldsymbol x(\\boldsymbol y) = \\boldsymbol h^{-1}(\\boldsymbol y)\\)\n\\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) with probability density function \\(f_{\\boldsymbol x}(\\boldsymbol x)\\)\nThe density \\(f_{\\boldsymbol y}(\\boldsymbol y)\\) of the transformed random vector \\(\\boldsymbol y\\) is then given by\n\\[f_{\\boldsymbol y}(\\boldsymbol y) = |\\det\\left( D\\boldsymbol x(\\boldsymbol y) \\right)| \\,\\,\\,  f_{\\boldsymbol x}\\left( \\boldsymbol x(\\boldsymbol y) \\right)\\]\nwhere \\(D\\boldsymbol x(\\boldsymbol y)\\) is the Jacobian matrix of the inverse transformation.\nSpecial cases:\n\nUnivariate version: \\(f_y(y) = \\left|Dx(y) \\right| \\, f_x\\left(x(y)\\right)\\) with \\(Dx(y) = \\frac{dx(y)}{dy}\\)\nLinear transformation \\(\\boldsymbol h(\\boldsymbol x) = \\boldsymbol a+ \\boldsymbol B\\boldsymbol x\\), with \\(\\boldsymbol x(\\boldsymbol y) = \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol a)\\) and \\(D\\boldsymbol x(\\boldsymbol y) = \\boldsymbol B^{-1}\\): \\[f_{\\boldsymbol y}(\\boldsymbol y)=\\left|\\det(\\boldsymbol B)\\right|^{-1} f_{\\boldsymbol x} \\left( \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol a)\\right)\\]\n\n\n\nNormalising flows\nIn this module we will focus mostly on linear transformations as these underpin much of classical multivariate statistics, but it is important to keep in mind for later study the importance of nonlinear transformations\nIn machine learning (sequences of) invertible nonlinear transformations are known as “normalising flows”. They are used both in a generative way (building complex models from simple models) and for simplification and dimension reduction.\nIf you are interested in normalising flows then a good start to learn more are the review papers by Kobyzev et al (2021 )2 and Papamakarios et al. (2021) 3.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations and dimension reduction</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#general-whitening-transformations",
    "href": "03-transformations.html#general-whitening-transformations",
    "title": "3  Transformations and dimension reduction",
    "section": "3.3 General whitening transformations",
    "text": "3.3 General whitening transformations\n\nOverview\nWhitening transformations are a special and widely used class of invertible location-scale transformations (Example 3.6).\nTerminology: whitening refers to the fact that after the transformation the covariance matrix is spherical, isotropic, white (\\(\\boldsymbol I_d\\))\nWhitening is useful in preprocessing, as they allow to turn multivariate models into uncorrelated univariate models (via decorrelation property). Some whitening transformations reduce the dimension in an optimal way (via compression property).\nThe Mahalanobis transform is a specific example of a whitening transformation. It is also know as “zero-phase component analysis” or short ZCA transform.\nIn so-called latent variable models whitening procedures are implicitly used in linear models to link observed (correlated) variables and latent variables (which typically are uncorrelated and standardised):\n\\[\\begin{align*}\n\\begin{array}{cl}\n\\text{Whitening} \\\\\n\\downarrow\n\\end{array}\n\\begin{array}{ll}\n\\boldsymbol x\\\\\n\\uparrow \\\\\n\\boldsymbol z\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{Observed variable (can be measured)} \\\\\n\\text{external, typically correlated} \\\\\n\\space \\\\\n\\text{Unobserved \"latent\" variable (cannot be directly measured)} \\\\\n\\text{internal, typically chosen to be uncorrelated} \\\\\n\\end{array}\n\\end{align*}\\]\n\n\nWhitening transformation and whitening constraint\nStarting point:\nRandom vector \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) not necessarily from multivariate normal.\n\\(\\boldsymbol x\\) has mean \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu\\) and a positive definite (invertible) covariance matrix \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\).\nNote that in the following we leave out the subscript \\(\\boldsymbol x\\) for the covariance of \\(\\boldsymbol x\\) unless it is needed for clarification.\nThe covariance can be split into positive variances \\(\\boldsymbol V\\) and a positive definite invertible correlation matrix \\(\\boldsymbol P\\) so that \\(\\boldsymbol \\Sigma= \\boldsymbol V^{1/2} \\boldsymbol P\\boldsymbol V^{1/2}\\).\nWhitening transformation:\n\\[\\underbrace{\\boldsymbol z}_{d \\times 1 \\text{ vector }} = \\underbrace{\\boldsymbol W}_{d \\times d \\text{ whitening matrix }} \\underbrace{\\boldsymbol x}_{d \\times 1 \\text{ vector }}\\] Objective: choose \\(\\boldsymbol W\\) so that \\(\\text{Var}(\\boldsymbol z)=\\boldsymbol I_d\\)\nFor Mahalanobis/ZCA whitening we already know from Example 3.7 that \\(\\boldsymbol W^{\\text{ZCA}}=\\boldsymbol \\Sigma^{-1/2}\\).\nIn general, the whitening matrix \\(\\boldsymbol W\\) needs to satisfy a constraint: \\[\n\\begin{array}{lll}\n                & \\text{Var}(\\boldsymbol z) & = \\boldsymbol I_d \\\\\n\\Longrightarrow & \\text{Var}(\\boldsymbol W\\boldsymbol x) &= \\boldsymbol W\\boldsymbol \\Sigma\\boldsymbol W^T = \\boldsymbol I_d \\\\\n\\Longrightarrow &  \\boldsymbol W\\, \\boldsymbol \\Sigma\\, \\boldsymbol W^T \\boldsymbol W= \\boldsymbol W& \\\\\n\\end{array}\n\\] \\[\\Longrightarrow \\text{constraint on whitening matrix: } \\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1}\\]\nClearly, the ZCA whitening matrix satisfies this constraint: \\((\\boldsymbol W^{ZCA})^T \\boldsymbol W^{ZCA} = \\boldsymbol \\Sigma^{-1/2}\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1}\\)\n\n\nParameterisation of whitening matrix\nCovariance-based parameterisation of whitening matrix:\nA general way to specify a valid whitening matrix is \\[\n\\boldsymbol W= \\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2}\n\\] where \\(\\boldsymbol Q_1\\) is an orthogonal matrix.\nRecall that an orthogonal matrix \\(\\boldsymbol Q\\) has the property that \\(\\boldsymbol Q^{-1} = \\boldsymbol Q^T\\) and and as a consequence \\(\\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol I\\).\nAs a result, the above \\(\\boldsymbol W\\) satisfies the whitening constraint:\n\\[\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1/2}\\underbrace{\\boldsymbol Q_1^T \\boldsymbol Q_1}_{\\boldsymbol I}\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1}\\]\nNote the converse is also true: any whitening whitening matrix, i.e. any \\(\\boldsymbol W\\) satisfying the whitening constraint, can be written in the above form as \\(\\boldsymbol Q_1 = \\boldsymbol W\\boldsymbol \\Sigma^{1/2}\\) is orthogonal by construction.\n\\(\\Longrightarrow\\) instead of choosing \\(\\boldsymbol W\\), we choose the orthogonal matrix \\(\\boldsymbol Q_1\\)!\n\nrecall that orthogonal matrices geometrically represent rotations (plus reflections).\nit is now clear that there are infinitely many whitening procedures, because there are infinitely many rotations! This also means we need to find ways to choose/select among whitening procedures.\nfor the Mahalanobis/ZCA transformation \\(\\boldsymbol Q_1^{\\text{ZCA}}=\\boldsymbol I\\)\nwhitening can be interpreted as Mahalanobis transformation followed by further rotation-reflection\n\nCorrelation-based parameterisation of whitening matrix:\nInstead of working with the covariance matrix \\(\\boldsymbol \\Sigma\\), we can express \\(\\boldsymbol W\\) also in terms of the corresponding correlation matrix \\(\\boldsymbol P= (\\rho_{ij}) = \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2}\\) where \\(\\boldsymbol V\\) is the diagonal matrix containing the variances.\nSpecifically, we can specify the whitening matrix as \\[\\boldsymbol W= \\boldsymbol Q_2 \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2}\\]\nIt is easy to verify that this \\(\\boldsymbol W\\) also satisfies the whitening constraint: \\[\n\\begin{split}\n\\boldsymbol W^T \\boldsymbol W& = \\boldsymbol V^{-1/2}\\boldsymbol P^{-1/2}\\underbrace{\\boldsymbol Q_2^T \\boldsymbol Q_2}_{\\boldsymbol I}\\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} \\\\\n& = \\boldsymbol V^{-1/2} \\boldsymbol P^{-1} \\boldsymbol V^{-1/2} = \\boldsymbol \\Sigma^{-1} \\\\\n\\end{split}\n\\] Conversely, any whitening matrix \\(\\boldsymbol W\\) can also be written in this form as \\(\\boldsymbol Q_2 = \\boldsymbol W\\boldsymbol V^{1/2} \\boldsymbol P^{1/2}\\) is orthogonal by construction.\n\nAnother interpretation of whitening: first standardising (\\(\\boldsymbol V^{-1/2}\\)), then decorrelation (\\(\\boldsymbol P^{-1/2}\\)), followed by rotation-reflection (\\(\\boldsymbol Q_2\\))\nfor Mahalanobis/ZCA transformation \\(\\boldsymbol Q_2^{\\text{ZCA}} = \\boldsymbol \\Sigma^{-1/2} \\boldsymbol V^{1/2} \\boldsymbol P^{1/2}\\)\n\nBoth forms to write \\(\\boldsymbol W\\) using \\(\\boldsymbol Q_1\\) and \\(\\boldsymbol Q_2\\) are equally valid (and interchangeable).\nNote that for the same \\(\\boldsymbol W\\) \\[\\boldsymbol Q_1\\neq\\boldsymbol Q_2 \\text{  Two different orthogonal matrices!}\\] and also \\[\\underbrace{\\boldsymbol \\Sigma^{-1/2}}_{\\text{Symmetric}}\\neq\\underbrace{\\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}}_{\\text{Not Symmetric}}\\] even though\n\\[\\boldsymbol \\Sigma^{-1/2}\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1} = \\boldsymbol V^{-1/2}\\boldsymbol P^{-1/2}\\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}\\]\n\n\nCross-covariance and cross-correlation for general whitening transformations\nA useful criterion to characterise and to distinguish among whitening transformations is the cross-covariance and cross-correlation matrix between the original variable \\(\\boldsymbol x\\) and the whitened variable \\(\\boldsymbol z\\):\n\nCross-covariance \\(\\boldsymbol \\Phi= \\Sigma_{\\boldsymbol x\\boldsymbol z}\\) between \\(\\boldsymbol x\\) and \\(\\boldsymbol z\\): \\[\n\\begin{split}\n\\boldsymbol \\Phi= \\text{Cov}(\\boldsymbol x, \\boldsymbol z) & = \\text{Cov}( \\boldsymbol x,\\boldsymbol W\\boldsymbol x)\\\\\n& = \\boldsymbol \\Sigma\\boldsymbol W^T \\\\\n&= \\boldsymbol \\Sigma\\, \\boldsymbol \\Sigma^{-1/2} \\boldsymbol Q_1^T \\\\\n&= \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T\\\\\n\\end{split}\n\\] In component notation we write \\(\\boldsymbol \\Phi= (\\phi_{ij})\\) where the row index \\(i\\) refers to \\(\\boldsymbol x\\) and the column index \\(j\\) to \\(\\boldsymbol z\\).\nCross-covariance is linked with \\(\\boldsymbol Q_1\\)! Thus, choosing cross-covariance determines \\(\\boldsymbol Q_1\\) (and vice versa).\nNote that the above cross-covariance matrix \\(\\boldsymbol \\Phi\\) satisfies the condition \\(\\boldsymbol \\Phi\\boldsymbol \\Phi^T = \\boldsymbol \\Sigma\\).\nThe whitening matrix expressed in terms of cross-covariance is \\(\\boldsymbol W= \\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1}\\), so as required \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Phi\\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1}\n=\\boldsymbol \\Sigma^{-1}\\). Furthermore, \\(\\boldsymbol \\Phi\\) is the inverse of the whitening matrix, as \\(\\boldsymbol W^{-1} = \\left( \\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2}  \\right)^{-1} =\n\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{-1} =\n\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{T}   = \\boldsymbol \\Phi\\).\nCross-correlation \\(\\boldsymbol \\Psi= \\boldsymbol P_{\\boldsymbol x\\boldsymbol z}\\) between \\(\\boldsymbol x\\) and \\(\\boldsymbol z\\): \\[\n\\begin{split}\n\\boldsymbol \\Psi= \\text{Cor}(\\boldsymbol x, \\boldsymbol z) & = \\boldsymbol V^{-1/2} \\boldsymbol \\Phi\\\\\n& =  \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol W^T \\\\\n&=\\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2} \\boldsymbol P^{-1/2} \\boldsymbol Q_2^T\\\\\n& =  \\boldsymbol P^{1/2}  \\boldsymbol Q_2^T\\\\\n\\end{split}\n\\]\nIn component notation we write \\(\\boldsymbol \\Psi= (\\psi_{ij})\\) where the row index \\(i\\) refers to \\(\\boldsymbol x\\) and the column index \\(j\\) to \\(\\boldsymbol z\\).\nCross-correlation is linked with \\(\\boldsymbol Q_2\\)! Hence, choosing cross-correlation determines \\(\\boldsymbol Q_2\\) (and vice versa). The whitening matrix expressed in terms of cross-correlation is \\(\\boldsymbol W= \\boldsymbol \\Psi^T \\boldsymbol P^{-1} \\boldsymbol V^{-1/2}\\).\n\nNote that the factorisation of the cross-covariance \\(\\boldsymbol \\Phi=\\boldsymbol \\Sigma^{1/2}\\boldsymbol Q_1^T\\) and the cross-correlation \\(\\boldsymbol \\Psi=\\boldsymbol P^{1/2}\\boldsymbol Q_2^T\\) into the product of a positive definite symmetric matrix and an orthogonal matrix are examples of a polar decomposition.\n\n\nInverse whitening transformation and loadings\nInverse transformation:\nRecall that \\(\\boldsymbol z= \\boldsymbol W\\boldsymbol x\\). Therefore, the reverse transformation going from the whitened to the original variable is \\(\\boldsymbol x= \\boldsymbol W^{-1} \\boldsymbol z\\). This can be expressed also in terms of cross-covariance and cross-correlation. With \\(\\boldsymbol W^{-1} = \\boldsymbol \\Phi\\) we get \\[\n\\boldsymbol x= \\boldsymbol \\Phi\\boldsymbol z\\, .\n\\] Furthermore, since \\(\\boldsymbol \\Psi= \\boldsymbol V^{-1/2} \\boldsymbol \\Phi\\) we have \\(\\boldsymbol W^{-1}  = \\boldsymbol V^{1/2}  \\boldsymbol \\Psi\\) and hence \\[\n\\boldsymbol V^{-1/2} \\boldsymbol x=   \\boldsymbol \\Psi\\boldsymbol z\\, .\n\\]\nThe reverse whitening transformation is also known as colouring transformation (the previously discussed inverse Mahalanobis transform is one example).\nDefinition of loadings:\nLoadings are the coefficients of the linear transformation from the latent variable back to the observed variable. If the variables are standardised to unit variance then the loadings are also called correlation loadings.\nHence, the cross-covariance matrix \\(\\boldsymbol \\Phi\\) plays the role of loadings linking the latent variable \\(\\boldsymbol z\\) with the original \\(\\boldsymbol x\\). Similarly, the cross-correlation matrix \\(\\boldsymbol \\Psi\\) contains the correlation loadings linking the (already standardised) latent variable \\(\\boldsymbol z\\) with the standardised \\(\\boldsymbol x\\).\nIn the convention we use here the rows correspond to the original variables and the columns to the whitened latent variables.\nMultiple correlation coefficients from \\(\\boldsymbol z\\) back to \\(\\boldsymbol x\\):\nWe consider the backtransformation from the whitened variable \\(\\boldsymbol z\\) to the original variables \\(\\boldsymbol x\\) and note that the components of \\(\\boldsymbol z\\) are all uncorrelated witth \\(\\boldsymbol P_{\\boldsymbol z} = \\boldsymbol I\\). The squared multiple correlation coefficient \\(\\text{MCor}(x_i, \\boldsymbol z)\\) between each \\(x_i\\) and all \\(\\boldsymbol z\\) is therefore just the sum of the corresponding squared correlations \\(\\text{Cor}(x_i, z_j)^2\\): \\[\n\\begin{split}\n\\text{MCor}(x_i, \\boldsymbol z)^2 &=  \\boldsymbol P_{x_i \\boldsymbol z} \\boldsymbol P_{\\boldsymbol z}^{-1} \\boldsymbol P_{\\boldsymbol zx_i} = \\\\\n             & \\sum_{j=1}^d  \\text{Cor}(x_i, z_j)^2  \\\\\n&  = \\sum_{j=1}^d \\psi_{ij}^2 = 1\n\\end{split}\n\\] As shown earlier for a general linear one-to-one- transformation (which includes whitening as special case) the squared multiple correlation must be 1 because there is no error. We can confirm this by computing the row sums of squares of the cross-correlation matrix \\(\\boldsymbol \\Psi\\) in matrix notation \\[\n\\begin{split}\n\\text{Diag}\\left(\\boldsymbol \\Psi\\boldsymbol \\Psi^T\\right) &= \\text{Diag}\\left(\\boldsymbol P^{1/2} \\boldsymbol Q_2^T \\boldsymbol Q_2\\boldsymbol P^{1/2}\\right) \\\\\n&= \\text{Diag}(\\boldsymbol P) \\\\\n&= (1, \\ldots, 1)^T\\\\\n\\end{split}\n\\] from which it is clear that the choice of \\(\\boldsymbol Q_2\\) is not relevant.\nSimilarly, the row sums of squares of the cross-covariance matrix \\(\\boldsymbol \\Phi\\) equal the variances of the original variables, regardless of \\(\\boldsymbol Q_1\\): \\[\n\\sum_{j=1}^d \\phi_{ij}^2 = \\text{Var}(x_i)\n\\] or in matrix notation \\[\n\\begin{split}\n\\text{Diag}\\left(\\boldsymbol \\Phi\\boldsymbol \\Phi^T\\right) &= \\text{Diag}\\left(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T \\boldsymbol Q_1 \\boldsymbol \\Sigma^{1/2}\\right) \\\\\n&= \\text{Diag}(\\boldsymbol \\Sigma) \\\\\n&= (\\text{Var}(x_1), \\ldots, \\text{Var}(x_d)^T\\\\\n\\end{split}\n\\]\n\n\nSummaries of cross-covariance \\(\\boldsymbol \\Phi\\) and cross-correlation \\(\\boldsymbol \\Psi\\) resulting from whitening transformations\nMatrix trace:\nA simply summary of a matrix is its trace. For the cross-covariance matrix \\(\\boldsymbol \\Phi\\) the trace is the sum of all covariances between corresponding elements in \\(\\boldsymbol x\\) and \\(\\boldsymbol z\\): \\[\n\\text{Tr}(\\boldsymbol \\Phi) =  \\sum_{i=1}^d \\text{Cov}(x_i, z_i) =  \\sum_{i=1}^d  \\phi_{ii} = \\text{Tr}\\left(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T\\right)\n\\] Likewise, for the cross-correlation matrix \\(\\boldsymbol \\Psi\\) the trace is the sum of all correlations between corresponding elements in \\(\\boldsymbol x\\) and \\(\\boldsymbol z\\): \\[\n\\text{Tr}(\\boldsymbol \\Psi) =  \\sum_{i=1}^d \\text{Cor}(x_i, z_i) =  \\sum_{i=1}^d  \\psi_{ii} = \\text{Tr}\\left(\\boldsymbol P^{1/2} \\boldsymbol Q_2^T\\right)\n\\]\nIn both cases the value of the trace depends on \\(\\boldsymbol Q_1\\) and \\(\\boldsymbol Q_2\\). Interestingly, there is unique choice such that the trace is maximised.\nSpecifically, to maximise \\(\\text{Tr}(\\boldsymbol \\Phi)\\) we conduct the following steps:\n\nApply eigendecomposition to \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\). Note that \\(\\boldsymbol \\Lambda\\) is diagonal with positive eigenvalues \\(\\lambda_i &gt; 0\\) as \\(\\boldsymbol \\Sigma\\) is positive definite and \\(\\boldsymbol U\\) is an orthogonal matrix.\nThe objective function becomes \\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol \\Phi) &= \\text{Tr}\\left(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T\\right)\\\\\n&= \\text{Tr}\\left( \\boldsymbol U\\boldsymbol \\Lambda^{1/2} \\boldsymbol U^T \\boldsymbol Q_1^T \\right) \\\\\n&= \\text{Tr}\\left(\\boldsymbol \\Lambda^{1/2} \\, \\boldsymbol U^T \\boldsymbol Q_1^T \\boldsymbol U\\right) \\\\\n& = \\text{Tr}\\left(\\boldsymbol \\Lambda^{1/2} \\, \\boldsymbol B\\right) \\\\\n& = \\sum_{i=1}^d \\lambda_i^{1/2} b_{ii}.\n\\end{split}\n\\] Note that the product of two orthogonal matrices is itself an orthogonal matrix. Therefore, \\(\\boldsymbol B= \\boldsymbol U^T \\boldsymbol Q_1^T \\boldsymbol U\\) is an orthogonal matrix and \\(\\boldsymbol Q_1 = \\boldsymbol U\\boldsymbol B^T \\boldsymbol U^T\\).\nAs \\(\\lambda_i &gt; 0\\) and all \\(b_{ii} \\in [-1, 1]\\) the objective function is maximised for \\(b_{ii}=1\\), i.e. for \\(\\boldsymbol B=\\boldsymbol I\\).\nIn turn this implies that \\(\\text{Tr}(\\boldsymbol \\Phi)\\) is maximised for \\(\\boldsymbol Q_1=\\boldsymbol I\\).\n\nSimilarly, to maximise \\(\\text{Tr}(\\boldsymbol \\Psi)\\) we\n\ndecompose \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\) and then, following the above,\n\nfind that \\(\\text{Tr}(\\boldsymbol \\Psi) = \\text{Tr}\\left(\\boldsymbol \\Theta^{1/2} \\, \\boldsymbol G^T \\boldsymbol Q_2^T \\boldsymbol G\\right)\\) is maximised for \\(\\boldsymbol Q_2=\\boldsymbol I\\).\n\nSquared Frobenius norm and total variation:\nAnother way to summarise and dissect the association between \\(\\boldsymbol x\\) and the corresponding whitened \\(\\boldsymbol z\\) is the squared Frobenius norm and the total variation based on \\(\\boldsymbol \\Phi\\) and \\(\\boldsymbol \\Psi\\).\nThe squared Frobenius norm (Euclidean) norm is the sum of squared elements of a matrix.\nIf we consider the squared Frobenius norm of the cross-covariance matrix, i.e. the sum of squared covariances between \\(\\boldsymbol x\\) and \\(\\boldsymbol z\\), \\[\n|| \\boldsymbol \\Phi||_F^2 = \\sum_{i=1}^d \\sum_{j=1}^d \\phi_{ij}^2 =  \\text{Tr}(\\boldsymbol \\Phi^T \\boldsymbol \\Phi) = \\text{Tr}( \\boldsymbol \\Sigma)\n\\] we find that this equals the total variation of \\(\\boldsymbol \\Sigma\\) and that it does not depend on \\(\\boldsymbol Q_1\\). Likewise, computing the squared Frobenius norm of the cross-correlation matrix, i.e. the sum of squared correlations between \\(\\boldsymbol x\\) and \\(\\boldsymbol z\\), \\[\n|| \\boldsymbol \\Psi||_F^2  = \\sum_{i=1}^d \\sum_{j=1}^d \\psi_{ij}^2= \\text{Tr}\\left(\\boldsymbol \\Psi^T  \\boldsymbol \\Psi\\right) =\\text{Tr}\\left( \\boldsymbol P\\right) = d\n\\] yields the total variation of \\(\\boldsymbol P\\) which also does not depend on \\(\\boldsymbol Q_2\\). Note this is because the squared Frobenius norm is invariant against rotations and reflections.\nProportion of total variation:\nWe can now compute the contribution of each whitened component \\(z_j\\) to the total variation. The sum of squared covariances of each \\(z_j\\) with all \\(x_1, \\ldots, x_d\\) is \\[\nh_j = \\sum^d_{i=1}\\text{Cov}(x_i,z_j)^2 = \\sum^d_{i=1} \\phi_{ij}^2\n\\] with \\(\\sum_{j=1}^d h_j = \\text{Tr}\\left(\\boldsymbol \\Sigma\\right)\\) the total variation. In vector notation the contributions are written as the column sums of squares of \\(\\boldsymbol \\Phi\\) \\[\n\\boldsymbol h= (h_1,\\ldots,h_d)^T = \\text{Diag}(\\boldsymbol \\Phi^T\\boldsymbol \\Phi) = \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\\,.\n\\] The relative contribution of \\(z_j\\) versus the total variation is \\[\n\\frac{ h_j }{\\text{Tr}\\left( \\boldsymbol \\Sigma\\right)} \\,.\n\\] Crucially, in contrast to total variation, the contributions \\(h_j\\) depend on the choice of \\(\\boldsymbol Q_1\\).\nSimilarly, the sum of squared correlations of each \\(z_j\\) with all \\(x_1, \\ldots, x_d\\) is \\[\nk_j = \\sum^d_{i=1}\\text{Cor}(x_i,z_j)^2 = \\sum^d_{i=1} \\psi_{ij}^2\n\\] with \\(\\sum_{i=j}^d k_j = \\text{Tr}( \\boldsymbol P) = d\\). In vector notation this correspoinds to the column sums of squares of \\(\\boldsymbol \\Psi\\) \\[\n\\boldsymbol k= (k_1,\\ldots,k_d)^T = \\text{Diag}\\left(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\right)=\\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\\,.\n\\] The relative contribution of \\(z_j\\) with regard to the total variation of the correlation \\(\\boldsymbol P\\) is \\[\n\\frac{ k_j  }{\\text{Tr}( \\boldsymbol P)} = \\frac{ k_j  }{d} \\,.\n\\] As above, the contributions \\(k_j\\) depend on the choice of \\(\\boldsymbol Q_2\\).\nMaximising the proportion of total variation:\nInterestingly, it is possible to choose a unique whitening transformation such that the contributions are maximised, i.e. that the sum of the \\(m\\) largest contributions of \\(h_j\\) and \\(k_j\\) is as large as possible.\nSpecifically, we note that \\(\\boldsymbol \\Phi^T\\boldsymbol \\Phi\\) and \\(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\) are symmetric real matrices. For these type of matrices we know from Schur’s theorem (1923) that the eigenvalues \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_d\\) majorise the diagonal elements \\(p_1 \\geq p_2 \\geq \\ldots \\geq p_d\\). More precisely, \\[\n\\sum_{i=1}^m \\lambda_i \\geq \\sum_{i=1}^m p_i \\, ,\n\\] i.e. the sum of the largest \\(m\\) eigenvalues is larger than or equal to the sum of the \\(m\\) largest diagonal elements. The maximum (and equality) is only achieved if the matrix is diagonal, as in this case the diagonal elements are equal to the eigenvalues.\nTherefore, the optimal solution to problem of maximising the relative contributions is obtained by computing the eigendecompositions \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) and \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\) and diagonalise \\(\\boldsymbol \\Phi^T\\boldsymbol \\Phi=\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\) and \\(\\boldsymbol \\Psi^T \\boldsymbol \\Psi=\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\) by setting \\(\\boldsymbol Q_1= \\boldsymbol U^T\\) and \\(\\boldsymbol Q_2= \\boldsymbol G^T\\), respectively. This yields for the maximised contributions \\[\n\\boldsymbol h= \\text{Diag}\\left(\\boldsymbol \\Lambda\\right)= (\\lambda_1, \\ldots, \\lambda_d)^T\n\\] and \\[\n\\boldsymbol k= \\text{Diag}\\left(\\boldsymbol \\Theta\\right) = (\\theta_1, \\ldots, \\theta_d)^T\n\\] with eigenvalues \\(\\lambda_i\\) and \\(\\theta_i\\) arranged in decreasing order.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations and dimension reduction</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#natural-whitening-procedures",
    "href": "03-transformations.html#natural-whitening-procedures",
    "title": "3  Transformations and dimension reduction",
    "section": "3.4 Natural whitening procedures",
    "text": "3.4 Natural whitening procedures\nWe now introduce several strategies (maximise correlation between individual components, maximise compression, structural constraints) to select an optimal whitening procedure.\nSpecifically, we discuss the following whitening transformations:\n\nMahalanobis whitening, also known as ZCA (zero-phase component analysis) whitening in machine learning (based on covariance)\nZCA-cor whitening (based on correlation)\nPCA whitening (based on covariance)\nPCA-cor whitening (based on correlation)\nCholesky whitening\n\nThus, in the following we consider three main types (ZCA, PCA, Cholesky) of whitening.\nIn the following \\(\\boldsymbol x_c = \\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x}\\) and \\(\\boldsymbol z_c = \\boldsymbol z-\\boldsymbol \\mu_{\\boldsymbol z}\\) denote the mean-centered variables.\n\nZCA whitening\nAim: remove correlations and standardise but otherwise make sure that the whitened vector \\(\\boldsymbol z\\) does not differ too much from the original vector \\(\\boldsymbol x\\). Specifically, each latent component \\(z_i\\) should be as close as as possible to the corresponding original variable \\(x_i\\): \\[\n\\begin{array}{cc}\nz_1\\leftrightarrow x_1 \\\\\nz_2\\leftrightarrow x_2\\\\\nz_3\\leftrightarrow x_3 \\\\\n\\ldots\\\\\nz_d\\leftrightarrow x_d \\\\\n\\end{array}\n\\] One possible way to implement this is to compute the expected squared difference between the two centered random vectors \\(\\boldsymbol z_c\\) and \\(\\boldsymbol x_c\\).\nZCA objective function: minimise \\(\\text{E}\\left( || \\boldsymbol x_c - \\boldsymbol z_c ||^2_F   \\right)\\) to find an optimal whitening procedure.\nThe ZCA objective function can be simplified as follows: \\[\n\\begin{split}\n\\text{E}\\left( || \\boldsymbol x_c-\\boldsymbol z_c ||^2_F   \\right)&=\\text{E}\\left( || \\boldsymbol x_c ||^2_F   \\right)  -2 \\text{E}\\left(  \\text{Tr}\\left( \\boldsymbol x_c \\boldsymbol z_c^T \\right) \\right)  + \\text{E}\\left( || \\boldsymbol z_c ||^2_F   \\right) \\\\\n& = \\text{Tr}(  \\text{E}( \\boldsymbol x_c \\boldsymbol x_c^T ) )  - 2 \\text{Tr}( \\text{E}(  \\boldsymbol x_c \\boldsymbol z_c^T ) ) + \\text{Tr}( \\text{E}( \\boldsymbol z_c \\boldsymbol z_c^T ) )\n   \\\\\n& = \\text{Tr}( \\text{Var}(\\boldsymbol x) ) - 2 \\text{Tr}( \\text{Cov}(\\boldsymbol x, \\boldsymbol z) ) +  \\text{Tr}( \\text{Var}(\\boldsymbol z) )  \\\\\n& = \\text{Tr}(\\boldsymbol \\Sigma) - 2\\text{Tr}(\\boldsymbol \\Phi)+ d \\\\\n\\end{split}\n\\] The same objective function can also be obtained by putting a diagonal constraint on the cross-covariance \\(\\boldsymbol \\Phi\\). Specifically, we are looking for the \\(\\boldsymbol \\Phi\\) that is closest to the diagonal matrix \\(\\boldsymbol I\\) by minimising \\[\n\\begin{split}\n|| \\boldsymbol \\Phi- \\boldsymbol I||^2_F &= || \\boldsymbol \\Phi||^2_F  - 2 \\text{Tr}(\\boldsymbol \\Phi^T \\boldsymbol I)  + || \\boldsymbol I||^2_F \\\\\n&= \\text{Tr}(\\boldsymbol \\Sigma) - 2 \\text{Tr}(\\boldsymbol \\Phi) + d \\\\\n\\end{split}\n\\] This will force the off-diagonal elements of \\(\\boldsymbol \\Phi\\) to be close to zero and thus leads to sparsity in the cross-covariance matrix.\nThe only term in the above that depends on the whitening transformation is \\(-2 \\text{Tr}(\\boldsymbol \\Phi)\\) as \\(\\boldsymbol \\Phi\\) is a function of \\(\\boldsymbol Q_1\\). Therefore we can use the following alternative objective:\nZCA equivalent objective: maximise \\(\\text{Tr}(\\boldsymbol \\Phi) = \\text{Tr}(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T)\\) to find the optimal \\(\\boldsymbol Q_1\\)\nSolution:\nFrom the earlier discussion we know that the optimal matrix is \\[\n\\boldsymbol Q_1^{\\text{ZCA}}=\\boldsymbol I\n\\] The corresponding whitening matrix for ZCA is therefore \\[\n\\boldsymbol W^{\\text{ZCA}} = \\boldsymbol \\Sigma^{-1/2}\n\\] and the cross-covariance matrix is \\[\n\\boldsymbol \\Phi^{\\text{ZCA}} = \\boldsymbol \\Sigma^{1/2}\n\\] and the cross-correlation matrix \\[\n\\boldsymbol \\Psi^{\\text{ZCA}} = \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma^{1/2}\n\\]\nNote that \\(\\boldsymbol \\Sigma^{1/2}\\) is a symmetric positive definite matrix, hence its diagonal elements are all positive. As a result, the diagonals of \\(\\boldsymbol \\Phi^{\\text{ZCA}}\\) and \\(\\boldsymbol \\Psi^{\\text{ZCA}}\\) are positive, i.e. \\(\\text{Cov}(x_i, z_i) &gt; 0\\) and \\(\\text{Cor}(x_i, z_i) &gt; 0\\). Hence, for ZCA two corresponding components \\(x_i\\) and \\(z_i\\) are always positively correlated!\nProportion of total variation:\nFor ZCA with \\(\\boldsymbol Q_1=\\boldsymbol I\\) we find that \\(\\boldsymbol h=\\text{Diag}(\\boldsymbol \\Sigma) = \\sum_{j=1}^d \\text{Var}(x_j)\\) with \\(h_i=\\text{Var}(x_i)\\) hence for ZCA the proportion of total variation contributed by the latent component \\(z_i\\) is the ratio \\(\\frac{\\text{Var}(x_i)}{\\sum_{j=1}^d \\text{Var}(x_j)}\\).\nSummary:\n\nZCA/Mahalanobis transform is the unique transformation that minimises the expected total squared component-wise difference between \\(\\boldsymbol x_c\\) and \\(\\boldsymbol z_c\\).\nIn ZCA corresponding components in the whitened and original variables are always positively correlated. This facilitates the interpretation of the whitened variables.\nUse ZCA aka Mahalanobis whitening if you want to “just” remove correlations.\n\n\n\nZCA-Cor whitening\nAim: same as above but remove scale in \\(\\boldsymbol x\\) first before comparing to \\(\\boldsymbol z\\).\nZCA-cor objective function: minimise \\(\\text{E}\\left( || \\boldsymbol V^{-1/2} \\boldsymbol x_c -\\boldsymbol z_c ||^2_F \\right)\\) to find an optimal whitening procedure.\nThis can be simplified as follows: \\[\n\\begin{split}\n\\text{E}\\left( || \\boldsymbol V^{-1/2} \\boldsymbol x_c -\\boldsymbol z_c||^2_F   \\right)&=\\text{E}\\left( || \\boldsymbol V^{-1/2} \\boldsymbol x_c ||^2_F   \\right)  -2 \\text{E}\\left(  \\text{Tr}\\left( \\boldsymbol V^{-1/2} \\boldsymbol x_c \\boldsymbol z_c^T  \\right) \\right) +  \\text{E}\\left( || \\boldsymbol z_c ||^2_F   \\right)\\\\\n& = \\text{Tr}(  \\text{E}(\\boldsymbol V^{-1/2} \\boldsymbol x_c \\boldsymbol x_c^T \\boldsymbol V^{-1/2}) )  \n- 2 \\text{Tr}( \\text{E}( \\boldsymbol V^{-1/2} \\boldsymbol x_c \\boldsymbol z_c^T  ) )\n+\\text{Tr}( \\text{E}( \\boldsymbol z_c \\boldsymbol z_c^T ) )\n  \\\\\n& = \\text{Tr}(  \\text{Cor}(\\boldsymbol x, \\boldsymbol x) ) - 2 \\text{Tr}( \\text{Cor}(\\boldsymbol x, \\boldsymbol z) ) + \\text{Tr}( \\text{Var}(\\boldsymbol z) )   \\\\\n& = d - 2\\text{Tr}(\\boldsymbol \\Psi)+ d \\\\\n& = 2d - 2\\text{Tr}(\\boldsymbol \\Psi)\n\\end{split}\n\\] The same objective function can also be obtained by putting a diagonal constraint on the cross-correlation \\(\\boldsymbol \\Psi\\). Specifically, we are looking for the \\(\\boldsymbol \\Psi\\) that is closest to the diagonal matrix \\(\\boldsymbol I\\) by minimising \\[\n\\begin{split}\n|| \\boldsymbol \\Psi- \\boldsymbol I||^2_F &= || \\boldsymbol \\Psi||^2_F  - 2 \\text{Tr}(\\boldsymbol \\Psi^T \\boldsymbol I)  + || \\boldsymbol I||^2_F \\\\\n&= d - 2 \\text{Tr}(\\boldsymbol \\Psi) + d \\\\\n&= 2 d - 2 \\text{Tr}(\\boldsymbol \\Psi) \\\\\n\\end{split}\n\\] This will force the off-diagonal elements of \\(\\boldsymbol \\Psi\\) to be close to zero and thus leads to sparsity in the cross-correlation matrix.\nThe only term in the above that depends on the whitening transformation is \\(-2 \\text{Tr}(\\boldsymbol \\Psi)\\) as \\(\\boldsymbol \\Psi\\) is a function of \\(\\boldsymbol Q_2\\). Thus we can use the following alternative objective instead:\nZCA-cor equivalent objective: maximise \\(\\text{Tr}(\\boldsymbol \\Psi)=\\text{Tr}(\\boldsymbol P^{1/2} \\boldsymbol Q_2^T)\\) to find optimal \\(\\boldsymbol Q_2\\)\nSolution: same as above for ZCA but using correlation instead of covariance:\nFrom the earlier discussion we know that the optimal matrix is \\[\n\\boldsymbol Q_2^{\\text{ZCA-Cor}}=\\boldsymbol I\n\\] The corresponding whitening matrix for ZCA-cor is therefore \\[\n\\boldsymbol W^{\\text{ZCA-Cor}} = \\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}\n\\] and the cross-covariance matrix is \\[\n\\boldsymbol \\Phi^{\\text{ZCA-Cor}} = \\boldsymbol V^{1/2} \\boldsymbol P^{1/2}\n\\] and the cross-correlation matrix is \\[\n\\boldsymbol \\Psi^{\\text{ZCA-Cor}} = \\boldsymbol P^{1/2}\n\\]\nFor the ZCA-cor transformation we also have \\(\\text{Cov}(x_i, z_i) &gt; 0\\) and \\(\\text{Cor}(x_i, z_i) &gt; 0\\) so that two corresponding components \\(x_i\\) and \\(z_i\\) are always positively correlated!\nProportion of total variation:\nFor ZCA-cor with \\(\\boldsymbol Q_2=\\boldsymbol I\\) we find that \\(\\boldsymbol k=\\text{Diag}(\\boldsymbol P) = d\\) with all \\(k_i =1\\). Thus, in ZCA-cor each whitened component \\(z_i\\) contributes equally to the total variation \\(\\text{Tr}(\\boldsymbol P) =d\\), with relative proportion \\(\\frac{1}{d}\\).\nSummary:\n\nZCA-cor whitening is the unique whitening transformation maximising the total correlation between corresponding elements in \\(\\boldsymbol x\\) and \\(\\boldsymbol z\\).\nZCA-cor leads to interpretable \\(\\boldsymbol z\\) because each individual element in \\(\\boldsymbol z\\) is (typically strongly) positively correlated with the corresponding element in the original \\(\\boldsymbol x\\).\nAs ZCA-cor is explicitly constructed to maximise the total pairwise correlations it achieves higher total correlation than ZCA.\nIf \\(\\boldsymbol x\\) is standardised to \\(\\text{Var}(x_i)=1\\) then ZCA and ZCA-cor are identical.\n\n\n\nPCA whitening\nAim: remove correlations and at the same time compress information into a few latent variables. Specifically, we would like that the first latent component \\(z_1\\) is maximally linked with all variables in \\(\\boldsymbol x\\), followed by the second component \\(z_2\\) and so on:\n\\[\n\\begin{array}{c}\nz_1 \\rightarrow   x_1, x_2, \\ldots, x_d \\\\\nz_2 \\rightarrow   x_1, x_2, \\ldots, x_d \\\\\n\\ldots\\\\\nz_d \\rightarrow   x_1, x_2, \\ldots, x_d \\\\\n\\end{array}\n\\] One way to measure the total association of the latent component \\(z_j\\) with all the original \\(x_1, \\ldots, x_d\\) is the sum of the corresponding squared covariances \\[\nh_j = \\sum^d_{i=1}\\text{Cov}(x_i,z_j)^2 = \\sum^d_{i=1} \\phi_{ij}^2\n\\] or equivalently the column sum of squares of \\(\\boldsymbol \\Phi\\) \\[\n\\boldsymbol h= (h_1,\\ldots,h_d)^T = \\text{Diag}(\\boldsymbol \\Phi^T\\boldsymbol \\Phi) = \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\n\\] Each \\(h_j\\) is the contribution of \\(z_j\\) to \\(\\text{Tr}\\left( \\boldsymbol Q_1 \\boldsymbol \\Sigma\\boldsymbol Q_1^T  \\right)= \\text{Tr}(\\boldsymbol \\Sigma)\\) i.e. to the total variation based on \\(\\boldsymbol \\Sigma\\). As \\(\\text{Tr}(\\boldsymbol \\Sigma)\\) is constant this implies that there are only \\(d-1\\) independent \\(h_j\\).\nIn PCA-whitening we wish to concentrate most of the contributions to the total variation based on \\(\\boldsymbol \\Sigma\\) in a small number of latent components.\nPCA whitening objective function: find an optimal optimal \\(\\boldsymbol Q_1\\) so that the resulting set \\(h_1 \\geq h_2 \\ldots \\geq h_d\\) in \\(\\boldsymbol h= \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\\) majorizes any other set of relative contributions.\nSolution:\nFollowing the earlier discussion we apply Schur’s theorem and find the optimal solution by diagonalising \\(\\boldsymbol \\Phi^T\\boldsymbol \\Phi\\) through eigendecomposition of \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\). Hence, the optimal value for the \\(\\boldsymbol Q_1\\) matrix is \\[\n\\boldsymbol Q_1^{\\text{PCA}}=\\boldsymbol U^T\n\\] However, recall that \\(\\boldsymbol U\\) is not uniquely defined — you are free to change the columns signs. The corresponding whitening matrix is \\[\n\\boldsymbol W^{\\text{PCA}} = \\boldsymbol U^T\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Lambda^{-1/2}\\boldsymbol U^T\n\\]\nand the cross-covariance matrix is \\[\n\\boldsymbol \\Phi^{\\text{PCA}} = \\boldsymbol U\\boldsymbol \\Lambda^{1/2}\n\\] and the cross-correlation matrix is \\[\n\\boldsymbol \\Psi^{\\text{PCA}} = \\boldsymbol V^{-1/2} \\boldsymbol U\\boldsymbol \\Lambda^{1/2}\n\\]\nIdentifiability:\nNote that all of the above (i.e. \\(\\boldsymbol Q_1^{\\text{PCA}}, \\boldsymbol W^{\\text{PCA}}, \\boldsymbol \\Phi^{\\text{PCA}}, \\boldsymbol \\Psi^{\\text{PCA}}\\)) is not unique due to the sign ambiguity in the columns of \\(\\boldsymbol U\\).\nTherefore, for identifiability reasons we may wish to impose a further constraint on \\(\\boldsymbol Q_1^{\\text{PCA}}\\) or equivalently \\(\\boldsymbol \\Phi^{\\text{PCA}}\\). A useful condition is to require (for the given ordering of the original variables!) that \\(\\boldsymbol Q_1^{\\text{PCA}}\\) has a positive diagonal or equivalently that \\(\\boldsymbol \\Phi^{\\text{PCA}}\\) has a positive diagonal. This implies that \\(\\text{Diag}(\\boldsymbol U) &gt; 0\\) and \\(\\text{Diag}(\\boldsymbol \\Psi^{\\text{PCA}}) &gt; 0\\), hence all pairs \\(x_i\\) and \\(z_i\\) are positively correlated.\nIt is particularly important to pay attention to the sign ambiguity when comparing different computer implementations of PCA whitening (and the related PCA approach).\nNote that the actual objective of PCA whitening \\(\\text{Diag}(\\boldsymbol \\Phi^T\\boldsymbol \\Phi)\\) is not affected by the sign ambiguity since the column signs of \\(\\boldsymbol \\Phi\\) do not matter.\nProportion of total variation:\nIn PCA whitening the contribution \\(h_i^{\\text{PCA}}\\) of each latent component \\(z_i\\) to the total variation based on the covariance \\(\\text{Tr}(\\boldsymbol \\Sigma) = \\sum_{j=1}^d \\lambda_j\\) is \\(h_i^{\\text{PCA}} = \\lambda_i\\). The fraction \\(\\frac{\\lambda_i}{\\sum^d_{j=1}\\lambda_j}\\) is the relative contribution of each element in \\(\\boldsymbol z\\) to explain the total variation.\nThus, low ranking components \\(z_i\\) with small \\(h_i^{\\text{PCA}}=\\lambda_i\\) may be discarded. In this way PCA whitening achieves both compression and dimension reduction.\nSummary:\n\nPCA whitening is a whitening transformation that maximises compression with the sum of squared cross-covariances as underlying optimality criterion.\nThere are sign ambiguities in the PCA whitened variables which are inherited from the sign ambiguities in eigenvectors.\nIf a positive-diagonal condition on the orthogonal matrices is imposed then these sign ambiguities are fully resolved and corresponding components \\(z_i\\) and \\(x_i\\) are always positively correlated.\n\n\n\nPCA-cor whitening\nAim: same as for PCA whitening but remove scale in \\(\\boldsymbol x\\) first. This means we use squared correlations rather than squared covariances to measure compression, i.e.\n\\[\nk_j = \\sum^d_{i=1}\\text{Cor}(x_i, z_j)^2 = \\sum^d_{i=1} \\psi_{ij}^2\n\\] or in vector notation the column sum of squares of \\(\\boldsymbol \\Psi\\) \\[\n\\boldsymbol k= (k_1,\\ldots,k_d)^T = \\text{Diag}\\left(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\right)=\\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\n\\] Each \\(k_j\\) is the contribution of \\(z_j\\) to \\(\\text{Tr}\\left( \\boldsymbol Q_2 \\boldsymbol P\\boldsymbol Q_2^T  \\right)= \\text{Tr}(\\boldsymbol P) = d\\) i.e. the total variation based on \\(\\boldsymbol P\\). As \\(\\text{Tr}(\\boldsymbol P)=d\\) is constant this implies that there are only \\(d-1\\) independent \\(k_j\\).\nIn PCA-cor-whitening we wish to concentrate most of the contributions to the total variation based on \\(\\boldsymbol P\\) in a small number of latent components.\nPCA-cor whitening objective function: find an optimal optimal \\(\\boldsymbol Q_2\\) so that the resulting set \\(k_1 \\geq k_2 \\ldots \\geq k_d\\) in \\(\\boldsymbol k= \\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\\) majorizes any other set of relative contributions.\nSolution:\nFollowing the earlier discussion we apply Schur’s theorem and find the optimal solution by diagonalising \\(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\) through eigendecomposition of \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\). Hence, the optimal value for the \\(\\boldsymbol Q_2\\) matrix is \\[\n\\boldsymbol Q_2^{\\text{PCA-Cor}}=\\boldsymbol G^T\n\\] Again \\(\\boldsymbol G\\) is not uniquely defined — you are free to change signs of the columns. The corresponding whitening matrix is \\[\n\\boldsymbol Q_2^{\\text{PCA-Cor}}=\\boldsymbol G^T\n\\] The corresponding whitening matrix is\n\\[\n\\boldsymbol W^{\\text{PCA-Cor}} = \\boldsymbol \\Theta^{-1/2} \\boldsymbol G^T \\boldsymbol V^{-1/2}\n\\]\nand the cross-covariance matrix is \\[\n\\boldsymbol \\Phi^{\\text{PCA-Cor}} = \\boldsymbol V^{1/2} \\boldsymbol G\\boldsymbol \\Theta^{1/2}\n\\] and the cross-correlation matrix is \\[\n\\boldsymbol \\Psi^{\\text{PCA-Cor}} = \\boldsymbol G\\boldsymbol \\Theta^{1/2}\n\\]\nIdentifiability:\nAs with PCA whitening, there are sign ambiguities in the above because the column signs of \\(\\boldsymbol G\\) can be freely chosen. For identifiability we may wish to impose further constraints on \\(\\boldsymbol Q_2^{\\text{PCA-Cor}}\\) or equivalently on \\(\\boldsymbol \\Psi^{\\text{PCA-Cor}}\\). A useful condition is to require (for the given ordering of the original variables!) that the diagonal elements of \\(\\boldsymbol Q_2^{\\text{PCA-Cor}}\\) are all positive or equivalently that \\(\\boldsymbol \\Psi^{\\text{PCA-Cor}}\\) has a positive diagonal. This implies that \\(\\text{Diag}(\\boldsymbol G) &gt; 0\\) and \\(\\text{Diag}(\\boldsymbol \\Phi^{\\text{PCA-Cor}}) &gt; 0\\).\nNote that the actual objective of PCA-cor whitening \\(\\text{Diag}(\\boldsymbol \\Psi^T\\boldsymbol \\Psi)\\) is not affected by the sign ambiguity since the column signs of \\(\\boldsymbol \\Psi\\) do not matter.\nProportion of total variation:\nIn PCA-cor whitening the contribution \\(k_i^{\\text{PCA-Cor}}\\) of each latent component \\(z_i\\) to the total variation based on the correlation \\(\\text{Tr}(\\boldsymbol P) = d\\) is \\(k_i^{\\text{PCA-Cor}} = \\theta_i\\). The fraction \\(\\frac{\\theta_i}{d}\\) is the relative contribution of each element in \\(\\boldsymbol z\\) to explain the total variation.\nSummary:\n\nPCA-cor whitening is a whitening transformation that maximises compression with the sum of squared cross-correlations as underlying optimality criterion.\nThere are sign ambiguities in the PCA-cor whitened variables which are inherited from the sign ambiguities in the eigenvectors.\nIf a positive-diagonal condition on the orthogonal matrices is imposed then these sign ambiguities are fully resolved and corresponding components \\(z_i\\) and \\(x_i\\) are always positively correlated.\nIf \\(\\boldsymbol x\\) is standardised to \\(\\text{Var}(x_i)=1\\), then PCA and PCA-cor whitening are identical.\n\n\n\nCholesky whitening\nCholesky matrix decomposition:\nThe Cholesky decomposition of a square matrix \\(\\boldsymbol A= \\boldsymbol L\\boldsymbol L^T\\) requires a positive definite \\(\\boldsymbol A\\) and is unique. \\(\\boldsymbol L\\) is a lower triangular matrix with positive diagonal elements. Its inverse \\(\\boldsymbol L^{-1}\\) is also lower triangular with positive diagonal elements. If \\(\\boldsymbol D\\) is a diagonal matrix with positive elements then \\(\\boldsymbol D\\boldsymbol L\\) is also a lower triangular matrix with a positive diagonal and the Cholesky factor for the matrix \\(\\boldsymbol D\\boldsymbol A\\boldsymbol D\\).\nAim in Cholesky whitening:\nFind a whitening transformation such that the cross-covariance \\(\\boldsymbol \\Phi\\) and cross-correlation \\(\\boldsymbol \\Psi\\) have lower triangular structure. Specifically, we wish that the the first whitened variable \\(z_1\\) is linked to all original variables \\(x_1, \\ldots, x_d\\), the second latent variable \\(z_2\\) is linked to \\(x_2, \\ldots, x_d\\), and so on, and the last variable \\(z_d\\) is linked only to \\(x_d\\). \\[\n\\begin{array}{cc}\nz_1 \\rightarrow &  x_1, x_2, x_3, \\ldots, x_d \\\\\nz_2 \\rightarrow &  x_2, x_3, \\ldots, x_d \\\\\nz_3 \\rightarrow &  x_3, \\ldots, x_d \\\\\n\\ldots\\\\\nz_d \\rightarrow & x_d \\\\\n\\end{array}\n\\] We also assume that \\(\\text{Cor}(x_i, z_i)&gt; 0\\), i.e. that the cross-correlations between corresponding pairs of original and whitened variables are positive. This requirement of a positive diagonal \\(\\boldsymbol \\Psi\\) ensures the uniqueness of the whitening transformation (similar as in PCA whitening above).\nThe Cholesky whitening procedure can be viewed as a middle ground between ZCA whitening and PCA whitening.\nSolution: In order to find such a whitening transformation we use the Cholesky decomposition and apply it to the covariance matrix \\(\\boldsymbol \\Sigma= \\boldsymbol L\\boldsymbol L^T\\)\nThe resulting whitening matrix is \\[\n\\boldsymbol W^{\\text{Chol}}=\\boldsymbol L^{-1}\n\\]\nBy construction, \\(\\boldsymbol W^{\\text{Chol}}\\) is a lower triangular matrix with positive diagonal. The whitening constraint is satisfied as \\((\\boldsymbol W^{\\text{Chol}})^T\\boldsymbol W^{\\text{Chol}} = (\\boldsymbol L^{-1})^T \\boldsymbol L^{-1} = (\\boldsymbol L^T)^{-1}  \\boldsymbol L^{-1} = (\\boldsymbol L\\boldsymbol L^T)^{-1} = \\boldsymbol \\Sigma^{-1}\\).\nThe cross-covariance matrix is the inverse of the whitening matrix \\[\n\\boldsymbol \\Phi^{\\text{Chol}} = \\boldsymbol L\n\\] and the cross-correlation matrix is \\[\n\\boldsymbol \\Psi^{\\text{Chol}} = \\boldsymbol V^{-1/2} \\boldsymbol L\n\\] Both \\(\\boldsymbol \\Phi^{\\text{Chol}}\\) and \\(\\boldsymbol \\Psi^{\\text{Chol}}\\) are lower triangular matrices with positive diagonal elements. Hence two corresponding components \\(x_i\\) and \\(z_i\\) are always positively correlated!\nFinally, the corresponding orthogonal matrices are \\[\n\\boldsymbol Q_1^{\\text{Chol}}  =  \\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1/2} =   \\boldsymbol L^T \\boldsymbol \\Sigma^{-1/2}\n\\] and \\[\n\\boldsymbol Q_2^{\\text{Chol}} =  \\boldsymbol \\Psi^T \\boldsymbol P^{-1/2} =  \\boldsymbol L^T \\boldsymbol V^{-1/2} \\boldsymbol P^{-1/2}\n\\]\nApplication to correlation instead of covariance:\nWe may also apply the Cholesky decomposition to the correlation rather than the covariance matrix. However, unlike for ZCA and PCA this does not lead to a different whitening transform:\nLet’s denote the Cholesky composition of the correlation matrix by \\(\\boldsymbol P= \\boldsymbol L_P \\boldsymbol L_P^T\\). Then the corresponding whitening matrix is \\(\\boldsymbol W^{\\text{Chol}}_P= \\boldsymbol L_P^{-1} \\boldsymbol V^{-1/2}\\). As \\(\\boldsymbol P= \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2} = \\boldsymbol V^{-1/2} \\boldsymbol L\\boldsymbol L^T \\boldsymbol V^{-1/2}\\) we see that \\(\\boldsymbol L_P = \\boldsymbol V^{-1/2} \\boldsymbol L\\) and hence \\(\\boldsymbol W^{\\text{Chol}}_P = (\\boldsymbol V^{-1/2} \\boldsymbol L)^{-1} \\boldsymbol V^{-1/2} =\\boldsymbol L^{-1} =  \\boldsymbol W^{\\text{Chol}}\\).\nDependence on the input order:\nCholesky whitening depends on the ordering of input variables. Each ordering of the original variables will yield a different triangular constraint and thus a different Cholesky whitening transform. For example, by inverting the ordering to \\(x_d, x_{d-1}, \\ldots, x_1\\) we effectively enforce an upper triangular shape.\nAn alternative formulation of Cholesky whitening decomposes the precision matrix rather than the covariance matrix. This yields the upper triangular structure directly and is otherwise fully equivalent to Cholesky whitening based on decomposing the covariance.\n\n\nComparison of whitening procedures - simulated data\nFor comparison, Figure 3.1 shows the results of ZCA, PCA and Cholesky whitening applied to a simulated bivariate normal data set with correlation \\(\\rho=0.8\\).\n\n\n\n\n\n\n\n\nFigure 3.1: Comparison of ZCA, PCA and Cholesky whitening procedures.\n\n\n\n\n\nIn column 1 you can see the simulated data as scatter plot.\nColumn 2 shows the scatter plots of the whitened data — as expect all three methods remove correlation and produce an isotropic covariance.\nHowever, the three approaches differ in the cross-correlations. Columns 3 and 4 show the cross-correlations between the first two corresponding components (\\(x_1\\) and \\(z_1\\), and \\(x_2\\) and \\(z_2\\)) for ZCA, PCA and Cholesky whitening. As expected, in ZCA both pairs show strong correlation, but this is not the case for PCA and Cholesky whitening.\nNote that for Cholesky whitening the first component \\(z_1\\) is perfectly positively correlated with the original component \\(x_1\\) because the whitening matrix is lower triangular with a positive diagonal and hence \\(z_1\\) is just \\(x_1\\) multiplied with a positive constant.\n\n\nComparison of whitening procedures - iris flowers\nAs an example we consider the well known iris flower data set. It consists of botanical measures (sepal length, sepal width, petal length and petal width) for 150 iris flowers comprising three species (Iris setosa, Iris versicolor, Iris virginica). Hence this data set has dimension \\(d=4\\) and sample size \\(n=150\\).\nWe apply all discussed whitening transforms to this data, and then sort the whitened components by their relative contribution to the total variation. For Cholesky whitening we used the input order for the shape constraint.\nFigure 3.2 shows the results for explained variation based on covariance loadings:\n\n\n\n\n\n\n\n\nFigure 3.2: Cumulative explained variation (covariance loadings) for various whitening procedures for the Iris flower data.\n\n\n\n\n\nAs expected, the two PCA whitening approaches compress the data most. On the other end of the spectrum, the ZCA whitening methods are the two least compressing approaches. Cholesky whitening is a compromise between ZCA and PCA in terms of compression.\nSimilar results are obtained based on correlation loadings (Figure 3.3). Note how ZCA-cor provides equal weight for each latent variable.\n\n\n\n\n\n\n\n\nFigure 3.3: Cumulative explained variation (correlation-loadings) for various whitening procedures for the Iris flower data.\n\n\n\n\n\n\n\nRecap\nSee Table 3.1 for a summery of the usuage types for the various whitening procedures.\nIf data are standardised then \\(\\boldsymbol \\Phi\\) and \\(\\boldsymbol \\Psi\\) will be the same and hence ZCA will become ZCA-cor and PCA becomes PCA-cor. The triangular shape constraint of Cholesky whitening depends on the ordering of the original variables.\n\n\n\nTable 3.1: Applications of natural whitening procedures.\n\n\n\n\n\n\n\n\n\nMethod\nType of usage\n\n\n\n\nZCA, ZCA-cor:\npure decorrelate, maintain similarity to original data set, interpretability\n\n\nPCA, PCA-cor:\ncompression, find effective dimension, reduce dimensionality, feature identification\n\n\nCholesky\ntriangular shaped \\(\\boldsymbol W\\), \\(\\boldsymbol \\Phi\\) and \\(\\boldsymbol \\Psi\\), sparsity",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations and dimension reduction</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#principal-component-analysis-pca",
    "href": "03-transformations.html#principal-component-analysis-pca",
    "title": "3  Transformations and dimension reduction",
    "section": "3.5 Principal Component Analysis (PCA)",
    "text": "3.5 Principal Component Analysis (PCA)\n\nPCA transformation\nPrincipal component analysis was proposed in 1933 by Harald Hotelling 4 and is very closely related to PCA whitening. The underlying mathematics was developed earlier in 1901 by Karl Pearson 5 for the problem of orthogonal regression.\nAssume random vector \\(\\boldsymbol x\\) with \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\). PCA is a particular orthogonal transformation (Example 3.5) of the original \\(\\boldsymbol x\\) such that the resulting components are orthogonal: \\[\n\\underbrace{\\boldsymbol t^{\\text{PCA}}}_{\\text{Principal components}} = \\underbrace{\\boldsymbol U^T}_{\\text{Orthogonal matrix}}   \\boldsymbol x\n\\] \\[\\text{Var}(\\boldsymbol t^{\\text{PCA}}) = \\boldsymbol \\Lambda= \\begin{pmatrix} \\lambda_1 & \\dots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & \\lambda_d\\end{pmatrix}\\] Note that while principal components are orthogonal they do not have unit variance. Instead, the variance of the principal components \\(t_i\\) is equal to the eigenvalues \\(\\lambda_i\\).\nThus PCA itself is not a whitening procedure but it is very closely linked to PCA whitening which is obtained by standardising the principal components to unit variance: \\(\\boldsymbol z^{\\text{PCA}} = \\boldsymbol \\Lambda^{-1/2} \\boldsymbol t^{\\text{PCA}} = \\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T \\boldsymbol x\n= \\boldsymbol U^T \\boldsymbol \\Sigma^{-1/2} \\boldsymbol x= \\boldsymbol Q_1^{\\text{PCA}} \\boldsymbol \\Sigma^{-1/2} \\boldsymbol x= \\boldsymbol W^{\\text{PCA}} \\boldsymbol x\\)\nCompression properties:\nThe total variation is \\(\\text{Tr}(\\text{Var}(\\boldsymbol t^{\\text{PCA}})) = \\text{Tr}( \\boldsymbol \\Lambda) = \\sum^d_{j=1}\\lambda_j\\). With principle components the fraction \\(\\frac{\\lambda_i}{\\sum^d_{j=1}\\lambda_j}\\) can be interpreted as the proportion of variation contributed by each component in \\(\\boldsymbol t^{\\text{PCA}}\\) to the total variation. Thus, low ranking components in \\(\\boldsymbol t^{\\text{PCA}}\\) with low variation may be discarded, thus leading to a reduction in dimension.\n\n\nApplication to data\nWritten in terms of a data matrix \\(\\boldsymbol X\\) instead of a random vector \\(\\boldsymbol x\\) PCA becomes: \\[\\underbrace{\\boldsymbol T}_{\\text{Sample version of principal components}}=\\underbrace{\\boldsymbol X}_{\\text{Data matrix}}\\boldsymbol U\\] There are now two ways to obtain \\(\\boldsymbol U\\):\n\nEstimate the covariance matrix, e.g. by \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c\\) where \\(\\boldsymbol X_c\\) is the column-centred data matrix; then apply the eigenvalue decomposition on \\(\\hat{\\boldsymbol \\Sigma}\\) to get \\(\\boldsymbol U\\).\nCompute the singular value decomposition of \\(\\boldsymbol X_c = \\boldsymbol V\\boldsymbol D\\boldsymbol U^T\\). As \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c = \\boldsymbol U(\\frac{1}{n}\\boldsymbol D^2)\\boldsymbol U^T\\) you can just use \\(\\boldsymbol U\\) from the SVD of \\(\\boldsymbol X_c\\) and there is no need to compute the covariance.\n\n\n\nIris flower data example\nWe first standardise the data, then compute PCA components and plot the proportion of total variation contributed by each component. Figure 3.4 shows that only two PCA components are needed to achieve 95% of the total variation.\n\n\n\n\n\n\n\n\nFigure 3.4: PCA proportion of total variation for the iris flower data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.5: Scatter plot of first two principal components.\n\n\n\n\n\nA scatter plot plot of the the first two principal components is also informative (Figure 3.5). Specifically, it shows that there are groupings among the 150 iris flowers, corresponding to the three known species, and that these three groups can be characterised by looking at just the first two principal components (rather than at all four components).\n\n\nPCA correlation loadings\nIn an earlier section we have learned that for a general whitening transformation the cross-correlations \\(\\boldsymbol \\Psi=\\text{Cor}(\\boldsymbol x, \\boldsymbol z)\\) play the role of correlation loadings in the inverse transformation: \\[\n\\boldsymbol V^{-1/2} \\boldsymbol x= \\boldsymbol \\Psi\\boldsymbol z\\, ,\n\\] i.e. they are the coefficients linking the whitened variable \\(\\boldsymbol z\\) with the standardised original variable \\(\\boldsymbol x\\). This relationship holds therefore also for PCA-whitening with \\(\\boldsymbol z^{\\text{PCA}}= \\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T \\boldsymbol x\\) and \\(\\boldsymbol \\Psi^{\\text{PCA}} = \\boldsymbol V^{-1/2} \\boldsymbol U\\boldsymbol \\Lambda^{1/2}\\).\nThe classical PCA is not a whitening approach because \\(\\text{Var}(\\boldsymbol t^{\\text{PCA}}) \\neq \\boldsymbol I\\). However, we can still compute cross-correlations between \\(\\boldsymbol x\\) and the principal components \\(\\boldsymbol t^{\\text{PCA}}\\), resulting in \\[\n\\text{Cor}(\\boldsymbol x, \\boldsymbol t^{\\text{PCA}}) = \\boldsymbol V^{-1/2} \\boldsymbol U\\boldsymbol \\Lambda^{1/2}  = \\boldsymbol \\Psi^{\\text{PCA}}\n\\] Note these are the same as the cross-correlations for PCA-whitening since \\(\\boldsymbol t^{\\text{PCA}}\\) and \\(\\boldsymbol z^{\\text{PCA}}\\) only differ in scale.\nThe inverse PCA transformation is \\[\n\\boldsymbol x= \\boldsymbol U\\boldsymbol t^{\\text{PCA}}\n\\] In terms of standardised PCA components \\(\\boldsymbol z^{\\text{PCA}} = \\boldsymbol \\Lambda^{-1/2} \\boldsymbol t^{\\text{PCA}}\\) and standardised original components it becomes \\[\n\\boldsymbol V^{-1/2} \\boldsymbol x= \\boldsymbol \\Psi\\boldsymbol \\Lambda^{-1/2} \\boldsymbol t^{\\text{PCA}}\n\\] Thus the cross-correlation matrix \\(\\boldsymbol \\Psi\\) plays the role of correlation loadings also in classical PCA, i.e. they are the coefficients linking the standardised PCA components with the standardised original components.\n\n\nPCA correlation loadings plot\nIn PCA and PCA-cor whitening as well as in classical PCA the aim is compression, i.e. to find latent variables such that most of the total variation is contributed by a small number of components.\nIn order to be able to better interpret the top ranking PCA component we can use a visual device called correlation loadings plot. For this we compute the correlation between the PCA components 1 and 2 (\\(t_1^{\\text{PCA}}\\) and \\(t_2^{\\text{PCA}})\\) with all original variables \\(x_1, \\ldots, x_d\\).\nFor each original variable \\(x_i\\) we therefore have two numbers between -1 and 1, the correlation \\(\\text{Cor}(x_i, t_1^{\\text{PCA}}) = \\psi_{i1}\\) and \\(\\text{Cor}(x_i, t_2^{\\text{PCA}}) = \\psi_{i2}\\) that we use as coordinates to draw a point in a plane. Recall that the row sums of squares of the correlation loadings \\(\\boldsymbol \\Psi\\) are all identical to 1. Hence, the sum of the squared loadings from just the first two components is also at most 1. Thus, by construction, all points have to lie within a unit circle around the origin.\nThe original variables most strongly influenced by the two latent variables will have strong correlation and thus lie near the outer circle, whereas variables that are not influenced by the two latent variables will lie near the origin.\n\n\n\n\n\n\n\n\nFigure 3.6: Correlation loadings plot between the first two principal correlations and the original variables.\n\n\n\n\n\nFor illustration Figure 3.6 shows the correlation loadings plot for the correlation between the first two PCA components and all four variables of the iris flower data set discussed earlier.\nThe interpretation of this plot is discussed in Worksheet 5.\n\n\nOutlook\nRelated methods not discussed in this course:\n\nFactor models: essentially this is a probabilistic version of whitening / PCA with dimension reduction and an additional error term. Factors have rotational freedom exactly as whitened variables.\nPartial Least Squares (PLS): similar to Principal Components Analysis (PCA) but in a regression setting, with the choice of latent variables depending both on predictors and on the response variable. One can also use PCA with regression (yielding principal components regression, PCR) but in this case the PCA components only depend on the predictor variables.\nNonlinear dimension reduction methods such as SNE, tSNE and UMAP.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations and dimension reduction</span>"
    ]
  },
  {
    "objectID": "03-transformations.html#footnotes",
    "href": "03-transformations.html#footnotes",
    "title": "3  Transformations and dimension reduction",
    "section": "",
    "text": "Gorroochurn, P. 2020. Who Invented the Delta Method, Really? The Mathematical Intelligencer 42:46–49. https://doi.org/10.1007/s00283-020-09982-0↩︎\nKobyzev et al. 2021. Normalizing Flows: Introduction and Ideas. IEEE Trans. Pattern Anal. Mach. Intell. 43:3964-3979↩︎\nPapamakarios et al. 2021. Normalizing Flows for Probabilistic Modeling and Inference.\nJMLR 22:1-64↩︎\nHotelling, H. 1933. Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology 24:417–441 (Part 1) and 24:498–520 (Part 2). https://doi.org/10.1037/h0071325 and https://doi.org/10.1037/h0070888↩︎\nPearson, K. 1901. On lines and planes of closest fit to systems of points in space. Philosophical Magazine 2:559–572. https://doi.org/10.1080/14786440109462720↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformations and dimension reduction</span>"
    ]
  },
  {
    "objectID": "04-clustering.html",
    "href": "04-clustering.html",
    "title": "4  Unsupervised learning and clustering",
    "section": "",
    "text": "4.1 Challenges in unsupervised learning",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unsupervised learning and clustering</span>"
    ]
  },
  {
    "objectID": "04-clustering.html#challenges-in-unsupervised-learning",
    "href": "04-clustering.html#challenges-in-unsupervised-learning",
    "title": "4  Unsupervised learning and clustering",
    "section": "",
    "text": "Objective\nWe observe data \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) for \\(n\\) objects (or subjects). Each sample \\(\\boldsymbol x_i\\) is a vector of dimension \\(d\\). Thus, for each of the \\(n\\) objects / subjects we have measurements on \\(d\\) variables. The aim of unsupervised learning is to identify patters relating the objects/subjects based on the information available in \\(\\boldsymbol x_i\\). Note that in unsupervised learning we use only the information in \\(\\boldsymbol x_i\\) and nothing else.\n\n\n\n\n\n\n\n\nFigure 4.1: Principal components scatter plot for the iris flower data.\n\n\n\n\n\nFor illustration (Figure 4.1) consider the first two principal components of the Iris flower data (see e.g. Worksheet 5). Clearly there is a group structure int the data that is linked to particular patterns in the first two principal components.\nNote that in this plot we have used additional information, the class labels (setosa, versicolor, virginica), to highlight the true underlying structure (the three flower species).\nIn unsupervised learning the class labels are (assumed to be) unknown, and the aim is to infer the clustering and thus the classes labels. 1\nThere are many methods for clustering and unsupervise learning, both purely algorithmic as well as probabilistic. In this chapter we will study a few of the most commonly used approaches.\n\n\nQuestions and problems\nIn order to implement unsupervised learning we need to address a number of questions:\n\nhow do we define clusters?\nhow do we learn / infer clusters?\nhow many clusters are there? (this is surprisingly difficult!)\nhow can we assess the uncertainty of clusters?\n\nOnce we know the clusters we are also interested in:\n\nwhich features define / separate each cluster?\n\n(note this is a feature / variable selection problem, discussed in in supervised learning).\nMany of these problems and questions are highly specific to the data at hand. Correspondingly, there are many different types of methods and models for clustering and unsupervised learning.\nIn terms of representing the data, unsupervised learning tries to balance between the following two extremes:\n\nall objects are grouped into a single cluster (low complexity model)\nall objects are put into their own cluster (high complexity model)\n\nIn practise, the aim is to find a compromise, i.e. a model that captures the structure in the data with appropriate complexity — not too low and not too complex.\n\n\nWhy is clustering difficult?\nPartioning problem (combinatorics): How many partitions of \\(n\\) objects (say flowers) into \\(K\\) groups (say species) exists?\nAnswer:\n\\[\nS(n,K) = \\left\\{\\begin{array}{l} n \\\\ K \\end{array} \\right\\}\n\\] this is the “Sterling number of the second type”.\nFor large n: \\[\nS(n,K) \\approx \\frac{K^n }{ K!}\n\\]\n\n\n\nTable 4.1: Number of ways to partition \\(n\\) samples into \\(K\\) groups.\n\n\n\n\n\n\\(n\\)\n\\(K\\)\nNumber of possible partitions\n\n\n\n\n15\n3\n\\(\\approx\\) 2.4 million (\\(10^6\\))\n\n\n20\n4\n\\(\\approx\\) 2.4 billion (\\(10^9\\))\n\n\n\\(\\vdots\\)\n\n\n\n\n100\n5\n\\(\\approx 6.6 \\times 10^{76}\\)\n\n\n\n\n\n\nThese are enormously large numbers even for relatively small problems!\n\\(\\Longrightarrow\\) Clustering / partitioning / structure discovery is not easy!\n\\(\\Longrightarrow\\) We cannot expect perfect answers or a single “true” clustering\nIn fact, as a model of the data many differnt clusterings may fit the data equally well.\n\\(\\Longrightarrow\\) We need to assesse the uncertainty of the clustering\nThis can be done as part of probabilistic modelling or by resampling (e.g., bootstrap).\n\n\nCommon types of clustering methods\nThere are very many different clustering algorithms!\nWe consider the following two broad types of methods:\n\nAlgorithmic clustering methods (these are not explicitly based on a probabilistic model)\n\n\n\\(K\\)-means\nPAM\nhierarchical clustering (distance or similarity-based, divise and agglomerative)\n\n\npros: fast, effective algorithms to find at least some grouping cons: no probabilistic interpretation, blackbox methods\n\n\nModel-based clustering (based on a probabilistic model)\n\n\nmixture models (e.g. Gaussian mixture models, GMMs, non-hierarchical)\ngraphical models (e.g. Bayesian networks, Gaussian graphical models GGM, trees and networks)\n\n\npros: full probabilistic model with all corresponding advantages cons: computationally very expensive, sometimes impossible to compute exactly.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unsupervised learning and clustering</span>"
    ]
  },
  {
    "objectID": "04-clustering.html#hierarchical-clustering",
    "href": "04-clustering.html#hierarchical-clustering",
    "title": "4  Unsupervised learning and clustering",
    "section": "4.2 Hierarchical clustering",
    "text": "4.2 Hierarchical clustering\n\nTree-like structures\nOften, categorisations of objects are nested, i.e. there sub-categories of categories etc. These can be naturally represented by tree-like hierarchical structures.\nIn many branches of science hierarchical clusterings are widely employed, for example in evolutionary biology: see e.g. \n\nTree of Life explaining the biodiversity of life\nphylogenetic trees among species (e.g. vertebrata)\npopulation genetic trees to describe human evolution\ntaxonomic trees for plant species\netc.\n\nNote that when visualising hierarchical structures typically the corresponding tree is depicted facing downwards, i.e. the root of the tree is shown on the top, and the tips/leaves of the tree are shown at the bottom!\nIn order to obtain such a hierarchical clustering from data two opposing strategies are commonly used:\n\ndivisive or recursive partitioning algorithms\n\ngrow the tree from the root downwards\nfirst determine the main two clusters, then recursively refine the clusters further.\n\nagglomerative algorithms\n\ngrow the tree from the leaves upwards\nsuccessively form partitions by first joining individual object together, then recursively join groups of items together, until all is merged.\n\n\nIn the following we discuss a number of popular hierarchical agglomerative clustering algorithms that are based on the pairwise distances / similarities (a \\(n \\times n\\) matrix) among all data points.\n\n\nAgglomerative hierarchical clustering algorithms\nA general algorithm for agglomerative construction of a hierarchical clustering works as follows:\nInitialisation:\nCompute a dissimilarity / distance matrix between all pairs of objects where “objects” are single data points at this stage but later are also be sets of data points.\nIterative procedure:\n\nidentify the pair of objects with the smallest distance. These two objects are then merged together into one set. Create an internal node in the tree to represent this set.\nupdate the distance matrix by computing the distances between the new set and all other objects. If the new set contains all data points the procedure terminates. The final node created is the root node.\n\nFor actual implementation of this algorithm two key ingredients are needed:\n\na distance measure \\(d(\\boldsymbol a, \\boldsymbol b)\\) between two individual elementary data points \\(\\boldsymbol a\\) and \\(\\boldsymbol b\\).\n\n\nThis is typically one of the following:\n\n\n\nEuclidean distance \\(d(\\boldsymbol a, \\boldsymbol b) = \\sqrt{\\sum_{i=1}^d ( a_i-b_i )^2} = \\sqrt{(\\boldsymbol a-\\boldsymbol b)^T (\\boldsymbol a-\\boldsymbol b)}\\)\nSquared Euclidean distance \\(d(\\boldsymbol a, \\boldsymbol b) = (\\boldsymbol a-\\boldsymbol b)^T (\\boldsymbol a-\\boldsymbol b)\\)\nManhattan distance \\(d(\\boldsymbol a, \\boldsymbol b) = \\sum_{i=1}^d | a_i-b_i |\\)\nMaximum norm \\(d(\\boldsymbol a, \\boldsymbol b) = \\underset{i \\in \\{1, \\ldots, d\\}}{\\max} | a_i-b_i |\\)\n\n\n\nIn the end, making the correct choice of distance will require subject knowledge about the data!\n\n\na distance measure \\(d(A, B)\\) between two sets of objects \\(A=\\{\\boldsymbol a_1, \\boldsymbol a_2, \\ldots,  \\boldsymbol a_{n_A} \\}\\) and \\(B=\\{\\boldsymbol b_1, \\boldsymbol b_2, \\ldots, \\boldsymbol b_{n_B}\\}\\) of size \\(n_A\\) and \\(n_B\\), respectively.\n\n\nTo determine the distance \\(d(A, B)\\) between these two sets the following measures are often employed:\n\n\n\ncomplete linkage (max. distance): \\(d(A, B) = \\underset{\\boldsymbol a_i \\in A, \\boldsymbol b_i \\in B}{\\max} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)\nsingle linkage (min. distance): \\(d(A, B) = \\underset{\\boldsymbol a_i \\in A, \\boldsymbol b_i \\in B}{\\min} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)\naverage linkage (avg. distance): \\(d(A, B) = \\frac{1}{n_A n_B} \\sum_{\\boldsymbol a_i \\in A} \\sum_{\\boldsymbol b_i \\in B} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)\n\n\n\n\nWard’s clustering method\nAnother agglomerative hierarchical procedure is Ward’s minimum variance approach 2 (see also 3). In this approach in each iteration the two sets \\(A\\) and \\(B\\) are merged that lead to the smallest increase in within-group variation. The centroids of the two sets is given by \\(\\boldsymbol \\mu_A = \\frac{1}{n_A} \\sum_{\\boldsymbol a_i \\in A} \\boldsymbol a_i\\) and \\(\\boldsymbol \\mu_B = \\frac{1}{n_B} \\sum_{\\boldsymbol b_i \\in B} \\boldsymbol b_i\\).\nThe within-group sum of squares for group \\(A\\) is \\[\nw_A = \\sum_{\\boldsymbol a_i \\in A} (\\boldsymbol a_i -\\boldsymbol \\mu_A)^T (\\boldsymbol a_i -\\boldsymbol \\mu_A)\n\\] and is computed here on the basis of the difference of the observations \\(\\boldsymbol a_i\\) relative to their mean \\(\\boldsymbol \\mu_A\\). However, since we typically only have pairwise distances available we don’t know the group means so this formula can’t be applied. Fortunately, it is also possible to compute \\(w_A\\) using only the pairwise differences using \\[\nw_A = \\frac{1}{n_A} \\sum_{\\boldsymbol a_i, \\boldsymbol a_j \\in A, i &lt; j} (\\boldsymbol a_i -\\boldsymbol a_j)^T (\\boldsymbol a_i -\\boldsymbol a_j)\n\\] This trick is employed in Ward’s clustering method by constructing a distance measure between two sets \\(A\\) and \\(B\\) as \\[\nd(A, B) = w_{A \\cup B} - w_A -w_B \\,\n\\] and using as the distance between two elementary data points \\(\\boldsymbol a\\) and \\(\\boldsymbol b\\) the squared Euclidean distance \\[\nd(\\boldsymbol a, \\boldsymbol b) = (\\boldsymbol a- \\boldsymbol b)^T (\\boldsymbol a- \\boldsymbol b) \\, .\n\\]\n\n\nApplication to Swiss banknote data set\nThis data set is reports 6 pysical measurements on 200 Swiss bank notes. Of the 200 notes 100 are genuine and 100 are counterfeit. The measurements are: length, left width, right width, bottom margin, top margin, diagonal length of the bank notes.\n\n\n\n\n\n\n\n\nFigure 4.2: Principal components Swiss banknote data.\n\n\n\n\n\nPlotting the first to PCAs of this data shows that there are indeed two well defined groups, and that these groups correspond precisely to the genuine and counterfeit banknotes (Figure 4.2).\n\n\n\n\n\n\n\n\nFigure 4.3: Ward clustering of Swiss banknote data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Average linkage clustering of Swiss banknote data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: Complete linkage clustering of Swiss banknote data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: Single linkage clustering of Swiss banknote data.\n\n\n\n\n\nWe now compare the hierarchical clusterings of the Swiss bank note data using four different methods using Euclidean distance:\n\nWard.D2 (=Ward’s method): Figure 4.3\nAverage linkage: Figure 4.4\nComplete linkage: Figure 4.5\nSingle linkage: Figure 4.6\n\nAn interactive R Shiny web app of this analysis (which also allows to explore further distance measures) is available online at https://minerva.it.manchester.ac.uk/shiny/strimmer/hclust/.\nResult:\n\nAll four trees / hierarchical clusterings are quite different!\nThe Ward.D2 method is the only one that finds the correct grouping (except for a single error).\n\n\n\nAssessment of the uncertainty of hierarchical clusterings\nIn practical application of hierarchical clustering methods is is essential to evaluate the stability and uncertainty of the obtained groupings. This is often done as follows using the “bootstrap”:\n\nSampling with replacement is used to generate a number of so-called bootstrap data sets (say \\(B=200\\)) similar to the original one. Specifically, we create new data matrices by repeately randomly selecting columns (variables) from the original data matrix for inclusion in the bootstrap data matrix. Note that we sample columns as our aim is to cluster the samples.\nSubsequently, a hierarchical clustering is computed for each of the bootstrap data sets. As a result, we now have an “ensemble” of \\(B\\) bootstrap trees.\nFinally, analysis of the clusters (bipartions) shown in all the bootstrap trees allows to count the clusters that appear frequently, and also those that appear less frequently. These counts provide a measure of the stability of the clusterings appearing in the original tree.\nAdditionally, from the bootstrap tree we can also compute a consensus tree containing the most stable clusters. This an be viewed as an “ensemble average” of all the bootstrap trees.\n\nA disadvantage of this procedure is that bootstrapping trees is computationally very expensive, as the original procedure is already time consuming but now needs to be repeated a large number of times.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unsupervised learning and clustering</span>"
    ]
  },
  {
    "objectID": "04-clustering.html#k-means-clustering",
    "href": "04-clustering.html#k-means-clustering",
    "title": "4  Unsupervised learning and clustering",
    "section": "4.3 \\(K\\)-means clustering",
    "text": "4.3 \\(K\\)-means clustering\n\nSet-up\n\nWe assume that there are \\(K\\) groups (i.e. \\(K\\) is known in advance).\nFor each group \\(k \\in \\{1, \\ldots, K\\}\\) we assume a group mean \\(\\boldsymbol \\mu_k\\).\nAim: partition the data points \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) into \\(K\\) non-overlapping groups.\nEach of the \\(n\\) data points \\(\\boldsymbol x_i\\) is assigned to exactly one of the \\(K\\) groups.\nMaximise the homogeneity within each group (i.e. each group should contain similar objects).\nMaximise the heterogeneity between the different groups (i.e each group should differ from the other groups).\n\n\n\nAlgorithm\nAfter running \\(K\\)-means we will get estimates of \\(\\hat{\\boldsymbol \\mu}_k\\) of the group means, as well allocations \\(y_i \\in \\{1, \\ldots, K\\}\\) of each data point \\(\\boldsymbol x_i\\) to one of the classes.\nInitialisation:\nAt the start of the algorithm the \\(n\\) observations \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) are randomly allocated with equal probability to one of the \\(K\\) groups. The resulting assignment is \\(y_1, \\ldots, y_n\\), with each \\(y_i=\\{1, \\ldots, K\\}\\). With \\(G_k = \\{ i | y_i = k\\}\\) we denote the set of indices of the data points in cluster \\(k\\), and with \\(n_k = | G_k |\\) the number of samples in cluster \\(k\\).\nIterative refinement:\n\nEstimate the group means by \\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n_k} \\sum_{i \\in G_k} \\boldsymbol x_i\n\\]\nUpdate the group allocations \\(y_i\\). Specifically, assign each data point \\(\\boldsymbol x_i\\) to the group \\(k\\) with the nearest \\(\\hat{\\boldsymbol \\mu}_k\\). The distance is measured in terms of the Euclidean norm: \\[\n\\begin{split}\ny_i & = \\underset{k}{\\arg \\min} \\,  \\left| \\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k \\right|_2 \\\\\n      & = \\underset{k}{\\arg \\min} \\, \\left(\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k\\right)^T \\left(\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k\\right) \\\\\n\\end{split}\n\\]\n\nSteps 1 and 2 are repeated until the algorithm converges (i.e. until the group allocations don’t change any more) or until a specified upper limit of iterations is reached.\n\n\nProperties\n\\(K\\)-means has been proposed in the 1950 to 1970s by various authors in diverse contexts 4. Despite its simplicity \\(K\\)-means is, perhaps surprisingly, a very effective clustering algorithm. The main reason for this is the close connection of \\(K\\)-means with probabilistic clustering based on Gaussian mixture models (for details see later section).\nSince the clustering depends on the initialisation it is often useful to run \\(K\\)-means several times with different starting group allocations of the data points. Furthermore, non-random or non-uniform initialisations can lead to improved and faster convergence, see the K-means++ algorithm.\nThe clusters constructed in \\(K\\)-means have linear boundaries and thus form a Voronoi tessellation around the cluster means. Again, this can be explained by the close link of \\(K\\)-means with a particular Gaussian mixture model.\n\n\nChoosing the number of clusters\nOnce the \\(K\\)-means algorithm has run we can assess the homogeneity and heterogeneity of the resulting clusters:\n\nthe total within-group sum of squares \\(SSW\\) (in R: tot.withinss), or total unexplained sum of squares: \\[\nSSW = \\sum_{k=1}^K \\, \\sum_{i \\in G_k} (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)\n\\] This quantity decreases with \\(K\\) and is zero for \\(K=n\\). The \\(K\\)-means algorithm tries to minimise this quantity but it will typically only find a local minimum rather than the global one.\nthe between-group sum of squares \\(SSB\\) (in R: betweenss), or explained sum of squares: \\[\nSSB = \\sum_{k=1}^K n_k (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)^T (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)\n\\] where \\(\\hat{\\boldsymbol \\mu}_0 = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol x_i = \\frac{1}{n} \\sum_{k=1}^K n_k \\hat{\\boldsymbol \\mu}_k\\) is the global mean of the samples. \\(SSB\\) increases with the number of clusters \\(K\\) until for \\(K=n\\) it becomes equal to the total sum of squares \\(SST\\).\nthe total sum of squares \\[\nSST = \\sum_{i=1}^n (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)^T (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0) \\, .\n\\] By construction \\(SST = SSB + SSW\\) for any \\(K\\) (i.e. \\(SST\\) is a constant independent of \\(K\\)).\n\nDividing the sum of squares by the sample size \\(n\\) we get\n\n\\(T = \\frac{SST}{n}\\) as the total variation,\n\\(B = \\frac{SSB}{n}\\) as the explained variation and\n\\(W = \\frac{SSW}{n}\\) as the total unexplained variation ,\nwith \\(T = B + W\\).\n\nIn order to decide on the optimal number of clusters we run \\(K\\)-means for different settings for \\(K\\) and then choose the smallest \\(K\\) for which the explained variation \\(B\\) is not significantly worse compared to a clustering with a substantially larger \\(K\\) (see example below).\n\n\n\\(K\\)-medoids aka PAM\nA closely related clustering method is \\(K\\)-medoids or PAM (“Partitioning Around Medoids”).\nThis works exactly like \\(K\\)-means, only that\n\ninstead of the estimated group means \\(\\hat{\\boldsymbol \\mu}_k\\) one member of each group is selected as its representative (the so-called “medoid”)\ninstead of squared Euclidean distance other dissimilarity measures are also allowed.\n\n\n\nApplication of \\(K\\)-means to Iris data\n\n\n\n\n\n\n\n\nFigure 4.7: Pairwise scatter plots for the original four variables of the iris flower data.\n\n\n\n\n\nScatter plots of Iris data (Figure 4.7):\nThe R output from a \\(K\\)-means analysis with known true number of clusters specified (\\(K=3\\)) is:\n\nkmeans.out = kmeans(X.iris, 3)\nkmeans.out\n\nK-means clustering with 3 clusters of sizes 47, 50, 53\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1   1.13217737  0.08812645    0.9928284   1.0141287\n2  -1.01119138  0.85041372   -1.3006301  -1.2507035\n3  -0.05005221 -0.88042696    0.3465767   0.2805873\n\nClustering vector:\n  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [38] 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 3 3 3 1 3 3 3 3 3 3 3 3 1 3 3 3 3 1 3 3 3\n [75] 3 1 1 1 3 3 3 3 3 3 3 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1 1 1 1 3 1 1 1 1\n[112] 1 1 3 3 1 1 1 1 3 1 3 1 3 1 1 3 1 1 1 1 1 1 3 3 1 1 1 3 1 1 1 3 1 1 1 3 1\n[149] 1 3\n\nWithin cluster sum of squares by cluster:\n[1] 47.45019 47.35062 44.08754\n (between_SS / total_SS =  76.7 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nThe corresponding total within-group sum of squares (\\(SSW\\), tot.withinss) is\n\nkmeans.out$tot.withinss\n\n[1] 138.8884\n\n\nand the between-group sum of squares (\\(SSB\\), betweenss) is\n\nkmeans.out$betweenss\n\n[1] 457.1116\n\n\nBy comparing with the known class assignments we can assess the accuracy of \\(K\\)-means clustering:\n\ntable(L.iris, kmeans.out$cluster)\n\n            \nL.iris        1  2  3\n  setosa      0 50  0\n  versicolor 11  0 39\n  virginica  36  0 14\n\n\n\n\n\n\n\n\n\n\nFigure 4.8: Within and between group error in dependence of the number of groups \\(K\\) in \\(K\\)-means.\n\n\n\n\n\nFor choosing \\(K\\) we run \\(K\\)-means several times and compute within and between cluster variation in dependence of \\(K\\) (see Figure 4.8).\nIn this example \\(K=3\\) clusters seem appropriate since the the explained variation does not significantly improve (and the unexplained variation does not significantly decrease) with a further increase of the number of clusters.\n\n\nArbitrariness of cluster labels and label switching\nIt is important to realise that in unsupervised learning and clustering the labels of each group are assigned in an arbitrary fashion. Recall that for \\(K\\) groups there are \\(K!\\) possibilities to attach the labels, corresponding to the number of permutations of \\(K\\) groups.\nThus, different runs of a clustering algorithm such as \\(K\\)-means may return the same clustering (groupings of samples) but with different labels. This phenomenon is called “label switching” and makes it difficult to automatise comparison of clusterings. In particular, one cannot simply rely on the automatically assigned group label, instead one needs to compare the actual members of the clusters.\nA way to resolve the problem of label switching is to relabel the clusters using additional information, such as requiring that some samples are in specific groups (e.g.: sample 1 is always in group labelled “1”), and/or linking labels to orderings or constraints on the group characteristics (e.g.: the group with label “1” has always a smaller mean that group with label “2”).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unsupervised learning and clustering</span>"
    ]
  },
  {
    "objectID": "04-clustering.html#mixture-models",
    "href": "04-clustering.html#mixture-models",
    "title": "4  Unsupervised learning and clustering",
    "section": "4.4 Mixture models",
    "text": "4.4 Mixture models\n\nFinite mixture model\n\n\\(K\\) groups / classes / categories, with finite \\(K\\) known in advance.\nProbability of class \\(k\\): \\(\\text{Pr}(k) = \\pi_k\\) with \\(\\sum_{k=1}^K \\pi_k = 1\\).\nEach class \\(k \\in C= \\{1, \\ldots, K\\}\\) is modelled by its own distribution \\(F_k\\) with own parameters \\(\\boldsymbol \\theta_k\\).\nDensity of class \\(k\\): \\(f_k(\\boldsymbol x) = f(\\boldsymbol x| k)\\).\nThe conditional means and variances for each class \\(k \\in C\\) are \\(\\text{E}(\\boldsymbol x| k) = \\boldsymbol \\mu_k\\) and \\(\\text{Var}(\\boldsymbol x| k) = \\boldsymbol \\Sigma_k\\).\nThe resulting mixture density for the observed variable \\(\\boldsymbol x\\) is \\[\nf_{\\text{mix}}(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\n\\]\n\nVery often one uses multivariate normal components \\(f_k(\\boldsymbol x) = N(\\boldsymbol x| \\boldsymbol \\mu_k, \\boldsymbol \\Sigma_k)\\) \\(\\\\ \\Longrightarrow\\) Gaussian mixture model (GMM)\nMixture models are fundamental not just in clustering but for many other applications (e.g. classification).\nNote: don’t confuse mixture model with mixed model (= terminology for a random effects regression model).\n\n\nTotal mean and variance of a multivariate mixture model\nUsing the law of total expectation we obtain the mean of the mixture density with multivariate \\(\\boldsymbol x\\) as follows: \\[\n\\begin{split}\n\\text{E}(\\boldsymbol x) & = \\text{E}(\\text{E}(\\boldsymbol x| k)) \\\\\n            & = \\text{E}( \\boldsymbol \\mu_k ) \\\\\n            &= \\sum_{k=1}^K \\pi_k \\boldsymbol \\mu_k \\\\\n            &= \\boldsymbol \\mu_0 \\\\\n\\end{split}\n\\] Note that we treat both \\(\\boldsymbol x\\) as well as \\(k\\) as random variables.\nSimilarly, using the law of total variance we compute the marginal variance: \\[\n\\begin{split}\n\\underbrace{\\text{Var}(\\boldsymbol x)}_{\\text{total}} & =  \\underbrace{ \\text{Var}( \\text{E}(\\boldsymbol x| k )  )}_{\\text{explained / between-group}} + \\underbrace{\\text{E}(\\text{Var}(\\boldsymbol x|k))}_{\\text{unexplained / expected within-group / pooled}} \\\\\n\\boldsymbol \\Sigma_0 & =  \\text{Var}( \\boldsymbol \\mu_k  ) + \\text{E}( \\boldsymbol \\Sigma_k )  \\\\\n               & =  \\sum_{k=1}^K \\pi_k (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T + \\sum_{k=1}^K \\pi_k \\boldsymbol \\Sigma_k  \\\\\n& =  \\boldsymbol \\Sigma_{\\text{explained}} +  \\boldsymbol \\Sigma_{\\text{unexplained}} \\\\\n\\end{split}\n\\]\nThus, the total variance decomposes into the explained (between group) variance and the unexplained (expected within group, pooled) variance.\n\n\nTotal variation\nThe total variation is given by the trace of the covariance matrix. The above decomposition for the total variation is \\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol \\Sigma_0) & =  \\text{Tr}( \\boldsymbol \\Sigma_{\\text{explained}} )  + \\text{Tr}( \\boldsymbol \\Sigma_{\\text{unexplained}} )  \\\\\n& =  \\sum_{k=1}^K \\pi_k \\text{Tr}((\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T) + \\sum_{k=1}^K \\pi_k \\text{Tr}(\\boldsymbol \\Sigma_k)  \\\\\n& =  \\sum_{k=1}^K \\pi_k (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) + \\sum_{k=1}^K \\pi_k \\text{Tr}(\\boldsymbol \\Sigma_k)\\\\\n\\end{split}\n\\] If the covariances are replaced by their empirical estimates we obtain the \\(T=B+W\\) decomposition of total variation familiar from \\(K\\)-means: \\[T = \\text{Tr}\\left( \\hat{\\boldsymbol \\Sigma}_0 \\right)  =\n\\frac{1}{n} \\sum_{i=1}^n (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)^T (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)\\] \\[B = \\text{Tr}( \\hat{\\boldsymbol \\Sigma}_{\\text{explained}} ) =  \\frac{1}{n} \\sum_{k=1}^K n_k (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)^T (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)\\] \\[W = \\text{Tr}( \\hat{\\boldsymbol \\Sigma}_{\\text{unexplained}} ) = \\frac{1}{n}  \\sum_{k=1}^K \\, \\sum_{i \\in G_k} (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)\n\\]\n\n\nUnivariate mixture\nFor a univariate mixture (\\(d=1\\)) with \\(K=2\\) components we get \\[\n\\mu_0 = \\pi_1 \\mu_1+ \\pi_2 \\mu_2 \\, ,\n\\] \\[\n\\sigma^2_{\\text{within}} = \\pi_1 \\sigma^2_1 + \\pi_2 \\sigma^2_2 = \\sigma^2_{\\text{pooled}}\\,,\n\\] also known as pooled variance, and \\[\n\\begin{split}\n\\sigma^2_{\\text{between}} &= \\pi_1 (\\mu_1 - \\mu_0)^2 + \\pi_2 (\\mu_2 - \\mu_0)^2 \\\\\n& =\\pi_1 \\pi_2^2 (\\mu_1 - \\mu_2)^2 + \\pi_2 \\pi_1^2 (\\mu_1 - \\mu_2)^2\\\\\n& = \\pi_1 \\pi_2 (\\mu_1 - \\mu_2)^2  \\\\\n& = \\left( \\frac{1}{\\pi_1} + \\frac{1}{\\pi_2}   \\right)^{-1} (\\mu_1 - \\mu_2)^2 \\\\\n\\end{split} \\,.\n\\] The ratio of the between-group variance and the within-group variance is proportional (by factor of \\(n\\)) to the squared pooled-variance \\(t\\)-score: \\[\n\\frac{\\sigma^2_{\\text{between}}}{\\sigma^2_{\\text{within}}} =\n  \\frac{ (\\mu_1 - \\mu_2)^2}{ \\left(\\frac{1}{\\pi_1} + \\frac{1}{\\pi_2}   \\right)  \\sigma^2_{\\text{pooled}} }= \\frac{t_{\\text{pooled}}^2}{n}\n\\] If you are familiar with ANOVA (e.g. linear models course) you will recognise this ratio as the \\(F\\)-score.\n\n\nExample of mixtures\n\n\n\n\n\n\n\n\nFigure 4.9: Mixture of two normal distributions with two modes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.10: Mixture of two normal distributions with a single mode.\n\n\n\n\n\nMixtures can take on many different shapes and forms, so it is instructive to study a number of examples. An interactive tool to visualise two component normal mixture is available online as R Shiny web app at https://minerva.it.manchester.ac.uk/shiny/strimmer/mixture/.\nThe first plot (Figure 4.9) shows the bimodal density of a mixture distribution consisting of two normals with \\(\\pi_1=0.7\\), \\(\\mu_1=-1\\), \\(\\mu_2=2\\) and the two variances equal to 1 (\\(\\sigma^2_1 = 1\\) and \\(\\sigma^2_2 = 1\\)). Because the two components are well-separated there are two clear modes. The plot also shows the density of a normal distribution with the same total mean (\\(\\mu_0=-0.1\\)) and variance (\\(\\sigma_0^2=2.89\\)) as the mixture distribution. Clearly the total normal and the mixture density are very different.\nHowever, a two-component mixture can also be unimodal (Figure 4.10). For example, if the mean of the second component is set to \\(\\mu_2=0\\) then there is only a single mode and the total normal density with \\(\\mu_0=-0.7\\) and \\(\\sigma_0^2=1.21\\) is now almost inistinguishable in form from the mixture density. Thus, in this case it will be very hard (or even impossible) to identify the two peaks from the data.\nMost mixtures we consider in this course are multivariate. For illustration, Figure 4.11 shows a plot of a density of the mixture of two bivariate normal distributions, with \\(\\pi_1=0.7\\), \\(\\boldsymbol \\mu_1 = \\begin{pmatrix}-1 \\\\1 \\\\ \\end{pmatrix}\\), \\(\\boldsymbol \\Sigma_1 = \\begin{pmatrix} 1 & 0.7 \\\\ 0.7 & 1  \\\\ \\end{pmatrix}\\), \\(\\boldsymbol \\mu_2 = \\begin{pmatrix}2.5 \\\\0.5 \\\\ \\end{pmatrix}\\) and \\(\\boldsymbol \\Sigma_2 = \\begin{pmatrix} 1 & -0.7 \\\\ -0.7 & 1  \\\\ \\end{pmatrix}\\):\n\n\n\n\n\n\n\n\nFigure 4.11: Mixture of two bivariate normal distributions with two modes.\n\n\n\n\n\n\n\nSampling from a mixture model\nAssuming we know how to sample from the component densities \\(f_k(\\boldsymbol x)\\) of the mixture model it is straightforward to set up a procedure for sampling from the mixture \\(f_{\\text{mix}}(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\\) itself.\nThis is done in a two-step process:\n\nDraw from categorical distribution with parameter \\(\\boldsymbol \\pi=(\\pi_1, \\ldots, \\pi_K)^T\\): \\[\\boldsymbol z\\sim \\text{Cat}(\\boldsymbol \\pi)\\] Here the vector \\(\\boldsymbol z= (z_1, \\ldots, z_K)^T\\) indicates a hard group 0/1 allocation, with all components \\(z_{\\neq k}=0\\) except for a single entry \\(z_k=1\\).\nSubsequently, sample from the component \\(k\\) selected in step 1: \\[\n\\boldsymbol x\\sim F_k\n\\]\n\nThis two-stage sampling approach is also known as hierarchical generative model for a mixture distribution. This generative view is not only useful for simulating data from a mixture model but also highlights the role of the latent variable (the class allocation).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unsupervised learning and clustering</span>"
    ]
  },
  {
    "objectID": "04-clustering.html#fitting-mixture-models-to-data-and-inferring-the-latent-states",
    "href": "04-clustering.html#fitting-mixture-models-to-data-and-inferring-the-latent-states",
    "title": "4  Unsupervised learning and clustering",
    "section": "4.5 Fitting mixture models to data and inferring the latent states",
    "text": "4.5 Fitting mixture models to data and inferring the latent states\nIn the following we denote by\n\n\\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)^T\\) the data matrix containing the observations of \\(n\\) independent and identically distributed samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\), and\n\\(\\boldsymbol y= (y_1, \\ldots, y_n)^T\\) the associated group memberships, as well as\nthe parameters \\(\\boldsymbol \\theta\\) which for a Gaussian mixture model are \\(\\boldsymbol \\theta= \\{\\boldsymbol \\pi, \\boldsymbol \\mu_1, \\ldots, \\boldsymbol \\mu_K, \\boldsymbol \\Sigma_1, \\ldots, \\boldsymbol \\Sigma_K\\}\\).\n\n\nObserved and latent variables\nWhen we observe data from a mixture model we collect samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\). Associated with each observed \\(\\boldsymbol x_i\\) is the corresponding underlying class allocation \\(y_1, \\ldots, y_n\\) where each \\(y_i\\) takes on a value from \\(C = \\{1, \\ldots, K\\}\\). Crucially, the class allocations \\(y_i\\) are unknown and cannot be directly observed, thus are latent.\n\nThe joint density for observed and unobserved variables: \\[f(\\boldsymbol x, y) = f(\\boldsymbol x| y) \\text{Pr}(y) = f_y(\\boldsymbol x) \\pi_y\\]\n\nThe mixture density is therefore a marginal density as it arises from the joint density \\(f(\\boldsymbol x, y)\\) by marginalising over the discrete variable \\(y\\).\n\nMarginalisation: \\(f(\\boldsymbol x) =  \\sum_{y \\in C} f(\\boldsymbol x, y)\\)\n\n\n\nComplete data likelihood and observed data likelihood\nIf we know \\(\\boldsymbol y\\) in advance, i.e. if we know which sample belongs to a particular group, we can construct a complete data log-likelihood based on the joint distribution \\(f(\\boldsymbol x, y) = \\pi_y f_y(\\boldsymbol x)\\). The log-likelihood for \\(\\boldsymbol \\theta\\) given the both \\(\\boldsymbol X\\) and \\(\\boldsymbol y\\) is \\[\n\\log L(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol y) = \\sum_{i=1}^n \\log f(\\boldsymbol x_i, y_i)  =  \\sum_{i=1}^n  \\log \\left(\\pi_{y_i} f_{y_i}(\\boldsymbol x_i) \\right)\n\\]\nOn the other hand, typically we do not know \\(\\boldsymbol y\\) and therefore use the marginal or mixture density \\(f(\\boldsymbol x)\\) to construct the observed data log-likelihood (sometimes also called incomplete data log-likelihood) \\(f(\\boldsymbol x| \\boldsymbol \\theta)\\) as \\[\n\\begin{split}\n\\log L(\\boldsymbol \\theta| \\boldsymbol X) & =\\sum_{i=1}^n \\log f(\\boldsymbol x_i | \\boldsymbol \\theta)\\\\\n& = \\sum_{i=1}^n \\log \\left( \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x_i)  \\right)\\\\\n\\end{split}\n\\]\nThe observed data log-likelihood can also be computed from the complete data likelihood function by marginalising over \\(\\boldsymbol y\\) \\[\n\\begin{split}\n\\log L(\\boldsymbol \\theta| \\boldsymbol X) &= \\log \\sum_{\\boldsymbol y}   L(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol y)\\\\\n&= \\log \\sum_{y_1, \\ldots, y_K}  \\prod_{i=1}^n f(\\boldsymbol x_i, y_i)\\\\\n&= \\log \\prod_{i=1}^n  \\sum_{k=1}^K f(\\boldsymbol x_i, k)\\\\\n& = \\sum_{i=1}^n \\log \\left(  \\sum_{k=1}^K f(\\boldsymbol x_i, k)     \\right)\n\\end{split}\n\\]\nClustering with a mixture model can be viewed as an incomplete or missing data problem (see also MATH27720 Statistics 2).\nSpecifically, we face the problem of\n\nfitting the model using only the observed data \\(\\boldsymbol X\\) and\nsimultaneously inferring the class allocations \\(\\boldsymbol y\\), i.e. states of the latent variable.\n\n\n\nFitting the mixture model to the observed data\nFor large sample size \\(n\\) the standard way to fit a mixture model is to employ maximum likelihood to find the MLEs of the parameters of the mixture model.\nThe direct way to fit a mixture model by maximum likelihood is to maximise the observed data log-likelihood function with regard to \\(\\boldsymbol \\theta\\): \\[\n\\hat{\\boldsymbol \\theta}^{ML} = \\underset{\\boldsymbol \\theta}{\\arg \\max}\\,\\, \\log L(\\boldsymbol \\theta| \\boldsymbol X)\n\\]\nUnfortunately, in practise evaluation and optimisation of the log-likelihood function can be difficult due to a number of reasons:\n\nThe form of the observed data log-likelihood function prevents analytic simplifications (note the sum inside the logarithm) and thus can be difficult to compute.\nBecause of the symmetries due to exchangeability of cluster labels the likelihood function is multimodal and thus hard to optimise. Note this is also linked to the general problem of label switching and non-identifiability of cluster labels — see the discussion for \\(K\\)-means clustering.\nFurther identifiability issues can arise if (for instance) two neighboring components of the mixture model are largely overlapping and thus are too close to each other to be discriminated as two different modes. In other words, it is difficult to determine the number of classes.\nFurthermore, the likelihood in Gaussian mixture models is singular if one of the fitted covariance matrices becomes singular. However, this can be easily adressed by using some form of regularisation (Bayes, penalised ML, etc.) or simply by requiring sufficient sample size per group.\n\n\n\nPredicting the group allocation of a given sample\nIn probabilistic clustering the aim is to infer the latent states \\(y_1, \\ldots, y_n\\) for all observed samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).\nAssuming that the mixture model is known (either in advance or after fitting it) Bayes’ theorem allows predict the probability that an observation \\(\\boldsymbol x_i\\) falls in group \\(k \\in \\{1, \\ldots, K\\}\\): \\[\nq_i(k) = \\text{Pr}(k | \\boldsymbol x_i) = \\frac{\\pi_k f_k(\\boldsymbol x_i ) }{ f(\\boldsymbol x_i)}\n\\] Thus, for each of the \\(n\\) samples we get a probability mass function over the \\(K\\) classes with \\(\\sum_{k=1}^K q_i(k)=1\\).\nThe posterior probabilities in \\(q_i(k)\\) provide a so-called soft assignment of the sample \\(\\boldsymbol x_i\\) to all classes rather than a 0/1 hard assignment to a specific class (as for example in the \\(K\\)-means algorithm).\nTo obtain at a hard clustering and to infer the most probable latent state we select the class with the highest probability \\[\ny_i =\\underset{k}{\\arg \\max}\\,\\,q_i(k)\n\\]\nThus, in probabilistic clustering we directly obtain an assessment of the uncertainty of the class assignment for a sample \\(\\boldsymbol x_i\\) (which is not the case in simple algorithmic clustering such \\(K\\)-means). We can use this information to check whether there are several classes with equal or similar probability. This will be the case, e.g., if \\(\\boldsymbol x_i\\) lies near the boundary between two neighbouring classes.\nUsing the interactive Shiny app for the univariate normal component mixture (online at https://minerva.it.manchester.ac.uk/shiny/strimmer/mixture/) you can explore the posterior probabilities of each class.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unsupervised learning and clustering</span>"
    ]
  },
  {
    "objectID": "04-clustering.html#application-of-gaussian-mixture-models",
    "href": "04-clustering.html#application-of-gaussian-mixture-models",
    "title": "4  Unsupervised learning and clustering",
    "section": "4.6 Application of Gaussian mixture models",
    "text": "4.6 Application of Gaussian mixture models\n\nChoosing the number of classes\nIn an application of a GMM we need to select a suitable value for \\(K\\), i.e. the number of classes.\nSince GMMs operate in a likelihood framework we can use penalised likelihood model selection criteria to choose among different models (i.e. GMMs with different numbers of classes).\nThe most popular choices are AIC (Akaike Information Criterion) and BIC (Bayesian Information criterion) defined as follows: \\[\\text{AIC}= -2 \\log L + 2 K \\] \\[\\text{BIC}= - 2 \\log L +K \\log(n)\\]\nIn order to choose a suitable model we evaluate different models with different \\(K\\) and then choose the model that minimises \\(\\text{AIC}\\) or \\(\\text{BIC}\\)\nNote that in both criteria more complex models with more parameters (in this case groups) are penalised over simpler models in order to prevent overfitting.\nAnother way of choosing optimal numbers of clusters is by cross-validation (see later chapter on supervised learning).\n\n\nApplication of GMMs to Iris flower data\nWe now explore the application of Gaussian mixture models to the Iris flower data set we also investigated with PCA and K-means.\nFirst, we fit a GMM with 3 clusters, using the R software “mclust” 5.\n\ndata(iris)\nX.iris = scale((iris[, 1:4]), scale=TRUE) # center and standardise\nL.iris = iris[, 5]\n\nlibrary(\"mclust\")\ngmm3 = Mclust(X.iris, G=3, verbose=FALSE)\nplot(gmm3, what=\"classification\")\n\n\n\n\n\n\n\nFigure 4.12: Mclust fit of mixture model to the iris flower data.\n\n\n\n\n\nThe “mclust” software has used the following model when fitting the mixture:\n\ngmm3$modelName\n\n[1] \"VVV\"\n\n\nHere “VVV” is the name used by the “mclust” software for a model allowing for an individual unrestricted covariance matrix \\(\\boldsymbol \\Sigma_k\\) for each class \\(k\\).\nThis GMM has a substantially lower misclassification error compared to \\(K\\)-means with the same number of clusters:\n\ntable(gmm3$classification, L.iris)\n\n   L.iris\n    setosa versicolor virginica\n  1     50          0         0\n  2      0         45         0\n  3      0          5        50\n\n\nNote that in “mclust” the BIC criterion is defined with the opposite sign (\\(\\text{BIC}_{\\text{mclust}} = 2 \\log L -K \\log(n)\\)), thus we need to find the maximum value rather than the smallest value.\nIf we compute BIC for various numbers of groups we find that the model with the best \\(\\text{BIC}_{\\text{mclust}}\\) is a model with 2 clusters but the model with 3 cluster has nearly as good a BIC:\n\n\n\n\n\n\n\n\nFigure 4.13: Mclust BIC plot to select optimal number of groups for the iris flower data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unsupervised learning and clustering</span>"
    ]
  },
  {
    "objectID": "04-clustering.html#the-em-algorithm",
    "href": "04-clustering.html#the-em-algorithm",
    "title": "4  Unsupervised learning and clustering",
    "section": "4.7 The EM algorithm",
    "text": "4.7 The EM algorithm\n\nMotivation\nAs discussed above, the observed data log-likelihood can be difficult to maximise directly due to its form as a log marginal likelihood. Intriguingly, it is possible to optimise it indirectly using the complete data log-likelihood, and what’s more this also allows in many cases for an analytic expression of the maximisation step.\nThis method is called the EM algorithm and has been formally proposed and described by Arthur Dempster (1929–) and others in 19776 but the algorithm was already know prior. It iteratively estimates both the parameters of the mixture model parameters and the latent states. The key idea behind the EM algorithm is to capitalise on the simplicity of the complete data likelihood and to obtain estimates of \\(\\boldsymbol \\theta\\) by imputing the missing group allocations and then subsequently iteratively refining both the imputations and the estimates of \\(\\boldsymbol \\theta\\).\nMore precisely, in the EM (=expectation-maximisation) algorithm we alternate between\n\nStep 1) updating the soft allocations of each sample using the current estimate of the parameters \\(\\boldsymbol \\theta\\) (obtained in step 2)\nStep 2) updating the parameter estimates by maximising the expected complete data log-likelihood. The expectation is taken with regard to the distribution over the latent states (obtained in step 1). Thus the complete data log-likelihood is averaged over the soft class assignments. For an exponential family (e.g. when the distributions for each group are normal) maximisation of the expected complete data log-likelihood can even be done analytically.\n\n\n\nThe EM algorithm\nSpecifically, the EM algorithm proceeds as follows:\n\nInitialisation:\n\n\nStart with a guess of the parameters \\(\\hat{\\boldsymbol \\theta}^{(1)}\\), then continue with “E” Step, Part A.\nAlternatively, start with a guess of the soft allocations for each sample \\(q_i(k)^{(1)}\\), collected in the matrix \\(\\boldsymbol Q^{(1)}\\), then continue with “E” Step, Part B.\nThis may be derived from some prior information, e.g., from running \\(K\\)-means. Caveat: some particular initialisations correspond to invariant states and hence should be avoided (see further below).\n\n\nE “expectation” step\n\n\nPart A: Use Bayes’ theorem to compute new probabilities of allocation to class \\(k\\) for all the samples \\(\\boldsymbol x_i\\): \\[\nq_i(k)^{(b+1)} \\leftarrow \\frac{ \\hat{\\pi}_k^{(b)} f_k(\\boldsymbol x_i | \\hat{\\boldsymbol \\theta}^{(b)})    }{  f(\\boldsymbol x_i |\\hat{\\boldsymbol \\theta}^{(b)} )  }\n\\] Note that to obtain \\(q_i(k)^{(b+1)}\\) the current estimate \\(\\hat{\\boldsymbol \\theta}^{(b)}\\) of the parameters of the mixture model is required.\nPart B: Construct the expected complete data log-likelihood function for \\(\\boldsymbol \\theta\\) using the soft allocations \\(q_i(k)^{(b+1)}\\) collected in the matrix \\(\\boldsymbol Q^{(b+1)}\\): \\[\nG(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)} ) = \\sum_{i=1}^n \\sum_{k=1}^K q_i(k)^{(b+1)}  \\log \\left( \\pi_k f_k(\\boldsymbol x_i) \\right)\n\\] Note that in the case that the soft allocations \\(\\boldsymbol Q^{(b+1)}\\) turn into hard 0/1 allocations then \\(G(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)})\\) becomes equivalent to the complete data log-likelihood.\n\n\nM “maximisation” step — Maximise the expected complete data log-likelihood to update the estimates of mixture model parameters: \\[\n\\hat{\\boldsymbol \\theta}^{(b+1)} \\leftarrow \\arg \\max_{\\boldsymbol \\theta}  G(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)} )\n\\]\nContinue with 2) “E” Step until the series \\(\\hat{\\boldsymbol \\theta}^{(1)}, \\hat{\\boldsymbol \\theta}^{(2)}, \\hat{\\boldsymbol \\theta}^{(3)}, \\ldots\\) has converged.\n\nCrucially, maximisation of the expected complete data log-likelihood is typically much easier than maximisation of the observed data log-likelihood, and in many cases even analytically tractable, which is why in this case the EM algorithm is often preferred over direct maximisation of the observed data log-likelihood.\nNote that to avoid singularities in the expected log-likelihood function we may need to adopt regularisation (i.e. penalised maximum likelihood or Bayesian learning) for estimating the parameters in the M-step.\n\n\nEM algorithm for multivariate normal mixture model\nWe now consider the EM algorithm applied to the case when the conditional group distributions are normal, i.e. when applied to the Gaussian mixture model (GMM). In this case the two iterative steps in the EM algorithm can be expressed as follows:\nE-step:\nUpdate the soft allocations: \\[\nq_i(k)^{(b+1)} = \\frac{ \\hat{\\pi}_k^{(b)} N(\\boldsymbol x_i | \\hat{\\boldsymbol \\mu}_k^{(b)}, \\hat{\\boldsymbol \\Sigma}_k^{(b)}) }{\n\\hat{f}^{(b)} \\left(\\boldsymbol x_i |  \n\\hat{\\pi}_1^{(b)}, \\ldots, \\hat{\\pi}_K^{(b)},\n\\hat{\\boldsymbol \\mu}_1^{(b)}, \\ldots, \\hat{\\boldsymbol \\mu}_K^{(b)},\n\\hat{\\boldsymbol \\Sigma}_1^{(b)}, \\ldots, \\hat{\\boldsymbol \\Sigma}_K^{(b)} \\right)  }\n\\]\nCorrespondingly, the number of samples assigned to class \\(k\\) in the current step is \\[\nn_k^{(b+1)} = \\sum_{i=1}^n q_i(k)^{(b+1)}\n\\] Note this is not necessarily an integer because of the soft allocations of samples to groups.\nThe expected complete data log-likelihood becomes: \\[\nG(\\pi_1, \\ldots \\pi_K,\\boldsymbol \\mu_1, \\ldots, \\boldsymbol \\mu_K,  \\boldsymbol \\Sigma_1, \\ldots, \\boldsymbol \\Sigma_K | \\boldsymbol X, \\boldsymbol Q^{(b+1)} ) =\n\\sum_{i=1}^n \\sum_{k=1}^K q_i(k)^{(b+1)}  \\log \\left( \\pi_k f_k(\\boldsymbol x_i) \\right)\n\\] with \\[\n\\log \\left( \\pi_k f_k(\\boldsymbol x_i) \\right) =  -\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma_k^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) -\\frac{1}{2} \\log \\det(\\boldsymbol \\Sigma_k) +\\log(\\pi_k)\n\\] (Remark: we will encounter this expression again in the next chapter when discussing quadratic discriminant analysis).\nM-step:\nThe maximisation of the expected complete data log-likelihood can be done analytically as it is a weighted version of the conventional single group multivariate normal log-likelihood. The resulting estimators are also weighted variants of the usual MLEs.\nThe updated estimates of the group probabilities are \\[\n\\hat{\\pi}_k^{(b+1)} = \\frac{n_k^{(b+1)}}{n}\n\\] The updated estimates of the means are \\[\n\\hat{\\boldsymbol \\mu}_k^{(b+1)} = \\frac{1}{n_k^{(b+1)}} \\sum_{i=1}^n q_i(k)^{(b+1)} \\boldsymbol x_i\n\\] and the updated covariance estimates are \\[\n\\hat{\\boldsymbol \\Sigma}_k^{(b+1)} =  \\frac{1}{n_k^{(b+1)}} \\sum_{i=1}^n q_i(k)^{(b+1)} \\left( \\boldsymbol x_i -\\boldsymbol \\mu_k^{(b+1)}\\right)   \\left( \\boldsymbol x_i -\\boldsymbol \\mu_k^{(b+1)}\\right)^T\n\\]\nNote that if \\(q_i(k)\\) is a hard allocation (so that for any \\(i\\) only one class has weight 1 and all others weight 0) then all estimators above reduce to the usual MLEs.\nIn Worksheet 8 you can find a simple R implementation of the EM algorithm for univariate normal mixtures.\nSimilar analytical expressions can also be found in more general mixtures where the components are exponential families. As mentioned above this is one of the advantages of using the EM algorithm.\n\n\nConvergence and invariant states\nUnder mild assumptions the EM algorithm is guaranteed to monotonically converge to local optima of the observed data log-likelihood 7. Thus the series \\(\\hat{\\boldsymbol \\theta}^{(1)}, \\hat{\\boldsymbol \\theta}^{(2)}, \\hat{\\boldsymbol \\theta}^{(3)}, \\ldots\\) converges to the estimate \\(\\hat{\\boldsymbol \\theta}\\) found when maximising the observed data log-likelihood. However, the speed of convergence in the EM algorithm can sometimes be slow, and there are also situations in which there is no convergence at all to \\(\\hat{\\boldsymbol \\theta}\\) because the EM algorithm remains in an invariant state.\nAn example of such an invariant state for a Gaussian mixture model is uniform initialisation of the latent variables \\(q_i(k) = \\frac{1}{K}\\), where \\(K\\) is the number of classes.\nWith this we get in the M step \\(n_k = \\frac{n}{K}\\) and as parameter estimates \\[\n\\hat{\\pi}_k = \\frac{1}{K}\n\\] \\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol x_i = \\bar{\\boldsymbol x}\n\\] \\[\n\\hat{\\boldsymbol \\Sigma}_k = \\frac{1}{n}  \\sum_{i=1}^n ( \\boldsymbol x_i -\\bar{\\boldsymbol x})   ( \\boldsymbol x_i -\\bar{\\boldsymbol x})^T = \\hat{\\boldsymbol \\Sigma}\n\\] Crucially, none of these actually depend on the group \\(k\\)! Thus, in the E step when the next soft allocations are determined this leads to \\[\nq_i(k) = \\frac{ \\frac{1}{K} N(\\boldsymbol x_i | \\bar{\\boldsymbol x}, \\hat{\\boldsymbol \\Sigma} ) }{ \\sum_{j=1}^K  \\frac{1}{K} N(\\boldsymbol x_i | \\bar{\\boldsymbol x}, \\hat{\\boldsymbol \\Sigma} )  } = \\frac{1}{K}\n\\] After one cycle in the EM algorithm we arrive at the same soft allocation that we started with, and the algorithm is trapped in an invariant state! Therefore uniform initialisation should clearly be avoided!\nYou will explore this effect in practise in Worksheet 8.\n\n\nConnection with the \\(K\\)-means clustering method\nThe \\(K\\)-means algorithm is very closely related to the EM algorithm and probabilistic clustering with a specific Gaussian mixture models.\nSpecifically, we assume a simplified model where the probabilities \\(\\pi_k\\) of all classes are equal (i.e. \\(\\pi_k=\\frac{1}{K}\\)) and where the covariances \\(\\boldsymbol \\Sigma_k\\) are all of the same spherical form \\(\\sigma^2 \\boldsymbol I\\). Thus, the covariance does not depend on the group, there is no correlation between the variables and the variance of all variables is the same.\nFirst, we consider the “E” step. Using the mixture model above the soft assignment for the class allocation becomes \\[\n\\log( q_i(k) ) = -\\frac{1}{2 \\sigma^2} (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k) +  \\text{const}\n\\] where \\(\\text{const}\\) does not depend on \\(k\\). This can turned into a hard class allocation by \\[\n\\begin{split}\ny_i &= \\underset{k}{\\arg \\max} \\log( q_i(k) ) \\\\\n          & = \\underset{k}{\\arg \\min}  (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)\\\\\n\\end{split}\n\\] which is exactly the \\(K\\)-means rule to dallocate of samples to groups.\nSecond, in the “M” step we compute the parameters of the model. If the class allocations are hard the expected log-likelihood becomes the observed data likelihood and the MLE of the group mean is the average of samples in that group.\nThus, \\(K\\)-means can be viewed as an EM type algorithm to provide hard classification based on a simple restricted Gaussian mixture model.\n\n\nWhy the EM algorithm works — an entropy point of view\nThe iterative (soft) imputation of the latent states in the EM algorithm is intuitively clear.\nHowever, in order to get a better understanding of EM we need to demonstrate\n\nwhy the expected observed log-likelihood needs to be maximised rather than, e.g., the observed log-likelihood with hard allocations, and\nthat applying the EM algorithm versus directly maximising the marginal likelihood both lead to the same fitted mixture model.\n\nIntriguingly, both these aspects of the EM algorithm are easiest to understand from an entropy point of view, i.e. considering the entropy foundations of maximum likelihood and Bayesian learning — for details see MATH27720 Statistics 2. Specifically, the reason for the need of using expectation is the link between likelihood and cross-entropy (which also is defined as expectation). Furthermore, the EM algorithm is an example of using an ELBO (“evidence lower bound”) to successively approximate the maximised marginal log-likelihood, with the bound only getting better in each step.\nFirst, recall that the method of maximum likelihood results from minimising the KL divergence between an empirical distribution \\(Q_{\\boldsymbol x}\\) representing the observations \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) and the model family \\(F_{\\boldsymbol x}^{\\boldsymbol \\theta}\\) with parameters \\(\\boldsymbol \\theta\\): \\[\n\\hat{\\boldsymbol \\theta}^{ML} =  \\underset{\\boldsymbol \\theta}{\\arg \\min}\\,\\, D_{\\text{KL}}(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta})\n\\] The KL divergence decomposes into a cross-entropy and an entropy part \\[\nD_{\\text{KL}}(Q_{\\boldsymbol x}, F_{\\boldsymbol \\theta}) = H(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta})- H(Q_{\\boldsymbol x})\n\\] hence minimising the KL divergence with regard to \\(\\boldsymbol \\theta\\) is the same as maximising the function \\[\n\\begin{split}\n-n H(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta}) &= n \\text{E}_{Q_{\\boldsymbol x}}( \\log f(\\boldsymbol x| \\boldsymbol \\theta)  ) \\\\\n&= \\sum_{i=1}^n  \\log f(\\boldsymbol x_i | \\boldsymbol \\theta)\\\\\n&= \\log L(\\boldsymbol \\theta| \\boldsymbol X)\\\\\n\\end{split}\n\\] which is indeed the observed data log-likelihood for \\(\\boldsymbol \\theta\\).\nSecond, we recall the chain rule for the KL divergence. Specifically, the KL divergence for the joint model forms an upper bound of the KL divergence for the marginal model: \\[\n\\begin{split}\nD_{\\text{KL}}(Q_{\\boldsymbol x,y} , F_{\\boldsymbol x, y}^{\\boldsymbol \\theta}) &= D_{\\text{KL}}(Q_{\\boldsymbol x} , F_{\\boldsymbol x}^{\\boldsymbol \\theta}) + \\underbrace{  D_{\\text{KL}}(Q_{y| \\boldsymbol x} , F_{y|\\boldsymbol x}^{\\boldsymbol \\theta})   }_{\\geq 0}\\\\\n&\\geq D_{\\text{KL}}(Q_{\\boldsymbol x} , F_{\\boldsymbol x}^{\\boldsymbol \\theta})\n\\end{split}\n\\] Unlike for \\(\\boldsymbol x\\) we do not have observations about the latent state \\(y\\). Nonetheless, we can model the joint distribution \\(Q_{\\boldsymbol x, y} =Q_{\\boldsymbol x}  Q_{y|\\boldsymbol x}\\) by assuming a distribution \\(Q_{y|\\boldsymbol x}\\) over the latent variable.\nThe EM algorithm arises from iteratively decreasing the joint KL divergence \\(D_{\\text{KL}}(Q_{\\boldsymbol x}  Q_{y|\\boldsymbol x} , F_{\\boldsymbol x, y}^{\\boldsymbol \\theta})\\) with regard to both \\(Q_{y|\\boldsymbol x}\\) and \\(\\boldsymbol \\theta\\):\n\n“E” Step: While keeping \\(\\boldsymbol \\theta\\) fixed we vary \\(Q_{y|\\boldsymbol x}\\) to minimise the joint KL divergence. The minimum is reached at \\(D_{\\text{KL}}(Q_{y| \\boldsymbol x} , F_{y|\\boldsymbol x}^{\\boldsymbol \\theta}) = 0\\). This is the case for \\(Q_{y| \\boldsymbol x} = F_{y|\\boldsymbol x}^{\\boldsymbol \\theta}\\), i.e. when the latent distribution \\(Q_{y| \\boldsymbol x}\\) representing the soft allocations is computed by conditioning, i.e. using Bayes’ theorem.\n“M” Step: While keeping \\(Q_{y| \\boldsymbol x}\\) fixed the joint KL divergence is further minimised with regard to \\(\\boldsymbol \\theta\\). This is equivalent to maximising the function \\(\\sum_{k=1}^K \\sum_{i=1}^n q(k | \\boldsymbol x_i)  \\log f(\\boldsymbol x_i, k| \\boldsymbol \\theta)\\) which is indeed the expected complete data log-likelihood.\n\nThus in the “E” step the first argument in the KL divergence is optimised (“I” projection) and in the “M” step it is the second argument that is optimised (“M” projection). In both steps the joint KL divergence always decreases and never increases. Furthermore, at the end of “E” step the joint KL divergence equals the marginal KL divergence. Thus, this procedure implicitly minimises the marginal KL divergence as well, and hence EM maximises the marginal log-likelihood.\nAn alternative way to look at the EM algorithm is in terms of cross-entropy. Using \\(H( Q_{\\boldsymbol x,y}) = H(Q_{\\boldsymbol x}) + H(Q_{y| \\boldsymbol x} )\\) we can rewrite the above upper bound for the joint KL divergence as an equivalent lower bound for \\(n\\) times the negative marginal cross-entropy: \\[\n\\begin{split}\n- n H(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta}) &= \\underbrace{ -n  H(Q_{\\boldsymbol x} Q_{y| \\boldsymbol x} , F_{\\boldsymbol x, y}^{\\boldsymbol \\theta})  + n H(Q_{y| \\boldsymbol x} )}_{\\text{lower bound, ELBO}}  + \\underbrace{ n D_{\\text{KL}}(Q_{y| \\boldsymbol x} , F_{y|\\boldsymbol x}^{\\boldsymbol \\theta})}_{\\geq 0}\\\\\n& \\geq {\\cal F}\\left( Q_{\\boldsymbol x}, Q_{y| \\boldsymbol x},  F_{\\boldsymbol x, y}^{\\boldsymbol \\theta}\\right)\\\\\n\\end{split}\n\\] The lower bound is known as the “ELBO” (“evidence lower bound”). Then the EM algorithm arises by iteratively maximising the ELBO \\(\\cal F\\) with regard to \\(Q_{y| \\boldsymbol x}\\) (“E” step”) and \\(\\boldsymbol \\theta\\) (“M” step).\nThe entropy interpretation of the EM algorithm is due to Csiszàr and Tusnàdy (1984)8 and the ELBO interpretation was introduced by Neal and Hinton (1998)9.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unsupervised learning and clustering</span>"
    ]
  },
  {
    "objectID": "04-clustering.html#footnotes",
    "href": "04-clustering.html#footnotes",
    "title": "4  Unsupervised learning and clustering",
    "section": "",
    "text": "In contrast, in supervised learning (to be discussed in a subsequent chapter) the class labels are known for a subset of the data (the training data set) and are required to learn a prediction function.↩︎\nWard, J.H. 1963. Hierarchical grouping to optimize an objective function. JASA 58:236–244.\nhttps://doi.org/10.1080/01621459.1963.10500845↩︎\nF. Murtagh and P Legendre. 2014. Ward’s hierarchical agglomerative clustering method: which algorithms implement Ward’s criterion? J. Classif. 31:274–295. https://doi.org/10.1007/s00357-014-9161-z↩︎\nH.-H. Bock. 2008. Origins and extensions of the \\(k\\)-means algorithm in cluster analysis. JEHPS 4, no. 2. https://www.jehps.net/Decembre2008/Bock.pdf↩︎\nL. Scrucca L. et. al. 2016. mclust 5: Clustering, classification and density estimation using Gaussian finite mixture models. The R Journal 8:205–233. See https://journal.r-project.org/archive/2016/RJ-2016-021/ and https://mclust-org.github.io/mclust/.↩︎\nDempster, A. P, N. M. Laird and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. JRSS B 39:1–38. https://doi.org/10.1111/j.2517-6161.1977.tb01600.x↩︎\nWu, C.F. 1983. On the convergence properties of the EM algorithm. The Annals of Statistics 11:95–103. https://doi.org/10.1214/aos/1176346060↩︎\nCsiszàr, I., and G, Tusnàdy. 1984. Information geometry and alternating minimization procedures. In Dudewicz, E. J. et al. (eds.) Recent Results in Estimation Theory and Related Topics Statistics and Decisions, Supplement Issue No. 1. pp. 205–237.↩︎\nNeal, R. M., and G. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Jordan, M.I. (eds.). Learning in Graphical Models. pp. 355–368. https://doi.org/10.1007/978-94-011-5014-9_12↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unsupervised learning and clustering</span>"
    ]
  },
  {
    "objectID": "05-classification.html",
    "href": "05-classification.html",
    "title": "5  Supervised learning and classification",
    "section": "",
    "text": "5.1 Aims of supervised learning",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning and classification</span>"
    ]
  },
  {
    "objectID": "05-classification.html#aims-of-supervised-learning",
    "href": "05-classification.html#aims-of-supervised-learning",
    "title": "5  Supervised learning and classification",
    "section": "",
    "text": "Supervised learning vs. unsupervised learning\nUnsupervised learning:\nStarting point:\n\nunlabelled data \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).\n\nAim: find labels \\(y_1, \\ldots, y_n\\) to attach to each sample \\(\\boldsymbol x_i\\).\nFor discrete labels \\(y\\) unsupervised learning is called clustering.\nSupervised learning:\nStarting point:\n\nlabelled training data: \\(\\{\\boldsymbol x_1^{\\text{train}}, y_1^{\\text{train}}\\}\\), \\(\\ldots\\), \\(\\{\\boldsymbol x_n^{\\text{train}}, y_n^{\\text{train}} \\}\\)\nIn addition, we have unlabelled test data: \\(\\boldsymbol x^{\\text{test}}\\)\n\nAim: use training data to learn a function, say \\(h(\\boldsymbol x)\\), to predict the label corresponding to the test data. The predictor function may provide a soft (probabilistic) assignment or a hard assignment of a class label to a test sample.\nFor \\(y\\) discrete supervised learning is called classification. For continuous \\(y\\) the label is called response and supervised learning becomes regression.\nThus, supervised learning is a two-step procedure:\n\nLearn predictor function \\(h(\\boldsymbol x)\\) using the training data \\(\\boldsymbol x_i^{\\text{train}}\\) plus labels \\(y_i^{\\text{train}}\\).\nPredict the label \\(y^{\\text{test}}\\) for the test data \\(\\boldsymbol x^{\\text{test}}\\) using the estimated classifier function: \\(\\hat{y}^{\\text{test}} = \\hat{h}(\\boldsymbol x^{\\text{test}})\\).\n\n\n\nTerminology\nThe function \\(h(\\boldsymbol x)\\) that predicts the class \\(y\\) is called a classifier.\nThere are many types of classifiers, we focus here primarily on probabilistic classifiers (i.e. those that output probabilities for each possible class/label).\nThe challenge is to find a classifier that\n\nexplains the current training data well and\nthat also generalises well to future unseen data.\n\nNote that it is relatively easy to find a predictor that explains the training data but especially in high dimensions (i.e. with many predictors) there is often overfitting and then the predictor does not generalise well!\n\n\n\n\n\n\nFigure 5.1: Illustration of a decision boundary in a two-group classification problem.\n\n\n\nThe decision boundary between the classes is defined as the set of all \\(\\boldsymbol x\\) for which the class assignment by the predictor \\(h(\\boldsymbol x)\\) switches from one class to another (cf. Figure 5.1).\nIn general, simple decision boundaries are preferred over complex decision boundaries to avoid overfitting.\nSome commonly used probabilistic methods for classifications:\n\nQDA (quadratic discriminant analysis)\nLDA (linear discriminant analysis)\nDDA (diagonal discriminant analysis),\nNaive Bayes classification\nlogistic regression\n\nDepending on how the classifiers are trainined there are many variations of the above methods, e.g. Fisher discriminant analysis, regularised LDA, shrinkage disciminant analysis etc.\nCommon non-probabilistic methods for classification include:\n\n\\(k\\)-NN (Nearest Neigbors)\nSVM (support vector machine),\nrandom forest\nneural networks\n\nNeural networks may in fact also be counted under probabilistic models as they essentially are high-dimensional complex nonlinear regression models. Just like a linear regression model can be fitted by “least squares” only without assuming an explicit probabilistic model neural networks are also most often trained by optimising a loss function.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning and classification</span>"
    ]
  },
  {
    "objectID": "05-classification.html#bayesian-discriminant-rule-or-bayes-classifier",
    "href": "05-classification.html#bayesian-discriminant-rule-or-bayes-classifier",
    "title": "5  Supervised learning and classification",
    "section": "5.2 Bayesian discriminant rule or Bayes classifier",
    "text": "5.2 Bayesian discriminant rule or Bayes classifier\nBayes classifiers are based on mixture models:\n\n\\(K\\) groups with \\(K\\) prespecified\neach group has its own distribution \\(F_k\\) with own parameters \\(\\boldsymbol \\theta_k\\)\nthe density of each class is \\(f_k(\\boldsymbol x) = f(\\boldsymbol x| k)\\).\nprior probability of group \\(k\\) is \\(\\text{Pr}(k) = \\pi_k\\) with \\(\\sum_{k=1}^K \\pi_k = 1\\)\nmarginal density is the mixture \\(f(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\\)\n\nFor the moment we do not assume anything particular about the conditional densities \\(f_k(\\boldsymbol x)\\) but later (see the following section) we will focus on normal densities and hence normal classifiers.\nThe posterior probability of group \\(k\\) is given by \\[\n\\text{Pr}(k | \\boldsymbol x) = \\frac{\\pi_k f_k(\\boldsymbol x) }{ f(\\boldsymbol x)}\n\\]\nThis provides a “soft” classification \\[\\boldsymbol h(\\boldsymbol x^{\\text{test}}) = (\\text{Pr}(k=1 | \\boldsymbol x^{\\text{test}}),\\ldots, \\text{Pr}(k=K | \\boldsymbol x^{\\text{test}})   )^T\\] where each possible class \\(k \\in \\{ 1, \\ldots, K\\}\\) is assigned a probability to be the label for the test sample \\(\\boldsymbol x\\).\nThe discriminant function is defined as the logarithm of the posterior probability: \\[\nd_k(\\boldsymbol x) = \\log \\text{Pr}(k | \\boldsymbol x) = \\log \\pi_k  + \\log f_k(\\boldsymbol x)  - \\log f(\\boldsymbol x)\n\\] Since we use \\(d_k\\) to compare the different classes \\(k\\) we can simplify the discriminant function by dropping all constant terms that do not depend on \\(k\\) — in the above this is the term \\(\\log f(\\boldsymbol x)\\). Hence we get for the Bayes discriminant function \\[\nd_k(\\boldsymbol x) = \\log \\pi_k + \\log f_k(\\boldsymbol x) \\,.\n\\]\nFor subsequent “hard” classification \\(h(\\boldsymbol x^{\\text{test}})\\) we then select the group/label for which the value of the discriminant function is maximised: \\[\n\\hat{y}^{\\text{test}} = h(\\boldsymbol x^{\\text{test}}) = \\arg \\max_k d_k(\\boldsymbol x^{\\text{test}}) \\,.\n\\]\nWe have already encountered the Bayes classifier in the EM algorithm to predict the state of the latent variables (soft assignment) and in the \\(K\\)-means algorithm (hard assignment).\nThe discriminant functions \\(d_k(\\boldsymbol x)\\) can be mapped back to the probabilistic class assignment by using the softargmax function (also known as softmax function): \\[\n\\text{Pr}(k | \\boldsymbol x) =\n\\frac{\\exp( d_k(\\boldsymbol x) )}{\\sum_{c=1}^K \\exp( d_c(\\boldsymbol x) ) }  \\,.\n\\] In practise this will be calculated as \\[\n\\text{Pr}(k | \\boldsymbol x) =\n\\frac{\\exp( d_k(\\boldsymbol x) - d_{\\max}(\\boldsymbol x) ) }{\\sum_{c=1}^K \\exp( d_c(\\boldsymbol x) - d_{\\max}(\\boldsymbol x) ) } \\,.\n\\] because subtracting \\(d_{\\max}(\\boldsymbol x) = \\max\\{ d_1(\\boldsymbol x), \\ldots,  d_K(\\boldsymbol x) \\}\\) from all discriminant functions, and thus standardising the maximum of the discriminant functions to zero, avoids numerical overflow problems when computing the exponential function on a computer.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning and classification</span>"
    ]
  },
  {
    "objectID": "05-classification.html#normal-bayes-classifier",
    "href": "05-classification.html#normal-bayes-classifier",
    "title": "5  Supervised learning and classification",
    "section": "5.3 Normal Bayes classifier",
    "text": "5.3 Normal Bayes classifier\n\nQuadratic discriminant analysis (QDA) and Gaussian assumption\nQuadratic discriminant analysis (QDA) is a special case of the Bayes classifier when all group-specific densities are multivariate normal with \\(f_k(\\boldsymbol x) = N(\\boldsymbol x| \\boldsymbol \\mu_k, \\boldsymbol \\Sigma_k)\\). Note in particular that each group \\(k \\in \\{1, \\ldots, K\\}\\) has its own covariance matrix \\(\\boldsymbol \\Sigma_k\\).\nSome calculation leads to the discriminant function for QDA: \\[\nd_k^{QDA}(\\boldsymbol x) = -\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma_k^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) -\\frac{1}{2} \\log \\det(\\boldsymbol \\Sigma_k) +\\log(\\pi_k)\n\\]\nThere are a number of noteworthy things here:\n\nAgain terms are dropped that do not depend on \\(k\\), such as \\(-\\frac{d}{2}\\log( 2\\pi)\\).\nNote the appearance of the squared Mahalanobis distance between \\(\\boldsymbol x\\) and \\(\\boldsymbol \\mu_k\\): \\(d^{\\text{Mahalanobis}}(\\boldsymbol x, \\boldsymbol \\mu| \\boldsymbol \\Sigma)^2 = (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)\\).\nThe QDA discriminant function is quadratic in \\(\\boldsymbol x\\) - hence its name!\nThis implies that the decision boundaries for QDA classification are quadratic (i.e. parabolas in two dimensional settings).\n\nFor Gaussian models specifically it can useful be to multiply the discriminant function by -2 to get rid of the factor \\(-\\frac{1}{2}\\), but note that in that case we then need to find the minimum of the discriminant function rather than the maximum: \\[\nd_k^{QDA (v2)}(\\boldsymbol x) =  (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma_k^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) + \\log \\det(\\boldsymbol \\Sigma_k)  -2 \\log(\\pi_k)\n\\] In the literature you will find both versions of Gaussian discriminant functions so you need to check carefully which convention is used. In the following we will use the first version only.\nDecision boundaries for the QDA classifier can be either linear or nonlinear (quadratic curve). The decision boundary between any two classes \\(i\\) and \\(j\\) require that \\(d^{QDA}_i(\\boldsymbol x) = d^{QDA}_j(\\boldsymbol x)\\), or equivalently &lt;\\(d^{QDA}_i(\\boldsymbol x) - d^{QDA}_j(\\boldsymbol x) = 0\\), which is a quadratic equation.\n\n\nLinear discriminant analysis (LDA)\nLDA is a special case of QDA, with the assumption of common overall covariance across all groups: \\(\\boldsymbol \\Sigma_k = \\boldsymbol \\Sigma\\).\nThis leads to a simplified discriminant function: \\[\nd_k^{LDA}(\\boldsymbol x) = -\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) +\\log(\\pi_k)\n\\] Note that term containing the log-determinant is now gone, and that LDA is essentially now a method that tries to minimize the Mahalanobis distance (while taking also into account the prior class probabilities).\nThe above function can be further simplified, by noting that the quadratic term \\(\\boldsymbol x^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x\\) does not depend on \\(k\\) and hence can be dropped: \\[\n\\begin{split}\nd_k^{LDA}(\\boldsymbol x) &=  \\boldsymbol \\mu_k^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_k^T \\boldsymbol \\Sigma^{-1} \\boldsymbol \\mu_k + \\log(\\pi_k) \\\\\n  &= \\boldsymbol b^T \\boldsymbol x+ a\n\\end{split}\n\\] Thus, the LDA discriminant function is linear in \\(\\boldsymbol x\\), and hence the resulting decision boundaries are linear as well (i.e. straight lines in two-dimensional settings)\n\n\n\n\n\n\nFigure 5.2: Comparison of the linear decision boundary for LDA (left) compared with the nonlinear boundary for QDA (right.\n\n\n\nFigure 5.2 shows an comparison of the linear decision boundaries of LDA compared with the nonlinear boundaries for QDA.\nNote that logistic regression (cf. GLM module) takes on exactly the above linear form and is indeed closely linked with the LDA classifier.\n\n\nDiagonal discriminant analysis (DDA)\nFor DDA we start with the same setting as for LDA, but now we simplify the model even further by additionally requiring a diagonal covariance containing only the variances (thus we assume that all correlations among the predictors \\(x_1, \\ldots, x_d\\) are zero): \\[\n\\boldsymbol \\Sigma= \\boldsymbol V= \\begin{pmatrix}\n    \\sigma^2_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma^2_{d}\n\\end{pmatrix}\n\\] This simplifies the inversion of \\(\\boldsymbol \\Sigma\\) as \\[\n\\boldsymbol \\Sigma^{-1} = \\boldsymbol V^{-1} = \\begin{pmatrix}\n    \\sigma^{-2}_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma^{-2}_{d}\n\\end{pmatrix}\n\\] and leads to the discriminant function \\[\n\\begin{split}\nd_k^{DDA}(\\boldsymbol x) &=  \\boldsymbol \\mu_k^T \\boldsymbol V^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_k^T \\boldsymbol V^{-1} \\boldsymbol \\mu_k + \\log(\\pi_k) \\\\\n  &= \\sum_{j=i}^d \\frac{\\mu_{k,j} x_j - \\mu_{k,j}^2/2}{\\sigma_d^2} + \\log(\\pi_k)\n\\end{split}\n\\] As special case of LDA, the DDA classifier is a linear classifier and thus has linear decision boundaries.\nThe Bayes classifier (using any distribution) assuming uncorrelated predictors is also known as the naive Bayes classifier.\nHence, DDA is a naive Bayes classifier assuming that the underlying densities are normal.\nHowever, don’t let the label “naive” mislead you as DDA and other “naive” Bayes classifiers are often very effective methods for classification and prediction, especially in high-dimensional settings!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning and classification</span>"
    ]
  },
  {
    "objectID": "05-classification.html#the-training-step-learning-qda-lda-and-dda-classifiers-from-data",
    "href": "05-classification.html#the-training-step-learning-qda-lda-and-dda-classifiers-from-data",
    "title": "5  Supervised learning and classification",
    "section": "5.4 The training step — learning QDA, LDA and DDA classifiers from data",
    "text": "5.4 The training step — learning QDA, LDA and DDA classifiers from data\n\nNumber of model parameters\nIn order to predict the class for new data using any of the above discriminant functions we need to first learn the underlying parameters from the training data \\(\\boldsymbol x_i^{\\text{train}}\\) and \\(y_i^{\\text{train}}\\):\n\nFor QDA, LDA and DDA we need to learn \\(\\pi_1, \\ldots, \\pi_K\\) with \\(\\sum_{k=1}^K \\pi_k = 1\\) and the mean vectors \\(\\boldsymbol \\mu_1, \\ldots, \\boldsymbol \\mu_K\\)\nFor QDA we additionally require \\(\\boldsymbol \\Sigma_1, \\ldots, \\boldsymbol \\Sigma_K\\)\nFor LDA we need \\(\\boldsymbol \\Sigma\\)\nFor DDA we estimate \\(\\sigma^2_1, \\ldots, \\sigma^2_d\\).\n\nOverall, the total number of parameters to be estimated when learning the discriminant functions from training data is as follows (see also Figure 5.3):\n\nQDA: \\(K-1+ K d + K \\frac{d(d+1)}{2}\\)\nLDA: \\(K-1+ K d + \\frac{d(d+1)}{2}\\)\nDDA: \\(K-1+ K d + d\\)\n\n\n\n\n\n\n\n\n\nFigure 5.3: Comparison of model complexity of QDA, LDA and DDA.\n\n\n\n\n\n\n\nEstimating the discriminant / predictor function\nFor QDA, LDA and DDA we learn the predictor by estimating the parameters of the discriminant function from the training data.\n\nLarge sample size\nIf the sample size of the training data set is sufficiently large compared to the model dimensions we can use maximum likelihood (ML) to estimate the model parameters. To be able to use ML we need a larger sample size for QDA and LDA (because full covariances need to be estimated) but for DDA a comparatively small sample size can be sufficient (which is one aspect why “naive” Bayes methods are very popular in practise).\nTo obtain the parameters estimates we use the known labels \\(y_i^{\\text{train}}\\) to sort the samples \\(\\boldsymbol x_i^{\\text{train}}\\) into the corresponding classes, and then apply the standard ML estimators. Let \\(g_k =\\{i: y_i^{\\text{train}}=k  \\}\\) be the set of all indices of training sample belonging to group \\(k\\), \\(n_k\\) the sample size in group \\(k\\)\nThe ML estimates of the class probabilities are the frequencies \\[\n\\hat{\\pi}_k = \\frac{n_k}{n}\n\\] and the ML estimate of the group means \\(k=1, \\ldots, K\\) are \\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n_k} \\sum_{i \\in g_k} \\boldsymbol x_i^{\\text{train}} \\, .\n\\] The ML estimate of the global mean \\(\\boldsymbol \\mu_0\\) (i.e. if we assume there is only a single class and ignore the group labels) is \\[\n\\hat{\\boldsymbol \\mu}_0 = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol x_i^{\\text{train}} = \\sum_{k=1}^K \\hat{\\pi}_k \\hat{\\boldsymbol \\mu}_k\n\\] Note the global mean is identical to the pooled mean (i.e. weighted average of the individual group means).\nThe ML estimates for the covariances \\(\\boldsymbol \\Sigma_k\\) for QDA are \\[\n\\widehat{\\boldsymbol \\Sigma}_k = \\frac{1}{n_k} \\sum_{i \\in g_k} ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k)^T\n\\]\nIn order to get the ML estimate of the pooled variance \\(\\boldsymbol \\Sigma\\) for use with LDA we compute \\[\n\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\sum_{k=1}^K \\sum_{i \\in g_k} ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k)^T =  \\sum_{k=1}^K \\hat{\\pi}_k \\widehat{\\boldsymbol \\Sigma}_k\n\\]\nNote that the pooled variance \\(\\boldsymbol \\Sigma\\) differs (substantially!) from the global variance \\(\\Sigma_0\\) that results from simply ignoring class labels and that is computed as \\[\n\\widehat{\\boldsymbol \\Sigma}_0^{ML} = \\frac{1}{n} \\sum_{i =1}^n ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_0) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_0)^T\n\\] You will recognise the above from the variance decomposition in mixture models, with \\(\\boldsymbol \\Sigma_0\\) being the total variance and the pooled \\(\\boldsymbol \\Sigma\\) the unexplained/with-in group variance.\n\n\nSmall sample size\nIf the dimension \\(d\\) is large compared to the sample size then the number of parameters in the predictor function grows fast. Especially QDA but also LDA is data hungry and ML estimation becomes an ill-posed problem.\nAs discussed in Section 1.3 in this instance we need to use a regularised estimator for the covariance(s) such as estimators derived in the framework of penalised ML, Bayesian learning, shrinkage estimation etc. This also ensures that the estimated covariance matrices are positive definite (which is automatically guaranteed only for DDA if all variances are positive).\nFurthermore, in small sample setting it is advised to reduce the number of parameters of the model. Thus using LDA or DDA is preferred over QDA. This can also prevent overfitting and lead to a predictor that generalises better.\nTo analyse high-dimensional data in the worksheets we will employ a regularised version of LDA and DDA using Stein-type shrinkage estimation as discussed in Section 1.3 and implemented in the R package “sda”.\n\n\n\nComparison of estimated decision boundaries: LDA vs. QDA\n\n\n\n\n\n\nFigure 5.4: Decision boundaries for LDA and QDA in the non-nested case.\n\n\n\n\n\n\n\n\n\nFigure 5.5: Decision boundaries for LDA and QDA in the nested case.\n\n\n\nWe compare two simple scenarios using simulated data.\nNon-nested case (\\(K=4\\)): See Figure 5.4. Both LDA and QDA clearly separate the 4 classes. Note the curved decision boundaries for QDA and the linear decision boundaries for LDA.\nNested case (\\(K=2\\)): See Figure 5.5. In the nested case LDA fails to separate the two classes because there is no way to separate two nested classes with a simple linear boundary.\nLater we will investigate the decision boundaries for further methods (see Figure 7.1 and Figure 7.2).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning and classification</span>"
    ]
  },
  {
    "objectID": "05-classification.html#quantifying-prediction-error",
    "href": "05-classification.html#quantifying-prediction-error",
    "title": "5  Supervised learning and classification",
    "section": "5.5 Quantifying prediction error",
    "text": "5.5 Quantifying prediction error\nOnce a classifier has been trained we are naturally interested in its performance to correctly classify previously unseen data points. This is useful for comparing different types of classifiers and also for comparing the same type of classifier using different sets of predictor variables.\n\nQuantification of prediction error based on validation data\nA measure of predictor error compares the predicted label \\(\\hat{y}\\) with the true label \\(y\\) for validation data. A validation data set contains both the \\(\\boldsymbol x_i\\) and the associated label \\(y_i\\) but unlike the training data it has not been used for learning the predictor function.\nFor continuous response often the squared loss is used: \\[\n\\text{err}(\\hat{y}, y) =  (\\hat{y} - y)^2\n\\]\nFor binary outcomes one often employs the 0/1 loss: \\[\n\\text{err}(\\hat{y}, y) =\n\\begin{cases}\n    0, & \\text{if  } \\hat{y}=y\\\\\n    1,  & \\text{otherwise}\n\\end{cases}\n\\] Alternatively, any other quantity derived from the confusion matrix (containing TP, TN, FP, FN) can be used.\nThe mean prediction error is the expectation \\[\nPE = \\text{E}(\\text{err}(\\hat{y}, y))\n\\] and thus the empirical mean prediction error is \\[\n\\widehat{PE} = \\frac{1}{m} \\sum_{i=1}^m \\text{err}(\\hat{y}_i, y_i)\n\\] where \\(m\\) is the sample size of the validation data set.\nMore generally, we can also quantify prediction error in the framework of so-called proper scoring rules, where the whole probabilistic forecast is taken into account (e.g. the individual probabilities for each class, rather than just the selected most probable class). A commonly used scoring rule is the negative log-probability (“surprise”), and the expected surprise is the cross-entropy. So this leads back to entropy and likelihood (see MATH27720 Statistics 2).\nOnce we have an estimate of the prediction error of a model we can use it to compare and choose among a set of candidate models, selecting those with a sufficiently low prediction error.\n\n\nEstimation of prediction error using cross-validation\nUnfortunately, quite often we do not have separate validation data available to evaluate a classifier.\nIn this case we need to rely on a simple algorithmic procedure called cross-validation.\nOutline of cross-validation:\n\nsplit the samples in the training data into a number (say \\(K\\)) parts (“folds”).\nuse each of the \\(K\\) folds as validation data and the other \\(K-1\\) folds as training data.\naverage over the resulting \\(K\\) individual estimates of prediction error, to get an overall aggregated predictor error, along with an error.\n\nNote that in each case one part of the data is reserved for validation and not used for training the predictor.\nWe choose \\(K\\) such that the folds are not too small (to allow estimation of prediction error) but also not too large (to make sure that we actually are able to train a reliable classifier from the remaining data). A typical value for \\(K\\) is 5 or 10, so that 80% respectively 90% of the samples are used for training and the other 20 % or 10% for validation.\nIf \\(K=n\\) there are as many folds as there are samples and the validation data set consists only of a single data point. This is called “leave one out” cross-validation (LOOCV). There are analytic approximations for the prediction error obtained by LOOCV so that this approach is computationally inexpensive for some standard models (including regression).\nFurther reading:\nTo study the technical details of cross-validation: read Section 5.1 Cross-Validation in James et al. (2021) (R version) or James et al. (2023) (Python version).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning and classification</span>"
    ]
  },
  {
    "objectID": "05-classification.html#goodness-of-fit-and-variable-ranking",
    "href": "05-classification.html#goodness-of-fit-and-variable-ranking",
    "title": "5  Supervised learning and classification",
    "section": "5.6 Goodness of fit and variable ranking",
    "text": "5.6 Goodness of fit and variable ranking\nAs in linear regression we are interested in finding out whether the fitted mixture model is an appropriate model, and which particular predictor(s) \\(x_j\\) from \\(\\boldsymbol x=(x_1, \\ldots, x_d)^T\\) are responsible prediction the outcome, i.e. for categorising a sample into group \\(k\\).\nIn order to study these problem it is helpful to rewrite the discriminant function to highlight the influence (or importance) of each predictor.\nWe focus on linear methods (LDA and DDA) and first look at the simple case \\(K=2\\) and then generalise to more than two groups.\n\nLDA with \\(K=2\\) classes\nFor two classes using the LDA discriminant rule will choose group \\(k=1\\) if \\(d_1^{LDA}(\\boldsymbol x) &gt; d_2^{LDA}(\\boldsymbol x)\\), or equivalently, if \\[\n\\Delta_{12}^{LDA} = d_1^{LDA}(\\boldsymbol x) - d_2^{LDA}(\\boldsymbol x) &gt; 0\n\\] Since \\(d_k(\\boldsymbol x)\\) is the log-posterior (plus/minus identical constants) \\(\\Delta^{LDA}\\) is in fact the log-posterior odds of class 1 versus class 2.\nThe difference \\(\\Delta_{12}^{LDA}\\) is \\[\n\\underbrace{ \\Delta_{12}^{LDA}}_{\\text{log posterior odds}} =\n\\underbrace{(\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} \\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right)}_{\\text{log Bayes factor } \\log B_{12}} + \\underbrace{\\log\\left( \\frac{\\pi_1}{\\pi_2} \\right)}_{\\text{log prior odds}}\n\\] Note that since we only consider simple non-composite models here the log-Bayes factor is identical with the log-likelihood ratio!\nThe log Bayes factor \\(\\log B_{12}\\) is known as the weight of evidence in favour of \\(F_1\\) given \\(\\boldsymbol x\\). The expected weight of evidence assuming \\(\\boldsymbol x\\) is indeed from \\(F_1\\) is the Kullback-Leibler discrimination information in favour of group 1, i.e. the KL divergence of from distribution \\(F_2\\) to \\(F_1\\): \\[\n\\text{E}_{F_1} ( \\log B_{12} ) = D_{\\text{KL}}(F_1,  F_2) = \\frac{1}{2} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2) = \\frac{1}{2} \\Omega^2\n\\] This yields, apart of a scale factor, a population version of the Hotelling \\(T^2\\) statistic defined as \\[T^2 =  c^2 (\\hat{\\boldsymbol \\mu}_1 -\\hat{\\boldsymbol \\mu}_2)^T \\hat{\\boldsymbol \\Sigma}^{-1} (\\hat{\\boldsymbol \\mu}_1 -\\hat{\\boldsymbol \\mu}_2)\\] where \\(c = (\\frac{1}{n_1} + \\frac{1}{n_2})^{-1/2} = \\sqrt{n \\pi_1 \\pi_2}\\) is a sample size dependent factor (for \\(\\text{SD}(\\hat{\\boldsymbol \\mu}_1 - \\hat{\\boldsymbol \\mu}_2)\\)). \\(T^2\\) is a measure of fit of the underlying two-component mixture.\nUsing the whitening transformation with \\(\\boldsymbol z= \\boldsymbol W\\boldsymbol x\\) and \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1}\\) we can rewrite the log Bayes factor as \\[\n\\begin{split}\n\\log B_{12} &= \\left( (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol W^T \\right)\\, \\left(\\boldsymbol W\\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right) \\right) \\\\\n&=\\boldsymbol \\omega^T \\boldsymbol \\delta(\\boldsymbol x)\n\\end{split}\n\\] i.e. as the product of two vectors:\n\n\\(\\boldsymbol \\delta(\\boldsymbol x)\\) is the whitened \\(\\boldsymbol x\\) (centered around average means) and\n\\(\\boldsymbol \\omega= (\\omega_1, \\ldots, \\omega_d)^T = \\boldsymbol W(\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)\\) gives the weight of each whitened component \\(\\boldsymbol \\delta(\\boldsymbol x)\\) in the log Bayes factor.\n\nA large positive or negative value of \\(\\omega_j\\) indicates that the corresponding whitened predictor is relevant for choosing a class, whereas small values of \\(\\omega_j\\) close to zero indicate that the corresponding ZCA whitened predictor is unimportant. Furthermore, \\(\\boldsymbol \\omega^T \\boldsymbol \\omega= \\sum_{j=1}^d \\omega_j^2 = (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2) = \\Omega^2\\), i.e. the squared \\(\\omega_j^2\\) provide a component-wise decomposition of the overall fit \\(\\Omega^2\\).\nChoosing ZCA-cor as whitening transformation with \\(\\boldsymbol W=\\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2}\\) we get \\[\n\\boldsymbol \\omega^{ZCA-cor} = \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)\n\\] A better understanding of \\(\\boldsymbol \\omega^{ZCA-cor}\\) is provided by comparing with the two-sample \\(t\\)-statistic \\[\n\\hat{\\boldsymbol \\tau} = c \\hat{\\boldsymbol V}^{-1/2} (\\hat{\\boldsymbol \\mu}_1 - \\hat{\\boldsymbol \\mu}_2)\n\\] With \\(\\boldsymbol \\tau\\) the population version of \\(\\hat{\\boldsymbol \\tau}\\) we can define \\[\\boldsymbol \\tau^{adj} = \\boldsymbol P^{-1/2} \\boldsymbol \\tau= c \\boldsymbol \\omega^{ZCA-cor}\\] as correlation-adjusted \\(t\\)-scores (cat scores). With \\(({\\hat{\\boldsymbol \\tau}}^{adj})^T {\\hat{\\boldsymbol \\tau}}^{adj} = T^2\\) we can see that the cat scores offer a component-wise decomposition of Hotelling’s \\(T^2\\).\nNote the choice of ZCA-cor whitening is to ensure that the whitened components are interpretable and stay maximally correlated to the original variables. However, you may also choose for example PCA whitening in which case the \\(\\boldsymbol \\omega^T \\boldsymbol \\omega\\) provide the variable importance for the PCA whitened variables.\nFor DDA, which assumes that correlations among predictors vanish, i.e. \\(\\boldsymbol P= \\boldsymbol I_d\\), we get \\[\n\\Delta_{12}^{DDA} =\\underbrace{ \\left( (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol V^{-1/2}  \\right)}_{\\text{ } c^{-1} \\boldsymbol \\tau^T }\\, \\underbrace{ \\left( \\boldsymbol V^{-1/2} \\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right) \\right) }_{\\text{centered standardised predictor}}+ \\log\\left( \\frac{\\pi_1}{\\pi_2} \\right) \\\\\n\\] Similarly as above, the \\(t\\)-score \\(\\boldsymbol \\tau\\) determines the impact of the standardised predictor in \\(\\Delta^{DDA}\\).\nConsequently, in DDA we can rank predictors by the squared \\(t\\)-score. Recall that in standard linear regression with uncorrelated predictors we can find the most important predictors by ranking the squared marginal correlations – ranking by (squared) \\(t\\)-scores in DDA is the exact analogy but for discrete response.\n\n\nMultiple classes\nFor more than two classes we need to refer to the so-called pooled centroids formulation of DDA and LDA (introduced by Tibshirani 2002).\nThe pooled centroid is given by \\(\\boldsymbol \\mu_0 = \\sum_{k=1}^K \\pi_k \\boldsymbol \\mu_k\\) — this is the centroid if there would be only a single class. The corresponding probability (for a single class) is \\(\\pi_0=1\\) and the distribution is called \\(F_0\\).\nThe LDA discriminant function for this “group 0” is \\[\nd_0^{LDA}(\\boldsymbol x) = \\boldsymbol \\mu_0^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_0^T \\boldsymbol \\Sigma^{-1} \\boldsymbol \\mu_0\n\\] and the log posterior odds for comparison of group \\(k\\) with the pooled group \\(0\\) is \\[\n\\begin{split}\n\\Delta_k^{LDA} &= d_k^{LDA}(\\boldsymbol x) - d_0^{LDA}(\\boldsymbol x) \\\\\n         &= \\log B_{k0} + \\log(\\pi_k) \\\\\n         &= \\boldsymbol \\omega_k^T \\boldsymbol \\delta_k(\\boldsymbol x) + \\log(\\pi_k)\n\\end{split}\n\\] with \\[\n\\boldsymbol \\omega_k = \\boldsymbol W(\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)  \n\\] and \\[\n\\boldsymbol \\delta_k(\\boldsymbol x) = \\boldsymbol W(\\boldsymbol x- \\frac{\\boldsymbol \\mu_k +\\boldsymbol \\mu_0}{2} )\n\\] The expected log Bayes factor is \\[\n\\text{E}_{F_k} ( \\log B_{k0} )= KL(F_k || F_0) = \\frac{1}{2} (\\boldsymbol \\mu_k -\\boldsymbol \\mu_0)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_k -\\boldsymbol \\mu_0) = \\frac{1}{2} \\Omega_k^2\n\\]\nWith scale factor \\(c_k  = (\\frac{1}{n_k} - \\frac{1}{n})^{-1/2} = \\sqrt{n \\frac{\\pi_k}{1-\\pi_k}}\\) (for \\(\\text{SD}(\\hat{\\boldsymbol \\mu}_k-\\hat{\\boldsymbol \\mu}_0)\\), with the minus sign before \\(\\frac{1}{n}\\) due to correlation between \\(\\hat{\\boldsymbol \\mu}_k\\) and pooled mean \\(\\hat{\\boldsymbol \\mu}_0\\)) we get as correlation-adjusted \\(t\\)-score for comparing mean of group \\(k\\) with the pooled mean \\[\n\\boldsymbol \\tau_k^{adj} = c_k \\boldsymbol \\omega_k^{ZCA-cor} \\,.\n\\]\nFor the two class case (\\(K=2\\)) we get with \\(\\boldsymbol \\mu_0 = \\pi_1 \\boldsymbol \\mu_1 + \\pi_2 \\boldsymbol \\mu_2\\) for the mean difference \\((\\boldsymbol \\mu_1 - \\boldsymbol \\mu_0) = \\pi_2 (\\boldsymbol \\mu_1 - \\boldsymbol \\mu_2)\\) and with \\(c_1 =  \\sqrt{n \\frac{\\pi_1}{\\pi_2}}\\) this yields \\[\n\\boldsymbol \\tau_1^{adj} = \\sqrt{n \\pi_1 \\pi_2 } \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} (\\boldsymbol \\mu_1 - \\boldsymbol \\mu_2) ,\n\\] i.e. the exact same score as in the two-class setting.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning and classification</span>"
    ]
  },
  {
    "objectID": "05-classification.html#variable-selection",
    "href": "05-classification.html#variable-selection",
    "title": "5  Supervised learning and classification",
    "section": "5.7 Variable selection",
    "text": "5.7 Variable selection\nIn the previous we saw that in DDA the natural score for ranking features with regard to their relevance in separating the classes is the (squared) \\(t\\)-score, and for LDA a whitened version such as the squared correlation-adjusted \\(t\\)-score (based on ZCA-cor whitening) may be used. Once such a ranking has been established the question of a suitable cutoff arises, i.e.  how many features need (or should) be retained in a model.\nFor large and high-dimensional models feature selection can also be viewed as a form of regularisation and also dimension reduction. Specifically, there may be many variables/ features that do no contribute to the class prediction. Despite having in principle no effect on the outcome the presence of these “null variables” can nonetheless deterioriate (sometimes dramatically!) the overall predictive accuracy of a trained predictor, because they add noise and increase the model dimension. Therefore, variables that do not contribute to prediction should be filtered out in order to be able to construct good prediction models and classifiers.\n\nChoosing a threshold by multiple testing using false discovery rates\nThe most simple way to determine a cutoff threshold is to use a standard technique for multiple testing.\nFor each predictor variable \\(x_1, \\ldots, x_d\\) we have a corresponding test statistic measuring the influence of this variable on the response, for example the the \\(t\\)-scores and related statistics discussed in the previous section. In addition to providing an overall ranking the set of all these statistics can be used to determine a suitable cutoff by trying to separate two populations of predictor variables:\n\n“Null” variables that do not contribute to prediction\n“Alternative” variables that are linked to prediction\n\nThis can be done as follows:\n\nThe distribution of the observed test statistics \\(z_i\\) is assumed to follow a two-component mixture where \\(F_0(z)\\) and \\(F_A(z)\\) are the distributions corresponding to the null and the alternative, \\(f_0(z)\\) and \\(f_a(z)\\) the densities, and \\(\\pi_0\\) and \\(\\pi_A=1-\\pi_0\\) are the weights: \\[\nf(z) = \\pi_0 f_0(z) + (1-\\pi_0) f_a(z)\n\\]\nThe null model is typically from a parametric family (e.g. normal around zero and with a free variance parameter) whereas the alternative is often modelled nonparametrically.\nAfter fitting the mixture model, often assuming some additional constraints to make the mixture identifiable, one can compute false discovery rates (FDR) as follows:\nLocal FDR: \\[\n\\widehat{\\text{fdr}}(z_i) = \\widehat{\\text{Pr}}(\\text{null} | z_i) = \\frac{\\hat{\\pi}_0 \\hat{f}_0(z_i)}{\\hat{f}(z_i)}  \n\\]\nTail-area-based FDR (=\\(q\\)-value): \\[\n\\widehat{\\text{Fdr}}(z_i) = \\widehat{\\text{Pr}}(\\text{null} | Z &gt; z_i) = \\frac{\\hat{\\pi}_0 \\hat{F}_0(z_i)}{\\hat{F}(z_i)}\n\\] Note these are essentially \\(p\\)-values adjusted for multiple testing (by a variant of the Benjamini-Hochberg method).\n\nBy thresholding false discovery rates it is possible to identify those variables that clearly belong to each of the two groups but also those features that cannot easily be discriminated to fall into either group:\n\n“alternative” variables have low local FDR, e.g, \\(\\widehat{\\text{fdr}}(z_i) \\leq 0.2\\)\n“null” variables have high local FDR, e.g. \\(\\widehat{\\text{fdr}}(z_i) \\geq 0.8\\)\nfeatures that cannot easily classified as null or alternative, e.g. \\(0.2 &lt; \\widehat{\\text{fdr}}(z_i) &lt; 0.8\\)\n\nFor feature selection in prediction settings we generally aim to remove only those variable that clearly belong to the null group, leaving all others in the model.\n\n\nVariable selection using cross-validation\nA conceptually simple but computationally more expensive approach to variable selection is to estimate the predicion error of the same type of predictor but with different sets of predictors using cross-validation, and then choosing a predictor that achieves good prediction accuracy while using only a small number of featurs.\nThis is a method that works very well in practise as is demonstrated in a number of problems in the worksheets.\n\n\n\n\nJames, G., D. Witten, T. Hastie, and R. Tibshirani. 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nJames, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023. An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-3-031-38747-0.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning and classification</span>"
    ]
  },
  {
    "objectID": "06-dependence.html",
    "href": "06-dependence.html",
    "title": "6  Multivariate dependencies",
    "section": "",
    "text": "6.1 Measuring the linear association between two sets of random variables",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate dependencies</span>"
    ]
  },
  {
    "objectID": "06-dependence.html#measuring-the-linear-association-between-two-sets-of-random-variables",
    "href": "06-dependence.html#measuring-the-linear-association-between-two-sets-of-random-variables",
    "title": "6  Multivariate dependencies",
    "section": "",
    "text": "Aim\nThe linear association between two scalar random variables \\(x\\) and \\(y\\) is measured by the correlation \\(\\text{Cor}(x, y) = \\rho\\).\nIn this chapter we now would like to explore how to generalise correlation to the case of two random vectors. Specifically, we would like to find a scalar measure of association between two random vectors (or equivalently two sets of random variables) \\(\\boldsymbol x= (x_1, \\ldots, x_p)^T\\) and \\(\\boldsymbol y= (y_1, \\ldots, y_q)^T\\) that contains correlation and also multiple correlation as special case.\nWe assume a joint correlation matrix \\[\n\\boldsymbol P=\n\\begin{pmatrix}\n\\boldsymbol P_{\\boldsymbol x} &  \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol P_{\\boldsymbol y\\boldsymbol x} & \\boldsymbol P_{\\boldsymbol y} \\\\\n\\end{pmatrix}\n\\] with cross-correlation matrix \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = \\boldsymbol P_{\\boldsymbol y\\boldsymbol x}^T\\) and the within-group group correlations \\(\\boldsymbol P_{\\boldsymbol x}\\) and \\(\\boldsymbol P_{\\boldsymbol y}\\). If the cross-correlations vanish, \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} =0\\), then the two random vectors are uncorrelated, and the joint correlation matrix becomes a diagonal block matrix \\[\n\\boldsymbol P_{\\text{indep}} =\n\\begin{pmatrix}\n\\boldsymbol P_{\\boldsymbol x} &  0 \\\\\n0 & \\boldsymbol P_{\\boldsymbol y} \\\\\n\\end{pmatrix} \\, .\n\\]\nTo characterise the total association between \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) we are looking for a scalar quantity measuring the divergence of a distribution assuming the general joint correlation matrix \\(\\boldsymbol P\\) from a distribution assuming the joint correlation matrix \\(\\boldsymbol P_{\\text{indep}}\\) for uncorrelated \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\).\n\n\nKnown special cases\nIdeally, in case of an univariate \\(y\\) but multivariate \\(\\boldsymbol x\\) this measure should reduce to the squared multiple correlation or coefficient of determination \\[\n\\text{MCor}(\\boldsymbol x, y)^2 = \\boldsymbol P_{y\\boldsymbol x} \\boldsymbol P_{\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\n\\] This is well-known in linear regression to describe the strength of the total linear association between the predictors \\(\\boldsymbol x= (x_1, \\ldots, x_p)^T\\) and the response \\(y\\).\nTo derive the squared multiple correlation we may proceed as follows. First we whiten the random vector \\(\\boldsymbol x\\) resulting in \\(\\boldsymbol z_{\\boldsymbol x} = \\boldsymbol W_{\\boldsymbol x}\\boldsymbol x=\\boldsymbol Q_{\\boldsymbol x}\\boldsymbol P_{\\boldsymbol x}^{-1/2}\\boldsymbol V_{\\boldsymbol x}^{-1/2}\\boldsymbol x\\) where \\(\\boldsymbol Q_{\\boldsymbol x}\\) is an orthogonal matrix. The correlations between each component in \\(\\boldsymbol z_{\\boldsymbol x}\\) and the response \\(y\\) are then \\[\n\\boldsymbol \\omega= \\text{Cor}(\\boldsymbol z_{\\boldsymbol x}, y) = \\boldsymbol Q_{\\boldsymbol x} \\boldsymbol P_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol xy}\n\\] As \\(\\text{Var}(\\boldsymbol z_{\\boldsymbol x}) = \\boldsymbol I\\) and thus the components in \\(\\boldsymbol z_{\\boldsymbol x}\\) are uncorrelated we can simply add up the squared individual correlations to get as total association measure \\[\n\\boldsymbol \\omega^T \\boldsymbol \\omega= \\sum_i \\omega_i^2 = \\boldsymbol P_{y\\boldsymbol x} \\boldsymbol P_{\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\n\\] Note that the particular choice of the orthogonal matrix \\(\\boldsymbol Q_{\\boldsymbol x}\\) for whitening \\(\\boldsymbol x\\) is not relevant for the squared multiple correlation.\nNote that if the cross-correlations vanish (\\(\\boldsymbol P_{\\boldsymbol xy} =0\\)) then \\(\\text{MCor}(\\boldsymbol x, y)^2=0\\). If the correlation between the predictors vanishes (\\(\\boldsymbol P_{\\boldsymbol x} = \\boldsymbol I\\)) then \\(\\text{MCor}(\\boldsymbol x, y)^2 = \\sum_i \\rho_{y x_i}^2\\), i.e. it is the sum of the squared cross-correlations.\nIf there is only a single predictor \\(x\\) then \\(\\boldsymbol P_{xy}=\\rho\\) and \\(\\boldsymbol P_{x} = 1\\) and the squared multiple correlation reduces to the squared Pearson correlation \\[\n\\text{Cor}(x, y)^2 = \\rho^2 \\, .\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate dependencies</span>"
    ]
  },
  {
    "objectID": "06-dependence.html#canonical-correlation-analysis-cca-aka-cca-whitening",
    "href": "06-dependence.html#canonical-correlation-analysis-cca-aka-cca-whitening",
    "title": "6  Multivariate dependencies",
    "section": "6.2 Canonical Correlation Analysis (CCA) aka CCA whitening",
    "text": "6.2 Canonical Correlation Analysis (CCA) aka CCA whitening\nCanonical correlation analysis was invented by Harald Hotelling in 1936 1. CCA aims to characterise the linear dependence between to random vectors \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) by a set of canonical correlations \\(\\lambda_i\\).\nCCA works by simultaneously whitening the two random vectors \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) where the whitening matrices are chosen in such a way that the cross-correlation matrix between the resulting whitened variables becomes diagonal, and the elements on the diagonal correspond to the canonical correlations.\n\\[\\begin{align*}\n\\begin{array}{ll}\n\\boldsymbol x= \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_p \\end{pmatrix} \\\\\n\\text{Dimension } p\n\\end{array}\n\\begin{array}{ll}\n\\boldsymbol y= \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_q \\end{pmatrix} \\\\\n\\text{Dimension } q\n\\end{array}\n\\begin{array}{ll}\n\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma_{\\boldsymbol x} = \\boldsymbol V_{\\boldsymbol x}^{1/2}\\boldsymbol P_{\\boldsymbol x}\\boldsymbol V_{\\boldsymbol x}^{1/2} \\\\\n\\text{Var}(\\boldsymbol y) = \\boldsymbol \\Sigma_{\\boldsymbol y} = \\boldsymbol V_{\\boldsymbol y}^{1/2}\\boldsymbol P_{\\boldsymbol y}\\boldsymbol V_{\\boldsymbol y}^{1/2} \\\\\n\\end{array}\n\\end{align*}\\]\n\\[\\begin{align*}\n\\begin{array}{cc}\n\\text{Whitening of } \\boldsymbol x\\text{:} \\\\\n\\text{Whitening of } \\boldsymbol y\\text{:}\n\\end{array}\n\\begin{array}{cc}\n\\boldsymbol z_{\\boldsymbol x} = \\boldsymbol W_{\\boldsymbol x}\\boldsymbol x=\\boldsymbol Q_{\\boldsymbol x}\\boldsymbol P_{\\boldsymbol x}^{-1/2}\\boldsymbol V_{\\boldsymbol x}^{-1/2}\\boldsymbol x\\\\\n\\boldsymbol z_{\\boldsymbol y} = \\boldsymbol W_{\\boldsymbol y}\\boldsymbol y=\\boldsymbol Q_{\\boldsymbol y}\\boldsymbol P_{\\boldsymbol y}^{-1/2}\\boldsymbol V_{\\boldsymbol y}^{-1/2}\\boldsymbol y\n\\end{array}\n\\end{align*}\\] (note we use the correlation-based form of \\(\\boldsymbol W\\))\nCross-correlation between \\(\\boldsymbol z_{\\boldsymbol y}\\) and \\(\\boldsymbol z_{\\boldsymbol y}\\):\n\\[\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})=\\boldsymbol Q_{\\boldsymbol x}\\boldsymbol K\\boldsymbol Q_{\\boldsymbol y}^T\\]\nwith \\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2}\\boldsymbol P_{\\boldsymbol x\\boldsymbol y}\\boldsymbol P_{\\boldsymbol y}^{-1/2}\\).\nIdea: we can choose suitable orthogonal matrices \\(\\boldsymbol Q_{\\boldsymbol x}\\) and \\(\\boldsymbol Q_{\\boldsymbol y}\\) by putting a structural constraint on the cross-correlation matrix.\nCCA: we aim for a diagonal \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})\\) so that each component in \\(\\boldsymbol z_{\\boldsymbol x}\\) only influences one (the corresponding) component in \\(\\boldsymbol z_{\\boldsymbol y}\\).\nMotivation: pairs of “modules” represented by components of \\(\\boldsymbol z_{\\boldsymbol x}\\) and \\(\\boldsymbol z_{\\boldsymbol y}\\) influencing each other (and not anyone other module).\n\\[\n\\begin{array}{ll}\n\\boldsymbol z_{\\boldsymbol x} = \\begin{pmatrix} z^x_1 \\\\ z^x_2 \\\\ \\vdots \\\\ z^x_p \\end{pmatrix} &\n\\boldsymbol z_{\\boldsymbol y} = \\begin{pmatrix} z^y_1 \\\\ z^y_2 \\\\ \\vdots \\\\ z^y_q \\end{pmatrix} \\\\\n\\end{array}\n\\]\n\\[\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y}) = \\begin{pmatrix} \\lambda_1 & \\dots & 0 \\\\ \\vdots &  \\vdots \\\\ 0 & \\dots & \\lambda_m \\end{pmatrix}\\]\nwhere \\(\\lambda_i\\) are the canonical correlations and \\(m=\\min(p,q)\\).\n\nHow to make cross-correlation matrix \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})\\) diagonal?\n\nUse Singular Value Decomposition (SVD) of matrix \\(\\boldsymbol K\\):\n\\[\\boldsymbol K= (\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}})^T  \\boldsymbol \\Lambda\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\] where \\(\\boldsymbol \\Lambda\\) is the diagonal matrix containing the singular values of \\(\\boldsymbol K\\)\nThis yields orthogonal matrices \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) and \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) and thus the desired whitening matrices \\(\\boldsymbol W_{\\boldsymbol x}^{\\text{CCA}}\\) and \\(\\boldsymbol W_{\\boldsymbol y}^{\\text{CCA}}\\)\nAs a result \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y}) = \\boldsymbol \\Lambda\\) i.e. singular values \\(\\lambda_i\\) of \\(\\boldsymbol K\\) are the desired canonical correlations!\n\n\\(\\longrightarrow\\) \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) and \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) are determined by the diagonality constraint (and note these are different to the other previously discussed whitening methods).\nNote that the signs of corresponding in columns in \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) and \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) are not identified. Traditionally, in an SVD the signs are chosen such that the singular values are positive. However, if we impose positive-diagonality on \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) and \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\), and thus positive-diagonality on the cross-correlations \\(\\boldsymbol \\Psi_{\\boldsymbol x}\\) and \\(\\boldsymbol \\Psi_{\\boldsymbol y}\\), then the canonical correlations may take on both positive and negative values.\n\n\nRelated methods\n\nO2PLS: similar to CCA but using orthogonal projections rather than whitening.\nVector correlation: aggregates the squared canonical correlations into a single overall measure (see below).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate dependencies</span>"
    ]
  },
  {
    "objectID": "06-dependence.html#vector-correlation-and-rv-coefficient",
    "href": "06-dependence.html#vector-correlation-and-rv-coefficient",
    "title": "6  Multivariate dependencies",
    "section": "6.3 Vector correlation and RV coefficient",
    "text": "6.3 Vector correlation and RV coefficient\n\nVector alienation coefficient\nIn his 1936 paper introducing canonical correlation analysis Hotelling also proposed the vector alienation coefficient defined as \\[\n\\begin{split}\na(\\boldsymbol x, \\boldsymbol y) &= \\frac{\\det(\\boldsymbol P)}{\\det(\\boldsymbol P_{\\text{indep}}) } \\\\\n            & = \\frac{\\det( \\boldsymbol P) }{  \\det(\\boldsymbol P_{\\boldsymbol x}) \\,  \\det(\\boldsymbol P_{\\boldsymbol y})  }\n\\end{split}\n\\] With \\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y}  \\boldsymbol P_{\\boldsymbol y}^{-1/2}\\) the vector alienation coefficient can be written (using the Weinstein-Aronszajn determinant identity and the formula for the determinant of block-structured matrices) as \\[\n\\begin{split}\na(\\boldsymbol x, \\boldsymbol y) & = \\det \\left( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T \\right) \\\\\n            & = \\det \\left( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K\\right) \\\\\n            &= \\prod_{i=1}^m (1-\\lambda_i^2) \\\\\n\\end{split}\n\\] where the \\(\\lambda_i\\) are the singular values of \\(\\boldsymbol K\\), i.e. the canonical correlations for the pair \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\). Therefore, the vector alienation coefficient is computed as a summary statistic of the canonical correlations.\nIf \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = 0\\) und thus \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) are uncorrelated then \\(\\boldsymbol P= \\boldsymbol P_{\\text{indep}}\\) and thus by construction the vector alienation coefficient \\(a(\\boldsymbol x, \\boldsymbol y)=1\\). Hence, the vector alienation coefficient is itself not a generalisation of the squared multiple correlation to the case of two random vectors as such a quantity should vanish in this case.\n\n\nRozeboom vector correlation\nInstead, Rozeboom (1965) 2 proposed to use as squared vector correlation the complement of the vector alienation coefficient \\[\n\\begin{split}\n\\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2 &= \\rho^2_{\\boldsymbol x\\boldsymbol y} = 1 - a(\\boldsymbol x, \\boldsymbol y) \\\\\n& = 1- \\det\\left( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T \\right) \\\\\n& = 1- \\det\\left( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K\\right) \\\\\n&  =1- \\prod_{i=1}^m (1-\\lambda_i^2) \\\\\n\\end{split}\n\\] If \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = 0\\) then \\(\\boldsymbol K=\\mathbf 0\\) and hence \\(\\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2 = 0\\).\nMoreover, if either \\(p=1\\) or \\(q=1\\) the squared vector correlation reduces to the corresponding squared multiple correlation, which in turn for both \\(p=1\\) and \\(q=1\\) becomes the squared Pearson correlation.\nYou can find the derivation in Example Sheet 10.\nThus, Rozeboom’s vector correlation indeed generalises both Pearson correlation and the multiple correlation coefficient.\n\n\nRV coefficient\nAnother common approach to measure association between two random vectors is the RV coefficient introduced by Robert and Escoufier in 1976 as \\[\nRV(\\boldsymbol x, \\boldsymbol y) = \\frac{ \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} )}{ \\sqrt{ \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol x}^2) \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol y}^2)  } }\n\\] The main advantage of the RV coefficient is that it is easier to compute than the Rozeboom vector correlation as it uses the matrix trace rather than the matrix determinant.\nFor \\(q=p=1\\) the RV coefficient reduces to the squared correlation. However, the RV coefficient does not reduce to the multiple correlation coefficient for \\(q=1\\) and \\(p &gt; 1\\), and therefore the RV coefficient cannot be considered a coherent generalisation of Pearson and multiple correlation to the case when \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) are random vectors.\nSee also Worksheet 10.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate dependencies</span>"
    ]
  },
  {
    "objectID": "06-dependence.html#limits-of-linear-models-and-correlation",
    "href": "06-dependence.html#limits-of-linear-models-and-correlation",
    "title": "6  Multivariate dependencies",
    "section": "6.4 Limits of linear models and correlation",
    "text": "6.4 Limits of linear models and correlation\n\nCorrelation measures only linear dependence\nLinear models and measures of linear association (correlation) are very effective tools. However, it is important to recognise their limits especially when modelling nonlinear relationships.\nA very simple demonstration of this is given by the following example. Assume \\(x\\) is a normally distributed random variable with \\(x \\sim N(0, \\sigma^2)\\) with mean zero and some variance \\(\\sigma^2\\). From \\(x\\) we construct a second random variable \\(y = x^2\\). Despite that \\(y\\) is a function of \\(x\\) with no extra added noise it is easy to show that \\(\\text{Cov}(x, y) = \\text{Cov}(x, x^2)=0\\). Hence, the correlation \\(\\text{Cor}(x, y)= \\text{Cor}(x, x^2) = 0\\) also vanishes.\nThis can be empirically verified by simulating data from a normal distribution (here with \\(\\sigma^2=4)\\) and estimating the correlation:\n\nx=rnorm(1000000, mean=0, sd=2)\ny = x^2\ncor(x,y)\n\n[1] 0.0006495226\n\n\nThus, correlation is zero even though \\(x\\) and \\(y\\) are fully dependent variables. This is because correlation only measures linear association, and the relationship between \\(x\\) and \\(y\\) is nonlinear.\n\n\nAnscombe datasets\n\n\n\n\n\n\n\n\nFigure 6.1: The Anscombe (1973) quartet of datasets.\n\n\n\n\n\nUsing correlation (and more generally linear models) blindly can easily hide the underlying complexity of the analysed data. This is demonstrated by the classic “Anscombe quartet” of datasets presented in his 1973 paper 3.\nAs evident from the scatter plots (Figure 6.1) the relationship between the two variables \\(x\\) and \\(y\\) is very different in the four cases. However, intriguingly all four data sets share exactly the same linear characteristics and summary statistics:\n\nMeans \\(m_x = 9\\) and \\(m_y = 7.5\\)\nVariances \\(s^2_x = 11\\) and \\(s^2_y = 4.13\\)\nCorrelation \\(r = 0.8162\\)\nLinear model fit with intercept \\(a=3.0\\) and slope \\(b=0.5\\)\n\nThus, in actual data analysis it is always a good idea to inspect the data visually to get a first impression whether using a linear model makes sense.\nIn the above only data set “a” follows a linear model. Data set “b” represents a quadratic relationship. Data set “c” is linear but with an outlier that disturbs the linear relationship. Finally data set “d” also contains an outlier but also represent a case where \\(y\\) is (apart from the outlier) is not dependent on \\(x\\).\nIn the Worksheet 10 a more recent version of the Anscombe quartet will be analysed in the form of the “datasauRus” dozen - 13 highly nonlinear datasets that all share the same linear characteristics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate dependencies</span>"
    ]
  },
  {
    "objectID": "06-dependence.html#mutual-information-as-generalisation-of-correlation",
    "href": "06-dependence.html#mutual-information-as-generalisation-of-correlation",
    "title": "6  Multivariate dependencies",
    "section": "6.5 Mutual information as generalisation of correlation",
    "text": "6.5 Mutual information as generalisation of correlation\n\nOverview\nA more general way than the vector correlation to measure multivariate association is mutual information (MI) which not only covers linear but also non-linear associations.\nAs we will see below the Rozeboom vector correlation arises naturally when computing the MI for the multivariate normal distribution, hence MI also recovers well-known measures of linear association (including multiple correlation and simple correlation), thus truly generalising correlation as measure of association.\n\n\nDefinition of mutual information\nRecall the definition of Kullback-Leibler (KL) divergence between two distributions: \\[\nD_{\\text{KL}}(F,  G) := \\text{E}_F \\log \\biggl( \\frac{f(\\boldsymbol x)}{g(\\boldsymbol x)} \\biggr)\n\\] Here \\(F\\) plays the role of the reference distribution and \\(G\\) is an approximating distribution, with \\(f\\) and \\(g\\) being the corresponding density functions (see MATH27720 Statistics 2 for more details about the KL divergence and its properties).\nThe Mutual Information (MI) between two random variables \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) is defined as the KL divergence between the corresponding joint distribution and the product distribution: \\[\n\\text{MI}(\\boldsymbol x, \\boldsymbol y) = D_{\\text{KL}}(F_{\\boldsymbol x,\\boldsymbol y}, F_{\\boldsymbol x}  F_{\\boldsymbol y}) = \\text{E}_{F_{\\boldsymbol x,\\boldsymbol y}}  \\log \\biggl( \\frac{f(\\boldsymbol x, \\boldsymbol y)}{f(\\boldsymbol x) \\, f(\\boldsymbol y)} \\biggr) .\n\\] Thus, MI measures how well the joint distribution can be approximated by the product distribution (which would be the appropriate joint distribution if \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) are independent). Since MI is an application of KL divergence is shares all its properties. In particular, \\(\\text{MI}(\\boldsymbol x, \\boldsymbol y)=0\\) implies that the joint distribution and product distributions are the same. Hence the two random variables \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) are independent if the mutual information vanishes.\n\n\nMutual information between two normal scalar variables\nThe KL divergence between two multivariate normal distributions \\(F_{\\text{ref}}\\) and \\(F\\) is \\[\nD_{\\text{KL}}(F_{\\text{ref}}, F)  = \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n     - d   \\biggr\\}\n\\] This allows compute the mutual information \\(\\text{MI}_{\\text{norm}}(x,y)\\) between two univariate random variables \\(x\\) and \\(y\\) that are correlated and assumed to be jointly bivariate normal. Let \\(\\boldsymbol z= (x, y)^T\\). The joint bivariate normal distribution is characterised by the mean \\(\\text{E}(\\boldsymbol z) = \\boldsymbol \\mu= (\\mu_x, \\mu_y)^T\\) and the covariance matrix \\[\n\\boldsymbol \\Sigma=\n\\begin{pmatrix}\n\\sigma^2_x & \\rho \\, \\sigma_x \\sigma_y \\\\\n\\rho \\, \\sigma_x  \\sigma_y & \\sigma^2_y \\\\\n\\end{pmatrix}\n\\] where \\(\\text{Cor}(x,y)= \\rho\\). If \\(x\\) and \\(y\\) are independent then \\(\\rho=0\\) and \\[\n\\boldsymbol \\Sigma_{\\text{indep}} =\n\\begin{pmatrix} \\sigma^2_x & 0 \\\\ 0 & \\sigma^2_y \\\\ \\end{pmatrix} \\,.\n\\] The product \\[\n\\boldsymbol A= \\boldsymbol \\Sigma_{\\text{indep}}^{-1} \\boldsymbol \\Sigma=\n\\begin{pmatrix}\n1 & \\rho \\frac{\\sigma_y}{\\sigma_x} \\\\\n\\rho \\frac{\\sigma_x}{\\sigma_y} & 1 \\\\\n\\end{pmatrix}\n\\] has trace \\(\\text{Tr}(\\boldsymbol A) = 2\\) and determinant \\(\\det(\\boldsymbol A) = 1-\\rho^2\\).\nWith this the mutual information between \\(x\\) and \\(y\\) can be computed as \\[\n\\begin{split}\n\\text{MI}_{\\text{norm}}(x, y) &= D_{\\text{KL}}(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma),  N(\\boldsymbol \\mu,\\boldsymbol \\Sigma_{\\text{indep}} )) \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n     - 2   \\biggr\\} \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}( \\boldsymbol A)\n    - \\log \\det( \\boldsymbol A)\n     - 2   \\biggr\\} \\\\\n&=  -\\frac{1}{2} \\log(1-\\rho^2) \\\\\n  & \\approx \\frac{\\rho^2}{2} \\\\\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\nFigure 6.2: Relationship between correlation and mutual information.\n\n\n\n\n\nThus \\(\\text{MI}_{\\text{norm}}(x,y)\\) is a one-to-one function of the squared correlation \\(\\rho^2\\) between \\(x\\) and \\(y\\) (see Figure 6.2).\nFor small values of \\(|\\rho| &lt; 0.5\\) around zero the relationship between mutual information and correlation is quadratic, \\(2 \\, \\text{MI}_{\\text{norm}}(x,y) \\approx \\rho^2\\) (dotted line in Figure 6.2).\n\n\nMutual information between two normally distributed random vectors\nThe mutual information \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x,\\boldsymbol y)\\) between two multivariate normal random vector \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) can be computed in a similar fashion as in the bivariate case.\nLet \\(\\boldsymbol z= (\\boldsymbol x, \\boldsymbol y)^T\\) with dimension \\(d=p+q\\). The joint multivariate normal distribution is characterised by the mean \\(\\text{E}(\\boldsymbol z) = \\boldsymbol \\mu= (\\boldsymbol \\mu_x^T, \\boldsymbol \\mu_y^T)^T\\) and the covariance matrix \\[\n\\boldsymbol \\Sigma=\n\\begin{pmatrix} \\boldsymbol \\Sigma_{\\boldsymbol x} & \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}^T & \\boldsymbol \\Sigma_{\\boldsymbol y} \\\\\n\\end{pmatrix} \\,.\n\\] If \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) are independent then \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} = 0\\) and \\[\n\\boldsymbol \\Sigma_{\\text{indep}} =\n\\begin{pmatrix}  \n\\boldsymbol \\Sigma_{\\boldsymbol x} & 0 \\\\\n0 & \\boldsymbol \\Sigma_{\\boldsymbol y} \\\\\n\\end{pmatrix} \\, .\n\\] The product \\[\n\\begin{split}\n\\boldsymbol A& =\n\\boldsymbol \\Sigma_{\\text{indep}}^{-1} \\boldsymbol \\Sigma=\n\\begin{pmatrix}\n\\boldsymbol I_p & \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol \\Sigma_{\\boldsymbol y}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x}    & \\boldsymbol I_q \\\\\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\n\\boldsymbol I_p &  \\boldsymbol V_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\boldsymbol V_{\\boldsymbol y}^{1/2} \\\\\n\\boldsymbol V_{\\boldsymbol y}^{-1/2} \\boldsymbol P_{\\boldsymbol y}^{-1} \\boldsymbol P_{\\boldsymbol y\\boldsymbol x} \\boldsymbol V_{\\boldsymbol x}^{1/2}   & \\boldsymbol I_q \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\] has trace \\(\\text{Tr}(\\boldsymbol A) = d\\) and determinant \\[\n\\begin{split}\n\\det(\\boldsymbol A) & = \\det( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T ) \\\\\n  &= \\det( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K) \\\\\n\\end{split}\n\\] with \\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\boldsymbol P_{\\boldsymbol y}^{-1/2}\\). With \\(\\lambda_1, \\ldots, \\lambda_m\\) the singular values of \\(\\boldsymbol K\\) (i.e. the canonical correlations between \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\)) we get \\[\n\\det(\\boldsymbol A) =  \\prod_{i=1}^m (1-\\lambda_i^2)\n\\]\nThe mutual information between \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) is then \\[\n\\begin{split}\n\\text{MI}_{\\text{norm}}(\\boldsymbol x, \\boldsymbol y) &= D_{\\text{KL}}(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma),  N(\\boldsymbol \\mu,\\boldsymbol \\Sigma_{\\text{indep}} )) \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}\\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n     - d   \\biggr\\} \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}( \\boldsymbol A)\n    - \\log \\det( \\boldsymbol A)\n     - d   \\biggr\\} \\\\\n&=-\\frac{1}{2} \\sum_{i=1}^m \\log(1-\\lambda_i^2)\\\\\n&=-\\frac{1}{2} \\log \\left( \\prod_{i=1}^m (1-\\lambda_i^2) \\right)\\\\\n&=-\\frac{1}{2} \\log \\left( 1- \\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2 \\right)\\\\\n\\end{split}\n\\]\nFrom the above we see that \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x, \\boldsymbol y)\\) is simply the sum of the MIs resulting from the individual canonical correlations \\(\\lambda_i\\) with the same functional link between the MI and the squared correlation as in the bivariate normal case.\nFurthermore we obtain that \\[\n\\text{MI}_{\\text{norm}}(\\boldsymbol x,\\boldsymbol y) = -\\frac{1}{2} \\log(1 - \\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2 ) \\approx \\frac{1}{2} \\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2\n\\] Thus, in the multivariate case \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x\\boldsymbol y)\\) has again exactly the same functional relationship with the squared vector correlation \\(\\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2\\) as the \\(\\text{MI}_{\\text{norm}}(x, y)\\) for two univariate variables with squared Pearson correlation \\(\\rho^2\\).\nThus, Rozeboom’s vector correlation emerges as as a special case of mutual information computed for jointly multivariate normally distributed variables.\n\n\nUsing MI for variable selection\nA very general way to write down a model predicting \\(\\boldsymbol y\\) by \\(\\boldsymbol x\\) is as follows:\n\n\\(F_{\\boldsymbol y|\\boldsymbol x}\\) is a conditional distribution of \\(\\boldsymbol y\\) given predictors \\(\\boldsymbol x\\) and\n\\(F_{\\boldsymbol y}\\) is the marginal distribution of \\(\\boldsymbol y\\) without predictors.\n\nTypically \\(F_{\\boldsymbol y|\\boldsymbol x}\\) is a complex model and \\(F_{\\boldsymbol y}\\) a simple model (no predictors). Note that the predictive model can assume any form (incl. nonlinear).\nIntriguingly the expected KL divergence between the conditional and the marginal distribution \\[\n\\text{E}_{F_{\\boldsymbol x}}\\, D_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x},  F_{\\boldsymbol y} ) = \\text{MI}(\\boldsymbol x, \\boldsymbol y)\n\\] is equal to mutual information between \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\)! Thus \\(\\text{MI}(\\boldsymbol x, \\boldsymbol y)\\) measures the impact of conditioning. If the MI is small (i.e. close to zero) then \\(\\boldsymbol x\\) is not useful in predicting \\(\\boldsymbol y\\).\nThe above identity can be verified as follows. The KL divergence between \\(F_{\\boldsymbol y|\\boldsymbol x}\\) and \\(F_{\\boldsymbol y}\\) is given by \\[\nD_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x}, F_{\\boldsymbol y} )  = \\text{E}_{F_{\\boldsymbol y|\\boldsymbol x}}  \\log\\biggl( \\frac{f(\\boldsymbol y|\\boldsymbol x) }{ f(\\boldsymbol y)}  \\biggr) \\, ,\n\\] which is a random variable since it depends on \\(\\boldsymbol x\\). Taking the expectation with regard to \\(F_{\\boldsymbol x}\\) (the distribution of \\(\\boldsymbol x\\)) we get \\[\n\\begin{split}\n\\text{E}_{F_{\\boldsymbol x}} D_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x}, F_{\\boldsymbol y} ) &=\n\\text{E}_{F_{\\boldsymbol x}}  \\text{E}_{F_{\\boldsymbol y|\\boldsymbol x}}  \\log \\biggl(\\frac{ f(\\boldsymbol y|\\boldsymbol x) f(\\boldsymbol x) }{ f(\\boldsymbol y) f(\\boldsymbol x) } \\biggr)\\\\\n& =\n\\text{E}_{F_{\\boldsymbol x,\\boldsymbol y}}   \\log \\biggl(\\frac{ f(\\boldsymbol x,\\boldsymbol y) }{ f(\\boldsymbol y) f(\\boldsymbol x) } \\biggr) = \\text{MI}(\\boldsymbol x,\\boldsymbol y) \\,. \\\\\n\\end{split}\n\\]\nBecause of this link of MI with conditioning the MI between response and predictor variables is often used for variable and feature selection in general models.\n\n\nOther measures of general dependence\nIn principle, MI can be computed for any distribution and model and thus applies to both normal and non-normal models, and to both linear and nonlinear relationships.\nBesides mutual information there are others measures of general dependence between multivariate random variables.\nTwo important measures to capture nonlinear association that have been proposed in recent literature are\n\ndistance correlation and\nthe maximal information coefficient (MIC and \\(\\text{MIC}_e\\)).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate dependencies</span>"
    ]
  },
  {
    "objectID": "06-dependence.html#graphical-models",
    "href": "06-dependence.html#graphical-models",
    "title": "6  Multivariate dependencies",
    "section": "6.6 Graphical models",
    "text": "6.6 Graphical models\n\nPurpose\nGraphical models combine features from\n\ngraph theory\nprobability\nstatistical inference\n\nThe literature on graphical models is huge, we focus here only on two commonly used models:\n\nDAGs (directed acyclic graphs), all edges are directed, no directed loops (i.e. no cycles, hence “acyclic”)\nGGM (Gaussian graphical models), all edges are undirected\n\nGraphical models provide probabilistic models for trees and for networks, with random variables represented by nodes in the graphs, and branches representing conditional dependencies. In this regard they generalise both the tree-based clustering approaches as well as the probabilistic non-hierarchical methods (GMMs).\nHowever, the class of graphical models goes much beyond simple unsupervised learning models. It also includes regression, classification, time series models etc. For an overview see, e.g., the reference book by Murphy (2023).\n\n\nBasic notions from graph theory\n\nMathematically, a graph \\(G = (V, E)\\) consists of a a set of vertices or nodes \\(V = \\{v_1, v_2, \\ldots\\}\\) and a set of branches or edges \\(E = \\{ e_1, e_2, \\ldots \\}\\).\nEdges can be undirected or directed.\nGraphs containing only directed edges are directed graphs, and likewise graphs containing only undirected edges are called undirected graphs. Graphs containing both directed and undirected edges are called partially directed graphs.\nA path is a sequence of of vertices such that from each of its vertices there is an edge to the next vertex in the sequence.\nA graph is connected when there is a path between every pair of vertices.\nA cycle is a path in a graph that connects a node with itself.\nA connected graph with no cycles is a called a tree.\nThe degree of a node is the number of edges it connects with. If edges are all directed the degree of a node is the sum of the in-degree and out-degree, which counts the incoming and outgoing edges, respectively.\nExternal nodes are nodes with degree 1. In a tree-structured graph these are also called leaves.\n\nSome notions are only relevant for graphs with directed edges:\n\nIn a directed graph the parent node(s) of vertex \\(v\\) is the set of nodes \\(\\text{pa}(v)\\) directly connected to \\(v\\) via edges directed from the parent node(s) towards \\(v\\).\nConversely, \\(v\\) is called a child node of \\(\\text{pa}(v)\\). Note that a parent node can have several child nodes, so \\(v\\) may not be the only child of \\(\\text{pa}(v)\\).\nIn a directed tree graph, each node has only a single parent, except for one particular node that has no parent at all (this node is called the root node).\nA DAG, or directed acyclic graph, is a directed graph with no directed cycles. A (directed) tree is a special version of a DAG.\n\n\n\nProbabilistic graphical models\nA graphical model uses a graph to describe the relationship between random variables \\(x_1, \\ldots, x_d\\). The variables are assumed to have a joint distribution with density/mass function \\(p(x_1, x_2, \\ldots, x_d)\\). Each random variable is placed in a node of the graph.\nThe structure of the graph and the type of the edges connecting (or not connecting) any pair of nodes/variables is used to describe the conditional dependencies, and to simplify the joint distribution.\nThus, a graphical model is in essence a visualisation of the joint distribution using structural information from the graph helping to understand the mutual relationship among the variables.\n\n\nDirected graphical models\nIn a directed graphical model the graph structure is assumed to be a DAG (or a directed tree, which is also a DAG).\nThen the joint probability distribution can be factorised into a product of conditional probabilities as follows: \\[\np(x_1, x_2, \\ldots, x_d) = \\prod_i p(x_i  | \\text{pa}(x_i))\n\\] Thus, the overall joint probability distribution is specified by local conditional distributions and the graph structure, with the directions of the edges providing the information about parent-child node relationships.\nProbabilistic DAGs are also known as “Bayesian networks”.\nIdea: by trying out all possible trees/graphs and fitting them to the data using maximum likelihood (or Bayesian inference) we hope to be able identify the graph structure of the data-generating process.\nChallenges\n\nin the tree/network the internal nodes are usually not known, and thus have to be treated as latent variables.\n\nAnswer: To impute the states at these nodes we may use the EM algorithm as in GMMs (which in fact can be viewed as graphical models, too!).\n\nIf we treat the internal nodes as unknowns we need to marginalise over the internal nodes, i.e. we need to sum / integrate over all possible set of states of the internal nodes!\n\nAnswer: This can be handled very effectively using the Viterbi algorithm which is essentially an application of the generalised distributive law. In particular for tree graphs this means that the summations occurs locally at each node and propagates recursively across the tree.\n\nIn order to infer the tree or network structure the space of all trees or networks need to be explored. This is not possible in an exhaustive fashion unless the number of variables in the tree is very small.\n\nAnswer: Solution: use heuristic approaches for tree and network search!\n\nFurthermore, there exist so-called “equivalence classes” of graphical models, i.e. sets of graphical models that share the same joint probability distribution. Thus, all graphical models within the same equivalence class cannot be distinguished from observational data, even with infinite sample size!\n\nAnswer: this is a fundamental mathematical problem of identifiability so there is now way around this issue. However, on the positive side, this also implies that the search through all graphical models can be restricted to finding the so-called “essential graph” (e.g. Anderson et al. 1997. https://projecteuclid.org/euclid.aos/1031833662).\nConclusion: using directed graphical models for structure discovery is very time consuming and computationally demanding for anything but small toy data sets.\nThis also explains why heuristic and non-model based approaches (such as hierarchical clustering) are so popular even though full statistical modelling is in principle possible.\n\n\nUndirected graphical models\nAnother class of graphical models are models that contain only undirected edges. These undirected graphical models are used to represent the pairwise conditional (in)dependencies among the variables in the graph, and the resulting model is therefore also called conditional independence graph.\nSuppose \\(x_i\\) and \\(x_j\\) are two random variables/nodes from \\(\\{x_1, \\ldots, x_d\\}\\), and the set \\(\\{x_k\\}\\) represents all other variables/nodes with \\(k\\neq i\\) and \\(k \\neq j\\). Then the variables \\(x_i\\) and \\(x_j\\) are conditionally independent given all the other variables \\(\\{x_k\\}\\) \\[\nx_i \\perp\\!\\!\\!\\perp x_j | \\{x_k\\}\n\\] if the joint probability density for all variables \\(\\{x_1, \\ldots, x_d\\}\\) factorises as \\[\np(x_1, x_2, \\ldots, x_d) = p(x_i | \\{x_k\\}) \\, p(x_j | \\{x_k\\}) \\, p(\\{x_k\\}) \\,.\n\\] or equivalently \\[\n\\frac{p(x_i, x_j, \\ldots, x_d)}{p(\\{x_k\\})}  \n= p(x_i, x_j | \\{x_k\\})\n= p(x_i | \\{x_k\\}) \\, p(x_j | \\{x_k\\}) \\,.\n\\]\nIn the corresponding conditional independence graph note there is no edge between \\(x_i\\) and \\(x_j\\), as in such a graph missing edges correspond to conditional independence between the respective non-connected nodes.\n\nGaussian graphical model\nAssuming that \\(x_1, \\ldots, x_d\\) are jointly normally distributed, i.e. \\(\\boldsymbol x\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\), it turns out that it is straightforward to identify the pairwise conditional independencies. From \\(\\boldsymbol \\Sigma\\) we first obtain the precision matrix \\[\\boldsymbol \\Omega= (\\omega_{ij}) = \\boldsymbol \\Sigma^{-1} \\,.\\] Crucially, it can be shown that \\(\\omega_{ij} = 0\\) implies \\(x_i \\perp\\!\\!\\!\\perp x_j \\,|\\, \\{ x_k \\}\\). Hence, from the precision matrix \\(\\boldsymbol \\Omega\\) we can directly read off all the pairwise conditional independencies among the variables \\(x_1, x_2, \\ldots, x_d\\).\nOften, the covariance matrix \\(\\boldsymbol \\Sigma\\) is dense (few zeros) but the corresponding precision matrix \\(\\boldsymbol \\Omega\\) is sparse (many zeros).\nThe conditional independence graph computed for normally distributed variables is called a Gaussian graphical model, or short GGM. A further alternative name commonly used is covariance selection model.\n\n\nRelated quantity: partial correlation\nFrom the precision matrix \\(\\boldsymbol \\Omega\\) we can also compute the matrix of pairwise full conditional partial correlations:\n\\[\n\\rho_{ij|\\text{rest}}=-\\frac{\\omega_{ij}}{\\sqrt{\\omega_{ii}\\omega_{jj}}}\n\\] which is essentially the standardised precision matrix (similar to correlation but with an extra minus sign!)\nThe partial correlations lie in the range between -1 and +1, \\(\\rho_{ij|\\text{rest}} \\in [-1, 1]\\), just like standard correlations.\nIf \\(\\boldsymbol x\\) is multivariate normal then \\(\\rho_{ij|\\text{rest}} = 0\\) indicates conditional independence between \\(x_i\\) and \\(x_j\\).\nRegression interpretation: partial correlation is the correlation that remains between the two variables if the effect of the other variables is “regressed away”. In other words, the partial correlation is exactly equivalent to the correlation between the residuals that remain after regressing \\(x_i\\) on the variables \\(\\{x_k\\}\\) and \\(x_j\\) on \\(\\{x_k\\}\\).\n\n\n\nNull distribution of the empirical correlation coefficient\nSuppose we have two uncorrelated random variables \\(x\\) and \\(y\\) with \\(\\rho = \\text{Cor}(x, y) =0\\). After observing data \\(x_1, \\ldots, x_n\\) and \\(y_1, \\ldots, y_n\\) we compute the the empirical covariance matrix \\(\\hat{\\boldsymbol \\Sigma}_{xy}\\) and from it the empirical correlation coefficient \\(r = \\widehat{\\text{Cor}}(x, y)\\).\nThe distribution of the empirical correlation assuming \\(\\rho=0\\) is useful as null-model for testing whether the underlying correlation is in fact zero having observed empirical correlation \\(r\\). If \\(x\\) and \\(y\\) are normally distributed with \\(\\rho=0\\) the distribution of the empirical correlation \\(r\\) has mean \\(\\text{E}(r)=0\\) and variance \\(\\text{Var}(r)=\\frac{1}{\\kappa}\\). Here \\(\\kappa\\) is the degree of freedom of the null distribution which for standard correlation is \\(\\kappa=n-1\\). Furthermore, the squared empirical correlation is distributed according to a Beta distribution \\[\nr^2 \\sim \\text{Beta}\\left(\\frac{1}{2}, \\frac{\\kappa-1}{2}\\right)\n\\]\nFor partial correlation the null distribution of \\(r^2\\) has the same form but with a different degree of freedom. Specifically, \\(\\kappa\\) is reduced by the number of variables being conditioned on. If for \\(d\\) dimensions we condition on \\(d-2\\) variables the resulting degree of freedom is \\(\\kappa =n-1 - (d-2) = n-d+1\\). For \\(d=2\\) we get back the degree of freedom for standard empirical correlation.\n\n\nAlgorithm for learning GGMs\nFrom the above we can devise a simple algorithm to learn Gaussian graphical model (GGM) from data:\n\nEstimate covariance \\(\\hat{\\boldsymbol \\Sigma}\\) (in such a way that it is invertible!)\nCompute corresponding partial correlations\nIf \\(\\hat{\\rho}_{ij|\\text{rest}} \\approx 0\\) then there is (approx.) conditional independence between \\(x_i\\) and \\(x_j\\).\n\nThe test for conditional independence is done by statistical testing for vanishing partial correlation. Specifically, we compute the \\(p\\)-value assuming that the true underlying partial correlation is zero and then decide whether to reject the null assumption of zero partial correlation.\nIf there are many edges tested simultaneously we may need to adjust (i.e reduce) the test threshold, for example applying Bonferroni or FDR methods.\n\n\nExample: exam score data\nThis is a data set from Mardia et al. (1979) and features \\(d=5\\) variables measured on \\(n=88\\) subjects.\nCorrelations (rounded to 2 digits):\n\n\n           mechanics vectors algebra analysis statistics\nmechanics       1.00    0.55    0.55     0.41       0.39\nvectors         0.55    1.00    0.61     0.49       0.44\nalgebra         0.55    0.61    1.00     0.71       0.66\nanalysis        0.41    0.49    0.71     1.00       0.61\nstatistics      0.39    0.44    0.66     0.61       1.00\n\n\nPartial correlations (rounded to 2 digits):\n\n\n           mechanics vectors algebra analysis statistics\nmechanics       1.00    0.33    0.23     0.00       0.02\nvectors         0.33    1.00    0.28     0.08       0.02\nalgebra         0.23    0.28    1.00     0.43       0.36\nanalysis        0.00    0.08    0.43     1.00       0.25\nstatistics      0.02    0.02    0.36     0.25       1.00\n\n\nNote that there are no zero correlations but there are four partial correlations close to 0, indicating conditional independence between:\n\nanalysis and mechanics,\nstatistics and mechanics,\nanalysis and vectors, and\nstatistics and vectors.\n\nThe can be verified by computing the normal \\(p\\)-values for the partial correlations (with \\(\\kappa=84\\) as degree of freedom):\n\n\n           mechanics vectors algebra analysis statistics\nmechanics         NA   0.002   0.034    0.988      0.823\nvectors           NA      NA   0.009    0.477      0.854\nalgebra           NA      NA      NA    0.000      0.001\nanalysis          NA      NA      NA       NA      0.020\nstatistics        NA      NA      NA       NA         NA\n\n\nThere are six edges with small \\(p\\)-value (smaller than say 0.05) and these correspond to the edges for which the null assumption of zero partial correlation can be rejected so that out of ten possible edges four are not statistically significant. Therefore the conditional independence graph looks as follows:\nMechanics      Analysis\n   |     \\    /    |\n   |    Algebra    |\n   |     /   \\     |\n Vectors      Statistics\n\n\n\n\nMurphy, K. P. 2023. Probabilistic Machine Learning: Advanced Topic. MIT Press. https://probml.github.io/pml-book/book2.html.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate dependencies</span>"
    ]
  },
  {
    "objectID": "06-dependence.html#footnotes",
    "href": "06-dependence.html#footnotes",
    "title": "6  Multivariate dependencies",
    "section": "",
    "text": "Hotelling, H. 1936. Relations between two sets of variates. Biometrika 28:321–377. https://doi.org/10.1093/biomet/28.3-4.321↩︎\nRozeboom, W. W. 1965. Linear correlations between sets of variables. Psychometrika 30:57–71. https://doi.org/10.1007/BF02289747↩︎\nAnscombe, F. J. 1973. Graphs in statistical analysis. The American Statistician 27:17–21. http://doi.org/10.1080/00031305.1973.10478966↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate dependencies</span>"
    ]
  },
  {
    "objectID": "07-nonlinear.html",
    "href": "07-nonlinear.html",
    "title": "7  Nonlinear and nonparametric models",
    "section": "",
    "text": "7.1 Random forests\nAnother widely used approach for prediction in nonlinear settings is the method of random forests.\nRelevant reading:\nPlease read: James et al. (2021) or James et al. (2023) Chapter 8 “Tree-Based Methods”\nSpecifically:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear and nonparametric models</span>"
    ]
  },
  {
    "objectID": "07-nonlinear.html#random-forests",
    "href": "07-nonlinear.html#random-forests",
    "title": "7  Nonlinear and nonparametric models",
    "section": "",
    "text": "Section 8.1 The Basics of Decision Trees\nSection 8.2.1 Bagging\nSection 8.2.2 Random Forests\n\n\nStochastic vs. algorithmic models\nTwo cultures in statistical modelling: stochastic vs. algorithmic models\nClassic discussion paper by Leo Breiman (2001): Statistical modeling: the two cultures. Statistical Science 16:199–231. https://doi.org/10.1214/ss/1009213726\nThis paper has recently be revisited in the following discussion paper by Efron (2020) and discussants: Prediction, estimation, and attribution. JASA 115:636–677. https://doi.org/10.1080/01621459.2020.1762613\n\n\nRandom forests\nProposed by Leo Breimann in 2001 as application of “bagging” (Breiman 1996) to decision trees.\nBasic idea:\n\nA single decision tree is unreliable and unstable (weak predictor/classifier).\nUse boostrap to generate multiple decision trees (=“forest”)\nAverage over predictions from all tree (=“bagging”, bootstrap aggregation)\n\nThe averaging procedure has the effect of variance stabilisation. Intringuingly, averaging across all decision trees dramatically improves the overall prediction accuracy!\nThe Random Forests approach is an example of an ensemble method (since it is based on using an “ensemble” of trees).\nVariations: boosting, XGBoost ( https://xgboost.ai )\nRandom forests will be applied in Worksheet 11.\nThey are computationally expensive but typically perform very well!\n\n\nComparison of decision boundaries: decision tree vs. random forest\n\n\n\n\n\n\nFigure 7.1: Decision boundaries for decision trees and random forests in the non-nested case.\n\n\n\n\n\n\n\n\n\nFigure 7.2: Decision boundaries for decision trees and random forests in the nested case.\n\n\n\nCompare the non-nested case (Figure 7.1) and the nested case (Figure 7.2).\nCompare also with the decision boundaries for LDA and QDA in Figure 5.4 and Figure 5.5.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear and nonparametric models</span>"
    ]
  },
  {
    "objectID": "07-nonlinear.html#gaussian-processes",
    "href": "07-nonlinear.html#gaussian-processes",
    "title": "7  Nonlinear and nonparametric models",
    "section": "7.2 Gaussian processes",
    "text": "7.2 Gaussian processes\nGaussian processes offer a nonparametric probabilistic approach to model nonlinear dependencies.\nRelevant reading:\nPlease read: Rogers and Girolami (2017) Chapter 8: Gaussian processes.\n\nMain concepts\n\nGaussian processes (GPs) belong the the family of Bayesian nonparametric models\nIdea:\n\nstart with prior over a function (!),\nthen condition on observed data to get posterior distribution (again over a function)\n\nGPs use an infinitely dimensional multivariate normal distribution as prior\n\n\n\nConditional multivariate normal distribution\nGPs make use of the fact that marginal and conditional distributions of a multivariate normal distribution are also multivariate normal.\nMultivariate normal distribution:\n\\[\\boldsymbol z\\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\]\nAssume: \\[\n\\boldsymbol z=\\begin{pmatrix}\n    \\boldsymbol z_1      \\\\\n    \\boldsymbol z_2      \\\\\n\\end{pmatrix}\n\\] with \\[\n\\boldsymbol \\mu=\\begin{pmatrix}\n    \\boldsymbol \\mu_1      \\\\\n    \\boldsymbol \\mu_2      \\\\\n\\end{pmatrix}\n\\] and \\[\n\\boldsymbol \\Sigma=\\begin{pmatrix}\n    \\boldsymbol \\Sigma_{1}   & \\boldsymbol \\Sigma_{12}   \\\\\n    \\boldsymbol \\Sigma_{12}^T & \\boldsymbol \\Sigma_{2}   \\\\\n\\end{pmatrix}\n\\] with corresponding dimensions \\(d_1\\) and \\(d_2\\) and \\(d_1+d_2=d\\).\nMarginal distributions:\nAny subset of \\(\\boldsymbol z\\) is also multivariate normally distributed. Specifically, \\[\n\\boldsymbol z_1 \\sim N_{d_1}(\\boldsymbol \\mu_1, \\boldsymbol \\Sigma_{1})\n\\] and \\[\n\\boldsymbol z_2 \\sim N_{d_2}(\\boldsymbol \\mu_2, \\boldsymbol \\Sigma_{2})\n\\]\nConditional multivariate normal:\nThe conditional distribution is also multivariate normal: \\[\n\\boldsymbol z_1 | \\boldsymbol z_2 = \\boldsymbol z_{1 | 2} \\sim N_{d_1}(\\boldsymbol \\mu_{1|2}, \\boldsymbol \\Sigma_{1 | 2})\n\\] with \\[\\boldsymbol \\mu_{1|2}=\\boldsymbol \\mu_1 + \\boldsymbol \\Sigma_{12} \\boldsymbol \\Sigma_{2}^{-1} (\\boldsymbol z_2 -\\boldsymbol \\mu_2)\\] and \\[\\boldsymbol \\Sigma_{1 | 2}=\\boldsymbol \\Sigma_{1} -  \\boldsymbol \\Sigma_{12} \\boldsymbol \\Sigma_{2}^{-1} \\boldsymbol \\Sigma_{12}^T\\]\n\\(\\boldsymbol z_{1 | 2}\\) and \\(\\boldsymbol \\mu_{1|2}\\) have dimension \\(d_1 \\times 1\\) and \\(\\boldsymbol \\Sigma_{1 | 2}\\) has dimension \\(d_1 \\times d_1\\), i.e. the same dimension as the unconditioned variables.\nYou may recall the above formula in the context of linear regression, with \\(y = z_1\\) and \\(\\boldsymbol x= \\boldsymbol z_2\\) so that the conditional mean becomes \\[\n\\begin{split}\n\\text{E}(y|\\boldsymbol x) &=\\boldsymbol \\mu_y + \\boldsymbol \\Sigma_{y\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} (\\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x})\\\\\n&= \\beta_0+ \\boldsymbol \\beta^T \\boldsymbol x\\\\\n\\end{split}\n\\] with \\(\\boldsymbol \\beta= \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\) and \\(\\beta_0 = \\boldsymbol \\mu_y-\\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x}\\), and the corresponding conditional variance is \\[\n\\text{Var}(y|\\boldsymbol x) = \\sigma^2_y -  \\boldsymbol \\Sigma_{y\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} \\,.\n\\]\n\n\nCovariance functions and kernels\nThe GP prior is an infinitely dimensional multivariate normal distribution with mean zero and the covariance specified by a function \\(k(x, x^{\\prime})\\):\nA widely used covariance function is \\[\nk(x, x^{\\prime}) = \\text{Cov}(x, x^{\\prime}) = \\sigma^2 e^{-\\frac{ (x-x^{\\prime})^2}{2 l^2}}\n\\] This is known as the squared-exponential kernel or Radial-basis function (RBF) kernel.\nNote that this kernel implies\n\n\\(k(x, x) = \\text{Var}(x) = \\sigma^2\\) and\n\\(\\text{Cor}(x, x^{\\prime}) =  e^{-\\frac{ (x-x^{\\prime})^2}{2 l^2}}\\).\n\nThe parameter \\(l\\) in the RBF kernel is the length scale parameter and describes the “wigglyness” or “stiffness” of the resulting function. Small values of \\(l\\) correspond to more complex, more wiggly functions, and to low spatial correlation, as the correlation decreases quicker with distance, and large values correspond to more rigid, stiffer functions, with longer range spatial correlation (note that in a time series context this would be called autocorrelation).\nThere are many other kernel functions, including linear, polynomial or periodic kernels.\n\n\nGP model\nNonlinear regression in the GP approach is conceptually very simple:\n\nstart with multivariate prior\nthen condition on the observed data\nthe resulting conditional multivariate normal can used to predict the function values at any unobserved values\nthe conditional variance can be used to compute credible intervals for predictions.\n\nGP regression also provides a direct link with classical Bayesian linear regression (when using a linear kernel). Furthermore, GPs are also linked with neural networks as their limit in the case of an infinitely wide network (see section on neural networks).\nDrawbacks of GPs: computationally expensive, typically \\(O(n^3)\\) because of the matrix inversion. However, there are now variations of GPs that help to overcome this issue (e.g. sparse GPs).\n\n\nGaussian process example\nWe now show how to apply Gaussian processes in R justing using standard matrix calculations.\nOur aim is to estimate the following nonlinear function from a number of observations. Note that initially we assume that there is no additional noise (so the observations lie directly on the curve):\n\ntruefunc = function(x) sin(x)\nXLIM = c(0, 2*pi)\nYLIM = c(-2, 2)\n\nn2 = 10\nx2 = runif(n2, min=XLIM[1], max=XLIM[2])\ny2 = truefunc(x2)  # no noise\n\ncurve( truefunc(x), xlim=XLIM, ylim=YLIM, xlab=\"x\", ylab=\"y\", \n      main=\"True Function\")\npoints(x2, y2)\n\n\n\n\n\n\n\n\nUse the RFB kernel as the prior covariance and assume that the prior has mean zero:\n\n# RBF kernel\nrbfkernel = function(xa, xb, s2=1, l=1/2) s2*exp(-1/2*(xa-xb)^2/l^2)\nkfun.mat = function(xavec, xbvec, FUN=rbfkernel) \n  outer(X=as.vector(xavec), Y=as.vector(xbvec), FUN=FUN)\n\n# prior mean\nmu.vec = function(x) rep(0, length(x))\n\nVisualise the functions sampled from the multivariate normal prior:\n\n# grid of x-values \nn1 = 100\nx1 = seq(XLIM[1], XLIM[2], length.out=n1)\n\n# unconditioned covariance and mean (unobserved samples x1)\nK1 = kfun.mat(x1, x1)  \nm1 = mu.vec(x1)\n\n## sample functions from GP prior  \nB = 5\nlibrary(\"MASS\") # for mvrnorm\ny1r = t(mvrnorm(B, mu = m1, Sigma=K1))\nplot(x1, y1r[,1], type=\"l\", lwd=2, ylab=\"y\", xlab=\"x\", ylim=YLIM, \n  main=\"Prior Functions (RBF Kernel with l=1/2)\")\nfor(i in 2:B)\n  lines(x1, y1r[,i], col=i, lwd=2)\n\n\n\n\n\n\n\n\nCompute the posterior mean and variance by conditioning on the observations:\n\n# unconditioned covariance and mean (observed samples x2)\nK2 = kfun.mat(x2, x2)\nm2 = mu.vec(x2)\niK2 = solve(K2) # inverse\n\n# cross-covariance\nK12 = kfun.mat(x1, x2)\n\n# Conditioning: x1 conditioned on x2\n# conditional mean\nm1.2 = m1 + K12 %*% iK2 %*% (y2 - m2)\n# conditional variance\nK1.2 = K1 - K12 %*% iK2 %*% t(K12)\n\nPlot the posterior mean and upper and lower bounds of a 95% credible interval:\n\n# upper and lower CI\nupper.bound = m1.2 + 1.96*sqrt(diag(K1.2))\nlower.bound = m1.2 - 1.96*sqrt(diag(K1.2))\n\nplot(x1, m1.2, type=\"l\", xlim=XLIM, ylim=YLIM, col=\"red\", lwd=3,\n  ylab=\"y\", xlab = \"x\", main = \"Posterior\")\n\npoints(x2,y2,pch=4,lwd=4,col=\"blue\")\nlines(x1,upper.bound,lty=2,lwd=3)\nlines(x1,lower.bound,lty=2,lwd=3)\ncurve(truefunc(x), xlim=XLIM, add=TRUE, col=\"gray\")\n\nlegend(x=\"topright\", \n  legend=c(\"posterior mean\", \"posterior quantiles\", \"true function\"),\n  lty=c(1, 2, 1),lwd=c(4, 4, 1), col=c(\"red\",\"black\", \"gray\"), cex=1.0)\n\n\n\n\n\n\n\n\nFinally, we can take into acount noise at the measured data points by adding an error term:\n\n# add some noise\nsdeps = 0.1\nK2 = K2 + sdeps^2*diag(1,length(x2))\n\n# update\niK2 = solve(K2) # inverse\nm1.2 = m1 + K12 %*% iK2 %*% (y2 - m2)\nK1.2 = K1 - K12 %*% iK2 %*% t(K12)\nupper.bound = m1.2 + 1.96*sqrt(diag(K1.2))\nlower.bound = m1.2 - 1.96*sqrt(diag(K1.2))\n\nplot(x1, m1.2, type=\"l\", xlim=XLIM, ylim=YLIM, col=\"red\", lwd=3, \n  ylab=\"y\", xlab = \"x\", main = \"Posterior (with noise)\")\n\npoints(x2,y2,pch=4,lwd=4,col=\"blue\")\nlines(x1,upper.bound,lty=2,lwd=3)\nlines(x1,lower.bound,lty=2,lwd=3)\ncurve(truefunc(x), xlim=XLIM, add=TRUE, col=\"gray\")\n\nlegend(x=\"topright\", \n  legend=c(\"posterior mean\", \"posterior quantiles\", \"true function\"),\n  lty=c(1, 2, 1),lwd=c(4, 4, 1), col=c(\"red\",\"black\", \"gray\"), cex=1.0)\n\n\n\n\n\n\n\n\nNote that in the vicinity of data points the CIs are small and the further away from data the more uncertain the estimate of the underlying function becomes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear and nonparametric models</span>"
    ]
  },
  {
    "objectID": "07-nonlinear.html#neural-networks",
    "href": "07-nonlinear.html#neural-networks",
    "title": "7  Nonlinear and nonparametric models",
    "section": "7.3 Neural networks",
    "text": "7.3 Neural networks\nAnother highly important class of models for nonlinear prediction (and nonlinear function approximation) are neural networks.\nRelevant reading:\nPlease read: James et al. (2021) or James et al. (2023) Chapter 10 “Deep Learning”\n\nHistory\nNeural networks are actually relatively old models, going back to the 1950s.\nThree phases of neural networks (NN)\n\n1950/60: replicating functions of neurons in the brain (e.g. perceptron)\n1980/90: neural networks as universal function approximators\n2010—today: deep learning\n\nThe first phase was biologically inspired, the second phase focused on mathematical properties, and the current phase is pushed forward by advances in computer science and numerical optimisation:\n\nbackpropagation algorithm\nefficient automatic symbolic differentiation (e.g. autograd)\nstochastic gradient descent algorithms (e.g. Adam)\nuse of GPUs and TPUs (e.g. for linear algebra)\navailability of packages for symbolic tensor computations and deep learning.\n\nCurrently the most popular frameworks are:\n\nPyTorch (PyTorch Foundation, formerly Meta/Facebook)\nTensorFlow (Google Research)\nFlax / JAX (Google Research)\n\nand high-level wrappers:\n\nskorch (scikit-learn wrapper for PyTorch)\nKeras 3 (for TensorFlow, JAX, and PyTorch)\n\n\n\nNeural networks\nNeural networks are essentially stacked systems of linear regressions, with nonlinear mappings between each layer, mapping the input to output via one or more layers of internal hidden nodes corresponding to internal latent variables:\n\nEach internal node is a nonlinear function of all or some of nodes in the previous layer\nTypically, the output of a node is computed using a non-linear activation function, such as the sigmoid function or a piecewise linear function (ReLU), from a linear combination of the input variables of that node.\n\nA simple architecture is a feedforward network with a single hidden layer. More complex models are multilayer perceptrons and convolutional neural networks.\nIt can be shown that even simple network architectures can (with sufficient number of nodes) approximate any arbitrary non-linear function. This is called the universal function approximation property.\n“Deep” neural networks have many layers, and the optimisation of their parameters requires advanced techniques (see above), with the objective function typically an empirical risk based on, e.g., squared error loss or cross-entropy loss. Neural networks are very highly parameterised models and therefore require lots of data for training, and typically also some form of regularisation (e.g. dropout).\nAs an extreme example, the neural network behind the ChatGPT 4 language model that is trained on essentially the whole freely accessible text available on the internet has an estimated 1.76 trillion (!) parameters (\\(1.76 \\times 10^{12}\\)).\nIn the limit of an infinite width a single layer fully connected neural network becomes equivalent to a Gaussian process. This was first shown by R. M. Neal (1996)1. More recently, this equivalence has also been demonstrated for other types of neural networks (with the kernel function of the GP being determined by the neural network architecture). This is formalised in the “neural tangent kernel” (NTK) framework.\nSome of the statistical aspects of neural networks are not well understood. For example, there is the paradox that neural networks typically overfit the training data but still generalise well - this clearly violates the traditional understanding of bias-variance tradeoff for classical modelling in statistics and machine learning — see for example Belkin et al. (2019)2. Some researchers argue that this contradiction can be resolved by better understanding the effective dimension of complex models. There is a lot of current research to explain this phenomenon of “multiple descent”, i.e. the decrease of prediction error for models with very many parameters. A further topic is robustness of the predictions, which is also caused by overfitting. It is well known that neural networks can sometimes be “fooled” by so-called adversarial examples, e.g., the classification of a sample may change if a small amount of noise is added to the test data.\n\n\nLearning more about deep learning\nA good place to learn more about the concepts of deep learning is the book “Understanding Deep Learning” by Prince (2023) available online at https://udlbook.github.io/udlbook/. For actual application in computer code using various software frameworks the book “Dive Into Deep Learning” by Zhang et al. (2023) available online at https://d2l.ai is recommended.\n\n\n\n\nJames, G., D. Witten, T. Hastie, and R. Tibshirani. 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nJames, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023. An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-3-031-38747-0.\n\n\nPrince, S. J. D. 2023. Understanding Deep Learning. MIT Press. https://mitpress.mit.edu/9780262048644/understanding-deep-learning/.\n\n\nRogers, S., and M. Girolami. 2017. A First Course in Machine Learning. 2nd ed. Chapman; Hall / CRC. https://doi.org/10.1201/9781315382159.\n\n\nZhang, A., Z. C. Lipton, M. Li, and A. J. Smola. 2023. Dive into Deep Learning. https://d2l.ai.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear and nonparametric models</span>"
    ]
  },
  {
    "objectID": "07-nonlinear.html#footnotes",
    "href": "07-nonlinear.html#footnotes",
    "title": "7  Nonlinear and nonparametric models",
    "section": "",
    "text": "Neal, R. M. 1996. Bayesian Learning for Neural Networks. Springer. https://doi.org/10.1007/978-1-4612-0745-0↩︎\nBelkin, M. et al. 2019. Reconciling modern machine-learning practice and the classical bias–variance trade-off. PNAS 116: 15849–15854. https://doi.org/10.1073/pnas.1903070116↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear and nonparametric models</span>"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Bishop, C. M. 2006. Pattern Recognition and Machine Learning.\nSpringer. https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/.\n\n\nIzenman, A. J. 2008. Modern Multivariate Statistical\nTechniques. New York: Springer. https://doi.org/10.1007/978-0-387-78189-1.\n\n\nJames, G., D. Witten, T. Hastie, and R. Tibshirani. 2021. An\nIntroduction to Statistical Learning with Applications in\nR. 2nd ed. Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nJames, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023.\nAn Introduction to Statistical Learning with Applications in\nPython. Springer. https://doi.org/10.1007/978-3-031-38747-0.\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate\nAnalysis. Academic Press.\n\n\nMurphy, K. P. 2022. Probabilistic Machine Learning: An\nIntroduction. MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\n———. 2023. Probabilistic Machine Learning: Advanced Topic. MIT\nPress. https://probml.github.io/pml-book/book2.html.\n\n\nPrince, S. J. D. 2023. Understanding Deep Learning. MIT Press.\nhttps://mitpress.mit.edu/9780262048644/understanding-deep-learning/.\n\n\nRogers, S., and M. Girolami. 2017. A First Course in Machine\nLearning. 2nd ed. Chapman; Hall / CRC. https://doi.org/10.1201/9781315382159.\n\n\nZhang, A., Z. C. Lipton, M. Li, and A. J. Smola. 2023. Dive into\nDeep Learning. https://d2l.ai.",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "08-further-study.html",
    "href": "08-further-study.html",
    "title": "Appendix A — Further study",
    "section": "",
    "text": "A.1 Recommended reading\nFor multivariate statistics and machine learning:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Further study</span>"
    ]
  },
  {
    "objectID": "08-further-study.html#recommended-reading",
    "href": "08-further-study.html#recommended-reading",
    "title": "Appendix A — Further study",
    "section": "",
    "text": "Izenman (2008) Modern Multivariate Statistical Techniques. Springer.\nRogers and Girolami (2017) A first course in machine learning (2nd Edition). Chapman and Hall / CRC.\nJames et al. (2021) An introduction to statistical learning with applications in R (2nd edition). Springer.\nJames et al. (2023) An introduction to statistical learning with applications in Python. Springer.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Further study</span>"
    ]
  },
  {
    "objectID": "08-further-study.html#advanced-reading",
    "href": "08-further-study.html#advanced-reading",
    "title": "Appendix A — Further study",
    "section": "A.2 Advanced reading",
    "text": "A.2 Advanced reading\nAdditional (advanced) reference books for probabilistic machine learning are:\n\nBishop (2006) Pattern recognition and machine learning. Springer.\nMurphy (2022) Probabilistic Machine Learning: An Introduction. MIT Press.\nMurphy (2023) Probabilistic Machine Learning: Advanced Topics. MIT Press.\nPrince (2023) Understanding Deep learning. MIT Press.\n\nYou can find further suggestions on my list of online textbooks in statistics and machine learning.\n\n\n\n\nBishop, C. M. 2006. Pattern Recognition and Machine Learning. Springer. https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/.\n\n\nIzenman, A. J. 2008. Modern Multivariate Statistical Techniques. New York: Springer. https://doi.org/10.1007/978-0-387-78189-1.\n\n\nJames, G., D. Witten, T. Hastie, and R. Tibshirani. 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nJames, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023. An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-3-031-38747-0.\n\n\nMurphy, K. P. 2022. Probabilistic Machine Learning: An Introduction. MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\n———. 2023. Probabilistic Machine Learning: Advanced Topic. MIT Press. https://probml.github.io/pml-book/book2.html.\n\n\nPrince, S. J. D. 2023. Understanding Deep Learning. MIT Press. https://mitpress.mit.edu/9780262048644/understanding-deep-learning/.\n\n\nRogers, S., and M. Girolami. 2017. A First Course in Machine Learning. 2nd ed. Chapman; Hall / CRC. https://doi.org/10.1201/9781315382159.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Further study</span>"
    ]
  }
]