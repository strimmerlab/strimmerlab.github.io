[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"lecture notes MATH38161, course Multivariate Statistics Machine Learning third year mathematics students Department Mathematics University Manchester.course text written Korbinian Strimmer 2018–2022. version 27 October 2022.notes updated time time. view current\nversion visit \nonline MATH38161 lecture notes.may also download MATH38161 lecture notes PDF.\npaper copy recommended print two pages per sheet.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"notes licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"about-the-author","chapter":"Preface","heading":"About the author","text":"Hello! name Korbinian Strimmer Professor Statistics.\nmember Statistics group\nDepartment Mathematics University Manchester. can find information home page.first taught module winter term 2018 University Manchester,\nsubsequently also years 2019–2022.hope enjoy course! questions, comments, corrections please email korbinian.strimmer@manchester.ac.uk.","code":""},{"path":"preface.html","id":"about-the-module","chapter":"Preface","heading":"About the module","text":"","code":""},{"path":"preface.html","id":"topics-covered","chapter":"Preface","heading":"Topics covered","text":"MATH38161 module designed run course 11 weeks.\nsix parts, covering particular aspect multivariate statistics machine learning:Multivariate random variables estimation \nlarge small sample settings (W1 W2)Transformations dimension reduction (W3 W4)Unsupervised learning/clustering (W5 W6)Supervised learning/classification (W7 W8)Measuring modelling multivariate dependencies (W9)Nonlinear nonparametric models (W10, W11)module focuses :Concepts methods (theory)Implementation application RPractical data analysis interpretation (incl. report writing)Modern tools data science statistics (R markdown, R studio)","code":""},{"path":"preface.html","id":"additional-support-material","chapter":"Preface","heading":"Additional support material","text":"University Manchester student enrolled module\nfind Blackboard:weekly learning plan 11 week study period,weekly worksheets examples (theory application R) solutions R Markdown, andexam papers previous years.Furthermore, also MATH38161 online reading list hosted University Manchester library.","code":""},{"path":"preface.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"Many thanks Beatriz Costa Gomes help compile first draft course notes winter term 2018 graduate teaching assistant course. also thank many students suggested corrections.","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-random-variables","chapter":"1 Multivariate random variables","heading":"1 Multivariate random variables","text":"","code":""},{"path":"multivariate-random-variables.html","id":"essentials-in-multivariate-statistics","chapter":"1 Multivariate random variables","heading":"1.1 Essentials in multivariate statistics","text":"","code":""},{"path":"multivariate-random-variables.html","id":"why-multivariate-statistics","chapter":"1 Multivariate random variables","heading":"1.1.1 Why multivariate statistics?","text":"science use experiments learn underlying mechanisms interest, deterministic stochastic, compare different models verify reject hypotheses world.\nStatistics provides tools quantify procedure offers methods \nlink data (experiments) probabilistic models (hypotheses).univariate statistics use relatively simple approaches based single random variable\nsingle parameter. However, practise often consider multiple random variables multiple parameters, need complex models also able deal complex data. Hence, need multivariate statistical approaches models.Specifically, multivariate statistics concerned methods models random vectors random matrices, rather just random univariate (scalar) variables. Therefore, multivariate statistics frequently make use matrix notation.Closely related multivariate statistics (traditionally subfield statistics) machine learning (ML) traditionally subfield computer science. ML used focus algorithms rather probabilistic modelling nowadays machine learning methods fully based statistical multivariate approaches, two fields converging.Multivariate models provide means learn dependencies interactions among \ncomponents random variables turn allow us draw conclusion underlying mechanisms interest (e.g. biological medical problems).Two main tasks:unsupervised learning (finding structure, clustering)supervised learning (training labelled data, followed prediction)Challenges:complexity model needs appropriate problem available datahigh dimensions make estimation inference difficultcomputational issues","code":""},{"path":"multivariate-random-variables.html","id":"univariate-vs.-multivariate-random-variables","chapter":"1 Multivariate random variables","heading":"1.1.2 Univariate vs. multivariate random variables","text":"Univariate random variable (dimension \\(d=1\\)):\n\\[x \\sim F\\]\n\\(x\\) scalar \\(F\\) distribution.\n\\(\\text{E}(x) = \\mu\\) denotes mean \\(\\text{Var}(x) = \\sigma^2\\) variance \\(x\\).Multivariate random vector dimension \\(d\\):\n\\[\\boldsymbol x= (x_1, x_2,...,x_d)^T  \\sim F\\]\\(\\boldsymbol x\\) vector valued random variable.vector \\(\\boldsymbol x\\) column vector (=matrix size \\(d \\times 1\\)).\ncomponents \\(x_1, x_2,...,x_d\\) univariate random variables.\ndimension \\(d\\) also often denoted \\(p\\) \\(q\\).","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-data","chapter":"1 Multivariate random variables","heading":"1.1.3 Multivariate data","text":"Vector notation:Samples multivariate distribution vectors (scalars univariate normal):\n\\[\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_n \\stackrel{\\text{iid}}\\sim F\\]Matrix component notation:data points commonly collected matrix \\(\\boldsymbol X\\).statistics convention store data vector rows data matrix \\(\\boldsymbol X\\):\\[\\boldsymbol X= (\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_n)^T = \\begin{pmatrix}\n    x_{11}  & x_{12} & \\dots & x_{1d}   \\\\\n    x_{21}  & x_{22} & \\dots & x_{2d}   \\\\\n    \\vdots \\\\\n    x_{n1}  & x_{n2} & \\dots & x_{nd}\n\\end{pmatrix}\\]Therefore,\n\\[\\boldsymbol x_1=\\begin{pmatrix}\n    x_{11}       \\\\\n    \\vdots \\\\\n    x_{1d}\n\\end{pmatrix} , \\space \\boldsymbol x_2=\\begin{pmatrix}\n    x_{21}       \\\\\n    \\vdots \\\\\n    x_{2d}\n\\end{pmatrix} , \\ldots , \\boldsymbol x_n=\\begin{pmatrix}\n    x_{n1}       \\\\\n    \\vdots \\\\\n    x_{nd}\n\\end{pmatrix}\\]Thus, statistics first index runs \\((1,...,n)\\) denotes samples second index runs \\((1,...,d)\\) refers variables.statistics convention data matrices universal! fact, machine learning literature engineering computer science data samples stored columns variables appear rows (thus engineering convention data matrix transposed compared statistics convention).order avoid confusion ambiguity recommended prefer vector notation describe data matrix component notation (see also section estimating covariance matrices examples).","code":""},{"path":"multivariate-random-variables.html","id":"mean-of-a-random-vector","chapter":"1 Multivariate random variables","heading":"1.1.4 Mean of a random vector","text":"mean / expectation random vector dimensions \\(d\\) also vector dimensions \\(d\\):\n\\[\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu= \\begin{pmatrix}\n    \\text{E}(x_1)       \\\\\n    \\text{E}(x_2)       \\\\\n    \\vdots \\\\\n    \\text{E}(x_d)\n\\end{pmatrix} = \\left( \\begin{array}{l}\n    \\mu_1       \\\\\n    \\mu_2       \\\\\n    \\vdots \\\\\n    \\mu_d\n\\end{array}\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"variance-of-a-random-vector","chapter":"1 Multivariate random variables","heading":"1.1.5 Variance of a random vector","text":"Recall definition mean variance univariate random variable:\\[\\text{E}(x) = \\mu\\]\\[\\text{Var}(x) = \\sigma^2 = \\text{E}( (x-\\mu)^2 )=\\text{E}( (x-\\mu)(x-\\mu) ) = \\text{E}(x^2)-\\mu^2\\]Definition variance random vector:\\[\\text{Var}(\\boldsymbol x) = \\underbrace{\\boldsymbol \\Sigma}_{d\\times d} = \n\\text{E}\\left(\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d\\times 1} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1\\times d} \\right) \n = \\text{E}(\\boldsymbol x\\boldsymbol x^T)-\\boldsymbol \\mu\\boldsymbol \\mu^T\\]variance random vector , therefore, vector matrix!\\[\\boldsymbol \\Sigma= (\\sigma_{ij}) = \\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix}\\]matrix called Covariance Matrix, -diagonal elements \\(\\sigma_{ij}= \\text{Cov}(x_i,x_j)\\) diagonal \\(\\sigma_{ii}= \\text{Var}(X_i) = \\sigma_i^2\\).","code":""},{"path":"multivariate-random-variables.html","id":"properties-of-the-covariance-matrix","chapter":"1 Multivariate random variables","heading":"1.1.6 Properties of the covariance matrix","text":"\\(\\boldsymbol \\Sigma\\) real valued: \\(\\sigma_{ij} \\\\mathbb{R}\\)\\(\\boldsymbol \\Sigma\\) symmetric: \\(\\sigma_{ij} = \\sigma_{ji}\\)diagonal \\(\\boldsymbol \\Sigma\\) contains \\(\\sigma_{ii} = \\text{Var}(x_i) = \\sigma_i^2\\), .e. \nvariances components \\(\\boldsymbol x\\).-diagonal elements \\(\\sigma_{ij} = \\text{Cov}(x_i,x_j)\\) represent linear dependencies among \\(x_i\\). \\(\\Longrightarrow\\) linear regression, correlationHow many separate entries \\(\\boldsymbol \\Sigma\\) ?\\[\\boldsymbol \\Sigma= (\\sigma_{ij}) = \\underbrace{\\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix}}_{d\\times d}\\]\n\\(\\sigma_{ij} = \\sigma_{ji}\\).Number separate entries: \\(\\frac{d(d+1)}{2}\\).numbers grows square dimension \\(d\\), .e. order \\(O(d^2)\\):large dimension \\(d\\) covariance matrix many components!–> computationally expensive (storage handling)\n–> challenging estimate high dimensions \\(d\\).Note: matrix inversion requires \\(O(d^3)\\) operations using standard algorithms Gauss Jordan elimination.1 Hence, computing \\(\\boldsymbol \\Sigma^{-1}\\) computationally expensive large \\(d\\)!","code":""},{"path":"multivariate-random-variables.html","id":"eigenvalue-decomposition-of-boldsymbol-sigma","chapter":"1 Multivariate random variables","heading":"1.1.7 Eigenvalue decomposition of \\(\\boldsymbol \\Sigma\\)","text":"Recall linear matrix algebra real symmetric matrix\nreal eigenvalues complete set orthogonal eigenvectors.\ncan obtained orthogonal eigendecomposition.Applying eigenvalue decomposition covariance matrix yields\n\\[\n\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\n\\]\n\\(\\boldsymbol U\\) orthogonal matrix containing eigenvectors covariance matrix\n\n\\[\\boldsymbol \\Lambda= \\begin{pmatrix}\n    \\lambda_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}\n\\end{pmatrix}\\]\ncontains corresponding eigenvalues \\(\\lambda_i\\).Importantly, eigenvalues covariance matrix real-valued\nconstruction constrained non-negative.\ncan seen computing quadratic form \\(\\boldsymbol z^T \\boldsymbol \\Sigma\\boldsymbol z\\)\n\\(\\boldsymbol z\\) non-random vector. non-zero \\(\\boldsymbol z\\)\n\\[\n\\begin{split}\n\\boldsymbol z^T  \\boldsymbol \\Sigma\\boldsymbol z& = \\boldsymbol z^T \\text{E}\\left(  (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T  \\right) \\boldsymbol z\\\\\n & =  \\text{E}\\left( \\boldsymbol z^T (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol z\\right) \\\\\n & =  \\text{E}\\left( \\left( \\boldsymbol z^T (\\boldsymbol x-\\boldsymbol \\mu) \\right)^2 \\right) \\geq 0 \\, .\\\\\n\\end{split}\n\\]\nFurthermore, \\(\\boldsymbol y= \\boldsymbol U^T \\boldsymbol z\\) get\n\\[\n\\begin{split}\n\\boldsymbol z^T  \\boldsymbol \\Sigma\\boldsymbol z& =  \\boldsymbol z^T\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T \\boldsymbol z\\\\\n                      & =  \\boldsymbol y^T \\boldsymbol \\Lambda\\boldsymbol y= \\sum_{=1}^d  y_i^2 \\lambda_i \\\\\n\\end{split}\n\\]\nhence \\(\\lambda_i \\geq 0\\).\nTherefore covariance matrix \\(\\boldsymbol \\Sigma\\) always\npositive semi-definite.fact, unless collinearity ( .e. variable linear function variables) eigenvalues positive \\(\\boldsymbol \\Sigma\\) positive definite.","code":""},{"path":"multivariate-random-variables.html","id":"joint-covariance-matrix","chapter":"1 Multivariate random variables","heading":"1.1.8 Joint covariance matrix","text":"Assume random vector \\(\\boldsymbol z\\) mean \\(\\text{E}(\\boldsymbol z) = \\boldsymbol \\mu_{\\boldsymbol z}\\)\ncovariance matrix \\(\\text{Var}(\\boldsymbol z) = \\boldsymbol \\Sigma_{\\boldsymbol z}\\).Often makes sense partion components \\(\\boldsymbol z\\) two groups\n\\[\n\\boldsymbol z= \\begin{pmatrix} \\boldsymbol x\\\\ \\boldsymbol y\\end{pmatrix}\n\\]\ninduces corresponding partition\nexpectation\n\\[\n \\boldsymbol \\mu_{\\boldsymbol z} =  \\begin{pmatrix} \\boldsymbol \\mu_{\\boldsymbol x} \\\\ \\boldsymbol \\mu_{\\boldsymbol y} \\end{pmatrix}\n\\]\n\\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu_{\\boldsymbol x}\\) \\(\\text{E}(\\boldsymbol y) = \\boldsymbol \\mu_{\\boldsymbol y}\\).Furthermore, covariance matrix can written \n\\[\n\\boldsymbol \\Sigma_{\\boldsymbol z} = \n\\begin{pmatrix} \n\\boldsymbol \\Sigma_{\\boldsymbol x} &  \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} & \\boldsymbol \\Sigma_{\\boldsymbol y} \\\\\n\\end{pmatrix} \n\\]\ncontaining within-group group covariance matrices\n\\(\\boldsymbol \\Sigma_{\\boldsymbol x}\\) \\(\\boldsymbol \\Sigma_{\\boldsymbol y}\\) diagonal \ncross-covariance matrix \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} = \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x}^T\\)\n-diagonal element. Note cross-covariance matrix rectangular\nsymmetric.covariance matrix often called joint covariance matrix\n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\).","code":""},{"path":"multivariate-random-variables.html","id":"quantities-related-to-the-covariance-matrix","chapter":"1 Multivariate random variables","heading":"1.1.9 Quantities related to the covariance matrix","text":"","code":""},{"path":"multivariate-random-variables.html","id":"correlation-matrix-boldsymbol-p","chapter":"1 Multivariate random variables","heading":"1.1.9.1 Correlation matrix \\(\\boldsymbol P\\)","text":"correlation matrix \\(\\boldsymbol P\\) (= upper case greek “rho”) standardised covariance matrix\\[\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}=\\text{Cor}(x_i,x_j)\\]\\[\\rho_{ii} = 1 = \\text{Cor}(x_i,x_i)\\]\\[ \\boldsymbol P= (\\rho_{ij}) = \\begin{pmatrix}\n    1 & \\dots & \\rho_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{d1} & \\dots & 1\n\\end{pmatrix}\\]\\(\\boldsymbol P\\) (“upper case rho”) symmetric matrix (\\(\\rho_{ij}=\\rho_{ji}\\)).Note variance-correlation decomposition\\[\\boldsymbol \\Sigma= \\boldsymbol V^{\\frac{1}{2}} \\boldsymbol P\\boldsymbol V^{\\frac{1}{2}}\\]\\(\\boldsymbol V\\) diagonal matrix containing variances:\\[ \\boldsymbol V= \\begin{pmatrix}\n    \\sigma_{11} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma_{dd}\n\\end{pmatrix}\\]\\[\\boldsymbol P= \\boldsymbol V^{-\\frac{1}{2}}\\boldsymbol \\Sigma\\boldsymbol V^{-\\frac{1}{2}}\\]definition correlation written matrix notation.covariance matrix, many applications makes sense \npartition joint correlation matrix\n\\[\n\\boldsymbol P_{\\boldsymbol z} = \n\\begin{pmatrix} \n\\boldsymbol P_{\\boldsymbol x} &  \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol P_{\\boldsymbol y\\boldsymbol x} & \\boldsymbol P_{\\boldsymbol y} \\\\\n\\end{pmatrix} \n\\]\nwithin-group group correlation matrices\n\\(\\boldsymbol P_{\\boldsymbol x}\\) \\(\\boldsymbol P_{\\boldsymbol y}\\) \ncross-correlation matrix \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = \\boldsymbol P_{\\boldsymbol y\\boldsymbol x}^T\\)","code":""},{"path":"multivariate-random-variables.html","id":"precision-matrix-or-concentration-matrix","chapter":"1 Multivariate random variables","heading":"1.1.9.2 Precision matrix or concentration matrix","text":"\\[\\boldsymbol \\Omega= (\\omega_{ij}) = \\boldsymbol \\Sigma^{-1}\\]\\(\\boldsymbol \\Omega\\) (“Omega”) inverse covariance matrix.inverse covariance matrix can obtained via\nspectral decomposition, followed inverting eigenvalues \\(\\lambda_i\\):\n\\[\\boldsymbol \\Sigma^{-1} = \\boldsymbol U\\boldsymbol \\Lambda^{-1} \\boldsymbol U^T = \n \\boldsymbol U\\begin{pmatrix}\n    \\lambda_{1}^{-1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}^{-1}\n\\end{pmatrix} \\boldsymbol U^T \\]Note eigenvalues \\(\\lambda_i\\) need positive \\(\\boldsymbol \\Sigma\\) can inverted. (.e., \\(\\boldsymbol \\Sigma\\) needs positive definite).\n\\(\\lambda_i = 0\\) \\(\\boldsymbol \\Sigma\\) singular invertible.Importance \\(\\boldsymbol \\Sigma^{-1}\\):Many expressions multivariate statistics contain \\(\\boldsymbol \\Sigma^{-1}\\) \\(\\boldsymbol \\Sigma\\).\\(\\boldsymbol \\Sigma^{-1}\\) close connection graphical models\n(e.g. conditional independence graph, partial correlations).\\(\\boldsymbol \\Sigma^{-1}\\) natural parameter exponential family perspective.","code":""},{"path":"multivariate-random-variables.html","id":"partial-correlation-matrix","chapter":"1 Multivariate random variables","heading":"1.1.9.3 Partial correlation matrix","text":"standardised version precision matrix, see later chapter graphical models.","code":""},{"path":"multivariate-random-variables.html","id":"total-variation-and-generalised-variance","chapter":"1 Multivariate random variables","heading":"1.1.9.4 Total variation and generalised variance","text":"summarise covariance matrix \\(\\boldsymbol \\Sigma\\) single scalar value two commonly used\nmeasures:total variation: \\(\\text{Tr}(\\boldsymbol \\Sigma) = \\sum_{=1}^d \\lambda_i\\)generalised variance: \\(\\det(\\boldsymbol \\Sigma) = \\prod_{=1}^d \\lambda_i\\)generalised variance \\(\\det(\\boldsymbol \\Sigma)\\) also known volume \\(\\boldsymbol \\Sigma\\).","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-normal-distribution","chapter":"1 Multivariate random variables","heading":"1.2 Multivariate normal distribution","text":"multivariate normal model generalisation univariate normal distribution\ndimension 1 dimension \\(d\\).","code":""},{"path":"multivariate-random-variables.html","id":"univariate-normal-distribution","chapter":"1 Multivariate random variables","heading":"1.2.1 Univariate normal distribution:","text":"\\[\\text{Dimension } d = 1\\]\n\\[x \\sim N(\\mu, \\sigma^2)\\]\n\\[\\text{E}(x) = \\mu \\space , \\space  \\text{Var}(x) = \\sigma^2\\]Density:\\[f(x |\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right) \\]Plot univariate normal density :Unimodal peak \\(\\mu\\), width determined \\(\\sigma\\) (plot: \\(\\mu=2, \\sigma^2=1\\) )Special case: standard normal \\(\\mu=0\\) \\(\\sigma^2=1\\):\\[f(x |\\mu=0,\\sigma^2=1)=\\frac{1}{\\sqrt{2\\pi}} \\exp\\left( {-\\frac{x^2}{2}} \\right) \\]Differential entropy:\\[\nH(F) = \\frac{1}{2} (\\log(2 \\pi \\sigma^2) + 1) \n\\]Cross-entropy:\\[\nH(F_{\\text{ref}}, F) = \\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 } \n +\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2}  +\\log(2 \\pi \\sigma^2) \\right)\n\\]\nKL divergence:\\[\nD_{\\text{KL}}(F_{\\text{ref}}, F) = H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) = \n\\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 } \n +\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2}  -\\log\\left(\\frac{\\sigma^2_{\\text{ref}}}{ \\sigma^2}\\right) -1\n\\right)\n\\]Maximum entropy characterisation: normal distribution unique distribution\n\nhighest (differential) entropy continuous distributions support minus infinity plus infinity given mean variance.fact one reasons normal distribution important (und useful) –\nknow random variable mean variance, much else, using \nnormal distribution reasonable well justified working assumption!","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-normal-model","chapter":"1 Multivariate random variables","heading":"1.2.2 Multivariate normal model","text":"\\[\\text{Dimension } d\\]\n\\[\\boldsymbol x\\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\]\n\\[\\boldsymbol x\\sim \\text{MVN}(\\boldsymbol \\mu,\\boldsymbol \\Sigma) \\]\n\\[\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\space , \\space  \\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\]Density:\\[f(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) = \\det(2 \\pi \\boldsymbol \\Sigma)^{-\\frac{1}{2}} \\exp\\left({{-\\frac{1}{2}} \\underbrace{\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1 \\times d} \\underbrace{\\boldsymbol \\Sigma^{-1}}_{d \\times d} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d \\times 1} }_{1 \\times 1 = \\text{scalar!}}}\\right)\\]note density contains precision matrix \\(\\boldsymbol \\Sigma^{-1}\\)inverting \\(\\boldsymbol \\Sigma\\) implies inverting eigenvalues \\(\\lambda_i\\) \\(\\boldsymbol \\Sigma\\)\n(thus need \\(\\lambda_i > 0\\))density also contains \\(\\det(\\boldsymbol \\Sigma) = \\prod\\limits_{=1}^d \\lambda_i\\) \\(\\equiv\\) product eigenvalues \\(\\boldsymbol \\Sigma\\)Special case: standard multivariate normal \\[\\boldsymbol \\mu=\\boldsymbol 0, \\boldsymbol \\Sigma=\\boldsymbol =\\begin{pmatrix}\n    1 & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & 1\n\\end{pmatrix}\\]\\[f(\\boldsymbol x| \\boldsymbol \\mu=\\boldsymbol 0,\\boldsymbol \\Sigma=\\boldsymbol )=(2\\pi)^{-d/2}\\exp\\left( -\\frac{1}{2} \\boldsymbol x^T \\boldsymbol x\\right) = \\prod\\limits_{=1}^d \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_i^2}{2}\\right)\\]\nequivalent product \\(d\\) univariate standard normals!Misc:\\(d=1\\), multivariate normal reduces normal.\\(\\boldsymbol \\Sigma\\) diagonal (.e. \\(\\boldsymbol P= \\boldsymbol \\), correlation), MVN product univariate normals (see Worksheet 2).Plot MVN density:Location: \\(\\boldsymbol \\mu\\)Shape: \\(\\boldsymbol \\Sigma\\)Unimodal: one peakSupport \\(-\\infty\\) \\(+\\infty\\) dimensionAn interactive R Shiny web app bivariate normal density plot\navailable online https://minerva..manchester.ac.uk/shiny/strimmer/bvn/ .Differential entropy:\\[\nH = \\frac{1}{2} (\\log \\det(2 \\pi \\boldsymbol \\Sigma) + d) \n\\]Cross-entropy:\\[\nH(F_{\\text{ref}}, F) = \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    + \\log \\det \\biggl( 2 \\pi \\boldsymbol \\Sigma\\biggr)    \\biggr\\} \n\\]\nKL divergence:\\[\n\\begin{split}\nD_{\\text{KL}}(F_{\\text{ref}}, F) &= H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) \\\\\n&= \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr) \n     - d   \\biggr\\} \\\\\n\\end{split}\n\\]","code":""},{"path":"multivariate-random-variables.html","id":"shape-of-the-multivariate-normal-density","chapter":"1 Multivariate random variables","heading":"1.2.3 Shape of the multivariate normal density","text":"Now show contour lines multivariate normal density always take form ellipse, radii ellipse determined eigenvalues \n\\(\\boldsymbol \\Sigma\\).start observing circle radius \\(r\\) around origin can described set points \\((x_1,x_2)\\) satisfying\n\\(x_1^2+x_2^2 = r^2\\), equivalently, \\(\\frac{x_1^2}{r^2} + \\frac{x_2^2}{r^2} = 1\\).\ngeneralised shape ellipse allowing (two dimensions) two radii\n\\(r_1\\) \\(r_2\\) \n\\(\\frac{x_1^2}{r_1^2} + \\frac{x_2^2}{r_2^2} = 1\\), vector notation\n\\(\\boldsymbol x^T \\text{Diag}(r_1^2, r_2^2)^{-1} \\boldsymbol x= 1\\). \\(d\\) dimensions allowing rotation \naxes shift origin 0 \\(\\boldsymbol \\mu\\) condition ellipse \n\\[(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol Q\\, \\text{Diag}(r_1^2, \\ldots , r_d^2)^{-1} \\boldsymbol Q^T (\\boldsymbol x-\\boldsymbol \\mu) = 1\\]\n\\(\\boldsymbol Q\\) orthogonal matrix whose column vectors indicate direction axes.contour line probability density function set connected points density assumes constant value. case multivariate normal distribution keeping density fixed value implies \\((\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu) = c\\) \\(c\\) constant. Using eigenvalue decomposition \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) can rewrite condition \n\\[\n(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol U\\boldsymbol \\Lambda^{-1} \\boldsymbol U^T (\\boldsymbol x-\\boldsymbol \\mu) = c \\,.\n\\]\nimplies thatthe contour lines multivariate normal density indeed ellipses,squared radii proportional eigenvalues \\(\\boldsymbol \\Sigma\\) andthe direction axes correspond eigenvectors \\(\\boldsymbol U\\).Equivalently, positive square roots eigenvalues proportional radii ellipse. Hence, singular covariance matrix one \\(\\lambda_i=0\\) corresponding radii zero.interactive R Shiny web app play contour lines \nbivariate normal distribution online https://minerva..manchester.ac.uk/shiny/strimmer/bvn/ .","code":""},{"path":"multivariate-random-variables.html","id":"three-types-of-covariances","chapter":"1 Multivariate random variables","heading":"1.2.4 Three types of covariances","text":"Following can parameterise covariance matrix terms itsvolume,shape, andorientationby writing\n\\[\n\\boldsymbol \\Sigma= \\kappa \\, \\boldsymbol U\\boldsymbol \\boldsymbol U^T = \\boldsymbol U\\; \\left(\\kappa \\boldsymbol \\right) \\; \\boldsymbol U^T\n\\]\n\\(\\boldsymbol =\\text{Diag}(a_1, \\ldots, a_d)\\) \\(\\det(\\boldsymbol ) = \\prod_{=1}^d a_i = 1\\).\nNote parameterisation eigenvalues \\(\\boldsymbol \\Sigma\\) \\(\\lambda_i = \\kappa a_i\\).volume \\(\\det(\\boldsymbol \\Sigma) = \\kappa^d\\), determined single parameter \\(\\kappa\\). parameter can interpreted length side \\(d\\)-dimensional hypercube.shape determined diagonal matrix \\(\\boldsymbol \\) \\(d-1\\) free parameters. Note \\(d-1\\) \\(d\\) free parameters constraint \\(\\det(\\boldsymbol ) = 1\\).orientation given orthogonal matrix \\(\\boldsymbol U\\), \\(d (d-1)/2\\) free parameters.leads classification covariances three varieties:Type 1: spherical covariance \\(\\boldsymbol \\Sigma=\\kappa \\boldsymbol \\),\nspherical contour lines, 1 free parameter (\\(\\boldsymbol =\\boldsymbol \\), \\(\\boldsymbol U=\\boldsymbol \\)).Example:\n\\(\\boldsymbol \\Sigma= \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1/ \\lambda_2} = 1\\):Type 2: diagonal covariance \\(\\boldsymbol \\Sigma= \\kappa \\boldsymbol \\), elliptical contour lines axes ellipse oriented parallel coordinates, \\(d\\) free parameters (\\(\\boldsymbol U=\\boldsymbol \\)).Example:\n\\(\\boldsymbol \\Sigma= \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1 / \\lambda_2} \\approx 1.41\\):Type 3: general unrestricted covariance \\(\\boldsymbol \\Sigma\\),\nelliptical contour lines, axes ellipse oriented according \ncolumn vectors \\(\\boldsymbol U\\),\n\\(d (d+1)/2\\) free parameters.Example:\n\\(\\boldsymbol \\Sigma= \\begin{pmatrix} 2 & 0.6 \\\\ 0.6 & 1 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1 / \\lambda_2} \\approx 2.20\\):","code":""},{"path":"multivariate-random-variables.html","id":"concentration-of-probability-mass-for-small-and-large-dimension","chapter":"1 Multivariate random variables","heading":"1.2.5 Concentration of probability mass for small and large dimension","text":"density multivariate normal distribution bell shape single mode. Intuitively, may assume probability mass always concentrated around mode, univariate case (\\(d=1\\)). still true small dimensions (small \\(d\\)) now show intuition incorrect high dimensions (large \\(d\\)).simplicity consider standard multivariate normal distribution dimension \\(d\\)\n\\[\\boldsymbol x\\sim N_d(\\boldsymbol 0, \\boldsymbol I_d)\\]\nspherical covariance \\(\\boldsymbol I_d\\) sample \\(\\boldsymbol x\\). squared Euclidean length \\(\\boldsymbol x\\) \n\\(r^2= || \\boldsymbol x||^2 = \\boldsymbol x^T \\boldsymbol x= \\sum_{=1}^d x_i^2\\). corresponding density \\(d\\)-dimensional standard multivariate normal distribution \n\\[\ng_d(\\boldsymbol x) = (2\\pi)^{-d/2} e^{-\\boldsymbol x^T \\boldsymbol x/2} \n\\]\nnatural way define main part “bell” standard multivariate normal set \n\\(\\boldsymbol x\\) density larger specified fraction \\(\\eta\\) (say 0.001) maximum value density \\(g_d(0)\\) peak zero.\nformalise\n\\[\nB = \\left\\{ \\boldsymbol x: \\frac{g_d(\\boldsymbol x)}{ g_d(0)} > \\eta    \\right\\}\n\\]\ncan equivalently written set\n\\[\nB = \\{ \\boldsymbol x: \\boldsymbol x^T \\boldsymbol x= r^2 < -2 \\log(\\eta) = r^2_{\\max} \\}\n\\]individual component sample \\(\\boldsymbol x\\) independently distributed \\(x_i \\sim N(0,1)\\), hence \\(r^2 \\sim \\text{$\\chi^2_{d}$}\\) chi-squared distributed degree freedom \\(d\\).\nprobability \\(\\text{Pr}(\\boldsymbol x\\B)\\) can thus obtained value \ncumulative density function chi-squared distribution \\(d\\) degrees\nfreedom \\(r^2_{\\max}\\). Computing probability fixed \\(\\eta\\) function dimension \\(d\\) obtain following curve:\nplot \\(\\eta=0.001\\). can see dimensions around \\(d=10\\) probability mass indeed concentrated bell center \\(d=30\\) onwards moved completely tail distribution.","code":""},{"path":"multivariate-random-variables.html","id":"estimation-in-large-and-small-sample-settings","chapter":"1 Multivariate random variables","heading":"1.3 Estimation in large and small sample settings","text":"practical application multivariate normal model need \nlearn parameters observed data points:\\[\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_n \\stackrel{\\text{iid}}\\sim N_d\\left(\\boldsymbol \\mu,\\boldsymbol \\Sigma\\right)\\]first consider case \nmany measurements available (\\(n\\) large), subsequently case \nnumber data points \\(n\\) small compared dimenions number parameters.previous course year 2\n(see MATH20802 Statistical Methods)\nmethod maximum likelihood well essentials Bayesian statistics\nintroduced. apply approaches problem estimating parameters \nmultivariate normal distribution.","code":""},{"path":"multivariate-random-variables.html","id":"strategies-for-large-sample-estimation","chapter":"1 Multivariate random variables","heading":"1.3.1 Strategies for large sample estimation","text":"","code":""},{"path":"multivariate-random-variables.html","id":"empirical-estimators-outline","chapter":"1 Multivariate random variables","heading":"1.3.1.1 Empirical estimators (outline)","text":"large \\(n\\) thanks law large numbers:\n\\[\\underbrace{F}_{\\text{true}} \\approx \\underbrace{\\widehat{F}_n}_{\\text{empirical}}\\]now like estimate \\(\\) functional \\(=m(F)\\) distribution \\(F\\)\n— recall functional function takes another function argument.\nexample standard distributional summaries mean, median etc. derived \\(F\\) hence \nfunctionals \\(F\\).empirical estimate obtained replacing unknown true distribution\n\\(F\\) observed empirical distribution: \\(\\hat{} = m(\\widehat{F}_n)\\).example, expectation random variable approximated/estimated\naverage observations:\n\\[\\text{E}_F(\\boldsymbol x) \\approx \\text{E}_{\\widehat{F}_n}(\\boldsymbol x) = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k\\]\n\\[\\text{E}_F(g(\\boldsymbol x)) \\approx  \\text{E}_{\\widehat{F}_n}(g(\\boldsymbol x)) = \\frac{1}{n}\\sum^{n}_{k=1} g(\\boldsymbol x_k)\\]Simple recipe obtain empirical estimator: simply replace expectation operator\nsample average.work: empirical distribution \\(\\widehat{F}_n\\) nonparametric maximum likelihood estimate \\(F\\) (see likelihood estimation).Note: approximation \\(F\\) \\(\\widehat{F}_n\\) also basis approaches Efron’s bootstrap method (1979).2","code":""},{"path":"multivariate-random-variables.html","id":"maximum-likelihood-estimation-outline","chapter":"1 Multivariate random variables","heading":"1.3.1.2 Maximum likelihood estimation (outline)","text":"R.. Fisher (1922):3 model-based estimators using density probability mass functionlog-likelihood function:\n\\[\\log L(\\boldsymbol \\theta) = \\sum^{n}_{k=1}  \\underbrace{\\log f}_{\\text{log-density}}(\\underbrace{x_i}_{\\text{data}} |\\underbrace{\\boldsymbol \\theta}_{\\text{parameters}})\\]\nlikelihood = probability observe data given model parametersMaximum likelihood estimate:\n\\[\\hat{\\boldsymbol \\theta}^{\\text{ML}}=\\underset{\\boldsymbol \\theta}{\\arg\\,\\max} \\log L(\\boldsymbol \\theta)\\]Maximum likelihood (ML) finds parameters make observed data likely (find probable model!)Recall MATH20802 Statistical Methods\nmaximum likelihood closely linked minimising relative entropy (KL divergence)\n\\(D_{\\text{KL}}(F, F_{\\boldsymbol \\theta})\\) unknown true model \\(F\\) specified model \\(F_{\\boldsymbol \\theta}\\). Specifically, large\nsample size \\(n\\) model \\(F_{\\hat{\\boldsymbol \\theta}}\\) fit maximum likelihood indeed model closest \\(F\\).Correspondingly, great appeal maximum likelihood estimates (MLEs) optimal large \\(\\mathbf{n}\\), .e. large sample size estimator can constructed outperforms MLE (note emphasis “large \\(n\\)”!).\nadvantage method maximum likelihood provide point estimate also asymptotic error (via Fisher information related curvature log-likelihood function).","code":""},{"path":"multivariate-random-variables.html","id":"large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma","chapter":"1 Multivariate random variables","heading":"1.3.2 Large sample estimates of mean \\(\\boldsymbol \\mu\\) and covariance \\(\\boldsymbol \\Sigma\\)","text":"","code":""},{"path":"multivariate-random-variables.html","id":"empirical-estimates","chapter":"1 Multivariate random variables","heading":"1.3.2.1 Empirical estimates:","text":"Recall definitions:\n\\[\n\\boldsymbol \\mu= \\text{E}(\\boldsymbol x)\n\\]\n\n\\[\n\\boldsymbol \\Sigma= \\text{E}\\left(   (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T \\right)\n\\]empirical estimate replace expectations \ncorresponding sample averages.resulting estimators can written three different ways:Vector notation:\\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k\\]\\[\n\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\sum^{n}_{k=1} (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})  (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})^T\n= \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k  \\boldsymbol x_k^T - \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\n\\]Data matrix notation:empirical mean covariance can also written terms data matrix \\(\\boldsymbol X\\) (using statistics convention):\\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n} \\boldsymbol X^T \\boldsymbol 1_n\\]\\[\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\boldsymbol X^T \\boldsymbol X- \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\\]Component notation:corresponding component notation \\(\\boldsymbol X= (x_{ki})\\) :\\[\\hat{\\mu}_i = \\frac{1}{n}\\sum^{n}_{k=1} x_{ki}\\]\\[\\hat{\\sigma}_{ij} = \\frac{1}{n}\\sum^{n}_{k=1} (x_{ki}-\\hat{\\mu}_i) ( \nx_{kj}-\\hat{\\mu}_j )\\]\\[\\hat{\\boldsymbol \\mu}=\\begin{pmatrix}\n    \\hat{\\mu}_{1}       \\\\\n    \\vdots \\\\\n    \\hat{\\mu}_{d}\n\\end{pmatrix}, \\widehat{\\boldsymbol \\Sigma} = (\\hat{\\sigma}_{ij})\\]Variance estimate:\\[\\hat{\\sigma}_{ii} = \\frac{1}{n}\\sum^{n}_{k=1} \\left(x_{ki}-\\hat{\\mu}_i\\right)^2\\]\nNote factor \\(\\frac{1}{n}\\) (\\(\\frac{1}{n-1}\\))Engineering machine learning convention:Using engineering machine learning convention data matrix \\(\\boldsymbol X\\) estimators written \\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n} \\boldsymbol X\\boldsymbol 1_n\\]\\[\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\boldsymbol X\\boldsymbol X^T - \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\\]corresponding component notation two indices columns rowns interchanged.avoid confusion using matrix component notation need always state \nconvention used! notes strictly follow statistics convention.","code":""},{"path":"multivariate-random-variables.html","id":"maximum-likelihood-estimates","chapter":"1 Multivariate random variables","heading":"1.3.2.2 Maximum likelihood estimates","text":"now derive MLE parameters \\(\\boldsymbol \\mu\\) \\(\\boldsymbol \\Sigma\\) multivariate normal distribution.\ncorresponding log-likelihood function \n\\[\n\\begin{split}\n\\log L(\\boldsymbol \\mu, \\boldsymbol \\Sigma) & = \\sum_{k=1}^n \\log f( \\boldsymbol x_k | \\boldsymbol \\mu, \\boldsymbol \\Sigma) \\\\\n  & = -\\frac{n d}{2} \\log(2\\pi) -\\frac{n}{2} \\log \\det(\\boldsymbol \\Sigma)  \n   - \\frac{1}{2}  \\sum_{k=1}^n  (\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x_k-\\boldsymbol \\mu) \\,.\\\\\n\\end{split}\n\\]\nWritten terms precision matrix \\(\\boldsymbol \\Omega= \\boldsymbol \\Sigma^{-1}\\) becomes\n\\[\n\\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega) = -\\frac{n d}{2} \\log(2\\pi) +\\frac{n}{2} \\log \\det(\\boldsymbol \\Omega)  - \\frac{1}{2}  \\sum_{k=1}^n  (\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Omega(\\boldsymbol x_k-\\boldsymbol \\mu) \\,.\n\\]\nFirst, find MLE \\(\\boldsymbol \\mu\\) compute\n\\[\\nabla_{\\boldsymbol \\mu} \\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega) =  \\frac{\\partial \\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega) }{\\partial \\boldsymbol \\mu}= \\sum_{k=1}^n  \\boldsymbol \\Omega(\\boldsymbol x_k-\\boldsymbol \\mu)\\]\nnoting \\(\\boldsymbol \\Omega\\) symmetric (see Appendix rules vector calculus).\nSetting equal zero get \\(\\sum_{k=1}^n \\boldsymbol x_k = n \\hat{\\boldsymbol \\mu}_{ML}\\) thus\n\\[\\hat{\\boldsymbol \\mu}_{ML} = \\frac{1}{n} \\sum_{k=1}^n \\boldsymbol x_k\\,.\\]Next, obtain MLE \\(\\boldsymbol \\Omega\\) compute\n\\[\\frac{\\partial \\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega) }{\\partial \\boldsymbol \\Omega}=\\frac{n}{2}\\boldsymbol \\Omega^{-1} - \\frac{1}{2}  \\sum_{k=1}^n (\\boldsymbol x_k-\\boldsymbol \\mu) (\\boldsymbol x_k-\\boldsymbol \\mu)^T\\]\n(see Appendix rules matrix calculus).\nSetting equal zero substituting MLE \\(\\boldsymbol \\mu\\) get\n\\[\\widehat{\\boldsymbol \\Omega}^{-1}_{ML}=  \\frac{1}{n} \\sum_{k=1}^n  (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu}) (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})^T=\\widehat{\\boldsymbol \\Sigma}_{ML}\\,.\\]Therefore, MLEs identical empirical estimates.Note factor \\(\\frac{1}{n}\\) MLE covariance matrix.","code":""},{"path":"multivariate-random-variables.html","id":"distribution-of-the-empirical-maximum-likelihood-estimates","chapter":"1 Multivariate random variables","heading":"1.3.2.3 Distribution of the empirical / maximum likelihood estimates","text":"\\(\\boldsymbol x_1,...,\\boldsymbol x_n \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) one can find exact distributions\nestimators.1. Distribution estimate mean:\\[\\hat{\\boldsymbol \\mu}_{ML} \\sim N_d\\left(\\boldsymbol \\mu, \\frac{\\boldsymbol \\Sigma}{n}\\right)\\]\nSince\n\\(\\text{E}(\\hat{\\boldsymbol \\mu}_{ML}) = \\boldsymbol \\mu\\Longrightarrow \\hat{\\boldsymbol \\mu}_{ML}\\) unbiased.2. Distribution covariance estimate:\\[\\widehat{\\boldsymbol \\Sigma}_{ML} \\sim \\text{Wishart}\\left(\\frac{\\boldsymbol \\Sigma}{n}, n-1\\right)\\]\nSince\n\\(\\text{E}(\\widehat{\\boldsymbol \\Sigma}_{ML}) = \\frac{n-1}{n}\\boldsymbol \\Sigma\\) \\(\\Longrightarrow \\widehat{\\boldsymbol \\Sigma}_{ML}\\) biased, \\(\\text{Bias}(\\widehat{\\boldsymbol \\Sigma}_{ML} ) = \\boldsymbol \\Sigma- \\text{E}(\\widehat{\\boldsymbol \\Sigma}_{ML}) = -\\frac{\\boldsymbol \\Sigma}{n}\\).Easy make unbiased:\n\\(\\widehat{\\boldsymbol \\Sigma}_{UB} = \\frac{n}{n-1}\\widehat{\\boldsymbol \\Sigma}_{ML}=\\frac{1}{n-1}\\sum^n_{k=1}\\left(\\boldsymbol x_k-\\hat{\\boldsymbol \\mu}\\right)\\left(\\boldsymbol x_k-\\hat{\\boldsymbol \\mu}\\right)^T\\) unbiased.unbiasedness estimator relevant criterion multivariate statistics see next section.","code":""},{"path":"multivariate-random-variables.html","id":"problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions","chapter":"1 Multivariate random variables","heading":"1.3.3 Problems with maximum likelihood in small sample settings and high dimensions","text":"Modern data high dimensional!Data sets \\(n<d\\), .e. high dimension \\(d\\) small sample size \\(n\\) now common \nmany fields, e.g., medicine, biology also finance business analytics.\\[n = 100 \\, \\text{(e.g, patients/samples)}\\]\n\\[d = 20000 \\, \\text{(e.g., genes/SNPs/proteins/variables)}\\]\nReasons:number measured variables increasing quickly technological advances (e.g. genomics)number samples similary increased (cost ethical reasons)General problems MLEs:ML estimators optimal sample size large compared number parameters. However, optimality valid sample size moderate smaller number parameters.enough data ML estimate overfits. means ML fits current data perfectly resulting model generalise well (.e. model perform poorly prediction)choice different models different complexity ML always select model largest number parameters.-> high-dimensional data small sample size maximum likelihood estimation work!!!History Statistics:Much modern statistics (1960 onwards) devoted development inference estimation techniques work complex, high-dimensional data.Maximum likelihood method classical statistics (time 1960).1960 modern (computational) statistics emerges, starting \n“Stein Paradox” (1956): Charles Stein showed multivariate setting ML estimators dominated (= always worse ) shrinkage estimators!example, shrinkage estimator mean better (terms MSE) average (MLE)!Modern statistics developed many different (related) methods use high-dimensional, small sample settings:regularised estimatorsshrinkage estimatorspenalised maximum likelihood estimatorsBayesian estimatorsEmpirical Bayes estimatorsKL / entropy-based estimatorsMost scope class, covered advanced statistical courses.Next, describe simple regularised estimator estimation covariance\nuse later (.e. classification).","code":""},{"path":"multivariate-random-variables.html","id":"estimation-of-covariance-matrix-in-small-sample-settings","chapter":"1 Multivariate random variables","heading":"1.3.4 Estimation of covariance matrix in small sample settings","text":"Problems ML estimate \\(\\boldsymbol \\Sigma\\)\\(\\Sigma\\) O(\\(d^2\\)) number parameters!\n\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}^{\\text{MLE}}\\) requires lot data! \\(n\\gg d \\text{ } d^2\\)\\(\\Sigma\\) O(\\(d^2\\)) number parameters!\n\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}^{\\text{MLE}}\\) requires lot data! \\(n\\gg d \\text{ } d^2\\)\\(n < d\\) \\(\\hat{\\boldsymbol \\Sigma}\\) positive semi-definite (even \\(\\Sigma\\) p.d.f.!)\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}\\) vanishing eigenvalues (\\(\\lambda_i=0\\)) thus inverted singular!\\(n < d\\) \\(\\hat{\\boldsymbol \\Sigma}\\) positive semi-definite (even \\(\\Sigma\\) p.d.f.!)\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}\\) vanishing eigenvalues (\\(\\lambda_i=0\\)) thus inverted singular!Note many expression multivariate statistics actually need use inverse covariances matrix, e.g. density multivariate normal distribution, essential get invertible covariance matrix estimate.Making ML estimate \\(\\boldsymbol \\Sigma\\) invertibleThere simple numerical trick credited . N. Tikhonov make \\(\\hat{\\boldsymbol \\Sigma}\\) invertible, adding small\nnumber (say \\(\\varepsilon=10^{-6}\\) diagonal elements \\(\\hat{\\boldsymbol \\Sigma}\\):\n\\[\n\\boldsymbol S_{\\text{Tik}} = \\hat{\\boldsymbol \\Sigma} + \\varepsilon \\boldsymbol \n\\]resulting \\(\\boldsymbol S_{\\text{Tik}}\\) positive definite sum symmetric positive definite matrix (\\(\\varepsilon \\boldsymbol \\)) symmetric positive semi-definite matrix (\\(\\hat{\\boldsymbol \\Sigma}\\)) \nalways positive definite (see Appendix ).However, simple regularisation results invertible matrix estimator improved MLE, matrix \\(\\boldsymbol S_{\\text{Tik}}\\) also poorly conditioned (.e. large condition number).Simple Bayes-type regularised estimate \\(\\boldsymbol \\Sigma\\)Regularised estimator \\(\\boldsymbol S^\\ast\\) = convex combination \\(\\boldsymbol S= \\hat{\\boldsymbol \\Sigma}^\\text{MLE}\\) \\(\\boldsymbol I_d\\) (identity matrix) getRegularisation:\n\\[\n\\underbrace{\\boldsymbol S^\\ast}_{\\text{regularised estimate}} = (1-\\lambda)\\underbrace{\\boldsymbol S}_{\\text{ML estimate}} +\\underbrace{\\lambda}_{\\text{shrinkage intensity}} \\, \\underbrace{\\boldsymbol I_d}_{\\text{target}}\\]Idea: choose \\(\\lambda \\[0,1]\\) \\(\\boldsymbol S^\\ast\\) better (e.g. terms MSE) \\(\\boldsymbol S\\) \\(\\boldsymbol I_d\\). Note \\(\\lambda\\) need small like \\(\\varepsilon\\).form estimator corresponds computing mean Bayesian posterior\ndirectly shrinking MLE towards prior mean (target):\n\\[\n\\underbrace{\\boldsymbol S^\\ast}_{\\text{posterior mean}} = \\underbrace{\\lambda \\boldsymbol I_d}_{\\text{prior information}}  + (1-\\lambda)\\underbrace{\\boldsymbol S}_{\\text{data summarised maximum likelihood}}\n\\]Prior information helps infer \\(\\boldsymbol \\Sigma\\) even small samples.also called shrinkage estimator since -diagonal entries shrunk towards zero.type linear shrinkage/regularisation natural exponential family models (Diaconis Ylvisaker, 1979).Instead diagonal target options possible, e.g. block-diagonal banded covariances.\\(\\lambda\\) prespecified learned data (see ) resulting estimate empirical Bayes estimator.resulting estimate typically biased mixing target increase bias.find optimal shrinkage / regularisation parameter \\(\\lambda\\)?One way minimise \\(\\text{MSE}\\) (Mean Squared Error). also called L2 regularisation\nRidge regularisation.Bias-variance trade-: \\(\\text{MSE}\\) composed squared bias variance.\\[\\text{MSE}(\\theta) = \\text{E}((\\hat{\\theta}-\\theta)^2) = \\text{Bias}(\\hat{\\theta})^2 + \\text{Var}(\\hat{\\theta})\\]\n\\(\\text{Bias}(\\hat{\\theta}) = \\text{E}(\\hat{\\theta})-\\theta\\)\\(\\boldsymbol S\\): ML estimate, many parameters, low bias, high variance\\(\\boldsymbol I_d\\): “target”, parameters, high bias, low variance\\(\\Longrightarrow\\) reduce high variance \\(\\boldsymbol S\\) introducing bit bias \\(\\boldsymbol I_d\\)!\\(\\Longrightarrow\\) overall, \\(\\text{MSE}\\) decreasedChallenge: since don’t know true \\(\\boldsymbol \\Sigma\\) actually compute \\(\\text{MSE}\\) directly estimate ! done practise?cross-validation (=resampling procedure)using analytic approximation (e.g. Stein’s formula)Worksheet 3 empirical estimator covariance compared regularised covariance estimator implemented R package “corpcor”. uses regularisation similar (correlation rather covariance matrix) employs analytic data-adaptive estimate shrinkage intensity \\(\\lambda\\).\nestimator variant empirical Bayes / James-Stein estimator (see MATH20802 Statistical Methods).SummaryIn multivariate statistics, useful (often necessary) utilise prior information!Regularisation introduces bias reduces variance, minimising overall MSEUnbiased estimation (highly valued property classical statistics!) good idea multivariate settings often leads poor estimators.","code":""},{"path":"multivariate-random-variables.html","id":"categorical-and-multinomial-distribution","chapter":"1 Multivariate random variables","heading":"1.4 Categorical and multinomial distribution","text":"previous section, seen multivariate normal distribution generalises univariate normal distribution.\nsection, consider multivariate generalisations Bernoulli binomial distribution,\ngiven categorical multinomial distributions, respectively.","code":""},{"path":"multivariate-random-variables.html","id":"categorical-distribution","chapter":"1 Multivariate random variables","heading":"1.4.1 Categorical distribution","text":"categorical distribution generalisation Bernoulli distribution\ncorrespondingly also known Multinoulli distribution.Assume \\(K\\) classes labelled “class 1”, “class 2”, …, “class K”.\ndiscrete random variable state space consisting \\(K\\) classes\ncategorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\).\nparameter vector\n\\(\\boldsymbol \\pi= (\\pi_1, \\ldots, \\pi_K)^T\\) specifies\nprobabilities \\(K\\) classes \\(\\text{Pr}(\\text{\"class k\"}) = \\pi_k\\).\nparameters satisfy \\(\\pi_k \\[0,1]\\) \n\\(\\sum_{k=1}^K \\pi_k = 1\\), hence \\(K-1\\) independent parameters categorical distribution (\\(K\\)).Sampling categorical distributions \\(\\text{Cat}(\\boldsymbol \\pi)\\) yields one \\(K\\) classes.\nseveral ways numerically\nrepresent “class k”, example simply corresponding number \\(k\\). However, instead\n“integer encoding” often\nconvenient use -called “one hot encoding” class\nrepresented indicator vector\n\\(\\boldsymbol x= (x_1, \\ldots, x_K)^T = (0, 0, \\ldots, 1, \\ldots, 0)^T\\) containing zeros everywhere except \nelement \\(x_k=1\\) position \\(k\\). Thus \\(x_k \\\\{ 0, 1\\}\\) \\(\\sum_{k=1}^K x_k = 1\\).expectation \\(\\boldsymbol x\\sim \\text{Cat}(\\boldsymbol \\pi)\\) \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\pi\\), \n\\(\\text{E}(x_k) = \\pi_k\\).\ncovariance matrix \\(\\text{Var}(\\boldsymbol x) = \\text{Diag}(\\boldsymbol \\pi) - \\boldsymbol \\pi\\boldsymbol \\pi^T\\).\ncomponent notation \\(\\text{Var}(x_i) = \\pi_i (1-\\pi_i)\\) \\(\\text{Cov}(x_i, x_j) = -\\pi_i \\pi_j\\).\nfollows directly definition variance \\(\\text{Var}(\\boldsymbol x) = \\text{E}( \\boldsymbol x\\boldsymbol x^T) - \\text{E}( \\boldsymbol x) \\text{E}( \\boldsymbol x)^T\\)\nnoting \\(x_i^2 = x_i\\) \\(x_i x_j = 0\\) \\(\\neq j\\).\nNote variance matrix \\(\\text{Var}(\\boldsymbol x)\\) singular construction, \\(K\\) random variables\n\\(x_1, \\ldots, x_K\\) dependent constraint \\(\\sum_{k=1}^K x_k = 1\\).corresponding probability mass function (pmf)\ncan written conveniently terms \\(x_k\\) \n\\[\nf(\\boldsymbol x) = \\prod_{k=1}^K \\pi_k^{x_k} = \n\\begin{cases} \n   \\pi_k  & \\text{} x_k = 1 \\\\\n\\end{cases}\n\\]\nlog pmf \n\\[\n\\log f(\\boldsymbol x) = \\sum_{k=1}^K x_k \\log \\pi_k   =\n\\begin{cases} \n   \\log \\pi_k  & \\text{} x_k = 1 \\\\\n\\end{cases}\n\\]order explicit categorical distribution \\(K-1\\) \\(K\\) parameters\nrewrite log-density \n\\(\\pi_K = 1 - \\sum_{k=1}^{K-1} \\pi_k\\) \\(x_K = 1 - \\sum_{k=1}^{K-1} x_k\\) \n\\[\n\\begin{split}\n\\log f(\\boldsymbol x) & =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + x_K \\log \\pi_K \\\\\n & =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_k  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\\\\n\\end{split}\n\\]\nNote particular reason choose \\(\\pi_K\\) derived, place \n\\(\\pi_k\\) may selected.\\(K=2\\) categorical distribution reduces Bernoulli \\(\\text{Ber}(p)\\) distribution,\n\\(\\pi_1=p\\) \\(\\pi_2=1-p\\).","code":""},{"path":"multivariate-random-variables.html","id":"multinomial-distribution","chapter":"1 Multivariate random variables","heading":"1.4.2 Multinomial distribution","text":"multinomial distribution arises repeated categorical sampling,\njust like binomial distribution arises repeated Bernoulli sampling.","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case","chapter":"1 Multivariate random variables","heading":"1.4.2.1 Univariate case","text":"Binomial distribution:Repeat Bernoulli \\(\\text{Ber}(\\pi)\\) experiment \\(n\\) times:\\[x \\sim \\text{Bin}(n, \\pi)\\]\n\\[ x \\\\{0,...,n\\}\\]\n\\[\\text{E}(x) = n \\, \\pi\\]\n\\[\\text{Var}(x)=n \\, \\pi(1-\\pi)\\]Standardised unit interval:\n\\[\\frac{x}{n} \\\\left\\{0,\\frac{1}{n},...,1\\right\\}\\]\n\\[\\text{E}\\left(\\frac{x}{n}\\right) = \\pi\\]\n\\[\\text{Var}\\left(\\frac{x}{n}\\right)=\\frac{\\pi(1-\\pi)}{n}\\]\\[\\textbf{Urn model:}\\]distribute \\(n\\) balls two bins","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case","chapter":"1 Multivariate random variables","heading":"1.4.2.2 Multivariate case","text":"Multinomial distribution:Draw \\(n\\) times categorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\):\\[\\boldsymbol x\\sim \\text{Mult}(n, \\boldsymbol \\pi)  \\]\n\\[ x_i \\\\{0,1,...,n\\}; \\, \\sum^{K}_{=1}x_i = n\\]\n\\[\\text{E}(\\boldsymbol x) = n \\,\\boldsymbol \\pi\\]\n\\[\\text{Var}(x_i)=n\\, \\pi_i(1-\\pi_i)\\]\n\\[\\text{Cov}(x_i,x_j)=-n\\, \\pi_i\\pi_j\\]Standardised unit interval:\n\\[\\frac{x_i}{n} \\\\left\\{0,\\frac{1}{n},\\frac{2}{n},...,1\\right\\}\\]\n\\[\\text{E}\\left(\\frac{\\boldsymbol x}{n}\\right) = \\boldsymbol \\pi\\]\n\\[\\text{Var}\\left(\\frac{x_i}{n}\\right)=\\frac{\\pi_i(1-\\pi_i)}{n}\\]\n\\[\\text{Cov}\\left(\\frac{x_i}{n},\\frac{x_j}{n}\\right)=-\\frac{\\pi_i\\pi_j}{n} \\]\n\\[\\textbf{Urn model:}\\]distribute \\(n\\) balls \\(K\\) bins:","code":""},{"path":"multivariate-random-variables.html","id":"entropy-and-maximum-likelihood-analysis-for-the-categorical-distribution","chapter":"1 Multivariate random variables","heading":"1.4.3 Entropy and maximum likelihood analysis for the categorical distribution","text":"following compute KL divergence, MLE related quantities categorical distribution.generalises calculations Bernoulli distribution discussed earlier module MATH20802 Statistical Methods.Example 1.1  KL divergence two categorical distributions \\(K\\) classes:\\(P=\\text{Cat}(\\boldsymbol p)\\) \\(Q=\\text{Cat}(\\boldsymbol q)\\) corresponding\nprobabilities \\(p_1,\\dots,p_K\\) \\(q_1,\\dots,q_K\\) satisfying \\(\\sum_{=1}^K p_i =1\\) \\(\\sum_{=1}^K q_i = 1\\) get:\\[\\begin{equation*}\nD_{\\text{KL}}(P, Q)=\\sum_{=1}^K p_i\\log\\left(\\frac{p_i}{q_i}\\right) \n\\end{equation*}\\]explicit \\(K-1\\) parameters categorical distribution can also write\n\\[\\begin{equation*}\nD_{\\text{KL}}(P, Q)=\\sum_{=1}^{K-1} p_i\\log\\left(\\frac{p_i}{q_i}\\right)  + p_K\\log\\left(\\frac{p_K}{q_K}\\right)\n\\end{equation*}\\]\n\\(p_K=\\left(1- \\sum_{=1}^{K-1} p_i\\right)\\) \n\\(q_K=\\left(1- \\sum_{=1}^{K-1} q_i\\right)\\).Example 1.2  Expected Fisher information categorical distribution:first compute Hessian matrix\n\\(\\nabla \\nabla^T \\log f(\\boldsymbol x)\\) log-probability mass function, \ndifferentiation regard \\(\\pi_1, \\ldots, \\pi_{K-1}\\).diagonal entries Hessian matrix (\\(=1, \\ldots, K-1\\)) \n\\[\n\\frac{\\partial^2}{\\partial \\pi_i^2} \\log f(\\boldsymbol x) =\n  -\\frac{x_i}{\\pi_i^2}-\\frac{x_K}{\\pi_K^2}\n\\]\n-diagonal entries (\\(j=1, \\ldots, K-1\\))\n\\[\n\\frac{\\partial^2}{\\partial \\pi_i \\partial \\pi_j} \\log f(\\boldsymbol x) =\n -\\frac{ x_K}{\\pi_K^2}\n\\]\nRecalling \\(\\text{E}(x_i) = \\pi_i\\) can compute expected Fisher information matrix categorical distribution \n\\[\n\\begin{split}\n\\boldsymbol ^{\\text{Fisher}}\\left( \\pi_1, \\ldots, \\pi_{K-1}  \\right) &= -\\text{E}\\left( \\nabla \\nabla^T \\log f(\\boldsymbol x) \\right) \\\\\n& =\n\\begin{pmatrix}\n \\frac{1}{\\pi_1} + \\frac{1}{\\pi_K} & \\cdots & \\frac{1}{\\pi_K} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{\\pi_K} & \\cdots & \\frac{1}{\\pi_{K-1}} + \\frac{1}{\\pi_K} \\\\\n\\end{pmatrix}\\\\\n& = \\text{Diag}\\left( \\frac{1}{\\pi_1} , \\ldots,  \\frac{1}{\\pi_{K-1}}   \\right) + \\frac{1}{\\pi_K} \\boldsymbol 1\\\\\n\\end{split}\n\\]\\(K=2\\) \\(\\pi_1=p\\) reduces expected Fisher information Bernoulli variable\n\\[\n\\begin{split}\n^{\\text{Fisher}}(p) & =  \\left(\\frac{1}{p} + \\frac{1}{1-p} \\right) \\\\\n  &= \\frac{1}{p (1-p)} \\\\\n\\end{split}\n\\]Example 1.3  Quadratic approximation KL divergence categorical distribution:expected Fisher information arises local quadratic approximation KL divergence:\n\\[\nD_{\\text{KL}}(F_{\\boldsymbol \\theta}, F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon})  \\approx  \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)  \\boldsymbol \\varepsilon\n\\]\n\n\\[\nD_{\\text{KL}}(F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon}, F_{\\boldsymbol \\theta}) \\approx  \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)  \\boldsymbol \\varepsilon\n\\]now consider KL divergence \\(D_{\\text{KL}}(P, Q)\\) categorical distribution \\(P=\\text{Cat}(\\boldsymbol p)\\) probabilities \\(\\boldsymbol p=(p_1, \\ldots, p_K)^T\\) categorical distribution \\(Q=\\text{Cat}(\\boldsymbol q)\\) probabilities \\(\\boldsymbol q= (q_1, \\ldots, q_K)^T\\).First, keep \\(P\\) fixed assume \\(Q\\) perturbed version \\(P\\) \\(\\boldsymbol q= \\boldsymbol p+\\boldsymbol \\varepsilon\\).\nNote perturbations \\(\\boldsymbol \\varepsilon=(\\varepsilon_1, \\ldots, \\varepsilon_K)^T\\) satisfy\n\\(\\sum_{k=1}^K \\varepsilon_k = 0\\) \\(\\sum_{k=1}^K p_i=1\\) \\(\\sum_{k=1}^K q_i=1\\).\nThus \\(\\varepsilon_K = -\\sum_{k=1}^{K-1} \\varepsilon_k\\). \n\\[\n\\begin{split}\nD_{\\text{KL}}(P, Q=P+\\varepsilon) &  = D_{\\text{KL}}(\\text{Cat}(\\boldsymbol p), \\text{Cat}(\\boldsymbol p+\\boldsymbol \\varepsilon)) \\\\\n&  \\approx \\frac{1}{2} (\\varepsilon_1, \\ldots,  \\varepsilon_{K-1}) \\,\n\\boldsymbol ^{\\text{Fisher}}\\left( p_1, \\ldots, p_{K-1}  \\right) \n\\begin{pmatrix} \\varepsilon_1 \\\\ \\vdots \\\\  \\varepsilon_{K-1}\\\\\n\\end{pmatrix} \\\\\n&= \\frac{1}{2} \\left( \\sum_{k=1}^{K-1} \\frac{\\varepsilon_k^2}{p_k}   + \\frac{ \\left(\\sum_{k=1}^{K-1} \\varepsilon_k\\right)^2}{p_K} \\right)  \\\\\n&= \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{\\varepsilon_k^2}{p_k}\\\\\n&= \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{(p_k-q_k)^2}{p_k}\\\\\n& = \\frac{1}{2} D_{\\text{Neyman}}(P, Q)\\\\\n\\end{split} \n\\]\nSimilarly, keep \\(Q\\) fixed consider \\(P\\) disturbed version \\(Q\\) get\n\\[\n\\begin{split}\nD_{\\text{KL}}(P=Q+\\varepsilon, Q) &  =D_{\\text{KL}}(\\text{Cat}(\\boldsymbol q+\\boldsymbol \\varepsilon), \\text{Cat}(\\boldsymbol q)) \\\\\n&\\approx \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{(p_k-q_k)^2}{q_k}\\\\\n&= \\frac{1}{2} D_{\\text{Pearson}}(P, Q)\n\\end{split}\n\\]\nNote approximations divide probabilities distribution \nkept fixed.Note appearance Pearson \\(\\chi^2\\) divergence Neyman \\(\\chi^2\\) divergence . , like KL divergence, part family \\(f\\)-divergences. Neyman \\(\\chi^2\\)\ndivergence also known reverse Pearson divergence \\(D_{\\text{Neyman}}(P, Q) = D_{\\text{Pearson}}(Q, P)\\).Example 1.4  Maximum likelihood estimation parameters categorical distribution:Maximum likelihood estimation seems trivial first sight fact bit complicated since \\(K-1\\) free parameters, \\(K\\). either need optimise regard specific set \\(K-1\\) parameters () use constrained optimisation procedure enforce \\(\\sum_{k=1}^K \\pi_k = 1\\) (example using Lagrange multiplier).data: observe \\(n\\) samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).\ndata matrix dimension \\(n \\times K\\) \n\\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)^T = (x_{ik})\\).\ncontains \\(\\boldsymbol x_i = (x_{i1}, \\ldots, x_{iK})^T\\). corresponding summary\n(minimal sufficient) statistics \n\\(\\bar{\\boldsymbol x} = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = (\\bar{x}_1, \\ldots, \\bar{x}_K)^T\\)\n\n\\(\\bar{x}_k = \\frac{1}{n} \\sum_{=1}^n x_{ik}\\). can also write\n\\(\\bar{x}_{K} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k}\\).\nnumber samples class \\(k\\) \\(n_k = n \\bar{x}_k\\) \n\\(\\sum_{k=1}^K n_k = n\\).data: observe \\(n\\) samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).\ndata matrix dimension \\(n \\times K\\) \n\\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)^T = (x_{ik})\\).\ncontains \\(\\boldsymbol x_i = (x_{i1}, \\ldots, x_{iK})^T\\). corresponding summary\n(minimal sufficient) statistics \n\\(\\bar{\\boldsymbol x} = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = (\\bar{x}_1, \\ldots, \\bar{x}_K)^T\\)\n\n\\(\\bar{x}_k = \\frac{1}{n} \\sum_{=1}^n x_{ik}\\). can also write\n\\(\\bar{x}_{K} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k}\\).\nnumber samples class \\(k\\) \\(n_k = n \\bar{x}_k\\) \n\\(\\sum_{k=1}^K n_k = n\\).log-likelihood \n\\[\n\\begin{split}\nl_n(\\pi_1, \\ldots, \\pi_{K-1}) & = \\sum_{=1}^n \\log f(\\boldsymbol x_i) \\\\\n& =\\sum_{=1}^n \\left( \\sum_{k=1}^{K-1}  x_{ik} \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_{ik}  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\right)\\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right) \\log\\left(1 - \\sum_{k=1}^{K-1} \\pi_k\\right) \\right) \\\\ \n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\bar{x}_K \\log \\pi_K \\right) \\\\  \n\\end{split}\n\\]log-likelihood \n\\[\n\\begin{split}\nl_n(\\pi_1, \\ldots, \\pi_{K-1}) & = \\sum_{=1}^n \\log f(\\boldsymbol x_i) \\\\\n& =\\sum_{=1}^n \\left( \\sum_{k=1}^{K-1}  x_{ik} \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_{ik}  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\right)\\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right) \\log\\left(1 - \\sum_{k=1}^{K-1} \\pi_k\\right) \\right) \\\\ \n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\bar{x}_K \\log \\pi_K \\right) \\\\  \n\\end{split}\n\\]Score function (gradient)\n\\[\n\\begin{split}\n\\boldsymbol S_n(\\pi_1, \\ldots, \\pi_{K-1}) &=  \\nabla l_n(\\pi_1, \\ldots, \\pi_{K-1} ) \\\\\n& = \n\\begin{pmatrix}\n \\frac{\\partial}{\\partial \\pi_1} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\vdots\\\\\n\\frac{\\partial}{\\partial \\pi_{K-1}} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\end{pmatrix}\\\\\n& = n\n\\begin{pmatrix}\n\\frac{\\bar{x}_1}{\\pi_1}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\vdots\\\\\n\\frac{\\bar{x}_{K-1}}{\\pi_{K-1}}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\]\nNote particular need second term arises \\(\\pi_K\\) depends \n\\(\\pi_1, \\ldots, \\pi_{K-1}\\).Score function (gradient)\n\\[\n\\begin{split}\n\\boldsymbol S_n(\\pi_1, \\ldots, \\pi_{K-1}) &=  \\nabla l_n(\\pi_1, \\ldots, \\pi_{K-1} ) \\\\\n& = \n\\begin{pmatrix}\n \\frac{\\partial}{\\partial \\pi_1} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\vdots\\\\\n\\frac{\\partial}{\\partial \\pi_{K-1}} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\end{pmatrix}\\\\\n& = n\n\\begin{pmatrix}\n\\frac{\\bar{x}_1}{\\pi_1}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\vdots\\\\\n\\frac{\\bar{x}_{K-1}}{\\pi_{K-1}}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\]\nNote particular need second term arises \\(\\pi_K\\) depends \n\\(\\pi_1, \\ldots, \\pi_{K-1}\\).Maximum likelihood estimate: Setting \\(\\boldsymbol S_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML})=0\\) yields\n\\(K-1\\) equations\n\\[\n\\bar{x}_i \\left(1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML}\\right)  = \\hat{\\pi}_i^{ML} \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right)\n\\]\n\\(=1, \\ldots, K-1\\) solution\n\\[\n\\hat{\\pi}_i^{ML} = \\bar{x}_i\n\\]\nalso follows \n\\[ \n\\hat{\\pi}_K^{ML} = 1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} = \\bar{x}_K\n\\]\nmaximum likelihood estimator therefore frequency\noccurance class among \\(n\\) samples.Maximum likelihood estimate: Setting \\(\\boldsymbol S_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML})=0\\) yields\n\\(K-1\\) equations\n\\[\n\\bar{x}_i \\left(1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML}\\right)  = \\hat{\\pi}_i^{ML} \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right)\n\\]\n\\(=1, \\ldots, K-1\\) solution\n\\[\n\\hat{\\pi}_i^{ML} = \\bar{x}_i\n\\]\nalso follows \n\\[ \n\\hat{\\pi}_K^{ML} = 1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} = \\bar{x}_K\n\\]\nmaximum likelihood estimator therefore frequency\noccurance class among \\(n\\) samples.Example 1.5  Observed Fisher information categorical distribution:first need compute negative Hessian matrix log likelihood function\n\\(- \\nabla \\nabla^T l_n(\\pi_1, \\ldots, \\pi_{K-1} )\\) evaluate \nMLEs \\(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}\\).diagonal entries Hessian matrix (\\(=1, \\ldots, K-1\\)) \n\\[\n\\frac{\\partial^2}{\\partial \\pi_i^2} l_n(\\pi_1, \\ldots, \\pi_{K-1} ) =\n -n \\left( \\frac{\\bar{x}_i}{\\pi_i^2} +\\frac{\\bar{x}_K}{\\pi_K^2}\\right)\n\\]\n-diagonal entries (\\(j=1, \\ldots, K-1\\))\n\\[\n\\frac{\\partial^2}{\\partial \\pi_i \\partial \\pi_j} l_n(\\pi_1, \\ldots, \\pi_{K-1} ) =\n -\\frac{n \\bar{x}_K}{\\pi_K^2}\n\\]\nThus, observed Fisher information matrix MLE categorical distribution \n\\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) =\nn \n\\begin{pmatrix}\n \\frac{1}{\\hat{\\pi}_1^{ML}} + \\frac{1}{\\hat{\\pi}_K^{ML}} & \\cdots & \\frac{1}{\\hat{\\pi}_K^{ML}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{\\hat{\\pi}_K^{ML}} & \\cdots & \\frac{1}{\\hat{\\pi}_{K-1}^{ML}} + \\frac{1}{\\hat{\\pi}_K^{ML}} \\\\\n\\end{pmatrix} \n\\]\\(K=2\\) reduces observed Fisher information Bernoulli variable\n\\[\n\\begin{split}\nJ_n(\\hat{p}_{ML}) & = n \\left(\\frac{1}{\\hat{p}_{ML}} + \\frac{1}{1-\\hat{p}_{ML}} \\right) \\\\\n  &= \\frac{n}{\\hat{p}_{ML} (1-\\hat{p}_{ML})} \\\\\n\\end{split}\n\\]inverse observed Fisher information :\n\\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  )^{-1} =\n\\frac{1}{n} \n\\begin{pmatrix}\n\\hat{\\pi}_1^{ML} (1- \\hat{\\pi}_1^{ML} )  & \\cdots & -  \\hat{\\pi}_{1}^{ML} \\hat{\\pi}_{K-1}^{ML}   \\\\\n\\vdots & \\ddots & \\vdots \\\\\n-  \\hat{\\pi}_{K-1}^{ML} \\hat{\\pi}_{1}^{ML} & \\cdots & \\hat{\\pi}_{K-1}^{ML} (1- \\hat{\\pi}_{K-1}^{ML} )  \\\\\n\\end{pmatrix}\n\\]show indeed inverse use Woodbury matrix identity (see Appendix)\n\\[\n(\\boldsymbol + \\boldsymbol U\\boldsymbol B\\boldsymbol V)^{-1} = \\boldsymbol ^{-1} - \\boldsymbol ^{-1} \\boldsymbol U(\\boldsymbol B^{-1} + \\boldsymbol V\\boldsymbol ^{-1} \\boldsymbol U)^{-1} \\boldsymbol V\\boldsymbol ^{-1}\n\\]\n\\(B=1\\),\\(\\boldsymbol u= (\\pi_1, \\ldots, \\pi_{K-1})^T\\),\\(\\boldsymbol v=-\\boldsymbol u^T\\),\\(\\boldsymbol = \\text{Diag}(\\boldsymbol u)\\) inverse \\(\\boldsymbol ^{-1} = \\text{Diag}(\\pi_1^{-1}, \\ldots, \\pi_{K-1}^{-1})\\).\\(\\boldsymbol ^{-1} \\boldsymbol u= \\boldsymbol 1_{K-1}\\) \\(1-\\boldsymbol u^T \\boldsymbol ^{-1} \\boldsymbol u= \\pi_K\\).\n\n\\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  )^{-1} = \\frac{1}{n}\n\\left( \\boldsymbol - \\boldsymbol u\\boldsymbol u^T \\right)\n\\]\n\n\\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) = n \\left( \\boldsymbol ^{-1} + \\frac{1}{\\pi_K} \\boldsymbol 1_{K-1 \\times K-1}  \\right)\n\\]\\(K=2\\) inverse observed Fisher information categorical distribution reduces Bernoulli distribution\n\\[\nJ_n(\\hat{p}_{ML})^{-1}=\\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\n\\]inverse observed Fisher information useful, e.g., \nasymptotic variance maximum likelihood estimate.Example 1.6  Wald statistic categorical distribution:squared Wald statistic \n\\[\n\\begin{split}\nt(\\boldsymbol p_0)^2 &= \n(\\hat{\\pi}_{1}^{ML}-p_1^0, \\ldots,  \\hat{\\pi}_{K-1}^{ML}-p_{K-1}^0)   \\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML} ) \\begin{pmatrix} \\hat{\\pi}_{1}^{ML}-p_1^0 \\\\\n\\vdots \\\\\n\\hat{\\pi}_{K-1}^{ML}-p_{K-1}^0\\\\\n\\end{pmatrix}\\\\\n&= n  \\left( \\sum_{k=1}^{K-1} \\frac{(\\hat{\\pi}_{k}^{ML}-p_{k}^0)^2}{\\hat{\\pi}_{k}^{ML}}   + \\frac{ \\left(\\sum_{k=1}^{K-1} (\\hat{\\pi}_{k}^{ML}-p_{k}^0)\\right)^2}{\\hat{\\pi}_{K}^{ML}} \\right)  \\\\\n&= n  \\left( \\sum_{k=1}^{K} \\frac{(\\hat{\\pi}_{k}^{ML}-p_{k}^0)^2}{\\hat{\\pi}_{k}^{ML}}    \\right)  \\\\\n& = n D_{\\text{Neyman}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) )\n\\end{split}\n\\]\\(n_1, \\ldots, n_K\\) observed counts \\(n = \\sum_{k=1}^K n_k\\)\n\\(\\hat{\\pi}_k^{ML} = \\frac{n_k}{n} = \\bar{x}_k\\),\n\\(n_1^{\\text{expect}}, \\ldots, n_K^{\\text{expect}}\\) \nexpected counts \\(n_k^{\\text{expect}} = n p_k^{0}\\) \\(\\boldsymbol p_0\\)\ncan write squared Wald statistic\nfollows:\n\\[\nt(\\boldsymbol p_0)^2 = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}} )^2}{n_k} =  \\chi^2_{\\text{Neyman}}\n\\]\nknown Neyman chi-squared statistic (note observed counts denominator) asymptotically distributed \\(\\chi^2_{K-1}\\) \n\\(K-1\\) free parameters \\(\\boldsymbol p_0\\).Example 1.7  Wilks log-likelihood ratio statistic categorical distribution:Wilks log-likelihood ratio \n\\[\nW(\\boldsymbol p_0) = 2 (l_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) - l_n(p_1^{0}, \\ldots, p_{K-1}^{0}    ))\n\\]\n\\(\\boldsymbol p_0 = c(p_1^{0}, \\ldots, p_{K}^{0} )^T\\).\nprobabilities sum 1 \\(K-1\\) free parameters.log-likelihood MLE \n\\[\nl_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log \\hat{\\pi}_k^{ML}  =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log \\bar{x}_k \n\\]\n\\(\\hat{\\pi}_k^{ML} = \\frac{n_k}{n} = \\bar{x}_k\\).\nNote following sums run \\(1\\) \\(K\\) \\(K\\)-th component always computed components \\(1\\) \\(K-1\\), previous section.\nlog-likelihood \\(\\boldsymbol p_0\\) \n\\[l_n( p_1^{0}, \\ldots, p_{K-1}^{0}    ) =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log p_k^{0} \n\\]\nWilks statistic becomes\n\\[\nW(\\boldsymbol p_0) = 2 n   \\sum_{k=1}^{K}  \\bar{x}_k \\log\\left(\\frac{\\bar{x}_k}{ p_k^{0}} \\right) \n\\]\nasymptotically chi-squared distributed \\(K-1\\) degrees freedom.Note model Wilks statistic equal KL Divergence\n\\[\nW(\\boldsymbol p_0) = 2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) )\n\\]Wilks log-likelihood ratio statistic categorical distribution also known \\(G\\) test statistic \\(\\hat{\\boldsymbol \\pi}_{ML}\\) corresponds observed frequencies (observed data) \\(\\boldsymbol p_0\\) expected frequencies (.e. hypothesised true frequencies).Using observed counts \\(n_k\\) expected counts \\(n_k^{\\text{expect}} = n p_k^{0}\\)\ncan write Wilks statistic respectively \\(G\\)-statistic\nfollows:\n\\[\nW(\\boldsymbol p_0) = 2   \\sum_{k=1}^{K}  n_k \\log\\left(\\frac{  n_k }{  n_k^{\\text{expect}}   } \\right) \n\\]Example 1.8  Quadratic approximation Wilks log-likelihood ratio statistic categorical distribution:Developing Wilks statistic \\(W(\\boldsymbol p_0)\\) around MLE \\(\\hat{\\boldsymbol \\pi}_{ML}\\) yields squared Wald statistic categorical distribution Neyman chi-squared statistic:\n\\[\n\\begin{split}\nW(\\boldsymbol p_0)& = 2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) \\\\\n& \\approx n D_{\\text{Neyman}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) \\\\\n& = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}} )^2}{n_k} \\\\\n& =  \\chi^2_{\\text{Neyman}}\\\\\n\\end{split}\n\\]instead approximate KL divergence assuming \\(\\boldsymbol p_0\\) fixed arrive \n\\[ \n\\begin{split}\n2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) &\\approx n D_{\\text{Pearson}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}),  \\text{Cat}(\\boldsymbol p_0 ) )\\\\\n& = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}})^2}{n_k^{\\text{expect}}} \\\\\n& = \\chi^2_{\\text{Pearson}}\n\\end{split}\n\\]\nwell-known Pearson chi-squared statistic (note expected counts denominator).","code":""},{"path":"multivariate-random-variables.html","id":"further-multivariate-distributions","chapter":"1 Multivariate random variables","heading":"1.5 Further multivariate distributions","text":"following describe multivariate distributions.\nSpecifically, discuss properties thethe Dirichlet distribution (generalisation Beta distribution),Wishart distribution (generalisation Gamma distribution, also known scaled \\(\\chi^2\\) distribution), ofthe inverse Wishart distribution (generalisation inverse Gamma distribution).","code":""},{"path":"multivariate-random-variables.html","id":"dirichlet-distribution","chapter":"1 Multivariate random variables","heading":"1.5.1 Dirichlet distribution","text":"","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case-1","chapter":"1 Multivariate random variables","heading":"1.5.1.1 Univariate case","text":"Beta distribution\\[x \\sim \\text{Beta}(\\alpha,\\beta)\\]\n\\[x \\[0,1]\\]\n\\[\\alpha > 0; \\beta > 0\\]\n\\[m = \\alpha + \\beta \\]\n\\[\\mu = \\frac{\\alpha}{m} \\\\left[0,1\\right]\\]\n\\[\\text{E}(x) = \\mu\\]\n\\[\\text{Var}(x)=\\frac{\\mu(1-\\mu)}{m+1}\\]\n\\(\\text{compare unit standardised binomial!}\\)\\(\\textbf{Different shapes}\\)\\[\\text{Useful distribution proportion } \\pi\\]\\[ \\text{ Bayesian Model:}\\]\\[\\text{Beta prior:} \\; \\pi \\sim  \\text{Beta}(\\alpha,\\beta)\\]\n\\[\\text{Binomial likelihood:} \\; x|\\pi \\sim \\text{Bin}(n, \\pi)\\]","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case-1","chapter":"1 Multivariate random variables","heading":"1.5.1.2 Multivariate case","text":"Dirichlet distribution\\[\\boldsymbol x\\sim \\text{Dir}(\\boldsymbol \\alpha)\\]\n\\[x_i \\[0,1]; \\, \\sum^{d}_{=1} x_i = 1\\]\n\\[\\boldsymbol \\alpha= (\\alpha_1,...,\\alpha_d)^T >0\\]\n\\[m = \\sum^{d}_{=1}\\alpha_i\\]\n\\[\\mu_i = \\frac{\\alpha_i}{m} \\\\left[0,1\\right]\\]\n\\[\\text{E}(x_i) = \\mu_i\\]\n\\[\\text{Var}(x_i)=\\frac{\\mu_i(1-\\mu_i)}{m+1}\\]\n\\[\\text{Cov}(x_i,x_j)=-\\frac{\\mu_i \\mu_j}{m+1}\\]\n\\(\\text{compare unit standardised multinomial!}\\)Stick breaking\" model\\[\\text{Useful distribution proportion } \\boldsymbol \\pi\\]\\[\\text{ Bayesian Model:}\\]\\[\\text{Dirichlet prior:} \\,  \\boldsymbol \\pi\\sim \\text{Dir}(\\boldsymbol \\alpha)\\]\n\\[\\text{Multinomial likelihood:} \\, \\boldsymbol x|\\boldsymbol \\pi\\sim \\text{Mult}(n, \\boldsymbol \\pi)\\]","code":""},{"path":"multivariate-random-variables.html","id":"wishart-distribution","chapter":"1 Multivariate random variables","heading":"1.5.2 Wishart distribution","text":"Wishart distribution multivariate generalisation gamma distribution,\nalso known univariate Wishart distribution scaled chi-squared distribution. exponential chi-squared distribution special cases.","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case-2","chapter":"1 Multivariate random variables","heading":"1.5.2.1 Univariate case","text":"Gamma distribution:\\[z_1,z_2,\\ldots,z_m \\stackrel{\\text{iid}}\\sim N(0,\\sigma^2_z)\\]\n\\[x = \\sum^{m}_{=1}z_i^2\\]\n\\[\\mu_x = m \\sigma^2_z\\]\n\\(x\\) distributed :\n\\[\nx \\sim  \\text{W}_1\\left(\\frac{\\mu_x}{m}, m\\right) =\n        \\text{Gam}\\left(\\alpha=\\frac{1}{2} m, \\beta=2 \\frac{\\mu_x}{m}\\right) = \n        \\frac{\\mu_x}{m} \\chi^2_m\n\\]\n\\(\\alpha\\) shape \\(\\beta\\) scale parameter gamma distribution.mean variance \\(x\\) :\n\\[\\text{E}(x) = \\mu_x\\]\n\\[\\text{Var}(x) = \\frac{2 \\mu^2_x}{m}\\]Useful distribution sample variance:\n\\[y_1, \\ldots, y_n \\sim N(\\mu_y, \\sigma^2)\\]\nKnown mean \\(\\mu_y\\):\n\\[\\frac{1}{n}\\sum_{=1}^n(y_i -\\mu_y)^2 \\sim \\text{W}_1\\left(\\frac{\\sigma^2}{n}, n\\right)\\]\nUnknown mean \\(\\mu_y\\) (estimated \\(\\bar{y}\\)):\n\\[\\widehat{\\sigma^2}_{ML} = \\frac{1}{n}\\sum_{=1}^n(y_i -\\bar{y})^2 \\sim \\text{W}_1\\left(\\frac{\\sigma^2}{n}, n-1\\right)\\]\n\\[\\widehat{\\sigma^2}_{UB} = \\frac{1}{n-1}\\sum_{=1}^n(y_i -\\bar{y})^2 \\sim \\text{W}_1\\left(\\frac{\\sigma^2}{n-1}, n-1\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case-2","chapter":"1 Multivariate random variables","heading":"1.5.2.2 Multivariate case","text":"Wishart distribution:\\[\\boldsymbol z_1,\\boldsymbol z_2,\\ldots,\\boldsymbol z_m \\stackrel{\\text{iid}}\\sim N_d(0,\\boldsymbol \\Sigma_{\\boldsymbol z})\\]\n\\[\\underbrace{\\boldsymbol X}_{d\\times d}=\\sum^{m}_{=1}\\underbrace{\\boldsymbol z_i\\boldsymbol z_i^T}_{d\\times d}\\]\\[\\boldsymbol M= (\\mu_{ij}) = m \\boldsymbol \\Sigma_{\\boldsymbol z}\\]\\(\\boldsymbol X\\) (random matrix!) distributed :\n\\[\\boldsymbol X\\sim  \\text{W}_d\\left(\\frac{\\boldsymbol M}{m}, m\\right) \\]\nmean variances:\n\\[\\text{E}(\\boldsymbol X) = \\boldsymbol M\\]\n\\[\\text{Var}(x_{ij})= \\frac{ \\mu^2_{ij}+\\mu_{ii}\\mu_{jj} }{m} \\]Useful distribution sample covariance:\n\\[\\boldsymbol y_1, \\ldots, \\boldsymbol y_n \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\]\n\\[\\frac{1}{n}\\sum_{=1}^n (\\boldsymbol y_i -\\boldsymbol \\mu)(\\boldsymbol y_i -\\boldsymbol \\mu)^T \\sim \\text{W}_d\\left(\\boldsymbol \\Sigma/n, n\\right)\\]\n\\[\\widehat{\\boldsymbol \\Sigma}_{ML} = \\frac{1}{n}\\sum_{=1}^n (\\boldsymbol y_i -\\bar{\\boldsymbol y})(\\boldsymbol y_i -\\bar{\\boldsymbol y})^T \\sim \\text{W}_d\\left(\\boldsymbol \\Sigma/n, n-1\\right)\\]\n\\[\\widehat{\\boldsymbol \\Sigma}_{UB} = \\frac{1}{n-1}\\sum_{=1}^n (\\boldsymbol y_i -\\bar{\\boldsymbol y})(\\boldsymbol y_i -\\bar{\\boldsymbol y})^T \\sim \\text{W}_d\\left(\\boldsymbol \\Sigma/(n-1), n-1\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"inverse-wishart-distribution","chapter":"1 Multivariate random variables","heading":"1.5.3 Inverse Wishart distribution","text":"inverse Wishart distribution multivariate generalisation inverse gamma\ndistribution (also known inverse scaled chi-squared distribution).","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case-3","chapter":"1 Multivariate random variables","heading":"1.5.3.1 Univariate case","text":"Inverse gamma distribution (\\(\\alpha\\) shape\n\\(\\beta\\) scale parameter):\\[x \\sim \\text{W}^{-1}_1(k \\mu, k+2) = \\text{Inv-Gam}\\left( \\alpha = \\frac{k+2}{2}, \\beta=\\frac{k \\mu}{2} \\right) = k \\mu\\,\\text{Inv-$\\chi^2_{k+2}$}\\]\\(x\\) mean variance\\[\\text{E}(x) = \\mu\\]\n\\[\\text{Var}(x)= \\frac{2\\mu^2}{k-2}\\]Relationship gamma distribution:\n\\[\n\\frac{1}{x} \\sim W_1\\left(\\frac{1}{k \\mu}, k+2\\right) \n=  \\text{Gam}\\left(\\frac{k+2}{2}, \\frac{2}{k \\mu}\\right) = \\frac{1}{k \\mu} \\, \\chi^2_{k+2}\n\\]","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case-3","chapter":"1 Multivariate random variables","heading":"1.5.3.2 Multivariate case","text":"Inverse Wishart distribution:\\[\\underbrace{\\boldsymbol X}_{d\\times d} \\sim \\text{W}^{-1}_d\\left( k \\underbrace{ \\boldsymbol M}_{d\\times d} \\, , \\, k+d+1\\right)\\]\n\\[\\text{E}(\\boldsymbol X) =\\boldsymbol M\\]\n\\[\\text{Var}(x_{ij})= \\frac{2 }{k-2} \\frac{(k+2) \\mu_{ij}^2 + k \\, \\mu_{ii} \\mu_{jj} }{2 k + 2}\\]Relationship Wishart:\n\\[\\boldsymbol X^{-1} \\sim \\text{W}_d\\left( \\frac{ \\boldsymbol M^{-1}}{k}  \\, , k+d+1\\right)\\]inverse Wishart distribution useful prior posterior distribution\nvariance \\(k\\) sample size parameter \\(\\boldsymbol M\\) resp. \\(\\mu\\) \nmean distribution \\(\\boldsymbol \\Sigma\\) \\(\\sigma^2\\) (univariate case).","code":""},{"path":"multivariate-random-variables.html","id":"further-distributions","chapter":"1 Multivariate random variables","heading":"1.5.4 Further distributions","text":"https://en.wikipedia.org/wiki/List_of_probability_distributionsWikipedia good source information distributions.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"transformations-and-dimension-reduction","chapter":"2 Transformations and dimension reduction","heading":"2 Transformations and dimension reduction","text":"Motivation:\nfollowing study transformations random vectors distributions.\ntransformation important\nsince either transform simple distributions complex distributions allow simplify\ncomplex models. machine learning invertible mappings transformations\nprobability distributions known “normalising flows”.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"linear-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.1 Linear Transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"location-scale-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.1.1 Location-scale transformation","text":"Also known affine transformation.\\[\\boldsymbol y= \\underbrace{\\boldsymbol }_{\\text{location parameter}}+\\underbrace{\\boldsymbol B}_{\\text{scale parameter}} \\boldsymbol x\\space\\]\n\\[\\boldsymbol y: m \\times 1 \\text{ random vector}\\]\n\\[\\boldsymbol : m \\times 1 \\text{ vector, location parameter}\\]\n\\[\\boldsymbol B: m \\times d \\text{ matrix, scale parameter },  m \\geq 1\\]\n\\[\\boldsymbol x: d \\times 1 \\text{ random vector}\\]Mean variance original vector \\(\\boldsymbol x\\):\\[\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\]\n\\[\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\]Mean variance transformed random vector \\(\\boldsymbol y\\):\\[\\text{E}(\\boldsymbol y)=\\boldsymbol + \\boldsymbol B\\boldsymbol \\mu_{\\boldsymbol x}\\]\n\\[\\text{Var}(\\boldsymbol y)= \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T\\]Cross-covariance \\(\\boldsymbol \\Phi= \\Sigma_{\\boldsymbol x\\boldsymbol y} = \\text{Cov}(\\boldsymbol x, \\boldsymbol y)\\) \\(\\boldsymbol x\\) \\(\\boldsymbol y\\):\n\\[\n\\boldsymbol \\Phi= \\text{Cov}(\\boldsymbol x, \\boldsymbol B\\boldsymbol x) = \\boldsymbol \\Sigma_{\\boldsymbol x}  \\boldsymbol B^T \n\\]\nNote \\(\\boldsymbol \\Phi\\) matrix dimensions \\(d \\times m\\)\ndimension \\(\\boldsymbol x\\) \\(d\\) dimension \\(\\boldsymbol y\\) \\(m\\).Cross-correlation \\(\\boldsymbol \\Psi= \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = \\text{Cor}(\\boldsymbol x, \\boldsymbol y)\\) \\(\\boldsymbol x\\) \\(\\boldsymbol y\\):\n\\[\n\\boldsymbol \\Psi= \\boldsymbol V_{\\boldsymbol x}^{-1/2}  \\boldsymbol \\Phi\\boldsymbol V_{\\boldsymbol y}^{-1/2} \n\\]\n\\(\\boldsymbol V_{\\boldsymbol x} = \\text{Diag}(\\boldsymbol \\Sigma_{\\boldsymbol x})\\) \\(\\boldsymbol V_{\\boldsymbol y} = \\text{Diag}(\\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T)\\)\ndiagonal matrices containing variances components\n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\). dimensions matrix \\(\\boldsymbol \\Psi\\) also \\(d \\times m\\).Special cases/examples:Example 2.1  Univariate case (\\(d=1, m=1\\)): \\(y=+ b x\\)\\(\\text{E}(y)=+b\\mu\\)\\(\\text{Var}(y)=b^2\\sigma^2\\)\\(\\text{Cov}(y, x) = b\\sigma^2\\)\\(\\text{Cor}(y, x) = \\frac{b \\sigma^2}{\\sqrt{b^2\\sigma^2} \\sqrt{\\sigma^2} } =1\\)Example 2.2  Sum two random univariate variables:\n\\(y = x_1 + x_2\\), .e. \\(=0\\) \\(\\boldsymbol B=(1,1)\\)\\(\\text{E}(y) = \\text{E}(x_1+x_2)=\\mu_1+\\mu_2\\)\\(\\text{Var}(y) = \\text{Var}(x_1+x_2) = (1,1)\\begin{pmatrix} \\sigma^2_1 & \\sigma_{12}\\\\ \\sigma_{12} & \\sigma^2_2 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix} = \\sigma^2_1+\\sigma^2_2+2\\sigma_{12} = \\text{Var}(x_1)+\\text{Var}(x_2)+2\\,\\text{Cov}(x_1,x_2)\\)Example 2.3  \\(y_1=a_1+b_1 x_1\\) \\(y_2=a_2+b_2 x_2\\),\n.e. \\(\\boldsymbol = \\begin{pmatrix} a_1\\\\ a_2 \\end{pmatrix}\\) \\(\\boldsymbol B= \\begin{pmatrix}b_1 & 0\\\\ 0 & b_2\\end{pmatrix}\\)\\(\\text{E}(\\boldsymbol y)= \\begin{pmatrix} a_1\\\\ a_2 \\end{pmatrix} + \\begin{pmatrix}b_1 & 0\\\\ 0 & b_2\\end{pmatrix}  \\begin{pmatrix} \\mu_1 \\\\ \\mu_2\\end{pmatrix}  = \\begin{pmatrix} a_1+b_1 \\mu_1\\\\ a_2+b_2 \\mu_2 \\end{pmatrix}\\)\\(\\text{Var}(\\boldsymbol y) = \\begin{pmatrix} b_1 & 0\\\\ 0 & b_2 \\end{pmatrix}  \\begin{pmatrix} \\sigma^2_1 & \\sigma_{12}\\\\ \\sigma_{12} & \\sigma^2_2 \\end{pmatrix} \\begin{pmatrix} b_1 & 0\\\\ 0 & b_2 \\end{pmatrix} = \\begin{pmatrix} b^2_1\\sigma^2_1 & b_1b_2\\sigma_{12}\\\\ b_1b_2\\sigma_{12} & b^2_2\\sigma^2_2 \\end{pmatrix}\\)\nnote \\(\\text{Cov}(y_1, y_2) = b_1 b_2\\text{Cov}(x_1,x_2)\\)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"squared-multiple-correlation","chapter":"2 Transformations and dimension reduction","heading":"2.1.2 Squared multiple correlation","text":"Definition squared multiple correlationSquared multiple correlation \\(\\text{MCor}(y, \\boldsymbol x)^2\\) scalar measure summarising linear association scalar response variable \\(y\\) set predictors \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\). defined \n\\[\n\\begin{split}\n\\text{MCor}(y, \\boldsymbol x)^2 &= \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} / \\sigma^2_y\\\\\n &=\\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{ \\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\\\\\n\\end{split}\n\\]\n\\(y\\) can perfectly linearly predicted \\(\\boldsymbol x\\) \\(\\text{MCor}(y, \\boldsymbol x)^2 = 1\\).empirical estimate \\(\\text{MCor}(y, \\boldsymbol x)^2\\) \\(R^2\\) coefficient find software linear regression.See corresponding section MATH20802 Statistical Methods.Squared multiple correlation affine transformationSince linearly transform \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) additional error involved expect\ncomponent \\(y_i\\) \\(\\boldsymbol y\\) \\(\\text{MCor}(y_i, \\boldsymbol x)^2=1\\).\ncan shown directly computing\n\\[\n\\begin{split}\n\\left(\\text{MCor}(y_1, \\boldsymbol x)^2, \\ldots, \\text{MCor}(y_m, \\boldsymbol x)^2 \\right)^T\n&=\\text{Diag}\\left(\\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}  \\right) / \\text{Diag}\\left( \\boldsymbol \\Sigma_{\\boldsymbol y} \\right) \\\\\n&= \\text{Diag}\\left(\\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) / \\text{Diag}\\left( \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) \\\\\n&= \\text{Diag}\\left(\\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) / \\text{Diag}\\left( \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) \\\\\n&=\\left(1, \\ldots, 1 \\right)^T\\\\\n\\end{split}\n\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"invertible-location-scale-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.1.3 Invertible location-scale transformation","text":"\\(m=d\\) (square \\(\\boldsymbol B\\)) \\(\\det(\\boldsymbol B) \\neq 0\\) affine transformation invertible.Forward transformation:\n\\[\\boldsymbol y= \\boldsymbol + \\boldsymbol B\\boldsymbol x\\]Back transformation:\n\\[\\boldsymbol x= \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\]Invertible transformations thus provide one--one map \\(\\boldsymbol x\\) \\(\\boldsymbol y\\).Example 2.4  Mahalanobis transformWe assume \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\) positive definite\ncovariance matrix \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\) \\(\\det(\\boldsymbol \\Sigma_{\\boldsymbol x}) > 0\\).Mahalanobis transformation given \n\\[\n\\boldsymbol y=\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x}(\\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x})\n\\]\ncorresponds affine transformation \n\\(\\boldsymbol = - \\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x} \\boldsymbol \\mu_{\\boldsymbol x}\\) \\(\\boldsymbol B= \\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x}\\).inverse principal matrix square root \\(\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x}\\) can computed\neigendecomposition (see Appendix).mean variance \\(\\boldsymbol y\\) becomes\n\\[\n\\text{E}(\\boldsymbol y) = \\boldsymbol 0\\]\n\n\\[\\text{Var}(\\boldsymbol y) = \\boldsymbol I_d\\].Mahalanobis transforms performs three functions:Centering (\\(-\\boldsymbol \\mu\\))Standardisation \\(\\text{Var}(y_i)=1\\)Decorrelation \\(\\text{Cor}(y_i,y_j)=0\\) \\(\\neq j\\)univariate case (\\(d=1\\)) coefficients reduce \n\\(= - \\frac{\\mu_x}{\\sigma_x}\\) \\(B = \\frac{1}{\\sigma_x}\\) Mahalanobis transform\nbecomes\n\\[y = \\frac{x-\\mu_x}{\\sigma_x}\\]\n.e. applies centering + standardisation.Mahalanobis transformation appears implicitly many places multivariate statistics,\ne.g. multivariate normal density. particular example whitening transformation (\ninfinitely many, see later course).Example 2.5  Inverse Mahalanobis transformationThe inverse Mahalanobis transform given \n\\[\n\\boldsymbol y= \\boldsymbol \\mu_{\\boldsymbol y}+\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y} \\boldsymbol x\n\\]\nMahalanobis transform whitening transform inverse Mahalonobis\ntransform sometimes called Mahalanobis colouring transformation.\ncoefficients affine transformation \n\\(\\boldsymbol =\\boldsymbol \\mu_{\\boldsymbol y}\\) \\(\\boldsymbol B=\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y}\\).Starting \\(\\text{E}(\\boldsymbol x)=\\boldsymbol 0\\) \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol I_d\\) \nmean variance transformed variable \n\\[\\text{E}(\\boldsymbol y) = \\boldsymbol \\mu_{\\boldsymbol y}\n\\]\n\n\\[\\text{Var}(\\boldsymbol y) = \\boldsymbol \\Sigma_{\\boldsymbol y}\n\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"transformation-of-a-density-under-an-invertible-location-scale-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.1.4 Transformation of a density under an invertible location-scale transformation:","text":"Assume \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) density \\(f_{\\boldsymbol x}(\\boldsymbol x)\\).linear transformation \\(\\boldsymbol y= \\boldsymbol + \\boldsymbol B\\boldsymbol x\\) get \\(\\boldsymbol y\\sim F_{\\boldsymbol y}\\) density\n\\[f_{\\boldsymbol y}(\\boldsymbol y)=|\\det(\\boldsymbol B)|^{-1} f_{\\boldsymbol x} \\left( \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\right)\\]Example 2.6  Transformation standard normal inverse Mahalanobis transformAssume \\(\\boldsymbol x\\) multivariate standard normal \\(\\boldsymbol x\\sim N_d(\\boldsymbol 0,\\boldsymbol I_d)\\) density\n\\[f_{\\boldsymbol x}(\\boldsymbol x) = (2\\pi)^{-d/2}\\exp\\left( -\\frac{1}{2} \\boldsymbol x^T \\boldsymbol x\\right)\\]\ndensity applying inverse Mahalanobis transform\\(\\boldsymbol y= \\boldsymbol \\mu_{\\boldsymbol y}+\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y} \\boldsymbol x\\) \n\\[\n\\begin{split}\nf_{\\boldsymbol y}(\\boldsymbol y) &= |\\det(\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y})|^{-1} (2\\pi)^{-d/2} \\exp\\left(-\\frac{1}{2}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})^T\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol y} \\,\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol y}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})\\right)\\\\\n& = (2\\pi)^{-d/2} \\det(\\boldsymbol \\Sigma_{\\boldsymbol y})^{-1/2} \\exp\\left(-\\frac{1}{2}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})^T\\boldsymbol \\Sigma^{-1}_{\\boldsymbol y}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})\\right) \\\\\n\\end{split}\n\\]\n\\(\\Longrightarrow\\) \\(\\boldsymbol y\\) multivariate normal density \\(N_d(\\boldsymbol \\mu_{\\boldsymbol y}, \\boldsymbol \\Sigma_{\\boldsymbol y})\\)Application: e.g. random number generation: draw \\(N_d(\\boldsymbol 0,\\boldsymbol I_d)\\) (easy!) convert multivariate normal tranformation\n(see Worksheet 4).","code":""},{"path":"transformations-and-dimension-reduction.html","id":"nonlinear-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.2 Nonlinear transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"general-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.2.1 General transformation","text":"\\[\\boldsymbol y= \\boldsymbol h(\\boldsymbol x)\\]\n\\(\\boldsymbol h\\) arbitrary vector-valued functionlinear case: \\(\\boldsymbol h(\\boldsymbol x) = \\boldsymbol +\\boldsymbol B\\boldsymbol x\\)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"delta-method","chapter":"2 Transformations and dimension reduction","heading":"2.2.2 Delta method","text":"Assume know mean \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\) variance \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\) \\(\\boldsymbol x\\).\npossible say something mean variance transformed\nrandom variable \\(\\boldsymbol y\\)?\n\\[\n\\text{E}(\\boldsymbol y)= \\text{E}(\\boldsymbol h(\\boldsymbol x))= ?\n\\]\n\\[\n\\text{Var}(\\boldsymbol y) = \\text{Var}(\\boldsymbol h(\\boldsymbol x))= ? \\\\\n\\]general, transformation \\(\\boldsymbol h(\\boldsymbol x)\\) exact mean variance transformed variable obtained analytically.However, can find linear approximation compute mean variance.\napproximation called “Delta Method”, “law propagation errors”, credited Gauss.4Linearisation \\(\\boldsymbol h(\\boldsymbol x)\\) achieved Taylor series approximation first order\n\\(\\boldsymbol h(\\boldsymbol x)\\) around \\(\\boldsymbol x_0\\):\n\\[\\boldsymbol h(\\boldsymbol x) \\approx \\boldsymbol h(\\boldsymbol x_0) + \\underbrace{J_{\\boldsymbol h}(\\boldsymbol x_0)}_{\\text{Jacobian matrix}}(\\boldsymbol x-\\boldsymbol x_0)  = \n\\underbrace{\\boldsymbol h(\\boldsymbol x_0) - J_{\\boldsymbol h}(\\boldsymbol x_0)\\, \\boldsymbol x_0}_{\\boldsymbol } + \\underbrace{J_{\\boldsymbol h}(\\boldsymbol x_0)}_{\\boldsymbol B} \\boldsymbol x\\]\\(h(\\boldsymbol x)\\) scalar-valued gradient \\(\\nabla h(\\boldsymbol x)\\) given vector partial correlations\n\\[\n\\nabla h(\\boldsymbol x) =\n\\begin{pmatrix}\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_1}  \\\\\n\\vdots\\\\\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_d} \\\\\n\\end{pmatrix}\n\\]\n\\(\\nabla\\) nabla operator.Jacobian matrix generalisation gradient \\(\\boldsymbol h(\\boldsymbol x)\\) vector-valued:\\[J_{\\boldsymbol h}(\\boldsymbol x) = \\begin{pmatrix}\\nabla h_1(\\boldsymbol x)^T\\\\ \\nabla h_2(\\boldsymbol x)^T \\\\ \\vdots \\\\ \\nabla h_m(\\boldsymbol x)^T \\end{pmatrix} = \\begin{pmatrix}\n    \\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_1} & \\dots & \\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_d}\\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_1} & \\dots & \\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_d}\n    \\end{pmatrix}\\]\nNote Jacobian matrix convention gradient individual component \\(\\boldsymbol h(\\boldsymbol x)\\) contained row matrix number rows corresponds dimension \\(\\boldsymbol h(\\boldsymbol x)\\) number columns dimension \\(\\boldsymbol x\\).First order approximation \\(\\boldsymbol h(\\boldsymbol x)\\) around \\(\\boldsymbol x_0=\\boldsymbol \\mu_{\\boldsymbol x}\\) yields\n\\(\\boldsymbol = \\boldsymbol h(\\boldsymbol \\mu_{\\boldsymbol x}) - J_{\\boldsymbol h}(\\boldsymbol \\mu_{\\boldsymbol x})\\, \\boldsymbol \\mu_{\\boldsymbol x}\\) \n\\(\\boldsymbol B= J_{\\boldsymbol h}(\\boldsymbol \\mu_{\\boldsymbol x})\\) leads directly multivariate Delta method:\\[\\text{E}(\\boldsymbol y)\\approx\\boldsymbol h(\\boldsymbol \\mu_{\\boldsymbol x})\\]\n\\[\\text{Var}(\\boldsymbol y)\\approx J_{\\boldsymbol h}(\\boldsymbol \\mu_{\\boldsymbol x}) \\, \\boldsymbol \\Sigma_{\\boldsymbol x} \\, J_{\\boldsymbol h}(\\boldsymbol \\mu_{\\boldsymbol x})^T\\]univariate Delta method special case:\n\\[\\text{E}(y) \\approx h(\\mu_x)\\]\n\\[\\text{Var}(y)\\approx \\sigma^2_x \\, h'(\\mu_x)^2\\]Note Delta approximation breaks \\(\\text{Var}(\\boldsymbol y)\\) singular,\nexample first derivative (gradient Jacobian matrix) \\(\\boldsymbol \\mu_{\\boldsymbol x}\\) zero.Example 2.7  Variance odds ratioThe proportion \\(\\hat{p} = \\frac{n_1}{n}\\) resulting \n\\(n\\) repeats Bernoulli experiment expectation \\(\\text{E}(\\hat{p})=p\\)\nvariance \\(\\text{Var}(\\hat{p}) = \\frac{p (1-p)}{n}\\).\n(approximate) mean variance corresponding odds ratio \\(\\widehat{}=\\frac{\\hat{p}}{1-\\hat{p}}\\)?\\(h(x) = \\frac{x}{1-x}\\),\n\\(\\widehat{} = h(\\hat{p})\\) \\(h'(x) = \\frac{1}{(1-x)^2}\\) get using \nDelta method\n\\(\\text{E}( \\widehat{} ) \\approx h(p) = \\frac{p}{1-p}\\) \n\\(\\text{Var}( \\widehat{} )\\approx h'(p)^2 \\text{Var}( \\hat{p} ) = \\frac{p}{n (1-p)^3}\\).Example 2.8  Log-transform variance stabilisationAssume \\(x\\) mean \\(\\text{E}(x)=\\mu\\) variance \\(\\text{Var}(x) = \\sigma^2 \\mu^2\\),\n.e. standard deviation \\(\\text{SD}(x)\\) proportional mean \\(\\mu\\).\n(approximate) mean variance log-transformed variable \\(\\log(x)\\)?\\(h(x) = \\log(x)\\) \\(h'(x) = \\frac{1}{x}\\) get using \nDelta method\n\\(\\text{E}( \\log(x) ) \\approx h(\\mu) = \\log(\\mu)\\) \n\\(\\text{Var}( \\log(x) )\\approx h'(\\mu)^2 \\text{Var}( x ) = \\left(\\frac{1}{\\mu} \\right)^2 \\sigma^2 \\mu^2 = \\sigma^2\\). Thus, applying log-transform variance depend mean!","code":""},{"path":"transformations-and-dimension-reduction.html","id":"transformation-of-a-probability-density-function-under-a-general-invertible-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.2.3 Transformation of a probability density function under a general invertible transformation","text":"Assume \\(\\boldsymbol h(\\boldsymbol x) = \\boldsymbol y(\\boldsymbol x)\\) invertible: \\(\\boldsymbol h^{-1}(\\boldsymbol y)=\\boldsymbol x(\\boldsymbol y)\\)\\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) probability density function \\(f_{\\boldsymbol x}(\\boldsymbol x)\\)density \\(f_{\\boldsymbol y}(\\boldsymbol y)\\) transformed random vector \\(\\boldsymbol y\\) given \\[f_{\\boldsymbol y}(\\boldsymbol y) = |\\det\\left( J_{\\boldsymbol x}(\\boldsymbol y) \\right)| \\,\\,\\,  f_{\\boldsymbol x}\\left( \\boldsymbol x(\\boldsymbol y) \\right)\\]\\(J_{\\boldsymbol x}(\\boldsymbol y)\\) Jacobian matrix inverse transformation.Special cases:Univariate version: \\(f_y(y) = \\left|\\frac{dx(y)}{dy} \\right| \\, f_x\\left(x(y)\\right)\\)Linear transformation \\(\\boldsymbol h(\\boldsymbol x) = \\boldsymbol + \\boldsymbol B\\boldsymbol x\\), \\(\\boldsymbol x(\\boldsymbol y) = \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\)\n\\(J_{\\boldsymbol x}(\\boldsymbol y) = \\boldsymbol B^{-1}\\):\n\\[f_{\\boldsymbol y}(\\boldsymbol y)=\\left|\\det(\\boldsymbol B)\\right|^{-1} f_{\\boldsymbol x} \\left( \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\right)\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"normalising-flows","chapter":"2 Transformations and dimension reduction","heading":"2.2.4 Normalising flows","text":"module focus mostly linear transformations underpin\nmuch classical multivariate statistics, important keep mind later study\nimportance nonlinear transformationsIn machine learning (sequences ) invertible nonlinear transformations known “normalising flows”. used generative way (building complex models \nsimple models) simplification dimension reduction.interested normalising flows good start learn review papers\nKobyzev et al (2021 )5 Papamakarios et al. (2021).6","code":""},{"path":"transformations-and-dimension-reduction.html","id":"general-whitening-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.3 General whitening transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"overview","chapter":"2 Transformations and dimension reduction","heading":"2.3.1 Overview","text":"Whitening transformations special widely used class invertible location-scale\ntransformations.Terminology: whitening refers fact transformation covariance matrix spherical, isotropic, white (\\(\\boldsymbol I_d\\))Whitening useful preprocessing, allow turn multivariate models uncorrelated univariate models (via decorrelation property). whitening transformations reduce dimension optimal way (via compression property)..Mahalanobis transform specific example whitening transformation.\nalso know “zero-phase component analysis” short ZCA transform.-called latent variable models whitening procedures link observed (correlated) variables latent variables (typically uncorrelated standardised):\\[\\begin{align*}\n\\begin{array}{cl}\n\\text{Whitening} \\\\\n\\downarrow\n\\end{array}\n\\begin{array}{ll}\n\\boldsymbol x\\\\\n\\uparrow \\\\\n\\boldsymbol z\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{Observed variable (can measured)} \\\\\n\\text{external, typically correlated} \\\\\n\\space \\\\\n\\text{Unobserved \"latent\" variable (directly measured)} \\\\\n\\text{internal, typically chosen uncorrelated} \\\\\n\\end{array}\n\\end{align*}\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"whitening-transformation-and-whitening-constraint","chapter":"2 Transformations and dimension reduction","heading":"2.3.2 Whitening transformation and whitening constraint","text":"Starting point:Random vector \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) necessarily multivariate normal.\\(\\boldsymbol x\\) mean \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu\\) positive definite (invertible) covariance matrix \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\).Note following leave subscript \\(\\boldsymbol x\\) covariance \\(\\boldsymbol x\\)\nunless needed clarification.covariance can split positive variances \\(\\boldsymbol V\\) \npositive definite invertible correlation matrix \\(\\boldsymbol P\\) \\(\\boldsymbol \\Sigma= \\boldsymbol V^{1/2} \\boldsymbol P\\boldsymbol V^{1/2}\\).Whitening transformation:\\[\\underbrace{\\boldsymbol z}_{d \\times 1 \\text{ vector }} = \\underbrace{\\boldsymbol W}_{d \\times d \\text{ whitening matrix }} \\underbrace{\\boldsymbol x}_{d \\times 1 \\text{ vector }}\\]\nObjective: choose \\(\\boldsymbol W\\) \\(\\text{Var}(\\boldsymbol z)=\\boldsymbol I_d\\)Mahalanobis/ZCA whitening already know \\(\\boldsymbol W^{\\text{ZCA}}=\\boldsymbol \\Sigma^{-1/2}\\).general, whitening matrix \\(\\boldsymbol W\\) needs satisfy constraint:\n\\[\n\\begin{array}{lll}\n                & \\text{Var}(\\boldsymbol z) & = \\boldsymbol I_d \\\\\n\\Longrightarrow & \\text{Var}(\\boldsymbol W\\boldsymbol x) &= \\boldsymbol W\\boldsymbol \\Sigma\\boldsymbol W^T = \\boldsymbol I_d \\\\\n\\Longrightarrow &  \\boldsymbol W\\, \\boldsymbol \\Sigma\\, \\boldsymbol W^T \\boldsymbol W= \\boldsymbol W& \\\\\n\\end{array}\n\\]\n\\[\\Longrightarrow \\text{constraint whitening matrix: } \\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1}\\]Clearly, ZCA whitening matrix satisfies constraint: \\((\\boldsymbol W^{ZCA})^T \\boldsymbol W^{ZCA} = \\boldsymbol \\Sigma^{-1/2}\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1}\\)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"parameterisation-of-whitening-matrix","chapter":"2 Transformations and dimension reduction","heading":"2.3.3 Parameterisation of whitening matrix","text":"Covariance-based parameterisation whitening matrix:general way specify valid whitening matrix \n\\[\n\\boldsymbol W= \\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2}\n\\]\n\\(\\boldsymbol Q_1\\) orthogonal matrix.7Recall orthogonal matrix \\(\\boldsymbol Q\\) property \\(\\boldsymbol Q^{-1} = \\boldsymbol Q^T\\) \nconsequence \\(\\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol \\).result, \\(\\boldsymbol W\\) satisfies whitening constraint:\\[\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1/2}\\underbrace{\\boldsymbol Q_1^T \\boldsymbol Q_1}_{\\boldsymbol }\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1}\\]Note converse also true: whitening whitening matrix, .e. \\(\\boldsymbol W\\) satisfying whitening constraint, can written form \n\\(\\boldsymbol Q_1 = \\boldsymbol W\\boldsymbol \\Sigma^{1/2}\\) orthogonal construction.\\(\\Longrightarrow\\) instead choosing \\(\\boldsymbol W\\), choose orthogonal matrix \\(\\boldsymbol Q_1\\)!recall orthogonal matrices geometrically represent rotations (plus reflections).now clear infinitely many whitening procedures, infinitely many rotations! also means need find ways choose/select among whitening procedures.Mahalanobis/ZCA transformation \\(\\boldsymbol Q_1^{\\text{ZCA}}=\\boldsymbol \\)whitening can interpreted Mahalanobis transformation followed rotation-reflectionCorrelation-based parameterisation whitening matrix:Instead working covariance matrix \\(\\boldsymbol \\Sigma\\), can express \\(\\boldsymbol W\\) also terms corresponding correlation matrix \\(\\boldsymbol P= (\\rho_{ij}) = \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2}\\)\n\\(\\boldsymbol V\\) diagonal matrix containing variances.Specifically, can specify whitening matrix \n\\[\\boldsymbol W= \\boldsymbol Q_2 \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2}\\]easy verify \\(\\boldsymbol W\\) also satisfies whitening constraint:\n\\[\n\\begin{split}\n\\boldsymbol W^T \\boldsymbol W& = \\boldsymbol V^{-1/2}\\boldsymbol P^{-1/2}\\underbrace{\\boldsymbol Q_2^T \\boldsymbol Q_2}_{\\boldsymbol }\\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} \\\\\n& = \\boldsymbol V^{-1/2} \\boldsymbol P^{-1} \\boldsymbol V^{-1/2} = \\boldsymbol \\Sigma^{-1} \\\\\n\\end{split}\n\\]\nConversely, whitening matrix \\(\\boldsymbol W\\) can also written form \n\\(\\boldsymbol Q_2 = \\boldsymbol W\\boldsymbol V^{1/2} \\boldsymbol P^{1/2}\\) orthogonal construction.Another interpretation whitening: first standardising (\\(\\boldsymbol V^{-1/2}\\)), decorrelation (\\(\\boldsymbol P^{-1/2}\\)), followed rotation-reflection (\\(\\boldsymbol Q_2\\))Mahalanobis/ZCA transformation \\(\\boldsymbol Q_2^{\\text{ZCA}} = \\boldsymbol \\Sigma^{-1/2} \\boldsymbol V^{1/2} \\boldsymbol P^{1/2}\\)forms write \\(\\boldsymbol W\\) using \\(\\boldsymbol Q_1\\) \\(\\boldsymbol Q_2\\) equally valid (interchangeable).Note \\(\\boldsymbol W\\)\n\\[\\boldsymbol Q_1\\neq\\boldsymbol Q_2 \\text{  Two different orthogonal matrices!}\\]\nalso\n\\[\\underbrace{\\boldsymbol \\Sigma^{-1/2}}_{\\text{Symmetric}}\\neq\\underbrace{\\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}}_{\\text{Symmetric}}\\]\neven though\\[\\boldsymbol \\Sigma^{-1/2}\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1} = \\boldsymbol V^{-1/2}\\boldsymbol P^{-1/2}\\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"cross-covariance-and-cross-correlation-for-general-whitening-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.3.4 Cross-covariance and cross-correlation for general whitening transformations","text":"useful criterion characterise distinguish among whitening transformations \ncross-covariance cross-correlation matrix \noriginal variable \\(\\boldsymbol x\\) whitened variable \\(\\boldsymbol z\\):Cross-covariance \\(\\boldsymbol \\Phi= \\Sigma_{\\boldsymbol x\\boldsymbol z}\\) \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Phi= \\text{Cov}(\\boldsymbol x, \\boldsymbol z) & = \\text{Cov}( \\boldsymbol x,\\boldsymbol W\\boldsymbol x)\\\\\n& = \\boldsymbol \\Sigma\\boldsymbol W^T \\\\\n&= \\boldsymbol \\Sigma\\, \\boldsymbol \\Sigma^{-1/2} \\boldsymbol Q_1^T \\\\\n&= \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T\\\\\n\\end{split}\n\\]\ncomponent notation write \\(\\boldsymbol \\Phi= (\\phi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol x\\) column index \\(j\\) \\(\\boldsymbol z\\).\nCross-covariance linked \\(\\boldsymbol Q_1\\)!\nThus, choosing cross-covariance determines \\(\\boldsymbol Q_1\\) (vice versa).\nNote cross-covariance matrix \\(\\boldsymbol \\Phi\\) satisfies condition\n\\(\\boldsymbol \\Phi\\boldsymbol \\Phi^T = \\boldsymbol \\Sigma\\).\nwhitening matrix expressed terms cross-covariance \\(\\boldsymbol W= \\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1}\\), required \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Phi\\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1} =\\boldsymbol \\Sigma^{-1}\\).\nFurthermore, \\(\\boldsymbol \\Phi\\) \ninverse whitening matrix,\n\\(\\boldsymbol W^{-1} = \\left( \\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2} \\right)^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{T} = \\boldsymbol \\Phi\\).Cross-covariance \\(\\boldsymbol \\Phi= \\Sigma_{\\boldsymbol x\\boldsymbol z}\\) \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Phi= \\text{Cov}(\\boldsymbol x, \\boldsymbol z) & = \\text{Cov}( \\boldsymbol x,\\boldsymbol W\\boldsymbol x)\\\\\n& = \\boldsymbol \\Sigma\\boldsymbol W^T \\\\\n&= \\boldsymbol \\Sigma\\, \\boldsymbol \\Sigma^{-1/2} \\boldsymbol Q_1^T \\\\\n&= \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T\\\\\n\\end{split}\n\\]\ncomponent notation write \\(\\boldsymbol \\Phi= (\\phi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol x\\) column index \\(j\\) \\(\\boldsymbol z\\).Cross-covariance linked \\(\\boldsymbol Q_1\\)!\nThus, choosing cross-covariance determines \\(\\boldsymbol Q_1\\) (vice versa).Note cross-covariance matrix \\(\\boldsymbol \\Phi\\) satisfies condition\n\\(\\boldsymbol \\Phi\\boldsymbol \\Phi^T = \\boldsymbol \\Sigma\\).whitening matrix expressed terms cross-covariance \\(\\boldsymbol W= \\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1}\\), required \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Phi\\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1} =\\boldsymbol \\Sigma^{-1}\\).\nFurthermore, \\(\\boldsymbol \\Phi\\) \ninverse whitening matrix,\n\\(\\boldsymbol W^{-1} = \\left( \\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2} \\right)^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{T} = \\boldsymbol \\Phi\\).Cross-correlation \\(\\boldsymbol \\Psi= \\boldsymbol P_{\\boldsymbol x\\boldsymbol z}\\) \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Psi= \\text{Cor}(\\boldsymbol x, \\boldsymbol z) & = \\boldsymbol V^{-1/2} \\boldsymbol \\Phi\\\\\n& =  \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol W^T \\\\\n&=\\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2} \\boldsymbol P^{-1/2} \\boldsymbol Q_2^T\\\\\n& =  \\boldsymbol P^{1/2}  \\boldsymbol Q_2^T\\\\\n\\end{split}\n\\]\ncomponent notation write \\(\\boldsymbol \\Psi= (\\psi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol x\\) column index \\(j\\) \\(\\boldsymbol z\\).\nCross-correlation linked \\(\\boldsymbol Q_2\\)!\nHence, choosing cross-correlation determines \\(\\boldsymbol Q_2\\) (vice versa). whitening\nmatrix expressed terms cross-correlation \n\\(\\boldsymbol W= \\boldsymbol \\Psi^T \\boldsymbol P^{-1} \\boldsymbol V^{-1/2}\\).Cross-correlation \\(\\boldsymbol \\Psi= \\boldsymbol P_{\\boldsymbol x\\boldsymbol z}\\) \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Psi= \\text{Cor}(\\boldsymbol x, \\boldsymbol z) & = \\boldsymbol V^{-1/2} \\boldsymbol \\Phi\\\\\n& =  \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol W^T \\\\\n&=\\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2} \\boldsymbol P^{-1/2} \\boldsymbol Q_2^T\\\\\n& =  \\boldsymbol P^{1/2}  \\boldsymbol Q_2^T\\\\\n\\end{split}\n\\]component notation write \\(\\boldsymbol \\Psi= (\\psi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol x\\) column index \\(j\\) \\(\\boldsymbol z\\).Cross-correlation linked \\(\\boldsymbol Q_2\\)!\nHence, choosing cross-correlation determines \\(\\boldsymbol Q_2\\) (vice versa). whitening\nmatrix expressed terms cross-correlation \n\\(\\boldsymbol W= \\boldsymbol \\Psi^T \\boldsymbol P^{-1} \\boldsymbol V^{-1/2}\\).Note factorisation cross-covariance \\(\\boldsymbol \\Phi=\\boldsymbol \\Sigma^{1/2}\\boldsymbol Q_1^T\\) \ncross-correlation \\(\\boldsymbol \\Psi=\\boldsymbol P^{1/2}\\boldsymbol Q_2^T\\) product \npositive definite symmetric matrix orthogonal matrix examples polar decomposition.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"inverse-whitening-transformation-and-loadings","chapter":"2 Transformations and dimension reduction","heading":"2.3.5 Inverse whitening transformation and loadings","text":"Inverse transformation:Recall \\(\\boldsymbol z= \\boldsymbol W\\boldsymbol x\\). Therefore, reverse transformation going whitened\noriginal variable \\(\\boldsymbol x= \\boldsymbol W^{-1} \\boldsymbol z\\).\ncan expressed also terms cross-covariance cross-correlation.\n\\(\\boldsymbol W^{-1} = \\boldsymbol \\Phi\\) get\n\\[\n\\boldsymbol x= \\boldsymbol \\Phi\\boldsymbol z\\, .\n\\]\nFurthermore, since \\(\\boldsymbol \\Psi= \\boldsymbol V^{-1/2} \\boldsymbol \\Phi\\) \n\\(\\boldsymbol W^{-1} = \\boldsymbol V^{1/2} \\boldsymbol \\Psi\\) hence\n\\[\n\\boldsymbol V^{-1/2} \\boldsymbol x=   \\boldsymbol \\Psi\\boldsymbol z\\, .\n\\]reverse whitening transformation also known colouring transformation\n(previously discussed inverse Mahalanobis transform one example).Definition loadings:Loadings coefficients linear transformation latent variable back observed variable. variables standardised unit variance loadings also called correlation loadings.Hence, cross-covariance matrix \\(\\boldsymbol \\Phi\\) plays role loadings linking latent variable \\(\\boldsymbol z\\)\noriginal \\(\\boldsymbol x\\). Similarly, cross-correlation matrix \\(\\boldsymbol \\Psi\\) contains correlation loadings\nlinking (already standardised) latent variable \\(\\boldsymbol z\\) standardised \\(\\boldsymbol x\\).convention use rows correspond original variables\ncolumns whitened latent variables.Multiple correlation coefficients \\(\\boldsymbol z\\) back \\(\\boldsymbol x\\):consider backtransformation whitened variable \\(\\boldsymbol z\\) original variables \\(\\boldsymbol x\\) note components \\(\\boldsymbol z\\) uncorrelated witth \\(\\boldsymbol P_{\\boldsymbol z} = \\boldsymbol \\).\nsquared multiple correlation coefficient \\(\\text{MCor}(x_i, \\boldsymbol z)\\) \\(x_i\\) \\(\\boldsymbol z\\)\ntherefore just sum corresponding squared correlations\n\\(\\text{Cor}(x_i, z_j)^2\\):\n\\[\n\\begin{split}\n\\text{MCor}(x_i, \\boldsymbol z)^2 &=  \\boldsymbol P_{x_i \\boldsymbol z} \\boldsymbol P_{\\boldsymbol z}^{-1} \\boldsymbol P_{\\boldsymbol zx_i} = \\\\\n             & \\sum_{j=1}^d  \\text{Cor}(x_i, z_j)^2  \\\\\n &  = \\sum_{j=1}^d \\psi_{ij}^2 = 1\n\\end{split}\n\\]\nshown earlier general linear one--one- transformation (includes whitening special case) squared multiple correlation must 1 \nerror. can confirm computing row sums squares cross-correlation matrix \\(\\boldsymbol \\Psi\\) matrix notation\n\\[\n\\begin{split}\n \\text{Diag}\\left(\\boldsymbol \\Psi\\boldsymbol \\Psi^T\\right) &= \\text{Diag}\\left(\\boldsymbol P^{1/2} \\boldsymbol Q_2^T \\boldsymbol Q_2\\boldsymbol P^{1/2}\\right) \\\\\n&= \\text{Diag}(\\boldsymbol P) \\\\\n&= (1, \\ldots, 1)^T\\\\\n\\end{split}\n\\]\nclear choice \\(\\boldsymbol Q_2\\) relevant.Similarly, row sums squares cross-covariance matrix \\(\\boldsymbol \\Phi\\)\nequal variances original variables, regardless \\(\\boldsymbol Q_1\\):\n\\[\n\\sum_{j=1}^d \\phi_{ij}^2 = \\text{Var}(x_i)\n\\]\nmatrix notation\n\\[\n\\begin{split}\n \\text{Diag}\\left(\\boldsymbol \\Phi\\boldsymbol \\Phi^T\\right) &= \\text{Diag}\\left(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T \\boldsymbol Q_1 \\boldsymbol \\Sigma^{1/2}\\right) \\\\\n&= \\text{Diag}(\\boldsymbol \\Sigma) \\\\\n&= (\\text{Var}(x_1), \\ldots, \\text{Var}(x_d)^T\\\\\n\\end{split}\n\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"summaries-of-cross-covariance-boldsymbol-phi-and-cross-correlation-boldsymbol-psi-resulting-from-whitening-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.3.6 Summaries of cross-covariance \\(\\boldsymbol \\Phi\\) and cross-correlation \\(\\boldsymbol \\Psi\\) resulting from whitening transformations","text":"Matrix trace:simply summary matrix trace. cross-covariance matrix \\(\\boldsymbol \\Phi\\) trace \nsum covariances corresponding elements \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\text{Tr}(\\boldsymbol \\Phi) =  \\sum_{=1}^d \\text{Cov}(x_i, z_i) =  \\sum_{=1}^d  \\phi_{ii} = \\text{Tr}\\left(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T\\right)\n\\]\nLikewise, cross-correlation matrix \\(\\boldsymbol \\Psi\\) trace \nsum correlations corresponding elements \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\text{Tr}(\\boldsymbol \\Psi) =  \\sum_{=1}^d \\text{Cor}(x_i, z_i) =  \\sum_{=1}^d  \\psi_{ii} = \\text{Tr}\\left(\\boldsymbol P^{1/2} \\boldsymbol Q_2^T\\right)\n\\]cases value trace depends \\(\\boldsymbol Q_1\\) \\(\\boldsymbol Q_2\\).\nInterestingly, unique choice trace maximised.Specifically, maximise \\(\\text{Tr}(\\boldsymbol \\Phi)\\) conduct following steps:Apply eigendecomposition \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\). Note \\(\\boldsymbol \\Lambda\\) diagonal positive eigenvalues \\(\\lambda_i > 0\\) \\(\\boldsymbol \\Sigma\\) positive definite \\(\\boldsymbol U\\) orthogonal matrix.objective function becomes\n\\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol \\Phi) &= \\text{Tr}\\left(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T\\right)\\\\\n&= \\text{Tr}\\left( \\boldsymbol U\\boldsymbol \\Lambda^{1/2} \\boldsymbol U^T \\boldsymbol Q_1^T \\right) \\\\\n&= \\text{Tr}\\left(\\boldsymbol \\Lambda^{1/2} \\, \\boldsymbol U^T \\boldsymbol Q_1^T \\boldsymbol U\\right) \\\\\n& = \\text{Tr}\\left(\\boldsymbol \\Lambda^{1/2} \\, \\boldsymbol B\\right) \\\\\n& = \\sum_{=1}^d \\lambda_i^{1/2} b_{ii}.\n\\end{split} \n\\]\nNote product two orthogonal matrices orthogonal matrix.\nTherefore, \\(\\boldsymbol B= \\boldsymbol U^T \\boldsymbol Q_1^T \\boldsymbol U\\) orthogonal matrix \n\\(\\boldsymbol Q_1 = \\boldsymbol U\\boldsymbol B^T \\boldsymbol U^T\\).\\(\\lambda_i > 0\\) \\(b_{ii} \\[-1, 1]\\) objective function maximised\n\\(b_{ii}=1\\), .e. \\(\\boldsymbol B=\\boldsymbol \\).turn implies \\(\\text{Tr}(\\boldsymbol \\Phi)\\) maximised \\(\\boldsymbol Q_1=\\boldsymbol \\).Similary, maximise \\(\\text{Tr}(\\boldsymbol \\Psi)\\) wedecompose \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\) , following ,find \\(\\text{Tr}(\\boldsymbol \\Psi) = \\text{Tr}\\left(\\boldsymbol \\Theta^{1/2} \\, \\boldsymbol G^T \\boldsymbol Q_2^T \\boldsymbol G\\right)\\) maximised \\(\\boldsymbol Q_2=\\boldsymbol \\).Squared Frobenius norm total variation:Another way summarise dissect association \\(\\boldsymbol x\\) corresponding whitened \\(\\boldsymbol z\\)\nsquared Frobenius norm total variation based \\(\\boldsymbol \\Phi\\) \\(\\boldsymbol \\Psi\\).squared Frobenius norm (Euclidean) norm sum squared elements matrix.consider squared Frobenius norm cross-covariance matrix, .e. sum squared covariances \n\\(\\boldsymbol x\\) \\(\\boldsymbol z\\),\n\\[\n|| \\boldsymbol \\Phi||_F^2 = \\sum_{=1}^d \\sum_{j=1}^d \\phi_{ij}^2 =  \\text{Tr}(\\boldsymbol \\Phi^T \\boldsymbol \\Phi) = \\text{Tr}( \\boldsymbol \\Sigma)\n\\]\nfind equals total variation \\(\\boldsymbol \\Sigma\\) depend \\(\\boldsymbol Q_1\\).\nLikewise, computing squared Frobenius norm cross-correlation matrix, .e. sum squared correlations \n\\(\\boldsymbol x\\) \\(\\boldsymbol z\\),\n\\[\n|| \\boldsymbol \\Psi||_F^2  = \\sum_{=1}^d \\sum_{j=1}^d \\psi_{ij}^2= \\text{Tr}\\left(\\boldsymbol \\Psi^T  \\boldsymbol \\Psi\\right) =\\text{Tr}\\left( \\boldsymbol P\\right) = d\n\\]\nyields total variation \\(\\boldsymbol P\\) also depend \\(\\boldsymbol Q_2\\).\nNote squared Frobenius norm invariant rotations reflections.Proportion total variation:can now compute contribution whitened component \\(z_j\\) total variation.\nsum squared covariances \\(z_j\\) \\(x_1, \\ldots, x_d\\) \n\\[\nh_j = \\sum^d_{=1}\\text{Cov}(x_i,z_j)^2 = \\sum^d_{=1} \\phi_{ij}^2\n\\]\n\\(\\sum_{j=1}^d h_j = \\text{Tr}\\left(\\boldsymbol \\Sigma\\right)\\) total variation.\nvector notation contributions written column sums squares \\(\\boldsymbol \\Phi\\)\n\\[\n\\boldsymbol h= (h_1,...,h_d)^T = \\text{Diag}(\\boldsymbol \\Phi^T\\boldsymbol \\Phi) = \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\\,.\n\\]\nrelative contribution \\(z_j\\) versus total variation \n\\[\n\\frac{ h_j }{\\text{Tr}\\left( \\boldsymbol \\Sigma\\right)} \\,.\n\\]\nCrucially, contrast total variation, contributions \\(h_j\\) depend \nchoice \\(\\boldsymbol Q_1\\).Similarly, sum squared correlations \\(z_j\\) \\(x_1, \\ldots, x_d\\) \n\\[\nk_j = \\sum^d_{=1}\\text{Cor}(x_i,z_j)^2 = \\sum^d_{=1} \\psi_{ij}^2\n\\]\n\\(\\sum_{=j}^d k_j = \\text{Tr}( \\boldsymbol P) = d\\). vector notation correspoinds column sums squares \\(\\boldsymbol \\Psi\\)\n\\[\n\\boldsymbol k= (k_1,...,k_d)^T = \\text{Diag}\\left(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\right)=\\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\\,.\n\\]\nrelative contribution \\(z_j\\) regard total variation correlation \\(\\boldsymbol P\\)\n\n\\[\n\\frac{ k_j  }{\\text{Tr}( \\boldsymbol P)} = \\frac{ k_j  }{d} \\,.\n\\]\n, contributions \\(k_j\\) depend choice \\(\\boldsymbol Q_2\\).Maximising proportion total variation:Interestingly, possible choose unique whitening transformation contributions maximised, .e. sum \\(m\\) largest contributions \\(h_j\\) \\(k_j\\) large possible.Specifically, note \\(\\boldsymbol \\Phi^T\\boldsymbol \\Phi\\) \\(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\) symmetric real matrices. type matrices know Schur’s theorem (1923)\neigenvalues \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_d\\) majorise diagonal elements \\(p_1 \\geq p_2 \\geq \\ldots \\geq p_d\\).\nprecisely,\n\\[\n\\sum_{=1}^m \\lambda_i \\geq \\sum_{=1}^m p_i \\, ,\n\\]\n.e. sum largest \\(m\\) eigenvalues larger equal sum \\(m\\) largest diagonal elements.\nmaximum (equality) achieved matrix diagonal, case diagonal elements equal eigenvalues.Therefore, optimal solution problem maximising relative contributions obtained computing\neigendecompositions \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\)\ndiagonalise \\(\\boldsymbol \\Phi^T\\boldsymbol \\Phi=\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\) \\(\\boldsymbol \\Psi^T \\boldsymbol \\Psi=\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\) \nsetting \\(\\boldsymbol Q_1= \\boldsymbol U^T\\) \\(\\boldsymbol Q_2= \\boldsymbol G^T\\), respectively. yields maximised\ncontributions\n\\[\n\\boldsymbol h= \\text{Diag}\\left(\\boldsymbol \\Lambda\\right)= (\\lambda_1, \\ldots, \\lambda_d)^T\n\\]\n\n\\[\n\\boldsymbol k= \\text{Diag}\\left(\\boldsymbol \\Theta\\right) = (\\theta_1, \\ldots, \\theta_d)^T\n\\]\neigenvalues \\(\\lambda_i\\) \\(\\theta_i\\) arranged decreasing order.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"natural-whitening-procedures","chapter":"2 Transformations and dimension reduction","heading":"2.4 Natural whitening procedures","text":"now introduce several strategies (maximise correlation individual components, maximise compression, structural constraints) select optimal whitening procedure.Specifically, discuss following whitening transformations:Mahalanobis whitening, also known ZCA (zero-phase component analysis) whitening machine learning (based covariance)ZCA-cor whitening (based correlation)PCA whitening (based covariance)PCA-cor whitening (based correlation)Cholesky whiteningThus, following consider three main types (ZCA, PCA, Cholesky) whitening.following \\(\\boldsymbol x_c = \\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x}\\) \\(\\boldsymbol z_c = \\boldsymbol z-\\boldsymbol \\mu_{\\boldsymbol z}\\) denote mean-centered variables.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"zca-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.1 ZCA whitening","text":"Aim: remove correlations standardise otherwise make sure \nwhitened vector \\(\\boldsymbol z\\) differ much original vector \\(\\boldsymbol x\\). Specifically, latent component \\(z_i\\) close possible corresponding original variable \\(x_i\\):\n\\[\n\\begin{array}{cc}\nx_1\\leftrightarrow z_1 \\\\\nx_2\\leftrightarrow z_2\\\\\nx_3\\leftrightarrow z_3 \\\\\n\\vdots\n\\end{array}\n\\]\nOne possible way implement compute expected squared difference two centered random vectors \\(\\boldsymbol z_c\\) \\(\\boldsymbol x_c\\).ZCA objective function: minimise \\(\\text{E}\\left( || \\boldsymbol x_c - \\boldsymbol z_c ||^2_F \\right)\\) find optimal whitening procedure.ZCA objective function can simplified follows:\n\\[\n\\begin{split}\n\\text{E}\\left( || \\boldsymbol x_c-\\boldsymbol z_c ||^2_F   \\right)&=\\text{E}\\left( || \\boldsymbol x_c ||^2_F   \\right)  -2 \\text{E}\\left(  \\text{Tr}\\left( \\boldsymbol x_c \\boldsymbol z_c^T \\right) \\right)  + \\text{E}\\left( || \\boldsymbol z_c ||^2_F   \\right) \\\\\n& = \\text{Tr}(  \\text{E}( \\boldsymbol x_c \\boldsymbol x_c^T ) )  - 2 \\text{Tr}( \\text{E}(  \\boldsymbol x_c \\boldsymbol z_c^T ) ) + \\text{Tr}( \\text{E}( \\boldsymbol z_c \\boldsymbol z_c^T ) )\n   \\\\\n& = \\text{Tr}( \\text{Var}(\\boldsymbol x) ) - 2 \\text{Tr}( \\text{Cov}(\\boldsymbol x, \\boldsymbol z) ) +  \\text{Tr}( \\text{Var}(\\boldsymbol z) )  \\\\\n& = \\text{Tr}(\\boldsymbol \\Sigma) - 2\\text{Tr}(\\boldsymbol \\Phi)+ d \\\\\n\\end{split}\n\\]\nobjective function can also obtained putting diagonal constraint cross-covariance \\(\\boldsymbol \\Phi\\). Specifically, looking \\(\\boldsymbol \\Phi\\) closest diagonal matrix \\(\\boldsymbol \\) minimising\n\\[\n\\begin{split}\n|| \\boldsymbol \\Phi- \\boldsymbol ||^2_F &= || \\boldsymbol \\Phi||^2_F  - 2 \\text{Tr}(\\boldsymbol \\Phi^T \\boldsymbol )  + || \\boldsymbol ||^2_F \\\\\n&= \\text{Tr}(\\boldsymbol \\Sigma) - 2 \\text{Tr}(\\boldsymbol \\Phi) + d \\\\\n\\end{split}\n\\]\nforce -diagonal elements \\(\\boldsymbol \\Phi\\) close \nzero thus leads sparsity cross-covariance matrix.term depends whitening transformation \\(-2 \\text{Tr}(\\boldsymbol \\Phi)\\) \\(\\boldsymbol \\Phi\\) function\n\\(\\boldsymbol Q_1\\). Therefore can use following\nalternative objective:ZCA equivalent objective: maximise \\(\\text{Tr}(\\boldsymbol \\Phi) = \\text{Tr}(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T)\\) find optimal \\(\\boldsymbol Q_1\\)Solution:earlier discussion know optimal matrix \n\\[\n\\boldsymbol Q_1^{\\text{ZCA}}=\\boldsymbol \n\\]\ncorresponding whitening matrix ZCA therefore\n\\[\n\\boldsymbol W^{\\text{ZCA}} = \\boldsymbol \\Sigma^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{ZCA}} = \\boldsymbol \\Sigma^{1/2}\n\\]\ncross-correlation matrix\n\\[\n\\boldsymbol \\Psi^{\\text{ZCA}} = \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma^{1/2} \n\\]Note \\(\\boldsymbol \\Sigma^{1/2}\\) symmetric positive definite matrix,\nhence diagonal elements positive. result,\ndiagonals \\(\\boldsymbol \\Phi^{\\text{ZCA}}\\) \\(\\boldsymbol \\Psi^{\\text{ZCA}}\\) positive,\n.e. \\(\\text{Cov}(x_i, z_i) > 0\\) \\(\\text{Cor}(x_i, z_i) > 0\\).\nHence, ZCA two corresponding components \\(x_i\\) \\(z_i\\) always positively correlated!Proportion total variation:ZCA \\(\\boldsymbol Q_1=\\boldsymbol \\) find \\(\\boldsymbol h=\\text{Diag}(\\boldsymbol \\Sigma) = \\sum_{j=1}^d \\text{Var}(x_j)\\) \\(h_i=\\text{Var}(x_i)\\) hence ZCA proportion total variation contributed \nlatent component \\(z_i\\) ratio \\(\\frac{\\text{Var}(x_i}{\\sum_{j=1}^d \\text{Var}(x_j)}\\).Summary:ZCA/Mahalanobis transform unique transformation minimises expected total squared component-wise difference \\(\\boldsymbol x_c\\) \\(\\boldsymbol z_c\\).ZCA corresponding components whitened original variables always positively correlated. facilitates interpretation whitened variables.Use ZCA aka Mahalanobis whitening want “just” remove correlations.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"zca-cor-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.2 ZCA-Cor whitening","text":"Aim: remove scale \\(\\boldsymbol x\\) first comparing \\(\\boldsymbol z\\).ZCA-cor objective function: minimise \\(\\text{E}\\left( || \\boldsymbol V^{-1/2} \\boldsymbol x_c -\\boldsymbol z_c ||^2_F \\right)\\) find optimal whitening procedure.can simplified follows:\n\\[\n\\begin{split}\n\\text{E}\\left( || \\boldsymbol V^{-1/2} \\boldsymbol x_c -\\boldsymbol z_c||^2_F   \\right)&=\\text{E}\\left( || \\boldsymbol V^{-1/2} \\boldsymbol x_c ||^2_F   \\right)  -2 \\text{E}\\left(  \\text{Tr}\\left( \\boldsymbol V^{-1/2} \\boldsymbol x_c \\boldsymbol z_c^T  \\right) \\right) +  \\text{E}\\left( || \\boldsymbol z_c ||^2_F   \\right)\\\\\n& = \\text{Tr}(  \\text{E}(\\boldsymbol V^{-1/2} \\boldsymbol x_c \\boldsymbol x_c^T \\boldsymbol V^{-1/2}) )  \n- 2 \\text{Tr}( \\text{E}( \\boldsymbol V^{-1/2} \\boldsymbol x_c \\boldsymbol z_c^T  ) ) \n+\\text{Tr}( \\text{E}( \\boldsymbol z_c \\boldsymbol z_c^T ) )\n  \\\\\n& = \\text{Tr}(  \\text{Cor}(\\boldsymbol x, \\boldsymbol x) ) - 2 \\text{Tr}( \\text{Cor}(\\boldsymbol x, \\boldsymbol z) ) + \\text{Tr}( \\text{Var}(\\boldsymbol z) )   \\\\\n& = d - 2\\text{Tr}(\\boldsymbol \\Psi)+ d \\\\\n& = 2d - 2\\text{Tr}(\\boldsymbol \\Psi)\n\\end{split}\n\\]\nobjective function can also obtained putting diagonal constraint cross-correlation \\(\\boldsymbol \\Psi\\). Specifically, looking \\(\\boldsymbol \\Psi\\) closest diagonal matrix \\(\\boldsymbol \\) minimising\n\\[\n\\begin{split}\n|| \\boldsymbol \\Psi- \\boldsymbol ||^2_F &= || \\boldsymbol \\Psi||^2_F  - 2 \\text{Tr}(\\boldsymbol \\Psi^T \\boldsymbol )  + || \\boldsymbol ||^2_F \\\\\n&= d - 2 \\text{Tr}(\\boldsymbol \\Psi) + d \\\\\n&= 2 d - 2 \\text{Tr}(\\boldsymbol \\Psi) \\\\\n\\end{split}\n\\]\nforce -diagonal elements \\(\\boldsymbol \\Psi\\) close \nzero thus leads sparsity cross-correlation matrix.term depends whitening transformation \\(-2 \\text{Tr}(\\boldsymbol \\Psi)\\) \\(\\boldsymbol \\Psi\\) function \\(\\boldsymbol Q_2\\). Thus can use following alternative objective instead:ZCA-cor equivalent objective: maximise \\(\\text{Tr}(\\boldsymbol \\Psi)=\\text{Tr}(\\boldsymbol P^{1/2} \\boldsymbol Q_2^T)\\) find optimal \\(\\boldsymbol Q_2\\)Solution: ZCA using correlation instead covariance:earlier discussion know optimal matrix \n\\[\n\\boldsymbol Q_2^{\\text{ZCA-Cor}}=\\boldsymbol \n\\]\ncorresponding whitening matrix ZCA-cor therefore\n\\[\n\\boldsymbol W^{\\text{ZCA-Cor}} = \\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{ZCA-Cor}} = \\boldsymbol V^{1/2} \\boldsymbol P^{1/2} \n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{ZCA-Cor}} = \\boldsymbol P^{1/2}\n\\]ZCA-cor transformation also \n\\(\\text{Cov}(x_i, z_i) > 0\\) \\(\\text{Cor}(x_i, z_i) > 0\\)\ntwo corresponding components \\(x_i\\) \\(z_i\\) always positively correlated!Proportion total variation:ZCA-cor \\(\\boldsymbol Q_2=\\boldsymbol \\) find \\(\\boldsymbol k=\\text{Diag}(\\boldsymbol P) = d\\) \\(k_i =1\\).\nThus, ZCA-cor whitened component \\(z_i\\) contributes equally total variation \\(\\text{Tr}(\\boldsymbol P) =d\\), relative proportion\n\\(\\frac{1}{d}\\).Summary:ZCA-cor whitening unique whitening transformation maximising \ntotal correlation corresponding elements \\(\\boldsymbol x\\) \\(\\boldsymbol z\\).ZCA-cor leads interpretable \\(\\boldsymbol z\\) individual element \\(\\boldsymbol z\\)\n(typically strongly) positively correlated corresponding element original \\(\\boldsymbol x\\).ZCA-cor explicitly constructed maximise total\npairwise correlations achieves higher total correlation ZCA.\\(\\boldsymbol x\\) standardised \\(\\text{Var}(x_i)=1\\) ZCA ZCA-cor identical.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.3 PCA whitening","text":"Aim: remove correlations time compress information latent variables.\nSpecifically, like first latent component \\(z_1\\) \nmaximally linked variables \\(\\boldsymbol x\\), followed \nsecond component \\(z_2\\) :\n\\[\n\\begin{array}{cc}\nx_1, x_2, \\ldots, x_d & \\rightarrow z_1 \\\\\nx_1, x_2, \\ldots, x_d & \\rightarrow z_2 \\\\\n\\vdots\\\\\nx_1, x_2, \\ldots, x_d & \\rightarrow z_d \\\\\n\\end{array}\n\\]\nOne way measure total association latent component \\(z_j\\) \noriginal \\(x_1, \\ldots, x_d\\) sum \ncorresponding squared covariances\n\\[\nh_j = \\sum^d_{=1}\\text{Cov}(x_i,z_j)^2 = \\sum^d_{=1} \\phi_{ij}^2\n\\]\nequivalently column sum squares \\(\\boldsymbol \\Phi\\)\n\\[\n\\boldsymbol h= (h_1,...,h_d)^T = \\text{Diag}(\\boldsymbol \\Phi^T\\boldsymbol \\Phi) = \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\n\\]\n\\(h_j\\) contribution\n\\(z_j\\) \\(\\text{Tr}\\left( \\boldsymbol Q_1 \\boldsymbol \\Sigma\\boldsymbol Q_1^T \\right)= \\text{Tr}(\\boldsymbol \\Sigma)\\)\n.e. total variation based \\(\\boldsymbol \\Sigma\\).\n\\(\\text{Tr}(\\boldsymbol \\Sigma)\\) constant implies \\(d-1\\) independent \\(h_j\\).PCA-whitening wish concentrate contributions total variation based\n\\(\\boldsymbol \\Sigma\\) small number\nlatent components.PCA whitening objective function: find optimal optimal \\(\\boldsymbol Q_1\\) \nresulting set \\(h_1 \\geq h_2 \\ldots \\geq h_d\\) \\(\\boldsymbol h= \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\\) majorizes set relative contributions.Solution:Following earlier discussion apply Schur’s theorem find optimal\nsolution diagonalising \\(\\boldsymbol \\Phi^T\\boldsymbol \\Phi\\) eigendecomposition \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\).\nHence, optimal value \\(\\boldsymbol Q_1\\) matrix \n\\[\n\\boldsymbol Q_1^{\\text{PCA}}=\\boldsymbol U^T\n\\]\nHowever, recall \\(\\boldsymbol U\\) uniquely defined — free change columns signs.\ncorresponding whitening matrix \n\\[\n\\boldsymbol W^{\\text{PCA}} = \\boldsymbol U^T\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Lambda^{-1/2}\\boldsymbol U^T\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{PCA}} = \\boldsymbol U\\boldsymbol \\Lambda^{1/2} \n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{PCA}} = \\boldsymbol V^{-1/2} \\boldsymbol U\\boldsymbol \\Lambda^{1/2} \n\\]Identifiability:Note (.e. \\(\\boldsymbol Q_1^{\\text{PCA}}, \\boldsymbol W^{\\text{PCA}}, \\boldsymbol \\Phi^{\\text{PCA}}, \\boldsymbol \\Psi^{\\text{PCA}}\\)) unique\ndue sign ambiguity columns \\(\\boldsymbol U\\).Therefore, identifiability reasons may wish impose constraint \\(\\boldsymbol Q_1^{\\text{PCA}}\\)\nequivalently \\(\\boldsymbol \\Phi^{\\text{PCA}}\\). useful condition require (given ordering\noriginal variables!) \\(\\boldsymbol Q_1^{\\text{PCA}}\\) positive diagonal\nequivalently \\(\\boldsymbol \\Phi^{\\text{PCA}}\\) positive diagonal. implies \n\\(\\text{Diag}(\\boldsymbol U) > 0\\) \\(\\text{Diag}(\\boldsymbol \\Psi^{\\text{PCA}}) > 0\\), hence\npairs \\(x_i\\) \\(z_i\\) positively correlated.particularly important pay attention sign ambiguity\ncomparing different computer implementations PCA whitening (related PCA approach).Note actual objective PCA whitening \\(\\text{Diag}(\\boldsymbol \\Phi^T\\boldsymbol \\Phi)\\) affected sign ambiguity\nsince column signs \\(\\boldsymbol \\Phi\\) matter.Proportion total variation:PCA whitening contribution \\(h_i^{\\text{PCA}}\\) latent component \\(z_i\\)\ntotal variation based covariance \\(\\text{Tr}(\\boldsymbol \\Sigma) = \\sum_{j=1}^d \\lambda_j\\) \n\\(h_i^{\\text{PCA}} = \\lambda_i\\).\nfraction \\(\\frac{\\lambda_i}{\\sum^d_{j=1}\\lambda_j}\\) relative\ncontribution element \\(\\boldsymbol z\\) explain total variation.Thus, low ranking components \\(z_i\\) small \\(h_i^{\\text{PCA}}=\\lambda_i\\) may discarded.\nway PCA whitening achieves compression \ndimension reduction.Summary:PCA whitening whitening transformation maximises compression sum squared cross-covariances underlying optimality criterion.sign ambiguities PCA whitened variables inherited sign ambiguities eigenvectors.positive-diagonal condition orthogonal matrices imposed sign ambiguities fully resolved corresponding components \\(z_i\\) \\(x_i\\) always positively correlated.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-cor-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.4 PCA-cor whitening","text":"Aim: PCA whitening remove scale \\(\\boldsymbol x\\) first. means use squared correlations rather squared covariances measure compression, .e.\\[\nk_j = \\sum^d_{=1}\\text{Cor}(x_i, z_j)^2 = \\sum^d_{=1} \\psi_{ij}^2\n\\]\nvector notation column sum squares \\(\\boldsymbol \\Psi\\)\n\\[\n\\boldsymbol k= (k_1,...,k_d)^T = \\text{Diag}\\left(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\right)=\\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\n\\]\n\\(k_j\\) contribution\n\\(z_j\\) \\(\\text{Tr}\\left( \\boldsymbol Q_2 \\boldsymbol P\\boldsymbol Q_2^T \\right)= \\text{Tr}(\\boldsymbol P) = d\\)\n.e. total variation based \\(\\boldsymbol P\\).\n\\(\\text{Tr}(\\boldsymbol P)=d\\) constant implies \\(d-1\\) independent \\(k_j\\).PCA-cor-whitening wish concentrate contributions total variation based \\(\\boldsymbol P\\) small number latent components.PCA-cor whitening objective function: find optimal optimal \\(\\boldsymbol Q_2\\) \nresulting set \\(k_1 \\geq k_2 \\ldots \\geq k_d\\) \\(\\boldsymbol k= \\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\\) majorizes set relative contributions.Solution:Following earlier discussion apply Schur’s theorem find optimal\nsolution diagonalising \\(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\) eigendecomposition \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\).\nHence, optimal value \\(\\boldsymbol Q_2\\) matrix \n\\[\n\\boldsymbol Q_2^{\\text{PCA-Cor}}=\\boldsymbol G^T\n\\]\n\\(\\boldsymbol G\\) uniquely defined — free change signs columns.\ncorresponding whitening matrix \n\\[\n\\boldsymbol Q_2^{\\text{PCA-Cor}}=\\boldsymbol G^T\n\\]\ncorresponding whitening matrix \\[\n\\boldsymbol W^{\\text{PCA-Cor}} = \\boldsymbol \\Theta^{-1/2} \\boldsymbol G^T \\boldsymbol V^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{PCA-Cor}} = \\boldsymbol V^{1/2} \\boldsymbol G\\boldsymbol \\Theta^{1/2} \n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{PCA-Cor}} = \\boldsymbol G\\boldsymbol \\Theta^{1/2} \n\\]Identifiability:PCA whitening, sign ambiguities column signs \\(\\boldsymbol G\\)\ncan freely chosen. identifiability may wish impose constraints\n\\(\\boldsymbol Q_2^{\\text{PCA-Cor}}\\) equivalently \\(\\boldsymbol \\Psi^{\\text{PCA-Cor}}\\). useful condition require (given\nordering original variables!)\ndiagonal elements \\(\\boldsymbol Q_2^{\\text{PCA-Cor}}\\) positive equivalently \\(\\boldsymbol \\Psi^{\\text{PCA-Cor}}\\) positive diagonal.\nimplies \n\\(\\text{Diag}(\\boldsymbol G) > 0\\) \\(\\text{Diag}(\\boldsymbol \\Phi^{\\text{PCA-Cor}}) > 0\\).Note actual objective PCA-cor whitening \\(\\text{Diag}(\\boldsymbol \\Psi^T\\boldsymbol \\Psi)\\) affected sign ambiguity\nsince column signs \\(\\boldsymbol \\Psi\\) matter.Proportion total variation:PCA-cor whitening contribution \\(k_i^{\\text{PCA-Cor}}\\) latent component\n\\(z_i\\)\ntotal variation based correlation \\(\\text{Tr}(\\boldsymbol P) = d\\) \n\\(k_i^{\\text{PCA-Cor}} = \\theta_i\\).\nfraction \\(\\frac{\\theta_i}{d}\\) relative\ncontribution element \\(\\boldsymbol z\\) explain total variation.Summary:PCA-cor whitening whitening transformation maximises compression sum squared cross-correlations underlying optimality criterion.sign ambiguities PCA-cor whitened variables inherited sign ambiguities eigenvectors.positive-diagonal condition orthogonal matrices imposed sign ambiguities fully resolved corresponding components \\(z_i\\) \\(x_i\\) always positively correlated.\\(\\boldsymbol x\\) standardised \\(\\text{Var}(x_i)=1\\), PCA PCA-cor whitening identical.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"cholesky-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.5 Cholesky whitening","text":"Aim Cholesky whitening:Find whitening transformation cross-covariance \\(\\boldsymbol \\Phi\\) cross-correlation \\(\\boldsymbol \\Psi\\) lower triangular structure. Specifically, wish original variable \\(x_1\\) linked\nfirst latent variable \\(z_1\\) , second original variable \\(x_2\\) linked \n\\(z_1\\) \\(z_2\\) , , last variable \\(x_d\\) linked latent variables\n\\(z_1, \\ldots, z_d\\):\\[\n\\begin{array}{cc}\nx_1 & \\rightarrow z_1 \\\\\nx_1, x_2 & \\rightarrow z_2 \\\\\n\\vdots\\\\\nx_1, x_2, \\ldots, x_d & \\rightarrow z_d \\\\\n\\end{array}\n\\]Thus, Cholesky whitening imposes structural (sparsity!) constraint loadings,\nnon-zero coefficients lower half whereas upper half\ncoefficients vanish.Cholesky matrix decomposition:order find whitening transformation use Cholesky decompositionThe Cholesky decomposition square matrix \\(\\boldsymbol = \\boldsymbol L\\boldsymbol L^T\\) requires positive definite \\(\\boldsymbol \\) unique.\n\\(\\boldsymbol L\\) lower triangular matrix positive diagonal elements.\ninverse \\(\\boldsymbol L^{-1}\\) also lower triangular positive diagonal elements.\n\\(\\boldsymbol D\\) diagonal matrix positive elements \\(\\boldsymbol D\\boldsymbol L\\) also lower triangular matrix positive diagonal Cholesky factor matrix \\(\\boldsymbol D\\boldsymbol \\boldsymbol D\\).Solution: Apply Cholesky decomposition \\(\\boldsymbol \\Sigma= \\boldsymbol L\\boldsymbol L^T\\)resulting whitening matrix \n\\[\n\\boldsymbol W^{\\text{Chol}}=\\boldsymbol L^{-1}\n\\]\nconstruction, \\(\\boldsymbol W^{\\text{Chol}}\\) lower triangular matrix positive\ndiagonal. whitening constraint satisfied \n\\((\\boldsymbol W^{\\text{Chol}})^T\\boldsymbol W^{\\text{Chol}} = (\\boldsymbol L^{-1})^T \\boldsymbol L^{-1} = (\\boldsymbol L^T)^{-1} \\boldsymbol L^{-1} = (\\boldsymbol L\\boldsymbol L^T)^{-1} = \\boldsymbol \\Sigma^{-1}\\).cross-covariance matrix inverse whitening matrix\n\\[\n\\boldsymbol \\Phi^{\\text{Chol}} = \\boldsymbol L\n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{Chol}} = \\boldsymbol V^{-1/2} \\boldsymbol L\n\\]\n\\(\\boldsymbol \\Phi^{\\text{Chol}}\\) \n\\(\\boldsymbol \\Psi^{\\text{Chol}}\\) \nlower triangular matrices positive diagonal elements.\nHence two corresponding components \\(x_i\\) \\(z_i\\) always positively correlated!Finally, corresponding orthogonal matrices \n\\[\n\\boldsymbol Q_1^{\\text{Chol}}  =  \\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1/2} =   \\boldsymbol L^T \\boldsymbol \\Sigma^{-1/2}\n\\]\n\n\\[\n\\boldsymbol Q_2^{\\text{Chol}} =  \\boldsymbol \\Psi^T \\boldsymbol P^{-1/2} =  \\boldsymbol L^T \\boldsymbol V^{-1/2} \\boldsymbol P^{-1/2} \n\\]Application correlation instead covariance:may also apply Cholesky decomposition correlation rather covariance matrix.\nHowever, lead different whitening transform (unlike ZCA PCA):Cholesky factor \\(\\boldsymbol P= \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2}\\) \\(\\boldsymbol V^{-1/2} \\boldsymbol L\\).\ncorresponding whitening matrix \\((\\boldsymbol V^{-1/2} \\boldsymbol L)^{-1} \\boldsymbol V^{-1/2} =\\boldsymbol L^{-1} = \\boldsymbol W^{\\text{Chol}}\\).also intuively clear covariance correlation loadings \nclosely linked, particular share triangular shape.Dependence input order:Cholesky whitening depends ordering input variables.\nordering original variables yield different triangular\nconstraint thus different Cholesky whitening transform. example,\ninverting ordering \\(x_d, x_{d-1}, \\ldots, x_1\\) effectively enforce upper triangular\nshape.alternative formulation Cholesky whitening decomposes \nprecision matrix rather covariance matrix. yields \nupper triangular structure directly otherwise fully equivalent Cholesky whitening\nbased decomposing covariance.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"comparison-of-whitening-procedures---simulated-data","chapter":"2 Transformations and dimension reduction","heading":"2.4.6 Comparison of whitening procedures - simulated data","text":"comparison, results ZCA, PCA Cholesky whitening applied simulated bivariate normal data set correlation \\(\\rho=0.8\\).column 1 can see simulated data scatter plot.Column 2 shows scatter plots whitened data — expect three methods remove correlation produce isotropic covariance.However, three approaches differ cross-correlations. Columns 3 4 show cross-correlations first two corresponding components (\\(x_1\\) \\(z_1\\), \\(x_2\\) \\(z_2\\)) ZCA, PCA Cholesky whitening. expected, ZCA pairs show strong correlation, case PCA Cholesky whitening.Note Cholesky whitening first component \\(z_1\\)\nperfectly positively correlated original component \\(x_1\\)\nwhitening matrix lower triangular positive diagonal hence \\(z_1\\) just \\(x_1\\) multiplied positive constant.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"comparison-of-whitening-procedures---iris-flowers","chapter":"2 Transformations and dimension reduction","heading":"2.4.7 Comparison of whitening procedures - iris flowers","text":"example consider well known iris flower data set. consists botanical measures (sepal length, sepal width,\npetal length petal width) 150 iris flowers comprising\nthree species (Iris setosa, Iris versicolor, Iris virginica). Hence data set dimension \\(d=4\\) sample size \\(n=150\\).apply discussed whitening transforms data, sort whitened components relative contribution total variation. Cholesky whitening used \ninput order shape constraint.results explained variation based covariance loadings:expected, two PCA whitening approaches compress data .\nend spectrum, ZCA whitening methods two least\ncompressing approaches. Cholesky whitening compromise ZCA PCA terms\ncompression.Similar results obtained based correlation loadings - note ZCA-cor provides\nequal weight latent variable.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"recap","chapter":"2 Transformations and dimension reduction","heading":"2.4.8 Recap","text":"data standardised \\(\\boldsymbol \\Phi\\) \\(\\boldsymbol \\Psi\\) \nhence ZCA become ZCA-cor PCA becomes PCA-cor. triangular shape constraint \nCholesky whitening depends ordering original variables.Related methods discussed course:Factor models: essentially whitening plus additional error term, factors rotational\nfreedom just like whiteningFactor models: essentially whitening plus additional error term, factors rotational\nfreedom just like whiteningPartial Least Squares (PLS): similar Principal Components Analysis (PCA) regression setting (choice \nlatent variables depending response)Partial Least Squares (PLS): similar Principal Components Analysis (PCA) regression setting (choice \nlatent variables depending response)Nonlinear dimension reduction methods SNE, tSNE, UMAP.Nonlinear dimension reduction methods SNE, tSNE, UMAP.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"principal-component-analysis-pca","chapter":"2 Transformations and dimension reduction","heading":"2.5 Principal Component Analysis (PCA)","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.5.1 PCA transformation","text":"Principal component analysis proposed 1933 Harald Hotelling8 closely related PCA whitening. underlying mathematics developed earlier 1901 Karl Pearson9 problem orthogonal regression.Assume random vector \\(\\boldsymbol x\\) \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\).\nPCA particular orthogonal transformation original \\(\\boldsymbol x\\)\nresulting components orthogonal:\n\\[\n\\underbrace{\\boldsymbol t^{\\text{PCA}}}_{\\text{Principal components}} = \\underbrace{\\boldsymbol U^T}_{\\text{Orthogonal matrix}}   \\boldsymbol x\n\\]\n\\[\\text{Var}(\\boldsymbol t^{\\text{PCA}}) = \\boldsymbol \\Lambda= \\begin{pmatrix} \\lambda_1 & \\dots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & \\lambda_d\\end{pmatrix}\\]\nNote principal components orthogonal unit variance variance principal components \\(t_i\\) equals eigenvalues \\(\\lambda_i\\).Thus PCA whitening procedure closely linked PCA whitening obtained standardising principal components: \\(\\boldsymbol z^{\\text{PCA}} = \\boldsymbol \\Lambda^{-1/2} \\boldsymbol t^{\\text{PCA}}\\)Compression properties:total variation \\(\\text{Tr}(\\text{Var}(\\boldsymbol t^{\\text{PCA}})) = \\text{Tr}( \\boldsymbol \\Lambda) = \\sum^d_{j=1}\\lambda_j\\).\nprinciple components fraction \\(\\frac{\\lambda_i}{\\sum^d_{j=1}\\lambda_j}\\) can interpreted proportion variation contributed \ncomponent \\(\\boldsymbol t^{\\text{PCA}}\\) total variation. Thus, low ranking components \\(\\boldsymbol t^{\\text{PCA}}\\) low variation may discarded, thus leading reduction dimension.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"application-to-data","chapter":"2 Transformations and dimension reduction","heading":"2.5.2 Application to data","text":"Written terms data matrix \\(\\boldsymbol X\\) instead random vector \\(\\boldsymbol x\\) PCA becomes:\n\\[\\underbrace{\\boldsymbol T}_{\\text{Sample version principal components}}=\\underbrace{\\boldsymbol X}_{\\text{Data matrix}}\\boldsymbol U\\]\nnow two ways obtain \\(\\boldsymbol U\\):Estimate covariance matrix, e.g. \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c\\) \\(\\boldsymbol X_c\\) column-centred data matrix; apply eigenvalue decomposition \\(\\hat{\\boldsymbol \\Sigma}\\) get \\(\\boldsymbol U\\).Estimate covariance matrix, e.g. \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c\\) \\(\\boldsymbol X_c\\) column-centred data matrix; apply eigenvalue decomposition \\(\\hat{\\boldsymbol \\Sigma}\\) get \\(\\boldsymbol U\\).Compute singular value decomposition \\(\\boldsymbol X_c = \\boldsymbol V\\boldsymbol D\\boldsymbol U^T\\). \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c = \\boldsymbol U(\\frac{1}{n}\\boldsymbol D^2)\\boldsymbol U^T\\) can just use \\(\\boldsymbol U\\) SVD \\(\\boldsymbol X_c\\) need compute covariance.Compute singular value decomposition \\(\\boldsymbol X_c = \\boldsymbol V\\boldsymbol D\\boldsymbol U^T\\). \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c = \\boldsymbol U(\\frac{1}{n}\\boldsymbol D^2)\\boldsymbol U^T\\) can just use \\(\\boldsymbol U\\) SVD \\(\\boldsymbol X_c\\) need compute covariance.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"iris-flower-data-example","chapter":"2 Transformations and dimension reduction","heading":"2.5.3 Iris flower data example","text":"first standardise data, compute PCA components plot proportion total variation contributed component.\nshows two PCA components needed achieve 95% total variation:scatter plot plot first two principal components also informative:shows groupings among \n150 flowers, corresponding species, groups can characterised\nprincipal components.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-correlation-loadings","chapter":"2 Transformations and dimension reduction","heading":"2.5.4 PCA correlation loadings","text":"earlier section learned general whitening transformation cross-correlations \\(\\boldsymbol \\Psi=\\text{Cor}(\\boldsymbol x, \\boldsymbol z)\\) play role correlation loadings inverse transformation:\n\\[\n\\boldsymbol V^{-1/2} \\boldsymbol x= \\boldsymbol \\Psi\\boldsymbol z\\, , \n\\]\n.e. coefficients linking whitened variable \\(\\boldsymbol z\\) standardised original variable \\(\\boldsymbol x\\).\nrelationship holds therefore also PCA-whitening\n\\(\\boldsymbol z^{\\text{PCA}}= \\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T \\boldsymbol x\\) \\(\\boldsymbol \\Psi^{\\text{PCA}} = \\boldsymbol V^{-1/2} \\boldsymbol U\\boldsymbol \\Lambda^{1/2}\\).classical PCA whitening approach \\(\\text{Var}(\\boldsymbol t^{\\text{PCA}}) \\neq \\boldsymbol \\). However, can still compute cross-correlations \\(\\boldsymbol x\\) \nprincipal components \\(\\boldsymbol t^{\\text{PCA}}\\), resulting \n\\[\n\\text{Cor}(\\boldsymbol x, \\boldsymbol t^{\\text{PCA}}) = \\boldsymbol V^{-1/2} \\boldsymbol U\\boldsymbol \\Lambda^{1/2}  = \\boldsymbol \\Psi^{\\text{PCA}}\n\\]\nNote cross-correlations PCA-whitening since\n\\(\\boldsymbol t^{\\text{PCA}}\\) \\(\\boldsymbol z^{\\text{PCA}}\\) differ scale.inverse PCA transformation \n\\[\n\\boldsymbol x= \\boldsymbol U\\boldsymbol t^{\\text{PCA}}\n\\]\nterms standardised PCA components \\(\\boldsymbol z^{\\text{PCA}} = \\boldsymbol \\Lambda^{-1/2} \\boldsymbol t^{\\text{PCA}}\\) standardised original components becomes\n\\[\n\\boldsymbol V^{-1/2} \\boldsymbol x= \\boldsymbol \\Psi\\boldsymbol \\Lambda^{-1/2} \\boldsymbol t^{\\text{PCA}}\n\\]\nThus cross-correlation matrix \\(\\boldsymbol \\Psi\\) plays role correlation loadings\nalso classical PCA, .e. \ncoefficients linking standardised PCA components standardised original components.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-correlation-loadings-plot","chapter":"2 Transformations and dimension reduction","heading":"2.5.5 PCA correlation loadings plot","text":"PCA PCA-cor whitening well classical PCA aim compression, .e.\nfind latent variables total variation contributed \nsmall number components.order able better interpret top ranking PCA component can use visual device called correlation loadings plot. compute correlation PCA components 1 2 (\\(t_1^{\\text{PCA}}\\) \\(t_2^{\\text{PCA}})\\) original variables \\(x_1, \\ldots, x_d\\).original variable \\(x_i\\) therefore two numbers -1 1, correlation\n\\(\\text{Cor}(x_i, t_1^{\\text{PCA}}) = \\psi_{i1}\\) \\(\\text{Cor}(x_i, t_2^{\\text{PCA}}) = \\psi_{i2}\\) use coordinates draw point plane. Recall \nrow sums squares correlation loadings \\(\\boldsymbol \\Psi\\) identical 1.\nHence, sum squared loadings just first two components also 1.\nThus, construction, points\nlie within unit circle around origin.\noriginal variables strongly influenced\ntwo latent variables strong correlation thus lie near outer circle, whereas variables influenced two latent variables lie near origin.example, correlation loadings plot showing cross-correlation first two\nPCA components four variables iris flower data set discussed earlier.interpretation plot discussed Worksheet 5.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"unsupervised-learning-and-clustering","chapter":"3 Unsupervised learning and clustering","heading":"3 Unsupervised learning and clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"challenges-in-unsupervised-learning","chapter":"3 Unsupervised learning and clustering","heading":"3.1 Challenges in unsupervised learning","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"objective","chapter":"3 Unsupervised learning and clustering","heading":"3.1.1 Objective","text":"observe data \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) \\(n\\) objects (subjects).\nsample \\(\\boldsymbol x_i\\) vector dimension \\(d\\). Thus, \\(n\\) objects / subjects measurements \\(d\\) variables.\naim unsupervised learning identify patters relating objects/subjects based information available \\(\\boldsymbol x_i\\). Note unsupervised learning use information\n\\(\\boldsymbol x_i\\) nothing else.illustration consider first two principal components Iris flower data (see e.g. Worksheet 5):Clearly group structure among samples linked particular\npatterns first two principal components.Note plot used additional information, class labels (setosa, versicolor, virginica), highlighting true underlying structure (three flower species).unsupervised learning class labels (assumed ) unknown, aim infer clustering thus classes labels.10There many methods clustering unsupervise learning, purely algorithmic well probabilistic. chapter study commonly used approaches.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"questions-and-problems","chapter":"3 Unsupervised learning and clustering","heading":"3.1.2 Questions and problems","text":"order implement unsupervised learning need address number questions:define clusters?learn / infer clusters?many clusters ? (surprisingly difficult!)can assess uncertainty clusters?know clusters also interested :features define / separate cluster?(note feature / variable selection problem, discussed supervised learning).Many problems questions highly specific data hand.\nCorrespondingly, many different types methods models clustering unsupervised learning.terms representing data, unsupervised learning tries balance following two extremes:objects grouped single cluster (low complexity model)objects put cluster (high complexity model)practise, aim find compromise, .e. model captures \nstructure data appropriate complexity — low complex.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"why-is-clustering-difficult","chapter":"3 Unsupervised learning and clustering","heading":"3.1.3 Why is clustering difficult?","text":"Partioning problem (combinatorics): many partitions \\(n\\) objects (say flowers) \\(K\\) groups (say species) exists?Answer:\\[\nS(n,K) = \\left\\{\\begin{array}{l} n \\\\ K \\end{array} \\right\\}\n\\]\n“Sterling number second type”.large n:\n\\[\nS(n,K) \\approx \\frac{K^n }{ K!}\n\\]\nExample:enormously big numbers even relatively small problems!\\(\\Longrightarrow\\) Clustering / partitioning / structure discovery easy!\\(\\Longrightarrow\\) expect perfect answers single “true” clusteringIn fact, model data many differnt clusterings may fit data equally well.\\(\\Longrightarrow\\) need assesse uncertainty clusteringThis can done part probabilistic modelling resampling (e.g., bootstrap).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"common-types-of-clustering-methods","chapter":"3 Unsupervised learning and clustering","heading":"3.1.4 Common types of clustering methods","text":"many different clustering algorithms!consider following two broad types methods:Algorithmic clustering methods (explicitly based probabilistic model)\\(K\\)-meansPAMhierarchical clustering (distance similarity-based, divise agglomerative)pros: fast, effective algorithms find least grouping\ncons: probabilistic interpretation, blackbox methodsModel-based clustering (based probabilistic model)mixture models (e.g. Gaussian mixture models, GMMs, non-hierarchical)graphical models (e.g. Bayesian networks, Gaussian graphical models GGM, trees networks)pros: full probabilistic model corresponding advantages\ncons: computationally expensive, sometimes impossible compute exactly.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"hierarchical-clustering","chapter":"3 Unsupervised learning and clustering","heading":"3.2 Hierarchical clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"tree-like-structures","chapter":"3 Unsupervised learning and clustering","heading":"3.2.1 Tree-like structures","text":"Often, categorisations objects nested, .e. sub-categories categories etc. can naturally represented tree-like hierarchical structures.many branches science hierarchical clusterings widely employed, example evolutionary biology: see e.g. Tree Life explaining biodiversity lifephylogenetic trees among species (e.g. vertebrata)population genetic trees describe human evolutiontaxonomic trees plant speciesetc.Note visualising hierarchical structures typically corresponding tree depicted facing downwards, .e. root tree shown top, tips/leaves tree shown bottom!order obtain hierarchical clustering data two opposing strategies commonly used:divisive recursive partitioning algorithms\ngrow tree root downwards\nfirst determine main two clusters, recursively refine clusters .\ngrow tree root downwardsfirst determine main two clusters, recursively refine clusters .agglomerative algorithms\ngrow tree leaves upwards\nsuccessively form partitions first joining individual object together,\nrecursively join groups items together, merged.\ngrow tree leaves upwardssuccessively form partitions first joining individual object together,\nrecursively join groups items together, merged.following discuss number popular hierarchical agglomerative clustering algorithms based pairwise distances / similarities (\\(n \\times n\\) matrix) among data points.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"agglomerative-hierarchical-clustering-algorithms","chapter":"3 Unsupervised learning and clustering","heading":"3.2.2 Agglomerative hierarchical clustering algorithms","text":"general algorithm agglomerative construction hierarchical clustering works follows:Initialisation:Compute dissimilarity / distance matrix pairs objects “objects” single data points stage later also sets data points.Iterative procedure:identify pair objects smallest distance. two objects merged together one set. Create internal node tree represent set.identify pair objects smallest distance. two objects merged together one set. Create internal node tree represent set.update distance matrix computing distances new set \nobjects. new set contains data points procedure terminates. final node created root node.update distance matrix computing distances new set \nobjects. new set contains data points procedure terminates. final node created root node.actual implementation algorithm two key ingredients needed:distance measure \\(d(\\boldsymbol , \\boldsymbol b)\\) two individual elementary data points \\(\\boldsymbol \\) \\(\\boldsymbol b\\).typically one following:Euclidean distance \\(d(\\boldsymbol , \\boldsymbol b) = \\sqrt{\\sum_{=1}^d ( a_i-b_i )^2} = \\sqrt{(\\boldsymbol -\\boldsymbol b)^T (\\boldsymbol -\\boldsymbol b)}\\)Squared Euclidean distance \\(d(\\boldsymbol , \\boldsymbol b) = (\\boldsymbol -\\boldsymbol b)^T (\\boldsymbol -\\boldsymbol b)\\)Manhattan distance \\(d(\\boldsymbol , \\boldsymbol b) = \\sum_{=1}^d | a_i-b_i |\\)Maximum norm \\(d(\\boldsymbol , \\boldsymbol b) = \\underset{\\\\{1, \\ldots, d\\}}{\\max} | a_i-b_i |\\)end, making correct choice distance require subject knowledge data!distance measure \\(d(, B)\\) two sets objects \\(=\\{\\boldsymbol a_1, \\boldsymbol a_2, \\ldots, \\boldsymbol a_{n_A} \\}\\) \\(B=\\{\\boldsymbol b_1, \\boldsymbol b_2, \\ldots, \\boldsymbol b_{n_B}\\}\\) size \\(n_A\\) \\(n_B\\), respectively.determine distance \\(d(, B)\\) two sets following measures often employed:complete linkage (max. distance): \\(d(, B) = \\underset{\\boldsymbol a_i \\, \\boldsymbol b_i \\B}{\\max} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)single linkage (min. distance): \\(d(, B) = \\underset{\\boldsymbol a_i \\, \\boldsymbol b_i \\B}{\\min} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)average linkage (avg. distance): \\(d(, B) = \\frac{1}{n_A n_B} \\sum_{\\boldsymbol a_i \\} \\sum_{\\boldsymbol b_i \\B} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"wards-clustering-method","chapter":"3 Unsupervised learning and clustering","heading":"3.2.3 Ward’s clustering method","text":"Another agglomerative hierarchical procedure Ward’s minimum variance approach11 (see also12). approach iteration two sets \\(\\) \\(B\\) merged lead smallest increase within-group variation. centroids two sets given \\(\\boldsymbol \\mu_A = \\frac{1}{n_A} \\sum_{\\boldsymbol a_i \\} \\boldsymbol a_i\\) \\(\\boldsymbol \\mu_B = \\frac{1}{n_B} \\sum_{\\boldsymbol b_i \\B} \\boldsymbol b_i\\).within-group sum squares group \\(\\) \n\\[\nw_A = \\sum_{\\boldsymbol a_i \\} (\\boldsymbol a_i -\\boldsymbol \\mu_A)^T (\\boldsymbol a_i -\\boldsymbol \\mu_A)\n\\]\ncomputed basis difference observations \\(\\boldsymbol a_i\\) relative mean \\(\\boldsymbol \\mu_A\\).\nHowever, since typically pairwise distances available don’t know group means formula can’t applied.\nFortunately, also possible compute \\(w_A\\) using pairwise differences using\n\\[\nw_A = \\frac{1}{n_A} \\sum_{\\boldsymbol a_i, \\boldsymbol a_j \\, < j} (\\boldsymbol a_i -\\boldsymbol a_j)^T (\\boldsymbol a_i -\\boldsymbol a_j)\n\\]\ntrick employed Ward’s clustering method constructing distance measure two sets \\(\\) \\(B\\) \n\\[\nd(, B) = w_{\\cup B} - w_A -w_B \\, \n\\]\nusing distance two elementary data points \\(\\boldsymbol \\) \\(\\boldsymbol b\\) squared Euclidean distance\n\\[\nd(\\boldsymbol , \\boldsymbol b) = (\\boldsymbol - \\boldsymbol b)^T (\\boldsymbol - \\boldsymbol b) \\, .\n\\]","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-to-swiss-banknote-data-set","chapter":"3 Unsupervised learning and clustering","heading":"3.2.4 Application to Swiss banknote data set","text":"data set reports 6 pysical measurements 200 Swiss bank notes. 200 notes\n100 genuine 100 counterfeit. measurements : length, left width, right width, bottom margin, top margin, diagonal length bank notes.Plotting first PCAs data shows indeed two well defined groups,\ngroups correspond precisely genuine counterfeit banknotes:now compare hierarchical clusterings Swiss bank note data using four different methods using Euclidean distance.interactive R Shiny web app analysis (also allows explore distance measures) available\nonline https://minerva..manchester.ac.uk/shiny/strimmer/hclust/ .Ward.D2 (=Ward’s method):Average linkage:Complete linkage:Single linkage:Result:four trees / hierarchical clusterings quite different!Ward.D2 method one finds correct grouping (except single error).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"assessment-of-the-uncertainty-of-hierarchical-clusterings","chapter":"3 Unsupervised learning and clustering","heading":"3.2.5 Assessment of the uncertainty of hierarchical clusterings","text":"practical application hierarchical clustering methods essential evaluate stability uncertainty obtained groupings. often done follows using “bootstrap”:Sampling replacement used generate number -called bootstrap data sets (say \\(B=200\\)) similar original one. Specifically, create new data matrices repeately randomly selecting columns (variables) original data matrix inclusion bootstrap data matrix. Note sample columns aim cluster samples.Subsequently, hierarchical clustering computed bootstrap data sets. result, now “ensemble” \\(B\\) bootstrap trees.Finally, analysis clusters (bipartions) shown bootstrap trees allows count clusters appear frequently, also appear less frequently. counts provide measure stability clusterings appearing original tree.Additionally, bootstrap tree can also compute consensus tree containing stable clusters. viewed “ensemble average” bootstrap trees.disadvantage procedure bootstrapping trees computationally expensive, original procedure already time consuming now needs repeated large number times.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"k-means-clustering","chapter":"3 Unsupervised learning and clustering","heading":"3.3 \\(K\\)-means clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"set-up","chapter":"3 Unsupervised learning and clustering","heading":"3.3.1 Set-up","text":"assume \\(K\\) groups (.e. \\(K\\) known advance).group \\(k \\\\{1, \\ldots, K\\}\\) assume group mean \\(\\boldsymbol \\mu_k\\).Aim: partition data points \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) \\(K\\) non-overlapping groups.\\(n\\) data points \\(\\boldsymbol x_i\\) assigned exactly one \\(K\\) groups.Maximise homogeneity within group (.e. group contain similar objects).Maximise heterogeneity different groups (.e group differ groups).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"algorithm","chapter":"3 Unsupervised learning and clustering","heading":"3.3.2 Algorithm","text":"running \\(K\\)-means get estimates \\(\\hat{\\boldsymbol \\mu}_k\\) group means,\nwell allocations \\(y_i \\\\{1, \\ldots, K\\}\\) data point \\(\\boldsymbol x_i\\) one classes.Initialisation:start algorithm \\(n\\) observations \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) randomly allocated equal probability one \\(K\\) groups. resulting assignment \\(y_1, \\ldots, y_n\\), \\(y_i=\\{1, \\ldots, K\\}\\).\n\\(G_k = \\{ | y_i = k\\}\\) denote set indices data points cluster \\(k\\), \\(n_k = | G_k |\\) \nnumber samples cluster \\(k\\).Iterative refinement:Estimate group means \n\\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n_k} \\sum_{\\G_k} \\boldsymbol x_i\n\\]Update group allocations \\(y_i\\). Specifically, assign data point \\(\\boldsymbol x_i\\) group \\(k\\) nearest \\(\\hat{\\boldsymbol \\mu}_k\\). distance measured terms Euclidean norm:\n\\[\n\\begin{split}\ny_i & = \\underset{k}{\\arg \\min} \\,  \\left| \\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k \\right|_2 \\\\\n      & = \\underset{k}{\\arg \\min} \\, \\left(\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k\\right)^T \\left(\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k\\right) \\\\\n\\end{split}\n\\]Steps 1 2 repeated algorithm converges (.e. group allocations don’t change ) specified upper limit iterations reached.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"properties","chapter":"3 Unsupervised learning and clustering","heading":"3.3.3 Properties","text":"\\(K\\)-means proposed 1950 1970s various authors diverse contexts.13\nDespite simplicity \\(K\\)-means , perhaps surprisingly, effective clustering algorithm.\nmain reason close connection \\(K\\)-means probabilistic clustering based Gaussian mixture models (details see later section).Since clustering depends initialisation often useful run \\(K\\)-means several\ntimes different starting group allocations data points. Furthermore, non-random non-uniform\ninitialisations can lead improved faster convergence, see\nK-means++ algorithm.clusters constructed \\(K\\)-means linear boundaries thus form \nVoronoi tessellation around cluster means.\n, can explained close link \\(K\\)-means particular Gaussian mixture model.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"choosing-the-number-of-clusters","chapter":"3 Unsupervised learning and clustering","heading":"3.3.4 Choosing the number of clusters","text":"\\(K\\)-means algorithm run can assess homogeneity \nheterogeneity resulting clusters:total within-group sum squares \\(SSW\\) (R: tot.withinss), total unexplained sum squares:\n\\[\nSSW = \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)\n\\]\nquantity decreases \\(K\\) zero \\(K=n\\).\n\\(K\\)-means algorithm tries minimise quantity typically find local minimum rather global one.total within-group sum squares \\(SSW\\) (R: tot.withinss), total unexplained sum squares:\n\\[\nSSW = \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)\n\\]\nquantity decreases \\(K\\) zero \\(K=n\\).\n\\(K\\)-means algorithm tries minimise quantity typically find local minimum rather global one.-group sum squares \\(SSB\\) (R: betweenss), explained sum squares:\n\\[\nSSB = \\sum_{k=1}^K n_k (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)^T (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)\n\\]\n\\(\\hat{\\boldsymbol \\mu}_0 = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = \\frac{1}{n} \\sum_{k=1}^K n_k \\hat{\\boldsymbol \\mu}_k\\)\nglobal mean samples. \\(SSB\\) increases number clusters \\(K\\) \\(K=n\\) \nbecomes equal total sum squares \\(SST\\).-group sum squares \\(SSB\\) (R: betweenss), explained sum squares:\n\\[\nSSB = \\sum_{k=1}^K n_k (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)^T (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)\n\\]\n\\(\\hat{\\boldsymbol \\mu}_0 = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = \\frac{1}{n} \\sum_{k=1}^K n_k \\hat{\\boldsymbol \\mu}_k\\)\nglobal mean samples. \\(SSB\\) increases number clusters \\(K\\) \\(K=n\\) \nbecomes equal total sum squares \\(SST\\).total sum squares\n\\[\nSST = \\sum_{=1}^n (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)^T (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0) \\, .\n\\]\nconstruction \\(SST = SSB + SSW\\) \\(K\\) (.e. \\(SST\\) constant independent \\(K\\)).total sum squares\n\\[\nSST = \\sum_{=1}^n (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)^T (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0) \\, .\n\\]\nconstruction \\(SST = SSB + SSW\\) \\(K\\) (.e. \\(SST\\) constant independent \\(K\\)).Dividing sum squares sample size \\(n\\) get\\(T = \\frac{SST}{n}\\) total variation,\\(B = \\frac{SSW}{n}\\) explained variation \\(W = \\frac{SSW}{n}\\) total unexplained variation ,\\(T = B + W\\).order decide optimal number clusters run \\(K\\)-means different settings \\(K\\) choose smallest \\(K\\) explained variation \\(B\\) significantly worse compared clustering substantially larger \\(K\\) (see example ).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"k-medoids-aka-pam","chapter":"3 Unsupervised learning and clustering","heading":"3.3.5 \\(K\\)-medoids aka PAM","text":"closely related clustering method \\(K\\)-medoids PAM (“Partitioning Around Medoids”).works exactly like \\(K\\)-means, thatinstead estimated group means \\(\\hat{\\boldsymbol \\mu}_k\\) one member group selected representative (-called “medoid”)instead squared Euclidean distance dissimilarity measures also allowed.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-of-k-means-to-iris-data","chapter":"3 Unsupervised learning and clustering","heading":"3.3.6 Application of \\(K\\)-means to Iris data","text":"Scatter plots Iris data:R output \\(K\\)-means analysis known true number clusters specified (\\(K=3\\)) :corresponding total within-group sum squares (\\(SSW\\), tot.withinss)\nisand -group sum squares (\\(SSB\\), betweenss) isBy comparing known class assignments can assess accuracy \\(K\\)-means clustering:choosing \\(K\\) run \\(K\\)-means several times compute\nwithin cluster variation dependence \\(K\\):Thus, \\(K=3\\) clusters seem appropriate since explained variation significantly improve\n(unexplained variation significantly decrease) increase number clusters.","code":"\nkmeans.out = kmeans(X.iris, 3)\nkmeans.out## K-means clustering with 3 clusters of sizes 34, 93, 23\n## \n## Cluster means:\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1   -0.8195329   1.2855175   -1.2874345  -1.2184456\n## 2    0.5982328  -0.3387642    0.7127006   0.6892835\n## 3   -1.2074580  -0.5305443   -0.9786251  -0.9859225\n## \n## Clustering vector:\n##   [1] 1 3 3 3 1 1 1 1 3 3 1 1 3 3 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 3 3 1 1 1 3 1 1\n##  [38] 1 3 1 1 3 3 1 1 3 1 3 1 1 2 2 2 2 2 2 2 3 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [75] 2 2 2 2 2 3 3 3 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2\n## [112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [149] 2 2\n## \n## Within cluster sum of squares by cluster:\n## [1]  18.35610 139.43520  32.58388\n##  (between_SS / total_SS =  68.1 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nkmeans.out$tot.withinss## [1] 190.3752\nkmeans.out$betweenss## [1] 405.6248\ntable(L.iris, kmeans.out$cluster)##             \n## L.iris        1  2  3\n##   setosa     34  0 16\n##   versicolor  0 43  7\n##   virginica   0 50  0"},{"path":"unsupervised-learning-and-clustering.html","id":"arbitrariness-of-cluster-labels-and-label-switching","chapter":"3 Unsupervised learning and clustering","heading":"3.3.7 Arbitrariness of cluster labels and label switching","text":"important realise unsupervised learning clustering labels group assigned arbitrary fashion.\nRecall \\(K\\) groups \\(K!\\) possibilities attach \nlabels, corresponding number permutations \\(K\\) groups.Thus, different runs clustering algorithm \\(K\\)-means may return clustering (groupings samples) different labels. phenomenon called “label switching”\nmakes difficult automatise comparison clusterings. particular, one simply rely automatically assigned group label, instead one needs compare actual members clusters.way resolve problem label switching relabel clusters using additional information, requiring samples specific groups\n(e.g.: sample 1 always group labelled “1”), /linking labels orderings constraints group characteristics (e.g.: group label “1” always smaller mean group label “2”).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"mixture-models","chapter":"3 Unsupervised learning and clustering","heading":"3.4 Mixture models","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"finite-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.4.1 Finite mixture model","text":"\\(K\\) groups / classes / categories, finite \\(K\\) known advance.Probability class \\(k\\): \\(\\text{Pr}(k) = \\pi_k\\) \\(\\sum_{k=1}^K \\pi_k = 1\\).class \\(k \\C= \\{1, \\ldots, K\\}\\) modelled distribution \\(F_k\\) parameters \\(\\boldsymbol \\theta_k\\).Density class \\(k\\): \\(f_k(\\boldsymbol x) = f(\\boldsymbol x| k)\\).conditional means variances class \\(k \\C\\) \\(\\text{E}(\\boldsymbol x| k) = \\boldsymbol \\mu_k\\) \\(\\text{Var}(\\boldsymbol x| k) = \\boldsymbol \\Sigma_k\\).resulting mixture density observed variable \\(\\boldsymbol x\\) \n\\[\nf_{\\text{mix}}(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\n\\]often one uses multivariate normal components \\(f_k(\\boldsymbol x) = N(\\boldsymbol x| \\boldsymbol \\mu_k, \\boldsymbol \\Sigma_k)\\) \\(\\\\ \\Longrightarrow\\) Gaussian mixture model (GMM)Mixture models fundamental just clustering many applications (e.g. classification).Note: don’t confuse mixture model mixed model (= terminology random effects regression model).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"total-mean-and-variance-of-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.4.2 Total mean and variance of mixture model","text":"Using law total expectation obtain mean mixture density follows:\n\\[\n\\begin{split}\n\\text{E}(\\boldsymbol x) & = \\text{E}(\\text{E}(\\boldsymbol x| k)) \\\\\n            & = \\sum_{k=1}^K \\pi_k \\boldsymbol \\mu_k \\\\\n            &= \\boldsymbol \\mu_0 \\\\\n\\end{split}\n\\]Similarly, using law total variance compute marginal variance:\n\\[\n\\begin{split}\n\\underbrace{\\text{Var}(\\boldsymbol x)}_{\\text{total}} & =  \\underbrace{ \\text{Var}( \\text{E}(\\boldsymbol x| k )  )}_{\\text{explained / -group}} + \\underbrace{\\text{E}(\\text{Var}(\\boldsymbol x|k))}_{\\text{unexplained / within-group}} \\\\\n\\boldsymbol \\Sigma_0 & =  \\sum_{k=1}^K \\pi_k (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T + \\sum_{k=1}^K \\pi_k \\boldsymbol \\Sigma_k  \\\\\n\\end{split}\n\\]Thus, total variance decomposes explained\n(group) variance unexplained (within group) variance. \ndecomposition linear regression (see MATH20802 Statistical Methods).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"total-variation","chapter":"3 Unsupervised learning and clustering","heading":"3.4.3 Total variation","text":"total variation given trace covariance matrix. decomposition total variation \n\\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol \\Sigma_0) & =  \\sum_{k=1}^K \\pi_k \\text{Tr}((\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T) + \\sum_{k=1}^K \\pi_k \\text{Tr}(\\boldsymbol \\Sigma_k)  \\\\\n& =  \\sum_{k=1}^K \\pi_k (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) + \\sum_{k=1}^K \\pi_k \\text{Tr}(\\boldsymbol \\Sigma_k)\\\\\n\\end{split}\n\\]\ncovariances replaced empirical estimates obtain\n\\(T=B+W\\) decomposition total variation familiar \\(K\\)-means:\n\\[T = \\text{Tr}\\left( \\hat{\\boldsymbol \\Sigma}_0 \\right)  = \n\\frac{1}{n} \\sum_{=1}^n (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)^T (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)\\]\n\\[B = \\frac{1}{n} \\sum_{k=1}^K n_k (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)^T (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)\\]\n\\[W = \\frac{1}{n}  \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)\n\\]","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"univariate-mixture","chapter":"3 Unsupervised learning and clustering","heading":"3.4.4 Univariate mixture","text":"univariate mixture (\\(d=1\\)) \\(K=2\\) components get\n\\[\n\\mu_0 = \\pi_1 \\mu_1+ \\pi_2 \\mu_2 \\, ,\n\\]\n\\[\n\\sigma^2_{\\text{within}} = \\pi_1 \\sigma^2_1 + \\pi_2 \\sigma^2_2 = \\sigma^2_{\\text{pooled}}\\,,\n\\]\nalso know pooled variance, \n\\[\n\\begin{split}\n\\sigma^2_{\\text{}} &= \\pi_1 (\\mu_1 - \\mu_0)^2 + \\pi_2 (\\mu_2 - \\mu_0)^2 \\\\\n& =\\pi_1 \\pi_2^2 (\\mu_1 - \\mu_2)^2 + \\pi_2 \\pi_1^2 (\\mu_1 - \\mu_2)^2\\\\\n& = \\pi_1 \\pi_2 (\\mu_1 - \\mu_2)^2  \\\\\n& = \\left( \\frac{1}{\\pi_1} + \\frac{1}{\\pi_2}   \\right)^{-1} (\\mu_1 - \\mu_2)^2 \\\\\n\\end{split} \\,.\n\\]\nratio -group variance within-group variance proportional\n(factor \\(n\\)) squared pooled-variance \\(t\\)-score:\n\\[\n\\frac{\\sigma^2_{\\text{}}}{\\sigma^2_{\\text{within}}} =\n  \\frac{ (\\mu_1 - \\mu_2)^2}{ \\left(\\frac{1}{\\pi_1} + \\frac{1}{\\pi_2}   \\right)  \\sigma^2_{\\text{pooled}} }= \\frac{t_{\\text{pooled}}^2}{n}\n\\]\nfamiliar ANOVA (e.g. linear models course) recognise ratio \\(F\\)-score.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"example-of-mixtures","chapter":"3 Unsupervised learning and clustering","heading":"3.4.5 Example of mixtures","text":"Mixtures can take many different shapes forms, instructive study examples.\ninteractive tool visualise two component normal mixture available online \nR Shiny web app https://minerva..manchester.ac.uk/shiny/strimmer/mixture/ .first plot shows bimodal density mixture distribution consisting two normals \\(\\pi_1=0.7\\),\n\\(\\mu_1=-1\\), \\(\\mu_2=2\\) two variances equal 1 (\\(\\sigma^2_1 = 1\\) \\(\\sigma^2_2 = 1\\)).\ntwo components well-separated two clear modes. plot also shows density normal distribution total mean (\\(\\mu_0=-0.1\\)) variance (\\(\\sigma_0^2=2.89\\)) mixture distribution. Clearly total normal mixture density different.However, two-component mixtures can also unimodal. example, mean second component adjusted \\(\\mu_2=0\\) single mode total normal density \\(\\mu_0=-0.7\\) \\(\\sigma_0^2=1.21\\) now almost inistinguishable form mixture density.\nThus, case hard (even impossible) identify two peaks data.mixtures consider course multivariate.\nillustration, plot mixture two bivariate normals,\n\\(\\pi_1=0.7\\), \\(\\boldsymbol \\mu_1 = \\begin{pmatrix}-1 \\\\1 \\\\ \\end{pmatrix}\\),\n\\(\\boldsymbol \\Sigma_1 = \\begin{pmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\\\ \\end{pmatrix}\\),\n\\(\\boldsymbol \\mu_2 = \\begin{pmatrix}2.5 \\\\0.5 \\\\ \\end{pmatrix}\\) \\(\\boldsymbol \\Sigma_2 = \\begin{pmatrix} 1 & -0.7 \\\\ -0.7 & 1 \\\\ \\end{pmatrix}\\):","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"sampling-from-a-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.4.6 Sampling from a mixture model","text":"Assuming know sample component densities \\(f_k(\\boldsymbol x)\\) mixture model straightforward set procedure sampling mixture \\(f_{\\text{mix}}(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\\) .done two-step process:Draw categorical distribution parameter \\(\\boldsymbol \\pi=(\\pi_1, \\ldots, \\pi_K)^T\\):\n\\[\\boldsymbol z\\sim \\text{Cat}(\\boldsymbol \\pi)\\]\nvector \\(\\boldsymbol z= (z_1, \\ldots, z_K)^T\\) indicates hard group 0/1 allocation, components \\(z_{\\neq k}=0\\) except single entry \\(z_k=1\\).Draw categorical distribution parameter \\(\\boldsymbol \\pi=(\\pi_1, \\ldots, \\pi_K)^T\\):\n\\[\\boldsymbol z\\sim \\text{Cat}(\\boldsymbol \\pi)\\]\nvector \\(\\boldsymbol z= (z_1, \\ldots, z_K)^T\\) indicates hard group 0/1 allocation, components \\(z_{\\neq k}=0\\) except single entry \\(z_k=1\\).Subsequently, sample component \\(k\\) selected step 1:\n\\[\n\\boldsymbol x\\sim F_k\n\\]Subsequently, sample component \\(k\\) selected step 1:\n\\[\n\\boldsymbol x\\sim F_k\n\\]two-stage sampling approach also known hierarchical generative model mixture distribution. generative view useful simulating data mixture model also highlights role latent variable (class allocation).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"fitting-mixture-models-to-data-and-inferring-the-latent-states","chapter":"3 Unsupervised learning and clustering","heading":"3.5 Fitting mixture models to data and inferring the latent states","text":"following denote \\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)^T\\) data matrix containing observations \\(n\\) independent identically distributed samples\n\\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\), \\(\\boldsymbol y= (y_1, \\ldots, y_n)^T\\) associated group memberships, well asthe parameters \\(\\boldsymbol \\theta\\) Gaussian mixture model \\(\\boldsymbol \\theta= \\{\\boldsymbol \\pi, \\boldsymbol \\mu_1, \\ldots, \\boldsymbol \\mu_K, \\boldsymbol \\Sigma_1, \\ldots, \\boldsymbol \\Sigma_K\\}\\).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"observed-and-latent-variables","chapter":"3 Unsupervised learning and clustering","heading":"3.5.1 Observed and latent variables","text":"observe data mixture model collect samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).\nAssociated observed \\(\\boldsymbol x_i\\) corresponding underlying class allocation \\(y_1, \\ldots, y_n\\) \n\\(y_i\\) takes value \\(C = \\{1, \\ldots, K\\}\\). Crucially, class allocations \\(y_i\\) unknown \ndirectly observed, thus latent.joint density observed unobserved variables:\n\\[f(\\boldsymbol x, y) = f(\\boldsymbol x| y) \\text{Pr}(y) = f_k(\\boldsymbol y) \\pi_y\\]mixture density therefore marginal density arises joint density \\(f(\\boldsymbol x, y)\\)\nmarginalising discrete variable \\(y\\).Marginalisation: \\(f(\\boldsymbol x) = \\sum_{y \\C} f(\\boldsymbol x, y)\\)","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"complete-data-likelihood-and-observed-data-likelihood","chapter":"3 Unsupervised learning and clustering","heading":"3.5.2 Complete data likelihood and observed data likelihood","text":"know \\(\\boldsymbol y\\) advance, .e. know sample belongs particular group,\ncan construct complete data log-likelihood\nbased joint distribution \\(f(\\boldsymbol x, y) = \\pi_y f_y(\\boldsymbol x)\\).\nlog-likelihood \\(\\boldsymbol \\theta\\) given \\(\\boldsymbol X\\) \\(\\boldsymbol y\\) \n\\[\n\\log L(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol y) = \\sum_{=1}^n \\log f(\\boldsymbol x_i, y_i)  =  \\sum_{=1}^n  \\log \\left(\\pi_{y_i} f_{y_i}(\\boldsymbol x_i) \\right) \n\\]hand, typically know \\(\\boldsymbol y\\) therefore use\nmarginal mixture density \\(f(\\boldsymbol x)\\) construct observed data log-likelihood\n(sometimes also called incomplete data log-likelihood) \\(f(\\boldsymbol x| \\boldsymbol \\theta)\\) \n\\[\n\\begin{split}\n\\log L(\\boldsymbol \\theta| \\boldsymbol X) & =\\sum_{=1}^n \\log f(\\boldsymbol x_i | \\boldsymbol \\theta)\\\\\n& = \\sum_{=1}^n \\log \\left( \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x_i)  \\right)\\\\\n\\end{split}\n\\]observed data log-likelihood can also computed complete data likelihood\nfunction marginalising \\(\\boldsymbol y\\)\n\\[\n\\begin{split}\n\\log L(\\boldsymbol \\theta| \\boldsymbol X) &= \\log \\sum_{\\boldsymbol y}   L(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol y)\\\\\n &= \\log \\sum_{y_1, \\ldots, y_K}  \\prod_{=1}^n f(\\boldsymbol x_i, y_i)\\\\\n&= \\log \\prod_{=1}^n  \\sum_{k=1}^K f(\\boldsymbol x_i, k)\\\\\n& = \\sum_{=1}^n \\log \\left(  \\sum_{k=1}^K f(\\boldsymbol x_i, k)     \\right)\n\\end{split} \n\\]Clustering mixture model can viewed incomplete missing data problem\n(see also part II MATH20802 Statistical Methods ).Specifically, face problem offitting model using observed data \\(\\boldsymbol X\\) andsimultaneously inferring class allocations \\(\\boldsymbol y\\), .e. states latent variable.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"fitting-the-mixture-model-to-the-observed-data","chapter":"3 Unsupervised learning and clustering","heading":"3.5.3 Fitting the mixture model to the observed data","text":"large sample size \\(n\\) standard way fit mixture model\nemploy maximum likelihood find MLEs parameters mixture model.direct way fit mixture model maximum likelihood maximise observed data log-likelihood function regard \\(\\boldsymbol \\theta\\):\n\\[\n\\hat{\\boldsymbol \\theta}^{ML} = \\underset{\\boldsymbol \\theta}{\\arg \\max}\\,\\, \\log L(\\boldsymbol \\theta| \\boldsymbol X)\n\\]Unfortunately, practise evaluation optimisation log-likelihood function can difficult due number reasons:form observed data log-likelihood function prevents analytic simplifications\n(note sum inside logarithm) thus can difficult compute.symmetries due exchangeability cluster labels likelihood function multimodal thus hard optimise. Note also linked general\nproblem label switching non-identifiability cluster labels — see discussion \\(K\\)-means clustering.identifiability issues can arise (instance) two neighboring components mixture model largely overlapping thus close discriminated two different modes. words, difficult determine number classes.Furthermore, likelihood Gaussian mixture models singular one fitted covariance matrices becomes singular. However, can easily adressed using form regularisation (Bayes, penalised ML, etc.) simply requiring sufficient sample size per group.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"predicting-the-group-allocation-of-a-given-sample","chapter":"3 Unsupervised learning and clustering","heading":"3.5.4 Predicting the group allocation of a given sample","text":"probabilistic clustering aim infer latent states \\(y_1, \\ldots, y_n\\) observed samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).Assuming mixture model known (either advance fitting ) Bayes’ theorem allows predict probability observation \\(\\boldsymbol x_i\\) falls group \\(k \\\\{1, \\ldots, K\\}\\):\n\\[\nq_i(k) = \\text{Pr}(k | \\boldsymbol x_i) = \\frac{\\pi_k f_k(\\boldsymbol x_i ) }{ f(\\boldsymbol x_i)}\n\\]\nThus, \\(n\\) samples get probability mass function \n\\(K\\) classes \\(\\sum_{k=1}^K q_i(k)=1\\).posterior probabilities \\(q_i(k)\\) provide -called soft assignment sample \\(\\boldsymbol x_i\\) classes rather 0/1 hard assignment specific class (example \\(K\\)-means algorithm).obtain hard clustering infer probable latent state select class highest probability\n\\[\ny_i =\\underset{k}{\\arg \\max}\\,\\,q_i(k)\n\\]Thus, probabilistic clustering directly obtain assessment uncertainty class assignment sample \\(\\boldsymbol x_i\\) (case simple algorithmic clustering \\(K\\)-means). can use information check whether several classes equal similar probability. case, e.g., \\(\\boldsymbol x_i\\) lies near boundary two neighbouring classes.Using interactive Shiny app univariate normal component mixture (online https://minerva..manchester.ac.uk/shiny/strimmer/mixture/ ) can explore posterior probabilities class.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-of-gaussian-mixture-models","chapter":"3 Unsupervised learning and clustering","heading":"3.6 Application of Gaussian mixture models","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"choosing-the-number-of-classes","chapter":"3 Unsupervised learning and clustering","heading":"3.6.1 Choosing the number of classes","text":"application GMM need select suitable value \\(K\\), .e. number classe.Since GMMs operate likelihood framework can use penalised likelihood model selection criteria choose among different models (.e. GMMs different numbers classes).popular choices AIC (Akaike Information Criterion) BIC (Bayesian Information criterion) defined follows:\n\\[\\text{AIC}= -2 \\log L + 2 K \\]\n\\[\\text{BIC}= - 2 \\log L +K \\log(n)\\]order choose suitable model evaluate different models different \\(K\\) choose model minimises \\(\\text{AIC}\\) \\(\\text{BIC}\\)Note criteria complex models parameters (case groups) penalised simpler models order prevent overfitting.Another way choosing optimal numbers clusters cross-validation (see later chapter supervised learning).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-of-gmms-to-iris-flower-data","chapter":"3 Unsupervised learning and clustering","heading":"3.6.2 Application of GMMs to Iris flower data","text":"now explore application Gaussian mixture models Iris flower data set also investigated PCA\nK-means.First, fit GMM 3 clusters, using R software “mclust.”14The “mclust” software used following model fitting mixture:“VVV” name used “mclust” software model\nallowing individual\nunrestricted covariance matrix \\(\\boldsymbol \\Sigma_k\\) class \\(k\\).GMM substantially lower misclassification error compared \\(K\\)-means number clusters:Note “mclust” BIC criterion defined opposite sign (\\(\\text{BIC}_{\\text{mclust}} = 2 \\log L -K \\log(n)\\)), thus need find maximum value rather smallest value.compute BIC various numbers groups find model best \\(\\text{BIC}_{\\text{mclust}}\\) model 2 clusters model 3 cluster nearly good BIC:","code":"\ndata(iris)\nX.iris = scale((iris[, 1:4]), scale=TRUE) # center and standardise\nL.iris = iris[, 5]\n\nlibrary(\"mclust\")\ngmm3 = Mclust(X.iris, G=3, verbose=FALSE)\nplot(gmm3, what=\"classification\")\ngmm3$modelName## [1] \"VVV\"\ntable(gmm3$classification, L.iris)##    L.iris\n##     setosa versicolor virginica\n##   1     50          0         0\n##   2      0         45         0\n##   3      0          5        50"},{"path":"unsupervised-learning-and-clustering.html","id":"the-em-algorithm","chapter":"3 Unsupervised learning and clustering","heading":"3.7 The EM algorithm","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"motivation","chapter":"3 Unsupervised learning and clustering","heading":"3.7.1 Motivation","text":"discussed , observed data log-likelihood can difficult maximise directly due form log marginal likelihood. Intriguingly, possible optimise indirectly using complete data log-likelihood!method called EM algorithm15 formaly proposed Arthur Dempster others 1977. iteratively estimates \nparameters mixture model parameters latent states\nkey idea behind EM algorithm exploits simplicity complete data likelihood obtain estimates \\(\\boldsymbol \\theta\\) imputing missing group allocations\nsubsequently iteratively refining imputations estimates \\(\\boldsymbol \\theta\\).precisely, EM (=expectation-maximisation) algorithm alternate betweenupdating soft allocations sample using current estimate parameters \\(\\boldsymbol \\theta\\) (obtained step 2)updating parameter estimates maximising expected complete data log-likelihood. expectation taken regard distribution latent states (obtained step 1). Thus\ncomplete data log-likelihood averaged soft class assignments.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"the-em-algorithm-1","chapter":"3 Unsupervised learning and clustering","heading":"3.7.2 The EM algorithm","text":"Specifically, EM algorithm proceeds follows:Initialisation:Start guess parameters \\(\\hat{\\boldsymbol \\theta}^{(1)}\\), continue “E” Step, Part .Alternatively, start guess soft allocations sample \\(q_i(k)^{(1)}\\), collected matrix \\(\\boldsymbol Q^{(1)}\\), continue “E” Step, Part B.\nmay derived prior information, e.g., running \\(K\\)-means. Caveat: particular initialisations correspond invariant states hence avoided (see ).E “expectation” stepPart : Use Bayes’ theorem compute new probabilities allocation class \\(k\\) samples \\(\\boldsymbol x_i\\):\n\\[\nq_i(k)^{(b+1)} \\leftarrow \\frac{ \\hat{\\pi}_k^{(b)} f_k(\\boldsymbol x_i | \\hat{\\boldsymbol \\theta}^{(b)})    }{  f(\\boldsymbol x_i |\\hat{\\boldsymbol \\theta}^{(b)} )  }\n\\]\nNote obtain \\(q_i(k)^{(b+1)}\\) current estimate\n\\(\\hat{\\boldsymbol \\theta}^{(b)}\\) parameters mixture model required.Part : Use Bayes’ theorem compute new probabilities allocation class \\(k\\) samples \\(\\boldsymbol x_i\\):\n\\[\nq_i(k)^{(b+1)} \\leftarrow \\frac{ \\hat{\\pi}_k^{(b)} f_k(\\boldsymbol x_i | \\hat{\\boldsymbol \\theta}^{(b)})    }{  f(\\boldsymbol x_i |\\hat{\\boldsymbol \\theta}^{(b)} )  }\n\\]\nNote obtain \\(q_i(k)^{(b+1)}\\) current estimate\n\\(\\hat{\\boldsymbol \\theta}^{(b)}\\) parameters mixture model required.Part B: Construct expected complete data log-likelihood function \\(\\boldsymbol \\theta\\) using soft allocations \\(q_i(k)^{(b+1)}\\)\ncollected matrix \\(\\boldsymbol Q^{(b+1)}\\):\n\\[\nG(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)} ) = \\sum_{=1}^n \\sum_{k=1}^K q_i(k)^{(b+1)}  \\log \\left( \\pi_k f_k(\\boldsymbol x_i) \\right)\n\\]\nNote case soft allocations \\(\\boldsymbol Q^{(b+1)}\\) turn hard 0/1 allocations \n\\(G(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)})\\) becomes equivalent complete data log-likelihood.Part B: Construct expected complete data log-likelihood function \\(\\boldsymbol \\theta\\) using soft allocations \\(q_i(k)^{(b+1)}\\)\ncollected matrix \\(\\boldsymbol Q^{(b+1)}\\):\n\\[\nG(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)} ) = \\sum_{=1}^n \\sum_{k=1}^K q_i(k)^{(b+1)}  \\log \\left( \\pi_k f_k(\\boldsymbol x_i) \\right)\n\\]\nNote case soft allocations \\(\\boldsymbol Q^{(b+1)}\\) turn hard 0/1 allocations \n\\(G(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)})\\) becomes equivalent complete data log-likelihood.M “maximisation” step — Maximise expected complete data log-likelihood update estimates mixture model parameters:\n\\[\n\\hat{\\boldsymbol \\theta}^{(b+1)} \\leftarrow \\arg \\max_{\\boldsymbol \\theta}  G(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)} )\n\\]M “maximisation” step — Maximise expected complete data log-likelihood update estimates mixture model parameters:\n\\[\n\\hat{\\boldsymbol \\theta}^{(b+1)} \\leftarrow \\arg \\max_{\\boldsymbol \\theta}  G(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)} )\n\\]Continue 2) “E” Step series \\(\\hat{\\boldsymbol \\theta}^{(1)}, \\hat{\\boldsymbol \\theta}^{(2)}, \\hat{\\boldsymbol \\theta}^{(3)}, \\ldots\\) converged.Continue 2) “E” Step series \\(\\hat{\\boldsymbol \\theta}^{(1)}, \\hat{\\boldsymbol \\theta}^{(2)}, \\hat{\\boldsymbol \\theta}^{(3)}, \\ldots\\) converged.Since maximisation expected complete data log-likelihood typically much easier (often also analytically tractable) EM algorithm often preferred direct\nmaximisation observed data log-likelihood.Note avoid singularities expected log-likelihood function \nmay need adopt regularisation (.e. penalised maximum likelihood Bayesian learning) estimating parameters M-step.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"em-algorithm-for-multivariate-normal-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.7.3 EM algorithm for multivariate normal mixture model","text":"Gaussian mixture model (GMM) steps EM algorithm can expressed analytically:E-step:Update soft allocations:\n\\[\nq_i(k)^{(b+1)} = \\frac{ \\hat{\\pi}_k^{(b)} N(\\boldsymbol x_i | \\hat{\\boldsymbol \\mu}_k^{(b)}, \\hat{\\boldsymbol \\Sigma}_k^{(b)}) }{  \\hat{f}^{(b)}(\\boldsymbol x_i)  }\n\\]M-step:number samples assigned class \\(k\\) current step \n\\[\nn_k^{(b+1)} = \\sum_{=1}^n q_i(k)^{(b+1)} \n\\]\nNote necessarily integer soft allocations samples groups!updated estimate group probabilities \n\\[\n\\hat{\\pi}_k^{(b+1)} = \\frac{n_k^{(b+1)}}{n}\n\\]\nupdated estimate mean \n\\[\n\\hat{\\boldsymbol \\mu}_k^{(b+1)} = \\frac{1}{n_k^{(b+1)}} \\sum_{=1}^n q_i(k)^{(b+1)} \\boldsymbol x_i\n\\]\nupdated covariance estimate \n\\[\n\\hat{\\boldsymbol \\Sigma}_k^{(b+1)} =  \\frac{1}{n_k^{(b+1)}} \\sum_{=1}^n q_i(k)^{(b+1)} \\left( \\boldsymbol x_i -\\boldsymbol \\mu_k^{(b+1)}\\right)   \\left( \\boldsymbol x_i -\\boldsymbol \\mu_k^{(b+1)}\\right)^T\n\\]Note \\(q_i(k)\\) hard allocation (\\(\\) one class weight 1 others weight 0) estimators reduce usual empirical estimators.Worksheet 8 can find simple R implementation EM algorithm \nunivariate normal mixtures.Similar analytical expressions normal case\ncan also found general mixtures components\nexponential families.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"connection-with-k-means-clustering-method","chapter":"3 Unsupervised learning and clustering","heading":"3.7.4 Connection with \\(K\\)-means clustering method","text":"\\(K\\)-means algorithm closely related EM algorithm \nprobabilistic clustering specific Gaussian mixture models.Specifically, assume simplified model probabilities \\(\\pi_k\\) classes equal (.e. \\(\\pi_k=\\frac{1}{K}\\)) covariances \\(\\boldsymbol \\Sigma_k\\) spherical form \\(\\sigma^2 \\boldsymbol \\). Thus, covariance depend group, correlation variables variance variables .First, consider “E” step. Using mixture model \nsoft assignment class allocation becomes\n\\[\n\\log( q_i(k) ) = -\\frac{1}{2 \\sigma^2} (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k) +  \\text{const} \n\\]\n\\(\\text{const}\\) depend \\(k\\). can turned hard class allocation \n\\[\n\\begin{split}\ny_i &= \\underset{k}{\\arg \\max} \\log( q_i(k) ) \\\\\n          & = \\underset{k}{\\arg \\min}  (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)\\\\\n\\end{split}\n\\]\nexactly \\(K\\)-means rule allocate samples groups.Second, “M” step compute parameters model. class allocations\nhard expected log-likelihood becomes observed data likelihood \nMLE group mean average samples group.Thus, \\(K\\)-means can viewed EM type algorithm provide hard classification\nbased simple restricted Gaussian mixture model.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"why-the-em-algorithm-works-an-entropy-point-of-view","chapter":"3 Unsupervised learning and clustering","heading":"3.7.5 Why the EM algorithm works — an entropy point of view","text":"iterative (soft) imputation latent states EM algorithm intuitive.\nHowever, immediately clear expected observed log-likelihood needs maximised\nrather , e.g., observed log-likelihood hard allocations. Furthermore,\nneed show maximising marginal likelihood applying \nEM algorithm Bayesian imputation latent states lead \nfitted mixture model.Intriguingly, EM algorithm easiest understand entropy point view,\nconsidering entropy foundations maximum likelihood Bayesian learning\n— details see part II MATH20802 Statistical Methods.First, recall method maximum likelihood results minimising \nKL divergence\nempirical distribution \\(Q_{\\boldsymbol x}\\) representing observations \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) model family \\(F_{\\boldsymbol x}^{\\boldsymbol \\theta}\\)\nparameters \\(\\boldsymbol \\theta\\):\n\\[\n\\hat{\\boldsymbol \\theta}^{ML} =  \\underset{\\boldsymbol \\theta}{\\arg \\min}\\,\\, D_{\\text{KL}}(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta}) \n\\]\nKL divergence decomposes cross-entropy entropy part\n\\[\nD_{\\text{KL}}(Q_{\\boldsymbol x}, F_{\\boldsymbol \\theta}) = H(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta})- H(Q_{\\boldsymbol x})\n\\]\nhence minimising KL divergence regard \\(\\boldsymbol \\theta\\) maximising function\n\\[\n\\begin{split}\n-n H(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta}) &= n \\text{E}_{Q_{\\boldsymbol x}}( \\log f(\\boldsymbol x| \\boldsymbol \\theta)  ) \\\\\n&= \\sum_{=1}^n  \\log f(\\boldsymbol x_i | \\boldsymbol \\theta)\\\\\n&= \\log L(\\boldsymbol \\theta| \\boldsymbol X)\\\\\n\\end{split}\n\\]\nindeed observed data log-likelihood \\(\\boldsymbol \\theta\\).Second, recall chain rule KL divergence. Specifically, KL divergence \njoint model forms upper bound KL divergence marginal model:\n\\[\n\\begin{split}\nD_{\\text{KL}}(Q_{\\boldsymbol x,y} , F_{\\boldsymbol x, y}^{\\boldsymbol \\theta}) &= D_{\\text{KL}}(Q_{\\boldsymbol x} , F_{\\boldsymbol x}^{\\boldsymbol \\theta}) + \\underbrace{  D_{\\text{KL}}(Q_{y| \\boldsymbol x} , F_{y|\\boldsymbol x}^{\\boldsymbol \\theta})   }_{\\geq 0}\\\\\n&\\geq D_{\\text{KL}}(Q_{\\boldsymbol x} , F_{\\boldsymbol x}^{\\boldsymbol \\theta})\n\\end{split}\n\\]\nUnlike \\(\\boldsymbol x\\) observations \\(y\\). Nonetheless, can model joint distribution \\(Q_{\\boldsymbol x, y} =Q_{\\boldsymbol x} Q_{y|\\boldsymbol x}\\) assuming \ndistribution \\(Q_{y|\\boldsymbol x}\\) latent variable.EM algorithm arises iteratively decreasing joint KL divergence \\(D_{\\text{KL}}(Q_{\\boldsymbol x} Q_{y|\\boldsymbol x} , F_{\\boldsymbol x, y}^{\\boldsymbol \\theta})\\) regard \\(Q_{y|\\boldsymbol x}\\) \\(\\boldsymbol \\theta\\):“E” Step: keeping \\(\\boldsymbol \\theta\\) fixed vary \\(Q_{y|\\boldsymbol x}\\) minimise joint KL divergence. minimum reached \\(D_{\\text{KL}}(Q_{y| \\boldsymbol x} , F_{y|\\boldsymbol x}^{\\boldsymbol \\theta}) = 0\\).\ncase \\(Q_{y| \\boldsymbol x} = F_{y|\\boldsymbol x}^{\\boldsymbol \\theta}\\), .e. \nlatent distribution \\(Q_{y| \\boldsymbol x}\\) representing soft allocations computed\nconditioning, .e. using Bayes’ theorem.“E” Step: keeping \\(\\boldsymbol \\theta\\) fixed vary \\(Q_{y|\\boldsymbol x}\\) minimise joint KL divergence. minimum reached \\(D_{\\text{KL}}(Q_{y| \\boldsymbol x} , F_{y|\\boldsymbol x}^{\\boldsymbol \\theta}) = 0\\).\ncase \\(Q_{y| \\boldsymbol x} = F_{y|\\boldsymbol x}^{\\boldsymbol \\theta}\\), .e. \nlatent distribution \\(Q_{y| \\boldsymbol x}\\) representing soft allocations computed\nconditioning, .e. using Bayes’ theorem.“M” Step: keeping \\(Q_{y| \\boldsymbol x}\\) fixed joint KL divergence minimised regard \\(\\boldsymbol \\theta\\). equivalent maximising \nfunction \\(\\sum_{k=1}^K \\sum_{=1}^n q(k | \\boldsymbol x_i) \\log f(\\boldsymbol x_i, k| \\boldsymbol \\theta)\\) \nindeed expected complete data log-likelihood.“M” Step: keeping \\(Q_{y| \\boldsymbol x}\\) fixed joint KL divergence minimised regard \\(\\boldsymbol \\theta\\). equivalent maximising \nfunction \\(\\sum_{k=1}^K \\sum_{=1}^n q(k | \\boldsymbol x_i) \\log f(\\boldsymbol x_i, k| \\boldsymbol \\theta)\\) \nindeed expected complete data log-likelihood.Note steps joint KL divergence always decreases never increases. Furthermore, end “E” step joint KL divergence equals marginal KL divergence. Thus, procedure implicitly minimises marginal KL divergence well, hence maximises marginal log-likelihood.Alternatively, using \\(H( Q_{\\boldsymbol x,y}) = H(Q_{\\boldsymbol x}) + H(Q_{y| \\boldsymbol x} )\\)\ncan rewrite upper bound joint KL divergence equivalent lower\nbound \\(n\\) times negative marginal cross-entropy:\n\\[\n\\begin{split}\n- n H(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta}) &= \\underbrace{ -n  H(Q_{\\boldsymbol x} Q_{y| \\boldsymbol x} , F_{\\boldsymbol x, y}^{\\boldsymbol \\theta})  + n H(Q_{y| \\boldsymbol x} )}_{\\text{lower bound, ELBO}}  + \\underbrace{ n D_{\\text{KL}}(Q_{y| \\boldsymbol x} , F_{y|\\boldsymbol x}^{\\boldsymbol \\theta})}_{\\geq 0}\\\\ \n& \\geq {\\cal F}\\left( Q_{\\boldsymbol x}, Q_{y| \\boldsymbol x},  F_{\\boldsymbol x, y}^{\\boldsymbol \\theta}\\right)\\\\\n\\end{split}\n\\]\nlower bound known “ELBO”. EM algorithm arises \nmaximising \\(\\cal F\\) regard \\(Q_{y| \\boldsymbol x}\\) (“E” step“) \\(\\boldsymbol \\theta\\) (”M\" step).entropy interpretation EM algorithm due Csiszàr Tusnàdy (1984)16 ELBO interpretation introduced Neal Hinton (1998).17","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"convergence-and-invariant-states","chapter":"3 Unsupervised learning and clustering","heading":"3.7.6 Convergence and invariant states","text":"mild assumptions EM algorithm guaranteed monotonically converge local optima observed data log-likelihood.18 Thus series \\(\\hat{\\boldsymbol \\theta}^{(1)}, \\hat{\\boldsymbol \\theta}^{(2)}, \\hat{\\boldsymbol \\theta}^{(3)}, \\ldots\\) converges estimate \\(\\hat{\\boldsymbol \\theta}\\) found maximising observed data log-likelihood.\nHowever, speed convergence EM algorithm can sometimes slow, also situations convergence \\(\\hat{\\boldsymbol \\theta}\\) EM algorithm remains invariant state.example invariant state Gaussian mixture model uniform initialisation latent variables \\(q_i(k) = \\frac{1}{K}\\), \\(K\\) number classes.\nget M step \\(n_k = \\frac{n}{K}\\) parameter estimates\n\\[\n\\hat{\\pi}_k = \\frac{1}{K}\n\\]\n\\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = \\bar{\\boldsymbol x}\n\\]\n\\[\n\\hat{\\boldsymbol \\Sigma}_k = \\frac{1}{n}  \\sum_{=1}^n ( \\boldsymbol x_i -\\bar{\\boldsymbol x})   ( \\boldsymbol x_i -\\bar{\\boldsymbol x})^T = \\hat{\\boldsymbol \\Sigma}\n\\]\nCrucially, none actually depend group \\(k\\)! Thus, E step next soft allocations determined leads \n\\[\nq_i(k) = \\frac{ \\frac{1}{K} N(\\boldsymbol x_i | \\bar{\\boldsymbol x}, \\hat{\\boldsymbol \\Sigma} ) }{ \\sum_{j=1}^K  \\frac{1}{K} N(\\boldsymbol x_i | \\bar{\\boldsymbol x}, \\hat{\\boldsymbol \\Sigma} )  } = \\frac{1}{K}\n\\]\none cycle EM algorithm arrive soft allocation started , algorithm trapped invariant state! Therefore uniform initialisation clearly avoided!","code":""},{"path":"supervised-learning-and-classification.html","id":"supervised-learning-and-classification","chapter":"4 Supervised learning and classification","heading":"4 Supervised learning and classification","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"aims-of-supervised-learning","chapter":"4 Supervised learning and classification","heading":"4.1 Aims of supervised learning","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"supervised-learning-vs.-unsupervised-learning","chapter":"4 Supervised learning and classification","heading":"4.1.1 Supervised learning vs. unsupervised learning","text":"Unsupervised learning:Starting point:unlabelled data \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).Aim: find labels \\(y_1, \\ldots, y_n\\) attach sample \\(\\boldsymbol x_i\\).discrete labels \\(y\\) unsupervised learning called clustering.Supervised learning:Starting point:labelled training data: \\(\\{\\boldsymbol x_1^{\\text{train}}, y_1^{\\text{train}}\\}\\),\n\\(\\ldots\\), \\(\\{\\boldsymbol x_n^{\\text{train}}, y_n^{\\text{train}} \\}\\)addition, unlabelled test data: \\(\\boldsymbol x^{\\text{test}}\\)Aim: use training data learn function, say \\(h(\\boldsymbol x)\\),\npredict label corresponding test data.\npredictor function may provide soft (probabilistic) assignment\nhard assignment class label test sample.\\(y\\) discrete supervised learning called classification.\ncontinuous \\(y\\) label called response supervised learning becomes regression.Thus, supervised learning two-step procedure:Learn predictor function \\(h(\\boldsymbol x)\\) using training data \\(\\boldsymbol x_i^{\\text{train}}\\) plus labels \\(y_i^{\\text{train}}\\).Predict label \\(y^{\\text{test}}\\) test data \\(\\boldsymbol x^{\\text{test}}\\) using estimated classifier function:\n\\(\\hat{y}^{\\text{test}} = \\hat{h}(\\boldsymbol x^{\\text{test}})\\).","code":""},{"path":"supervised-learning-and-classification.html","id":"terminology","chapter":"4 Supervised learning and classification","heading":"4.1.2 Terminology","text":"function \\(h(\\boldsymbol x)\\) predicts class \\(y\\) called classifier.many types classifiers, focus primarily probabilistic classifiers\n(.e. output probabilities possible class/label).challenge find classifier thatexplains current training data well andthat also generalises well future unseen data.Note relatively easy find predictor explains training data especially high dimensions (.e. many predictors) often overfitting predictor generalise well!decision boundary classes defined set \\(\\boldsymbol x\\) \nclass assignment predictor \\(h(\\boldsymbol x)\\) switches one class another.general, simple decision boundaries preferred complex decision boundaries avoid overfitting.commonly used probabilistic methods classifications:QDA (quadratic discriminant analysis)LDA (linear discriminant analysis)DDA (diagonal discriminant analysis),Naive Bayes classificationlogistic regressionCommon non-probabilistic methods include:SVM (support vector machine),random forestneural networksDepending classifiers trainined many variations\nmethods, e.g. Fisher discriminant analysis, regularised LDA, shrinkage disciminant analysis etc.","code":""},{"path":"supervised-learning-and-classification.html","id":"bayesian-discriminant-rule-or-bayes-classifier","chapter":"4 Supervised learning and classification","heading":"4.2 Bayesian discriminant rule or Bayes classifier","text":"setup mixture models:\\(K\\) groups \\(K\\) prespecifiedeach group distribution \\(F_k\\) parameters \\(\\boldsymbol \\theta_k\\)density class \\(f_k(\\boldsymbol x) = f(\\boldsymbol x| k)\\).prior probability group \\(k\\) \\(\\text{Pr}(k) = \\pi_k\\) \\(\\sum_{k=1}^K \\pi_k = 1\\)marginal density mixture \\(f(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\\)posterior probability group \\(k\\) \n\\[\n\\text{Pr}(k | \\boldsymbol x) = \\frac{\\pi_k f_k(\\boldsymbol x) }{ f(\\boldsymbol x)}\n\\]already provides “soft” classification\n\\[\\boldsymbol h(\\boldsymbol x^{\\text{test}}) = (\\text{Pr}(k=1 | \\boldsymbol x^{\\text{test}}),\\ldots, \\text{Pr}(k=K | \\boldsymbol x^{\\text{test}})   )^T\\]\npossible class \\(k \\\\{ 1, \\ldots, K\\}\\) assigned probability label \ntest sample \\(\\boldsymbol x\\).discriminant function logarithm posterior probability:\n\\[\nd_k(\\boldsymbol x) = \\log \\text{Pr}(k | \\boldsymbol x) = \\log \\pi_k  + \\log f_k(\\boldsymbol x)  - \\log f(\\boldsymbol x) \n\\]\nSince use \\(d_k\\) compare different classes \\(k\\) can\nsimplify discriminant function dropping constant terms depend \\(k\\) — term \\(\\log f(\\boldsymbol x)\\). Hence get Bayes discriminant function\n\\[\nd_k(\\boldsymbol x) = \\log \\pi_k + \\log f_k(\\boldsymbol x) \\,.\n\\]subsequent “hard” classification \\(h(\\boldsymbol x^{\\text{test}})\\) select group/label value discriminant function maximised:\n\\[\n\\hat{y}^{\\text{test}} = h(\\boldsymbol x^{\\text{test}}) = \\arg \\max_k d_k(\\boldsymbol x^{\\text{test}}) \\,.\n\\]discriminant functions \\(d_k(\\boldsymbol x)\\) can mapped back probabilistic class assignment using softargmax function (also known softmax function):\n\\[\n\\text{Pr}(k | \\boldsymbol x) = \n\\frac{\\exp( d_k(\\boldsymbol x) )}{\\sum_{c=1}^K \\exp( d_c(\\boldsymbol x) ) } = \n\\frac{\\exp( d_k(\\boldsymbol x) - d_{\\max} ) }{\\sum_{c=1}^K \\exp( d_c(\\boldsymbol x) - d_{\\max} ) } \\,.\n\\]\nNote subtracting \\(d_{\\max} = \\max\\{ d_1(\\boldsymbol x), \\ldots, d_K(\\boldsymbol x) \\}\\) avoids numerical overflow problems computing exponential\nstandardising maximum discriminant functions zero.already encountered Bayes classifier EM algorithm predict state\nlatent variables (soft assignment) \\(K\\)-means algorithm (hard assignment).","code":""},{"path":"supervised-learning-and-classification.html","id":"normal-bayes-classifier","chapter":"4 Supervised learning and classification","heading":"4.3 Normal Bayes classifier","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"quadratic-discriminant-analysis-qda-and-gaussian-assumption","chapter":"4 Supervised learning and classification","heading":"4.3.1 Quadratic discriminant analysis (QDA) and Gaussian assumption","text":"Quadratic discriminant analysis (QDA) special case Bayes classifier densities multivariate normal \\(f_k(\\boldsymbol x) = N(\\boldsymbol x| \\boldsymbol \\mu_k, \\boldsymbol \\Sigma_k)\\).leads discriminant function QDA:\n\\[\nd_k^{QDA}(\\boldsymbol x) = -\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma_k^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) -\\frac{1}{2} \\log \\det(\\boldsymbol \\Sigma_k) +\\log(\\pi_k)\n\\]number noteworthy things :terms dropped depend \\(k\\), \\(-\\frac{d}{2}\\log( 2\\pi)\\).Note appearance Mahalanobis distance \\(\\boldsymbol x\\) \\(\\boldsymbol \\mu_k\\)\nlast term — recall \\(d^{Mahalanobis}(\\boldsymbol x, \\boldsymbol \\mu| \\boldsymbol \\Sigma) = (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)\\).QDA discriminant function quadratic \\(\\boldsymbol x\\) - hence name!\nimplies decision boundaries QDA classification quadratic (.e. parabolas two dimensional settings).Gaussian models specifically can useful multiply discriminant function -2 get rid factor \\(-\\frac{1}{2}\\), note case need find minimum discriminant function rather maximum:\n\\[\nd_k^{QDA (v2)}(\\boldsymbol x) =  (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma_k^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) + \\log \\det(\\boldsymbol \\Sigma_k)  -2 \\log(\\pi_k)\n\\]\nliterature find versions Gaussian discriminant functions need check carefully convention used.\nfollowing use first version .Decision boundaries QDA classifier can either linear nonlinear (quadratic curve).\ndecision boundary two classes \\(\\) \\(j\\)\nrequire \\(d^{QDA}_i(\\boldsymbol x) = d^{QDA}_j(\\boldsymbol x)\\), equivalently\n\\(d^{QDA}_i(\\boldsymbol x) - d^{QDA}_j(\\boldsymbol x) = 0\\), quadratic equation.","code":""},{"path":"supervised-learning-and-classification.html","id":"linear-discriminant-analysis-lda","chapter":"4 Supervised learning and classification","heading":"4.3.2 Linear discriminant analysis (LDA)","text":"LDA special case QDA, assumption common overall covariance across groups: \\(\\boldsymbol \\Sigma_k = \\boldsymbol \\Sigma\\).leads simplified discriminant function:\n\\[\nd_k^{LDA}(\\boldsymbol x) = -\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) +\\log(\\pi_k)\n\\]\nNote term containing log-determinant now gone, LDA essentially now method tries minimize Mahalanobis distance\n(taking also account prior class probabilities).function can simplified, noting quadratic term \\(\\boldsymbol x^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x\\) depend \\(k\\) hence can dropped:\n\\[\n\\begin{split}\nd_k^{LDA}(\\boldsymbol x) &=  \\boldsymbol \\mu_k^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_k^T \\boldsymbol \\Sigma^{-1} \\boldsymbol \\mu_k + \\log(\\pi_k) \\\\\n  &= \\boldsymbol b^T \\boldsymbol x+ \n\\end{split}\n\\]\nThus, LDA discriminant function linear \\(\\boldsymbol x\\), hence \nresulting decision boundaries linear well (.e. straight lines two-dimensional settings).Comparison linear decision boundaries LDA (left) compared QDA (right):Note logistic regression (cf. GLM module) takes exactly linear form indeed closely linked LDA classifier.","code":""},{"path":"supervised-learning-and-classification.html","id":"diagonal-discriminant-analysis-dda","chapter":"4 Supervised learning and classification","heading":"4.3.3 Diagonal discriminant analysis (DDA)","text":"DDA start setting LDA, now simplify model even additionally requiring diagonal covariance containing variances (thus assume correlations among predictors \\(x_1, \\ldots, x_d\\) zero):\n\\[\n\\boldsymbol \\Sigma= \\boldsymbol V= \\begin{pmatrix}\n    \\sigma^2_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma^2_{d}\n\\end{pmatrix}\n\\]\nsimplifies inversion \\(\\boldsymbol \\Sigma\\) \n\\[\n\\boldsymbol \\Sigma^{-1} = \\boldsymbol V^{-1} = \\begin{pmatrix}\n    \\sigma^{-2}_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma^{-2}_{d}\n\\end{pmatrix}\n\\]\nleads discriminant function\n\\[\n\\begin{split}\nd_k^{DDA}(\\boldsymbol x) &=  \\boldsymbol \\mu_k^T \\boldsymbol V^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_k^T \\boldsymbol V^{-1} \\boldsymbol \\mu_k + \\log(\\pi_k) \\\\\n  &= \\sum_{j=}^d \\frac{\\mu_{k,j} x_j - \\mu_{k,j}^2/2}{\\sigma_d^2} + \\log(\\pi_k)\n\\end{split}\n\\]\nspecial case LDA, DDA classifier linear classifier thus linear decision boundaries.Bayes classifier (using distribution) assuming uncorrelated predictors\nalso known naive Bayes classifier.Hence, DDA naive Bayes classifier assuming underlying Gaussian distributions.However, don’t let misguide name “naive”: fact DDA “naive” Bayes classifier often effective classifiers, especially high-dimensional settings!","code":""},{"path":"supervised-learning-and-classification.html","id":"the-training-step-learning-qda-lda-and-dda-classifiers-from-data","chapter":"4 Supervised learning and classification","heading":"4.4 The training step — learning QDA, LDA and DDA classifiers from data","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"number-of-model-parameters","chapter":"4 Supervised learning and classification","heading":"4.4.1 Number of model parameters","text":"order predict class new data using discriminant functions need first learn underlying parameters training data \\(\\boldsymbol x_i^{\\text{train}}\\) \\(y_i^{\\text{train}}\\):QDA, LDA DDA need learn \\(\\pi_1, \\ldots, \\pi_K\\) \\(\\sum_{k=1}^K \\pi_k = 1\\) mean vectors \\(\\boldsymbol \\mu_1, \\ldots, \\boldsymbol \\mu_K\\)QDA additionally require \\(\\boldsymbol \\Sigma_1, \\ldots, \\boldsymbol \\Sigma_K\\)LDA need \\(\\boldsymbol \\Sigma\\)DDA estimate \\(\\sigma^2_1, \\ldots, \\sigma^2_d\\).Overall, total number parameters estimated learning discriminant functions\ntraining data follows:QDA: \\(K-1+ K d + K \\frac{d(d+1)}{2}\\)LDA: \\(K-1+ K d + \\frac{d(d+1)}{2}\\)DDA: \\(K-1+ K d + d\\)","code":""},{"path":"supervised-learning-and-classification.html","id":"estimating-the-discriminant-predictor-function","chapter":"4 Supervised learning and classification","heading":"4.4.2 Estimating the discriminant / predictor function","text":"QDA, LDA DDA learn predictor estimating \nparameters discriminant function training data.","code":""},{"path":"supervised-learning-and-classification.html","id":"large-sample-size","chapter":"4 Supervised learning and classification","heading":"4.4.2.1 Large sample size","text":"sample size training data set sufficiently large compared model dimensions can use maximum likelihood (ML) estimate model parameters. able use ML need larger sample size QDA LDA (full covariances need estimated) DDA relatively small sample size can sufficient (explains “naive” Bayes methods popular practise).obtain parameters estimates use known labels \\(y_i^{\\text{train}}\\) sort \nsamples \\(\\boldsymbol x_i^{\\text{train}}\\) corresponding classes, apply standard ML estimators.\nLet \\(g_k =\\{: y_i^{\\text{train}}=k \\}\\) set indices training sample belonging group \\(k\\), \\(n_k\\) sample size group \\(k\\)ML estimates class probabilities frequencies\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n}\n\\]\nML estimate group means \\(k=1, \\ldots, K\\) \n\\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n_k} \\sum_{\\g_k} \\boldsymbol x_i^{\\text{train}} \\, .\n\\]\nML estimate global mean \\(\\boldsymbol \\mu_0\\) (.e. assume single class ignore group labels) \n\\[\n\\hat{\\boldsymbol \\mu}_0 = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i^{\\text{train}} = \\sum_{k=1}^K \\hat{\\pi}_k \\hat{\\boldsymbol \\mu}_k\n\\]\nNote global mean identical pooled mean (.e. weighted average \nindividual group means).ML estimates covariances \\(\\boldsymbol \\Sigma_k\\) QDA \n\\[\n\\widehat{\\boldsymbol \\Sigma}_k = \\frac{1}{n_k} \\sum_{\\g_k} ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k)^T\n\\]order get ML estimate pooled variance \\(\\boldsymbol \\Sigma\\) use LDA compute\n\\[\n\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\sum_{k=1}^K \\sum_{\\g_k} ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k)^T =  \\sum_{k=1}^K \\hat{\\pi}_k \\widehat{\\boldsymbol \\Sigma}_k \n\\]Note pooled variance \\(\\boldsymbol \\Sigma\\) differs (substantially!) global variance \\(\\Sigma_0\\) results simply\nignoring class labels computed \n\\[\n\\widehat{\\boldsymbol \\Sigma}_0^{ML} = \\frac{1}{n} \\sum_{=1}^n ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_0) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_0)^T\n\\]\nrecognise variance decomposition mixture models, \\(\\boldsymbol \\Sigma_0\\) total variance\npooled \\(\\boldsymbol \\Sigma\\) unexplained/-group variance.","code":""},{"path":"supervised-learning-and-classification.html","id":"small-sample-size","chapter":"4 Supervised learning and classification","heading":"4.4.2.2 Small sample size","text":"dimension \\(d\\) large compared sample size number parameters predictor function grows fast. Especially QDA also LDA data hungry ML estimation becomes ill-posed problem.discussed Section 1.3 instance need use regularised estimator covariance(s) estimators derived framework penalised ML, Bayesian learning, shrinkage estimation etc.\nalso ensures estimated covariance matrices positive definite (\nautomatically guaranteed DDA variances positive).Furthermore, small sample setting advised reduce number parameters model. Thus using LDA DDA preferred QDA. can also prevent overfitting lead predictor generalises better.analyse high-dimensional data worksheets employ regularised version LDA DDA using Stein-type shrinkage estimation discussed Section 1.3 implemented R package “sda”.","code":""},{"path":"supervised-learning-and-classification.html","id":"comparison-of-estimated-decision-boundaries-lda-vs.-qda","chapter":"4 Supervised learning and classification","heading":"4.4.3 Comparison of estimated decision boundaries: LDA vs. QDA","text":"compare two simple scenarios using simulated data.Non-nested case (\\(K=4\\)):Nested case (\\(K=2\\)):nested case LDA fails separate two classes \nway separate two nested classes \nsimple linear boundary.","code":""},{"path":"supervised-learning-and-classification.html","id":"quantifying-prediction-error","chapter":"4 Supervised learning and classification","heading":"4.5 Quantifying prediction error","text":"classifier trained naturally interested performance\ncorrectly classify previously unseen data points. useful comparing different types\nclassifiers also comparing type classifier using different sets\npredictor variables.","code":""},{"path":"supervised-learning-and-classification.html","id":"quantification-of-prediction-error-based-on-validation-data","chapter":"4 Supervised learning and classification","heading":"4.5.1 Quantification of prediction error based on validation data","text":"measure predictor error compares predicted label \\(\\hat{y}\\) true\nlabel \\(y\\) validation data. validation data set contains \n\\(\\boldsymbol x_i\\) associated label \\(y_i\\) unlike training data \nused learning predictor function.continuous response often squared loss used:\n\\[\n\\text{err}(\\hat{y}, y) =  (\\hat{y} - y)^2\n\\]binary outcomes one often employs 0/1 loss:\n\\[\n\\text{err}(\\hat{y}, y) =\n\\begin{cases}\n    0, & \\text{ } \\hat{y}=y\\\\\n    1,  & \\text{otherwise}\n\\end{cases}\n\\]\nAlternatively, quantity derived confusion matrix\n(containing TP, TN, FP, FN) can used.mean prediction error expectation\n\\[\nPE = \\text{E}(\\text{err}(\\hat{y}, y))\n\\]\nthus empirical mean prediction error \n\\[\n\\widehat{PE} = \\frac{1}{m} \\sum_{=1}^m \\text{err}(\\hat{y}_i, y_i)\n \\]\n\\(m\\) sample size validation data set.generally, can also quantify prediction error framework -called proper scoring rules, whole probabilistic forecast taken account (e.g. individual probabilities class, rather just selected probable class). commonly used scoring rule negative log-probability (“surprise”), expected surprise cross-entropy (cf. Statistical Methods module). leads back entropy likelihood (see MATH20802 Statistical Methods).estimate prediction error model can use compare choose among set candidate models, selecting sufficiently low prediction\nerror.","code":""},{"path":"supervised-learning-and-classification.html","id":"estimation-of-prediction-error-using-cross-validation","chapter":"4 Supervised learning and classification","heading":"4.5.2 Estimation of prediction error using cross-validation","text":"Unfortunately, quite often separate validation data available evaluate classifier.case need rely simple algorithmic procedure called cross-validation.Outline cross-validation:split samples training data number (say \\(K\\)) parts (“folds”).use \\(K\\) folds validation data \\(K-1\\) folds training data.average resulting \\(K\\) individual estimates prediction error, get overall aggregated predictor error, along error.Note case one part data reserved validation \nused training predictor.choose \\(K\\) folds small (allow estimation \nprediction error) also large (make sure actually able train reliable classifier remaining data). typical value \\(K\\) 5 10, 80% respectively 90% samples used training 20 %\n10% validation.\\(K=n\\) many folds samples validation data set consists single data point. called “leave one ” cross-validation (LOOCV). analytic approximations prediction error obtained LOOCV\napproach computationally inexpensive standard models (including regression).reading:study technical details cross-validation: read Section 5.1 Cross-Validation James et al. (2021) introduction statistical learning applications R (2nd edition). Springer.","code":""},{"path":"supervised-learning-and-classification.html","id":"goodness-of-fit-and-variable-ranking","chapter":"4 Supervised learning and classification","heading":"4.6 Goodness of fit and variable ranking","text":"linear regression (cf. “Statistical Methods” module) interested finding \nwhether fitted mixture model appropriate model, \nparticular predictor(s) \\(x_j\\) \\(\\boldsymbol x=(x_1, \\ldots, x_d)^T\\)\nresponsible prediction outcome, .e. categorizing sample group \\(k\\).order study problem helpful rewrite discriminant function highlight influence (importance) predictor.focus linear methods (LDA DDA) first look simple case \\(K=2\\) generalise two groups.","code":""},{"path":"supervised-learning-and-classification.html","id":"lda-with-k2-classes","chapter":"4 Supervised learning and classification","heading":"4.6.1 LDA with \\(K=2\\) classes","text":"two classes using LDA discriminant rule choose group \\(k=1\\)\n\\(d_1^{LDA}(\\boldsymbol x) > d_2^{LDA}(\\boldsymbol x)\\), equivalently, \n\\[\n\\Delta_{12}^{LDA} = d_1^{LDA}(\\boldsymbol x) - d_2^{LDA}(\\boldsymbol x) > 0\n\\]\nSince \\(d_k(\\boldsymbol x)\\) log-posterior (plus/minus identical constants)\n\\(\\Delta^{LDA}\\) fact log-posterior odds class 1 versus class 2 (see Statistical Methods, Bayesian inference).difference \\(\\Delta_{12}^{LDA}\\) \n\\[\n\\underbrace{ \\Delta_{12}^{LDA}}_{\\text{log posterior odds}} = \n\\underbrace{(\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} \\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right)}_{\\text{log Bayes factor } \\log B_{12}} + \\underbrace{\\log\\left( \\frac{\\pi_1}{\\pi_2} \\right)}_{\\text{log prior odds}}\n\\]\nNote since consider simple non-composite models log-Bayes factor identical\nlog-likelihood ratio!log Bayes factor \\(\\log B_{12}\\) known weight evidence favour\n\\(F_1\\) given \\(\\boldsymbol x\\). expected weight evidence assuming \\(\\boldsymbol x\\) indeed \\(F_1\\)\nKullback-Leibler discrimination information favour group 1,\n.e. KL divergence distribution \\(F_2\\) \\(F_1\\):\n\\[\n\\text{E}_{F_1} ( \\log B_{12} ) = D_{\\text{KL}}(F_1,  F_2) = \\frac{1}{2} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2) = \\frac{1}{2} \\Omega^2\n\\]\nyields, apart scale factor, population version \nHotelling \\(T^2\\)\nstatistic defined \n\\[T^2 =  c^2 (\\hat{\\boldsymbol \\mu}_1 -\\hat{\\boldsymbol \\mu}_2)^T \\hat{\\boldsymbol \\Sigma}^{-1} (\\hat{\\boldsymbol \\mu}_1 -\\hat{\\boldsymbol \\mu}_2)\\]\n\n\\(c = (\\frac{1}{n_1} + \\frac{1}{n_2})^{-1/2} = \\sqrt{n \\pi_1 \\pi_2}\\)\nsample size dependent factor (\\(\\text{SD}(\\hat{\\boldsymbol \\mu}_1 - \\hat{\\boldsymbol \\mu}_2)\\)).\n\\(T^2\\) measure fit underlying two-component mixture.Using whitening transformation \\(\\boldsymbol z= \\boldsymbol W\\boldsymbol x\\) \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1}\\)\ncan rewrite log Bayes factor \n\\[\n\\begin{split}\n\\log B_{12} &= \\left( (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol W^T \\right)\\, \\left(\\boldsymbol W\\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right) \\right) \\\\\n&=\\boldsymbol \\omega^T \\boldsymbol \\delta(\\boldsymbol x)\n\\end{split}\n\\]\n.e. product two vectors:\\(\\boldsymbol \\delta(\\boldsymbol x)\\) whitened \\(\\boldsymbol x\\) (centered around average means)\n\\(\\boldsymbol \\omega= (\\omega_1, \\ldots, \\omega_d)^T = \\boldsymbol W(\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)\\) gives weight \nwhitened component \\(\\boldsymbol \\delta(\\boldsymbol x)\\)\nlog Bayes factor.large positive negative value \\(\\omega_j\\)\nindicates corresponding whitened predictor relevant choosing class,\nwhereas small values \\(\\omega_j\\) close zero indicate corresponding ZCA whitened predictor unimportant. Furthermore,\n\\(\\boldsymbol \\omega^T \\boldsymbol \\omega= \\sum_{j=1}^d \\omega_j^2 = (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2) = \\Omega^2\\),\n.e. squared \\(\\omega_j^2\\) provide component-wise decomposition overall fit \\(\\Omega^2\\).Choosing ZCA-cor whitening transformation \\(\\boldsymbol W=\\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2}\\)\nget\n\\[\n\\boldsymbol \\omega^{ZCA-cor} = \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)\n\\]\nbetter understanding \\(\\boldsymbol \\omega^{ZCA-cor}\\) provided \ncomparing two-sample \\(t\\)-statistic\n\\[\n\\hat{\\boldsymbol \\tau} = c \\hat{\\boldsymbol V}^{-1/2} (\\hat{\\boldsymbol \\mu}_1 - \\hat{\\boldsymbol \\mu}_2)\n\\]\n\\(\\boldsymbol \\tau\\) population version \\(\\hat{\\boldsymbol \\tau}\\) can define\n\\[\\boldsymbol \\tau^{adj} = \\boldsymbol P^{-1/2} \\boldsymbol \\tau= c \\boldsymbol \\omega^{ZCA-cor}\\]\ncorrelation-adjusted \\(t\\)-scores (cat scores). \\(({\\hat{\\boldsymbol \\tau}}^{adj})^T {\\hat{\\boldsymbol \\tau}}^{adj} = T^2\\) can see cat scores offer component-wise decomposition Hotelling’s \\(T^2\\).Note choice ZCA-cor whitening ensure whitened components interpretable\nstay maximally correlated original variables. However, may also choose example PCA whitening\ncase \\(\\boldsymbol \\omega^T \\boldsymbol \\omega\\) provide variable importance PCA whitened variables.DDA, assumes correlations among predictors vanish, .e. \\(\\boldsymbol P= \\boldsymbol I_d\\), get\n\\[\n\\Delta_{12}^{DDA} =\\underbrace{ \\left( (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol V^{-1/2}  \\right)}_{\\text{ } c^{-1} \\boldsymbol \\tau^T }\\, \\underbrace{ \\left( \\boldsymbol V^{-1/2} \\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right) \\right) }_{\\text{centered standardised predictor}}+ \\log\\left( \\frac{\\pi_1}{\\pi_2} \\right) \\\\\n\\]\nSimilarly , \\(t\\)-score \\(\\boldsymbol \\tau\\) determines impact standardised predictor \\(\\Delta^{DDA}\\).Consequently, DDA can rank predictors squared \\(t\\)-score.\nRecall standard linear regression uncorrelated predictors can find important predictors\nranking squared marginal correlations – ranking (squared) \\(t\\)-scores DDA exact analogy discrete response.","code":""},{"path":"supervised-learning-and-classification.html","id":"multiple-classes","chapter":"4 Supervised learning and classification","heading":"4.6.2 Multiple classes","text":"two classes need refer -called pooled centroids formulation DDA LDA (introduced Tibshirani 2002).pooled centroid given \\(\\boldsymbol \\mu_0 = \\sum_{k=1}^K \\pi_k \\boldsymbol \\mu_k\\) — centroid\nsingle class. corresponding probability (single class) \\(\\pi_0=1\\) distribution\ncalled \\(F_0\\).LDA discriminant function “group 0” \n\\[\nd_0^{LDA}(\\boldsymbol x) = \\boldsymbol \\mu_0^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_0^T \\boldsymbol \\Sigma^{-1} \\boldsymbol \\mu_0 \n\\]\nlog posterior odds comparison group \\(k\\) pooled group \\(0\\)\n\n\\[\n\\begin{split}\n\\Delta_k^{LDA} &= d_k^{LDA}(\\boldsymbol x) - d_0^{LDA}(\\boldsymbol x) \\\\\n         &= \\log B_{k0} + \\log(\\pi_k) \\\\\n         &= \\boldsymbol \\omega_k^T \\boldsymbol \\delta_k(\\boldsymbol x) + \\log(\\pi_k)\n\\end{split}\n\\]\n\n\\[\n\\boldsymbol \\omega_k = \\boldsymbol W(\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)  \n\\]\n\n\\[\n\\boldsymbol \\delta_k(\\boldsymbol x) = \\boldsymbol W(\\boldsymbol x- \\frac{\\boldsymbol \\mu_k +\\boldsymbol \\mu_0}{2} )\n\\]\nexpected log Bayes factor \n\\[\n\\text{E}_{F_k} ( \\log B_{k0} )= KL(F_k || F_0) = \\frac{1}{2} (\\boldsymbol \\mu_k -\\boldsymbol \\mu_0)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_k -\\boldsymbol \\mu_0) = \\frac{1}{2} \\Omega_k^2\n\\]scale factor \\(c_k = (\\frac{1}{n_k} - \\frac{1}{n})^{-1/2} = \\sqrt{n \\frac{\\pi_k}{1-\\pi_k}}\\) (\\(\\text{SD}(\\hat{\\boldsymbol \\mu}_k-\\hat{\\boldsymbol \\mu}_0)\\), minus sign \\(\\frac{1}{n}\\) due correlation \n\\(\\hat{\\boldsymbol \\mu}_k\\) pooled mean \\(\\hat{\\boldsymbol \\mu}_0\\))\nget correlation-adjusted \\(t\\)-score comparing mean group \\(k\\) \npooled mean\n\\[\n\\boldsymbol \\tau_k^{adj} = c_k \\boldsymbol \\omega_k^{ZCA-cor} \\,.\n\\]two class case (\\(K=2\\)) get \n\\(\\boldsymbol \\mu_0 = \\pi_1 \\boldsymbol \\mu_1 + \\pi_2 \\boldsymbol \\mu_2\\) mean difference\n\\((\\boldsymbol \\mu_1 - \\boldsymbol \\mu_0) = \\pi_2 (\\boldsymbol \\mu_1 - \\boldsymbol \\mu_2)\\)\n\\(c_1 = \\sqrt{n \\frac{\\pi_1}{\\pi_2}}\\)\nyields\n\\[\n\\boldsymbol \\tau_1^{adj} = \\sqrt{n \\pi_1 \\pi_2 } \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} (\\boldsymbol \\mu_1 - \\boldsymbol \\mu_2) , \n\\]\n.e. exact score two-class setting.","code":""},{"path":"supervised-learning-and-classification.html","id":"variable-selection","chapter":"4 Supervised learning and classification","heading":"4.7 Variable selection","text":"previous saw DDA natural score\nranking features regard relevance separating classes \n(squared) \\(t\\)-score, LDA whitened version \nsquared correlation-adjusted \\(t\\)-score (based ZCA-cor whitening) may used.\nranking established question suitable cutoff arises, .e. \nmany features need () retained model.large high-dimensional models feature selection can also viewed\nform regularisation also dimension reduction. Specifically, may many variables/ features contribute class prediction. Despite \nprinciple effect outcome presence “null variables”\ncan nonetheless deterioriate (sometimes dramatically!) overall predictive accuracy trained predictor, add noise increase model dimension. Therefore, variables contribute prediction\nfiltered order able construct good prediction models classifiers.","code":""},{"path":"supervised-learning-and-classification.html","id":"choosing-a-threshold-by-multiple-testing-using-false-discovery-rates","chapter":"4 Supervised learning and classification","heading":"4.7.1 Choosing a threshold by multiple testing using false discovery rates","text":"simple way determine cutoff threshold use standard technique \nmultiple testing.predictor variable \\(x_1, \\ldots, x_d\\) corresponding test statistic\nmeasuring influence variable response, example \n\\(t\\)-scores related statistics discussed previous section.\naddition providing overall ranking set statistics can used\ndetermine suitable cutoff trying separate two populations predictor variables:“Null” variables contribute prediction“Alternative” variables linked predictionAs discussed “Statistical Methods” module last term (Part 2 - Section 8) can done follows:distribution observed test statistics \\(z_i\\) assumed follow two-component mixture \\(F_0(z)\\) \\(F_A(z)\\) distributions corresponding null alternative, \\(f_0(z)\\) \\(f_a(z)\\) densities, \\(\\pi_0\\) \\(\\pi_A=1-\\pi_0\\) weights:\n\\[\nf(z) = \\pi_0 f_0(z) + (1-\\pi_0) f_a(z)\n\\]distribution observed test statistics \\(z_i\\) assumed follow two-component mixture \\(F_0(z)\\) \\(F_A(z)\\) distributions corresponding null alternative, \\(f_0(z)\\) \\(f_a(z)\\) densities, \\(\\pi_0\\) \\(\\pi_A=1-\\pi_0\\) weights:\n\\[\nf(z) = \\pi_0 f_0(z) + (1-\\pi_0) f_a(z)\n\\]null model typically parametric family (e.g. normal around zero \nfree variance parameter) whereas alternative often modelled nonparametrically.null model typically parametric family (e.g. normal around zero \nfree variance parameter) whereas alternative often modelled nonparametrically.fitting mixture model, often assuming additional constraints make mixture identifiable, one can compute false discovery rates (FDR) follows:\nLocal FDR:\n\\[\n\\widehat{fdr}(z_i) = \\hat{\\text{Pr}}(\\text{null} | z_i) = \\frac{\\hat{\\pi}_0 \\hat{f}_0(z_i)}{\\hat{f}(z_i)}  \n\\]\nTail-area-based FDR (=\\(q\\)-value):\n\\[\n\\widehat{Fdr}(z_i) = \\hat{\\text{Pr}}(\\text{null} | Z > z_i) = \\frac{\\hat{\\pi}_0 \\hat{F}_0(z_i)}{\\hat{F}(z_i)}\n\\]\nNote essentially \\(p\\)-values adjusted multiple testing (variant Benjamini-Hochberg method).fitting mixture model, often assuming additional constraints make mixture identifiable, one can compute false discovery rates (FDR) follows:Local FDR:\n\\[\n\\widehat{fdr}(z_i) = \\hat{\\text{Pr}}(\\text{null} | z_i) = \\frac{\\hat{\\pi}_0 \\hat{f}_0(z_i)}{\\hat{f}(z_i)}  \n\\]Tail-area-based FDR (=\\(q\\)-value):\n\\[\n\\widehat{Fdr}(z_i) = \\hat{\\text{Pr}}(\\text{null} | Z > z_i) = \\frac{\\hat{\\pi}_0 \\hat{F}_0(z_i)}{\\hat{F}(z_i)}\n\\]\nNote essentially \\(p\\)-values adjusted multiple testing (variant Benjamini-Hochberg method).thresholding false discovery rates possible identify \nvariables clearly belong two groups also features\neasily discriminated fall either group:“alternative” variables low local FDR, e.g, \\(\\widehat{fdr}(z_i) \\leq 0.2\\)“null” variables high local FDR, e.g. \\(\\widehat{fdr}(z_i) \\geq 0.8\\)features easily classified null alternative, e.g. \\(0.2 < \\widehat{fdr}(z_i) < 0.8\\)feature selection prediction settings generally aim remove \nvariable clearly belong null group, leaving others model.","code":""},{"path":"supervised-learning-and-classification.html","id":"variable-selection-using-cross-validation","chapter":"4 Supervised learning and classification","heading":"4.7.2 Variable selection using cross-validation","text":"conceptually simple computationally expensive approach variable selection estimate predicion error type predictor different sets predictors using cross-validation, choosing predictor achieves good prediction accuracy using small number featurs.method works well practise demonstrated \nnumber problems worksheets.","code":""},{"path":"multivariate-dependencies.html","id":"multivariate-dependencies","chapter":"5 Multivariate dependencies","heading":"5 Multivariate dependencies","text":"","code":""},{"path":"multivariate-dependencies.html","id":"canonical-correlation-analysis-cca-aka-cca-whitening","chapter":"5 Multivariate dependencies","heading":"5.1 Canonical Correlation Analysis (CCA) aka CCA whitening","text":"Canonical correlation analysis invented Harald Hotelling 1936.19 CCA aims characterise linear dependence random vectors \\(\\boldsymbol x\\) \\(\\boldsymbol y\\).CCA works simultaneously whitening two random vectors \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) whitening matrices chosen way cross-correlation matrix resulting whitened variables\nbecomes diagonal, elements diagonal correspond canonical correlations.\\[\\begin{align*}\n\\begin{array}{ll}\n\\boldsymbol x= \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_p \\end{pmatrix} \\\\\n\\text{Dimension } p\n\\end{array}\n\\begin{array}{ll}\n\\boldsymbol y= \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_q \\end{pmatrix} \\\\\n\\text{Dimension } q\n\\end{array}\n\\begin{array}{ll}\n\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma_{\\boldsymbol x} = \\boldsymbol V_{\\boldsymbol x}^{1/2}\\boldsymbol P_{\\boldsymbol x}\\boldsymbol V_{\\boldsymbol x}^{1/2} \\\\\n\\text{Var}(\\boldsymbol y) = \\boldsymbol \\Sigma_{\\boldsymbol y} = \\boldsymbol V_{\\boldsymbol y}^{1/2}\\boldsymbol P_{\\boldsymbol y}\\boldsymbol V_{\\boldsymbol y}^{1/2} \\\\\n\\end{array}\n\\end{align*}\\]\\[\\begin{align*}\n\\begin{array}{cc}\n\\text{Whitening } \\boldsymbol x\\text{:} \\\\\n\\text{Whitening } \\boldsymbol y\\text{:}\n\\end{array}\n\\begin{array}{cc}\n\\boldsymbol z_{\\boldsymbol x} = \\boldsymbol W_{\\boldsymbol x}\\boldsymbol x=\\boldsymbol Q_{\\boldsymbol x}\\boldsymbol P_{\\boldsymbol x}^{-1/2}\\boldsymbol V_{\\boldsymbol x}^{-1/2}\\boldsymbol x\\\\\n\\boldsymbol z_{\\boldsymbol y} = \\boldsymbol W_{\\boldsymbol y}\\boldsymbol y=\\boldsymbol Q_{\\boldsymbol y}\\boldsymbol P_{\\boldsymbol y}^{-1/2}\\boldsymbol V_{\\boldsymbol y}^{-1/2}\\boldsymbol y\n\\end{array}\n\\end{align*}\\]\n(note use correlation-based form \\(\\boldsymbol W\\))Cross-correlation \\(\\boldsymbol z_{\\boldsymbol y}\\) \\(\\boldsymbol z_{\\boldsymbol y}\\):\\[\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})=\\boldsymbol Q_{\\boldsymbol x}\\boldsymbol K\\boldsymbol Q_{\\boldsymbol y}^T\\]\\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2}\\boldsymbol P_{\\boldsymbol x\\boldsymbol y}\\boldsymbol P_{\\boldsymbol y}^{-1/2}\\).Idea: can choose suitable orthogonal matrices \\(\\boldsymbol Q_{\\boldsymbol x}\\) \\(\\boldsymbol Q_{\\boldsymbol y}\\) putting constraints cross-correlation.CCA: aim diagonal \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})\\) component \\(\\boldsymbol z_{\\boldsymbol x}\\) influences one (corresponding) component \\(\\boldsymbol z_{\\boldsymbol y}\\).Motivation: pairs “modules” represented components \\(\\boldsymbol z_{\\boldsymbol x}\\)\n\\(\\boldsymbol z_{\\boldsymbol y}\\) influencing (anyone module).\\[\n\\begin{array}{ll}\n\\boldsymbol z_{\\boldsymbol x} = \\begin{pmatrix} z^x_1 \\\\ z^x_2 \\\\ \\vdots \\\\ z^x_p \\end{pmatrix} &\n\\boldsymbol z_{\\boldsymbol y} = \\begin{pmatrix} z^y_1 \\\\ z^y_2 \\\\ \\vdots \\\\ z^y_q \\end{pmatrix} \\\\\n\\end{array}\n\\]\\[\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y}) = \\begin{pmatrix} d_1 & \\dots & 0 \\\\ \\vdots &  \\vdots \\\\ 0 & \\dots & d_m \\end{pmatrix}\\]\\(d_i\\) canonical correlations \\(m=\\min(p,q)\\).","code":""},{"path":"multivariate-dependencies.html","id":"how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal","chapter":"5 Multivariate dependencies","heading":"5.1.1 How to make cross-correlation matrix \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})\\) diagonal?","text":"Use Singular Value Decomposition (SVD) matrix \\(\\boldsymbol K\\):\\[\\boldsymbol K= (\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}})^T  \\boldsymbol \\Lambda\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\]\n\\(\\boldsymbol \\Lambda\\) diagonal matrix containing singular values \\(\\boldsymbol K\\)yields orthogonal matrices \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) thus desired whitening matrices \\(\\boldsymbol W_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol W_{\\boldsymbol y}^{\\text{CCA}}\\)result \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y}) = \\boldsymbol \\Lambda\\) .e. singular values \\(\\lambda_i\\) \\(\\boldsymbol K\\) desired canonical correlations!\\(\\longrightarrow\\) \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) determined diagonality constraint (note different previously discussed whitening methods).Note signs corresponding columns \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) identified. Traditionally, SVD \nsigns chosen singular values positive. However, \nimpose positive-diagonality \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\),\nthus positive-diagonality cross-correlations \\(\\boldsymbol \\Psi_{\\boldsymbol x}\\) \n\\(\\boldsymbol \\Psi_{\\boldsymbol y}\\), canonical correlations may take positive \nnegative values.","code":""},{"path":"multivariate-dependencies.html","id":"related-methods","chapter":"5 Multivariate dependencies","heading":"5.1.2 Related methods","text":"O2PLS: similar CCA using orthogonal projections rather whitening.O2PLS: similar CCA using orthogonal projections rather whitening.Vector correlation: aggregates squared canonical correlations single overall measure (see ).Vector correlation: aggregates squared canonical correlations single overall measure (see ).","code":""},{"path":"multivariate-dependencies.html","id":"vector-correlation-and-rv-coefficient","chapter":"5 Multivariate dependencies","heading":"5.2 Vector correlation and RV coefficient","text":"","code":""},{"path":"multivariate-dependencies.html","id":"measuring-the-linear-association-between-two-sets-of-random-variables","chapter":"5 Multivariate dependencies","heading":"5.2.1 Measuring the linear association between two sets of random variables","text":"linear association two scalar random variables \\(x\\) \\(y\\) measured\ncorrelation \\(\\text{Cor}(x, y) = \\rho\\). now like explore generalise quantity case two random vectors, .e. problem measure total linear association two\nrandom vectors (equivalently two sets random variables) \\(\\boldsymbol x= (x_1, \\ldots, x_p)^T\\) \n\\(\\boldsymbol y= (y_1, \\ldots, y_q)^T\\).assume joint correlation matrix\n\\[\n\\boldsymbol P= \n\\begin{pmatrix} \n\\boldsymbol P_{\\boldsymbol x} &  \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol P_{\\boldsymbol y\\boldsymbol x} & \\boldsymbol P_{\\boldsymbol y} \\\\\n\\end{pmatrix} \n\\]\ncross-correlation matrix \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = \\boldsymbol P_{\\boldsymbol y\\boldsymbol x}^T\\)\nwithin-group group correlations \\(\\boldsymbol P_{\\boldsymbol x}\\) \\(\\boldsymbol P_{\\boldsymbol y}\\).\ncross-correlations vanish, \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} =0\\), \ntwo random vectors uncorrelated, joint correlation matrix\nbecomes diagonal block matrix\n\\[\n\\boldsymbol P_{\\text{indep}} = \n\\begin{pmatrix} \n\\boldsymbol P_{\\boldsymbol x} &  0 \\\\\n0 & \\boldsymbol P_{\\boldsymbol y} \\\\\n\\end{pmatrix} \\, .\n\\]Specifically, looking scalar\nquantity quantifies divergence general joint correlation matrix \\(\\boldsymbol P\\)\njoint correlation matrix \\(\\boldsymbol P_{\\text{indep}}\\) assuming uncorrelated\n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\).Ideally, case univariate \\(y\\)\nmeasure reduce \nsquared multiple correlation coefficient determination\n\\[\n\\text{MCor}(\\boldsymbol x, y)^2 = \\boldsymbol P_{y\\boldsymbol x} \\boldsymbol P_{\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\n\\]linear regression \nstandard measure describe strength total linear association \npredictors \\(\\boldsymbol x\\) response \\(y\\). Note marginal correlations\nvanish \\(\\boldsymbol P_{\\boldsymbol xy} =0\\) \\(\\text{MCor}(\\boldsymbol x, y)^2=0\\).single predictor \\(x\\) \\(\\boldsymbol P_{xy}=\\rho\\) \\(\\boldsymbol P_{x} = 1\\)\nsquared multiple correlation reduces squared Pearson correlation\n\\[\n\\text{Cor}(x, y)^2 = \\rho^2 \\, .\n\\]","code":""},{"path":"multivariate-dependencies.html","id":"vector-alienation-coefficient","chapter":"5 Multivariate dependencies","heading":"5.2.2 Vector alienation coefficient","text":"Hotelling’s vector alienation\ncoefficient given 1936 CCA paper20 \n\\[\n\\begin{split}\n(\\boldsymbol x, \\boldsymbol y) &= \\frac{\\det(\\boldsymbol P)}{\\det(\\boldsymbol P_{\\text{indep}}) } \\\\\n            & = \\frac{\\det( \\boldsymbol P) }{  \\det(\\boldsymbol P_{\\boldsymbol x}) \\,  \\det(\\boldsymbol P_{\\boldsymbol y})  }\n\\end{split}\n\\]\n\\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\boldsymbol P_{\\boldsymbol y}^{-1/2}\\) vector alienation coefficient can written\n(using Weinstein-Aronszajn determinant identity formula determinant\nblock-structured matrices, see Appendix) \n\\[\n\\begin{split}\n(\\boldsymbol x, \\boldsymbol y) & = \\det \\left( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T \\right) \\\\\n            & = \\det \\left( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K\\right) \\\\\n            &= \\prod_{=1}^m (1-\\lambda_i^2) \\\\\n\\end{split}\n\\]\n\\(\\lambda_i\\) singular values \\(\\boldsymbol K\\), .e. canonical correlations\npair \\(\\boldsymbol x\\) \\(\\boldsymbol y\\).\\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = 0\\) und thus \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) uncorrelated \\(\\boldsymbol P= \\boldsymbol P_{\\text{indep}}\\) \nthus construction vector alienation coefficient \\((\\boldsymbol x, \\boldsymbol y)=1\\).\nHence, generalisation squared multiple correlation.","code":""},{"path":"multivariate-dependencies.html","id":"rozeboom-vector-correlation","chapter":"5 Multivariate dependencies","heading":"5.2.3 Rozeboom vector correlation","text":"Instead, Rozeboom (1965)21 proposed use squared vector correlation\ncomplement vector alienation\n\\[\n\\begin{split}\n\\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2 &= \\rho^2_{\\boldsymbol x\\boldsymbol y} = 1 - (\\boldsymbol x, \\boldsymbol y) \\\\\n & = 1- \\det\\left( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T \\right) \\\\\n & = 1- \\det\\left( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K\\right) \\\\\n &  =1- \\prod_{=1}^m (1-\\lambda_i^2) \\\\\n\\end{split}\n\\]\n\\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = 0\\) \\(\\boldsymbol K=\\boldsymbol 0\\) hence \\(\\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2 = 0\\).Moreover, either \\(p=1\\) \\(q=1\\) squared vector correlation\nreduces corresponding squared multiple correlation,\n\\(p=1\\) \\(q=1\\) becomes squared Pearson correlation.\nThus, Rozeboom’s vector correlation generalised Pearson correlation\ncofficient determination.general way measure multivariate association mutual information (MI)\ncovers linear also non-linear association.\ndiscuss subsequent section Rozeboom vector\ncorrelation arises naturally computing MI multivariate normal distribution.","code":""},{"path":"multivariate-dependencies.html","id":"rv-coefficient","chapter":"5 Multivariate dependencies","heading":"5.2.4 RV coefficient","text":"Another common classical approach measure association two random vectors RV coefficient introduced Robert Escoufier 1976 \n\\[\nRV(\\boldsymbol x, \\boldsymbol y) = \\frac{ \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} )}{ \\sqrt{ \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol x}^2) \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol y}^2)  } }\n\\]\neasier compute Rozeboom vector correlation since based\nmatrix trace rather matrix determinant.\\(q=p=1\\) RV coefficient reduces squared correlation.\nHowever, RV coefficient reduce multiple correlation coefficient\n\\(q=1\\) \\(p > 1\\), therefore RV coefficient considered coherent generalisation\nPearson multiple correlation case two random vectors.See also Worksheet 10.","code":""},{"path":"multivariate-dependencies.html","id":"limits-of-linear-models-and-correlation","chapter":"5 Multivariate dependencies","heading":"5.3 Limits of linear models and correlation","text":"","code":""},{"path":"multivariate-dependencies.html","id":"correlation-measures-only-linear-dependence","chapter":"5 Multivariate dependencies","heading":"5.3.1 Correlation measures only linear dependence","text":"Linear models measures linear association (correlation) effective tools. However, important\nrecognise limits especially modelling complex nonlinear relationships.simple demonstration given following example. Assume \\(x\\) normally distributed\nrandom variable \\(x \\sim N(0,1)\\). \\(x\\) construct second random variable \\(y = x^2\\) — thus \\(y\\) fully depends \\(x\\) added extra noise. correlation \\(x\\) \\(y\\)?Let’s answer question running small computer simulation:Thus, correlation (almost) zero even though \\(x\\) \\(y\\) dependent variables.\ncorrelation measures linear association, relationship \n\\(x\\) \\(y\\) nonlinear.","code":"\nx=rnorm(10000)\ny = x^2\ncor(x,y)## [1] -0.007153187"},{"path":"multivariate-dependencies.html","id":"anscombe-data-sets","chapter":"5 Multivariate dependencies","heading":"5.3.2 Anscombe data sets","text":"Using correlation, generally linear models, blindly can easily hide underlying complexity analysed\ndata. demonstrated classic “Anscombe quartet” data sets presented 1973 paper22 -evident scatter plots relationship \ntwo variables \\(x\\) \\(y\\) different four cases!\nHowever, intriguingly four data sets share exactly linear characteristics summary statistics:Means \\(m_x = 9\\) \\(m_y = 7.5\\)Variances \\(s^2_x = 11\\) \\(s^2_y = 4.13\\)Correlation \\(r = 0.8162\\)Linear model fit intercept \\(=3.0\\) slope \\(b=0.5\\)Thus, actual data analysis always good idea inspect data visually get first impression whether using linear model makes sense.data “” follows linear model. Data “b” represents quadratic relationship. Data “c” linear outlier disturbs linear relationship. Finally data “d” also contains outlier also represent case \\(y\\) (apart outlier) dependent \\(x\\).Worksheet 10 recent version Anscombe quartet analysed form “datasauRus” dozen - 13 highly nonlinear datasets share \nlinear characteristics.","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-as-generalisation-of-correlation","chapter":"5 Multivariate dependencies","heading":"5.4 Mutual information as generalisation of correlation","text":"","code":""},{"path":"multivariate-dependencies.html","id":"definition-of-mutual-information","chapter":"5 Multivariate dependencies","heading":"5.4.1 Definition of mutual information","text":"Recall definition\nKullback-Leibler divergence, relative entropy, two distributions:\n\\[\nD_{\\text{KL}}(F,  G) := \\text{E}_F \\log \\biggl( \\frac{f(\\boldsymbol x)}{g(\\boldsymbol x)} \\biggr) \n\\]\n\\(F\\) plays role reference distribution \\(G\\) approximating distribution,\n\\(f\\) \\(g\\) corresponding density functions\n(see MATH20802 Statistical Methods).Mutual Information (MI) two random variables \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) defined \nKL divergence corresponding joint distribution product distribution:\n\\[\n\\text{MI}(\\boldsymbol x, \\boldsymbol y) = D_{\\text{KL}}(F_{\\boldsymbol x,\\boldsymbol y}, F_{\\boldsymbol x}  F_{\\boldsymbol y}) = \\text{E}_{F_{\\boldsymbol x,\\boldsymbol y}}  \\log \\biggl( \\frac{f(\\boldsymbol x, \\boldsymbol y)}{f(\\boldsymbol x) \\, f(\\boldsymbol y)} \\biggr) .\n\\]\nThus, MI measures well joint distribution can approximated product\ndistribution (appropriate joint distribution \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) independent).\nSince MI application KL divergence shares properties. particular,\n\\(\\text{MI}(\\boldsymbol x, \\boldsymbol y)=0\\) implies joint distribution product distributions . Hence two random variables \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) independent mutual information vanishes.","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-between-two-normal-variables","chapter":"5 Multivariate dependencies","heading":"5.4.2 Mutual information between two normal variables","text":"KL divergence two multivariate normal distributions \\(F_{\\text{ref}}\\) \\(F\\) \n\\[\nD_{\\text{KL}}(F_{\\text{ref}}, F)  = \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr) \n     - d   \\biggr\\} \n\\]\nallows compute mutual information \\(\\text{MI}_{\\text{norm}}(x,y)\\) two univariate random variables \\(x\\) \\(y\\) correlated assumed jointly bivariate normal. Let \\(\\boldsymbol z= (x, y)^T\\). joint bivariate normal distribution characterised mean \\(\\text{E}(\\boldsymbol z) = \\boldsymbol \\mu= (\\mu_x, \\mu_y)^T\\) covariance matrix\n\\[\n\\boldsymbol \\Sigma=\n\\begin{pmatrix} \n\\sigma^2_x & \\rho \\, \\sigma_x \\sigma_y \\\\\n\\rho \\, \\sigma_x  \\sigma_y & \\sigma^2_y \\\\ \n\\end{pmatrix}\n\\]\n\\(\\text{Cor}(x,y)= \\rho\\). \\(x\\) \\(y\\) independent \n\\(\\rho=0\\) \n\\[\n\\boldsymbol \\Sigma_{\\text{indep}} = \n\\begin{pmatrix} \\sigma^2_x & 0 \\\\ 0 & \\sigma^2_y \\\\ \\end{pmatrix} \\,.\n\\]\nproduct\n\\[\n\\boldsymbol = \\boldsymbol \\Sigma_{\\text{indep}}^{-1} \\boldsymbol \\Sigma= \n\\begin{pmatrix}\n1 & \\rho \\frac{\\sigma_y}{\\sigma_x} \\\\\n\\rho \\frac{\\sigma_x}{\\sigma_y} & 1 \\\\\n\\end{pmatrix}\n\\]\ntrace \\(\\text{Tr}(\\boldsymbol ) = 2\\) determinant \\(\\det(\\boldsymbol ) = 1-\\rho^2\\).mutual information \\(x\\) \\(y\\) can computed \n\\[\n\\begin{split}\n\\text{MI}_{\\text{norm}}(x, y) &= D_{\\text{KL}}(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma),  N(\\boldsymbol \\mu,\\boldsymbol \\Sigma_{\\text{indep}} )) \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr) \n     - 2   \\biggr\\} \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}( \\boldsymbol )\n    - \\log \\det( \\boldsymbol ) \n     - 2   \\biggr\\} \\\\\n&=  -\\frac{1}{2} \\log(1-\\rho^2) \\\\\n  & \\approx \\frac{\\rho^2}{2} \\\\\n\\end{split}\n\\]Thus \\(\\text{MI}_{\\text{norm}}(x,y)\\) one--one function squared correlation \\(\\rho^2\\) \\(x\\) \\(y\\):small values correlation \\(2 \\, \\text{MI}_{\\text{norm}}(x,y) \\approx \\rho^2\\).","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-between-two-normally-distributed-random-vectors","chapter":"5 Multivariate dependencies","heading":"5.4.3 Mutual information between two normally distributed random vectors","text":"mutual information \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x,\\boldsymbol y)\\) two multivariate normal random vector \\(\\boldsymbol x\\) \\(\\boldsymbol y\\)\ncan computed similar fashion bivariate case.Let \\(\\boldsymbol z= (\\boldsymbol x, \\boldsymbol y)^T\\) dimension \\(d=p+q\\). joint multivariate\nnormal distribution characterised mean \\(\\text{E}(\\boldsymbol z) = \\boldsymbol \\mu= (\\boldsymbol \\mu_x^T, \\boldsymbol \\mu_y^T)^T\\) covariance matrix\n\\[\n\\boldsymbol \\Sigma=\n\\begin{pmatrix} \\boldsymbol \\Sigma_{\\boldsymbol x} & \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\\\ \n\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}^T & \\boldsymbol \\Sigma_{\\boldsymbol y} \\\\ \n\\end{pmatrix} \\,.\n\\]\n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\) independent \n\\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} = 0\\) \n\\[\n\\boldsymbol \\Sigma_{\\text{indep}} =\n\\begin{pmatrix}  \n\\boldsymbol \\Sigma_{\\boldsymbol x} & 0 \\\\ \n0 & \\boldsymbol \\Sigma_{\\boldsymbol y} \\\\ \n\\end{pmatrix} \\, .\n\\]\nproduct\n\\[\n\\begin{split}\n\\boldsymbol & = \n\\boldsymbol \\Sigma_{\\text{indep}}^{-1} \\boldsymbol \\Sigma= \n\\begin{pmatrix}\n\\boldsymbol I_p & \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol \\Sigma_{\\boldsymbol y}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x}    & \\boldsymbol I_q \\\\\n\\end{pmatrix} \\\\\n& = \n\\begin{pmatrix}\n\\boldsymbol I_p &  \\boldsymbol V_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\boldsymbol V_{\\boldsymbol y}^{1/2} \\\\\n\\boldsymbol V_{\\boldsymbol y}^{-1/2} \\boldsymbol P_{\\boldsymbol y}^{-1} \\boldsymbol P_{\\boldsymbol y\\boldsymbol x} \\boldsymbol V_{\\boldsymbol x}^{1/2}   & \\boldsymbol I_q \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\]\ntrace \\(\\text{Tr}(\\boldsymbol ) = d\\) determinant\n\\[\n\\begin{split}\n\\det(\\boldsymbol ) & = \\det( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T ) \\\\\n  &= \\det( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K) \\\\\n\\end{split}\n\\]\n\\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\boldsymbol P_{\\boldsymbol y}^{-1/2}\\).\n\\(\\lambda_1, \\ldots, \\lambda_m\\) singular values \\(\\boldsymbol K\\) (.e.\ncanonical correlations \\(\\boldsymbol x\\) \\(\\boldsymbol y\\)) get\n\\[\n\\det(\\boldsymbol ) =  \\prod_{=1}^m (1-\\lambda_i^2)\n\\]mutual information \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) \n\\[\n\\begin{split}\n\\text{MI}_{\\text{norm}}(\\boldsymbol x, \\boldsymbol y) &= D_{\\text{KL}}(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma),  N(\\boldsymbol \\mu,\\boldsymbol \\Sigma_{\\text{indep}} )) \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}\\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr) \n     - d   \\biggr\\} \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}( \\boldsymbol )\n    - \\log \\det( \\boldsymbol ) \n     - d   \\biggr\\} \\\\\n&=-\\frac{1}{2} \\sum_{=1}^m \\log(1-\\lambda_i^2)\\\\\n\\end{split}\n\\]Note \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x, \\boldsymbol y)\\) sum MIs resulting \nindividual canonical correlations \\(\\lambda_i\\) functional\nform bivariate normal case.comparison squared Rozeboom vector correlation coefficient \\(\\rho^2_{\\boldsymbol x\\boldsymbol y}\\)\nrecognize \n\\[\n\\text{MI}_{\\text{norm}}(\\boldsymbol x,\\boldsymbol y) = -\\frac{1}{2} \\log(1 - \\rho^2_{\\boldsymbol x\\boldsymbol y} ) \\approx \\frac{1}{2} \\rho^2_{\\boldsymbol x\\boldsymbol y}\n\\]\nThus, multivariate case \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x\\boldsymbol y)\\) exactly functional relationship \nvector correlation \\(\\rho^2_{\\boldsymbol x, \\boldsymbol y}\\) \\(\\text{MI}_{\\text{norm}}(x, y)\\)\ntwo univariate variables squared Pearson correlation \\(\\rho^2\\).Thus, Rozeboom’s vector correlation directly linked mutual information jointly multivariate normally distributed variables.","code":""},{"path":"multivariate-dependencies.html","id":"using-mi-for-variable-selection","chapter":"5 Multivariate dependencies","heading":"5.4.4 Using MI for variable selection","text":"general way write model predicting \\(\\boldsymbol y\\) \\(\\boldsymbol x\\) follows:\\(F_{\\boldsymbol y|\\boldsymbol x}\\) conditional distribution \\(\\boldsymbol y\\) given predictors \\(\\boldsymbol x\\) \\(F_{\\boldsymbol y}\\) marginal distribution \\(\\boldsymbol y\\) without predictors.Typically \\(F_{\\boldsymbol y|\\boldsymbol x}\\) complex model \\(F_{\\boldsymbol y}\\)\nsimple model (predictors). Note predictive model can assume form (incl. nonlinear).Intriguingly expected KL divergence\nconditional marginal distribution\n\\[\n\\text{E}_{F_{\\boldsymbol x}}\\, D_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x},  F_{\\boldsymbol y} ) = \\text{MI}(\\boldsymbol x, \\boldsymbol y)\n\\]\nequal mutual information \n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\)! Thus \\(\\text{MI}(\\boldsymbol x, \\boldsymbol y)\\) measures impact conditioning. MI small (.e. close zero) \n\\(\\boldsymbol x\\) useful predicting \\(\\boldsymbol y\\).identity can verified follows.\nKL divergence \\(F_{\\boldsymbol y|\\boldsymbol x}\\) \\(F_{\\boldsymbol y}\\)\ngiven \n\\[\nD_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x}, F_{\\boldsymbol y} )  = \\text{E}_{F_{\\boldsymbol y|\\boldsymbol x}}  \\log\\biggl( \\frac{f(\\boldsymbol y|\\boldsymbol x) }{ f(\\boldsymbol y)}  \\biggr) \\, , \n\\]\nrandom variable since depends \\(\\boldsymbol x\\).\nTaking expectation regard \\(F_{\\boldsymbol x}\\) (distribution \\(\\boldsymbol x\\))\nget\n\\[\n\\begin{split}\n\\text{E}_{F_{\\boldsymbol x}} D_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x}, F_{\\boldsymbol y} ) &= \n\\text{E}_{F_{\\boldsymbol x}}  \\text{E}_{F_{\\boldsymbol y|\\boldsymbol x}}  \\log \\biggl(\\frac{ f(\\boldsymbol y|\\boldsymbol x) f(\\boldsymbol x) }{ f(\\boldsymbol y) f(\\boldsymbol x) } \\biggr)\\\\\n& = \n\\text{E}_{F_{\\boldsymbol x,\\boldsymbol y}}   \\log \\biggl(\\frac{ f(\\boldsymbol x,\\boldsymbol y) }{ f(\\boldsymbol y) f(\\boldsymbol x) } \\biggr) = \\text{MI}(\\boldsymbol x,\\boldsymbol y) \\,. \\\\\n\\end{split}\n\\]link MI conditioning MI response predictor variables often used variable feature selection general models.","code":""},{"path":"multivariate-dependencies.html","id":"other-measures-of-general-dependence","chapter":"5 Multivariate dependencies","heading":"5.4.5 Other measures of general dependence","text":"principle, MI can computed distribution model thus applies normal non-normal models,\nlinear nonlinear relationships.Besides mutual information others measures general dependence multivariate random variables.Two important measures capture nonlinear association proposed recent literature aredistance correlation andthe maximal information coefficient (MIC \\(\\text{MIC}_e\\)).","code":""},{"path":"multivariate-dependencies.html","id":"graphical-models","chapter":"5 Multivariate dependencies","heading":"5.5 Graphical models","text":"","code":""},{"path":"multivariate-dependencies.html","id":"purpose","chapter":"5 Multivariate dependencies","heading":"5.5.1 Purpose","text":"Graphical models combine features fromgraph theoryprobabilitystatistical inferenceThe literature graphical models huge, focus two commonly\nused models:DAGs (directed acyclic graphs), edges directed, directed loops (.e. cycles, hence “acyclic”)GGM (Gaussian graphical models), edges undirectedGraphical models provide probabilistic models trees networks, \nrandom variables represented nodes graphs, branches representing\nconditional dependencies. regard generalise tree-based clustering approaches well probabilistic non-hierarchical methods (GMMs).However, class graphical models goes much beyond simple\nunsupervised learning models. also includes regression, classification,\ntime series models etc. See e.g. reference book Murphy (2012).","code":""},{"path":"multivariate-dependencies.html","id":"basic-notions-from-graph-theory","chapter":"5 Multivariate dependencies","heading":"5.5.2 Basic notions from graph theory","text":"Mathematically, graph \\(G = (V, E)\\) consists set vertices nodes \\(V = \\{v_1, v_2, \\ldots\\}\\) set branches edges \\(E = \\{ e_1, e_2, \\ldots \\}\\).Edges can undirected directed.Graphs containing directed edges directed graphs, likewise graphs containing undirected edges called undirected graphs. Graphs containing directed undirected edges called partially directed graphs.path sequence vertices vertices edge next vertex sequence.graph connected path every pair vertices.cycle path graph connects node .connected graph cycles called tree.degree node number edges connects . edges directed degree node sum -degree -degree, counts incoming outgoing edges, respectively.External nodes nodes degree 1. tree-structured graph also called leaves.notions relevant graphs directed edges:directed graph parent node(s) vertex \\(v\\) set nodes \\(\\text{pa}(v)\\) directly connected \\(v\\) via edges directed parent node(s) towards \\(v\\).Conversely, \\(v\\) called child node \\(\\text{pa}(v)\\). Note parent node can several child nodes, \\(v\\) may child \\(\\text{pa}(v)\\).directed tree graph, node single parent, except one particular node parent (node called root node).DAG, directed acyclic graph, directed graph directed cycles. (directed) tree special version DAG.","code":""},{"path":"multivariate-dependencies.html","id":"probabilistic-graphical-models","chapter":"5 Multivariate dependencies","heading":"5.5.3 Probabilistic graphical models","text":"graphical model uses graph describe relationship random variables \\(x_1, \\ldots, x_d\\). variables assumed joint distribution density/mass function \\(p(x_1, x_2, \\ldots, x_d)\\).\nrandom variable placed node graph.structure graph type edges connecting (connecting) pair nodes/variables used describe conditional dependencies, simplify joint distribution.Thus, graphical model essence visualisation joint distribution using structural information graph helping understand mutual relationship among variables.","code":""},{"path":"multivariate-dependencies.html","id":"directed-graphical-models","chapter":"5 Multivariate dependencies","heading":"5.5.4 Directed graphical models","text":"directed graphical model graph structure assumed \nDAG (directed tree, also DAG).joint probability distribution can factorised product conditional probabilities follows:\n\\[\np(x_1, x_2, \\ldots, x_d) = \\prod_i p(x_i  | \\text{pa}(x_i))\n\\]\nThus, overall joint probability distribution specified local conditional distributions graph structure, directions edges providing information parent-child node relationships.Probabilistic DAGs also known “Bayesian networks”.Idea: trying possible trees/graphs fitting data using maximum likelihood (Bayesian inference) hope able identify graph structure data-generating process.Challengesin tree/network internal nodes usually known, thus \ntreated latent variables.Answer: impute states nodes may use EM algorithm GMMs\n(fact can viewed graphical models, !).treat internal nodes unknowns need marginalise \ninternal nodes, .e. need sum / integrate possible set states\ninternal nodes!Answer: can handled effectively using Viterbi algorithm essentially\napplication generalised distributive law. particular tree graphs \nmeans summations occurs locally node propagates recursively across tree.order infer tree network structure space trees networks need \nexplored. possible exhaustive fashion unless number variables\ntree small.Answer: Solution: use heuristic approaches tree network search!Furthermore, exist -called “equivalence classes” graphical models, .e. sets graphical models share joint probability distribution. Thus, graphical models within equivalence class distinguished observational data, even infinite sample size!Answer: fundamental mathematical problem identifiability now way around issue. However,\npositive side, also implies search graphical models can restricted finding -called “essential graph” (e.g. https://projecteuclid.org/euclid.aos/1031833662 )Conclusion: using directed graphical models structure discovery time consuming computationally\ndemanding anything small toy data sets.also explains heuristic non-model based approaches (hierarchical clustering) popular even though full statistical modelling principle possible.","code":""},{"path":"multivariate-dependencies.html","id":"undirected-graphical-models","chapter":"5 Multivariate dependencies","heading":"5.5.5 Undirected graphical models","text":"Another class graphical models models contain undirected edges. undirected graphical models\nused represent pairwise conditional ()dependencies among variables graph, resulting model therefore also called conditional independence graph.\\(x_i\\) \\(x_j\\) two selected random variables/nodes, set \\(\\{x_k\\}\\) represents variables/nodes \\(k\\neq \\) \\(k \\neq j\\). say variables \\(x_i\\) \\(x_j\\) conditionally independent\ngiven variables \\(\\{x_k\\}\\)\n\\[\nx_i \\perp\\!\\!\\!\\perp x_j | \\{x_k\\}\n\\]\njoint probability density \\(x_i, x_j\\) \\(x_k\\)\nfactorises \n\\[\n p(x_1, x_2, \\ldots, x_d) = p(x_i | \\{x_k\\}) p(x_j | \\{x_k\\}) p(\\{x_k\\}) \\,.\n \\]\nequivalently\n\\[\n p(x_i, x_j | \\{x_k\\}) = p(x_i | \\{x_k\\}) p(x_j | \\{x_k\\}) \\,.\n \\]corresponding conditional independence graph, edge \\(x_i\\) \\(x_j\\),\ngraph missing edges correspond conditional independence respective non-connected nodes.","code":""},{"path":"multivariate-dependencies.html","id":"gaussian-graphical-model","chapter":"5 Multivariate dependencies","heading":"5.5.5.1 Gaussian graphical model","text":"Assuming \\(x_1, \\ldots, x_d\\) jointly normally distributed, .e. \\(\\boldsymbol x\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\),\nturns straightforward identify pairwise conditional independencies.\n\\(\\boldsymbol \\Sigma\\) first obtain precision matrix\n\\[\\boldsymbol \\Omega= (\\omega_{ij}) = \\boldsymbol \\Sigma^{-1} \\,.\\]\nCrucially, can shown \n\\(\\omega_{ij} = 0\\) implies\n\\(x_i \\perp\\!\\!\\!\\perp x_j \\,|\\, \\{ x_k \\}\\).\nHence, precision matrix \\(\\boldsymbol \\Omega\\) can directly read pairwise conditional independencies among variables \\(x_1, x_2, \\ldots, x_d\\).Often, covariance matrix \\(\\boldsymbol \\Sigma\\) dense (zeros) corresponding precision matrix\n\\(\\boldsymbol \\Omega\\) sparse (many zeros).conditional independence graph computed normally distributed variables called\nGaussian graphical model, short GGM. alternative name\ncommonly used covariance selection model.","code":""},{"path":"multivariate-dependencies.html","id":"related-quantity-partial-correlation","chapter":"5 Multivariate dependencies","heading":"5.5.5.2 Related quantity: partial correlation","text":"precision matrix \\(\\boldsymbol \\Omega\\) can also compute matrix pairwise full conditional partial correlations:\\[\n\\rho_{ij|\\text{rest}}=-\\frac{\\omega_{ij}}{\\sqrt{\\omega_{ii}\\omega_{jj}}}\n\\]\nessentially standardised precision matrix (similar correlation extra minus sign!)partial correlations lie range -1 +1, \\(\\rho_{ij|\\text{rest}} \\[-1, 1]\\), just like standard correlations.\\(\\boldsymbol x\\) multivariate normal \\(\\rho_{ij|\\text{rest}} = 0\\) indicates conditional independence\n\\(x_i\\) \\(x_j\\).Regression interpretation: partial correlation correlation remains \ntwo variables effect variables “regressed away”.\nwords, partial correlation exactly equivalent correlation \nresiduals remain regressing \\(x_i\\) variables \\(\\{x_k\\}\\) \\(x_j\\) \\(\\{x_k\\}\\).","code":""},{"path":"multivariate-dependencies.html","id":"null-distribution-of-the-empirical-correlation-coefficient","chapter":"5 Multivariate dependencies","heading":"5.5.6 Null distribution of the empirical correlation coefficient","text":"Suppose two uncorrelated random variables \\(x\\) \\(y\\) \\(\\rho = \\text{Cor}(x, y) =0\\).\nobserving data \\(x_1, \\ldots, x_n\\) \\(y_1, \\ldots, y_n\\) compute \nempirical covariance matrix \\(\\hat{\\boldsymbol \\Sigma}_{xy}\\) \nempirical correlation coefficient \\(r = \\widehat{\\text{Cor}}(x, y)\\).distribution empirical correlation assuming \\(\\rho=0\\) useful null-model testing whether underlying correlation fact zero observed empirical correlation \\(r\\).\n\\(x\\) \\(y\\) normally distributed \\(\\rho=0\\) distribution empirical correlation \\(r\\) mean \\(\\text{E}(r)=0\\) variance \\(\\text{Var}(r)=\\frac{1}{\\kappa}\\).\n\\(\\kappa\\) degree freedom null distribution standard correlation \\(\\kappa=n-1\\).\nFurthermore, squared empirical correlation distributed according Beta distribution\n\\[\nr^2 \\sim \\text{Beta}\\left(\\frac{1}{2}, \\frac{\\kappa-1}{2}\\right)\n\\]partial correlation null distribution \\(r^2\\) form different degree freedom.\nSpecifically, \\(\\kappa\\) reduced number variables conditioned .\n\\(d\\) dimensions condition \\(d-2\\) variables resulting degree freedom \\(\\kappa =n-1 - (d-2) = n-d+1\\). \\(d=2\\) get back degree freedom \nstandard empirical correlation.","code":""},{"path":"multivariate-dependencies.html","id":"algorithm-for-learning-ggms","chapter":"5 Multivariate dependencies","heading":"5.5.7 Algorithm for learning GGMs","text":"can devise simple algorithm learn Gaussian graphical model (GGM)\ndata:Estimate covariance \\(\\hat{\\boldsymbol \\Sigma}\\) (way invertible!)Compute corresponding partial correlationsIf \\(\\hat{\\rho}_{ij|\\text{rest}} \\approx 0\\) (approx.) conditional\nindependence \\(x_i\\) \\(x_j\\).test conditional independence done statistical testing vanishing partial correlation. Specifically, compute \\(p\\)-value assuming true underlying partial correlation zero decide whether reject null assumption zero partial correlation.many edges tested simultaneously may need adjust (.e reduce) \ntest threshold, example applying Bonferroni FDR methods.","code":""},{"path":"multivariate-dependencies.html","id":"example-exam-score-data","chapter":"5 Multivariate dependencies","heading":"5.5.8 Example: exam score data","text":"data set Mardia et al. (1979) features \\(d=5\\) variables measured \n\\(n=88\\) subjects.Correlations (rounded 2 digits):Partial correlations (rounded 2 digits):Note zero correlations \nfour partial correlations close 0, indicating conditional independence :analysis mechanics,statistics mechanics,analysis vectors, andstatistics vectors.can verified computing normal \\(p\\)-values partial correlations (\\(\\kappa=84\\) degree freedom):six edges small \\(p\\)-value (smaller say 0.05) correspond\nedges null assumption zero partial correlation can rejected\nten possible edges four statistically significant.\nTherefore conditional independence graph looks follows:","code":"##            mechanics vectors algebra analysis statistics\n## mechanics       1.00    0.55    0.55     0.41       0.39\n## vectors         0.55    1.00    0.61     0.49       0.44\n## algebra         0.55    0.61    1.00     0.71       0.66\n## analysis        0.41    0.49    0.71     1.00       0.61\n## statistics      0.39    0.44    0.66     0.61       1.00##            mechanics vectors algebra analysis statistics\n## mechanics       1.00    0.33    0.23     0.00       0.02\n## vectors         0.33    1.00    0.28     0.08       0.02\n## algebra         0.23    0.28    1.00     0.43       0.36\n## analysis        0.00    0.08    0.43     1.00       0.25\n## statistics      0.02    0.02    0.36     0.25       1.00##            mechanics vectors algebra analysis statistics\n## mechanics         NA   0.002   0.034    0.988      0.823\n## vectors           NA      NA   0.009    0.477      0.854\n## algebra           NA      NA      NA    0.000      0.001\n## analysis          NA      NA      NA       NA      0.020\n## statistics        NA      NA      NA       NA         NAMechanics      Analysis\n   |     \\    /    |\n   |    Algebra    |\n   |     /   \\     |\n Vectors      Statistics"},{"path":"nonlinear-and-nonparametric-models.html","id":"nonlinear-and-nonparametric-models","chapter":"6 Nonlinear and nonparametric models","heading":"6 Nonlinear and nonparametric models","text":"last part module discuss methods go beyond \nlinear parametric methods prevalent classical multivariate statistics.Relevant textbooks:lectures much part module follow selected chapters following text books:James et al. (2021) introduction statistical learning applications R (2nd edition). Springer.James et al. (2021) introduction statistical learning applications R (2nd edition). Springer.Rogers Girolami (2017) first course machine learning (2nd edition). CRC Press.Rogers Girolami (2017) first course machine learning (2nd edition). CRC Press.Please study relevant section chapters indicated subsection!","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"random-forests","chapter":"6 Nonlinear and nonparametric models","heading":"6.1 Random forests","text":"Another widely used approach prediction nonlinear settings\nmethod random forests.Relevant reading:Please read: James et al. (2021) Chapter 8 “Tree-Based Methods”Specifically:Section 8.1 Basics Decision TreesSection 8.2.1 BaggingSection 8.2.2 Random Forests","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"stochastic-vs.-algorithmic-models","chapter":"6 Nonlinear and nonparametric models","heading":"6.1.1 Stochastic vs. algorithmic models","text":"Two cultures statistical modelling: stochastic vs. algorithmic modelsClassic discussion paper Leo Breiman (2001): Statistical modeling: two cultures.\nStatistical Science 16:199–231. https://doi.org/10.1214/ss/1009213726This paper recently revisited following discussion paper Efron (2020) discussants:\nPrediction, estimation, attribution. JASA 115:636–677. https://doi.org/10.1080/01621459.2020.1762613","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"random-forests-1","chapter":"6 Nonlinear and nonparametric models","heading":"6.1.2 Random forests","text":"Proposed Leo Breimann 2001 application “bagging” (Breiman 1996) decision trees.Basic idea:single decision tree unreliable unstable (weak predictor/classifier).Use boostrap generate multiple decision trees (=“forest”)Average predictions tree (=“bagging”, bootstrap aggregation)averaging procedure effect variance stabilisation.\nIntringuingly, averaging across decision trees dramatically improves \noverall prediction accuracy!Random Forests approach example ensemble method\n(since based using “ensemble” trees).Variations: boosting, XGBoost ( https://xgboost.ai/ )Random forests applied Worksheet 11.computationally expensive typically perform well!","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"comparison-of-decision-boundaries-decision-tree-vs.-random-forest","chapter":"6 Nonlinear and nonparametric models","heading":"6.1.3 Comparison of decision boundaries: decision tree vs. random forest","text":"Non-nested case:Compare also decision boundaries LDA QDA (Chapter 4).","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gaussian-processes","chapter":"6 Nonlinear and nonparametric models","heading":"6.2 Gaussian processes","text":"Gaussian processes offer another nonparametric approach model\nnonlinear dependencies. provide probabilistic model \nunknown nonlinear function.Relevant reading:Please read: Rogers Girolami (2017) Chapter 8: Gaussian processes.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"main-concepts","chapter":"6 Nonlinear and nonparametric models","heading":"6.2.1 Main concepts","text":"Gaussian processes (GPs) belong family Bayesian nonparametric modelsIdea:\nstart prior function (!),\ncondition observed data get posterior distribution (function)\nstart prior function (!),condition observed data get posterior distribution (function)GPs use infinitely dimensional multivariate normal distribution prior","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"conditional-multivariate-normal-distribution","chapter":"6 Nonlinear and nonparametric models","heading":"6.2.2 Conditional multivariate normal distribution","text":"GPs make use fact marginal conditional distributions multivariate normal\ndistribution also multivariate normal.Multivariate normal distribution:\\[\\boldsymbol z\\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\]Assume:\n\\[\n\\boldsymbol z=\\begin{pmatrix}\n    \\boldsymbol z_1      \\\\\n    \\boldsymbol z_2      \\\\\n\\end{pmatrix}\n\\]\n\n\\[\n\\boldsymbol \\mu=\\begin{pmatrix}\n    \\boldsymbol \\mu_1      \\\\\n    \\boldsymbol \\mu_2      \\\\\n\\end{pmatrix}\n\\]\n\n\\[\n\\boldsymbol \\Sigma=\\begin{pmatrix}\n    \\boldsymbol \\Sigma_{1}   & \\boldsymbol \\Sigma_{12}   \\\\\n    \\boldsymbol \\Sigma_{12}^T & \\boldsymbol \\Sigma_{2}   \\\\\n\\end{pmatrix}\n\\]\ncorresponding dimensions \\(d_1\\) \\(d_2\\) \\(d_1+d_2=d\\).Marginal distributions:subset \\(\\boldsymbol z\\) also multivariate normally distributed.\nSpecifically,\n\\[\n\\boldsymbol z_1 \\sim N_{d_1}(\\boldsymbol \\mu_1, \\boldsymbol \\Sigma_{1}) \n\\]\n\n\\[\n\\boldsymbol z_2 \\sim N_{d_2}(\\boldsymbol \\mu_2, \\boldsymbol \\Sigma_{2}) \n\\]Conditional multivariate normal:conditional distribution also multivariate normal:\n\\[\n\\boldsymbol z_1 | \\boldsymbol z_2 = \\boldsymbol z_{1 | 2} \\sim N_{d_1}(\\boldsymbol \\mu_{1|2}, \\boldsymbol \\Sigma_{1 | 2}) \n\\]\n\n\\[\\boldsymbol \\mu_{1|2}=\\boldsymbol \\mu_1 + \\boldsymbol \\Sigma_{12} \\boldsymbol \\Sigma_{2}^{-1} (\\boldsymbol z_2 -\\boldsymbol \\mu_2)\\]\n\n\\[\\boldsymbol \\Sigma_{1 | 2}=\\boldsymbol \\Sigma_{1} -  \\boldsymbol \\Sigma_{12} \\boldsymbol \\Sigma_{2}^{-1} \\boldsymbol \\Sigma_{12}^T\\]\\(\\boldsymbol z_{1 | 2}\\) \n\\(\\boldsymbol \\mu_{1|2}\\) dimension \\(d_1 \\times 1\\)\n\\(\\boldsymbol \\Sigma_{1 | 2}\\) dimension \\(d_1 \\times d_1\\),\n.e. dimension unconditioned variables.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"covariance-functions-and-kernels","chapter":"6 Nonlinear and nonparametric models","heading":"6.2.3 Covariance functions and kernels","text":"GP prior infinitely dimensional multivariate normal\nmean zero covariance specified function \\(k(x, x^{\\prime})\\):widely used covariance function \n\\[\nk(x, x^{\\prime}) = \\text{Cov}(x, x^{\\prime}) = \\sigma^2 e^{-\\frac{ (x-x^{\\prime})^2}{2 l^2}}\n\\]\nknown squared-exponential kernel Radial-basis function (RBF) kernel.Note kernel implies\\(k(x, x) = \\text{Var}(x) = \\sigma^2\\) \\(\\text{Cor}(x, x^{\\prime}) = e^{-\\frac{ (x-x^{\\prime})^2}{2 l^2}}\\).parameter \\(l\\) RBF kernel length scale parameter describes\n“wigglyness” smoothness resulting function.\nSmall values \\(l\\) mean complex, wiggly functions, low autocorrelation.many kernel functions, including linear, polynomial periodic kernels.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gp-model","chapter":"6 Nonlinear and nonparametric models","heading":"6.2.4 GP model","text":"Nonlinear regression GP approach conceptually simple:start multivariate priorthen condition observed datathe resulting conditional multivariate normal can used predict\nfunction values unobserved valuesthe conditional variance can used compute credible intervals predictions.GP regression also provides direct link classical\nBayesian linear regression (using linear kernel).Drawbacks: computationally expensive (\\(O(n^3)\\) matrix inversion)","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gaussian-process-example","chapter":"6 Nonlinear and nonparametric models","heading":"6.2.5 Gaussian process example","text":"now show apply Gaussian processes R justing using standard matrix calculations.aim estimate following nonlinear function number observations. Note initially assume additional noise (observations lie directly curve):can now visualise functions samples multivariate normal prior:Now compute posterior mean variance conditioning observations:Now can plot posterior mean upper lower bounds 95% credible interval:Finally, can take acount noise measured data points adding error term:Note vicinity data points CIs small away\ndata uncertain estimate underlying function becomes.","code":"\ntruefunc = function(x) sin(x)\nXLIM = c(0, 2*pi)\nYLIM = c(-2, 2)\n\nn2 = 10\nx2 = runif(n2, min=XLIM[1], max=XLIM[2])\ny2 = truefunc(x2)  # no noise\n\ncurve( truefunc(x), xlim=XLIM, ylim=YLIM, xlab=\"x\", ylab=\"y\", \n      main=\"True Function\")\npoints(x2, y2)\n# RBF kernel\nrbfkernel = function(xa, xb, s2=1, l=1/2) s2*exp(-1/2*(xa-xb)^2/l^2)\nkfun.mat = function(xavec, xbvec, FUN=rbfkernel) \n  outer(X=as.vector(xavec), Y=as.vector(xbvec), FUN=FUN)\n\n# prior mean\nmu.vec = function(x) rep(0, length(x))\n# grid of x-values \nn1 = 100\nx1 = seq(XLIM[1], XLIM[2], length.out=n1)\n\n# unconditioned covariance and mean (unobserved samples x1)\nK1 = kfun.mat(x1, x1)  \nm1 = mu.vec(x1)\n\n## sample functions from GP prior  \nB = 5\nlibrary(\"MASS\") # for mvrnorm\ny1r = t(mvrnorm(B, mu = m1, Sigma=K1))\n\nplot(x1, y1r[,1], type=\"l\", lwd=2, ylab=\"y\", xlab=\"x\", ylim=YLIM, \n  main=\"Prior Functions (RBF Kernel with l=1/2)\")\nfor(i in 2:B)\n  lines(x1, y1r[,i], col=i, lwd=2)\n# unconditioned covariance and mean (observed samples x2)\nK2 = kfun.mat(x2, x2)\nm2 = mu.vec(x2)\niK2 = solve(K2) # inverse\n\n# cross-covariance\nK12 = kfun.mat(x1, x2)\n\n# Conditioning: x1 conditioned on x2\n\n# conditional mean\nm1.2 = m1 + K12 %*% iK2 %*% (y2 - m2)\n\n# conditional variance\nK1.2 = K1 - K12 %*% iK2 %*% t(K12)\n# upper and lower CI\nupper.bound = m1.2 + 1.96*sqrt(diag(K1.2))\nlower.bound = m1.2 - 1.96*sqrt(diag(K1.2))\n\nplot(x1, m1.2, type=\"l\", xlim=XLIM, ylim=YLIM, col=\"red\", lwd=3,\n  ylab=\"y\", xlab = \"x\", main = \"Posterior\")\n\npoints(x2,y2,pch=4,lwd=4,col=\"blue\")\nlines(x1,upper.bound,lty=2,lwd=3)\nlines(x1,lower.bound,lty=2,lwd=3)\ncurve(truefunc(x), xlim=XLIM, add=TRUE, col=\"gray\")\n\nlegend(x=\"topright\", \n  legend=c(\"posterior mean\", \"posterior quantiles\", \"true function\"),\n  lty=c(1, 2, 1),lwd=c(4, 4, 1), col=c(\"red\",\"black\", \"gray\"), cex=1.0)\n# add some noise\nsdeps = 0.1\nK2 = K2 + sdeps^2*diag(1,length(x2))\n\n# update\niK2 = solve(K2) # inverse\nm1.2 = m1 + K12 %*% iK2 %*% (y2 - m2)\nK1.2 = K1 - K12 %*% iK2 %*% t(K12)\nupper.bound = m1.2 + 1.96*sqrt(diag(K1.2))\nlower.bound = m1.2 - 1.96*sqrt(diag(K1.2))\n\nplot(x1, m1.2, type=\"l\", xlim=XLIM, ylim=YLIM, col=\"red\", lwd=3, \n  ylab=\"y\", xlab = \"x\", main = \"Posterior (with noise)\")\n\npoints(x2,y2,pch=4,lwd=4,col=\"blue\")\nlines(x1,upper.bound,lty=2,lwd=3)\nlines(x1,lower.bound,lty=2,lwd=3)\ncurve(truefunc(x), xlim=XLIM, add=TRUE, col=\"gray\")\n\nlegend(x=\"topright\", \n  legend=c(\"posterior mean\", \"posterior quantiles\", \"true function\"),\n  lty=c(1, 2, 1),lwd=c(4, 4, 1), col=c(\"red\",\"black\", \"gray\"), cex=1.0)"},{"path":"nonlinear-and-nonparametric-models.html","id":"neural-networks","chapter":"6 Nonlinear and nonparametric models","heading":"6.3 Neural networks","text":"Another highly important class models\nnonlinear prediction (nonlinear function approximation) \nneural networks.Relevant reading:Please read: Hastie, Tibshirani, Friedman (2009) Chapter 11 “Neural networks”\nJames et al. (2021) Chapter 10 “Deep Learning”","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"history","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.1 History","text":"Neural networks actually relatively old models, going back\n1950s!Three phases neural networks (NN)1950/60: replicating functions neurons brain (perceptron)1980/90: neural networks universal function approximators2010—today: deep learningThe first phase biologically inspired, second phase focused \nmathematical properties, current phase pushed forward \nadvances computer science numerical optimisation:backpropagation algorithmbackpropagation algorithmauto-differentiation,auto-differentiation,stochastic gradient descentstochastic gradient descentuse GPUs TPUs (e.g. linear algebra)use GPUs TPUs (e.g. linear algebra)availability development deep learning packages:\nAesara (PyMC), formely Theano (University Montreal)\nPyTorch (PyTorch Foundation, formerly Meta/Facebook),\nFlax / JAX (Google Research),\nTensorFlow (Google Research),\nMXNet (Amazon),\nPaddlePaddle (Baidu)\navailability development deep learning packages:Aesara (PyMC), formely Theano (University Montreal)PyTorch (PyTorch Foundation, formerly Meta/Facebook),Flax / JAX (Google Research),TensorFlow (Google Research),MXNet (Amazon),PaddlePaddle (Baidu)high-level wrappers:PyTorch-Lightning (PyTorch)Keras (Tensorflow, MXNet, Theano)","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"neural-networks-1","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.2 Neural networks","text":"Neural networks essentially stacked systems linear regressions,\nmapping input nodes (random variables) outputs (response nodes).\ninternal layer corresponds internal latent variables.\nlayer connected next layer non-linear activation functions.feedforward single layer NNstacked nonlinear multiple regression hidden variablesoptimise empirical risk minimisationIt can shown NN can approximate arbitrary non-linear function mapping\ninput output.“Deep” neural networks many layers, optimisation requires advanced\ntechniques (see ).Neural networks highly parameterised models require typically lot data\ntraining.statistical aspects NN well understood: particular known\nNN overfit data can still generalise well. hand, also know NN\ncan also “fooled”, .e. prediction can unstable (adversarial examples).Current statistical research NN focuses interpretability links Bayesian inference models (e.g. GPs). example:https://link.springer.com/book/10.1007/978-3-030-28954-6https://arxiv.org/abs/1910.12478","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"learning-more-about-deep-learning","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.3 Learning more about deep learning","text":"good place learn deep learning actual\nimplementations computer code various platforms book “Dive deep learning” \nZhang et al. (2021) available online https://d2l.ai/","code":""},{"path":"brief-refresher-on-matrices.html","id":"brief-refresher-on-matrices","chapter":"A Brief refresher on matrices","heading":"A Brief refresher on matrices","text":"multivariate statistics frequently make use matrix calculations matrix notation.\nhelps make multivariate equations simpler enables better understanding \nunderlying concepts.Throughout module mostly work real matrices, .e. assume matrix elements \nreal numbers. However, one important matrix decomposition — eigenvalue decomposition — can yield complex-valued matrices even applied real matrices. Thus occasionally need deal also complex numbers.details matrix theory please consult lecture notes related modules (e.g. linear algebra).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-basics","chapter":"A Brief refresher on matrices","heading":"A.1 Matrix basics","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-notation","chapter":"A Brief refresher on matrices","heading":"A.1.1 Matrix notation","text":"matrix notation distinguish scalars, vectors, matrices:Scalar: \\(x\\), \\(X\\), lower upper case, plain type.Vector: \\(\\boldsymbol x\\), lower case, bold type. handwriting arrow \\(\\vec{x}\\) indicates vector.component notation write \\(\\boldsymbol x= \\begin{pmatrix} x_1 \\\\ \\vdots\\\\ x_d\\end{pmatrix}\\). default, vector \ncolumn vector, .e. elements arranged column index components \\(x_i\\) refers row.transpose vector (indicated superscript \\(T\\)) turns row vector. save space can write column vector \\(\\boldsymbol x\\)\n\\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\) \\(\\boldsymbol x^T\\) row vector.Matrix: \\(\\boldsymbol X\\), upper case, bold type. handwriting underscore\n\\(\\underline{X}\\) indicates matrix.component notation write \\(\\boldsymbol X= (x_{ij})\\). convention, first index (\\(\\))\nscalar elements \\(x_{ij}\\) denotes row second index (\\(j\\)) column matrix.\n\\(n\\) number rows \\(d\\) number columns\ncan view matrix\n\\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_d)\\)\neither composed \\(d\\) column vectors\n\\(\\boldsymbol x_j = \\begin{pmatrix} x_{1j} \\\\ \\vdots \\\\ x_{nj}\\\\ \\end{pmatrix}\\)\n\\(\\boldsymbol X= \\begin{pmatrix} \\boldsymbol z_1^T \\\\ \\vdots \\\\ \\boldsymbol z_n^T\\end{pmatrix}\\)\n\ncomposed \\(n\\) row vectors \\(\\boldsymbol z_i^T = (x_{i1}, \\ldots, x_{id})\\).(column) vector dimension \\(d\\) matrix size \\(d\\times 1\\). row vector dimension \\(d\\) matrix size \\(1\\times d\\).\nscalar dimension \\(1\\) matrix size \\(1 \\times 1\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"random-matrix","chapter":"A Brief refresher on matrices","heading":"A.1.2 Random matrix","text":"random matrix (vector) matrix (vector) whose elements random variables.Note standard\nnotation used univariate statistics distinguish\nrandom variables realisations (.e. upper versus lower case) \nwork multivariate statistics. Therefore, need determine\ncontext whether quantity represents \nrandom variable, whether constant.","code":""},{"path":"brief-refresher-on-matrices.html","id":"special-matrices","chapter":"A Brief refresher on matrices","heading":"A.1.3 Special matrices","text":"\\(\\boldsymbol I_d\\) identity matrix. square matrix size\n\\(d \\times d\\) diagonal\nfilled 1 -diagonals filled 0.\n\\[\\boldsymbol I_d =\n\\begin{pmatrix}\n    1 & 0 & 0 & \\dots & 0\\\\\n    0 & 1 & 0 & \\dots & 0\\\\\n    0 & 0 & 1 &   & 0\\\\\n    \\vdots & \\vdots & & \\ddots &  \\\\\n    0 & 0 & 0 &  & 1 \\\\\n\\end{pmatrix}\\]\\(\\boldsymbol 1\\) matrix contains ones. often\nused form column vector \\(d\\) rows:\n\\[\\boldsymbol 1_d =\n\\begin{pmatrix}\n    1 \\\\\n    1 \\\\\n    1 \\\\\n    \\vdots   \\\\\n    1  \\\\\n\\end{pmatrix}\\]Similarly, \\(\\boldsymbol 0\\) matrix contains zeros. often\nused form column vector \\(d\\) rows:\n\\[\\boldsymbol 0_d =\n\\begin{pmatrix}\n    0 \\\\\n    0 \\\\\n    0 \\\\\n    \\vdots   \\\\\n    0  \\\\\n\\end{pmatrix}\\]diagonal matrix matrix -diagonal elements zero.\n\\(\\text{Diag}(\\boldsymbol )\\) access diagonal elements matrix vector \n\\(\\text{Diag}(a_1, \\ldots, a_d)\\) specify diagonal matrix listing diagonal elements.triangular matrix square matrix whose elements either diagonal zero (upper vs. lower triangular matrix).","code":""},{"path":"brief-refresher-on-matrices.html","id":"simple-matrix-operations","chapter":"A Brief refresher on matrices","heading":"A.2 Simple matrix operations","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-addition-and-multiplication","chapter":"A Brief refresher on matrices","heading":"A.2.1 Matrix addition and multiplication","text":"Matrices behave much like common numbers. example, can add matrices\n\\(\\boldsymbol C= \\boldsymbol + \\boldsymbol B\\)\nmultiply matrices \\(\\boldsymbol C= \\boldsymbol \\boldsymbol B\\).matrix addition \\(\\boldsymbol C= \\boldsymbol + \\boldsymbol B\\) add corresponding elements \\(c_{ij} = a_{ij} + b_{ij}\\). matrix addition \\(\\boldsymbol \\) \\(\\boldsymbol B\\) must dimensions, .e.\nnumber rows columns.dot product, scalar product, two vectors \\(\\boldsymbol \\) \\(\\boldsymbol b\\) scalar given \\(\\boldsymbol \\cdot \\boldsymbol b= \\langle \\boldsymbol , \\boldsymbol b\\rangle = \\boldsymbol ^T \\boldsymbol b= \\boldsymbol b^T \\boldsymbol = \\sum_{=1}^d a_{} b_{}\\).Matrix multiplication \\(\\boldsymbol C= \\boldsymbol \\boldsymbol B\\) obtained setting \\(c_{ij} = \\sum_{k=1}^m a_{ik} b_{kj}\\) \\(m\\) \nnumber columns \\(\\boldsymbol \\) number rows \\(\\boldsymbol B\\). Thus, \\(\\boldsymbol C\\) contains possible\ndot products row vectors \\(\\boldsymbol \\) column vectors \\(\\boldsymbol B\\).\nmatrix multiplication number columns \\(\\boldsymbol \\) must match number rows \\(\\boldsymbol B\\).\nNote matrix multiplication general (\\(m > 1\\)) commute, .e. \\(\\boldsymbol \\boldsymbol B\\neq \\boldsymbol B\\boldsymbol \\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-transpose","chapter":"A Brief refresher on matrices","heading":"A.2.2 Matrix transpose","text":"matrix transpose \\(\\boldsymbol ^T\\) indicate superscript \\(T\\) interchanges rows columns matrix. transpose\nlinear operator \\((\\boldsymbol + \\boldsymbol B)^T = \\boldsymbol ^T + \\boldsymbol B^T\\) \napplied matrix\nproduct reverses ordering, .e. \\((\\boldsymbol \\boldsymbol B)^T =\\boldsymbol B^T \\boldsymbol ^T\\).\\(\\boldsymbol = \\boldsymbol ^T\\) \\(\\boldsymbol \\) symmetric (square).construction given rectangular \\(\\boldsymbol \\) matrices\n\\(\\boldsymbol ^T \\boldsymbol \\) \\(\\boldsymbol \\boldsymbol ^T\\) symmetric non-negative diagonal.","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-summaries","chapter":"A Brief refresher on matrices","heading":"A.3 Matrix summaries","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"row-column-and-grand-sum","chapter":"A Brief refresher on matrices","heading":"A.3.1 Row, column and grand sum","text":"Assume matrix \\(\\boldsymbol \\) size \\(n \\times m\\).sum \\(m\\) entries row \\(\\) \\(\\sum_{j=1}^m a_{ij}\\).\nmatrix notation \\(n\\) row sums given \n\\(\\boldsymbol \\, \\boldsymbol 1_m\\).sum \\(n\\) entries column \\(j\\) \\(\\sum_{=1}^n a_{ij}\\).\nmatrix notation \\(m\\) column sums \\(\\boldsymbol ^T \\boldsymbol 1_n\\).grand sum matrix entries \\(\\boldsymbol \\) obtained \n\\[\n\\sum_{=1}^n \\sum_{j=1}^m a_{ij} = \\boldsymbol 1_n^T \\, \\boldsymbol \\, \\boldsymbol 1_m \n\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-trace","chapter":"A Brief refresher on matrices","heading":"A.3.2 Matrix trace","text":"trace matrix sum diagonal elements \\(\\text{Tr}(\\boldsymbol ) = \\sum a_{ii}\\).trace invariant transposition, .e.\n\\[\n\\text{Tr}(\\boldsymbol ) = \\text{Tr}(\\boldsymbol ^T )\n\\]useful identity matrix trace product two matrices \n\\[\n\\text{Tr}(\\boldsymbol \\boldsymbol B) = \\text{Tr}( \\boldsymbol B\\boldsymbol )  \n\\]Intriguingly, trace matrix equals sum eigenvalues matrix (see ).","code":""},{"path":"brief-refresher-on-matrices.html","id":"row-column-and-grand-sum-of-squares","chapter":"A Brief refresher on matrices","heading":"A.3.3 Row, column and grand sum of squares","text":"sum \\(m\\) squared entries row \\(\\) \\(\\sum_{j=1}^m a_{ij}^2\\).\nmatrix notation \\(n\\) row sums squares given \n\\(\\text{Diag}( \\boldsymbol \\boldsymbol ^T)\\).sum \\(n\\) squared entries column \\(j\\) \\(\\sum_{=1}^n a_{ij}^2\\).\nmatrix notation \\(m\\) column sums squares \\(\\text{Diag}( \\boldsymbol ^T \\boldsymbol )\\).grand sum squared elements \\(\\boldsymbol \\) obtained \n\\[\n\\sum_{=1}^n \\sum_{j=1}^m a_{ij}^2 = \\text{Tr}(\\boldsymbol ^T \\boldsymbol ) = \\text{Tr}(\\boldsymbol \\boldsymbol ^T)\n\\]\nalso known squared Frobenius norm \\(\\boldsymbol \\) (see ).","code":""},{"path":"brief-refresher-on-matrices.html","id":"sum-of-squared-diagonal-entries","chapter":"A Brief refresher on matrices","heading":"A.3.4 Sum of squared diagonal entries","text":"sum squared entries diagonal matrix notation\n\\[\n\\text{Diag}(\\boldsymbol )^T \\text{Diag}(\\boldsymbol ) = \\sum_{=1}^{\\min(n,m)} a_{ii}^2\n\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"frobenius-inner-product","chapter":"A Brief refresher on matrices","heading":"A.3.5 Frobenius inner product","text":"Frobenius inner product two rectangular matrices dimension scalar\n\\[\n\\begin{split}\n\\langle \\boldsymbol , \\boldsymbol B\\rangle &=  \\text{Tr}(\\boldsymbol \\boldsymbol B^T) = \\text{Tr}(\\boldsymbol B\\boldsymbol ^T)\\\\\n&=  \\text{Tr}(\\boldsymbol ^T \\boldsymbol B) = \\text{Tr}(\\boldsymbol B^T \\boldsymbol )\\\\\n&= \\sum_{,j} a_{ij} b_{ij} \\,.\n\\end{split}\n\\]\ngeneralises dot product two vectors.\nNote dot product can therefore also written trace matrix\n\\[\n\\langle \\boldsymbol , \\boldsymbol b\\rangle = \\text{Tr}( \\boldsymbol \\boldsymbol b^T ) = \\text{Tr}( \\boldsymbol b\\boldsymbol ^T) \\,.\n\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"euclidean-norm","chapter":"A Brief refresher on matrices","heading":"A.3.6 Euclidean norm","text":"squared Euclidean norm squared length vector \\(\\boldsymbol \\)\n\ndot product \\(||\\boldsymbol ||^2_2 = \\boldsymbol \\cdot \\boldsymbol = \\langle \\boldsymbol , \\boldsymbol \\rangle = \\boldsymbol ^T \\boldsymbol = \\boldsymbol \\boldsymbol ^T = \\sum_{=1}^d a_i^2\\).squared Frobenius norm generalisation squared Euclidean vector norm rectangular matrix \nsum squares elements.\nUsing trace can written \n\\[\n\\begin{split}\n||\\boldsymbol ||_F^2 &= \\langle \\boldsymbol , \\boldsymbol \\rangle \\\\\n &= \\text{Tr}(\\boldsymbol ^T \\boldsymbol ) = \\text{Tr}(\\boldsymbol \\boldsymbol ^T) \\\\\n &= \\sum_{,j} a_{ij}^2 \\,.\n\\end{split}\n\\]useful identity squared Frobenius norm difference two matrices\n\n\\[\n\\begin{split}\n||\\boldsymbol - \\boldsymbol B||_F^2 &= ||\\boldsymbol ||_F^2 + ||\\boldsymbol B||_F^2  - 2 \\langle \\boldsymbol , \\boldsymbol B\\rangle \\\\\n &= \\text{Tr}(\\boldsymbol ^T \\boldsymbol ) +\\text{Tr}(\\boldsymbol B^T \\boldsymbol B) - 2 \\text{Tr}(\\boldsymbol ^T \\boldsymbol B) \\\\\n &= \\sum_{,j} (a_{ij}-b_{ij})^2 \\,.\n\\end{split}\n\\]Frobenius norm matrix \\(||\\boldsymbol ||_F\\) confused induced \\(2\\)-norm matrix \\(||\\boldsymbol ||_2\\). latter equals maximum\nabsolute eigenvalue matrix, \\(||\\boldsymbol ||_2 \\leq ||\\boldsymbol ||_F\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"determinant-of-a-matrix","chapter":"A Brief refresher on matrices","heading":"A.3.7 Determinant of a matrix","text":"\\(\\boldsymbol \\) square matrix determinant \\(\\det(\\boldsymbol )\\) scalar measuring volume spanned column vectors \\(\\boldsymbol \\) sign determined orientation vectors.\\(\\det(\\boldsymbol ) \\neq 0\\) matrix \\(\\boldsymbol \\) non-singular non-degenerate. Conversely, \n\\(\\det(\\boldsymbol ) =0\\) matrix \\(\\boldsymbol \\) singular degenerate.Intriguingly, determinant \\(\\boldsymbol \\) product eigenvalues \\(\\boldsymbol \\) (see ).One way compute determinant matrix \\(\\boldsymbol \\) Laplace cofactor\nexpansion approach proceeds recursively based determinants submatrices \\(\\boldsymbol A_{-,-j}\\) obtained deleting row \\(\\) column \\(j\\) \\(\\boldsymbol \\). Specifically, \nlevel compute thecofactor expansion either\nalong \\(\\)-th row — pick row \\(\\):\n\\[\\det(\\boldsymbol ) = \\sum_{j=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})  \\text{ , }\\]\nalong \\(j\\)-th column — pick \\(j\\):\n\\[\\det(\\boldsymbol ) = \\sum_{=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})\\].\nalong \\(\\)-th row — pick row \\(\\):\n\\[\\det(\\boldsymbol ) = \\sum_{j=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})  \\text{ , }\\]along \\(j\\)-th column — pick \\(j\\):\n\\[\\det(\\boldsymbol ) = \\sum_{=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})\\].repeat submatrix scalar \\(\\) \\(\\det()=\\,.\\)recursive nature algorithm leads complexity order \\(O(d!)\\) practical except small \\(d\\).\nTherefore, practice efficient algorithms computing determinants used still algorithmic complexity order \\(O(d^3)\\) large dimensions obtaining determinants \nexpensive.However, specially structured matrices allow fast calculation.determinant triangular matrix (thus also diagonal matrix)\n\\[\n\\boldsymbol = \\begin{pmatrix}\n a_{11} & 0       & \\cdots & 0\\\\\n a_{21} & a_{22}  & \\cdots & 0\\\\\n\\vdots  & \\vdots & \\ddots & 0 \\\\\n a_{d1} & a_{d2} & \\cdots & a_{dd} \\\\\n\\end{pmatrix}\n\\]\nproduct diagonal elements, .e. \\(\\det(\\boldsymbol ) = \\prod_{=1}^d a_{ii}\\).two-dimensional matrix \\(\\boldsymbol = \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\\\end{pmatrix}\\)\ndeterminant \\(\\det() = a_{11} a_{22} - a_{12} a_{21}\\).block-structured square matrix\n\\[\n\\boldsymbol = \\begin{pmatrix} \\boldsymbol A_{11} & \\boldsymbol A_{12} \\\\ \\boldsymbol A_{21} & \\boldsymbol A_{22} \\\\ \\end{pmatrix} \\, ,\n\\]\nmatrices diagonal \\(\\boldsymbol A_{11}\\) \\(\\boldsymbol A_{22}\\) square \n\\(\\boldsymbol A_{21}\\) \\(\\boldsymbol A_{21}\\) can shape,\ndeterminant \n\\[\n\\det(\\boldsymbol ) = \\det(\\boldsymbol A_{22}) \\det(\\boldsymbol C_1) = \\det(\\boldsymbol A_{11}) \\det(\\boldsymbol C_2) \n\\]\n(Schur complement \\(\\boldsymbol A_{22}\\))\n\\[\n\\boldsymbol C_1 = \\boldsymbol A_{11} -  \\boldsymbol A_{12}  \\boldsymbol A_{22}^{-1}  \\boldsymbol A_{21} \n\\]\n(Schur complement \\(\\boldsymbol A_{11}\\))\n\\[\n\\boldsymbol C_2 = \\boldsymbol A_{22} -  \\boldsymbol A_{21}  \\boldsymbol A_{11}^{-1}  \\boldsymbol A_{12} \n\\]\nNote \\(\\boldsymbol C_1\\) \\(\\boldsymbol C_2\\) square matrices.block-diagonal matrix \\(\\boldsymbol \\) \\(\\boldsymbol A_{12} = 0\\) \\(\\boldsymbol A_{21} = 0\\)\ndeterminant \\(\\det(\\boldsymbol ) = \\det(\\boldsymbol A_{11}) \\det(\\boldsymbol A_{22})\\), .e. \nproduct determinants submatrices along diagonal.Determinants multiplicative property,\n\\[\\det(\\boldsymbol \\boldsymbol B) = \\det(\\boldsymbol B\\boldsymbol ) = \\det(\\boldsymbol ) \\det(\\boldsymbol B) \\,.\\]\n\\(\\boldsymbol \\) \\(\\boldsymbol B\\) square dimension.rectangular \\(\\boldsymbol \\) (\\(n \\times m\\)) rectangular \\(\\boldsymbol B\\) (\\(m \\times n\\))\n\\(m \\geq n\\)\ngeneralises Cauchy-Binet formula\n\\[\n\\det(\\boldsymbol \\boldsymbol B) = \\sum_{w} \\det(\\boldsymbol A_{,w}) \\det(\\boldsymbol B_{w,})\n\\]\nsummation \\(\\binom{m}{n}\\) index subsets \\(w\\) size \\(n\\) taken \\(\\{1, \\ldots, m\\}\\) keeping ordering\n\\(\\boldsymbol A_{,w}\\) \\(\\boldsymbol B_{w,}\\) corresponding square \\(n \\times n\\) submatrices. \\(m < n\\) \\(\\det(\\boldsymbol \\boldsymbol B) = 0\\).scalar \\(\\)\n\\(\\det(\\boldsymbol B) = ^d \\det(\\boldsymbol B)\\) \\(d\\) dimension \\(\\boldsymbol B\\).Another important identity \n\\[\\det(\\boldsymbol I_n + \\boldsymbol \\boldsymbol B) = \\det(\\boldsymbol I_m + \\boldsymbol B\\boldsymbol )\\]\n\\(\\boldsymbol \\) \\(\\boldsymbol B\\) rectangular matrices. called Weinstein-Aronszajn determinant identity (also credited Sylvester).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-inverse","chapter":"A Brief refresher on matrices","heading":"A.4 Matrix inverse","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"inversion-of-square-matrix","chapter":"A Brief refresher on matrices","heading":"A.4.1 Inversion of square matrix","text":"\\(\\boldsymbol \\) square matrix inverse matrix \\(\\boldsymbol ^{-1}\\) matrix\n\n\\[\\boldsymbol ^{-1} \\boldsymbol = \\boldsymbol \\boldsymbol ^{-1}=  \\boldsymbol \\, .\\]\nnon-singular matrices \\(\\det(\\boldsymbol ) \\neq 0\\) invertible.\\(\\det(\\boldsymbol ^{-1} \\boldsymbol ) = \\det(\\boldsymbol ) = 1\\) \ndeterminant inverse matrix equals\ninverse determinant,\n\\[\\det(\\boldsymbol ^{-1}) = \\det(\\boldsymbol )^{-1} \\,.\\]transpose inverse inverse transpose\n\n\\[\n\\begin{split}\n(\\boldsymbol ^{-1})^T &= (\\boldsymbol ^{-1})^T \\,  \\boldsymbol ^T (\\boldsymbol ^{T})^{-1}   \\\\\n &= (\\boldsymbol \\boldsymbol ^{-1})^T \\, (\\boldsymbol ^{T})^{-1} = (\\boldsymbol ^{T})^{-1} \\,. \\\\\n\\end{split}\n\\]inverse matrix product \\((\\boldsymbol \\boldsymbol B)^{-1} = \\boldsymbol B^{-1} \\boldsymbol ^{-1}\\)\nproduct indivdual matrix inverses reverse order.many different algorithms compute inverse matrix\n(essentially problem solving system equations).\ncomputational complexity matrix inversion order \\(O(d^3)\\)\n\\(d\\) dimension \\(\\boldsymbol \\). Therefore matrix inversion costly higher dimensions.Example .1  Inversion \\(2 \\times 2\\) matrix:inverse matrix \\(= \\begin{pmatrix} & b \\\\ c & d \\end{pmatrix}\\) \n\\(^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & \\end{pmatrix}\\)","code":""},{"path":"brief-refresher-on-matrices.html","id":"inversion-of-structured-matrices","chapter":"A Brief refresher on matrices","heading":"A.4.2 Inversion of structured matrices","text":"However, specially structured matrices inversion can done effectively:inverse diagonal matrix another diagonal matrix obtained inverting diagonal elements.generally, inverse block-diagonal matrix obtained individually inverting blocks along diagonal.Woodbury matrix identity simplifies inversion matrices can \nwritten \\(\\boldsymbol + \\boldsymbol U\\boldsymbol B\\boldsymbol V\\) \\(\\boldsymbol \\) \\(\\boldsymbol B\\) square \n\\(\\boldsymbol U\\) \\(\\boldsymbol V\\) suitable rectangular matrices:\n\\[\n(\\boldsymbol + \\boldsymbol U\\boldsymbol B\\boldsymbol V)^{-1} = \\boldsymbol ^{-1} - \\boldsymbol ^{-1} \\boldsymbol U(\\boldsymbol B^{-1} + \\boldsymbol V\\boldsymbol ^{-1} \\boldsymbol U)^{-1} \\boldsymbol V\\boldsymbol ^{-1}\n\\]\nTypically, inverse \\(\\boldsymbol ^{-1}\\) either already known can easily obtained \ndimension \\(\\boldsymbol B\\) much lower \\(\\boldsymbol \\).class matrices can easily inverted orthogonal matrices whose inverse \nobtained simply transposing matrix.","code":""},{"path":"brief-refresher-on-matrices.html","id":"orthogonal-matrices","chapter":"A Brief refresher on matrices","heading":"A.5 Orthogonal matrices","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"properties-1","chapter":"A Brief refresher on matrices","heading":"A.5.1 Properties","text":"orthogonal matrix \\(\\boldsymbol Q\\) square matrix property \\(\\boldsymbol Q^T = \\boldsymbol Q^{-1}\\), .e.\ntranspose also inverse. implies \\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol \\).column row vectors \\(\\boldsymbol Q\\) length 1. implies \nelement \\(q_{ij}\\) \\(\\boldsymbol Q\\) can take value interval \\([-1, 1]\\).identity matrix \\(\\boldsymbol \\) simplest example orthogonal matrix.squared Euclidean Frobenius norm preserved vector \\(\\boldsymbol \\) matrix \\(\\boldsymbol \\) multiplied orthogonal matrix \\(\\boldsymbol Q\\):\n\\[\n|| \\boldsymbol Q\\boldsymbol ||^2_2 = (\\boldsymbol Q\\boldsymbol )^T \\boldsymbol Q\\boldsymbol = \\boldsymbol ^T \\boldsymbol = || \\boldsymbol ||^2_2\n\\]\n\n\\[\n|| \\boldsymbol Q\\boldsymbol ||^2_F = \\text{Tr}\\left((\\boldsymbol Q\\boldsymbol )^T \\boldsymbol Q\\boldsymbol \\right) = \\text{Tr}\\left(\\boldsymbol ^T \\boldsymbol \\right) = || \\boldsymbol ||^2_F \n\\]Multiplication \\(\\boldsymbol Q\\) vector results \nnew vector length change direction (unless \\(\\boldsymbol Q=\\boldsymbol \\)).\northogonal matrix\n\\(\\boldsymbol Q\\) can thus interpreted geometrically operator performing\nrotation, reflection /permutation.product \\(\\boldsymbol Q_3 = \\boldsymbol Q_1 \\boldsymbol Q_2\\) two orthogonal matrices \\(\\boldsymbol Q_1\\) \\(\\boldsymbol Q_2\\) yields another orthogonal matrix \\(\\boldsymbol Q_3 \\boldsymbol Q_3^T = \\boldsymbol Q_1 \\boldsymbol Q_2 (\\boldsymbol Q_1 \\boldsymbol Q_2)^T = \\boldsymbol Q_1 \\boldsymbol Q_2 \\boldsymbol Q_2^T \\boldsymbol Q_1^T = \\boldsymbol \\).determinant \\(\\det(\\boldsymbol Q)\\) orthogonal matrix either +1 -1,\n\\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol \\) thus \\(\\det(\\boldsymbol Q)\\det(\\boldsymbol Q^T) = \\det(\\boldsymbol Q)^2 = \\det(\\boldsymbol ) = 1\\).set orthogonal matrices dimension \\(d\\) together multiplication\nform group called orthogonal group \\(O(d)\\).\nsubset orthogonal matrices \\(\\det(\\boldsymbol Q)=1\\) called rotation matrices form multiplication special orthogonal group \\((d)\\).\nOrthogonal matrices \\(\\det(\\boldsymbol Q)=-1\\) rotation-reflection matrices.","code":""},{"path":"brief-refresher-on-matrices.html","id":"semi-orthogonal-matrices","chapter":"A Brief refresher on matrices","heading":"A.5.2 Semi-orthogonal matrices","text":"rectangular \\(d \\times k\\) matrix \\(\\boldsymbol Q\\) semi-orthogonal\n\\(k < d\\) \\(k\\) column vectors orthonormal hence \\(\\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol I_k\\), \\(k > d\\) \\(d\\) row vectors orthonormal \n\\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol I_d\\).set (semi)-orthogonal matrices \\(\\boldsymbol Q\\) \\(k \\leq d\\) column vectors known Stiefel manifold \\(\\text{St}(d, k)\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"generating-orthogonal-matrices","chapter":"A Brief refresher on matrices","heading":"A.5.3 Generating orthogonal matrices","text":"two dimensions \\((d=2)\\) orthogonal matrices \\(\\boldsymbol R\\) representing rotations \\(\\det(\\boldsymbol R)=1\\) \ngiven \n\\[\n\\boldsymbol R(\\theta) = \n\\begin{pmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta \n\\end{pmatrix}\n\\]\nrepresenting rotation-reflections \\(\\boldsymbol G\\) \\(\\det(\\boldsymbol G)=-1\\) \n\\[\n\\boldsymbol G(\\theta) = \n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix}\\,.\n\\]\nEvery orthogonal matrix dimension \\(d=2\\)\ncan represented product two rotation-reflection\nmatrices \n\\[\n\\boldsymbol R(\\theta) = \\boldsymbol G(\\theta)\\, \\boldsymbol G(0) =  \n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix} \n\\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}\\,.\n\\]\nThus, matrix \\(\\boldsymbol G\\) generator two-dimensional orthogonal matrices.\nNote \\(\\boldsymbol G(\\theta)\\) symmetric, orthogonal determinant -1.generally, applicable arbitrary dimension, role generator taken Householder reflection matrix\n\\[\n\\boldsymbol Q_{HH}(\\boldsymbol v) = \\boldsymbol - 2 \\boldsymbol v\\boldsymbol v^T\n\\]\n\\(\\boldsymbol v\\) vector unit length (\\(\\boldsymbol v^T \\boldsymbol v=1\\)) orthogonal \nreflection hyperplane. Note \\(\\boldsymbol Q_{HH}(\\boldsymbol v) = \\boldsymbol Q_{HH}(-\\boldsymbol v)\\).\nconstruction matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\) symmetric, orthogonal determinant -1.can shown \\(d\\)-dimensional orthogonal matrix \\(\\boldsymbol Q\\) can represented product \\(d\\) Householder reflection matrices.\ntwo-dimensional generator \\(\\boldsymbol G(\\theta)\\) recovered Householder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\)\n\\(\\boldsymbol v= \\begin{pmatrix} -\\sin \\frac{\\theta}{2} \\\\ \\cos \\frac{\\theta}{2} \\end{pmatrix}\\)\n\\(\\boldsymbol v= \\begin{pmatrix} \\sin \\frac{\\theta}{2} \\\\ -\\cos \\frac{\\theta}{2} \\end{pmatrix}\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"permutation-matrix","chapter":"A Brief refresher on matrices","heading":"A.5.4 Permutation matrix","text":"special type orthogonal matrix permutation matrix \\(\\boldsymbol P\\) created \npermuting rows /columns identity matrix \\(\\boldsymbol \\). Thus, row column\n\\(\\boldsymbol P\\) contains exactly one entry 1, necessarily diagonal.permutation matrix \\(\\boldsymbol P\\) multiplied matrix \\(\\boldsymbol \\) acts operator\npermuting columns (\\(\\boldsymbol \\boldsymbol P\\)) rows (\\(\\boldsymbol P\\boldsymbol \\)).\nset \\(d\\) elements exist \\(d!\\) permutations. Thus, dimension \\(d\\) \n\\(d!\\) possible permutation matrices (including identity matrix).determinant permutation matrix either +1 -1.\nproduct two permutation matrices yields another permutation matrix.Symmetric permutation matrices correspond self-inverse permutations\n(.e. permutation matrix inverse), also called permutation involutions.\ncan determinant +1 -1.transposition permutation two elements exchanged.\nThus, transposition matrix \\(\\boldsymbol T\\)\nexactly two rows /columns exchanged compared identity matrix \\(\\boldsymbol \\).\nTranspositions self-inverse, transposition matrices symmetric.\n\\(\\frac{d (d-1)}{2}\\) different transposition matrices.\ndeterminant transposition matrix \\(\\det(\\boldsymbol T)= -1\\).Note transposition matrix instance Householder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\)\nvector \\(\\boldsymbol v\\) filled zeros except two elements value\n\\(\\frac{\\sqrt{2}}{2}\\) \\(-\\frac{\\sqrt{2}}{2}\\).permutation \\(d\\) elements can generated series \\(d-1\\) transpositions.\nCorrespondingly, permutation matrix \\(\\boldsymbol P\\) can constructed multiplication identity\nmatrix \\(d-1\\) transposition matrices. number transpositions even \\(\\det(\\boldsymbol P) = 1\\) otherwise\nuneven number \\(\\det(\\boldsymbol P) = -1\\). called sign signature permutation.set permutations form symmetric group \\(S_d\\), subset even permutations (positive sign \\(\\det(\\boldsymbol P)=1\\)) alternating group \\(A_d\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-and-eigenvectors","chapter":"A Brief refresher on matrices","heading":"A.6 Eigenvalues and eigenvectors","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"definition","chapter":"A Brief refresher on matrices","heading":"A.6.1 Definition","text":"Assume square matrix \\(\\boldsymbol \\) size \\(d \\times d\\).\nvector \\(\\boldsymbol u\\neq 0\\) called eigenvector matrix \\(\\boldsymbol \\) \\(\\lambda\\) corresponding\neigenvalue \\[\\boldsymbol \\boldsymbol u= \\boldsymbol u\\lambda \\, .\\]\ncalled eigenvalue equation eigenequation.","code":""},{"path":"brief-refresher-on-matrices.html","id":"finding-eigenvalues-and-vectors","chapter":"A Brief refresher on matrices","heading":"A.6.2 Finding eigenvalues and vectors","text":"find eigenvalues eigenvectors eigenequation rewritten \n\\[(\\boldsymbol -\\boldsymbol \\lambda ) \\; \\boldsymbol u= \\boldsymbol 0\\,.\\]\nequation hold eigenvector \\(\\boldsymbol u\\neq 0\\) eigenvalue\n\\(\\lambda\\) implies matrix \\(\\boldsymbol -\\boldsymbol \\lambda\\) singular.\nCorrespondingly, determinant must vanish\n\\[\\det(\\boldsymbol -\\boldsymbol \\lambda ) =0 \\,.\\]\ncalled characteristic equation matrix \\(\\boldsymbol \\), solution yields \\(d\\)\neigenvalues \\(\\lambda_1, \\ldots, \\lambda_d\\). Note eigenvalues need \ndistinct may complex even matrix \\(\\boldsymbol \\) real.complex eigenvalues, real matrix eigenvalues come conjugate pairs.\nHence, complex \\(\\lambda_1 = r e^{\\phi}\\) also corresponding complex eigenvalue \\(\\lambda_2 = r e^{-\\phi}\\).Given eigenvalues solve eigenequation corresponding non-zero eigenvectors\n\\(\\boldsymbol u_1, \\ldots, \\boldsymbol u_d\\). Note eigenvectors real matrices can complex components.\nAlso eigenvector defined eigenequation scalar.\nconvention eigenvectors therefore typically standardised unit length still leaves\nsign ambiguity real eigenvectors implies complex eigenvectors defined factor modulus 1.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenequation-in-matrix-notation","chapter":"A Brief refresher on matrices","heading":"A.6.3 Eigenequation in matrix notation","text":"matrix\n\\[\\boldsymbol U= (\\boldsymbol u_1, \\ldots, \\boldsymbol u_d)\\] containing standardised eigenvectors columns diagonal matrix\n\\[\\boldsymbol \\Lambda= \\begin{pmatrix}\n    \\lambda_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}\n\\end{pmatrix}\\]\ncontaining eigenvalues (typically sorted order magnitude) eigenvalue equation can written \n\\[\\boldsymbol \\boldsymbol U= \\boldsymbol U\\boldsymbol \\Lambda\\,.\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"permutation-of-eigenvalues","chapter":"A Brief refresher on matrices","heading":"A.6.4 Permutation of eigenvalues","text":"eigenvalues order, may apply permutation matrix \\(\\boldsymbol P\\) arrange order.\n\\(\\boldsymbol \\Lambda^{\\text{sort}} = \\boldsymbol P^T \\boldsymbol \\Lambda\\boldsymbol P\\) sorted eigenvalues\n\\(\\boldsymbol U^{\\text{sort}} = \\boldsymbol U\\boldsymbol P\\) corresponding eigenvectors eigenequation becomes\n\\[\\boldsymbol \\boldsymbol U^{\\text{sort}} =  \\boldsymbol \\boldsymbol U\\boldsymbol P= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol P=  \\boldsymbol U\\boldsymbol P\\boldsymbol P^T \\boldsymbol \\Lambda\\boldsymbol P=  \\boldsymbol U^{\\text{sort}} \\boldsymbol \\Lambda^{\\text{sort}} \\,.\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"similar-matrices","chapter":"A Brief refresher on matrices","heading":"A.6.5 Similar matrices","text":"Two matrices \\(\\boldsymbol \\) \\(\\boldsymbol B\\) called similar share eigenvalues.\\(\\boldsymbol \\) eigenvalues \\(\\boldsymbol \\Lambda\\) eigenvectors \\(\\boldsymbol U\\) can construct similar \\(\\boldsymbol B\\) via similarity transformation \\(\\boldsymbol B= \\boldsymbol M\\boldsymbol \\boldsymbol M^{-1}\\) \\(\\boldsymbol M\\) invertible matrix.\\(\\boldsymbol \\Lambda\\) eigenvalues \\(\\boldsymbol B\\) \\(\\boldsymbol V= \\boldsymbol M\\boldsymbol U\\) eigenvectors \n\\[\n\\boldsymbol B\\boldsymbol V= \\boldsymbol M\\boldsymbol \\boldsymbol M^{-1} \\boldsymbol M\\boldsymbol U= \\boldsymbol M\\boldsymbol \\boldsymbol U= \\boldsymbol M\\boldsymbol U\\boldsymbol \\Lambda= \\boldsymbol V\\boldsymbol \\Lambda\\,.\n\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"defective-matrix","chapter":"A Brief refresher on matrices","heading":"A.6.6 Defective matrix","text":"cases eigenvectors \\(\\boldsymbol u_i\\) linearly independent form basis span \\(d\\) dimensional space.However, case \nmatrix \\(\\boldsymbol \\) complete basis eigenvectors, matrix called defective. case\nmatrix \\(\\boldsymbol U\\) containing eigenvectors singular \\(\\det(\\boldsymbol U)=0\\).example defective matrix \n\\(\\begin{pmatrix} 1 &1 \\\\ 0 & 1 \\\\ \\end{pmatrix}\\)\ndeterminant 1 can inverted column vectors form complete basis\none distinct eigenvector \\((1,0)^T\\) eigenvector basis incomplete.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-of-a-diagonal-or-triangular-matrix","chapter":"A Brief refresher on matrices","heading":"A.6.7 Eigenvalues of a diagonal or triangular matrix","text":"special case \\(\\boldsymbol \\) diagonal triangular matrix eigenvalues easily determined.\nfollows simple form determinants product diagonal elements.\nHence matrices characteristic equation becomes \\(\\prod_{}^d (a_{ii} -\\lambda) = 0\\) solution\n\\(\\lambda_i=a_{ii}\\), .e. eigenvalues equal diagonal elements.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-and-vectors-of-a-symmetric-matrix","chapter":"A Brief refresher on matrices","heading":"A.6.8 Eigenvalues and vectors of a symmetric matrix","text":"\\(\\boldsymbol \\) symmetric, .e. \\(\\boldsymbol = \\boldsymbol ^T\\), eigenvalues eigenvectors special properties:eigenvalues \\(\\boldsymbol \\) real,eigenvectors orthogonal, .e \\(\\boldsymbol u_i^T \\boldsymbol u_j = 0\\) \\(\\neq j\\), real. Thus, matrix \\(\\boldsymbol U\\) containing standardised orthonormal eigenvectors orthogonal.\\(\\boldsymbol \\) never defective \\(\\boldsymbol U\\) forms complete basis.Furthermore, symmetric matrix \\(\\boldsymbol \\) diagonal elements \\(p_1 \\geq \\ldots \\geq p_d\\)\neigenvalues \\(\\lambda_1 \\geq \\ldots \\geq \\lambda_d\\) (note written decreasing order) sum \nlargest \\(k\\) eigenvalues forms upper bound sum largest \\(k\\) diagonal elements:\n\\[\n\\sum_{}^k \\lambda_i \\geq \\sum_{}^k p_i\n\\]\ntheorem due Schur (1923).23 equality holds \\(k=d\\) (trace \\(\\boldsymbol \\) equals sum\neigenvalues) \\(k\\) \\(\\boldsymbol \\) diagonal (case diagonal elements equal eigenvalues).","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-of-orthogonal-matrices","chapter":"A Brief refresher on matrices","heading":"A.6.9 Eigenvalues of orthogonal matrices","text":"eigenvalues orthogonal matrix \\(\\boldsymbol Q\\) necessarily real \nmodulus 1 lie unit circle . Thus, eigenvalues \\(\\boldsymbol Q\\)\nform \\(\\lambda = e^{\\phi} = \\cos \\phi + \\sin \\phi\\).real matrix complex eigenvalues come conjugate\npairs. Hence orthogonal matrix \\(\\boldsymbol Q\\) complex eigenvalue \\(e^{\\phi}\\) also \ncomplex eigenvalue \\(e^{-\\phi} =\\cos \\phi - \\sin \\phi\\). product two conjugate\neigenvalues 1. Thus, orthogonal matrix uneven dimension least one\nreal eigenvalue (+1 -1).eigenvalues Hausholder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\) real (recall symmetric!).\nfact, dimension \\(d\\) eigenvalues -1 (one time) 1 ( \\(d-1\\) times).\nSince transposition matrix \\(\\boldsymbol T\\) special Householder matrix eigenvalues.","code":""},{"path":"brief-refresher-on-matrices.html","id":"positive-definite-matrices","chapter":"A Brief refresher on matrices","heading":"A.6.10 Positive definite matrices","text":"eigenvalues square matrix \\(\\boldsymbol \\) real \\(\\lambda_i \\geq 0\\) \\(\\boldsymbol \\) called positive semi-definite.\neigenvalues strictly positive\n\\(\\lambda_i > 0\\) \\(\\boldsymbol \\) called positive definite.Note matrix need symmetric positive\ndefinite, e.g.\n\\(\\begin{pmatrix} 2 & 3 \\\\ 1 & 4 \\\\ \\end{pmatrix}\\)\npositive eigenvalues 5 1. also complete\nset eigenvectors diagonisable.symmetric matrix \\(\\boldsymbol \\) positive definite\nquadratic form \\(\\boldsymbol x^T \\boldsymbol \\boldsymbol x> 0\\) non-zero \\(\\boldsymbol x\\),\npositive semi-definite \\(\\boldsymbol x^T \\boldsymbol \\boldsymbol x\\geq 0\\).\nholds also way around:\nsymmetric positive definite matrix (positive eigenvalues) \npositive quadratic form, symmetric positive semi-definite matrix (non-negative eigenvalues) non-negative quadratic form.symmetric positive definite matrix always positive diagonal\n(can seen setting \\(\\boldsymbol x\\) unit vector 1 \nsingle position, 0 elements).\nHowever, just requiring positive diagonal weak ensure positive definiteness symmetric matrix, example \\(\\begin{pmatrix} 1 &10 \\\\ 10 & 1 \\\\ \\end{pmatrix}\\) negative eigenvalue -9.\nhand, symmetric matrix indeed positive definite strictly\ndiagonally dominant, .e. diagonal elements positive larger absolute value corresponding row column elements.\nHowever, diagonal dominance restrictive criterion \ncharacterise \nsymmetric positive definite matrices, since\nmany symmetric matrices positive definite diagonally dominant, \n\\(\\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\\\ \\end{pmatrix}\\).Finally, sum symmetric positive semi-definite matrix \\(\\boldsymbol \\)\nsymmetric positive definite matrix \\(\\boldsymbol B\\) symmetric positive definite corresponding\nquadratic form \\(\\boldsymbol x^T ( \\boldsymbol +\\boldsymbol B) \\boldsymbol x= \\boldsymbol x^T \\boldsymbol \\boldsymbol x+ \\boldsymbol x^T \\boldsymbol B\\boldsymbol x> 0\\) positive. Similarly, sum\ntwo symmetric positive (semi)-definite matrices symmetric positive (semi)-definite.","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-decompositions","chapter":"A Brief refresher on matrices","heading":"A.7 Matrix decompositions","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"diagonalisation-and-eigenvalue-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.1 Diagonalisation and eigenvalue decomposition","text":"\\(\\boldsymbol \\) square non-defective matrix \\(\\boldsymbol U\\) invertible \ncan rewrite eigenvalue equation \n\\[\\boldsymbol = \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1} \\,.\\]\ncalled eigendecomposition, spectral decomposition, \\(\\boldsymbol \\) equivalently\n\\[\\boldsymbol \\Lambda= \\boldsymbol U^{-1} \\boldsymbol \\boldsymbol U\\]\ndiagonalisation \\(\\boldsymbol \\).\nThus matrix \\(\\boldsymbol \\) defective diagonalisable using eigenvalue decomposition.","code":""},{"path":"brief-refresher-on-matrices.html","id":"orthogonal-eigenvalue-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.2 Orthogonal eigenvalue decomposition","text":"symmetric \\(\\boldsymbol \\) real eigenvalues orthogonal matrix \\(\\boldsymbol U\\) spectral decomposition\nbecomes\n\\[\\boldsymbol = \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\]\n\n\\[\\boldsymbol \\Lambda= \\boldsymbol U^T \\boldsymbol \\boldsymbol U\\,.\\]\nspecial case known orthogonal diagonalisation\n\\(\\boldsymbol \\).orthogonal decomposition symmetric \\(\\boldsymbol \\) \nunique apart signs\neigenvectors (columns \\(\\boldsymbol U\\)). Thus, computer application\ndepending specific implementation numerical algorithm eigenvalue\ndecomposition\nsigns may vary.","code":""},{"path":"brief-refresher-on-matrices.html","id":"singular-value-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.3 Singular value decomposition","text":"singular value decomposition (SVD) \ngeneralisation orthogonal eigenvalue decomposition\nsymmetric matrices.(!) rectangular matrix \\(\\boldsymbol \\) size \\(n\\times d\\) can factored\nproduct\n\\[\\boldsymbol = \\boldsymbol U\\boldsymbol D\\boldsymbol V^T\\]\n\\(\\boldsymbol U\\) \\(n \\times n\\) orthogonal matrix, \\(\\boldsymbol V\\) second \\(d \\times d\\) orthogonal matrix \\(\\boldsymbol D\\) diagonal rectangular matrix\nsize \\(n\\times d\\) \\(m=min(n,d)\\) real diagonal elements \\(d_1, \\ldots d_m\\). \\(d_i\\) called singular values, appear\nalong diagonal \\(\\boldsymbol D\\) order magnitude.SVD unique apart \nsigns columns vectors \\(\\boldsymbol U\\), \\(\\boldsymbol V\\) \\(\\boldsymbol D\\) (can freely specify column signs two \nthree matrices). convention \nsigns chosen singular values \\(\\boldsymbol D\\) non-negative, leaves ambiguity\ncolumns signs \\(\\boldsymbol U\\) \\(\\boldsymbol V\\). Alternatively, one may\nfix columns signs \\(\\boldsymbol U\\) \\(\\boldsymbol V\\), e.g. requiring positive diagonal, determines sign singular values (thus allowing negative singular values well).\\(\\boldsymbol \\) symmetric SVD orthogonal eigenvalue decomposition coincide (apart different sign conventions singular values, eigenvalues eigenvectors).Since \\(\\boldsymbol ^T \\boldsymbol = \\boldsymbol V\\boldsymbol D^T \\boldsymbol D\\boldsymbol V^T\\) \\(\\boldsymbol \\boldsymbol ^T = \\boldsymbol U\\boldsymbol D\\boldsymbol D^T \\boldsymbol U^T\\) squared singular values correspond eigenvalues \\(\\boldsymbol ^T \\boldsymbol \\) \\(\\boldsymbol \\boldsymbol ^T\\).\nalso follows \\(\\boldsymbol ^T \\boldsymbol \\) \\(\\boldsymbol \\boldsymbol ^T\\) positive\nsemi-definite symmetric matrices, \\(\\boldsymbol V\\) \\(\\boldsymbol U\\) contain respective sets eigenvectors.","code":""},{"path":"brief-refresher-on-matrices.html","id":"polar-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.4 Polar decomposition","text":"square matrix \\(\\boldsymbol \\) can factored product\n\\[\n\\boldsymbol = \\boldsymbol Q\\boldsymbol B\n\\]\northogonal matrix \\(\\boldsymbol Q\\) symmetric positive semi-definite matrix \\(\\boldsymbol B\\).follows SVD \\(\\boldsymbol \\) given \n\\[\n\\begin{split}\n\\boldsymbol &= \\boldsymbol U\\boldsymbol D\\boldsymbol V^T \\\\\n    &= ( \\boldsymbol U\\boldsymbol V^T ) ( \\boldsymbol V\\boldsymbol D\\boldsymbol V^T ) \\\\\n    &= \\boldsymbol Q\\boldsymbol B\\\\\n\\end{split}\n\\]\nnon-negative \\(\\boldsymbol D\\). Note decomposition unique sign ambiguities columns \\(\\boldsymbol U\\) \\(\\boldsymbol V\\) cancel \\(\\boldsymbol Q\\) \\(\\boldsymbol B\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"cholesky-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.5 Cholesky decomposition","text":"symmetric positive definite matrix \\(\\boldsymbol \\) can decomposed product\ntriangular matrix \\(\\boldsymbol L\\) transpose\n\\[\n\\boldsymbol = \\boldsymbol L\\boldsymbol L^T \\,.\n\\]\n, \\(\\boldsymbol L\\) lower triangular matrix positive diagonal elements.decomposition unique called Cholesky factorisation. \noften used check whether symmetric matrix positive definite algorithmically\nless demanding eigenvalue decomposition.Note implementations Cholesky decomposition (e.g. R) use\nupper triangular matrices \\(\\boldsymbol K\\) positive diagonal \n\\(\\boldsymbol = \\boldsymbol K^T \\boldsymbol K\\) \\(\\boldsymbol L= \\boldsymbol K^T\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-summaries-based-on-eigenvalues-and-singular-values","chapter":"A Brief refresher on matrices","heading":"A.8 Matrix summaries based on eigenvalues and singular values","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"trace-and-determinant-computed-from-eigenvalues","chapter":"A Brief refresher on matrices","heading":"A.8.1 Trace and determinant computed from eigenvalues","text":"eigendecomposition \\(\\boldsymbol =\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1}\\)\nallows establish link trace determinant eigenvalues.Specifically,\n\\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol ) & = \\text{Tr}(\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1}  ) =\n\\text{Tr}( \\boldsymbol \\Lambda\\boldsymbol U^{-1} \\boldsymbol U) \\\\\n &= \\text{Tr}( \\boldsymbol \\Lambda) = \\sum_{=1}^d \\lambda_i \\\\\n\\end{split}\n\\]\nthus trace square matrix \\(\\boldsymbol \\) equal sum eigenvalues. Likewise,\n\\[\n\\begin{split}\n\\det(\\boldsymbol ) & = \\det(\\boldsymbol U) \\det(\\boldsymbol \\Lambda) \\det(\\boldsymbol U^{-1}  ) \\\\\n &=\\det( \\boldsymbol \\Lambda) = \\prod_{=1}^d \\lambda_i \\\\\n\\end{split}\n\\]\ntherefore determinant \\(\\boldsymbol \\) product eigenvalues.relationship eigenvalues trace determinant\ndemonstrated diagonisable non-defective matrices.\nHowever, hold also general matrix. can shown using certain non-diagonal matrix decompositions (e.g. Jordan decomposition).result, eigenvalues equal zero \\(\\det(\\boldsymbol ) = 0\\) hence \\(\\boldsymbol \\) singular invertible.trace determinant real matrix always real even though individual eigenvalues may complex.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-of-a-squared-matrix","chapter":"A Brief refresher on matrices","heading":"A.8.2 Eigenvalues of a squared matrix","text":"eigendecomposition \\(\\boldsymbol =\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1}\\)\neasy see eigenvalues \\(\\boldsymbol ^2\\) simply \nsquared eigenvalues \\(\\boldsymbol \\) \n\\[\n\\boldsymbol ^2 = \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1} \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1} = \\boldsymbol U\\boldsymbol \\Lambda^2 \\boldsymbol U^{-1} \n\\]\nresult can compute trace \\(\\boldsymbol ^2\\) sum squared\neigenvalues \\(\\boldsymbol \\), .e. \\(\\text{Tr}(\\boldsymbol ^2) = \\sum_{=1}^d \\lambda_i^2\\),\ndeterminant product squared eigenvalues, .e\n\\(\\det(\\boldsymbol ^2) = \\prod_{=1}^d \\lambda_i^2\\).\\(\\boldsymbol \\) symmetric \\(\\text{Tr}(\\boldsymbol ^2) = \\text{Tr}(\\boldsymbol \\boldsymbol ^T) = || ||^2_F = \\sum_{=1}^d \\sum_{j=1}^d a_{ij}^2\\)\nleads identity\n\\[\n\\sum_{=1}^d \\lambda_i^2 =  \\sum_{=1}^d \\sum_{j=1}^d a_{ij}^2\n\\]\nsum squared eigenvalues sum squared entries symmetric matrix \\(\\boldsymbol \\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"rank-and-condition-number","chapter":"A Brief refresher on matrices","heading":"A.8.3 Rank and condition number","text":"rank dimension space spanned column row vectors. rectangular matrix dimension \\(n \\times d\\) \nrank \\(m = \\min(n, d)\\), maximum indeed achieved full rank.condition number describes well- ill-conditioned\nfull rank matrix . example, square matrix large condition number implies matrix close singular\nthus ill-conditioned.\ncondition number infinite matrix full rank.rank condition matrix can determined \\(m\\) singular values \\(d_1, \\ldots, d_m\\) matrix obtained SVD:rank number non-zero singular values.condition number ratio largest singular value\ndivided smallest singular value (absolute values signs allowed).square matrix \\(\\boldsymbol \\) singular condition number infinite, full rank.\nhand, non-singular square matrix, \npositive definite matrix, full rank.","code":""},{"path":"brief-refresher-on-matrices.html","id":"functions-of-symmetric-matrices","chapter":"A Brief refresher on matrices","heading":"A.9 Functions of symmetric matrices","text":"focus symmetric square matrices \\(\\boldsymbol =\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) always diagonisable real eigenvalues \\(\\boldsymbol \\Lambda\\) orthogonal eigenvectors \\(\\boldsymbol U\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"definition-of-a-matrix-function","chapter":"A Brief refresher on matrices","heading":"A.9.1 Definition of a matrix function","text":"Assume real-valued function \\(f()\\) real number \\(\\). corresponding\nmatrix function \\(f(\\boldsymbol )\\)\ndefined \n\\[\nf(\\boldsymbol ) =  \\boldsymbol Uf(\\boldsymbol \\Lambda) \\boldsymbol U^T =  \\boldsymbol U\\begin{pmatrix}\n    f(\\lambda_{1}) & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & f(\\lambda_{d})\n\\end{pmatrix} \\boldsymbol U^T\n\\]\nfunction \\(f()\\) applied eigenvalues \\(\\boldsymbol \\).\nconstruction \\(f(\\boldsymbol )\\) real, symmetric \nreal eigenvalues \\(f(\\lambda_i)\\).Examples:Example .2  Matrix power: \\(f() = ^p\\) (\\(p\\) real number)Special cases matrix power include :Matrix inversion: \\(f() = ^{-1}\\)\nNote matrix \\(\\boldsymbol \\) singular, .e. contains one eigenvalues \\(\\lambda_i=0\\),\n\\(\\boldsymbol ^{-1}\\) defined therefore \\(\\boldsymbol \\) invertible.However, -called pseudoinverse can still computed, inverting non-zero eigenvalues, \nkeeping zero eigenvalues zero.Matrix square root: \\(f() = ^{1/2}\\)\nSince multiple solutions square root also multiple\nmatrix square roots. principal matrix square root obtained using\npositive square roots eigenvalues. Thus principal matrix square root\npositive semi-definite matrix also positive semi-definite unique.Example .3  Matrix exponential: \\(f() = \\exp()\\)\nNote \\(\\exp() \\geq 0\\) real \\(\\) matrix \\(\\exp(\\boldsymbol )\\) positive\nsemi-definite. Thus, matrix exponential can used generate positive semi-definite\nmatrices.\\(\\boldsymbol \\) \\(\\boldsymbol B\\) commute, .e. \\(\\boldsymbol \\boldsymbol B= \\boldsymbol B\\boldsymbol \\), \n\\(\\exp(\\boldsymbol +\\boldsymbol B) = \\exp(\\boldsymbol ) \\exp(\\boldsymbol B)\\). However, case\notherwise!Example .4  Matrix logarithm: \\(f() = \\log()\\)\nlogarithm requires \\(>0\\) matrix \\(\\boldsymbol \\) needs positive definite\n\\(\\log(\\boldsymbol )\\) defined.","code":""},{"path":"brief-refresher-on-matrices.html","id":"identities-for-the-matrix-exponential-and-logarithm","chapter":"A Brief refresher on matrices","heading":"A.9.2 Identities for the matrix exponential and logarithm","text":"give rise useful identities:symmetric matrix \\(\\boldsymbol \\) \n\\[\n\\det(\\exp(\\boldsymbol )) = \\exp(\\text{Tr}(\\boldsymbol ))\n\\]\n\n\\(\\prod_i \\exp(\\lambda_i) = \\exp( \\sum_i \\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol \\).symmetric matrix \\(\\boldsymbol \\) \n\\[\n\\det(\\exp(\\boldsymbol )) = \\exp(\\text{Tr}(\\boldsymbol ))\n\\]\n\n\\(\\prod_i \\exp(\\lambda_i) = \\exp( \\sum_i \\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol \\).take logarithm sides replace \\(\\exp(\\boldsymbol )=\\boldsymbol B\\) get another\nidentity symmetric positive definite matrix \\(\\boldsymbol B\\):\n\\[\n\\log \\det(\\boldsymbol B) = \\text{Tr}(\\log(\\boldsymbol B))\n\\]\n\n\\(\\log( \\prod_i \\lambda_i) = \\sum_i \\log(\\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol B\\).take logarithm sides replace \\(\\exp(\\boldsymbol )=\\boldsymbol B\\) get another\nidentity symmetric positive definite matrix \\(\\boldsymbol B\\):\n\\[\n\\log \\det(\\boldsymbol B) = \\text{Tr}(\\log(\\boldsymbol B))\n\\]\n\n\\(\\log( \\prod_i \\lambda_i) = \\sum_i \\log(\\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol B\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-calculus","chapter":"A Brief refresher on matrices","heading":"A.10 Matrix calculus","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"first-order-vector-derivatives","chapter":"A Brief refresher on matrices","heading":"A.10.1 First order vector derivatives","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"gradient","chapter":"A Brief refresher on matrices","heading":"A.10.1.1 Gradient","text":"gradient scalar-valued function\n\\(h(\\boldsymbol x)\\) vector argument \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\)\nvector containing first order partial derivatives\n\\(h(\\boldsymbol x)\\) regard \\(x_1, \\ldots, x_d\\):\n\\[\n\\begin{split}\n\\nabla h(\\boldsymbol x) &= \\begin{pmatrix}\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_1} \\\\\n\\vdots\\\\\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_d}\n\\end{pmatrix}\\\\\n &=  \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} \\\\\n& = \\text{grad } h(\\boldsymbol x) \\\\\n\\end{split}\n\\]\nsymbol \\(\\nabla\\) called nabla operator (also known del operator).Note write gradient column vector. called \ndenominator layout convention, see https://en.wikipedia.org/wiki/Matrix_calculus details.\ncontrast, many textbooks (also earlier versions lecture notes) assume gradients row vectors, following -called numerator layout convention.Example .5  Examples gradient:\\(h(\\boldsymbol x)=\\boldsymbol ^T \\boldsymbol x+ b\\). \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol \\).\\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol x\\). \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = 2 \\boldsymbol x\\).\\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol \\boldsymbol x\\). \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = (\\boldsymbol + \\boldsymbol ^T) \\boldsymbol x\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"jacobian-matrix","chapter":"A Brief refresher on matrices","heading":"A.10.1.2 Jacobian matrix","text":"vector-valued function\n\\[\n\\boldsymbol h(\\boldsymbol x) = ( h_1(\\boldsymbol x), \\ldots, h_m(\\boldsymbol x) )^T \n\\]\ncan also compute vector derivative\n\\[\n\\begin{split}\n\\frac{\\partial \\boldsymbol h(\\boldsymbol x)}{\\partial \\boldsymbol x}  &= \n\\begin{pmatrix}\n\\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_1} & \\cdots  & \\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_1} \\\\\n \\vdots &\\ddots & \\vdots \\\\\n\\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_d} & \\cdots & \\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_d} \\\\\n\\end{pmatrix} \\\\\n&=\\left(\\frac{\\partial h_j(\\boldsymbol x)}{\\partial x_i}\\right) \\\\\n& = \\left(\\nabla h_1(\\boldsymbol x), \\ldots, \\nabla h_m(\\boldsymbol x)  \\right)\n\\end{split}\n\\]\nyields matrix whose columns contain \ngradient vectors component \\(\\boldsymbol h(\\boldsymbol x)\\).transpose matrix called Jacobian matrix (size \\(m\\) rows \\(d\\) columns):\n\\[\n\\begin{split}\n J_{\\boldsymbol h}(\\boldsymbol x) &= \n\\left( {\\begin{array}{c}\n \\nabla h_1(\\boldsymbol x)^T   \\\\\n \\vdots   \\\\\n \\nabla h_m(\\boldsymbol x)^T  \\\\\n \\end{array} } \\right) \\\\\n& = \\left(\\frac{\\partial h_i(\\boldsymbol x)}{\\partial x_j}\\right) \\\\\n& = \\left( \\frac{\\partial \\boldsymbol h(\\boldsymbol x)}{\\partial \\boldsymbol x}   \\right)^T\n\\end{split}\n\\]\nNote convention Jacobian matrix contains gradients rows.Example .6  \\(\\boldsymbol h(\\boldsymbol x)=\\boldsymbol ^T \\boldsymbol x+ \\boldsymbol b\\). \\(\\frac{\\partial \\boldsymbol h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol \\)\n\\(J_{\\boldsymbol h}(\\boldsymbol x) =\\boldsymbol ^T\\).\\(m=d\\) Jacobian matrix square matrix allows compute \nJacobian determinant \\(\\det J_{\\boldsymbol h}(\\boldsymbol x)\\).Jacobian matrix Jacobian determinant often called simply “Jacobian”.\\(\\boldsymbol y= \\boldsymbol h(\\boldsymbol x)\\) invertible function \\(\\boldsymbol x= \\boldsymbol h^{-1}(\\boldsymbol y)\\)\nJacobian matrix invertible inverted matrix fact \nJacobian inverse function!allows compute Jacobian determinant backtransformation \ninverse Jacobian determinant original function:\n\\[\\det  J_{\\boldsymbol h^{-1}}(\\boldsymbol y) = ( \\det  J_{\\boldsymbol h}(\\boldsymbol x) )^{-1}\\]\nalternative notation\n\\[\\det  J_{\\boldsymbol x}(\\boldsymbol y) = \\frac{1}{ \\det J_{\\boldsymbol y}(\\boldsymbol x) }\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"second-order-vector-derivatives","chapter":"A Brief refresher on matrices","heading":"A.10.2 Second order vector derivatives","text":"matrix second order partial derivates scalar-valued\nfunction vector-valued argument called Hessian matrix:\n\\[\n\\begin{split}\n\\nabla \\nabla^T h(\\boldsymbol x) &=\n\\begin{pmatrix}\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1^2}\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1 \\partial x_2} \n     & \\cdots \n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1 \\partial x_d} \\\\\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2 \\partial x_1} \n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2^2}\n     & \\cdots \n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2 \\partial x_d} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d \\partial x_1} \n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d \\partial x_2}  \n     & \\cdots \n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d^2}\n \\end{pmatrix} \\\\\n&= \\left(\\frac{\\partial h(\\boldsymbol x)}{\\partial x_i \\partial x_j}\\right) \\\\ \n& = \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial \\boldsymbol x\\partial \\boldsymbol x^T} \\\\\n\\end{split}\n\\]\nconstruction Hessian matrix square symmetric.Example .7  \\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol \\boldsymbol x\\). \\(\\nabla \\nabla^T h(\\boldsymbol x) = \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial \\boldsymbol x\\partial \\boldsymbol x^T} = (\\boldsymbol + \\boldsymbol ^T)\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"first-order-matrix-derivatives","chapter":"A Brief refresher on matrices","heading":"A.10.3 First order matrix derivatives","text":"derivative scalar-valued function \\(f(\\boldsymbol X)\\) regard matrix argument \\(\\boldsymbol X\\)\ncan also defined results matrix. find matrix calculus rules (written \ndenominator layout convention). See\nhttps://en.wikipedia.org/wiki/Matrix_calculus examples.Example .8  \\(\\frac{\\partial \\text{Tr}(\\boldsymbol ^T \\boldsymbol X)}{\\partial \\boldsymbol X} = \\boldsymbol \\)Example .9  \\(\\frac{\\partial \\text{Tr}(\\boldsymbol ^T \\boldsymbol X\\boldsymbol B)}{\\partial \\boldsymbol X} = \\boldsymbol \\boldsymbol B^T\\)Example .10  \\(\\frac{\\partial \\text{Tr}(\\boldsymbol X^T \\boldsymbol \\boldsymbol X)}{\\partial \\boldsymbol X} = (\\boldsymbol + \\boldsymbol ^T) \\boldsymbol X\\)Example .11  \\(\\frac{\\partial \\log \\det(\\boldsymbol X)}{\\partial \\boldsymbol X} = \\frac{\\partial \\text{Tr}(\\log \\boldsymbol X)}{\\partial \\boldsymbol X} = (\\boldsymbol X^{-1})^T\\)","code":""},{"path":"further-study.html","id":"further-study","chapter":"B Further study","heading":"B Further study","text":"module can touch surface field multivariate statistics machine learning. like study \nrecommend following books starting point.","code":""},{"path":"further-study.html","id":"recommended-reading","chapter":"B Further study","heading":"B.1 Recommended reading","text":"multivariate statistics machine learning:Marden (2015) Multivariate Statistics: Old SchoolRogers Girolami (2017) first course machine learning (2nd Edition). Chapman Hall / CRC.James et al. (2021) introduction statistical learning applications R (2nd edition). Springer.","code":""},{"path":"further-study.html","id":"advanced-reading","chapter":"B Further study","heading":"B.2 Advanced reading","text":"Additional (advanced) reference books probabilistic machine learning :Bishop (2006) Pattern recognition machine learning. Springer.Hastie, Tibshirani, Friedman (2009) elements statistical learning: data mining, inference, prediction. Springer.Murphy (2012) Machine learning: probabilistic perspective. MIT Press.Fan et al. (2020) Statistical Foundations Data Science. Chapman Hall / CRC.Murphy (2022) Probabilistic Machine Learning: Introduction. MIT Press.can find suggestions list online textbooks statistics machine learning.","code":""},{"path":"bibliography.html","id":"bibliography","chapter":"Bibliography","heading":"Bibliography","text":"Bishop, C. M. 2006. Pattern Recognition Machine Learning. Springer. https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/.Fan, J., R. Li, C.-H. Zhang, H. Zou. 2020. Statistical Foundations Data Science. Chapman; Hall / CRC.Hastie, T., R. Tibshirani, J. Friedman. 2009. Elements Statistical Learning: Data Mining, Inference, Prediction. 2nd ed. Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.James, G., D. Witten, T. Hastie, R. Tibshirani. 2021. Introduction Statistical Learning Applications R. 2nd ed. Springer. https://www.statlearning.com.Marden, J. . 2015. Multivariate Statistics: Old School. CreateSpace. http://stat.istics.net/Multivariate.Murphy, K. P. 2012. Machine Learning: Probabilistic Perspective. MIT Press.———. 2022. Probabilistic Machine Learning: Introduction. MIT Press. https://probml.github.io/pml-book/book1.html.Rogers, S., M. Girolami. 2017. First Course Machine Learning. 2nd ed. Chapman; Hall / CRC.Zhang, ., Z. C. Lipton, M. Li, . J. Smola. 2021. Dive Deep Learning. https://d2l.ai.","code":""}]
