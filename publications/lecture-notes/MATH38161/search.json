[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"lecture notes MATH38161, course Multivariate Statistics Machine Learning third year mathematics students Department Mathematics University Manchester.course text written Korbinian Strimmer 2018–2024. version 18 January 2024.notes updated time time. view current\nversion visit \nonline MATH38161 lecture notes.may also wish download MATH38161 lecture notes PDF A4 format printing (double page layout) 6x9 inch PDF use tablets (single page layout).","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"notes licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"about-the-author","chapter":"Preface","heading":"About the author","text":"Hello! name Korbinian Strimmer Professor Statistics.\nmember Statistics group\nDepartment Mathematics University Manchester. can find information home page.first taught module winter semester 2018 University Manchester, subsequently following years (including) winter semester 2023.hope enjoy course! questions, comments, corrections please email korbinian.strimmer@manchester.ac.uk.","code":""},{"path":"preface.html","id":"about-the-module","chapter":"Preface","heading":"About the module","text":"","code":""},{"path":"preface.html","id":"topics-covered","chapter":"Preface","heading":"Topics covered","text":"MATH38161 module designed run course 11 weeks.\nsix parts, covering particular aspect multivariate statistics machine learning:Multivariate random variables estimation \nlarge small sample settings (W1 W2)Transformations dimension reduction (W3 W4)Unsupervised learning/clustering (W5 W6)Supervised learning/classification (W7 W8)Measuring modelling multivariate dependencies (W9)Nonlinear nonparametric models (W10, W11)module focuses :Concepts methods (theory)Implementation application RPractical data analysis interpretation (incl. report writing)Modern tools data science statistics (R markdown, R studio)","code":""},{"path":"preface.html","id":"additional-support-material","chapter":"Preface","heading":"Additional support material","text":"University Manchester student enrolled module\nfind Blackboard:weekly learning plan 11 week study period,weekly worksheets examples (theory application R) solutions R Markdown, andexam papers previous years.Furthermore, also MATH38161 online reading list hosted University Manchester library.","code":""},{"path":"preface.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"Multivariate statistics relies heavily matrix algebra vector matrix calculus.\nrefresher essentials please refer supplementary\nMatrix Calculus Refresher notes.Furthermore, module builds earlier statistics modules, especially \nlikelihood estimation Bayesian statistics discussed, e.g., module\nMATH27720 Statistics 2.overview essential probability distributions\nsee Probability Distributions Refresher notes.","code":""},{"path":"preface.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"Many thanks Beatriz Costa Gomes help compile first draft course notes winter term 2018 graduate teaching assistant course. also thank many students suggested corrections.","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-random-variables","chapter":"1 Multivariate random variables","heading":"1 Multivariate random variables","text":"","code":""},{"path":"multivariate-random-variables.html","id":"essentials-in-multivariate-statistics","chapter":"1 Multivariate random variables","heading":"1.1 Essentials in multivariate statistics","text":"","code":""},{"path":"multivariate-random-variables.html","id":"why-multivariate-statistics","chapter":"1 Multivariate random variables","heading":"1.1.1 Why multivariate statistics?","text":"science use experiments learn underlying mechanisms interest, deterministic stochastic, compare different models verify reject hypotheses world.\nStatistics provides tools quantify procedure offers methods \nlink data (experiments) probabilistic models (hypotheses).univariate statistics use relatively simple approaches based single random variable\nsingle parameter. However, practise often consider multiple random variables multiple parameters, need complex models also able deal complex data. Hence, need multivariate statistical approaches models.Specifically, multivariate statistics concerned methods models random vectors random matrices, rather just random univariate (scalar) variables. Therefore, multivariate statistics frequently make use matrix notation.Closely related multivariate statistics (traditionally subfield statistics) machine learning (ML) traditionally subfield computer science. ML used focus algorithms rather probabilistic modelling nowadays machine learning methods fully based statistical multivariate approaches, two fields converging.Multivariate models provide means learn dependencies interactions among \ncomponents random variables turn allow us draw conclusion underlying mechanisms interest (e.g. biological medical problems).Two main tasks:unsupervised learning (finding structure, clustering)supervised learning (training labelled data, followed prediction)Challenges:complexity model needs appropriate problem available datahigh dimensions make estimation inference difficultcomputational issues","code":""},{"path":"multivariate-random-variables.html","id":"univariate-vs.-multivariate-random-variables","chapter":"1 Multivariate random variables","heading":"1.1.2 Univariate vs. multivariate random variables","text":"Univariate random variable (dimension \\(d=1\\)):\n\\[x \\sim F\\]\n\\(x\\) scalar \\(F\\) distribution.\n\\(\\text{E}(x) = \\mu\\) denotes mean \\(\\text{Var}(x) = \\sigma^2\\) variance \\(x\\).Multivariate random vector dimension \\(d\\):\n\\[\\boldsymbol x= (x_1, x_2,...,x_d)^T  \\sim F\\]\\(\\boldsymbol x\\) vector valued random variable.vector \\(\\boldsymbol x\\) column vector (=matrix size \\(d \\times 1\\)).\ncomponents \\(x_1, x_2,...,x_d\\) univariate random variables.\ndimension \\(d\\) also often denoted \\(p\\) \\(q\\).","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-data","chapter":"1 Multivariate random variables","heading":"1.1.3 Multivariate data","text":"Vector notation:Samples multivariate distribution vectors (scalars univariate normal):\n\\[\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_n \\stackrel{\\text{iid}}\\sim F\\]Matrix component notation:data points commonly collected matrix \\(\\boldsymbol X\\).statistics convention store data vector rows data matrix \\(\\boldsymbol X\\):\\[\\boldsymbol X= (\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_n)^T = \\begin{pmatrix}\n    x_{11}  & x_{12} & \\dots & x_{1d}   \\\\\n    x_{21}  & x_{22} & \\dots & x_{2d}   \\\\\n    \\vdots \\\\\n    x_{n1}  & x_{n2} & \\dots & x_{nd}\n\\end{pmatrix}\\]Therefore,\n\\[\\boldsymbol x_1=\\begin{pmatrix}\n    x_{11}       \\\\\n    \\vdots \\\\\n    x_{1d}\n\\end{pmatrix} , \\space \\boldsymbol x_2=\\begin{pmatrix}\n    x_{21}       \\\\\n    \\vdots \\\\\n    x_{2d}\n\\end{pmatrix} , \\ldots , \\boldsymbol x_n=\\begin{pmatrix}\n    x_{n1}       \\\\\n    \\vdots \\\\\n    x_{nd}\n\\end{pmatrix}\\]Thus, statistics first index runs \\((1,...,n)\\) denotes samples second index runs \\((1,...,d)\\) refers variables.statistics convention data matrices universal! fact, machine learning literature engineering computer science data samples stored columns variables appear rows (thus engineering convention data matrix transposed compared statistics convention).order avoid confusion ambiguity recommended prefer vector notation describe data matrix component notation (see also section estimating covariance matrices examples).","code":""},{"path":"multivariate-random-variables.html","id":"mean-of-a-random-vector","chapter":"1 Multivariate random variables","heading":"1.1.4 Mean of a random vector","text":"mean / expectation random vector dimensions \\(d\\) also vector dimensions \\(d\\):\n\\[\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu= \\begin{pmatrix}\n    \\text{E}(x_1)       \\\\\n    \\text{E}(x_2)       \\\\\n    \\vdots \\\\\n    \\text{E}(x_d)\n\\end{pmatrix} = \\left( \\begin{array}{l}\n    \\mu_1       \\\\\n    \\mu_2       \\\\\n    \\vdots \\\\\n    \\mu_d\n\\end{array}\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"variance-of-a-random-vector","chapter":"1 Multivariate random variables","heading":"1.1.5 Variance of a random vector","text":"Recall definition mean variance univariate random variable:\\[\\text{E}(x) = \\mu\\]\\[\\text{Var}(x) = \\sigma^2 = \\text{E}( (x-\\mu)^2 )=\\text{E}( (x-\\mu)(x-\\mu) ) = \\text{E}(x^2)-\\mu^2\\]Definition variance random vector:\\[\\text{Var}(\\boldsymbol x) = \\underbrace{\\boldsymbol \\Sigma}_{d\\times d} =\n\\text{E}\\left(\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d\\times 1} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1\\times d} \\right)\n= \\text{E}(\\boldsymbol x\\boldsymbol x^T)-\\boldsymbol \\mu\\boldsymbol \\mu^T\\]variance random vector , therefore, vector matrix!\\[\\boldsymbol \\Sigma= (\\sigma_{ij}) = \\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix}\\]matrix called Covariance Matrix, -diagonal elements \\(\\sigma_{ij}= \\text{Cov}(x_i,x_j)\\) diagonal \\(\\sigma_{ii}= \\text{Var}(X_i) = \\sigma_i^2\\).","code":""},{"path":"multivariate-random-variables.html","id":"properties-of-the-covariance-matrix","chapter":"1 Multivariate random variables","heading":"1.1.6 Properties of the covariance matrix","text":"\\(\\boldsymbol \\Sigma\\) real valued: \\(\\sigma_{ij} \\\\mathbb{R}\\)\\(\\boldsymbol \\Sigma\\) symmetric: \\(\\sigma_{ij} = \\sigma_{ji}\\)diagonal \\(\\boldsymbol \\Sigma\\) contains \\(\\sigma_{ii} = \\text{Var}(x_i) = \\sigma_i^2\\), .e. \nvariances components \\(\\boldsymbol x\\).-diagonal elements \\(\\sigma_{ij} = \\text{Cov}(x_i,x_j)\\) represent linear dependencies among \\(x_i\\). \\(\\Longrightarrow\\) linear regression, correlationHow many separate entries \\(\\boldsymbol \\Sigma\\) ?\\[\\boldsymbol \\Sigma= (\\sigma_{ij}) = \\underbrace{\\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix}}_{d\\times d}\\]\n\\(\\sigma_{ij} = \\sigma_{ji}\\).Number separate entries: \\(\\frac{d(d+1)}{2}\\).numbers grows square dimension \\(d\\), .e. order \\(O(d^2)\\):large dimension \\(d\\) covariance matrix many components!–> computationally expensive (storage handling)\n–> challenging estimate high dimensions \\(d\\).Note: matrix inversion requires \\(O(d^3)\\) operations using standard algorithms Gauss Jordan elimination. 1 Hence, computing \\(\\boldsymbol \\Sigma^{-1}\\) computationally expensive large \\(d\\)!","code":""},{"path":"multivariate-random-variables.html","id":"eigenvalue-decomposition-of-boldsymbol-sigma","chapter":"1 Multivariate random variables","heading":"1.1.7 Eigenvalue decomposition of \\(\\boldsymbol \\Sigma\\)","text":"Recall linear matrix algebra real symmetric matrix\nreal eigenvalues complete set orthogonal eigenvectors.\ncan obtained orthogonal eigendecomposition.Applying eigenvalue decomposition covariance matrix yields\n\\[\n\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\n\\]\n\\(\\boldsymbol U\\) orthogonal matrix containing eigenvectors covariance matrix\n\n\\[\\boldsymbol \\Lambda= \\begin{pmatrix}\n    \\lambda_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}\n\\end{pmatrix}\\]\ncontains corresponding eigenvalues \\(\\lambda_i\\).Importantly, eigenvalues covariance matrix real-valued\nconstruction constrained non-negative.\ncan seen computing quadratic form \\(\\boldsymbol z^T \\boldsymbol \\Sigma\\boldsymbol z\\)\n\\(\\boldsymbol z\\) non-random vector. non-zero \\(\\boldsymbol z\\)\n\\[\n\\begin{split}\n\\boldsymbol z^T  \\boldsymbol \\Sigma\\boldsymbol z& = \\boldsymbol z^T \\text{E}\\left(  (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T  \\right) \\boldsymbol z\\\\\n& =  \\text{E}\\left( \\boldsymbol z^T (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol z\\right) \\\\\n& =  \\text{E}\\left( \\left( \\boldsymbol z^T (\\boldsymbol x-\\boldsymbol \\mu) \\right)^2 \\right) \\geq 0 \\, .\\\\\n\\end{split}\n\\]\nFurthermore, \\(\\boldsymbol y= \\boldsymbol U^T \\boldsymbol z\\) get\n\\[\n\\begin{split}\n\\boldsymbol z^T  \\boldsymbol \\Sigma\\boldsymbol z& =  \\boldsymbol z^T\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T \\boldsymbol z\\\\\n                      & =  \\boldsymbol y^T \\boldsymbol \\Lambda\\boldsymbol y= \\sum_{=1}^d  y_i^2 \\lambda_i \\\\\n\\end{split}\n\\]\nhence \\(\\lambda_i \\geq 0\\).\nTherefore covariance matrix \\(\\boldsymbol \\Sigma\\) always\npositive semi-definite.fact, unless collinearity ( .e. variable linear function variables) eigenvalues positive \\(\\boldsymbol \\Sigma\\) positive definite.","code":""},{"path":"multivariate-random-variables.html","id":"joint-covariance-matrix","chapter":"1 Multivariate random variables","heading":"1.1.8 Joint covariance matrix","text":"Assume random vector \\(\\boldsymbol z\\) mean \\(\\text{E}(\\boldsymbol z) = \\boldsymbol \\mu_{\\boldsymbol z}\\)\ncovariance matrix \\(\\text{Var}(\\boldsymbol z) = \\boldsymbol \\Sigma_{\\boldsymbol z}\\).Often makes sense partion components \\(\\boldsymbol z\\) two groups\n\\[\n\\boldsymbol z= \\begin{pmatrix} \\boldsymbol x\\\\ \\boldsymbol y\\end{pmatrix}\n\\]\ninduces corresponding partition\nexpectation\n\\[\n\\boldsymbol \\mu_{\\boldsymbol z} =  \\begin{pmatrix} \\boldsymbol \\mu_{\\boldsymbol x} \\\\ \\boldsymbol \\mu_{\\boldsymbol y} \\end{pmatrix}\n\\]\n\\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu_{\\boldsymbol x}\\) \\(\\text{E}(\\boldsymbol y) = \\boldsymbol \\mu_{\\boldsymbol y}\\).Furthermore, joint covariance matrix \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) can\nwritten \n\\[\n\\boldsymbol \\Sigma_{\\boldsymbol z} =\n\\begin{pmatrix}\n\\boldsymbol \\Sigma_{\\boldsymbol x} &  \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} & \\boldsymbol \\Sigma_{\\boldsymbol y} \\\\\n\\end{pmatrix}\n\\]\ncontains within-group group covariance matrices\n\\(\\boldsymbol \\Sigma_{\\boldsymbol x}\\) \\(\\boldsymbol \\Sigma_{\\boldsymbol y}\\) diagonal elements \ncross-covariance matrix \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} = \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x}^T\\)\n-diagonal element.Note cross-covariance matrix \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}\\) rectangular\nsymmetric. also write\n\\(\\text{Cov}(\\boldsymbol x, \\boldsymbol y) = \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}\\)\ncan define cross-covariance directly \n\\[\n\\text{Cov}(\\boldsymbol x, \\boldsymbol y) = \\text{E}\\left( (\\boldsymbol x- \\boldsymbol \\mu_{\\boldsymbol x}) ( \\boldsymbol y- \\boldsymbol \\mu_{\\boldsymbol y} )^T \\right) = \\text{E}(\\boldsymbol x\\boldsymbol y^T)-\\boldsymbol \\mu_{\\boldsymbol x} \\boldsymbol \\mu_{\\boldsymbol y}^T\n\\]","code":""},{"path":"multivariate-random-variables.html","id":"quantities-related-to-the-covariance-matrix","chapter":"1 Multivariate random variables","heading":"1.1.9 Quantities related to the covariance matrix","text":"","code":""},{"path":"multivariate-random-variables.html","id":"correlation-matrix-boldsymbol-p","chapter":"1 Multivariate random variables","heading":"1.1.9.1 Correlation matrix \\(\\boldsymbol P\\)","text":"correlation matrix \\(\\boldsymbol P\\) (= upper case greek “rho”) standardised covariance matrix\\[\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}=\\text{Cor}(x_i,x_j)\\]\\[\\rho_{ii} = 1 = \\text{Cor}(x_i,x_i)\\]\\[ \\boldsymbol P= (\\rho_{ij}) = \\begin{pmatrix}\n    1 & \\dots & \\rho_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{d1} & \\dots & 1\n\\end{pmatrix}\\]\\(\\boldsymbol P\\) (“upper case rho”) symmetric matrix (\\(\\rho_{ij}=\\rho_{ji}\\)).Note variance-correlation decomposition\\[\\boldsymbol \\Sigma= \\boldsymbol V^{\\frac{1}{2}} \\boldsymbol P\\boldsymbol V^{\\frac{1}{2}}\\]\\(\\boldsymbol V\\) diagonal matrix containing variances:\\[ \\boldsymbol V= \\begin{pmatrix}\n    \\sigma_{11} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma_{dd}\n\\end{pmatrix}\\]\\[\\boldsymbol P= \\boldsymbol V^{-\\frac{1}{2}}\\boldsymbol \\Sigma\\boldsymbol V^{-\\frac{1}{2}}\\]definition correlation written matrix notation.covariance matrix, many applications makes sense \npartition joint correlation matrix\n\\[\n\\boldsymbol P_{\\boldsymbol z} =\n\\begin{pmatrix}\n\\boldsymbol P_{\\boldsymbol x} &  \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol P_{\\boldsymbol y\\boldsymbol x} & \\boldsymbol P_{\\boldsymbol y} \\\\\n\\end{pmatrix}\n\\]\nwithin-group group correlation matrices\n\\[\n\\boldsymbol P_{\\boldsymbol x} = \\boldsymbol V_{\\boldsymbol x}^{-\\frac{1}{2}} \\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol V_{\\boldsymbol x}^{-\\frac{1}{2}}\n\\]\n\n\\[\n\\boldsymbol P_{\\boldsymbol y} = \\boldsymbol V_{\\boldsymbol y}^{-\\frac{1}{2}} \\boldsymbol \\Sigma_{\\boldsymbol y} \\boldsymbol V_{\\boldsymbol y}^{-\\frac{1}{2}}\n\\]\ncross-correlation matrix\n\\[\n\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = \\boldsymbol V_{\\boldsymbol x}^{-\\frac{1}{2}} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\boldsymbol V_{\\boldsymbol y}^{-\\frac{1}{2}}\n\\]\n\n\\[\n\\boldsymbol P_{\\boldsymbol y\\boldsymbol x} = \\boldsymbol P_{\\boldsymbol x\\boldsymbol y}^T\n= \\boldsymbol V_{\\boldsymbol y}^{-\\frac{1}{2}} \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} \\boldsymbol V_{\\boldsymbol x}^{-\\frac{1}{2}} \\,.\n\\]","code":""},{"path":"multivariate-random-variables.html","id":"precision-matrix-or-concentration-matrix","chapter":"1 Multivariate random variables","heading":"1.1.9.2 Precision matrix or concentration matrix","text":"\\[\\boldsymbol \\Omega= (\\omega_{ij}) = \\boldsymbol \\Sigma^{-1}\\]\\(\\boldsymbol \\Omega\\) (“Omega”) inverse covariance matrix.inverse covariance matrix can obtained via\nspectral decomposition, followed inverting eigenvalues \\(\\lambda_i\\):\n\\[\\boldsymbol \\Sigma^{-1} = \\boldsymbol U\\boldsymbol \\Lambda^{-1} \\boldsymbol U^T =\n\\boldsymbol U\\begin{pmatrix}\n    \\lambda_{1}^{-1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}^{-1}\n\\end{pmatrix} \\boldsymbol U^T \\]Note eigenvalues \\(\\lambda_i\\) need positive \\(\\boldsymbol \\Sigma\\) can inverted. (.e., \\(\\boldsymbol \\Sigma\\) needs positive definite).\n\\(\\lambda_i = 0\\) \\(\\boldsymbol \\Sigma\\) singular invertible.Importance \\(\\boldsymbol \\Sigma^{-1}\\):Many expressions multivariate statistics contain \\(\\boldsymbol \\Sigma^{-1}\\) \\(\\boldsymbol \\Sigma\\).\\(\\boldsymbol \\Sigma^{-1}\\) close connection graphical models\n(e.g. conditional independence graph, partial correlations).\\(\\boldsymbol \\Sigma^{-1}\\) natural parameter exponential family perspective.","code":""},{"path":"multivariate-random-variables.html","id":"partial-correlation-matrix","chapter":"1 Multivariate random variables","heading":"1.1.9.3 Partial correlation matrix","text":"standardised version precision matrix, see later chapter graphical models.","code":""},{"path":"multivariate-random-variables.html","id":"total-variation-and-generalised-variance","chapter":"1 Multivariate random variables","heading":"1.1.9.4 Total variation and generalised variance","text":"summarise covariance matrix \\(\\boldsymbol \\Sigma\\) single scalar value two commonly used\nmeasures:total variation: \\(\\text{Tr}(\\boldsymbol \\Sigma) = \\sum_{=1}^d \\lambda_i\\)generalised variance: \\(\\det(\\boldsymbol \\Sigma) = \\prod_{=1}^d \\lambda_i\\)generalised variance \\(\\det(\\boldsymbol \\Sigma)\\) also known volume \\(\\boldsymbol \\Sigma\\).","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-distributions","chapter":"1 Multivariate random variables","heading":"1.2 Multivariate distributions","text":"","code":""},{"path":"multivariate-random-variables.html","id":"common-distributions","chapter":"1 Multivariate random variables","heading":"1.2.1 Common distributions","text":"multivariate statistics make use multivariate distributions. typically generalisations corresponding univariate distribution.Among commonly used multivariate distributions :multivariate normal distribution \\(N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) generalisation univariate normal distribution \\(N(\\mu, \\sigma^2)\\)categorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\) generalisation Bernoulli distribution \\(\\text{Ber}(\\theta)\\)multinomial distribution \\(\\text{Mult}(n, \\boldsymbol \\pi)\\) generalisation binomial distribution \\(\\text{Bin}(n, \\theta)\\)distribution already introduced earlier \nMATH27720 Statistics 2.Conceptually, multivariate generalisation work behave exactly\nunivariate counterparts employed settings.","code":""},{"path":"multivariate-random-variables.html","id":"further-multivariate-distributions","chapter":"1 Multivariate random variables","heading":"1.2.2 Further multivariate distributions","text":"multivariate Bayesian analyis also need consider number multivariate distributions:Dirichlet distribution \\(\\text{Dir}(\\boldsymbol \\alpha)\\) generalisation beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\),Wishart distribution generalisation gamma distribution \\(\\text{Gam}(\\alpha, \\theta)\\),inverse Wishart distribution generalisation inverse gamma distribution \\(\\text{Inv-Gam}(\\alpha, \\beta)\\).technical details densities etc. multivariate distribution families refer supplementary Probability Distribution refresher notes.","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-normal-distribution","chapter":"1 Multivariate random variables","heading":"1.3 Multivariate normal distribution","text":"multivariate normal disribution ubiquitous multivariate statistics hence important discuss detail.multivariate normal model generalisation univariate normal distribution\ndimension 1 dimension \\(d\\).","code":""},{"path":"multivariate-random-variables.html","id":"univariate-normal-distribution","chapter":"1 Multivariate random variables","heading":"1.3.1 Univariate normal distribution:","text":"\\[\\text{Dimension } d = 1\\]\n\\[x \\sim N(\\mu, \\sigma^2)\\]\n\\[\\text{E}(x) = \\mu \\space , \\space  \\text{Var}(x) = \\sigma^2\\]Probability Density Function:\\[f(x |\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right) \\]Plot univariate normal density :Unimodal peak \\(\\mu\\), width determined \\(\\sigma\\) (plot: \\(\\mu=2, \\sigma^2=1\\) )Special case: standard normal \\(\\mu=0\\) \\(\\sigma^2=1\\):\\[f(x |\\mu=0,\\sigma^2=1)=\\frac{1}{\\sqrt{2\\pi}} \\exp\\left( {-\\frac{x^2}{2}} \\right) \\]Differential entropy:\\[\nH(F) = \\frac{1}{2} (\\log(2 \\pi \\sigma^2) + 1)\n\\]Cross-entropy:\\[\nH(F_{\\text{ref}}, F) = \\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 }\n+\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2}  +\\log(2 \\pi \\sigma^2) \\right)\n\\]\nKL divergence:\\[\nD_{\\text{KL}}(F_{\\text{ref}}, F) = H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) =\n\\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 }\n+\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2}  -\\log\\left(\\frac{\\sigma^2_{\\text{ref}}}{ \\sigma^2}\\right) -1\n\\right)\n\\]Maximum entropy characterisation: normal distribution unique distribution\n\nhighest (differential) entropy continuous distributions support minus infinity plus infinity given mean variance.fact one reasons normal distribution important (und useful) –\nknow random variable mean variance, much else, using \nnormal distribution reasonable well justified model.","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-normal-model","chapter":"1 Multivariate random variables","heading":"1.3.2 Multivariate normal model","text":"\\[\\text{Dimension } d\\]\n\\[\\boldsymbol x\\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\]\n\\[\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\space , \\space  \\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\]Density:\\[f(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) = \\det(2 \\pi \\boldsymbol \\Sigma)^{-\\frac{1}{2}} \\exp\\left({{-\\frac{1}{2}} \\underbrace{\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1 \\times d} \\underbrace{\\boldsymbol \\Sigma^{-1}}_{d \\times d} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d \\times 1} }_{1 \\times 1 = \\text{scalar!}}}\\right)\\]density contains precision matrix \\(\\boldsymbol \\Sigma^{-1}\\)invert covariance matrix \\(\\boldsymbol \\Sigma\\) need invert eigenvalues \\(\\lambda_i\\) (hence require \\(\\lambda_i > 0\\))density also contains \\(\\det(\\boldsymbol \\Sigma) = \\prod\\limits_{=1}^d \\lambda_i\\) \\(\\equiv\\) product eigenvalues \\(\\boldsymbol \\Sigma\\)note \\(\\det(2 \\pi \\boldsymbol \\Sigma)^{-\\frac{1}{2}} = \\det(2 \\pi \\boldsymbol I_d)^{-\\frac{1}{2}} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}} = (2 \\pi)^{-d/2} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}}\\)Special case: standard multivariate normal \\[\\boldsymbol \\mu=\\boldsymbol 0, \\boldsymbol \\Sigma=\\boldsymbol =\\begin{pmatrix}\n    1 & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & 1\n\\end{pmatrix}\\]\\[f(\\boldsymbol x| \\boldsymbol \\mu=\\boldsymbol 0,\\boldsymbol \\Sigma=\\boldsymbol )=(2\\pi)^{-d/2}\\exp\\left( -\\frac{1}{2} \\boldsymbol x^T \\boldsymbol x\\right) = \\prod\\limits_{=1}^d \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_i^2}{2}\\right)\\]\nequivalent product \\(d\\) univariate standard normals!Misc:\\(d=1\\), multivariate normal density reduces univariate normal density.\\(\\boldsymbol \\Sigma\\) diagonal (.e. \\(\\boldsymbol P= \\boldsymbol \\), correlation), multivariate normal density product univariate normal densities (see Worksheet 2).Plot multivariate normal density:Location: \\(\\boldsymbol \\mu\\)Shape: \\(\\boldsymbol \\Sigma\\)Unimodal: one peakSupport \\(-\\infty\\) \\(+\\infty\\) dimensionAn interactive R Shiny web app bivariate normal density plot\navailable online https://minerva..manchester.ac.uk/shiny/strimmer/bvn/ .Differential entropy:\\[\nH = \\frac{1}{2} (\\log \\det(2 \\pi \\boldsymbol \\Sigma) + d)\n\\]Cross-entropy:\\[\nH(F_{\\text{ref}}, F) = \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    + \\log \\det \\biggl( 2 \\pi \\boldsymbol \\Sigma\\biggr)    \\biggr\\}\n\\]\nKL divergence:\\[\n\\begin{split}\nD_{\\text{KL}}(F_{\\text{ref}}, F) &= H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) \\\\\n&= \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n     - d   \\biggr\\} \\\\\n\\end{split}\n\\]","code":""},{"path":"multivariate-random-variables.html","id":"shape-of-the-multivariate-normal-density","chapter":"1 Multivariate random variables","heading":"1.3.3 Shape of the multivariate normal density","text":"Now show contour lines multivariate normal density always take form ellipse, radii ellipse determined eigenvalues \n\\(\\boldsymbol \\Sigma\\).start observing circle radius \\(r\\) around origin can described set points \\((x_1,x_2)\\) satisfying\n\\(x_1^2+x_2^2 = r^2\\), equivalently, \\(\\frac{x_1^2}{r^2} + \\frac{x_2^2}{r^2} = 1\\).\ngeneralised shape ellipse allowing (two dimensions) two radii\n\\(r_1\\) \\(r_2\\) \n\\(\\frac{x_1^2}{r_1^2} + \\frac{x_2^2}{r_2^2} = 1\\), vector notation\n\\(\\boldsymbol x^T \\text{Diag}(r_1^2, r_2^2)^{-1} \\boldsymbol x= 1\\). \\(d\\) dimensions allowing rotation \naxes shift origin 0 \\(\\boldsymbol \\mu\\) condition ellipse \n\\[(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol Q\\, \\text{Diag}(r_1^2, \\ldots , r_d^2)^{-1} \\boldsymbol Q^T (\\boldsymbol x-\\boldsymbol \\mu) = 1\\]\n\\(\\boldsymbol Q\\) orthogonal matrix whose column vectors indicate direction axes. also called principal axes ellipse.contour line probability density function set connected points density assumes constant value. case multivariate normal distribution keeping density fixed value implies \\((\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu) = c\\) \\(c\\) constant. Using eigenvalue decomposition \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) can rewrite condition \n\\[\n(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol U\\boldsymbol \\Lambda^{-1} \\boldsymbol U^T (\\boldsymbol x-\\boldsymbol \\mu) = c \\,.\n\\]\nimplies thatthe contour lines multivariate normal density indeed ellipses,squared radii ellipse proportional eigenvalues \\(\\boldsymbol \\Sigma\\) andthe direction axes ellipse given principal axes \ncorrespond colum vectors \\(\\boldsymbol U\\) (.e. eigenvectors \\(\\boldsymbol \\Sigma\\)).Equivalently, positive square roots eigenvalues proportional radii ellipse. Hence, singular covariance matrix one \\(\\lambda_i=0\\) corresponding radii zero.interactive R Shiny web app play contour lines \nbivariate normal distribution online https://minerva..manchester.ac.uk/shiny/strimmer/bvn/ .","code":""},{"path":"multivariate-random-variables.html","id":"three-types-of-covariances","chapter":"1 Multivariate random variables","heading":"1.3.4 Three types of covariances","text":"Following can parameterise covariance matrix terms ) volume, ii) shape iii) orientation writing\n\\[\n\\boldsymbol \\Sigma= \\kappa \\, \\boldsymbol U\\boldsymbol \\boldsymbol U^T = \\boldsymbol U\\; \\left(\\kappa \\boldsymbol \\right) \\; \\boldsymbol U^T\n\\]\n\\(\\boldsymbol =\\text{Diag}(a_1, \\ldots, a_d)\\) \\(\\det(\\boldsymbol ) = \\prod_{=1}^d a_i = 1\\).\nNote parameterisation eigenvalues \\(\\boldsymbol \\Sigma\\) \\(\\lambda_i = \\kappa a_i\\).volume \\(\\det(\\boldsymbol \\Sigma) = \\kappa^d\\), determined single parameter \\(\\kappa\\). parameter can interpreted length side \\(d\\)-dimensional hypercube.shape determined diagonal matrix \\(\\boldsymbol \\) \\(d-1\\) free parameters. Note \\(d-1\\) \\(d\\) free parameters constraint \\(\\det(\\boldsymbol ) = 1\\).orientation given orthogonal matrix \\(\\boldsymbol U\\), \\(d (d-1)/2\\) free parameters.leads classification covariances three varieties:Type 1: spherical covariance \\(\\boldsymbol \\Sigma=\\kappa \\boldsymbol \\),\nspherical contour lines, 1 free parameter (\\(\\boldsymbol =\\boldsymbol \\), \\(\\boldsymbol U=\\boldsymbol \\)).Example:\n\\(\\boldsymbol \\Sigma= \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1/ \\lambda_2} = 1\\):Type 2: diagonal covariance \\(\\boldsymbol \\Sigma= \\kappa \\boldsymbol \\), elliptical contour lines axes ellipse oriented parallel coordinates, \\(d\\) free parameters (\\(\\boldsymbol U=\\boldsymbol \\)).Example:\n\\(\\boldsymbol \\Sigma= \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1 / \\lambda_2} \\approx 1.41\\):Type 3: general unrestricted covariance \\(\\boldsymbol \\Sigma\\),\nelliptical contour lines, axes ellipse oriented according \ncolumn vectors \\(\\boldsymbol U\\),\n\\(d (d+1)/2\\) free parameters.Example:\n\\(\\boldsymbol \\Sigma= \\begin{pmatrix} 2 & 0.6 \\\\ 0.6 & 1 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1 / \\lambda_2} \\approx 2.20\\):","code":""},{"path":"multivariate-random-variables.html","id":"concentration-of-probability-mass-for-small-and-large-dimension","chapter":"1 Multivariate random variables","heading":"1.3.5 Concentration of probability mass for small and large dimension","text":"density multivariate normal distribution bell shape single mode. Intuitively, may assume probability mass always concentrated around mode, univariate case (\\(d=1\\)). still true small dimensions (small \\(d\\)) now show intuition incorrect high dimensions (large \\(d\\)).simplicity consider standard multivariate normal distribution dimension \\(d\\)\n\\[\\boldsymbol x\\sim N_d(\\boldsymbol 0, \\boldsymbol I_d)\\]\nspherical covariance \\(\\boldsymbol I_d\\) sample \\(\\boldsymbol x\\). squared Euclidean length \\(\\boldsymbol x\\) \n\\(r^2= || \\boldsymbol x||^2 = \\boldsymbol x^T \\boldsymbol x= \\sum_{=1}^d x_i^2\\). corresponding density \\(d\\)-dimensional standard multivariate normal distribution \n\\[\ng_d(\\boldsymbol x) = (2\\pi)^{-d/2} e^{-\\boldsymbol x^T \\boldsymbol x/2}\n\\]\nnatural way define main part “bell” standard multivariate normal set \n\\(\\boldsymbol x\\) density larger specified fraction \\(\\eta\\) (say 0.001) maximum value density \\(g_d(0)\\) peak zero.\nformalise\n\\[\nB = \\left\\{ \\boldsymbol x: \\frac{g_d(\\boldsymbol x)}{ g_d(0)} > \\eta    \\right\\}\n\\]\ncan equivalently written set\n\\[\nB = \\{ \\boldsymbol x: \\boldsymbol x^T \\boldsymbol x= r^2 < -2 \\log(\\eta) = r^2_{\\max} \\}\n\\]individual component sample \\(\\boldsymbol x\\) independently distributed \\(x_i \\sim N(0,1)\\), hence \\(r^2 \\sim \\text{$\\chi^2_{d}$}\\) chi-squared distributed degree freedom \\(d\\).\nprobability \\(\\text{Pr}(\\boldsymbol x\\B)\\) can thus obtained value \ncumulative density function chi-squared distribution \\(d\\) degrees\nfreedom \\(r^2_{\\max}\\). Computing probability fixed \\(\\eta\\) function dimension \\(d\\) obtain following curve:\nplot \\(\\eta=0.001\\). can see dimensions around \\(d=10\\) probability mass indeed concentrated center distribution\n\\(d=30\\) onwards moved completely tails.","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"multivariate-estimation-in-large-sample-and-small-sample-settings","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2 Multivariate estimation in large sample and small sample settings","text":"","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"overview","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.1 Overview","text":"practical application multivariate normal model need \nlearn parameters observed data points:\\[\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_n \\stackrel{\\text{iid}}\\sim F_{\\boldsymbol \\theta}\\]first consider case \nmany measurements available (\\(n\\) large), subsequently case \nnumber data points \\(n\\) small compared dimensions number parameters.previous course year 2\n(see MATH27720 Statistics 2)\nmethod maximum likelihood well essentials Bayesian statistics\nintroduced. apply approaches problem estimating parameters \nmultivariate normal distribution also show main Bayesian modelling strategies extend multivariate case.","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"empirical-estimates","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.2 Empirical estimates","text":"","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"general-principle","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.2.1 General principle","text":"large \\(n\\) thanks law large numbers:\n\\[\\underbrace{F}_{\\text{true}} \\approx \\underbrace{\\widehat{F}_n}_{\\text{empirical}}\\]now like estimate \\(\\) functional \\(=m(F)\\) distribution \\(F\\)\n— recall functional function takes another function argument.\nexample standard distributional summaries mean, median etc. derived \\(F\\) hence \nfunctionals \\(F\\).empirical estimate obtained replacing unknown true distribution\n\\(F\\) observed empirical distribution: \\(\\hat{} = m(\\widehat{F}_n)\\).example, expectation random variable approximated/estimated\naverage observations:\n\\[\\text{E}_F(\\boldsymbol x) \\approx \\text{E}_{\\widehat{F}_n}(\\boldsymbol x) = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k\\]\n\\[\\text{E}_F(g(\\boldsymbol x)) \\approx  \\text{E}_{\\widehat{F}_n}(g(\\boldsymbol x)) = \\frac{1}{n}\\sum^{n}_{k=1} g(\\boldsymbol x_k)\\]Simple recipe obtain empirical estimator: simply replace expectation operator\nsample average.work: empirical distribution \\(\\widehat{F}_n\\) nonparametric maximum likelihood estimate \\(F\\) (see likelihood estimation).Note: approximation \\(F\\) \\(\\widehat{F}_n\\) also basis approaches Efron’s bootstrap method (1979) 2.","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"empirical-estimates-of-mean-and-covariance","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.2.2 Empirical estimates of mean and covariance","text":"Recall definitions:\n\\[\n\\boldsymbol \\mu= \\text{E}(\\boldsymbol x)\n\\]\n\n\\[\n\\boldsymbol \\Sigma= \\text{E}\\left(   (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T \\right)\n\\]empirical estimate replace expectations \ncorresponding sample averages.resulting estimators can written three different ways:Vector notation:\\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k\\]\\[\n\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\sum^{n}_{k=1} (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})  (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})^T\n= \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k  \\boldsymbol x_k^T - \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\n\\]Component notation:corresponding component notation \\(\\boldsymbol X= (x_{ki})\\)\nfollowing statistics convention samples\ncontained rows \\(\\boldsymbol X\\) get:\\[\\hat{\\mu}_i = \\frac{1}{n}\\sum^{n}_{k=1} x_{ki}\\]\\[\\hat{\\sigma}_{ij} = \\frac{1}{n}\\sum^{n}_{k=1} (x_{ki}-\\hat{\\mu}_i) (\nx_{kj}-\\hat{\\mu}_j )\\]\\[\\hat{\\boldsymbol \\mu}=\\begin{pmatrix}\n    \\hat{\\mu}_{1}       \\\\\n    \\vdots \\\\\n    \\hat{\\mu}_{d}\n\\end{pmatrix}, \\widehat{\\boldsymbol \\Sigma} = (\\hat{\\sigma}_{ij})\\]Variance estimate:\\[\\hat{\\sigma}_{ii} = \\frac{1}{n}\\sum^{n}_{k=1} \\left(x_{ki}-\\hat{\\mu}_i\\right)^2\\]\nNote factor \\(\\frac{1}{n}\\) (\\(\\frac{1}{n-1}\\))Data matrix notation:empirical mean covariance can also written terms data matrix \\(\\boldsymbol X\\).data matrix \\(\\boldsymbol X\\) follows statistics convention\ncan write\\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n} \\boldsymbol X^T \\boldsymbol 1_n\\]\\[\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\boldsymbol X^T \\boldsymbol X- \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\\]hand, \\(\\boldsymbol X\\) follows engineering convention\nsamples columns\nestimators written :\\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n} \\boldsymbol X\\boldsymbol 1_n\\]\\[\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\boldsymbol X\\boldsymbol X^T - \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\\]avoid confusion using matrix component notation need always state \nconvention used! notes exlusively follow statistics convention.","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"maximum-likelihood-estimation","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.3 Maximum likelihood estimation","text":"","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"general-principle-1","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.3.1 General principle","text":"R.. Fisher (1922) 3: model-based estimators using density probability mass functionLog-likelihood function:Observing data \\(D=\\{ \\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) log-likelihood function \\[\\log L(\\boldsymbol \\theta| D ) = \\sum^{n}_{k=1}  \\underbrace{\\log f}_{\\text{log-density}}(\\boldsymbol x_k |\\boldsymbol \\theta)\\]Maximum likelihood estimate:\n\\[\\hat{\\boldsymbol \\theta}_{\\text{ML}}=\\underset{\\boldsymbol \\theta}{\\arg\\,\\max} \\log L(\\boldsymbol \\theta| D)\\]Maximum likelihood (ML) finds parameters make observed data likely (find probable model!)Recall MATH27720 Statistics 2\nmaximum likelihood closely linked minimising relative entropy (KL divergence)\n\\(D_{\\text{KL}}(F, F_{\\boldsymbol \\theta})\\) unknown true model \\(F\\) specified model \\(F_{\\boldsymbol \\theta}\\). Specifically, large\nsample size \\(n\\) model \\(F_{\\hat{\\boldsymbol \\theta}}\\) fit maximum likelihood indeed model closest \\(F\\).Correspondingly, great appeal maximum likelihood estimates (MLEs) optimal large \\(\\mathbf{n}\\), .e. large sample size estimator can constructed outperforms MLE (note emphasis “large \\(n\\)”!).\nadvantage method maximum likelihood provide point estimate also asymptotic error (via observed Fisher information related curvature log-likelihood function).","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"maximum-likelihood-estimates-of-the-parameters-of-the-multivariate-normal-distribution","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.3.2 Maximum likelihood estimates of the parameters of the multivariate normal distribution","text":"now derive MLE parameters \\(\\boldsymbol \\mu\\) \\(\\boldsymbol \\Sigma\\) multivariate normal distribution.\ncorresponding log-likelihood function \n\\[\n\\begin{split}\n\\log L(\\boldsymbol \\mu, \\boldsymbol \\Sigma| D) & = \\sum_{k=1}^n \\log f( \\boldsymbol x_k | \\boldsymbol \\mu, \\boldsymbol \\Sigma) \\\\\n  & = -\\frac{n d}{2} \\log(2\\pi) -\\frac{n}{2} \\log \\det(\\boldsymbol \\Sigma)  \n   - \\frac{1}{2}  \\sum_{k=1}^n  (\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x_k-\\boldsymbol \\mu) \\,.\\\\\n\\end{split}\n\\]\nWritten terms precision matrix \\(\\boldsymbol \\Omega= \\boldsymbol \\Sigma^{-1}\\) becomes\n\\[\n\\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega| D) = -\\frac{n d}{2} \\log(2\\pi) +\\frac{n}{2} \\log \\det(\\boldsymbol \\Omega)  - \\frac{1}{2}  \\sum_{k=1}^n  (\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Omega(\\boldsymbol x_k-\\boldsymbol \\mu) \\,.\n\\]\nFirst, find MLE \\(\\boldsymbol \\mu\\) compute\n\\[\\nabla_{\\boldsymbol \\mu} \\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega| D) =  \\frac{\\partial \\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega| D) }{\\partial \\boldsymbol \\mu}= \\sum_{k=1}^n  \\boldsymbol \\Omega(\\boldsymbol x_k-\\boldsymbol \\mu)\\]\nnoting \\(\\boldsymbol \\Omega\\) symmetric.\nSetting equal zero get \\(\\sum_{k=1}^n \\boldsymbol x_k = n \\hat{\\boldsymbol \\mu}_{ML}\\) thus\n\\[\\hat{\\boldsymbol \\mu}_{ML} = \\frac{1}{n} \\sum_{k=1}^n \\boldsymbol x_k\\,.\\]Next, obtain MLE \\(\\boldsymbol \\Omega\\) compute\n\\[\\frac{\\partial \\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega| D) }{\\partial \\boldsymbol \\Omega}=\\frac{n}{2}\\boldsymbol \\Omega^{-1} - \\frac{1}{2}  \\sum_{k=1}^n (\\boldsymbol x_k-\\boldsymbol \\mu) (\\boldsymbol x_k-\\boldsymbol \\mu)^T\\].\nSetting equal zero substituting MLE \\(\\boldsymbol \\mu\\) get\n\\[\\widehat{\\boldsymbol \\Omega}^{-1}_{ML}=  \\frac{1}{n} \\sum_{k=1}^n  (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu}) (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})^T=\\widehat{\\boldsymbol \\Sigma}_{ML}\\,.\\]See supplementary Matrix Refresher notes relevant formulas vector matrix calculus.Therefore, MLEs identical empirical estimates.Note factor \\(\\frac{1}{n}\\) MLE covariance matrix.","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"sampling-distribution-of-the-empirical-maximum-likelihood-estimates","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.4 Sampling distribution of the empirical / maximum likelihood estimates","text":"\\(\\boldsymbol x_1,...,\\boldsymbol x_n \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) one can find exact distributions\nestimators.\nsample average denoted \\(\\bar{\\boldsymbol x}= \\frac{1}{n}\\sum_{=1}^n \\boldsymbol x_i\\)1. Distribution estimate mean:empirical estimate mean normally distributed:\\[\\hat{\\boldsymbol \\mu}_{ML}=\\bar{\\boldsymbol x} \\sim N_d\\left(\\boldsymbol \\mu, \\frac{\\boldsymbol \\Sigma}{n}\\right)\\]\nSince\n\\(\\text{E}(\\hat{\\boldsymbol \\mu}_{ML}) = \\boldsymbol \\mu\\Longrightarrow \\hat{\\boldsymbol \\mu}_{ML}\\) unbiased.2. Distribution covariance estimate:empirical unbiased estimate covariance matrix\nWishart distributed:Case unknown mean \\(\\boldsymbol \\mu\\) (estimated \\(\\bar{\\boldsymbol x}\\)):\\[\\widehat{\\boldsymbol \\Sigma}_{ML} = \\frac{1}{n}\\sum_{=1}^n (\\boldsymbol x_i -\\bar{\\boldsymbol x})(\\boldsymbol x_i -\\bar{\\boldsymbol x})^T \\sim \\text{W}_d\\left(\\frac{\\boldsymbol \\Sigma}{n}, n-1\\right)\\]Since\n\\(\\text{E}(\\widehat{\\boldsymbol \\Sigma}_{ML}) = \\frac{n-1}{n}\\boldsymbol \\Sigma\\) \\(\\Longrightarrow \\widehat{\\boldsymbol \\Sigma}_{ML}\\) biased, \\(\\text{Bias}(\\widehat{\\boldsymbol \\Sigma}_{ML} ) = \\boldsymbol \\Sigma- \\text{E}(\\widehat{\\boldsymbol \\Sigma}_{ML}) = -\\frac{\\boldsymbol \\Sigma}{n}\\).Easy make unbiased:\n\\(\\widehat{\\boldsymbol \\Sigma}_{UB} = \\frac{n}{n-1}\\widehat{\\boldsymbol \\Sigma}_{ML}= \\frac{1}{n-1}\\sum^n_{k=1}\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)^T\\)\ndistributed \n\\[\\widehat{\\boldsymbol \\Sigma}_{UB}  \\sim \\text{W}_d\\left(\\frac{\\boldsymbol \\Sigma}{n-1}, n-1\\right)\\]Hence \\(\\text{E}(\\widehat{\\boldsymbol \\Sigma}_{UB}) = \\boldsymbol \\Sigma\\) \\(\\Longrightarrow \\widehat{\\boldsymbol \\Sigma}_{UB}\\) unbiased.unbiasedness estimator relevant criterion multivariate statistics, especially number samples \nsmall compared dimension (see ).Covariance estimator known mean \\(\\boldsymbol \\mu\\):\\[\\frac{1}{n}\\sum_{=1}^n (\\boldsymbol x_i -\\boldsymbol \\mu)(\\boldsymbol x_i -\\boldsymbol \\mu)^T \\sim \\text{W}_d\\left(\\frac{\\boldsymbol \\Sigma}{n}, n\\right)\\]","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"small-sample-estimation","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.5 Small sample estimation","text":"","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.5.1 Problems with maximum likelihood in small sample settings and high dimensions","text":"Modern data high dimensional!Data sets \\(n<d\\), .e. high dimension \\(d\\) small sample size \\(n\\) now common \nmany fields, e.g., medicine, biology also finance business analytics.\\[n = 100 \\, \\text{(e.g, patients/samples)}\\]\n\\[d = 20000 \\, \\text{(e.g., genes/SNPs/proteins/variables)}\\]\nReasons:number measured variables increasing quickly technological advances (e.g. genomics)number samples similary increased (cost ethical reasons)General problems MLEs:ML estimators optimal sample size large compared number parameters. However, optimality valid sample size moderate smaller number parameters.enough data ML estimate overfits. means ML fits current data perfectly resulting model generalise well (.e. model perform poorly prediction)choice different models different complexity ML always select model largest number parameters.-> high-dimensional data small sample size maximum likelihood estimation work!!!History Statistics:Much modern statistics (1960 onwards) devoted development inference estimation techniques work complex, high-dimensional data.Maximum likelihood method classical statistics (time 1960).1960 modern (computational) statistics emerges, starting \n“Stein Paradox” (1956): Charles Stein showed multivariate setting ML estimators dominated (= always worse ) shrinkage estimators!example, shrinkage estimator mean better (terms MSE) average (MLE)!Modern statistics developed many different (related) methods use high-dimensional, small sample settings:regularised estimatorsshrinkage estimatorspenalised maximum likelihood estimatorsBayesian estimatorsEmpirical Bayes estimatorsKL / entropy-based estimatorsMost scope class, covered advanced statistical courses.Next, describe simple regularised estimator estimation covariance\nuse later (.e. classification).","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"estimation-of-covariance-matrix-in-small-sample-settings","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.5.2 Estimation of covariance matrix in small sample settings","text":"Problems ML estimate \\(\\boldsymbol \\Sigma\\)\\(\\Sigma\\) O(\\(d^2\\)) number parameters!\n\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}^{\\text{MLE}}\\) requires lot data! \\(n\\gg d \\text{ } d^2\\)\\(\\Sigma\\) O(\\(d^2\\)) number parameters!\n\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}^{\\text{MLE}}\\) requires lot data! \\(n\\gg d \\text{ } d^2\\)\\(n < d\\) \\(\\hat{\\boldsymbol \\Sigma}\\) positive semi-definite (even true \\(\\Sigma\\) positive definite!)\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}\\) vanishing eigenvalues (\\(\\lambda_i=0\\)) thus inverted singular!\\(n < d\\) \\(\\hat{\\boldsymbol \\Sigma}\\) positive semi-definite (even true \\(\\Sigma\\) positive definite!)\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}\\) vanishing eigenvalues (\\(\\lambda_i=0\\)) thus inverted singular!Note many expression multivariate statistics actually need use inverse covariance matrix, e.g., density multivariate normal distribution, essential obtain non-singular invertible estimate covariance matrix.Making ML estimate \\(\\boldsymbol \\Sigma\\) invertibleThere simple numerical trick credited . N. Tikhonov make \\(\\hat{\\boldsymbol \\Sigma}\\) invertible, adding small\nnumber (say \\(\\varepsilon=10^{-6}\\) diagonal elements \\(\\hat{\\boldsymbol \\Sigma}\\):\n\\[\n\\boldsymbol S_{\\text{Tik}} = \\hat{\\boldsymbol \\Sigma} + \\varepsilon \\boldsymbol \n\\]resulting \\(\\boldsymbol S_{\\text{Tik}}\\) positive definite sum symmetric positive definite matrix (\\(\\varepsilon \\boldsymbol \\)) symmetric positive semi-definite matrix (\\(\\hat{\\boldsymbol \\Sigma}\\)) \nalways positive definite.However, simple regularisation results invertible matrix estimator improved MLE, matrix \\(\\boldsymbol S_{\\text{Tik}}\\) also poorly conditioned (.e. large condition number).Simple regularised estimate \\(\\boldsymbol \\Sigma\\)Regularised estimator \\(\\boldsymbol S^\\ast\\) = convex combination \\(\\boldsymbol S= \\hat{\\boldsymbol \\Sigma}^\\text{MLE}\\) \\(\\boldsymbol I_d\\) (identity matrix) getRegularisation:\n\\[\n\\underbrace{\\boldsymbol S^\\ast}_{\\text{regularised estimate}} = (1-\\lambda)\\underbrace{\\boldsymbol S}_{\\text{ML estimate}} +\\underbrace{\\lambda}_{\\text{shrinkage intensity}} \\, \\underbrace{\\boldsymbol I_d}_{\\text{target}}\\]Idea: choose \\(\\lambda \\[0,1]\\) \\(\\boldsymbol S^\\ast\\) better (e.g. terms MSE) \\(\\boldsymbol S\\) \\(\\boldsymbol I_d\\). Note \\(\\lambda\\) need small like \\(\\varepsilon\\).form estimator corresponds computing mean Bayesian posterior\ndirectly shrinking MLE towards prior mean (target):\n\\[\n\\underbrace{\\boldsymbol S^\\ast}_{\\text{posterior mean}} = \\underbrace{\\lambda \\boldsymbol I_d}_{\\text{prior information}}  + (1-\\lambda)\\underbrace{\\boldsymbol S}_{\\text{data summarised maximum likelihood}}\n\\]Prior information helps infer \\(\\boldsymbol \\Sigma\\) even small samples.also called shrinkage estimator since -diagonal entries shrunk towards zero.type linear shrinkage/regularisation natural exponential family models (Diaconis Ylvisaker, 1979).Instead diagonal target options possible, e.g. block-diagonal banded covariances.\\(\\lambda\\) prespecified learned data (see ) resulting estimate empirical Bayes estimator.resulting estimate typically biased mixing target increase bias.find optimal shrinkage / regularisation parameter \\(\\lambda\\)?One way minimise \\(\\text{MSE}\\) (Mean Squared Error). also called L2 regularisation\nRidge regularisation.Bias-variance trade-: \\(\\text{MSE}\\) composed squared bias variance.\\[\\text{MSE}(\\theta) = \\text{E}((\\hat{\\theta}-\\theta)^2) = \\text{Bias}(\\hat{\\theta})^2 + \\text{Var}(\\hat{\\theta})\\]\n\\(\\text{Bias}(\\hat{\\theta}) = \\text{E}(\\hat{\\theta})-\\theta\\)\\(\\boldsymbol S\\): ML estimate, many parameters, low bias, high variance\\(\\boldsymbol I_d\\): “target”, parameters, high bias, low variance\\(\\Longrightarrow\\) reduce high variance \\(\\boldsymbol S\\) introducing bit bias \\(\\boldsymbol I_d\\)!\\(\\Longrightarrow\\) overall, \\(\\text{MSE}\\) decreasedChallenge: since don’t know true \\(\\boldsymbol \\Sigma\\) actually compute \\(\\text{MSE}\\) directly estimate ! done practise?cross-validation (=resampling procedure)using analytic approximation (e.g. Stein’s formula)Worksheet 3 empirical estimator covariance compared regularised covariance estimator implemented R package “corpcor”. uses regularisation similar (correlation rather covariance matrix) employs analytic data-adaptive estimate shrinkage intensity \\(\\lambda\\).\nestimator variant empirical Bayes / James-Stein estimator (see MATH27720 Statistics 2).","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"full-bayesian-multivariate-modelling","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.6 Full Bayesian multivariate modelling","text":"See also section multivariate distributions \nProbability Distribution refresher details distributions used\n.","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"three-main-scenarios","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.6.1 Three main scenarios","text":"discussed MATH27720 Statistics 2 three main\nBayesian models univariate case cover large range applications:beta-binomial model estimate proportionsthe normal-normal model estimate meansthe inverse gamma-normal model estimate variancesBelow briefly sketch extensions three elementary models multivariate case.","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"dirichlet-multinomial-model","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.6.2 Dirichlet-multinomial model","text":"generalises univariate beta-binomial model.Dirichlet distribution useful conjugate prior posterior distribution parameters categorical distribution.Data: \\(D=\\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) \\(\\boldsymbol x_i \\sim \\text{Cat}(\\boldsymbol \\pi)\\)Data: \\(D=\\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) \\(\\boldsymbol x_i \\sim \\text{Cat}(\\boldsymbol \\pi)\\)MLE: \\(\\hat{\\boldsymbol \\pi}_{ML} = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i\\)MLE: \\(\\hat{\\boldsymbol \\pi}_{ML} = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i\\)Prior parameters (Dirichlet mean parameterisation): \\(k_0\\), \\(\\boldsymbol \\pi_0\\),\\[\\boldsymbol \\pi\\sim \\text{Dir}(\\boldsymbol \\pi_0, k_0)\\]\n\\[\\text{E}(\\boldsymbol \\pi) = \\boldsymbol \\pi_0\\]Prior parameters (Dirichlet mean parameterisation): \\(k_0\\), \\(\\boldsymbol \\pi_0\\),\\[\\boldsymbol \\pi\\sim \\text{Dir}(\\boldsymbol \\pi_0, k_0)\\]\n\\[\\text{E}(\\boldsymbol \\pi) = \\boldsymbol \\pi_0\\]Posterior parameters: \\(k_1 = k_0+n\\), \\(\\boldsymbol \\pi_1 = \\lambda \\boldsymbol \\pi_0 + (1-\\lambda) \\, \\hat{\\boldsymbol \\pi}_{ML}\\) \\(\\lambda = \\frac{k_0}{k_1}\\)\\[\\boldsymbol \\pi\\,| \\, D \\sim \\text{Dir}(\\boldsymbol \\pi_1, k_1)\\]\n\\[\\text{E}(\\boldsymbol \\pi\\,| \\, D) = \\boldsymbol \\pi_1\\]Posterior parameters: \\(k_1 = k_0+n\\), \\(\\boldsymbol \\pi_1 = \\lambda \\boldsymbol \\pi_0 + (1-\\lambda) \\, \\hat{\\boldsymbol \\pi}_{ML}\\) \\(\\lambda = \\frac{k_0}{k_1}\\)\\[\\boldsymbol \\pi\\,| \\, D \\sim \\text{Dir}(\\boldsymbol \\pi_1, k_1)\\]\n\\[\\text{E}(\\boldsymbol \\pi\\,| \\, D) = \\boldsymbol \\pi_1\\]Equivalent update rule (Dirichlet \\(\\boldsymbol \\alpha\\) parameter): \\(\\boldsymbol \\alpha_0\\) \\(\\rightarrow\\) \\(\\boldsymbol \\alpha_1 = \\boldsymbol \\alpha_0 + \\sum_{=1}^n \\boldsymbol x_i = \\boldsymbol \\alpha_0 + n \\hat{\\boldsymbol \\pi}_{ML}\\)Equivalent update rule (Dirichlet \\(\\boldsymbol \\alpha\\) parameter): \\(\\boldsymbol \\alpha_0\\) \\(\\rightarrow\\) \\(\\boldsymbol \\alpha_1 = \\boldsymbol \\alpha_0 + \\sum_{=1}^n \\boldsymbol x_i = \\boldsymbol \\alpha_0 + n \\hat{\\boldsymbol \\pi}_{ML}\\)","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"multivariate-normal-normal-model","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.6.3 Multivariate normal-normal model","text":"generalises univariate normal-normal model.multivariate normal distribution useful conjugate prior posterior distribution mean:Data: \\(D =\\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) \\(\\boldsymbol x_i \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) known mean \\(\\boldsymbol \\Sigma\\)Data: \\(D =\\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) \\(\\boldsymbol x_i \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) known mean \\(\\boldsymbol \\Sigma\\)MLE: \\(\\widehat{\\boldsymbol \\mu}_{ML} = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i\\)MLE: \\(\\widehat{\\boldsymbol \\mu}_{ML} = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i\\)Prior parameters: \\(k_0\\), \\(\\boldsymbol \\mu_0\\)\n\\[\\boldsymbol \\mu\\sim N_d\\left(\\boldsymbol \\mu_0, \\frac{\\boldsymbol \\Sigma}{k_0}\\right)\\]\n\\[\\text{E}(\\boldsymbol \\mu) = \\boldsymbol \\mu_0\\]Prior parameters: \\(k_0\\), \\(\\boldsymbol \\mu_0\\)\n\\[\\boldsymbol \\mu\\sim N_d\\left(\\boldsymbol \\mu_0, \\frac{\\boldsymbol \\Sigma}{k_0}\\right)\\]\n\\[\\text{E}(\\boldsymbol \\mu) = \\boldsymbol \\mu_0\\]Posterior parameters: \\(k_1 = k_0+n\\), \\(\\boldsymbol \\mu_1 = \\lambda \\boldsymbol \\mu_0 + (1-\\lambda) \\widehat{\\boldsymbol \\mu}_{ML}\\) \\(\\lambda = \\frac{k_0}{k_1}\\)\\[\\boldsymbol \\mu\\, |\\, D \\sim N_d\\left( \\boldsymbol \\mu_1, \\frac{\\boldsymbol \\Sigma}{k_1}  \\right)\\]\n\\[\\text{E}(\\boldsymbol \\mu\\, |\\, D) = \\boldsymbol \\mu_1\\]Posterior parameters: \\(k_1 = k_0+n\\), \\(\\boldsymbol \\mu_1 = \\lambda \\boldsymbol \\mu_0 + (1-\\lambda) \\widehat{\\boldsymbol \\mu}_{ML}\\) \\(\\lambda = \\frac{k_0}{k_1}\\)\\[\\boldsymbol \\mu\\, |\\, D \\sim N_d\\left( \\boldsymbol \\mu_1, \\frac{\\boldsymbol \\Sigma}{k_1}  \\right)\\]\n\\[\\text{E}(\\boldsymbol \\mu\\, |\\, D) = \\boldsymbol \\mu_1\\]","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"inverse-wishart-normal-model","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.6.4 Inverse Wishart-normal model","text":"generalises univariate inverse gamma-normal model variance.inverse Wishart distribution useful conjugate prior posterior distribution\ncovariance:Data: \\(D =\\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) \\(\\boldsymbol x_i \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) known mean \\(\\boldsymbol \\mu\\)Data: \\(D =\\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\}\\) \\(\\boldsymbol x_i \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) known mean \\(\\boldsymbol \\mu\\)MLE: \\(\\widehat{\\boldsymbol \\Sigma}_{ML} = \\frac{1}{n} \\sum_{=1}^n (\\boldsymbol x_i-\\boldsymbol \\mu)(\\boldsymbol x_i-\\boldsymbol \\mu)^T\\)MLE: \\(\\widehat{\\boldsymbol \\Sigma}_{ML} = \\frac{1}{n} \\sum_{=1}^n (\\boldsymbol x_i-\\boldsymbol \\mu)(\\boldsymbol x_i-\\boldsymbol \\mu)^T\\)Prior parameters: \\(\\kappa_0\\), \\(\\boldsymbol \\Sigma_0\\)\n\\[\\boldsymbol \\Sigma\\sim \\text{W}^{-1}_d\\left( \\kappa_0 \\boldsymbol \\Sigma_0 \\, , \\,  \\kappa_0+d+1\\right)\\]\n\\[\\text{E}(\\boldsymbol \\Sigma) = \\boldsymbol \\Sigma_0\\]Prior parameters: \\(\\kappa_0\\), \\(\\boldsymbol \\Sigma_0\\)\n\\[\\boldsymbol \\Sigma\\sim \\text{W}^{-1}_d\\left( \\kappa_0 \\boldsymbol \\Sigma_0 \\, , \\,  \\kappa_0+d+1\\right)\\]\n\\[\\text{E}(\\boldsymbol \\Sigma) = \\boldsymbol \\Sigma_0\\]Posterior parameters: \\(\\kappa_1 = \\kappa_0+n\\), \\(\\boldsymbol \\Sigma_1 = \\lambda \\boldsymbol \\Sigma_0 + (1-\\lambda) \\widehat{\\boldsymbol \\Sigma}_{ML}\\) \\(\\lambda = \\frac{\\kappa_0}{\\kappa_1}\\)\\[\\boldsymbol \\Sigma\\, |\\, D \\sim \\text{W}^{-1}_d\\left( \\kappa_1 \\boldsymbol \\Sigma_1 \\, , \\,  \\kappa_1+d+1\\right)\\]\n\\[\\text{E}(\\boldsymbol \\Sigma\\, |\\, D) = \\boldsymbol \\Sigma_1\\]Posterior parameters: \\(\\kappa_1 = \\kappa_0+n\\), \\(\\boldsymbol \\Sigma_1 = \\lambda \\boldsymbol \\Sigma_0 + (1-\\lambda) \\widehat{\\boldsymbol \\Sigma}_{ML}\\) \\(\\lambda = \\frac{\\kappa_0}{\\kappa_1}\\)\\[\\boldsymbol \\Sigma\\, |\\, D \\sim \\text{W}^{-1}_d\\left( \\kappa_1 \\boldsymbol \\Sigma_1 \\, , \\,  \\kappa_1+d+1\\right)\\]\n\\[\\text{E}(\\boldsymbol \\Sigma\\, |\\, D) = \\boldsymbol \\Sigma_1\\]Equivalent update rule:\n\\(\\nu_0\\) \\(\\rightarrow\\) \\(\\nu_1 = \\nu_0+n\\),\n\\(\\boldsymbol \\Psi_0\\) \\(\\rightarrow\\) \\(\\boldsymbol \\Psi_1 = \\boldsymbol \\Psi_0 + \\sum_{=1}^n (\\boldsymbol x_i-\\boldsymbol \\mu)(\\boldsymbol x_i-\\boldsymbol \\mu)^T = \\boldsymbol \\Psi_0 + n \\widehat{\\boldsymbol \\Sigma}_{ML}\\)Equivalent update rule:\n\\(\\nu_0\\) \\(\\rightarrow\\) \\(\\nu_1 = \\nu_0+n\\),\n\\(\\boldsymbol \\Psi_0\\) \\(\\rightarrow\\) \\(\\boldsymbol \\Psi_1 = \\boldsymbol \\Psi_0 + \\sum_{=1}^n (\\boldsymbol x_i-\\boldsymbol \\mu)(\\boldsymbol x_i-\\boldsymbol \\mu)^T = \\boldsymbol \\Psi_0 + n \\widehat{\\boldsymbol \\Sigma}_{ML}\\)","code":""},{"path":"multivariate-estimation-in-large-sample-and-small-sample-settings.html","id":"conclusion","chapter":"2 Multivariate estimation in large sample and small sample settings","heading":"2.7 Conclusion","text":"Multivariate models often high-dimensional \nlarge number parameters often small number samples available. instance useful (often necessary) introduce additional information (via priors regularisation).Unbiased estimation, highly valued property classical univariate statistics sample size large number parameters small, typically good idea multivariate settings often leads poor estimators.Regularisation introduces bias reduces variance, minimising overall MSE. Likewise, Bayesian estimators also introduce bias regularise (via prior) thus useful multivariate settings.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"transformations-and-dimension-reduction","chapter":"3 Transformations and dimension reduction","heading":"3 Transformations and dimension reduction","text":"Motivation:\nfollowing study transformations random vectors distributions.\ntransformation important\nsince either transform simple distributions complex distributions allow simplify\ncomplex models. machine learning invertible mappings transformations\nprobability distributions known “normalising flows”.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"linear-transformations","chapter":"3 Transformations and dimension reduction","heading":"3.1 Linear Transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"location-scale-transformation","chapter":"3 Transformations and dimension reduction","heading":"3.1.1 Location-scale transformation","text":"Also known affine transformation.\\[\\boldsymbol y= \\underbrace{\\boldsymbol }_{\\text{location parameter}}+\\underbrace{\\boldsymbol B}_{\\text{scale parameter}} \\boldsymbol x\\space\\]\n\\[\\boldsymbol y: m \\times 1 \\text{ random vector}\\]\n\\[\\boldsymbol : m \\times 1 \\text{ vector, location parameter}\\]\n\\[\\boldsymbol B: m \\times d \\text{ matrix, scale parameter },  m \\geq 1\\]\n\\[\\boldsymbol x: d \\times 1 \\text{ random vector}\\]Mean variance original vector \\(\\boldsymbol x\\):\\[\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\]\n\\[\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\]Mean variance transformed random vector \\(\\boldsymbol y\\):\\[\\text{E}(\\boldsymbol y)=\\boldsymbol + \\boldsymbol B\\boldsymbol \\mu_{\\boldsymbol x}\\]\n\\[\\text{Var}(\\boldsymbol y)= \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T\\]Cross-covariance \\(\\boldsymbol \\Phi= \\Sigma_{\\boldsymbol x\\boldsymbol y} = \\text{Cov}(\\boldsymbol x, \\boldsymbol y)\\) \\(\\boldsymbol x\\) \\(\\boldsymbol y\\):\n\\[\n\\boldsymbol \\Phi= \\text{Cov}(\\boldsymbol x, \\boldsymbol B\\boldsymbol x) = \\boldsymbol \\Sigma_{\\boldsymbol x}  \\boldsymbol B^T\n\\]\nNote \\(\\boldsymbol \\Phi\\) matrix dimensions \\(d \\times m\\)\ndimension \\(\\boldsymbol x\\) \\(d\\) dimension \\(\\boldsymbol y\\) \\(m\\).Cross-correlation \\(\\boldsymbol \\Psi= \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = \\text{Cor}(\\boldsymbol x, \\boldsymbol y)\\) \\(\\boldsymbol x\\) \\(\\boldsymbol y\\):\n\\[\n\\boldsymbol \\Psi= \\boldsymbol V_{\\boldsymbol x}^{-1/2}  \\boldsymbol \\Phi\\boldsymbol V_{\\boldsymbol y}^{-1/2}\n\\]\n\\(\\boldsymbol V_{\\boldsymbol x} = \\text{Diag}(\\boldsymbol \\Sigma_{\\boldsymbol x})\\) \\(\\boldsymbol V_{\\boldsymbol y} = \\text{Diag}(\\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T)\\)\ndiagonal matrices containing variances components\n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\). dimensions matrix \\(\\boldsymbol \\Psi\\) also \\(d \\times m\\).Special cases/examples:Example 3.1  Univariate case (\\(d=1, m=1\\)): \\(y=+ b x\\)\\(\\text{E}(y)=+b\\mu\\)\\(\\text{Var}(y)=b^2\\sigma^2\\)\\(\\text{Cov}(y, x) = b\\sigma^2\\)\\(\\text{Cor}(y, x) = \\frac{b \\sigma^2}{\\sqrt{b^2\\sigma^2} \\sqrt{\\sigma^2} } =1\\)Note \\(y\\) can predicted perfectly \\(x\\) \\(\\text{Cor}(y, x)=1\\). \nerror term transformation. See also general case\nmultiple correlation .Example 3.2  Sum two random univariate variables:\n\\(y = x_1 + x_2\\), .e. \\(=0\\) \\(\\boldsymbol B=(1,1)\\)\\(\\text{E}(y) = \\text{E}(x_1+x_2)=\\mu_1+\\mu_2\\)\\(\\text{Var}(y) = \\text{Var}(x_1+x_2) = (1,1)\\begin{pmatrix} \\sigma^2_1 & \\sigma_{12}\\\\ \\sigma_{12} & \\sigma^2_2 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix} = \\sigma^2_1+\\sigma^2_2+2\\sigma_{12} = \\text{Var}(x_1)+\\text{Var}(x_2)+2\\,\\text{Cov}(x_1,x_2)\\)Example 3.3  \\(y_1=a_1+b_1 x_1\\) \\(y_2=a_2+b_2 x_2\\),\n.e. \\(\\boldsymbol = \\begin{pmatrix} a_1\\\\ a_2 \\end{pmatrix}\\) \\(\\boldsymbol B= \\begin{pmatrix}b_1 & 0\\\\ 0 & b_2\\end{pmatrix}\\)\\(\\text{E}(\\boldsymbol y)= \\begin{pmatrix} a_1\\\\ a_2 \\end{pmatrix} + \\begin{pmatrix}b_1 & 0\\\\ 0 & b_2\\end{pmatrix}  \\begin{pmatrix} \\mu_1 \\\\ \\mu_2\\end{pmatrix}  = \\begin{pmatrix} a_1+b_1 \\mu_1\\\\ a_2+b_2 \\mu_2 \\end{pmatrix}\\)\\(\\text{Var}(\\boldsymbol y) = \\begin{pmatrix} b_1 & 0\\\\ 0 & b_2 \\end{pmatrix}  \\begin{pmatrix} \\sigma^2_1 & \\sigma_{12}\\\\ \\sigma_{12} & \\sigma^2_2 \\end{pmatrix} \\begin{pmatrix} b_1 & 0\\\\ 0 & b_2 \\end{pmatrix} = \\begin{pmatrix} b^2_1\\sigma^2_1 & b_1b_2\\sigma_{12}\\\\ b_1b_2\\sigma_{12} & b^2_2\\sigma^2_2 \\end{pmatrix}\\)\nnote \\(\\text{Cov}(y_1, y_2) = b_1 b_2\\text{Cov}(x_1,x_2)\\)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"squared-multiple-correlation","chapter":"3 Transformations and dimension reduction","heading":"3.1.2 Squared multiple correlation","text":"Squared multiple correlation \\(\\text{MCor}(y, \\boldsymbol x)^2\\) scalar measure summarising linear association scalar response variable \\(y\\) set predictors \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\). defined \n\\[\n\\begin{split}\n\\text{MCor}(y, \\boldsymbol x)^2 &= \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} / \\sigma^2_y\\\\\n&=\\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{ \\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\\\\\n\\end{split}\n\\]\n\\(y\\) can perfectly linearly predicted \\(\\boldsymbol x\\) \\(\\text{MCor}(y, \\boldsymbol x)^2 = 1\\).empirical estimate \\(\\text{MCor}(y, \\boldsymbol x)^2\\) \\(R^2\\) coefficient find software linear regression.Example 3.4  Squared multiple correlation affine transformation.Since linearly transform \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) additional error involved expect\ncomponent \\(y_i\\) \\(\\boldsymbol y\\) \\(\\text{MCor}(y_i, \\boldsymbol x)^2=1\\).\ncan shown directly computing\n\\[\n\\begin{split}\n\\left(\\text{MCor}(y_1, \\boldsymbol x)^2, \\ldots, \\text{MCor}(y_m, \\boldsymbol x)^2 \\right)^T\n&=\\text{Diag}\\left(\\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}  \\right) / \\text{Diag}\\left( \\boldsymbol \\Sigma_{\\boldsymbol y} \\right) \\\\\n&= \\text{Diag}\\left(\\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) / \\text{Diag}\\left( \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) \\\\\n&= \\text{Diag}\\left(\\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) / \\text{Diag}\\left( \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) \\\\\n&=\\left(1, \\ldots, 1 \\right)^T\\\\\n\\end{split}\n\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"invertible-location-scale-transformation","chapter":"3 Transformations and dimension reduction","heading":"3.1.3 Invertible location-scale transformation","text":"\\(m=d\\) (square \\(\\boldsymbol B\\)) \\(\\det(\\boldsymbol B) \\neq 0\\) affine transformation invertible.Forward transformation:\n\\[\\boldsymbol y= \\boldsymbol + \\boldsymbol B\\boldsymbol x\\]Back transformation:\n\\[\\boldsymbol x= \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\]Invertible transformations thus provide one--one map \\(\\boldsymbol x\\) \\(\\boldsymbol y\\).Example 3.5  Mahalanobis transformWe assume \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\) positive definite\ncovariance matrix \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\) \\(\\det(\\boldsymbol \\Sigma_{\\boldsymbol x}) > 0\\).Mahalanobis transformation given \n\\[\n\\boldsymbol y=\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x}(\\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x})\n\\]\ncorresponds affine transformation \n\\(\\boldsymbol = - \\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x} \\boldsymbol \\mu_{\\boldsymbol x}\\) \\(\\boldsymbol B= \\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x}\\).Note transformation contains inverse principal matrix square\nroot \\(\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x}\\). can obtained\neigendecomposition \\(\\boldsymbol \\Sigma_{\\boldsymbol x} = \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) \n\\(\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol x} =\\boldsymbol U\\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T\\).mean variance \\(\\boldsymbol y\\) becomes\n\\[\n\\text{E}(\\boldsymbol y) = \\boldsymbol 0\\]\n\n\\[\\text{Var}(\\boldsymbol y) = \\boldsymbol I_d\\].Mahalanobis transforms performs three functions:Centering (\\(-\\boldsymbol \\mu\\))Standardisation \\(\\text{Var}(y_i)=1\\)Decorrelation \\(\\text{Cor}(y_i,y_j)=0\\) \\(\\neq j\\)univariate case (\\(d=1\\)) coefficients reduce \n\\(= - \\frac{\\mu_x}{\\sigma_x}\\) \\(B = \\frac{1}{\\sigma_x}\\) Mahalanobis transform\nbecomes\n\\[y = \\frac{x-\\mu_x}{\\sigma_x}\\]\n.e. applies centering + standardisation.Mahalanobis transformation appears implicitly many places multivariate statistics,\ne.g. multivariate normal density. particular example whitening transformation (\ninfinitely many, see later course).Example 3.6  Inverse Mahalanobis transformationThe inverse Mahalanobis transform given \n\\[\n\\boldsymbol y= \\boldsymbol \\mu_{\\boldsymbol y}+\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y} \\boldsymbol x\n\\]\nMahalanobis transform whitening transform inverse Mahalonobis\ntransform sometimes called Mahalanobis colouring transformation.\ncoefficients affine transformation \n\\(\\boldsymbol =\\boldsymbol \\mu_{\\boldsymbol y}\\) \\(\\boldsymbol B=\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y}\\).Starting \\(\\text{E}(\\boldsymbol x)=\\boldsymbol 0\\) \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol I_d\\) \nmean variance transformed variable \n\\[\\text{E}(\\boldsymbol y) = \\boldsymbol \\mu_{\\boldsymbol y}\n\\]\n\n\\[\\text{Var}(\\boldsymbol y) = \\boldsymbol \\Sigma_{\\boldsymbol y}\n\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"transformation-of-a-density-under-an-invertible-location-scale-transformation","chapter":"3 Transformations and dimension reduction","heading":"3.1.4 Transformation of a density under an invertible location-scale transformation:","text":"Assume \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) density \\(f_{\\boldsymbol x}(\\boldsymbol x)\\).linear transformation \\(\\boldsymbol y= \\boldsymbol + \\boldsymbol B\\boldsymbol x\\) get \\(\\boldsymbol y\\sim F_{\\boldsymbol y}\\) density\n\\[f_{\\boldsymbol y}(\\boldsymbol y)=|\\det(\\boldsymbol B)|^{-1} f_{\\boldsymbol x} \\left( \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\right)\\]Example 3.7  Transformation standard normal inverse Mahalanobis transformAssume \\(\\boldsymbol x\\) multivariate standard normal \\(\\boldsymbol x\\sim N_d(\\boldsymbol 0,\\boldsymbol I_d)\\) density\n\\[f_{\\boldsymbol x}(\\boldsymbol x) = (2\\pi)^{-d/2}\\exp\\left( -\\frac{1}{2} \\boldsymbol x^T \\boldsymbol x\\right)\\]\ndensity applying inverse Mahalanobis transform\\(\\boldsymbol y= \\boldsymbol \\mu_{\\boldsymbol y}+\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y} \\boldsymbol x\\) \n\\[\n\\begin{split}\nf_{\\boldsymbol y}(\\boldsymbol y) &= |\\det(\\boldsymbol \\Sigma^{1/2}_{\\boldsymbol y})|^{-1} (2\\pi)^{-d/2} \\exp\\left(-\\frac{1}{2}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})^T\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol y} \\,\\boldsymbol \\Sigma^{-1/2}_{\\boldsymbol y}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})\\right)\\\\\n& = (2\\pi)^{-d/2} \\det(\\boldsymbol \\Sigma_{\\boldsymbol y})^{-1/2} \\exp\\left(-\\frac{1}{2}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})^T\\boldsymbol \\Sigma^{-1}_{\\boldsymbol y}(\\boldsymbol y-\\boldsymbol \\mu_{\\boldsymbol y})\\right) \\\\\n\\end{split}\n\\]\n\\(\\Longrightarrow\\) \\(\\boldsymbol y\\) multivariate normal density \\(N_d(\\boldsymbol \\mu_{\\boldsymbol y}, \\boldsymbol \\Sigma_{\\boldsymbol y})\\)Application: e.g. random number generation: draw \\(N_d(\\boldsymbol 0,\\boldsymbol I_d)\\) (easy!) convert multivariate normal tranformation\n(see Worksheet 4).","code":""},{"path":"transformations-and-dimension-reduction.html","id":"nonlinear-transformations","chapter":"3 Transformations and dimension reduction","heading":"3.2 Nonlinear transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"general-transformation","chapter":"3 Transformations and dimension reduction","heading":"3.2.1 General transformation","text":"\\[\\boldsymbol y= \\boldsymbol h(\\boldsymbol x)\\]\n\\(\\boldsymbol h\\) arbitrary vector-valued functionlinear case: \\(\\boldsymbol h(\\boldsymbol x) = \\boldsymbol +\\boldsymbol B\\boldsymbol x\\)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"delta-method","chapter":"3 Transformations and dimension reduction","heading":"3.2.2 Delta method","text":"Assume know mean \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu_{\\boldsymbol x}\\) variance \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma_{\\boldsymbol x}\\) \\(\\boldsymbol x\\).\npossible say something mean variance transformed\nrandom variable \\(\\boldsymbol y\\)?\n\\[\n\\text{E}(\\boldsymbol y)= \\text{E}(\\boldsymbol h(\\boldsymbol x))= ?\n\\]\n\\[\n\\text{Var}(\\boldsymbol y) = \\text{Var}(\\boldsymbol h(\\boldsymbol x))= ? \\\\\n\\]general, transformation \\(\\boldsymbol h(\\boldsymbol x)\\) exact mean variance transformed variable obtained analytically.However, can find linear approximation compute mean variance.\napproximation called “Delta Method”, “law propagation errors”, credited Gauss 4.Linearisation \\(\\boldsymbol h(\\boldsymbol x)\\) achieved Taylor series approximation first order\n\\(\\boldsymbol h(\\boldsymbol x)\\) around \\(\\boldsymbol x_0\\):\n\\[\\boldsymbol h(\\boldsymbol x) \\approx \\boldsymbol h(\\boldsymbol x_0) + \\underbrace{J_{\\boldsymbol h}(\\boldsymbol x_0)}_{\\text{Jacobian matrix}}(\\boldsymbol x-\\boldsymbol x_0)  =\n\\underbrace{\\boldsymbol h(\\boldsymbol x_0) - J_{\\boldsymbol h}(\\boldsymbol x_0)\\, \\boldsymbol x_0}_{\\boldsymbol } + \\underbrace{J_{\\boldsymbol h}(\\boldsymbol x_0)}_{\\boldsymbol B} \\boldsymbol x\\]\\(h(\\boldsymbol x)\\) scalar-valued gradient \\(\\nabla h(\\boldsymbol x)\\) given vector partial correlations\n\\[\n\\nabla h(\\boldsymbol x) =\n\\begin{pmatrix}\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_1}  \\\\\n\\vdots\\\\\n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_d} \\\\\n\\end{pmatrix}\n\\]\n\\(\\nabla\\) nabla operator.Jacobian matrix generalisation gradient \\(\\boldsymbol h(\\boldsymbol x)\\) vector-valued:\\[J_{\\boldsymbol h}(\\boldsymbol x) = \\begin{pmatrix}\\nabla h_1(\\boldsymbol x)^T\\\\ \\nabla h_2(\\boldsymbol x)^T \\\\ \\vdots \\\\ \\nabla h_m(\\boldsymbol x)^T \\end{pmatrix} = \\begin{pmatrix}\n    \\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_1} & \\dots & \\frac{\\partial h_1(\\boldsymbol x)}{\\partial x_d}\\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_1} & \\dots & \\frac{\\partial h_m(\\boldsymbol x)}{\\partial x_d}\n    \\end{pmatrix}\\]\nNote Jacobian matrix convention gradient individual component \\(\\boldsymbol h(\\boldsymbol x)\\) contained row matrix number rows corresponds dimension \\(\\boldsymbol h(\\boldsymbol x)\\) number columns dimension \\(\\boldsymbol x\\).First order approximation \\(\\boldsymbol h(\\boldsymbol x)\\) around \\(\\boldsymbol x_0=\\boldsymbol \\mu_{\\boldsymbol x}\\) yields\n\\(\\boldsymbol = \\boldsymbol h(\\boldsymbol \\mu_{\\boldsymbol x}) - J_{\\boldsymbol h}(\\boldsymbol \\mu_{\\boldsymbol x})\\, \\boldsymbol \\mu_{\\boldsymbol x}\\) \n\\(\\boldsymbol B= J_{\\boldsymbol h}(\\boldsymbol \\mu_{\\boldsymbol x})\\) leads directly multivariate Delta method:\\[\\text{E}(\\boldsymbol y)\\approx\\boldsymbol h(\\boldsymbol \\mu_{\\boldsymbol x})\\]\n\\[\\text{Var}(\\boldsymbol y)\\approx J_{\\boldsymbol h}(\\boldsymbol \\mu_{\\boldsymbol x}) \\, \\boldsymbol \\Sigma_{\\boldsymbol x} \\, J_{\\boldsymbol h}(\\boldsymbol \\mu_{\\boldsymbol x})^T\\]univariate Delta method special case:\n\\[\\text{E}(y) \\approx h(\\mu_x)\\]\n\\[\\text{Var}(y)\\approx \\sigma^2_x \\, h'(\\mu_x)^2\\]Note Delta approximation breaks \\(\\text{Var}(\\boldsymbol y)\\) singular,\nexample first derivative (gradient Jacobian matrix) \\(\\boldsymbol \\mu_{\\boldsymbol x}\\) zero.Example 3.8  Variance odds ratioThe proportion \\(\\hat{p} = \\frac{n_1}{n}\\) resulting \n\\(n\\) repeats Bernoulli experiment expectation \\(\\text{E}(\\hat{p})=p\\)\nvariance \\(\\text{Var}(\\hat{p}) = \\frac{p (1-p)}{n}\\).\n(approximate) mean variance corresponding odds ratio \\(\\widehat{}=\\frac{\\hat{p}}{1-\\hat{p}}\\)?\\(h(x) = \\frac{x}{1-x}\\),\n\\(\\widehat{} = h(\\hat{p})\\) \\(h'(x) = \\frac{1}{(1-x)^2}\\) get using \nDelta method\n\\(\\text{E}( \\widehat{} ) \\approx h(p) = \\frac{p}{1-p}\\) \n\\(\\text{Var}( \\widehat{} )\\approx h'(p)^2 \\text{Var}( \\hat{p} ) = \\frac{p}{n (1-p)^3}\\).Example 3.9  Log-transform variance stabilisationAssume \\(x\\) mean \\(\\text{E}(x)=\\mu\\) variance \\(\\text{Var}(x) = \\sigma^2 \\mu^2\\),\n.e. standard deviation \\(\\text{SD}(x)\\) proportional mean \\(\\mu\\).\n(approximate) mean variance log-transformed variable \\(\\log(x)\\)?\\(h(x) = \\log(x)\\) \\(h'(x) = \\frac{1}{x}\\) get using \nDelta method\n\\(\\text{E}( \\log(x) ) \\approx h(\\mu) = \\log(\\mu)\\) \n\\(\\text{Var}( \\log(x) )\\approx h'(\\mu)^2 \\text{Var}( x ) = \\left(\\frac{1}{\\mu} \\right)^2 \\sigma^2 \\mu^2 = \\sigma^2\\). Thus, applying log-transform variance depend mean!","code":""},{"path":"transformations-and-dimension-reduction.html","id":"transformation-of-a-probability-density-function-under-a-general-invertible-transformation","chapter":"3 Transformations and dimension reduction","heading":"3.2.3 Transformation of a probability density function under a general invertible transformation","text":"Assume \\(\\boldsymbol h(\\boldsymbol x) = \\boldsymbol y(\\boldsymbol x)\\) invertible: \\(\\boldsymbol h^{-1}(\\boldsymbol y)=\\boldsymbol x(\\boldsymbol y)\\)\\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) probability density function \\(f_{\\boldsymbol x}(\\boldsymbol x)\\)density \\(f_{\\boldsymbol y}(\\boldsymbol y)\\) transformed random vector \\(\\boldsymbol y\\) given \\[f_{\\boldsymbol y}(\\boldsymbol y) = |\\det\\left( J_{\\boldsymbol x}(\\boldsymbol y) \\right)| \\,\\,\\,  f_{\\boldsymbol x}\\left( \\boldsymbol x(\\boldsymbol y) \\right)\\]\\(J_{\\boldsymbol x}(\\boldsymbol y)\\) Jacobian matrix inverse transformation.Special cases:Univariate version: \\(f_y(y) = \\left|\\frac{dx(y)}{dy} \\right| \\, f_x\\left(x(y)\\right)\\)Linear transformation \\(\\boldsymbol h(\\boldsymbol x) = \\boldsymbol + \\boldsymbol B\\boldsymbol x\\), \\(\\boldsymbol x(\\boldsymbol y) = \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\)\n\\(J_{\\boldsymbol x}(\\boldsymbol y) = \\boldsymbol B^{-1}\\):\n\\[f_{\\boldsymbol y}(\\boldsymbol y)=\\left|\\det(\\boldsymbol B)\\right|^{-1} f_{\\boldsymbol x} \\left( \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\right)\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"normalising-flows","chapter":"3 Transformations and dimension reduction","heading":"3.2.4 Normalising flows","text":"module focus mostly linear transformations underpin\nmuch classical multivariate statistics, important keep mind later study\nimportance nonlinear transformationsIn machine learning (sequences ) invertible nonlinear transformations known “normalising flows”. used generative way (building complex models \nsimple models) simplification dimension reduction.interested normalising flows good start learn review papers\nKobyzev et al (2021 )5 Papamakarios et al. (2021) 6.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"general-whitening-transformations","chapter":"3 Transformations and dimension reduction","heading":"3.3 General whitening transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"overview-1","chapter":"3 Transformations and dimension reduction","heading":"3.3.1 Overview","text":"Whitening transformations special widely used class invertible location-scale\ntransformations.Terminology: whitening refers fact transformation covariance matrix spherical, isotropic, white (\\(\\boldsymbol I_d\\))Whitening useful preprocessing, allow turn multivariate models uncorrelated univariate models (via decorrelation property). whitening transformations reduce dimension optimal way (via compression property).Mahalanobis transform specific example whitening transformation.\nalso know “zero-phase component analysis” short ZCA transform.-called latent variable models whitening procedures link observed (correlated) variables latent variables (typically uncorrelated standardised):\\[\\begin{align*}\n\\begin{array}{cl}\n\\text{Whitening} \\\\\n\\downarrow\n\\end{array}\n\\begin{array}{ll}\n\\boldsymbol x\\\\\n\\uparrow \\\\\n\\boldsymbol z\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{Observed variable (can measured)} \\\\\n\\text{external, typically correlated} \\\\\n\\space \\\\\n\\text{Unobserved \"latent\" variable (directly measured)} \\\\\n\\text{internal, typically chosen uncorrelated} \\\\\n\\end{array}\n\\end{align*}\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"whitening-transformation-and-whitening-constraint","chapter":"3 Transformations and dimension reduction","heading":"3.3.2 Whitening transformation and whitening constraint","text":"Starting point:Random vector \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) necessarily multivariate normal.\\(\\boldsymbol x\\) mean \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu\\) positive definite (invertible) covariance matrix \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\).Note following leave subscript \\(\\boldsymbol x\\) covariance \\(\\boldsymbol x\\)\nunless needed clarification.covariance can split positive variances \\(\\boldsymbol V\\) \npositive definite invertible correlation matrix \\(\\boldsymbol P\\) \\(\\boldsymbol \\Sigma= \\boldsymbol V^{1/2} \\boldsymbol P\\boldsymbol V^{1/2}\\).Whitening transformation:\\[\\underbrace{\\boldsymbol z}_{d \\times 1 \\text{ vector }} = \\underbrace{\\boldsymbol W}_{d \\times d \\text{ whitening matrix }} \\underbrace{\\boldsymbol x}_{d \\times 1 \\text{ vector }}\\]\nObjective: choose \\(\\boldsymbol W\\) \\(\\text{Var}(\\boldsymbol z)=\\boldsymbol I_d\\)Mahalanobis/ZCA whitening already know Example 3.5 \\(\\boldsymbol W^{\\text{ZCA}}=\\boldsymbol \\Sigma^{-1/2}\\).general, whitening matrix \\(\\boldsymbol W\\) needs satisfy constraint:\n\\[\n\\begin{array}{lll}\n                & \\text{Var}(\\boldsymbol z) & = \\boldsymbol I_d \\\\\n\\Longrightarrow & \\text{Var}(\\boldsymbol W\\boldsymbol x) &= \\boldsymbol W\\boldsymbol \\Sigma\\boldsymbol W^T = \\boldsymbol I_d \\\\\n\\Longrightarrow &  \\boldsymbol W\\, \\boldsymbol \\Sigma\\, \\boldsymbol W^T \\boldsymbol W= \\boldsymbol W& \\\\\n\\end{array}\n\\]\n\\[\\Longrightarrow \\text{constraint whitening matrix: } \\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1}\\]Clearly, ZCA whitening matrix satisfies constraint: \\((\\boldsymbol W^{ZCA})^T \\boldsymbol W^{ZCA} = \\boldsymbol \\Sigma^{-1/2}\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1}\\)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"parameterisation-of-whitening-matrix","chapter":"3 Transformations and dimension reduction","heading":"3.3.3 Parameterisation of whitening matrix","text":"Covariance-based parameterisation whitening matrix:general way specify valid whitening matrix \n\\[\n\\boldsymbol W= \\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2}\n\\]\n\\(\\boldsymbol Q_1\\) orthogonal matrix.Recall orthogonal matrix \\(\\boldsymbol Q\\) property \\(\\boldsymbol Q^{-1} = \\boldsymbol Q^T\\) \nconsequence \\(\\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol \\).result, \\(\\boldsymbol W\\) satisfies whitening constraint:\\[\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1/2}\\underbrace{\\boldsymbol Q_1^T \\boldsymbol Q_1}_{\\boldsymbol }\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1}\\]Note converse also true: whitening whitening matrix, .e. \\(\\boldsymbol W\\) satisfying whitening constraint, can written form \n\\(\\boldsymbol Q_1 = \\boldsymbol W\\boldsymbol \\Sigma^{1/2}\\) orthogonal construction.\\(\\Longrightarrow\\) instead choosing \\(\\boldsymbol W\\), choose orthogonal matrix \\(\\boldsymbol Q_1\\)!recall orthogonal matrices geometrically represent rotations (plus reflections).now clear infinitely many whitening procedures, infinitely many rotations! also means need find ways choose/select among whitening procedures.Mahalanobis/ZCA transformation \\(\\boldsymbol Q_1^{\\text{ZCA}}=\\boldsymbol \\)whitening can interpreted Mahalanobis transformation followed rotation-reflectionCorrelation-based parameterisation whitening matrix:Instead working covariance matrix \\(\\boldsymbol \\Sigma\\), can express \\(\\boldsymbol W\\) also terms corresponding correlation matrix \\(\\boldsymbol P= (\\rho_{ij}) = \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2}\\)\n\\(\\boldsymbol V\\) diagonal matrix containing variances.Specifically, can specify whitening matrix \n\\[\\boldsymbol W= \\boldsymbol Q_2 \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2}\\]easy verify \\(\\boldsymbol W\\) also satisfies whitening constraint:\n\\[\n\\begin{split}\n\\boldsymbol W^T \\boldsymbol W& = \\boldsymbol V^{-1/2}\\boldsymbol P^{-1/2}\\underbrace{\\boldsymbol Q_2^T \\boldsymbol Q_2}_{\\boldsymbol }\\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} \\\\\n& = \\boldsymbol V^{-1/2} \\boldsymbol P^{-1} \\boldsymbol V^{-1/2} = \\boldsymbol \\Sigma^{-1} \\\\\n\\end{split}\n\\]\nConversely, whitening matrix \\(\\boldsymbol W\\) can also written form \n\\(\\boldsymbol Q_2 = \\boldsymbol W\\boldsymbol V^{1/2} \\boldsymbol P^{1/2}\\) orthogonal construction.Another interpretation whitening: first standardising (\\(\\boldsymbol V^{-1/2}\\)), decorrelation (\\(\\boldsymbol P^{-1/2}\\)), followed rotation-reflection (\\(\\boldsymbol Q_2\\))Mahalanobis/ZCA transformation \\(\\boldsymbol Q_2^{\\text{ZCA}} = \\boldsymbol \\Sigma^{-1/2} \\boldsymbol V^{1/2} \\boldsymbol P^{1/2}\\)forms write \\(\\boldsymbol W\\) using \\(\\boldsymbol Q_1\\) \\(\\boldsymbol Q_2\\) equally valid (interchangeable).Note \\(\\boldsymbol W\\)\n\\[\\boldsymbol Q_1\\neq\\boldsymbol Q_2 \\text{  Two different orthogonal matrices!}\\]\nalso\n\\[\\underbrace{\\boldsymbol \\Sigma^{-1/2}}_{\\text{Symmetric}}\\neq\\underbrace{\\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}}_{\\text{Symmetric}}\\]\neven though\\[\\boldsymbol \\Sigma^{-1/2}\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1} = \\boldsymbol V^{-1/2}\\boldsymbol P^{-1/2}\\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"cross-covariance-and-cross-correlation-for-general-whitening-transformations","chapter":"3 Transformations and dimension reduction","heading":"3.3.4 Cross-covariance and cross-correlation for general whitening transformations","text":"useful criterion characterise distinguish among whitening transformations \ncross-covariance cross-correlation matrix \noriginal variable \\(\\boldsymbol x\\) whitened variable \\(\\boldsymbol z\\):Cross-covariance \\(\\boldsymbol \\Phi= \\Sigma_{\\boldsymbol x\\boldsymbol z}\\) \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Phi= \\text{Cov}(\\boldsymbol x, \\boldsymbol z) & = \\text{Cov}( \\boldsymbol x,\\boldsymbol W\\boldsymbol x)\\\\\n& = \\boldsymbol \\Sigma\\boldsymbol W^T \\\\\n&= \\boldsymbol \\Sigma\\, \\boldsymbol \\Sigma^{-1/2} \\boldsymbol Q_1^T \\\\\n&= \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T\\\\\n\\end{split}\n\\]\ncomponent notation write \\(\\boldsymbol \\Phi= (\\phi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol x\\) column index \\(j\\) \\(\\boldsymbol z\\).\nCross-covariance linked \\(\\boldsymbol Q_1\\)!\nThus, choosing cross-covariance determines \\(\\boldsymbol Q_1\\) (vice versa).\nNote cross-covariance matrix \\(\\boldsymbol \\Phi\\) satisfies condition\n\\(\\boldsymbol \\Phi\\boldsymbol \\Phi^T = \\boldsymbol \\Sigma\\).\nwhitening matrix expressed terms cross-covariance \\(\\boldsymbol W= \\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1}\\), required \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Phi\\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1} =\\boldsymbol \\Sigma^{-1}\\).\nFurthermore, \\(\\boldsymbol \\Phi\\) \ninverse whitening matrix,\n\\(\\boldsymbol W^{-1} = \\left( \\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2} \\right)^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{T} = \\boldsymbol \\Phi\\).Cross-covariance \\(\\boldsymbol \\Phi= \\Sigma_{\\boldsymbol x\\boldsymbol z}\\) \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Phi= \\text{Cov}(\\boldsymbol x, \\boldsymbol z) & = \\text{Cov}( \\boldsymbol x,\\boldsymbol W\\boldsymbol x)\\\\\n& = \\boldsymbol \\Sigma\\boldsymbol W^T \\\\\n&= \\boldsymbol \\Sigma\\, \\boldsymbol \\Sigma^{-1/2} \\boldsymbol Q_1^T \\\\\n&= \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T\\\\\n\\end{split}\n\\]\ncomponent notation write \\(\\boldsymbol \\Phi= (\\phi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol x\\) column index \\(j\\) \\(\\boldsymbol z\\).Cross-covariance linked \\(\\boldsymbol Q_1\\)!\nThus, choosing cross-covariance determines \\(\\boldsymbol Q_1\\) (vice versa).Note cross-covariance matrix \\(\\boldsymbol \\Phi\\) satisfies condition\n\\(\\boldsymbol \\Phi\\boldsymbol \\Phi^T = \\boldsymbol \\Sigma\\).whitening matrix expressed terms cross-covariance \\(\\boldsymbol W= \\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1}\\), required \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Phi\\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1} =\\boldsymbol \\Sigma^{-1}\\).\nFurthermore, \\(\\boldsymbol \\Phi\\) \ninverse whitening matrix,\n\\(\\boldsymbol W^{-1} = \\left( \\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2} \\right)^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{T} = \\boldsymbol \\Phi\\).Cross-correlation \\(\\boldsymbol \\Psi= \\boldsymbol P_{\\boldsymbol x\\boldsymbol z}\\) \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Psi= \\text{Cor}(\\boldsymbol x, \\boldsymbol z) & = \\boldsymbol V^{-1/2} \\boldsymbol \\Phi\\\\\n& =  \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol W^T \\\\\n&=\\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2} \\boldsymbol P^{-1/2} \\boldsymbol Q_2^T\\\\\n& =  \\boldsymbol P^{1/2}  \\boldsymbol Q_2^T\\\\\n\\end{split}\n\\]\ncomponent notation write \\(\\boldsymbol \\Psi= (\\psi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol x\\) column index \\(j\\) \\(\\boldsymbol z\\).\nCross-correlation linked \\(\\boldsymbol Q_2\\)!\nHence, choosing cross-correlation determines \\(\\boldsymbol Q_2\\) (vice versa). whitening\nmatrix expressed terms cross-correlation \n\\(\\boldsymbol W= \\boldsymbol \\Psi^T \\boldsymbol P^{-1} \\boldsymbol V^{-1/2}\\).Cross-correlation \\(\\boldsymbol \\Psi= \\boldsymbol P_{\\boldsymbol x\\boldsymbol z}\\) \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Psi= \\text{Cor}(\\boldsymbol x, \\boldsymbol z) & = \\boldsymbol V^{-1/2} \\boldsymbol \\Phi\\\\\n& =  \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol W^T \\\\\n&=\\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2} \\boldsymbol P^{-1/2} \\boldsymbol Q_2^T\\\\\n& =  \\boldsymbol P^{1/2}  \\boldsymbol Q_2^T\\\\\n\\end{split}\n\\]component notation write \\(\\boldsymbol \\Psi= (\\psi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol x\\) column index \\(j\\) \\(\\boldsymbol z\\).Cross-correlation linked \\(\\boldsymbol Q_2\\)!\nHence, choosing cross-correlation determines \\(\\boldsymbol Q_2\\) (vice versa). whitening\nmatrix expressed terms cross-correlation \n\\(\\boldsymbol W= \\boldsymbol \\Psi^T \\boldsymbol P^{-1} \\boldsymbol V^{-1/2}\\).Note factorisation cross-covariance \\(\\boldsymbol \\Phi=\\boldsymbol \\Sigma^{1/2}\\boldsymbol Q_1^T\\) \ncross-correlation \\(\\boldsymbol \\Psi=\\boldsymbol P^{1/2}\\boldsymbol Q_2^T\\) product \npositive definite symmetric matrix orthogonal matrix examples polar decomposition.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"inverse-whitening-transformation-and-loadings","chapter":"3 Transformations and dimension reduction","heading":"3.3.5 Inverse whitening transformation and loadings","text":"Inverse transformation:Recall \\(\\boldsymbol z= \\boldsymbol W\\boldsymbol x\\). Therefore, reverse transformation going whitened\noriginal variable \\(\\boldsymbol x= \\boldsymbol W^{-1} \\boldsymbol z\\).\ncan expressed also terms cross-covariance cross-correlation.\n\\(\\boldsymbol W^{-1} = \\boldsymbol \\Phi\\) get\n\\[\n\\boldsymbol x= \\boldsymbol \\Phi\\boldsymbol z\\, .\n\\]\nFurthermore, since \\(\\boldsymbol \\Psi= \\boldsymbol V^{-1/2} \\boldsymbol \\Phi\\) \n\\(\\boldsymbol W^{-1} = \\boldsymbol V^{1/2} \\boldsymbol \\Psi\\) hence\n\\[\n\\boldsymbol V^{-1/2} \\boldsymbol x=   \\boldsymbol \\Psi\\boldsymbol z\\, .\n\\]reverse whitening transformation also known colouring transformation\n(previously discussed inverse Mahalanobis transform one example).Definition loadings:Loadings coefficients linear transformation latent variable back observed variable. variables standardised unit variance loadings also called correlation loadings.Hence, cross-covariance matrix \\(\\boldsymbol \\Phi\\) plays role loadings linking latent variable \\(\\boldsymbol z\\)\noriginal \\(\\boldsymbol x\\). Similarly, cross-correlation matrix \\(\\boldsymbol \\Psi\\) contains correlation loadings\nlinking (already standardised) latent variable \\(\\boldsymbol z\\) standardised \\(\\boldsymbol x\\).convention use rows correspond original variables\ncolumns whitened latent variables.Multiple correlation coefficients \\(\\boldsymbol z\\) back \\(\\boldsymbol x\\):consider backtransformation whitened variable \\(\\boldsymbol z\\) original variables \\(\\boldsymbol x\\) note components \\(\\boldsymbol z\\) uncorrelated witth \\(\\boldsymbol P_{\\boldsymbol z} = \\boldsymbol \\).\nsquared multiple correlation coefficient \\(\\text{MCor}(x_i, \\boldsymbol z)\\) \\(x_i\\) \\(\\boldsymbol z\\)\ntherefore just sum corresponding squared correlations\n\\(\\text{Cor}(x_i, z_j)^2\\):\n\\[\n\\begin{split}\n\\text{MCor}(x_i, \\boldsymbol z)^2 &=  \\boldsymbol P_{x_i \\boldsymbol z} \\boldsymbol P_{\\boldsymbol z}^{-1} \\boldsymbol P_{\\boldsymbol zx_i} = \\\\\n             & \\sum_{j=1}^d  \\text{Cor}(x_i, z_j)^2  \\\\\n&  = \\sum_{j=1}^d \\psi_{ij}^2 = 1\n\\end{split}\n\\]\nshown earlier general linear one--one- transformation (includes whitening special case) squared multiple correlation must 1 \nerror. can confirm computing row sums squares cross-correlation matrix \\(\\boldsymbol \\Psi\\) matrix notation\n\\[\n\\begin{split}\n\\text{Diag}\\left(\\boldsymbol \\Psi\\boldsymbol \\Psi^T\\right) &= \\text{Diag}\\left(\\boldsymbol P^{1/2} \\boldsymbol Q_2^T \\boldsymbol Q_2\\boldsymbol P^{1/2}\\right) \\\\\n&= \\text{Diag}(\\boldsymbol P) \\\\\n&= (1, \\ldots, 1)^T\\\\\n\\end{split}\n\\]\nclear choice \\(\\boldsymbol Q_2\\) relevant.Similarly, row sums squares cross-covariance matrix \\(\\boldsymbol \\Phi\\)\nequal variances original variables, regardless \\(\\boldsymbol Q_1\\):\n\\[\n\\sum_{j=1}^d \\phi_{ij}^2 = \\text{Var}(x_i)\n\\]\nmatrix notation\n\\[\n\\begin{split}\n\\text{Diag}\\left(\\boldsymbol \\Phi\\boldsymbol \\Phi^T\\right) &= \\text{Diag}\\left(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T \\boldsymbol Q_1 \\boldsymbol \\Sigma^{1/2}\\right) \\\\\n&= \\text{Diag}(\\boldsymbol \\Sigma) \\\\\n&= (\\text{Var}(x_1), \\ldots, \\text{Var}(x_d)^T\\\\\n\\end{split}\n\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"summaries-of-cross-covariance-boldsymbol-phi-and-cross-correlation-boldsymbol-psi-resulting-from-whitening-transformations","chapter":"3 Transformations and dimension reduction","heading":"3.3.6 Summaries of cross-covariance \\(\\boldsymbol \\Phi\\) and cross-correlation \\(\\boldsymbol \\Psi\\) resulting from whitening transformations","text":"Matrix trace:simply summary matrix trace. cross-covariance matrix \\(\\boldsymbol \\Phi\\) trace \nsum covariances corresponding elements \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\text{Tr}(\\boldsymbol \\Phi) =  \\sum_{=1}^d \\text{Cov}(x_i, z_i) =  \\sum_{=1}^d  \\phi_{ii} = \\text{Tr}\\left(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T\\right)\n\\]\nLikewise, cross-correlation matrix \\(\\boldsymbol \\Psi\\) trace \nsum correlations corresponding elements \\(\\boldsymbol x\\) \\(\\boldsymbol z\\):\n\\[\n\\text{Tr}(\\boldsymbol \\Psi) =  \\sum_{=1}^d \\text{Cor}(x_i, z_i) =  \\sum_{=1}^d  \\psi_{ii} = \\text{Tr}\\left(\\boldsymbol P^{1/2} \\boldsymbol Q_2^T\\right)\n\\]cases value trace depends \\(\\boldsymbol Q_1\\) \\(\\boldsymbol Q_2\\).\nInterestingly, unique choice trace maximised.Specifically, maximise \\(\\text{Tr}(\\boldsymbol \\Phi)\\) conduct following steps:Apply eigendecomposition \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\). Note \\(\\boldsymbol \\Lambda\\) diagonal positive eigenvalues \\(\\lambda_i > 0\\) \\(\\boldsymbol \\Sigma\\) positive definite \\(\\boldsymbol U\\) orthogonal matrix.objective function becomes\n\\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol \\Phi) &= \\text{Tr}\\left(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T\\right)\\\\\n&= \\text{Tr}\\left( \\boldsymbol U\\boldsymbol \\Lambda^{1/2} \\boldsymbol U^T \\boldsymbol Q_1^T \\right) \\\\\n&= \\text{Tr}\\left(\\boldsymbol \\Lambda^{1/2} \\, \\boldsymbol U^T \\boldsymbol Q_1^T \\boldsymbol U\\right) \\\\\n& = \\text{Tr}\\left(\\boldsymbol \\Lambda^{1/2} \\, \\boldsymbol B\\right) \\\\\n& = \\sum_{=1}^d \\lambda_i^{1/2} b_{ii}.\n\\end{split}\n\\]\nNote product two orthogonal matrices orthogonal matrix.\nTherefore, \\(\\boldsymbol B= \\boldsymbol U^T \\boldsymbol Q_1^T \\boldsymbol U\\) orthogonal matrix \n\\(\\boldsymbol Q_1 = \\boldsymbol U\\boldsymbol B^T \\boldsymbol U^T\\).\\(\\lambda_i > 0\\) \\(b_{ii} \\[-1, 1]\\) objective function maximised\n\\(b_{ii}=1\\), .e. \\(\\boldsymbol B=\\boldsymbol \\).turn implies \\(\\text{Tr}(\\boldsymbol \\Phi)\\) maximised \\(\\boldsymbol Q_1=\\boldsymbol \\).Similarly, maximise \\(\\text{Tr}(\\boldsymbol \\Psi)\\) wedecompose \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\) , following ,find \\(\\text{Tr}(\\boldsymbol \\Psi) = \\text{Tr}\\left(\\boldsymbol \\Theta^{1/2} \\, \\boldsymbol G^T \\boldsymbol Q_2^T \\boldsymbol G\\right)\\) maximised \\(\\boldsymbol Q_2=\\boldsymbol \\).Squared Frobenius norm total variation:Another way summarise dissect association \\(\\boldsymbol x\\) corresponding whitened \\(\\boldsymbol z\\)\nsquared Frobenius norm total variation based \\(\\boldsymbol \\Phi\\) \\(\\boldsymbol \\Psi\\).squared Frobenius norm (Euclidean) norm sum squared elements matrix.consider squared Frobenius norm cross-covariance matrix, .e. sum squared covariances \n\\(\\boldsymbol x\\) \\(\\boldsymbol z\\),\n\\[\n|| \\boldsymbol \\Phi||_F^2 = \\sum_{=1}^d \\sum_{j=1}^d \\phi_{ij}^2 =  \\text{Tr}(\\boldsymbol \\Phi^T \\boldsymbol \\Phi) = \\text{Tr}( \\boldsymbol \\Sigma)\n\\]\nfind equals total variation \\(\\boldsymbol \\Sigma\\) depend \\(\\boldsymbol Q_1\\).\nLikewise, computing squared Frobenius norm cross-correlation matrix, .e. sum squared correlations \n\\(\\boldsymbol x\\) \\(\\boldsymbol z\\),\n\\[\n|| \\boldsymbol \\Psi||_F^2  = \\sum_{=1}^d \\sum_{j=1}^d \\psi_{ij}^2= \\text{Tr}\\left(\\boldsymbol \\Psi^T  \\boldsymbol \\Psi\\right) =\\text{Tr}\\left( \\boldsymbol P\\right) = d\n\\]\nyields total variation \\(\\boldsymbol P\\) also depend \\(\\boldsymbol Q_2\\).\nNote squared Frobenius norm invariant rotations reflections.Proportion total variation:can now compute contribution whitened component \\(z_j\\) total variation.\nsum squared covariances \\(z_j\\) \\(x_1, \\ldots, x_d\\) \n\\[\nh_j = \\sum^d_{=1}\\text{Cov}(x_i,z_j)^2 = \\sum^d_{=1} \\phi_{ij}^2\n\\]\n\\(\\sum_{j=1}^d h_j = \\text{Tr}\\left(\\boldsymbol \\Sigma\\right)\\) total variation.\nvector notation contributions written column sums squares \\(\\boldsymbol \\Phi\\)\n\\[\n\\boldsymbol h= (h_1,\\ldots,h_d)^T = \\text{Diag}(\\boldsymbol \\Phi^T\\boldsymbol \\Phi) = \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\\,.\n\\]\nrelative contribution \\(z_j\\) versus total variation \n\\[\n\\frac{ h_j }{\\text{Tr}\\left( \\boldsymbol \\Sigma\\right)} \\,.\n\\]\nCrucially, contrast total variation, contributions \\(h_j\\) depend \nchoice \\(\\boldsymbol Q_1\\).Similarly, sum squared correlations \\(z_j\\) \\(x_1, \\ldots, x_d\\) \n\\[\nk_j = \\sum^d_{=1}\\text{Cor}(x_i,z_j)^2 = \\sum^d_{=1} \\psi_{ij}^2\n\\]\n\\(\\sum_{=j}^d k_j = \\text{Tr}( \\boldsymbol P) = d\\). vector notation correspoinds column sums squares \\(\\boldsymbol \\Psi\\)\n\\[\n\\boldsymbol k= (k_1,\\ldots,k_d)^T = \\text{Diag}\\left(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\right)=\\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\\,.\n\\]\nrelative contribution \\(z_j\\) regard total variation correlation \\(\\boldsymbol P\\)\n\n\\[\n\\frac{ k_j  }{\\text{Tr}( \\boldsymbol P)} = \\frac{ k_j  }{d} \\,.\n\\]\n, contributions \\(k_j\\) depend choice \\(\\boldsymbol Q_2\\).Maximising proportion total variation:Interestingly, possible choose unique whitening transformation contributions maximised, .e. sum \\(m\\) largest contributions \\(h_j\\) \\(k_j\\) large possible.Specifically, note \\(\\boldsymbol \\Phi^T\\boldsymbol \\Phi\\) \\(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\) symmetric real matrices. type matrices know Schur’s theorem (1923)\neigenvalues \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_d\\) majorise diagonal elements \\(p_1 \\geq p_2 \\geq \\ldots \\geq p_d\\).\nprecisely,\n\\[\n\\sum_{=1}^m \\lambda_i \\geq \\sum_{=1}^m p_i \\, ,\n\\]\n.e. sum largest \\(m\\) eigenvalues larger equal sum \\(m\\) largest diagonal elements.\nmaximum (equality) achieved matrix diagonal, case diagonal elements equal eigenvalues.Therefore, optimal solution problem maximising relative contributions obtained computing\neigendecompositions \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\)\ndiagonalise \\(\\boldsymbol \\Phi^T\\boldsymbol \\Phi=\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\) \\(\\boldsymbol \\Psi^T \\boldsymbol \\Psi=\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\) \nsetting \\(\\boldsymbol Q_1= \\boldsymbol U^T\\) \\(\\boldsymbol Q_2= \\boldsymbol G^T\\), respectively. yields maximised\ncontributions\n\\[\n\\boldsymbol h= \\text{Diag}\\left(\\boldsymbol \\Lambda\\right)= (\\lambda_1, \\ldots, \\lambda_d)^T\n\\]\n\n\\[\n\\boldsymbol k= \\text{Diag}\\left(\\boldsymbol \\Theta\\right) = (\\theta_1, \\ldots, \\theta_d)^T\n\\]\neigenvalues \\(\\lambda_i\\) \\(\\theta_i\\) arranged decreasing order.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"natural-whitening-procedures","chapter":"3 Transformations and dimension reduction","heading":"3.4 Natural whitening procedures","text":"now introduce several strategies (maximise correlation individual components, maximise compression, structural constraints) select optimal whitening procedure.Specifically, discuss following whitening transformations:Mahalanobis whitening, also known ZCA (zero-phase component analysis) whitening machine learning (based covariance)ZCA-cor whitening (based correlation)PCA whitening (based covariance)PCA-cor whitening (based correlation)Cholesky whiteningThus, following consider three main types (ZCA, PCA, Cholesky) whitening.following \\(\\boldsymbol x_c = \\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x}\\) \\(\\boldsymbol z_c = \\boldsymbol z-\\boldsymbol \\mu_{\\boldsymbol z}\\) denote mean-centered variables.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"zca-whitening","chapter":"3 Transformations and dimension reduction","heading":"3.4.1 ZCA whitening","text":"Aim: remove correlations standardise otherwise make sure \nwhitened vector \\(\\boldsymbol z\\) differ much original vector \\(\\boldsymbol x\\). Specifically, latent component \\(z_i\\) close possible corresponding original variable \\(x_i\\):\n\\[\n\\begin{array}{cc}\nz_1\\leftrightarrow x_1 \\\\\nz_2\\leftrightarrow x_2\\\\\nz_3\\leftrightarrow x_3 \\\\\n\\hdots\\\\\nz_d\\leftrightarrow x_d \\\\\n\\end{array}\n\\]\nOne possible way implement compute expected squared difference two centered random vectors \\(\\boldsymbol z_c\\) \\(\\boldsymbol x_c\\).ZCA objective function: minimise \\(\\text{E}\\left( || \\boldsymbol x_c - \\boldsymbol z_c ||^2_F \\right)\\) find optimal whitening procedure.ZCA objective function can simplified follows:\n\\[\n\\begin{split}\n\\text{E}\\left( || \\boldsymbol x_c-\\boldsymbol z_c ||^2_F   \\right)&=\\text{E}\\left( || \\boldsymbol x_c ||^2_F   \\right)  -2 \\text{E}\\left(  \\text{Tr}\\left( \\boldsymbol x_c \\boldsymbol z_c^T \\right) \\right)  + \\text{E}\\left( || \\boldsymbol z_c ||^2_F   \\right) \\\\\n& = \\text{Tr}(  \\text{E}( \\boldsymbol x_c \\boldsymbol x_c^T ) )  - 2 \\text{Tr}( \\text{E}(  \\boldsymbol x_c \\boldsymbol z_c^T ) ) + \\text{Tr}( \\text{E}( \\boldsymbol z_c \\boldsymbol z_c^T ) )\n   \\\\\n& = \\text{Tr}( \\text{Var}(\\boldsymbol x) ) - 2 \\text{Tr}( \\text{Cov}(\\boldsymbol x, \\boldsymbol z) ) +  \\text{Tr}( \\text{Var}(\\boldsymbol z) )  \\\\\n& = \\text{Tr}(\\boldsymbol \\Sigma) - 2\\text{Tr}(\\boldsymbol \\Phi)+ d \\\\\n\\end{split}\n\\]\nobjective function can also obtained putting diagonal constraint cross-covariance \\(\\boldsymbol \\Phi\\). Specifically, looking \\(\\boldsymbol \\Phi\\) closest diagonal matrix \\(\\boldsymbol \\) minimising\n\\[\n\\begin{split}\n|| \\boldsymbol \\Phi- \\boldsymbol ||^2_F &= || \\boldsymbol \\Phi||^2_F  - 2 \\text{Tr}(\\boldsymbol \\Phi^T \\boldsymbol )  + || \\boldsymbol ||^2_F \\\\\n&= \\text{Tr}(\\boldsymbol \\Sigma) - 2 \\text{Tr}(\\boldsymbol \\Phi) + d \\\\\n\\end{split}\n\\]\nforce -diagonal elements \\(\\boldsymbol \\Phi\\) close \nzero thus leads sparsity cross-covariance matrix.term depends whitening transformation \\(-2 \\text{Tr}(\\boldsymbol \\Phi)\\) \\(\\boldsymbol \\Phi\\) function\n\\(\\boldsymbol Q_1\\). Therefore can use following\nalternative objective:ZCA equivalent objective: maximise \\(\\text{Tr}(\\boldsymbol \\Phi) = \\text{Tr}(\\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^T)\\) find optimal \\(\\boldsymbol Q_1\\)Solution:earlier discussion know optimal matrix \n\\[\n\\boldsymbol Q_1^{\\text{ZCA}}=\\boldsymbol \n\\]\ncorresponding whitening matrix ZCA therefore\n\\[\n\\boldsymbol W^{\\text{ZCA}} = \\boldsymbol \\Sigma^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{ZCA}} = \\boldsymbol \\Sigma^{1/2}\n\\]\ncross-correlation matrix\n\\[\n\\boldsymbol \\Psi^{\\text{ZCA}} = \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma^{1/2}\n\\]Note \\(\\boldsymbol \\Sigma^{1/2}\\) symmetric positive definite matrix,\nhence diagonal elements positive. result,\ndiagonals \\(\\boldsymbol \\Phi^{\\text{ZCA}}\\) \\(\\boldsymbol \\Psi^{\\text{ZCA}}\\) positive,\n.e. \\(\\text{Cov}(x_i, z_i) > 0\\) \\(\\text{Cor}(x_i, z_i) > 0\\).\nHence, ZCA two corresponding components \\(x_i\\) \\(z_i\\) always positively correlated!Proportion total variation:ZCA \\(\\boldsymbol Q_1=\\boldsymbol \\) find \\(\\boldsymbol h=\\text{Diag}(\\boldsymbol \\Sigma) = \\sum_{j=1}^d \\text{Var}(x_j)\\) \\(h_i=\\text{Var}(x_i)\\) hence ZCA proportion total variation contributed \nlatent component \\(z_i\\) ratio \\(\\frac{\\text{Var}(x_i)}{\\sum_{j=1}^d \\text{Var}(x_j)}\\).Summary:ZCA/Mahalanobis transform unique transformation minimises expected total squared component-wise difference \\(\\boldsymbol x_c\\) \\(\\boldsymbol z_c\\).ZCA corresponding components whitened original variables always positively correlated. facilitates interpretation whitened variables.Use ZCA aka Mahalanobis whitening want “just” remove correlations.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"zca-cor-whitening","chapter":"3 Transformations and dimension reduction","heading":"3.4.2 ZCA-Cor whitening","text":"Aim: remove scale \\(\\boldsymbol x\\) first comparing \\(\\boldsymbol z\\).ZCA-cor objective function: minimise \\(\\text{E}\\left( || \\boldsymbol V^{-1/2} \\boldsymbol x_c -\\boldsymbol z_c ||^2_F \\right)\\) find optimal whitening procedure.can simplified follows:\n\\[\n\\begin{split}\n\\text{E}\\left( || \\boldsymbol V^{-1/2} \\boldsymbol x_c -\\boldsymbol z_c||^2_F   \\right)&=\\text{E}\\left( || \\boldsymbol V^{-1/2} \\boldsymbol x_c ||^2_F   \\right)  -2 \\text{E}\\left(  \\text{Tr}\\left( \\boldsymbol V^{-1/2} \\boldsymbol x_c \\boldsymbol z_c^T  \\right) \\right) +  \\text{E}\\left( || \\boldsymbol z_c ||^2_F   \\right)\\\\\n& = \\text{Tr}(  \\text{E}(\\boldsymbol V^{-1/2} \\boldsymbol x_c \\boldsymbol x_c^T \\boldsymbol V^{-1/2}) )  \n- 2 \\text{Tr}( \\text{E}( \\boldsymbol V^{-1/2} \\boldsymbol x_c \\boldsymbol z_c^T  ) )\n+\\text{Tr}( \\text{E}( \\boldsymbol z_c \\boldsymbol z_c^T ) )\n  \\\\\n& = \\text{Tr}(  \\text{Cor}(\\boldsymbol x, \\boldsymbol x) ) - 2 \\text{Tr}( \\text{Cor}(\\boldsymbol x, \\boldsymbol z) ) + \\text{Tr}( \\text{Var}(\\boldsymbol z) )   \\\\\n& = d - 2\\text{Tr}(\\boldsymbol \\Psi)+ d \\\\\n& = 2d - 2\\text{Tr}(\\boldsymbol \\Psi)\n\\end{split}\n\\]\nobjective function can also obtained putting diagonal constraint cross-correlation \\(\\boldsymbol \\Psi\\). Specifically, looking \\(\\boldsymbol \\Psi\\) closest diagonal matrix \\(\\boldsymbol \\) minimising\n\\[\n\\begin{split}\n|| \\boldsymbol \\Psi- \\boldsymbol ||^2_F &= || \\boldsymbol \\Psi||^2_F  - 2 \\text{Tr}(\\boldsymbol \\Psi^T \\boldsymbol )  + || \\boldsymbol ||^2_F \\\\\n&= d - 2 \\text{Tr}(\\boldsymbol \\Psi) + d \\\\\n&= 2 d - 2 \\text{Tr}(\\boldsymbol \\Psi) \\\\\n\\end{split}\n\\]\nforce -diagonal elements \\(\\boldsymbol \\Psi\\) close \nzero thus leads sparsity cross-correlation matrix.term depends whitening transformation \\(-2 \\text{Tr}(\\boldsymbol \\Psi)\\) \\(\\boldsymbol \\Psi\\) function \\(\\boldsymbol Q_2\\). Thus can use following alternative objective instead:ZCA-cor equivalent objective: maximise \\(\\text{Tr}(\\boldsymbol \\Psi)=\\text{Tr}(\\boldsymbol P^{1/2} \\boldsymbol Q_2^T)\\) find optimal \\(\\boldsymbol Q_2\\)Solution: ZCA using correlation instead covariance:earlier discussion know optimal matrix \n\\[\n\\boldsymbol Q_2^{\\text{ZCA-Cor}}=\\boldsymbol \n\\]\ncorresponding whitening matrix ZCA-cor therefore\n\\[\n\\boldsymbol W^{\\text{ZCA-Cor}} = \\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{ZCA-Cor}} = \\boldsymbol V^{1/2} \\boldsymbol P^{1/2}\n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{ZCA-Cor}} = \\boldsymbol P^{1/2}\n\\]ZCA-cor transformation also \n\\(\\text{Cov}(x_i, z_i) > 0\\) \\(\\text{Cor}(x_i, z_i) > 0\\)\ntwo corresponding components \\(x_i\\) \\(z_i\\) always positively correlated!Proportion total variation:ZCA-cor \\(\\boldsymbol Q_2=\\boldsymbol \\) find \\(\\boldsymbol k=\\text{Diag}(\\boldsymbol P) = d\\) \\(k_i =1\\).\nThus, ZCA-cor whitened component \\(z_i\\) contributes equally total variation \\(\\text{Tr}(\\boldsymbol P) =d\\), relative proportion\n\\(\\frac{1}{d}\\).Summary:ZCA-cor whitening unique whitening transformation maximising \ntotal correlation corresponding elements \\(\\boldsymbol x\\) \\(\\boldsymbol z\\).ZCA-cor leads interpretable \\(\\boldsymbol z\\) individual element \\(\\boldsymbol z\\)\n(typically strongly) positively correlated corresponding element original \\(\\boldsymbol x\\).ZCA-cor explicitly constructed maximise total\npairwise correlations achieves higher total correlation ZCA.\\(\\boldsymbol x\\) standardised \\(\\text{Var}(x_i)=1\\) ZCA ZCA-cor identical.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-whitening","chapter":"3 Transformations and dimension reduction","heading":"3.4.3 PCA whitening","text":"Aim: remove correlations time compress information latent variables.\nSpecifically, like first latent component \\(z_1\\) \nmaximally linked variables \\(\\boldsymbol x\\), followed \nsecond component \\(z_2\\) :\\[\n\\begin{array}{c}\nz_1 \\rightarrow   x_1, x_2, \\ldots, x_d \\\\\nz_2 \\rightarrow   x_1, x_2, \\ldots, x_d \\\\\n\\hdots\\\\\nz_d \\rightarrow   x_1, x_2, \\ldots, x_d \\\\\n\\end{array}\n\\]\nOne way measure total association latent component \\(z_j\\) \noriginal \\(x_1, \\ldots, x_d\\) sum \ncorresponding squared covariances\n\\[\nh_j = \\sum^d_{=1}\\text{Cov}(x_i,z_j)^2 = \\sum^d_{=1} \\phi_{ij}^2\n\\]\nequivalently column sum squares \\(\\boldsymbol \\Phi\\)\n\\[\n\\boldsymbol h= (h_1,\\ldots,h_d)^T = \\text{Diag}(\\boldsymbol \\Phi^T\\boldsymbol \\Phi) = \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\n\\]\n\\(h_j\\) contribution\n\\(z_j\\) \\(\\text{Tr}\\left( \\boldsymbol Q_1 \\boldsymbol \\Sigma\\boldsymbol Q_1^T \\right)= \\text{Tr}(\\boldsymbol \\Sigma)\\)\n.e. total variation based \\(\\boldsymbol \\Sigma\\).\n\\(\\text{Tr}(\\boldsymbol \\Sigma)\\) constant implies \\(d-1\\) independent \\(h_j\\).PCA-whitening wish concentrate contributions total variation based\n\\(\\boldsymbol \\Sigma\\) small number\nlatent components.PCA whitening objective function: find optimal optimal \\(\\boldsymbol Q_1\\) \nresulting set \\(h_1 \\geq h_2 \\ldots \\geq h_d\\) \\(\\boldsymbol h= \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\\) majorizes set relative contributions.Solution:Following earlier discussion apply Schur’s theorem find optimal\nsolution diagonalising \\(\\boldsymbol \\Phi^T\\boldsymbol \\Phi\\) eigendecomposition \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\).\nHence, optimal value \\(\\boldsymbol Q_1\\) matrix \n\\[\n\\boldsymbol Q_1^{\\text{PCA}}=\\boldsymbol U^T\n\\]\nHowever, recall \\(\\boldsymbol U\\) uniquely defined — free change columns signs.\ncorresponding whitening matrix \n\\[\n\\boldsymbol W^{\\text{PCA}} = \\boldsymbol U^T\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Lambda^{-1/2}\\boldsymbol U^T\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{PCA}} = \\boldsymbol U\\boldsymbol \\Lambda^{1/2}\n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{PCA}} = \\boldsymbol V^{-1/2} \\boldsymbol U\\boldsymbol \\Lambda^{1/2}\n\\]Identifiability:Note (.e. \\(\\boldsymbol Q_1^{\\text{PCA}}, \\boldsymbol W^{\\text{PCA}}, \\boldsymbol \\Phi^{\\text{PCA}}, \\boldsymbol \\Psi^{\\text{PCA}}\\)) unique\ndue sign ambiguity columns \\(\\boldsymbol U\\).Therefore, identifiability reasons may wish impose constraint \\(\\boldsymbol Q_1^{\\text{PCA}}\\)\nequivalently \\(\\boldsymbol \\Phi^{\\text{PCA}}\\). useful condition require (given ordering\noriginal variables!) \\(\\boldsymbol Q_1^{\\text{PCA}}\\) positive diagonal\nequivalently \\(\\boldsymbol \\Phi^{\\text{PCA}}\\) positive diagonal. implies \n\\(\\text{Diag}(\\boldsymbol U) > 0\\) \\(\\text{Diag}(\\boldsymbol \\Psi^{\\text{PCA}}) > 0\\), hence\npairs \\(x_i\\) \\(z_i\\) positively correlated.particularly important pay attention sign ambiguity\ncomparing different computer implementations PCA whitening (related PCA approach).Note actual objective PCA whitening \\(\\text{Diag}(\\boldsymbol \\Phi^T\\boldsymbol \\Phi)\\) affected sign ambiguity\nsince column signs \\(\\boldsymbol \\Phi\\) matter.Proportion total variation:PCA whitening contribution \\(h_i^{\\text{PCA}}\\) latent component \\(z_i\\)\ntotal variation based covariance \\(\\text{Tr}(\\boldsymbol \\Sigma) = \\sum_{j=1}^d \\lambda_j\\) \n\\(h_i^{\\text{PCA}} = \\lambda_i\\).\nfraction \\(\\frac{\\lambda_i}{\\sum^d_{j=1}\\lambda_j}\\) relative\ncontribution element \\(\\boldsymbol z\\) explain total variation.Thus, low ranking components \\(z_i\\) small \\(h_i^{\\text{PCA}}=\\lambda_i\\) may discarded.\nway PCA whitening achieves compression \ndimension reduction.Summary:PCA whitening whitening transformation maximises compression sum squared cross-covariances underlying optimality criterion.sign ambiguities PCA whitened variables inherited sign ambiguities eigenvectors.positive-diagonal condition orthogonal matrices imposed sign ambiguities fully resolved corresponding components \\(z_i\\) \\(x_i\\) always positively correlated.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-cor-whitening","chapter":"3 Transformations and dimension reduction","heading":"3.4.4 PCA-cor whitening","text":"Aim: PCA whitening remove scale \\(\\boldsymbol x\\) first. means use squared correlations rather squared covariances measure compression, .e.\\[\nk_j = \\sum^d_{=1}\\text{Cor}(x_i, z_j)^2 = \\sum^d_{=1} \\psi_{ij}^2\n\\]\nvector notation column sum squares \\(\\boldsymbol \\Psi\\)\n\\[\n\\boldsymbol k= (k_1,\\ldots,k_d)^T = \\text{Diag}\\left(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\right)=\\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\n\\]\n\\(k_j\\) contribution\n\\(z_j\\) \\(\\text{Tr}\\left( \\boldsymbol Q_2 \\boldsymbol P\\boldsymbol Q_2^T \\right)= \\text{Tr}(\\boldsymbol P) = d\\)\n.e. total variation based \\(\\boldsymbol P\\).\n\\(\\text{Tr}(\\boldsymbol P)=d\\) constant implies \\(d-1\\) independent \\(k_j\\).PCA-cor-whitening wish concentrate contributions total variation based \\(\\boldsymbol P\\) small number latent components.PCA-cor whitening objective function: find optimal optimal \\(\\boldsymbol Q_2\\) \nresulting set \\(k_1 \\geq k_2 \\ldots \\geq k_d\\) \\(\\boldsymbol k= \\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\\) majorizes set relative contributions.Solution:Following earlier discussion apply Schur’s theorem find optimal\nsolution diagonalising \\(\\boldsymbol \\Psi^T\\boldsymbol \\Psi\\) eigendecomposition \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\).\nHence, optimal value \\(\\boldsymbol Q_2\\) matrix \n\\[\n\\boldsymbol Q_2^{\\text{PCA-Cor}}=\\boldsymbol G^T\n\\]\n\\(\\boldsymbol G\\) uniquely defined — free change signs columns.\ncorresponding whitening matrix \n\\[\n\\boldsymbol Q_2^{\\text{PCA-Cor}}=\\boldsymbol G^T\n\\]\ncorresponding whitening matrix \\[\n\\boldsymbol W^{\\text{PCA-Cor}} = \\boldsymbol \\Theta^{-1/2} \\boldsymbol G^T \\boldsymbol V^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{PCA-Cor}} = \\boldsymbol V^{1/2} \\boldsymbol G\\boldsymbol \\Theta^{1/2}\n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{PCA-Cor}} = \\boldsymbol G\\boldsymbol \\Theta^{1/2}\n\\]Identifiability:PCA whitening, sign ambiguities column signs \\(\\boldsymbol G\\)\ncan freely chosen. identifiability may wish impose constraints\n\\(\\boldsymbol Q_2^{\\text{PCA-Cor}}\\) equivalently \\(\\boldsymbol \\Psi^{\\text{PCA-Cor}}\\). useful condition require (given\nordering original variables!)\ndiagonal elements \\(\\boldsymbol Q_2^{\\text{PCA-Cor}}\\) positive equivalently \\(\\boldsymbol \\Psi^{\\text{PCA-Cor}}\\) positive diagonal.\nimplies \n\\(\\text{Diag}(\\boldsymbol G) > 0\\) \\(\\text{Diag}(\\boldsymbol \\Phi^{\\text{PCA-Cor}}) > 0\\).Note actual objective PCA-cor whitening \\(\\text{Diag}(\\boldsymbol \\Psi^T\\boldsymbol \\Psi)\\) affected sign ambiguity\nsince column signs \\(\\boldsymbol \\Psi\\) matter.Proportion total variation:PCA-cor whitening contribution \\(k_i^{\\text{PCA-Cor}}\\) latent component\n\\(z_i\\)\ntotal variation based correlation \\(\\text{Tr}(\\boldsymbol P) = d\\) \n\\(k_i^{\\text{PCA-Cor}} = \\theta_i\\).\nfraction \\(\\frac{\\theta_i}{d}\\) relative\ncontribution element \\(\\boldsymbol z\\) explain total variation.Summary:PCA-cor whitening whitening transformation maximises compression sum squared cross-correlations underlying optimality criterion.sign ambiguities PCA-cor whitened variables inherited sign ambiguities eigenvectors.positive-diagonal condition orthogonal matrices imposed sign ambiguities fully resolved corresponding components \\(z_i\\) \\(x_i\\) always positively correlated.\\(\\boldsymbol x\\) standardised \\(\\text{Var}(x_i)=1\\), PCA PCA-cor whitening identical.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"cholesky-whitening","chapter":"3 Transformations and dimension reduction","heading":"3.4.5 Cholesky whitening","text":"Cholesky matrix decomposition:Cholesky decomposition square matrix \\(\\boldsymbol = \\boldsymbol L\\boldsymbol L^T\\) requires positive definite \\(\\boldsymbol \\) unique.\n\\(\\boldsymbol L\\) lower triangular matrix positive diagonal elements.\ninverse \\(\\boldsymbol L^{-1}\\) also lower triangular positive diagonal elements.\n\\(\\boldsymbol D\\) diagonal matrix positive elements \\(\\boldsymbol D\\boldsymbol L\\) also lower triangular matrix positive diagonal Cholesky factor matrix \\(\\boldsymbol D\\boldsymbol \\boldsymbol D\\).Aim Cholesky whitening:Find whitening transformation cross-covariance \\(\\boldsymbol \\Phi\\) cross-correlation \\(\\boldsymbol \\Psi\\) lower triangular structure. Specifically, wish \nfirst latent variable \\(z_1\\) linked original variables \\(x_1, \\ldots, x_d\\),\nsecond latent variable \\(z_2\\) \\(x_3, \\ldots, x_d\\), ,\nlast variable \\(z_d\\) linked \\(x_d\\).\n\\[\n\\begin{array}{cc}\nz_1 \\rightarrow &  x_1, x_2, \\ldots, x_d \\\\\nz_2 \\rightarrow &  x_2, \\ldots, x_d \\\\\n\\hdots\\\\\nz_d \\rightarrow & x_d \\\\\n\\end{array}\n\\]\nalso impose (whitening transformations discussed earlier)\ncross-correlation pair \\(x_i\\) \\(z_i\\) positive.Thus, Cholesky whitening imposes structural (sparsity!) constraint loadings,\nnon-zero coefficients lower half whereas upper half\ncoefficients vanish.Therefore procedure can seen hybrid ZCA PCA whitening.Solution: order find whitening transformation use \nCholesky decomposition apply covariance matrix \\(\\boldsymbol \\Sigma= \\boldsymbol L\\boldsymbol L^T\\)resulting whitening matrix \n\\[\n\\boldsymbol W^{\\text{Chol}}=\\boldsymbol L^{-1}\n\\]\nconstruction, \\(\\boldsymbol W^{\\text{Chol}}\\) lower triangular matrix positive\ndiagonal. whitening constraint satisfied \n\\((\\boldsymbol W^{\\text{Chol}})^T\\boldsymbol W^{\\text{Chol}} = (\\boldsymbol L^{-1})^T \\boldsymbol L^{-1} = (\\boldsymbol L^T)^{-1} \\boldsymbol L^{-1} = (\\boldsymbol L\\boldsymbol L^T)^{-1} = \\boldsymbol \\Sigma^{-1}\\).cross-covariance matrix inverse whitening matrix\n\\[\n\\boldsymbol \\Phi^{\\text{Chol}} = \\boldsymbol L\n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{Chol}} = \\boldsymbol V^{-1/2} \\boldsymbol L\n\\]\n\\(\\boldsymbol \\Phi^{\\text{Chol}}\\) \n\\(\\boldsymbol \\Psi^{\\text{Chol}}\\) \nlower triangular matrices positive diagonal elements.\nHence two corresponding components \\(x_i\\) \\(z_i\\) always positively correlated!Finally, corresponding orthogonal matrices \n\\[\n\\boldsymbol Q_1^{\\text{Chol}}  =  \\boldsymbol \\Phi^T \\boldsymbol \\Sigma^{-1/2} =   \\boldsymbol L^T \\boldsymbol \\Sigma^{-1/2}\n\\]\n\n\\[\n\\boldsymbol Q_2^{\\text{Chol}} =  \\boldsymbol \\Psi^T \\boldsymbol P^{-1/2} =  \\boldsymbol L^T \\boldsymbol V^{-1/2} \\boldsymbol P^{-1/2}\n\\]Application correlation instead covariance:may also apply Cholesky decomposition correlation rather covariance matrix.\nHowever, unlike ZCA PCA lead different whitening transform:Let’s denote Cholesky composition correlation matrix \\(\\boldsymbol P= \\boldsymbol L_P \\boldsymbol L_P^T\\). corresponding whitening matrix \\(\\boldsymbol W^{\\text{Chol}}_P= \\boldsymbol L_P^{-1} \\boldsymbol V^{-1/2}\\). \\(\\boldsymbol P= \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2} = \\boldsymbol V^{-1/2} \\boldsymbol L\\boldsymbol L^T \\boldsymbol V^{-1/2}\\) see \n\\(\\boldsymbol L_P = \\boldsymbol V^{-1/2} \\boldsymbol L\\) hence \\(\\boldsymbol W^{\\text{Chol}}_P = (\\boldsymbol V^{-1/2} \\boldsymbol L)^{-1} \\boldsymbol V^{-1/2} =\\boldsymbol L^{-1} = \\boldsymbol W^{\\text{Chol}}\\).Dependence input order:Cholesky whitening depends ordering input variables.\nordering original variables yield different triangular\nconstraint thus different Cholesky whitening transform. example,\ninverting ordering \\(x_d, x_{d-1}, \\ldots, x_1\\) effectively enforce upper triangular\nshape.alternative formulation Cholesky whitening decomposes \nprecision matrix rather covariance matrix. yields \nupper triangular structure directly otherwise fully equivalent Cholesky whitening\nbased decomposing covariance.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"comparison-of-whitening-procedures---simulated-data","chapter":"3 Transformations and dimension reduction","heading":"3.4.6 Comparison of whitening procedures - simulated data","text":"comparison, results ZCA, PCA Cholesky whitening applied simulated bivariate normal data set correlation \\(\\rho=0.8\\).column 1 can see simulated data scatter plot.Column 2 shows scatter plots whitened data — expect three methods remove correlation produce isotropic covariance.However, three approaches differ cross-correlations. Columns 3 4 show cross-correlations first two corresponding components (\\(x_1\\) \\(z_1\\), \\(x_2\\) \\(z_2\\)) ZCA, PCA Cholesky whitening. expected, ZCA pairs show strong correlation, case PCA Cholesky whitening.Note Cholesky whitening first component \\(z_1\\)\nperfectly positively correlated original component \\(x_1\\)\nwhitening matrix lower triangular positive diagonal hence \\(z_1\\) just \\(x_1\\) multiplied positive constant.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"comparison-of-whitening-procedures---iris-flowers","chapter":"3 Transformations and dimension reduction","heading":"3.4.7 Comparison of whitening procedures - iris flowers","text":"example consider well known iris flower data set. consists botanical measures (sepal length, sepal width,\npetal length petal width) 150 iris flowers comprising\nthree species (Iris setosa, Iris versicolor, Iris virginica). Hence data set dimension \\(d=4\\) sample size \\(n=150\\).apply discussed whitening transforms data, sort whitened components relative contribution total variation. Cholesky whitening used \ninput order shape constraint.results explained variation based covariance loadings:expected, two PCA whitening approaches compress data .\nend spectrum, ZCA whitening methods two least\ncompressing approaches. Cholesky whitening compromise ZCA PCA terms\ncompression.Similar results obtained based correlation loadings - note ZCA-cor provides\nequal weight latent variable.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"recap","chapter":"3 Transformations and dimension reduction","heading":"3.4.8 Recap","text":"data standardised \\(\\boldsymbol \\Phi\\) \\(\\boldsymbol \\Psi\\) \nhence ZCA become ZCA-cor PCA becomes PCA-cor. triangular shape constraint \nCholesky whitening depends ordering original variables.Related methods discussed course:Factor models: essentially whitening plus additional error term, factors rotational\nfreedom just like whiteningFactor models: essentially whitening plus additional error term, factors rotational\nfreedom just like whiteningPartial Least Squares (PLS): similar Principal Components Analysis (PCA) regression setting (choice \nlatent variables depending response)Partial Least Squares (PLS): similar Principal Components Analysis (PCA) regression setting (choice \nlatent variables depending response)Nonlinear dimension reduction methods SNE, tSNE, UMAP.Nonlinear dimension reduction methods SNE, tSNE, UMAP.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"principal-component-analysis-pca","chapter":"3 Transformations and dimension reduction","heading":"3.5 Principal Component Analysis (PCA)","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-transformation","chapter":"3 Transformations and dimension reduction","heading":"3.5.1 PCA transformation","text":"Principal component analysis proposed 1933 Harald Hotelling 7 closely related PCA whitening. underlying mathematics developed earlier 1901 Karl Pearson 8 problem orthogonal regression.Assume random vector \\(\\boldsymbol x\\) \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\).\nPCA particular orthogonal transformation original \\(\\boldsymbol x\\)\nresulting components orthogonal:\n\\[\n\\underbrace{\\boldsymbol t^{\\text{PCA}}}_{\\text{Principal components}} = \\underbrace{\\boldsymbol U^T}_{\\text{Orthogonal matrix}}   \\boldsymbol x\n\\]\n\\[\\text{Var}(\\boldsymbol t^{\\text{PCA}}) = \\boldsymbol \\Lambda= \\begin{pmatrix} \\lambda_1 & \\dots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & \\lambda_d\\end{pmatrix}\\]\nNote principal components orthogonal unit variance.\nInstead, variance principal components \\(t_i\\) equal eigenvalues \\(\\lambda_i\\).Thus PCA whitening procedure closely linked PCA whitening obtained standardising principal components unit variance:\n\\(\\boldsymbol z^{\\text{PCA}} = \\boldsymbol \\Lambda^{-1/2} \\boldsymbol t^{\\text{PCA}} = \\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T \\boldsymbol x= \\boldsymbol U^T \\boldsymbol \\Sigma^{-1/2} \\boldsymbol x= \\boldsymbol Q_1^{\\text{PCA}} \\boldsymbol \\Sigma^{-1/2} \\boldsymbol x= \\boldsymbol W^{\\text{PCA}} \\boldsymbol x\\)Compression properties:total variation \\(\\text{Tr}(\\text{Var}(\\boldsymbol t^{\\text{PCA}})) = \\text{Tr}( \\boldsymbol \\Lambda) = \\sum^d_{j=1}\\lambda_j\\).\nprinciple components fraction \\(\\frac{\\lambda_i}{\\sum^d_{j=1}\\lambda_j}\\) can interpreted proportion variation contributed \ncomponent \\(\\boldsymbol t^{\\text{PCA}}\\) total variation. Thus, low ranking components \\(\\boldsymbol t^{\\text{PCA}}\\) low variation may discarded, thus leading reduction dimension.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"application-to-data","chapter":"3 Transformations and dimension reduction","heading":"3.5.2 Application to data","text":"Written terms data matrix \\(\\boldsymbol X\\) instead random vector \\(\\boldsymbol x\\) PCA becomes:\n\\[\\underbrace{\\boldsymbol T}_{\\text{Sample version principal components}}=\\underbrace{\\boldsymbol X}_{\\text{Data matrix}}\\boldsymbol U\\]\nnow two ways obtain \\(\\boldsymbol U\\):Estimate covariance matrix, e.g. \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c\\) \\(\\boldsymbol X_c\\) column-centred data matrix; apply eigenvalue decomposition \\(\\hat{\\boldsymbol \\Sigma}\\) get \\(\\boldsymbol U\\).Estimate covariance matrix, e.g. \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c\\) \\(\\boldsymbol X_c\\) column-centred data matrix; apply eigenvalue decomposition \\(\\hat{\\boldsymbol \\Sigma}\\) get \\(\\boldsymbol U\\).Compute singular value decomposition \\(\\boldsymbol X_c = \\boldsymbol V\\boldsymbol D\\boldsymbol U^T\\). \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c = \\boldsymbol U(\\frac{1}{n}\\boldsymbol D^2)\\boldsymbol U^T\\) can just use \\(\\boldsymbol U\\) SVD \\(\\boldsymbol X_c\\) need compute covariance.Compute singular value decomposition \\(\\boldsymbol X_c = \\boldsymbol V\\boldsymbol D\\boldsymbol U^T\\). \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c = \\boldsymbol U(\\frac{1}{n}\\boldsymbol D^2)\\boldsymbol U^T\\) can just use \\(\\boldsymbol U\\) SVD \\(\\boldsymbol X_c\\) need compute covariance.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"iris-flower-data-example","chapter":"3 Transformations and dimension reduction","heading":"3.5.3 Iris flower data example","text":"first standardise data, compute PCA components plot proportion total variation contributed component.\nshows two PCA components needed achieve 95% total variation:scatter plot plot first two principal components also informative:shows groupings among \n150 flowers, corresponding species, groups can characterised\nprincipal components.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-correlation-loadings","chapter":"3 Transformations and dimension reduction","heading":"3.5.4 PCA correlation loadings","text":"earlier section learned general whitening transformation cross-correlations \\(\\boldsymbol \\Psi=\\text{Cor}(\\boldsymbol x, \\boldsymbol z)\\) play role correlation loadings inverse transformation:\n\\[\n\\boldsymbol V^{-1/2} \\boldsymbol x= \\boldsymbol \\Psi\\boldsymbol z\\, ,\n\\]\n.e. coefficients linking whitened variable \\(\\boldsymbol z\\) standardised original variable \\(\\boldsymbol x\\).\nrelationship holds therefore also PCA-whitening\n\\(\\boldsymbol z^{\\text{PCA}}= \\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T \\boldsymbol x\\) \\(\\boldsymbol \\Psi^{\\text{PCA}} = \\boldsymbol V^{-1/2} \\boldsymbol U\\boldsymbol \\Lambda^{1/2}\\).classical PCA whitening approach \\(\\text{Var}(\\boldsymbol t^{\\text{PCA}}) \\neq \\boldsymbol \\). However, can still compute cross-correlations \\(\\boldsymbol x\\) \nprincipal components \\(\\boldsymbol t^{\\text{PCA}}\\), resulting \n\\[\n\\text{Cor}(\\boldsymbol x, \\boldsymbol t^{\\text{PCA}}) = \\boldsymbol V^{-1/2} \\boldsymbol U\\boldsymbol \\Lambda^{1/2}  = \\boldsymbol \\Psi^{\\text{PCA}}\n\\]\nNote cross-correlations PCA-whitening since\n\\(\\boldsymbol t^{\\text{PCA}}\\) \\(\\boldsymbol z^{\\text{PCA}}\\) differ scale.inverse PCA transformation \n\\[\n\\boldsymbol x= \\boldsymbol U\\boldsymbol t^{\\text{PCA}}\n\\]\nterms standardised PCA components \\(\\boldsymbol z^{\\text{PCA}} = \\boldsymbol \\Lambda^{-1/2} \\boldsymbol t^{\\text{PCA}}\\) standardised original components becomes\n\\[\n\\boldsymbol V^{-1/2} \\boldsymbol x= \\boldsymbol \\Psi\\boldsymbol \\Lambda^{-1/2} \\boldsymbol t^{\\text{PCA}}\n\\]\nThus cross-correlation matrix \\(\\boldsymbol \\Psi\\) plays role correlation loadings\nalso classical PCA, .e. \ncoefficients linking standardised PCA components standardised original components.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-correlation-loadings-plot","chapter":"3 Transformations and dimension reduction","heading":"3.5.5 PCA correlation loadings plot","text":"PCA PCA-cor whitening well classical PCA aim compression, .e.\nfind latent variables total variation contributed \nsmall number components.order able better interpret top ranking PCA component can use visual device called correlation loadings plot. compute correlation PCA components 1 2 (\\(t_1^{\\text{PCA}}\\) \\(t_2^{\\text{PCA}})\\) original variables \\(x_1, \\ldots, x_d\\).original variable \\(x_i\\) therefore two numbers -1 1, correlation\n\\(\\text{Cor}(x_i, t_1^{\\text{PCA}}) = \\psi_{i1}\\) \\(\\text{Cor}(x_i, t_2^{\\text{PCA}}) = \\psi_{i2}\\) use coordinates draw point plane. Recall \nrow sums squares correlation loadings \\(\\boldsymbol \\Psi\\) identical 1.\nHence, sum squared loadings just first two components also 1.\nThus, construction, points\nlie within unit circle around origin.\noriginal variables strongly influenced\ntwo latent variables strong correlation thus lie near outer circle, whereas variables influenced two latent variables lie near origin.example, correlation loadings plot showing cross-correlation first two\nPCA components four variables iris flower data set discussed earlier.interpretation plot discussed Worksheet 5.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"unsupervised-learning-and-clustering","chapter":"4 Unsupervised learning and clustering","heading":"4 Unsupervised learning and clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"challenges-in-unsupervised-learning","chapter":"4 Unsupervised learning and clustering","heading":"4.1 Challenges in unsupervised learning","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"objective","chapter":"4 Unsupervised learning and clustering","heading":"4.1.1 Objective","text":"observe data \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) \\(n\\) objects (subjects).\nsample \\(\\boldsymbol x_i\\) vector dimension \\(d\\). Thus, \\(n\\) objects / subjects measurements \\(d\\) variables.\naim unsupervised learning identify patters relating objects/subjects based information available \\(\\boldsymbol x_i\\). Note unsupervised learning use information\n\\(\\boldsymbol x_i\\) nothing else.illustration consider first two principal components Iris flower data (see e.g. Worksheet 5):Clearly group structure among samples linked particular\npatterns first two principal components.Note plot used additional information, class labels (setosa, versicolor, virginica), highlighting true underlying structure (three flower species).unsupervised learning class labels (assumed ) unknown, aim infer clustering thus classes labels. 9There many methods clustering unsupervise learning, purely algorithmic well probabilistic. chapter study commonly used approaches.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"questions-and-problems","chapter":"4 Unsupervised learning and clustering","heading":"4.1.2 Questions and problems","text":"order implement unsupervised learning need address number questions:define clusters?learn / infer clusters?many clusters ? (surprisingly difficult!)can assess uncertainty clusters?know clusters also interested :features define / separate cluster?(note feature / variable selection problem, discussed supervised learning).Many problems questions highly specific data hand.\nCorrespondingly, many different types methods models clustering unsupervised learning.terms representing data, unsupervised learning tries balance following two extremes:objects grouped single cluster (low complexity model)objects put cluster (high complexity model)practise, aim find compromise, .e. model captures \nstructure data appropriate complexity — low complex.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"why-is-clustering-difficult","chapter":"4 Unsupervised learning and clustering","heading":"4.1.3 Why is clustering difficult?","text":"Partioning problem (combinatorics): many partitions \\(n\\) objects (say flowers) \\(K\\) groups (say species) exists?Answer:\\[\nS(n,K) = \\left\\{\\begin{array}{l} n \\\\ K \\end{array} \\right\\}\n\\]\n“Sterling number second type”.large n:\n\\[\nS(n,K) \\approx \\frac{K^n }{ K!}\n\\]\nExample:enormously big numbers even relatively small problems!\\(\\Longrightarrow\\) Clustering / partitioning / structure discovery easy!\\(\\Longrightarrow\\) expect perfect answers single “true” clusteringIn fact, model data many differnt clusterings may fit data equally well.\\(\\Longrightarrow\\) need assesse uncertainty clusteringThis can done part probabilistic modelling resampling (e.g., bootstrap).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"common-types-of-clustering-methods","chapter":"4 Unsupervised learning and clustering","heading":"4.1.4 Common types of clustering methods","text":"many different clustering algorithms!consider following two broad types methods:Algorithmic clustering methods (explicitly based probabilistic model)\\(K\\)-meansPAMhierarchical clustering (distance similarity-based, divise agglomerative)pros: fast, effective algorithms find least grouping\ncons: probabilistic interpretation, blackbox methodsModel-based clustering (based probabilistic model)mixture models (e.g. Gaussian mixture models, GMMs, non-hierarchical)graphical models (e.g. Bayesian networks, Gaussian graphical models GGM, trees networks)pros: full probabilistic model corresponding advantages\ncons: computationally expensive, sometimes impossible compute exactly.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"hierarchical-clustering","chapter":"4 Unsupervised learning and clustering","heading":"4.2 Hierarchical clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"tree-like-structures","chapter":"4 Unsupervised learning and clustering","heading":"4.2.1 Tree-like structures","text":"Often, categorisations objects nested, .e. sub-categories categories etc. can naturally represented tree-like hierarchical structures.many branches science hierarchical clusterings widely employed, example evolutionary biology: see e.g. Tree Life explaining biodiversity lifephylogenetic trees among species (e.g. vertebrata)population genetic trees describe human evolutiontaxonomic trees plant speciesetc.Note visualising hierarchical structures typically corresponding tree depicted facing downwards, .e. root tree shown top, tips/leaves tree shown bottom!order obtain hierarchical clustering data two opposing strategies commonly used:divisive recursive partitioning algorithms\ngrow tree root downwards\nfirst determine main two clusters, recursively refine clusters .\ngrow tree root downwardsfirst determine main two clusters, recursively refine clusters .agglomerative algorithms\ngrow tree leaves upwards\nsuccessively form partitions first joining individual object together,\nrecursively join groups items together, merged.\ngrow tree leaves upwardssuccessively form partitions first joining individual object together,\nrecursively join groups items together, merged.following discuss number popular hierarchical agglomerative clustering algorithms based pairwise distances / similarities (\\(n \\times n\\) matrix) among data points.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"agglomerative-hierarchical-clustering-algorithms","chapter":"4 Unsupervised learning and clustering","heading":"4.2.2 Agglomerative hierarchical clustering algorithms","text":"general algorithm agglomerative construction hierarchical clustering works follows:Initialisation:Compute dissimilarity / distance matrix pairs objects “objects” single data points stage later also sets data points.Iterative procedure:identify pair objects smallest distance. two objects merged together one set. Create internal node tree represent set.identify pair objects smallest distance. two objects merged together one set. Create internal node tree represent set.update distance matrix computing distances new set \nobjects. new set contains data points procedure terminates. final node created root node.update distance matrix computing distances new set \nobjects. new set contains data points procedure terminates. final node created root node.actual implementation algorithm two key ingredients needed:distance measure \\(d(\\boldsymbol , \\boldsymbol b)\\) two individual elementary data points \\(\\boldsymbol \\) \\(\\boldsymbol b\\).typically one following:Euclidean distance \\(d(\\boldsymbol , \\boldsymbol b) = \\sqrt{\\sum_{=1}^d ( a_i-b_i )^2} = \\sqrt{(\\boldsymbol -\\boldsymbol b)^T (\\boldsymbol -\\boldsymbol b)}\\)Squared Euclidean distance \\(d(\\boldsymbol , \\boldsymbol b) = (\\boldsymbol -\\boldsymbol b)^T (\\boldsymbol -\\boldsymbol b)\\)Manhattan distance \\(d(\\boldsymbol , \\boldsymbol b) = \\sum_{=1}^d | a_i-b_i |\\)Maximum norm \\(d(\\boldsymbol , \\boldsymbol b) = \\underset{\\\\{1, \\ldots, d\\}}{\\max} | a_i-b_i |\\)end, making correct choice distance require subject knowledge data!distance measure \\(d(, B)\\) two sets objects \\(=\\{\\boldsymbol a_1, \\boldsymbol a_2, \\ldots, \\boldsymbol a_{n_A} \\}\\) \\(B=\\{\\boldsymbol b_1, \\boldsymbol b_2, \\ldots, \\boldsymbol b_{n_B}\\}\\) size \\(n_A\\) \\(n_B\\), respectively.determine distance \\(d(, B)\\) two sets following measures often employed:complete linkage (max. distance): \\(d(, B) = \\underset{\\boldsymbol a_i \\, \\boldsymbol b_i \\B}{\\max} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)single linkage (min. distance): \\(d(, B) = \\underset{\\boldsymbol a_i \\, \\boldsymbol b_i \\B}{\\min} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)average linkage (avg. distance): \\(d(, B) = \\frac{1}{n_A n_B} \\sum_{\\boldsymbol a_i \\} \\sum_{\\boldsymbol b_i \\B} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"wards-clustering-method","chapter":"4 Unsupervised learning and clustering","heading":"4.2.3 Ward’s clustering method","text":"Another agglomerative hierarchical procedure Ward’s minimum variance approach 10 (see also 11). approach iteration two sets \\(\\) \\(B\\) merged lead smallest increase within-group variation. centroids two sets given \\(\\boldsymbol \\mu_A = \\frac{1}{n_A} \\sum_{\\boldsymbol a_i \\} \\boldsymbol a_i\\) \\(\\boldsymbol \\mu_B = \\frac{1}{n_B} \\sum_{\\boldsymbol b_i \\B} \\boldsymbol b_i\\).within-group sum squares group \\(\\) \n\\[\nw_A = \\sum_{\\boldsymbol a_i \\} (\\boldsymbol a_i -\\boldsymbol \\mu_A)^T (\\boldsymbol a_i -\\boldsymbol \\mu_A)\n\\]\ncomputed basis difference observations \\(\\boldsymbol a_i\\) relative mean \\(\\boldsymbol \\mu_A\\).\nHowever, since typically pairwise distances available don’t know group means formula can’t applied.\nFortunately, also possible compute \\(w_A\\) using pairwise differences using\n\\[\nw_A = \\frac{1}{n_A} \\sum_{\\boldsymbol a_i, \\boldsymbol a_j \\, < j} (\\boldsymbol a_i -\\boldsymbol a_j)^T (\\boldsymbol a_i -\\boldsymbol a_j)\n\\]\ntrick employed Ward’s clustering method constructing distance measure two sets \\(\\) \\(B\\) \n\\[\nd(, B) = w_{\\cup B} - w_A -w_B \\,\n\\]\nusing distance two elementary data points \\(\\boldsymbol \\) \\(\\boldsymbol b\\) squared Euclidean distance\n\\[\nd(\\boldsymbol , \\boldsymbol b) = (\\boldsymbol - \\boldsymbol b)^T (\\boldsymbol - \\boldsymbol b) \\, .\n\\]","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-to-swiss-banknote-data-set","chapter":"4 Unsupervised learning and clustering","heading":"4.2.4 Application to Swiss banknote data set","text":"data set reports 6 pysical measurements 200 Swiss bank notes. 200 notes\n100 genuine 100 counterfeit. measurements : length, left width, right width, bottom margin, top margin, diagonal length bank notes.Plotting first PCAs data shows indeed two well defined groups,\ngroups correspond precisely genuine counterfeit banknotes:now compare hierarchical clusterings Swiss bank note data using four different methods using Euclidean distance.interactive R Shiny web app analysis (also allows explore distance measures) available\nonline https://minerva..manchester.ac.uk/shiny/strimmer/hclust/ .Ward.D2 (=Ward’s method):Average linkage:Complete linkage:Single linkage:Result:four trees / hierarchical clusterings quite different!Ward.D2 method one finds correct grouping (except single error).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"assessment-of-the-uncertainty-of-hierarchical-clusterings","chapter":"4 Unsupervised learning and clustering","heading":"4.2.5 Assessment of the uncertainty of hierarchical clusterings","text":"practical application hierarchical clustering methods essential evaluate stability uncertainty obtained groupings. often done follows using “bootstrap”:Sampling replacement used generate number -called bootstrap data sets (say \\(B=200\\)) similar original one. Specifically, create new data matrices repeately randomly selecting columns (variables) original data matrix inclusion bootstrap data matrix. Note sample columns aim cluster samples.Subsequently, hierarchical clustering computed bootstrap data sets. result, now “ensemble” \\(B\\) bootstrap trees.Finally, analysis clusters (bipartions) shown bootstrap trees allows count clusters appear frequently, also appear less frequently. counts provide measure stability clusterings appearing original tree.Additionally, bootstrap tree can also compute consensus tree containing stable clusters. viewed “ensemble average” bootstrap trees.disadvantage procedure bootstrapping trees computationally expensive, original procedure already time consuming now needs repeated large number times.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"k-means-clustering","chapter":"4 Unsupervised learning and clustering","heading":"4.3 \\(K\\)-means clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"set-up","chapter":"4 Unsupervised learning and clustering","heading":"4.3.1 Set-up","text":"assume \\(K\\) groups (.e. \\(K\\) known advance).group \\(k \\\\{1, \\ldots, K\\}\\) assume group mean \\(\\boldsymbol \\mu_k\\).Aim: partition data points \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) \\(K\\) non-overlapping groups.\\(n\\) data points \\(\\boldsymbol x_i\\) assigned exactly one \\(K\\) groups.Maximise homogeneity within group (.e. group contain similar objects).Maximise heterogeneity different groups (.e group differ groups).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"algorithm","chapter":"4 Unsupervised learning and clustering","heading":"4.3.2 Algorithm","text":"running \\(K\\)-means get estimates \\(\\hat{\\boldsymbol \\mu}_k\\) group means,\nwell allocations \\(y_i \\\\{1, \\ldots, K\\}\\) data point \\(\\boldsymbol x_i\\) one classes.Initialisation:start algorithm \\(n\\) observations \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) randomly allocated equal probability one \\(K\\) groups. resulting assignment \\(y_1, \\ldots, y_n\\), \\(y_i=\\{1, \\ldots, K\\}\\).\n\\(G_k = \\{ | y_i = k\\}\\) denote set indices data points cluster \\(k\\), \\(n_k = | G_k |\\) \nnumber samples cluster \\(k\\).Iterative refinement:Estimate group means \n\\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n_k} \\sum_{\\G_k} \\boldsymbol x_i\n\\]Update group allocations \\(y_i\\). Specifically, assign data point \\(\\boldsymbol x_i\\) group \\(k\\) nearest \\(\\hat{\\boldsymbol \\mu}_k\\). distance measured terms Euclidean norm:\n\\[\n\\begin{split}\ny_i & = \\underset{k}{\\arg \\min} \\,  \\left| \\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k \\right|_2 \\\\\n      & = \\underset{k}{\\arg \\min} \\, \\left(\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k\\right)^T \\left(\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k\\right) \\\\\n\\end{split}\n\\]Steps 1 2 repeated algorithm converges (.e. group allocations don’t change ) specified upper limit iterations reached.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"properties","chapter":"4 Unsupervised learning and clustering","heading":"4.3.3 Properties","text":"\\(K\\)-means proposed 1950 1970s various authors diverse contexts 12.\nDespite simplicity \\(K\\)-means , perhaps surprisingly, effective clustering algorithm.\nmain reason close connection \\(K\\)-means probabilistic clustering based Gaussian mixture models (details see later section).Since clustering depends initialisation often useful run \\(K\\)-means several\ntimes different starting group allocations data points. Furthermore, non-random non-uniform\ninitialisations can lead improved faster convergence, see\nK-means++ algorithm.clusters constructed \\(K\\)-means linear boundaries thus form \nVoronoi tessellation around cluster means.\n, can explained close link \\(K\\)-means particular Gaussian mixture model.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"choosing-the-number-of-clusters","chapter":"4 Unsupervised learning and clustering","heading":"4.3.4 Choosing the number of clusters","text":"\\(K\\)-means algorithm run can assess homogeneity \nheterogeneity resulting clusters:total within-group sum squares \\(SSW\\) (R: tot.withinss), total unexplained sum squares:\n\\[\nSSW = \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)\n\\]\nquantity decreases \\(K\\) zero \\(K=n\\).\n\\(K\\)-means algorithm tries minimise quantity typically find local minimum rather global one.total within-group sum squares \\(SSW\\) (R: tot.withinss), total unexplained sum squares:\n\\[\nSSW = \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)\n\\]\nquantity decreases \\(K\\) zero \\(K=n\\).\n\\(K\\)-means algorithm tries minimise quantity typically find local minimum rather global one.-group sum squares \\(SSB\\) (R: betweenss), explained sum squares:\n\\[\nSSB = \\sum_{k=1}^K n_k (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)^T (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)\n\\]\n\\(\\hat{\\boldsymbol \\mu}_0 = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = \\frac{1}{n} \\sum_{k=1}^K n_k \\hat{\\boldsymbol \\mu}_k\\)\nglobal mean samples. \\(SSB\\) increases number clusters \\(K\\) \\(K=n\\) \nbecomes equal total sum squares \\(SST\\).-group sum squares \\(SSB\\) (R: betweenss), explained sum squares:\n\\[\nSSB = \\sum_{k=1}^K n_k (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)^T (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)\n\\]\n\\(\\hat{\\boldsymbol \\mu}_0 = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = \\frac{1}{n} \\sum_{k=1}^K n_k \\hat{\\boldsymbol \\mu}_k\\)\nglobal mean samples. \\(SSB\\) increases number clusters \\(K\\) \\(K=n\\) \nbecomes equal total sum squares \\(SST\\).total sum squares\n\\[\nSST = \\sum_{=1}^n (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)^T (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0) \\, .\n\\]\nconstruction \\(SST = SSB + SSW\\) \\(K\\) (.e. \\(SST\\) constant independent \\(K\\)).total sum squares\n\\[\nSST = \\sum_{=1}^n (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)^T (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0) \\, .\n\\]\nconstruction \\(SST = SSB + SSW\\) \\(K\\) (.e. \\(SST\\) constant independent \\(K\\)).Dividing sum squares sample size \\(n\\) get\\(T = \\frac{SST}{n}\\) total variation,\\(B = \\frac{SSB}{n}\\) explained variation \\(W = \\frac{SSW}{n}\\) total unexplained variation ,\\(T = B + W\\).order decide optimal number clusters run \\(K\\)-means different settings \\(K\\) choose smallest \\(K\\) explained variation \\(B\\) significantly worse compared clustering substantially larger \\(K\\) (see example ).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"k-medoids-aka-pam","chapter":"4 Unsupervised learning and clustering","heading":"4.3.5 \\(K\\)-medoids aka PAM","text":"closely related clustering method \\(K\\)-medoids PAM (“Partitioning Around Medoids”).works exactly like \\(K\\)-means, thatinstead estimated group means \\(\\hat{\\boldsymbol \\mu}_k\\) one member group selected representative (-called “medoid”)instead squared Euclidean distance dissimilarity measures also allowed.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-of-k-means-to-iris-data","chapter":"4 Unsupervised learning and clustering","heading":"4.3.6 Application of \\(K\\)-means to Iris data","text":"Scatter plots Iris data:R output \\(K\\)-means analysis known true number clusters specified (\\(K=3\\)) :corresponding total within-group sum squares (\\(SSW\\), tot.withinss)\nisand -group sum squares (\\(SSB\\), betweenss) isBy comparing known class assignments can assess accuracy \\(K\\)-means clustering:choosing \\(K\\) run \\(K\\)-means several times compute\nwithin cluster variation dependence \\(K\\):Thus, \\(K=3\\) clusters seem appropriate since explained variation significantly improve\n(unexplained variation significantly decrease) increase number clusters.","code":"\nkmeans.out = kmeans(X.iris, 3)\nkmeans.out## K-means clustering with 3 clusters of sizes 53, 47, 50\n## \n## Cluster means:\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1  -0.05005221 -0.88042696    0.3465767   0.2805873\n## 2   1.13217737  0.08812645    0.9928284   1.0141287\n## 3  -1.01119138  0.85041372   -1.3006301  -1.2507035\n## \n## Clustering vector:\n##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n##  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1\n##  [75] 1 2 2 2 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n## [112] 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 2 2 2 2 2 2 1 1 2 2 2 1 2 2 2 1 2 2 2 1 2\n## [149] 2 1\n## \n## Within cluster sum of squares by cluster:\n## [1] 44.08754 47.45019 47.35062\n##  (between_SS / total_SS =  76.7 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nkmeans.out$tot.withinss## [1] 138.8884\nkmeans.out$betweenss## [1] 457.1116\ntable(L.iris, kmeans.out$cluster)##             \n## L.iris        1  2  3\n##   setosa      0  0 50\n##   versicolor 39 11  0\n##   virginica  14 36  0"},{"path":"unsupervised-learning-and-clustering.html","id":"arbitrariness-of-cluster-labels-and-label-switching","chapter":"4 Unsupervised learning and clustering","heading":"4.3.7 Arbitrariness of cluster labels and label switching","text":"important realise unsupervised learning clustering labels group assigned arbitrary fashion.\nRecall \\(K\\) groups \\(K!\\) possibilities attach \nlabels, corresponding number permutations \\(K\\) groups.Thus, different runs clustering algorithm \\(K\\)-means may return clustering (groupings samples) different labels. phenomenon called “label switching”\nmakes difficult automatise comparison clusterings. particular, one simply rely automatically assigned group label, instead one needs compare actual members clusters.way resolve problem label switching relabel clusters using additional information, requiring samples specific groups\n(e.g.: sample 1 always group labelled “1”), /linking labels orderings constraints group characteristics (e.g.: group label “1” always smaller mean group label “2”).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"mixture-models","chapter":"4 Unsupervised learning and clustering","heading":"4.4 Mixture models","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"finite-mixture-model","chapter":"4 Unsupervised learning and clustering","heading":"4.4.1 Finite mixture model","text":"\\(K\\) groups / classes / categories, finite \\(K\\) known advance.Probability class \\(k\\): \\(\\text{Pr}(k) = \\pi_k\\) \\(\\sum_{k=1}^K \\pi_k = 1\\).class \\(k \\C= \\{1, \\ldots, K\\}\\) modelled distribution \\(F_k\\) parameters \\(\\boldsymbol \\theta_k\\).Density class \\(k\\): \\(f_k(\\boldsymbol x) = f(\\boldsymbol x| k)\\).conditional means variances class \\(k \\C\\) \\(\\text{E}(\\boldsymbol x| k) = \\boldsymbol \\mu_k\\) \\(\\text{Var}(\\boldsymbol x| k) = \\boldsymbol \\Sigma_k\\).resulting mixture density observed variable \\(\\boldsymbol x\\) \n\\[\nf_{\\text{mix}}(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\n\\]often one uses multivariate normal components \\(f_k(\\boldsymbol x) = N(\\boldsymbol x| \\boldsymbol \\mu_k, \\boldsymbol \\Sigma_k)\\) \\(\\\\ \\Longrightarrow\\) Gaussian mixture model (GMM)Mixture models fundamental just clustering many applications (e.g. classification).Note: don’t confuse mixture model mixed model (= terminology random effects regression model).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"total-mean-and-variance-of-a-multivariate-mixture-model","chapter":"4 Unsupervised learning and clustering","heading":"4.4.2 Total mean and variance of a multivariate mixture model","text":"Using law total expectation obtain mean mixture density\nmultivariate \\(\\boldsymbol x\\) follows:\n\\[\n\\begin{split}\n\\text{E}(\\boldsymbol x) & = \\text{E}(\\text{E}(\\boldsymbol x| k)) \\\\\n            & = \\text{E}( \\boldsymbol \\mu_k ) \\\\\n            &= \\sum_{k=1}^K \\pi_k \\boldsymbol \\mu_k \\\\\n            &= \\boldsymbol \\mu_0 \\\\\n\\end{split}\n\\]\nNote treat \\(\\boldsymbol x\\) well \\(k\\) random variables.Similarly, using law total variance compute marginal variance:\n\\[\n\\begin{split}\n\\underbrace{\\text{Var}(\\boldsymbol x)}_{\\text{total}} & =  \\underbrace{ \\text{Var}( \\text{E}(\\boldsymbol x| k )  )}_{\\text{explained / -group}} + \\underbrace{\\text{E}(\\text{Var}(\\boldsymbol x|k))}_{\\text{unexplained / expected within-group / pooled}} \\\\\n\\boldsymbol \\Sigma_0 & =  \\text{Var}( \\boldsymbol \\mu_k  ) + \\text{E}( \\boldsymbol \\Sigma_k )  \\\\\n               & =  \\sum_{k=1}^K \\pi_k (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T + \\sum_{k=1}^K \\pi_k \\boldsymbol \\Sigma_k  \\\\\n& =  \\boldsymbol \\Sigma_{\\text{explained}} +  \\boldsymbol \\Sigma_{\\text{unexplained}} \\\\\n\\end{split}\n\\]Thus, total variance decomposes explained\n(group) variance unexplained (expected within group, pooled) variance.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"total-variation","chapter":"4 Unsupervised learning and clustering","heading":"4.4.3 Total variation","text":"total variation given trace covariance matrix. decomposition total variation \n\\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol \\Sigma_0) & =  \\text{Tr}( \\boldsymbol \\Sigma_{\\text{explained}} )  + \\text{Tr}( \\boldsymbol \\Sigma_{\\text{unexplained}} )  \\\\\n& =  \\sum_{k=1}^K \\pi_k \\text{Tr}((\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T) + \\sum_{k=1}^K \\pi_k \\text{Tr}(\\boldsymbol \\Sigma_k)  \\\\\n& =  \\sum_{k=1}^K \\pi_k (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) + \\sum_{k=1}^K \\pi_k \\text{Tr}(\\boldsymbol \\Sigma_k)\\\\\n\\end{split}\n\\]\ncovariances replaced empirical estimates obtain\n\\(T=B+W\\) decomposition total variation familiar \\(K\\)-means:\n\\[T = \\text{Tr}\\left( \\hat{\\boldsymbol \\Sigma}_0 \\right)  =\n\\frac{1}{n} \\sum_{=1}^n (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)^T (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)\\]\n\\[B = \\text{Tr}( \\hat{\\boldsymbol \\Sigma}_{\\text{explained}} ) =  \\frac{1}{n} \\sum_{k=1}^K n_k (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)^T (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)\\]\n\\[W = \\text{Tr}( \\hat{\\boldsymbol \\Sigma}_{\\text{unexplained}} ) = \\frac{1}{n}  \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)\n\\]","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"univariate-mixture","chapter":"4 Unsupervised learning and clustering","heading":"4.4.4 Univariate mixture","text":"univariate mixture (\\(d=1\\)) \\(K=2\\) components get\n\\[\n\\mu_0 = \\pi_1 \\mu_1+ \\pi_2 \\mu_2 \\, ,\n\\]\n\\[\n\\sigma^2_{\\text{within}} = \\pi_1 \\sigma^2_1 + \\pi_2 \\sigma^2_2 = \\sigma^2_{\\text{pooled}}\\,,\n\\]\nalso known pooled variance, \n\\[\n\\begin{split}\n\\sigma^2_{\\text{}} &= \\pi_1 (\\mu_1 - \\mu_0)^2 + \\pi_2 (\\mu_2 - \\mu_0)^2 \\\\\n& =\\pi_1 \\pi_2^2 (\\mu_1 - \\mu_2)^2 + \\pi_2 \\pi_1^2 (\\mu_1 - \\mu_2)^2\\\\\n& = \\pi_1 \\pi_2 (\\mu_1 - \\mu_2)^2  \\\\\n& = \\left( \\frac{1}{\\pi_1} + \\frac{1}{\\pi_2}   \\right)^{-1} (\\mu_1 - \\mu_2)^2 \\\\\n\\end{split} \\,.\n\\]\nratio -group variance within-group variance proportional\n(factor \\(n\\)) squared pooled-variance \\(t\\)-score:\n\\[\n\\frac{\\sigma^2_{\\text{}}}{\\sigma^2_{\\text{within}}} =\n  \\frac{ (\\mu_1 - \\mu_2)^2}{ \\left(\\frac{1}{\\pi_1} + \\frac{1}{\\pi_2}   \\right)  \\sigma^2_{\\text{pooled}} }= \\frac{t_{\\text{pooled}}^2}{n}\n\\]\nfamiliar ANOVA (e.g. linear models course) recognise ratio \\(F\\)-score.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"example-of-mixtures","chapter":"4 Unsupervised learning and clustering","heading":"4.4.5 Example of mixtures","text":"Mixtures can take many different shapes forms, instructive study examples.\ninteractive tool visualise two component normal mixture available online \nR Shiny web app https://minerva..manchester.ac.uk/shiny/strimmer/mixture/ .first plot shows bimodal density mixture distribution consisting two normals \\(\\pi_1=0.7\\),\n\\(\\mu_1=-1\\), \\(\\mu_2=2\\) two variances equal 1 (\\(\\sigma^2_1 = 1\\) \\(\\sigma^2_2 = 1\\)).\ntwo components well-separated two clear modes. plot also shows density normal distribution total mean (\\(\\mu_0=-0.1\\)) variance (\\(\\sigma_0^2=2.89\\)) mixture distribution. Clearly total normal mixture density different.However, two-component mixtures can also unimodal. example, mean second component adjusted \\(\\mu_2=0\\) single mode total normal density \\(\\mu_0=-0.7\\) \\(\\sigma_0^2=1.21\\) now almost inistinguishable form mixture density.\nThus, case hard (even impossible) identify two peaks data.mixtures consider course multivariate.\nillustration, plot mixture two bivariate normals,\n\\(\\pi_1=0.7\\), \\(\\boldsymbol \\mu_1 = \\begin{pmatrix}-1 \\\\1 \\\\ \\end{pmatrix}\\),\n\\(\\boldsymbol \\Sigma_1 = \\begin{pmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\\\ \\end{pmatrix}\\),\n\\(\\boldsymbol \\mu_2 = \\begin{pmatrix}2.5 \\\\0.5 \\\\ \\end{pmatrix}\\) \\(\\boldsymbol \\Sigma_2 = \\begin{pmatrix} 1 & -0.7 \\\\ -0.7 & 1 \\\\ \\end{pmatrix}\\):","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"sampling-from-a-mixture-model","chapter":"4 Unsupervised learning and clustering","heading":"4.4.6 Sampling from a mixture model","text":"Assuming know sample component densities \\(f_k(\\boldsymbol x)\\) mixture model straightforward set procedure sampling mixture \\(f_{\\text{mix}}(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\\) .done two-step process:Draw categorical distribution parameter \\(\\boldsymbol \\pi=(\\pi_1, \\ldots, \\pi_K)^T\\):\n\\[\\boldsymbol z\\sim \\text{Cat}(\\boldsymbol \\pi)\\]\nvector \\(\\boldsymbol z= (z_1, \\ldots, z_K)^T\\) indicates hard group 0/1 allocation, components \\(z_{\\neq k}=0\\) except single entry \\(z_k=1\\).Draw categorical distribution parameter \\(\\boldsymbol \\pi=(\\pi_1, \\ldots, \\pi_K)^T\\):\n\\[\\boldsymbol z\\sim \\text{Cat}(\\boldsymbol \\pi)\\]\nvector \\(\\boldsymbol z= (z_1, \\ldots, z_K)^T\\) indicates hard group 0/1 allocation, components \\(z_{\\neq k}=0\\) except single entry \\(z_k=1\\).Subsequently, sample component \\(k\\) selected step 1:\n\\[\n\\boldsymbol x\\sim F_k\n\\]Subsequently, sample component \\(k\\) selected step 1:\n\\[\n\\boldsymbol x\\sim F_k\n\\]two-stage sampling approach also known hierarchical generative model mixture distribution. generative view useful simulating data mixture model also highlights role latent variable (class allocation).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"fitting-mixture-models-to-data-and-inferring-the-latent-states","chapter":"4 Unsupervised learning and clustering","heading":"4.5 Fitting mixture models to data and inferring the latent states","text":"following denote \\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)^T\\) data matrix containing observations \\(n\\) independent identically distributed samples\n\\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\), \\(\\boldsymbol y= (y_1, \\ldots, y_n)^T\\) associated group memberships, well asthe parameters \\(\\boldsymbol \\theta\\) Gaussian mixture model \\(\\boldsymbol \\theta= \\{\\boldsymbol \\pi, \\boldsymbol \\mu_1, \\ldots, \\boldsymbol \\mu_K, \\boldsymbol \\Sigma_1, \\ldots, \\boldsymbol \\Sigma_K\\}\\).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"observed-and-latent-variables","chapter":"4 Unsupervised learning and clustering","heading":"4.5.1 Observed and latent variables","text":"observe data mixture model collect samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).\nAssociated observed \\(\\boldsymbol x_i\\) corresponding underlying class allocation \\(y_1, \\ldots, y_n\\) \n\\(y_i\\) takes value \\(C = \\{1, \\ldots, K\\}\\). Crucially, class allocations \\(y_i\\) unknown \ndirectly observed, thus latent.joint density observed unobserved variables:\n\\[f(\\boldsymbol x, y) = f(\\boldsymbol x| y) \\text{Pr}(y) = f_y(\\boldsymbol x) \\pi_y\\]mixture density therefore marginal density arises joint density \\(f(\\boldsymbol x, y)\\)\nmarginalising discrete variable \\(y\\).Marginalisation: \\(f(\\boldsymbol x) = \\sum_{y \\C} f(\\boldsymbol x, y)\\)","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"complete-data-likelihood-and-observed-data-likelihood","chapter":"4 Unsupervised learning and clustering","heading":"4.5.2 Complete data likelihood and observed data likelihood","text":"know \\(\\boldsymbol y\\) advance, .e. know sample belongs particular group,\ncan construct complete data log-likelihood\nbased joint distribution \\(f(\\boldsymbol x, y) = \\pi_y f_y(\\boldsymbol x)\\).\nlog-likelihood \\(\\boldsymbol \\theta\\) given \\(\\boldsymbol X\\) \\(\\boldsymbol y\\) \n\\[\n\\log L(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol y) = \\sum_{=1}^n \\log f(\\boldsymbol x_i, y_i)  =  \\sum_{=1}^n  \\log \\left(\\pi_{y_i} f_{y_i}(\\boldsymbol x_i) \\right)\n\\]hand, typically know \\(\\boldsymbol y\\) therefore use\nmarginal mixture density \\(f(\\boldsymbol x)\\) construct observed data log-likelihood\n(sometimes also called incomplete data log-likelihood) \\(f(\\boldsymbol x| \\boldsymbol \\theta)\\) \n\\[\n\\begin{split}\n\\log L(\\boldsymbol \\theta| \\boldsymbol X) & =\\sum_{=1}^n \\log f(\\boldsymbol x_i | \\boldsymbol \\theta)\\\\\n& = \\sum_{=1}^n \\log \\left( \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x_i)  \\right)\\\\\n\\end{split}\n\\]observed data log-likelihood can also computed complete data likelihood\nfunction marginalising \\(\\boldsymbol y\\)\n\\[\n\\begin{split}\n\\log L(\\boldsymbol \\theta| \\boldsymbol X) &= \\log \\sum_{\\boldsymbol y}   L(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol y)\\\\\n&= \\log \\sum_{y_1, \\ldots, y_K}  \\prod_{=1}^n f(\\boldsymbol x_i, y_i)\\\\\n&= \\log \\prod_{=1}^n  \\sum_{k=1}^K f(\\boldsymbol x_i, k)\\\\\n& = \\sum_{=1}^n \\log \\left(  \\sum_{k=1}^K f(\\boldsymbol x_i, k)     \\right)\n\\end{split}\n\\]Clustering mixture model can viewed incomplete missing data problem\n(see also MATH27720 Statistics 2).Specifically, face problem offitting model using observed data \\(\\boldsymbol X\\) andsimultaneously inferring class allocations \\(\\boldsymbol y\\), .e. states latent variable.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"fitting-the-mixture-model-to-the-observed-data","chapter":"4 Unsupervised learning and clustering","heading":"4.5.3 Fitting the mixture model to the observed data","text":"large sample size \\(n\\) standard way fit mixture model\nemploy maximum likelihood find MLEs parameters mixture model.direct way fit mixture model maximum likelihood maximise observed data log-likelihood function regard \\(\\boldsymbol \\theta\\):\n\\[\n\\hat{\\boldsymbol \\theta}^{ML} = \\underset{\\boldsymbol \\theta}{\\arg \\max}\\,\\, \\log L(\\boldsymbol \\theta| \\boldsymbol X)\n\\]Unfortunately, practise evaluation optimisation log-likelihood function can difficult due number reasons:form observed data log-likelihood function prevents analytic simplifications\n(note sum inside logarithm) thus can difficult compute.symmetries due exchangeability cluster labels likelihood function multimodal thus hard optimise. Note also linked general\nproblem label switching non-identifiability cluster labels — see discussion \\(K\\)-means clustering.identifiability issues can arise (instance) two neighboring components mixture model largely overlapping thus close discriminated two different modes. words, difficult determine number classes.Furthermore, likelihood Gaussian mixture models singular one fitted covariance matrices becomes singular. However, can easily adressed using form regularisation (Bayes, penalised ML, etc.) simply requiring sufficient sample size per group.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"predicting-the-group-allocation-of-a-given-sample","chapter":"4 Unsupervised learning and clustering","heading":"4.5.4 Predicting the group allocation of a given sample","text":"probabilistic clustering aim infer latent states \\(y_1, \\ldots, y_n\\) observed samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).Assuming mixture model known (either advance fitting ) Bayes’ theorem allows predict probability observation \\(\\boldsymbol x_i\\) falls group \\(k \\\\{1, \\ldots, K\\}\\):\n\\[\nq_i(k) = \\text{Pr}(k | \\boldsymbol x_i) = \\frac{\\pi_k f_k(\\boldsymbol x_i ) }{ f(\\boldsymbol x_i)}\n\\]\nThus, \\(n\\) samples get probability mass function \n\\(K\\) classes \\(\\sum_{k=1}^K q_i(k)=1\\).posterior probabilities \\(q_i(k)\\) provide -called soft assignment sample \\(\\boldsymbol x_i\\) classes rather 0/1 hard assignment specific class (example \\(K\\)-means algorithm).obtain hard clustering infer probable latent state select class highest probability\n\\[\ny_i =\\underset{k}{\\arg \\max}\\,\\,q_i(k)\n\\]Thus, probabilistic clustering directly obtain assessment uncertainty class assignment sample \\(\\boldsymbol x_i\\) (case simple algorithmic clustering \\(K\\)-means). can use information check whether several classes equal similar probability. case, e.g., \\(\\boldsymbol x_i\\) lies near boundary two neighbouring classes.Using interactive Shiny app univariate normal component mixture (online https://minerva..manchester.ac.uk/shiny/strimmer/mixture/ ) can explore posterior probabilities class.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-of-gaussian-mixture-models","chapter":"4 Unsupervised learning and clustering","heading":"4.6 Application of Gaussian mixture models","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"choosing-the-number-of-classes","chapter":"4 Unsupervised learning and clustering","heading":"4.6.1 Choosing the number of classes","text":"application GMM need select suitable value \\(K\\), .e. number classes.Since GMMs operate likelihood framework can use penalised likelihood model selection criteria choose among different models (.e. GMMs different numbers classes).popular choices AIC (Akaike Information Criterion) BIC (Bayesian Information criterion) defined follows:\n\\[\\text{AIC}= -2 \\log L + 2 K \\]\n\\[\\text{BIC}= - 2 \\log L +K \\log(n)\\]order choose suitable model evaluate different models different \\(K\\) choose model minimises \\(\\text{AIC}\\) \\(\\text{BIC}\\)Note criteria complex models parameters (case groups) penalised simpler models order prevent overfitting.Another way choosing optimal numbers clusters cross-validation (see later chapter supervised learning).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-of-gmms-to-iris-flower-data","chapter":"4 Unsupervised learning and clustering","heading":"4.6.2 Application of GMMs to Iris flower data","text":"now explore application Gaussian mixture models Iris flower data set also investigated PCA\nK-means.First, fit GMM 3 clusters, using R software “mclust” 13.“mclust” software used following model fitting mixture:“VVV” name used “mclust” software model\nallowing individual\nunrestricted covariance matrix \\(\\boldsymbol \\Sigma_k\\) class \\(k\\).GMM substantially lower misclassification error compared \\(K\\)-means number clusters:Note “mclust” BIC criterion defined opposite sign (\\(\\text{BIC}_{\\text{mclust}} = 2 \\log L -K \\log(n)\\)), thus need find maximum value rather smallest value.compute BIC various numbers groups find model best \\(\\text{BIC}_{\\text{mclust}}\\) model 2 clusters model 3 cluster nearly good BIC:","code":"\ndata(iris)\nX.iris = scale((iris[, 1:4]), scale=TRUE) # center and standardise\nL.iris = iris[, 5]\n\nlibrary(\"mclust\")\ngmm3 = Mclust(X.iris, G=3, verbose=FALSE)\nplot(gmm3, what=\"classification\")\ngmm3$modelName## [1] \"VVV\"\ntable(gmm3$classification, L.iris)##    L.iris\n##     setosa versicolor virginica\n##   1     50          0         0\n##   2      0         45         0\n##   3      0          5        50"},{"path":"unsupervised-learning-and-clustering.html","id":"the-em-algorithm","chapter":"4 Unsupervised learning and clustering","heading":"4.7 The EM algorithm","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"motivation","chapter":"4 Unsupervised learning and clustering","heading":"4.7.1 Motivation","text":"discussed , observed data log-likelihood can difficult maximise directly due form log marginal likelihood. Intriguingly, possible optimise indirectly using complete data log-likelihood, ’s also allows many cases analytic expression maximisation step.method called EM algorithm formally proposed described \nArthur Dempster (1929–)\nothers 197714 algorithm already know prior.\niteratively estimates parameters mixture model parameters latent states. key idea behind EM algorithm capitalise simplicity complete data likelihood obtain estimates \\(\\boldsymbol \\theta\\) imputing missing group allocations subsequently iteratively refining imputations estimates \\(\\boldsymbol \\theta\\).precisely, EM (=expectation-maximisation) algorithm alternate betweenStep 1) updating soft allocations sample using current estimate parameters \\(\\boldsymbol \\theta\\) (obtained step 2)Step 1) updating soft allocations sample using current estimate parameters \\(\\boldsymbol \\theta\\) (obtained step 2)Step 2) updating parameter estimates maximising expected complete data log-likelihood. expectation taken regard distribution latent states (obtained step 1). Thus\ncomplete data log-likelihood averaged soft class assignments. exponential\nfamily (e.g. distributions group normal) maximisation expected complete data log-likelihood can even done analytically.Step 2) updating parameter estimates maximising expected complete data log-likelihood. expectation taken regard distribution latent states (obtained step 1). Thus\ncomplete data log-likelihood averaged soft class assignments. exponential\nfamily (e.g. distributions group normal) maximisation expected complete data log-likelihood can even done analytically.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"the-em-algorithm-1","chapter":"4 Unsupervised learning and clustering","heading":"4.7.2 The EM algorithm","text":"Specifically, EM algorithm proceeds follows:Initialisation:Start guess parameters \\(\\hat{\\boldsymbol \\theta}^{(1)}\\), continue “E” Step, Part .Alternatively, start guess soft allocations sample \\(q_i(k)^{(1)}\\), collected matrix \\(\\boldsymbol Q^{(1)}\\), continue “E” Step, Part B.\nmay derived prior information, e.g., running \\(K\\)-means. Caveat: particular initialisations correspond invariant states hence avoided (see ).E “expectation” stepPart : Use Bayes’ theorem compute new probabilities allocation class \\(k\\) samples \\(\\boldsymbol x_i\\):\n\\[\nq_i(k)^{(b+1)} \\leftarrow \\frac{ \\hat{\\pi}_k^{(b)} f_k(\\boldsymbol x_i | \\hat{\\boldsymbol \\theta}^{(b)})    }{  f(\\boldsymbol x_i |\\hat{\\boldsymbol \\theta}^{(b)} )  }\n\\]\nNote obtain \\(q_i(k)^{(b+1)}\\) current estimate\n\\(\\hat{\\boldsymbol \\theta}^{(b)}\\) parameters mixture model required.Part : Use Bayes’ theorem compute new probabilities allocation class \\(k\\) samples \\(\\boldsymbol x_i\\):\n\\[\nq_i(k)^{(b+1)} \\leftarrow \\frac{ \\hat{\\pi}_k^{(b)} f_k(\\boldsymbol x_i | \\hat{\\boldsymbol \\theta}^{(b)})    }{  f(\\boldsymbol x_i |\\hat{\\boldsymbol \\theta}^{(b)} )  }\n\\]\nNote obtain \\(q_i(k)^{(b+1)}\\) current estimate\n\\(\\hat{\\boldsymbol \\theta}^{(b)}\\) parameters mixture model required.Part B: Construct expected complete data log-likelihood function \\(\\boldsymbol \\theta\\) using soft allocations \\(q_i(k)^{(b+1)}\\)\ncollected matrix \\(\\boldsymbol Q^{(b+1)}\\):\n\\[\nG(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)} ) = \\sum_{=1}^n \\sum_{k=1}^K q_i(k)^{(b+1)}  \\log \\left( \\pi_k f_k(\\boldsymbol x_i) \\right)\n\\]\nNote case soft allocations \\(\\boldsymbol Q^{(b+1)}\\) turn hard 0/1 allocations \n\\(G(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)})\\) becomes equivalent complete data log-likelihood.Part B: Construct expected complete data log-likelihood function \\(\\boldsymbol \\theta\\) using soft allocations \\(q_i(k)^{(b+1)}\\)\ncollected matrix \\(\\boldsymbol Q^{(b+1)}\\):\n\\[\nG(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)} ) = \\sum_{=1}^n \\sum_{k=1}^K q_i(k)^{(b+1)}  \\log \\left( \\pi_k f_k(\\boldsymbol x_i) \\right)\n\\]\nNote case soft allocations \\(\\boldsymbol Q^{(b+1)}\\) turn hard 0/1 allocations \n\\(G(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)})\\) becomes equivalent complete data log-likelihood.M “maximisation” step — Maximise expected complete data log-likelihood update estimates mixture model parameters:\n\\[\n\\hat{\\boldsymbol \\theta}^{(b+1)} \\leftarrow \\arg \\max_{\\boldsymbol \\theta}  G(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)} )\n\\]M “maximisation” step — Maximise expected complete data log-likelihood update estimates mixture model parameters:\n\\[\n\\hat{\\boldsymbol \\theta}^{(b+1)} \\leftarrow \\arg \\max_{\\boldsymbol \\theta}  G(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol Q^{(b+1)} )\n\\]Continue 2) “E” Step series \\(\\hat{\\boldsymbol \\theta}^{(1)}, \\hat{\\boldsymbol \\theta}^{(2)}, \\hat{\\boldsymbol \\theta}^{(3)}, \\ldots\\) converged.Continue 2) “E” Step series \\(\\hat{\\boldsymbol \\theta}^{(1)}, \\hat{\\boldsymbol \\theta}^{(2)}, \\hat{\\boldsymbol \\theta}^{(3)}, \\ldots\\) converged.Crucially, maximisation expected complete data log-likelihood typically much easier\nmaximisation observed data log-likelihood, many cases even analytically tractable, case EM algorithm often preferred direct\nmaximisation observed data log-likelihood.Note avoid singularities expected log-likelihood function \nmay need adopt regularisation (.e. penalised maximum likelihood Bayesian learning) estimating parameters M-step.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"em-algorithm-for-multivariate-normal-mixture-model","chapter":"4 Unsupervised learning and clustering","heading":"4.7.3 EM algorithm for multivariate normal mixture model","text":"now consider EM algorithm applied case conditional group distributions\nnormal, .e. applied Gaussian mixture model (GMM).\ncase two iterative steps EM algorithm can expressed follows:E-step:Update soft allocations:\n\\[\nq_i(k)^{(b+1)} = \\frac{ \\hat{\\pi}_k^{(b)} N(\\boldsymbol x_i | \\hat{\\boldsymbol \\mu}_k^{(b)}, \\hat{\\boldsymbol \\Sigma}_k^{(b)}) }{\n\\hat{f}^{(b)} \\left(\\boldsymbol x_i |  \n\\hat{\\pi}_1^{(b)}, \\ldots, \\hat{\\pi}_K^{(b)},\n\\hat{\\boldsymbol \\mu}_1^{(b)}, \\ldots, \\hat{\\boldsymbol \\mu}_K^{(b)},\n\\hat{\\boldsymbol \\Sigma}_1^{(b)}, \\ldots, \\hat{\\boldsymbol \\Sigma}_K^{(b)} \\right)  }\n\\]Correspondingly, number samples assigned class \\(k\\) current step \n\\[\nn_k^{(b+1)} = \\sum_{=1}^n q_i(k)^{(b+1)}\n\\]\nNote necessarily integer soft allocations samples groups.expected complete data log-likelihood becomes:\n\\[\nG(\\pi_1, \\ldots \\pi_K,\\boldsymbol \\mu_1, \\ldots, \\boldsymbol \\mu_K,  \\boldsymbol \\Sigma_1, \\ldots, \\boldsymbol \\Sigma_K | \\boldsymbol X, \\boldsymbol Q^{(b+1)} ) =\n\\sum_{=1}^n \\sum_{k=1}^K q_i(k)^{(b+1)}  \\log \\left( \\pi_k f_k(\\boldsymbol x_i) \\right)\n\\]\n\n\\[\n\\log \\left( \\pi_k f_k(\\boldsymbol x_i) \\right) =  -\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma_k^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) -\\frac{1}{2} \\log \\det(\\boldsymbol \\Sigma_k) +\\log(\\pi_k)\n\\]\n(Remark: encounter expression next chapter discussing\nquadratic discriminant analysis).M-step:maximisation expected complete data log-likelihood can \ndone analytically weighted version conventional single group multivariate\nnormal log-likelihood. resulting estimators also weighted variants\nusual MLEs.updated estimates group probabilities \n\\[\n\\hat{\\pi}_k^{(b+1)} = \\frac{n_k^{(b+1)}}{n}\n\\]\nupdated estimates means \n\\[\n\\hat{\\boldsymbol \\mu}_k^{(b+1)} = \\frac{1}{n_k^{(b+1)}} \\sum_{=1}^n q_i(k)^{(b+1)} \\boldsymbol x_i\n\\]\nupdated covariance estimates \n\\[\n\\hat{\\boldsymbol \\Sigma}_k^{(b+1)} =  \\frac{1}{n_k^{(b+1)}} \\sum_{=1}^n q_i(k)^{(b+1)} \\left( \\boldsymbol x_i -\\boldsymbol \\mu_k^{(b+1)}\\right)   \\left( \\boldsymbol x_i -\\boldsymbol \\mu_k^{(b+1)}\\right)^T\n\\]Note \\(q_i(k)\\) hard allocation (\\(\\) one class weight 1 others weight 0) estimators reduce usual MLEs.Worksheet 8 can find simple R implementation EM algorithm \nunivariate normal mixtures.Similar analytical expressions\ncan also found general mixtures components\nexponential families. mentioned one advantages \nusing EM algorithm.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"convergence-and-invariant-states","chapter":"4 Unsupervised learning and clustering","heading":"4.7.4 Convergence and invariant states","text":"mild assumptions EM algorithm guaranteed monotonically converge local optima observed data log-likelihood 15. Thus series \\(\\hat{\\boldsymbol \\theta}^{(1)}, \\hat{\\boldsymbol \\theta}^{(2)}, \\hat{\\boldsymbol \\theta}^{(3)}, \\ldots\\) converges estimate \\(\\hat{\\boldsymbol \\theta}\\) found maximising observed data log-likelihood.\nHowever, speed convergence EM algorithm can sometimes slow, also situations convergence \\(\\hat{\\boldsymbol \\theta}\\) EM algorithm remains invariant state.example invariant state Gaussian mixture model uniform initialisation latent variables \\(q_i(k) = \\frac{1}{K}\\), \\(K\\) number classes.\nget M step \\(n_k = \\frac{n}{K}\\) parameter estimates\n\\[\n\\hat{\\pi}_k = \\frac{1}{K}\n\\]\n\\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = \\bar{\\boldsymbol x}\n\\]\n\\[\n\\hat{\\boldsymbol \\Sigma}_k = \\frac{1}{n}  \\sum_{=1}^n ( \\boldsymbol x_i -\\bar{\\boldsymbol x})   ( \\boldsymbol x_i -\\bar{\\boldsymbol x})^T = \\hat{\\boldsymbol \\Sigma}\n\\]\nCrucially, none actually depend group \\(k\\)! Thus, E step next soft allocations determined leads \n\\[\nq_i(k) = \\frac{ \\frac{1}{K} N(\\boldsymbol x_i | \\bar{\\boldsymbol x}, \\hat{\\boldsymbol \\Sigma} ) }{ \\sum_{j=1}^K  \\frac{1}{K} N(\\boldsymbol x_i | \\bar{\\boldsymbol x}, \\hat{\\boldsymbol \\Sigma} )  } = \\frac{1}{K}\n\\]\none cycle EM algorithm arrive soft allocation started , algorithm trapped invariant state! Therefore uniform initialisation clearly avoided!explore effect practise Worksheet 8.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"connection-with-the-k-means-clustering-method","chapter":"4 Unsupervised learning and clustering","heading":"4.7.5 Connection with the \\(K\\)-means clustering method","text":"\\(K\\)-means algorithm closely related EM algorithm \nprobabilistic clustering specific Gaussian mixture models.Specifically, assume simplified model probabilities \\(\\pi_k\\) classes equal (.e. \\(\\pi_k=\\frac{1}{K}\\)) covariances \\(\\boldsymbol \\Sigma_k\\) spherical form \\(\\sigma^2 \\boldsymbol \\). Thus, covariance depend group, correlation variables variance variables .First, consider “E” step. Using mixture model \nsoft assignment class allocation becomes\n\\[\n\\log( q_i(k) ) = -\\frac{1}{2 \\sigma^2} (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k) +  \\text{const}\n\\]\n\\(\\text{const}\\) depend \\(k\\). can turned hard class allocation \n\\[\n\\begin{split}\ny_i &= \\underset{k}{\\arg \\max} \\log( q_i(k) ) \\\\\n          & = \\underset{k}{\\arg \\min}  (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)\\\\\n\\end{split}\n\\]\nexactly \\(K\\)-means rule dallocate samples groups.Second, “M” step compute parameters model. class allocations\nhard expected log-likelihood becomes observed data likelihood \nMLE group mean average samples group.Thus, \\(K\\)-means can viewed EM type algorithm provide hard classification\nbased simple restricted Gaussian mixture model.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"why-the-em-algorithm-works-an-entropy-point-of-view","chapter":"4 Unsupervised learning and clustering","heading":"4.7.6 Why the EM algorithm works — an entropy point of view","text":"iterative (soft) imputation latent states EM algorithm intuitively clear.However, order get better understanding EM need demonstratewhy expected observed log-likelihood needs maximised\nrather , e.g., observed log-likelihood hard allocations, andthat applying \nEM algorithm versus directly maximising marginal likelihood lead \nfitted mixture model.Intriguingly, aspects EM algorithm easiest understand \nentropy point view, .e. considering entropy foundations maximum likelihood Bayesian learning\n— details see MATH27720 Statistics 2. Specifically, reason need using expectation link likelihood\ncross-entropy (also defined expectation). Furthermore, EM\nalgorithm example using ELBO (“evidence lower bound”) successively\napproximate maximised marginal log-likelihood, bound getting better\nstep.First, recall method maximum likelihood results minimising \nKL divergence\nempirical distribution \\(Q_{\\boldsymbol x}\\) representing observations \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) model family \\(F_{\\boldsymbol x}^{\\boldsymbol \\theta}\\)\nparameters \\(\\boldsymbol \\theta\\):\n\\[\n\\hat{\\boldsymbol \\theta}^{ML} =  \\underset{\\boldsymbol \\theta}{\\arg \\min}\\,\\, D_{\\text{KL}}(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta})\n\\]\nKL divergence decomposes cross-entropy entropy part\n\\[\nD_{\\text{KL}}(Q_{\\boldsymbol x}, F_{\\boldsymbol \\theta}) = H(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta})- H(Q_{\\boldsymbol x})\n\\]\nhence minimising KL divergence regard \\(\\boldsymbol \\theta\\) maximising function\n\\[\n\\begin{split}\n-n H(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta}) &= n \\text{E}_{Q_{\\boldsymbol x}}( \\log f(\\boldsymbol x| \\boldsymbol \\theta)  ) \\\\\n&= \\sum_{=1}^n  \\log f(\\boldsymbol x_i | \\boldsymbol \\theta)\\\\\n&= \\log L(\\boldsymbol \\theta| \\boldsymbol X)\\\\\n\\end{split}\n\\]\nindeed observed data log-likelihood \\(\\boldsymbol \\theta\\).Second, recall chain rule KL divergence. Specifically, KL divergence \njoint model forms upper bound KL divergence marginal model:\n\\[\n\\begin{split}\nD_{\\text{KL}}(Q_{\\boldsymbol x,y} , F_{\\boldsymbol x, y}^{\\boldsymbol \\theta}) &= D_{\\text{KL}}(Q_{\\boldsymbol x} , F_{\\boldsymbol x}^{\\boldsymbol \\theta}) + \\underbrace{  D_{\\text{KL}}(Q_{y| \\boldsymbol x} , F_{y|\\boldsymbol x}^{\\boldsymbol \\theta})   }_{\\geq 0}\\\\\n&\\geq D_{\\text{KL}}(Q_{\\boldsymbol x} , F_{\\boldsymbol x}^{\\boldsymbol \\theta})\n\\end{split}\n\\]\nUnlike \\(\\boldsymbol x\\) observations latent state \\(y\\). Nonetheless, can model joint distribution \\(Q_{\\boldsymbol x, y} =Q_{\\boldsymbol x} Q_{y|\\boldsymbol x}\\) assuming \ndistribution \\(Q_{y|\\boldsymbol x}\\) latent variable.EM algorithm arises iteratively decreasing joint KL divergence \\(D_{\\text{KL}}(Q_{\\boldsymbol x} Q_{y|\\boldsymbol x} , F_{\\boldsymbol x, y}^{\\boldsymbol \\theta})\\) regard \\(Q_{y|\\boldsymbol x}\\) \\(\\boldsymbol \\theta\\):“E” Step: keeping \\(\\boldsymbol \\theta\\) fixed vary \\(Q_{y|\\boldsymbol x}\\) minimise joint KL divergence. minimum reached \\(D_{\\text{KL}}(Q_{y| \\boldsymbol x} , F_{y|\\boldsymbol x}^{\\boldsymbol \\theta}) = 0\\).\ncase \\(Q_{y| \\boldsymbol x} = F_{y|\\boldsymbol x}^{\\boldsymbol \\theta}\\), .e. \nlatent distribution \\(Q_{y| \\boldsymbol x}\\) representing soft allocations computed\nconditioning, .e. using Bayes’ theorem.“E” Step: keeping \\(\\boldsymbol \\theta\\) fixed vary \\(Q_{y|\\boldsymbol x}\\) minimise joint KL divergence. minimum reached \\(D_{\\text{KL}}(Q_{y| \\boldsymbol x} , F_{y|\\boldsymbol x}^{\\boldsymbol \\theta}) = 0\\).\ncase \\(Q_{y| \\boldsymbol x} = F_{y|\\boldsymbol x}^{\\boldsymbol \\theta}\\), .e. \nlatent distribution \\(Q_{y| \\boldsymbol x}\\) representing soft allocations computed\nconditioning, .e. using Bayes’ theorem.“M” Step: keeping \\(Q_{y| \\boldsymbol x}\\) fixed joint KL divergence minimised regard \\(\\boldsymbol \\theta\\). equivalent maximising \nfunction \\(\\sum_{k=1}^K \\sum_{=1}^n q(k | \\boldsymbol x_i) \\log f(\\boldsymbol x_i, k| \\boldsymbol \\theta)\\) \nindeed expected complete data log-likelihood.“M” Step: keeping \\(Q_{y| \\boldsymbol x}\\) fixed joint KL divergence minimised regard \\(\\boldsymbol \\theta\\). equivalent maximising \nfunction \\(\\sum_{k=1}^K \\sum_{=1}^n q(k | \\boldsymbol x_i) \\log f(\\boldsymbol x_i, k| \\boldsymbol \\theta)\\) \nindeed expected complete data log-likelihood.Thus “E” step first argument KL divergence optimised (“” projection)\n“M” step second argument optimised (“M” projection).\nsteps joint KL divergence always decreases never increases. Furthermore, end “E” step joint KL divergence equals marginal KL divergence. Thus, procedure implicitly minimises marginal KL divergence well, hence EM maximises marginal log-likelihood.alternative way look EM algorithm terms cross-entropy.\nUsing \\(H( Q_{\\boldsymbol x,y}) = H(Q_{\\boldsymbol x}) + H(Q_{y| \\boldsymbol x} )\\)\ncan rewrite upper bound joint KL divergence equivalent lower\nbound \\(n\\) times negative marginal cross-entropy:\n\\[\n\\begin{split}\n- n H(Q_{\\boldsymbol x}, F_{\\boldsymbol x}^{\\boldsymbol \\theta}) &= \\underbrace{ -n  H(Q_{\\boldsymbol x} Q_{y| \\boldsymbol x} , F_{\\boldsymbol x, y}^{\\boldsymbol \\theta})  + n H(Q_{y| \\boldsymbol x} )}_{\\text{lower bound, ELBO}}  + \\underbrace{ n D_{\\text{KL}}(Q_{y| \\boldsymbol x} , F_{y|\\boldsymbol x}^{\\boldsymbol \\theta})}_{\\geq 0}\\\\\n& \\geq {\\cal F}\\left( Q_{\\boldsymbol x}, Q_{y| \\boldsymbol x},  F_{\\boldsymbol x, y}^{\\boldsymbol \\theta}\\right)\\\\\n\\end{split}\n\\]\nlower bound known “ELBO” (“evidence lower bound”). EM algorithm arises \niteratively maximising ELBO \\(\\cal F\\) regard \\(Q_{y| \\boldsymbol x}\\) (“E” step”) \\(\\boldsymbol \\theta\\) (“M” step).entropy interpretation EM algorithm due Csiszàr Tusnàdy (1984)16 ELBO interpretation introduced Neal Hinton (1998)17.","code":""},{"path":"supervised-learning-and-classification.html","id":"supervised-learning-and-classification","chapter":"5 Supervised learning and classification","heading":"5 Supervised learning and classification","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"aims-of-supervised-learning","chapter":"5 Supervised learning and classification","heading":"5.1 Aims of supervised learning","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"supervised-learning-vs.-unsupervised-learning","chapter":"5 Supervised learning and classification","heading":"5.1.1 Supervised learning vs. unsupervised learning","text":"Unsupervised learning:Starting point:unlabelled data \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).Aim: find labels \\(y_1, \\ldots, y_n\\) attach sample \\(\\boldsymbol x_i\\).discrete labels \\(y\\) unsupervised learning called clustering.Supervised learning:Starting point:labelled training data: \\(\\{\\boldsymbol x_1^{\\text{train}}, y_1^{\\text{train}}\\}\\),\n\\(\\ldots\\), \\(\\{\\boldsymbol x_n^{\\text{train}}, y_n^{\\text{train}} \\}\\)addition, unlabelled test data: \\(\\boldsymbol x^{\\text{test}}\\)Aim: use training data learn function, say \\(h(\\boldsymbol x)\\),\npredict label corresponding test data.\npredictor function may provide soft (probabilistic) assignment\nhard assignment class label test sample.\\(y\\) discrete supervised learning called classification.\ncontinuous \\(y\\) label called response supervised learning becomes regression.Thus, supervised learning two-step procedure:Learn predictor function \\(h(\\boldsymbol x)\\) using training data \\(\\boldsymbol x_i^{\\text{train}}\\) plus labels \\(y_i^{\\text{train}}\\).Predict label \\(y^{\\text{test}}\\) test data \\(\\boldsymbol x^{\\text{test}}\\) using estimated classifier function:\n\\(\\hat{y}^{\\text{test}} = \\hat{h}(\\boldsymbol x^{\\text{test}})\\).","code":""},{"path":"supervised-learning-and-classification.html","id":"terminology","chapter":"5 Supervised learning and classification","heading":"5.1.2 Terminology","text":"function \\(h(\\boldsymbol x)\\) predicts class \\(y\\) called classifier.many types classifiers, focus primarily probabilistic classifiers\n(.e. output probabilities possible class/label).challenge find classifier thatexplains current training data well andthat also generalises well future unseen data.Note relatively easy find predictor explains training data especially high dimensions (.e. many predictors) often overfitting predictor generalise well!decision boundary classes defined set \\(\\boldsymbol x\\) \nclass assignment predictor \\(h(\\boldsymbol x)\\) switches one class another.general, simple decision boundaries preferred complex decision boundaries avoid overfitting.commonly used probabilistic methods classifications:QDA (quadratic discriminant analysis)LDA (linear discriminant analysis)DDA (diagonal discriminant analysis),Naive Bayes classificationlogistic regressionDepending classifiers trainined many variations\nmethods, e.g. Fisher discriminant analysis, regularised LDA,\nshrinkage disciminant analysis etc.Common non-probabilistic methods classification include:\\(k\\)-NN (Nearest Neigbors)SVM (support vector machine),random forestneural networksNeural networks may fact also counted probabilistic models\nessentially high-dimensional complex nonlinear\nregression models. Just like linear regression model can \nfitted “least squares” without assuming explicit probabilistic model\nneural networks also often trained optimising loss function.","code":""},{"path":"supervised-learning-and-classification.html","id":"bayesian-discriminant-rule-or-bayes-classifier","chapter":"5 Supervised learning and classification","heading":"5.2 Bayesian discriminant rule or Bayes classifier","text":"Bayes classifiers based mixture models:\\(K\\) groups \\(K\\) prespecifiedeach group distribution \\(F_k\\) parameters \\(\\boldsymbol \\theta_k\\)density class \\(f_k(\\boldsymbol x) = f(\\boldsymbol x| k)\\).prior probability group \\(k\\) \\(\\text{Pr}(k) = \\pi_k\\) \\(\\sum_{k=1}^K \\pi_k = 1\\)marginal density mixture \\(f(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\\)moment assume anything particular conditional densities \\(f_k(\\boldsymbol x)\\)\nlater (see following section) focus normal densities hence normal classifiers.posterior probability group \\(k\\) given \n\\[\n\\text{Pr}(k | \\boldsymbol x) = \\frac{\\pi_k f_k(\\boldsymbol x) }{ f(\\boldsymbol x)}\n\\]provides “soft” classification\n\\[\\boldsymbol h(\\boldsymbol x^{\\text{test}}) = (\\text{Pr}(k=1 | \\boldsymbol x^{\\text{test}}),\\ldots, \\text{Pr}(k=K | \\boldsymbol x^{\\text{test}})   )^T\\]\npossible class \\(k \\\\{ 1, \\ldots, K\\}\\) assigned probability label \ntest sample \\(\\boldsymbol x\\).discriminant function defined logarithm posterior probability:\n\\[\nd_k(\\boldsymbol x) = \\log \\text{Pr}(k | \\boldsymbol x) = \\log \\pi_k  + \\log f_k(\\boldsymbol x)  - \\log f(\\boldsymbol x)\n\\]\nSince use \\(d_k\\) compare different classes \\(k\\) can\nsimplify discriminant function dropping constant terms depend \\(k\\) — term \\(\\log f(\\boldsymbol x)\\). Hence get Bayes discriminant function\n\\[\nd_k(\\boldsymbol x) = \\log \\pi_k + \\log f_k(\\boldsymbol x) \\,.\n\\]subsequent “hard” classification \\(h(\\boldsymbol x^{\\text{test}})\\) select group/label value discriminant function maximised:\n\\[\n\\hat{y}^{\\text{test}} = h(\\boldsymbol x^{\\text{test}}) = \\arg \\max_k d_k(\\boldsymbol x^{\\text{test}}) \\,.\n\\]already encountered Bayes classifier EM algorithm predict state\nlatent variables (soft assignment) \\(K\\)-means algorithm (hard assignment).discriminant functions \\(d_k(\\boldsymbol x)\\) can mapped back probabilistic class assignment using softargmax function (also known softmax function):\n\\[\n\\text{Pr}(k | \\boldsymbol x) =\n\\frac{\\exp( d_k(\\boldsymbol x) )}{\\sum_{c=1}^K \\exp( d_c(\\boldsymbol x) ) }  \\,.\n\\]\npractise calculated \n\\[\n\\text{Pr}(k | \\boldsymbol x) =\n\\frac{\\exp( d_k(\\boldsymbol x) - d_{\\max}(\\boldsymbol x) ) }{\\sum_{c=1}^K \\exp( d_c(\\boldsymbol x) - d_{\\max}(\\boldsymbol x) ) } \\,.\n\\]\nsubtracting \\(d_{\\max}(\\boldsymbol x) = \\max\\{ d_1(\\boldsymbol x), \\ldots, d_K(\\boldsymbol x) \\}\\)\ndiscriminant functions, thus standardising maximum discriminant functions zero,\navoids numerical overflow problems computing exponential function computer.","code":""},{"path":"supervised-learning-and-classification.html","id":"normal-bayes-classifier","chapter":"5 Supervised learning and classification","heading":"5.3 Normal Bayes classifier","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"quadratic-discriminant-analysis-qda-and-gaussian-assumption","chapter":"5 Supervised learning and classification","heading":"5.3.1 Quadratic discriminant analysis (QDA) and Gaussian assumption","text":"Quadratic discriminant analysis (QDA) special case Bayes classifier \ngroup-specific densities multivariate normal \\(f_k(\\boldsymbol x) = N(\\boldsymbol x| \\boldsymbol \\mu_k, \\boldsymbol \\Sigma_k)\\). Note particular group \\(k \\\\{1, \\ldots, K\\}\\) covariance matrix \\(\\boldsymbol \\Sigma_k\\).calculation leads discriminant function QDA:\n\\[\nd_k^{QDA}(\\boldsymbol x) = -\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma_k^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) -\\frac{1}{2} \\log \\det(\\boldsymbol \\Sigma_k) +\\log(\\pi_k)\n\\]number noteworthy things :terms dropped depend \\(k\\), \\(-\\frac{d}{2}\\log( 2\\pi)\\).Note appearance squared Mahalanobis distance \\(\\boldsymbol x\\) \\(\\boldsymbol \\mu_k\\): \\(d^{\\text{Mahalanobis}}(\\boldsymbol x, \\boldsymbol \\mu| \\boldsymbol \\Sigma)^2 = (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)\\).QDA discriminant function quadratic \\(\\boldsymbol x\\) - hence name!\nimplies decision boundaries QDA classification quadratic (.e. parabolas two dimensional settings).Gaussian models specifically can useful multiply discriminant function -2 get rid factor \\(-\\frac{1}{2}\\), note case need find minimum discriminant function rather maximum:\n\\[\nd_k^{QDA (v2)}(\\boldsymbol x) =  (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma_k^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) + \\log \\det(\\boldsymbol \\Sigma_k)  -2 \\log(\\pi_k)\n\\]\nliterature find versions Gaussian discriminant functions need check carefully convention used. following use first version .Decision boundaries QDA classifier can either linear nonlinear (quadratic curve).\ndecision boundary two classes \\(\\) \\(j\\) require \\(d^{QDA}_i(\\boldsymbol x) = d^{QDA}_j(\\boldsymbol x)\\), equivalently <\\(d^{QDA}_i(\\boldsymbol x) - d^{QDA}_j(\\boldsymbol x) = 0\\), quadratic equation.","code":""},{"path":"supervised-learning-and-classification.html","id":"linear-discriminant-analysis-lda","chapter":"5 Supervised learning and classification","heading":"5.3.2 Linear discriminant analysis (LDA)","text":"LDA special case QDA, assumption common overall covariance across groups: \\(\\boldsymbol \\Sigma_k = \\boldsymbol \\Sigma\\).leads simplified discriminant function:\n\\[\nd_k^{LDA}(\\boldsymbol x) = -\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) +\\log(\\pi_k)\n\\]\nNote term containing log-determinant now gone, LDA essentially now method tries minimize Mahalanobis distance\n(taking also account prior class probabilities).function can simplified, noting quadratic term \\(\\boldsymbol x^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x\\) depend \\(k\\) hence can dropped:\n\\[\n\\begin{split}\nd_k^{LDA}(\\boldsymbol x) &=  \\boldsymbol \\mu_k^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_k^T \\boldsymbol \\Sigma^{-1} \\boldsymbol \\mu_k + \\log(\\pi_k) \\\\\n  &= \\boldsymbol b^T \\boldsymbol x+ \n\\end{split}\n\\]\nThus, LDA discriminant function linear \\(\\boldsymbol x\\), hence \nresulting decision boundaries linear well (.e. straight lines two-dimensional settings).Comparison linear decision boundaries LDA (left) compared QDA (right):Note logistic regression (cf. GLM module) takes exactly linear form indeed closely linked LDA classifier.","code":""},{"path":"supervised-learning-and-classification.html","id":"diagonal-discriminant-analysis-dda","chapter":"5 Supervised learning and classification","heading":"5.3.3 Diagonal discriminant analysis (DDA)","text":"DDA start setting LDA, now simplify model even additionally requiring diagonal covariance containing variances (thus assume correlations among predictors \\(x_1, \\ldots, x_d\\) zero):\n\\[\n\\boldsymbol \\Sigma= \\boldsymbol V= \\begin{pmatrix}\n    \\sigma^2_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma^2_{d}\n\\end{pmatrix}\n\\]\nsimplifies inversion \\(\\boldsymbol \\Sigma\\) \n\\[\n\\boldsymbol \\Sigma^{-1} = \\boldsymbol V^{-1} = \\begin{pmatrix}\n    \\sigma^{-2}_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma^{-2}_{d}\n\\end{pmatrix}\n\\]\nleads discriminant function\n\\[\n\\begin{split}\nd_k^{DDA}(\\boldsymbol x) &=  \\boldsymbol \\mu_k^T \\boldsymbol V^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_k^T \\boldsymbol V^{-1} \\boldsymbol \\mu_k + \\log(\\pi_k) \\\\\n  &= \\sum_{j=}^d \\frac{\\mu_{k,j} x_j - \\mu_{k,j}^2/2}{\\sigma_d^2} + \\log(\\pi_k)\n\\end{split}\n\\]\nspecial case LDA, DDA classifier linear classifier thus linear decision boundaries.Bayes classifier (using distribution) assuming uncorrelated predictors\nalso known naive Bayes classifier.Hence, DDA naive Bayes classifier assuming underlying densities normal.However, don’t let label “naive” mislead DDA “naive” Bayes classifiers\noften effective methods classification prediction,\nespecially high-dimensional settings!","code":""},{"path":"supervised-learning-and-classification.html","id":"the-training-step-learning-qda-lda-and-dda-classifiers-from-data","chapter":"5 Supervised learning and classification","heading":"5.4 The training step — learning QDA, LDA and DDA classifiers from data","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"number-of-model-parameters","chapter":"5 Supervised learning and classification","heading":"5.4.1 Number of model parameters","text":"order predict class new data using discriminant functions need first learn underlying parameters training data \\(\\boldsymbol x_i^{\\text{train}}\\) \\(y_i^{\\text{train}}\\):QDA, LDA DDA need learn \\(\\pi_1, \\ldots, \\pi_K\\) \\(\\sum_{k=1}^K \\pi_k = 1\\) mean vectors \\(\\boldsymbol \\mu_1, \\ldots, \\boldsymbol \\mu_K\\)QDA additionally require \\(\\boldsymbol \\Sigma_1, \\ldots, \\boldsymbol \\Sigma_K\\)LDA need \\(\\boldsymbol \\Sigma\\)DDA estimate \\(\\sigma^2_1, \\ldots, \\sigma^2_d\\).Overall, total number parameters estimated learning discriminant functions\ntraining data follows:QDA: \\(K-1+ K d + K \\frac{d(d+1)}{2}\\)LDA: \\(K-1+ K d + \\frac{d(d+1)}{2}\\)DDA: \\(K-1+ K d + d\\)","code":""},{"path":"supervised-learning-and-classification.html","id":"estimating-the-discriminant-predictor-function","chapter":"5 Supervised learning and classification","heading":"5.4.2 Estimating the discriminant / predictor function","text":"QDA, LDA DDA learn predictor estimating \nparameters discriminant function training data.","code":""},{"path":"supervised-learning-and-classification.html","id":"large-sample-size","chapter":"5 Supervised learning and classification","heading":"5.4.2.1 Large sample size","text":"sample size training data set sufficiently large compared model dimensions can use maximum likelihood (ML) estimate model parameters. able use ML need larger sample size QDA LDA (full covariances need estimated) DDA comparatively small sample size can sufficient (one aspect “naive” Bayes methods popular practise).obtain parameters estimates use known labels \\(y_i^{\\text{train}}\\) sort \nsamples \\(\\boldsymbol x_i^{\\text{train}}\\) corresponding classes, apply standard ML estimators.\nLet \\(g_k =\\{: y_i^{\\text{train}}=k \\}\\) set indices training sample belonging group \\(k\\), \\(n_k\\) sample size group \\(k\\)ML estimates class probabilities frequencies\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n}\n\\]\nML estimate group means \\(k=1, \\ldots, K\\) \n\\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n_k} \\sum_{\\g_k} \\boldsymbol x_i^{\\text{train}} \\, .\n\\]\nML estimate global mean \\(\\boldsymbol \\mu_0\\) (.e. assume single class ignore group labels) \n\\[\n\\hat{\\boldsymbol \\mu}_0 = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i^{\\text{train}} = \\sum_{k=1}^K \\hat{\\pi}_k \\hat{\\boldsymbol \\mu}_k\n\\]\nNote global mean identical pooled mean (.e. weighted average \nindividual group means).ML estimates covariances \\(\\boldsymbol \\Sigma_k\\) QDA \n\\[\n\\widehat{\\boldsymbol \\Sigma}_k = \\frac{1}{n_k} \\sum_{\\g_k} ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k)^T\n\\]order get ML estimate pooled variance \\(\\boldsymbol \\Sigma\\) use LDA compute\n\\[\n\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\sum_{k=1}^K \\sum_{\\g_k} ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k)^T =  \\sum_{k=1}^K \\hat{\\pi}_k \\widehat{\\boldsymbol \\Sigma}_k\n\\]Note pooled variance \\(\\boldsymbol \\Sigma\\) differs (substantially!) global variance \\(\\Sigma_0\\) results simply\nignoring class labels computed \n\\[\n\\widehat{\\boldsymbol \\Sigma}_0^{ML} = \\frac{1}{n} \\sum_{=1}^n ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_0) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_0)^T\n\\]\nrecognise variance decomposition mixture models, \\(\\boldsymbol \\Sigma_0\\) total variance\npooled \\(\\boldsymbol \\Sigma\\) unexplained/-group variance.","code":""},{"path":"supervised-learning-and-classification.html","id":"small-sample-size","chapter":"5 Supervised learning and classification","heading":"5.4.2.2 Small sample size","text":"dimension \\(d\\) large compared sample size number parameters predictor function grows fast. Especially QDA also LDA data hungry ML estimation becomes ill-posed problem.discussed Section 1.3 instance need use regularised estimator covariance(s) estimators derived framework penalised ML, Bayesian learning, shrinkage estimation etc.\nalso ensures estimated covariance matrices positive definite (\nautomatically guaranteed DDA variances positive).Furthermore, small sample setting advised reduce number parameters model. Thus using LDA DDA preferred QDA. can also prevent overfitting lead predictor generalises better.analyse high-dimensional data worksheets employ regularised version LDA DDA using Stein-type shrinkage estimation discussed Section 1.3 implemented R package “sda”.","code":""},{"path":"supervised-learning-and-classification.html","id":"comparison-of-estimated-decision-boundaries-lda-vs.-qda","chapter":"5 Supervised learning and classification","heading":"5.4.3 Comparison of estimated decision boundaries: LDA vs. QDA","text":"compare two simple scenarios using simulated data.Non-nested case (\\(K=4\\)):Nested case (\\(K=2\\)):nested case LDA fails separate two classes \nway separate two nested classes \nsimple linear boundary.","code":""},{"path":"supervised-learning-and-classification.html","id":"quantifying-prediction-error","chapter":"5 Supervised learning and classification","heading":"5.5 Quantifying prediction error","text":"classifier trained naturally interested performance\ncorrectly classify previously unseen data points. useful comparing different types\nclassifiers also comparing type classifier using different sets\npredictor variables.","code":""},{"path":"supervised-learning-and-classification.html","id":"quantification-of-prediction-error-based-on-validation-data","chapter":"5 Supervised learning and classification","heading":"5.5.1 Quantification of prediction error based on validation data","text":"measure predictor error compares predicted label \\(\\hat{y}\\) true\nlabel \\(y\\) validation data. validation data set contains \n\\(\\boldsymbol x_i\\) associated label \\(y_i\\) unlike training data \nused learning predictor function.continuous response often squared loss used:\n\\[\n\\text{err}(\\hat{y}, y) =  (\\hat{y} - y)^2\n\\]binary outcomes one often employs 0/1 loss:\n\\[\n\\text{err}(\\hat{y}, y) =\n\\begin{cases}\n    0, & \\text{ } \\hat{y}=y\\\\\n    1,  & \\text{otherwise}\n\\end{cases}\n\\]\nAlternatively, quantity derived confusion matrix\n(containing TP, TN, FP, FN) can used.mean prediction error expectation\n\\[\nPE = \\text{E}(\\text{err}(\\hat{y}, y))\n\\]\nthus empirical mean prediction error \n\\[\n\\widehat{PE} = \\frac{1}{m} \\sum_{=1}^m \\text{err}(\\hat{y}_i, y_i)\n\\]\n\\(m\\) sample size validation data set.generally, can also quantify prediction error framework -called proper scoring rules, whole probabilistic forecast taken account (e.g. individual probabilities class, rather just selected probable class). commonly used scoring rule negative log-probability (“surprise”), expected surprise cross-entropy. leads back entropy likelihood (see MATH27720 Statistics 2).estimate prediction error model can use compare choose among set candidate models, selecting sufficiently low prediction\nerror.","code":""},{"path":"supervised-learning-and-classification.html","id":"estimation-of-prediction-error-using-cross-validation","chapter":"5 Supervised learning and classification","heading":"5.5.2 Estimation of prediction error using cross-validation","text":"Unfortunately, quite often separate validation data available evaluate classifier.case need rely simple algorithmic procedure called cross-validation.Outline cross-validation:split samples training data number (say \\(K\\)) parts (“folds”).use \\(K\\) folds validation data \\(K-1\\) folds training data.average resulting \\(K\\) individual estimates prediction error, get overall aggregated predictor error, along error.Note case one part data reserved validation \nused training predictor.choose \\(K\\) folds small (allow estimation \nprediction error) also large (make sure actually able train reliable classifier remaining data). typical value \\(K\\) 5 10, 80% respectively 90% samples used training 20 %\n10% validation.\\(K=n\\) many folds samples validation data set consists single data point. called “leave one ” cross-validation (LOOCV). analytic approximations prediction error obtained LOOCV\napproach computationally inexpensive standard models (including regression).reading:study technical details cross-validation: read Section 5.1 Cross-Validation James et al. (2021) (R version) James et al. (2023) (Python version).","code":""},{"path":"supervised-learning-and-classification.html","id":"goodness-of-fit-and-variable-ranking","chapter":"5 Supervised learning and classification","heading":"5.6 Goodness of fit and variable ranking","text":"linear regression interested finding \nwhether fitted mixture model appropriate model, \nparticular predictor(s) \\(x_j\\) \\(\\boldsymbol x=(x_1, \\ldots, x_d)^T\\)\nresponsible prediction outcome, .e. categorising sample group \\(k\\).order study problem helpful rewrite discriminant function highlight influence (importance) predictor.focus linear methods (LDA DDA) first look simple case \\(K=2\\) generalise two groups.","code":""},{"path":"supervised-learning-and-classification.html","id":"lda-with-k2-classes","chapter":"5 Supervised learning and classification","heading":"5.6.1 LDA with \\(K=2\\) classes","text":"two classes using LDA discriminant rule choose group \\(k=1\\)\n\\(d_1^{LDA}(\\boldsymbol x) > d_2^{LDA}(\\boldsymbol x)\\), equivalently, \n\\[\n\\Delta_{12}^{LDA} = d_1^{LDA}(\\boldsymbol x) - d_2^{LDA}(\\boldsymbol x) > 0\n\\]\nSince \\(d_k(\\boldsymbol x)\\) log-posterior (plus/minus identical constants)\n\\(\\Delta^{LDA}\\) fact log-posterior odds class 1 versus class 2.difference \\(\\Delta_{12}^{LDA}\\) \n\\[\n\\underbrace{ \\Delta_{12}^{LDA}}_{\\text{log posterior odds}} =\n\\underbrace{(\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} \\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right)}_{\\text{log Bayes factor } \\log B_{12}} + \\underbrace{\\log\\left( \\frac{\\pi_1}{\\pi_2} \\right)}_{\\text{log prior odds}}\n\\]\nNote since consider simple non-composite models log-Bayes factor identical\nlog-likelihood ratio!log Bayes factor \\(\\log B_{12}\\) known weight evidence favour\n\\(F_1\\) given \\(\\boldsymbol x\\). expected weight evidence assuming \\(\\boldsymbol x\\) indeed \\(F_1\\)\nKullback-Leibler discrimination information favour group 1,\n.e. KL divergence distribution \\(F_2\\) \\(F_1\\):\n\\[\n\\text{E}_{F_1} ( \\log B_{12} ) = D_{\\text{KL}}(F_1,  F_2) = \\frac{1}{2} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2) = \\frac{1}{2} \\Omega^2\n\\]\nyields, apart scale factor, population version \nHotelling \\(T^2\\)\nstatistic defined \n\\[T^2 =  c^2 (\\hat{\\boldsymbol \\mu}_1 -\\hat{\\boldsymbol \\mu}_2)^T \\hat{\\boldsymbol \\Sigma}^{-1} (\\hat{\\boldsymbol \\mu}_1 -\\hat{\\boldsymbol \\mu}_2)\\]\n\n\\(c = (\\frac{1}{n_1} + \\frac{1}{n_2})^{-1/2} = \\sqrt{n \\pi_1 \\pi_2}\\)\nsample size dependent factor (\\(\\text{SD}(\\hat{\\boldsymbol \\mu}_1 - \\hat{\\boldsymbol \\mu}_2)\\)).\n\\(T^2\\) measure fit underlying two-component mixture.Using whitening transformation \\(\\boldsymbol z= \\boldsymbol W\\boldsymbol x\\) \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1}\\)\ncan rewrite log Bayes factor \n\\[\n\\begin{split}\n\\log B_{12} &= \\left( (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol W^T \\right)\\, \\left(\\boldsymbol W\\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right) \\right) \\\\\n&=\\boldsymbol \\omega^T \\boldsymbol \\delta(\\boldsymbol x)\n\\end{split}\n\\]\n.e. product two vectors:\\(\\boldsymbol \\delta(\\boldsymbol x)\\) whitened \\(\\boldsymbol x\\) (centered around average means)\n\\(\\boldsymbol \\omega= (\\omega_1, \\ldots, \\omega_d)^T = \\boldsymbol W(\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)\\) gives weight \nwhitened component \\(\\boldsymbol \\delta(\\boldsymbol x)\\)\nlog Bayes factor.large positive negative value \\(\\omega_j\\)\nindicates corresponding whitened predictor relevant choosing class,\nwhereas small values \\(\\omega_j\\) close zero indicate corresponding ZCA whitened predictor unimportant. Furthermore,\n\\(\\boldsymbol \\omega^T \\boldsymbol \\omega= \\sum_{j=1}^d \\omega_j^2 = (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2) = \\Omega^2\\),\n.e. squared \\(\\omega_j^2\\) provide component-wise decomposition overall fit \\(\\Omega^2\\).Choosing ZCA-cor whitening transformation \\(\\boldsymbol W=\\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2}\\)\nget\n\\[\n\\boldsymbol \\omega^{ZCA-cor} = \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)\n\\]\nbetter understanding \\(\\boldsymbol \\omega^{ZCA-cor}\\) provided \ncomparing two-sample \\(t\\)-statistic\n\\[\n\\hat{\\boldsymbol \\tau} = c \\hat{\\boldsymbol V}^{-1/2} (\\hat{\\boldsymbol \\mu}_1 - \\hat{\\boldsymbol \\mu}_2)\n\\]\n\\(\\boldsymbol \\tau\\) population version \\(\\hat{\\boldsymbol \\tau}\\) can define\n\\[\\boldsymbol \\tau^{adj} = \\boldsymbol P^{-1/2} \\boldsymbol \\tau= c \\boldsymbol \\omega^{ZCA-cor}\\]\ncorrelation-adjusted \\(t\\)-scores (cat scores). \\(({\\hat{\\boldsymbol \\tau}}^{adj})^T {\\hat{\\boldsymbol \\tau}}^{adj} = T^2\\) can see cat scores offer component-wise decomposition Hotelling’s \\(T^2\\).Note choice ZCA-cor whitening ensure whitened components interpretable\nstay maximally correlated original variables. However, may also choose example PCA whitening\ncase \\(\\boldsymbol \\omega^T \\boldsymbol \\omega\\) provide variable importance PCA whitened variables.DDA, assumes correlations among predictors vanish, .e. \\(\\boldsymbol P= \\boldsymbol I_d\\), get\n\\[\n\\Delta_{12}^{DDA} =\\underbrace{ \\left( (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol V^{-1/2}  \\right)}_{\\text{ } c^{-1} \\boldsymbol \\tau^T }\\, \\underbrace{ \\left( \\boldsymbol V^{-1/2} \\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right) \\right) }_{\\text{centered standardised predictor}}+ \\log\\left( \\frac{\\pi_1}{\\pi_2} \\right) \\\\\n\\]\nSimilarly , \\(t\\)-score \\(\\boldsymbol \\tau\\) determines impact standardised predictor \\(\\Delta^{DDA}\\).Consequently, DDA can rank predictors squared \\(t\\)-score.\nRecall standard linear regression uncorrelated predictors can find important predictors\nranking squared marginal correlations – ranking (squared) \\(t\\)-scores DDA exact analogy discrete response.","code":""},{"path":"supervised-learning-and-classification.html","id":"multiple-classes","chapter":"5 Supervised learning and classification","heading":"5.6.2 Multiple classes","text":"two classes need refer -called pooled centroids formulation DDA LDA (introduced Tibshirani 2002).pooled centroid given \\(\\boldsymbol \\mu_0 = \\sum_{k=1}^K \\pi_k \\boldsymbol \\mu_k\\) — centroid\nsingle class. corresponding probability (single class) \\(\\pi_0=1\\) distribution\ncalled \\(F_0\\).LDA discriminant function “group 0” \n\\[\nd_0^{LDA}(\\boldsymbol x) = \\boldsymbol \\mu_0^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_0^T \\boldsymbol \\Sigma^{-1} \\boldsymbol \\mu_0\n\\]\nlog posterior odds comparison group \\(k\\) pooled group \\(0\\)\n\n\\[\n\\begin{split}\n\\Delta_k^{LDA} &= d_k^{LDA}(\\boldsymbol x) - d_0^{LDA}(\\boldsymbol x) \\\\\n         &= \\log B_{k0} + \\log(\\pi_k) \\\\\n         &= \\boldsymbol \\omega_k^T \\boldsymbol \\delta_k(\\boldsymbol x) + \\log(\\pi_k)\n\\end{split}\n\\]\n\n\\[\n\\boldsymbol \\omega_k = \\boldsymbol W(\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)  \n\\]\n\n\\[\n\\boldsymbol \\delta_k(\\boldsymbol x) = \\boldsymbol W(\\boldsymbol x- \\frac{\\boldsymbol \\mu_k +\\boldsymbol \\mu_0}{2} )\n\\]\nexpected log Bayes factor \n\\[\n\\text{E}_{F_k} ( \\log B_{k0} )= KL(F_k || F_0) = \\frac{1}{2} (\\boldsymbol \\mu_k -\\boldsymbol \\mu_0)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_k -\\boldsymbol \\mu_0) = \\frac{1}{2} \\Omega_k^2\n\\]scale factor \\(c_k = (\\frac{1}{n_k} - \\frac{1}{n})^{-1/2} = \\sqrt{n \\frac{\\pi_k}{1-\\pi_k}}\\) (\\(\\text{SD}(\\hat{\\boldsymbol \\mu}_k-\\hat{\\boldsymbol \\mu}_0)\\), minus sign \\(\\frac{1}{n}\\) due correlation \n\\(\\hat{\\boldsymbol \\mu}_k\\) pooled mean \\(\\hat{\\boldsymbol \\mu}_0\\))\nget correlation-adjusted \\(t\\)-score comparing mean group \\(k\\) \npooled mean\n\\[\n\\boldsymbol \\tau_k^{adj} = c_k \\boldsymbol \\omega_k^{ZCA-cor} \\,.\n\\]two class case (\\(K=2\\)) get \n\\(\\boldsymbol \\mu_0 = \\pi_1 \\boldsymbol \\mu_1 + \\pi_2 \\boldsymbol \\mu_2\\) mean difference\n\\((\\boldsymbol \\mu_1 - \\boldsymbol \\mu_0) = \\pi_2 (\\boldsymbol \\mu_1 - \\boldsymbol \\mu_2)\\)\n\\(c_1 = \\sqrt{n \\frac{\\pi_1}{\\pi_2}}\\)\nyields\n\\[\n\\boldsymbol \\tau_1^{adj} = \\sqrt{n \\pi_1 \\pi_2 } \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} (\\boldsymbol \\mu_1 - \\boldsymbol \\mu_2) ,\n\\]\n.e. exact score two-class setting.","code":""},{"path":"supervised-learning-and-classification.html","id":"variable-selection","chapter":"5 Supervised learning and classification","heading":"5.7 Variable selection","text":"previous saw DDA natural score\nranking features regard relevance separating classes \n(squared) \\(t\\)-score, LDA whitened version \nsquared correlation-adjusted \\(t\\)-score (based ZCA-cor whitening) may used.\nranking established question suitable cutoff arises, .e. \nmany features need () retained model.large high-dimensional models feature selection can also viewed\nform regularisation also dimension reduction. Specifically, may many variables/ features contribute class prediction. Despite \nprinciple effect outcome presence “null variables”\ncan nonetheless deterioriate (sometimes dramatically!) overall predictive accuracy trained predictor, add noise increase model dimension. Therefore, variables contribute prediction\nfiltered order able construct good prediction models classifiers.","code":""},{"path":"supervised-learning-and-classification.html","id":"choosing-a-threshold-by-multiple-testing-using-false-discovery-rates","chapter":"5 Supervised learning and classification","heading":"5.7.1 Choosing a threshold by multiple testing using false discovery rates","text":"simple way determine cutoff threshold use standard technique \nmultiple testing.predictor variable \\(x_1, \\ldots, x_d\\) corresponding test statistic\nmeasuring influence variable response, example \n\\(t\\)-scores related statistics discussed previous section.\naddition providing overall ranking set statistics can used\ndetermine suitable cutoff trying separate two populations predictor variables:“Null” variables contribute prediction“Alternative” variables linked predictionThis can done follows:distribution observed test statistics \\(z_i\\) assumed follow two-component mixture \\(F_0(z)\\) \\(F_A(z)\\) distributions corresponding null alternative, \\(f_0(z)\\) \\(f_a(z)\\) densities, \\(\\pi_0\\) \\(\\pi_A=1-\\pi_0\\) weights:\n\\[\nf(z) = \\pi_0 f_0(z) + (1-\\pi_0) f_a(z)\n\\]distribution observed test statistics \\(z_i\\) assumed follow two-component mixture \\(F_0(z)\\) \\(F_A(z)\\) distributions corresponding null alternative, \\(f_0(z)\\) \\(f_a(z)\\) densities, \\(\\pi_0\\) \\(\\pi_A=1-\\pi_0\\) weights:\n\\[\nf(z) = \\pi_0 f_0(z) + (1-\\pi_0) f_a(z)\n\\]null model typically parametric family (e.g. normal around zero \nfree variance parameter) whereas alternative often modelled nonparametrically.null model typically parametric family (e.g. normal around zero \nfree variance parameter) whereas alternative often modelled nonparametrically.fitting mixture model, often assuming additional constraints make mixture identifiable, one can compute false discovery rates (FDR) follows:\nLocal FDR:\n\\[\n\\widehat{\\text{fdr}}(z_i) = \\widehat{\\text{Pr}}(\\text{null} | z_i) = \\frac{\\hat{\\pi}_0 \\hat{f}_0(z_i)}{\\hat{f}(z_i)}  \n\\]\nTail-area-based FDR (=\\(q\\)-value):\n\\[\n\\widehat{\\text{Fdr}}(z_i) = \\widehat{\\text{Pr}}(\\text{null} | Z > z_i) = \\frac{\\hat{\\pi}_0 \\hat{F}_0(z_i)}{\\hat{F}(z_i)}\n\\]\nNote essentially \\(p\\)-values adjusted multiple testing (variant Benjamini-Hochberg method).fitting mixture model, often assuming additional constraints make mixture identifiable, one can compute false discovery rates (FDR) follows:Local FDR:\n\\[\n\\widehat{\\text{fdr}}(z_i) = \\widehat{\\text{Pr}}(\\text{null} | z_i) = \\frac{\\hat{\\pi}_0 \\hat{f}_0(z_i)}{\\hat{f}(z_i)}  \n\\]Tail-area-based FDR (=\\(q\\)-value):\n\\[\n\\widehat{\\text{Fdr}}(z_i) = \\widehat{\\text{Pr}}(\\text{null} | Z > z_i) = \\frac{\\hat{\\pi}_0 \\hat{F}_0(z_i)}{\\hat{F}(z_i)}\n\\]\nNote essentially \\(p\\)-values adjusted multiple testing (variant Benjamini-Hochberg method).thresholding false discovery rates possible identify \nvariables clearly belong two groups also features\neasily discriminated fall either group:“alternative” variables low local FDR, e.g, \\(\\widehat{\\text{fdr}}(z_i) \\leq 0.2\\)“null” variables high local FDR, e.g. \\(\\widehat{\\text{fdr}}(z_i) \\geq 0.8\\)features easily classified null alternative, e.g. \\(0.2 < \\widehat{\\text{fdr}}(z_i) < 0.8\\)feature selection prediction settings generally aim remove \nvariable clearly belong null group, leaving others model.","code":""},{"path":"supervised-learning-and-classification.html","id":"variable-selection-using-cross-validation","chapter":"5 Supervised learning and classification","heading":"5.7.2 Variable selection using cross-validation","text":"conceptually simple computationally expensive approach variable selection estimate predicion error type predictor different sets predictors using cross-validation, choosing predictor achieves good prediction accuracy using small number featurs.method works well practise demonstrated \nnumber problems worksheets.","code":""},{"path":"multivariate-dependencies.html","id":"multivariate-dependencies","chapter":"6 Multivariate dependencies","heading":"6 Multivariate dependencies","text":"","code":""},{"path":"multivariate-dependencies.html","id":"measuring-the-linear-association-between-two-sets-of-random-variables","chapter":"6 Multivariate dependencies","heading":"6.1 Measuring the linear association between two sets of random variables","text":"","code":""},{"path":"multivariate-dependencies.html","id":"aim","chapter":"6 Multivariate dependencies","heading":"6.1.1 Aim","text":"linear association two scalar random variables \\(x\\) \\(y\\) measured\ncorrelation \\(\\text{Cor}(x, y) = \\rho\\).chapter now like explore generalise correlation case two random vectors.\nSpecifically, like find scalar measure\nassociation two random vectors\n(equivalently two sets random variables) \\(\\boldsymbol x= (x_1, \\ldots, x_p)^T\\) \n\\(\\boldsymbol y= (y_1, \\ldots, y_q)^T\\) contains correlation also multiple correlation\nspecial case.assume joint correlation matrix\n\\[\n\\boldsymbol P=\n\\begin{pmatrix}\n\\boldsymbol P_{\\boldsymbol x} &  \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol P_{\\boldsymbol y\\boldsymbol x} & \\boldsymbol P_{\\boldsymbol y} \\\\\n\\end{pmatrix}\n\\]\ncross-correlation matrix \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = \\boldsymbol P_{\\boldsymbol y\\boldsymbol x}^T\\)\nwithin-group group correlations \\(\\boldsymbol P_{\\boldsymbol x}\\) \\(\\boldsymbol P_{\\boldsymbol y}\\).\ncross-correlations vanish, \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} =0\\), \ntwo random vectors uncorrelated, joint correlation matrix\nbecomes diagonal block matrix\n\\[\n\\boldsymbol P_{\\text{indep}} =\n\\begin{pmatrix}\n\\boldsymbol P_{\\boldsymbol x} &  0 \\\\\n0 & \\boldsymbol P_{\\boldsymbol y} \\\\\n\\end{pmatrix} \\, .\n\\]characterise total association \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) looking scalar\nquantity measuring divergence distribution assuming general joint correlation matrix \\(\\boldsymbol P\\)\ndistribution assuming joint correlation matrix \\(\\boldsymbol P_{\\text{indep}}\\) uncorrelated\n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\).","code":""},{"path":"multivariate-dependencies.html","id":"known-special-cases","chapter":"6 Multivariate dependencies","heading":"6.1.2 Known special cases","text":"Ideally, case univariate \\(y\\) multivariate \\(\\boldsymbol x\\)\nmeasure reduce \nsquared multiple correlation coefficient determination\n\\[\n\\text{MCor}(\\boldsymbol x, y)^2 = \\boldsymbol P_{y\\boldsymbol x} \\boldsymbol P_{\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\n\\]\nwell-known linear regression describe strength total linear association \npredictors \\(\\boldsymbol x= (x_1, \\ldots, x_p)^T\\) response \\(y\\).derive squared multiple correlation may proceed follows. First whiten\nrandom vector \\(\\boldsymbol x\\) resulting \\(\\boldsymbol z_{\\boldsymbol x} = \\boldsymbol W_{\\boldsymbol x}\\boldsymbol x=\\boldsymbol Q_{\\boldsymbol x}\\boldsymbol P_{\\boldsymbol x}^{-1/2}\\boldsymbol V_{\\boldsymbol x}^{-1/2}\\boldsymbol x\\)\n\\(\\boldsymbol Q_{\\boldsymbol x}\\) orthogonal matrix.\ncorrelations component \\(\\boldsymbol z_{\\boldsymbol x}\\) response \\(y\\) \n\\[\n\\boldsymbol \\omega= \\text{Cor}(\\boldsymbol z_{\\boldsymbol x}, y) = \\boldsymbol Q_{\\boldsymbol x} \\boldsymbol P_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol xy}\n\\]\n\\(\\text{Var}(\\boldsymbol z_{\\boldsymbol x}) = \\boldsymbol \\) thus components \\(\\boldsymbol z_{\\boldsymbol x}\\) uncorrelated can simply add squared individual correlations get total association measure\n\\[\n\\boldsymbol \\omega^T \\boldsymbol \\omega= \\sum_i \\omega_i^2 = \\boldsymbol P_{y\\boldsymbol x} \\boldsymbol P_{\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\n\\]\nNote particular choice orthogonal matrix \\(\\boldsymbol Q_{\\boldsymbol x}\\) whitening \\(\\boldsymbol x\\) relevant squared multiple correlation.Note cross-correlations\nvanish (\\(\\boldsymbol P_{\\boldsymbol xy} =0\\)) \\(\\text{MCor}(\\boldsymbol x, y)^2=0\\). correlation predictors\nvanishes (\\(\\boldsymbol P_{\\boldsymbol x} = \\boldsymbol \\)) \\(\\text{MCor}(\\boldsymbol x, y)^2 = \\sum_i \\rho_{y x_i}^2\\), .e. \nsum squared cross-correlations.single predictor \\(x\\) \\(\\boldsymbol P_{xy}=\\rho\\) \\(\\boldsymbol P_{x} = 1\\)\nsquared multiple correlation reduces squared Pearson correlation\n\\[\n\\text{Cor}(x, y)^2 = \\rho^2 \\, .\n\\]","code":""},{"path":"multivariate-dependencies.html","id":"canonical-correlation-analysis-cca-aka-cca-whitening","chapter":"6 Multivariate dependencies","heading":"6.2 Canonical Correlation Analysis (CCA) aka CCA whitening","text":"Canonical correlation analysis invented Harald Hotelling 1936 18. CCA aims characterise linear dependence random vectors \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) set canonical correlations \\(\\lambda_i\\).CCA works simultaneously whitening two random vectors \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) whitening matrices chosen way cross-correlation matrix resulting whitened variables\nbecomes diagonal, elements diagonal correspond canonical correlations.\\[\\begin{align*}\n\\begin{array}{ll}\n\\boldsymbol x= \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_p \\end{pmatrix} \\\\\n\\text{Dimension } p\n\\end{array}\n\\begin{array}{ll}\n\\boldsymbol y= \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_q \\end{pmatrix} \\\\\n\\text{Dimension } q\n\\end{array}\n\\begin{array}{ll}\n\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma_{\\boldsymbol x} = \\boldsymbol V_{\\boldsymbol x}^{1/2}\\boldsymbol P_{\\boldsymbol x}\\boldsymbol V_{\\boldsymbol x}^{1/2} \\\\\n\\text{Var}(\\boldsymbol y) = \\boldsymbol \\Sigma_{\\boldsymbol y} = \\boldsymbol V_{\\boldsymbol y}^{1/2}\\boldsymbol P_{\\boldsymbol y}\\boldsymbol V_{\\boldsymbol y}^{1/2} \\\\\n\\end{array}\n\\end{align*}\\]\\[\\begin{align*}\n\\begin{array}{cc}\n\\text{Whitening } \\boldsymbol x\\text{:} \\\\\n\\text{Whitening } \\boldsymbol y\\text{:}\n\\end{array}\n\\begin{array}{cc}\n\\boldsymbol z_{\\boldsymbol x} = \\boldsymbol W_{\\boldsymbol x}\\boldsymbol x=\\boldsymbol Q_{\\boldsymbol x}\\boldsymbol P_{\\boldsymbol x}^{-1/2}\\boldsymbol V_{\\boldsymbol x}^{-1/2}\\boldsymbol x\\\\\n\\boldsymbol z_{\\boldsymbol y} = \\boldsymbol W_{\\boldsymbol y}\\boldsymbol y=\\boldsymbol Q_{\\boldsymbol y}\\boldsymbol P_{\\boldsymbol y}^{-1/2}\\boldsymbol V_{\\boldsymbol y}^{-1/2}\\boldsymbol y\n\\end{array}\n\\end{align*}\\]\n(note use correlation-based form \\(\\boldsymbol W\\))Cross-correlation \\(\\boldsymbol z_{\\boldsymbol y}\\) \\(\\boldsymbol z_{\\boldsymbol y}\\):\\[\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})=\\boldsymbol Q_{\\boldsymbol x}\\boldsymbol K\\boldsymbol Q_{\\boldsymbol y}^T\\]\\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2}\\boldsymbol P_{\\boldsymbol x\\boldsymbol y}\\boldsymbol P_{\\boldsymbol y}^{-1/2}\\).Idea: can choose suitable orthogonal matrices \\(\\boldsymbol Q_{\\boldsymbol x}\\) \\(\\boldsymbol Q_{\\boldsymbol y}\\) putting structural constraint cross-correlation matrix.CCA: aim diagonal \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})\\) component \\(\\boldsymbol z_{\\boldsymbol x}\\) influences one (corresponding) component \\(\\boldsymbol z_{\\boldsymbol y}\\).Motivation: pairs “modules” represented components \\(\\boldsymbol z_{\\boldsymbol x}\\)\n\\(\\boldsymbol z_{\\boldsymbol y}\\) influencing (anyone module).\\[\n\\begin{array}{ll}\n\\boldsymbol z_{\\boldsymbol x} = \\begin{pmatrix} z^x_1 \\\\ z^x_2 \\\\ \\vdots \\\\ z^x_p \\end{pmatrix} &\n\\boldsymbol z_{\\boldsymbol y} = \\begin{pmatrix} z^y_1 \\\\ z^y_2 \\\\ \\vdots \\\\ z^y_q \\end{pmatrix} \\\\\n\\end{array}\n\\]\\[\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y}) = \\begin{pmatrix} \\lambda_1 & \\dots & 0 \\\\ \\vdots &  \\vdots \\\\ 0 & \\dots & \\lambda_m \\end{pmatrix}\\]\\(\\lambda_i\\) canonical correlations \\(m=\\min(p,q)\\).","code":""},{"path":"multivariate-dependencies.html","id":"how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal","chapter":"6 Multivariate dependencies","heading":"6.2.1 How to make cross-correlation matrix \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})\\) diagonal?","text":"Use Singular Value Decomposition (SVD) matrix \\(\\boldsymbol K\\):\\[\\boldsymbol K= (\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}})^T  \\boldsymbol \\Lambda\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\]\n\\(\\boldsymbol \\Lambda\\) diagonal matrix containing singular values \\(\\boldsymbol K\\)yields orthogonal matrices \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) thus desired whitening matrices \\(\\boldsymbol W_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol W_{\\boldsymbol y}^{\\text{CCA}}\\)result \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y}) = \\boldsymbol \\Lambda\\) .e. singular values \\(\\lambda_i\\) \\(\\boldsymbol K\\) desired canonical correlations!\\(\\longrightarrow\\) \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) determined diagonality constraint (note different previously discussed whitening methods).Note signs corresponding columns \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) identified. Traditionally, SVD \nsigns chosen singular values positive. However, \nimpose positive-diagonality \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\),\nthus positive-diagonality cross-correlations \\(\\boldsymbol \\Psi_{\\boldsymbol x}\\) \n\\(\\boldsymbol \\Psi_{\\boldsymbol y}\\), canonical correlations may take positive \nnegative values.","code":""},{"path":"multivariate-dependencies.html","id":"related-methods","chapter":"6 Multivariate dependencies","heading":"6.2.2 Related methods","text":"O2PLS: similar CCA using orthogonal projections rather whitening.O2PLS: similar CCA using orthogonal projections rather whitening.Vector correlation: aggregates squared canonical correlations single overall measure (see ).Vector correlation: aggregates squared canonical correlations single overall measure (see ).","code":""},{"path":"multivariate-dependencies.html","id":"vector-correlation-and-rv-coefficient","chapter":"6 Multivariate dependencies","heading":"6.3 Vector correlation and RV coefficient","text":"","code":""},{"path":"multivariate-dependencies.html","id":"vector-alienation-coefficient","chapter":"6 Multivariate dependencies","heading":"6.3.1 Vector alienation coefficient","text":"1936 paper introducing canonical correlation analysis\nHotelling also proposed vector alienation\ncoefficient defined \n\\[\n\\begin{split}\n(\\boldsymbol x, \\boldsymbol y) &= \\frac{\\det(\\boldsymbol P)}{\\det(\\boldsymbol P_{\\text{indep}}) } \\\\\n            & = \\frac{\\det( \\boldsymbol P) }{  \\det(\\boldsymbol P_{\\boldsymbol x}) \\,  \\det(\\boldsymbol P_{\\boldsymbol y})  }\n\\end{split}\n\\]\n\\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\boldsymbol P_{\\boldsymbol y}^{-1/2}\\) vector alienation coefficient can written\n(using Weinstein-Aronszajn determinant identity formula determinant\nblock-structured matrices) \n\\[\n\\begin{split}\n(\\boldsymbol x, \\boldsymbol y) & = \\det \\left( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T \\right) \\\\\n            & = \\det \\left( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K\\right) \\\\\n            &= \\prod_{=1}^m (1-\\lambda_i^2) \\\\\n\\end{split}\n\\]\n\\(\\lambda_i\\) singular values \\(\\boldsymbol K\\), .e. canonical correlations\npair \\(\\boldsymbol x\\) \\(\\boldsymbol y\\). Therefore, vector alienation coefficient computed \nsummary statistic canonical correlations.\\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = 0\\) und thus \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) uncorrelated \\(\\boldsymbol P= \\boldsymbol P_{\\text{indep}}\\) \nthus construction vector alienation coefficient \\((\\boldsymbol x, \\boldsymbol y)=1\\).\nHence, vector alienation coefficient generalisation squared multiple correlation\ncase two random vectors quantity vanish case.","code":""},{"path":"multivariate-dependencies.html","id":"rozeboom-vector-correlation","chapter":"6 Multivariate dependencies","heading":"6.3.2 Rozeboom vector correlation","text":"Instead, Rozeboom (1965) 19 proposed use squared vector correlation\ncomplement vector alienation coefficient\n\\[\n\\begin{split}\n\\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2 &= \\rho^2_{\\boldsymbol x\\boldsymbol y} = 1 - (\\boldsymbol x, \\boldsymbol y) \\\\\n& = 1- \\det\\left( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T \\right) \\\\\n& = 1- \\det\\left( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K\\right) \\\\\n&  =1- \\prod_{=1}^m (1-\\lambda_i^2) \\\\\n\\end{split}\n\\]\n\\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = 0\\) \\(\\boldsymbol K=\\boldsymbol 0\\) hence \\(\\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2 = 0\\).Moreover, either \\(p=1\\) \\(q=1\\) squared vector correlation\nreduces corresponding squared multiple correlation,\nturn \\(p=1\\) \\(q=1\\) becomes squared Pearson correlation.can find derivation Example Sheet 10.Thus, Rozeboom’s vector correlation indeed generalises Pearson correlation\nmultiple correlation coefficient.","code":""},{"path":"multivariate-dependencies.html","id":"rv-coefficient","chapter":"6 Multivariate dependencies","heading":"6.3.3 RV coefficient","text":"Another common approach measure association two random vectors RV coefficient introduced Robert Escoufier 1976 \n\\[\nRV(\\boldsymbol x, \\boldsymbol y) = \\frac{ \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} )}{ \\sqrt{ \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol x}^2) \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol y}^2)  } }\n\\]\nmain advantage RV coefficient easier compute Rozeboom vector correlation uses matrix trace rather matrix determinant.\\(q=p=1\\) RV coefficient reduces squared correlation.\nHowever, RV coefficient reduce multiple correlation coefficient\n\\(q=1\\) \\(p > 1\\), therefore RV coefficient considered coherent generalisation\nPearson multiple correlation case \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) random vectors.See also Worksheet 10.","code":""},{"path":"multivariate-dependencies.html","id":"limits-of-linear-models-and-correlation","chapter":"6 Multivariate dependencies","heading":"6.4 Limits of linear models and correlation","text":"","code":""},{"path":"multivariate-dependencies.html","id":"correlation-measures-only-linear-dependence","chapter":"6 Multivariate dependencies","heading":"6.4.1 Correlation measures only linear dependence","text":"Linear models measures linear association (correlation) effective tools. However, important\nrecognise limits especially modelling complex nonlinear relationships.simple demonstration given following example. Assume \\(x\\) normally distributed\nrandom variable \\(x \\sim N(0,1)\\). \\(x\\) construct second random variable \\(y = x^2\\) — thus \\(y\\) fully depends \\(x\\) added extra noise. correlation \\(x\\) \\(y\\)?Let’s answer question running small computer simulation:Thus, correlation (almost) zero even though \\(x\\) \\(y\\) dependent variables.\ncorrelation measures linear association, relationship \n\\(x\\) \\(y\\) nonlinear.","code":"\nx=rnorm(10000)\ny = x^2\ncor(x,y)## [1] 0.009732879"},{"path":"multivariate-dependencies.html","id":"anscombe-data-sets","chapter":"6 Multivariate dependencies","heading":"6.4.2 Anscombe data sets","text":"Using correlation (generally linear models) blindly can easily hide underlying complexity analysed data. demonstrated classic “Anscombe quartet” data sets presented 1973 paper 20 -evident scatter plots relationship \ntwo variables \\(x\\) \\(y\\) different four cases!\nHowever, intriguingly four data sets share exactly linear characteristics summary statistics:Means \\(m_x = 9\\) \\(m_y = 7.5\\)Variances \\(s^2_x = 11\\) \\(s^2_y = 4.13\\)Correlation \\(r = 0.8162\\)Linear model fit intercept \\(=3.0\\) slope \\(b=0.5\\)Thus, actual data analysis always good idea inspect data visually get first impression whether using linear model makes sense.data set “” follows linear model. Data set “b” represents quadratic relationship. Data set “c” linear outlier disturbs linear relationship. Finally data set “d” also contains outlier also represent case \\(y\\) (apart outlier) dependent \\(x\\).Worksheet 10 recent version Anscombe quartet analysed form “datasauRus” dozen - 13 highly nonlinear datasets share \nlinear characteristics.","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-as-generalisation-of-correlation","chapter":"6 Multivariate dependencies","heading":"6.5 Mutual information as generalisation of correlation","text":"","code":""},{"path":"multivariate-dependencies.html","id":"overview-2","chapter":"6 Multivariate dependencies","heading":"6.5.1 Overview","text":"general way vector correlation measure multivariate association mutual information (MI)\ncovers linear also non-linear associations.see Rozeboom vector\ncorrelation arises naturally computing MI multivariate normal distribution,\nhence MI also recovers well-known measures linear association (including multiple correlation simple correlation), thus truly generalising correlation measure association.","code":""},{"path":"multivariate-dependencies.html","id":"definition-of-mutual-information","chapter":"6 Multivariate dependencies","heading":"6.5.2 Definition of mutual information","text":"Recall definition\nKullback-Leibler divergence, relative entropy, two distributions:\n\\[\nD_{\\text{KL}}(F,  G) := \\text{E}_F \\log \\biggl( \\frac{f(\\boldsymbol x)}{g(\\boldsymbol x)} \\biggr)\n\\]\n\\(F\\) plays role reference distribution \\(G\\) approximating distribution,\n\\(f\\) \\(g\\) corresponding density functions\n(see MATH27720 Statistics 2).Mutual Information (MI) two random variables \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) defined \nKL divergence corresponding joint distribution product distribution:\n\\[\n\\text{MI}(\\boldsymbol x, \\boldsymbol y) = D_{\\text{KL}}(F_{\\boldsymbol x,\\boldsymbol y}, F_{\\boldsymbol x}  F_{\\boldsymbol y}) = \\text{E}_{F_{\\boldsymbol x,\\boldsymbol y}}  \\log \\biggl( \\frac{f(\\boldsymbol x, \\boldsymbol y)}{f(\\boldsymbol x) \\, f(\\boldsymbol y)} \\biggr) .\n\\]\nThus, MI measures well joint distribution can approximated product\ndistribution (appropriate joint distribution \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) independent).\nSince MI application KL divergence shares properties. particular,\n\\(\\text{MI}(\\boldsymbol x, \\boldsymbol y)=0\\) implies joint distribution product distributions . Hence two random variables \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) independent mutual information vanishes.","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-between-two-normal-scalar-variables","chapter":"6 Multivariate dependencies","heading":"6.5.3 Mutual information between two normal scalar variables","text":"KL divergence two multivariate normal distributions \\(F_{\\text{ref}}\\) \\(F\\) \n\\[\nD_{\\text{KL}}(F_{\\text{ref}}, F)  = \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n     - d   \\biggr\\}\n\\]\nallows compute mutual information \\(\\text{MI}_{\\text{norm}}(x,y)\\) two univariate random variables \\(x\\) \\(y\\) correlated assumed jointly bivariate normal. Let \\(\\boldsymbol z= (x, y)^T\\). joint bivariate normal distribution characterised mean \\(\\text{E}(\\boldsymbol z) = \\boldsymbol \\mu= (\\mu_x, \\mu_y)^T\\) covariance matrix\n\\[\n\\boldsymbol \\Sigma=\n\\begin{pmatrix}\n\\sigma^2_x & \\rho \\, \\sigma_x \\sigma_y \\\\\n\\rho \\, \\sigma_x  \\sigma_y & \\sigma^2_y \\\\\n\\end{pmatrix}\n\\]\n\\(\\text{Cor}(x,y)= \\rho\\). \\(x\\) \\(y\\) independent \n\\(\\rho=0\\) \n\\[\n\\boldsymbol \\Sigma_{\\text{indep}} =\n\\begin{pmatrix} \\sigma^2_x & 0 \\\\ 0 & \\sigma^2_y \\\\ \\end{pmatrix} \\,.\n\\]\nproduct\n\\[\n\\boldsymbol = \\boldsymbol \\Sigma_{\\text{indep}}^{-1} \\boldsymbol \\Sigma=\n\\begin{pmatrix}\n1 & \\rho \\frac{\\sigma_y}{\\sigma_x} \\\\\n\\rho \\frac{\\sigma_x}{\\sigma_y} & 1 \\\\\n\\end{pmatrix}\n\\]\ntrace \\(\\text{Tr}(\\boldsymbol ) = 2\\) determinant \\(\\det(\\boldsymbol ) = 1-\\rho^2\\).mutual information \\(x\\) \\(y\\) can computed \n\\[\n\\begin{split}\n\\text{MI}_{\\text{norm}}(x, y) &= D_{\\text{KL}}(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma),  N(\\boldsymbol \\mu,\\boldsymbol \\Sigma_{\\text{indep}} )) \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n     - 2   \\biggr\\} \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}( \\boldsymbol )\n    - \\log \\det( \\boldsymbol )\n     - 2   \\biggr\\} \\\\\n&=  -\\frac{1}{2} \\log(1-\\rho^2) \\\\\n  & \\approx \\frac{\\rho^2}{2} \\\\\n\\end{split}\n\\]Thus \\(\\text{MI}_{\\text{norm}}(x,y)\\) one--one function squared correlation \\(\\rho^2\\) \\(x\\) \\(y\\):small values correlation \\(2 \\, \\text{MI}_{\\text{norm}}(x,y) \\approx \\rho^2\\).","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-between-two-normally-distributed-random-vectors","chapter":"6 Multivariate dependencies","heading":"6.5.4 Mutual information between two normally distributed random vectors","text":"mutual information \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x,\\boldsymbol y)\\) two multivariate normal random vector \\(\\boldsymbol x\\) \\(\\boldsymbol y\\)\ncan computed similar fashion bivariate case.Let \\(\\boldsymbol z= (\\boldsymbol x, \\boldsymbol y)^T\\) dimension \\(d=p+q\\). joint multivariate\nnormal distribution characterised mean \\(\\text{E}(\\boldsymbol z) = \\boldsymbol \\mu= (\\boldsymbol \\mu_x^T, \\boldsymbol \\mu_y^T)^T\\) covariance matrix\n\\[\n\\boldsymbol \\Sigma=\n\\begin{pmatrix} \\boldsymbol \\Sigma_{\\boldsymbol x} & \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}^T & \\boldsymbol \\Sigma_{\\boldsymbol y} \\\\\n\\end{pmatrix} \\,.\n\\]\n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\) independent \n\\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} = 0\\) \n\\[\n\\boldsymbol \\Sigma_{\\text{indep}} =\n\\begin{pmatrix}  \n\\boldsymbol \\Sigma_{\\boldsymbol x} & 0 \\\\\n0 & \\boldsymbol \\Sigma_{\\boldsymbol y} \\\\\n\\end{pmatrix} \\, .\n\\]\nproduct\n\\[\n\\begin{split}\n\\boldsymbol & =\n\\boldsymbol \\Sigma_{\\text{indep}}^{-1} \\boldsymbol \\Sigma=\n\\begin{pmatrix}\n\\boldsymbol I_p & \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol \\Sigma_{\\boldsymbol y}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x}    & \\boldsymbol I_q \\\\\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\n\\boldsymbol I_p &  \\boldsymbol V_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\boldsymbol V_{\\boldsymbol y}^{1/2} \\\\\n\\boldsymbol V_{\\boldsymbol y}^{-1/2} \\boldsymbol P_{\\boldsymbol y}^{-1} \\boldsymbol P_{\\boldsymbol y\\boldsymbol x} \\boldsymbol V_{\\boldsymbol x}^{1/2}   & \\boldsymbol I_q \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\]\ntrace \\(\\text{Tr}(\\boldsymbol ) = d\\) determinant\n\\[\n\\begin{split}\n\\det(\\boldsymbol ) & = \\det( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T ) \\\\\n  &= \\det( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K) \\\\\n\\end{split}\n\\]\n\\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\boldsymbol P_{\\boldsymbol y}^{-1/2}\\).\n\\(\\lambda_1, \\ldots, \\lambda_m\\) singular values \\(\\boldsymbol K\\) (.e.\ncanonical correlations \\(\\boldsymbol x\\) \\(\\boldsymbol y\\)) get\n\\[\n\\det(\\boldsymbol ) =  \\prod_{=1}^m (1-\\lambda_i^2)\n\\]mutual information \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) \n\\[\n\\begin{split}\n\\text{MI}_{\\text{norm}}(\\boldsymbol x, \\boldsymbol y) &= D_{\\text{KL}}(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma),  N(\\boldsymbol \\mu,\\boldsymbol \\Sigma_{\\text{indep}} )) \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}\\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n     - d   \\biggr\\} \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}( \\boldsymbol )\n    - \\log \\det( \\boldsymbol )\n     - d   \\biggr\\} \\\\\n&=-\\frac{1}{2} \\sum_{=1}^m \\log(1-\\lambda_i^2)\\\\\n&=-\\frac{1}{2} \\log \\left( \\prod_{=1}^m (1-\\lambda_i^2) \\right)\\\\\n&=-\\frac{1}{2} \\log \\left( 1- \\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2 \\right)\\\\\n\\end{split}\n\\]see \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x, \\boldsymbol y)\\) simply sum\nMIs resulting individual canonical correlations \\(\\lambda_i\\) \nfunctional link MI squared correlation\nbivariate normal case.Furthermore obtain \n\\[\n\\text{MI}_{\\text{norm}}(\\boldsymbol x,\\boldsymbol y) = -\\frac{1}{2} \\log(1 - \\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2 ) \\approx \\frac{1}{2} \\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2\n\\]\nThus, multivariate case \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x\\boldsymbol y)\\) exactly functional relationship \nsquared vector correlation \\(\\text{VCor}(\\boldsymbol x, \\boldsymbol y)^2\\) \\(\\text{MI}_{\\text{norm}}(x, y)\\)\ntwo univariate variables squared Pearson correlation \\(\\rho^2\\).Thus, Rozeboom’s vector correlation emerges special case mutual information\ncomputed jointly multivariate normally distributed variables.","code":""},{"path":"multivariate-dependencies.html","id":"using-mi-for-variable-selection","chapter":"6 Multivariate dependencies","heading":"6.5.5 Using MI for variable selection","text":"general way write model predicting \\(\\boldsymbol y\\) \\(\\boldsymbol x\\) follows:\\(F_{\\boldsymbol y|\\boldsymbol x}\\) conditional distribution \\(\\boldsymbol y\\) given predictors \\(\\boldsymbol x\\) \\(F_{\\boldsymbol y}\\) marginal distribution \\(\\boldsymbol y\\) without predictors.Typically \\(F_{\\boldsymbol y|\\boldsymbol x}\\) complex model \\(F_{\\boldsymbol y}\\)\nsimple model (predictors). Note predictive model can assume form (incl. nonlinear).Intriguingly expected KL divergence\nconditional marginal distribution\n\\[\n\\text{E}_{F_{\\boldsymbol x}}\\, D_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x},  F_{\\boldsymbol y} ) = \\text{MI}(\\boldsymbol x, \\boldsymbol y)\n\\]\nequal mutual information \n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\)! Thus \\(\\text{MI}(\\boldsymbol x, \\boldsymbol y)\\) measures impact conditioning. MI small (.e. close zero) \n\\(\\boldsymbol x\\) useful predicting \\(\\boldsymbol y\\).identity can verified follows.\nKL divergence \\(F_{\\boldsymbol y|\\boldsymbol x}\\) \\(F_{\\boldsymbol y}\\)\ngiven \n\\[\nD_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x}, F_{\\boldsymbol y} )  = \\text{E}_{F_{\\boldsymbol y|\\boldsymbol x}}  \\log\\biggl( \\frac{f(\\boldsymbol y|\\boldsymbol x) }{ f(\\boldsymbol y)}  \\biggr) \\, ,\n\\]\nrandom variable since depends \\(\\boldsymbol x\\).\nTaking expectation regard \\(F_{\\boldsymbol x}\\) (distribution \\(\\boldsymbol x\\))\nget\n\\[\n\\begin{split}\n\\text{E}_{F_{\\boldsymbol x}} D_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x}, F_{\\boldsymbol y} ) &=\n\\text{E}_{F_{\\boldsymbol x}}  \\text{E}_{F_{\\boldsymbol y|\\boldsymbol x}}  \\log \\biggl(\\frac{ f(\\boldsymbol y|\\boldsymbol x) f(\\boldsymbol x) }{ f(\\boldsymbol y) f(\\boldsymbol x) } \\biggr)\\\\\n& =\n\\text{E}_{F_{\\boldsymbol x,\\boldsymbol y}}   \\log \\biggl(\\frac{ f(\\boldsymbol x,\\boldsymbol y) }{ f(\\boldsymbol y) f(\\boldsymbol x) } \\biggr) = \\text{MI}(\\boldsymbol x,\\boldsymbol y) \\,. \\\\\n\\end{split}\n\\]link MI conditioning MI response predictor variables often used variable feature selection general models.","code":""},{"path":"multivariate-dependencies.html","id":"other-measures-of-general-dependence","chapter":"6 Multivariate dependencies","heading":"6.5.6 Other measures of general dependence","text":"principle, MI can computed distribution model thus applies normal non-normal models,\nlinear nonlinear relationships.Besides mutual information others measures general dependence multivariate random variables.Two important measures capture nonlinear association proposed recent literature aredistance correlation andthe maximal information coefficient (MIC \\(\\text{MIC}_e\\)).","code":""},{"path":"multivariate-dependencies.html","id":"graphical-models","chapter":"6 Multivariate dependencies","heading":"6.6 Graphical models","text":"","code":""},{"path":"multivariate-dependencies.html","id":"purpose","chapter":"6 Multivariate dependencies","heading":"6.6.1 Purpose","text":"Graphical models combine features fromgraph theoryprobabilitystatistical inferenceThe literature graphical models huge, focus two commonly\nused models:DAGs (directed acyclic graphs), edges directed, directed loops (.e. cycles, hence “acyclic”)GGM (Gaussian graphical models), edges undirectedGraphical models provide probabilistic models trees networks, \nrandom variables represented nodes graphs, branches representing\nconditional dependencies. regard generalise tree-based clustering approaches well probabilistic non-hierarchical methods (GMMs).However, class graphical models goes much beyond simple\nunsupervised learning models. also includes regression, classification,\ntime series models etc. overview see, e.g., reference book Murphy (2023).","code":""},{"path":"multivariate-dependencies.html","id":"basic-notions-from-graph-theory","chapter":"6 Multivariate dependencies","heading":"6.6.2 Basic notions from graph theory","text":"Mathematically, graph \\(G = (V, E)\\) consists set vertices nodes \\(V = \\{v_1, v_2, \\ldots\\}\\) set branches edges \\(E = \\{ e_1, e_2, \\ldots \\}\\).Edges can undirected directed.Graphs containing directed edges directed graphs, likewise graphs containing undirected edges called undirected graphs. Graphs containing directed undirected edges called partially directed graphs.path sequence vertices vertices edge next vertex sequence.graph connected path every pair vertices.cycle path graph connects node .connected graph cycles called tree.degree node number edges connects . edges directed degree node sum -degree -degree, counts incoming outgoing edges, respectively.External nodes nodes degree 1. tree-structured graph also called leaves.notions relevant graphs directed edges:directed graph parent node(s) vertex \\(v\\) set nodes \\(\\text{pa}(v)\\) directly connected \\(v\\) via edges directed parent node(s) towards \\(v\\).Conversely, \\(v\\) called child node \\(\\text{pa}(v)\\). Note parent node can several child nodes, \\(v\\) may child \\(\\text{pa}(v)\\).directed tree graph, node single parent, except one particular node parent (node called root node).DAG, directed acyclic graph, directed graph directed cycles. (directed) tree special version DAG.","code":""},{"path":"multivariate-dependencies.html","id":"probabilistic-graphical-models","chapter":"6 Multivariate dependencies","heading":"6.6.3 Probabilistic graphical models","text":"graphical model uses graph describe relationship random variables \\(x_1, \\ldots, x_d\\). variables assumed joint distribution density/mass function \\(p(x_1, x_2, \\ldots, x_d)\\).\nrandom variable placed node graph.structure graph type edges connecting (connecting) pair nodes/variables used describe conditional dependencies, simplify joint distribution.Thus, graphical model essence visualisation joint distribution using structural information graph helping understand mutual relationship among variables.","code":""},{"path":"multivariate-dependencies.html","id":"directed-graphical-models","chapter":"6 Multivariate dependencies","heading":"6.6.4 Directed graphical models","text":"directed graphical model graph structure assumed \nDAG (directed tree, also DAG).joint probability distribution can factorised product conditional probabilities follows:\n\\[\np(x_1, x_2, \\ldots, x_d) = \\prod_i p(x_i  | \\text{pa}(x_i))\n\\]\nThus, overall joint probability distribution specified local conditional distributions graph structure, directions edges providing information parent-child node relationships.Probabilistic DAGs also known “Bayesian networks”.Idea: trying possible trees/graphs fitting data using maximum likelihood (Bayesian inference) hope able identify graph structure data-generating process.Challengesin tree/network internal nodes usually known, thus \ntreated latent variables.Answer: impute states nodes may use EM algorithm GMMs\n(fact can viewed graphical models, !).treat internal nodes unknowns need marginalise \ninternal nodes, .e. need sum / integrate possible set states\ninternal nodes!Answer: can handled effectively using Viterbi algorithm essentially\napplication generalised distributive law. particular tree graphs \nmeans summations occurs locally node propagates recursively across tree.order infer tree network structure space trees networks need \nexplored. possible exhaustive fashion unless number variables\ntree small.Answer: Solution: use heuristic approaches tree network search!Furthermore, exist -called “equivalence classes” graphical models, .e. sets graphical models share joint probability distribution. Thus, graphical models within equivalence class distinguished observational data, even infinite sample size!Answer: fundamental mathematical problem identifiability now way around issue. However,\npositive side, also implies search graphical models can restricted finding -called “essential graph” (e.g. https://projecteuclid.org/euclid.aos/1031833662 )Conclusion: using directed graphical models structure discovery time consuming computationally\ndemanding anything small toy data sets.also explains heuristic non-model based approaches (hierarchical clustering) popular even though full statistical modelling principle possible.","code":""},{"path":"multivariate-dependencies.html","id":"undirected-graphical-models","chapter":"6 Multivariate dependencies","heading":"6.6.5 Undirected graphical models","text":"Another class graphical models models contain undirected edges. undirected graphical models\nused represent pairwise conditional ()dependencies among variables graph, resulting model therefore also called conditional independence graph.Suppose \\(x_i\\) \\(x_j\\) two random variables/nodes \\(\\{x_1, \\ldots, x_d\\}\\), set \\(\\{x_k\\}\\) represents variables/nodes \\(k\\neq \\) \\(k \\neq j\\). variables \\(x_i\\) \\(x_j\\) conditionally independent\ngiven variables \\(\\{x_k\\}\\)\n\\[\nx_i \\perp\\!\\!\\!\\perp x_j | \\{x_k\\}\n\\]\njoint probability density variables \\(\\{x_1, \\ldots, x_d\\}\\)\nfactorises \n\\[\np(x_1, x_2, \\ldots, x_d) = p(x_i | \\{x_k\\}) \\, p(x_j | \\{x_k\\}) \\, p(\\{x_k\\}) \\,.\n\\]\nequivalently\n\\[\n\\frac{p(x_i, x_j, \\ldots, x_d)}{p(\\{x_k\\})}  \n= p(x_i, x_j | \\{x_k\\})\n= p(x_i | \\{x_k\\}) \\, p(x_j | \\{x_k\\}) \\,.\n\\]corresponding conditional independence graph note edge \\(x_i\\) \\(x_j\\),\ngraph missing edges correspond conditional independence respective non-connected nodes.","code":""},{"path":"multivariate-dependencies.html","id":"gaussian-graphical-model","chapter":"6 Multivariate dependencies","heading":"6.6.5.1 Gaussian graphical model","text":"Assuming \\(x_1, \\ldots, x_d\\) jointly normally distributed, .e. \\(\\boldsymbol x\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\),\nturns straightforward identify pairwise conditional independencies.\n\\(\\boldsymbol \\Sigma\\) first obtain precision matrix\n\\[\\boldsymbol \\Omega= (\\omega_{ij}) = \\boldsymbol \\Sigma^{-1} \\,.\\]\nCrucially, can shown \n\\(\\omega_{ij} = 0\\) implies\n\\(x_i \\perp\\!\\!\\!\\perp x_j \\,|\\, \\{ x_k \\}\\).\nHence, precision matrix \\(\\boldsymbol \\Omega\\) can directly read pairwise conditional independencies among variables \\(x_1, x_2, \\ldots, x_d\\).Often, covariance matrix \\(\\boldsymbol \\Sigma\\) dense (zeros) corresponding precision matrix\n\\(\\boldsymbol \\Omega\\) sparse (many zeros).conditional independence graph computed normally distributed variables called\nGaussian graphical model, short GGM. alternative name\ncommonly used covariance selection model.","code":""},{"path":"multivariate-dependencies.html","id":"related-quantity-partial-correlation","chapter":"6 Multivariate dependencies","heading":"6.6.5.2 Related quantity: partial correlation","text":"precision matrix \\(\\boldsymbol \\Omega\\) can also compute matrix pairwise full conditional partial correlations:\\[\n\\rho_{ij|\\text{rest}}=-\\frac{\\omega_{ij}}{\\sqrt{\\omega_{ii}\\omega_{jj}}}\n\\]\nessentially standardised precision matrix (similar correlation extra minus sign!)partial correlations lie range -1 +1, \\(\\rho_{ij|\\text{rest}} \\[-1, 1]\\), just like standard correlations.\\(\\boldsymbol x\\) multivariate normal \\(\\rho_{ij|\\text{rest}} = 0\\) indicates conditional independence\n\\(x_i\\) \\(x_j\\).Regression interpretation: partial correlation correlation remains \ntwo variables effect variables “regressed away”.\nwords, partial correlation exactly equivalent correlation \nresiduals remain regressing \\(x_i\\) variables \\(\\{x_k\\}\\) \\(x_j\\) \\(\\{x_k\\}\\).","code":""},{"path":"multivariate-dependencies.html","id":"null-distribution-of-the-empirical-correlation-coefficient","chapter":"6 Multivariate dependencies","heading":"6.6.6 Null distribution of the empirical correlation coefficient","text":"Suppose two uncorrelated random variables \\(x\\) \\(y\\) \\(\\rho = \\text{Cor}(x, y) =0\\).\nobserving data \\(x_1, \\ldots, x_n\\) \\(y_1, \\ldots, y_n\\) compute \nempirical covariance matrix \\(\\hat{\\boldsymbol \\Sigma}_{xy}\\) \nempirical correlation coefficient \\(r = \\widehat{\\text{Cor}}(x, y)\\).distribution empirical correlation assuming \\(\\rho=0\\) useful null-model testing whether underlying correlation fact zero observed empirical correlation \\(r\\).\n\\(x\\) \\(y\\) normally distributed \\(\\rho=0\\) distribution empirical correlation \\(r\\) mean \\(\\text{E}(r)=0\\) variance \\(\\text{Var}(r)=\\frac{1}{\\kappa}\\).\n\\(\\kappa\\) degree freedom null distribution standard correlation \\(\\kappa=n-1\\).\nFurthermore, squared empirical correlation distributed according Beta distribution\n\\[\nr^2 \\sim \\text{Beta}\\left(\\frac{1}{2}, \\frac{\\kappa-1}{2}\\right)\n\\]partial correlation null distribution \\(r^2\\) form different degree freedom.\nSpecifically, \\(\\kappa\\) reduced number variables conditioned .\n\\(d\\) dimensions condition \\(d-2\\) variables resulting degree freedom \\(\\kappa =n-1 - (d-2) = n-d+1\\). \\(d=2\\) get back degree freedom \nstandard empirical correlation.","code":""},{"path":"multivariate-dependencies.html","id":"algorithm-for-learning-ggms","chapter":"6 Multivariate dependencies","heading":"6.6.7 Algorithm for learning GGMs","text":"can devise simple algorithm learn Gaussian graphical model (GGM)\ndata:Estimate covariance \\(\\hat{\\boldsymbol \\Sigma}\\) (way invertible!)Compute corresponding partial correlationsIf \\(\\hat{\\rho}_{ij|\\text{rest}} \\approx 0\\) (approx.) conditional\nindependence \\(x_i\\) \\(x_j\\).test conditional independence done statistical testing vanishing partial correlation. Specifically, compute \\(p\\)-value assuming true underlying partial correlation zero decide whether reject null assumption zero partial correlation.many edges tested simultaneously may need adjust (.e reduce) \ntest threshold, example applying Bonferroni FDR methods.","code":""},{"path":"multivariate-dependencies.html","id":"example-exam-score-data","chapter":"6 Multivariate dependencies","heading":"6.6.8 Example: exam score data","text":"data set Mardia et al. (1979) features \\(d=5\\) variables measured \n\\(n=88\\) subjects.Correlations (rounded 2 digits):Partial correlations (rounded 2 digits):Note zero correlations \nfour partial correlations close 0, indicating conditional independence :analysis mechanics,statistics mechanics,analysis vectors, andstatistics vectors.can verified computing normal \\(p\\)-values partial correlations (\\(\\kappa=84\\) degree freedom):six edges small \\(p\\)-value (smaller say 0.05) correspond\nedges null assumption zero partial correlation can rejected\nten possible edges four statistically significant.\nTherefore conditional independence graph looks follows:","code":"##            mechanics vectors algebra analysis statistics\n## mechanics       1.00    0.55    0.55     0.41       0.39\n## vectors         0.55    1.00    0.61     0.49       0.44\n## algebra         0.55    0.61    1.00     0.71       0.66\n## analysis        0.41    0.49    0.71     1.00       0.61\n## statistics      0.39    0.44    0.66     0.61       1.00##            mechanics vectors algebra analysis statistics\n## mechanics       1.00    0.33    0.23     0.00       0.02\n## vectors         0.33    1.00    0.28     0.08       0.02\n## algebra         0.23    0.28    1.00     0.43       0.36\n## analysis        0.00    0.08    0.43     1.00       0.25\n## statistics      0.02    0.02    0.36     0.25       1.00##            mechanics vectors algebra analysis statistics\n## mechanics         NA   0.002   0.034    0.988      0.823\n## vectors           NA      NA   0.009    0.477      0.854\n## algebra           NA      NA      NA    0.000      0.001\n## analysis          NA      NA      NA       NA      0.020\n## statistics        NA      NA      NA       NA         NAMechanics      Analysis\n   |     \\    /    |\n   |    Algebra    |\n   |     /   \\     |\n Vectors      Statistics"},{"path":"nonlinear-and-nonparametric-models.html","id":"nonlinear-and-nonparametric-models","chapter":"7 Nonlinear and nonparametric models","heading":"7 Nonlinear and nonparametric models","text":"last part module discuss methods go beyond \nlinear parametric methods prevalent classical multivariate statistics.Relevant textbooks:lectures much part module follow selected chapters following text books:James et al. (2021) introduction statistical learning applications R (2nd edition). Springer.James et al. (2021) introduction statistical learning applications R (2nd edition). Springer.Rogers Girolami (2017) first course machine learning (2nd edition). CRC Press.Rogers Girolami (2017) first course machine learning (2nd edition). CRC Press.Please study relevant section chapters indicated subsection!first book also available version whith examples Python:James et al. (2023) introduction statistical learning applications Python. Springer.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"random-forests","chapter":"7 Nonlinear and nonparametric models","heading":"7.1 Random forests","text":"Another widely used approach prediction nonlinear settings\nmethod random forests.Relevant reading:Please read: James et al. (2021) James et al. (2023) Chapter 8 “Tree-Based Methods”Specifically:Section 8.1 Basics Decision TreesSection 8.2.1 BaggingSection 8.2.2 Random Forests","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"stochastic-vs.-algorithmic-models","chapter":"7 Nonlinear and nonparametric models","heading":"7.1.1 Stochastic vs. algorithmic models","text":"Two cultures statistical modelling: stochastic vs. algorithmic modelsClassic discussion paper Leo Breiman (2001): Statistical modeling: two cultures.\nStatistical Science 16:199–231. https://doi.org/10.1214/ss/1009213726This paper recently revisited following discussion paper Efron (2020) discussants:\nPrediction, estimation, attribution. JASA 115:636–677. https://doi.org/10.1080/01621459.2020.1762613","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"random-forests-1","chapter":"7 Nonlinear and nonparametric models","heading":"7.1.2 Random forests","text":"Proposed Leo Breimann 2001 application “bagging” (Breiman 1996) decision trees.Basic idea:single decision tree unreliable unstable (weak predictor/classifier).Use boostrap generate multiple decision trees (=“forest”)Average predictions tree (=“bagging”, bootstrap aggregation)averaging procedure effect variance stabilisation.\nIntringuingly, averaging across decision trees dramatically improves \noverall prediction accuracy!Random Forests approach example ensemble method\n(since based using “ensemble” trees).Variations: boosting, XGBoost ( https://xgboost.ai/ )Random forests applied Worksheet 11.computationally expensive typically perform well!","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"comparison-of-decision-boundaries-decision-tree-vs.-random-forest","chapter":"7 Nonlinear and nonparametric models","heading":"7.1.3 Comparison of decision boundaries: decision tree vs. random forest","text":"Non-nested case:Compare also decision boundaries LDA QDA (Chapter 4).","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gaussian-processes","chapter":"7 Nonlinear and nonparametric models","heading":"7.2 Gaussian processes","text":"Gaussian processes offer another nonparametric approach model\nnonlinear dependencies. provide probabilistic model \nunknown nonlinear function.Relevant reading:Please read: Rogers Girolami (2017) Chapter 8: Gaussian processes.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"main-concepts","chapter":"7 Nonlinear and nonparametric models","heading":"7.2.1 Main concepts","text":"Gaussian processes (GPs) belong family Bayesian nonparametric modelsIdea:\nstart prior function (!),\ncondition observed data get posterior distribution (function)\nstart prior function (!),condition observed data get posterior distribution (function)GPs use infinitely dimensional multivariate normal distribution prior","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"conditional-multivariate-normal-distribution","chapter":"7 Nonlinear and nonparametric models","heading":"7.2.2 Conditional multivariate normal distribution","text":"GPs make use fact marginal conditional distributions multivariate normal\ndistribution also multivariate normal.Multivariate normal distribution:\\[\\boldsymbol z\\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\]Assume:\n\\[\n\\boldsymbol z=\\begin{pmatrix}\n    \\boldsymbol z_1      \\\\\n    \\boldsymbol z_2      \\\\\n\\end{pmatrix}\n\\]\n\n\\[\n\\boldsymbol \\mu=\\begin{pmatrix}\n    \\boldsymbol \\mu_1      \\\\\n    \\boldsymbol \\mu_2      \\\\\n\\end{pmatrix}\n\\]\n\n\\[\n\\boldsymbol \\Sigma=\\begin{pmatrix}\n    \\boldsymbol \\Sigma_{1}   & \\boldsymbol \\Sigma_{12}   \\\\\n    \\boldsymbol \\Sigma_{12}^T & \\boldsymbol \\Sigma_{2}   \\\\\n\\end{pmatrix}\n\\]\ncorresponding dimensions \\(d_1\\) \\(d_2\\) \\(d_1+d_2=d\\).Marginal distributions:subset \\(\\boldsymbol z\\) also multivariate normally distributed.\nSpecifically,\n\\[\n\\boldsymbol z_1 \\sim N_{d_1}(\\boldsymbol \\mu_1, \\boldsymbol \\Sigma_{1})\n\\]\n\n\\[\n\\boldsymbol z_2 \\sim N_{d_2}(\\boldsymbol \\mu_2, \\boldsymbol \\Sigma_{2})\n\\]Conditional multivariate normal:conditional distribution also multivariate normal:\n\\[\n\\boldsymbol z_1 | \\boldsymbol z_2 = \\boldsymbol z_{1 | 2} \\sim N_{d_1}(\\boldsymbol \\mu_{1|2}, \\boldsymbol \\Sigma_{1 | 2})\n\\]\n\n\\[\\boldsymbol \\mu_{1|2}=\\boldsymbol \\mu_1 + \\boldsymbol \\Sigma_{12} \\boldsymbol \\Sigma_{2}^{-1} (\\boldsymbol z_2 -\\boldsymbol \\mu_2)\\]\n\n\\[\\boldsymbol \\Sigma_{1 | 2}=\\boldsymbol \\Sigma_{1} -  \\boldsymbol \\Sigma_{12} \\boldsymbol \\Sigma_{2}^{-1} \\boldsymbol \\Sigma_{12}^T\\]\\(\\boldsymbol z_{1 | 2}\\) \n\\(\\boldsymbol \\mu_{1|2}\\) dimension \\(d_1 \\times 1\\)\n\\(\\boldsymbol \\Sigma_{1 | 2}\\) dimension \\(d_1 \\times d_1\\),\n.e. dimension unconditioned variables.may recall formula context linear regression, \n\\(y = z_1\\) \\(\\boldsymbol x= \\boldsymbol z_2\\) conditional mean becomes\n\\[\n\\begin{split}\n\\text{E}(y|\\boldsymbol x) &=\\boldsymbol \\mu_y + \\boldsymbol \\Sigma_{y\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} (\\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x})\\\\\n&= \\beta_0+ \\boldsymbol \\beta^T \\boldsymbol x\\\\\n\\end{split}\n\\]\n\\(\\boldsymbol \\beta= \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\)\n\\(\\beta_0 = \\boldsymbol \\mu_y-\\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x}\\), \ncorresponding conditional variance \n\\[\n\\text{Var}(y|\\boldsymbol x) = \\sigma^2_y -  \\boldsymbol \\Sigma_{y\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} \\,.\n\\]","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"covariance-functions-and-kernels","chapter":"7 Nonlinear and nonparametric models","heading":"7.2.3 Covariance functions and kernels","text":"GP prior infinitely dimensional multivariate normal distribution\nmean zero covariance specified function \\(k(x, x^{\\prime})\\):widely used covariance function \n\\[\nk(x, x^{\\prime}) = \\text{Cov}(x, x^{\\prime}) = \\sigma^2 e^{-\\frac{ (x-x^{\\prime})^2}{2 l^2}}\n\\]\nknown squared-exponential kernel Radial-basis function (RBF) kernel.Note kernel implies\\(k(x, x) = \\text{Var}(x) = \\sigma^2\\) \\(\\text{Cor}(x, x^{\\prime}) = e^{-\\frac{ (x-x^{\\prime})^2}{2 l^2}}\\).parameter \\(l\\) RBF kernel length scale parameter describes\n“wigglyness” “stiffness” resulting function.\nSmall values \\(l\\) correspond complex, wiggly functions, low spatial correlation,\ncorrelation decreases quicker distance, large values correspond rigid, stiffer\nfunctions, longer range spatial correlation (note time series context \ncalled autocorrelation).many kernel functions, including linear, polynomial periodic kernels.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gp-model","chapter":"7 Nonlinear and nonparametric models","heading":"7.2.4 GP model","text":"Nonlinear regression GP approach conceptually simple:start multivariate priorthen condition observed datathe resulting conditional multivariate normal can used predict\nfunction values unobserved valuesthe conditional variance can used compute credible intervals predictions.GP regression also provides direct link classical\nBayesian linear regression (using linear kernel).\nFurthermore, GPs also linked neural networks limit\ncase infinitely wide network (see section neural networks).Drawbacks GPs: computationally expensive, typically \\(O(n^3)\\) matrix inversion.\nHowever, now variations GPs help overcome issue (e.g. sparse GPs).","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gaussian-process-example","chapter":"7 Nonlinear and nonparametric models","heading":"7.2.5 Gaussian process example","text":"now show apply Gaussian processes R justing using standard matrix calculations.aim estimate following nonlinear function number observations. Note initially assume additional noise (observations lie directly curve):can now visualise functions sampled multivariate normal prior:Now compute posterior mean variance conditioning observations:Now can plot posterior mean upper lower bounds 95% credible interval:Finally, can take acount noise measured data points adding error term:Note vicinity data points CIs small away\ndata uncertain estimate underlying function becomes.","code":"\ntruefunc = function(x) sin(x)\nXLIM = c(0, 2*pi)\nYLIM = c(-2, 2)\n\nn2 = 10\nx2 = runif(n2, min=XLIM[1], max=XLIM[2])\ny2 = truefunc(x2)  # no noise\n\ncurve( truefunc(x), xlim=XLIM, ylim=YLIM, xlab=\"x\", ylab=\"y\", \n      main=\"True Function\")\npoints(x2, y2)\n# RBF kernel\nrbfkernel = function(xa, xb, s2=1, l=1/2) s2*exp(-1/2*(xa-xb)^2/l^2)\nkfun.mat = function(xavec, xbvec, FUN=rbfkernel) \n  outer(X=as.vector(xavec), Y=as.vector(xbvec), FUN=FUN)\n\n# prior mean\nmu.vec = function(x) rep(0, length(x))\n# grid of x-values \nn1 = 100\nx1 = seq(XLIM[1], XLIM[2], length.out=n1)\n\n# unconditioned covariance and mean (unobserved samples x1)\nK1 = kfun.mat(x1, x1)  \nm1 = mu.vec(x1)\n\n## sample functions from GP prior  \nB = 5\nlibrary(\"MASS\") # for mvrnorm\ny1r = t(mvrnorm(B, mu = m1, Sigma=K1))\n\nplot(x1, y1r[,1], type=\"l\", lwd=2, ylab=\"y\", xlab=\"x\", ylim=YLIM, \n  main=\"Prior Functions (RBF Kernel with l=1/2)\")\nfor(i in 2:B)\n  lines(x1, y1r[,i], col=i, lwd=2)\n# unconditioned covariance and mean (observed samples x2)\nK2 = kfun.mat(x2, x2)\nm2 = mu.vec(x2)\niK2 = solve(K2) # inverse\n\n# cross-covariance\nK12 = kfun.mat(x1, x2)\n\n# Conditioning: x1 conditioned on x2\n\n# conditional mean\nm1.2 = m1 + K12 %*% iK2 %*% (y2 - m2)\n\n# conditional variance\nK1.2 = K1 - K12 %*% iK2 %*% t(K12)\n# upper and lower CI\nupper.bound = m1.2 + 1.96*sqrt(diag(K1.2))\nlower.bound = m1.2 - 1.96*sqrt(diag(K1.2))\n\nplot(x1, m1.2, type=\"l\", xlim=XLIM, ylim=YLIM, col=\"red\", lwd=3,\n  ylab=\"y\", xlab = \"x\", main = \"Posterior\")\n\npoints(x2,y2,pch=4,lwd=4,col=\"blue\")\nlines(x1,upper.bound,lty=2,lwd=3)\nlines(x1,lower.bound,lty=2,lwd=3)\ncurve(truefunc(x), xlim=XLIM, add=TRUE, col=\"gray\")\n\nlegend(x=\"topright\", \n  legend=c(\"posterior mean\", \"posterior quantiles\", \"true function\"),\n  lty=c(1, 2, 1),lwd=c(4, 4, 1), col=c(\"red\",\"black\", \"gray\"), cex=1.0)\n# add some noise\nsdeps = 0.1\nK2 = K2 + sdeps^2*diag(1,length(x2))\n\n# update\niK2 = solve(K2) # inverse\nm1.2 = m1 + K12 %*% iK2 %*% (y2 - m2)\nK1.2 = K1 - K12 %*% iK2 %*% t(K12)\nupper.bound = m1.2 + 1.96*sqrt(diag(K1.2))\nlower.bound = m1.2 - 1.96*sqrt(diag(K1.2))\n\nplot(x1, m1.2, type=\"l\", xlim=XLIM, ylim=YLIM, col=\"red\", lwd=3, \n  ylab=\"y\", xlab = \"x\", main = \"Posterior (with noise)\")\n\npoints(x2,y2,pch=4,lwd=4,col=\"blue\")\nlines(x1,upper.bound,lty=2,lwd=3)\nlines(x1,lower.bound,lty=2,lwd=3)\ncurve(truefunc(x), xlim=XLIM, add=TRUE, col=\"gray\")\n\nlegend(x=\"topright\", \n  legend=c(\"posterior mean\", \"posterior quantiles\", \"true function\"),\n  lty=c(1, 2, 1),lwd=c(4, 4, 1), col=c(\"red\",\"black\", \"gray\"), cex=1.0)"},{"path":"nonlinear-and-nonparametric-models.html","id":"neural-networks","chapter":"7 Nonlinear and nonparametric models","heading":"7.3 Neural networks","text":"Another highly important class models\nnonlinear prediction (nonlinear function approximation) \nneural networks.Relevant reading:Please read: James et al. (2021) James et al. (2023) Chapter 10 “Deep Learning”","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"history","chapter":"7 Nonlinear and nonparametric models","heading":"7.3.1 History","text":"Neural networks actually relatively old models, going back\n1950s.Three phases neural networks (NN)1950/60: replicating functions neurons brain (e.g. perceptron)1980/90: neural networks universal function approximators2010—today: deep learningThe first phase biologically inspired, second phase focused \nmathematical properties, current phase pushed forward \nadvances computer science numerical optimisation:backpropagation algorithmefficient automatic symbolic differentiation (e.g. autograd)stochastic gradient descent algorithms (e.g. Adam)use GPUs TPUs (e.g. linear algebra)availability packages \nsymbolic tensor computations deep learning.Currently popular frameworks :PyTorch (PyTorch Foundation, formerly Meta/Facebook)TensorFlow (Google Research)Flax / JAX (Google Research)high-level wrappers:skorch (scikit-learn wrapper PyTorch)Keras 3 (TensorFlow, JAX, PyTorch)","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"neural-networks-1","chapter":"7 Nonlinear and nonparametric models","heading":"7.3.2 Neural networks","text":"Neural networks essentially stacked systems linear regressions,\nnonlinear mappings layer,\nmapping input output via one \nlayers internal hidden nodes corresponding internal latent variables:internal node nonlinear function nodes previous\nlayerTypically, output node computed using non-linear activation function,\nsigmoid function piecewise linear function (ReLU), \nlinear combination input variables node.simple architecture feedforward network single hidden layer.\ncomplex models multilayer perceptrons convolutional neural networks.can shown even simple network architectures can (sufficient number nodes)\napproximate arbitrary non-linear function. called universal function approximation property.“Deep” neural networks many layers, optimisation parameters requires advanced\ntechniques (see ), objective function typically empirical risk based , e.g.,\nsquared error loss cross-entropy loss. Neural networks highly parameterised models \ntherefore require lots data training, typically also form regularisation (e.g. dropout).extreme example, neural network behind ChatGPT 4 language model\ntrained essentially whole freely accessible text available internet \nestimated 1.76 trillion (!) parameters (\\(1.76 \\times 10^{12}\\)).limit infinite width single layer fully connected neural network becomes\nequivalent Gaussian process. first shown R. M. Neal (1996)21. recently,\nequivalence also demonstrated types neural networks (kernel function \nGP determined neural network architecture). formalised “neural tangent kernel” (NTK)\nframework.statistical aspects neural networks well understood. example, paradox \nneural networks typically overfit training data still generalise well - clearly violates traditional understanding bias-variance tradeoff classical modelling statistics machine learning — see example Belkin et al. (2019)22. researchers argue contradiction can resolved better understanding effective dimension complex models.\nlot current research explain phenomenon “multiple descent”, .e. decrease prediction\nerror models many parameters. topic robustness predictions, also caused overfitting. well known neural networks can sometimes “fooled” -called adversarial examples, e.g., classification sample may change small amount noise added test data.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"learning-more-about-deep-learning","chapter":"7 Nonlinear and nonparametric models","heading":"7.3.3 Learning more about deep learning","text":"good place learn deep learning actual application computer code using various\ndeep learning software frameworks online book “Dive deep learning” \nZhang et al. (2023) available online https://d2l.ai/","code":""},{"path":"further-study.html","id":"further-study","chapter":"A Further study","heading":"A Further study","text":"module can touch surface field multivariate statistics machine learning. like study \nrecommend following books starting point.","code":""},{"path":"further-study.html","id":"recommended-reading","chapter":"A Further study","heading":"A.1 Recommended reading","text":"multivariate statistics machine learning:Izenman (2008) Modern Multivariate Statistical Techniques. Springer.Rogers Girolami (2017) first course machine learning (2nd Edition). Chapman Hall / CRC.James et al. (2021) introduction statistical learning applications R (2nd edition). Springer.James et al. (2023) introduction statistical learning applications Python. Springer.","code":""},{"path":"further-study.html","id":"advanced-reading","chapter":"A Further study","heading":"A.2 Advanced reading","text":"Additional (advanced) reference books probabilistic machine learning :Bishop (2006) Pattern recognition machine learning. Springer.Murphy (2022) Probabilistic Machine Learning: Introduction. MIT Press.Murphy (2023) Probabilistic Machine Learning: Advanced Topics. MIT Press.can find suggestions list online textbooks statistics machine learning.","code":""},{"path":"bibliography.html","id":"bibliography","chapter":"Bibliography","heading":"Bibliography","text":"","code":""}]
