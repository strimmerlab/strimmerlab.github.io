[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"lecture notes MATH20802, course Multivariate Statistics Machine Learning third year mathematics students Department Mathematics University Manchester.course text written Korbinian Strimmer 2018–2021. version 14 November 2021.notes updated time time. view current\nversion visit \nonline MATH38161 lecture notes.\nmay also download MATH38161 lecture notes PDF.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"notes licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"about-the-author","chapter":"Preface","heading":"About the author","text":"Hello! name Korbinian Strimmer Professor Statistics.\npart Statistics group\nDepartment Mathematics University Manchester. can find information home page.first taught module winter term 2018 University Manchester,\nsubsequently also 2019, 2020, 2021.hope enjoy course! questions, comments, corrections please email korbinian.strimmer@manchester.ac.uk.","code":""},{"path":"preface.html","id":"about-the-module","chapter":"Preface","heading":"About the module","text":"","code":""},{"path":"preface.html","id":"topics-covered","chapter":"Preface","heading":"Topics covered","text":"MATH38161 module designed run course 11 weeks.\nsix parts, covering particular aspect multivariate statistics machine learning:Multivariate random variables estimation \nlarge small sample settings (W1 W2)Transformations dimension reduction (W3 W4)Unsupervised learning/clustering (W5 W6)Supervised learning/classification (W7 W8)Measuring modelling multivariate dependencies (W9)Nonlinear nonparametric models (W10, W11)module focuses :Concepts methods (theory)Implementation application RPractical data analysis interpretation (incl. report writing)Modern tools data science statistics (R markdown, R studio)","code":""},{"path":"preface.html","id":"additional-support-material","chapter":"Preface","heading":"Additional support material","text":"Accompanying notes arelecture videos (visualiser style).Furthermore, also MATH38161 online reading list hosted University Manchester library.University Manchester student enrolled module\nfind Blackboard:weekly learning plan 11 week study period,weekly worksheets examples (theory application R) solutions R Markdown, andexam papers previous years.","code":""},{"path":"preface.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"Many thanks Beatriz Costa Gomes help compile first draft course notes winter term 2018 graduate teaching assistant course. also thank many students suggested corrections.","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-random-variables","chapter":"1 Multivariate random variables","heading":"1 Multivariate random variables","text":"","code":""},{"path":"multivariate-random-variables.html","id":"why-multivariate-statistics","chapter":"1 Multivariate random variables","heading":"1.1 Why multivariate statistics?","text":"Science uses experiments verify hypotheses world.\nStatistics provides tools quantify procedure offers methods \nlink data (experiments) probabilistic models (hypotheses).\nSince world complex need complex models complex data, hence\nneed multivariate statistics machine learning.Specifically, multivariate statistics (opposed univariate statistics) concerned methods models random vectors random matrices, rather just random univariate (scalar) variables. Therefore, multivariate statistics frequently make use matrix notation.Closely related multivariate statistics (traditionally subfield statistics) machine learning (ML) traditionally subfield computer science. ML used focus algorithms rather probabilistic modelling nowadays machine learning methods fully based statistical multivariate approaches, two fields converging.Multivariate models provide means learn dependencies interactions among \ncomponents random variables turn allow us draw conclusion underlying mechanisms interest (e.g. biological medical).Two main tasks:unsupervised learning (finding structure, clustering)supervised learning (training labeled data, followed prediction)Challenges:complexity model needs appropriate problem available data,high dimensions make estimation inference difficultcomputational issues.","code":""},{"path":"multivariate-random-variables.html","id":"essentials-in-multivariate-statistics","chapter":"1 Multivariate random variables","heading":"1.2 Essentials in multivariate statistics","text":"","code":""},{"path":"multivariate-random-variables.html","id":"univariate-vs.-multivariate-random-variables","chapter":"1 Multivariate random variables","heading":"1.2.1 Univariate vs. multivariate random variables","text":"Univariate random variable (dimension \\(d=1\\)):\n\\[x \\sim F\\]\n\\(x\\) scalar \\(F\\) distribution.\n\\(\\text{E}(x) = \\mu\\) denotes mean \\(\\text{Var}(x) = \\sigma^2\\) variance \\(x\\).Multivariate random vector dimension \\(d\\):\n\\[\\boldsymbol x= (x_1, x_2,...,x_d)^T  \\sim F\\]\\(\\boldsymbol x\\) vector valued random variable.vector \\(\\boldsymbol x\\) column vector (=matrix size \\(d \\times 1\\)).\ncomponents \\(x_1, x_2,...,x_d\\) univariate random variables.\ndimension \\(d\\) also often denoted \\(p\\) \\(q\\).","code":""},{"path":"multivariate-random-variables.html","id":"mean-of-a-random-vector","chapter":"1 Multivariate random variables","heading":"1.2.2 Mean of a random vector","text":"mean / expectation random vector dimensions \\(d\\) also vector dimensions \\(d\\):\n\\[\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu= \\begin{pmatrix}\n    \\text{E}(x_1)       \\\\\n    \\text{E}(x_2)       \\\\\n    \\vdots \\\\\n    \\text{E}(x_d)\n\\end{pmatrix} = \\left( \\begin{array}{l}\n    \\mu_1       \\\\\n    \\mu_2       \\\\\n    \\vdots \\\\\n    \\mu_d\n\\end{array}\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"variance-of-a-random-vector","chapter":"1 Multivariate random variables","heading":"1.2.3 Variance of a random vector","text":"Recall definition mean variance univariate random variable:\\[\\text{E}(x) = \\mu\\]\\[\\text{Var}(x) = \\sigma^2 = \\text{E}( (x-\\mu)^2 )=\\text{E}( (x-\\mu)(x-\\mu) ) = \\text{E}(x^2)-\\mu^2\\]Definition variance random vector:\\[\\text{Var}(\\boldsymbol x) = \\underbrace{\\boldsymbol \\Sigma}_{d\\times d} = \n\\text{E}\\left(\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d\\times 1} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1\\times d} \\right) \n = \\text{E}(\\boldsymbol x\\boldsymbol x^T)-\\boldsymbol \\mu\\boldsymbol \\mu^T\\]variance random vector , therefore, vector matrix!\\[\\boldsymbol \\Sigma= (\\sigma_{ij}) = \\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix}\\]matrix called Covariance Matrix, -diagonal elements \\(\\sigma_{ij}= \\text{Cov}(x_i,x_j)\\) diagonal \\(\\sigma_{ii}= \\text{Var}(X_i) = \\sigma_i^2\\).","code":""},{"path":"multivariate-random-variables.html","id":"properties-of-the-covariance-matrix","chapter":"1 Multivariate random variables","heading":"1.2.4 Properties of the covariance matrix","text":"\\(\\boldsymbol \\Sigma\\) real valued: \\(\\sigma_{ij} \\\\mathbb{R}\\)\\(\\boldsymbol \\Sigma\\) symmetric: \\(\\sigma_{ij} = \\sigma_{ji}\\)diagonal \\(\\boldsymbol \\Sigma\\) contains \\(\\sigma_{ii} = \\text{Var}(x_i) = \\sigma_i^2\\), .e. \nvariances components \\(\\boldsymbol x\\).-diagonal elements \\(\\sigma_{ij} = \\text{Cov}(x_i,x_j)\\) represent linear dependencies among \\(x_i\\). \\(\\Longrightarrow\\) linear regression, correlationHow many separate entries \\(\\boldsymbol \\Sigma\\) ?\\[\\boldsymbol \\Sigma= (\\sigma_{ij}) = \\underbrace{\\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix}}_{d\\times d}\\]\n\\(\\sigma_{ij} = \\sigma_{ji}\\).Number separate entries: \\(\\frac{d(d+1)}{2}\\).numbers grows square dimension \\(d\\), .e. order \\(O(d^2)\\):large dimension \\(d\\) covariance matrix many components!–> computationally expensive (storage handling)\n–> challenging estimate high dimensions \\(d\\).Note: matrix inversion requires \\(O(d^3)\\) operations using standard algorithms Gauss Jordan elimination.1 Hence, computing \\(\\boldsymbol \\Sigma^{-1}\\) computationally expensive large \\(d\\)!","code":""},{"path":"multivariate-random-variables.html","id":"eigenvalue-decomposition-of-boldsymbol-sigma","chapter":"1 Multivariate random variables","heading":"1.2.5 Eigenvalue decomposition of \\(\\boldsymbol \\Sigma\\)","text":"Recall orthogonal eigendecomposition matrix analysis linear algebra: symmetric matrix real entries real eigenvalues complete set orthogonal eigenvectors.Applying eigenvalue decomposition covariance matrix yields\n\\[\n\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\n\\]\n\\(\\boldsymbol U\\) orthogonal matrix containing eigenvectors\n\n\\[\\boldsymbol \\Lambda= \\begin{pmatrix}\n    \\lambda_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}\n\\end{pmatrix}\\]\ncontains eigenvalues \\(\\lambda_i\\).Importantly, eigenvalues covariance matrix real valued\nconstrained non-negative.\ncan seen computing quadratic form \\(\\boldsymbol z^T \\boldsymbol \\Sigma\\boldsymbol z\\)\nnon-zero non-random vector \\(\\boldsymbol z\\) yields\n\\[\n\\begin{split}\n\\boldsymbol z^T  \\boldsymbol \\Sigma\\boldsymbol z& = \\boldsymbol z^T \\text{E}\\left(  (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T  \\right) \\boldsymbol z\\\\\n & =  \\text{E}\\left( \\boldsymbol z^T (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol z\\right) \\\\\n & =  \\text{E}\\left( \\left( \\boldsymbol z^T (\\boldsymbol x-\\boldsymbol \\mu) \\right)^2 \\right) \\geq 0 \\, .\\\\\n\\end{split}\n\\]\nTherefore covariance matrix \\(\\boldsymbol \\Sigma\\) always\npositive semi-definite.fact, unless collinearity ( .e. variable linear function variables) eigenvalues positive \\(\\boldsymbol \\Sigma\\) positive definite.","code":""},{"path":"multivariate-random-variables.html","id":"quantities-related-to-the-covariance-matrix","chapter":"1 Multivariate random variables","heading":"1.2.6 Quantities related to the covariance matrix","text":"","code":""},{"path":"multivariate-random-variables.html","id":"correlation-matrix-boldsymbol-p","chapter":"1 Multivariate random variables","heading":"1.2.6.1 Correlation matrix \\(\\boldsymbol P\\)","text":"correlation matrix \\(\\boldsymbol P\\) (= upper case greek “rho”) standardised covariance matrix\\[\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}=\\text{Cor}(x_i,x_j)\\]\\[\\rho_{ii} = 1 = \\text{Cor}(x_i,x_i)\\]\\[ \\boldsymbol P= (\\rho_{ij}) = \\begin{pmatrix}\n    1 & \\dots & \\rho_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{d1} & \\dots & 1\n\\end{pmatrix}\\]\\(\\boldsymbol P\\) (“capital rho”) symmetric matrix (\\(\\rho_{ij}=\\rho_{ji}\\)).Note variance-correlation decomposition\\[\\boldsymbol \\Sigma= \\boldsymbol V^{\\frac{1}{2}} \\boldsymbol P\\boldsymbol V^{\\frac{1}{2}}\\]\\(\\boldsymbol V\\) diagonal matrix containing variances:\\[ \\boldsymbol V= \\begin{pmatrix}\n    \\sigma_{11} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma_{dd}\n\\end{pmatrix}\\]\\[\\boldsymbol P= \\boldsymbol V^{-\\frac{1}{2}}\\boldsymbol \\Sigma\\boldsymbol V^{-\\frac{1}{2}}\\]definition correlation written matrix notation.","code":""},{"path":"multivariate-random-variables.html","id":"precision-matrix-or-concentration-matrix","chapter":"1 Multivariate random variables","heading":"1.2.6.2 Precision matrix or concentration matrix","text":"\\[\\boldsymbol \\Omega= (\\omega_{ij}) = \\boldsymbol \\Sigma^{-1}\\]\\(\\boldsymbol \\Omega\\) (“Omega”) inverse covariance matrix.inverse covariance matrix can obtained via\nspectral decomposition, followed inverting eigenvalues \\(\\lambda_i\\):\n\\[\\boldsymbol \\Sigma^{-1} = \\boldsymbol U\\boldsymbol \\Lambda^{-1} \\boldsymbol U^T = \n \\boldsymbol U\\begin{pmatrix}\n    \\lambda_{1}^{-1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}^{-1}\n\\end{pmatrix} \\boldsymbol U^T \\]Note eigenvalues \\(\\lambda_i\\) need positive \\(\\boldsymbol \\Sigma\\) can inverted. (.e., \\(\\boldsymbol \\Sigma\\) needs positive definite).\n\\(\\lambda_i = 0\\) \\(\\boldsymbol \\Sigma\\) singular invertible.Importance \\(\\boldsymbol \\Sigma^{-1}\\):Many expressions multivariate statistics contain \\(\\boldsymbol \\Sigma^{-1}\\) \\(\\boldsymbol \\Sigma\\).\\(\\boldsymbol \\Sigma^{-1}\\) close connection graphical models\n(e.g. conditional independence graph, partial correlations).\\(\\boldsymbol \\Sigma^{-1}\\) natural parameter exponential family perspective.","code":""},{"path":"multivariate-random-variables.html","id":"partial-correlation-matrix","chapter":"1 Multivariate random variables","heading":"1.2.6.3 Partial correlation matrix","text":"standardised version precision matrix, see later chapter graphical models.","code":""},{"path":"multivariate-random-variables.html","id":"total-variation-and-generalised-variance","chapter":"1 Multivariate random variables","heading":"1.2.6.4 Total variation and generalised variance","text":"summarise covariance matrix \\(\\boldsymbol \\Sigma\\) single scalar value two commonly used\nmeasures:total variation: \\(\\text{Tr}(\\boldsymbol \\Sigma) = \\sum_{=1}^d \\lambda_i\\)generalised variance: \\(\\det(\\boldsymbol \\Sigma) = \\prod_{=1}^d \\lambda_i\\)generalised variance \\(\\det(\\boldsymbol \\Sigma)\\) also known volume \\(\\boldsymbol \\Sigma\\).","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-normal-distribution","chapter":"1 Multivariate random variables","heading":"1.3 Multivariate normal distribution","text":"multivariate normal model generalisation univariate normal distribution\ndimension 1 dimension \\(d\\).","code":""},{"path":"multivariate-random-variables.html","id":"univariate-normal-distribution","chapter":"1 Multivariate random variables","heading":"1.3.1 Univariate normal distribution:","text":"\\[\\text{Dimension } d = 1\\]\n\\[x \\sim N(\\mu, \\sigma^2)\\]\n\\[\\text{E}(x) = \\mu \\space , \\space  \\text{Var}(x) = \\sigma^2\\]Density:\\[f(x |\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right) \\]Plot univariate normal density :Unimodal peak \\(\\mu\\), width determined \\(\\sigma\\) (plot: \\(\\mu=2, \\sigma^2=1\\) )Special case: standard normal \\(\\mu=0\\) \\(\\sigma^2=1\\):\\[f(x |\\mu=0,\\sigma^2=1)=\\frac{1}{\\sqrt{2\\pi}} \\exp\\left( {-\\frac{x^2}{2}} \\right) \\]Differential entropy:\\[\nH(F) = \\frac{1}{2} (\\log(2 \\pi \\sigma^2) + 1) \n\\]Cross-entropy:\\[\nH(F_{\\text{ref}}, F) = \\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 } \n +\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2}  +\\log(2 \\pi \\sigma^2) \\right)\n\\]\nKL divergence:\\[\nD_{\\text{KL}}(F_{\\text{ref}}, F) = H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) = \n\\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 } \n +\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2}  -\\log\\left(\\frac{\\sigma^2_{\\text{ref}}}{ \\sigma^2}\\right) -1\n\\right)\n\\]Maximum entropy characterisation: normal distribution unique distribution\n\nhighest (differential) entropy continuous distributions support minus infinity plus infinity given mean variance.fact one reasons normal distribution important (und useful) –\nknow random variable mean variance, much else, using \nnormal distribution reasonable well justified working assumption!","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-normal-model","chapter":"1 Multivariate random variables","heading":"1.3.2 Multivariate normal model","text":"\\[\\text{Dimension } d\\]\n\\[\\boldsymbol x\\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\]\n\\[\\boldsymbol x\\sim \\text{MVN}(\\boldsymbol \\mu,\\boldsymbol \\Sigma) \\]\n\\[\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\space , \\space  \\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\]Density:\\[f(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) = \\det(2 \\pi \\boldsymbol \\Sigma)^{-\\frac{1}{2}} \\exp\\left({{-\\frac{1}{2}} \\underbrace{\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1 \\times d} \\underbrace{\\boldsymbol \\Sigma^{-1}}_{d \\times d} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d \\times 1} }_{1 \\times 1 = \\text{scalar!}}}\\right)\\]note density contains precision matrix \\(\\boldsymbol \\Sigma^{-1}\\)inverting \\(\\boldsymbol \\Sigma\\) implies inverting eigenvalues \\(\\lambda_i\\) \\(\\boldsymbol \\Sigma\\)\n(thus need \\(\\lambda_i > 0\\))density also contains \\(\\det(\\boldsymbol \\Sigma) = \\prod\\limits_{=1}^d \\lambda_i\\) \\(\\equiv\\) product eigenvalues \\(\\boldsymbol \\Sigma\\)Special case: standard multivariate normal \\[\\boldsymbol \\mu=\\boldsymbol 0, \\boldsymbol \\Sigma=\\boldsymbol =\\begin{pmatrix}\n    1 & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & 1\n\\end{pmatrix}\\]\\[f(\\boldsymbol x| \\boldsymbol \\mu=\\boldsymbol 0,\\boldsymbol \\Sigma=\\boldsymbol )=(2\\pi)^{-d/2}\\exp\\left( -\\frac{1}{2} \\boldsymbol x^T \\boldsymbol x\\right) = \\prod\\limits_{=1}^d \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_i^2}{2}\\right)\\]\nequivalent product \\(d\\) univariate standard normals!Misc:\\(d=1\\), multivariate normal reduces normal.\\(\\boldsymbol \\Sigma\\) diagonal (.e. \\(\\boldsymbol P= \\boldsymbol \\), correlation), MVN product univariate normals (see Worksheet 2).Plot MVN density:Location: \\(\\boldsymbol \\mu\\)Shape: \\(\\boldsymbol \\Sigma\\)Unimodal: one peakSupport \\(-\\infty\\) \\(+\\infty\\) dimensionAn interactive R Shiny web app bivariate normal density plot\navailable online https://minerva..manchester.ac.uk/shiny/strimmer/bvn/ .Differential entropy:\\[\nH = \\frac{1}{2} (\\log \\det(2 \\pi \\boldsymbol \\Sigma) + d) \n\\]Cross-entropy:\\[\nH(F_{\\text{ref}}, F) = \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    + \\log \\det \\biggl( 2 \\pi \\boldsymbol \\Sigma\\biggr)    \\biggr\\} \n\\]\nKL divergence:\\[\n\\begin{split}\nD_{\\text{KL}}(F_{\\text{ref}}, F) &= H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) \\\\\n&= \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr) \n     - d   \\biggr\\} \\\\\n\\end{split}\n\\]","code":""},{"path":"multivariate-random-variables.html","id":"shape-of-the-multivariate-normal-density","chapter":"1 Multivariate random variables","heading":"1.3.3 Shape of the multivariate normal density","text":"Now show contour lines multivariate normal density always take form ellipse, radii ellipse determined eigenvalues \n\\(\\boldsymbol \\Sigma\\).start observing circle radius \\(r\\) around origin can described set points \\((x_1,x_2)\\) satisfying\n\\(x_1^2+x_2^2 = r^2\\), equivalently, \\(\\frac{x_1^2}{r^2} + \\frac{x_2^2}{r^2} = 1\\).\ngeneralised shape ellipse allowing (two dimensions) two radii\n\\(r_1\\) \\(r_2\\) \n\\(\\frac{x_1^2}{r_1^2} + \\frac{x_2^2}{r_2^2} = 1\\), vector notation\n\\(\\boldsymbol x^T \\text{Diag}(r_1^2, r_2^2)^{-1} \\boldsymbol x= 1\\). \\(d\\) dimensions allowing rotation \naxes shift origin 0 \\(\\boldsymbol \\mu\\) condition ellipse \n\\[(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol Q\\, \\text{Diag}(r_1^2, \\ldots , r_d^2)^{-1} \\boldsymbol Q^T (\\boldsymbol x-\\boldsymbol \\mu) = 1\\]\n\\(\\boldsymbol Q\\) orthogonal matrix whose column vectors indicate direction axes.contour line probability density function set connected points density assumes constant value. case multivariate normal distribution keeping density fixed value implies \\((\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu) = c\\) \\(c\\) constant. Using eigenvalue decomposition \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) can rewrite condition \n\\[\n(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol U\\boldsymbol \\Lambda^{-1} \\boldsymbol U^T (\\boldsymbol x-\\boldsymbol \\mu) = c \\,.\n\\]\nimplies thatthe contour lines multivariate normal density indeed ellipses,squared radii proportional eigenvalues \\(\\boldsymbol \\Sigma\\) andthe direction axes correspond eigenvectors \\(\\boldsymbol U\\).Equivalently, positive square roots eigenvalues proportional radii ellipse. Hence, singular covariance matrix one \\(\\lambda_i=0\\) corresponding radii zero.interactive R Shiny web app play contour lines \nbivariate normal distribution online https://minerva..manchester.ac.uk/shiny/strimmer/bvn/ .","code":""},{"path":"multivariate-random-variables.html","id":"three-types-of-covariances","chapter":"1 Multivariate random variables","heading":"1.3.4 Three types of covariances","text":"Following can parameterise covariance matrix terms \n) volume, ii) shape, iii) orientation writing\n\\[\n\\boldsymbol \\Sigma= \\kappa \\, \\boldsymbol U\\boldsymbol \\boldsymbol U^T\n\\]\n\\(\\boldsymbol =\\text{Diag}(a_1, \\ldots, a_d)\\) \\(\\prod_{=1}^d a_i = 1\\).Note eigenvalues \\(\\boldsymbol \\Sigma\\) \\(\\lambda_i = \\kappa a_i\\).volume \\(\\det(\\boldsymbol \\Sigma) = \\kappa^d\\), determined single parameter \\(\\kappa\\).shape determined \\(\\boldsymbol \\), \\(d-1\\) free parameters.orientation given orthogonal matrix \\(\\boldsymbol U\\), \\(d (d-1)/2\\) free parameters.leads classification covariances three varieties:Type 1: spherical covariance \\(\\boldsymbol \\Sigma=\\kappa \\boldsymbol \\),\nspherical contour lines, 1 free parameter (\\(\\boldsymbol =\\boldsymbol \\), \\(\\boldsymbol U=\\boldsymbol \\)).Example:\n\\(\\boldsymbol \\Sigma= \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1/ \\lambda_2} = 1\\):Type 2: diagonal covariance \\(\\boldsymbol \\Sigma= \\kappa \\boldsymbol \\), elliptical contour lines axes ellipse oriented parallel coordinates, \\(d\\) free parameters (\\(\\boldsymbol U=\\boldsymbol \\)).Example:\n\\(\\boldsymbol \\Sigma= \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1 / \\lambda_2} \\approx 1.41\\):Type 3: general unrestricted covariance \\(\\boldsymbol \\Sigma\\),\nelliptical contour lines, axes ellipse oriented according \ncolumn vectors \\(\\boldsymbol U\\),\n\\(d (d+1)/2\\) free parameters.Example:\n\\(\\boldsymbol \\Sigma= \\begin{pmatrix} 2 & 0.6 \\\\ 0.6 & 1 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1 / \\lambda_2} \\approx 2.20\\):","code":""},{"path":"multivariate-random-variables.html","id":"concentration-of-probability-mass-for-small-and-large-dimension","chapter":"1 Multivariate random variables","heading":"1.3.5 Concentration of probability mass for small and large dimension","text":"density multivariate normal distribution bell shape single mode. Thus, intuitively one may believe probability mass must concentrated around peak. true small dimensions now show completely incorrect high dimensions.simplicity consider standard multivariate normal distribution \\(\\boldsymbol x\\sim N_d(\\boldsymbol 0, \\boldsymbol I_d)\\) spherical covariance \\(\\boldsymbol I_d\\) sample \\(\\boldsymbol x\\).\nsquared Euclidean length \\(\\boldsymbol x\\) given \\(= || \\boldsymbol x||^2 = \\boldsymbol x^T \\boldsymbol x= \\sum_{=1}^d x_i^2\\). note \\(\\sim \\text{$\\chi^2_{d}$}\\) chi-squared distributed\ndegree freedom \\(d\\) individual component distributed \\(x_i \\sim N(0,1)\\).\ndensity \\(d\\)-dimensional multivariate normal function \\(\\) \\(g_d() = (2\\pi)^{-d/2} e^{-/2}\\).natural way define main part “bell” multivariate normal \nset values \\(\\) density larger given small fraction \\(\\eta\\) (say 0.001) value density peak.\nformalise\n\\[\nB = \\{ : g_d() > \\eta \\, g_d(0)   \\}\n\\]\nEquivalently, \\(B = \\{ : \\frac{g_d()}{ g_d(0)} =e^{-/2} > \\eta \\}\\) \n\\(B = \\{ : < -2 \\log(\\eta) \\}\\).Since know \\(\\) chi-squared distributed\nprobability \\(\\text{Pr}(\\B)\\) random \\(\\) lie within bell \\(B\\) now easily computed using cumulative density function chi-squared distribution. analytic formula can compute probability numerically dependence dimension \\(d\\):\nplot \\(\\eta=0.001\\). can see dimensions around \\(d=10\\)\nprobability mass concentrated bell center \\(d=30\\) moved completely tail distribution outside bell!","code":""},{"path":"multivariate-random-variables.html","id":"estimation-in-large-sample-and-small-sample-settings","chapter":"1 Multivariate random variables","heading":"1.4 Estimation in large sample and small sample settings","text":"practical application multivariate normal model need \nlearn parameters data. first consider case \nmany measurements available, second case \nnumber data points small compared number parameters.previous course year 2\n(see MATH20802 Statistical Methods)\nmethod maximum likelihood well essential Bayesian statistics\nintroduced. apply approaches setting \nmultivariate normal distribution.","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-data","chapter":"1 Multivariate random variables","heading":"1.4.1 Multivariate data","text":"Vector notation:Samples multivariate normal distribution vectors (scalars univariate normal):\n\\[\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_n \\stackrel{\\text{iid}}\\sim N_d\\left(\\boldsymbol \\mu,\\boldsymbol \\Sigma\\right)\\]Matrix component notation:data points commonly collected matrix \\(\\boldsymbol X\\).statistics convention store data vector rows \\(\\boldsymbol X\\):\\[\\boldsymbol X= (\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_n)^T = \\begin{pmatrix}\n    x_{11}  & x_{12} & \\dots & x_{1d}   \\\\\n    x_{21}  & x_{22} & \\dots & x_{2d}   \\\\\n    \\vdots \\\\\n    x_{n1}  & x_{n2} & \\dots & x_{nd}\n\\end{pmatrix}\\]Therefore,\n\\[\\boldsymbol x_1=\\begin{pmatrix}\n    x_{11}       \\\\\n    \\vdots \\\\\n    x_{1d}\n\\end{pmatrix} , \\space \\boldsymbol x_2=\\begin{pmatrix}\n    x_{21}       \\\\\n    \\vdots \\\\\n    x_{2d}\n\\end{pmatrix} , \\ldots , \\boldsymbol x_n=\\begin{pmatrix}\n    x_{n1}       \\\\\n    \\vdots \\\\\n    x_{nd}\n\\end{pmatrix}\\]Thus, statistics first index runs \\((1,...,n)\\) denotes samples second index runs \\((1,...,d)\\) refers variables.statistics convention data matrices universal! fact, machine learning literature engineering computer science data samples stored columns variables appear rows (thus engineering convention data matrix transposed compared statistics convention).order avoid confusion recommended use vector notation data instead \nmatrix notation ambiguous.","code":""},{"path":"multivariate-random-variables.html","id":"strategies-for-large-sample-estimation","chapter":"1 Multivariate random variables","heading":"1.4.2 Strategies for large sample estimation","text":"","code":""},{"path":"multivariate-random-variables.html","id":"empirical-estimators-outline","chapter":"1 Multivariate random variables","heading":"1.4.2.1 Empirical estimators (outline)","text":"large \\(n\\) thanks law large numbers:\n\\[\\underbrace{F}_{\\text{true}} \\approx \\underbrace{\\widehat{F}}_{\\text{empirical}}\\]now like estimate \\(\\) functional \\(F\\), .e. \\(=m(F)\\).\nexample mean, median quantity.empirical estimate obtained replacing unknown true distribution\n\\(F\\) observed empirical distribution: \\(\\hat{} = m(\\widehat{F})\\).example, expectation random variable approximated/estimated\naverage observation:\n\\[\\text{E}_F(\\boldsymbol x) \\approx \\text{E}_{\\widehat{F}}(\\boldsymbol x) = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k\\]\n\\[\\text{E}_F(g(\\boldsymbol x)) \\approx  \\text{E}_{\\widehat{F}}(g(\\boldsymbol x)) = \\frac{1}{n}\\sum^{n}_{k=1} g(\\boldsymbol x_k)\\]Simple recipe obtain empirical estimator: simply replace expectation operator\nsample average quantity interest.work: empirical distribution \\(\\widehat{F}\\) actually nonparametric maximum likelihood estimate \\(F\\) (see likelihood estimation).Note: approximation \\(F\\) \\(\\widehat{F}\\) also basis approaches Efron’s bootstrap method.","code":""},{"path":"multivariate-random-variables.html","id":"maximum-likelihood-estimation-outline","chapter":"1 Multivariate random variables","heading":"1.4.2.2 Maximum likelihood estimation (outline)","text":"R.. Fisher (1922): model-based estimators using density probability mass functionlog-likelihood function:\n\\[\\log L(\\boldsymbol \\theta) = \\sum^{n}_{k=1}  \\underbrace{\\log f}_{\\text{log-density}}(\\underbrace{x_i}_{\\text{data}} |\\underbrace{\\boldsymbol \\theta}_{\\text{parameters}})\\]\nlikelihood = probability observe data given model parametersMaximum likelihood estimate:\n\\[\\hat{\\boldsymbol \\theta}^{\\text{ML}}=\\underset{\\boldsymbol \\theta}{\\arg\\,\\max} \\log L(\\boldsymbol \\theta)\\]Maximum likelihood (ML) finds parameters make observed data likely (find probable model!)Recall MATH20802 Statistical Methods\nmaximum likelihood closely linked minimising relative entropy (KL divergence)\n\\(D_{\\text{KL}}(F, F_{\\boldsymbol \\theta})\\) unknown true model \\(F\\) specified model \\(F_{\\boldsymbol \\theta}\\). Specifically, large\nsample size \\(n\\) model \\(F_{\\hat{\\boldsymbol \\theta}}\\) fit maximum likelihood indeed model closest \\(F\\).Correspondingly, great appeal maximum likelihood estimates (MLEs) optimal large \\(\\mathbf{n}\\), .e. large sample size estimator can constructed outperforms MLE (note emphasis “large \\(n\\)”!).\nadvantage method maximum likelihood provide point estimate also asymptotic error (via Fisher information related curvature log-likelihood function).","code":""},{"path":"multivariate-random-variables.html","id":"large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma","chapter":"1 Multivariate random variables","heading":"1.4.3 Large sample estimates of mean \\(\\boldsymbol \\mu\\) and covariance \\(\\boldsymbol \\Sigma\\)","text":"","code":""},{"path":"multivariate-random-variables.html","id":"empirical-estimates","chapter":"1 Multivariate random variables","heading":"1.4.3.1 Empirical estimates:","text":"Recall definitions:\n\\[\n\\boldsymbol \\mu= \\text{E}(\\boldsymbol x)\n\\]\n\n\\[\n\\boldsymbol \\Sigma= \\text{E}\\left(   (\\boldsymbol x-\\boldsymbol \\mu) (\\boldsymbol x-\\boldsymbol \\mu)^T \\right)\n\\]empirical estimate replace expectations \ncorresponding sample averages.resulting estimators can written three different ways:Vector notation:\\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k\\]\\[\n\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\sum^{n}_{k=1} (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})  (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})^T\n= \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k  \\boldsymbol x_k^T - \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\n\\]Data matrix notation:empirical mean covariance can also written terms data matrix \\(\\boldsymbol X\\) (using statistics convention):\\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n} \\boldsymbol X^T \\boldsymbol 1_n\\]\\[\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\boldsymbol X^T \\boldsymbol X- \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\\]See Worksheet 2 details.Component notation:corresponding component notation \\(\\boldsymbol X= (x_{ki})\\) :\\[\\hat{\\mu}_i = \\frac{1}{n}\\sum^{n}_{k=1} x_{ki}\\]\\[\\hat{\\sigma}_{ij} = \\frac{1}{n}\\sum^{n}_{k=1} (x_{ki}-\\hat{\\mu}_i) ( \nx_{kj}-\\hat{\\mu}_j )\\]\\[\\hat{\\boldsymbol \\mu}=\\begin{pmatrix}\n    \\hat{\\mu}_{1}       \\\\\n    \\vdots \\\\\n    \\hat{\\mu}_{d}\n\\end{pmatrix}, \\widehat{\\boldsymbol \\Sigma} = (\\hat{\\sigma}_{ij})\\]Variance estimate:\\[\\hat{\\sigma}_{ii} = \\frac{1}{n}\\sum^{n}_{k=1} \\left(x_{ki}-\\hat{\\mu}_i\\right)^2\\]\nNote factor \\(\\frac{1}{n}\\) (\\(\\frac{1}{n-1}\\))Engineering machine learning convention:Using engineering machine learning convention data matrix \\(\\boldsymbol X\\) estimators written \\[\\hat{\\boldsymbol \\mu} = \\frac{1}{n} \\boldsymbol X\\boldsymbol 1_n\\]\\[\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\boldsymbol X\\boldsymbol X^T - \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\\]corresponding component notation two indices columns rowns interchanged.avoid confusion using matrix component notation need always state \nconvention used! notes strictly follow statistics convention.","code":""},{"path":"multivariate-random-variables.html","id":"maximum-likelihood-estimates","chapter":"1 Multivariate random variables","heading":"1.4.3.2 Maximum likelihood estimates","text":"now derive MLE parameters \\(\\boldsymbol \\mu\\) \\(\\boldsymbol \\Sigma\\) multivariate normal distribution.\ncorresponding log-likelihood function \n\\[\n\\begin{split}\n\\log L(\\boldsymbol \\mu, \\boldsymbol \\Sigma) & = \\sum_{k=1}^n \\log f( \\boldsymbol x_k | \\boldsymbol \\mu, \\boldsymbol \\Sigma) \\\\\n  & = -\\frac{n d}{2} \\log(2\\pi) -\\frac{n}{2} \\log \\det(\\boldsymbol \\Sigma)  \n   - \\frac{1}{2}  \\sum_{k=1}^n  (\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x_k-\\boldsymbol \\mu) \\,.\\\\\n\\end{split}\n\\]\nWritten terms precision matrix \\(\\boldsymbol \\Omega= \\boldsymbol \\Sigma^{-1}\\) becomes\n\\[\n\\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega) = -\\frac{n d}{2} \\log(2\\pi) +\\frac{n}{2} \\log \\det(\\boldsymbol \\Omega)  - \\frac{1}{2}  \\sum_{k=1}^n  (\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Omega(\\boldsymbol x_k-\\boldsymbol \\mu) \\,.\n\\]\nExploiting identities trace log det (see Appendix) can rewrite\n\\((\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Omega(\\boldsymbol x_k-\\boldsymbol \\mu) = \\text{Tr}( (\\boldsymbol x_k-\\boldsymbol \\mu) (\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Omega)\\)\n\\(\\log \\det(\\boldsymbol \\Omega) = \\text{Tr}( \\log \\boldsymbol \\Omega)\\). leads log-likelihood\n\\[\\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega) =  -\\frac{n d}{2} \\log(2\\pi) +\\frac{n}{2}  \\text{Tr}( \\log \\boldsymbol \\Omega)  - \\frac{1}{2}  \\sum_{k=1}^n  \\text{Tr}( (\\boldsymbol x_k-\\boldsymbol \\mu) (\\boldsymbol x_k-\\boldsymbol \\mu)^T \\boldsymbol \\Omega) \\,.\\]First, find MLE \\(\\boldsymbol \\mu\\) compute (see Appendix rules vector matrix calculus)\n\\[\\frac{\\partial \\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega) }{\\partial \\boldsymbol \\mu}= \\sum_{k=1}^n (\\boldsymbol x_k-\\boldsymbol \\mu)^T  \\boldsymbol \\Omega\\,.\\]\nSetting equal zero get \\(\\sum_{k=1}^n \\boldsymbol x_k = n \\hat{\\boldsymbol \\mu}_{ML}\\) thus\n\\[\\hat{\\boldsymbol \\mu}_{ML} = \\frac{1}{n} \\sum_{k=1}^n \\boldsymbol x_k\\,.\\]Next, obtain MLE \\(\\boldsymbol \\Omega\\) compute\n\\[\\frac{\\partial \\log L(\\boldsymbol \\mu, \\boldsymbol \\Omega) }{\\partial \\boldsymbol \\Omega}=\\frac{n}{2}\\boldsymbol \\Omega^{-1} - \\frac{1}{2}  \\sum_{k=1}^n (\\boldsymbol x_k-\\boldsymbol \\mu) (\\boldsymbol x_k-\\boldsymbol \\mu)^T\\,.\\]\nSetting equal zero substituting MLE \\(\\boldsymbol \\mu\\) get\n\\[\\widehat{\\boldsymbol \\Omega}^{-1}_{ML}=  \\frac{1}{n} \\sum_{k=1}^n  (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu}) (\\boldsymbol x_k-\\hat{\\boldsymbol \\mu})^T=\\widehat{\\boldsymbol \\Sigma}_{ML}\\,.\\]Therefore, MLEs identical empirical estimates.Note factor \\(\\frac{1}{n}\\) MLE covariance matrix.","code":""},{"path":"multivariate-random-variables.html","id":"distribution-of-the-empirical-maximum-likelihood-estimates","chapter":"1 Multivariate random variables","heading":"1.4.3.3 Distribution of the empirical / maximum likelihood estimates","text":"\\(\\boldsymbol x_1,...,\\boldsymbol x_n \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) one can find exact distributions\nestimators.1. Distribution estimate mean:\\[\\hat{\\boldsymbol \\mu}_{ML} \\sim N_d\\left(\\boldsymbol \\mu, \\frac{\\boldsymbol \\Sigma}{n}\\right)\\]\nSince\n\\(\\text{E}(\\hat{\\boldsymbol \\mu}_{ML}) = \\boldsymbol \\mu\\Longrightarrow \\hat{\\boldsymbol \\mu}_{ML}\\) unbiased.2. Distribution covariance estimate:\\[\\widehat{\\boldsymbol \\Sigma}_{ML} \\sim \\text{Wishart}(\\frac{\\boldsymbol \\Sigma}{n}, n-1)\\]\nSince\n\\(\\text{E}(\\widehat{\\boldsymbol \\Sigma}_{ML}) = \\frac{n-1}{n}\\boldsymbol \\Sigma\\) \\(\\Longrightarrow \\widehat{\\boldsymbol \\Sigma}_{ML}\\) biased, \\(\\text{Bias}(\\widehat{\\boldsymbol \\Sigma}_{ML} ) = \\boldsymbol \\Sigma- \\text{E}(\\widehat{\\boldsymbol \\Sigma}_{ML}) = -\\frac{\\boldsymbol \\Sigma}{n}\\).Easy make unbiased:\n\\(\\widehat{\\boldsymbol \\Sigma}_{UB} = \\frac{n}{n-1}\\widehat{\\boldsymbol \\Sigma}=\\frac{1}{n-1}\\sum^n_{k=1}\\left(\\boldsymbol x_k-\\hat{\\boldsymbol \\mu}\\right)\\left(\\boldsymbol x_k-\\hat{\\boldsymbol \\mu}\\right)^T\\) unbiased.unbiasedness estimator relevant criterion multivariate statistics see next section.","code":""},{"path":"multivariate-random-variables.html","id":"problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions","chapter":"1 Multivariate random variables","heading":"1.4.4 Problems with maximum likelihood in small sample settings and high dimensions","text":"Modern data high dimensional!Data sets \\(n<d\\), .e. high dimension \\(d\\) small sample size \\(n\\) now common \nmany fields, e.g., medicine, biology also finance business analytics.\\[n = 100 \\, \\text{(e.g, patients/samples)}\\]\n\\[d = 20000 \\, \\text{(e.g., genes/SNPs/proteins/variables)}\\]\nReasons:number measured variables increasing quickly technological advances (e.g. genomics)number samples similary increased (cost ethical reasons)General problems MLEs:ML estimators optimal sample size large compared number parameters. However, optimality valid sample size moderate smaller number parameters.enough data ML estimate overfits. means ML fits current data perfectly resulting model generalise well (.e. model perform poorly prediction)choice different models different complexity ML always select model largest number parameters.-> high-dimensional data small sample size maximum likelihood estimation work!!!History Statistics:Much modern statistics (1960 onwards) devoted development inference estimation techniques work complex, high-dimensional data.Maximum likelihood method classical statistics (time 1960).1960 modern (computational) statistics emerges, starting \n“Stein Paradox” (1956): Charles Stein showed multivariate setting ML estimators dominated (= always worse ) shrinkage estimators!example, shrinkage estimator mean better (terms MSE) average (MLE)!Modern statistics developed many different (related) methods use high-dimensional, small sample settings:regularised estimatorsshrinkage estimatorspenalised maximum likelihood estimatorsBayesian estimatorsEmpirical Bayes estimatorsKL / entropy-based estimatorsMost scope class, covered advanced statistical courses.Next, describe simple regularised estimator estimation covariance\nuse later (.e. classification).","code":""},{"path":"multivariate-random-variables.html","id":"estimation-of-covariance-matrix-in-small-sample-settings","chapter":"1 Multivariate random variables","heading":"1.4.5 Estimation of covariance matrix in small sample settings","text":"Problems ML estimate \\(\\boldsymbol \\Sigma\\)\\(\\Sigma\\) O(\\(d^2\\)) number parameters!\n\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}^{\\text{MLE}}\\) requires lot data! \\(n\\gg d \\text{ } d^2\\)\\(\\Sigma\\) O(\\(d^2\\)) number parameters!\n\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}^{\\text{MLE}}\\) requires lot data! \\(n\\gg d \\text{ } d^2\\)\\(n < d\\) \\(\\hat{\\boldsymbol \\Sigma}\\) positive semi-definite (even \\(\\Sigma\\) p.d.f.!)\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}\\) vanishing eigenvalues (\\(\\lambda_i=0\\)) thus inverted singular!\\(n < d\\) \\(\\hat{\\boldsymbol \\Sigma}\\) positive semi-definite (even \\(\\Sigma\\) p.d.f.!)\\(\\Longrightarrow \\hat{\\boldsymbol \\Sigma}\\) vanishing eigenvalues (\\(\\lambda_i=0\\)) thus inverted singular!Simple regularised estimate \\(\\boldsymbol \\Sigma\\)Regularised estimator \\(\\boldsymbol S^\\ast\\) = convex combination \\(\\boldsymbol S= \\hat{\\boldsymbol \\Sigma}^\\text{MLE}\\) \\(\\boldsymbol I_d\\) (identity matrix) getRegularisation:\n\\[\\underbrace{\\boldsymbol S^\\ast}_{\\text{regularised estimate}} = \\underbrace{\\lambda}_{\\text{shrinkage intensity}} \\, \\underbrace{\\boldsymbol I_d}_{\\text{target}} + (1-\\lambda)\\underbrace{\\boldsymbol S}_{\\text{ML estimate}}\\]\nNext, choose \\(\\lambda \\[0,1]\\) \\(\\boldsymbol S^\\ast\\) better (terms MSE) \\(\\boldsymbol S\\) \\(\\boldsymbol I_d\\).Bias-variance trade-\\(\\text{MSE}\\) Mean Squared Error, composed squared bias variance.\\[\\text{MSE}(\\theta) = \\text{E}((\\hat{\\theta}-\\theta)^2) = \\text{Bias}(\\hat{\\theta})^2 + \\text{Var}(\\hat{\\theta})\\]\n\\(\\text{Bias}(\\hat{\\theta}) = \\text{E}(\\hat{\\theta})-\\theta\\)\\(\\boldsymbol S\\): ML estimate, many parameters, low bias, high variance\\(\\boldsymbol I_d\\): “target”, parameters, high bias, low variance\\(\\Longrightarrow\\) reduce high variance \\(\\boldsymbol S\\) introducing bit bias \\(\\boldsymbol I_d\\)!\\(\\Longrightarrow\\) overall, \\(\\text{MSE}\\) decreasedHow find optimal shrinkage / regularisation parameter \\(\\lambda\\)? Minimise \\(\\text{MSE}\\)!Challenge: since don’t know true \\(\\boldsymbol \\Sigma\\) actually compute \\(\\text{MSE}\\) directly estimate ! done practise?cross-validation (=resampling procedure)using analytic approximation (e.g. Stein’s formula)regularisation \\(\\hat{\\boldsymbol \\Sigma}\\) work?\\(\\boldsymbol S^\\ast\\) positive definite:\nMatrix Theory:\\[\\underbrace{\\boldsymbol M_1}_{ \\text{symmetric positive definite, } \\lambda \\boldsymbol } + \\underbrace{\\boldsymbol M_2}_{\\text{symmetric positive semi-definite, } (1-\\lambda) \\boldsymbol S} = \\underbrace{\\boldsymbol M_3}_{\\text{symmetric positive definite, } \\boldsymbol S^\\ast} \\]\\(\\Longrightarrow \\boldsymbol S^\\ast\\) can inverted even \\(n<d\\)(see Appendix details).’s Bayesian disguise!\n\\[\\underbrace{\\boldsymbol S^\\ast}_{\\text{posterior mean}} = \\underbrace{\\lambda \\boldsymbol I_d}_{\\text{prior information}}  + (1-\\lambda)\\underbrace{\\boldsymbol S}_{\\text{data summarised maximum likelihood}}\\]\nPrior information helps infer \\(\\boldsymbol \\Sigma\\) even small samples\nSince \\(\\lambda\\) chosen data, actually empirical Bayes.\nalso called shrinkage estimator since -diagonal entries shrunk towards zero\ntype linear shrinkage/regularisation natural exponential family models (Diaconis Ylvisaker, 1979)\nPrior information helps infer \\(\\boldsymbol \\Sigma\\) even small samplesSince \\(\\lambda\\) chosen data, actually empirical Bayes.also called shrinkage estimator since -diagonal entries shrunk towards zerothis type linear shrinkage/regularisation natural exponential family models (Diaconis Ylvisaker, 1979)Worksheet 2 empirical estimator covariance compared covariance estimator implemented R package\n“corpcor”. uses regularisation similar (correlation rather \ncovariance matrix) \nemploys analytic data-adaptive estimate shrinkage intensity \\(\\lambda\\).\nestimator variant empirical Bayes / James-Stein estimator (see MATH20802 Statistical Methods).\n.SummaryIn multivariate statistics, useful (often necessary) utilise prior information!Regularisation introduces bias reduces variance, minimising overall MSEUnbiased estimation (highly valued property classical statistics!) good idea multivariate settings often leads poor estimators.","code":""},{"path":"multivariate-random-variables.html","id":"categorical-and-multinomial-distribution","chapter":"1 Multivariate random variables","heading":"1.5 Categorical and multinomial distribution","text":"","code":""},{"path":"multivariate-random-variables.html","id":"categorical-distribution","chapter":"1 Multivariate random variables","heading":"1.5.1 Categorical distribution","text":"categorical distribution generalisation Bernoulli distribution\ncorrespondingly also known Multinoulli distribution.Assume \\(K\\) classes labeled “class 1”, “class 2”, …, “class K”.\ndiscrete random variable state space consisting \\(K\\) classes\ncategorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\).\nparameter vector\n\\(\\boldsymbol \\pi= (\\pi_1, \\ldots, \\pi_K)^T\\) specifies\nprobabilities \\(K\\) classes \\(\\text{Pr}(\\text{\"class k\"}) = \\pi_k\\).\nparameters satisfy \\(\\pi_k \\[0,1]\\) \n\\(\\sum_{k=1}^K \\pi_k = 1\\), hence \\(K-1\\) independent parameters categorical distribution (\\(K\\)).Sampling categorical distributions \\(\\text{Cat}(\\boldsymbol \\pi)\\) yields one \\(K\\) classes.\nseveral ways numerically\nrepresent “class k”, example simply corresponding number \\(k\\). However, instead\n“integer encoding” often\nconvenient use -called “one hot encoding” class\nrepresented indicator vector\n\\(\\boldsymbol x= (x_1, \\ldots, x_K)^T = (0, 0, \\ldots, 1, \\ldots, 0)^T\\) containing zeros everywhere except \nelement \\(x_k=1\\) position \\(k\\). Thus \\(x_k \\\\{ 0, 1\\}\\) \\(\\sum_{k=1}^K x_k = 1\\).expectation \\(\\boldsymbol x\\sim \\text{Cat}(\\boldsymbol \\pi)\\) \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\pi\\), \n\\(\\text{E}(x_k) = \\pi_k\\).\ncovariance matrix \\(\\text{Var}(\\boldsymbol x) = \\text{Diag}(\\boldsymbol \\pi) - \\boldsymbol \\pi\\boldsymbol \\pi^T\\).\ncomponent notation \\(\\text{Var}(x_i) = \\pi_i (1-\\pi_i)\\) \\(\\text{Cov}(x_i, x_j) = -\\pi_i \\pi_j\\).\nfollows directly definition variance \\(\\text{Var}(\\boldsymbol x) = \\text{E}( \\boldsymbol x\\boldsymbol x^T) - \\text{E}( \\boldsymbol x) \\text{E}( \\boldsymbol x)^T\\)\nnoting \\(x_i^2 = x_i\\) \\(x_i x_j = 0\\) \\(\\neq j\\).\nNote variance matrix \\(\\text{Var}(\\boldsymbol x)\\) singular construction, \\(K\\) random variables\n\\(x_1, \\ldots, x_K\\) dependent constraint \\(\\sum_{k=1}^K x_k = 1\\).corresponding probability mass function (pmf)\ncan written conveniently terms \\(x_k\\) \n\\[\nf(\\boldsymbol x) = \\prod_{k=1}^K \\pi_k^{x_k} = \n\\begin{cases} \n   \\pi_k  & \\text{} x_k = 1 \\\\\n\\end{cases}\n\\]\nlog pmf \n\\[\n\\log f(\\boldsymbol x) = \\sum_{k=1}^K x_k \\log \\pi_k   =\n\\begin{cases} \n   \\log \\pi_k  & \\text{} x_k = 1 \\\\\n\\end{cases}\n\\]order explicit categorical distribution \\(K-1\\) \\(K\\) parameters\nrewrite log-density \n\\(\\pi_K = 1 - \\sum_{k=1}^{K-1} \\pi_k\\) \\(x_K = 1 - \\sum_{k=1}^{K-1} x_k\\) \n\\[\n\\begin{split}\n\\log f(\\boldsymbol x) & =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + x_K \\log \\pi_K \\\\\n & =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_k  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\\\\n\\end{split}\n\\]\nNote particular reason choose \\(\\pi_K\\) derived, place \n\\(\\pi_k\\) may selected.\\(K=2\\) categorical distribution reduces Bernoulli \\(\\text{Ber}(p)\\) distribution,\n\\(\\pi_1=p\\) \\(\\pi_2=1-p\\).","code":""},{"path":"multivariate-random-variables.html","id":"multinomial-distribution","chapter":"1 Multivariate random variables","heading":"1.5.2 Multinomial distribution","text":"multinomial distribution arises repeated categorical sampling,\njust like Binomial distribution arises repeated Bernoulli sampling.","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case","chapter":"1 Multivariate random variables","heading":"1.5.2.1 Univariate case","text":"Binomial Distribution:Repeat Bernoulli \\(\\text{Ber}(\\pi)\\) experiment \\(n\\) times:\\[x \\sim \\text{Bin}(n, \\pi)\\]\n\\[ x \\\\{0,...,n\\}\\]\n\\[\\text{E}(x) = n \\, \\pi\\]\n\\[\\text{Var}(x)=n \\, \\pi(1-\\pi)\\]Standardised unit interval:\n\\[\\frac{x}{n} \\\\left\\{0,\\frac{1}{n},...,1\\right\\}\\]\n\\[\\text{E}\\left(\\frac{x}{n}\\right) = \\pi\\]\n\\[\\text{Var}\\left(\\frac{x}{n}\\right)=\\frac{\\pi(1-\\pi)}{n}\\]\\[\\textbf{Urn model:}\\]distribute \\(n\\) balls two bins","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case","chapter":"1 Multivariate random variables","heading":"1.5.2.2 Multivariate case","text":"Multinomial distribution:Draw \\(n\\) times categorical distribution \\(\\text{Cat}(\\boldsymbol \\pi)\\):\\[\\boldsymbol x\\sim \\text{Mult}(n, \\boldsymbol \\pi)  \\]\n\\[ x_i \\\\{0,1,...,n\\}; \\, \\sum^{K}_{=1}x_i = n\\]\n\\[\\text{E}(\\boldsymbol x) = n \\,\\boldsymbol \\pi\\]\n\\[\\text{Var}(x_i)=n\\, \\pi_i(1-\\pi_i)\\]\n\\[\\text{Cov}(x_i,x_j)=-n\\, \\pi_i\\pi_j\\]Standardised unit interval:\n\\[\\frac{x_i}{n} \\\\left\\{0,\\frac{1}{n},\\frac{2}{n},...,1\\right\\}\\]\n\\[\\text{E}\\left(\\frac{\\boldsymbol x}{n}\\right) = \\boldsymbol \\pi\\]\n\\[\\text{Var}\\left(\\frac{x_i}{n}\\right)=\\frac{\\pi_i(1-\\pi_i)}{n}\\]\n\\[\\text{Cov}\\left(\\frac{x_i}{n},\\frac{x_j}{n}\\right)=-\\frac{\\pi_i\\pi_j}{n} \\]\n\\[\\textbf{Urn model:}\\]distribute \\(n\\) balls \\(K\\) bins:","code":""},{"path":"multivariate-random-variables.html","id":"entropy-and-maximum-likelihood-analysis-for-the-categorical-distribution","chapter":"1 Multivariate random variables","heading":"1.5.3 Entropy and maximum likelihood analysis for the categorical distribution","text":"folling examples compute KL divergence MLE well related quantities categorical distribution.generalise calculations Bernoulli distribution discussed earlier module MATH20802 Statistical Methods.Example 1.1  KL divergence two categorical distributions \\(K\\) classes:\\(P=\\text{Cat}(\\boldsymbol p)\\) \\(Q=\\text{Cat}(\\boldsymbol q)\\) corresponding\nprobabilities \\(p_1,\\dots,p_K\\) \\(q_1,\\dots,q_K\\) satisfying \\(\\sum_{=1}^K p_i =1\\) \\(\\sum_{=1}^K q_i = 1\\) get:\\[\\begin{equation*}\nD_{\\text{KL}}(P, Q)=\\sum_{=1}^K p_i\\log\\left(\\frac{p_i}{q_i}\\right) \n\\end{equation*}\\]explicit \\(K-1\\) parameters categorical distribution can also write\n\\[\\begin{equation*}\nD_{\\text{KL}}(P, Q)=\\sum_{=1}^{K-1} p_i\\log\\left(\\frac{p_i}{q_i}\\right)  + p_K\\log\\left(\\frac{p_K}{q_K}\\right)\n\\end{equation*}\\]\n\\(p_K=\\left(1- \\sum_{=1}^{K-1} p_i\\right)\\) \n\\(q_K=\\left(1- \\sum_{=1}^{K-1} q_i\\right)\\).Example 1.2  Expected Fisher information categorical distribution:first compute Hessian matrix\n\\(\\nabla^T \\nabla \\log f(\\boldsymbol x)\\) log-probability mass function, \ndifferentiation regard \\(\\pi_1, \\ldots, \\pi_{K-1}\\).diagonal entries Hessian matrix (\\(=1, \\ldots, K-1\\)) \n\\[\n\\frac{\\partial^2}{\\partial \\pi_i^2} \\log f(\\boldsymbol x) =\n  -\\frac{x_i}{\\pi_i^2}-\\frac{x_K}{\\pi_K^2}\n\\]\n-diagonal entries (\\(j=1, \\ldots, K-1\\))\n\\[\n\\frac{\\partial^2}{\\partial \\pi_i \\partial \\pi_j} \\log f(\\boldsymbol x) =\n -\\frac{ x_K}{\\pi_K^2}\n\\]\nRecalling \\(\\text{E}(x_i) = \\pi_i\\) can compute expected Fisher information matrix categorical distribution \n\\[\n\\begin{split}\n\\boldsymbol ^{\\text{Fisher}}\\left( \\pi_1, \\ldots, \\pi_{K-1}  \\right) &= -\\text{E}\\left( \\nabla^T \\nabla \\log f(\\boldsymbol x) \\right) \\\\\n& =\n\\begin{pmatrix}\n \\frac{1}{\\pi_1} + \\frac{1}{\\pi_K} & \\cdots & \\frac{1}{\\pi_K} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{\\pi_K} & \\cdots & \\frac{1}{\\pi_{K-1}} + \\frac{1}{\\pi_K} \\\\\n\\end{pmatrix}\\\\\n& = \\text{Diag}\\left( \\frac{1}{\\pi_1} , \\ldots,  \\frac{1}{\\pi_{K-1}}   \\right) + \\frac{1}{\\pi_K} \\boldsymbol 1\\\\\n\\end{split}\n\\]\\(K=2\\) \\(\\pi_1=p\\) reduces expected Fisher information Bernoulli variable\n\\[\n\\begin{split}\n^{\\text{Fisher}}(p) & =  \\left(\\frac{1}{p} + \\frac{1}{1-p} \\right) \\\\\n  &= \\frac{1}{p (1-p)} \\\\\n\\end{split}\n\\]Example 1.3  Quadratic approximation KL divergence categorical distribution:expected Fisher information arises local quadratic approximation KL divergence:\n\\[\nD_{\\text{KL}}(F_{\\boldsymbol \\theta}, F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon})  \\approx  \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)  \\boldsymbol \\varepsilon\n\\]\n\n\\[\nD_{\\text{KL}}(F_{\\boldsymbol \\theta+\\boldsymbol \\varepsilon}, F_{\\boldsymbol \\theta}) \\approx  \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)  \\boldsymbol \\varepsilon\n\\]now consider KL divergence \\(D_{\\text{KL}}(P, Q)\\) categorical distribution \\(P=\\text{Cat}(\\boldsymbol p)\\) probabilities \\(\\boldsymbol p=(p_1, \\ldots, p_K)^T\\) categorical distribution \\(Q=\\text{Cat}(\\boldsymbol q)\\) probabilities \\(\\boldsymbol q= (q_1, \\ldots, q_K)^T\\).First, keep \\(P\\) fixed assume \\(Q\\) perturbed version \\(P\\) \\(\\boldsymbol q= \\boldsymbol p+\\boldsymbol \\varepsilon\\).\nNote perturbations \\(\\boldsymbol \\varepsilon=(\\varepsilon_1, \\ldots, \\varepsilon_K)^T\\) satisfy\n\\(\\sum_{k=1}^K \\varepsilon_k = 0\\) \\(\\sum_{k=1}^K p_i=1\\) \\(\\sum_{k=1}^K q_i=1\\).\nThus \\(\\varepsilon_K = -\\sum_{k=1}^{K-1} \\varepsilon_k\\). \n\\[\n\\begin{split}\nD_{\\text{KL}}(P, Q=P+\\varepsilon) &  = D_{\\text{KL}}(\\text{Cat}(\\boldsymbol p), \\text{Cat}(\\boldsymbol p+\\boldsymbol \\varepsilon)) \\\\\n&  \\approx \\frac{1}{2} (\\varepsilon_1, \\ldots,  \\varepsilon_{K-1}) \\,\n\\boldsymbol ^{\\text{Fisher}}\\left( p_1, \\ldots, p_{K-1}  \\right) \n\\begin{pmatrix} \\varepsilon_1 \\\\ \\vdots \\\\  \\varepsilon_{K-1}\\\\\n\\end{pmatrix} \\\\\n&= \\frac{1}{2} \\left( \\sum_{k=1}^{K-1} \\frac{\\varepsilon_k^2}{p_k}   + \\frac{ \\left(\\sum_{k=1}^{K-1} \\varepsilon_k\\right)^2}{p_K} \\right)  \\\\\n&= \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{\\varepsilon_k^2}{p_k}\\\\\n&= \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{(p_k-q_k)^2}{p_k}\\\\\n& = \\frac{1}{2} D_{\\text{Neyman}}(P, Q)\\\\\n\\end{split} \n\\]\nSimilarly, keep \\(Q\\) fixed consider \\(P\\) disturbed version \\(Q\\) get\n\\[\n\\begin{split}\nD_{\\text{KL}}(P=Q+\\varepsilon, Q) &  =D_{\\text{KL}}(\\text{Cat}(\\boldsymbol q+\\boldsymbol \\varepsilon), \\text{Cat}(\\boldsymbol q)) \\\\\n&\\approx \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{(p_k-q_k)^2}{q_k}\\\\\n&= \\frac{1}{2} D_{\\text{Pearson}}(P, Q)\n\\end{split}\n\\]\nNote approximations divide probabilities distribution \nkept fixed.Note appearance Pearson \\(\\chi^2\\) divergence Neyman \\(\\chi^2\\) divergence . , like KL divergence, part family \\(f\\)-divergences. Neyman \\(\\chi^2\\)\ndivergence also known reverse Pearson divergence \\(D_{\\text{Neyman}}(P, Q) = D_{\\text{Pearson}}(Q, P)\\).Example 1.4  Maximum likelihood estimation parameters categorical distribution:Maximum likelihood estimation seems trivial first sight fact bit complicated since \\(K-1\\) free parameters, \\(K\\). either need optimise regard specific set \\(K-1\\) parameters () use constrained optimisation procedure enforce \\(\\sum_{k=1}^K \\pi_k = 1\\) (example using Lagrange multiplier).data: observe \\(n\\) samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).\ndata matrix dimension \\(n \\times K\\) \n\\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)^T = (x_{ik})\\).\ncontains \\(\\boldsymbol x_i = (x_{i1}, \\ldots, x_{iK})^T\\). corresponding summary\n(minimal sufficient) statistics \n\\(\\bar{\\boldsymbol x} = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = (\\bar{x}_1, \\ldots, \\bar{x}_K)^T\\)\n\n\\(\\bar{x}_k = \\frac{1}{n} \\sum_{=1}^n x_{ik}\\). can also write\n\\(\\bar{x}_{K} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k}\\).\nnumber samples class \\(k\\) \\(n_k = n \\bar{x}_k\\) \n\\(\\sum_{k=1}^K n_k = n\\).data: observe \\(n\\) samples \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).\ndata matrix dimension \\(n \\times K\\) \n\\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)^T = (x_{ik})\\).\ncontains \\(\\boldsymbol x_i = (x_{i1}, \\ldots, x_{iK})^T\\). corresponding summary\n(minimal sufficient) statistics \n\\(\\bar{\\boldsymbol x} = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = (\\bar{x}_1, \\ldots, \\bar{x}_K)^T\\)\n\n\\(\\bar{x}_k = \\frac{1}{n} \\sum_{=1}^n x_{ik}\\). can also write\n\\(\\bar{x}_{K} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k}\\).\nnumber samples class \\(k\\) \\(n_k = n \\bar{x}_k\\) \n\\(\\sum_{k=1}^K n_k = n\\).log-likelihood \n\\[\n\\begin{split}\nl_n(\\pi_1, \\ldots, \\pi_{K-1}) & = \\sum_{=1}^n \\log f(\\boldsymbol x_i) \\\\\n& =\\sum_{=1}^n \\left( \\sum_{k=1}^{K-1}  x_{ik} \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_{ik}  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\right)\\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right) \\log\\left(1 - \\sum_{k=1}^{K-1} \\pi_k\\right) \\right) \\\\ \n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\bar{x}_K \\log \\pi_K \\right) \\\\  \n\\end{split}\n\\]log-likelihood \n\\[\n\\begin{split}\nl_n(\\pi_1, \\ldots, \\pi_{K-1}) & = \\sum_{=1}^n \\log f(\\boldsymbol x_i) \\\\\n& =\\sum_{=1}^n \\left( \\sum_{k=1}^{K-1}  x_{ik} \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_{ik}  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\right)\\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right) \\log\\left(1 - \\sum_{k=1}^{K-1} \\pi_k\\right) \\right) \\\\ \n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\bar{x}_K \\log \\pi_K \\right) \\\\  \n\\end{split}\n\\]Score function (gradient, row vector)\n\\[\n\\begin{split}\n\\boldsymbol S_n(\\pi_1, \\ldots, \\pi_{K-1}) &=  \\nabla l_n(\\pi_1, \\ldots, \\pi_{K-1} ) \\\\\n& = \n\\begin{pmatrix}\n \\frac{\\partial}{\\partial \\pi_1} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\vdots\\\\\n\\frac{\\partial}{\\partial \\pi_{K-1}} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\end{pmatrix}^T\\\\\n& = n\n\\begin{pmatrix}\n\\frac{\\bar{x}_1}{\\pi_1}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\vdots\\\\\n\\frac{\\bar{x}_{K-1}}{\\pi_{K-1}}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\end{pmatrix}^T\\\\\n\\end{split}\n\\]\nNote particular need second term arises \\(\\pi_K\\) depends \n\\(\\pi_1, \\ldots, \\pi_{K-1}\\).Score function (gradient, row vector)\n\\[\n\\begin{split}\n\\boldsymbol S_n(\\pi_1, \\ldots, \\pi_{K-1}) &=  \\nabla l_n(\\pi_1, \\ldots, \\pi_{K-1} ) \\\\\n& = \n\\begin{pmatrix}\n \\frac{\\partial}{\\partial \\pi_1} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\vdots\\\\\n\\frac{\\partial}{\\partial \\pi_{K-1}} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\end{pmatrix}^T\\\\\n& = n\n\\begin{pmatrix}\n\\frac{\\bar{x}_1}{\\pi_1}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\vdots\\\\\n\\frac{\\bar{x}_{K-1}}{\\pi_{K-1}}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\end{pmatrix}^T\\\\\n\\end{split}\n\\]\nNote particular need second term arises \\(\\pi_K\\) depends \n\\(\\pi_1, \\ldots, \\pi_{K-1}\\).Maximum likelihood estimate: Setting \\(\\boldsymbol S_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML})=0\\) yields\n\\(K-1\\) equations\n\\[\n\\bar{x}_i \\left(1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML}\\right)  = \\hat{\\pi}_i^{ML} \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right)\n\\]\n\\(=1, \\ldots, K-1\\) solution\n\\[\n\\hat{\\pi}_i^{ML} = \\bar{x}_i\n\\]\nalso follows \n\\[ \n\\hat{\\pi}_K^{ML} = 1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} = \\bar{x}_K\n\\]\nmaximum likelihood estimator therefore frequency\noccurance class among \\(n\\) samples.Maximum likelihood estimate: Setting \\(\\boldsymbol S_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML})=0\\) yields\n\\(K-1\\) equations\n\\[\n\\bar{x}_i \\left(1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML}\\right)  = \\hat{\\pi}_i^{ML} \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right)\n\\]\n\\(=1, \\ldots, K-1\\) solution\n\\[\n\\hat{\\pi}_i^{ML} = \\bar{x}_i\n\\]\nalso follows \n\\[ \n\\hat{\\pi}_K^{ML} = 1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} = \\bar{x}_K\n\\]\nmaximum likelihood estimator therefore frequency\noccurance class among \\(n\\) samples.Example 1.5  Observed Fisher information categorical distribution:first need compute negative Hessian matrix log likelihood function\n\\(- \\nabla^T \\nabla l_n(\\pi_1, \\ldots, \\pi_{K-1} )\\) evaluate \nMLEs \\(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}\\).diagonal entries Hessian matrix (\\(=1, \\ldots, K-1\\)) \n\\[\n\\frac{\\partial^2}{\\partial \\pi_i^2} l_n(\\pi_1, \\ldots, \\pi_{K-1} ) =\n -n \\left( \\frac{\\bar{x}_i}{\\pi_i^2} +\\frac{\\bar{x}_K}{\\pi_K^2}\\right)\n\\]\n-diagonal entries (\\(j=1, \\ldots, K-1\\))\n\\[\n\\frac{\\partial^2}{\\partial \\pi_i \\partial \\pi_j} l_n(\\pi_1, \\ldots, \\pi_{K-1} ) =\n -\\frac{n \\bar{x}_K}{\\pi_K^2}\n\\]\nThus, observed Fisher information matrix MLE categorical distribution \n\\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) =\nn \n\\begin{pmatrix}\n \\frac{1}{\\hat{\\pi}_1^{ML}} + \\frac{1}{\\hat{\\pi}_K^{ML}} & \\cdots & \\frac{1}{\\hat{\\pi}_K^{ML}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{\\hat{\\pi}_K^{ML}} & \\cdots & \\frac{1}{\\hat{\\pi}_{K-1}^{ML}} + \\frac{1}{\\hat{\\pi}_K^{ML}} \\\\\n\\end{pmatrix} \n\\]\\(K=2\\) reduces observed Fisher information Bernoulli variable\n\\[\n\\begin{split}\nJ_n(\\hat{p}_{ML}) & = n \\left(\\frac{1}{\\hat{p}_{ML}} + \\frac{1}{1-\\hat{p}_{ML}} \\right) \\\\\n  &= \\frac{n}{\\hat{p}_{ML} (1-\\hat{p}_{ML})} \\\\\n\\end{split}\n\\]inverse observed Fisher information :\n\\[\n\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  )^{-1} =\n\\frac{1}{n} \n\\begin{pmatrix}\n\\hat{\\pi}_1^{ML} (1- \\hat{\\pi}_1^{ML} )  & \\cdots & -  \\hat{\\pi}_{1}^{ML} \\hat{\\pi}_{K-1}^{ML}   \\\\\n\\vdots & \\ddots & \\vdots \\\\\n-  \\hat{\\pi}_{K-1}^{ML} \\hat{\\pi}_{1}^{ML} & \\cdots & \\hat{\\pi}_{K-1}^{ML} (1- \\hat{\\pi}_{K-1}^{ML} )  \\\\\n\\end{pmatrix}\n\\]show indeed inverse use Woodbury matrix identity (see Appendix), B=1, \\(\\boldsymbol u= (\\pi_1, \\ldots, \\pi_{K-1})^T\\), \\(\\boldsymbol v=-\\boldsymbol u^T\\),\n\\(\\boldsymbol = \\text{Diag}(\\boldsymbol u)\\) inverse \\(\\boldsymbol ^{-1} = \\text{Diag}(\\pi_1^{-1}, \\ldots, \\pi_{K-1}^{-1})\\). \\(\\boldsymbol ^{-1} \\boldsymbol u= \\boldsymbol 1_{K-1}\\) \\(1-\\boldsymbol u^T \\boldsymbol ^{-1} \\boldsymbol u= \\pi_K\\).\n\n\\(\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML} )^{-1} = \\frac{1}{n} \\left( \\boldsymbol - \\boldsymbol u\\boldsymbol u^T \\right)\\)\n\n\\(\\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML} ) = n \\left( \\boldsymbol ^{-1} + \\frac{1}{\\pi_K} \\boldsymbol 1_{K-1 \\times K-1} \\right)\\).\\(K=2\\) inverse observed Fisher information categorical distribution reduceds Bernoulli distribution\n\\[\nJ_n(\\hat{p}_{ML})^{-1}=\\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\n\\]inverse observed Fisher information useful, e.g., \nasymptotic variance maximum likelihood estimates.Example 1.6  Wald statistic categorical distribution:squared Wald statistic \n\\[\n\\begin{split}\nt(\\boldsymbol p_0)^2 &= \n(\\hat{\\pi}_{1}^{ML}-p_1^0, \\ldots,  \\hat{\\pi}_{K-1}^{ML}-p_{K-1}^0)   \\boldsymbol J_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML} ) \\begin{pmatrix} \\hat{\\pi}_{1}^{ML}-p_1^0 \\\\\n\\vdots \\\\\n\\hat{\\pi}_{K-1}^{ML}-p_{K-1}^0\\\\\n\\end{pmatrix}\\\\\n&= n  \\left( \\sum_{k=1}^{K-1} \\frac{(\\hat{\\pi}_{k}^{ML}-p_{k}^0)^2}{\\hat{\\pi}_{k}^{ML}}   + \\frac{ \\left(\\sum_{k=1}^{K-1} (\\hat{\\pi}_{k}^{ML}-p_{k}^0)\\right)^2}{\\hat{\\pi}_{K}^{ML}} \\right)  \\\\\n&= n  \\left( \\sum_{k=1}^{K} \\frac{(\\hat{\\pi}_{k}^{ML}-p_{k}^0)^2}{\\hat{\\pi}_{k}^{ML}}    \\right)  \\\\\n& = n D_{\\text{Neyman}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) )\n\\end{split}\n\\]\\(n_1, \\ldots, n_K\\) observed counts \\(n = \\sum_{k=1}^K n_k\\)\n\\(\\hat{\\pi}_k^{ML} = \\frac{n_k}{n} = \\bar{x}_k\\),\n\\(n_1^{\\text{expect}}, \\ldots, n_K^{\\text{expect}}\\) \nexpected counts \\(n_k^{\\text{expect}} = n p_k^{0}\\) \\(\\boldsymbol p_0\\)\ncan write squared Wald statistic\nfollows:\n\\[\nt(\\boldsymbol p_0)^2 = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}} )^2}{n_k} =  \\chi^2_{\\text{Neyman}}\n\\]\nknown Neyman chi-squared statistic (note observed counts denominator) asymptotically distributed \\(\\chi^2_{K-1}\\) \n\\(K-1\\) free parameters \\(\\boldsymbol p_0\\).Example 1.7  Wilks log-likelihood ratio statistic categorical distribution:Wilks log-likelihood ratio \n\\[\nW(\\boldsymbol p_0) = 2 (l_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) - l_n(p_1^{0}, \\ldots, p_{K-1}^{0}    ))\n\\]\n\\(\\boldsymbol p_0 = c(p_1^{0}, \\ldots, p_{K}^{0} )^T\\).\nprobabilities sum 1 \\(K-1\\) free parameters.log-likelihood MLE \n\\[\nl_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log \\hat{\\pi}_k^{ML}  =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log \\bar{x}_k \n\\]\n\\(\\hat{\\pi}_k^{ML} = \\frac{n_k}{n} = \\bar{x}_k\\).\nNote following sums run \\(1\\) \\(K\\) \\(K\\)-th component always computed components \\(1\\) \\(K-1\\), previous section.\nlog-likelihood \\(\\boldsymbol p_0\\) \n\\[l_n( p_1^{0}, \\ldots, p_{K-1}^{0}    ) =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log p_k^{0} \n\\]\nWilks statistic becomes\n\\[\nW(\\boldsymbol p_0) = 2 n   \\sum_{k=1}^{K}  \\bar{x}_k \\log\\left(\\frac{\\bar{x}_k}{ p_k^{0}} \\right) \n\\]\nasymptotically chi-squared distributed \\(K-1\\) degrees freedom.Note model Wilks statistic equal KL Divergence\n\\[\nW(\\boldsymbol p_0) = 2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) )\n\\]Wilks log-likelihood ratio statistic categorical distribution also known \\(G\\) test statistic \\(\\hat{\\boldsymbol \\pi}_{ML}\\) corresponds observed frequencies (observed data) \\(\\boldsymbol p_0\\) expected frequencies (.e. hypothesised true frequencies).Using observed counts \\(n_k\\) expected counts \\(n_k^{\\text{expect}} = n p_k^{0}\\)\ncan write Wilks statistic respectively \\(G\\)-statistic\nfollows:\n\\[\nW(\\boldsymbol p_0) = 2   \\sum_{k=1}^{K}  n_k \\log\\left(\\frac{  n_k }{  n_k^{\\text{expect}}   } \\right) \n\\]Example 1.8  Quadratic approximation Wilks log-likelihood ratio statistic categorical distribution:Developing Wilks statistic \\(W(\\boldsymbol p_0)\\) around MLE \\(\\hat{\\boldsymbol \\pi}_{ML}\\) yields squared Wald statistic categorical distribution Neyman chi-squared statistic:\n\\[\n\\begin{split}\nW(\\boldsymbol p_0)& = 2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) \\\\\n& \\approx n D_{\\text{Neyman}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) \\\\\n& = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}} )^2}{n_k} \\\\\n& =  \\chi^2_{\\text{Neyman}}\\\\\n\\end{split}\n\\]instead approximate KL divergence assuming \\(\\boldsymbol p_0\\) fixed arrive \n\\[ \n\\begin{split}\n2 n D_{\\text{KL}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}), \\text{Cat}(\\boldsymbol p_0 ) ) &\\approx n D_{\\text{Pearson}}( \\text{Cat}(\\hat{\\boldsymbol \\pi}_{ML}),  \\text{Cat}(\\boldsymbol p_0 ) )\\\\\n& = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}})^2}{n_k^{\\text{expect}}} \\\\\n& = \\chi^2_{\\text{Pearson}}\n\\end{split}\n\\]\nwell-known Pearson chi-squared statistic (note expected counts denominator).","code":""},{"path":"multivariate-random-variables.html","id":"further-multivariate-distributions","chapter":"1 Multivariate random variables","heading":"1.6 Further multivariate distributions","text":"univariate distributions multivariate versions.following describe multivariate versions Beta distribution,\nGamma distribution (also known scaled \\(\\chi^2\\) distribution)\ninverse Gamma distribution.","code":""},{"path":"multivariate-random-variables.html","id":"dirichlet-distribution","chapter":"1 Multivariate random variables","heading":"1.6.1 Dirichlet distribution","text":"","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case-1","chapter":"1 Multivariate random variables","heading":"1.6.1.1 Univariate case","text":"Beta distribution\\[x \\sim \\text{Beta}(\\alpha,\\beta)\\]\n\\[x \\[0,1]\\]\n\\[\\alpha > 0; \\beta > 0\\]\n\\[m = \\alpha + \\beta \\]\n\\[\\mu = \\frac{\\alpha}{m} \\\\left[0,1\\right]\\]\n\\[\\text{E}(x) = \\mu\\]\n\\[\\text{Var}(x)=\\frac{\\mu(1-\\mu)}{m+1}\\]\n\\(\\text{compare unit standardised binomial!}\\)\\(\\textbf{Different shapes}\\)\\[\\text{Useful distribution proportion } \\pi\\]\\[ \\text{ Bayesian Model:}\\]\\[\\text{Beta prior:} \\; \\pi \\sim  \\text{Beta}(\\alpha,\\beta)\\]\n\\[\\text{Binomial likelihood:} \\; x|\\pi \\sim \\text{Bin}(n, \\pi)\\]","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case-1","chapter":"1 Multivariate random variables","heading":"1.6.1.2 Multivariate case","text":"Dirichlet distribution\\[\\boldsymbol x\\sim \\text{Dir}(\\boldsymbol \\alpha)\\]\n\\[x_i \\[0,1]; \\, \\sum^{d}_{=1} x_i = 1\\]\n\\[\\boldsymbol \\alpha= (\\alpha_1,...,\\alpha_d)^T >0\\]\n\\[m = \\sum^{d}_{=1}\\alpha_i\\]\n\\[\\mu_i = \\frac{\\alpha_i}{m} \\\\left[0,1\\right]\\]\n\\[\\text{E}(x_i) = \\mu_i\\]\n\\[\\text{Var}(x_i)=\\frac{\\mu_i(1-\\mu_i)}{m+1}\\]\n\\[\\text{Cov}(x_i,x_j)=-\\frac{\\mu_i \\mu_j}{m+1}\\]\n\\(\\text{compare unit standardised multinomial!}\\)Stick breaking\" model\\[\\text{Useful distribution proportion } \\boldsymbol \\pi\\]\\[\\text{ Bayesian Model:}\\]\\[\\text{Dirichlet prior:} \\,  \\boldsymbol \\pi\\sim \\text{Dir}(\\boldsymbol \\alpha)\\]\n\\[\\text{Multinomial likelihood:} \\, \\boldsymbol x|\\boldsymbol \\pi\\sim \\text{Mult}(n, \\boldsymbol \\pi)\\]","code":""},{"path":"multivariate-random-variables.html","id":"wishart-distribution","chapter":"1 Multivariate random variables","heading":"1.6.2 Wishart distribution","text":"multivariate distribution generalises univariate\nscaled \\(\\chi^2\\) distribution (also known Gamma distribution).","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case-2","chapter":"1 Multivariate random variables","heading":"1.6.2.1 Univariate case","text":"Scaled \\(\\chi^2\\) distribution (=Gamma distribution, see )\\[z_1,z_2,\\ldots,z_m \\stackrel{\\text{iid}}\\sim N(0,\\sigma^2)\\]\n\\[x = \\sum^{m}_{=1}z_i^2\\]\\[x \\sim \\sigma^2 \\chi^2_m = \\text{W}_1(\\sigma^2, m)\\]\n\\[\\text{E}(x) = m \\, \\sigma^2\\]\n\\[\\text{Var}(x)= m \\, 2 \\sigma^4\\]Useful distribution sample variance:\n\\[y_1, \\ldots, y_n \\sim N(\\mu, \\sigma^2)\\]\nKnown mean \\(\\mu\\):\n\\[\\frac{1}{n}\\sum_{=1}^n(y_i -\\mu)^2 \\sim \\text{W}_1\\left(\\frac{\\sigma^2}{n}, n\\right)\\]\nUnknown mean \\(\\mu\\) (estimated \\(\\bar{y}\\)):\n\\[\\widehat{\\sigma^2}_{ML} = \\frac{1}{n}\\sum_{=1}^n(y_i -\\bar{y})^2 \\sim \\text{W}_1\\left(\\frac{\\sigma^2}{n}, n-1\\right)\\]\n\\[\\widehat{\\sigma^2}_{UB} = \\frac{1}{n-1}\\sum_{=1}^n(y_i -\\bar{y})^2 \\sim \\text{W}_1\\left(\\frac{\\sigma^2}{n-1}, n-1\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case-2","chapter":"1 Multivariate random variables","heading":"1.6.2.2 Multivariate case","text":"Wishart distribution\\[\\boldsymbol z_1,\\boldsymbol z_2,\\ldots,\\boldsymbol z_m \\stackrel{\\text{iid}}\\sim N_d(0,\\boldsymbol \\Sigma)\\]\n\\[\\underbrace{\\boldsymbol X}_{d\\times d}=\\sum^{m}_{=1}\\underbrace{\\boldsymbol z_i\\boldsymbol z_i^T}_{d\\times d}\\]\nNote \\(\\boldsymbol X\\) matrix!\\[\\boldsymbol X\\sim \\text{W}_d\\left(\\boldsymbol \\Sigma, m\\right)\\]\n\\[\\text{E}(\\boldsymbol X) = m \\boldsymbol \\Sigma\\]\n\\[\\text{Var}(x_{ij})=m \\, \\left(\\sigma^2_{ij}+\\sigma_{ii}\\sigma_{jj}\\right)\\]Useful distribution sample covariance:\n\\[\\boldsymbol y_1, \\ldots, \\boldsymbol y_n \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\]\n\\[\\frac{1}{n}\\sum_{=1}^n (\\boldsymbol y_i -\\boldsymbol \\mu)(\\boldsymbol y_i -\\boldsymbol \\mu)^T \\sim \\text{W}_d\\left(\\boldsymbol \\Sigma/n, n\\right)\\]\n\\[\\widehat{\\boldsymbol \\Sigma}_{ML} = \\frac{1}{n}\\sum_{=1}^n (\\boldsymbol y_i -\\bar{\\boldsymbol y})(\\boldsymbol y_i -\\bar{\\boldsymbol y})^T \\sim \\text{W}_d\\left(\\boldsymbol \\Sigma/n, n-1\\right)\\]\n\\[\\widehat{\\boldsymbol \\Sigma}_{UB} = \\frac{1}{n-1}\\sum_{=1}^n (\\boldsymbol y_i -\\bar{\\boldsymbol y})(\\boldsymbol y_i -\\bar{\\boldsymbol y})^T \\sim \\text{W}_d\\left(\\boldsymbol \\Sigma/(n-1), n-1\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"relationship-to-gamma-distribution","chapter":"1 Multivariate random variables","heading":"1.6.2.3 Relationship to Gamma distribution","text":"scaled \\(\\chi^2\\) distribution (=one-dimensional Wishart distribution) parameters \\(\\sigma^2\\) \\(m\\) reparameterised Gamma distribution shape parameter \\(\\alpha\\) scale parameter \\(\\beta\\):\\[\\text{Gam}\\left(  \\underbrace{\\frac{m}{2}}_{\\text{shape} } \\, , \\underbrace{ 2 \\sigma^{2}}_{\\text{scale}} \\right)=  \\sigma^2\\chi^2_m = \\text{W}_1(\\sigma^2, m)\\], equivalently (\\(m = 2 \\alpha\\), \\(\\sigma^2 = \\beta/2\\))\n\\[\\text{Gam}\\left(  \\underbrace{\\alpha}_{\\text{shape} } \\, , \\underbrace{\\beta}_{\\text{scale}} \\right) = \\frac{\\beta}{2} \\chi^2_{2 \\alpha} =  \\text{W}_1(\\frac{\\beta}{2}, 2 \\alpha)\\]mean Gamma distribution \\(\\text{E}(x) = \\alpha \\beta\\) variance \\(\\text{Var}(x) = \\alpha \\beta^2\\).exponential distribution scale parameter \\(\\beta\\) special\ncase Gamma distribution \\(\\alpha=1\\):\n\\[\n\\text{Exp}(\\beta) = \\text{Gam}(1, \\beta) = \\frac{\\beta}{2} \\chi^2_{2} = \\text{W}_1\\left(\\frac{\\beta}{2}, 2 \\right)\n\\]\ncorresponding mean \\(\\beta\\) variance \\(\\beta^2\\).density expressed terms scale parameter \\(\\beta\\) \n\\[\nf(x| \\beta) = \\frac{1}{\\beta} e^{-x/\\beta}\n\\]Instead scale parameter \\(\\beta\\) exponential distribution often\nalso parameterised terms rate parameter \\(\\lambda = \\frac{1}{\\beta}\\).","code":""},{"path":"multivariate-random-variables.html","id":"inverse-wishart-distribution","chapter":"1 Multivariate random variables","heading":"1.6.3 Inverse Wishart distribution","text":"","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case-3","chapter":"1 Multivariate random variables","heading":"1.6.3.1 Univariate case","text":"Inverse \\(\\chi^2\\) Distribution:\\[x \\sim \\text{W}^{-1}_1(\\psi, k+2) = \\psi\\,\\text{Inv-$\\chi^2_{k+2}$}\\]\n\\[\\text{E}(x) = \\frac{\\psi}{k}\\]\n\\[\\text{Var}(x)= \\frac{2\\psi^2}{k^2 (k-2)}\\]Relationship scaled \\(\\chi^2\\) :\n\\[\n\\frac{1}{x} \\sim W_1(\\psi^{-1}, k+2) =  \\psi^{-1} \\, \\chi^2_{k+2}\n\\]","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case-3","chapter":"1 Multivariate random variables","heading":"1.6.3.2 Multivariate case","text":"Inverse Wishart distribution:\\[\\underbrace{\\boldsymbol X}_{d\\times d} \\sim \\text{W}^{-1}_d\\left( \\underbrace{\\boldsymbol \\Psi}_{d\\times d} \\, , \\, k+d+1\\right)\\]\n\\[\\text{E}(\\boldsymbol X) =\\boldsymbol \\Psi/ k\\]\n\\[\\text{Var}(x_{ij})= \\frac{2 }{k^2 (k-2)} \\frac{(k+2) \\psi_{ij} + k \\, \\psi_{ii} \\psi_{jj} }{2 k + 2}\\]Relationship Wishart:\n\\[\\boldsymbol X^{-1} \\sim \\text{W}_d\\left( \\boldsymbol \\Psi^{-1} \\, , k+d+1\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"relationship-to-inverse-gamma-distribution","chapter":"1 Multivariate random variables","heading":"1.6.3.3 Relationship to inverse Gamma distribution","text":"Another way express univariate inverse Wishart distribution via inverse Gamma distribution:\n\\[\\text{Inv-Gam}(\\underbrace{1+\\frac{k}{2}}_{\\text{shape } \\alpha}, \\underbrace{\\frac{\\psi}{2}}_{\\text{scale }\\beta}) = \\psi\\,\\text{Inv-$\\chi^2_{k+2}$} =  \\text{W}^{-1}_1(\\psi, k+2) \\]\nequivalently (\\(k=2(\\alpha-1)\\) \\(\\psi=2\\beta\\))\n\\[\\text{Inv-Gam}( \\alpha, \\beta) = 2\\beta\\,\\text{Inv-$\\chi^2_{2\\alpha}$} = \\text{W}^{-1}_1(2 \\beta, 2 \\alpha) \\]\nmean inverse Gamma distribution \n\\(\\text{E}(x) = \\frac{\\beta}{\\alpha-1} = \\mu\\) variance\n\\(\\text{Var}(x)= \\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)} = \\frac{2 \\mu^2}{k-2}\\).inverse \\(x\\) Gamma distributed:\n\\[\n\\frac{1}{x} \\sim \\text{Gam}(1+\\frac{k}{2}, 2\\psi^{-1})=\\text{Gam}(\\alpha, \\beta^{-1})\n\\]inverse Wishart distribution useful conjugate distribution Bayesian modelling\nvariance, \\(k\\) sample size parameter\n\\(\\Psi = k \\Sigma\\) (\\(\\psi = k \\sigma^2\\)).","code":""},{"path":"multivariate-random-variables.html","id":"further-distributions","chapter":"1 Multivariate random variables","heading":"1.6.4 Further distributions","text":"https://en.wikipedia.org/wiki/List_of_probability_distributionsWikipedia quite good source information distributions!","code":""},{"path":"transformations-and-dimension-reduction.html","id":"transformations-and-dimension-reduction","chapter":"2 Transformations and dimension reduction","heading":"2 Transformations and dimension reduction","text":"Motivation:\nfollowing study transformations random vectors distributions.\ntransformation important\nsince either transform simple distributions complex distributions allow simplify\ncomplex models. machine learning invertible mappings transformations\nprobability distributions known “normalising flows”.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"linear-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.1 Linear Transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"location-scale-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.1.1 Location-scale transformation","text":"Also known affine transformation.\\[\\boldsymbol y= \\underbrace{\\boldsymbol }_{\\text{location parameter}}+\\underbrace{\\boldsymbol B}_{\\text{scale parameter}} \\boldsymbol x\\space\\]\n\\[\\boldsymbol y: m \\times 1 \\text{ random vector}\\]\n\\[\\boldsymbol : m \\times 1 \\text{ vector, location parameter}\\]\n\\[\\boldsymbol B: m \\times d \\text{ matrix, scale parameter },  m \\geq 1\\]\n\\[\\boldsymbol x: d \\times 1 \\text{ random vector}\\]Mean variance original vector \\(\\boldsymbol x\\):\\[\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu\\]\n\\[\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma\\]Mean variance transformed random vector \\(\\boldsymbol y\\):\\[\\text{E}(\\boldsymbol y)=\\boldsymbol + \\boldsymbol B\\boldsymbol \\mu\\]\n\\[\\text{Var}(\\boldsymbol y)= \\boldsymbol B\\boldsymbol \\Sigma\\boldsymbol B^T\\]Cross-covariance \\(\\boldsymbol \\Phi= \\Sigma_{\\boldsymbol y\\boldsymbol x}\\) \\(\\boldsymbol y\\) \\(\\boldsymbol x\\):\n\\[\n\\boldsymbol \\Phi= \\text{Cov}(\\boldsymbol y,\\boldsymbol x) = \\text{Cov}(\\boldsymbol B\\boldsymbol x,\\boldsymbol x) = \\boldsymbol B\\boldsymbol \\Sigma\n\\]Cross-correlation \\(\\boldsymbol \\Psi= \\boldsymbol P_{\\boldsymbol y\\boldsymbol x}\\) \\(\\boldsymbol y\\) \\(\\boldsymbol x\\):\n\\[\n\\boldsymbol \\Psi= \\text{Cor}(\\boldsymbol y,\\boldsymbol x) = \\boldsymbol V_{\\boldsymbol y}^{-1/2} \\boldsymbol \\Phi\\boldsymbol V_{\\boldsymbol x}^{-1/2}\n\\]\n\\(\\boldsymbol V_{\\boldsymbol x} = \\text{Diag}(\\boldsymbol \\Sigma)\\) \\(\\boldsymbol V_{\\boldsymbol y} = \\text{Diag}(\\boldsymbol B\\boldsymbol \\Sigma\\boldsymbol B^T)\\)Special cases/examples:Example 2.1  Univariate case (\\(d=1, m=1\\)): \\(y=+ b x\\)\\(\\text{E}(y)=+b\\mu\\)\\(\\text{Var}(y)=b^2\\sigma^2\\)\\(\\text{Cov}(y, x) = b\\sigma^2\\)\\(\\text{Cor}(y, x) = \\frac{b \\sigma^2}{\\sqrt{b^2\\sigma^2} \\sqrt{\\sigma^2} } =1\\)Example 2.2  Sum two random univariate variables:\n\\(y = x_1 + x_2\\), .e. \\(=0\\) \\(\\boldsymbol B=(1,1)\\)\\(\\text{E}(y) = \\text{E}(x_1+x_2)=\\mu_1+\\mu_2\\)\\(\\text{Var}(y) = \\text{Var}(x_1+x_2) = (1,1)\\begin{pmatrix} \\sigma^2_1 & \\sigma_{12}\\\\ \\sigma_{12} & \\sigma^2_2 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix} = \\sigma^2_1+\\sigma^2_2+2\\sigma_{12} = \\text{Var}(x_1)+\\text{Var}(x_2)+2\\text{Cov}(x_1,x_2)\\)Example 2.3  \\(y_1=a_1+b_1 x_1\\) \\(y_2=a_2+b_2 x_2\\),\n.e. \\(\\boldsymbol = \\begin{pmatrix} a_1\\\\ a_2 \\end{pmatrix}\\) \\(\\boldsymbol B= \\begin{pmatrix}b_1 & 0\\\\ 0 & b_2\\end{pmatrix}\\)\\(\\text{E}(\\boldsymbol y)=\\begin{pmatrix} a_1+b_1 \\mu_1\\\\ a_2+b_2 \\mu_2 \\end{pmatrix}\\)\\(\\text{Var}(\\boldsymbol y) = \\begin{pmatrix} b_1 & 0\\\\ 0 & b_2 \\end{pmatrix}  \\begin{pmatrix} \\sigma^2_1 & \\sigma_{12}\\\\ \\sigma_{12} & \\sigma^2_2 \\end{pmatrix} \\begin{pmatrix} b_1 & 0\\\\ 0 & b_2 \\end{pmatrix} = \\begin{pmatrix} b^2_1\\sigma^2_1 & b_1b_2\\sigma_{12}\\\\ b_1b_2\\sigma_{12} & b^2_2\\sigma^2_2 \\end{pmatrix}\\)\nnote \\(\\text{Cov}(y_1, y_2) = b_1 b_2\\text{Cov}(x_1,x_2)\\)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"squared-multiple-correlation","chapter":"2 Transformations and dimension reduction","heading":"2.1.2 Squared multiple correlation","text":"Definition squared multiple correlationSquared multiple correlation measure summarising linear association scalar response variable \\(y\\)\nset predictors \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\). given \n\\[\n\\begin{split}\n\\text{Cor}(y, \\boldsymbol x)^2 &= \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} / \\sigma^2_y\\\\\n &=\\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{ \\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\\\\\n\\end{split}\n\\]\n\\(y\\) can perfectly linearly predicted \\(\\boldsymbol x\\) \\(\\text{Cor}(y, \\boldsymbol x)^2 = 1\\).empirical estimate \\(\\text{Cor}(y, \\boldsymbol x)^2\\) \\(R^2\\) coefficient find software linear regression.See corresponding section MATH20802 Statistical Methods.Squared multiple correlation affine transformationSince linearly transform \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) additional error involved expect\ncomponent \\(y_i\\) \\(\\boldsymbol y\\) \\(\\text{Cor}(y_i, \\boldsymbol x)^2=1\\).\ncan shown directly computing\n\\[\n\\begin{split}\n\\left(\\text{Cor}(y_1, \\boldsymbol x)^2, \\ldots, \\text{Cor}(y_m, \\boldsymbol x)^2 \\right)^T\n&=\\text{Diag}\\left(\\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}  \\right) / \\text{Diag}\\left( \\boldsymbol \\Sigma_{\\boldsymbol y} \\right) \\\\\n&= \\text{Diag}\\left(\\boldsymbol \\Phi\\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Phi^T  \\right) / \\text{Diag}\\left( \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) \\\\\n&= \\text{Diag}\\left(\\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) / \\text{Diag}\\left( \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) \\\\\n&= \\text{Diag}\\left(\\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) / \\text{Diag}\\left( \\boldsymbol B\\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol B^T \\right) \\\\\n&=\\text{Diag}\\left(1, \\ldots, 1 \\right)^T\\\\\n\\end{split}\n\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"invertible-location-scale-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.1.3 Invertible location-scale transformation","text":"\\(m=d\\) (square \\(\\boldsymbol B\\)) \\(\\det(\\boldsymbol B) \\neq 0\\) affine transformation invertible.Forward transformation:\n\\[\\boldsymbol y= \\boldsymbol + \\boldsymbol B\\boldsymbol x\\]Back transformation:\n\\[\\boldsymbol x= \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\]Invertible transformations thus provide one--one map \\(\\boldsymbol x\\) \\(\\boldsymbol y\\).Example 2.4  Mahalanobis transformWe assume positive definite thus invertible \\(\\boldsymbol \\Sigma\\), \ninverse principal matrix square root \\(\\boldsymbol \\Sigma^{-1/2}\\) can computed,\ntransformation invertible.Mahalanobis transformation given \n\\[\n\\boldsymbol y=\\boldsymbol \\Sigma^{-1/2}(\\boldsymbol x-\\boldsymbol \\mu)\n\\]\ncorresponds affine transformation \n\\(\\boldsymbol = - \\boldsymbol \\Sigma^{-1/2} \\boldsymbol \\mu\\) \\(\\boldsymbol B= \\boldsymbol \\Sigma^{-1/2}\\).Starting \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu\\) \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma\\)\narrive \n\\[\n\\text{E}(\\boldsymbol y) = \\boldsymbol 0\\]\n\n\\[\\text{Var}(\\boldsymbol y) = \\boldsymbol I_d\\].Mahalanobis transforms performs three functions:Centering (\\(-\\boldsymbol \\mu\\))Standardisation \\(\\text{Var}(y_i)=1\\)Decorrelation \\(\\text{Cor}(y_i,y_j)=0\\) \\(\\neq j\\)univariate case (\\(d=1\\)) coefficients reduce \n\\(= - \\frac{\\mu}{\\sigma}\\) \\(B = \\frac{1}{\\sigma}\\) Mahalanobis transform\nbecomes\n\\[y = \\frac{x-\\mu}{\\sigma}\\]\n.e. applies centering + standardisation.Mahalanobis transformation appears implicitly many places multivariate statistics,\ne.g. multivariate normal density. particular example whitening transformation (\ninfinitely many, see later course).Example 2.5  Inverse Mahalanobis transformationThe inverse Mahalanobis transform given \n\\[\n\\boldsymbol y= \\boldsymbol \\mu+\\boldsymbol \\Sigma^{1/2} \\boldsymbol x\n\\]\nMahalanobis transform whitening transform inverse Mahalonobis\ntransform sometimes called Mahalanobis colouring transformation.\ncoefficients affine transformation \n\\(\\boldsymbol =\\boldsymbol \\mu\\) \\(\\boldsymbol B=\\boldsymbol \\Sigma^{1/2}\\).Starting \\(\\text{E}(\\boldsymbol x)=\\boldsymbol 0\\) \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol I_d\\) \nmean variance transformed variable \n\\[\\text{E}(\\boldsymbol y) = \\boldsymbol \\mu\n\\]\n\n\\[\\text{Var}(\\boldsymbol y) = \\boldsymbol \\Sigma\n\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"transformation-of-a-density-under-an-invertible-location-scale-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.1.4 Transformation of a density under an invertible location-scale transformation:","text":"Assume \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) density \\(f_{\\boldsymbol x}(\\boldsymbol x)\\).linear transformation \\(\\boldsymbol y= \\boldsymbol + \\boldsymbol B\\boldsymbol x\\) get \\(\\boldsymbol y\\sim F_{\\boldsymbol y}\\) density\n\\[f_{\\boldsymbol y}(\\boldsymbol y)=|\\det(\\boldsymbol B)|^{-1} f_{\\boldsymbol x} \\left( \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\right)\\]Example 2.6  Transformation standard normal inverse Mahalanobis transformAssume \\(\\boldsymbol x\\) multivariate standard normal \\(\\boldsymbol x\\sim N_d(\\boldsymbol 0,\\boldsymbol I_d)\\) density\n\\[f_{\\boldsymbol x}(\\boldsymbol x) = (2\\pi)^{-d/2}\\exp\\left( -\\frac{1}{2} \\boldsymbol x^T \\boldsymbol x\\right)\\]\ndensity applying inverse Mahalanobis transform\\(\\boldsymbol y= \\boldsymbol \\mu+\\boldsymbol \\Sigma^{1/2} \\boldsymbol x\\) \n\\[\n\\begin{split}\nf_{\\boldsymbol y}(\\boldsymbol y) &= |\\det(\\boldsymbol \\Sigma^{1/2})|^{-1} (2\\pi)^{-d/2} \\exp\\left(-\\frac{1}{2}(\\boldsymbol y-\\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1/2} \\,\\boldsymbol \\Sigma^{-1/2}(\\boldsymbol y-\\boldsymbol \\mu)\\right)\\\\\n& = (2\\pi)^{-d/2} \\det(\\boldsymbol \\Sigma)^{-1/2} \\exp\\left(-\\frac{1}{2}(\\boldsymbol y-\\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol y-\\boldsymbol \\mu)\\right) \\\\\n\\end{split}\n\\]\n\\(\\Longrightarrow\\) \\(\\boldsymbol y\\) multivariate normal density!!Application: e.g. random number generation: draw \\(N_d(\\boldsymbol 0,\\boldsymbol I_d)\\) (easy!) convert multivariate normal tranformation\n(see Worksheet 3).","code":""},{"path":"transformations-and-dimension-reduction.html","id":"nonlinear-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.2 Nonlinear transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"general-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.2.1 General transformation","text":"\\[\\boldsymbol y= \\boldsymbol h(\\boldsymbol x)\\]\n\\(\\boldsymbol h\\) arbitrary vector-valued functionlinear case: \\(\\boldsymbol h(\\boldsymbol x) = \\boldsymbol +\\boldsymbol B\\boldsymbol x\\)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"delta-method","chapter":"2 Transformations and dimension reduction","heading":"2.2.2 Delta method","text":"Assume know mean \\(\\text{E}(\\boldsymbol x)=\\boldsymbol \\mu\\) variance \\(\\text{Var}(\\boldsymbol x)=\\boldsymbol \\Sigma\\) \\(\\boldsymbol x\\).\npossible say something mean variance transformed\nrandom variable \\(\\boldsymbol y\\)?\n\\[\n\\text{E}(\\boldsymbol y)= \\text{E}(\\boldsymbol h(\\boldsymbol x))= ?\n\\]\n\\[\n\\text{Var}(\\boldsymbol y) = \\text{Var}(\\boldsymbol h(\\boldsymbol x))= ? \\\\\n\\]general, transformation \\(\\boldsymbol h(\\boldsymbol x)\\) exact mean variance transformed variable obtained analytically.However, can find linear approximation compute mean variance.\napproximation called “Delta Method”, “law propagation errors”, credited Gauss.2Linearisation \\(\\boldsymbol h(\\boldsymbol x)\\) achieved Taylor series approximation first order\n\\(\\boldsymbol h(\\boldsymbol x)\\) around \\(\\boldsymbol x_0\\):\n\\[\\boldsymbol h(\\boldsymbol x) \\approx \\boldsymbol h(\\boldsymbol x_0) + \\underbrace{D\\boldsymbol h(\\boldsymbol x_0)}_{\\text{Jacobian matrix}}(\\boldsymbol x-\\boldsymbol x_0)  = \n\\underbrace{\\boldsymbol h(\\boldsymbol x_0) - D\\boldsymbol h(\\boldsymbol x_0)\\, \\boldsymbol x_0}_{\\boldsymbol } + \\underbrace{D\\boldsymbol h(\\boldsymbol x_0)}_{\\boldsymbol B} \\boldsymbol x\\]\\(\\nabla\\), nabla operator, row vector \\((\\frac{\\partial}{\\partial x_1},...,\\frac{\\partial}{\\partial x_d})\\), applied univariate \\(h\\) gives gradient:\\[\\nabla h(\\boldsymbol x) = \\left(\\frac{\\partial h}{\\partial x_1},...,\\frac{\\partial h}{\\partial x_d}\\right)\\]Jacobian matrix generalisation gradient \\(\\boldsymbol h\\) vector-valued:\\[D\\boldsymbol h(\\boldsymbol x) = \\begin{pmatrix}\\nabla h_1(\\boldsymbol x)\\\\ \\nabla h_2(\\boldsymbol x) \\\\ \\vdots \\\\ \\nabla h_m(\\boldsymbol x) \\end{pmatrix} = \\begin{pmatrix}\n    \\frac{\\partial h_1}{\\partial x_1} & \\dots & \\frac{\\partial h_1}{\\partial x_d}\\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial h_m}{\\partial x_1} & \\dots & \\frac{\\partial h_m}{\\partial x_d}\n    \\end{pmatrix}\\]First order approximation \\(\\boldsymbol h(\\boldsymbol x)\\) around \\(\\boldsymbol x_0=\\boldsymbol \\mu\\) yields\n\\(\\boldsymbol = \\boldsymbol h(\\boldsymbol \\mu) - D\\boldsymbol h(\\boldsymbol \\mu)\\, \\boldsymbol \\mu\\) \n\\(\\boldsymbol B= D\\boldsymbol h(\\boldsymbol \\mu)\\) leads directly multivariate Delta method:\\[\\text{E}(\\boldsymbol y)\\approx\\boldsymbol h(\\boldsymbol \\mu)\\]\n\\[\\text{Var}(\\boldsymbol y)\\approx D\\boldsymbol h(\\boldsymbol \\mu) \\, \\boldsymbol \\Sigma\\, (D\\boldsymbol h(\\boldsymbol \\mu))^T\\]univariate Delta method special case:\n\\[\\text{E}(y) \\approx h(\\mu)\\]\n\\[\\text{Var}(y)\\approx \\sigma^2 h'(\\mu)^2\\]Note Delta approximation breaks \\(\\text{Var}(\\boldsymbol y)\\) singular,\nexample first derivative (gradient Jacobian matrix) \\(\\boldsymbol \\mu\\) zero.Example 2.7  Variance odds ratioThe proportion \\(\\hat{p} = \\frac{n_1}{n}\\) resulting \n\\(n\\) repeats Bernoulli experiment expectation \\(\\text{E}(\\hat{p})=p\\)\nvariance \\(\\text{Var}(\\hat{p}) = \\frac{p (1-p)}{n}\\).\n(approximate) mean variance corresponding odds ratio \\(\\widehat{}=\\frac{\\hat{p}}{1-\\hat{p}}\\)?\\(h(x) = \\frac{x}{1-x}\\),\n\\(\\widehat{} = h(\\hat{p})\\) \\(h'(x) = \\frac{1}{(1-x)^2}\\) get using \nDelta method\n\\(\\text{E}( \\widehat{} ) \\approx h(p) = \\frac{p}{1-p}\\) \n\\(\\text{Var}( \\widehat{} )\\approx h'(p)^2 \\text{Var}( \\hat{p} ) = \\frac{p}{n (1-p)^3}\\).Example 2.8  Log-transform variance stabilisationAssume \\(x\\) mean \\(\\text{E}(x)=\\mu\\) variance \\(\\text{Var}(x) = \\sigma^2 \\mu^2\\),\n.e. standard deviation \\(\\text{SD}(x)\\) proportional mean \\(\\mu\\).\n(approximate) mean variance log-transformed variable \\(\\log(x)\\)?\\(h(x) = \\log(x)\\) \\(h'(x) = \\frac{1}{x}\\) get using \nDelta method\n\\(\\text{E}( \\log(x) ) \\approx h(\\mu) = \\log(\\mu)\\) \n\\(\\text{Var}( \\log(x) )\\approx h'(\\mu)^2 \\text{Var}( x ) = \\left(\\frac{1}{\\mu} \\right)^2 \\sigma^2 \\mu^2 = \\sigma^2\\). Thus, applying log-transform variance depend mean!","code":""},{"path":"transformations-and-dimension-reduction.html","id":"transformation-of-a-probability-density-function-under-a-general-invertible-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.2.3 Transformation of a probability density function under a general invertible transformation","text":"Assume \\(\\boldsymbol h(\\boldsymbol x) = \\boldsymbol y(\\boldsymbol x)\\) invertible: \\(\\boldsymbol h^{-1}(\\boldsymbol y)=\\boldsymbol x(\\boldsymbol y)\\)\\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) probability density function \\(f_{\\boldsymbol x}(\\boldsymbol x)\\)density \\(f_{\\boldsymbol y}(\\boldsymbol y)\\) transformed random vector \\(\\boldsymbol y\\) given \\[f_{\\boldsymbol y}(\\boldsymbol y) = |\\det\\left( D\\boldsymbol x(\\boldsymbol y) \\right)| \\,\\,\\,  f_{\\boldsymbol x}\\left( \\boldsymbol x(\\boldsymbol y) \\right)\\]\\(D\\boldsymbol x(\\boldsymbol y)\\) Jacobian matrix inverse transformation.Special cases:Univariate version: \\(f_y(y) = |\\frac{dx(y)}{dy}| \\, f_x\\left(x(y)\\right)\\)Linear transformation \\(\\boldsymbol h(\\boldsymbol x) = \\boldsymbol + \\boldsymbol B\\boldsymbol x\\), \\(\\boldsymbol x(\\boldsymbol y) = \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\)\n\\(D\\boldsymbol x(\\boldsymbol y) = \\boldsymbol B^{-1}\\):\n\\[f_{\\boldsymbol y}(\\boldsymbol y)=|\\det(\\boldsymbol B)|^{-1} f_{\\boldsymbol x} \\left( \\boldsymbol B^{-1}(\\boldsymbol y-\\boldsymbol )\\right)\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"normalising-flows","chapter":"2 Transformations and dimension reduction","heading":"2.2.4 Normalising flows","text":"module focus mostly linear transformations underpin\nmuch classical multivariate statistics, important keep mind later study\nimportance nonlinear transformationsIn machine learning (sequences ) invertible nonlinear transformations known “normalising flows”. used generative way (building complex models \nsimple models) simplification dimension reduction.interested normalising flows good start learn review papers\nKobyzev et al (2021 )3 Papamakarios et al. (2021).4","code":""},{"path":"transformations-and-dimension-reduction.html","id":"whitening-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.3 Whitening transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"overview","chapter":"2 Transformations and dimension reduction","heading":"2.3.1 Overview","text":"Mahalanobis transform (also know “zero-phase component analysis” short ZCA transform machine learning) specific example whitening transformation. constitute important widely used class invertible location-scale transformations.Terminology: whitening refers fact transformation covariance matrix spherical, isotrop, white (\\(\\boldsymbol I_d\\))Whitening useful preprocessing, turn multivariate problems simple univariate models reduce dimension optimal way.-called latent variable models whitening procedures link observed latent variables (usually uncorrelated standardised random variables):\\[\\begin{align*}\n\\begin{array}{cl}\n\\text{Whitening} \\\\\n\\downarrow\n\\end{array}\n\\begin{array}{ll}\n\\boldsymbol x\\\\\n\\uparrow \\\\\n\\boldsymbol z\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{Observed variable (can measured)} \\\\\n\\text{external, typically correlated} \\\\\n\\space \\\\\n\\text{Unobserved \"latent\" variable (directly measured)} \\\\\n\\text{internal, typically chosen uncorrelated} \\\\\n\\end{array}\n\\end{align*}\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"general-whitening-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.3.2 General whitening transformation","text":"Starting point:Random vector \\(\\boldsymbol x\\sim F_{\\boldsymbol x}\\) (necessarily multivariate normal).random variance \\(\\boldsymbol x\\) mean \\(\\text{E}(\\boldsymbol z)=\\boldsymbol \\mu\\) positive definite (invertible) covariance matrix \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\).\ncovariance can split positive variances \\(\\boldsymbol V\\) \npositive definite invertible correlation matrix \\(\\boldsymbol P\\) \\(\\boldsymbol \\Sigma= \\boldsymbol V^{1/2} \\boldsymbol P\\boldsymbol V^{1/2}\\).Whitening transformation:\\[\\underbrace{\\boldsymbol z}_{d \\times 1 \\text{ vector }} = \\underbrace{\\boldsymbol W}_{d \\times d \\text{ whitening matrix }} \\underbrace{\\boldsymbol x}_{d \\times 1 \\text{ vector }}\\]\nObjective: choose \\(\\boldsymbol W\\) \\(\\text{Var}(\\boldsymbol z)=\\boldsymbol I_d\\)Mahalanobis/ZCA whitening already know \\(\\boldsymbol W^{\\text{ZCA}}=\\boldsymbol \\Sigma^{-1/2}\\).general, whitening matrix \\(\\boldsymbol W\\) needs satisfy constraint:\n\\[\n\\begin{array}{lll}\n                & \\text{Var}(\\boldsymbol z) & = \\boldsymbol I_d \\\\\n\\Longrightarrow & \\text{Var}(\\boldsymbol W\\boldsymbol x) &= \\boldsymbol W\\boldsymbol \\Sigma\\boldsymbol W^T = \\boldsymbol I_d \\\\\n\\Longrightarrow &  \\boldsymbol W\\, \\boldsymbol \\Sigma\\, \\boldsymbol W^T \\boldsymbol W= \\boldsymbol W& \\\\\n\\end{array}\n\\]\n\\[\\Longrightarrow \\text{constraint whitening matrix: } \\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1}\\]Clearly, ZCA whitening matrix satisfies constraint: \\((\\boldsymbol W^{ZCA})^T \\boldsymbol W^{ZCA} = \\boldsymbol \\Sigma^{-1/2}\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1}\\)Covariance-based parameterisation whitening matrix:general way specify valid whitening matrix \n\\[\n\\boldsymbol W= \\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2}\n\\]\n\\(\\boldsymbol Q_1\\) orthogonal matrix.5Recall orthogonal matrix \\(\\boldsymbol Q\\) property \\(\\boldsymbol Q^{-1} = \\boldsymbol Q^T\\) \nconsequence \\(\\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol \\).result, \\(\\boldsymbol W\\) satisfies whitening constraint:\\[\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1/2}\\underbrace{\\boldsymbol Q_1^T \\boldsymbol Q_1}_{\\boldsymbol }\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1}\\]Note converse also true: whitening whitening matrix, .e. \\(\\boldsymbol W\\) satisfying whitening constraint, can written form \n\\(\\boldsymbol Q_1 = \\boldsymbol W\\boldsymbol \\Sigma^{1/2}\\) orthogonal construction.\\(\\Longrightarrow\\) instead choosing \\(\\boldsymbol W\\), choose orthogonal matrix \\(\\boldsymbol Q_1\\)!recall orthogonal matrices geometrically represent rotations (plus reflections).now clear infinitely many whitening procedures, infinitely many rotations! also means need find ways choose/select among whitening procedures.Mahalanobis/ZCA transformation \\(\\boldsymbol Q_1^{\\text{ZCA}}=\\boldsymbol \\)whitening can interpreted Mahalanobis transform followed rotationCorrelation-based parameterisation whitening matrix:Instead working covariance matrix \\(\\boldsymbol \\Sigma\\), can express \\(\\boldsymbol W\\) also terms corresponding correlation matrix \\(\\boldsymbol P= (\\rho_{ij}) = \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2}\\)\n\\(\\boldsymbol V^{1/2}\\) diagonal matrix containing variances.Specifically can specify whitening matrix \n\\[\\boldsymbol W= \\boldsymbol Q_2 \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2}\\]easy verify \\(\\boldsymbol W\\) also satisfies whitening constraint:\n\\[\n\\begin{split}\n\\boldsymbol W^T \\boldsymbol W& = \\boldsymbol V^{-1/2}\\boldsymbol P^{-1/2}\\underbrace{\\boldsymbol Q_2^T \\boldsymbol Q_2}_{\\boldsymbol }\\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} \\\\\n& = \\boldsymbol V^{-1/2} \\boldsymbol P^{-1} \\boldsymbol V^{-1/2} = \\boldsymbol \\Sigma^{-1} \\\\\n\\end{split}\n\\]\nConversely, whitening matrix \\(\\boldsymbol W\\) can also written form \n\\(\\boldsymbol Q_2 = \\boldsymbol W\\boldsymbol V^{1/2} \\boldsymbol P^{1/2}\\) orthogonal construction.Another interpretation whitening: first standardising (\\(\\boldsymbol V^{-1/2}\\)), decorrelation (\\(\\boldsymbol P^{-1/2}\\)), followed rotation (\\(\\boldsymbol Q_2\\))Mahalanobis/ZCA transformation \\(\\boldsymbol Q_2^{\\text{ZCA}} = \\boldsymbol \\Sigma^{-1/2} \\boldsymbol V^{1/2} \\boldsymbol P^{1/2}\\)forms write \\(\\boldsymbol W\\) using \\(\\boldsymbol Q_1\\) \\(\\boldsymbol Q_2\\) equally valid (interchangeable).Note \\(\\boldsymbol W\\)\n\\[\\boldsymbol Q_1\\neq\\boldsymbol Q_2 \\text{  Two different orthogonal matrices!}\\]\nalso\n\\[\\underbrace{\\boldsymbol \\Sigma^{-1/2}}_{\\text{Symmetric}}\\neq\\underbrace{\\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}}_{\\text{Symmetric}}\\]\neven though\\[\\boldsymbol \\Sigma^{-1/2}\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Sigma^{-1} = \\boldsymbol V^{-1/2}\\boldsymbol P^{-1/2}\\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"cross-covariance-and-cross-correlation-for-whitening-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.3.3 Cross-covariance and cross-correlation for whitening transformations","text":"useful criterion characterise distinguish among whitening transformations \ncross-covariance cross-correlation matrix whitened variable \\(\\boldsymbol z\\) \noriginal variable \\(\\boldsymbol x\\):Cross-covariance \\(\\boldsymbol \\Phi= \\Sigma_{\\boldsymbol z\\boldsymbol x}\\) \\(\\boldsymbol z\\) \\(\\boldsymbol x\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Phi= \\text{Cov}(\\boldsymbol z,\\boldsymbol x) & = \\text{Cov}(\\boldsymbol W\\boldsymbol x,\\boldsymbol x)\\\\\n& = \\boldsymbol W\\boldsymbol \\Sigma\\\\\n&= \\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2} \\boldsymbol \\Sigma\\\\\n&= \\boldsymbol Q_1\\boldsymbol \\Sigma^{1/2} \\\\\n\\end{split}\n\\]\ncomponent notation write \\(\\boldsymbol \\Phi= (\\phi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol z\\) column index \\(j\\) \\(\\boldsymbol x\\).\nCross-covariance linked \\(\\boldsymbol Q_1\\)!\nThus, choosing cross-covariance determines \\(\\boldsymbol Q_1\\) (vice versa).\nNote cross-covariance matrix \\(\\boldsymbol \\Phi\\) satisfies condition\n\\(\\boldsymbol \\Phi^T \\boldsymbol \\Phi= \\boldsymbol \\Sigma\\), reminiscent condition \\(\\boldsymbol W\\),\nnow covariance used rather inverse covariance.\nwhitening matrix expressed terms cross-covariance \\(\\boldsymbol W= \\boldsymbol \\Phi\\boldsymbol \\Sigma^{-1}\\), required \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Phi^T \\boldsymbol \\Phi\\boldsymbol \\Sigma^{-1} =\\boldsymbol \\Sigma^{-1}\\).\nFurthermore, transpose \\(\\boldsymbol \\Phi\\) \ninverse whitening matrix,\n\\(\\boldsymbol W^{-1} = \\left(\\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2} \\right)^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{T} = \\left( \\boldsymbol Q_1 \\boldsymbol \\Sigma^{1/2} \\right)^{T} = \\boldsymbol \\Phi^T\\).Cross-covariance \\(\\boldsymbol \\Phi= \\Sigma_{\\boldsymbol z\\boldsymbol x}\\) \\(\\boldsymbol z\\) \\(\\boldsymbol x\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Phi= \\text{Cov}(\\boldsymbol z,\\boldsymbol x) & = \\text{Cov}(\\boldsymbol W\\boldsymbol x,\\boldsymbol x)\\\\\n& = \\boldsymbol W\\boldsymbol \\Sigma\\\\\n&= \\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2} \\boldsymbol \\Sigma\\\\\n&= \\boldsymbol Q_1\\boldsymbol \\Sigma^{1/2} \\\\\n\\end{split}\n\\]\ncomponent notation write \\(\\boldsymbol \\Phi= (\\phi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol z\\) column index \\(j\\) \\(\\boldsymbol x\\).Cross-covariance linked \\(\\boldsymbol Q_1\\)!\nThus, choosing cross-covariance determines \\(\\boldsymbol Q_1\\) (vice versa).Note cross-covariance matrix \\(\\boldsymbol \\Phi\\) satisfies condition\n\\(\\boldsymbol \\Phi^T \\boldsymbol \\Phi= \\boldsymbol \\Sigma\\), reminiscent condition \\(\\boldsymbol W\\),\nnow covariance used rather inverse covariance.whitening matrix expressed terms cross-covariance \\(\\boldsymbol W= \\boldsymbol \\Phi\\boldsymbol \\Sigma^{-1}\\), required \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Phi^T \\boldsymbol \\Phi\\boldsymbol \\Sigma^{-1} =\\boldsymbol \\Sigma^{-1}\\).\nFurthermore, transpose \\(\\boldsymbol \\Phi\\) \ninverse whitening matrix,\n\\(\\boldsymbol W^{-1} = \\left(\\boldsymbol Q_1 \\boldsymbol \\Sigma^{-1/2} \\right)^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{-1} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol Q_1^{T} = \\left( \\boldsymbol Q_1 \\boldsymbol \\Sigma^{1/2} \\right)^{T} = \\boldsymbol \\Phi^T\\).Cross-correlation \\(\\boldsymbol \\Psi= \\boldsymbol P_{\\boldsymbol z\\boldsymbol x}\\) \\(\\boldsymbol z\\) \\(\\boldsymbol x\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Psi= \\text{Cor}(\\boldsymbol z,\\boldsymbol x) & = \\boldsymbol \\Phi\\boldsymbol V^{-1/2}\\\\\n& = \\boldsymbol W\\boldsymbol \\Sigma\\boldsymbol V^{-1/2}\\\\\n&=\\boldsymbol Q_2 \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2} \\\\\n& =  \\boldsymbol Q_2\\boldsymbol P^{1/2}\\\\\n\\end{split}\n\\]\ncomponent notation write \\(\\boldsymbol \\Psi= (\\psi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol z\\) column index \\(j\\) \\(\\boldsymbol x\\).\nCross-correlation linked \\(\\boldsymbol Q_2\\)!\nHence, choosing cross-correlation determines \\(\\boldsymbol Q_2\\) (vice versa). whitening\nmatrix expressed terms cross-correlation \n\\(\\boldsymbol W= \\boldsymbol \\Psi\\boldsymbol P^{-1} \\boldsymbol V^{-1/2}\\).Cross-correlation \\(\\boldsymbol \\Psi= \\boldsymbol P_{\\boldsymbol z\\boldsymbol x}\\) \\(\\boldsymbol z\\) \\(\\boldsymbol x\\):\n\\[\n\\begin{split}\n\\boldsymbol \\Psi= \\text{Cor}(\\boldsymbol z,\\boldsymbol x) & = \\boldsymbol \\Phi\\boldsymbol V^{-1/2}\\\\\n& = \\boldsymbol W\\boldsymbol \\Sigma\\boldsymbol V^{-1/2}\\\\\n&=\\boldsymbol Q_2 \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2} \\\\\n& =  \\boldsymbol Q_2\\boldsymbol P^{1/2}\\\\\n\\end{split}\n\\]component notation write \\(\\boldsymbol \\Psi= (\\psi_{ij})\\) row index \\(\\)\nrefers \\(\\boldsymbol z\\) column index \\(j\\) \\(\\boldsymbol x\\).Cross-correlation linked \\(\\boldsymbol Q_2\\)!\nHence, choosing cross-correlation determines \\(\\boldsymbol Q_2\\) (vice versa). whitening\nmatrix expressed terms cross-correlation \n\\(\\boldsymbol W= \\boldsymbol \\Psi\\boldsymbol P^{-1} \\boldsymbol V^{-1/2}\\).Note factorisation cross-covariance \\(\\boldsymbol \\Phi=\\boldsymbol Q_1\\boldsymbol \\Sigma^{1/2}\\) \ncross-correlation \\(\\boldsymbol \\Psi=\\boldsymbol Q_2\\boldsymbol P^{1/2}\\) product orthogonal matrix\npositive semi-definite symmetric matrix examples polar decomposition.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"inverse-whitening-transformation-and-loadings","chapter":"2 Transformations and dimension reduction","heading":"2.3.4 Inverse whitening transformation and loadings","text":"Inverse transformation:Recall \\(\\boldsymbol z= \\boldsymbol W\\boldsymbol x\\). Therefore, reverse transformation going whitened\noriginal variable \\(\\boldsymbol x= \\boldsymbol W^{-1} \\boldsymbol z\\).\ncan expressed also terms cross-covariance cross-correlation.\n\\(\\boldsymbol W^{-1} = \\boldsymbol \\Phi^T\\) get\n\\[\n\\boldsymbol x= \\boldsymbol \\Phi^T \\boldsymbol z\\, .\n\\]\nFurthermore, since \\(\\boldsymbol \\Psi= \\boldsymbol \\Phi\\boldsymbol V^{-1/2}\\) \n\\(\\boldsymbol W^{-1} = \\boldsymbol V^{1/2} \\boldsymbol \\Psi^T\\) \n\\[\n\\boldsymbol V^{-1/2} \\boldsymbol x=   \\boldsymbol \\Psi^T \\boldsymbol z\\, .\n\\]reverse whitening transformation also known colouring transformation\n(previously discussed inverse Mahalanobis transform one example).Definition loadings:Loadings coefficients linear transformation latent variable back observed variable. variables standardised unit variance loadings also called correlation loadings.Hence, cross-covariance matrix plays role loadings linking latent variable \\(\\boldsymbol z\\)\noriginal \\(\\boldsymbol x\\). Similarly, cross-correlation matrix correlation loadings\nlinking (already standardised) latent variable \\(\\boldsymbol z\\) standardised \\(\\boldsymbol x\\).Multiple correlation coefficients \\(\\boldsymbol z\\) back \\(\\boldsymbol x\\):Consider backtransformation whitened variable \\(\\boldsymbol z\\) original variables \\(\\boldsymbol x\\).\nNote particular components \\(\\boldsymbol z\\) uncorrelated. Therefore, can compute squared multiple correlation coefficient \\(x_j\\) \\(\\boldsymbol z\\)\nsimply sum squared correlations\n\\(\\text{Cor}(z_i, x_j)^2\\):\n\\[\n\\text{Cor}(\\boldsymbol z, x_j)^2 = \\sum_{=1}^d  \\text{Cor}(z_i, x_j)^2    = \\sum_{=1}^d \\psi_{ij}^2\n\\]\nvector notation \\(\\boldsymbol \\Psi= (\\psi_{ij})\\) get\n\\[\n\\begin{split}\n\\left(\\text{Cor}(\\boldsymbol z, x_1)^2, \\ldots, \\text{Cor}(\\boldsymbol z, x_d)^2 \\right)^T &= \\text{Diag}\\left(\\boldsymbol \\Psi^T \\boldsymbol \\Psi\\right) \\\\\n&= \\text{Diag}\\left(\\boldsymbol P^{1/2} \\boldsymbol Q_2^T \\boldsymbol Q_2\\boldsymbol P^{1/2}\\right) \\\\\n&= \\text{Diag}(\\boldsymbol P) \\\\\n&= (1, \\ldots, 1)^T\\\\\n\\end{split}\n\\]\nTherefore, whitening transformation, column sums matrix \\((\\psi_{ij}^2)\\) 1 regardless choice \\(\\boldsymbol Q_2\\):\n\\[\n\\sum_{=1}^d \\psi_{ij}^2 = 1 \\text{ } j\n\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"summaries-of-cross-covariance-boldsymbol-phi-and-cross-correlation-boldsymbol-psi-resulting-from-whitening-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.3.5 Summaries of cross-covariance \\(\\boldsymbol \\Phi\\) and cross-correlation \\(\\boldsymbol \\Psi\\) resulting from whitening transformations","text":"Matrix trace:simply summary matrix trace. cross-covariance matrix \\(\\boldsymbol \\Phi\\) trace \nsum covariances corresponding elements \\(\\boldsymbol z\\) \\(\\boldsymbol x\\):\n\\[\n\\text{Tr}(\\boldsymbol \\Phi) =  \\sum_{=1}^d \\text{Cov}(z_i, x_i) =  \\sum_{=1}^d  \\phi_{ii} = \\text{Tr}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma^{1/2}\\right)\n\\]\nLikewise, cross-correlation matrix \\(\\boldsymbol \\Psi\\) trace \nsum correlations corresponding elements \\(\\boldsymbol z\\) \\(\\boldsymbol x\\):\n\\[\n\\text{Tr}(\\boldsymbol \\Psi) =  \\sum_{=1}^d \\text{Cor}(z_i, x_i) =  \\sum_{=1}^d  \\psi_{ii} = \\text{Tr}\\left(\\boldsymbol Q_2\\boldsymbol P^{1/2}\\right)\n\\]cases value trace depends \\(\\boldsymbol Q_1\\) \\(\\boldsymbol Q_2\\).\nInterestingly, unique choice orthogonal matrices trace maximised.Specifically, maximise \\(\\text{Tr}(\\boldsymbol \\Phi)\\) conduct following steps:Apply eigendecomposition \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\). Note \\(\\boldsymbol \\Lambda\\) diagonal positive eigenvalues \\(\\lambda_i > 0\\) \\(\\boldsymbol \\Sigma\\) positive definite \\(\\boldsymbol U\\) orthogonal matrix.objective function becomes\n\\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol \\Phi) &= \\text{Tr}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma^{1/2}\\right)\\\\\n&= \\text{Tr}\\left(\\boldsymbol Q_1 \\boldsymbol U\\boldsymbol \\Lambda^{1/2} \\boldsymbol U^T  \\right) \\\\\n&= \\text{Tr}\\left(\\boldsymbol \\Lambda^{1/2} \\, \\boldsymbol U^T \\boldsymbol Q_1 \\boldsymbol U\\right) \\\\\n& = \\text{Tr}\\left(\\boldsymbol \\Lambda^{1/2} \\, \\boldsymbol B\\right) \\\\\n& = \\sum_{=1}^d \\lambda_i^{1/2} b_{ii}.\n\\end{split} \n\\]\nNote product two orthogonal matrices orthogonal matrix.\nTherefore, \\(\\boldsymbol B= \\boldsymbol U^T \\boldsymbol Q_1 \\boldsymbol U\\) orthogonal matrix \n\\(\\boldsymbol Q_1 = \\boldsymbol U\\boldsymbol B\\boldsymbol U^T\\).\\(\\lambda_i > 0\\) \\(b_{ii} \\[-1, 1]\\) objective function maximised\n\\(b_{ii}=1\\), .e. \\(\\boldsymbol B=\\boldsymbol \\).turn implies \\(\\text{Tr}(\\boldsymbol \\Phi)\\) maximised \\(\\boldsymbol Q_1=\\boldsymbol \\).Similary, maximise \\(\\text{Tr}(\\boldsymbol \\Psi)\\) wedecompose \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\) , following ,find \\(\\text{Tr}(\\boldsymbol \\Psi) = \\text{Tr}\\left(\\boldsymbol \\Theta^{1/2} \\, \\boldsymbol G^T \\boldsymbol Q_2 \\boldsymbol G\\right)\\) maximised \\(\\boldsymbol Q_2=\\boldsymbol \\).Frobenius norm total variation:Another way summarise dissect association \\(\\boldsymbol x\\) corresponding whitened \\(\\boldsymbol z\\)\nFrobenius norm total variation based \\(\\boldsymbol \\Phi\\) \\(\\boldsymbol \\Psi\\).Frobenius norm (Euclidean) norm sum squared elements matrix.consider Frobenius norm cross-covariance matrix, .e. sum squared covariances \n\\(\\boldsymbol z\\) \\(\\boldsymbol x\\),\n\\[\n|| \\boldsymbol \\Phi||_F = \\sum_{=1}^d \\sum_{j=1}^d \\phi_{ij}^2 =  \\text{Tr}(\\boldsymbol \\Phi\\boldsymbol \\Phi^T) = \\text{Tr}( \\boldsymbol \\Sigma)\n\\]\nfind equals total variation \\(\\boldsymbol \\Sigma\\) depend \\(\\boldsymbol Q_1\\).\nLikewise, computing Frobenius norm cross-correlation matrix, .e. sum squared correlations \n\\(\\boldsymbol z\\) \\(\\boldsymbol x\\),\n\\[\n|| \\boldsymbol \\Psi||_F  = \\sum_{=1}^d \\sum_{j=1}^d \\psi_{ij}^2= \\text{Tr}(\\boldsymbol \\Psi\\boldsymbol \\Psi^T) =\\text{Tr}( \\boldsymbol P) = d\n\\]\nyields total variation \\(\\boldsymbol P\\) also depend \\(\\boldsymbol Q_2\\).\nNote Frobenius norm invariant rotations reflections.Proportion total variation:can now compute \nrelative contribution whitened component \\(z_i\\) total variation.\nsum squared covariances \\(z_i\\) \\(x_1, \\ldots, x_d\\) \n\\[\nh_i = \\sum^d_{j=1}\\text{Cov}(z_i,x_j)^2 = \\sum^d_{j=1} \\phi_{ij}^2\n\\]\nvector notation written\n\\[\n\\boldsymbol h= (h_1,...,h_d)^T = \\text{Diag}(\\boldsymbol \\Phi\\boldsymbol \\Phi^T) = \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\n\\]\nalso highlights \\(\\boldsymbol h\\) function \\(\\boldsymbol Q_1\\).\n\n\\[\n\\frac{ h_i }{\\text{Tr}( \\boldsymbol \\Sigma)}\n\\]\ngives relative contribution \\(z_i\\) total variation \\(\\boldsymbol \\Sigma\\).Similarly, sum squared correlations \\(z_i\\) \\(x_1, \\ldots, x_d\\) \n\\[\nk_i = \\sum^d_{j=1}\\text{Cor}(z_i,x_j)^2 = \\sum^d_{j=1} \\psi_{ij}^2\n\\]\nvector notation\n\\[\n\\boldsymbol k= (k_1,...,k_d)^T = \\text{Diag}\\left(\\boldsymbol \\Psi\\boldsymbol \\Psi^T\\right)=\\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\n\\]\nyields\n\\[\n\\frac{ k_i  }{\\text{Tr}( \\boldsymbol P)} = \\frac{ k_i  }{d}\n\\]\nrelative contribution \\(z_i\\) total variation correlation \\(\\boldsymbol P\\).Note contrast total variation , relative contributions \\(h_i\\) \\(k_i\\) depend \\(\\boldsymbol Q_1\\) \\(\\boldsymbol Q_2\\), respectively.Maximising proportion total variation:Crucially, \\(h_i\\) \\(k_i\\) quadratic form \\(q(\\boldsymbol v) = \\boldsymbol v^T \\boldsymbol \\boldsymbol v\\)\n\\(\\boldsymbol \\) symmetric, real positive definite matrix \\(\\boldsymbol v\\) unit length vector.\ngeneral result constrained optimisation quadratic form maximum value can take equal largest eigenvalue \\(\\boldsymbol \\), conversely smallest value equal smallest eigenvalue \\(\\boldsymbol \\)., generally, one can choose set orthonormal vectors \n\\(h_1\\) maximised, \\(h_2\\) maximised, , final values ordered\n\\(h_1 \\geq h_2 \\geq \\ldots \\geq h_d\\) (similary \\(k_1, k_2, \\ldots, k_d\\)).\noptimal solution maximisation problem obtained computing\neigendecompositions \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\)\nsetting \\(\\boldsymbol Q_1= \\boldsymbol U^T\\) \\(\\boldsymbol Q_2= \\boldsymbol G^T\\), respectively. yields\n\\[\n\\boldsymbol h= (h_1,...,h_d)^T = (\\lambda_1, \\ldots, \\lambda_d)^T\n\\]\n\n\\[\n\\boldsymbol k= (k_1,...,k_d)^T = (\\theta_1, \\ldots, \\theta_d)^T\n\\]\nRecall sign (direction) eigenvector undetermined. Thus, columns signs \\(\\boldsymbol U\\) \\(\\boldsymbol G\\) can freely chosen, multiple sets orthormal\nvectors solve maximisation problem.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"natural-whitening-procedures","chapter":"2 Transformations and dimension reduction","heading":"2.4 Natural whitening procedures","text":"Now discuss several strategies (maximise correlation individual components, maximise compression, etc.) arrive optimal whitening transformation.leads following “natural” whitening transformations:Mahalanobis whitening, also known ZCA (zero-phase component analysis) whitening machine learning (based covariance)ZCA-cor whitening (based correlation)PCA whitening (based covariance)PCA-cor whitening (based correlation)Cholesky whitening (based precision covariance matrix)Note three main types (ZCA, PCA, Cholesky) two variations .following \\(\\boldsymbol x_c = \\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x}\\) \\(\\boldsymbol z_c = \\boldsymbol z-\\boldsymbol \\mu_{\\boldsymbol z}\\) denote mean-centered variables.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"zca-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.1 ZCA whitening","text":"Aim: remove correlations otherwise make sure whitening \\(\\boldsymbol z\\) differ much \\(\\boldsymbol x\\). Specifically, element \\(z_i\\) close possible corresponding element \\(x_i\\):\n\\[\n\\begin{array}{cc}\nz_1\\leftrightarrow x_1 \\\\\nz_2\\leftrightarrow x_2\\\\\nz_3\\leftrightarrow x_3 \\\\\n\\vdots\n\\end{array}\n\\]\nOne possible way implement compute expected squared difference two centered random vectors \\(\\boldsymbol z_c\\) \\(\\boldsymbol x_c\\).ZCA objective function: minimise \\(\\text{E}\\left((\\boldsymbol z_c-\\boldsymbol x_c)^T(\\boldsymbol z_c-\\boldsymbol x_c)\\right)\\) find optimal whitening procedure.ZCA objective function can simplified follows:\n\\[\n\\begin{split}\n& = \\text{E}( \\boldsymbol z_c^T \\boldsymbol z_c ) - 2 \\text{E}( \\boldsymbol x_c^T \\boldsymbol z_c ) + \\text{E}(\\boldsymbol x_c^T \\boldsymbol x_c)  \\\\\n& = \\text{E}( \\text{Tr}( \\boldsymbol z_c \\boldsymbol z_c^T ) ) - 2 \\text{E}( \\text{Tr}( \\boldsymbol z_c \\boldsymbol x_c^T ) ) +\n \\text{E}(  \\text{Tr}( \\boldsymbol x_c \\boldsymbol x_c^T ) )  \\\\\n& = \\text{Tr}( \\text{E}( \\boldsymbol z_c \\boldsymbol z_c^T ) ) - 2 \\text{Tr}( \\text{E}(  \\boldsymbol z_c \\boldsymbol x_c^T ) ) +\n \\text{Tr}(  \\text{E}( \\boldsymbol x_c \\boldsymbol x_c^T ) )  \\\\\n& = \\text{Tr}( \\text{Var}(\\boldsymbol z) ) - 2 \\text{Tr}( \\text{Cov}(\\boldsymbol z, \\boldsymbol x) ) + \\text{Tr}( \\text{Var}(\\boldsymbol x) )   \\\\\n& = d - 2\\text{Tr}(\\boldsymbol \\Phi)+\\text{Tr}(\\boldsymbol V) \\\\\n\\end{split}\n\\]\nterm depends whitening transformation \\(-2 \\text{Tr}(\\boldsymbol \\Phi)\\) function\n\\(\\boldsymbol Q_1\\). Therefore can use following\nalternative objective:ZCA equivalent objective: maximise \\(\\text{Tr}(\\boldsymbol \\Phi) = \\text{Tr}(\\boldsymbol Q_1\\boldsymbol \\Sigma^{1/2})\\) find optimal \\(\\boldsymbol Q_1\\)Solution:previous discussion know optimal matrix \n\\[\n\\boldsymbol Q_1^{\\text{ZCA}}=\\boldsymbol \n\\]\ncorresponding whitening matrix ZCA therefore\n\\[\n\\boldsymbol W^{\\text{ZCA}} = \\boldsymbol \\Sigma^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{ZCA}} = \\boldsymbol \\Sigma^{1/2}\n\\]\ncross-correlation matrix\n\\[\n\\boldsymbol \\Psi^{\\text{ZCA}} = \\boldsymbol \\Sigma^{1/2} \\boldsymbol V^{-1/2}\n\\]Note \\(\\boldsymbol \\Sigma^{1/2}\\) symmetric positive definite matrix,\nhence diagonal elements positive. result,\ndiagonals \\(\\boldsymbol \\Phi^{\\text{ZCA}}\\) \\(\\boldsymbol \\Psi^{\\text{ZCA}}\\) positive,\n.e. \\(\\text{Cov}(z_i, x_i) > 0\\) \\(\\text{Cor}(z_i, x_i) > 0\\).\nHence, ZCA two corresponding components \\(z_i\\) \\(x_i\\) always positively correlated!Proportion total variation:ZCA \\(\\boldsymbol Q_1=\\boldsymbol \\) find \\(\\boldsymbol h=\\text{Diag}(\\boldsymbol \\Sigma)\\) \\(h_i=\\text{Var}(x_i)\\) hence ZCA proportion total variation contributed \nZCA component \\(z_i\\) ratio variance \ncorresponding \\(x_i\\) total variation \\(\\text{Tr}(\\boldsymbol \\Sigma)\\).Summary:ZCA/Mahalanobis transform unique transformation minimises expected total squared component-wise difference \\(\\boldsymbol x_c\\) \\(\\boldsymbol z_c\\).ZCA corresponding components whitened original variables always positively correlated. facilitates interpretation whitened variables.Use ZCA aka Mahalanobis whitening want “just” remove correlations.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"zca-cor-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.2 ZCA-Cor whitening","text":"Aim: remove scale \\(\\boldsymbol x\\) first comparing \\(\\boldsymbol z\\)ZCA-cor objective function: minimise \\(\\text{E}\\left((\\boldsymbol z_c-\\boldsymbol V^{-1/2}\\boldsymbol x_c)^T(\\boldsymbol z_c-\\boldsymbol V^{-1/2}\\boldsymbol x_c)\\right)\\) find optimal whitening procedure.can simplified follows:\n\\[\n\\begin{split}\n& = \\text{E}( \\boldsymbol z_c^T \\boldsymbol z_c ) - 2 \\text{E}( \\boldsymbol x_c^T \\boldsymbol V^{-1/2} \\boldsymbol z_c ) + \\text{E}(\\boldsymbol x_c^T \\boldsymbol V^{-1} \\boldsymbol x_c)  \\\\\n& = \\text{Tr}( \\text{Var}(\\boldsymbol z) ) - 2 \\text{Tr}( \\text{Cor}(\\boldsymbol z, \\boldsymbol x) ) + \\text{Tr}(  \\text{Cor}(\\boldsymbol x, \\boldsymbol x) )   \\\\\n& = d - 2\\text{Tr}(\\boldsymbol \\Psi)+ d \\\\\n& = 2d - 2\\text{Tr}(\\boldsymbol \\Psi)\n\\end{split}\n\\]\nterm depends whitening transformation via \\(\\boldsymbol Q_2\\) \\(-2 \\text{Tr}(\\boldsymbol \\Psi)\\) can use following alternative objective instead:ZCA-cor equivalent objective: maximise \\(\\text{Tr}(\\boldsymbol \\Psi)=\\text{Tr}(\\boldsymbol Q_2\\boldsymbol P^{1/2})\\) find optimal \\(\\boldsymbol Q_2\\)Solution: ZCA using correlation instead covariance:previous discussion know optimal matrix \n\\[\n\\boldsymbol Q_2^{\\text{ZCA-Cor}}=\\boldsymbol \n\\]\ncorresponding whitening matrix ZCA-cor therefore\n\\[\n\\boldsymbol W^{\\text{ZCA-Cor}} = \\boldsymbol P^{-1/2}\\boldsymbol V^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{ZCA-Cor}} = \\boldsymbol P^{1/2} \\boldsymbol V^{1/2}\n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{ZCA-Cor}} = \\boldsymbol P^{1/2}\n\\]ZCA-cor transformation also \n\\(\\text{Cov}(z_i, x_i) > 0\\) \\(\\text{Cor}(z_i, x_i) > 0\\)\ntwo corresponding components \\(z_i\\) \\(x_i\\) always positively correlated!Proportion total variation:ZCA-cor \\(\\boldsymbol Q_2=\\boldsymbol \\) find \\(\\boldsymbol k=\\text{Diag}(\\boldsymbol P)\\) \\(k_i =1\\).\nThus, ZCA-cor whitened component \\(z_i\\) contributes equally total variation \\(\\text{Tr}(\\boldsymbol P) =d\\).Summary:ZCA-cor whitening unique whitening transformation maximising \ntotal correlation corresponding elements \\(\\boldsymbol x\\) \\(\\boldsymbol z\\).ZCA-cor leads interpretable \\(\\boldsymbol z\\) individual element \\(\\boldsymbol z\\)\n(typically strongly) positively correlated corresponding element original \\(\\boldsymbol x\\).ZCA-cor explicitly constructed maximise total\npairwise correlations achieves higher correlation ZCA.\\(\\boldsymbol x\\) standardised \\(\\text{Var}(x_i)=1\\) ZCA ZCA-cor identical.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.3 PCA whitening","text":"Aim: remove correlations compress information latent variables.\nSpecifically, like first latent component \\(z_1\\) \nmaximally linked variables \\(\\boldsymbol x\\), followed \nsecond component \\(z_2\\) :\n\\[\n\\begin{array}{ccccccc}\nz_1 & \\leftarrow x_1 & & z_2 & \\leftarrow x_1  && \\ldots \\\\\nz_1 & \\leftarrow x_2 & & z_2 & \\leftarrow x_2  \\\\\n\\vdots\\\\\nz_1 & \\leftarrow x_d & & z_2 & \\leftarrow x_d  \\\\\n\\end{array}\n\\]\nOne way measure total association latent component \\(z_i\\) \nobserved \\(x_1, \\ldots, x_d\\) sum \ncorresponding squared covariances\n\\[\nh_i = \\sum^d_{j=1}\\text{Cov}(z_i,x_j)^2 = \\sum^d_{j=1} \\phi_{ij}^2\n\\]\nequivalently\n\\[\n\\boldsymbol h= (h_1,...,h_d)^T = \\text{Diag}(\\boldsymbol \\Phi\\boldsymbol \\Phi^T) = \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\n\\]\nhighlights \\(\\boldsymbol h\\) function \\(\\boldsymbol Q_1\\). \\(h_i\\) contribution\n\\(z_i\\) \\(\\sum_{=1}^d h_i = \\text{Tr}\\left( \\boldsymbol Q_1 \\boldsymbol \\Sigma\\boldsymbol Q_1^T \\right)= \\text{Tr}(\\boldsymbol \\Sigma)\\)\n.e. total variation based \\(\\boldsymbol \\Sigma\\).\n\\(\\text{Tr}(\\boldsymbol \\Sigma)\\) constant implies \\(d-1\\) independent \\(h_i\\).PCA-whitening wish concentrate contributions total variation based\n\\(\\boldsymbol \\Sigma\\) small number\nlatent components.PCA whitening objective function: maximise \\(h_1, \\ldots, h_{d-1}\\) \\(\\boldsymbol h= \\text{Diag}\\left(\\boldsymbol Q_1\\boldsymbol \\Sigma\\boldsymbol Q_1^T\\right)\\) \\(h_1 \\geq h_2 \\geq \\dots \\geq h_d\\)\nfind optimal optimal \\(\\boldsymbol Q_1\\) corresponding whitening transformation.Solution:Following previous discussion (Frobenius norm \\(\\boldsymbol \\Phi\\) constrained optimisation quadratic forms) optimal\nsolution obtained eigendecomposition \\(\\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\).\nHence, optimal value \\(\\boldsymbol Q_1\\) matrix \n\\[\n\\boldsymbol Q_1^{\\text{PCA}}=\\boldsymbol U^T\n\\]\nHowever, recall \\(\\boldsymbol U\\) uniquely defined — free change columns signs.\ncorresponding whitening matrix \n\\[\n\\boldsymbol W^{\\text{PCA}} = \\boldsymbol U^T\\boldsymbol \\Sigma^{-1/2}=\\boldsymbol \\Lambda^{-1/2}\\boldsymbol U^T\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{PCA}} = \\boldsymbol \\Lambda^{1/2} \\boldsymbol U^T\n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{PCA}} = \\boldsymbol \\Lambda^{1/2} \\boldsymbol U^T \\boldsymbol V^{-1/2}\n\\]Identifiability:Note (.e. \\(\\boldsymbol Q_1^{\\text{PCA}}, \\boldsymbol W^{\\text{PCA}}, \\boldsymbol \\Phi^{\\text{PCA}}, \\boldsymbol \\Psi^{\\text{PCA}}\\)) unique\ndue sign ambiguity columns\n\\(\\boldsymbol U\\).Therefore, identifiability reasons need impose constraint \\(\\boldsymbol Q_1^{\\text{PCA}}\\).\nuseful condition require positive\ndiagonal, .e. \\(\\text{Diag}(\\boldsymbol Q_1^{\\text{PCA}}) > 0\\) also \\(\\text{Diag}(\\boldsymbol U) > 0\\).\nresult, \\(\\text{Diag}(\\boldsymbol \\Phi^{\\text{PCA}}) > 0\\) \\(\\text{Diag}(\\boldsymbol \\Psi^{\\text{PCA}}) > 0\\). \nconstraint place pairs \\(x_i\\) \\(z_i\\) positively correlated.particularly important pay attention sign ambiguity\ndifferent computer implementations PCA whitening (related PCA approach)\nused.Proportion total variation:PCA whitening contribution \\(h_i^{\\text{PCA}}\\) latent component\ntotal variation based covariance \\(\\text{Tr}{\\boldsymbol \\Sigma} = \\sum_{=1}^d \\lambda_i\\) \n\\(h_i^{\\text{PCA}} = \\lambda_i\\).\nfraction \\(\\frac{\\lambda_i}{\\sum^d_{j=1}\\lambda_j}\\) proportional\ncontribution element \\(\\boldsymbol z\\) explain total variation.Thus, low ranking components \\(z_i\\) small \\(h_i^{\\text{PCA}}=\\lambda_i\\) may discarded.\nway PCA whitening achieve compressions corresponding\nreduction dimension.Summary:PCA whitening whitening transformation maximises compression sum squared cross-covariances underlying optimality criterion.sign ambiguities PCA whitened variables inherited sign ambiguities eigenvectors.positive-diagonal condition orthogonal matrices imposed sign ambiguities fully resolved corresponding components \\(z_i\\) \\(x_i\\) always positively correlated.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-cor-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.4 PCA-cor whitening","text":"Aim: PCA whitening remove scale \\(\\boldsymbol x\\) first. means use squared correlations rather squared covariances measure compression, .e.\\[\nk_i = \\sum^d_{j=1}\\text{Cor}(z_i, x_j)^2 = \\sum^d_{j=1} \\psi_{ij}^2\n\\]\nvector notation\n\\[\n\\boldsymbol k= (k_1,...,k_d)^T = \\text{Diag}\\left(\\boldsymbol \\Psi\\boldsymbol \\Psi^T\\right)=\\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\n\\]\n\\(k_i\\) contribution\n\\(z_i\\) \\(\\sum_{=1}^d k_i = \\text{Tr}\\left( \\boldsymbol Q_2 \\boldsymbol P\\boldsymbol Q_2^T \\right)= \\text{Tr}(\\boldsymbol P) = d\\)\n.e. total variation based \\(\\boldsymbol P\\).\n\\(\\text{Tr}(\\boldsymbol P)=d\\) constant implies \\(d-1\\) independent \\(k_i\\).PCA-cor-whitening wish concentrate contributions total variation based\n\\(\\boldsymbol P\\) small number\nlatent components.PCA-cor whitening objective function:\nmaximise \\(k_1, \\ldots, k_{d-1}\\) \\(\\boldsymbol k= \\text{Diag}\\left(\\boldsymbol Q_2\\boldsymbol P\\boldsymbol Q_2^T\\right)\\) \\(k_1 \\geq k_2 \\geq \\dots \\geq k_d\\)\nfind optimal optimal \\(\\boldsymbol Q_2\\) corresponding whitening transformation.Solution:Following optimal\nsolution obtained eigendecomposition \\(\\boldsymbol P= \\boldsymbol G\\boldsymbol \\Theta\\boldsymbol G^T\\).\nHence, optimal value \\(\\boldsymbol Q_2\\) matrix \n\\[\n\\boldsymbol Q_2^{\\text{PCA-Cor}}=\\boldsymbol G^T\n\\]\n\\(\\boldsymbol G\\) uniquely defined — free change signs columns.\ncorresponding whitening matrix \n\\[\n\\boldsymbol Q_2^{\\text{PCA-Cor}}=\\boldsymbol G^T\n\\]\ncorresponding whitening matrix \\[\n\\boldsymbol W^{\\text{PCA-Cor}} = \\boldsymbol \\Theta^{-1/2} \\boldsymbol G^T \\boldsymbol V^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\boldsymbol \\Phi^{\\text{PCA-Cor}} = \\boldsymbol \\Theta^{1/2} \\boldsymbol G^T \\boldsymbol V^{1/2}\n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{PCA-Cor}} = \\boldsymbol \\Theta^{1/2} \\boldsymbol G^T\n\\]Identifiability:PCA whitening, sign ambiguities column signs \\(\\boldsymbol G\\)\ncan freely chosen. identifiability choose set \\(\\text{Diag}(\\boldsymbol Q_2^{\\text{PCA-Cor}}) > 0\\) also\n\\(\\text{Diag}(\\boldsymbol G) > 0\\) \\(\\text{Diag}(\\boldsymbol \\Phi^{\\text{PCA-Cor}}) > 0\\) \\(\\text{Diag}(\\boldsymbol \\Psi^{\\text{PCA-Cor}}) > 0\\).Proportion total variation:PCA-cor whitening contribution \\(k_i^{\\text{PCA-Cor}}\\) latent component\ntotal variation based correlation \\(\\text{Tr}{\\boldsymbol P} = d\\) \n\\(k_i^{\\text{PCA-Cor}} = \\theta_i\\).\nfraction \\(\\frac{\\theta_i}{d}\\) proportional\ncontribution element \\(\\boldsymbol z\\) explain total variation.Summary:PCA-cor whitening whitening transformation maximises compression sum squared cross-correlations underlying optimality criterion.sign ambiguities PCA-cor whitened variables inherited sign ambiguities eigenvectors.positive-diagonal condition orthogonal matrices imposed sign ambiguities fully resolved corresponding components \\(z_i\\) \\(x_i\\) always positively correlated.\\(\\boldsymbol x\\) standardised \\(\\text{Var}(x_i)=1\\), PCA PCA-cor whitening identical.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"cholesky-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.5 Cholesky whitening","text":"Cholesky matrix decomposition:Cholesky decomposition square matrix \\(\\boldsymbol = \\boldsymbol L\\boldsymbol L^T\\) requires positive definite \\(\\boldsymbol \\) unique.\n\\(\\boldsymbol L\\) lower triangular matrix positive diagonal elements.\ninverse \\(\\boldsymbol L^{-1}\\) also lower triangular positive diagonal elements.\n\\(\\boldsymbol D\\) diagonal matrix positive elements \\(\\boldsymbol D\\boldsymbol L\\) also lower triangular matrix positive\ndiagonal Cholesky factor matrix \\(\\boldsymbol D\\boldsymbol \\boldsymbol D\\).Aim Cholesky whitening:Find whitening transformation cross-covariance \\(\\boldsymbol \\Phi\\) cross-correlation \\(\\boldsymbol \\Psi\\) triangular structure. structural constraint occurs example models time course data.obtain whitening transformation can apply Cholesky decomposition\neither precision matrix covariance matrix. Furthermore, can apply inverse correlation correlation matrix.Solution 1: Apply Cholesky decomposition \\(\\boldsymbol \\Sigma^{-1} = \\boldsymbol L\\boldsymbol L^T\\)resulting whitening matrix \n\\[\n\\boldsymbol W^{\\text{Chol}.1}=\\boldsymbol L^T\n\\]\nconstruction, \\(\\boldsymbol W^{\\text{Chol}.1}\\) upper triangular matrix positive\ndiagonal, clearly satisfies whitening constraint since \\((\\boldsymbol W^{\\text{Chol}.1})^T\\boldsymbol W^{\\text{Chol}.1} = \\boldsymbol \\Sigma^{-1}\\).cross-covariance matrix (\\(\\boldsymbol \\Sigma= (\\boldsymbol L^{-1})^T \\boldsymbol L^{-1}\\))\n\\[\n\\boldsymbol \\Phi^{\\text{Chol}.1} = \\boldsymbol L^T\\boldsymbol \\Sigma=  \\boldsymbol L^{-1}\n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{Chol}.1} = \\boldsymbol L^T \\boldsymbol \\Sigma\\boldsymbol V^{-1/2} =  \\boldsymbol L^{-1} \\boldsymbol V^{-1/2}\n\\]\nNote \\(\\boldsymbol \\Phi^{\\text{Chol}.1}\\) \n\\(\\boldsymbol \\Psi^{\\text{Chol}.1}\\) \nlower triangular matrices positive diagonal elements.\nHence two corresponding components \\(z_i\\) \\(x_i\\) always positively correlated!Finally, corresponding orthogonal matrices \n\\[\n\\boldsymbol Q_1^{\\text{Chol}.1} = \\boldsymbol L^T \\boldsymbol \\Sigma^{1/2} = \\boldsymbol L^{-1} \\boldsymbol \\Sigma^{-1/2}\n\\]\n\n\\[\n\\boldsymbol Q_2^{\\text{Chol}.1} = \\boldsymbol L^T \\boldsymbol V^{1/2} \\boldsymbol P^{1/2} = \\boldsymbol L^{-1} \\boldsymbol V^{-1/2} \\boldsymbol P^{-1/2}\n\\]Solution 2: Apply Cholesky decomposition \\(\\boldsymbol \\Sigma= \\boldsymbol F\\boldsymbol F^T\\)resulting whitening matrix \n\\[\n\\boldsymbol W^{\\text{Chol}.2}=\\boldsymbol F^{-1}\n\\]\nconstruction, \\(\\boldsymbol W^{\\text{Chol}.2}\\) lower triangular matrix positive\ndiagonal. whitening constraint satisfied \n\\((\\boldsymbol W^{\\text{Chol}.2})^T\\boldsymbol W^{\\text{Chol}.2} = (\\boldsymbol F^{-1})^T \\boldsymbol F^{-1} = (\\boldsymbol F^T)^{-1} \\boldsymbol F^{-1} = (\\boldsymbol F\\boldsymbol F^T)^{-1} = \\boldsymbol \\Sigma^{-1}\\).cross-covariance matrix (recall general \\(\\boldsymbol W^{-1} = \\boldsymbol \\Phi^T\\))\n\\[\n\\boldsymbol \\Phi^{\\text{Chol}.2} = \\boldsymbol F^T\n\\]\ncross-correlation matrix \n\\[\n\\boldsymbol \\Psi^{\\text{Chol}.2} =  \\boldsymbol F^T  \\boldsymbol V^{-1/2}\n\\]\n\\(\\boldsymbol \\Phi^{\\text{Chol}.2}\\) \n\\(\\boldsymbol \\Psi^{\\text{Chol}.2}\\) \nupper triangular matrices positive diagonal elements.\nHence two corresponding components \\(z_i\\) \\(x_i\\) always positively correlated!Finally, corresponding orthogonal matrices \n\\[\n\\boldsymbol Q_1^{\\text{Chol}.2}  = \\boldsymbol F^T \\boldsymbol \\Sigma^{-1/2} = \\boldsymbol F^{-1} \\boldsymbol \\Sigma^{1/2}\n\\]\n\n\\[\n\\boldsymbol Q_2^{\\text{Chol}.2} = \\boldsymbol F^T \\boldsymbol V^{-1/2} \\boldsymbol P^{-1/2} = \\boldsymbol F^{-1} \\boldsymbol V^{1/2} \\boldsymbol P^{1/2}\n\\]Note two Cholesky whitening procedures different general\n\\(\\boldsymbol W^{\\text{Chol}.1} \\neq \\boldsymbol W^{\\text{Chol}.2}\\).Application correlation instead covariance:may also wish apply Cholesky decomposition (inverse) correlation rather (inverse) covariance matrix. Intriguingly, lead different whitening transforms (unlike ZCA PCA).example, Cholesky factor \\(\\boldsymbol P^{-1} = \\boldsymbol V^{1/2} \\boldsymbol \\Sigma^{-1} \\boldsymbol V^{1/2}\\) \\(\\boldsymbol V^{1/2} \\boldsymbol L\\).\ncorresponding whitening matrix \\((\\boldsymbol V^{1/2} \\boldsymbol L)^T \\boldsymbol V^{-1/2} =\\boldsymbol L^T = \\boldsymbol W^{\\text{Chol}.1}\\).Similarly, Cholesky factor \\(\\boldsymbol P= \\boldsymbol V^{-1/2} \\boldsymbol \\Sigma\\boldsymbol V^{-1/2}\\) \\(\\boldsymbol V^{-1/2} \\boldsymbol F\\).\ncorresponding whitening matrix \\((\\boldsymbol V^{-1/2} \\boldsymbol F)^{-1} \\boldsymbol V^{-1/2} =\\boldsymbol F^{-1} = \\boldsymbol W^{\\text{Chol}.2}\\).","code":""},{"path":"transformations-and-dimension-reduction.html","id":"comparison-of-zca-pca-and-cholesky-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.6 Comparison of ZCA, PCA and Cholesky whitening","text":"comparison, results ZCA, PCA Cholesky whitening (precision matrix) applied simulated bivariate normal data set correlation \\(\\rho=0.8\\).column 1 can see simulated data scatter plot.Column 2 shows scatter plots whitened data — expect three methods removed correlation produce isotropic covariance.three approached differ differ cross-correlations. Columns 3 4 show cross-correlations first two corresponding components (\\(x_1\\) \\(z_1\\), \\(x_2\\) \\(z_2\\)) ZCA, PCA Cholesky whitening. expected, ZCA pairs show strong correlation, case PCA Cholesky whitening.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"recap","chapter":"2 Transformations and dimension reduction","heading":"2.4.7 Recap","text":"Related models discussed course:Factor models: essentially whitening plus additional error term, factors rotational\nfreedom just like whiteningFactor models: essentially whitening plus additional error term, factors rotational\nfreedom just like whiteningPLS: similar PCA regression setting (choice \nlatent variables depending response)PLS: similar PCA regression setting (choice \nlatent variables depending response)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"principal-component-analysis-pca","chapter":"2 Transformations and dimension reduction","heading":"2.5 Principal Component Analysis (PCA)","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.5.1 PCA transformation","text":"Principal component analysis proposed 1933 Harald Hotelling6 closely related PCA whitening. underlying mathematics developed earlier 1901 Karl Pearson7 problem orthogonal regression.Assume random vector \\(\\boldsymbol x\\) \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\).\nPCA particular orthogonal transformation original \\(\\boldsymbol x\\)\nresulting components orthogonal:\n\\[\n\\underbrace{\\boldsymbol t^{\\text{PCA}}}_{\\text{Principal components}} = \\underbrace{\\boldsymbol U^T}_{\\text{Orthogonal matrix}}   \\boldsymbol x\n\\]\n\\[\\text{Var}(\\boldsymbol t^{\\text{PCA}}) = \\boldsymbol \\Lambda= \\begin{pmatrix} \\lambda_1 & \\dots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & \\lambda_d\\end{pmatrix}\\]\nNote principal components orthogonal unit variance variance principal components \\(t_i\\) equals eigenvalues \\(\\lambda_i\\).Thus PCA whitening procedure. However, arrive PCA whitening simply standardising PCA components: \\(\\boldsymbol z^{\\text{PCA}} = \\boldsymbol \\Lambda^{-1/2} \\boldsymbol t^{\\text{PCA}}\\)Compression properties:total variation \\(\\text{Tr}(\\text{Var}(\\boldsymbol t^{\\text{PCA}})) = \\text{Tr}( \\boldsymbol \\Lambda) = \\sum^d_{j=1}\\lambda_j\\).\nprinciple components fraction \\(\\frac{\\lambda_i}{\\sum^d_{j=1}\\lambda_j}\\) can interpreted proportion variation contributed \ncomponent \\(\\boldsymbol t^{\\text{PCA}}\\) total variation. Thus, low ranking components \\(\\boldsymbol t^{\\text{PCA}}\\) low variation may discarded, thus leading reduction dimension.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"application-to-data","chapter":"2 Transformations and dimension reduction","heading":"2.5.2 Application to data","text":"Written terms data matrix \\(\\boldsymbol X\\) instead random vector \\(\\boldsymbol x\\) PCA becomes:\n\\[\\underbrace{\\boldsymbol T}_{\\text{Sample version principal components}}=\\underbrace{\\boldsymbol X}_{\\text{Data matrix}}\\boldsymbol U\\]\nnow two ways obtain \\(\\boldsymbol U\\):Estimate covariance matrix, e.g. \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c\\) \\(\\boldsymbol X_c\\) column-centred data matrix; apply eigenvalue decomposition \\(\\hat{\\boldsymbol \\Sigma}\\) get \\(\\boldsymbol U\\).Estimate covariance matrix, e.g. \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c\\) \\(\\boldsymbol X_c\\) column-centred data matrix; apply eigenvalue decomposition \\(\\hat{\\boldsymbol \\Sigma}\\) get \\(\\boldsymbol U\\).Compute singular value decomposition \\(\\boldsymbol X_c = \\boldsymbol V\\boldsymbol D\\boldsymbol U^T\\). \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c = \\boldsymbol U(\\frac{1}{n}\\boldsymbol D^2)\\boldsymbol U^T\\) can just use \\(\\boldsymbol U\\) SVD \\(\\boldsymbol X_c\\) need compute covariance.Compute singular value decomposition \\(\\boldsymbol X_c = \\boldsymbol V\\boldsymbol D\\boldsymbol U^T\\). \\(\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n}\\boldsymbol X_c^T\\boldsymbol X_c = \\boldsymbol U(\\frac{1}{n}\\boldsymbol D^2)\\boldsymbol U^T\\) can just use \\(\\boldsymbol U\\) SVD \\(\\boldsymbol X_c\\) need compute covariance.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"iris-data-example","chapter":"2 Transformations and dimension reduction","heading":"2.5.3 Iris data example","text":"example consider famous iris flower data set. consists data botanical variables (sepal length, sepal width,\npetal length petal width) measured 150 flowers \nthree iris species (setosa, versicolr, virginica). Thus data set \\(d=4\\) \\(n=150\\).first standardise data, compute PCA components plot proportion total variation contributed component.\nshows two PCA components needed achieve 95% total variation:scatter plot plot first two principal components also informative:shows groupings among \n150 flowers, corresponding species, groups can characterised\nprincipal components.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"correlation-loadings-plot-to-interpret-pca-components","chapter":"2 Transformations and dimension reduction","heading":"2.6 Correlation loadings plot to interpret PCA components","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-correlation-loadings","chapter":"2 Transformations and dimension reduction","heading":"2.6.1 PCA correlation loadings","text":"earlier section learned general whitening transformation cross-correlations \\(\\boldsymbol \\Psi=\\text{Cor}(\\boldsymbol z, \\boldsymbol x)\\) play role correlation loadings inverse transformation:\n\\[\n\\boldsymbol V^{-1/2} \\boldsymbol x= \\boldsymbol \\Psi^T  \\boldsymbol z\\, , \n\\]\n.e. coefficients linking whitened variable \\(\\boldsymbol z\\) standardised original variable \\(\\boldsymbol x\\).\nrelationship holds therefore also PCA-whitening\n\\(\\boldsymbol z^{\\text{PCA}}= \\boldsymbol \\Lambda^{-1/2} \\boldsymbol U^T \\boldsymbol x\\) \\(\\boldsymbol \\Psi^{\\text{PCA}} = \\boldsymbol \\Lambda^{1/2} \\boldsymbol U^T \\boldsymbol V^{-1/2}\\).classical PCA whitening approach \\(\\text{Var}(\\boldsymbol t^{\\text{PCA}}) \\neq \\boldsymbol \\). However, can still compute cross-correlations principal components \\(\\boldsymbol t^{\\text{PCA}}\\) \\(\\boldsymbol x\\), resulting \n\\[\n\\text{Cor}(\\boldsymbol t^{\\text{PCA}}, \\boldsymbol x) = \\boldsymbol \\Lambda^{1/2} \\boldsymbol U^T \\boldsymbol V^{-1/2} = \\boldsymbol \\Psi^{\\text{PCA}}\n\\]\nNote cross-correlations PCA-whitening since\n\\(\\boldsymbol t^{\\text{PCA}}\\) \\(\\boldsymbol z^{\\text{PCA}}\\) differ scale.inverse PCA transformation \n\\[\n\\boldsymbol x= \\boldsymbol U\\boldsymbol t^{\\text{PCA}}\n\\]\nterms standardised PCA components standardised original components becomes\n\\[\n\\boldsymbol V^{-1/2} \\boldsymbol x= \\boldsymbol \\Psi^T  \\boldsymbol \\Lambda^{-1/2} \\boldsymbol t^{\\text{PCA}}\n\\]\nThus cross-correlation matrix \\(\\boldsymbol \\Psi\\) plays role correlation loadings\nalso classical PCA, .e. \ncoefficients linking standardised PCA components standardised original components.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-correlation-loadings-plot","chapter":"2 Transformations and dimension reduction","heading":"2.6.2 PCA correlation loadings plot","text":"PCA PCA-cor whitening well classical PCA aim compression, .e.\nfind latent variables total variation contributed \nsmall number components.order able better interpret top ranking PCA component can use visual device called correlation loadings plot. compute correlation PCA components 1 2 (\\(t_1^{\\text{PCA}}\\) \\(t_2^{\\text{PCA}})\\) original variables \\(x_1, \\ldots, x_d\\).original variable \\(x_j\\) therefore two numbers -1 1, correlation\n\\(\\text{Cor}(t_1^{\\text{PCA}}, x_j)\\) \\(\\text{Cor}(t_2^{\\text{PCA}}, x_j)\\) use coordinates draw point plane. construction, points\nlie within unit circle around origin. sum squared correlation loadings latent component one specific original variable sum one, sum squared loadings just first two components also 1.\noriginal variables strongly influenced\ntwo latent variables strong correlation thus lie near outer circle, whereas variables influenced two latent variables lie near origin.example, correlation loadings plot showing cross-correlation first two\nPCA components four variables iris flower data set discussed earlier.interpretation plot discussed Worksheet 4.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"cca-whitening-canonical-correlation-analysis","chapter":"2 Transformations and dimension reduction","heading":"2.7 CCA whitening (Canonical Correlation Analysis)","text":"Canonical correlation analysis invented Harald Hotelling 1936.8So far, looked whitening single vector \\(\\boldsymbol x\\). CCA whitening consider two vectors \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) simultaneously:\\[\\begin{align*}\n\\begin{array}{ll}\n\\boldsymbol x= \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_p \\end{pmatrix} \\\\\n\\text{Dimension } p\n\\end{array}\n\\begin{array}{ll}\n\\boldsymbol y= \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_q \\end{pmatrix} \\\\\n\\text{Dimension } q\n\\end{array}\n\\begin{array}{ll}\n\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma_{\\boldsymbol x} = \\boldsymbol V_{\\boldsymbol x}^{1/2}\\boldsymbol P_{\\boldsymbol x}\\boldsymbol V_{\\boldsymbol x}^{1/2} \\\\\n\\text{Var}(\\boldsymbol y) = \\boldsymbol \\Sigma_{\\boldsymbol y} = \\boldsymbol V_{\\boldsymbol y}^{1/2}\\boldsymbol P_{\\boldsymbol y}\\boldsymbol V_{\\boldsymbol y}^{1/2} \\\\\n\\end{array}\n\\end{align*}\\]\\[\\begin{align*}\n\\begin{array}{cc}\n\\text{Whitening } \\boldsymbol x\\text{:} \\\\\n\\text{Whitening } \\boldsymbol y\\text{:}\n\\end{array}\n\\begin{array}{cc}\n\\boldsymbol z_{\\boldsymbol x} = \\boldsymbol W_{\\boldsymbol x}\\boldsymbol x=\\boldsymbol Q_{\\boldsymbol x}\\boldsymbol P_{\\boldsymbol x}^{-1/2}\\boldsymbol V_{\\boldsymbol x}^{-1/2}\\boldsymbol x\\\\\n\\boldsymbol z_{\\boldsymbol y} = \\boldsymbol W_{\\boldsymbol y}\\boldsymbol y=\\boldsymbol Q_{\\boldsymbol y}\\boldsymbol P_{\\boldsymbol y}^{-1/2}\\boldsymbol V_{\\boldsymbol y}^{-1/2}\\boldsymbol y\n\\end{array}\n\\end{align*}\\]\n(note use correlation-based form \\(\\boldsymbol W\\))Cross-correlation \\(\\boldsymbol z_{\\boldsymbol y}\\) \\(\\boldsymbol z_{\\boldsymbol y}\\):\\[\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})=\\boldsymbol Q_{\\boldsymbol x}\\boldsymbol K\\boldsymbol Q_{\\boldsymbol y}^T\\]\\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2}\\boldsymbol P_{\\boldsymbol x\\boldsymbol y}\\boldsymbol P_{\\boldsymbol y}^{-1/2}\\).Idea: can choose suitable orthogonal matrices \\(\\boldsymbol Q_{\\boldsymbol x}\\) \\(\\boldsymbol Q_{\\boldsymbol y}\\) putting constraints cross-correlation.CCA: aim diagonal \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})\\) component \\(\\boldsymbol z_{\\boldsymbol x}\\) influences one (corresponding) component \\(\\boldsymbol z_{\\boldsymbol y}\\).Motivation: pairs “modules” represented components \\(\\boldsymbol z_{\\boldsymbol x}\\)\n\\(\\boldsymbol z_{\\boldsymbol y}\\) influencing (anyone module).\\[\n\\begin{array}{ll}\n\\boldsymbol z_{\\boldsymbol x} = \\begin{pmatrix} z^x_1 \\\\ z^x_2 \\\\ \\vdots \\\\ z^x_p \\end{pmatrix} &\n\\boldsymbol z_{\\boldsymbol y} = \\begin{pmatrix} z^y_1 \\\\ z^y_2 \\\\ \\vdots \\\\ z^y_q \\end{pmatrix} \\\\\n\\end{array}\n\\]\\[\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y}) = \\begin{pmatrix} d_1 & \\dots & 0 \\\\ \\vdots &  \\vdots \\\\ 0 & \\dots & d_m \\end{pmatrix}\\]\\(d_i\\) canonical correlations \\(m=\\min(p,q)\\).","code":""},{"path":"transformations-and-dimension-reduction.html","id":"how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal","chapter":"2 Transformations and dimension reduction","heading":"2.7.1 How to make cross-correlation matrix \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y})\\) diagonal?","text":"Use Singular Value Decomposition (SVD) matrix \\(\\boldsymbol K\\):\\[\\boldsymbol K= (\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}})^T  \\boldsymbol D\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\]\n\\(\\boldsymbol D\\) diagonal matrix containing singular values \\(\\boldsymbol K\\)yields orthogonal matrices \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) thus desired whitened matrices \\(\\boldsymbol W_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol W_{\\boldsymbol y}^{\\text{CCA}}\\)result \\(\\text{Cor}(\\boldsymbol z_{\\boldsymbol x},\\boldsymbol z_{\\boldsymbol y}) = \\boldsymbol D\\) .e. singular values \\(\\boldsymbol K\\) identical canonical correlations \\(d_i\\)!\\(\\longrightarrow\\) \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) determined diagonality constraint (different previously discussed whitening methods).Note signs corresponding columns \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\) identified. Traditionally, SVD \nsigns chosen singular values positive. However, \nimpose positive-diagonality \\(\\boldsymbol Q_{\\boldsymbol x}^{\\text{CCA}}\\) \\(\\boldsymbol Q_{\\boldsymbol y}^{\\text{CCA}}\\),\nthus positive-diagonality cross-correlations \\(\\boldsymbol \\Psi_{\\boldsymbol x}\\) \n\\(\\boldsymbol \\Psi_{\\boldsymbol y}\\), canonical correlations may take positive \nnegative values.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"related-methods","chapter":"2 Transformations and dimension reduction","heading":"2.7.2 Related methods","text":"O2PLS: similar CCA using orthogonal projections\n(thus O2PLS latent variables underlying \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) orthogonal)O2PLS: similar CCA using orthogonal projections\n(thus O2PLS latent variables underlying \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) orthogonal)Vector correlation: aggregates squared canonical correlations single overall measure\nassociation two random vectors \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) (see Chapter 5\nmultivariate dependencies).Vector correlation: aggregates squared canonical correlations single overall measure\nassociation two random vectors \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) (see Chapter 5\nmultivariate dependencies).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"unsupervised-learning-and-clustering","chapter":"3 Unsupervised learning and clustering","heading":"3 Unsupervised learning and clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"challenges-in-unsupervised-learning","chapter":"3 Unsupervised learning and clustering","heading":"3.1 Challenges in unsupervised learning","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"objective","chapter":"3 Unsupervised learning and clustering","heading":"3.1.1 Objective","text":"observe data \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) \\(n\\) objects (subjects).\nsample \\(\\boldsymbol x_i\\) vector dimension \\(d\\). Thus, \\(n\\) objects / subjects measurements \\(d\\) variables.\naim unsupervised learning identify patters relating objects/subjects based information available \\(\\boldsymbol x_i\\). Note unsupervised learning use information\n\\(\\boldsymbol x_i\\) nothing else.illustration consider first two principal components Iris flower data (see e.g. Worksheet 4):Clearly group structure among samples linked particular\npatterns first two principal components.Note plot used additional information, class labels (setosa, versicolor, virginica), highlighting true underlying structure (three flower species).unsupervised learning class labels (assumed ) unknown, aim infer clustering thus classes labels.9There many methods clustering unsupervise learning, purely algorithmic well probabilistic. chapter study commonly used approaches.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"questions-and-problems","chapter":"3 Unsupervised learning and clustering","heading":"3.1.2 Questions and problems","text":"order implement unsupervised learning need address number questions:define clusters?learn / infer clusters?many clusters ? (surprisingly difficult!)can assess uncertainty clusters?know clusters also interested :features define / separate cluster?(note feature / variable selection problem, discussed supervised learning).Many problems questions highly specific data hand.\nCorrespondingly, many different types methods models clustering unsupervised learning.terms representing data, unsupervised learning tries balance following two extremes:objects grouped single cluster (low complexity model)objects put cluster (high complexity model)practise, aim find compromise, .e. model captures \nstructure data appropriate complexity — low complex.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"why-is-clustering-difficult","chapter":"3 Unsupervised learning and clustering","heading":"3.1.3 Why is clustering difficult?","text":"Partioning problem (combinatorics): many partitions \\(n\\) objects (say flowers) \\(K\\) groups (say species) exists?Answer:\\[\nS(n,K) = \\left\\{\\begin{array}{l} n \\\\ K \\end{array} \\right\\}\n\\]\n“Sterling number second type”.large n:\n\\[\nS(n,K) \\approx \\frac{K^n }{ K!}\n\\]\nExample:enormously big numbers even relatively small problems!\\(\\Longrightarrow\\) Clustering / partitioning / structure discovery easy!\\(\\Longrightarrow\\) expect perfect answers single “true” clusteringIn fact, model data many differnt clusterings may fit data equally well.\\(\\Longrightarrow\\) need assesse uncertainty clusteringThis can done part probabilistic modelling resampling (e.g., bootstrap).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"common-types-of-clustering-methods","chapter":"3 Unsupervised learning and clustering","heading":"3.1.4 Common types of clustering methods","text":"many different clustering algorithms!consider following two broad types methods:Algorithmic clustering methods (explicitly based probabilistic model)\\(K\\)-meansPAMhierarchical clustering (distance similarity-based, divise agglomerative)pros: fast, effective algorithms find least grouping\ncons: probabilistic interpretation, blackbox methodsModel-based clustering (based probabilistic model)mixture models (e.g. Gaussian mixture models, GMMs, non-hierarchical)graphical models (e.g. Bayesian networks, Gaussian graphical models GGM, trees networks)pros: full probabilistic model corresponding advantages\ncons: computationally expensive, sometimes impossible compute exactly.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"hierarchical-clustering","chapter":"3 Unsupervised learning and clustering","heading":"3.2 Hierarchical clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"tree-like-structures","chapter":"3 Unsupervised learning and clustering","heading":"3.2.1 Tree-like structures","text":"Often, categorisations objects nested, .e. sub-categories categories etc. can naturally represented tree-like hierarchical structures.many branches science hierarchical clusterings widely employed, example evolutionary biology: see e.g. Tree Life explaining biodiversity lifephylogenetic trees among species (e.g. vertebrata)population genetic trees describe human evolutiontaxonomic trees plant speciesetc.Note visualising hierarchical structures typically corresponding tree depicted facing downwards, .e. root tree shown top, tips/leaves tree shown bottom!order obtain hierarchical clustering data two opposing strategies commonly used:divisive recursive partitioning algorithms\ngrow tree root downwards\nfirst determine main two clusters, recursively refine clusters .\ngrow tree root downwardsfirst determine main two clusters, recursively refine clusters .agglomerative algorithms\ngrow tree leafs upwards\nsuccessively form partitions first joining individual object together,\nrecursively join groups items together, merged.\ngrow tree leafs upwardssuccessively form partitions first joining individual object together,\nrecursively join groups items together, merged.following discuss number popular hierarchical agglomerative clustering algorithms based pairwise distances / similarities (\\(n \\times n\\) matrix) among data points.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"agglomerative-hierarchical-clustering-algorithms","chapter":"3 Unsupervised learning and clustering","heading":"3.2.2 Agglomerative hierarchical clustering algorithms","text":"general algorithm agglomerative construction hierarchical clustering works follows:Initialisation:Compute dissimilarity / distance matrix pairs objects “objects” single data points stage later also sets data points.Iterative procedure:identify pair objects smallest distance. two objects merged together one set. Create internal node tree represent set.identify pair objects smallest distance. two objects merged together one set. Create internal node tree represent set.update distance matrix computing distances new set \nobjects. new set contains data points procedure terminates. final node created root node.update distance matrix computing distances new set \nobjects. new set contains data points procedure terminates. final node created root node.actual implementation algorithm two key ingredients needed:distance measure \\(d(\\boldsymbol , \\boldsymbol b)\\) two individual elementary data points \\(\\boldsymbol \\) \\(\\boldsymbol b\\).typically one following:Euclidean distance \\(d(\\boldsymbol , \\boldsymbol b) = \\sqrt{\\sum_{=1}^d ( a_i-b_i )^2} = \\sqrt{(\\boldsymbol -\\boldsymbol b)^T (\\boldsymbol -\\boldsymbol b)}\\)Manhattan distance \\(d(\\boldsymbol , \\boldsymbol b) = \\sum_{=1}^d | a_i-b_i |\\)Maximum norm \\(d(\\boldsymbol , \\boldsymbol b) = \\underset{\\\\{1, \\ldots, d\\}}{\\max} | a_i-b_i |\\)end, making correct choice distance require subject knowledge data!distance measure \\(d(, B)\\) two sets objects \\(=\\{\\boldsymbol a_1, \\boldsymbol a_2, \\ldots, \\boldsymbol a_{n_A} \\}\\) \\(B=\\{\\boldsymbol b_1, \\boldsymbol b_2, \\ldots, \\boldsymbol b_{n_B}\\}\\) size \\(n_A\\) \\(n_B\\), respectively.determine distance \\(d(, B)\\) two sets following measures often employed:complete linkage (max. distance): \\(d(, B) = \\underset{\\boldsymbol a_i \\, \\boldsymbol b_i \\B}{\\max} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)single linkage (min. distance): \\(d(, B) = \\underset{\\boldsymbol a_i \\, \\boldsymbol b_i \\B}{\\min} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)average linkage (avg. distance): \\(d(, B) = \\frac{1}{n_A n_B} \\sum_{\\boldsymbol a_i \\} \\sum_{\\boldsymbol b_i \\B} d(\\boldsymbol a_i, \\boldsymbol b_i)\\)","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"wards-clustering-method","chapter":"3 Unsupervised learning and clustering","heading":"3.2.3 Ward’s clustering method","text":"Another agglomerative hierarchical procedure Ward’s minimum variance approach. approach iteration two sets \\(\\) \\(B\\) merged lead smallest increase within-group variation, equivalenty, 5h3 total within-group sum squares (cf. \\(K\\)-means). centroids two sets given \\(\\boldsymbol \\mu_A = \\frac{1}{n_A} \\sum_{\\boldsymbol a_i \\} \\boldsymbol a_i\\) \\(\\boldsymbol \\mu_B = \\frac{1}{n_B} \\sum_{\\boldsymbol b_i \\B} \\boldsymbol b_i\\).within-group sum squares group \\(\\) \n\\[\nw_A = \\sum_{\\boldsymbol a_i \\} (\\boldsymbol a_i -\\boldsymbol \\mu_A)^T (\\boldsymbol a_i -\\boldsymbol \\mu_A)\n\\]\ncomputed basis difference observations \\(\\boldsymbol a_i\\) relative mean \\(\\boldsymbol \\mu_A\\).\nHowever, also possible compute pairwise differences\nobservations using\n\\[\nw_A = \\frac{1}{n_A} \\sum_{\\boldsymbol a_i, \\boldsymbol a_j \\, < j} (\\boldsymbol a_i -\\boldsymbol a_j)^T (\\boldsymbol a_i -\\boldsymbol a_j)\n\\]\ntrick used Ward’s clustering method constructing distance measure sets \\(\\) \\(B\\) \n\\[\nd(, B) = w_{\\cup B} - w_A -w_B \\, .\n\\]\nCorrespondingly, distance two elementary data points \\(\\boldsymbol \\) \\(\\boldsymbol b\\) squared Euclidean distance\n\\[\nd(\\boldsymbol , \\boldsymbol b) = (\\boldsymbol - \\boldsymbol b)^T (\\boldsymbol - \\boldsymbol b) \\, .\n\\]","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-to-swiss-banknote-data-set","chapter":"3 Unsupervised learning and clustering","heading":"3.2.4 Application to Swiss banknote data set","text":"data set reports 6 pysical measurements 200 Swiss bank notes. 200 notes\n100 genuine 100 counterfeit. measurements : length, left width, right width, bottom margin, top margin, diagonal length bank notes.Plotting first PCAs data shows indeed two well defined groups,\ngroups correspond precisely genuine counterfeit banknotes:now compare hierarchical clusterings Swiss bank note data using four different methods using Euclidean distance.interactive R Shiny web app analysis (also allows explore distance measures) available\nonline https://minerva..manchester.ac.uk/shiny/strimmer/hclust/ .Ward.D2 (=Ward’s method):Average linkage:Complete linkage:Single linkage:Result:four trees / hierarchical clusterings quite different!Ward.D2 method one finds correct grouping (except single error).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"assessment-of-the-uncertainty-of-hierarchical-clusterings","chapter":"3 Unsupervised learning and clustering","heading":"3.2.5 Assessment of the uncertainty of hierarchical clusterings","text":"practical application hierarchical clustering methods essential evaluate stability uncertainty obtained groupings. often done follows using “bootstrap”:Sampling replacement used generate number -called bootstrap data sets (say \\(B=200\\)) similar original one. Specifically, create new data matrices repeately randomly selecting columns (variables) original data matrix inclusion bootstrap data matrix. Note sample columns aim cluster samples.Subsequently, hierarchical clustering computed bootstrap data sets. result, now “ensemble” \\(B\\) bootstrap trees.Finally, analysis clusters (bipartions) shown bootstrap trees allows count clusters appear frequently, also appear less frequently. counts provide measure stability clusterings appearing original tree.Additionally, bootstrap tree can also compute consensus tree containing stable clusters. viewed “ensemble average” bootstrap trees.disadvantage procedure bootstrapping trees computationally expensive, original procedure already time consuming now needs repeated large number times.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"k-means-clustering","chapter":"3 Unsupervised learning and clustering","heading":"3.3 \\(K\\)-means clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"general-aims","chapter":"3 Unsupervised learning and clustering","heading":"3.3.1 General aims","text":"Partition data \\(K\\) groups, \\(K\\) given advanceThe groups non-overlapping, \\(n\\) data points / objects \\(\\boldsymbol x_i\\) assigned exactly one \\(K\\) groupsmaximise homogeneity group (.e. group contain similar objects)maximise heterogeneity among different groups (.e group differ groups)","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"algorithm","chapter":"3 Unsupervised learning and clustering","heading":"3.3.2 Algorithm","text":"group \\(k \\\\{1, \\ldots, K\\}\\) assume group mean \\(\\boldsymbol \\mu_k\\).\nrunning \\(K\\)-means get estimates \\(\\hat{\\boldsymbol \\mu}_k\\) group means,\nwell allocation data point one classes.Initialisation:start algorithm \\(n\\) observations \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) randomly allocated equal probability one \\(K\\) groups. resulting assignment given function \\(C(\\boldsymbol x_i) \\\\{1, \\ldots, K\\}\\). \\(G_k = \\{ | C(\\boldsymbol x_i) = k\\}\\) denote set indices data points cluster \\(k\\), \\(n_k = | G_k |\\) \nnumber samples cluster \\(k\\).Iterative refinement:estimate group means \n\\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n_k} \\sum_{\\G_k} \\boldsymbol x_i\n\\]update group assignment: data point \\(\\boldsymbol x_i\\) (re)assigned group \\(k\\) nearest \\(\\hat{\\boldsymbol \\mu}_k\\) (terms Euclidean norm).\nSpecifically, assignment \\(C(\\boldsymbol x_i)\\) updated \n\\[\n\\begin{split}\nC(\\boldsymbol x_i) & = \\underset{k}{\\arg \\min} \\, | \\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k |_2 \\\\\n      & = \\underset{k}{\\arg \\min} \\, (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k) \\\\\n\\end{split}\n\\]\nSteps 1 2 repeated algorithm converges (upper limit repeats reached).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"properties","chapter":"3 Unsupervised learning and clustering","heading":"3.3.3 Properties","text":"Despite simplicity \\(K\\)-means surprisingly effective clustering algorithms.final clustering depends initialisation often useful run \\(K\\)-means several\ntimes different starting allocations data points. Furthermore, non-random non-uniform\ninitialisations can lead improved faster convergence, see e.g. \nK-means++ algorithm.result way clusters assigned \\(K\\)-means corresponding cluster boundaries form \nVoronoi tesselation (cf. https://en.wikipedia.org/wiki/Voronoi_diagram ) around cluster means.Later also discuss connection \\(K\\)-means probabilistic clustering using Gaussian mixture models.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"choosing-the-number-of-clusters","chapter":"3 Unsupervised learning and clustering","heading":"3.3.4 Choosing the number of clusters","text":"\\(K\\)-means clustering obtained insightful compute:total within-group sum squares \\(SSW\\) (tot.withinss), total unexplained sum squares:\n\\[\nSSW = \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)\n\\]\nquantity decreases \\(K\\) zero \\(K=n\\).\n\\(K\\)-means algorithm tries minimise quantity typically find local minimum rather global one.total within-group sum squares \\(SSW\\) (tot.withinss), total unexplained sum squares:\n\\[\nSSW = \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)\n\\]\nquantity decreases \\(K\\) zero \\(K=n\\).\n\\(K\\)-means algorithm tries minimise quantity typically find local minimum rather global one.-group sum squares \\(SSB\\) (betweenss), explained sum squares:\n\\[\nSSB = \\sum_{k=1}^K n_k (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)^T (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)\n\\]\n\\(\\hat{\\boldsymbol \\mu}_0 = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = \\frac{1}{n} \\sum_{k=1}^K n_k \\hat{\\boldsymbol \\mu}_k\\)\nglobal mean samples. \\(SSB\\) increases number clusters \\(K\\) \\(K=n\\) \nbecomes equal total sum squares \\(SST\\).-group sum squares \\(SSB\\) (betweenss), explained sum squares:\n\\[\nSSB = \\sum_{k=1}^K n_k (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)^T (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)\n\\]\n\\(\\hat{\\boldsymbol \\mu}_0 = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = \\frac{1}{n} \\sum_{k=1}^K n_k \\hat{\\boldsymbol \\mu}_k\\)\nglobal mean samples. \\(SSB\\) increases number clusters \\(K\\) \\(K=n\\) \nbecomes equal total sum squares \\(SST\\).total sum squares\n\\[\nSST = \\sum_{=1}^n (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)^T (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0) \\, .\n\\]\nconstruction \\(SST = SSB + SSW\\) \\(K\\) (.e. \\(SST\\) constant independent \\(K\\)).total sum squares\n\\[\nSST = \\sum_{=1}^n (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)^T (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0) \\, .\n\\]\nconstruction \\(SST = SSB + SSW\\) \\(K\\) (.e. \\(SST\\) constant independent \\(K\\)).Dividing sum squares sample size \\(n\\) get\\(T = \\frac{SST}{n}\\) total variation,\\(B = \\frac{SSW}{n}\\) explained variation \\(W = \\frac{SSW}{n}\\) total unexplained variation ,\\(T = B + W\\).order decide optimal number clusters run \\(K\\)-means different settings \\(K\\) choose smallest \\(K\\) explained variation \\(B\\) significantly worse compared clustering substantially larger \\(K\\) (see example ).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"k-medoids-aka-pam","chapter":"3 Unsupervised learning and clustering","heading":"3.3.5 \\(K\\)-medoids aka PAM","text":"closely related clustering method \\(K\\)-medoids PAM (“Partitioning Around Medoids”).works exactly like \\(K\\)-means, thatinstead estimated group means \\(\\hat{\\boldsymbol \\mu}_k\\) one member group selected representative (socalled “medoid”)instead squared Euclidean distance dissimilarity measures also allowed.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-of-k-means-to-iris-data","chapter":"3 Unsupervised learning and clustering","heading":"3.3.6 Application of \\(K\\)-means to Iris data","text":"Scatter plots Iris data:R output \\(K\\)-means analysis true number clusters specified (\\(K=3\\)) :corresponding total within-group sum squares (\\(SSW\\), tot.withinss)\nisand -group sum squares (\\(SSB\\), betweenss) isBy comparing known class assignments can assess accuracy \\(K\\)-means clustering:choosing \\(K\\) run \\(K\\)-means several times compute\nwithin cluster variation dependence \\(K\\):Thus, \\(K=3\\) clusters seem appropriate since explained variation significantly improve\n(unexplained variation significantly decrease) increase number clusters.","code":"## K-means clustering with 3 clusters of sizes 53, 50, 47\n## \n## Cluster means:\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1  -0.05005221 -0.88042696    0.3465767   0.2805873\n## 2  -1.01119138  0.85041372   -1.3006301  -1.2507035\n## 3   1.13217737  0.08812645    0.9928284   1.0141287\n## \n## Clustering vector:\n##   [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [38] 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 1 1 1 3 1 1 1 1 1 1 1 1 3 1 1 1 1 3 1 1 1\n##  [75] 1 3 3 3 1 1 1 1 1 1 1 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 3 3 3 3 1 3 3 3 3\n## [112] 3 3 1 1 3 3 3 3 1 3 1 3 1 3 3 1 3 3 3 3 3 3 1 1 3 3 3 1 3 3 3 1 3 3 3 1 3\n## [149] 3 1\n## \n## Within cluster sum of squares by cluster:\n## [1] 44.08754 47.35062 47.45019\n##  (between_SS / total_SS =  76.7 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nkmeans.out$tot.withinss## [1] 138.8884\nkmeans.out$betweenss## [1] 457.1116\ntable(L.iris, kmeans.out$cluster)##             \n## L.iris        1  2  3\n##   setosa      0 50  0\n##   versicolor 39  0 11\n##   virginica  14  0 36"},{"path":"unsupervised-learning-and-clustering.html","id":"arbitrariness-of-cluster-labels-and-label-switching","chapter":"3 Unsupervised learning and clustering","heading":"3.3.7 Arbitrariness of cluster labels and label switching","text":"important realise unsupervised learning clustering labels group assigned arbitrary fashion.\nSpecifically, \\(K\\) cluster \\(K!\\) possibilities attach \nlabels, corresponding number permutations \\(K\\) groups.Thus, rerun clustering algorithm \\(K\\)-means may return clustering (groupings samples) different labels. phenomenon called “label switching”.Therefore comparing clusterings obtained algorithm just rely group label, need compare actual members clusters. Likewise, interested properties particular group rely label identify group.order resolve problem label switching one may wish relabel clusters using additional information, requiring samples specfic groups\n(e.g.: sample 1 always group labeled “1”), /linking labels orderings constraints group characteristics (e.g.: group label “1” always smaller mean group label “2”).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"mixture-models","chapter":"3 Unsupervised learning and clustering","heading":"3.4 Mixture models","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"finite-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.4.1 Finite mixture model","text":"\\(K\\) groups / classes / categories, number \\(K\\) specified finiteeach class \\(k \\\\{1, \\ldots, K\\}\\) modeled distribution \\(F_k\\) parameters \\(\\boldsymbol \\theta_k\\).density class: \\(f_k(\\boldsymbol x) = f(\\boldsymbol x| k)\\) \\(k \\1, \\ldots, K\\)mixing weight class: \\(\\text{Pr}(k) = \\pi_k\\) \\(\\sum_{k=1}^K \\pi_k = 1\\)joint density \\(f(\\boldsymbol x, k) = f(\\boldsymbol x| k) \\text{Pr}(k) = f_k(\\boldsymbol x) \\pi_k\\)results mixture density\n\\[\nf_{\\text{mix}}(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\n\\]\nalso called marginal density arised joint density \\(f(\\boldsymbol x, k)\\) marginalising \\(k\\).often one uses multivariate normal components \\(f_k(\\boldsymbol x) = N(\\boldsymbol x| \\boldsymbol \\mu_k, \\boldsymbol \\Sigma_k)\\) \\(\\\\ \\Longrightarrow\\) Gaussian mixture model (GMM)Mixture models fundamental just clustering many applications (e.g. classification).Note: don’t confuse mixture model mixed model (=random effects regression model)","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"total-variance-and-variation-of-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.4.2 Total variance and variation of mixture model","text":"conditional means variances class \\(\\text{E}(\\boldsymbol x| k) = \\boldsymbol \\mu_k\\) \\(\\text{Var}(\\boldsymbol x| k) = \\boldsymbol \\Sigma_k\\), probability\nclass \\(k\\) given \\(\\text{Pr}(k)=\\pi_k\\). Using law total expectation can therefore obtain mean mixture density follows:\n\\[\n\\begin{split}\n\\text{E}(\\boldsymbol x) & = \\text{E}(\\text{E}(\\boldsymbol x| k)) \\\\\n            & = \\sum_{k=1}^K \\pi_k \\boldsymbol \\mu_k \\\\\n            &= \\boldsymbol \\mu_0 \\\\\n\\end{split}\n\\]\nSimilarly, using law total variance compute marginal variance:\n\\[\n\\begin{split}\n\\underbrace{\\text{Var}(\\boldsymbol x)}_{\\text{total}} & =  \\underbrace{ \\text{Var}( \\text{E}(\\boldsymbol x| k )  )}_{\\text{explained / -group}} + \\underbrace{\\text{E}(\\text{Var}(\\boldsymbol x|k))}_{\\text{unexplained / within-group}} \\\\\n\\boldsymbol \\Sigma_0 & =  \\sum_{k=1}^K \\pi_k (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T + \\sum_{k=1}^K \\pi_k \\boldsymbol \\Sigma_k  \\\\\n\\end{split}\n\\]\nThus, just like linear regression (see MATH20802 Statistical Methods) can decompose total variance explained\n(group) part unexplained (within group) part.total variation given trace covariance matrix, decomposition turns \n\\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol \\Sigma_0) & =  \\sum_{k=1}^K \\pi_k \\text{Tr}((\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T) + \\sum_{k=1}^K \\pi_k \\text{Tr}(\\boldsymbol \\Sigma_k)  \\\\\n& =  \\sum_{k=1}^K \\pi_k (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)^T (\\boldsymbol \\mu_k - \\boldsymbol \\mu_0) + \\sum_{k=1}^K \\pi_k \\text{Tr}(\\boldsymbol \\Sigma_k)\\\\\n\\end{split}\n\\]\ncovariances replaced empirical estimates obtain\n\\(T=B+W\\) decomposition total variation familiar \\(K\\)-means:\n\\[T = \\text{Tr}\\left( \\hat{\\boldsymbol \\Sigma}_0 \\right)  = \n\\frac{1}{n} \\sum_{=1}^n (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)^T (\\boldsymbol x_i - \\hat{\\boldsymbol \\mu}_0)\\]\n\\[B = \\frac{1}{n} \\sum_{k=1}^K n_k (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)^T (\\hat{\\boldsymbol \\mu}_k - \\hat{\\boldsymbol \\mu}_0)\\]\n\\[W = \\frac{1}{n}  \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_k)\n\\]univariate mixture (\\(d=1\\)) \\(K=2\\) components get\n\\[\n\\mu_0 = \\pi_1 \\mu_1+ \\pi_2 \\mu_2 \\, ,\n\\]\n\\[\n\\sigma^2_{\\text{within}} = \\pi_1 \\sigma^2_1 + \\pi_2 \\sigma^2_2 = \\sigma^2_{\\text{pooled}}\\,,\n\\]\nalso know pooled variance, \n\\[\n\\begin{split}\n\\sigma^2_{\\text{}} &= \\pi_1 (\\mu_1 - \\mu_0)^2 + \\pi_2 (\\mu_2 - \\mu_0)^2 \\\\\n& =\\pi_1 \\pi_2^2 (\\mu_1 - \\mu_2)^2 + \\pi_2 \\pi_1^2 (\\mu_1 - \\mu_2)^2\\\\\n& = \\pi_1 \\pi_2 (\\mu_1 - \\mu_2)^2  \\\\\n& = \\left( \\frac{1}{\\pi_1} + \\frac{1}{\\pi_2}   \\right)^{-1} (\\mu_1 - \\mu_2)^2 \\\\\n\\end{split} \\,.\n\\]\nratio -group variance within-group variance proportional\n(factor \\(n\\)) squared pooled-variance \\(t\\)-score:\n\\[\n\\frac{\\sigma^2_{\\text{}}}{\\sigma^2_{\\text{within}}} =\n  \\frac{ (\\mu_1 - \\mu_2)^2}{ \\left(\\frac{1}{\\pi_1} + \\frac{1}{\\pi_2}   \\right)  \\sigma^2_{\\text{pooled}} }= \\frac{t_{\\text{pooled}}^2}{n}\n\\]\nfamiliar ANOVA (e.g. linear models course) recognise ratio \\(F\\)-score.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"example-of-mixtures","chapter":"3 Unsupervised learning and clustering","heading":"3.4.3 Example of mixtures","text":"Mixtures can take many different shapes forms, instructive study examples.first plot show density mixture distribution consisting two normals \\(\\pi_1=0.7\\),\n\\(\\mu_1=-1\\), \\(\\mu_2=2\\) two variances equal 1 (\\(\\sigma^2_1 = 1\\) \\(\\sigma^2_2 = 1\\)).\ntwo components well-separated two clear modes. plot also shows density normal distribution total mean (\\(\\mu_0=-0.1\\)) variance (\\(\\sigma_0^2=2.89\\)) mixture distribution. Clearly total normal mixture density different.However, mixtures can also look different. example, mean second component adjusted \\(\\mu_2=0\\) single mode total normal density \\(\\mu_0=-0.7\\) \\(\\sigma_0^2=1.21\\) now almost inistinguishable form mixture density.\nThus, case hard (even impossible) identify two peaks data.interactive version two normal component mixture available online \nR Shiny web app https://minerva..manchester.ac.uk/shiny/strimmer/mixture/ .general, learning mixture models data can challenging due various issues.\nFirst, permutation symmetries due arbitrariness group labels group specific parameters identiable without additional restrictions.\nSecond, identifiability issues can arise \n— example — two neighboring components mixture model largely overlapping\nthus close discriminated two different modes.\nFurthermore, likelihood estimation challenging singularities likelihood function,\nexample due singular estimated covariance matrices. However, can easily fixed \nregularising /requiring sufficient sample size per group.Mixture models need univariate, fact mixtures consider course multivariate.\nillustration, plot mixture two bivariate normals,\n\\(\\pi_1=0.7\\), \\(\\boldsymbol \\mu_1 = \\begin{pmatrix}-1 \\\\1 \\\\ \\end{pmatrix}\\),\n\\(\\boldsymbol \\Sigma_1 = \\begin{pmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\\\ \\end{pmatrix}\\),\n\\(\\boldsymbol \\mu_2 = \\begin{pmatrix}2.5 \\\\0.5 \\\\ \\end{pmatrix}\\) \\(\\boldsymbol \\Sigma_2 = \\begin{pmatrix} 1 & -0.7 \\\\ -0.7 & 1 \\\\ \\end{pmatrix}\\):","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"generative-view-sampling-from-a-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.4.4 Generative view: sampling from a mixture model","text":"Assuming know sample component densities \\(f_k(\\boldsymbol x)\\) mixture model straightforward set procedure sampling mixture \\(f_{\\text{mix}}(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\\).done two-step generative process:draw categorical distribution parameters \\(\\boldsymbol \\pi=(\\pi_1, \\ldots, \\pi_K)^T\\):\n\\[\\boldsymbol z\\sim \\text{Cat}(\\boldsymbol \\pi)\\]\nvector \\(\\boldsymbol z= (z_1, \\ldots, z_K)^T\\) indicating group allocation. group index \\(k\\) given \\(\\{k : z_k=1\\}\\).draw categorical distribution parameters \\(\\boldsymbol \\pi=(\\pi_1, \\ldots, \\pi_K)^T\\):\n\\[\\boldsymbol z\\sim \\text{Cat}(\\boldsymbol \\pi)\\]\nvector \\(\\boldsymbol z= (z_1, \\ldots, z_K)^T\\) indicating group allocation. group index \\(k\\) given \\(\\{k : z_k=1\\}\\).Subsequently, sample component \\(k\\) selected step 1:\n\\[\n\\boldsymbol x\\sim F_k\n\\]Subsequently, sample component \\(k\\) selected step 1:\n\\[\n\\boldsymbol x\\sim F_k\n\\]two-stage approach also called latent allocation variable formulation mixture model, \\(\\boldsymbol z\\) (equivalently \\(k\\)) latent variable.two-step process needs repeated sample drawn mixture (.e. every time new latent variable \\(\\boldsymbol z\\) generated).probabilistic clustering aim infer state \\(\\boldsymbol z\\) observed samples.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"predicting-the-group-allocation-of-a-given-sample","chapter":"3 Unsupervised learning and clustering","heading":"3.4.5 Predicting the group allocation of a given sample","text":"know mixture model components can predict probability observation \\(\\boldsymbol x\\) falls group \\(k\\) via application Bayes theorem:\n\\[\nz_k = \\text{Pr}(k | \\boldsymbol x) = \\frac{\\pi_k f_k(\\boldsymbol x) }{ f(\\boldsymbol x)}\n\\]mentioned interactive Shiny app normal component mixture (available online https://minerva..manchester.ac.uk/shiny/strimmer/mixture/ ) can also explore probabilities \nclass.Thus, calculating class probabilities using fitted mixture model can perform probabilistic clustering assigning sample class largest probability.\nUnlike algorithmic clustering, also get assessment uncertainty class assignment, since sample \\(\\boldsymbol x\\) obtain vector\n\\[\n\\boldsymbol z= (z_1, \\ldots, z_K)^T\n\\]\nthus can see several classes similar assignment probability. case, e.g.,\n\\(\\boldsymbol x\\) lies near boundary two classes. Note \\(\\sum_{k=1}^K z_k=1\\).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"variation-1-infinite-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.4.6 Variation 1: Infinite mixture model","text":"possible construct mixture models infinitely many components!commonly known example Dirichlet process mixture model (DPM):\\[\nf_{\\text{mix}}(\\boldsymbol x) = \\sum_{k=1}^\\infty \\pi_k f_k(\\boldsymbol x)\n\\]\n\\(\\sum_{k=1}^\\infty\\pi_k =1\\) weight \\(\\pi_k\\) taken infinitely dimensional Dirichlet distribution (=Dirichlet process).DPMs useful clustering since necessary determine number clusters priori (since definition infinitely many!). Instead, number clusters -product fit model observed data.Related: “Chinese restaurant process” - https://en.wikipedia.org/wiki/Chinese_restaurant_processThis describes algorithm allocation process samples (“persons”) groups (“restaurant tables”) DPM.See also “stick-breaking process”: https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"variation-2-semiparametric-mixture-model-with-two-classes","chapter":"3 Unsupervised learning and clustering","heading":"3.4.7 Variation 2: Semiparametric mixture model with two classes","text":"common model following two-component univariate mixture model\\[f_{\\text{mix}}(x) = \\pi_0 f_0(x) + (1-\\pi_0) f_A(\\boldsymbol x)\\]\\(f_0\\): null model, typically parametric normal distribution\\(f_A\\): alternative model, typically nonparametric\\(\\pi_0\\): prior probability null modelUsing Bayes theorem allows compute probability observation \\(x\\) belongs null model:\n\\[\\text{Pr}(\\text{Null} | x ) = \\frac{\\pi_0 f_0(x ) }{ f(x) }\\]\ncalled local false discovery rate.semi-parametric mixture model foundation statistical testing based defining decision thresholds separate null model (“significant”) alternative model (“significant”):See MATH20802 Statistical Methods\ndetails.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"fitting-mixture-models-to-data","chapter":"3 Unsupervised learning and clustering","heading":"3.5 Fitting mixture models to data","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"direct-estimation-of-mixture-model-parameters","chapter":"3 Unsupervised learning and clustering","heading":"3.5.1 Direct estimation of mixture model parameters","text":"Given data matrix \\(\\boldsymbol X= (\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)^T\\) containing observations \\(n\\) samples\n\\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) like fit mixture model\n\\(f_{\\text{mix}}(\\boldsymbol x| \\boldsymbol \\theta) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\\) learn parameters \\(\\boldsymbol \\theta\\).\nGaussian mixture model parameters \\(\\boldsymbol \\theta= \\{\\boldsymbol \\pi, \\boldsymbol \\mu_1, \\ldots, \\boldsymbol \\mu_K, \\boldsymbol \\Sigma_1, \\ldots, \\boldsymbol \\Sigma_K\\}\\).standard way (large sample size \\(n\\)) find MLEs parameters maximising corresponding marginal log-likelihood function regard \\(\\boldsymbol \\theta\\):\n\\[\n\\log L(\\boldsymbol \\theta| \\boldsymbol X) =  \\sum_{=1}^n \\log \\left( \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x_i)  \\right)\n\\]\n\n\\[\n\\hat{\\boldsymbol \\theta}^{ML} = \\underset{\\boldsymbol \\theta}{\\arg \\max}\\,\\, \\log L(\\boldsymbol \\theta| \\boldsymbol X)\n\\]\nlog-likelihood function also called observed data log-likelihood,\nincomplete data log-likelihood (contrast complete data log-likelihood described ).model fitted estimate probabilities group allocation sample\n\\(\\boldsymbol x_i\\) computing\n\\[\nz_{ik} = \\text{Pr}(k| \\boldsymbol x_i) =  \\frac{ \\hat{\\pi}_k \\hat{f}_k(\\boldsymbol x_i) }{  \\hat{f}(\\boldsymbol x_i)  }\n\\]\nNote \\(z_{ik}\\) provide -called soft assignment samples \\(\\boldsymbol x_i\\) classes rather 0/1 hard assignment example \\(K\\)-means algorithm.\narrive hard clustering can select class highest probability\n\\[\n\\hat{k}_i =\\underset{k}{\\arg \\max}\\,\\,z_{ik}\n\\]Unfortunately, practise evaluation optimisation observed data log-likelihood function can difficult due number reasons:form log-likelihood function prevents analytic simplifications\n(note sum inside logarithm).symmetries due exchangeability cluster labels likelihood function multimodal (note also linked general\nproblem label switching non-identifiability cluster labels mixtures).Furthermore, likelihood Gaussian mixture models can become singular one fitted covariance matrices becomes singular. Note can adressed using form regularisation (Bayes, penalised ML, etc.).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"estimating-mixture-model-parameters-using-the-em-algorithm","chapter":"3 Unsupervised learning and clustering","heading":"3.5.2 Estimating mixture model parameters using the EM algorithm","text":"Clustering mixture model can viewed incomplete missing data problem.\nSpecifically, missing data unknown group allocations \\(\\boldsymbol k= (k_1, \\ldots, k_n)^T\\) belonging sample \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).knew advance sample belongs particular group estimation \nparameters \\(\\boldsymbol \\theta\\) done using -called complete data log-likelihood\nbased joint distribution \\(f(\\boldsymbol x, k) = \\pi_k f_k(\\boldsymbol x)\\)\n\\[\n\\log L(\\boldsymbol \\theta| \\boldsymbol X, \\boldsymbol k) = \\sum_{=1}^n  \\log \\left(\\pi_{k_i} f_{k_i}(\\boldsymbol x_i) \\right) \n\\]\nNote much simpler function incomplete data log-likelihood .idea -called EM algorithm10 proposed Arthur Dempster others 1977 exploit simplicity complete data likelihood\nobtain estimates \\(\\boldsymbol \\theta\\) treating group allocations missing data impute , subsequently iteratively refining imputations estimates \\(\\boldsymbol \\theta\\).precisely, EM (=expectation-maximisation) algorithm alternate theupdating soft allocations \\(z_{ik}\\) using current estimate parameters \\(\\boldsymbol \\theta\\) (obtained step 2)updating parameter estimates maximising expected complete data log-likelihood. expectation taken regard distribution \\(z_{ik}\\) (obtained step 1). Thus\ncomplete data log-likelihood averaged soft class assignments.Specifically, EM algorithm proceeds follows:Initialisation: Start guess parameters \\(\\hat{\\boldsymbol \\theta}^{(1)}\\), continue “E” Step, Part .\nAlternatively, start guess \\(z_{ik}^{(1)}\\), continue\n“E” Step, Part B. initialisation may derived prior information, e.g., running \\(K\\)-means, simply random. Note particular initialisations correspond invariant states hence avoided (see ).Initialisation: Start guess parameters \\(\\hat{\\boldsymbol \\theta}^{(1)}\\), continue “E” Step, Part .\nAlternatively, start guess \\(z_{ik}^{(1)}\\), continue\n“E” Step, Part B. initialisation may derived prior information, e.g., running \\(K\\)-means, simply random. Note particular initialisations correspond invariant states hence avoided (see ).E “expectation” step — Part : Use Bayes’ theorem compute new probabilities allocation samples \\(\\boldsymbol x_i\\):\n\\[\nz_{ik}^{(b+1)} \\leftarrow \\frac{ \\hat{\\pi}_k^{(b)} \\hat{f}_k^{(b)}(\\boldsymbol x_i) }{  \\hat{f}^{(b)}(\\boldsymbol x_i)  }\n\\]\nNote obtain \\(z_{ik}^{(b+1)}\\) current estimated value\n\\(\\hat{\\boldsymbol \\theta}^{(b)}\\) parameters required.\n— Part B: Construct expected complete data log-likelihood function \\(\\boldsymbol \\theta\\) using soft allocations \\(z_{ik}^{(b+1)}\\):\n\\[\nQ^{(b+1)}(\\boldsymbol \\theta| \\boldsymbol X) = \\sum_{=1}^n \\sum_{k=1}^K z_{ik}^{(b+1)}  \\log \\left( \\pi_k f_k(\\boldsymbol x_i) \\right)\n\\]E “expectation” step — Part : Use Bayes’ theorem compute new probabilities allocation samples \\(\\boldsymbol x_i\\):\n\\[\nz_{ik}^{(b+1)} \\leftarrow \\frac{ \\hat{\\pi}_k^{(b)} \\hat{f}_k^{(b)}(\\boldsymbol x_i) }{  \\hat{f}^{(b)}(\\boldsymbol x_i)  }\n\\]\nNote obtain \\(z_{ik}^{(b+1)}\\) current estimated value\n\\(\\hat{\\boldsymbol \\theta}^{(b)}\\) parameters required.\n— Part B: Construct expected complete data log-likelihood function \\(\\boldsymbol \\theta\\) using soft allocations \\(z_{ik}^{(b+1)}\\):\n\\[\nQ^{(b+1)}(\\boldsymbol \\theta| \\boldsymbol X) = \\sum_{=1}^n \\sum_{k=1}^K z_{ik}^{(b+1)}  \\log \\left( \\pi_k f_k(\\boldsymbol x_i) \\right)\n\\]M “maximisation” step — Maximise expected complete data log-likelihood update estimates mixture model parameters:\n\\[\n\\hat{\\boldsymbol \\theta}^{(b+1)} \\leftarrow \\arg \\max_{\\boldsymbol \\theta}  Q^{(b+1)}(\\boldsymbol \\theta| \\boldsymbol X)\n\\]M “maximisation” step — Maximise expected complete data log-likelihood update estimates mixture model parameters:\n\\[\n\\hat{\\boldsymbol \\theta}^{(b+1)} \\leftarrow \\arg \\max_{\\boldsymbol \\theta}  Q^{(b+1)}(\\boldsymbol \\theta| \\boldsymbol X)\n\\]Continue 2) “E” Step series \\(\\hat{\\boldsymbol \\theta}^{(1)}, \\hat{\\boldsymbol \\theta}^{(2)}, \\hat{\\boldsymbol \\theta}^{(3)}, \\ldots\\) converged , equivalently, series maximised complete data log-likelihoods\n\\(Q^{(1)}(\\hat{\\boldsymbol \\theta}^{(1)} | \\boldsymbol X), Q^{(2)}(\\hat{\\boldsymbol \\theta}^{(2)} | \\boldsymbol X), Q^{(3)}(\\hat{\\boldsymbol \\theta}^{(3)} | \\boldsymbol X), \\ldots\\) converged.Continue 2) “E” Step series \\(\\hat{\\boldsymbol \\theta}^{(1)}, \\hat{\\boldsymbol \\theta}^{(2)}, \\hat{\\boldsymbol \\theta}^{(3)}, \\ldots\\) converged , equivalently, series maximised complete data log-likelihoods\n\\(Q^{(1)}(\\hat{\\boldsymbol \\theta}^{(1)} | \\boldsymbol X), Q^{(2)}(\\hat{\\boldsymbol \\theta}^{(2)} | \\boldsymbol X), Q^{(3)}(\\hat{\\boldsymbol \\theta}^{(3)} | \\boldsymbol X), \\ldots\\) converged.Since maximisation expected complete data log-likelihood typically much easier (often also analytically tractable) EM algorithm often preferred direct\nmaximisation observed data log-likelihood.Note avoid singularities expected log-likelihood function \nmay need adopt regularisation (.e. penalised maximum likelihood Bayesian learning) estimating parameters M-step.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"em-algorithm-for-multivariate-normal-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.5.3 EM algorithm for multivariate normal mixture model","text":"Gaussian mixture model (GMM) steps EM algorithm can expressed analytically:E-step:Update soft allocations:\n\\[\nz_{ik}^{(b+1)} = \\frac{ \\hat{\\pi}_k^{(b)} N(\\boldsymbol x_i | \\hat{\\boldsymbol \\mu}_k^{(b)}, \\hat{\\boldsymbol \\Sigma}_k^{(b)}) }{  \\hat{f}^{(b)}(\\boldsymbol x_i)  }\n\\]M-step:number samples assigned class \\(k\\) current step \n\\[\nn_k^{(b+1)} = \\sum_{=1}^n z_{ik}^{(b+1)} \n\\]\nNote necessarily integer soft allocations samples groups!updated estimate group probabilities \n\\[\n\\hat{\\pi}_k^{(b+1)} = \\frac{n_k^{(b+1)}}{n}\n\\]\nupdated estimate mean \n\\[\n\\hat{\\boldsymbol \\mu}_k^{(b+1)} = \\frac{1}{n_k^{(b+1)}} \\sum_{=1}^n z_{ik}^{(b+1)} \\boldsymbol x_i\n\\]\nupdated covariance estimate \n\\[\n\\hat{\\boldsymbol \\Sigma}_k^{(b+1)} =  \\frac{1}{n_k^{(b+1)}} \\sum_{=1}^n z_{ik}^{(b+1)} \\left( \\boldsymbol x_i -\\boldsymbol \\mu_k^{(b+1)}\\right)   \\left( \\boldsymbol x_i -\\boldsymbol \\mu_k^{(b+1)}\\right)^T\n\\]Note \\(z_{ik}\\) hard allocation (\\(\\) one class weight 1 others weight 0) estimators reduce usual empirical estimators.Worksheet 7 can find simple R implementation EM algorithm \nunivariate normal mixtures.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"convergence","chapter":"3 Unsupervised learning and clustering","heading":"3.5.4 Convergence","text":"mild assumptions EM algorithm guaranteed monotonically converge local optima marginal log-likelihood.11 Thus series \\(\\hat{\\boldsymbol \\theta}^{(1)}, \\hat{\\boldsymbol \\theta}^{(2)}, \\hat{\\boldsymbol \\theta}^{(3)}, \\ldots\\) converges estimate \\(\\hat{\\boldsymbol \\theta}\\) found maximising marginal log-likelihood.\nHowever, speed convergence EM algorithm can sometimes slow, also situations convergence \\(\\hat{\\boldsymbol \\theta}\\) EM algorithm remains invariant state.example invariant state Gaussian mixture model uniform initialisation latent variables \\(z_{ik} = \\frac{1}{K}\\), \\(K\\) number classes.\nget M step \\(n_k = \\frac{n}{K}\\) parameter estimates\n\\[\n\\hat{\\pi}_k = \\frac{1}{K}\n\\]\n\\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i = \\bar{\\boldsymbol x}\n\\]\n\\[\n\\hat{\\boldsymbol \\Sigma}_k = \\frac{1}{n}  \\sum_{=1}^n ( \\boldsymbol x_i -\\bar{\\boldsymbol x})   ( \\boldsymbol x_i -\\bar{\\boldsymbol x})^T = \\hat{\\boldsymbol \\Sigma}\n\\]\nCrucially, none actually depend group \\(k\\)! Thus, E step next soft allocations determined leads \n\\[\nz_{ik} = \\frac{ \\frac{1}{K} N(\\boldsymbol x_i | \\bar{\\boldsymbol x}, \\hat{\\boldsymbol \\Sigma} ) }{ \\sum_{j=1}^K  \\frac{1}{K} N(\\boldsymbol x_i | \\bar{\\boldsymbol x}, \\hat{\\boldsymbol \\Sigma} )  } = \\frac{1}{K}\n\\]\none cycle EM algorithm arrive soft allocation started , algorithm trapped invariant state! Therefore uniform initialisation clearly avoided!","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"connection-with-k-means-clustering-method","chapter":"3 Unsupervised learning and clustering","heading":"3.5.5 Connection with \\(K\\)-means clustering method","text":"\\(K\\)-means algorithm closely related probabilistic clustering Gaussian mixture models.Specifically, class assigment \\(K\\)-means \n\\[C(\\boldsymbol x_i) = \\underset{k}{\\arg \\min} \\, (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)\\]Gaussian mixture model probabilities \\(\\pi_k\\) classes asssumed identical (.e. \\(\\pi_k=\\frac{1}{K}\\))\ncovariances \\(\\boldsymbol \\Sigma_k\\) spherical form \\(\\sigma^2 \\boldsymbol \\), .e. dependence groups, correlation identical variance variables,\nsoft assignment class allocation becomes\n\\[\\log( z_{ik} ) = -\\frac{1}{2 \\sigma^2} (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k) +  C \n\\]\n\\(C\\) constant depending \\(\\boldsymbol x_i\\) \\(k\\). order select hard class allocation based \\(z_{ik}\\) may use rule\n\\[\n\\begin{split}\nC(\\boldsymbol x_i) &= \\underset{k}{\\arg \\max} \\log( z_{ik} ) \\\\\n          & = \\underset{k}{\\arg \\min}  (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)^T (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_k)\\\\\n\\end{split}\n\\]\nThus, \\(K\\)-means can viewed algorithm provide hard classifications\nsimple restricted Gaussian mixture model.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"choosing-the-number-of-classes","chapter":"3 Unsupervised learning and clustering","heading":"3.5.6 Choosing the number of classes","text":"Since GMMs operate likelihood framework can use penalised likelihood model selection criteria choose among different models (.e. GMMs different numbers classes).popular choices AIC (Akaike Information Criterion) BIC (Bayesian Information criterion) defined follows:\n\\[\\text{AIC}= -2 \\log L + 2 K \\]\n\\[\\text{BIC}= - 2 \\log L +K \\log(n)\\]Instead maximising log-likehood minimise \\(\\text{AIC}\\) \\(\\text{BIC}\\).Note criteria complex models parameters (case groups) penalised\nsimpler models order prevent overfitting.\\(\\Longrightarrow\\) find optimal number groups \\(K\\).Another way choosing optimal numbers clusters cross-validation (see later chapter supervised learning).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-of-gmms-to-iris-flower-data","chapter":"3 Unsupervised learning and clustering","heading":"3.5.7 Application of GMMs to Iris flower data","text":"now explore application Gaussian mixture models Iris flower data set also investigated PCA\nK-means.First, fit GMM 3 clusters, using R software “mclust.”12The “mclust” software used following model fitting mixture:“VVV” name used “mclust” software model\nallowing individual\nunrestricted covariance matrix \\(\\boldsymbol \\Sigma_k\\) class \\(k\\).GMM substantially lower misclassification error compared \\(K\\)-means number clusters:Note “mclust” BIC criterion defined opposite sign (\\(\\text{BIC}_{\\text{mclust}} = 2 \\log L -K \\log(n)\\)), thus need find maximum value rather smallest value.compute BIC various numbers groups find model best \\(\\text{BIC}_{\\text{mclust}}\\) model 2 clusters model 3 cluster nearly good BIC:","code":"\nlibrary(\"mclust\")\ngmm3 = Mclust(X.iris, G=3, verbose=FALSE)\nplot(gmm3, what=\"classification\")\ngmm3$modelName## [1] \"VVV\"\ntable(gmm3$classification, L.iris)##    L.iris\n##     setosa versicolor virginica\n##   1     50          0         0\n##   2      0         45         0\n##   3      0          5        50"},{"path":"supervised-learning-and-classification.html","id":"supervised-learning-and-classification","chapter":"4 Supervised learning and classification","heading":"4 Supervised learning and classification","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"introduction","chapter":"4 Supervised learning and classification","heading":"4.1 Introduction","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"supervised-learning-vs.-unsupervised-learning","chapter":"4 Supervised learning and classification","heading":"4.1.1 Supervised learning vs. unsupervised learning","text":"Unsupervised learning:Starting point:unlabeled data \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).Aim: find labels \\(y_1, \\ldots, y_n\\) attach sample \\(\\boldsymbol x_i\\).discrete labels \\(y\\) unsupervised learning called clustering.Supervised learning:Starting point:labeled training data: \\(\\{\\boldsymbol x_1^{\\text{train}}, y_1^{\\text{train}}\\}\\),\n\\(\\ldots\\), \\(\\{\\boldsymbol x_n^{\\text{train}}, y_n^{\\text{train}} \\}\\)addition, unlabeled test data: \\(\\boldsymbol x^{\\text{test}}\\)Aim: use training data learn function, say \\(h(\\boldsymbol x)\\),\npredict label corresponding test data.\npredictor function may provide soft (probabilistic) assignment\nhard assignment class label test sample.\\(y\\) discrete supervised learning called classification.\ncontinuous \\(y\\) label called response supervised learning becomes regression.Thus, supervised learning two-step procedure:Learn predictor function \\(h(\\boldsymbol x)\\) using training data \\(\\boldsymbol x_i^{\\text{train}}\\) plus labels \\(y_i^{\\text{train}}\\).Predict label \\(y^{\\text{test}}\\) test data \\(\\boldsymbol x^{\\text{test}}\\) using estimated classifier function:\n\\(\\hat{y}^{\\text{test}} = \\hat{h}(\\boldsymbol x^{\\text{test}})\\).","code":""},{"path":"supervised-learning-and-classification.html","id":"terminology","chapter":"4 Supervised learning and classification","heading":"4.1.2 Terminology","text":"function \\(h(\\boldsymbol x)\\) predicts class \\(y\\) called classifier.many types classifiers, focus primarily probabilistic classifiers\n(.e. output probabilities possible class/label).challenge find classifier thatexplains current training data well andthat also generalises well future unseen data.Note relatively easy find predictor explains training data especially high dimensions (.e. many predictors) often overfitting predictor generalise well!decision boundary classes defined set \\(\\boldsymbol x\\) \nclass assignment predictor \\(h(\\boldsymbol x)\\) switches one class another.general, simple decision boundaries preferred complex decision boundaries avoid overfitting.commonly used probabilistic methods classifications:QDA (quadratic discriminant analysis)LDA (linear discriminant analysis)DDA (diagonal discriminant analysis),Naive Bayes classificationlogistic regressionCommon non-probabilistic methods include:SVM (support vector machine),random forestneural networksDepending classifiers trainined many variations\nmethods, e.g. Fisher discriminant analysis, regularised LDA, shrinkage disciminant analysis etc.","code":""},{"path":"supervised-learning-and-classification.html","id":"bayesian-discriminant-rule-or-bayes-classifier","chapter":"4 Supervised learning and classification","heading":"4.2 Bayesian discriminant rule or Bayes classifier","text":"setup mixture models:\\(K\\) groups \\(K\\) prespecifiedeach group distribution \\(F_k\\) parameters \\(\\boldsymbol \\theta_k\\)density class \\(f_k(\\boldsymbol x) = f(\\boldsymbol x| k)\\).prior probability group \\(k\\) \\(\\text{Pr}(k) = \\pi_k\\) \\(\\sum_{k=1}^K \\pi_k = 1\\)marginal density mixture \\(f(\\boldsymbol x) = \\sum_{k=1}^K \\pi_k f_k(\\boldsymbol x)\\)posterior probability group \\(k\\) \n\\[\n\\text{Pr}(k | \\boldsymbol x) = \\frac{\\pi_k f_k(\\boldsymbol x) }{ f(\\boldsymbol x)}\n\\]already provides “soft” classification\n\\[\\boldsymbol h(\\boldsymbol x^{\\text{test}}) = (\\text{Pr}(k=1 | \\boldsymbol x^{\\text{test}}),\\ldots, \\text{Pr}(k=K | \\boldsymbol x^{\\text{test}})   )^T\\]\npossible class \\(k \\\\{ 1, \\ldots, K\\}\\) assigned probability label \ntest sample \\(\\boldsymbol x\\).discriminant function logarithm posterior probability:\n\\[\nd_k(\\boldsymbol x) = \\log \\text{Pr}(k | \\boldsymbol x) = \\log \\pi_k  + \\log f_k(\\boldsymbol x)  - \\log f(\\boldsymbol x) \n\\]\nSince use \\(d_k\\) compare different classes \\(k\\) can\nsimplify discriminant function dropping constant terms depend \\(k\\) — term \\(\\log f(\\boldsymbol x)\\). Hence get Bayes discriminant function\n\\[\nd_k(\\boldsymbol x) = \\log \\pi_k + \\log f_k(\\boldsymbol x) \\,.\n\\]subsequent “hard” classification \\(h(\\boldsymbol x^{\\text{test}})\\) select group/label value discriminant function maximised:\n\\[\n\\hat{y}^{\\text{test}} = h(\\boldsymbol x^{\\text{test}}) = \\arg \\max_k d_k(\\boldsymbol x^{\\text{test}}) \\,.\n\\]discriminant functions \\(d_k(\\boldsymbol x)\\) can mapped back probabilistic class assignment using softargmax function (also known softmax function):\n\\[\n\\text{Pr}(k | \\boldsymbol x) = \n\\frac{\\exp( d_k(\\boldsymbol x) )}{\\sum_{c=1}^K \\exp( d_c(\\boldsymbol x) ) } = \n\\frac{\\exp( d_k(\\boldsymbol x) - d_{\\max} ) }{\\sum_{c=1}^K \\exp( d_c(\\boldsymbol x) - d_{\\max} ) } \\,.\n\\]\nNote subtracting \\(d_{\\max} = \\max\\{ d_1(\\boldsymbol x), \\ldots, d_K(\\boldsymbol x) \\}\\) avoids numerical overflow problems computing exponential\nstandardising maximum discriminant functions zero.already encountered Bayes classifier EM algorithm predict state\nlatent variables (soft assignment) \\(K\\)-means algorithm (hard assignment), see previous Chapter also Worksheet 7.","code":""},{"path":"supervised-learning-and-classification.html","id":"normal-bayes-classifier","chapter":"4 Supervised learning and classification","heading":"4.3 Normal Bayes classifier","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"quadratic-discriminant-analysis-qda-and-gaussian-assumption","chapter":"4 Supervised learning and classification","heading":"4.3.1 Quadratic discriminant analysis (QDA) and Gaussian assumption","text":"Quadratic discriminant analysis (QDA) special case Bayes classifier densities multivariate normal \\(f_k(\\boldsymbol x) = N(\\boldsymbol x| \\boldsymbol \\mu_k, \\boldsymbol \\Sigma_k)\\).leads discriminant function QDA:\n\\[\nd_k^{QDA}(\\boldsymbol x) = -\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma_k^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) -\\frac{1}{2} \\log \\det(\\boldsymbol \\Sigma_k) +\\log(\\pi_k)\n\\]number noteworthy things :terms dropped depend \\(k\\), \\(-\\frac{d}{2}\\log( 2\\pi)\\).Note appearance Mahalanobis distance \\(\\boldsymbol x\\) \\(\\boldsymbol \\mu_k\\)\nlast term — recall \\(d^{Mahalanobis}(\\boldsymbol x, \\boldsymbol \\mu| \\boldsymbol \\Sigma) = (\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)\\).QDA discriminant function quadratic \\(\\boldsymbol x\\) - hence name!\nimplies decision boundaries QDA classification quadratic (.e. parabolas two dimensional settings).Gaussian models specifically can useful multiply discriminant function -2 get rid factor \\(-\\frac{1}{2}\\), note case need find minimum discriminant function rather maximum:\n\\[\nd_k^{QDA (v2)}(\\boldsymbol x) =  (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma_k^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) + \\log \\det(\\boldsymbol \\Sigma_k)  -2 \\log(\\pi_k)\n\\]\nliterature find versions Gaussian discriminant functions need check carefully convention used.\nfollowing use first version .Decision boundaries QDA classifier can either linear nonlinear (quadratic curve).\ndecision boundary two classes \\(\\) \\(j\\)\nrequire \\(d^{QDA}_i(\\boldsymbol x) = d^{QDA}_j(\\boldsymbol x)\\), equivalently\n\\(d^{QDA}_i(\\boldsymbol x) - d^{QDA}_j(\\boldsymbol x) = 0\\), quadratic equation.","code":""},{"path":"supervised-learning-and-classification.html","id":"linear-discriminant-analysis-lda","chapter":"4 Supervised learning and classification","heading":"4.3.2 Linear discriminant analysis (LDA)","text":"LDA special case QDA, assumption common overall covariance across groups: \\(\\boldsymbol \\Sigma_k = \\boldsymbol \\Sigma\\).leads simplified discriminant function:\n\\[\nd_k^{LDA}(\\boldsymbol x) = -\\frac{1}{2} (\\boldsymbol x-\\boldsymbol \\mu_k)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu_k) +\\log(\\pi_k)\n\\]\nNote term containing log-determinant now gone, LDA essentially now method tries minimize Mahalanobis distance\n(taking also account prior class probabilities).function can simplified, noting quadratic term \\(\\boldsymbol x^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x\\) depend \\(k\\) hence can dropped:\n\\[\n\\begin{split}\nd_k^{LDA}(\\boldsymbol x) &=  \\boldsymbol \\mu_k^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_k^T \\boldsymbol \\Sigma^{-1} \\boldsymbol \\mu_k + \\log(\\pi_k) \\\\\n  &= \\boldsymbol b^T \\boldsymbol x+ \n\\end{split}\n\\]\nThus, LDA discriminant function linear \\(\\boldsymbol x\\), hence \nresulting decision boundaries linear well (.e. straight lines two-dimensional settings).Comparison linear decision boundaries LDA (left) compared QDA (right):Note logistic regression (cf. GLM module) takes exactly linear form indeed closely linked LDA classifier.","code":""},{"path":"supervised-learning-and-classification.html","id":"diagonal-discriminant-analysis-dda","chapter":"4 Supervised learning and classification","heading":"4.3.3 Diagonal discriminant analysis (DDA)","text":"DDA start setting LDA, now simplify model even additionally requiring diagonal covariance containing variances (thus assume correlations among predictors \\(x_1, \\ldots, x_d\\) zero):\n\\[\n\\boldsymbol \\Sigma= \\boldsymbol V= \\begin{pmatrix}\n    \\sigma^2_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma^2_{d}\n\\end{pmatrix}\n\\]\nsimplifies inversion \\(\\boldsymbol \\Sigma\\) \n\\[\n\\boldsymbol \\Sigma^{-1} = \\boldsymbol V^{-1} = \\begin{pmatrix}\n    \\sigma^{-2}_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma^{-2}_{d}\n\\end{pmatrix}\n\\]\nleads discriminant function\n\\[\n\\begin{split}\nd_k^{DDA}(\\boldsymbol x) &=  \\boldsymbol \\mu_k^T \\boldsymbol V^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_k^T \\boldsymbol V^{-1} \\boldsymbol \\mu_k + \\log(\\pi_k) \\\\\n  &= \\sum_{j=}^d \\frac{\\mu_{k,j} x_j - \\mu_{k,j}^2/2}{\\sigma_d^2} + \\log(\\pi_k)\n\\end{split}\n\\]\nspecial case LDA, DDA classifier linear classifier thus linear decision boundaries.Bayes classifier (using distribution) assuming uncorrelated predictors\nalso known naive Bayes classifier.Hence, DDA naive Bayes classifier assuming underlying Gaussian distributions.However, don’t let misguide name “naive”: fact DDA “naive” Bayes classifier often effective classifiers, especially high-dimensional settings!","code":""},{"path":"supervised-learning-and-classification.html","id":"the-training-step-learning-qda-lda-and-dda-classifiers-from-data","chapter":"4 Supervised learning and classification","heading":"4.4 The training step — learning QDA, LDA and DDA classifiers from data","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"number-of-model-parameters","chapter":"4 Supervised learning and classification","heading":"4.4.1 Number of model parameters","text":"order predict class new data using discriminant functions need first learn underlying parameters training data \\(\\boldsymbol x_i^{\\text{train}}\\) \\(y_i^{\\text{train}}\\):QDA, LDA DDA need learn \\(\\pi_1, \\ldots, \\pi_K\\) \\(\\sum_{k=1}^K \\pi_k = 1\\) mean vectors \\(\\boldsymbol \\mu_1, \\ldots, \\boldsymbol \\mu_K\\)QDA additionally require \\(\\boldsymbol \\Sigma_1, \\ldots, \\boldsymbol \\Sigma_K\\)LDA need \\(\\boldsymbol \\Sigma\\)DDA estimate \\(\\sigma^2_1, \\ldots, \\sigma^2_d\\).Overall, total number parameters estimated learning discriminant functions\ntraining data follows:QDA: \\(K-1+ K d + K \\frac{d(d+1)}{2}\\)LDA: \\(K-1+ K d + \\frac{d(d+1)}{2}\\)DDA: \\(K-1+ K d + d\\)","code":""},{"path":"supervised-learning-and-classification.html","id":"estimating-the-discriminant-predictor-function","chapter":"4 Supervised learning and classification","heading":"4.4.2 Estimating the discriminant / predictor function","text":"QDA, LDA DDA learn predictor estimating \nparameters discriminant function training data.","code":""},{"path":"supervised-learning-and-classification.html","id":"large-sample-size","chapter":"4 Supervised learning and classification","heading":"4.4.2.1 Large sample size","text":"sample size training data set sufficiently large compared model dimensions can use maximum likelihood (ML) estimate model parameters. able use ML need larger sample size QDA LDA (full covariances need estimated) DDA relatively small sample size can sufficient (explains “naive” Bayes methods popular practise).obtain parameters estimates use known labels \\(y_i^{\\text{train}}\\) sort \nsamples \\(\\boldsymbol x_i^{\\text{train}}\\) corresponding classes, apply standard ML estimators.\nLet \\(g_k =\\{: y_i^{\\text{train}}=k \\}\\) set indices training sample belonging group \\(k\\), \\(n_k\\) sample size group \\(k\\)ML estimates class probabilities frequencies\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n}\n\\]\nML estimate group means \\(k=1, \\ldots, K\\) \n\\[\n\\hat{\\boldsymbol \\mu}_k = \\frac{1}{n_k} \\sum_{\\g_k} \\boldsymbol x_i^{\\text{train}} \\, .\n\\]\nML estimate global mean \\(\\boldsymbol \\mu_0\\) (.e. assume single class ignore group labels) \n\\[\n\\hat{\\boldsymbol \\mu}_0 = \\frac{1}{n} \\sum_{=1}^n \\boldsymbol x_i^{\\text{train}} = \\sum_{k=1}^K \\hat{\\pi}_k \\hat{\\boldsymbol \\mu}_k\n\\]\nNote global mean identical pooled mean (.e. weighted average \nindividual group means).ML estimates covariances \\(\\boldsymbol \\Sigma_k\\) QDA \n\\[\n\\widehat{\\boldsymbol \\Sigma}_k = \\frac{1}{n_k} \\sum_{\\g_k} ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k)^T\n\\]order get ML estimate pooled variance \\(\\boldsymbol \\Sigma\\) use LDA compute\n\\[\n\\widehat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\sum_{k=1}^K \\sum_{\\g_k} ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_k)^T =  \\sum_{k=1}^K \\hat{\\pi}_k \\widehat{\\boldsymbol \\Sigma}_k \n\\]Note pooled variance \\(\\boldsymbol \\Sigma\\) differs (substantially!) global variance \\(\\Sigma_0\\) results simply\nignoring class labels computed \n\\[\n\\widehat{\\boldsymbol \\Sigma}_0^{ML} = \\frac{1}{n} \\sum_{=1}^n ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_0) ( \\boldsymbol x_i^{\\text{train}} -\\hat{\\boldsymbol \\mu}_0)^T\n\\]\nrecognise variance decomposition mixture models, \\(\\boldsymbol \\Sigma_0\\) total variance\npooled \\(\\boldsymbol \\Sigma\\) unexplained/-group variance.","code":""},{"path":"supervised-learning-and-classification.html","id":"small-sample-size","chapter":"4 Supervised learning and classification","heading":"4.4.2.2 Small sample size","text":"dimension \\(d\\) large compared sample size number parameters predictor function grows fast. Especially QDA also LDA data hungry ML estimation becomes ill-posed problem.discussed Section 1.5 instance need use regularised estimator covariance(s) estimators derived framework penalised ML, Bayesian learning, shrinkage estimation etc.\nalso ensures estimated covariance matrices positive definite (\nautomatically guaranteed DDA variances positive).Furthermore, small sample setting advised reduce number parameters model. Thus using LDA DDA preferred QDA. can also prevent overfitting lead predictor generalises better.analyse high-dimensional data worksheets employ regularised version LDA DDA using Stein-type shrinkage estimation discussed Section 1.5 implemented R package “sda”.","code":""},{"path":"supervised-learning-and-classification.html","id":"comparison-of-estimated-decision-boundaries-lda-vs.-qda","chapter":"4 Supervised learning and classification","heading":"4.4.3 Comparison of estimated decision boundaries: LDA vs. QDA","text":"compare two simple scenarios using simulated data.Non-nested case (\\(K=4\\)):Nested case (\\(K=2\\)):nested case LDA fails separate two classes \nway separate two nested classes \nsimple linear boundary.","code":""},{"path":"supervised-learning-and-classification.html","id":"goodness-of-fit-and-variable-ranking","chapter":"4 Supervised learning and classification","heading":"4.5 Goodness of fit and variable ranking","text":"linear regression (cf. “Statistical Methods” module) interested finding \nwhether fitted mixture model appropriate model, \nparticular predictor(s) \\(x_j\\) \\(\\boldsymbol x=(x_1, \\ldots, x_d)^T\\)\nresponsible prediction outcome, .e. categorizing sample group \\(k\\).order study problem helpful rewrite discriminant function highlight influence (importance) predictor.focus linear methods (LDA DDA) first look simple case \\(K=2\\) generalise two groups.","code":""},{"path":"supervised-learning-and-classification.html","id":"lda-with-k2-classes","chapter":"4 Supervised learning and classification","heading":"4.5.1 LDA with \\(K=2\\) classes","text":"two classes using LDA discriminant rule choose group \\(k=1\\)\n\\(d_1^{LDA}(\\boldsymbol x) > d_2^{LDA}(\\boldsymbol x)\\), equivalently, \n\\[\n\\Delta_{12}^{LDA} = d_1^{LDA}(\\boldsymbol x) - d_2^{LDA}(\\boldsymbol x) > 0\n\\]\nSince \\(d_k(\\boldsymbol x)\\) log-posterior (plus/minus identical constants)\n\\(\\Delta^{LDA}\\) fact log-posterior odds class 1 versus class 2 (see Statistical Methods, Bayesian inference).difference \\(\\Delta_{12}^{LDA}\\) \n\\[\n\\underbrace{ \\Delta_{12}^{LDA}}_{\\text{log posterior odds}} = \n\\underbrace{(\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} \\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right)}_{\\text{log Bayes factor } \\log B_{12}} + \\underbrace{\\log\\left( \\frac{\\pi_1}{\\pi_2} \\right)}_{\\text{log prior odds}}\n\\]\nNote since consider simple non-composite models log-Bayes factor identical\nlog-likelihood ratio!log Bayes factor \\(\\log B_{12}\\) known weight evidence favour\n\\(F_1\\) given \\(\\boldsymbol x\\). expected weight evidence assuming \\(\\boldsymbol x\\) indeed \\(F_1\\)\nKullback-Leibler discrimination information favour group 1,\n.e. KL divergence distribution \\(F_2\\) \\(F_1\\):\n\\[\n\\text{E}_{F_1} ( \\log B_{12} ) = D_{\\text{KL}}(F_1,  F_2) = \\frac{1}{2} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2) = \\frac{1}{2} \\Omega^2\n\\]\nyields, apart scale factor, population version \nHotelling \\(T^2\\)\nstatistic defined \n\\[T^2 =  c^2 (\\hat{\\boldsymbol \\mu}_1 -\\hat{\\boldsymbol \\mu}_2)^T \\hat{\\boldsymbol \\Sigma}^{-1} (\\hat{\\boldsymbol \\mu}_1 -\\hat{\\boldsymbol \\mu}_2)\\]\n\n\\(c = (\\frac{1}{n_1} + \\frac{1}{n_2})^{-1/2} = \\sqrt{n \\pi_1 \\pi_2}\\)\nsample size dependent factor (\\(\\text{SD}(\\hat{\\boldsymbol \\mu}_1 - \\hat{\\boldsymbol \\mu}_2)\\)).\n\\(T^2\\) measure fit underlying two-component mixture.Using whitening transformation \\(\\boldsymbol z= \\boldsymbol W\\boldsymbol x\\) \\(\\boldsymbol W^T \\boldsymbol W= \\boldsymbol \\Sigma^{-1}\\)\ncan rewrite log Bayes factor \n\\[\n\\begin{split}\n\\log B_{12} &= \\left( (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol W^T \\right)\\, \\left(\\boldsymbol W\\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right) \\right) \\\\\n&=\\boldsymbol \\omega^T \\boldsymbol \\delta(\\boldsymbol x)\n\\end{split}\n\\]\n.e. product two vectors:\\(\\boldsymbol \\delta(\\boldsymbol x)\\) whitened \\(\\boldsymbol x\\) (centered around average means)\n\\(\\boldsymbol \\omega= (\\omega_1, \\ldots, \\omega_d)^T = \\boldsymbol W(\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)\\) gives weight \nwhitened component \\(\\boldsymbol \\delta(\\boldsymbol x)\\)\nlog Bayes factor.large positive negative value \\(\\omega_j\\)\nindicates corresponding whitened predictor relevant choosing class,\nwhereas small values \\(\\omega_j\\) close zero indicate corresponding ZCA whitened predictor unimportant. Furthermore,\n\\(\\boldsymbol \\omega^T \\boldsymbol \\omega= \\sum_{j=1}^d \\omega_j^2 = (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2) = \\Omega^2\\),\n.e. squared \\(\\omega_j^2\\) provide component-wise decomposition overall fit \\(\\Omega^2\\).Choosing ZCA-cor whitening transformation \\(\\boldsymbol W=\\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2}\\)\nget\n\\[\n\\boldsymbol \\omega^{ZCA-cor} = \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)\n\\]\nbetter understanding \\(\\boldsymbol \\omega^{ZCA-cor}\\) provided \ncomparing two-sample \\(t\\)-statistic\n\\[\n\\hat{\\boldsymbol \\tau} = c \\hat{\\boldsymbol V}^{-1/2} (\\hat{\\boldsymbol \\mu}_1 - \\hat{\\boldsymbol \\mu}_2)\n\\]\n\\(\\boldsymbol \\tau\\) population version \\(\\hat{\\boldsymbol \\tau}\\) can define\n\\[\\boldsymbol \\tau^{adj} = \\boldsymbol P^{-1/2} \\boldsymbol \\tau= c \\boldsymbol \\omega^{ZCA-cor}\\]\ncorrelation-adjusted \\(t\\)-scores (cat scores). \\(({\\hat{\\boldsymbol \\tau}}^{adj})^T {\\hat{\\boldsymbol \\tau}}^{adj} = T^2\\) can see cat scores offer component-wise decomposition Hotelling’s \\(T^2\\).Note choice ZCA-cor whitening ensure whitened components interpretable\nstay maximally correlated original variables. However, may also choose example PCA whitening\ncase \\(\\boldsymbol \\omega^T \\boldsymbol \\omega\\) provide variable importance PCA whitened variables.DDA, assumes correlations among predictors vanish, .e. \\(\\boldsymbol P= \\boldsymbol I_d\\), get\n\\[\n\\Delta_{12}^{DDA} =\\underbrace{ \\left( (\\boldsymbol \\mu_1 -\\boldsymbol \\mu_2)^T \\boldsymbol V^{-1/2}  \\right)}_{\\text{ } c^{-1} \\boldsymbol \\tau^T }\\, \\underbrace{ \\left( \\boldsymbol V^{-1/2} \\left(\\boldsymbol x- \\frac{\\boldsymbol \\mu_1+\\boldsymbol \\mu_2}{2}\\right) \\right) }_{\\text{centered standardised predictor}}+ \\log\\left( \\frac{\\pi_1}{\\pi_2} \\right) \\\\\n\\]\nSimilarly , \\(t\\)-score \\(\\boldsymbol \\tau\\) determines impact standardised predictor \\(\\Delta^{DDA}\\).Consequently, DDA can rank predictors squared \\(t\\)-score.\nRecall standard linear regression uncorrelated predictors can find important predictors\nranking squared marginal correlations – ranking (squared) \\(t\\)-scores DDA exact analogy discrete response.","code":""},{"path":"supervised-learning-and-classification.html","id":"multiple-classes","chapter":"4 Supervised learning and classification","heading":"4.5.2 Multiple classes","text":"two classes need refer -called pooled centroids formulation DDA LDA (introduced Tibshirani 2002).pooled centroid given \\(\\boldsymbol \\mu_0 = \\sum_{k=1}^K \\pi_k \\boldsymbol \\mu_k\\) — centroid\nsingle class. corresponding probability (single class) \\(\\pi_0=1\\) distribution\ncalled \\(F_0\\).LDA discriminant function “group 0” \n\\[\nd_0^{LDA}(\\boldsymbol x) = \\boldsymbol \\mu_0^T \\boldsymbol \\Sigma^{-1} \\boldsymbol x- \\frac{1}{2}\\boldsymbol \\mu_0^T \\boldsymbol \\Sigma^{-1} \\boldsymbol \\mu_0 \n\\]\nlog posterior odds comparison group \\(k\\) pooled group \\(0\\)\n\n\\[\n\\begin{split}\n\\Delta_k^{LDA} &= d_k^{LDA}(\\boldsymbol x) - d_0^{LDA}(\\boldsymbol x) \\\\\n         &= \\log B_{k0} + \\log(\\pi_k) \\\\\n         &= \\boldsymbol \\omega_k^T \\boldsymbol \\delta_k(\\boldsymbol x) + \\log(\\pi_k)\n\\end{split}\n\\]\n\n\\[\n\\boldsymbol \\omega_k = \\boldsymbol W(\\boldsymbol \\mu_k - \\boldsymbol \\mu_0)  \n\\]\n\n\\[\n\\boldsymbol \\delta_k(\\boldsymbol x) = \\boldsymbol W(\\boldsymbol x- \\frac{\\boldsymbol \\mu_k +\\boldsymbol \\mu_0}{2} )\n\\]\nexpected log Bayes factor \n\\[\n\\text{E}_{F_k} ( \\log B_{k0} )= KL(F_k || F_0) = \\frac{1}{2} (\\boldsymbol \\mu_k -\\boldsymbol \\mu_0)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_k -\\boldsymbol \\mu_0) = \\frac{1}{2} \\Omega_k^2\n\\]scale factor \\(c_k = (\\frac{1}{n_k} - \\frac{1}{n})^{-1/2} = \\sqrt{n \\frac{\\pi_k}{1-\\pi_k}}\\) (\\(\\text{SD}(\\hat{\\boldsymbol \\mu}_k-\\hat{\\boldsymbol \\mu}_0)\\), minus sign \\(\\frac{1}{n}\\) due correlation \n\\(\\hat{\\boldsymbol \\mu}_k\\) pooled mean \\(\\hat{\\boldsymbol \\mu}_0\\))\nget correlation-adjusted \\(t\\)-score comparing mean group \\(k\\) \npooled mean\n\\[\n\\boldsymbol \\tau_k^{adj} = c_k \\boldsymbol \\omega_k^{ZCA-cor} \\,.\n\\]two class case (\\(K=2\\)) get \n\\(\\boldsymbol \\mu_0 = \\pi_1 \\boldsymbol \\mu_1 + \\pi_2 \\boldsymbol \\mu_2\\) mean difference\n\\((\\boldsymbol \\mu_1 - \\boldsymbol \\mu_0) = \\pi_2 (\\boldsymbol \\mu_1 - \\boldsymbol \\mu_2)\\)\n\\(c_1 = \\sqrt{n \\frac{\\pi_1}{\\pi_2}}\\)\nyields\n\\[\n\\boldsymbol \\tau_1^{adj} = \\sqrt{n \\pi_1 \\pi_2 } \\boldsymbol P^{-1/2} \\boldsymbol V^{-1/2} (\\boldsymbol \\mu_1 - \\boldsymbol \\mu_2) , \n\\]\n.e. exact score two-class setting.","code":""},{"path":"supervised-learning-and-classification.html","id":"variable-selection-and-cross-validation","chapter":"4 Supervised learning and classification","heading":"4.6 Variable selection and cross-validation","text":"previous saw DDA natural score\nranking features regard relevance separating classes \n(squared) \\(t\\)-score, LDA whitened version \nsquared correlation-adjusted \\(t\\)-score (based ZCA-cor whitening) may used.\nranking established question suitable cutoff arises, .e. \nmany features need () retained model.large high-dimensional models feature selection can also viewed\nform regularisation also dimension reduction. Specifically, may many variables/ features contribute class prediction. Despite \nprinciple effect outcome presence “null variables”\ncan nonetheless deterioriate (sometimes dramatically!) overall predictive accuracy trained predictor, add noise increase model dimension. Therefore, variables contribute prediction\nfiltered order able construct good prediction models classifiers.","code":""},{"path":"supervised-learning-and-classification.html","id":"choosing-a-threshold-by-multiple-testing-using-false-discovery-rates","chapter":"4 Supervised learning and classification","heading":"4.6.1 Choosing a threshold by multiple testing using false discovery rates","text":"simple way determine cutoff threshold use standard technique \nmultiple testing.predictor variable \\(x_1, \\ldots, x_d\\) corresponding test statistic\nmeasuring influence variable response, example \n\\(t\\)-scores related statistics discussed previous section.\naddition providing overall ranking set statistics can used\ndetermine suitable cutoff trying separate two populations predictor variables:“Null” variables contribute prediction“Alternative” variables linked predictionAs discussed “Statistical Methods” module last term (Part 2 - Section 8) can done follows:distribution observed test statistics \\(z_i\\) assumed follow two-component mixture \\(F_0(z)\\) \\(F_A(z)\\) distributions corresponding null alternative, \\(f_0(z)\\) \\(f_a(z)\\) densities, \\(\\pi_0\\) \\(\\pi_A=1-\\pi_0\\) weights:\n\\[\nf(z) = \\pi_0 f_0(z) + (1-\\pi_0) f_a(z)\n\\]distribution observed test statistics \\(z_i\\) assumed follow two-component mixture \\(F_0(z)\\) \\(F_A(z)\\) distributions corresponding null alternative, \\(f_0(z)\\) \\(f_a(z)\\) densities, \\(\\pi_0\\) \\(\\pi_A=1-\\pi_0\\) weights:\n\\[\nf(z) = \\pi_0 f_0(z) + (1-\\pi_0) f_a(z)\n\\]null model typically parametric family (e.g. normal around zero \nfree variance parameter) whereas alternative often modelled nonparametrically.null model typically parametric family (e.g. normal around zero \nfree variance parameter) whereas alternative often modelled nonparametrically.fitting mixture model, often assuming additional constraints make mixture identifiable, one can compute false discovery rates (FDR) follows:\nLocal FDR:\n\\[\n\\widehat{fdr}(z_i) = \\hat{\\text{Pr}}(\\text{null} | z_i) = \\frac{\\hat{\\pi}_0 \\hat{f}_0(z_i)}{\\hat{f}(z_i)}  \n\\]\nTail-area-based FDR (=\\(q\\)-value):\n\\[\n\\widehat{Fdr}(z_i) = \\hat{\\text{Pr}}(\\text{null} | Z > z_i) = \\frac{\\hat{\\pi}_0 \\hat{F}_0(z_i)}{\\hat{F}(z_i)}\n\\]\nNote essentially \\(p\\)-values adjusted multiple testing (variant Benjamini-Hochberg method).fitting mixture model, often assuming additional constraints make mixture identifiable, one can compute false discovery rates (FDR) follows:Local FDR:\n\\[\n\\widehat{fdr}(z_i) = \\hat{\\text{Pr}}(\\text{null} | z_i) = \\frac{\\hat{\\pi}_0 \\hat{f}_0(z_i)}{\\hat{f}(z_i)}  \n\\]Tail-area-based FDR (=\\(q\\)-value):\n\\[\n\\widehat{Fdr}(z_i) = \\hat{\\text{Pr}}(\\text{null} | Z > z_i) = \\frac{\\hat{\\pi}_0 \\hat{F}_0(z_i)}{\\hat{F}(z_i)}\n\\]\nNote essentially \\(p\\)-values adjusted multiple testing (variant Benjamini-Hochberg method).thresholding false discovery rates possible identify \nvariables clearly belong two groups also features\neasily discriminated fall either group:“alternative” variables low local FDR, e.g, \\(\\widehat{fdr}(z_i) \\leq 0.2\\)“null” variables high local FDR, e.g. \\(\\widehat{fdr}(z_i) \\geq 0.8\\)features easily classified null alternative, e.g. \\(0.2 < \\widehat{fdr}(z_i) < 0.8\\)feature selection prediction settings generally aim remove \nvariable clearly belong null group, leaving others model.","code":""},{"path":"supervised-learning-and-classification.html","id":"quantifying-prediction-error","chapter":"4 Supervised learning and classification","heading":"4.6.2 Quantifying prediction error","text":"Another direct way compare models compare predictive performance\nquantification prediction error. Specifically, interested relative\nperformance models diverse sets predictor. variables.measure predictor error compares predicted label \\(\\hat{y}\\) true\nlabel \\(y\\) validation data set. validation data set contains \n\\(\\boldsymbol x_i\\) associated label \\(y_i\\) unlike training data \nused learning predictor function.continuous response often squared loss used:\n\\[\n\\text{err}(\\hat{y}, y) =  (\\hat{y} - y)^2\n\\]binary outcomes one often employs 0/1 loss:\n\\[\n\\text{err}(\\hat{y}, y) =\n\\begin{cases}\n    1, & \\text{ } \\hat{y}=y\\\\\n    0,  & \\text{otherwise}\n\\end{cases}\n\\]\nAlternatively, quantity derived confusion matrix\n(containing TP, TN, FP, FN) can used.mean prediction error expectation\n\\[\nPE = \\text{E}(\\text{err}(\\hat{y}, y))\n\\]\nthus empirical mean prediction error \n\\[\n\\widehat{PE} = \\frac{1}{m} \\sum_{=1}^m \\text{err}(\\hat{y}_i, y_i)\n \\]\n\\(m\\) sample size validation data set.generally, can also quantify prediction error framework -called proper scoring rules, whole probabilistic forecast taken account (e.g. individual probabilities class, rather just selected probable class). commonly used scoring rule negative log-probability (“surprise”), expected surprise cross-entropy (cf. Statistical Methods module). leads back entropy likelihood (see MATH20802 Statistical Methods).estimate prediction error model can use compare choose among set candidate models, selecting sufficiently low prediction\nerror.","code":""},{"path":"supervised-learning-and-classification.html","id":"estimation-of-prediction-error-without-validation-data-using-cross-validation","chapter":"4 Supervised learning and classification","heading":"4.6.3 Estimation of prediction error without validation data using cross-validation","text":"Unfortunately, quite often separate validation data available evaluate classifier.case need rely simple algorithmic procedure called cross-validation.Outline cross-validation:split samples training data number (say \\(K\\)) parts (“folds”).use \\(K\\) folds validation data \\(K-1\\) folds training data.average resulting \\(K\\) individual estimates prediction error, get overall aggregated predictor error, along error.Note case one part data reserved validation \nused training predictor.choose \\(K\\) folds small (allow estimation \nprediction error) also large (make sure actually able train reliable classifier remaining data). typical value \\(K\\) 5 10, 80% respectively 90% samples used training 20 %\n10% validation.\\(K=n\\) many folds samples validation data set consists single data point. called “leave one ” cross-validation (LOOCV). analytic approximations prediction error obtained LOOCV\napproach computationally inexpensive standard models (including regression).number worksheets cross-validation employed evaluate classification models\ndemonstrate practise feature selection useful construct compact models small number variables nonetheless generalise predict well.reading:study technical details cross-validation: read Section 5.1 Cross-Validation James et al. (2021) introduction statistical learning applications R (2nd edition). Springer.","code":""},{"path":"multivariate-dependencies.html","id":"multivariate-dependencies","chapter":"5 Multivariate dependencies","heading":"5 Multivariate dependencies","text":"","code":""},{"path":"multivariate-dependencies.html","id":"measuring-the-linear-association-between-two-sets-of-random-variables","chapter":"5 Multivariate dependencies","heading":"5.1 Measuring the linear association between two sets of random variables","text":"","code":""},{"path":"multivariate-dependencies.html","id":"outline","chapter":"5 Multivariate dependencies","heading":"5.1.1 Outline","text":"section discuss measure total linear association two sets \nrandom variables \\(\\boldsymbol x= (x_1, \\ldots, x_p)^T\\) \n\\(\\boldsymbol y= (y_1, \\ldots, y_q)^T\\). assume joint correlation matrix\n\\[\n\\boldsymbol P= \n\\begin{pmatrix} \n\\boldsymbol P_{\\boldsymbol x} &  \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol P_{\\boldsymbol y\\boldsymbol x} & \\boldsymbol P_{\\boldsymbol y} \\\\\n\\end{pmatrix} \n\\]\ncross-correlation matrix \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = \\boldsymbol P_{\\boldsymbol y\\boldsymbol x}^T\\)\nwithin-group group correlations \\(\\boldsymbol P_{\\boldsymbol x}\\) \\(\\boldsymbol P_{\\boldsymbol y}\\).\ncross-correlations vanish, \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} =0\\), \ntwo random vectors uncorrelated, joint correlation matrix\nbecomes diagonal block matrix\n\\[\n\\boldsymbol P_{\\text{indep}} = \n\\begin{pmatrix} \n\\boldsymbol P_{\\boldsymbol x} &  0 \\\\\n0 & \\boldsymbol P_{\\boldsymbol y} \\\\\n\\end{pmatrix} \\, .\n\\]","code":""},{"path":"multivariate-dependencies.html","id":"special-cases","chapter":"5 Multivariate dependencies","heading":"5.1.2 Special cases","text":"linear regression model \nsquared multiple correlation coefficient determination\n\\[\n\\text{Cor}(\\boldsymbol x, y)^2 = \\boldsymbol P_{y\\boldsymbol x} \\boldsymbol P_{\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\n\\]\nstandard measure describe strength total linear association \npredictors \\(\\boldsymbol x\\) response \\(y\\). \\(\\boldsymbol P_{\\boldsymbol xy} =0\\) \\(\\text{Cor}(\\boldsymbol x, y)^2=0\\).single predictor \\(x\\) \\(\\boldsymbol P_{xy}=\\rho\\) \\(\\boldsymbol P_{x} = 1\\)\ntherefore squared multiple correlation reduces squared Pearson correlation\n\\[\n\\text{Cor}(x, y)^2 = \\rho^2 \\, .\n\\]","code":""},{"path":"multivariate-dependencies.html","id":"rozeboom-vector-correlation","chapter":"5 Multivariate dependencies","heading":"5.1.3 Rozeboom vector correlation","text":"general case two random vectors looking scalar\nquantity quantifies divergence general joint correlation matrix \\(\\boldsymbol P\\)\njoint correlation matrix \\(\\boldsymbol P_{\\text{indep}}\\) assuming uncorrelated\n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\).One quantity Hotelling’s vector alienation\ncoefficient (given 1936 CCA paper13)\n\\[\n\\begin{split}\n(\\boldsymbol x, \\boldsymbol y) &= \\frac{\\det(\\boldsymbol P)}{\\det(\\boldsymbol P_{\\text{indep}}) } \\\\\n            & = \\frac{\\det( \\boldsymbol P) }{  \\det(\\boldsymbol P_{\\boldsymbol x}) \\,  \\det(\\boldsymbol P_{\\boldsymbol y})  }\n\\end{split}\n\\]\n\\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\boldsymbol P_{\\boldsymbol y}^{-1/2}\\) (cf. Chapter 2, Canonical correlation analysis) vector alienation coefficient can written\n(using Weinstein-Aronszajn determinant identity formula determinant\nblock-structured matrices, see Appendix) \n\\[\n\\begin{split}\n(\\boldsymbol x, \\boldsymbol y) & = \\det \\left( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T \\right) \\\\\n            & = \\det \\left( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K\\right) \\\\\n            &= \\prod_{=1}^m (1-\\lambda_i^2) \\\\\n\\end{split}\n\\]\n\\(\\lambda_i\\) singular values \\(\\boldsymbol K\\), .e. canonical correlations\npair \\(\\boldsymbol x\\) \\(\\boldsymbol y\\).\\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = 0\\) und thus \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) uncorrelated \\(\\boldsymbol P= \\boldsymbol P_{\\text{indep}}\\) \nthus construction vector alienation coefficient \\((\\boldsymbol x, \\boldsymbol y)=1\\).\nHence, generalisation squared multiple correlation.Instead, Rozeboom (1965) proposed use squared vector correlation\ncomplement\n\\[\n\\begin{split}\n\\text{Cor}(\\boldsymbol x, \\boldsymbol y)^2 &= \\rho^2_{\\boldsymbol x\\boldsymbol y} = 1 - (\\boldsymbol x, \\boldsymbol y) \\\\\n & = 1- \\det\\left( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T \\right) \\\\\n & = 1- \\det\\left( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K\\right) \\\\\n &  =1- \\prod_{=1}^m (1-\\lambda_i^2) \\\\\n\\end{split}\n\\]\n\\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol y} = 0\\) \\(\\text{Cor}(\\boldsymbol x, \\boldsymbol y)^2 = 0\\).\nMoreover, either \\(p=1\\) \\(q=1\\) squared vector correlation\nreduces corresponding squared multiple correlation,\n\\(p=1\\) \\(q=1\\) becomes squared Pearson correlation.general way measure multivariate association mutual information (MI)\ncovers linear also non-linear association.\ndiscuss subsequent section Rozeboom vector\ncorrelation arises naturally MI\n\nmultivariate normal distribution.","code":""},{"path":"multivariate-dependencies.html","id":"rv-coefficient","chapter":"5 Multivariate dependencies","heading":"5.1.4 RV coefficient","text":"Another common approach measure association two random vectors RV coefficient introduced Robert Escoufier (1976)\n\n\\[\nRV(\\boldsymbol x, \\boldsymbol y) = \\frac{ \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} )}{ \\sqrt{ \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol x}^2) \\text{Tr}(\\boldsymbol \\Sigma_{\\boldsymbol y}^2)  } }\n\\]\neasier compute Rozeboom vector correlation since based\nmatrix trace rather matrix determinant.\\(q=p=1\\) RV coefficient reduces squared correlation.\nHowever, RV coefficient reduce multiple correlation coefficient\n\\(q=1\\) \\(p > 1\\), therefore RV coefficient considered coherent generalisation\nPearson multiple correlation case two random vectors.","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-as-generalisation-of-correlation","chapter":"5 Multivariate dependencies","heading":"5.2 Mutual information as generalisation of correlation","text":"","code":""},{"path":"multivariate-dependencies.html","id":"definition-of-mutual-information","chapter":"5 Multivariate dependencies","heading":"5.2.1 Definition of mutual information","text":"Recall definition\nKullback-Leibler divergence, relative entropy, two distributions:\n\\[\nD_{\\text{KL}}(F,  G) := \\text{E}_F \\log \\biggl( \\frac{f(\\boldsymbol x)}{g(\\boldsymbol x)} \\biggr) \n\\]\n\\(F\\) plays role reference distribution \\(G\\) approximating distribution,\n\\(f\\) \\(g\\) corresponding density functions\n(see MATH20802 Statistical Methods).Mutual Information (MI) two random variables \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) defined \nKL divergence corresponding joint distribution product distribution:\n\\[\n\\text{MI}(\\boldsymbol x, \\boldsymbol y) = D_{\\text{KL}}(F_{\\boldsymbol x,\\boldsymbol y}, F_{\\boldsymbol x}  F_{\\boldsymbol y}) = \\text{E}_{F_{\\boldsymbol x,\\boldsymbol y}}  \\log \\biggl( \\frac{f(\\boldsymbol x, \\boldsymbol y)}{f(\\boldsymbol x) \\, f(\\boldsymbol y)} \\biggr) .\n\\]\nThus, MI measures well joint distribution can approximated product\ndistribution (appropriate joint distribution \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) independent).\nSince MI application KL divergence shares properties. particular,\n\\(\\text{MI}(\\boldsymbol x, \\boldsymbol y)=0\\) implies joint distribution product distributions . Hence two random variables \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) independent mutual information vanishes.","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-between-two-normal-variables","chapter":"5 Multivariate dependencies","heading":"5.2.2 Mutual information between two normal variables","text":"KL divergence two multivariate normal distributions \\(F_{\\text{ref}}\\) \\(F\\) \n\\[\nD_{\\text{KL}}(F_{\\text{ref}}, F)  = \\frac{1}{2}   \\biggl\\{\n        (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu-\\boldsymbol \\mu_{\\text{ref}})\n      + \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1} \\boldsymbol \\Sigma_{\\text{ref}} \\biggr) \n     - d   \\biggr\\} \n\\]\nallows compute mutual information \\(\\text{MI}_{\\text{norm}}(x,y)\\) two univariate random variables \\(x\\) \\(y\\) correlated assumed jointly bivariate normal. Let \\(\\boldsymbol z= (x, y)^T\\). joint bivariate normal distribution characterised mean \\(\\text{E}(\\boldsymbol z) = \\boldsymbol \\mu= (\\mu_x, \\mu_y)^T\\) covariance matrix\n\\[\n\\boldsymbol \\Sigma=\n\\begin{pmatrix} \n\\sigma^2_x & \\rho \\, \\sigma_x \\sigma_y \\\\\n\\rho \\, \\sigma_x  \\sigma_y & \\sigma^2_y \\\\ \n\\end{pmatrix}\n\\]\n\\(\\text{Cor}(x,y)= \\rho\\). \\(x\\) \\(y\\) independent \n\\(\\rho=0\\) \n\\[\n\\boldsymbol \\Sigma_{\\text{indep}} = \n\\begin{pmatrix} \\sigma^2_x & 0 \\\\ 0 & \\sigma^2_y \\\\ \\end{pmatrix} \\,.\n\\]\nproduct\n\\[\n\\boldsymbol = \\boldsymbol \\Sigma_{\\text{indep}}^{-1} \\boldsymbol \\Sigma= \n\\begin{pmatrix}\n1 & \\rho \\frac{\\sigma_y}{\\sigma_x} \\\\\n\\rho \\frac{\\sigma_x}{\\sigma_y} & 1 \\\\\n\\end{pmatrix}\n\\]\ntrace \\(\\text{Tr}(\\boldsymbol ) = 2\\) determinant \\(\\det(\\boldsymbol ) = 1-\\rho^2\\).mutual information \\(x\\) \\(y\\) can computed \n\\[\n\\begin{split}\n\\text{MI}_{\\text{norm}}(x, y) &= D_{\\text{KL}}(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma),  N(\\boldsymbol \\mu,\\boldsymbol \\Sigma_{\\text{indep}} )) \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}\\biggl(\\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr) \n     - 2   \\biggr\\} \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}( \\boldsymbol )\n    - \\log \\det( \\boldsymbol ) \n     - 2   \\biggr\\} \\\\\n&=  -\\frac{1}{2} \\log(1-\\rho^2) \\\\\n  & \\approx \\frac{\\rho^2}{2} \\\\\n\\end{split}\n\\]Thus \\(\\text{MI}_{\\text{norm}}(x,y)\\) one--one function squared correlation \\(\\rho^2\\) \\(x\\) \\(y\\):small values correlation \\(2 \\, \\text{MI}_{\\text{norm}}(x,y) \\approx \\rho^2\\).","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-between-two-normally-distributed-random-vectors","chapter":"5 Multivariate dependencies","heading":"5.2.3 Mutual information between two normally distributed random vectors","text":"mutual information \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x,\\boldsymbol y)\\) two multivariate normal random vector \\(\\boldsymbol x\\) \\(\\boldsymbol y\\)\ncan computed similar fashion bivariate case.Let \\(\\boldsymbol z= (\\boldsymbol x, \\boldsymbol y)^T\\) dimension \\(d=p+q\\). joint multivariate\nnormal distribution characterised mean \\(\\text{E}(\\boldsymbol z) = \\boldsymbol \\mu= (\\boldsymbol \\mu_x^T, \\boldsymbol \\mu_y^T)^T\\) covariance matrix\n\\[\n\\boldsymbol \\Sigma=\n\\begin{pmatrix} \\boldsymbol \\Sigma_{\\boldsymbol x} & \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\\\ \n\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}^T & \\boldsymbol \\Sigma_{\\boldsymbol y} \\\\ \n\\end{pmatrix} \\,.\n\\]\n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\) independent \n\\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} = 0\\) \n\\[\n\\boldsymbol \\Sigma_{\\text{indep}} =\n\\begin{pmatrix}  \n\\boldsymbol \\Sigma_{\\boldsymbol x} & 0 \\\\ \n0 & \\boldsymbol \\Sigma_{\\boldsymbol y} \\\\ \n\\end{pmatrix} \\, .\n\\]\nproduct\n\\[\n\\begin{split}\n\\boldsymbol & = \n\\boldsymbol \\Sigma_{\\text{indep}}^{-1} \\boldsymbol \\Sigma= \n\\begin{pmatrix}\n\\boldsymbol I_p & \\boldsymbol \\Sigma_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\\\\n\\boldsymbol \\Sigma_{\\boldsymbol y}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x}    & \\boldsymbol I_q \\\\\n\\end{pmatrix} \\\\\n& = \n\\begin{pmatrix}\n\\boldsymbol I_p &  \\boldsymbol V_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\boldsymbol V_{\\boldsymbol y}^{1/2} \\\\\n\\boldsymbol V_{\\boldsymbol y}^{-1/2} \\boldsymbol P_{\\boldsymbol y}^{-1} \\boldsymbol P_{\\boldsymbol y\\boldsymbol x} \\boldsymbol V_{\\boldsymbol x}^{1/2}   & \\boldsymbol I_q \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\]\ntrace \\(\\text{Tr}(\\boldsymbol ) = d\\) determinant\n\\[\n\\begin{split}\n\\det(\\boldsymbol ) & = \\det( \\boldsymbol I_p - \\boldsymbol K\\boldsymbol K^T ) \\\\\n  &= \\det( \\boldsymbol I_q - \\boldsymbol K^T \\boldsymbol K) \\\\\n\\end{split}\n\\]\n\\(\\boldsymbol K= \\boldsymbol P_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol y} \\boldsymbol P_{\\boldsymbol y}^{-1/2}\\).\n\\(\\lambda_1, \\ldots, \\lambda_m\\) singular values \\(\\boldsymbol K\\) (.e.\ncanonical correlations \\(\\boldsymbol x\\) \\(\\boldsymbol y\\)) get\n\\[\n\\det(\\boldsymbol ) =  \\prod_{=1}^m (1-\\lambda_i^2)\n\\]mutual information \\(\\boldsymbol x\\) \\(\\boldsymbol y\\) \n\\[\n\\begin{split}\n\\text{MI}_{\\text{norm}}(\\boldsymbol x, \\boldsymbol y) &= D_{\\text{KL}}(N(\\boldsymbol \\mu, \\boldsymbol \\Sigma),  N(\\boldsymbol \\mu,\\boldsymbol \\Sigma_{\\text{indep}} )) \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}\\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr)\n    - \\log \\det \\biggl( \\boldsymbol \\Sigma^{-1}_{\\text{indep}} \\boldsymbol \\Sigma\\biggr) \n     - d   \\biggr\\} \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\text{Tr}( \\boldsymbol )\n    - \\log \\det( \\boldsymbol ) \n     - d   \\biggr\\} \\\\\n&=-\\frac{1}{2} \\sum_{=1}^m \\log(1-\\lambda_i^2)\\\\\n\\end{split}\n\\]Note \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x, \\boldsymbol y)\\) sum MIs resulting \nindividual canonical correlations \\(\\lambda_i\\) functional\nform bivariate normal case.comparison squared Rozeboom vector correlation coefficient \\(\\rho^2_{\\boldsymbol x\\boldsymbol y}\\)\nrecognize \n\\[\n\\text{MI}_{\\text{norm}}(\\boldsymbol x,\\boldsymbol y) = -\\frac{1}{2} \\log(1 - \\rho^2_{\\boldsymbol x\\boldsymbol y} ) \\approx \\frac{1}{2} \\rho^2_{\\boldsymbol x\\boldsymbol y}\n\\]\nThus, multivariate case \\(\\text{MI}_{\\text{norm}}(\\boldsymbol x\\boldsymbol y)\\) exactly functional relationship \nvector correlation \\(\\rho^2_{\\boldsymbol x, \\boldsymbol y}\\) \\(\\text{MI}_{\\text{norm}}(x, y)\\)\ntwo univariate variables squared Pearson correlation \\(\\rho^2\\).Thus, Rozebooms vector correlation directly linked mutual information jointly multivariate normally distributed variables.","code":""},{"path":"multivariate-dependencies.html","id":"using-mi-for-variable-selection","chapter":"5 Multivariate dependencies","heading":"5.2.4 Using MI for variable selection","text":"principle, MI can computed distribution model thus applies normal non-normal models, linear nonlinear relationships.general way may denote \\(F_{\\boldsymbol y|\\boldsymbol x}\\) denote predictive model \\(\\boldsymbol y\\) conditioned \\(\\boldsymbol x\\) \\(F_{\\boldsymbol y}\\) marginal distribution \\(\\boldsymbol y\\) without predictors. Note predictive model can assume form (incl. nonlinear). Typically \\(F_{\\boldsymbol y|\\boldsymbol x}\\) complex model \\(F_{\\boldsymbol y}\\)\nsimple model (predictors).mutual information \n\\(\\boldsymbol x\\) \\(\\boldsymbol y\\) can also understood expectated KL divergence\nconditional marginal distributions:\n\\[\n\\text{E}_{F_{\\boldsymbol x}}\\, D_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x},  F_{\\boldsymbol y} ) = \\text{MI}(\\boldsymbol x, \\boldsymbol y)\n\\]can shown follows.\nKL divergence \\(F_{\\boldsymbol y|\\boldsymbol x}\\) \\(F_{\\boldsymbol y}\\)\ngiven \n\\[\nD_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x}, F_{\\boldsymbol y} )  = \\text{E}_{F_{\\boldsymbol y|\\boldsymbol x}}  \\log\\biggl( \\frac{f(\\boldsymbol y|\\boldsymbol x) }{ f(\\boldsymbol y)}  \\biggr) \\, , \n\\]\nrandom variable since depends \\(\\boldsymbol x\\).\nTaking expectation regard \\(F_{\\boldsymbol x}\\) (distribution \\(\\boldsymbol x\\))\nget\n\\[\n\\text{E}_{F_{\\boldsymbol x}} D_{\\text{KL}}(F_{\\boldsymbol y|\\boldsymbol x}, F_{\\boldsymbol y} ) = \n\\text{E}_{F_{\\boldsymbol x}}  \\text{E}_{F_{\\boldsymbol y|\\boldsymbol x}}  \\log \\biggl(\\frac{ f(\\boldsymbol y|\\boldsymbol x) f(\\boldsymbol x) }{ f(\\boldsymbol y) f(\\boldsymbol x) } \\biggr) = \n\\text{E}_{F_{\\boldsymbol x,\\boldsymbol y}}   \\log \\biggl(\\frac{ f(\\boldsymbol x,\\boldsymbol y) }{ f(\\boldsymbol y) f(\\boldsymbol x) } \\biggr) = \\text{MI}(\\boldsymbol x,\\boldsymbol y) \\,.\n\\]link MI conditioning MI response predictor variables often used variable feature selection general models.","code":""},{"path":"multivariate-dependencies.html","id":"other-measures-of-general-dependence","chapter":"5 Multivariate dependencies","heading":"5.2.5 Other measures of general dependence","text":"Besides mutual information others measures general dependence multivariate random variables.two important ones proposed recent literature ) distance correlation\nii) maximal information coefficient (MIC \\(\\text{MIC}_e\\)).","code":""},{"path":"multivariate-dependencies.html","id":"graphical-models","chapter":"5 Multivariate dependencies","heading":"5.3 Graphical models","text":"","code":""},{"path":"multivariate-dependencies.html","id":"purpose","chapter":"5 Multivariate dependencies","heading":"5.3.1 Purpose","text":"Graphical models combine features fromgraph theoryprobabilitystatistical inferenceThe literature graphical models huge, focus two commonly\nused models:DAGs (directed acyclic graphs), edges directed, directed loops (.e. cycles, hence “acyclic”)GGM (Gaussian graphical models), edges undirectedGraphical models provide probabilistic models trees networks, \nrandom variables represented nodes graphs, branches representing\nconditional dependencies. regard generalise tree-based clustering approaches well probabilistic non-hierarchical methods (GMMs).However, class graphical models goes much beyond simple\nunsupervised learning models. also includes regression, classification,\ntime series models etc. See e.g. reference book Murphy (2012).","code":""},{"path":"multivariate-dependencies.html","id":"basic-notions-from-graph-theory","chapter":"5 Multivariate dependencies","heading":"5.3.2 Basic notions from graph theory","text":"Mathematically, graph \\(G = (V, E)\\) consists set vertices nodes \\(V = \\{v_1, v_2, \\ldots\\}\\) set branches edges \\(E = \\{ e_1, e_2, \\ldots \\}\\).Edges can undirected directed.Graphs containing directed edges directed graphs, likewise graphs containing undirected edges called undirected graphs. Graphs containing directed undirected edges called partially directed graphs.path sequence vertices vertices edge next vertex sequence.graph connected path every pair vertices.cycle path graph connects node .connected graph cycles called tree.degree node number edges connects . edges directed degree node sum -degree -degree, counts incoming outgoing edges, respectively.External nodes nodes degree 1. tree-structed graph also called leafs.notions relevant graphs directed edges:directed graph parent node(s) vertex \\(v\\) set nodes \\(\\text{pa}(v)\\) directly connected \\(v\\) via edges directed parent node(s) towards \\(v\\).Conversely, \\(v\\) called child node \\(\\text{pa}(v)\\). Note parent node can several child nodes, \\(v\\) may child \\(\\text{pa}(v)\\).directed tree graph, node single parent, except one particular node parent (node called root node).DAG, directed acyclic graph, directed graph directed cycles. (directed) tree special version DAG.","code":""},{"path":"multivariate-dependencies.html","id":"probabilistic-graphical-models","chapter":"5 Multivariate dependencies","heading":"5.3.3 Probabilistic graphical models","text":"graphical model uses graph describe relationship random variables \\(x_1, \\ldots, x_d\\). variables assumed joint distribution density/mass function \\(\\text{Pr}(x_1, x_2, \\ldots, x_d)\\).\nrandom variable placed node graph.structure graph type edges connecting (connecting) pair nodes/variables used describe conditional dependencies, simplify joint distribution.Thus, graphical model essence visualisation joint distribution using structural information graph helping understand mutual relationship among variables.","code":""},{"path":"multivariate-dependencies.html","id":"directed-graphical-models","chapter":"5 Multivariate dependencies","heading":"5.3.4 Directed graphical models","text":"directed graphical model graph structure assumed \nDAG (directed tree, also DAG).joint probability distribution can factorised product conditional probabilities follows:\n\\[\n\\text{Pr}(x_1, x_2, \\ldots, x_d) = \\prod_i \\text{Pr}(x_i  | \\text{pa}(x_i))\n\\]\nThus, overall joint probability distribution specified local conditional distributions graph structure, directions edges providing information parent-child node relationships.Probabilistic DAGs also known “Bayesian networks”.Idea: trying possible trees/graphs fitting data using maximum likelihood (Bayesian inference) hope able identify graph structure data-generating process.Challengesin tree/network internal nodes usually known, thus \ntreated latent variables.Answer: impute states nodes may use EM algorithm GMMs\n(fact can viewed graphical models, !).treat internal nodes unknowns need marginalise \ninternal nodes, .e. need sum / integrate possible set states\ninternal nodes!Answer: can handled effectively using Viterbi algorithm essentially\napplication generalised distributive law. particular tree graphs \nmeans summations occurs locally nodes propagates recursively accross tree.order infer tree network structure space trees networks need \nexplored. possible exhaustive fashion unless number variables\ntree small.Answer: Solution: use heuristic approaches tree network search!Furthermore, exist -called “equivalence classes” graphical models, .e. sets graphical models share joint probability distribution. Thus, graphical models within equivalence class distinguished observational data, even infinite sample size!Answer: fundamental mathematical problem identifiability now way around issue. However,\npositive side, also implies search graphical models can restricted finding -called “essential graph” (e.g. https://projecteuclid.org/euclid.aos/1031833662 )Conclusion: using directed graphical models structure discovery time consuming computationally\ndemanding anything small toy data sets.also explains heuristic non-model based approaches (hierarchical clustering) popular even though full statistical modelling principle possible.","code":""},{"path":"multivariate-dependencies.html","id":"undirected-graphical-models","chapter":"5 Multivariate dependencies","heading":"5.3.5 Undirected graphical models","text":"Another class graphical models models contain undirected edges. undirected graphical models\nused represent pairwise conditional ()dependencies among variables graph, resulting model therefore also called conditional independence graph.\\(x_i\\) \\(x_j\\) two selected random variables/nodes, set \\(\\{x_k\\}\\) represents variables/nodes \\(k\\neq \\) \\(k \\neq j\\). say variables \\(x_i\\) \\(x_j\\) conditionally independent\ngiven variables \\(\\{x_k\\}\\)\n\\[\nx_i \\perp\\!\\!\\!\\perp x_j | \\{x_k\\}\n\\]\njoint probability density \\(x_i, x_j\\) \\(x_k\\)\nfactorises \n\\[\n \\text{Pr}(x_1, x_2, \\ldots, x_d) = \\text{Pr}(x_i | \\{x_k\\}) \\text{Pr}(x_j | \\{x_k\\}) \\text{Pr}(\\{x_k\\}) \\,.\n \\]\nequivalently\n\\[\n \\text{Pr}(x_i, x_j | \\{x_k\\}) = \\text{Pr}(x_i | \\{x_k\\}) \\text{Pr}(x_j | \\{x_k\\}) \\,.\n \\]corresponding conditional independence graph, edge \\(x_i\\) \\(x_j\\),\ngraph missing edges correspond conditional independence respective non-connected nodes.","code":""},{"path":"multivariate-dependencies.html","id":"gaussian-graphical-model","chapter":"5 Multivariate dependencies","heading":"5.3.5.1 Gaussian graphical model","text":"Assuming \\(x_1, \\ldots, x_d\\) jointly normally distributed, .e. \\(\\boldsymbol x\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\),\nturns straightforward identify pairwise conditional independencies.\n\\(\\boldsymbol \\Sigma\\) first obtain precision matrix\n\\[\\boldsymbol \\Omega= (\\omega_{ij}) = \\boldsymbol \\Sigma^{-1} \\,.\\]\nCrucially, can shown \n\\(\\omega_{ij} = 0\\) implies\n\\(x_i \\perp\\!\\!\\!\\perp x_j | \\{ x_k \\}\\)!\nHence, precision matrix \\(\\boldsymbol \\Omega\\) can directly read pairwise conditional independencies among variables \\(x_1, x_2, \\ldots, x_d\\)!Often, covariance matrix \\(\\boldsymbol \\Sigma\\) dense (zeros) corresponding precision matrix\n\\(\\boldsymbol \\Omega\\) sparse (many zeros).conditional independence graph computed normally distributed variables called\nGaussian graphical model, GGM. alternative name\ncovariance selection model.","code":""},{"path":"multivariate-dependencies.html","id":"related-quantity-partial-correlation","chapter":"5 Multivariate dependencies","heading":"5.3.5.2 Related quantity: partial correlation","text":"precision matrix \\(\\boldsymbol \\Omega\\) can also compute matrix pairwise full conditional partial correlations:\\[\n\\rho_{ij|\\text{rest}}=-\\frac{\\omega_{ij}}{\\sqrt{\\omega_{ii}\\omega_{jj}}}\n\\]\nessentially standardised precision matrix (similar correlation extra minus sign!)partial correlations lie range -1 +1, \\(\\rho_{ij|\\text{rest}} \\[-1, 1]\\), just like standard correlations.\\(\\boldsymbol x\\) multivariate normal \\(\\rho_{ij|\\text{rest}} = 0\\) indicates conditional independence\n\\(x_i\\) \\(x_j\\).Regression interpretation: partial correlation correlation remains \ntwo variables effect variables “regressed away”.\nwords, partial correlation exactly equivalent correlation \nresiduals remain regressing \\(x_i\\) variables \\(\\{x_k\\}\\) \\(x_j\\) \\(\\{x_k\\}\\).","code":""},{"path":"multivariate-dependencies.html","id":"algorithm-for-learning-ggms","chapter":"5 Multivariate dependencies","heading":"5.3.6 Algorithm for learning GGMs","text":"can devise simple algorithm learn Gaussian graphical model (GGM)\ndata:Estimate covariance \\(\\hat{\\boldsymbol \\Sigma}\\) (way invertible!)Compute corresponding partial correlationsIf \\(\\hat{\\rho}_{ij|\\text{rest}} \\approx 0\\) (approx). conditional\nindependence \\(x_i\\) \\(x_j\\).\npractise done statistical testing vanishing partial correlations. many edges also need\nadjustment simultaneous multiple testing since edges tested parallel.","code":""},{"path":"multivariate-dependencies.html","id":"example-exam-score-data-mardia-et-al-1979","chapter":"5 Multivariate dependencies","heading":"5.3.7 Example: exam score data (Mardia et al 1979:)","text":"Correlations (rounded 2 digits):Partial correlations (rounded 2 digits):Note zero correlations \nfour partial correlations close 0, indicating conditional independence :analysis mechanics,statistics mechanics,analysis vectors, andstatistics vectors.Thus, 10 possible edges four missing, thus\nconditional independence graph looks follows:","code":"##            mechanics vectors algebra analysis statistics\n## mechanics       1.00    0.55    0.55     0.41       0.39\n## vectors         0.55    1.00    0.61     0.49       0.44\n## algebra         0.55    0.61    1.00     0.71       0.66\n## analysis        0.41    0.49    0.71     1.00       0.61\n## statistics      0.39    0.44    0.66     0.61       1.00##            mechanics vectors algebra analysis statistics\n## mechanics       1.00    0.33    0.23     0.00       0.02\n## vectors         0.33    1.00    0.28     0.08       0.02\n## algebra         0.23    0.28    1.00     0.43       0.36\n## analysis        0.00    0.08    0.43     1.00       0.25\n## statistics      0.02    0.02    0.36     0.25       1.00Mechanics      Analysis\n   |     \\    /    |\n   |    Algebra    |\n   |     /   \\     |\n Vectors      Statistics"},{"path":"nonlinear-and-nonparametric-models.html","id":"nonlinear-and-nonparametric-models","chapter":"6 Nonlinear and nonparametric models","heading":"6 Nonlinear and nonparametric models","text":"last part module discuss methods go beyond \nlinear methods prevalent classical multivariate statistics.Relevant textbooks:lectures much part module follow selected chapters following three text books:James et al. (2021) introduction statistical learning applications R (2nd edition). Springer.James et al. (2021) introduction statistical learning applications R (2nd edition). Springer.Rogers Girolami (2017) first course machine learning (2nd edition). CRC Press.Rogers Girolami (2017) first course machine learning (2nd edition). CRC Press.Please study relevant section chapters indicated subsection!","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"limits-of-linear-models-and-correlation","chapter":"6 Nonlinear and nonparametric models","heading":"6.1 Limits of linear models and correlation","text":"Linear models effective tools. However, important\nrecognise limits especially modelling complex nonlinear relationships.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"correlation-only-measures-linear-dependence","chapter":"6 Nonlinear and nonparametric models","heading":"6.1.1 Correlation only measures linear dependence","text":"simple demonstration given following example. Assume \\(x\\) normally distributed\nrandom variable \\(x \\sim N(0,1)\\). \\(x\\) construct second random variable \\(y = x^2\\) — thus \\(y\\) fully depends \\(x\\) added extra noise. correlation \\(x\\) \\(y\\)?Let’s ansers question running small computer simulation:Thus, correlation (almost) zero even though \\(x\\) \\(y\\) fully dependent!\ncorrelation measures linear association!","code":"\nx=rnorm(10000)\ny = x^2\ncor(x,y)## [1] 0.02659249"},{"path":"nonlinear-and-nonparametric-models.html","id":"anscombe-data-sets","chapter":"6 Nonlinear and nonparametric models","heading":"6.1.2 Anscombe data sets","text":"Using correlation, generally linear models, blindly can thus hide underlying complexity analysed\ndata. demonstrated classic “Anscombe quartet” data sets\n(F. J. Anscombe. 1973. Graphs statistical analysis.\nAmerican Statistician 27:17-21, http://dx.doi.org/10.1080/00031305.1973.10478966 ):evident scatter plots relationship \ntwo variables \\(x\\) \\(y\\) different four cases!\nHowever, intriguingly four data sets share exactly linear characteristics summary statistics:Means \\(m_x = 9\\) \\(m_y = 7.5\\)Variances \\(s^2_x = 11\\) \\(s^2_y = 4.13\\)Correlation \\(r = 0.8162\\)Linear model fit intercept \\(=3.0\\) slope \\(b=0.5\\)Thus, actual data analysis always good idea inspect data visually get first impression whether using linear model makes sense.data “” follows linear model. Data “b” represents quadratic relationship. Data “c” linear outlier disturbs linear relationship. Finally data “d” also contains outlier also represent case \\(y\\) (apart outlier) dependent \\(x\\).Worksheet 10 recent version Anscombe quartet analysed form “datasauRus” dozen - 13 highly nonlinear datasets share \nlinear characteristics.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"alternatives","chapter":"6 Nonlinear and nonparametric models","heading":"6.1.3 Alternatives","text":"Mutual information (based relative entropy) discussed previous chapterOther measures designed capture nonlinear association, distance correlation.\nii) maximal information coefficient (MIC \\(\\text{MIC}_e\\)).","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"random-forests","chapter":"6 Nonlinear and nonparametric models","heading":"6.2 Random forests","text":"Another widely used approach prediction nonlinear settings\nmethod random forests.Relevant reading:Please read: James et al. (2013) Chapter 8 “Tree-Based Methods”Specifically:Section 8.1 Basics Decision TreesSection 8.2.1 BaggingSection 8.2.2 Random Forests","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"stochastic-vs.-algorithmic-models","chapter":"6 Nonlinear and nonparametric models","heading":"6.2.1 Stochastic vs. algorithmic models","text":"Two cultures statistical modelling: stochastic vs. algorithmic modelsClassic discussion paper Leo Breiman (2001): Statistical modeling: two cultures.\nStatistical Science 16:199–231. https://projecteuclid.org/euclid.ss/1009213726This paper recently revisited following discussion paper Efron (2020) discussants:\nPrediction, estimation, attribution. JASA 115:636–677. https://doi.org/10.1080/01621459.2020.1762613","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"random-forests-1","chapter":"6 Nonlinear and nonparametric models","heading":"6.2.2 Random forests","text":"Invented Breimann 1996.Basic idea:single decision tree unreliable unstable (weak predictor/classifier).Use boostrap generate multiple decision trees (=“forest”)Average predictions tree (=“bagging”, bootstrap aggregation)averaging procedure effect variance stabilisation.\nIntringuingly, averaging across decision trees dramatically improves \noverall prediction accuracy!Random Forests approach example ensemble method\n(since based using “ensemble” trees).Variations: boosting, XGBoost ( https://xgboost.ai/ )Random forests applied Worksheet 10.computationally expensive typically perform well!","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"comparison-of-decision-boundaries-decision-tree-vs.-random-forest","chapter":"6 Nonlinear and nonparametric models","heading":"6.2.3 Comparison of decision boundaries: decision tree vs. random forest","text":"Non-nested case:Compare also decision boundaries LDA QDA (previous chapter).","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gaussian-processes","chapter":"6 Nonlinear and nonparametric models","heading":"6.3 Gaussian processes","text":"Gaussian processes offer another nonparametric approach model\nnonlinear dependencies. provide probabilistic model \nunknown nonlinear function.Relevant reading:Please read: Rogers Girolami (2017) Chapter 8: Gaussian processes.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"main-concepts","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.1 Main concepts","text":"Gaussian processes (GPs) belong family Bayesian nonparametric modelsIdea:\nstart prior function (!),\ncondition observed data get posterior distribution (function)\nstart prior function (!),condition observed data get posterior distribution (function)GPs use infinitely dimensional multivariate normal distribution prior","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"conditional-multivariate-normal-distribution","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.2 Conditional multivariate normal distribution","text":"GPs make use fact marginal conditional distributions multivariate normal\ndistribution also multivariate normal.Multivariate normal distribution:\\[\\boldsymbol z\\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\]Assume:\n\\[\n\\boldsymbol z=\\begin{pmatrix}\n    \\boldsymbol z_1      \\\\\n    \\boldsymbol z_2      \\\\\n\\end{pmatrix}\n\\]\n\n\\[\n\\boldsymbol \\mu=\\begin{pmatrix}\n    \\boldsymbol \\mu_1      \\\\\n    \\boldsymbol \\mu_2      \\\\\n\\end{pmatrix}\n\\]\n\n\\[\n\\boldsymbol \\Sigma=\\begin{pmatrix}\n    \\boldsymbol \\Sigma_{1}   & \\boldsymbol \\Sigma_{12}   \\\\\n    \\boldsymbol \\Sigma_{12}^T & \\boldsymbol \\Sigma_{2}   \\\\\n\\end{pmatrix}\n\\]\ncorresponding dimensions \\(d_1\\) \\(d_2\\) \\(d_1+d_2=d\\).Marginal distributions:subset \\(\\boldsymbol z\\) also multivariate normally distributed.\nSpecifically,\n\\[\n\\boldsymbol z_1 \\sim N_{d_1}(\\boldsymbol \\mu_1, \\boldsymbol \\Sigma_{1}) \n\\]\n\n\\[\n\\boldsymbol z_2 \\sim N_{d_2}(\\boldsymbol \\mu_2, \\boldsymbol \\Sigma_{2}) \n\\]Conditional multivariate normal:conditional distribution also multivariate normal:\n\\[\n\\boldsymbol z_1 | \\boldsymbol z_2 = \\boldsymbol z_{1 | 2} \\sim N_{d_1}(\\boldsymbol \\mu_{1|2}, \\boldsymbol \\Sigma_{1 | 2}) \n\\]\n\n\\[\\boldsymbol \\mu_{1|2}=\\boldsymbol \\mu_1 + \\boldsymbol \\Sigma_{12} \\boldsymbol \\Sigma_{2}^{-1} (\\boldsymbol z_2 -\\boldsymbol \\mu_2)\\]\n\n\\[\\boldsymbol \\Sigma_{1 | 2}=\\boldsymbol \\Sigma_{1} -  \\boldsymbol \\Sigma_{12} \\boldsymbol \\Sigma_{2}^{-1} \\boldsymbol \\Sigma_{12}^T\\]\\(\\boldsymbol z_{1 | 2}\\) \n\\(\\boldsymbol \\mu_{1|2}\\) dimension \\(d_1 \\times 1\\)\n\\(\\boldsymbol \\Sigma_{1 | 2}\\) dimension \\(d_1 \\times d_1\\),\n.e. dimension unconditioned variables.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"covariance-functions-and-kernels","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.3 Covariance functions and kernels","text":"GP prior infinitely dimensional multivariate normal\nmean zero covariance specified function \\(k(x, x^{\\prime})\\):widely used covariance function \n\\[\nk(x, x^{\\prime}) = \\text{Cov}(x, x^{\\prime}) = \\sigma^2 e^{-\\frac{ (x-x^{\\prime})^2}{2 l^2}}\n\\]\nknown squared-exponential kernel Radial-basis function (RBF) kernel.Note kernel implies\\(k(x, x) = \\text{Var}(x) = \\sigma^2\\) \\(\\text{Cor}(x, x^{\\prime}) = e^{-\\frac{ (x-x^{\\prime})^2}{2 l^2}}\\).parameter \\(l\\) RBF kernel length scale parameter describes\n“wigglyness” smoothness resulting function.\nSmall values \\(l\\) mean complex, wiggly functions, low autocorrelation.many kernel functions, including linear, polynomial periodic kernels.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gp-model","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.4 GP model","text":"Nonlinear regression GP approach conceptually simple:start multivariate priorthen condition observed datathe resulting conditional multivariate normal can used predict\nfunction values unobserved valuesthe conditional variance can used compute credible intervals predictions.GP regression also provides direct link classical\nBayesian linear regression (using linear kernel).Drawbacks: computationally expensive (\\(O(n^3)\\) matrix inversion)","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gaussian-process-example","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.5 Gaussian process example","text":"now show apply Gaussian processes R justing using standard matrix calculations.aim estimate following nonlinear function number observations. Note initially assume additional noise (observations lie directly curve):can now visualise functions samples multivariate normal prior:Now compute posterior mean variance conditioning observations:Now can plot posterior mean upper lower bounds 95% credible interval:Finally, can take acount noise measured data points adding error term:Note vicinity data points CIs small away\ndata uncertain estimate underlying function becomes.","code":"\ntruefunc = function(x) sin(x)\nXLIM = c(0, 2*pi)\nYLIM = c(-2, 2)\n\nn2 = 10\nx2 = runif(n2, min=XLIM[1], max=XLIM[2])\ny2 = truefunc(x2)  # no noise\n\ncurve( truefunc(x), xlim=XLIM, ylim=YLIM, xlab=\"x\", ylab=\"y\", \n      main=\"True Function\")\npoints(x2, y2)\n# RBF kernel\nrbfkernel = function(xa, xb, s2=1, l=1/2) s2*exp(-1/2*(xa-xb)^2/l^2)\nkfun.mat = function(xavec, xbvec, FUN=rbfkernel) \n  outer(X=as.vector(xavec), Y=as.vector(xbvec), FUN=FUN)\n\n# prior mean\nmu.vec = function(x) rep(0, length(x))\n# grid of x-values \nn1 = 100\nx1 = seq(XLIM[1], XLIM[2], length.out=n1)\n\n# unconditioned covariance and mean (unobserved samples x1)\nK1 = kfun.mat(x1, x1)  \nm1 = mu.vec(x1)\n\n## sample functions from GP prior  \nB = 5\nlibrary(\"MASS\") # for mvrnorm\ny1r = t(mvrnorm(B, mu = m1, Sigma=K1))\n\nplot(x1, y1r[,1], type=\"l\", lwd=2, ylab=\"y\", xlab=\"x\", ylim=YLIM, \n  main=\"Prior Functions (RBF Kernel with l=1/2)\")\nfor(i in 2:B)\n  lines(x1, y1r[,i], col=i, lwd=2)\n# unconditioned covariance and mean (observed samples x2)\nK2 = kfun.mat(x2, x2)\nm2 = mu.vec(x2)\niK2 = solve(K2) # inverse\n\n# cross-covariance\nK12 = kfun.mat(x1, x2)\n\n# Conditioning: x1 conditioned on x2\n\n# conditional mean\nm1.2 = m1 + K12 %*% iK2 %*% (y2 - m2)\n\n# conditional variance\nK1.2 = K1 - K12 %*% iK2 %*% t(K12)\n# upper and lower CI\nupper.bound = m1.2 + 1.96*sqrt(diag(K1.2))\nlower.bound = m1.2 - 1.96*sqrt(diag(K1.2))\n\nplot(x1, m1.2, type=\"l\", xlim=XLIM, ylim=YLIM, col=\"red\", lwd=3,\n  ylab=\"y\", xlab = \"x\", main = \"Posterior\")\n\npoints(x2,y2,pch=4,lwd=4,col=\"blue\")\nlines(x1,upper.bound,lty=2,lwd=3)\nlines(x1,lower.bound,lty=2,lwd=3)\ncurve(truefunc(x), xlim=XLIM, add=TRUE, col=\"gray\")\n\nlegend(x=\"topright\", \n  legend=c(\"posterior mean\", \"posterior quantiles\", \"true function\"),\n  lty=c(1, 2, 1),lwd=c(4, 4, 1), col=c(\"red\",\"black\", \"gray\"), cex=1.0)\n# add some noise\nsdeps = 0.1\nK2 = K2 + sdeps^2*diag(1,length(x2))\n\n# update\niK2 = solve(K2) # inverse\nm1.2 = m1 + K12 %*% iK2 %*% (y2 - m2)\nK1.2 = K1 - K12 %*% iK2 %*% t(K12)\nupper.bound = m1.2 + 1.96*sqrt(diag(K1.2))\nlower.bound = m1.2 - 1.96*sqrt(diag(K1.2))\n\nplot(x1, m1.2, type=\"l\", xlim=XLIM, ylim=YLIM, col=\"red\", lwd=3, \n  ylab=\"y\", xlab = \"x\", main = \"Posterior (with noise)\")\n\npoints(x2,y2,pch=4,lwd=4,col=\"blue\")\nlines(x1,upper.bound,lty=2,lwd=3)\nlines(x1,lower.bound,lty=2,lwd=3)\ncurve(truefunc(x), xlim=XLIM, add=TRUE, col=\"gray\")\n\nlegend(x=\"topright\", \n  legend=c(\"posterior mean\", \"posterior quantiles\", \"true function\"),\n  lty=c(1, 2, 1),lwd=c(4, 4, 1), col=c(\"red\",\"black\", \"gray\"), cex=1.0)"},{"path":"nonlinear-and-nonparametric-models.html","id":"neural-networks","chapter":"6 Nonlinear and nonparametric models","heading":"6.4 Neural networks","text":"Another highly important class models\nnonlinear prediction (nonlinear function approximation) \nneural networks.Relevant reading:Please read: Hastie, Tibshirani, Friedman (2009) Chapter 11 “Neural networks”\nJames et al. (2021) Chapter 10 “Deep Learning”","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"history","chapter":"6 Nonlinear and nonparametric models","heading":"6.4.1 History","text":"Neural networks actually relatively old models, going back\n1950s!Three phases neural networks (NN)1950/60: replicating functions neurons brain (perceptron)1980/90: neural networks universal function approximators2010—today: deep learningThe first phase biologically inspired, second phase focused \nmathematical properties, current phase pushed forward \nadvances computer science numerical optimisation:backpropagation algorithmbackpropagation algorithmauto-differentiation,auto-differentiation,stochastic gradient descentstochastic gradient descentuse GPUs TPUs (e.g. linear algebra)use GPUs TPUs (e.g. linear algebra)availability development deep learning packages:\nTheano (University Montreal), now Theano-PyMC/Aesara (PyMC3)\nTensorFlow (Google),\nFlax / JAX (Google),\nMXNet (Amazon),\nPyTorch (Facebook),\nPaddlePaddle (Baidu) etc.\navailability development deep learning packages:Theano (University Montreal), now Theano-PyMC/Aesara (PyMC3)TensorFlow (Google),Flax / JAX (Google),MXNet (Amazon),PyTorch (Facebook),PaddlePaddle (Baidu) etc.high-level wrappers:Keras (Tensorflow, MXNet, Theano)PyTorch-Lightning (PyTorch)","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"neural-networks-1","chapter":"6 Nonlinear and nonparametric models","heading":"6.4.2 Neural networks","text":"Neural networks essentially stacked systems linear regressions,\nmapping input nodes (random variables) outputs (response nodes).\ninternal layer corresponds internal latent variables.\nlayer connected next layer non-linear activation functions.feedforward single layer NNstacked nonlinear multiple regression hidden variablesoptimise empirical risk minimisationIt can shown NN can approximate arbitrary non-linear function mapping\ninput output.“Deep” neural networks many layers, optimisation requires advanced\ntechniques (see ).Neural networks highly parameterised models require typically lot data\ntraining.statistical aspects NN well understood: particular known\nNN overfit data can still generalise well. hand, also know NN\ncan also “fooled”, .e. prediction can unstable (adversarial examples).Current statistical research NN focuses interpretability links Bayesian inference models (e.g. GPs). example:https://link.springer.com/book/10.1007/978-3-030-28954-6https://arxiv.org/abs/1910.12478","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"learning-more-about-deep-learning","chapter":"6 Nonlinear and nonparametric models","heading":"6.4.3 Learning more about deep learning","text":"good place learn deep learning actual\nimplementations computer code various platforms book “Dive deep learning” \nZhang et al. (2020) available online https://d2l.ai/","code":""},{"path":"brief-refresher-on-matrices.html","id":"brief-refresher-on-matrices","chapter":"A Brief refresher on matrices","heading":"A Brief refresher on matrices","text":"intended short recap essentials need know matrices.\nfrequently make use matrix calculations.\nMatrix notation helps make multivariate equations simpler better understand\nunderlying concepts.details please consult lecture notes earlier modules (e.g. linear algebra).course mostly work real matrices, .e. assume matrix elements \nreal numbers. However, one important matrix decomposition — eigenvalues decomposition — can yield complex-valued matrices applied real matrices. point case.","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-basics","chapter":"A Brief refresher on matrices","heading":"A.1 Matrix basics","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-notation","chapter":"A Brief refresher on matrices","heading":"A.1.1 Matrix notation","text":"matrix notation distinguish scalars, vectors, matrices:Scalar: \\(x\\), \\(X\\), lower upper case, plain type.Vector: \\(\\boldsymbol x\\), lower case, bold type. handwriting arrow \\(\\vec{x}\\) indicates vector.component notation write \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T = (x_i)^T\\). convention, vector \ncolumn vector, .e. elements arranged column index (\\(\\)) refers row\ncolumn. transpose column vector becomes row vector\n\\(\\boldsymbol x^T = (x_1, \\ldots, x_d) =(x_i)\\) index now refers column.Matrix: \\(\\boldsymbol X\\), upper case, bold type. handwriting underscore\n\\(\\underline{X}\\) indicates matrix.component notation write \\(\\boldsymbol X= (x_{ij})\\). convention, first index (\\(\\))\nscalar elements \\(x_{ij}\\) denotes row second index (\\(j\\)) column matrix.\nAssuming \\(n\\) number rows \\(d\\) number columns\ncan also view matrix \\(\\boldsymbol X= (\\boldsymbol x_j) = (\\boldsymbol z_i)^T\\) composed column vectors\n\\(\\boldsymbol x_j = (x_{1j}, \\ldots, x_{nj})^T\\)\nrow vectors \\(\\boldsymbol z_i^T = (x_{i1}, \\ldots, x_{id})\\).(column) vector matrix size \\(d\\times 1\\). row vector matrix size \\(1\\times d\\).\nscalar matrix size \\(1 \\times 1\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"random-matrix","chapter":"A Brief refresher on matrices","heading":"A.1.2 Random matrix","text":"random matrix (vector) matrix (vector) whose elements random variables.Note standard\nnotation used univariate statistics distinguish\nrandom variables realisations (.e. upper versus lower case) \nwork multivariate statistics. Therefore, need determine\ncontext whether quantity represents \nrandom variable, whether constant.","code":""},{"path":"brief-refresher-on-matrices.html","id":"special-matrices","chapter":"A Brief refresher on matrices","heading":"A.1.3 Special matrices","text":"\\(\\boldsymbol I_d\\) identity matrix. square matrix size\n\\(d \\times d\\) diagonal\nfilled 1 -diagonals filled 0.\n\\[\\boldsymbol I_d =\n\\begin{pmatrix}\n    1 & 0 & 0 & \\dots & 0\\\\\n    0 & 1 & 0 & \\dots & 0\\\\\n    0 & 0 & 1 &   & 0\\\\\n    \\vdots & \\vdots & & \\ddots &  \\\\\n    0 & 0 & 0 &  & 1 \\\\\n\\end{pmatrix}\\]\\(\\boldsymbol 1\\) matrix contains 1s. often\nused form column vector \\(d\\) rows:\n\\[\\boldsymbol 1_d =\n\\begin{pmatrix}\n    1 \\\\\n    1 \\\\\n    1 \\\\\n    \\vdots   \\\\\n    1  \\\\\n\\end{pmatrix}\\]diagonal matrix matrix -diagonal elements zero.triangular matrix square matrix whose elements either diagonal zero (upper vs. lower triangular matrix).","code":""},{"path":"brief-refresher-on-matrices.html","id":"simple-matrix-operations","chapter":"A Brief refresher on matrices","heading":"A.2 Simple matrix operations","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-addition-and-multiplication","chapter":"A Brief refresher on matrices","heading":"A.2.1 Matrix addition and multiplication","text":"Matrices behave much like common numbers. example, can add matrices\n\\(\\boldsymbol C= \\boldsymbol + \\boldsymbol B\\)\nmultiply matrices \\(\\boldsymbol C= \\boldsymbol \\boldsymbol B\\).matrix addition \\(\\boldsymbol C= \\boldsymbol + \\boldsymbol B\\) add corresponding elements \\(c_{ij} = a_{ij} + b_{ij}\\). matrix addition \\(\\boldsymbol \\) \\(\\boldsymbol B\\) must dimensions, .e.\nnumber rows columns.dot product, scalar product, two vectors \\(\\boldsymbol \\) \\(\\boldsymbol b\\) given \\(\\boldsymbol \\cdot \\boldsymbol b= \\boldsymbol ^T \\boldsymbol b= \\boldsymbol \\boldsymbol b^T = \\sum_{=1}^d a_{} b_{}\\).Matrix multiplication \\(\\boldsymbol C= \\boldsymbol \\boldsymbol B\\) obtained setting \\(c_{ij} = \\sum_{k=1}^m a_{ik} b_{kj}\\) \\(m\\) \nnumber columns \\(\\boldsymbol \\) number rows \\(\\boldsymbol B\\). Thus, \\(\\boldsymbol C\\) contains possible\ndot products row vectors \\(\\boldsymbol \\) column vectors \\(\\boldsymbol B\\).\nmatrix multiplication number columns \\(\\boldsymbol \\) must match number rows \\(\\boldsymbol B\\).\nNote matrix multiplication general (\\(d > 1\\) commutative, .e. \\(\\boldsymbol \\boldsymbol B\\neq \\boldsymbol B\\boldsymbol \\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-transpose","chapter":"A Brief refresher on matrices","heading":"A.2.2 Matrix transpose","text":"matrix transpose \\(t(\\boldsymbol ) = \\boldsymbol ^T\\) interchanges rows columns. transpose\nlinear operator \\((\\boldsymbol + \\boldsymbol B)^T = \\boldsymbol ^T + \\boldsymbol B^T\\) \napplied matrix\nproduct reverses ordering, .e. \\((\\boldsymbol \\boldsymbol B)^T =\\boldsymbol B^T \\boldsymbol ^T\\).\\(\\boldsymbol = \\boldsymbol ^T\\) \\(\\boldsymbol \\) symmetric (square).construction given rectangular \\(\\boldsymbol \\) matrices\n\\(\\boldsymbol ^T \\boldsymbol \\) \\(\\boldsymbol \\boldsymbol ^T\\) symmetric non-negative diagonal.","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-summaries","chapter":"A Brief refresher on matrices","heading":"A.3 Matrix summaries","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-trace","chapter":"A Brief refresher on matrices","heading":"A.3.1 Matrix trace","text":"trace matrix sum diagonal elements \\(\\text{Tr}(\\boldsymbol ) = \\sum a_{ii}\\).useful identity matrix trace \n\\[\n\\text{Tr}(\\boldsymbol \\boldsymbol B) = \\text{Tr}( \\boldsymbol B\\boldsymbol )  \n\\]\ntwo vectors becomes\n\\[\n\\boldsymbol ^T \\boldsymbol b= \\text{Tr}( \\boldsymbol b\\boldsymbol ^T) \\,.\n\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"euclidean-norm","chapter":"A Brief refresher on matrices","heading":"A.3.2 Euclidean norm","text":"dot product \\(\\boldsymbol \\cdot \\boldsymbol = \\boldsymbol ^T \\boldsymbol = \\boldsymbol \\boldsymbol ^T = \\sum_{=1}^d a_i^2 = ||\\boldsymbol ||^2_2\\) yields \nsquared length squared Euclidean norm vector \\(\\boldsymbol \\).squared Frobenius norm generalisation Euclidean vector norm rectangular matrix \\(\\boldsymbol = (a_{ij})\\) \nsum squares elements \\(a_{ij}\\).\nUsing trace can written \n\\[\n\\begin{split}\n||\\boldsymbol ||_F^2 &= \\sum_{,j} a_{ij}^2 \\\\\n &= \\text{Tr}(\\boldsymbol ^T \\boldsymbol ) = \\text{Tr}(\\boldsymbol \\boldsymbol ^T) \\,.\n\\end{split}\n\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"determinant-of-a-matrix","chapter":"A Brief refresher on matrices","heading":"A.3.3 Determinant of a matrix","text":"\\(\\boldsymbol \\) square matrix determinant \\(\\det(\\boldsymbol )\\) scalar measuring volume spanned column vectors \\(\\boldsymbol \\) sign determined orientation vectors.\\(\\det(\\boldsymbol ) \\neq 0\\) matrix \\(\\boldsymbol \\) non-singular non-degenerate. Conversely, \n\\(\\det(\\boldsymbol ) =0\\) matrix \\(\\boldsymbol \\) singular degenerate.One way compute determinant matrix \\(\\boldsymbol \\) Laplace cofactor\nexpansion approach proceeds recursively based determinants submatrices \\(\\boldsymbol A_{-,-j}\\) obtained deleting row \\(\\) column \\(j\\) \\(\\boldsymbol \\). Specifically, \nlevel compute thecofactor expansion either\nalong \\(\\)-th row — pick row \\(\\):\n\\[\\det(\\boldsymbol ) = \\sum_{j=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})  \\text{ , }\\]\nalong \\(j\\)-th column — pick \\(j\\):\n\\[\\det(\\boldsymbol ) = \\sum_{=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})\\].\nalong \\(\\)-th row — pick row \\(\\):\n\\[\\det(\\boldsymbol ) = \\sum_{j=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})  \\text{ , }\\]along \\(j\\)-th column — pick \\(j\\):\n\\[\\det(\\boldsymbol ) = \\sum_{=1}^d a_{ij} (-1)^{+j} \\det(\\boldsymbol A_{-,-j})\\].repeat submatrix scalar \\(\\) \\(\\det()=\\,.\\)recursive nature algorithm leads complexity order \\(O(d!)\\) practical except small \\(d\\).\nTherefore, practice efficient algorithms computing determinants used still algorithmic complexity order \\(O(d^3)\\) large dimensions obtaining determinants \nexpensive.However, specially structured matrices allow fast calculation.\nparticular, turns determinant triangular matrix (includes diagonal matrices)\nsimply product diagonal elements.two-dimensional matrix \\(\\boldsymbol = \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\\\end{pmatrix}\\)\ndeterminant \\(\\det() = a_{11} a_{22} - a_{12} a_{21}\\).block-structured square matrix\n\\[\n\\boldsymbol = \\begin{pmatrix} \\boldsymbol A_{11} & \\boldsymbol A_{12} \\\\ \\boldsymbol A_{21} & \\boldsymbol A_{22} \\\\ \\end{pmatrix} \\, ,\n\\]\nmatrices diagonal \\(\\boldsymbol A_{11}\\) \\(\\boldsymbol A_{22}\\) square \n\\(\\boldsymbol A_{21}\\) \\(\\boldsymbol A_{21}\\) can shape,\ndeterminant \n\\[\n\\det(\\boldsymbol ) = \\det(\\boldsymbol A_{22}) \\det(\\boldsymbol C_1) = \\det(\\boldsymbol A_{11}) \\det(\\boldsymbol C_2) \n\\]\n(Schur complement \\(\\boldsymbol A_{22}\\))\n\\[\n\\boldsymbol C_1 = \\boldsymbol A_{11} -  \\boldsymbol A_{12}  \\boldsymbol A_{22}^{-1}  \\boldsymbol A_{21} \n\\]\n(Schur complement \\(\\boldsymbol A_{11}\\))\n\\[\n\\boldsymbol C_2 = \\boldsymbol A_{22} -  \\boldsymbol A_{21}  \\boldsymbol A_{11}^{-1}  \\boldsymbol A_{12} \n\\]\nNote \\(\\boldsymbol C_1\\) \\(\\boldsymbol C_2\\) square matrices.block-diagonal matrix \\(\\boldsymbol \\) \\(\\boldsymbol A_{12} = 0\\) \\(\\boldsymbol A_{21} = 0\\)\ndeterminant \\(\\det(\\boldsymbol ) = \\det(\\boldsymbol A_{11}) \\det(\\boldsymbol A_{22})\\).Determinants multiplicative property,\n\\[\\det(\\boldsymbol \\boldsymbol B) = \\det(\\boldsymbol B\\boldsymbol ) = \\det(\\boldsymbol ) \\det(\\boldsymbol B) \\,.\\]\nscalar \\(\\) becomes\n\\(\\det(\\boldsymbol B) = ^d \\det(\\boldsymbol B)\\) \\(d\\) dimension \\(\\boldsymbol B\\).Another important identity \n\\[\\det(\\boldsymbol I_n + \\boldsymbol \\boldsymbol B) = \\det(\\boldsymbol I_m + \\boldsymbol B\\boldsymbol )\\]\n\\(\\boldsymbol \\) \\(n \\times m\\) \\(\\boldsymbol B\\) \\(m \\times n\\) matrix. called \nWeinstein-Aronszajn determinant identity (also credited Sylvester).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-inverse","chapter":"A Brief refresher on matrices","heading":"A.4 Matrix inverse","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"inversion-of-square-matrix","chapter":"A Brief refresher on matrices","heading":"A.4.1 Inversion of square matrix","text":"\\(\\boldsymbol \\) square matrix inverse matrix \\(\\boldsymbol ^{-1}\\) matrix\n\n\\[\\boldsymbol ^{-1} \\boldsymbol = \\boldsymbol \\boldsymbol ^{-1}=  \\boldsymbol \\, .\\]\nnon-singular matrices \\(\\det(\\boldsymbol ) \\neq 0\\) invertible.\\(\\det(\\boldsymbol ^{-1} \\boldsymbol ) = \\det(\\boldsymbol ) = 1\\) \ndeterminant inverse matrix equals\ninverse determinant,\n\\[\\det(\\boldsymbol ^{-1}) = \\det(\\boldsymbol )^{-1} \\,.\\]transpose inverse inverse transpose\n\n\\[\n\\begin{split}\n(\\boldsymbol ^{-1})^T &= (\\boldsymbol ^{-1})^T \\,  \\boldsymbol ^T (\\boldsymbol ^{T})^{-1}   \\\\\n &= (\\boldsymbol \\boldsymbol ^{-1})^T \\, (\\boldsymbol ^{T})^{-1} = (\\boldsymbol ^{T})^{-1} \\,. \\\\\n\\end{split}\n\\]inverse matrix product \\((\\boldsymbol \\boldsymbol B)^{-1} = \\boldsymbol B^{-1} \\boldsymbol ^{-1}\\)\nproduct indivdual matrix inverses reverse order.many different algorithms compute inverse matrix\n(essentially problem solving system equations).\ncomputational complexity matrix inversion order \\(O(d^3)\\)\n\\(d\\) dimension \\(\\boldsymbol \\). Therefore matrix inversion costly higher dimensions.Example .1  Inversion \\(2 \\times 2\\) matrix:inverse matrix \\(= \\begin{pmatrix} & b \\\\ c & d \\end{pmatrix}\\) \n\\(^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & \\end{pmatrix}\\)","code":""},{"path":"brief-refresher-on-matrices.html","id":"inversion-of-structured-matrices","chapter":"A Brief refresher on matrices","heading":"A.4.2 Inversion of structured matrices","text":"However, specially structured matrices inversion can done effectively:inverse diagonal matrix another diagonal matrix obtained inverting diagonal elements.generally, inverse block-diagonal matrix obtained individually inverting blocks along diagonal.Woodbury matrix identity simplifies inversion matrices can \nwritten \\(\\boldsymbol + \\boldsymbol U\\boldsymbol B\\boldsymbol V\\) \\(\\boldsymbol \\) \\(\\boldsymbol B\\) square \n\\(\\boldsymbol U\\) \\(\\boldsymbol V\\) suitable rectangular matrices:\n\\[\n(\\boldsymbol + \\boldsymbol U\\boldsymbol B\\boldsymbol V)^{-1} = \\boldsymbol ^{-1} - \\boldsymbol ^{-1} \\boldsymbol U(\\boldsymbol B^{-1} + \\boldsymbol V\\boldsymbol ^{-1} \\boldsymbol U)^{-1} \\boldsymbol V\\boldsymbol ^{-1}\n\\]\nTypically, inverse \\(\\boldsymbol ^{-1}\\) either already known can easily obtained \ndimension \\(\\boldsymbol B\\) much lower \\(\\boldsymbol \\).class matrices can easily inverted orthogonal matrices whose inverse \nobtained simply transposing matrix.","code":""},{"path":"brief-refresher-on-matrices.html","id":"orthogonal-matrices","chapter":"A Brief refresher on matrices","heading":"A.5 Orthogonal matrices","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"properties-1","chapter":"A Brief refresher on matrices","heading":"A.5.1 Properties","text":"orthogonal matrix \\(\\boldsymbol Q\\) square matrix property \\(\\boldsymbol Q^T = \\boldsymbol Q^{-1}\\), .e.\ntranspose also inverse. implies \\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol Q^T \\boldsymbol Q= \\boldsymbol \\).column row vectors \\(\\boldsymbol Q\\) length 1. implies \nelement \\(q_{ij}\\) \\(\\boldsymbol Q\\) can take value interval \\([-1, 1]\\).identity matrix \\(\\boldsymbol \\) simplest example orthogonal matrix.squared Euclidean Frobenius norm preserved vector \\(\\boldsymbol \\) matrix \\(\\boldsymbol \\) multiplied orthogonal matrix \\(\\boldsymbol Q\\):\n\\[\n|| \\boldsymbol Q\\boldsymbol ||^2_2 = (\\boldsymbol Q\\boldsymbol )^T \\boldsymbol Q\\boldsymbol = \\boldsymbol ^T \\boldsymbol = || \\boldsymbol ||^2_2\n\\]\n\n\\[\n|| \\boldsymbol Q\\boldsymbol ||^2_F = \\text{Tr}\\left((\\boldsymbol Q\\boldsymbol )^T \\boldsymbol Q\\boldsymbol \\right) = \\text{Tr}\\left(\\boldsymbol ^T \\boldsymbol \\right) = || \\boldsymbol ||^2_F \n\\]Multiplication \\(\\boldsymbol Q\\) vector results \nnew vector length change direction (unless \\(\\boldsymbol Q=\\boldsymbol \\)).\northogonal matrix\n\\(\\boldsymbol Q\\) can thus interpreted geometrically operator performing\nrotation, reflection /permutation.product \\(\\boldsymbol Q_3 = \\boldsymbol Q_1 \\boldsymbol Q_2\\) two orthogonal matrices \\(\\boldsymbol Q_1\\) \\(\\boldsymbol Q_2\\) yields another orthogonal matrix \\(\\boldsymbol Q_3 \\boldsymbol Q_3^T = \\boldsymbol Q_1 \\boldsymbol Q_2 (\\boldsymbol Q_1 \\boldsymbol Q_2)^T = \\boldsymbol Q_1 \\boldsymbol Q_2 \\boldsymbol Q_2^T \\boldsymbol Q_1^T = \\boldsymbol \\).determinant \\(\\det(\\boldsymbol Q)\\) orthogonal matrix either +1 -1,\n\\(\\boldsymbol Q\\boldsymbol Q^T = \\boldsymbol \\) thus \\(\\det(\\boldsymbol Q)\\det(\\boldsymbol Q^T) = \\det(\\boldsymbol Q)^2 = \\det(\\boldsymbol ) = 1\\).set orthogonal matrices dimension \\(d\\) together multiplication\nform group called orthogonal group \\(O(d)\\).\nsubset orthogonal matrices \\(\\det(\\boldsymbol Q)=1\\) called rotation matrices form multiplication special orthogonal group \\((d)\\).\nOrthogonal matrices \\(\\det(\\boldsymbol Q)=-1\\) rotation-reflection matrices.","code":""},{"path":"brief-refresher-on-matrices.html","id":"generating-orthogonal-matrices","chapter":"A Brief refresher on matrices","heading":"A.5.2 Generating orthogonal matrices","text":"two dimensions \\((d=2)\\) orthogonal matrices \\(\\boldsymbol R\\) representing rotations \\(\\det(\\boldsymbol R)=1\\) \ngiven \n\\[\n\\boldsymbol R(\\theta) = \n\\begin{pmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta \n\\end{pmatrix}\n\\]\nrepresenting rotation-reflections \\(\\boldsymbol G\\) \\(\\det(\\boldsymbol G)=-1\\) \n\\[\n\\boldsymbol G(\\theta) = \n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix}\\,.\n\\]\nEvery orthogonal matrix dimension \\(d=2\\)\ncan represented product two rotation-reflection\nmatrices \n\\[\n\\boldsymbol R(\\theta) = \\boldsymbol G(\\theta)\\, \\boldsymbol G(0) =  \n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix} \n\\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}\\,.\n\\]\nThus, matrix \\(\\boldsymbol G\\) generator two-dimensional orthogonal matrices.\nNote \\(\\boldsymbol G(\\theta)\\) symmetric, orthogonal determinant -1.generally, applicable arbitrary dimension, role generator taken Householder reflection matrix\n\\[\n\\boldsymbol Q_{HH}(\\boldsymbol v) = \\boldsymbol - 2 \\boldsymbol v\\boldsymbol v^T\n\\]\n\\(\\boldsymbol v\\) vector unit length (\\(\\boldsymbol v^T \\boldsymbol v=1\\)) orthogonal \nreflection hyperplane. Note \\(\\boldsymbol Q_{HH}(\\boldsymbol v) = \\boldsymbol Q_{HH}(-\\boldsymbol v)\\).\nconstruction matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\) symmetric, orthogonal determinant -1.can shown \\(d\\)-dimensional orthogonal matrix \\(\\boldsymbol Q\\) can represented product \\(d\\) Householder reflection matrices.\ntwo-dimensional generator \\(\\boldsymbol G(\\theta)\\) recovered Householder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\)\n\\(\\boldsymbol v= \\begin{pmatrix} -\\sin \\frac{\\theta}{2} \\\\ \\cos \\frac{\\theta}{2} \\end{pmatrix}\\)\n\\(\\boldsymbol v= \\begin{pmatrix} \\sin \\frac{\\theta}{2} \\\\ -\\cos \\frac{\\theta}{2} \\end{pmatrix}\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"permutation-matrix","chapter":"A Brief refresher on matrices","heading":"A.5.3 Permutation matrix","text":"special type orthogonal matrix permutation matrix \\(\\boldsymbol P\\) created \npermuting rows /columns identity matrix \\(\\boldsymbol \\). Thus, row column\n\\(\\boldsymbol P\\) contains exactly one entry 1, necessarily diagonal.permutation matrix \\(\\boldsymbol P\\) multiplied matrix \\(\\boldsymbol \\) acts operator\npermuting columns (\\(\\boldsymbol \\boldsymbol P\\)) rows (\\(\\boldsymbol P\\boldsymbol \\)).\nset \\(d\\) elements exist \\(d!\\) permutations. Thus, dimension \\(d\\) \n\\(d!\\) possible permutation matrices (including identity matrix).determinant permutation matrix either +1 -1.\nproduct two permutation matrices yields another permutation matrix.Symmetric permutation matrices correspond self-inverse permutations\n(.e. permutation matrix inverse), also called permutation involutions.\ncan determinant +1 -1.transposition permutation two elements exchanged.\nThus, transposition matrix \\(\\boldsymbol T\\)\nexactly two rows /columns exchanged compared identity matrix \\(\\boldsymbol \\).\nTranspositions self-inverse, transposition matrices symmetric.\n\\(\\frac{d (d-1)}{2}\\) different transposition matrices.\ndeterminant transposition matrix \\(\\det(\\boldsymbol T)= -1\\).Note transposition matrix instance Householder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\)\nvector \\(\\boldsymbol v\\) filled zeros except two elements value\n\\(\\frac{\\sqrt{2}}{2}\\) \\(-\\frac{\\sqrt{2}}{2}\\).permutation \\(d\\) elements can generated series \\(d-1\\) transpositions.\nCorrespondingly, permutation matrix \\(\\boldsymbol P\\) can constructed multiplication identity\nmatrix \\(d-1\\) transposition matrices. number transpositions even \\(\\det(\\boldsymbol P) = 1\\) otherwise\nuneven number \\(\\det(\\boldsymbol P) = -1\\). called sign signature permutation.set permutations form symmetric group \\(S_d\\), subset even permutations (positive sign \\(\\det(\\boldsymbol P)=1\\)) alternating group \\(A_d\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-and-eigenvectors","chapter":"A Brief refresher on matrices","heading":"A.6 Eigenvalues and eigenvectors","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"definition","chapter":"A Brief refresher on matrices","heading":"A.6.1 Definition","text":"Assume square symmetric matrix \\(\\boldsymbol \\) size \\(d \\times d\\).\nvector \\(\\boldsymbol u\\neq 0\\) called eigenvector matrix \\(\\boldsymbol \\) \\(\\lambda\\) corresponding\neigenvalue \\[\\boldsymbol \\boldsymbol u= \\boldsymbol u\\lambda \\, .\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"finding-eigenvalues-and-vectors","chapter":"A Brief refresher on matrices","heading":"A.6.2 Finding eigenvalues and vectors","text":"find eigenvalues eigenvector eigenequation rewritten \n\\[(\\boldsymbol -\\boldsymbol \\lambda ) \\boldsymbol u= 0 \\,.\\] solution \\(\\boldsymbol u\\neq 0\\) corresponding eigenvalue\n\\(\\lambda\\) must make matrix \\(\\boldsymbol -\\boldsymbol \\lambda\\) singular, .e. determinant must vanish\n\\[\\det(\\boldsymbol -\\boldsymbol \\lambda ) =0 \\,.\\]\ncharacteristic equation matrix \\(\\boldsymbol \\), solution yields \\(d\\)\nnecessarily distinct also potentially complex eigenvalues \\(\\lambda_1, \\ldots, \\lambda_d\\).complex eigenvalues, real matrix eigenvalues come conjugate pairs.\nSpecifically, complex \\(\\lambda_1 = r e^{\\phi}\\) also corresponding complex eigenvalue \\(\\lambda_2 = r e^{-\\phi}\\).Given eigenvalues solve eigenequation corresponding non-zero eigenvectors\n\\(\\boldsymbol u_1, \\ldots, \\boldsymbol u_d\\). Note eigenvectors real matrices can complex components.\nAlso eigenvector defined eigenequation scalar.\nconvention eigenvectors therefore typically standardised unit length still leaves\nsign ambiguity real eigenvectors implies complex eigenvectors defined factor modulus 1.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenequation-in-matrix-notation","chapter":"A Brief refresher on matrices","heading":"A.6.3 Eigenequation in matrix notation","text":"matrix\n\\[\\boldsymbol U= (\\boldsymbol u_1, \\ldots, \\boldsymbol u_d)\\] containing standardised eigenvectors columns diagonal matrix\n\\[\\boldsymbol \\Lambda= \\begin{pmatrix}\n    \\lambda_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}\n\\end{pmatrix}\\]\ncontaining eigenvalues (typically sorted order magnitude) eigenvalue equation can written \n\\[\\boldsymbol \\boldsymbol U= \\boldsymbol U\\boldsymbol \\Lambda\\,.\\]Note eigenvalues order, can alwas apply permutation matrix \\(\\boldsymbol P\\) sort order, \\(\\boldsymbol U' = \\boldsymbol U\\boldsymbol P\\) reorders eigenvectors \\(\\boldsymbol \\Lambda' = \\boldsymbol P^T \\boldsymbol \\Lambda\\boldsymbol P\\)\neigenvalues, \n\\[\\boldsymbol \\boldsymbol U' =  \\boldsymbol \\boldsymbol U\\boldsymbol P= \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol P=  \\boldsymbol U\\boldsymbol P\\boldsymbol P^T \\boldsymbol \\Lambda\\boldsymbol P=  \\boldsymbol U' \\boldsymbol \\Lambda' \\,.\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"defective-matrix","chapter":"A Brief refresher on matrices","heading":"A.6.4 Defective matrix","text":"cases eigenvectors \\(\\boldsymbol u_i\\) linearly independent form basis span \\(d\\) dimensional space.However, case \nmatrix \\(\\boldsymbol \\) complete basis eigenvectors, matrix called defective. case\nmatrix \\(\\boldsymbol U\\) containing eigenvectors singular \\(\\det(\\boldsymbol U)=0\\).example defective matrix \n\\(\\begin{pmatrix} 1 &1 \\\\ 0 & 1 \\\\ \\end{pmatrix}\\)\ndeterminant 1 can inverted column vectors form complete basis\none distinct eigenvector \\((1,0)^T\\) eigenvector basis incomplete.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-of-a-diagonal-or-triangular-matrix","chapter":"A Brief refresher on matrices","heading":"A.6.5 Eigenvalues of a diagonal or triangular matrix","text":"special case \\(\\boldsymbol \\) diagonal triangular matrix eigenvalues easily determined.\nfollows simple form determinants product diagonal elements.\nHence matrices characteristic equation becomes \\(\\prod_{}^d (a_{ii} -\\lambda) = 0\\) solution\n\\(\\lambda_i=a_{ii}\\), .e. eigenvalues equal diagonal elements.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-and-vectors-of-a-symmetric-matrix","chapter":"A Brief refresher on matrices","heading":"A.6.6 Eigenvalues and vectors of a symmetric matrix","text":"\\(\\boldsymbol \\) symmetric, .e. \\(\\boldsymbol = \\boldsymbol ^T\\), eigenvalues eigenvectors special properties:eigenvalues \\(\\boldsymbol \\) real,eigenvectors orthogonal, .e \\(\\boldsymbol u_i^T \\boldsymbol u_j = 0\\) \\(\\neq j\\), real. Thus, matrix \\(\\boldsymbol U\\) containing standardised orthonormal eigenvectors orthogonal.\\(\\boldsymbol \\) never defective \\(\\boldsymbol U\\) forms complete basis.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-of-orthogonal-matrices","chapter":"A Brief refresher on matrices","heading":"A.6.7 Eigenvalues of orthogonal matrices","text":"eigenvalues orthogonal matrix \\(\\boldsymbol Q\\) necessarily real \nmodulus 1 lie unit circle . Thus, eigenvalues \\(\\boldsymbol Q\\)\nform \\(\\lambda = e^{\\phi} = \\cos \\phi + \\sin \\phi\\).real matrix complex eigenvalues come conjugate\npairs. Hence orthogonal matrix \\(\\boldsymbol Q\\) complex eigenvalue \\(e^{\\phi}\\) also \ncomplex eigenvalue \\(e^{-\\phi} =\\cos \\phi - \\sin \\phi\\). product two conjugate\neigenvalues 1. Thus, orthogonal matrix uneven dimension least one\nreal eigenvalue (+1 -1).eigenvalues Hausholder matrix \\(\\boldsymbol Q_{HH}(\\boldsymbol v)\\) real (recall symmetric!).\nfact, dimension \\(d\\) eigenvalues -1 (one time) 1 ( \\(d-1\\) times).\nSince transposition matrix \\(\\boldsymbol T\\) special Householder matrix eigenvalues.","code":""},{"path":"brief-refresher-on-matrices.html","id":"positive-definite-matrices","chapter":"A Brief refresher on matrices","heading":"A.6.8 Positive definite matrices","text":"eigenvalues square matrix \\(\\boldsymbol \\) real \\(\\lambda_i \\geq 0\\) \\(\\boldsymbol \\) called positive semi-definite.\neigenvalues strictly positive\n\\(\\lambda_i > 0\\) \\(\\boldsymbol \\) called positive definite.Note matrix need symmetric positive\ndefinite, e.g.\n\\(\\begin{pmatrix} 2 & 3 \\\\ 1 & 4 \\\\ \\end{pmatrix}\\)\npositive eigenvalues 5 1. also complete\nset eigenvectors diagonisable.symmetric matrix \\(\\boldsymbol \\) positive definite\nquadratic form \\(\\boldsymbol x^T \\boldsymbol \\boldsymbol x> 0\\) non-zero \\(\\boldsymbol x\\),\npositive semi-definite \\(\\boldsymbol x^T \\boldsymbol \\boldsymbol x\\geq 0\\).\nholds also way around:\nsymmetric positive definite matrix (positive eigenvalues) \npositive quadratic form, symmetric positive semi-definite matrix (non-negative eigenvalues) non-negative quadratic form.symmetric positive definite matrix always positive diagonal\n(can seen setting \\(\\boldsymbol x\\) unit vector 1 \nsingle position, 0 elements).\nHowever, just requiring positive diagonal weak ensure positive definiteness symmetric matrix, example \\(\\begin{pmatrix} 1 &10 \\\\ 10 & 1 \\\\ \\end{pmatrix}\\) negative eigenvalue -9.\nhand, symmetric matrix indeed positive definite strictly\ndiagonally dominant, .e. diagonal elements positive larger absolute value corresponding row column elements.\nHowever, diagonal dominance restrictive criterion \ncharacterise \nsymmetric positive definite matrices, since\nmany symmetric matrices positive definite diagonally dominant, \n\\(\\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\\\ \\end{pmatrix}\\).Finally, sum symmetric positive semi-definite matrix \\(\\boldsymbol \\)\nsymmetric positive definite matrix \\(\\boldsymbol B\\) symmetric positive definite corresponding\nquadratic form \\(\\boldsymbol x^T ( \\boldsymbol +\\boldsymbol B) \\boldsymbol x= \\boldsymbol x^T \\boldsymbol \\boldsymbol x+ \\boldsymbol x^T \\boldsymbol B\\boldsymbol x> 0\\) positive. Similarly, sum\ntwo symmetric positive (semi)-definite matrices symmetric positive (semi)-definite.","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-decompositions","chapter":"A Brief refresher on matrices","heading":"A.7 Matrix decompositions","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"diagonalisation-and-eigenvalue-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.1 Diagonalisation and eigenvalue decomposition","text":"\\(\\boldsymbol \\) square non-defective matrix \\(\\boldsymbol U\\) invertible \ncan rewrite eigenvalue equation \n\\[\\boldsymbol = \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1} \\,.\\]\ncalled eigendecomposition, spectral decomposition, \\(\\boldsymbol \\) equivalently\n\\[\\boldsymbol \\Lambda= \\boldsymbol U^{-1} \\boldsymbol \\boldsymbol U\\]\ndiagonalisation \\(\\boldsymbol \\).\nThus matrix \\(\\boldsymbol \\) defective diagonalisable using eigenvalue decomposition.","code":""},{"path":"brief-refresher-on-matrices.html","id":"orthogonal-eigenvalue-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.2 Orthogonal eigenvalue decomposition","text":"symmetric \\(\\boldsymbol \\) becomes\n\\[\\boldsymbol = \\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\]\nreal eigenvalues orthogonal matrix \\(\\boldsymbol U\\)\n\n\\[\\boldsymbol \\Lambda= \\boldsymbol U^T \\boldsymbol \\boldsymbol U\\,.\\]\nspecial case known orthogonal diagonalisation\n\\(\\boldsymbol \\).orthogonal decomposition symmetric \\(\\boldsymbol \\) \nunique apart signs\neigenvectors.\norder make fully unique one needs impose restrictions (e.g. require positive diagonal\n\\(\\boldsymbol U\\)). Note can particularly important computer application sign\ncan vary depending specific implementation underlying numerical algorithms.","code":""},{"path":"brief-refresher-on-matrices.html","id":"singular-value-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.3 Singular value decomposition","text":"singular value decomposition (SVD) \ngeneralisation orthogonal eigenvalue decomposition\nsymmetric matrices.(!) rectangular matrix \\(\\boldsymbol \\) size \\(n\\times d\\) can factored\nproduct\n\\[\\boldsymbol = \\boldsymbol U\\boldsymbol D\\boldsymbol V^T\\]\n\\(\\boldsymbol U\\) \\(n \\times n\\) orthogonal matrix, \\(\\boldsymbol V\\) second \\(d \\times d\\) orthogonal matrix \\(\\boldsymbol D\\) diagonal rectangular matrix\nsize \\(n\\times d\\) \\(m=min(n,d)\\) real diagonal elements \\(d_1, \\ldots d_m\\). \\(d_i\\) called singular values, appear\nalong diagonal \\(\\boldsymbol D\\) order magnitude.SVD unique apart \nsigns columns vectors \\(\\boldsymbol U\\), \\(\\boldsymbol V\\) \\(\\boldsymbol D\\) (can freely specify column signs two \nthree matrices). convention \nsigns chosen singular values \\(\\boldsymbol D\\) non-negative, leaves ambiguity\ncolumns signs \\(\\boldsymbol U\\) \\(\\boldsymbol V\\). Alternatively, one may\nfix columns signs \\(\\boldsymbol U\\) \\(\\boldsymbol V\\), e.g. requiring positive diagonal, determines sign singular values (thus allowing negative singular values well).\\(\\boldsymbol \\) symmetric SVD orthogonal eigenvalue decomposition coincide (apart different sign conventions singular values, eigenvalues eigenvectors).Since \\(\\boldsymbol ^T \\boldsymbol = \\boldsymbol V\\boldsymbol D^T \\boldsymbol D\\boldsymbol V^T\\) \\(\\boldsymbol \\boldsymbol ^T = \\boldsymbol U\\boldsymbol D\\boldsymbol D^T \\boldsymbol U^T\\) squared singular values correspond eigenvalues \\(\\boldsymbol ^T \\boldsymbol \\) \\(\\boldsymbol \\boldsymbol ^T\\).\nalso follows \\(\\boldsymbol ^T \\boldsymbol \\) \\(\\boldsymbol \\boldsymbol ^T\\) positive\nsemi-definite symmetric matrices, \\(\\boldsymbol V\\) \\(\\boldsymbol U\\) contain respective sets eigenvectors.","code":""},{"path":"brief-refresher-on-matrices.html","id":"polar-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.4 Polar decomposition","text":"square matrix \\(\\boldsymbol \\) can factored product\n\\[\n\\boldsymbol = \\boldsymbol Q\\boldsymbol B\n\\]\northogonal matrix \\(\\boldsymbol Q\\) symmetric positive semi-definite matrix \\(\\boldsymbol B\\).follows SVD \\(\\boldsymbol \\) given \n\\[\n\\begin{split}\n\\boldsymbol &= \\boldsymbol U\\boldsymbol D\\boldsymbol V^T \\\\\n    &= ( \\boldsymbol U\\boldsymbol V^T ) ( \\boldsymbol V\\boldsymbol D\\boldsymbol V^T ) \\\\\n    &= \\boldsymbol Q\\boldsymbol B\\\\\n\\end{split}\n\\]\nnon-negative \\(\\boldsymbol D\\). Note decomposition unique sign ambiguities columns \\(\\boldsymbol U\\) \\(\\boldsymbol V\\) cancel \\(\\boldsymbol Q\\) \\(\\boldsymbol B\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"cholesky-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.5 Cholesky decomposition","text":"symmetric positive definite matrix \\(\\boldsymbol \\) can decomposed product\ntriangular matrix \\(\\boldsymbol L\\) transpose\n\\[\n\\boldsymbol = \\boldsymbol L\\boldsymbol L^T \\,.\n\\]\n, \\(\\boldsymbol L\\) lower triangular matrix positive diagonal elements.decomposition unique called Cholesky factorisation. \noften used check whether symmetric matrix positive definite algorithmically\nless demanding eigenvalue decomposition.Note implementations Cholesky decomposition (e.g. R) use\nupper triangular matrices \\(\\boldsymbol K\\) positive diagonal \n\\(\\boldsymbol = \\boldsymbol K^T \\boldsymbol K\\) \\(\\boldsymbol L= \\boldsymbol K^T\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-summaries-based-on-eigenvalues-and-singular-values","chapter":"A Brief refresher on matrices","heading":"A.8 Matrix summaries based on eigenvalues and singular values","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"trace-and-determinant-computed-from-eigenvalues","chapter":"A Brief refresher on matrices","heading":"A.8.1 Trace and determinant computed from eigenvalues","text":"eigendecomposition \\(\\boldsymbol =\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1}\\)\nallows establish link trace determinant eigenvalues.Specifically,\n\\[\n\\begin{split}\n\\text{Tr}(\\boldsymbol ) & = \\text{Tr}(\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^{-1}  ) =\n\\text{Tr}( \\boldsymbol \\Lambda\\boldsymbol U^{-1} \\boldsymbol U) \\\\\n &= \\text{Tr}( \\boldsymbol \\Lambda) = \\sum_{=1}^d \\lambda_i \\\\\n\\end{split}\n\\]\nthus trace square matrix \\(\\boldsymbol \\) equal sum eigenvalues. Likewise,\n\\[\n\\begin{split}\n\\det(\\boldsymbol ) & = \\det(\\boldsymbol U) \\det(\\boldsymbol \\Lambda) \\det(\\boldsymbol U^{-1}  ) \\\\\n &=\\det( \\boldsymbol \\Lambda) = \\prod_{=1}^d \\lambda_i \\\\\n\\end{split}\n\\]\ntherefore determinant \\(\\boldsymbol \\) product eigenvalues.relationship eigenvalues trace determinant\ndemonstrated diagonisable non-defective matrices.\nHowever, hold also general matrix. can shown using certain non-diagonal matrix decompositions (e.g. Jordan decomposition).result, eigenvalues equal zero \\(\\det(\\boldsymbol ) = 0\\) hence \\(\\boldsymbol \\) singular invertible.trace determinant real matrix always real even though individual eigenvalues may complex.","code":""},{"path":"brief-refresher-on-matrices.html","id":"rank-and-condition-number","chapter":"A Brief refresher on matrices","heading":"A.8.2 Rank and condition number","text":"rank dimension space spanned column row vectors. rectangular matrix dimension \\(n \\times d\\) \nrank \\(m = \\min(n, d)\\), maximum indeed achieved full rank.condition number describes well- ill-conditioned\nfull rank matrix . example, square matrix large condition number implies matrix close singular\nthus ill-conditioned.\ncondition number infinite matrix full rank.rank condition matrix can determined \\(m\\) singular values \\(d_1, \\ldots, d_m\\) matrix obtained SVD:rank number non-zero singular values.condition number ratio largest singular value\ndivided smallest singular value (absolute values signs allowed).square matrix \\(\\boldsymbol \\) singular condition number infinite, full rank.\nhand, non-singular square matrix, \npositive definite matrix, full rank.","code":""},{"path":"brief-refresher-on-matrices.html","id":"functions-of-symmetric-matrices","chapter":"A Brief refresher on matrices","heading":"A.9 Functions of symmetric matrices","text":"focus symmetric square matrices \\(\\boldsymbol =\\boldsymbol U\\boldsymbol \\Lambda\\boldsymbol U^T\\) always diagonisable real eigenvalues \\(\\boldsymbol \\Lambda\\) orthogonal eigenvectors \\(\\boldsymbol U\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"definition-of-a-matrix-function","chapter":"A Brief refresher on matrices","heading":"A.9.1 Definition of a matrix function","text":"Assume real-valued function \\(f()\\) real number \\(\\). corresponding\nmatrix function \\(f(\\boldsymbol )\\)\ndefined \n\\[\nf(\\boldsymbol ) =  \\boldsymbol Uf(\\boldsymbol \\Lambda) \\boldsymbol U^T =  \\boldsymbol U\\begin{pmatrix}\n    f(\\lambda_{1}) & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & f(\\lambda_{d})\n\\end{pmatrix} \\boldsymbol U^T\n\\]\nfunction \\(f()\\) applied eigenvalues \\(\\boldsymbol \\).\nconstruction \\(f(\\boldsymbol )\\) real, symmetric \nreal eigenvalues \\(f(\\lambda_i)\\).Examples:Example .2  Matrix power: \\(f() = ^p\\) (\\(p\\) real number)Special cases matrix power include :Matrix inversion: \\(f() = ^{-1}\\)\nNote matrix \\(\\boldsymbol \\) singular, .e. contains one eigenvalues \\(\\lambda_i=0\\),\n\\(\\boldsymbol ^{-1}\\) defined therefore \\(\\boldsymbol \\) invertible.However, -called pseudoinverse can still computed, inverting non-zero eigenvalues, \nkeeping zero eigenvalues zero.Matrix square root: \\(f() = ^{1/2}\\)\nSince multiple solutions square root also multiple\nmatrix square roots. principal matrix square root obtained using\npositive square roots eigenvalues. Thus principal matrix square root\npositive semi-definite matrix also positive semi-definite unique.Example .3  Matrix exponential: \\(f() = \\exp()\\)\nNote \\(\\exp() \\geq 0\\) real \\(\\) matrix \\(\\exp(\\boldsymbol )\\) positive\nsemi-definite. Thus, matrix exponential can used generate positive semi-definite\nmatrices.\\(\\boldsymbol \\) \\(\\boldsymbol B\\) commute, .e. \\(\\boldsymbol \\boldsymbol B= \\boldsymbol B\\boldsymbol \\), \n\\(\\exp(\\boldsymbol +\\boldsymbol B) = \\exp(\\boldsymbol ) \\exp(\\boldsymbol B)\\). However, case\notherwise!Example .4  Matrix logarithm: \\(f() = \\log()\\)\nlogarithm requires \\(>0\\) matrix \\(\\boldsymbol \\) needs positive definite\n\\(\\log(\\boldsymbol )\\) defined.","code":""},{"path":"brief-refresher-on-matrices.html","id":"identities-for-the-matrix-exponential-and-logarithm","chapter":"A Brief refresher on matrices","heading":"A.9.2 Identities for the matrix exponential and logarithm","text":"give rise useful identities:symmetric matrix \\(\\boldsymbol \\) \n\\[\n\\det(\\exp(\\boldsymbol )) = \\exp(\\text{Tr}(\\boldsymbol ))\n\\]\n\n\\(\\prod_i \\exp(\\lambda_i) = \\exp( \\sum_i \\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol \\).symmetric matrix \\(\\boldsymbol \\) \n\\[\n\\det(\\exp(\\boldsymbol )) = \\exp(\\text{Tr}(\\boldsymbol ))\n\\]\n\n\\(\\prod_i \\exp(\\lambda_i) = \\exp( \\sum_i \\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol \\).take logarithm sides replace \\(\\exp(\\boldsymbol )=\\boldsymbol B\\) get another\nidentity symmetric positive definite matrix \\(\\boldsymbol B\\):\n\\[\n\\log \\det(\\boldsymbol B) = \\text{Tr}(\\log(\\boldsymbol B))\n\\]\n\n\\(\\log( \\prod_i \\lambda_i) = \\sum_i \\log(\\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol B\\).take logarithm sides replace \\(\\exp(\\boldsymbol )=\\boldsymbol B\\) get another\nidentity symmetric positive definite matrix \\(\\boldsymbol B\\):\n\\[\n\\log \\det(\\boldsymbol B) = \\text{Tr}(\\log(\\boldsymbol B))\n\\]\n\n\\(\\log( \\prod_i \\lambda_i) = \\sum_i \\log(\\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\boldsymbol B\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-calculus","chapter":"A Brief refresher on matrices","heading":"A.10 Matrix calculus","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"first-order-vector-derivatives","chapter":"A Brief refresher on matrices","heading":"A.10.1 First order vector derivatives","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"gradient","chapter":"A Brief refresher on matrices","heading":"A.10.1.1 Gradient","text":"nabla operator (also known del operator) row vector\n\\[\n\\nabla =  (\\frac{\\partial}{\\partial x_1}, \\ldots, \n\\frac{\\partial}{\\partial x_d}) = \\frac{\\partial}{\\partial \\boldsymbol x}\n\\]\ncontaining\nfirst order partial derivative operators.gradient scalar-valued function\n\\(f(\\boldsymbol x)\\) vector argument \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\)\nalso row vector (\\(d\\) columns) \ncan expressed using nabla operator\n\\[\n\\begin{split}\n\\nabla f(\\boldsymbol x) &= \\left( \\frac{\\partial f(\\boldsymbol x)}{\\partial x_1}, \\ldots, \n\\frac{\\partial f(\\boldsymbol x)}{\\partial x_d} \\right) \\\\\n& = \n \\frac{\\partial f(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\text{grad} f(\\boldsymbol x) \\, .\\\\\n\\end{split}\n\\]\nNote various notations gradient.Example .5  \\(f(\\boldsymbol x)=\\boldsymbol ^T \\boldsymbol x+ b\\). \\(\\nabla f(\\boldsymbol x) = \\frac{\\partial f(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol ^T\\).Example .6  \\(f(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol x\\). \\(\\nabla f(\\boldsymbol x) = \\frac{\\partial f(\\boldsymbol x)}{\\partial \\boldsymbol x} = 2 \\boldsymbol x^T\\).Example .7  \\(f(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol \\boldsymbol x\\). \\(\\nabla f(\\boldsymbol x) = \\frac{\\partial f(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol x^T (\\boldsymbol + \\boldsymbol ^T)\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"jacobian-matrix","chapter":"A Brief refresher on matrices","heading":"A.10.1.2 Jacobian matrix","text":"vector-valued function\n\\[\n\\boldsymbol f(\\boldsymbol x) = ( f_1(\\boldsymbol x), \\ldots, f_m(\\boldsymbol x) )^T \\,.\n\\]\ncomputation gradient component yields\nJacobian matrix (\\(m\\) rows \\(d\\) columns)\n\\[\n\\begin{split}\n D \\boldsymbol f(\\boldsymbol x) &= \n\\left( {\\begin{array}{c}\n \\nabla f_1(\\boldsymbol x)   \\\\\n \\vdots   \\\\\n \\nabla f_m(\\boldsymbol x)   \\\\\n \\end{array} } \\right) \\\\\n& = \\left(\\frac{\\partial f_i(\\boldsymbol x)}{\\partial x_j}\\right) \\\\\n&= \\frac{\\partial \\boldsymbol f(\\boldsymbol x)}{\\partial \\boldsymbol x} \\\\\n\\end{split}\n\\]\n, note various notations Jacobian matrix!Example .8  \\(\\boldsymbol f(\\boldsymbol x)=\\boldsymbol \\boldsymbol x+ \\boldsymbol b\\). \\(D \\boldsymbol f(\\boldsymbol x) = \\frac{\\partial \\boldsymbol f(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol \\).\\(m=d\\) Jacobian matrix square matrix allows compute \nJacobian determinant \\[\\det  D \\boldsymbol f(\\boldsymbol x) = \\det\\left(\\frac{\\partial \\boldsymbol f(\\boldsymbol x)}{\\partial \\boldsymbol x}\\right)\\]\\(\\boldsymbol y= \\boldsymbol f(\\boldsymbol x)\\) invertible function \\(\\boldsymbol x= \\boldsymbol f^{-1}(\\boldsymbol y)\\)\nJacobian matrix invertible inverted matrix fact \nJacobian inverse function!allows compute Jacobian determinant backtransformation \ninverse Jacobian determinant original function:\n\\[\\det  D \\boldsymbol f^{-1}(\\boldsymbol y) = ( \\det  D \\boldsymbol f(\\boldsymbol x) )^{-1}\\]\nalternative notation\n\\[\\det  D \\boldsymbol x(\\boldsymbol y) = \\frac{1}{ \\det  D \\boldsymbol y(\\boldsymbol x) }\\].","code":""},{"path":"brief-refresher-on-matrices.html","id":"second-order-vector-derivatives","chapter":"A Brief refresher on matrices","heading":"A.10.2 Second order vector derivatives","text":"matrix second order partial derivates scalar-valued\nfunction vector-valued argument called Hessian matrix\ncomputed double application nabla operator:\n\\[\n\\begin{split}\n\\nabla^T \\nabla f(\\boldsymbol x) &=\n\\begin{pmatrix}\n  \\frac{\\partial^2 f(\\boldsymbol x)}{\\partial x_1^2}\n     & \\frac{\\partial^2 f(\\boldsymbol x)}{\\partial x_1 \\partial x_2} \n     & \\cdots \n     & \\frac{\\partial^2 f(\\boldsymbol x)}{\\partial x_1 \\partial x_d} \\\\\n  \\frac{\\partial^2 f(\\boldsymbol x)}{\\partial x_2 \\partial x_1} \n     & \\frac{\\partial^2 f(\\boldsymbol x)}{\\partial x_2^2}\n     & \\cdots \n     & \\frac{\\partial^2 f(\\boldsymbol x)}{\\partial x_2 \\partial x_d} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\frac{\\partial^2 f(\\boldsymbol x)}{\\partial x_d \\partial x_1} \n     & \\frac{\\partial^2 f(\\boldsymbol x)}{\\partial x_d \\partial x_2}  \n     & \\cdots \n     & \\frac{\\partial^2 f(\\boldsymbol x)}{\\partial x_d^2}\n \\end{pmatrix} \\\\\n& = \\left(\\frac{\\partial f(\\boldsymbol x)}{\\partial x_i \\partial x_j}\\right)  \n  = {\\left(\\frac{\\partial}{\\partial \\boldsymbol x}\\right)}^T \\frac{\\partial f(\\boldsymbol x)}{\\partial \\boldsymbol x}\n\\,.\\\\\n\\end{split}\n\\]\nconstruction square symmetric.","code":""},{"path":"brief-refresher-on-matrices.html","id":"first-order-matrix-derivatives","chapter":"A Brief refresher on matrices","heading":"A.10.3 First order matrix derivatives","text":"derivative scalar-valued function \\(f(\\boldsymbol X)\\) regard matrix argument \\(\\boldsymbol X\\)\ncan also defined results matrix\ntransposed dimensions compared \\(\\boldsymbol X\\).Two important specific examples :Example .9  \\(\\frac{\\partial \\text{Tr}(\\boldsymbol \\boldsymbol X)}{\\partial \\boldsymbol X} = \\boldsymbol \\)Example .10  \\(\\frac{\\partial \\log \\det(\\boldsymbol X)}{\\partial \\boldsymbol X} = \\frac{\\partial \\text{Tr}(\\log \\boldsymbol X)}{\\partial \\boldsymbol X} = \\boldsymbol X^{-1}\\)","code":""},{"path":"further-study.html","id":"further-study","chapter":"B Further study","heading":"B Further study","text":"module can touch surface field multivariate statistics machine learning. like study \nrecommend following books starting point.","code":""},{"path":"further-study.html","id":"recommended-reading","chapter":"B Further study","heading":"B.1 Recommended reading","text":"multivariate statistics machine learning:Härdle Simar (2015) Applied multivariate statistical analysis. 4th edition. Springer.Marden (2015) Multivariate Statistics: Old SchoolRogers Girolami (2017) first course machine learning (2nd Edition). Chapman Hall / CRC.James et al. (2021) introduction statistical learning applications R (2nd edition). Springer.","code":""},{"path":"further-study.html","id":"advanced-reading","chapter":"B Further study","heading":"B.2 Advanced reading","text":"Additional (advanced) reference books probabilistic machine learning :Bishop (2006) Pattern recognition machine learning. Springer.Hastie, Tibshirani, Friedman (2009) elements statistical learning: data mining, inference, prediction. Springer.Murphy (2012) Machine learning: probabilistic perspective. MIT Press.can find suggestions list online textbooks statistics machine learning.","code":""},{"path":"bibliography.html","id":"bibliography","chapter":"Bibliography","heading":"Bibliography","text":"Bishop, C. M. 2006. Pattern Recognition Machine Learning. Springer. https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/.Härdle, W. K., L. Simar. 2015. Applied Multivariate Statistical Analysis. Berlin: Springer.Hastie, T., R. Tibshirani, J. Friedman. 2009. Elements Statistical Learning: Data Mining, Inference, Prediction. 2nd ed. Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.James, G., D. Witten, T. Hastie, R. Tibshirani. 2013. Introduction Statistical Learning Applications R. Springer. https://www.statlearning.com.———. 2021. Introduction Statistical Learning Applications R. 2nd ed. Springer. https://www.statlearning.com.Marden, J. . 2015. Multivariate Statistics: Old School. CreateSpace. http://stat.istics.net/Multivariate.Murphy, K. P. 2012. Machine Learning: Probabilistic Perspective. MIT Press.Rogers, S., M. Girolami. 2017. First Course Machine Learning. 2nd ed. Chapman; Hall / CRC.Zhang, ., Z. C. Lipton, M. Li, . J. Smola. 2020. Dive Deep Learning. https://d2l.ai.","code":""}]
