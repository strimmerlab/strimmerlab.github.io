[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"lecture notes MATH20802, course Multivariate Statistics Machine Learning third year mathematics students Department Mathematics University Manchester.course text written Korbinian Strimmer 2018–2021. version 11 May 2021.notes updated time time. view current\nversion visit \nonline MATH38161 lecture notes.\nmay also download MATH38161 lecture notes PDF.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"notes licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"about-the-author","chapter":"Preface","heading":"About the author","text":"Hello! name Korbinian Strimmer Professor Statistics.\npart Statistics group\nDepartment Mathematics University Manchester. can find information home page.first taught module winter term 2018 University Manchester,\nsubsequently also 2019 2020.hope enjoy course! questions, comments, corrections please email korbinian.strimmer@manchester.ac.uk.","code":""},{"path":"preface.html","id":"about-the-module","chapter":"Preface","heading":"About the module","text":"","code":""},{"path":"preface.html","id":"topics-covered","chapter":"Preface","heading":"Topics covered","text":"MATH38161 module designed run course 11 weeks.\nsix parts, covering particular aspect multivariate statistics machine learning:Multivariate random variables estimation \nlarge small sample settings (W1 W2)Transformations dimension reduction (W3 W4)Unsupervised learning/clustering (W5 W6)Supervised learning/classification (W7 W8)Measuring modelling multivariate dependencies (W9)Nonlinear nonparametric models (W10, W11)module focuses :Concepts methods (theory)Implementation application RPractical data analysis interpretation (incl. report writing)Modern tools data science statistics (R markdown, R studio)","code":""},{"path":"preface.html","id":"additional-support-material","chapter":"Preface","heading":"Additional support material","text":"Accompanying notes arelecture videos (visualiser style).Furthermore, also MATH38161 online reading list hosted University Manchester library.University Manchester student enrolled module\nfind Blackboard:weekly learning plan 11 week study period,weekly worksheets examples (theory application R) solutions R Markdown, andexam papers previous years.","code":""},{"path":"preface.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"Many thanks Beatriz Costa Gomes help compile first draft course notes winter term 2018 graduate teaching assistant course. also thank many students suggested corrections.","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-random-variables","chapter":"1 Multivariate random variables","heading":"1 Multivariate random variables","text":"","code":""},{"path":"multivariate-random-variables.html","id":"why-multivariate-statistics","chapter":"1 Multivariate random variables","heading":"1.1 Why multivariate statistics?","text":"Science uses experiments verify hypotheses world.\nStatistics provides tools quantify procedure offers methods \nlink data (experiments) probabilistic models (hyptheses).\nSince world complex need complex models complex data, hence\nneed multivariate statistics machine learning.Specifically, multivariate statistics (opposed univariate statistics) concerned methods models random vectors random matrices, rather just random univariate (scalar) variables. Therefore, multivariate statistics frequently make use matrix notation.Closely related multivariate statistics (traditionally subfield statistics) machine learning (ML) traditionally subfield computer science. ML used focus algorithms rather probabilistic modelling nowadays machine learning methods fully based statistical multivariate approaches, two fields converging.Learning multivariate models allows us learn dependencies interactions among \ncomponents random variables turns allows draw conclusion world.Two main tasks:unsupervised learning (finding structure, clustering)supervised learning (training labeled data, followed prediction)Challenges:complexity model needs appropriate problem available data,high dimensions make estimation inference difficultcomputational issues.","code":""},{"path":"multivariate-random-variables.html","id":"essentials-in-multivariate-statistics","chapter":"1 Multivariate random variables","heading":"1.2 Essentials in multivariate statistics","text":"","code":""},{"path":"multivariate-random-variables.html","id":"univariate-vs.-multivariate-random-variables","chapter":"1 Multivariate random variables","heading":"1.2.1 Univariate vs. multivariate random variables","text":"Univariate random variable (dimension \\(d=1\\)):\n\\[x \\sim F\\]\n\\(x\\) scalar \\(F\\) distribution.\n\\(\\expect(x) = \\mu\\) denotes mean \\(\\var(x) = \\sigma^2\\) variance \\(x\\).Multivariate random vector dimension \\(d\\):\n\\[\\bx = (x_1, x_2,...,x_d)^T  \\sim F\\]\\(\\bx\\) vector valued random variable.vector \\(\\bx\\) column vector (=matrix size \\(d \\times 1\\)).\ncomponents \\(x_1, x_2,...,x_d\\) univariate random variables.\ndimension \\(d\\) also often denoted \\(p\\) \\(q\\).","code":""},{"path":"multivariate-random-variables.html","id":"mean-of-a-random-vector","chapter":"1 Multivariate random variables","heading":"1.2.2 Mean of a random vector","text":"mean / expectation random vector dimensions \\(d\\) also vector dimensions \\(d\\):\n\\[\\expect(\\bx) = \\bmu = \\begin{pmatrix}\n    \\expect(x_1)       \\\\\n    \\expect(x_2)       \\\\\n    \\vdots \\\\\n    \\expect(x_d)\n\\end{pmatrix} = \\left( \\begin{array}{l}\n    \\mu_1       \\\\\n    \\mu_2       \\\\\n    \\vdots \\\\\n    \\mu_d\n\\end{array}\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"variance-of-a-random-vector","chapter":"1 Multivariate random variables","heading":"1.2.3 Variance of a random vector","text":"Recall definition mean variance univariate random variable:\\[\\expect(x) = \\mu\\]\\[\\var(x) = \\sigma^2 = \\expect( (x-\\mu)^2 )=\\expect( (x-\\mu)(x-\\mu) ) = \\expect(x^2)-\\mu^2\\]Definition variance random vector:\\[\\var(\\bx) = \\underbrace{\\bSigma}_{d\\times d} = \n\\expect \\left(\\underbrace{(\\bx-\\bmu)}_{d\\times 1} \\underbrace{(\\bx-\\bmu)^T}_{1\\times d} \\right) \n = \\expect(\\bx \\bx^T)-\\bmu \\bmu^T\\]variance random vector , therefore, vector matrix!\\[\\bSigma = (\\sigma_{ij}) = \\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix}\\]matrix called Covariance Matrix, -diagonal elements \\(\\sigma_{ij}= \\cov(x_i,x_j)\\) diagonal \\(\\sigma_{ii}= \\var(X_i) = \\sigma_i^2\\).","code":""},{"path":"multivariate-random-variables.html","id":"properties-of-the-covariance-matrix","chapter":"1 Multivariate random variables","heading":"1.2.4 Properties of the covariance matrix","text":"\\(\\bSigma\\) real valued: \\(\\sigma_{ij} \\\\mathbb{R}\\)\\(\\bSigma\\) symmetric: \\(\\sigma_{ij} = \\sigma_{ji}\\)diagonal \\(\\bSigma\\) contains \\(\\sigma_{ii} = \\var(x_i) = \\sigma_i^2\\), .e. \nvariances components \\(\\bx\\).-diagonal elements \\(\\sigma_{ij} = \\cov(x_i,x_j)\\) represent linear dependencies among \\(x_i\\). \\(\\Longrightarrow\\) linear regression, correlationHow many separate entries \\(\\bSigma\\) ?\\[\\bSigma = (\\sigma_{ij}) = \\underbrace{\\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix}}_{d\\times d}\\]\n\\(\\sigma_{ij} = \\sigma_{ji}\\).Number separate entries: \\(\\frac{d(d+1)}{2}\\).numbers grows square dimension \\(d\\), .e. order O(\\(d^2\\)):large dimension \\(d\\) covariance matrix many components!–> computationally expensive (storage handling)\n–> challenging estimate high dimensions \\(d\\).Note: matrix inversion requires O(\\(d^3\\)) operations! , computing \\(\\bSigma^{-1}\\) difficult large \\(d\\)!","code":""},{"path":"multivariate-random-variables.html","id":"eigenvalue-decomposition-of-bsigma","chapter":"1 Multivariate random variables","heading":"1.2.5 Eigenvalue decomposition of \\(\\bSigma\\)","text":"Recall orthogonal eigendecomposition matrix analysis linear algebra: symmetric matrix real entries real eigenvalues complete set orthogonal eigenvectors.Applying eigenvalue decomposition covariance matrix yields\n\\[\n\\bSigma = \\bU \\bLambda \\bU^T\n\\]\n\\(\\bU\\) orthogonal matrix containing eigenvectors\n\n\\[\\bLambda = \\begin{pmatrix}\n    \\lambda_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}\n\\end{pmatrix}\\]\ncontains eigenvalues \\(\\lambda_i\\).Importantly, eigenvalues covariance matrix real valued\nconstrained non-negative.\ncan seen computing quadratic form \\(\\bz^T \\bSigma \\bz\\)\nnon-zero non-random vector \\(\\bz\\) yields\n\\[\n\\begin{split}\n\\bz^T  \\bSigma  \\bz   & = \\bz^T \\expect \\left(  (\\bx-\\bmu) (\\bx-\\bmu)^T  \\right) \\bz \\\\\n & =  \\expect \\left( \\bz^T (\\bx-\\bmu) (\\bx-\\bmu)^T \\bz  \\right) \\\\\n & =  \\expect \\left( \\left( \\bz^T (\\bx-\\bmu) \\right)^2 \\right) \\geq 0 \\, .\\\\\n\\end{split}\n\\]\nHence covariance matrix \\(\\bSigma\\) always\npositive semi-definite.fact, unless collinearity ( .e. variable linear function variables) eigenvalues positive \\(\\bSigma\\) positive definite.","code":""},{"path":"multivariate-random-variables.html","id":"quantities-related-to-the-covariance-matrix","chapter":"1 Multivariate random variables","heading":"1.2.6 Quantities related to the covariance matrix","text":"","code":""},{"path":"multivariate-random-variables.html","id":"correlation-matrix-brho","chapter":"1 Multivariate random variables","heading":"1.2.6.1 Correlation matrix \\(\\bRho\\)","text":"correlation matrix \\(\\bRho\\) (= upper case greek “rho”) standardised covariance matrix\\[\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}=\\cor(x_i,x_j)\\]\\[\\rho_{ii} = 1 = \\cor(x_i,x_i)\\]\\[ \\bRho = (\\rho_{ij}) = \\begin{pmatrix}\n    1 & \\dots & \\rho_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{d1} & \\dots & 1\n\\end{pmatrix}\\]\\(\\bRho\\) (“capital rho”) symmetric matrix (\\(\\rho_{ij}=\\rho_{ji}\\)).Note variance-correlation decomposition\\[\\bSigma = \\bV^{\\frac{1}{2}} \\bRho \\bV^{\\frac{1}{2}}\\]\\(\\bV\\) diagonal matrix containing variances:\\[ \\bV = \\begin{pmatrix}\n    \\sigma_{11} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma_{dd}\n\\end{pmatrix}\\]\\[\\bRho = \\bV^{-\\frac{1}{2}}\\bSigma \\bV^{-\\frac{1}{2}}\\]definition correlation written matrix notation.","code":""},{"path":"multivariate-random-variables.html","id":"precision-matrix-or-concentration-matrix","chapter":"1 Multivariate random variables","heading":"1.2.6.2 Precision matrix or concentration matrix","text":"\\[\\bOmega = (\\omega_{ij}) = \\bSigma^{-1}\\]\\(\\bOmega\\) (“Omega”) inverse covariance matrix.inverse covariance matrix can obtained via\nspectral decomposition, followed inverting eigenvalues \\(\\lambda_i\\):\n\\[\\bSigma^{-1} = \\bU \\bLambda^{-1} \\bU^T = \n \\bU \\begin{pmatrix}\n    \\lambda_{1}^{-1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}^{-1}\n\\end{pmatrix} \\bU^T \\]Note eigenvalues \\(\\lambda_i\\) need positive \\(\\bSigma\\) can inverted. (.e., \\(\\bSigma\\) needs positive definite).\n\\(\\lambda_i = 0\\) \\(\\bSigma\\) singular invertible.Importance \\(\\bSigma^{-1}\\):Many expressions multivariate statistics contain \\(\\bSigma^{-1}\\) \\(\\bSigma\\)\\(\\bSigma^{-1}\\) close connection graphical models\n(e.g. conditional independence graph, partial correlations, see later chapter)generally, \\(\\bSigma^{-1}\\) natural parameter exponential family.","code":""},{"path":"multivariate-random-variables.html","id":"partial-correlation-matrix","chapter":"1 Multivariate random variables","heading":"1.2.6.3 Partial correlation matrix","text":"standardised version precision matrix, see later chapter graphical models.","code":""},{"path":"multivariate-random-variables.html","id":"total-variation-and-generalised-variance","chapter":"1 Multivariate random variables","heading":"1.2.6.4 Total variation and generalised variance","text":"summarise covariance matrix \\(\\bSigma\\) single scalar value two commonly used\nmeasures:total variation: \\(\\trace(\\bSigma) = \\sum_{=1}^d \\lambda_i\\)generalised variance: \\(\\det(\\bSigma) = \\prod_{=1}^d \\lambda_i\\)generalised variance \\(\\det(\\bSigma)\\) also known volume \\(\\bSigma\\).","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-normal-distribution","chapter":"1 Multivariate random variables","heading":"1.3 Multivariate normal distribution","text":"multivariate normal model generalisation univariate normal distribution\ndimension 1 dimension \\(d\\).","code":""},{"path":"multivariate-random-variables.html","id":"univariate-normal-distribution","chapter":"1 Multivariate random variables","heading":"1.3.1 Univariate normal distribution:","text":"\\[\\text{Dimension } d = 1\\]\n\\[x \\sim N(\\mu, \\sigma^2)\\]\n\\[\\expect(x) = \\mu \\space , \\space  \\var(x) = \\sigma^2\\]Density:\\[f(x |\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right) \\]Plot univariate normal density :\n- Unimodal peak \\(\\mu\\), width determined \\(\\sigma\\) (plot: \\(\\mu=2, \\sigma=1\\) )Special case: standard normal \\(\\mu=0\\) \\(\\sigma^2=1\\):\\[f(x |\\mu=0,\\sigma^2=1)=\\frac{1}{\\sqrt{2\\pi}} \\exp\\left( {-\\frac{x^2}{2}} \\right) \\]Differential entropy:\\[\nH(F) = \\frac{1}{2} (\\log(2 \\pi \\sigma^2) + 1) \n\\]Cross-entropy:\\[\nH(F_{\\tref}, F) = \\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\tref})^2}{ \\sigma^2 } \n +\\frac{\\sigma^2_{\\tref}}{\\sigma^2}  +\\log(2 \\pi \\sigma^2) \\right)\n\\]\nKL divergence:\\[\n\\ikl(F_{\\tref}, F) = H(F_{\\tref}, F) - H(F_{\\tref}) = \n\\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\tref})^2}{ \\sigma^2 } \n +\\frac{\\sigma^2_{\\tref}}{\\sigma^2}  -\\log\\left(\\frac{\\sigma^2_{\\tref}}{ \\sigma^2}\\right) -1\n\\right)\n\\]Maximum entropy characterisation: normal distribution unique distribution\n\nhighest (differential) entropy continuous distributions support minus infinity plus infinity given mean variance.fact one reasons normal distribution important (und useful) –\nknow random variable mean variance, much else, using \nnormal distribution reasonable well justified working assumption!","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-normal-model","chapter":"1 Multivariate random variables","heading":"1.3.2 Multivariate normal model","text":"\\[\\text{Dimension } d\\]\n\\[\\bx \\sim N_d(\\bmu, \\bSigma)\\]\n\\[\\bx \\sim \\mvn(\\bmu,\\bSigma) \\]\n\\[\\expect(\\bx) = \\bmu \\space , \\space  \\var(\\bx) = \\bSigma\\]Density:\\[f(\\bx | \\bmu, \\bSigma) = \\det(2 \\pi \\bSigma)^{-\\frac{1}{2}} \\exp\\left({{-\\frac{1}{2}} \\underbrace{\\underbrace{(\\bx-\\bmu)^T}_{1 \\times d} \\underbrace{\\bSigma^{-1}}_{d \\times d} \\underbrace{(\\bx-\\bmu)}_{d \\times 1} }_{1 \\times 1 = \\text{scalar!}}}\\right)\\]note density contains precision matrix \\(\\bSigma^{-1}\\)inverting \\(\\bSigma\\) implies inverting eigenvalues \\(\\lambda_i\\) \\(\\bSigma\\)\n(thus need \\(\\lambda_i > 0\\))density also contains \\(\\det(\\bSigma) = \\prod\\limits_{=1}^d \\lambda_i\\) \\(\\equiv\\) product eigenvalues \\(\\bSigma\\)Special case: standard multivariate normal \\[\\bmu=\\bZero, \\bSigma=\\bI=\\begin{pmatrix}\n    1 & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & 1\n\\end{pmatrix}\\]\\[f(\\bx| \\bmu=\\bZero,\\bSigma=\\bI)=(2\\pi)^{-d/2}\\exp\\left( -\\frac{1}{2} \\bx^T \\bx \\right) = \\prod\\limits_{=1}^d \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_i^2}{2}\\right)\\]\nequivalent product \\(d\\) univariate standard normals!Misc:\\(d=1\\), multivariate normal reduces normal.\\(\\bSigma\\) diagonal (.e. \\(\\bRho = \\bI\\), correlation), MVN product univariate normals (see Worksheet 2).Plot MVN density:Location: \\(\\bmu\\)Shape: \\(\\bSigma\\)Unimodal: one peakSupport \\(-\\infty\\) \\(+\\infty\\) dimensionAn interactive R Shiny web app bivariate normal density plot\navailable online https://minerva..manchester.ac.uk/shiny/strimmer/bvn/ .Differential entropy:\\[\nH = \\frac{1}{2} (\\log \\det(2 \\pi \\bSigma) + d) \n\\]Cross-entropy:\\[\nH(F_{\\tref}, F) = \\frac{1}{2}   \\biggl\\{\n        (\\bmu-\\bmu_{\\tref})^T \\bSigma^{-1} (\\bmu-\\bmu_{\\tref})\n      + \\trace \\biggl(\\bSigma^{-1} \\bSigma_{\\tref} \\biggr)\n    + \\log \\det \\biggl( 2 \\pi \\bSigma \\biggr)    \\biggr\\} \n\\]\nKL divergence:\\[\n\\begin{split}\n\\ikl(F_{\\tref}, F) &= H(F_{\\tref}, F) - H(F_{\\tref}) \\\\\n&= \\frac{1}{2}   \\biggl\\{\n        (\\bmu-\\bmu_{\\tref})^T \\bSigma^{-1} (\\bmu-\\bmu_{\\tref})\n      + \\trace \\biggl(\\bSigma^{-1} \\bSigma_{\\tref} \\biggr)\n    - \\log \\det \\biggl( \\bSigma^{-1} \\bSigma_{\\tref} \\biggr) \n     - d   \\biggr\\} \\\\\n\\end{split}\n\\]","code":""},{"path":"multivariate-random-variables.html","id":"shape-of-the-multivariate-normal-density","chapter":"1 Multivariate random variables","heading":"1.3.3 Shape of the multivariate normal density","text":"Now show contour lines multivariate normal density always take form ellipse, radii ellipse determined eigenvalues \n\\(\\bSigma\\).start observing circle radius \\(r\\) around origin can described set points \\((x_1,x_2)\\) satisfying\n\\(x_1^2+x_2^2 = r^2\\), equivalently, \\(\\frac{x_1^2}{r^2} + \\frac{x_2^2}{r^2} = 1\\).\ngeneralised shape ellipse allowing (two dimensions) two radii\n\\(r_1\\) \\(r_2\\) \n\\(\\frac{x_1^2}{r_1^2} + \\frac{x_2^2}{r_2^2} = 1\\), vector notation\n\\(\\bx^T \\diag(r_1^2, r_2^2)^{-1} \\bx = 1\\). \\(d\\) dimensions allowing rotation \naxes shift origin 0 \\(\\bmu\\) condition ellipse \n\\[(\\bx-\\bmu)^T \\bQ \\, \\diag(r_1^2, \\ldots , r_d^2)^{-1} \\bQ^T (\\bx-\\bmu) = 1\\]\n\\(\\bQ\\) orthogonal rotation matrix.contour line probability density function set connected points density assumes constant value. case multivariate normal distribution keeping density fixed value implies \\((\\bx-\\bmu)^T \\bSigma^{-1} (\\bx-\\bmu) = c\\) \\(c\\) constant. Using eigenvalue decomposition \\(\\bSigma = \\bU \\bLambda \\bU^T\\) can rewrite condition \n\\[\n(\\bx-\\bmu)^T \\bU \\bLambda^{-1} \\bU^T (\\bx-\\bmu) = c \\,.\n\\]\nimplies contour lines multivariate normal density indeed ellipses squared radii proportional eigenvalues \\(\\bSigma\\). Equivalently, positive square roots eigenvalues proportional radii ellipse. Hence, singular covariance matrix one \\(\\lambda_i=0\\) corresponding radii zero.interactive R Shiny web app play contour lines \nbivariate normal distribution online https://minerva..manchester.ac.uk/shiny/strimmer/bvn/ .","code":""},{"path":"multivariate-random-variables.html","id":"three-types-of-covariances","chapter":"1 Multivariate random variables","heading":"1.3.4 Three types of covariances","text":"Following can parameterise covariance matrix terms \n) volume, ii) shape, iii) orientation writing\n\\[\n\\bSigma = \\lambda \\, \\bU \\bA \\bU^T\n\\]\n\\(\\bA=\\diag(a_1, \\ldots, a_d)\\) \\(\\prod_{=1}^d a_i = 1\\) (\\(\\bLambda=\\lambda \\bA\\)).volume \\(\\det(\\bSigma) = \\lambda^d\\), determined single parameter \\(\\lambda\\).shape determined \\(\\bA\\), \\(d-1\\) free parameters.orientation given orthogonal matrix \\(\\bU\\), \\(d (d-1)/2\\) free parameters.leads classification covariances three varieties:Type 1: spherical covariance \\(\\bSigma=\\lambda \\bI\\),\nspherical contour lines, 1 free parameter (\\(\\bA=\\bI\\), \\(\\bU=\\bI\\)).Example:\n\\(\\bSigma = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1/ \\lambda_2} = 1\\):Type 2: diagonal covariance \\(\\bSigma = \\lambda \\bA\\), elliptical contour lines oriented along coordinate axes, \\(d\\) free parameters (\\(\\bU=\\bI\\)).Example:\n\\(\\bSigma = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1 / \\lambda_2} \\approx 1.41\\):Type 3: general unrestricted covariance \\(\\bSigma\\),\nelliptical contour lines oriented direction,\n\\(d (d+1)/2\\) free parameters.Example:\n\\(\\bSigma = \\begin{pmatrix} 2 & 0.6 \\\\ 0.6 & 1 \\end{pmatrix}\\)\n\\(\\sqrt{\\lambda_1 / \\lambda_2} \\approx 2.20\\):","code":""},{"path":"multivariate-random-variables.html","id":"estimation-in-large-sample-and-small-sample-settings","chapter":"1 Multivariate random variables","heading":"1.4 Estimation in large sample and small sample settings","text":"practical application multivariate normal model need \nlearn parameters data. first consider case \nmany measurements available, second case \nnumber data points small compared number parameters.previous course year 2\n(see MATH20802 Statistical Methods)\nmethod maximum likelihood well essential Bayesian statistics\nintroduced. apply approaches setting \nmultivariate normal distribution.","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-data","chapter":"1 Multivariate random variables","heading":"1.4.1 Multivariate data","text":"Vector notation:Samples multivariate normal distribution vectors (scalars univariate normal):\n\\[\\bx_1,\\bx_2,...,\\bx_n \\stackrel{\\text{iid}}\\sim N_d\\left(\\bmu,\\bSigma\\right)\\]Matrix component notation:data points commonly collected matrix \\(\\bX\\).statistics convention store data vector rows \\(\\bX\\):\\[\\bX = (\\bx_1,\\bx_2,...,\\bx_n)^T = \\begin{pmatrix}\n    x_{11}  & x_{12} & \\dots & x_{1d}   \\\\\n    x_{21}  & x_{22} & \\dots & x_{2d}   \\\\\n    \\vdots \\\\\n    x_{n1}  & x_{n2} & \\dots & x_{nd}\n\\end{pmatrix}\\]Therefore,\n\\[\\bx_1=\\begin{pmatrix}\n    x_{11}       \\\\\n    \\vdots \\\\\n    x_{1d}\n\\end{pmatrix} , \\space \\bx_2=\\begin{pmatrix}\n    x_{21}       \\\\\n    \\vdots \\\\\n    x_{2d}\n\\end{pmatrix} , \\ldots , \\bx_n=\\begin{pmatrix}\n    x_{n1}       \\\\\n    \\vdots \\\\\n    x_{nd}\n\\end{pmatrix}\\]Thus, statistics first index runs \\((1,...,n)\\) denotes samples second index runs \\((1,...,d)\\) refers variables.statistics convention data matrices universal! fact, machine learning literature engineering computer science data samples stored columns variables appear rows (thus engineering convention data matrix transposed compared statistics convention).order avoid confusion recommended use vector notation data instead \nmatrix notation ambiguous.","code":""},{"path":"multivariate-random-variables.html","id":"strategies-for-large-sample-estimation","chapter":"1 Multivariate random variables","heading":"1.4.2 Strategies for large sample estimation","text":"","code":""},{"path":"multivariate-random-variables.html","id":"empirical-estimators-outline","chapter":"1 Multivariate random variables","heading":"1.4.2.1 Empirical estimators (outline)","text":"large \\(n\\) thanks law large numbers:\n\\[\\underbrace{F}_{\\text{true}} \\approx \\underbrace{\\widehat{F}}_{\\text{empirical}}\\]now like estimate \\(\\) functional \\(F\\), .e. \\(=m(F)\\).\nexample mean, median quantity.empirical estimate obtained replacing unknown true distribution\n\\(F\\) observed empirical distribution: \\(\\hat{} = m(\\widehat{F})\\).example, expectation random variable approximated/estimated\naverage observation:\n\\[\\expect_F(\\bx) \\approx \\expect_{\\widehat{F}}(\\bx) = \\frac{1}{n}\\sum^{n}_{k=1} \\bx_k\\]\n\\[\\expect_F(g(\\bx)) \\approx  \\expect_{\\widehat{F}}(g(\\bx)) = \\frac{1}{n}\\sum^{n}_{k=1} g(\\bx_k)\\]Simple recipe obtain empirical estimator: simply replace expectation operator\nsample average quantity interest.work: empirical distribution \\(\\widehat{F}\\) actually nonparametric maximum likelihood estimate \\(F\\) (see likelihood estimation).Note: approximation \\(F\\) \\(\\widehat{F}\\) also basis approaches Efron’s bootstrap method.","code":""},{"path":"multivariate-random-variables.html","id":"maximum-likelihood-estimation-outline","chapter":"1 Multivariate random variables","heading":"1.4.2.2 Maximum likelihood estimation (outline)","text":"R.. Fisher (1922): model-based estimators using density probability mass functionlog-likelihood function:\n\\[\\log L(\\btheta) = \\sum^{n}_{k=1}  \\underbrace{\\log f}_{\\text{log-density}}(\\underbrace{x_i}_{\\text{data}} |\\underbrace{\\btheta}_{\\text{parameters}})\\]\nlikelihood = probability observe data given model parametersMaximum likelihood estimate:\n\\[\\hat{\\btheta}^{\\text{ML}}=\\underset{\\btheta}{\\argmax} \\log L(\\btheta)\\]Maximum likelihood (ML) finds parameters make observed data likely (find probable model!)Recall MATH20802 Statistical Methods\nmaximum likelihood closely linked minimising relative entropy (KL divergence)\n\\(\\ikl(F, F_{\\btheta})\\) unknown true model \\(F\\) specified model \\(F_{\\btheta}\\). Specifically, large\nsample size \\(n\\) model \\(F_{\\hat{\\btheta}}\\) fit maximum likelihood indeed model closest \\(F\\).Correspondingly, great appeal maximum likelihood estimates (MLEs) optimal large \\(\\mathbf{n}\\), .e. large sample size estimator can constructed outperforms MLE (note emphasis “large \\(n\\)”!).\nadvantage method maximum likelihood provide point estimate also asymptotic error (via Fisher information related curvature log-likelihood function).","code":""},{"path":"multivariate-random-variables.html","id":"large-sample-estimates-of-mean-bmu-and-covariance-bsigma","chapter":"1 Multivariate random variables","heading":"1.4.3 Large sample estimates of mean \\(\\bmu\\) and covariance \\(\\bSigma\\)","text":"","code":""},{"path":"multivariate-random-variables.html","id":"empirical-estimates","chapter":"1 Multivariate random variables","heading":"1.4.3.1 Empirical estimates:","text":"Recall definitions:\n\\[\n\\bmu = \\expect(\\bx)\n\\]\n\n\\[\n\\bSigma = \\expect \\left(   (\\bx-\\bmu) (\\bx-\\bmu )^T \\right)\n\\]empirical estimate replace expectations \ncorresponding sample averages.resulting estimators can written three different ways:Vector notation:\\[\\hat{\\bmu} = \\frac{1}{n}\\sum^{n}_{k=1} \\bx_k\\]\\[\n\\widehat{\\bSigma} = \\frac{1}{n}\\sum^{n}_{k=1} (\\bx_k-\\hat{\\bmu})  (\\bx_k-\\hat{\\bmu})^T\n= \\frac{1}{n}\\sum^{n}_{k=1} \\bx_k  \\bx_k^T - \\hat{\\bmu} \\hat{\\bmu}^T\n\\]Data matrix notation:empirical mean covariance can also written terms data matrix \\(\\bX\\) (using statistics convention):\\[\\hat{\\bmu} = \\frac{1}{n} \\bX^T \\bOne_n\\]\\[\\widehat{\\bSigma} = \\frac{1}{n} \\bX^T \\bX - \\hat{\\bmu} \\hat{\\bmu}^T\\]See Worksheet 2 details.Component notation:corresponding component notation \\(\\bX = (x_{ki})\\) :\\[\\hat{\\mu}_i = \\frac{1}{n}\\sum^{n}_{k=1} x_{ki}\\]\\[\\hat{\\sigma}_{ij} = \\frac{1}{n}\\sum^{n}_{k=1} (x_{ki}-\\hat{\\mu}_i) ( \nx_{kj}-\\hat{\\mu}_j )\\]\\[\\hat{\\bmu}=\\begin{pmatrix}\n    \\hat{\\mu}_{1}       \\\\\n    \\vdots \\\\\n    \\hat{\\mu}_{d}\n\\end{pmatrix}, \\widehat{\\bSigma} = (\\hat{\\sigma}_{ij})\\]Variance estimate:\\[\\hat{\\sigma}_{ii} = \\frac{1}{n}\\sum^{n}_{k=1} \\left(x_{ki}-\\hat{\\mu}_i\\right)^2\\]\nNote factor \\(\\frac{1}{n}\\) (\\(\\frac{1}{n-1}\\))Engineering machine learning convention:Using engineering machine learning convention data matrix \\(\\bX\\) estimators written \\[\\hat{\\bmu} = \\frac{1}{n} \\bX \\bOne_n\\]\\[\\widehat{\\bSigma} = \\frac{1}{n} \\bX \\bX^T - \\hat{\\bmu} \\hat{\\bmu}^T\\]corresponding component notation two indices columns rowns interchanged.avoid confusion using matrix component notation need always state \nconvention used! notes strictly follow statistics convention.","code":""},{"path":"multivariate-random-variables.html","id":"maximum-likelihood-estimates","chapter":"1 Multivariate random variables","heading":"1.4.3.2 Maximum likelihood estimates","text":"now derive MLE parameters \\(\\bmu\\) \\(\\bSigma\\) multivariate normal distribution.\ncorresponding log-likelihood function \n\\[\n\\begin{split}\n\\log L(\\bmu, \\bSigma) & = \\sum_{k=1}^n \\log f( \\bx_k | \\bmu, \\bSigma ) \\\\\n  & = -\\frac{n d}{2} \\log(2\\pi) -\\frac{n}{2} \\log \\det(\\bSigma)  \n   - \\frac{1}{2}  \\sum_{k=1}^n  (\\bx_k-\\bmu)^T \\bSigma^{-1} (\\bx_k-\\bmu) \\,.\\\\\n\\end{split}\n\\]\nWritten terms precision matrix \\(\\bOmega = \\bSigma^{-1}\\) becomes\n\\[\n\\log L(\\bmu, \\bOmega) = -\\frac{n d}{2} \\log(2\\pi) +\\frac{n}{2} \\log \\det(\\bOmega)  - \\frac{1}{2}  \\sum_{k=1}^n  (\\bx_k-\\bmu)^T \\bOmega (\\bx_k-\\bmu) \\,.\n\\]\nExploiting identities trace log det (see Appendix) can rewrite\n\\((\\bx_k-\\bmu)^T \\bOmega (\\bx_k-\\bmu) = \\trace( (\\bx_k-\\bmu) (\\bx_k-\\bmu)^T \\bOmega )\\)\n\\(\\log \\det(\\bOmega) = \\trace( \\log \\bOmega)\\)\nrewrite log-likelihood \n\\[\\log L(\\bmu, \\bOmega) =  -\\frac{n d}{2} \\log(2\\pi) +\\frac{n}{2}  \\trace( \\log \\bOmega)  - \\frac{1}{2}  \\sum_{k=1}^n  \\trace( (\\bx_k-\\bmu) (\\bx_k-\\bmu)^T \\bOmega) \\,.\\]First, find MLE \\(\\bmu\\) compute (see Appendix rules vector matrix calculus)\n\\[\\frac{\\partial \\log L(\\bmu, \\bOmega) }{\\partial \\bmu}= \\sum_{k=1}^n (\\bx_k-\\bmu)^T  \\bOmega \\,.\\]\nSetting equal zero get \\(\\sum_{k=1}^n \\bx_k = n \\hat{\\bmu}_{ML}\\) thus\n\\[\\hat{\\bmu}_{ML} = \\frac{1}{n} \\sum_{k=1}^n \\bx_k\\,.\\]Next, obtain MLE \\(\\bOmega\\) compute\n\\[\\frac{\\partial \\log L(\\bmu, \\bOmega) }{\\partial \\bOmega}=\\frac{n}{2}\\bOmega^{-1} - \\frac{1}{2}  \\sum_{k=1}^n (\\bx_k-\\bmu) (\\bx_k-\\bmu)^T\\,.\\]\nSetting equal zero substituting MLE \\(\\bmu\\) get\n\\[\\widehat{\\bOmega}^{-1}_{ML}=  \\frac{1}{n} \\sum_{k=1}^n  (\\bx_k-\\hat{\\bmu}) (\\bx_k-\\hat{\\bmu})^T=\\widehat{\\bSigma}_{ML}\\,.\\]Therefore, MLEs identical empirical estimates.Note factor \\(\\frac{1}{n}\\) MLE covariance matrix.","code":""},{"path":"multivariate-random-variables.html","id":"distribution-of-the-empirical-maximum-likelihood-estimates","chapter":"1 Multivariate random variables","heading":"1.4.3.3 Distribution of the empirical / maximum likelihood estimates","text":"\\(\\bx_1,...,\\bx_n \\sim N_d(\\bmu, \\bSigma)\\) one can find exact distributions\nestimators.1. Distribution estimate mean:\\[\\hat{\\bmu}_{ML} \\sim N_d\\left(\\bmu, \\frac{\\bSigma}{n}\\right)\\]\nSince\n\\(\\expect(\\hat{\\bmu}_{ML}) = \\bmu \\Longrightarrow \\hat{\\bmu}_{ML}\\) unbiased.2. Distribution covariance estimate:\\[\\widehat{\\bSigma}_{ML} \\sim \\text{Wishart}(\\frac{\\bSigma}{n}, n-1)\\]\nSince\n\\(\\expect(\\widehat{\\bSigma}_{ML}) = \\frac{n-1}{n}\\bSigma\\) \\(\\Longrightarrow \\widehat{\\bSigma}_{ML}\\) biased, \\(\\bias(\\widehat{\\bSigma}_{ML} ) = \\bSigma- \\expect(\\widehat{\\bSigma}_{ML}) = -\\frac{\\bSigma}{n}\\).Easy make unbiased:\n\\(\\widehat{\\bSigma}_{UB} = \\frac{n}{n-1}\\widehat{\\bSigma}=\\frac{1}{n-1}\\sum^n_{k=1}\\left(\\bx_k-\\hat{\\bmu}\\right)\\left(\\bx_k-\\hat{\\bmu}\\right)^T\\) unbiased.unbiasedness estimator relevant criterion multivariate statistics see next section.","code":""},{"path":"multivariate-random-variables.html","id":"problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions","chapter":"1 Multivariate random variables","heading":"1.4.4 Problems with maximum likelihood in small sample settings and high dimensions","text":"Modern data high dimensional!Data sets \\(n<d\\), .e. high dimension \\(d\\) small sample size \\(n\\) now common \nmany fields, e.g., medicine, biology also finance business analytics.\\[n = 100 \\, \\text{(e.g, patients/samples)}\\]\n\\[d = 20000 \\, \\text{(e.g., genes/SNPs/proteins/variables)}\\]\nReasons:number measured variables increasing quickly technological advances (e.g. genomics)number samples similary increased (cost ethical reasons)General problems MLEs:ML estimators optimal sample size large compared number parameters. However, optimality valid sample size moderate smaller number parameters.enough data ML estimate overfits. means ML fits current data perfectly resulting model generalise well (.e. model perform poorly prediction)choice different models different complexity ML always select model largest number parameters.-> high-dimensional data small sample size maximum likelihood estimation work!!!History Statistics:Much modern statistics (1960 onwards) devoted development inference estimation techniques work complex, high-dimensional data.Maximum likelihood method classical statistics (time 1960).1960 modern (computational) statistics emerges, starting \n“Stein Paradox” (1956): Charles Stein showed multivariate setting ML estimators dominated (= always worse ) shrinkage estimators!example, shrinkage estimator mean better (terms MSE) average (MLE)!Modern statistics developed many different (related) methods use high-dimensional, small sample settings:regularised estimatorsshrinkage estimatorspenalised maximum likelihood estimatorsBayesian estimatorsEmpirical Bayes estimatorsKL / entropy-based estimatorsMost scope class, covered advanced statistical courses.Next, describe simple regularised estimator estimation covariance\nuse later (.e. classification).","code":""},{"path":"multivariate-random-variables.html","id":"estimation-of-covariance-matrix-in-small-sample-settings","chapter":"1 Multivariate random variables","heading":"1.4.5 Estimation of covariance matrix in small sample settings","text":"Problems ML estimate \\(\\bSigma\\)\\(\\Sigma\\) O(\\(d^2\\)) number parameters!\n\\(\\Longrightarrow \\hat{\\bSigma}^{\\mle}\\) requires lot data! \\(n\\gg d \\text{ } d^2\\)\\(\\Sigma\\) O(\\(d^2\\)) number parameters!\n\\(\\Longrightarrow \\hat{\\bSigma}^{\\mle}\\) requires lot data! \\(n\\gg d \\text{ } d^2\\)\\(n < d\\) \\(\\hat{\\bSigma}\\) positive semi-definite (even \\(\\Sigma\\) p.d.f.!)\\(\\Longrightarrow \\hat{\\bSigma}\\) vanishing eigenvalues (\\(\\lambda_i=0\\)) thus inverted singular!\\(n < d\\) \\(\\hat{\\bSigma}\\) positive semi-definite (even \\(\\Sigma\\) p.d.f.!)\\(\\Longrightarrow \\hat{\\bSigma}\\) vanishing eigenvalues (\\(\\lambda_i=0\\)) thus inverted singular!Simple regularised estimate \\(\\bSigma\\)Regularised estimator \\(\\bS^\\ast\\) = convex combination \\(\\bS = \\hat{\\bSigma}^\\mle\\) \\(\\bI_d\\) (identity matrix) getRegularisation:\n\\[\\underbrace{\\bS^\\ast}_{\\text{regularised estimate}} = \\underbrace{\\lambda}_{\\text{shrinkage intensity}} \\, \\underbrace{\\bI_d}_{\\text{target}} + (1-\\lambda)\\underbrace{\\bS}_{\\text{ML estimate}}\\]\nNext, choose \\(\\lambda \\[0,1]\\) \\(\\bS^\\ast\\) better (terms MSE) \\(\\bS\\) \\(\\bI_d\\).Bias-variance trade-\\(\\mse\\) Mean Squared Error, composed squared bias variance.\\[\\mse(\\theta) = \\expect((\\hat{\\theta}-\\theta)^2) = \\bias(\\hat{\\theta})^2 + \\var(\\hat{\\theta})\\]\n\\(\\bias(\\hat{\\theta}) = \\expect(\\hat{\\theta})-\\theta\\)\\(\\bS\\): ML estimate, many parameters, low bias, high variance\\(\\bI_d\\): “target”, parameters, high bias, low variance\\(\\Longrightarrow\\) reduce high variance \\(\\bS\\) introducing bit bias \\(\\bI_d\\)!\\(\\Longrightarrow\\) overall, \\(\\mse\\) decreasedHow find optimal shrinkage / regularisation parameter \\(\\lambda\\)? Minimise \\(\\mse\\)!Challenge: since don’t know true \\(\\bSigma\\) actually compute \\(\\mse\\) directly estimate ! done practise?cross-validation (=resampling procedure)using analytic approximation (e.g. Stein’s formula)regularisation \\(\\hat{\\bSigma}\\) work?\\(\\bS^\\ast\\) positive definite:\nMatrix Theory:\\[\\underbrace{\\bM_1}_{ \\text{symmetric positive definite, } \\lambda \\bI} + \\underbrace{\\bM_2}_{\\text{symmetric positive semi-definite, } (1-\\lambda) \\bS} = \\underbrace{\\bM_3}_{\\text{symmetric positive definite, } \\bS^\\ast} \\]\\(\\Longrightarrow \\bS^\\ast\\) can inverted even \\(n<d\\)(see Appendix details).’s Bayesian disguise!\n\\[\\underbrace{\\bS^\\ast}_{\\text{posterior mean}} = \\underbrace{\\lambda \\bI_d}_{\\text{prior information}}  + (1-\\lambda)\\underbrace{\\bS}_{\\text{data summarised maximum likelihood}}\\]\nPrior information helps infer \\(\\bSigma\\) even small samples\nSince \\(\\lambda\\) chosen data, actually empirical Bayes.\nalso called shrinkage estimator since -diagonal entries shrunk towards zero\ntype linear shrinkage/regularisation natural models exponential family (Diaconis Ylvisaker, 1979)\nPrior information helps infer \\(\\bSigma\\) even small samplesSince \\(\\lambda\\) chosen data, actually empirical Bayes.also called shrinkage estimator since -diagonal entries shrunk towards zerothis type linear shrinkage/regularisation natural models exponential family (Diaconis Ylvisaker, 1979)Worksheet 2 empirical estimator covariance compared covariance estimator implemented R package\n“corpcor”. uses regularisation similar (correlation rather \ncovariance matrix) \nemploys analytic data-adaptive estimate shrinkage intensity \\(\\lambda\\).\nestimator variant empirical Bayes / James-Stein estimator (see MATH20802 Statistical Methods).\n.SummaryIn multivariate statistics, useful (often necessary) utilise prior information!Regularisation introduces bias reduces variance, minimising overall MSEUnbiased estimation (highly valued property classical statistics!) good idea multivariate settings often leads poor estimators.","code":""},{"path":"multivariate-random-variables.html","id":"categorical-and-multinomial-distribution","chapter":"1 Multivariate random variables","heading":"1.5 Categorical and multinomial distribution","text":"","code":""},{"path":"multivariate-random-variables.html","id":"categorical-distribution","chapter":"1 Multivariate random variables","heading":"1.5.1 Categorical distribution","text":"categorical distribution generalisation Bernoulli distribution\ncorrespondingly also known Multinoulli distribution.Assume \\(K\\) classes labeled “class 1”, “class 2”, …, “class K”.\ndiscrete random variable state space consisting \\(K\\) classes\ncategorical distribution \\(\\catdist(\\bpi)\\).\nparameter vector\n\\(\\bpi = (\\pi_1, \\ldots, \\pi_K)^T\\) specifies\nprobabilities \\(K\\) classes \\(\\prob(\\text{\"class k\"}) = \\pi_k\\).\nparameters satisfy \\(\\pi_k \\[0,1]\\) \n\\(\\sum_{k=1}^K \\pi_k = 1\\), hence \\(K-1\\) independent parameters categorical distribution (\\(K\\)).Sampling categorical distributions \\(\\catdist(\\bpi)\\) yields one \\(K\\) classes.\nseveral ways numerically\nrepresent “class k”, example simply corresponding number \\(k\\). However, instead\n“integer encoding” often\nconvenient use -called “one hot encoding” class\nrepresented indicator vector\n\\(\\bx = (x_1, \\ldots, x_K)^T = (0, 0, \\ldots, 1, \\ldots, 0)^T\\) containing zeros everywhere except \nelement \\(x_k=1\\) position \\(k\\). Thus \\(x_k \\\\{ 0, 1\\}\\) \\(\\sum_{k=1}^K x_k = 1\\).expectation \\(\\bx \\sim \\catdist(\\bpi)\\) \\(\\expect(\\bx) = \\bpi\\), \n\\(\\expect(x_k) = \\pi_k\\).\ncovariance matrix \\(\\var(\\bx) = \\diag(\\bpi) - \\bpi \\bpi^T\\).\ncomponent notation \\(\\var(x_i) = \\pi_i (1-\\pi_i)\\) \\(\\cov(x_i, x_j) = -\\pi_i \\pi_j\\).\nfollows directly definition variance \\(\\var(\\bx) = \\expect( \\bx \\bx^T) - \\expect( \\bx) \\expect( \\bx)^T\\)\nnoting \\(x_i^2 = x_i\\) \\(x_i x_j = 0\\) \\(\\neq j\\).\nNote variance matrix \\(\\var(\\bx)\\) singular construction, \\(K\\) random variables\n\\(x_1, \\ldots, x_K\\) dependent constraint \\(\\sum_{k=1}^K x_k = 1\\).corresponding probability mass function (pmf)\ncan written conveniently terms \\(x_k\\) \n\\[\nf(\\bx) = \\prod_{k=1}^K \\pi_k^{x_k} = \n\\begin{cases} \n   \\pi_k  & \\text{} x_k = 1 \\\\\n\\end{cases}\n\\]\nlog pmf \n\\[\n\\log f(\\bx) = \\sum_{k=1}^K x_k \\log \\pi_k   =\n\\begin{cases} \n   \\log \\pi_k  & \\text{} x_k = 1 \\\\\n\\end{cases}\n\\]order explicit categorical distribution \\(K-1\\) \\(K\\) parameters\nrewrite log-density \n\\(\\pi_K = 1 - \\sum_{k=1}^{K-1} \\pi_k\\) \\(x_K = 1 - \\sum_{k=1}^{K-1} x_k\\) \n\\[\n\\begin{split}\n\\log f(\\bx) & =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + x_K \\log \\pi_K \\\\\n & =\\sum_{k=1}^{K-1}  x_k \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_k  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\\\\n\\end{split}\n\\]\nNote particular reason choose \\(\\pi_K\\) derived, place \n\\(\\pi_k\\) may selected.\\(K=2\\) categorical distribution reduces Bernoulli \\(\\berdist(p)\\) distribution,\n\\(\\pi_1=p\\) \\(\\pi_2=1-p\\).","code":""},{"path":"multivariate-random-variables.html","id":"multinomial-distribution","chapter":"1 Multivariate random variables","heading":"1.5.2 Multinomial distribution","text":"multinomial distribution arises repeated categorical sampling,\njust like Binomial distribution arises repeated Bernoulli sampling.","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case","chapter":"1 Multivariate random variables","heading":"1.5.2.1 Univariate case","text":"Binomial Distribution:Repeat Bernoulli \\(\\berdist(\\pi)\\) experiment \\(n\\) times:\\[x \\sim \\bindist(n, \\pi)\\]\n\\[ x \\\\{0,...,n\\}\\]\n\\[\\expect(x) = n \\, \\pi\\]\n\\[\\var(x)=n \\, \\pi(1-\\pi)\\]Standardised unit interval:\n\\[\\frac{x}{n} \\\\left\\{0,\\frac{1}{n},...,1\\right\\}\\]\n\\[\\expect\\left(\\frac{x}{n}\\right) = \\pi\\]\n\\[\\var\\left(\\frac{x}{n}\\right)=\\frac{\\pi(1-\\pi)}{n}\\]\\[\\textbf{Urn model:}\\]distribute \\(n\\) balls two bins","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case","chapter":"1 Multivariate random variables","heading":"1.5.2.2 Multivariate case","text":"Multinomial distribution:Draw \\(n\\) times categorical distribution \\(\\catdist(\\bpi)\\):\\[\\bx \\sim \\multdist(n, \\bpi)  \\]\n\\[ x_i \\\\{0,1,...,n\\}; \\, \\sum^{K}_{=1}x_i = n\\]\n\\[\\expect(\\bx) = n \\,\\bpi\\]\n\\[\\var(x_i)=n\\, \\pi_i(1-\\pi_i)\\]\n\\[\\cov(x_i,x_j)=-n\\, \\pi_i\\pi_j\\]Standardised unit interval:\n\\[\\frac{x_i}{n} \\\\left\\{0,\\frac{1}{n},\\frac{2}{n},...,1\\right\\}\\]\n\\[\\expect\\left(\\frac{\\bx}{n}\\right) = \\bpi\\]\n\\[\\var\\left(\\frac{x_i}{n}\\right)=\\frac{\\pi_i(1-\\pi_i)}{n}\\]\n\\[\\cov\\left(\\frac{x_i}{n},\\frac{x_j}{n}\\right)=-\\frac{\\pi_i\\pi_j}{n} \\]\n\\[\\textbf{Urn model:}\\]distribute \\(n\\) balls \\(K\\) bins:","code":""},{"path":"multivariate-random-variables.html","id":"entropy-and-maximum-likelihood-analysis-for-the-categorical-distribution","chapter":"1 Multivariate random variables","heading":"1.5.3 Entropy and maximum likelihood analysis for the categorical distribution","text":"folling examples compute KL divergence MLE well related quantities categorical distribution.generalise calculations Bernoulli distribution discussed earlier module MATH20802 Statistical Methods.Example 1.1  KL divergence two categorical distributions \\(K\\) classes:\\(P=\\catdist(\\bp)\\) \\(Q=\\catdist(\\bq)\\) corresponding\nprobabilities \\(p_1,\\dots,p_K\\) \\(q_1,\\dots,q_K\\) satisfying \\(\\sum_{=1}^K p_i =1\\) \\(\\sum_{=1}^K q_i = 1\\) get:\\[\\begin{equation*}\n\\ikl(P, Q)=\\sum_{=1}^K p_i\\log\\left(\\frac{p_i}{q_i}\\right) \n\\end{equation*}\\]explicit \\(K-1\\) parameters categorical distribution can also write\n\\[\\begin{equation*}\n\\ikl(P, Q)=\\sum_{=1}^{K-1} p_i\\log\\left(\\frac{p_i}{q_i}\\right)  + p_K\\log\\left(\\frac{p_K}{q_K}\\right)\n\\end{equation*}\\]\n\\(p_K=\\left(1- \\sum_{=1}^{K-1} p_i\\right)\\) \n\\(q_K=\\left(1- \\sum_{=1}^{K-1} q_i\\right)\\).Example 1.2  Expected Fisher information categorical distribution:first compute Hessian matrix\n\\(\\nabla^T \\nabla \\log f(\\bx)\\) log-probability mass function, \ndifferentiation regard \\(\\pi_1, \\ldots, \\pi_{K-1}\\).diagonal entries Hessian matrix (\\(=1, \\ldots, K-1\\)) \n\\[\n\\frac{\\partial^2}{\\partial \\pi_i^2} \\log f(\\bx) =\n  -\\frac{x_i}{\\pi_i^2}-\\frac{x_K}{\\pi_K^2}\n\\]\n-diagonal entries (\\(j=1, \\ldots, K-1\\))\n\\[\n\\frac{\\partial^2}{\\partial \\pi_i \\partial \\pi_j} \\log f(\\bx) =\n -\\frac{ x_K}{\\pi_K^2}\n\\]\nRecalling \\(\\expect(x_i) = \\pi_i\\) can compute expected Fisher information matrix categorical distribution \n\\[\n\\begin{split}\n\\bI^{\\fisher}\\left( \\pi_1, \\ldots, \\pi_{K-1}  \\right) &= -\\expect \\left( \\nabla^T \\nabla \\log f(\\bx) \\right) \\\\\n& =\n\\begin{pmatrix}\n \\frac{1}{\\pi_1} + \\frac{1}{\\pi_K} & \\cdots & \\frac{1}{\\pi_K} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{\\pi_K} & \\cdots & \\frac{1}{\\pi_{K-1}} + \\frac{1}{\\pi_K} \\\\\n\\end{pmatrix}\\\\\n& = \\diag\\left( \\frac{1}{\\pi_1} , \\ldots,  \\frac{1}{\\pi_{K-1}}   \\right) + \\frac{1}{\\pi_K} \\bOne\\\\\n\\end{split}\n\\]\\(K=2\\) \\(\\pi_1=p\\) reduces expected Fisher information Bernoulli variable\n\\[\n\\begin{split}\n^{\\fisher}(p) & =  \\left(\\frac{1}{p} + \\frac{1}{1-p} \\right) \\\\\n  &= \\frac{1}{p (1-p)} \\\\\n\\end{split}\n\\]Example 1.3  Quadratic approximation KL divergence categorical distribution:expected Fisher information arises local quadratic approximation KL divergence:\n\\[\n\\ikl(F_{\\btheta}, F_{\\btheta+\\bepsilon})  \\approx  \\frac{1}{2}\\bepsilon^T \\bI^{\\fisher}(\\btheta)  \\bepsilon \n\\]\n\n\\[\n\\ikl(F_{\\btheta+\\bepsilon}, F_{\\btheta}) \\approx  \\frac{1}{2}\\bepsilon^T \\bI^{\\fisher}(\\btheta)  \\bepsilon \n\\]now consider KL divergence \\(\\ikl(P, Q)\\) categorical distribution \\(P=\\catdist(\\bp)\\) probabilities \\(\\bp=(p_1, \\ldots, p_K)^T\\) categorical distribution \\(Q=\\catdist(\\bq)\\) probabilities \\(\\bq = (q_1, \\ldots, q_K)^T\\).First, keep \\(P\\) fixed assume \\(Q\\) perturbed version \\(P\\) \\(\\bq = \\bp+\\bepsilon\\).\nNote perturbations \\(\\bepsilon=(\\varepsilon_1, \\ldots, \\varepsilon_K)^T\\) satisfy\n\\(\\sum_{k=1}^K \\varepsilon_k = 0\\) \\(\\sum_{k=1}^K p_i=1\\) \\(\\sum_{k=1}^K q_i=1\\).\nThus \\(\\varepsilon_K = -\\sum_{k=1}^{K-1} \\varepsilon_k\\). \n\\[\n\\begin{split}\n\\ikl(P, Q=P+\\varepsilon) &  = \\ikl(\\catdist(\\bp), \\catdist(\\bp+\\bepsilon)) \\\\\n&  \\approx \\frac{1}{2} (\\varepsilon_1, \\ldots,  \\varepsilon_{K-1}) \\,\n\\bI^{\\fisher}\\left( p_1, \\ldots, p_{K-1}  \\right) \n\\begin{pmatrix} \\varepsilon_1 \\\\ \\vdots \\\\  \\varepsilon_{K-1}\\\\\n\\end{pmatrix} \\\\\n&= \\frac{1}{2} \\left( \\sum_{k=1}^{K-1} \\frac{\\varepsilon_k^2}{p_k}   + \\frac{ \\left(\\sum_{k=1}^{K-1} \\varepsilon_k\\right)^2}{p_K} \\right)  \\\\\n&= \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{\\varepsilon_k^2}{p_k}\\\\\n&= \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{(p_k-q_k)^2}{p_k}\\\\\n& = \\frac{1}{2} D_{\\neyman}(P, Q)\\\\\n\\end{split} \n\\]\nSimilarly, keep \\(Q\\) fixed consider \\(P\\) disturbed version \\(Q\\) get\n\\[\n\\begin{split}\n\\ikl(P=Q+\\varepsilon, Q) &  =\\ikl(\\catdist(\\bq+\\bepsilon), \\catdist(\\bq)) \\\\\n&\\approx \\frac{1}{2}  \\sum_{k=1}^{K} \\frac{(p_k-q_k)^2}{q_k}\\\\\n&= \\frac{1}{2} D_{\\pearson}(P, Q)\n\\end{split}\n\\]\nNote approximations divide probabilities distribution \nkept fixed.Note appearance Pearson \\(\\chi^2\\) divergence Neyman \\(\\chi^2\\) divergence . , like KL divergence, part family \\(f\\)-divergences. Neyman \\(\\chi^2\\)\ndivergence also known reverse Pearson divergence \\(D_{\\neyman}(P, Q) = D_{\\pearson}(Q, P)\\).Example 1.4  Maximum likelihood estimation parameters categorical distribution:Maximum likelihood estimation seems trivial first sight fact bit complicated since \\(K-1\\) free parameters, \\(K\\). either need optimise regard specific set \\(K-1\\) parameters () use constrained optimisation procedure enforce \\(\\sum_{k=1}^K \\pi_k = 1\\) (example using Lagrange multiplier).data: observe \\(n\\) samples \\(\\bx_1, \\ldots, \\bx_n\\).\ndata matrix dimension \\(n \\times K\\) \n\\(\\bX = (\\bx_1, \\ldots, \\bx_n)^T = (x_{ik})\\).\ncontains \\(\\bx_i = (x_{i1}, \\ldots, x_{iK})^T\\). corresponding summary\n(minimal sufficient) statistics \n\\(\\bar{\\bx} = \\frac{1}{n} \\sum_{=1}^n \\bx_i = (\\bar{x}_1, \\ldots, \\bar{x}_K)^T\\)\n\n\\(\\bar{x}_k = \\frac{1}{n} \\sum_{=1}^n x_{ik}\\). can also write\n\\(\\bar{x}_{K} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k}\\).\nnumber samples class \\(k\\) \\(n_k = n \\bar{x}_k\\) \n\\(\\sum_{k=1}^K n_k = n\\).data: observe \\(n\\) samples \\(\\bx_1, \\ldots, \\bx_n\\).\ndata matrix dimension \\(n \\times K\\) \n\\(\\bX = (\\bx_1, \\ldots, \\bx_n)^T = (x_{ik})\\).\ncontains \\(\\bx_i = (x_{i1}, \\ldots, x_{iK})^T\\). corresponding summary\n(minimal sufficient) statistics \n\\(\\bar{\\bx} = \\frac{1}{n} \\sum_{=1}^n \\bx_i = (\\bar{x}_1, \\ldots, \\bar{x}_K)^T\\)\n\n\\(\\bar{x}_k = \\frac{1}{n} \\sum_{=1}^n x_{ik}\\). can also write\n\\(\\bar{x}_{K} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k}\\).\nnumber samples class \\(k\\) \\(n_k = n \\bar{x}_k\\) \n\\(\\sum_{k=1}^K n_k = n\\).log-likelihood \n\\[\n\\begin{split}\nl_n(\\pi_1, \\ldots, \\pi_{K-1}) & = \\sum_{=1}^n \\log f(\\bx_i) \\\\\n& =\\sum_{=1}^n \\left( \\sum_{k=1}^{K-1}  x_{ik} \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_{ik}  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\right)\\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right) \\log\\left(1 - \\sum_{k=1}^{K-1} \\pi_k\\right) \\right) \\\\ \n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\bar{x}_K \\log \\pi_K \\right) \\\\  \n\\end{split}\n\\]log-likelihood \n\\[\n\\begin{split}\nl_n(\\pi_1, \\ldots, \\pi_{K-1}) & = \\sum_{=1}^n \\log f(\\bx_i) \\\\\n& =\\sum_{=1}^n \\left( \\sum_{k=1}^{K-1}  x_{ik} \\log \\pi_k    + \\left( 1 - \\sum_{k=1}^{K-1} x_{ik}  \\right) \\log \\left( 1 - \\sum_{k=1}^{K-1} \\pi_k \\right) \\right)\\\\\n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right) \\log\\left(1 - \\sum_{k=1}^{K-1} \\pi_k\\right) \\right) \\\\ \n& = n \\left(  \\sum_{k=1}^{K-1}  \\bar{x}_k \\log \\pi_k    + \\bar{x}_K \\log \\pi_K \\right) \\\\  \n\\end{split}\n\\]Score function (gradient, row vector)\n\\[\n\\begin{split}\n\\bS_n(\\pi_1, \\ldots, \\pi_{K-1}) &=  \\nabla l_n(\\pi_1, \\ldots, \\pi_{K-1} ) \\\\\n& = \n\\begin{pmatrix}\n \\frac{\\partial}{\\partial \\pi_1} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\vdots\\\\\n\\frac{\\partial}{\\partial \\pi_{K-1}} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\end{pmatrix}^T\\\\\n& = n\n\\begin{pmatrix}\n\\frac{\\bar{x}_1}{\\pi_1}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\vdots\\\\\n\\frac{\\bar{x}_{K-1}}{\\pi_{K-1}}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\end{pmatrix}^T\\\\\n\\end{split}\n\\]\nNote particular need second term arises \\(\\pi_K\\) depends \n\\(\\pi_1, \\ldots, \\pi_{K-1}\\).Score function (gradient, row vector)\n\\[\n\\begin{split}\n\\bS_n(\\pi_1, \\ldots, \\pi_{K-1}) &=  \\nabla l_n(\\pi_1, \\ldots, \\pi_{K-1} ) \\\\\n& = \n\\begin{pmatrix}\n \\frac{\\partial}{\\partial \\pi_1} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\vdots\\\\\n\\frac{\\partial}{\\partial \\pi_{K-1}} l_n(\\pi_1, \\ldots, \\pi_{K-1} )  \\\\\n\\end{pmatrix}^T\\\\\n& = n\n\\begin{pmatrix}\n\\frac{\\bar{x}_1}{\\pi_1}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\vdots\\\\\n\\frac{\\bar{x}_{K-1}}{\\pi_{K-1}}-\\frac{\\bar{x}_K}{\\pi_K}  \\\\\n\\end{pmatrix}^T\\\\\n\\end{split}\n\\]\nNote particular need second term arises \\(\\pi_K\\) depends \n\\(\\pi_1, \\ldots, \\pi_{K-1}\\).Maximum likelihood estimate: Setting \\(\\bS_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML})=0\\) yields\n\\(K-1\\) equations\n\\[\n\\bar{x}_i \\left(1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML}\\right)  = \\hat{\\pi}_i^{ML} \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right)\n\\]\n\\(=1, \\ldots, K-1\\) solution\n\\[\n\\hat{\\pi}_i^{ML} = \\bar{x}_i\n\\]\nalso follows \n\\[ \n\\hat{\\pi}_K^{ML} = 1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} = \\bar{x}_K\n\\]\nmaximum likelihood estimator therefore frequency\noccurance class among \\(n\\) samples.Maximum likelihood estimate: Setting \\(\\bS_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML})=0\\) yields\n\\(K-1\\) equations\n\\[\n\\bar{x}_i \\left(1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML}\\right)  = \\hat{\\pi}_i^{ML} \\left(\n1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} \\right)\n\\]\n\\(=1, \\ldots, K-1\\) solution\n\\[\n\\hat{\\pi}_i^{ML} = \\bar{x}_i\n\\]\nalso follows \n\\[ \n\\hat{\\pi}_K^{ML} = 1 - \\sum_{k=1}^{K-1} \\hat{\\pi}_k^{ML} = 1 - \\sum_{k=1}^{K-1} \\bar{x}_{k} = \\bar{x}_K\n\\]\nmaximum likelihood estimator therefore frequency\noccurance class among \\(n\\) samples.Example 1.5  Observed Fisher information categorical distribution:first need compute negative Hessian matrix log likelihood function\n\\(- \\nabla^T \\nabla l_n(\\pi_1, \\ldots, \\pi_{K-1} )\\) evaluate \nMLEs \\(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}\\).diagonal entries Hessian matrix (\\(=1, \\ldots, K-1\\)) \n\\[\n\\frac{\\partial^2}{\\partial \\pi_i^2} l_n(\\pi_1, \\ldots, \\pi_{K-1} ) =\n -n \\left( \\frac{\\bar{x}_i}{\\pi_i^2} +\\frac{\\bar{x}_K}{\\pi_K^2}\\right)\n\\]\n-diagonal entries (\\(j=1, \\ldots, K-1\\))\n\\[\n\\frac{\\partial^2}{\\partial \\pi_i \\partial \\pi_j} l_n(\\pi_1, \\ldots, \\pi_{K-1} ) =\n -\\frac{n \\bar{x}_K}{\\pi_K^2}\n\\]\nThus, observed Fisher information matrix MLE categorical distribution \n\\[\n\\bJ_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) =\nn \n\\begin{pmatrix}\n \\frac{1}{\\hat{\\pi}_1^{ML}} + \\frac{1}{\\hat{\\pi}_K^{ML}} & \\cdots & \\frac{1}{\\hat{\\pi}_K^{ML}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{\\hat{\\pi}_K^{ML}} & \\cdots & \\frac{1}{\\hat{\\pi}_{K-1}^{ML}} + \\frac{1}{\\hat{\\pi}_K^{ML}} \\\\\n\\end{pmatrix} \n\\]\\(K=2\\) reduces observed Fisher information Bernoulli variable\n\\[\n\\begin{split}\nJ_n(\\hat{p}_{ML}) & = n \\left(\\frac{1}{\\hat{p}_{ML}} + \\frac{1}{1-\\hat{p}_{ML}} \\right) \\\\\n  &= \\frac{n}{\\hat{p}_{ML} (1-\\hat{p}_{ML})} \\\\\n\\end{split}\n\\]inverse observed Fisher information :\n\\[\n\\bJ_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  )^{-1} =\n\\frac{1}{n} \n\\begin{pmatrix}\n\\hat{\\pi}_1^{ML} (1- \\hat{\\pi}_1^{ML} )  & \\cdots & -  \\hat{\\pi}_{1}^{ML} \\hat{\\pi}_{K-1}^{ML}   \\\\\n\\vdots & \\ddots & \\vdots \\\\\n-  \\hat{\\pi}_{K-1}^{ML} \\hat{\\pi}_{1}^{ML} & \\cdots & \\hat{\\pi}_{K-1}^{ML} (1- \\hat{\\pi}_{K-1}^{ML} )  \\\\\n\\end{pmatrix}\n\\]show indeed inverse use Woodbury matrix identity (see Appendix), B=1, \\(\\bu = (\\pi_1, \\ldots, \\pi_{K-1})^T\\), \\(\\bv=-\\bu^T\\),\n\\(\\bA = \\diag(\\bu)\\) inverse \\(\\bA^{-1} = \\diag(\\pi_1^{-1}, \\ldots, \\pi_{K-1}^{-1})\\). \\(\\bA^{-1} \\bu = \\bOne_{K-1}\\) \\(1-\\bu^T \\bA^{-1} \\bu = \\pi_K\\).\n\n\\(\\bJ_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML} )^{-1} = \\frac{1}{n} \\left( \\bA - \\bu \\bu^T \\right)\\)\n\n\\(\\bJ_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML} ) = n \\left( \\bA^{-1} + \\frac{1}{\\pi_K} \\bOne_{K-1 \\times K-1} \\right)\\).\\(K=2\\) inverse observed Fisher information categorical distribution reduceds Bernoulli distribution\n\\[\nJ_n(\\hat{p}_{ML})^{-1}=\\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\n\\]inverse observed Fisher information useful, e.g., \nasymptotic variance maximum likelihood estimates.Example 1.6  Wald statistic categorical distribution:squared Wald statistic \n\\[\n\\begin{split}\nt(\\bp_0)^2 &= \n(\\hat{\\pi}_{1}^{ML}-p_1^0, \\ldots,  \\hat{\\pi}_{K-1}^{ML}-p_{K-1}^0)   \\bJ_n(\\hat{\\pi}_{1}^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML} ) \\begin{pmatrix} \\hat{\\pi}_{1}^{ML}-p_1^0 \\\\\n\\vdots \\\\\n\\hat{\\pi}_{K-1}^{ML}-p_{K-1}^0\\\\\n\\end{pmatrix}\\\\\n&= n  \\left( \\sum_{k=1}^{K-1} \\frac{(\\hat{\\pi}_{k}^{ML}-p_{k}^0)^2}{\\hat{\\pi}_{k}^{ML}}   + \\frac{ \\left(\\sum_{k=1}^{K-1} (\\hat{\\pi}_{k}^{ML}-p_{k}^0)\\right)^2}{\\hat{\\pi}_{K}^{ML}} \\right)  \\\\\n&= n  \\left( \\sum_{k=1}^{K} \\frac{(\\hat{\\pi}_{k}^{ML}-p_{k}^0)^2}{\\hat{\\pi}_{k}^{ML}}    \\right)  \\\\\n& = n D_{\\neyman}( \\catdist(\\hat{\\bpi}_{ML}), \\catdist(\\bp_0 ) )\n\\end{split}\n\\]\\(n_1, \\ldots, n_K\\) observed counts \\(n = \\sum_{k=1}^K n_k\\)\n\\(\\hat{\\pi}_k^{ML} = \\frac{n_k}{n} = \\bar{x}_k\\),\n\\(n_1^{\\text{expect}}, \\ldots, n_K^{\\text{expect}}\\) \nexpected counts \\(n_k^{\\text{expect}} = n p_k^{0}\\) \\(\\bp_0\\)\ncan write squared Wald statistic\nfollows:\n\\[\nt(\\bp_0)^2 = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}} )^2}{n_k} =  \\chi^2_{\\neyman}\n\\]\nknown Neyman chi-squared statistic (note observed counts denominator) asymptotically distributed \\(\\chi^2_{K-1}\\) \n\\(K-1\\) free parameters \\(\\bp_0\\).Example 1.7  Wilks log-likelihood ratio statistic categorical distribution:Wilks log-likelihood ratio \n\\[\nW(\\bp_0) = 2 (l_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) - l_n(p_1^{0}, \\ldots, p_{K-1}^{0}    ))\n\\]\n\\(\\bp_0 = c(p_1^{0}, \\ldots, p_{K}^{0} )^T\\).\nprobabilities sum 1 \\(K-1\\) free parameters.log-likelihood MLE \n\\[\nl_n(\\hat{\\pi}_1^{ML}, \\ldots, \\hat{\\pi}_{K-1}^{ML}  ) =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log \\hat{\\pi}_k^{ML}  =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log \\bar{x}_k \n\\]\n\\(\\hat{\\pi}_k^{ML} = \\frac{n_k}{n} = \\bar{x}_k\\).\nNote following sums run \\(1\\) \\(K\\) \\(K\\)-th component always computed components \\(1\\) \\(K-1\\), previous section.\nlog-likelihood \\(\\bp_0\\) \n\\[l_n( p_1^{0}, \\ldots, p_{K-1}^{0}    ) =  n   \\sum_{k=1}^{K}  \\bar{x}_k \\log p_k^{0} \n\\]\nWilks statistic becomes\n\\[\nW(\\bp_0) = 2 n   \\sum_{k=1}^{K}  \\bar{x}_k \\log\\left(\\frac{\\bar{x}_k}{ p_k^{0}} \\right) \n\\]\nasymptotically chi-squared distributed \\(K-1\\) degrees freedom.Note model Wilks statistic equal KL Divergence\n\\[\nW(\\bp_0) = 2 n \\ikl( \\catdist(\\hat{\\bpi}_{ML}), \\catdist(\\bp_0 ) )\n\\]Wilks log-likelihood ratio statistic categorical distribution also known \\(G\\) test statistic \\(\\hat{\\bpi}_{ML}\\) corresponds observed frequencies (observed data) \\(\\bp_0\\) expected frequencies (.e. hypothesised true frequencies).Using observed counts \\(n_k\\) expected counts \\(n_k^{\\text{expect}} = n p_k^{0}\\)\ncan write Wilks statistic respectively \\(G\\)-statistic\nfollows:\n\\[\nW(\\bp_0) = 2   \\sum_{k=1}^{K}  n_k \\log\\left(\\frac{  n_k }{  n_k^{\\text{expect}}   } \\right) \n\\]Example 1.8  Quadratic approximation Wilks log-likelihood ratio statistic categorical distribution:Developing Wilks statistic \\(W(\\bp_0)\\) around MLE \\(\\hat{\\bpi}_{ML}\\) yields squared Wald statistic categorical distribution Neyman chi-squared statistic:\n\\[\n\\begin{split}\nW(\\bp_0)& = 2 n \\ikl( \\catdist(\\hat{\\bpi}_{ML}), \\catdist(\\bp_0 ) ) \\\\\n& \\approx n D_{\\neyman}( \\catdist(\\hat{\\bpi}_{ML}), \\catdist(\\bp_0 ) ) \\\\\n& = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}} )^2}{n_k} \\\\\n& =  \\chi^2_{\\neyman}\\\\\n\\end{split}\n\\]instead approximate KL divergence assuming \\(\\bp_0\\) fixed arrive \n\\[ \n\\begin{split}\n2 n \\ikl( \\catdist(\\hat{\\bpi}_{ML}), \\catdist(\\bp_0 ) ) &\\approx n D_{\\pearson}( \\catdist(\\hat{\\bpi}_{ML}),  \\catdist(\\bp_0 ) )\\\\\n& = \\sum_{k=1}^K \\frac{(n_k-n_k^{\\text{expect}})^2}{n_k^{\\text{expect}}} \\\\\n& = \\chi^2_{\\pearson}\n\\end{split}\n\\]\nwell-known Pearson chi-squared statistic (note expected counts denominator).","code":""},{"path":"multivariate-random-variables.html","id":"further-multivariate-distributions","chapter":"1 Multivariate random variables","heading":"1.6 Further multivariate distributions","text":"univariate distributions multivariate versions.following describe multivariate versions Beta distribution,\nGamma distribution (also known scaled \\(\\chi^2\\) distribution)\ninverse Gamma distribution.","code":""},{"path":"multivariate-random-variables.html","id":"dirichlet-distribution","chapter":"1 Multivariate random variables","heading":"1.6.1 Dirichlet distribution","text":"","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case-1","chapter":"1 Multivariate random variables","heading":"1.6.1.1 Univariate case","text":"Beta distribution\\[x \\sim \\betadist(\\alpha,\\beta)\\]\n\\[x \\[0,1]\\]\n\\[\\alpha > 0; \\beta > 0\\]\n\\[m = \\alpha + \\beta \\]\n\\[\\mu = \\frac{\\alpha}{m} \\\\left[0,1\\right]\\]\n\\[\\expect(x) = \\mu\\]\n\\[\\var(x)=\\frac{\\mu(1-\\mu)}{m+1}\\]\n\\(\\text{compare unit standardised binomial!}\\)\\(\\textbf{Different shapes}\\)\\[\\text{Useful distribution proportion } \\pi\\]\\[ \\text{ Bayesian Model:}\\]\\[\\text{Beta prior:} \\; \\pi \\sim  \\betadist(\\alpha,\\beta)\\]\n\\[\\text{Binomial likelihood:} \\; x|\\pi \\sim \\bindist(n, \\pi)\\]","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case-1","chapter":"1 Multivariate random variables","heading":"1.6.1.2 Multivariate case","text":"Dirichlet distribution\\[\\bx \\sim \\dirdist(\\balpha)\\]\n\\[x_i \\[0,1]; \\, \\sum^{d}_{=1} x_i = 1\\]\n\\[\\balpha = (\\alpha_1,...,\\alpha_d)^T >0\\]\n\\[m = \\sum^{d}_{=1}\\alpha_i\\]\n\\[\\mu_i = \\frac{\\alpha_i}{m} \\\\left[0,1\\right]\\]\n\\[\\expect(x_i) = \\mu_i\\]\n\\[\\var(x_i)=\\frac{\\mu_i(1-\\mu_i)}{m+1}\\]\n\\[\\cov(x_i,x_j)=-\\frac{\\mu_i \\mu_j}{m+1}\\]\n\\(\\text{compare unit standardised multinomial!}\\)Stick breaking\" model\\[\\text{Useful distribution proportion } \\bpi\\]\\[\\text{ Bayesian Model:}\\]\\[\\text{Dirichlet prior:} \\,  \\bpi \\sim \\dirdist(\\balpha)\\]\n\\[\\text{Multinomial likelihood:} \\, \\bx |\\bpi \\sim \\multdist(n, \\bpi)\\]","code":""},{"path":"multivariate-random-variables.html","id":"wishart-distribution","chapter":"1 Multivariate random variables","heading":"1.6.2 Wishart distribution","text":"multivariate distribution generalises univariate\nscaled \\(\\chi^2\\) distribution (also known Gamma distribution).","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case-2","chapter":"1 Multivariate random variables","heading":"1.6.2.1 Univariate case","text":"Scaled \\(\\chi^2\\) distribution (=Gamma distribution, see )\\[z_1,z_2,\\ldots,z_m \\stackrel{\\text{iid}}\\sim N(0,\\sigma^2)\\]\n\\[x = \\sum^{m}_{=1}z_i^2\\]\\[x \\sim \\sigma^2 \\chi^2_m = \\text{W}_1(\\sigma^2, m)\\]\n\\[\\expect(x) = m \\, \\sigma^2\\]\n\\[\\var(x)= m \\, 2 \\sigma^4\\]Useful distribution sample variance:\n\\[y_1, \\ldots, y_n \\sim N(\\mu, \\sigma^2)\\]\nKnown mean \\(\\mu\\):\n\\[\\frac{1}{n}\\sum_{=1}^n(y_i -\\mu)^2 \\sim \\text{W}_1\\left(\\frac{\\sigma^2}{n}, n\\right)\\]\nUnknown mean \\(\\mu\\) (estimated \\(\\bar{y}\\)):\n\\[\\widehat{\\sigma^2}_{ML} = \\frac{1}{n}\\sum_{=1}^n(y_i -\\bar{y})^2 \\sim \\text{W}_1\\left(\\frac{\\sigma^2}{n}, n-1\\right)\\]\n\\[\\widehat{\\sigma^2}_{UB} = \\frac{1}{n-1}\\sum_{=1}^n(y_i -\\bar{y})^2 \\sim \\text{W}_1\\left(\\frac{\\sigma^2}{n-1}, n-1\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case-2","chapter":"1 Multivariate random variables","heading":"1.6.2.2 Multivariate case","text":"Wishart distribution\\[\\bz_1,\\bz_2,\\ldots,\\bz_m \\stackrel{\\text{iid}}\\sim N_d(0,\\bSigma)\\]\n\\[\\underbrace{\\bX}_{d\\times d}=\\sum^{m}_{=1}\\underbrace{\\bz_i\\bz_i^T}_{d\\times d}\\]\nNote \\(\\bX\\) matrix!\\[\\bX \\sim \\text{W}_d\\left(\\bSigma, m\\right)\\]\n\\[\\expect(\\bX) = m \\bSigma\\]\n\\[\\var(x_{ij})=m \\, \\left(\\sigma^2_{ij}+\\sigma_{ii}\\sigma_{jj}\\right)\\]Useful distribution sample covariance:\n\\[\\by_1, \\ldots, \\by_n \\sim N_d(\\bmu, \\bSigma)\\]\n\\[\\frac{1}{n}\\sum_{=1}^n (\\by_i -\\bmu)(\\by_i -\\bmu)^T \\sim \\text{W}_d\\left(\\bSigma/n, n\\right)\\]\n\\[\\widehat{\\bSigma}_{ML} = \\frac{1}{n}\\sum_{=1}^n (\\by_i -\\bar{\\})(\\by_i -\\bar{\\})^T \\sim \\text{W}_d\\left(\\bSigma/n, n-1\\right)\\]\n\\[\\widehat{\\bSigma}_{UB} = \\frac{1}{n-1}\\sum_{=1}^n (\\by_i -\\bar{\\})(\\by_i -\\bar{\\})^T \\sim \\text{W}_d\\left(\\bSigma/(n-1), n-1\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"relationship-to-gamma-distribution","chapter":"1 Multivariate random variables","heading":"1.6.2.3 Relationship to Gamma distribution","text":"scaled \\(\\chi^2\\) distribution (=one-dimensional Wishart distribution) parameters \\(\\sigma^2\\) \\(m\\) reparameterised Gamma distribution shape parameter \\(\\alpha\\) scale parameter \\(\\beta\\):\\[\\gamdist\\left(  \\underbrace{\\frac{m}{2}}_{\\text{shape} } \\, , \\underbrace{ 2 \\sigma^{2}}_{\\text{scale}} \\right)=  \\sigma^2\\chi^2_m = \\text{W}_1(\\sigma^2, m)\\], equivalently (\\(m = 2 \\alpha\\), \\(\\sigma^2 = \\beta/2\\))\n\\[\\gamdist\\left(  \\underbrace{\\alpha}_{\\text{shape} } \\, , \\underbrace{\\beta}_{\\text{scale}} \\right) = \\frac{\\beta}{2} \\chi^2_{2 \\alpha} =  \\text{W}_1(\\frac{\\beta}{2}, 2 \\alpha)\\]mean Gamma distribution \\(\\expect(x) = \\alpha \\beta\\) variance \\(\\var(x) = \\alpha \\beta^2\\).exponential distribution scale parameter \\(\\beta\\) special\ncase Gamma distribution \\(\\alpha=1\\):\n\\[\n\\expdist(\\beta) = \\gamdist(1, \\beta) = \\frac{\\beta}{2} \\chi^2_{2} = \\text{W}_1\\left(\\frac{\\beta}{2}, 2 \\right)\n\\]\ncorresponding mean \\(\\beta\\) variance \\(\\beta^2\\).density expressed terms scale parameter \\(\\beta\\) \n\\[\nf(x| \\beta) = \\frac{1}{\\beta} e^{-x/\\beta}\n\\]Instead scale parameter \\(\\beta\\) exponential distribution often\nalso parameterised terms rate parameter \\(\\lambda = \\frac{1}{\\beta}\\).","code":""},{"path":"multivariate-random-variables.html","id":"inverse-wishart-distribution","chapter":"1 Multivariate random variables","heading":"1.6.3 Inverse Wishart distribution","text":"","code":""},{"path":"multivariate-random-variables.html","id":"univariate-case-3","chapter":"1 Multivariate random variables","heading":"1.6.3.1 Univariate case","text":"Inverse \\(\\chi^2\\) Distribution:\\[x \\sim \\text{W}^{-1}_1(\\psi, k+2) = \\psi\\,\\invchisqdist{k+2}\\]\n\\[\\expect(x) = \\frac{\\psi}{k}\\]\n\\[\\var(x)= \\frac{2\\psi^2}{k^2 (k-2)}\\]Relationship scaled \\(\\chi^2\\) :\n\\[\n\\frac{1}{x} \\sim W_1(\\psi^{-1}, k+2) =  \\psi^{-1} \\, \\chi^2_{k+2}\n\\]","code":""},{"path":"multivariate-random-variables.html","id":"multivariate-case-3","chapter":"1 Multivariate random variables","heading":"1.6.3.2 Multivariate case","text":"Inverse Wishart distribution:\\[\\underbrace{\\bX}_{d\\times d} \\sim \\text{W}^{-1}_d\\left( \\underbrace{\\bPsi}_{d\\times d} \\, , \\, k+d+1\\right)\\]\n\\[\\expect(\\bX) =\\bPsi / k\\]\n\\[\\var(x_{ij})= \\frac{2 }{k^2 (k-2)} \\frac{(k+2) \\psi_{ij} + k \\, \\psi_{ii} \\psi_{jj} }{2 k + 2}\\]Relationship Wishart:\n\\[\\bX^{-1} \\sim \\text{W}_d\\left( \\bPsi^{-1} \\, , k+d+1\\right)\\]","code":""},{"path":"multivariate-random-variables.html","id":"relationship-to-inverse-gamma-distribution","chapter":"1 Multivariate random variables","heading":"1.6.3.3 Relationship to inverse Gamma distribution","text":"Another way express univariate inverse Wishart distribution via inverse Gamma distribution:\n\\[\\invgamdist(\\underbrace{1+\\frac{k}{2}}_{\\text{shape } \\alpha}, \\underbrace{\\frac{\\psi}{2}}_{\\text{scale }\\beta}) = \\psi\\,\\invchisqdist{k+2} =  \\text{W}^{-1}_1(\\psi, k+2) \\]\nequivalently (\\(k=2(\\alpha-1)\\) \\(\\psi=2\\beta\\))\n\\[\\invgamdist( \\alpha, \\beta) = 2\\beta\\,\\invchisqdist{2\\alpha} = \\text{W}^{-1}_1(2 \\beta, 2 \\alpha) \\]\nmean inverse Gamma distribution \n\\(\\expect(x) = \\frac{\\beta}{\\alpha-1} = \\mu\\) variance\n\\(\\var(x)= \\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)} = \\frac{2 \\mu^2}{k-2}\\).inverse \\(x\\) Gamma distributed:\n\\[\n\\frac{1}{x} \\sim \\gamdist(1+\\frac{k}{2}, 2\\psi^{-1})=\\gamdist(\\alpha, \\beta^{-1})\n\\]inverse Wishart distribution useful conjugate distribution Bayesian modelling\nvariance, \\(k\\) sample size parameter\n\\(\\Psi = k \\Sigma\\) (\\(\\psi = k \\sigma^2\\)).","code":""},{"path":"multivariate-random-variables.html","id":"further-distributions","chapter":"1 Multivariate random variables","heading":"1.6.4 Further distributions","text":"https://en.wikipedia.org/wiki/List_of_probability_distributionsWikipedia quite good source information distributions!","code":""},{"path":"transformations-and-dimension-reduction.html","id":"transformations-and-dimension-reduction","chapter":"2 Transformations and dimension reduction","heading":"2 Transformations and dimension reduction","text":"Motivation:\nfollowing study transformations random vectors distributions.\ntransformation important\nsince either transform simple distributions complex distributions allow simplify\ncomplex models. machine learning invertible mappings transformations\nprobability distributions known “normalising flows” (play key role\ne.g. neural networks).","code":""},{"path":"transformations-and-dimension-reduction.html","id":"linear-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.1 Linear Transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"location-scale-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.1.1 Location-scale transformation","text":"Also known affine transformation.\\[\\= \\underbrace{\\ba}_{\\text{location parameter}}+\\underbrace{\\bB}_{\\text{scale parameter}} \\bx  \\space\\]\n\\[\\: m \\times 1 \\text{ random vector}\\]\n\\[\\ba: m \\times 1 \\text{ vector, location parameter}\\]\n\\[\\bB: m \\times d \\text{ matrix, scale parameter },  m \\geq 1\\]\n\\[\\bx: d \\times 1 \\text{ random vector}\\]\\[\\begin{align*}\n\\begin{array}{ll}\n\\expect(\\bx)=\\bmu\\\\\n\\var(\\bx)=\\bSigma \\\\\n\\end{array}\n\\Longrightarrow\n\\begin{array}{ll}\n\\expect(\\)=\\ba + \\bB \\bmu \\\\\n\\var(\\)= \\bB \\bSigma \\bB^T \\\\\n\\end{array}\n\\end{align*}\\]Special cases/examples:Example 2.1  Univariate case (\\(d=1, m=1\\)):\\(\\expect(y)=+b\\mu\\)\\(\\var(y)=b^2\\sigma^2\\)Example 2.2  Sum two random univariate variables:\n\\(y = x_1 + x_2\\), .e. \\(=0\\) \\(\\bB=(1,1)\\)\\(\\expect(x_1+x_2)=\\mu_1+\\mu_2\\)\\(\\var(x_1+x_2) = (1,1)\\begin{pmatrix} \\sigma^2_1 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma^2_2 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix} = \\sigma^2_1+\\sigma^2_2+2\\sigma_{12} = \\var(x_1)+\\var(x_2)+2\\cov(x_1,x_2)\\)Example 2.3  \\(y_1=a_1+b_1 x_1\\) \\(y_2=a_2+b_2 x_2\\),\n.e. \\(\\ba= \\begin{pmatrix} a_1\\\\ a_2 \\end{pmatrix}\\) \\(\\bB= \\begin{pmatrix}b_1 & 0\\\\ 0 & b_2\\end{pmatrix}\\)\\(\\expect(\\)=\\begin{pmatrix} a_1+b_1 \\mu_1\\\\ a_2+b_2 \\mu_2 \\end{pmatrix}\\)\\(\\var(\\) = \\begin{pmatrix} b_1 & 0\\\\ 0 & b_2 \\end{pmatrix} \\begin{pmatrix} \\sigma^2_1 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma^2_2 \\end{pmatrix} \\begin{pmatrix} b_1 & 0\\\\ 0 & b_2 \\end{pmatrix} = \\begin{pmatrix} b^2_1\\sigma^2_1 & b_1b_2\\sigma_{12}\\\\ b_1b_2\\sigma_{21} & b^2_2\\sigma^2_2 \\end{pmatrix}\\)\nnote \\(\\cov(y_1, y_2) = b_1 b_2\\cov(x_1,x_2)\\)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"invertible-location-scale-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.1.2 Invertible location-scale transformation","text":"\\(m=d\\) \\(\\det(\\bB) \\neq 0\\) get invertible transformation:\n\\[\\= \\ba + \\bB\\bx\\]\n\\[\\bx = \\bB^{-1}(\\-\\ba)\\]Transformation density:\n\\(\\bx \\sim F_{\\bx}\\) density \\(f_{\\bx}(\\bx)\\)\\(\\Longrightarrow\\) \\(\\\\sim F_{\\}\\) density\n\\[ f_{\\}(\\)=|\\det(\\bB)|^{-1} f_{\\bx} \\left( \\bB^{-1}(\\-\\ba)\\right)\\]Example 2.4  Mahalanobis transform \\(\\=\\bSigma^{-1/2}(\\bx-\\bmu)\\)assume positive definite thus invertible \\(\\bSigma\\), \ninverse principal matrix square root \\(\\bSigma^{-1/2}\\) can computed,\ntransformation invertible.\\[\\ba = - \\bSigma^{-1/2} \\bmu\\]\n\\[\\bB = \\bSigma^{-1/2}\\]\n\\[\\expect(\\bx)=\\bmu \\text{ } \\var(\\bx)=\\bSigma\\]\n\\[\\Longrightarrow\\expect(\\) = \\bZero \\text{ } \\var(\\) = \\bI_d\\]\nMahalanobis transformation performs three functions:Centering (\\(-\\bmu\\))Standardisation \\(\\var(y_i)=1\\)Decorrelation \\(\\cor(y_i,y_j)=0\\) \\(\\neq j\\)Univariate case (\\(d=1\\))\\[y = \\frac{x-\\mu}{\\sigma}\\]= centering + standardisationThe Mahalanobis transformation appears implicitly many places multivariate statistics,\ne.g. multivariate normal density.particular example whitening transformation (\ninfinitely many, see later chapters).Example 2.5  Inverse Mahalanobis transformation \\(\\= \\bmu+\\bSigma^{1/2} \\bx\\)transformation inverse Mahalanobis transformation.\nMahalanobis transform particular whitening transform inverse\ntransform sometimes called Mahalanobis colouring transformation.\\[\\ba=\\bmu\\]\n\\[\\bB=\\bSigma^{1/2}\\]\\[\\expect(\\bx)=\\bZero \\text{ } \\var(\\bx)=\\bI_d\\]\n\\[\\Longrightarrow\\expect(\\) = \\bmu \\text{ } \\var(\\) = \\bSigma\\]\nAssume \\(\\bx\\) multivariate standard normal \\(\\bx \\sim N_d(\\bZero,\\bI_d)\\) density\n\\[f_{\\bx}(\\bx) = (2\\pi)^{-d/2}\\exp\\left( -\\frac{1}{2} \\bx^T \\bx \\right)\\]\ndensity applying inverse Mahalanobis transform \\[f_{\\}(\\) = |\\det(\\bSigma^{1/2})|^{-1} (2\\pi)^{-d/2} \\exp\\left(-\\frac{1}{2}(\\-\\bmu)^T\\bSigma^{-1/2} \\,\\bSigma^{-1/2}(\\-\\bmu)\\right)\\]\\[= (2\\pi)^{-d/2} \\det(\\bSigma)^{-1/2} \\exp\\left(-\\frac{1}{2}(\\-\\bmu)^T\\bSigma^{-1}(\\-\\bmu)\\right)\\]\n\\(\\Longrightarrow\\) \\(\\\\) multivariate normal density!!Application: e.g. random number generation: draw \\(N_d(\\bZero,\\bI_d)\\) (easy!) convert multivariate normal tranformation\n(see Worksheet 3).","code":""},{"path":"transformations-and-dimension-reduction.html","id":"nonlinear-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.2 Nonlinear transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"general-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.2.1 General transformation","text":"\\[\\= \\bh(\\bx)\\]\n\\(\\bh\\) arbitrary vector-valued functionlinear case: \\(\\bh(\\bx) = \\ba+\\bB\\bx\\)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"delta-method","chapter":"2 Transformations and dimension reduction","heading":"2.2.2 Delta method","text":"Assume know mean \\(\\expect(\\bx)=\\bmu\\) variance \\(\\var(\\bx)=\\bSigma\\) \\(\\bx\\).\npossible say something mean variance transformed\nrandom variable \\(\\\\)?\n\\[\n\\expect(\\)= \\expect(\\bh(\\bx))= ?\n\\]\n\\[\n\\var(\\) = \\var(\\bh(\\bx))= ? \\\\\n\\]general, transformation \\(\\bh(\\bx)\\) exact mean variance transformed variable obtained analytically.However, can find linear approximation compute mean variance.\napproximation called “Delta Method”, “law propagation errors”, credited Gauss.1Linearisation \\(\\bh(\\bx)\\) achieved Taylor series approximation first order\n\\(\\bh(\\bx)\\) around \\(\\bx_0\\):\n\\[\\bh(\\bx) \\approx \\bh(\\bx_0) + \\underbrace{D\\bh(\\bx_0)}_{\\text{Jacobian matrix}}(\\bx-\\bx_0)  = \n\\underbrace{\\bh(\\bx_0) - D\\bh(\\bx_0)\\, \\bx_0}_{\\ba} + \\underbrace{D\\bh(\\bx_0)}_{\\bB} \\bx    \\]\\(\\nabla\\), nabla operator, row vector \\((\\frac{\\partial}{\\partial x_1},...,\\frac{\\partial}{\\partial x_d})\\), applied univariate \\(h\\) gives gradient:\\[\\nabla h(\\bx) = \\left(\\frac{\\partial h}{\\partial x_1},...,\\frac{\\partial h}{\\partial x_d}\\right)\\]Jacobian matrix generalisation gradient \\(\\bh\\) vector-valued:\\[D\\bh(\\bx) = \\begin{pmatrix}\\nabla h_1(\\bx)\\\\ \\nabla h_2(\\bx) \\\\ \\vdots \\\\ \\nabla h_m(\\bx) \\end{pmatrix} = \\begin{pmatrix}\n    \\frac{\\partial h_1}{\\partial x_1} & \\dots & \\frac{\\partial h_1}{\\partial x_d}\\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial h_m}{\\partial x_1} & \\dots & \\frac{\\partial h_m}{\\partial x_d}\n    \\end{pmatrix}\\]First order approximation \\(\\bh(\\bx)\\) around \\(\\bx_0=\\bmu\\) yields\n\\(\\ba = \\bh(\\bmu) - D\\bh(\\bmu)\\, \\bmu\\) \n\\(\\bB = D\\bh(\\bmu)\\) leads directly multivariate Delta method:\\[\\expect(\\)\\approx\\bh(\\bmu)\\]\n\\[\\var(\\)\\approx D\\bh(\\bmu) \\, \\bSigma \\, (D\\bh(\\bmu))^T\\]univariate Delta method special case:\n\\[\\expect(y) \\approx h(\\mu)\\]\n\\[\\var(y)\\approx \\sigma^2 h'(\\mu)^2\\]Note Delta approximation breaks \\(\\var(\\)\\) singular,\nexample first derivative (gradient Jacobian matrix) \\(\\bmu\\) zero.Example 2.6  Variance odds ratioThe proportion \\(\\hat{p} = \\frac{n_1}{n}\\) resulting \n\\(n\\) repeats Bernoulli experiment expectation \\(\\expect(\\hat{p})=p\\)\nvariance \\(\\var(\\hat{p}) = \\frac{p (1-p)}{n}\\).\n(approximate) mean variance corresponding odds ratio \\(\\widehat{}=\\frac{\\hat{p}}{1-\\hat{p}}\\)?\\(h(x) = \\frac{x}{1-x}\\),\n\\(\\widehat{} = h(\\hat{p})\\) \\(h'(x) = \\frac{1}{(1-x)^2}\\) get using \nDelta method\n\\(\\expect( \\widehat{} ) \\approx h(p) = \\frac{p}{1-p}\\) \n\\(\\var( \\widehat{} )\\approx h'(p)^2 \\var( \\hat{p} ) = \\frac{p}{n (1-p)^3}\\).Example 2.7  Log-transform variance stabilisationAssume \\(x\\) mean \\(\\expect(x)=\\mu\\) variance \\(\\var(x) = \\sigma^2 \\mu^2\\),\n.e. standard deviation \\(\\sd(x)\\) proportional mean \\(\\mu\\).\n(approximate) mean variance log-transformed variable \\(\\log(x)\\)?\\(h(x) = \\log(x)\\) \\(h'(x) = \\frac{1}{x}\\) get using \nDelta method\n\\(\\expect( \\log(x) ) \\approx h(\\mu) = \\log(\\mu)\\) \n\\(\\var( \\log(x) )\\approx h'(\\mu)^2 \\var( x ) = \\left(\\frac{1}{\\mu} \\right)^2 \\sigma^2 \\mu^2 = \\sigma^2\\). Thus, applying log-transform variance depend mean!","code":""},{"path":"transformations-and-dimension-reduction.html","id":"transformation-of-a-probability-density-function-under-a-general-invertible-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.2.3 Transformation of a probability density function under a general invertible transformation","text":"Assume \\(\\bh(\\bx) = \\(\\bx)\\) invertible: \\(\\bh^{-1}(\\)=\\bx(\\)\\)\\(\\bx \\sim F_{\\bx}\\) probability density function \\(f_{\\bx}(\\bx)\\)density \\(f_{\\}(\\)\\) transformed random vector \\(\\\\) given \\[f_{\\}(\\) = |\\det\\left( D\\bx(\\) \\right)| \\,\\,\\,  f_{\\bx}\\left( \\bx(\\) \\right)\\]\\(D\\bx(\\)\\) Jacobian matrix inverse transformation.Special cases:Univariate version: \\(f_y(y) = |\\frac{dx(y)}{dy}| \\, f_x\\left(x(y)\\right)\\)Linear transformation \\(\\bh(\\bx) = \\ba + \\bB \\bx\\), \\(\\bx(\\) = \\bB^{-1}(\\-\\ba)\\)\n\\(D\\bx(\\) = \\bB^{-1}\\):\n\\[f_{\\}(\\)=|\\det(\\bB)|^{-1} f_{\\bx} \\left( \\bB^{-1}(\\-\\ba)\\right)\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"normalising-flows","chapter":"2 Transformations and dimension reduction","heading":"2.2.4 Normalising flows","text":"machine learning (sequences ) invertible nonlinear transformations known “normalising flows”. used generative way (building complex models \nsimple models) also simplification dimension reduction context.module focus mostly linear transformations underpin\nmuch classical multivariate statistics, important keep mind later study\nimportance nonlinear transformations —see, e.g, review paper Kobyzev et al. “Normalizing Flows: Introduction Ideas”, available https://arxiv.org/abs/1908.09257 .","code":""},{"path":"transformations-and-dimension-reduction.html","id":"whitening-transformations","chapter":"2 Transformations and dimension reduction","heading":"2.3 Whitening transformations","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"overview","chapter":"2 Transformations and dimension reduction","heading":"2.3.1 Overview","text":"Mahalanobis transform (also know “zero-phase component analysis” short ZCA transform machine learning) specific example whitening transformation. constitute important widely used class invertible location-scale transformations.Terminology: whitening refers fact transformation covariance matrix spherical, isotrop, white (\\(\\bI_d\\))Whitening useful preprocessing, turn multivariate problems simple univariate models reduce dimension optimal way.-called latent variable models whitening procedures link observed latent variables (usually uncorrelated standardised random variables):\\[\\begin{align*}\n\\begin{array}{cl}\n\\text{Whitening} \\\\\n\\downarrow\n\\end{array}\n\\begin{array}{ll}\n\\bx \\\\\n\\uparrow \\\\\n\\bz \\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{Observed variable (can measured), external, typically correlated} \\\\\n\\space \\\\\n\\text{Unobserved \"latent\" variable, internal, typically correlated} \\\\\n\\end{array}\n\\end{align*}\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"general-whitening-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.3.2 General whitening transformation","text":"Starting point:Random vector \\(\\bx \\sim F_{\\bx}\\) (necessarily multivariate normal).random variance \\(\\bx\\) mean \\(\\expect(\\bz)=\\bmu\\) positive definite (invertible) covariance matrix \\(\\var(\\bx) = \\bSigma\\).\ncovariance can split positive variances \\(\\bV\\) \npositive definite invertible correlation matrix \\(\\bRho\\) \\(\\bSigma = \\bV^{1/2} \\bRho \\bV^{1/2}\\).Whitening transformation:\\[\\underbrace{\\bz}_{d \\times 1 \\text{ vector }} = \\underbrace{\\bW}_{d \\times d \\text{ whitening matrix }} \\underbrace{\\bx}_{d \\times 1 \\text{ vector }}\\]\nObjective: choose \\(\\bW\\) \\(\\var(\\bz)=\\bI_d\\)Mahalanobis/ZCA whitening already know \\(\\bW^{\\zca}=\\bSigma^{-1/2}\\).general, whitening matrix \\(\\bW\\) needs satisfy constraint:\n\\[\n\\begin{array}{lll}\n                & \\var(\\bz) & = \\bI_d \\\\\n\\Longrightarrow & \\var(\\bW\\bx) &= \\bW  \\bSigma \\bW^T = \\bI_d \\\\\n\\Longrightarrow &  \\bW \\, \\bSigma \\, \\bW^T \\bW = \\bW & \\\\\n\\end{array}\n\\]\n\\[\\Longrightarrow \\text{constraint whitening matrix: } \\bW^T \\bW = \\bSigma^{-1}\\]Clearly, ZCA whitening matrix satisfies constraint: \\((\\bW^{ZCA})^T \\bW^{ZCA} = \\bSigma^{-1/2}\\bSigma^{-1/2}=\\bSigma^{-1}\\)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"solution-of-whitening-constraint-covariance-based","chapter":"2 Transformations and dimension reduction","heading":"2.3.3 Solution of whitening constraint (covariance-based)","text":"general way specify valid whitening matrix \n\\[\n\\bW = \\bQ_1 \\bSigma^{-1/2}\n\\]\n\\(\\bQ_1\\) orthogonal matrixRecall orthogonal matrix \\(\\bQ\\) property \\(\\bQ^{-1} = \\bQ^T\\) \nconsequence \\(\\bQ^T \\bQ = \\bQ \\bQ^T = \\bI\\).result, \\(\\bW\\) satisfies whitening constraint:\\[\\bW^T \\bW = \\bSigma^{-1/2}\\underbrace{\\bQ_1^T \\bQ_1}_{\\bI}\\bSigma^{-1/2}=\\bSigma^{-1}\\]Note converse also true: whitening whitening matrix, .e. \\(\\bW\\) satisfying whitening constraint, can written form \n\\(\\bQ_1 = \\bW \\bSigma^{1/2}\\) orthogonal construction.\\(\\Longrightarrow\\) instead choosing \\(\\bW\\), choose orthogonal matrix \\(\\bQ_1\\)!recall orthogonal matrices geometrically represent rotations (plus reflections).now clear infinitely many whitening procedures, infinitely many rotations! also means need find ways choose/select among whitening procedures.Mahalanobis/ZCA transformation \\(\\bQ_1^{\\zca}=\\bI\\)whitening can interpreted Mahalanobis transform followed rotation","code":""},{"path":"transformations-and-dimension-reduction.html","id":"another-solution-correlation-based","chapter":"2 Transformations and dimension reduction","heading":"2.3.4 Another solution (correlation-based)","text":"Instead working covariance matrix \\(\\bSigma\\), can express \\(\\bW\\) also terms corresponding correlation matrix \\(\\bRho = (\\rho_{ij}) = \\bV^{-1/2} \\bSigma \\bV^{-1/2}\\)\n\\(\\bV^{1/2}\\) diagonal matrix containing variances.Specifically can specify whitening matrix \n\\[\\bW = \\bQ_2 \\bRho^{-1/2} \\bV^{-1/2}\\]easy verify \\(\\bW\\) also satisfies whitening constraint:\n\\[\n\\begin{split}\n\\bW^T \\bW & = \\bV^{-1/2}\\bRho^{-1/2}\\underbrace{\\bQ_2^T \\bQ_2}_{\\bI}\\bRho^{-1/2} \\bV^{-1/2} \\\\\n& = \\bV^{-1/2} \\bRho^{-1} \\bV^{-1/2} = \\bSigma^{-1} \\\\\n\\end{split}\n\\]\nConversely, whitening matrix \\(\\bW\\) can also written form \n\\(\\bQ_2 = \\bW \\bV^{1/2} \\bRho^{1/2}\\) orthogonal construction.Another interpretation whitening: first standardising (\\(\\bV^{-1/2}\\)), decorrelation (\\(\\bRho^{-1/2}\\)), followed rotation (\\(\\bQ_2\\))Mahalanobis/ZCA transformation \\(\\bQ_2^{\\zca} = \\bSigma^{-1/2} \\bV^{1/2} \\bRho^{1/2}\\)forms write \\(\\bW\\) using \\(\\bQ_1\\) \\(\\bQ_2\\) equally valid (interchangeable).Note \\(\\bW\\)\n\\[\\bQ_1\\neq\\bQ_2 \\text{  Two different orthogonal matrices!}\\]\nalso\n\\[\\underbrace{\\bSigma^{-1/2}}_{\\text{Symmetric}}\\neq\\underbrace{\\bRho^{-1/2}\\bV^{-1/2}}_{\\text{Symmetric}}\\]\neven though\\[\\bSigma^{-1/2}\\bSigma^{-1/2}=\\bSigma^{-1} = \\bV^{-1/2}\\bRho^{-1/2}\\bRho^{-1/2}\\bV^{-1/2}\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"cross-covariance-and-cross-correlation","chapter":"2 Transformations and dimension reduction","heading":"2.3.5 Cross-covariance and cross-correlation","text":"useful criterion distinguish whitening transformation consider \ncross-covariance cross-correlation:Cross-covariance \\(\\bPhi = \\Sigma_{\\bz \\bx}\\) \\(\\bz\\) \\(\\bx\\):\n\\[\n\\begin{split}\n\\bPhi = \\cov(\\bz,\\bx) & = \\cov(\\bW \\bx,\\bx)\\\\\n& = \\bW\\bSigma \\\\\n&= \\bQ_1 \\bSigma^{-1/2} \\bSigma \\\\\n&= \\bQ_1\\bSigma^{1/2} \\\\\n\\end{split}\n\\]\nCross-covariance linked \\(\\bQ_1\\)!\nThus, choosing cross-covariance determines \\(\\bQ_1\\) (vice versa).Note cross-covariance matrix \\(\\bPhi\\) satisfies condition\n\\(\\bPhi^T \\bPhi = \\bSigma\\), reminiscent condition \\(\\bW\\),\nnow covariance used rather inverse covariance.whitening matrix expressed terms cross-covariance \\(\\bW= \\bPhi \\bSigma^{-1}\\), required \\(\\bW^T \\bW = \\bSigma^{-1} \\bPhi^T \\bPhi \\bSigma^{-1} =\\bSigma^{-1}\\).\nFurthermore, transpose \\(\\bPhi\\) \ninverse whitening matrix,\n\\(\\bW^{-1} = \\bSigma^{1/2} \\bQ_1^{-1} = \\bSigma^{1/2} \\bQ_1^{-T} = \\bPhi^T\\).Cross-correlation \\(\\bPsi = \\bRho_{\\bz \\bx}\\) \\(\\bz\\) \\(\\bx\\):\n\\[\n\\begin{split}\n\\bPsi = \\cor(\\bz,\\bx) & = \\bPhi \\bV^{-1/2}\\\\\n& = \\bW \\bSigma \\bV^{-1/2}\\\\\n&=\\bQ_2 \\bRho^{-1/2} \\bV^{-1/2} \\bSigma \\bV^{-1/2} \\\\\n& =  \\bQ_2\\bRho^{1/2}\\\\\n\\end{split}\n\\]\nCross-correlation linked \\(\\bQ_2\\)!\nHence, choosing cross-correlation determines \\(\\bQ_2\\) (vice versa). whitening\nmatrix expressed terms cross-correlation \n\\(\\bW = \\bPsi \\bRho^{-1} \\bV^{-1/2}\\).Note factorisation cross-covariance \\(\\bPhi=\\bQ_1\\bSigma^{1/2}\\) \ncross-correlation \\(\\bPsi=\\bQ_2\\bRho^{1/2}\\) product orthogonal matrix\npositive semi-definite symmetric matrix -called polar decomposition.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"inverse-whitening-transformation-loadings-and-multiple-correlation","chapter":"2 Transformations and dimension reduction","heading":"2.3.6 Inverse whitening transformation, loadings, and multiple correlation","text":"Reverse transformation:Recall \\(\\bz = \\bW \\bx\\). Therefore, reverse transformation going whitened\noriginal variable \\(\\bx = \\bW^{-1} \\bz\\).\ncan expressed also terms cross-covariance cross-correlation.\n\\(\\bW^{-1} = \\bPhi^T\\) get\n\\[\n\\bx = \\bPhi^T \\bz \\, .\n\\]\nFurthermore, since \\(\\bPsi = \\bPhi \\bV^{-1/2}\\) \n\\(\\bW^{-1} = \\bV^{1/2} \\bPsi^T\\) \n\\[\n\\bV^{-1/2} \\bx =   \\bPsi^T \\bz \\, .\n\\]reverse whitening transformation also known colouring transformation\n(previously discussed inverse Mahalanobis transform one example).Definition loadings:Loadings coefficients linear transformation latent variable back observed variable. variables standardised unit variance loadings also called correlation loadings.Hence, cross-covariance matrix plays role loadings linking latent variable \\(\\bz\\)\noriginal \\(\\bx\\). Similarly, cross-correlation matrix correlation loadings\nlinking (already standardised) latent variable \\(\\bz\\) standardised \\(\\bx\\).Multiple correlation coefficients \\(\\bz\\) \\(\\bx\\):Consider backtransformation \\(\\bz\\) \\(\\bx\\).\ncomponents \\(\\bz\\) uncorrelated. Therefore, can compute squared multiple correlation coefficient \\(x_j\\) \\(\\bz\\) sum squared correlations\n\\(\\cor(z_i, x_j)^2\\):\n\\[\n\\cor(\\bz, x_j)^2 = \\sum_{=1}^d  \\cor(z_i, x_j)^2    = \\sum_{=1}^d \\psi_{ij}^2\n\\]\nvector notation \\(\\bPsi = (\\psi_{ij})\\) get\n\\[\n\\begin{split}\n(\\cor(\\bz, x_j)^2 )^T &= \\diag(\\bPsi^T \\bPsi) \\\\\n&= \\diag(\\bRho^{1/2} \\bQ_2^T \\bQ_2\\bRho^{1/2}) \\\\\n&= \\diag(\\bRho) \\\\\n&= (1, \\ldots, 1)^T\\\\\n\\end{split}\n\\]\nTherefore column sums matrix \\((\\psi_{ij}^2)\\) 1 regardless choice \\(\\bQ_2\\):\n\\[\n\\sum_{=1}^d \\psi_{ij}^2 = 1 \\text{ } j\n\\]easy understand get multiple squared correlations value 1 — \\(x_j\\) linear function \\(z_1, \\ldots, z_d\\) noise term, means \\(x_j\\) can predicted perfectly \\(\\bz\\) error.Multiple correlation coefficients \\(\\bx\\) \\(\\bz\\):original direction going \\(x_1, \\ldots, x_d\\) \\(z_i\\) corresponding squared multiple correlations \\(\\cor(z_i, \\bx)^2\\) also 1, \\(x_j\\) correlated simply sum squared correlations get \\(\\cor(z_i, \\bx)^2\\) also need take account correlations among \\(\\bx_j\\) (.e. \\(\\bRho\\)).\nvector notation:\n\\[\n\\begin{split}\n(\\cor(z_i, \\bx)^2 )^T &= \\diag( \\bPsi \\bRho^{-1} \\bPsi^T) \\\\\n &= \\diag( \\bQ_2 \\bRho^{1/2} \\bRho^{-1} \\bRho^{1/2} \\bQ_2^T ) \\\\\n & = \\diag(\\bI) \\\\\n & = (1, \\ldots, 1)^T\\\\\n\\end{split}\n\\]","code":""},{"path":"transformations-and-dimension-reduction.html","id":"natural-whitening-procedures","chapter":"2 Transformations and dimension reduction","heading":"2.4 Natural whitening procedures","text":"Now discuss several strategies (maximise correlation individual components, maximise compression, etc.) arrive optimal whitening transformation.leads following “natural” whitening transformations:Mahalanobis whitening, also known ZCA (zero-phase component analysis) whitening machine learningZCA-cor whiteningPCA whiteningPCA-cor whiteningCholesky whiteningIn following \\(\\bx_c = \\bx-\\bmu_{\\bx}\\) \\(\\bz_c = \\bz-\\bmu_{\\bz}\\) denote mean-centered variables.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"zca-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.1 ZCA whitening","text":"Aim: remove correlations otherwise make sure whitening \\(\\bz\\) differ much \\(\\bx\\). Specifically, element \\(z_i\\) close possible corresponding element \\(x_i\\):\n\\[\n\\begin{array}{cc}\nz_1\\leftrightarrow x_1 \\\\\nz_2\\leftrightarrow x_2\\\\\nz_3\\leftrightarrow x_3 \\\\\n\\vdots\n\\end{array}\n\\]\nOne possible way implement compute expected squared difference two centered random vectors \\(\\bz_c\\) \\(\\bx_c\\).ZCA objective function: minimise \\(\\expect \\left((\\bz_c-\\bx_c)^T(\\bz_c-\\bx_c)\\right)\\) find optimal whitening procedure.ZCA objective function can simplified follows:\n\\[\n\\begin{split}\n& = \\expect ( \\bz_c^T \\bz_c ) - 2 \\expect ( \\bx_c^T \\bz_c ) + \\expect (\\bx_c^T \\bx_c)  \\\\\n& = \\expect ( \\trace( \\bz_c \\bz_c^T ) ) - 2 \\expect ( \\trace( \\bz_c \\bx_c^T ) ) +\n \\expect (  \\trace( \\bx_c \\bx_c^T ) )  \\\\\n& = \\trace ( \\expect( \\bz_c \\bz_c^T ) ) - 2 \\trace ( \\expect(  \\bz_c \\bx_c^T ) ) +\n \\trace (  \\expect( \\bx_c \\bx_c^T ) )  \\\\\n& = \\trace ( \\var(\\bz) ) - 2 \\trace ( \\cov(\\bz, \\bx ) ) + \\trace ( \\var(\\bx) )   \\\\\n& = d - 2\\trace(\\bPhi)+\\trace(\\bV) \\\\\n\\end{split}\n\\]\nterm depends whitening transformation \\(-2 \\trace(\\bPhi)\\) function\n\\(\\bQ_1\\). Therefore can use following\nalternative objective:ZCA equivalent objective: maximise \\(\\trace(\\bPhi) = \\trace(\\bQ_1\\bSigma^{1/2})\\) find optimal \\(\\bQ_1\\)sum \\(\\sum_{=1}^d \\cov(z_i, x_i)\\) covariances corresponding\nelements \\(\\bz\\) \\(\\bx\\).Solution:Apply eigendecomposition \\(\\bSigma= \\bU \\bLambda \\bU^T\\). Note \\(\\bLambda\\) diagonal positive entries \\(\\lambda_i > 0\\) \\(\\bSigma\\) positive definite.objective function becomes\n\\[\n\\begin{split}\n\\trace(\\bQ_1\\bSigma^{1/2}) &= \\trace(\\bQ_1 \\bU \\bLambda^{1/2} \\bU^T  ) \\\\\n&= \\trace(\\bLambda^{1/2} \\, \\bU^T \\bQ_1 \\bU) \\\\\n& = \\trace(\\bLambda^{1/2} \\, \\bB) \\\\\n& = \\sum_{=1}^d \\lambda_i b_{ii}.\n\\end{split} \n\\]\nNote product orthogonal matrices \\(\\bB = \\bU^T \\bQ_1 \\bU\\) orthogonal matrix, \n\\(\\bQ_1 = \\bU \\bB \\bU^T\\).\\(\\lambda_i > 0\\) objective function maximised orthogonal matrix \\(\\bB =\\bI\\).Thus, optimal \\(\\bQ_1\\) matrix \\[\\bQ_1^{\\zca}=\\bI\\]corresponding whitening matrix ZCA \n\\[\n\\bW^{\\zca} = \\bSigma^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\bPhi^{\\zca} = \\bSigma^{1/2}\n\\]\ncross-correlation matrix\n\\[\n\\bPsi^{\\zca} = \\bSigma^{1/2} \\bV^{-1/2}\n\\]Note \\(\\bSigma^{1/2}\\) symmetric positive definite matrix,\nhence diagonal elements positive. result,\ndiagonals \\(\\bPhi^{\\zca}\\) \\(\\bPsi^{\\zca}\\) positive,\n.e. \\(\\cov(z_i, x_i) > 0\\) \\(\\cor(z_i, x_i) > 0\\).\nHence, ZCA two corresponding components \\(z_i\\) \\(x_i\\) always positively correlated!Summary:ZCA/Mahalanobis transform unique transformation minimises expected total squared component-wise difference \\(\\bx_c\\) \\(\\bz_c\\).corresponding components whitened original variables always positively correlated facilitates interpretation whitening variables.Use ZCA aka Mahalanobis whitening want “just” remove correlations.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"zca-cor-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.2 ZCA-Cor whitening","text":"Aim: remove scale \\(\\bx\\) first comparing \\(\\bz\\)ZCA-cor objective function: minimise \\(\\expect \\left((\\bz_c-\\bV^{-1/2}\\bx_c)^T(\\bz_c-\\bV^{-1/2}\\bx_c)\\right)\\) find optimal whitening procedure.can simplified follows:\n\\[\n\\begin{split}\n& = \\expect ( \\bz_c^T \\bz_c ) - 2 \\expect ( \\bx_c^T \\bV^{-1/2} \\bz_c ) + \\expect (\\bx_c^T \\bV^{-1} \\bx_c)  \\\\\n& = \\trace ( \\var(\\bz) ) - 2 \\trace ( \\cor(\\bz, \\bx ) ) + \\trace (  \\cor(\\bx, \\bx) )   \\\\\n& = d - 2\\trace(\\bPsi)+ d \\\\\n& = 2d - 2\\trace(\\bPsi)\n\\end{split}\n\\]\nterm depends whitening transformation via \\(\\bQ_2\\) \\(-2 \\trace(\\bPsi)\\) can use following alternative objective instead:ZCA-cor equivalent objective: maximise \\(\\trace(\\bPsi)=\\trace(\\bQ_2\\bRho^{1/2})\\) find optimal \\(\\bQ_2\\)sum \\(\\sum_{=1}^d \\cor(z_i, x_i)\\) correlations corresponding\nelements \\(\\bz\\) \\(\\bx\\).Solution: ZCA using correlation instead covariance:Apply eigendecomposition \\(\\bRho = \\bG \\bTheta \\bG^T\\) positive diagonal \\(\\bTheta\\).objective function becomes \\(\\trace(\\bQ_2\\bRho^{1/2}) ) = \\trace( \\bTheta^{1/2} \\bG^T \\bQ_2 \\bG )\\)maximised \n\\[\\bQ_2^{\\zcacor}=\\bI\\]corresponding whitening matrix ZCA-cor \n\\[\\bW^{\\zcacor} = \\bRho^{-1/2}\\bV^{-1/2}\\]\ncross-covariance matrix \n\\[\n\\bPhi^{\\zcacor} = \\bRho^{1/2} \\bV^{1/2}\n\\]\ncross-correlation matrix \n\\[\n\\bPsi^{\\zcacor} = \\bRho^{1/2}\n\\]ZCA-cor transformation also \n\\(\\cov(z_i, x_i) > 0\\) \\(\\cor(z_i, x_i) > 0\\)\ntwo corresponding components \\(z_i\\) \\(x_i\\) always positively correlated!Summary:ZCA-cor whitening unique whitening transformation maximising \ntotal correlation corresponding elements \\(\\bx\\) \\(\\bz\\).ZCA-cor leads interpretable \\(\\bz\\) individual element \\(\\bz\\)\n(typically strongly) positively correlated corresponding element original \\(\\bx\\).ZCA-cor explicitly constructed maximise total\npairwise correlations achieves higher correlation ZCA.\\(\\bx\\) standardised \\(\\var(x_i)=1\\) ZCA ZCA-cor identical otherwise different whitening transformations.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.3 PCA whitening","text":"Aim: remove correlations compress information latent variables.\nSpecfically, like first latent component \\(z_1\\) \nmaximally linked variables \\(\\bx\\), followed \nsecond component \\(z_2\\) :\n\\[\n\\begin{array}{ccccccc}\nz_1 & \\leftarrow x_1 & & z_2 & \\leftarrow x_1  && \\ldots \\\\\nz_1 & \\leftarrow x_2 & & z_2 & \\leftarrow x_2  \\\\\n\\vdots\\\\\nz_1 & \\leftarrow x_d & & z_2 & \\leftarrow x_d  \\\\\n\\end{array}\n\\]\nOne way measure total link \\(z_i\\) \\(x_j\\) sum \ncorresponding squared covariances\n\\[\np_i = \\sum^d_{j=1}\\cov(z_i,x_j)^2 = \\sum^d_{j=1} \\phi_{ij}^2\n\\]\nvector notation write\n\\[\n\\bp= (p_1,...,p_d)^T = \\diag(\\bPhi\\bPhi^T)\n\\]\n\\(\\bPhi = \\bQ_1 \\bSigma^{1/2}\\) can written \\(\\bp =\\diag(\\bQ_1\\bSigma\\bQ_1^T)\\)\nfunction \\(\\bQ_1\\).PCA objective functions: maximise \\(p_1, \\ldots, p_{d-1}\\) \\(\\bp = \\diag(\\bQ_1\\bSigma\\bQ_1^T)\\) \\(p_1 \\geq p_2 \\geq \\dots \\geq p_d\\)\nfind optimal optimal \\(\\bQ_1\\) corresponding whitening transformation.Note \\(\\sum_{=1}^d p_i = \\trace( \\bQ_1 \\bSigma \\bQ_1 ) = \\trace(\\bSigma)\\)\nconstant regardless choice \\(\\bQ_1\\) \\(d-1\\) independent \\(p_i\\).Solution:Apply eigendecomposition \\(\\bSigma= \\bU \\bLambda \\bU^T\\). Note \\(\\bLambda\\) diagonal positive entries \\(\\lambda_1 \\geq \\lambda_2 \\ldots \\geq \\lambda_d > 0\\) \\(\\bSigma\\) positive definite eigenvalues already arranged non-increasing order.\nAlso recall \\(\\bU\\) uniquely defined — free change columns signs.Apply eigendecomposition \\(\\bSigma= \\bU \\bLambda \\bU^T\\). Note \\(\\bLambda\\) diagonal positive entries \\(\\lambda_1 \\geq \\lambda_2 \\ldots \\geq \\lambda_d > 0\\) \\(\\bSigma\\) positive definite eigenvalues already arranged non-increasing order.\nAlso recall \\(\\bU\\) uniquely defined — free change columns signs.objective functions become\n\\(\\bp = \\diag( (\\bQ_1 \\bU) \\bLambda (\\bQ_1 \\bU)^T ) = \\diag( \\bB \\bLambda \\bB^T)\\) \\(\\bB\\) orthogonal matrix, \\(\\bQ_1= \\bB \\bU^T\\).objective functions become\n\\(\\bp = \\diag( (\\bQ_1 \\bU) \\bLambda (\\bQ_1 \\bU)^T ) = \\diag( \\bB \\bLambda \\bB^T)\\) \\(\\bB\\) orthogonal matrix, \\(\\bQ_1= \\bB \\bU^T\\).optimal (maximum) values achieved \\(\\bB = \\bI\\), \\(p_i^{\\pca} = \\lambda_i\\).\nHowever, solution — can arbitrarily change\ncolumn signs matrix \\(\\bB\\) arrive maximum!optimal (maximum) values achieved \\(\\bB = \\bI\\), \\(p_i^{\\pca} = \\lambda_i\\).\nHowever, solution — can arbitrarily change\ncolumn signs matrix \\(\\bB\\) arrive maximum!corresponding optimal value \\(\\bQ_1\\) matrix \n\\[\n\\bQ_1^{\\pca}=\\bU^T\n\\]\ncorresponding whitening matrix \n\\[\n\\bW^{\\pca} = \\bU^T\\bSigma^{-1/2}=\\bLambda^{-1/2}\\bU^T\n\\]\ncross-covariance matrix \n\\[\n\\bPhi^{\\pca} = \\bLambda^{1/2} \\bU^T\n\\]\ncross-correlation matrix \n\\[\n\\bPsi^{\\pca} = \\bLambda^{1/2} \\bU^T \\bV^{-1/2}\n\\]corresponding optimal value \\(\\bQ_1\\) matrix \n\\[\n\\bQ_1^{\\pca}=\\bU^T\n\\]\ncorresponding whitening matrix \n\\[\n\\bW^{\\pca} = \\bU^T\\bSigma^{-1/2}=\\bLambda^{-1/2}\\bU^T\n\\]\ncross-covariance matrix \n\\[\n\\bPhi^{\\pca} = \\bLambda^{1/2} \\bU^T\n\\]\ncross-correlation matrix \n\\[\n\\bPsi^{\\pca} = \\bLambda^{1/2} \\bU^T \\bV^{-1/2}\n\\]Note (.e. \\(\\bQ_1^{\\pca}, \\bW^{\\pca}, \\bPhi^{\\pca}, \\bPsi^{\\pca}\\)) unique\nstill sign ambiguity columns\n\\(\\bU\\) (also absorbed sign ambiguity \\(\\bB\\))!Identifiability:Therefore, identifiability reasons need impose constraint \\(\\bQ_1^{\\pca}\\).\nuseful condition require positive\ndiagonal, .e. \\(\\diag(\\bQ_1^{\\pca}) > 0\\) also \\(\\diag(\\bU) > 0\\).\nresult, \\(\\diag(\\bPhi^{\\pca}) > 0\\) \\(\\diag(\\bPsi^{\\pca}) > 0\\). \nconstraint place pairs \\(x_i\\) \\(z_i\\) positively correlated.particularly important pay attention sign ambiguity\ndifferent computer implementations PCA whitening (related PCA approach)\nused.Proportion total variation:sum maximised squared covariances latent component \\(z_i\\) \n\\(\\sum_{=1}^d p_i^{\\pca} = \\sum_{=1}^d \\lambda_i\\)\nequals total variation \\(\\trace(\\bSigma)\\).fraction \\(\\frac{\\lambda_i}{\\sum^d_{j=1}\\lambda_j}\\) proportional\ncontribution element \\(\\bz\\) explain total variation.\nThus, low ranking components \\(z_i\\) small \\(p_i^{\\pca}=\\lambda_i\\) may discarded.\nfact, aim PCA whitening achieve kind compression resuling\nreduction dimension latent space.Summary:PCA whitening whitening transformation maximises compression sum squared cross-covariances underlying optimality criterion.sign ambiguities PCA whitened variables inherited sign ambiguities eigenvectors.positive-diagonal condition orthogonal matrices imposed sign ambiguities fully resolved corresponding components \\(z_i\\) \\(x_i\\) always positively correlated.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-cor-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.4 PCA-cor whitening","text":"Aim: PCA whitening remove scale \\(\\bx\\) first. means use squared correlations rather squared covariances measure compression, .e.\\[\np_i = \\sum^d_{j=1}\\cor(z_i, x_j)^2 = \\sum^d_{j=1} \\psi_{ij}^2\\]\nvector notation becomes\n\\[\n\\bp = \\diag(\\bPsi\\bPsi^T)=\\diag(\\bQ_2\\bRho\\bQ_2^T)\n\\]PCA-cor objective functions:\nmaximise \\(p_1, \\ldots, p_{d-1}\\) \\(\\bp = \\diag(\\bQ_2\\bRho\\bQ_2^T)\\) \\(p_1 \\geq p_2 \\geq \\dots \\geq p_d\\)\nfind optimal optimal \\(\\bQ_2\\) corresponding whitening transformation.Note \\(\\sum_{=1}^d p_i = \\trace( \\bQ_2 \\bRho \\bQ_2 ) = \\trace(\\bRho) = d\\)\nconstant regardless choice \\(\\bQ_2\\) \\(d-1\\) independent \\(p_i\\).Solution:Apply eigendecomposition \\(\\bRho= \\bG \\bTheta \\bG^T\\). Note \\(\\bTheta\\) diagonal positive entries \\(\\theta_1 \\geq \\theta_2 \\ldots \\geq \\theta_d > 0\\) \\(\\bRho\\) positive definite eigenvalues already arranged non-increasing order. Also recall \\(\\bG\\) uniquely defined — free change columns signs.Apply eigendecomposition \\(\\bRho= \\bG \\bTheta \\bG^T\\). Note \\(\\bTheta\\) diagonal positive entries \\(\\theta_1 \\geq \\theta_2 \\ldots \\geq \\theta_d > 0\\) \\(\\bRho\\) positive definite eigenvalues already arranged non-increasing order. Also recall \\(\\bG\\) uniquely defined — free change columns signs.objective functions become\n\\(\\bp = \\diag( (\\bQ_2 \\bG) \\bTheta (\\bQ_2 \\bG)^T ) = \\diag( \\bB \\bTheta \\bB^T)\\) \\(\\bB\\) orthogonal matrix, \\(\\bQ_2= \\bB \\bG^T\\).objective functions become\n\\(\\bp = \\diag( (\\bQ_2 \\bG) \\bTheta (\\bQ_2 \\bG)^T ) = \\diag( \\bB \\bTheta \\bB^T)\\) \\(\\bB\\) orthogonal matrix, \\(\\bQ_2= \\bB \\bG^T\\).optimal (maximum) values achieved \\(\\bB = \\bI\\), \\(p_i^{\\pcacor} = \\theta_i\\).\nHowever, solution — can arbitrarily change\ncolumn signs matrix \\(\\bB\\) arrive maximum!optimal (maximum) values achieved \\(\\bB = \\bI\\), \\(p_i^{\\pcacor} = \\theta_i\\).\nHowever, solution — can arbitrarily change\ncolumn signs matrix \\(\\bB\\) arrive maximum!corresponding optimal value \\(\\bQ_2\\) matrix \n\\[\n\\bQ_2^{\\pcacor}=\\bG^T\n\\]\ncorresponding whitening matrix \\[\n\\bW^{\\pcacor} = \\bTheta^{-1/2} \\bG^T \\bV^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\bPhi^{\\pcacor} = \\bTheta^{1/2} \\bG^T \\bV^{1/2}\n\\]\ncross-correlation matrix \n\\[\n\\bPsi^{\\pcacor} = \\bTheta^{1/2} \\bG^T\n\\]\nPCA whitening, sign ambiguities column signs \\(\\bG\\)\ncan freely chosen.corresponding optimal value \\(\\bQ_2\\) matrix \n\\[\n\\bQ_2^{\\pcacor}=\\bG^T\n\\]\ncorresponding whitening matrix \\[\n\\bW^{\\pcacor} = \\bTheta^{-1/2} \\bG^T \\bV^{-1/2}\n\\]\ncross-covariance matrix \n\\[\n\\bPhi^{\\pcacor} = \\bTheta^{1/2} \\bG^T \\bV^{1/2}\n\\]\ncross-correlation matrix \n\\[\n\\bPsi^{\\pcacor} = \\bTheta^{1/2} \\bG^T\n\\]\nPCA whitening, sign ambiguities column signs \\(\\bG\\)\ncan freely chosen.Identifiability:identifiability choose set \\(\\diag(\\bQ_2^{\\pcacor}) > 0\\) also\n\\(\\diag(\\bG) > 0\\) \\(\\diag(\\bPhi^{\\pcacor}) > 0\\) \\(\\diag(\\bPsi^{\\pcacor}) > 0\\).Proportion total variation:sum maximised squared correlations latent component \\(z_i\\) \n\\(\\sum_{=1}^d p_i^{\\pcacor} = \\sum_{=1}^d \\theta_i = d\\)\nequals total variation \\(\\trace(\\bRho)\\).\nTherefore fraction\n\\(\\frac{\\theta_i}{\\sum^d_{j=1} \\theta_j} = \\frac{\\theta_j}{d}\\) \nproportional\ncontribution element \\(\\bz\\) explain total variation.Summary:PCA-cor whitening whitening transformation maximises compression sum squared cross-correlations underlying optimality criterion.sign ambiguities PCA-cor whitened variables inherited sign ambiguities eigenvectors.positive-diagonal condition orthogonal matrices imposed sign ambiguities fully resolved corresponding components \\(z_i\\) \\(x_i\\) always positively correlated.\\(\\bx\\) standardised \\(\\var(x_i)=1\\), PCA PCA-cor whitening identical.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"cholesky-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.5 Cholesky whitening","text":"Aim: find whitening transformation cross-covariance \\(\\bPhi\\) cross-correlation \\(\\bPsi\\) triangular structure. useful models time course data, e.g. ensure \nfuture influence past.Cholesky descomposition square matrix \\(\\bA = \\bL \\bL^T\\) requires positive definite \\(\\bA\\) unique.\n\\(\\bL\\) lower triangular matrix positive diagonal elements.\ninverse \\(\\bL^{-1}\\) also lower triangular positive diagonal elements.obtain whitening transformation can apply Cholesky decomposition\neither precision matrix covariance matrix.Solution 1: Apply Cholesky decomposition \\(\\bSigma^{-1} = \\bL\\bL^T\\)resulting whitening matrix \n\\[\n\\bW^{\\chol.1}=\\bL^T\n\\]\nconstruction, \\(\\bW^{\\chol.1}\\) upper triangular matrix positive\ndiagonal, clearly satisfies whitening constraint since \\((\\bW^{\\chol.1})^T\\bW^{\\chol.1} = \\bSigma^{-1}\\).cross-covariance matrix (\\(\\bSigma = (\\bL^{-1})^T \\bL^{-1}\\))\n\\[\n\\bPhi^{\\chol.1} = \\bL^T\\bSigma =  \\bL^{-1}\n\\]\ncross-correlation matrix \n\\[\n\\bPsi^{\\chol.1} = \\bL^T \\bSigma \\bV^{-1/2} =  \\bL^{-1} \\bV^{-1/2}\n\\]\nNote \\(\\bPhi^{\\chol.1}\\) \n\\(\\bPsi^{\\chol.1}\\) \nlower triangular matrices positive diagonal elements.\nHence two corresponding components \\(z_i\\) \\(x_i\\) always positively correlated!Finally, corresponding orthogonal matrices \n\\[\n\\bQ_1^{\\chol.1} = \\bL^T \\bSigma^{1/2} = \\bL^{-1} \\bSigma^{-1/2}\n\\]\n\n\\[\n\\bQ_2^{\\chol.1} = \\bL^T \\bV^{1/2} \\bRho^{1/2} = \\bL^{-1} \\bV^{-1/2} \\bRho^{-1/2}\n\\]Solution 2: Apply Cholesky decomposition \\(\\bSigma = \\bF \\bF^T\\)resulting whitening matrix \n\\[\n\\bW^{\\chol.2}=\\bF^{-1}\n\\]\nconstruction, \\(\\bW^{\\chol.2}\\) lower triangular matrix positive\ndiagonal. whitening constraint satisfied \n\\((\\bW^{\\chol.2})^T\\bW^{\\chol.2} = (\\bF^{-1})^T \\bF^{-1} = (\\bF^T)^{-1} \\bF^{-1} = (\\bF \\bF^T)^{-1} = \\bSigma^{-1}\\).cross-covariance matrix (recall general \\(\\bW^{-1} = \\bPhi^T\\))\n\\[\n\\bPhi^{\\chol.2} = \\bF^T\n\\]\ncross-correlation matrix \n\\[\n\\bPsi^{\\chol.2} =  \\bF^T  \\bV^{-1/2}\n\\]\n\\(\\bPhi^{\\chol.2}\\) \n\\(\\bPsi^{\\chol.2}\\) \nupper triangular matrices positive diagonal elements.\nHence two corresponding components \\(z_i\\) \\(x_i\\) always positively correlated!Finally, corresponding orthogonal matrices \n\\[\n\\bQ_1^{\\chol.2}  = \\bF^T \\bSigma^{-1/2} = \\bF^{-1} \\bSigma^{1/2}\n\\]\n\n\\[\n\\bQ_2^{\\chol.2} = \\bF^T \\bV^{-1/2} \\bRho^{-1/2} = \\bF^{-1} \\bV^{1/2} \\bRho^{1/2}\n\\]Note two Cholesky whitening procedures different general\n\\(\\bW^{\\chol.1} \\neq \\bW^{\\chol.2}\\).","code":""},{"path":"transformations-and-dimension-reduction.html","id":"comparison-of-zca-pca-and-cholesky-whitening","chapter":"2 Transformations and dimension reduction","heading":"2.4.6 Comparison of ZCA, PCA and Cholesky whitening","text":"comparison, results ZCA, PCA Cholesky whitening (precision matrix) applied simulated bivariate normal data set correlation \\(\\rho=0.8\\).column 1 can see simulated data scatter plot.Column 2 shows scatter plots whitened data — expect three methods removed correlation produce isotropic covariance.three approached differ differ cross-correlations. Columns 3 4 show cross-correlations first two corresponding components (\\(x_1\\) \\(z_1\\), \\(x_2\\) \\(z_2\\)) ZCA, PCA Cholesky whitening. expected, ZCA pairs show strong correlation, case PCA Cholesky whitening.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"recap","chapter":"2 Transformations and dimension reduction","heading":"2.4.7 Recap","text":"Related models discussed course:Factor models: essentially whitening plus additional error term, factors rotational\nfreedom just like whiteningFactor models: essentially whitening plus additional error term, factors rotational\nfreedom just like whiteningPLS: similar PCA regression setting (choice \nlatent variables depending response)PLS: similar PCA regression setting (choice \nlatent variables depending response)","code":""},{"path":"transformations-and-dimension-reduction.html","id":"principal-component-analysis-pca","chapter":"2 Transformations and dimension reduction","heading":"2.5 Principal Component Analysis (PCA)","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-transformation","chapter":"2 Transformations and dimension reduction","heading":"2.5.1 PCA transformation","text":"Traditional PCA invented 1901 Karl Pearson2 closely related PCA whitening.Assume random vector \\(\\bx\\) \\(\\var(\\bx) = \\bSigma = \\bU \\bLambda \\bU^T\\).\nPCA particular orthogonal transformation original \\(\\bx\\)\nresulting components orthogonal:\n\\[\n\\underbrace{\\bt^{\\pca}}_{\\text{Principal components}} = \\underbrace{\\bU^T}_{\\text{Orthogonal matrix}}   \\bx\n\\]\n\\[\\var(\\bt^{\\pca}) = \\bLambda = \\begin{pmatrix} \\lambda_1 & \\dots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & \\lambda_d\\end{pmatrix}\\]\nNote principal components orthogonal unit variance variance principal components \\(t_i\\) equals eigenvalues \\(\\lambda_i\\).Thus PCA whitening procedure. However, arrive PCA whitening simply standardising PCA components: \\(\\bz^{\\pca} = \\bLambda^{-1/2} \\bt^{\\pca}\\)Compression properties:total variation \\(\\trace(\\var(\\bt^{\\pca})) = \\trace( \\bLambda ) = \\sum^d_{j=1}\\lambda_j\\).\nprinciple components fraction \\(\\frac{\\lambda_i}{\\sum^d_{j=1}\\lambda_j}\\) can interpreted proportion variation contributed \ncomponent \\(\\bt^{\\pca}\\) total variation. Thus, low ranking components \\(\\bt^{\\pca}\\) low variation may discarded, thus leading reduction dimension.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"application-to-data","chapter":"2 Transformations and dimension reduction","heading":"2.5.2 Application to data","text":"Written terms data matrix \\(\\bX\\) instead random vector \\(\\bx\\) PCA becomes:\n\\[\\underbrace{\\bT}_{\\text{Sample version principal components}}=\\underbrace{\\bX}_{\\text{Data matrix}}\\bU\\]\nnow two ways obtain \\(\\bU\\):Estimate covariance matrix, e.g. \\(\\hat{\\bSigma} = \\frac{1}{n}\\bX_c^T\\bX_c\\) \\(\\bX_c\\) column-centred data matrix; apply eigenvalue decomposition \\(\\hat{\\bSigma}\\) get \\(\\bU\\).Estimate covariance matrix, e.g. \\(\\hat{\\bSigma} = \\frac{1}{n}\\bX_c^T\\bX_c\\) \\(\\bX_c\\) column-centred data matrix; apply eigenvalue decomposition \\(\\hat{\\bSigma}\\) get \\(\\bU\\).Compute singular value decomposition \\(\\bX_c = \\bV\\bD\\bU^T\\). \\(\\hat{\\bSigma} = \\frac{1}{n}\\bX_c^T\\bX_c = \\bU (\\frac{1}{n}\\bD^2)\\bU^T\\) can just use \\(\\bU\\) SVD \\(\\bX_c\\) need compute covariance.Compute singular value decomposition \\(\\bX_c = \\bV\\bD\\bU^T\\). \\(\\hat{\\bSigma} = \\frac{1}{n}\\bX_c^T\\bX_c = \\bU (\\frac{1}{n}\\bD^2)\\bU^T\\) can just use \\(\\bU\\) SVD \\(\\bX_c\\) need compute covariance.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"iris-data-example","chapter":"2 Transformations and dimension reduction","heading":"2.5.3 Iris data example","text":"example consider famous iris flower data set. consists data botanical variables (sepal length, sepal width,\npetal length petal width) measured 150 flowers \nthree iris species (setosa, versicolr, virginica). Thus data set \\(d=4\\) \\(n=150\\).first standardise data, compute PCA components plot proportion total variance contributed component.\nshows two PCA components needed achieve 95% total variation:scatter plot plot first two principal components also informative:shows groupings among \n150 flowers, corresponding species, groups can characterised\nprincipal components.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"correlation-loadings-plot-to-interpret-pca-components","chapter":"2 Transformations and dimension reduction","heading":"2.6 Correlation loadings plot to interpret PCA components","text":"","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-correlation-loadings","chapter":"2 Transformations and dimension reduction","heading":"2.6.1 PCA correlation loadings","text":"earlier section learned general whitening transformation cross-correlations \\(\\bPsi =\\cor(\\bz, \\bx)\\) play role correlation loadings inverse transformation:\n\\[\n\\bV^{-1/2} \\bx = \\bPsi^T  \\bz \\, , \n\\]\n.e. coefficients linking whitening variable \\(\\bz\\) standardised \\(\\bx\\).\nrelationship holds therefore also PCA whitening\n\\(\\bz^{\\pca}= \\bLambda^{-1/2} \\bU^T \\bx\\) \\(\\bPsi^{\\pca} = \\bLambda^{1/2} \\bU^T \\bV^{-1/2}\\).classical PCA whitening approach \\(\\var(\\bt^{\\pca}) \\neq \\bI\\). However, can still compute cross-correlations principal components \\(\\bt^{\\pca}\\) \\(\\bx\\), resulting \n\\[\n\\cor(\\bt^{\\pca}, \\bx) = \\bLambda^{1/2} \\bU^T \\bV^{-1/2} = \\bPsi^{\\pca}\n\\]\nNote cross-correlations PCA-whitening since\n\\(\\bt^{\\pca}\\) \\(\\bz^{\\pca}\\) differ scale.inverse PCA transformation \n\\[\n\\bx = \\bU \\bt^{\\pca}\n\\]\nterms standardised PCA components standardised original components becomes\n\\[\n\\bV^{-1/2} \\bx = \\bPsi^T  \\bLambda^{-1/2} \\bt^{\\pca}\n\\]\nThus cross-correlation matrix \\(\\bPsi\\) plays role correlation loadings\nalso classical PCA, .e. \ncoefficients linking standardised PCA components standardised original components.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"pca-correlation-loadings-plot","chapter":"2 Transformations and dimension reduction","heading":"2.6.2 PCA correlation loadings plot","text":"PCA PCA-cor whitening well classical PCA aim compression, .e.\nfind latent variables total variation contributed \nsmall number components.order able better interpret top ranking PCA component can use visual device called correlation loadings plot. compute correlation PCA components 1 2 (\\(t_1^{\\pca}\\) \\(t_2^{\\pca})\\) original variables \\(x_1, \\ldots, x_d\\).original variable \\(x_j\\) therefore two numbers betweem -1 1, correlationa\n\\(\\cor(t_1^{\\pca}, x_j)\\) \\(cor(t_2^{\\pca}, x_j)\\) use coordinates draw point plane. construction, points\nlie within unit circle around origin. sum squared correlation loadings latents component one specific original variable sum one, sum squared loadings just first two components also 1.\norginal variables strongly influenced\ntwo latent variables strong correlation thus lie near outer circle, whereas variables influenced two latent variables lie near origin.example, correlation loadings plot showing cross-correlation first two\nPCA components four variables iris flower data set discussed earlier.interpretation plot discussed Worksheet 4.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"cca-whitening-canonical-correlation-analysis","chapter":"2 Transformations and dimension reduction","heading":"2.7 CCA whitening (Canonical Correlation Analysis)","text":"Canonical correlation analysis invented Harald Hotelling 1936.3So far, looked whitening single vector \\(\\bx\\). CCA whitening consider two vectors \\(\\bx\\) \\(\\\\) simultaneously:\\[\\begin{align*}\n\\begin{array}{ll}\n\\bx = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_p \\end{pmatrix} \\\\\n\\text{Dimension } p\n\\end{array}\n\\begin{array}{ll}\n\\= \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_q \\end{pmatrix} \\\\\n\\text{Dimension } q\n\\end{array}\n\\begin{array}{ll}\n\\var(\\bx) = \\bSigma_{\\bx} = \\bV_{\\bx}^{1/2}\\bRho_{\\bx}\\bV_{\\bx}^{1/2} \\\\\n\\var(\\) = \\bSigma_{\\} = \\bV_{\\}^{1/2}\\bRho_{\\}\\bV_{\\}^{1/2} \\\\\n\\end{array}\n\\end{align*}\\]\\[\\begin{align*}\n\\begin{array}{cc}\n\\text{Whitening } \\bx \\text{:} \\\\\n\\text{Whitening } \\\\text{:}\n\\end{array}\n\\begin{array}{cc}\n\\bz_{\\bx} = \\bW_{\\bx}\\bx=\\bQ_{\\bx}\\bRho_{\\bx}^{-1/2}\\bV_{\\bx}^{-1/2}\\bx \\\\\n\\bz_{\\} = \\bW_{\\}\\=\\bQ_{\\}\\bRho_{\\}^{-1/2}\\bV_{\\}^{-1/2}\\\n\\end{array}\n\\end{align*}\\]\n(note use correlation-based form \\(\\bW\\))Cross-correlation \\(\\bz_{\\}\\) \\(\\bz_{\\}\\):\\[\\cor(\\bz_{\\bx},\\bz_{\\})=\\bQ_{\\bx}\\bK\\bQ_{\\}^T\\]\\(\\bK = \\bRho_{\\bx}^{-1/2}\\bRho_{\\bx\\}\\bRho_{\\}^{-1/2}\\).Idea: can choose suitable orthogonal matrices \\(\\bQ_{\\bx}\\) \\(\\bQ_{\\}\\) putting constraints cross-correlation.CCA: aim diagonal \\(\\cor(\\bz_{\\bx},\\bz_{\\})\\) component \\(\\bz_{\\bx}\\) influences one (corresponding) component \\(\\bz_{\\}\\).Motivation: pairs “modules” represented components \\(\\bz_{\\bx}\\)\n\\(\\bz_{\\}\\) influencing (anyone module).\\[\n\\begin{array}{ll}\n\\bz_{\\bx} = \\begin{pmatrix} z^x_1 \\\\ z^x_2 \\\\ \\vdots \\\\ z^x_p \\end{pmatrix} &\n\\bz_{\\} = \\begin{pmatrix} z^y_1 \\\\ z^y_2 \\\\ \\vdots \\\\ z^y_q \\end{pmatrix} \\\\\n\\end{array}\n\\]\\end{align}\\[\\cor(\\bz_{\\bx},\\bz_{\\}) = \\begin{pmatrix} d_1 & \\dots & 0 \\\\ \\vdots &  \\vdots \\\\ 0 & \\dots & d_m \\end{pmatrix}\\]\\(d_i\\) canonical correlations \\(m=\\min(p,q)\\).","code":""},{"path":"transformations-and-dimension-reduction.html","id":"how-to-make-cross-correlation-matrix-corbz_bxbz_by-diagonal","chapter":"2 Transformations and dimension reduction","heading":"2.7.1 How to make cross-correlation matrix \\(\\cor(\\bz_{\\bx},\\bz_{\\by})\\) diagonal?","text":"Use Singular Value Decomposition (SVD) matrix \\(\\bK\\):\\[\\bK = (\\bQ_{\\bx}^{\\cca})^T  \\bD \\bQ_{\\}^{\\cca}\\]\n\\(\\bD\\) diagonal matrix containing singular values \\(\\bK\\)yields orthogonal matrices \\(\\bQ_{\\bx}^{\\cca}\\) \\(\\bQ_{\\}^{\\cca}\\) thus desired whitened matrices \\(\\bW_{\\bx}^{\\cca}\\) \\(\\bW_{\\}^{\\cca}\\)result \\(\\cor(\\bz_{\\bx},\\bz_{\\}) = \\bD\\) .e. singular values \\(\\bK\\) identical canonical correlations \\(d_i\\)!\\(\\longrightarrow\\) \\(\\bQ_{\\bx}^{\\cca}\\) \\(\\bQ_{\\}^{\\cca}\\) determined diagonality constraint (different previously discussed whitening methods).Note signs corresponding columns \\(\\bQ_{\\bx}^{\\cca}\\) \\(\\bQ_{\\}^{\\cca}\\) identified. Traditionally, SVD \nsigns chosen singular values positive. However, \nimpose positive-diagonality \\(\\bQ_{\\bx}^{\\cca}\\) \\(\\bQ_{\\}^{\\cca}\\),\nthus positive-diagonality cross-correlations \\(\\bPsi_{\\bx}\\) \n\\(\\bPsi_{\\}\\), canonical correlations may take positive \nnegative values.","code":""},{"path":"transformations-and-dimension-reduction.html","id":"related-methods","chapter":"2 Transformations and dimension reduction","heading":"2.7.2 Related methods","text":"O2PLS: similar CCA using orthogonal projections\n(thus O2PLS latent variables underlying \\(\\bx\\) \\(\\\\) orthogonal)O2PLS: similar CCA using orthogonal projections\n(thus O2PLS latent variables underlying \\(\\bx\\) \\(\\\\) orthogonal)Vector correlation: aggregates squared canonical correlations single overall measure\nassociation two random vectors \\(\\bx\\) \\(\\\\) (see Chapter 5\nmultivariate dependencies).Vector correlation: aggregates squared canonical correlations single overall measure\nassociation two random vectors \\(\\bx\\) \\(\\\\) (see Chapter 5\nmultivariate dependencies).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"unsupervised-learning-and-clustering","chapter":"3 Unsupervised learning and clustering","heading":"3 Unsupervised learning and clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"challenges-in-supervised-learning","chapter":"3 Unsupervised learning and clustering","heading":"3.1 Challenges in supervised learning","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"objective","chapter":"3 Unsupervised learning and clustering","heading":"3.1.1 Objective","text":"observe data \\(\\bx_1, \\ldots, \\bx_n\\) \\(n\\) objects (subjects).\nsample \\(\\bx_i\\) vector dimension \\(d\\). Thus, \\(n\\) objects / subjects measurements \\(d\\) variables.\naim unsupervised learning identify patters relating objects/subjects based information available \\(\\bx_i\\). Note unsupervised learning uses information nothing else.illustration consider first two principal components Iris flower data (see e.g. Worksheet 4):Clearly group structure among samples linked particular\npatterns first two principal components.Note plot used additional information, class labels (setosa, versicolor, virginica), highlighting true underlying structure (three flower species).unsupervised learning class labels (assumed ) unknown, aim infer clustering thus classes labels.4There many methods clustering unsupervise learning, purely algorithmic well probabilistic. chapter study commonly used approaches.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"questions-and-problems","chapter":"3 Unsupervised learning and clustering","heading":"3.1.2 Questions and problems","text":"order implement unsupervised learning need address number questions:define clusters?learn / infer clusters?many clusters ? (surprisingly difficult!)can assess uncertainty clusters?know clusters also interested :features define / separate cluster?(note feature / variable selection problem, discussed supervised learning).Many problems questions highly specific data hand.\nCorrespondingly, many different types methods models clustering unsupervised learning.terms representing data, unsupervised learning tries balance following two extremes:objects grouped single cluster (low complexity model)objects put cluster (high complexity model)practise, aim find compromise, .e. model captures \nstructure data appropriate complexity — low complex.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"why-is-clustering-difficult","chapter":"3 Unsupervised learning and clustering","heading":"3.1.3 Why is clustering difficult?","text":"Partioning problem (combinatorics): many partitions \\(n\\) objects (say flowers) \\(K\\) groups (say species) exists?Answer:\\[\nS(n,K) = \\left\\{\\begin{array}{l} n \\\\ K \\end{array} \\right\\}\n\\]\n“Sterling number second type”.large n:\n\\[\nS(n,K) \\approx \\frac{K^n }{ K!}\n\\]\nExample:enormously big numbers even relatively small problems!\\(\\Longrightarrow\\) Clustering / partitioning / structure discovery easy!\\(\\Longrightarrow\\) expect perfect answers single “true” clusteringIn fact, model data many differnt clusterings may fit data equally well.\\(\\Longrightarrow\\) need assesse uncertainty clusteringThis can done part probabilistic modelling resampling (e.g., bootstrap).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"common-types-of-clustering-methods","chapter":"3 Unsupervised learning and clustering","heading":"3.1.4 Common types of clustering methods","text":"many different clustering algorithms!consider following two broad types methods:Algorithmic clustering methods (explicitly based probabilistic model)\\(K\\)-meansPAMhierarchical clustering (distance similarity-based, divise agglomerative)pros: fast, effective algorithms find least grouping\ncons: probabilistic interpretation, blackbox methodsModel-based clustering (based probabilistic model)mixture models (e.g. Gaussian mixture models, GMMs, non-hierarchical)graphical models (e.g. Bayesian networks, Gaussian graphical models GGM, trees networks)pros: full probabilistic model corresponding advantages\ncons: computationally expensive, sometimes impossible compute exactly.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"hierarchical-clustering","chapter":"3 Unsupervised learning and clustering","heading":"3.2 Hierarchical clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"tree-like-structures","chapter":"3 Unsupervised learning and clustering","heading":"3.2.1 Tree-like structures","text":"Often, categorisations objects nested, .e. sub-categories categories etc. can naturally represented tree-like hierarchical structures.many branches science hierarchical clusterings widely employed, example evolutionary biology: see e.g. Tree Life explaining biodiversity lifephylogenetic trees among species (e.g. vertebrata)population genetic trees describe human evolutiontaxonomic trees plant speciesetc.Note visualising hierarchical structures typically corresponding tree depicted facing downwards, .e. root tree shown top, tips/leaves tree shown bottom!order obtain hierarchical clustering data two opposing strategies commonly used:divisive recursive partitioning algorithms\ngrow tree root downwards\nfirst determine main two clusters, recursively refine clusters .\ngrow tree root downwardsfirst determine main two clusters, recursively refine clusters .agglomerative algorithms\ngrow tree leafs upwards\nsuccessively form partitions first joining individual object together,\nrecursively join groups items together, merged.\ngrow tree leafs upwardssuccessively form partitions first joining individual object together,\nrecursively join groups items together, merged.following discuss number popular hierarchical agglomerative clustering algorithms based pairwise distances / similarities (\\(n \\times n\\) matrix) among data points.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"agglomerative-hierarchical-clustering-algorithms","chapter":"3 Unsupervised learning and clustering","heading":"3.2.2 Agglomerative hierarchical clustering algorithms","text":"general algorithm agglomerative construction hierarchical clustering works follows:Initialisation:Compute dissimilarity / distance matrix pairs objects “objects” single data points stage later also sets data points.Iterative procedure:identify pair objects smallest distance. two objects merged together common set. Create internal node tree describe coalescent event.identify pair objects smallest distance. two objects merged together common set. Create internal node tree describe coalescent event.update distance matrix computing distances new set \nobjects. new set contains data points procedure terminates.update distance matrix computing distances new set \nobjects. new set contains data points procedure terminates.actual implementation algorithm two key ingredients needed:distance measure \\(d(\\ba, \\bb)\\) two individual elementary data points \\(\\ba\\) \\(\\bb\\).typically one following:Euclidean distance \\(d(\\ba, \\bb) = \\sqrt{\\sum_{=1}^d ( a_i-b_i )^2} = \\sqrt{(\\ba-\\bb)^T (\\ba-\\bb)}\\)Manhattan distance \\(d(\\ba, \\bb) = \\sum_{=1}^d | a_i-b_i |\\)Maximum norm \\(d(\\ba, \\bb) = \\underset{\\\\{1, \\ldots, d\\}}{\\max} | a_i-b_i |\\)end, making correct choice distance require subject knowledge data!distance measure \\(d(, B)\\) two sets objects \\(=\\{\\ba_1, \\ba_2, \\ldots, \\ba_{n_A} \\}\\) \\(B=\\{\\bb_1, \\bb_2, \\ldots, \\bb_{n_B}\\}\\) size \\(n_A\\) \\(n_B\\), respectively.determine distance \\(d(, B)\\) two sets following measures often employed:complete linkage (max. distance): \\(d(, B) = \\underset{\\ba_i \\, \\bb_i \\B}{\\max} d(\\ba_i, \\bb_i)\\)single linkage (min. distance): \\(d(, B) = \\underset{\\ba_i \\, \\bb_i \\B}{\\min} d(\\ba_i, \\bb_i)\\)average linkage (avg. distance): \\(d(, B) = \\frac{1}{n_A n_B} \\sum_{\\ba_i \\} \\sum_{\\bb_i \\B} d(\\ba_i, \\bb_i)\\)","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"wards-clustering-method","chapter":"3 Unsupervised learning and clustering","heading":"3.2.3 Ward’s clustering method","text":"Another agglomerative hierarchical procedure Ward’s minimum variance approach. approach iteration two sets \\(\\) \\(B\\) merged lead smallest increase within-group variation, equivalenty, 5h3 total within-group sum squares (cf. \\(K\\)-means). centroids two sets given \\(\\bmu_A = \\frac{1}{n_A} \\sum_{\\ba_i \\} \\ba_i\\) \\(\\bmu_B = \\frac{1}{n_B} \\sum_{\\bb_i \\B} \\bb_i\\).within-group sum squares group \\(\\) \n\\[\nw_A = \\sum_{\\ba_i \\} (\\ba_i -\\bmu_A)^T (\\ba_i -\\bmu_A)\n\\]\ncomputed basis difference observations \\(\\ba_i\\) relative mean \\(\\bmu_A\\).\nHowever, also possible compute pairwise differences\nobservations using\n\\[\nw_A = \\frac{1}{n_A} \\sum_{\\ba_i, \\ba_j \\, < j} (\\ba_i -\\ba_j)^T (\\ba_i -\\ba_j)\n\\]\ntrick used Ward’s clustering method constructing distance measure sets \\(\\) \\(B\\) \n\\[\nd(, B) = w_{\\cup B} - w_A -w_B \\, .\n\\]\nCorrespondingly, distance two elementary data points \\(\\ba\\) \\(\\bb\\) squared Euclidean distance\n\\[\nd(\\ba, \\bb) = (\\ba- \\bb)^T (\\ba- \\bb) \\, .\n\\]","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-to-swiss-banknote-data-set","chapter":"3 Unsupervised learning and clustering","heading":"3.2.4 Application to Swiss banknote data set","text":"data set reports 6 pysical measurements 200 Swiss bank notes. 200 notes\n100 genuine 100 counterfeit. measurements : length, left width, right width, bottom margin, top margin, diagonal length bank notes.Plotting first PCAs data shows indeed two well defined groups,\ngroups correspond precisely genuine counterfeit banknotes:now compare hierarchical clusterings Swiss bank note data using four different methods using Euclidean distance.interactive R Shiny web app analysis (also allows explore distance measures) available\nonline https://minerva..manchester.ac.uk/shiny/strimmer/hclust/ .Ward.D2 (=Ward’s method):Average linkage:Complete linkage:Single linkage:Result:four trees / hierarchical clusterings quite different!Ward.D2 method one finds correct grouping (except single error).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"assessment-of-the-uncertainty-of-hierarchical-clusterings","chapter":"3 Unsupervised learning and clustering","heading":"3.2.5 Assessment of the uncertainty of hierarchical clusterings","text":"practical application hierarchical clustering methods essential evaluate stability uncertainty obtained groupings. often done follows using “bootstrap”:Sampling replacement used generate number -called bootstrap data sets (say \\(B=200\\)) similar original one. Specifically, create new data matrices repeately randomly selecting columns (variables) original data matrix inclusion bootstrap data matrix. Note sample columns aim cluster samples.Subsequently, hierarchical clustering computed bootstrap data sets. result, now “ensemble” \\(B\\) bootstrap trees.Finally, analysis clusters (bipartions) shown bootstrap trees allows count clusters appear frequently, also appear less frequently. counts provide measure stability clusterings appearing original tree.Additionally, bootstrap tree can also compute consensus tree containing stable clusters. viewed “ensemble average” bootstrap trees.disadvantage procedure bootstrapping trees computationally expensive, original procedure already time consuming now needs repeated large number times.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"k-means-clustering","chapter":"3 Unsupervised learning and clustering","heading":"3.3 \\(K\\)-means clustering","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"general-aims","chapter":"3 Unsupervised learning and clustering","heading":"3.3.1 General aims","text":"Partition data \\(K\\) groups, \\(K\\) given advanceThe groups non-overlapping, \\(n\\) data points / objects \\(\\bx_i\\) assigned exactly one \\(K\\) groupsmaximise homogeneity group (.e. group contain similar objects)maximise heterogeneity among different groups (.e group differ groups)","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"algorithm","chapter":"3 Unsupervised learning and clustering","heading":"3.3.2 Algorithm","text":"group \\(k \\\\{1, \\ldots, K\\}\\) assume group mean \\(\\bmu_k\\).\nrunning \\(K\\)-means get estimates \\(\\hat{\\bmu}_k\\) group means,\nwell allocation data point one classes.Initialisation:start algorithm \\(n\\) observations \\(\\bx_1, \\ldots, \\bx_n\\) randomly allocated equal probability one \\(K\\) groups. resulting assignment given function \\(C(\\bx_i) \\\\{1, \\ldots, K\\}\\). \\(G_k = \\{ | C(\\bx_i) = k\\}\\) denote set indices data points cluster \\(k\\), \\(n_k = | G_k |\\) \nnumber samples cluster \\(k\\).Iterative refinement:estimate group means \n\\[\n\\hat{\\bmu}_k = \\frac{1}{n_k} \\sum_{\\G_k} \\bx_i\n\\]update group assignment: data point \\(\\bx_i\\) (re)assigned group \\(k\\) nearest \\(\\hat{\\bmu}_k\\) (terms Euclidean norm).\nSpecifically, assignment \\(C(\\bx_i)\\) updated \n\\[\n\\begin{split}\nC(\\bx_i) & = \\underset{k}{\\arg \\min} \\, | \\bx_i-\\hat{\\bmu}_k |_2 \\\\\n      & = \\underset{k}{\\arg \\min} \\, (\\bx_i-\\hat{\\bmu}_k)^T (\\bx_i-\\hat{\\bmu}_k) \\\\\n\\end{split}\n\\]\nSteps 1 2 repeated algorithm converges (upper limit repeats reached).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"properties","chapter":"3 Unsupervised learning and clustering","heading":"3.3.3 Properties","text":"Despite simplicity \\(K\\)-means surprisingly effective clustering algorithms.final clustering depends initialisation often useful run \\(K\\)-means several\ntimes different starting allocations data points. Furthermore, non-random non-uniform\ninitialisations can lead improved faster convergence, see e.g. \nK-means++ algorithm.result way clusters assigned \\(K\\)-means corresponding cluster boundaries form \nVoronoi tesselation (cf. https://en.wikipedia.org/wiki/Voronoi_diagram ) around cluster means.Later also discuss connection \\(K\\)-means probabilistic clustering using Gaussian mixture models.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"choosing-the-number-of-clusters","chapter":"3 Unsupervised learning and clustering","heading":"3.3.4 Choosing the number of clusters","text":"\\(K\\)-means clustering obtained insightful compute:total within-group sum squares \\(SSW\\) (tot.withinss), total unexplained sum squares:\n\\[\nSSW = \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\bx_i -\\hat{\\bmu}_k)^T (\\bx_i -\\hat{\\bmu}_k)\n\\]\nquantity decreases \\(K\\) zero \\(K=n\\).\n\\(K\\)-means algorithm tries minimise quantity typically find local minimum rather global one.total within-group sum squares \\(SSW\\) (tot.withinss), total unexplained sum squares:\n\\[\nSSW = \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\bx_i -\\hat{\\bmu}_k)^T (\\bx_i -\\hat{\\bmu}_k)\n\\]\nquantity decreases \\(K\\) zero \\(K=n\\).\n\\(K\\)-means algorithm tries minimise quantity typically find local minimum rather global one.-group sum squares \\(SSB\\) (betweenss), explained sum squares:\n\\[\nSSB = \\sum_{k=1}^K n_k (\\hat{\\bmu}_k - \\hat{\\bmu}_0)^T (\\hat{\\bmu}_k - \\hat{\\bmu}_0)\n\\]\n\\(\\hat{\\bmu}_0 = \\frac{1}{n} \\sum_{=1}^n \\bx_i = \\frac{1}{n} \\sum_{k=1}^K n_k \\hat{\\bmu}_k\\)\nglobal mean samples. \\(SSB\\) increases number clusters \\(K\\) \\(K=n\\) \nbecomes equal tothe -group sum squares \\(SSB\\) (betweenss), explained sum squares:\n\\[\nSSB = \\sum_{k=1}^K n_k (\\hat{\\bmu}_k - \\hat{\\bmu}_0)^T (\\hat{\\bmu}_k - \\hat{\\bmu}_0)\n\\]\n\\(\\hat{\\bmu}_0 = \\frac{1}{n} \\sum_{=1}^n \\bx_i = \\frac{1}{n} \\sum_{k=1}^K n_k \\hat{\\bmu}_k\\)\nglobal mean samples. \\(SSB\\) increases number clusters \\(K\\) \\(K=n\\) \nbecomes equal tothe total sum squares\n\\[\nSST = \\sum_{=1}^n (\\bx_i - \\hat{\\bmu}_0)^T (\\bx_i - \\hat{\\bmu}_0) \\, .\n\\]\nconstruction \\(SST = SSB + SSW\\) \\(K\\) (.e. \\(SST\\) constant independent \\(K\\)).total sum squares\n\\[\nSST = \\sum_{=1}^n (\\bx_i - \\hat{\\bmu}_0)^T (\\bx_i - \\hat{\\bmu}_0) \\, .\n\\]\nconstruction \\(SST = SSB + SSW\\) \\(K\\) (.e. \\(SST\\) constant independent \\(K\\)).divide sum squares sample size \\(n\\) get \\(T = \\frac{SST}{n}\\) total variation,\n\\(B = \\frac{SSW}{n}\\) explained variation \\(W = \\frac{SSW}{n}\\) total unexplained variation , \n\\(T = B + W\\).deciding optimal number clusters can run \\(K\\)-means various settings \\(K\\) choose smallest \\(K\\) explained variation \\(B\\) significantly worse compared model substantially larger number clusters (see example ).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"k-medoids-aka-pam","chapter":"3 Unsupervised learning and clustering","heading":"3.3.5 \\(K\\)-medoids aka PAM","text":"closely related clustering method \\(K\\)-medoids PAM (“Partitioning Around Medoids”).works exactly like \\(K\\)-means, thatinstead estimated group means \\(\\hat{\\bmu}_k\\) one member group selected representative (socalled “medoid”)instead squared Euclidean distance dissimilarity measures also allowed.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-of-k-means-to-iris-data","chapter":"3 Unsupervised learning and clustering","heading":"3.3.6 Application of \\(K\\)-means to Iris data","text":"Scatter plots Iris data:R output \\(K\\)-means analysis true number clusters specified (\\(K=3\\)) :corresponding total within-group sum squares (\\(SSW\\), tot.withinss)\nisand -group sum squares (\\(SSB\\), betweenss) isBy comparing known class assignments can assess accuracy \\(K\\)-means clustering:choosing \\(K\\) run \\(K\\)-means several times compute\nwithin cluster variation dependence \\(K\\):Thus, \\(K=3\\) clusters seem appropriate since explained variation significantly improve\n(unexplained variation significantly decrease) increase number clusters.","code":"## K-means clustering with 3 clusters of sizes 50, 47, 53\n## \n## Cluster means:\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1  -1.01119138  0.85041372   -1.3006301  -1.2507035\n## 2   1.13217737  0.08812645    0.9928284   1.0141287\n## 3  -0.05005221 -0.88042696    0.3465767   0.2805873\n## \n## Clustering vector:\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 3 3 2 3 3 3 3 3 3 3 3 2 3 3 3 3 2 3 3 3\n##  [75] 3 2 2 2 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 2 2 2 2 3 2 2 2 2\n## [112] 2 2 3 3 2 2 2 2 3 2 3 2 3 2 2 3 2 2 2 2 2 2 3 3 2 2 2 3 2 2 2 3 2 2 2 3 2\n## [149] 2 3\n## \n## Within cluster sum of squares by cluster:\n## [1] 47.35062 47.45019 44.08754\n##  (between_SS / total_SS =  76.7 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nkmeans.out$tot.withinss## [1] 138.8884\nkmeans.out$betweenss## [1] 457.1116\ntable(L.iris, kmeans.out$cluster)##             \n## L.iris        1  2  3\n##   setosa     50  0  0\n##   versicolor  0 11 39\n##   virginica   0 36 14"},{"path":"unsupervised-learning-and-clustering.html","id":"arbitrariness-of-cluster-labels-and-label-switching","chapter":"3 Unsupervised learning and clustering","heading":"3.3.7 Arbitrariness of cluster labels and label switching","text":"important realise unsupervised learning clustering labels group assigned arbitrary fashion.\nSpecifically, \\(K\\) cluster \\(K!\\) possibilities attach \nlabels, corresponding number permutations \\(K\\) groups.Thus, rerun clustering algorithm \\(K\\)-means may return clustering (groupings samples) different labels. phenomenon called “label switching”.Therefore comparing clusterings obtained algorithm just rely group label, need compare actual members clusters. Likewise, interested properties particular group rely label identify group.order resolve problem label switching one may wish relabel clusters using additional information, requiring samples specfic groups\n(e.g.: sample 1 always group labeled “1”), /linking labels orderings constraints group characteristics (e.g.: group label “1” always smaller mean group label “2”).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"mixture-models","chapter":"3 Unsupervised learning and clustering","heading":"3.4 Mixture models","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"finite-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.4.1 Finite mixture model","text":"\\(K\\) groups / classes / categories, number \\(K\\) specified finiteeach class \\(k \\\\{1, \\ldots, K\\}\\) modeled distribution \\(F_k\\) parameters \\(\\btheta_k\\).density class: \\(f_k(\\bx ) = f(\\bx | k)\\) \\(k \\1, \\ldots, K\\)mixing weight class: \\(\\prob(k) = \\pi_k\\) \\(\\sum_{k=1}^K \\pi_k = 1\\)joint density \\(f(\\bx, k) = f(\\bx | k) \\prob(k) = f_k(\\bx) \\pi_k\\)results mixture density\n\\[\nf_{\\mix}(\\bx) = \\sum_{k=1}^K \\pi_k f_k(\\bx)\n\\]\nalso called marginal density arised joint density \\(f(\\bx, k)\\) marginalising \\(k\\).often one uses multivariate normal components \\(f_k(\\bx) = N(\\bx | \\bmu_k, \\bSigma_k)\\) \\(\\\\ \\Longrightarrow\\) Gaussian mixture model (GMM)Mixture models fundamental just clustering many applications (e.g. classification).Note: don’t confuse mixture model mixed model (=random effects regression model)","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"total-variance-and-variation-of-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.4.2 Total variance and variation of mixture model","text":"conditional means variances class \\(\\expect(\\bx| k) = \\bmu_k\\) \\(\\var(\\bx| k) = \\bSigma_k\\), probability\nclass \\(k\\) given \\(\\prob(k)=\\pi_k\\). Using law total expectation can therefore obtain mean mixture density follows:\n\\[\n\\begin{split}\n\\expect(\\bx) & = \\expect(\\expect(\\bx | k)) \\\\\n            & = \\sum_{k=1}^K \\pi_k \\bmu_k \\\\\n            &= \\bmu_0 \\\\\n\\end{split}\n\\]\nSimilarly, using law total variance compute marginal variance:\n\\[\n\\begin{split}\n\\underbrace{\\var(\\bx)}_{\\text{total}} & =  \\underbrace{ \\var( \\expect(\\bx | k )  )}_{\\text{explained / -group}} + \\underbrace{\\expect(\\var(\\bx |k))}_{\\text{unexplained / within-group}} \\\\\n\\bSigma_0 & =  \\sum_{k=1}^K \\pi_k (\\bmu_k - \\bmu_0) (\\bmu_k - \\bmu_0)^T + \\sum_{k=1}^K \\pi_k \\bSigma_k  \\\\\n\\end{split}\n\\]\nThus, just like linear regression (see MATH20802 Statistical Methods) can decompose total variance explained\n(group) part unexplained (within group) part.total variation given trace covariance matrix, decomposition turns \n\\[\n\\begin{split}\n\\trace(\\bSigma_0) & =  \\sum_{k=1}^K \\pi_k \\trace((\\bmu_k - \\bmu_0) (\\bmu_k - \\bmu_0)^T) + \\sum_{k=1}^K \\pi_k \\trace(\\bSigma_k)  \\\\\n& =  \\sum_{k=1}^K \\pi_k (\\bmu_k - \\bmu_0)^T (\\bmu_k - \\bmu_0) + \\sum_{k=1}^K \\pi_k \\trace(\\bSigma_k)\\\\\n\\end{split}\n\\]\ncovariances replaced empirical estimates obtain\n\\(T=B+W\\) decomposition total variation familiar \\(K\\)-means:\n\\[T = \\trace \\left( \\hat{\\bSigma}_0 \\right)  = \n\\frac{1}{n} \\sum_{=1}^n (\\bx_i - \\hat{\\bmu}_0)^T (\\bx_i - \\hat{\\bmu}_0)\\]\n\\[B = \\frac{1}{n} \\sum_{k=1}^K n_k (\\hat{\\bmu}_k - \\hat{\\bmu}_0)^T (\\hat{\\bmu}_k - \\hat{\\bmu}_0)\\]\n\\[W = \\frac{1}{n}  \\sum_{k=1}^K \\, \\sum_{\\G_k} (\\bx_i -\\hat{\\bmu}_k)^T (\\bx_i -\\hat{\\bmu}_k)\n\\]univariate mixture (\\(d=1\\)) \\(K=2\\) components get\n\\[\n\\mu_0 = \\pi_1 \\mu_1+ \\pi_2 \\mu_2 \\, ,\n\\]\n\\[\n\\sigma^2_{\\text{within}} = \\pi_1 \\sigma^2_1 + \\pi_2 \\sigma^2_2 = \\sigma^2_{\\text{pooled}}\\,,\n\\]\nalso know pooled variance, \n\\[\n\\begin{split}\n\\sigma^2_{\\text{}} &= \\pi_1 (\\mu_1 - \\mu_0)^2 + \\pi_2 (\\mu_2 - \\mu_0)^2 \\\\\n& =\\pi_1 \\pi_2^2 (\\mu_1 - \\mu_2)^2 + \\pi_2 \\pi_1^2 (\\mu_1 - \\mu_2)^2\\\\\n& = \\pi_1 \\pi_2 (\\mu_1 - \\mu_2)^2  \\\\\n& = \\left( \\frac{1}{\\pi_1} + \\frac{1}{\\pi_2}   \\right)^{-1} (\\mu_1 - \\mu_2)^2 \\\\\n\\end{split} \\,.\n\\]\nratio -group variance within-group variance proportional\n(factor \\(n\\)) squared pooled-variance \\(t\\)-score:\n\\[\n\\frac{\\sigma^2_{\\text{}}}{\\sigma^2_{\\text{within}}} =\n  \\frac{ (\\mu_1 - \\mu_2)^2}{ \\left(\\frac{1}{\\pi_1} + \\frac{1}{\\pi_2}   \\right)  \\sigma^2_{\\text{pooled}} }= \\frac{t_{\\text{pooled}}^2}{n}\n\\]\nfamiliar ANOVA (e.g. linear models course) recognise ratio \\(F\\)-score.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"example-of-mixtures","chapter":"3 Unsupervised learning and clustering","heading":"3.4.3 Example of mixtures","text":"Mixtures can take many different shapes forms, instructive study examples.first plot show density mixture distribution consisting two normals \\(\\pi_1=0.7\\),\n\\(\\mu_1=-1\\), \\(\\mu_2=2\\) two variances equal 1 (\\(\\sigma^2_1 = 1\\) \\(\\sigma^2_2 = 1\\)).\ntwo components well-separated two clear modes. plot also shows density normal distribution total mean (\\(\\mu_0=-0.1\\)) variance (\\(\\sigma_0^2=2.89\\)) mixture distribution. Clearly total normal mixture density different.However, mixtures can also look different. example, mean second component adjusted \\(\\mu_2=0\\) single mode total normal density \\(\\mu_0=-0.7\\) \\(\\sigma_0^2=1.21\\) now almost inistinguishable form mixture density.\nThus, case hard (even impossible) identify two peaks data.interactive version two normal component mixture available online \nR Shiny web app https://minerva..manchester.ac.uk/shiny/strimmer/mixture/ .general, learning mixture models data can challenging due various issues.\nFirst, permutation symmetries due arbitrariness group labels group specific parameters identiable without additional restrictions.\nSecond, identifiability issues can arise \n— example — two neighboring components mixture model largely overlapping\nthus close discriminated two different modes.\nFurthermore, likelihood estimation challenging singularities likelihood function,\nexample due singular estimated covariance matrices. However, can easily fixed \nregularising /requiring sufficient sample size per group.Mixture models need univariate, fact mixtures consider course multivariate.\nillustration, plot mixture two bivariate normals,\n\\(\\pi_1=0.7\\), \\(\\bmu_1 = \\begin{pmatrix}-1 \\\\1 \\\\ \\end{pmatrix}\\),\n\\(\\bSigma_1 = \\begin{pmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\\\ \\end{pmatrix}\\),\n\\(\\bmu_2 = \\begin{pmatrix}2.5 \\\\0.5 \\\\ \\end{pmatrix}\\) \\(\\bSigma_2 = \\begin{pmatrix} 1 & -0.7 \\\\ -0.7 & 1 \\\\ \\end{pmatrix}\\):","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"generative-view-sampling-from-a-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.4.4 Generative view: sampling from a mixture model","text":"Assuming know sample component densities \\(f_k(\\bx)\\) mixture model straightforward set procedure sampling mixture \\(f_{\\mix}(\\bx) = \\sum_{k=1}^K \\pi_k f_k(\\bx)\\).done two-step generative process:draw categorical distribution parameters \\(\\bpi=(\\pi_1, \\ldots, \\pi_K)^T\\):\n\\[\\bz \\sim \\catdist(\\bpi)\\]\nvector \\(\\bz = (z_1, \\ldots, z_K)^T\\) indicating group allocation. group index \\(k\\) given \\(\\{k : z_k=1\\}\\).draw categorical distribution parameters \\(\\bpi=(\\pi_1, \\ldots, \\pi_K)^T\\):\n\\[\\bz \\sim \\catdist(\\bpi)\\]\nvector \\(\\bz = (z_1, \\ldots, z_K)^T\\) indicating group allocation. group index \\(k\\) given \\(\\{k : z_k=1\\}\\).Subsequently, sample component \\(k\\) selected step 1:\n\\[\n\\bx \\sim F_k\n\\]Subsequently, sample component \\(k\\) selected step 1:\n\\[\n\\bx \\sim F_k\n\\]two-stage approach also called latent allocation variable formulation mixture model, \\(\\bz\\) (equivalently \\(k\\)) latent variable.two-step process needs repeated sample drawn mixture (.e. every time new latent variable \\(\\bz\\) generated).probabilistic clustering aim infer state \\(\\bz\\) observed samples.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"predicting-the-group-allocation-of-a-given-sample","chapter":"3 Unsupervised learning and clustering","heading":"3.4.5 Predicting the group allocation of a given sample","text":"know mixture model components can predict probability observation \\(\\bx\\) falls group \\(k\\) via application Bayes theorem:\n\\[\nz_k = \\prob(k | \\bx) = \\frac{\\pi_k f_k(\\bx ) }{ f(\\bx)}\n\\]mentioned interactive Shiny app normal component mixture (available online https://minerva..manchester.ac.uk/shiny/strimmer/mixture/ ) can also explore probabilities \nclass.Thus, calculating class probabilities using fitted mixture model can perform probabilistic clustering assigning sample class largest probability.\nUnlike algorithmic clustering, also get assessment uncertainty class assignment, since sample \\(\\bx\\) obtain vector\n\\[\n\\bz = (z_1, \\ldots, z_K)^T\n\\]\nthus can see several classes similar assignment probability. case, e.g.,\n\\(\\bx\\) lies near boundary two classes. Note \\(\\sum_{k=1}^K z_k=1\\).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"variation-1-infinite-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.4.6 Variation 1: Infinite mixture model","text":"possible construct mixture models infinitely many components!commonly known example Dirichlet process mixture model (DPM):\\[\nf_{\\mix}(\\bx) = \\sum_{k=1}^\\infty \\pi_k f_k(\\bx)\n\\]\n\\(\\sum_{k=1}^\\infty\\pi_k =1\\) weight \\(\\pi_k\\) taken infinitely dimensional Dirichlet distribution (=Dirichlet process).DPMs useful clustering since necessary determine number clusters priori (since definition infinitely many!). Instead, number clusters -product fit model observed data.Related: “Chinese restaurant process” - https://en.wikipedia.org/wiki/Chinese_restaurant_processThis describes algorithm allocation process samples (“persons”) groups (“restaurant tables”) DPM.See also “stick-breaking process”: https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"variation-2-semiparametric-mixture-model-with-two-classes","chapter":"3 Unsupervised learning and clustering","heading":"3.4.7 Variation 2: Semiparametric mixture model with two classes","text":"common model following two-component univariate mixture model\\[f_{\\mix}(x) = \\pi_0 f_0(x) + (1-\\pi_0) f_A(\\bx)\\]\\(f_0\\): null model, typically parametric normal distribution\\(f_A\\): alternative model, typically nonparametric\\(\\pi_0\\): prior probability null modelUsing Bayes theorem allows compute probability observation \\(x\\) belongs null model:\n\\[\\prob(\\text{Null} | x ) = \\frac{\\pi_0 f_0(x ) }{ f(x) }\\]\ncalled local false discovery rate.semi-parametric mixture model foundation statistical testing based defining decision thresholds separate null model (“significant”) alternative model (“significant”):See MATH20802 Statistical Methods\ndetails.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"fitting-mixture-models-to-data","chapter":"3 Unsupervised learning and clustering","heading":"3.5 Fitting mixture models to data","text":"","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"direct-estimation-of-mixture-model-parameters","chapter":"3 Unsupervised learning and clustering","heading":"3.5.1 Direct estimation of mixture model parameters","text":"Given data matrix \\(\\bX= (\\bx_1, \\ldots, \\bx_n)^T\\) observations \\(n\\) samples like fit mixture model\n\\(f_{\\mix}(\\bx) = \\sum_{k=1}^K \\pi_k f_k(\\bx)\\) learn parameters \\(\\btheta\\), example maximising corresponding marginal log-likelihood function regard \\(\\btheta\\):\n\\[\n\\log L(\\btheta | \\bX) = \\sum_{=1}^n \\log \\left( \\sum_{k=1}^K \\pi_k f_k(\\bx_i)  \\right)\n\\]\nGaussian mixture model parameters \\(\\btheta = \\{\\bpi, \\bmu_1, \\ldots, \\bmu_K, \\bSigma_1, \\ldots, \\bSigma_K\\}\\).However, practise evaluation optimisation likelihood function may difficult due\nnumber reasons:form log-likelihood function prevents analytic simplifications\n(note sum inside logarithm).symmetries due exchangeability cluster labels likelihood function multimodal (note also linked general\nproblem label switching non-identifiability cluster labels mixtures).Furthermore, likelihood Gaussian mixture models can become singular one fitted covariance matrices becomes singular. Note can adressed using regularisation (Bayes, penalised ML, etc.).log-likelihood function also called observed data log-likelihood,\nincomplete data log-likelihood, contrast complete data log-likelihood described .","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"estimating-mixture-model-parameters-using-the-em-algorithm","chapter":"3 Unsupervised learning and clustering","heading":"3.5.2 Estimating mixture model parameters using the EM algorithm","text":"mixture model may viewed incomplete missing data problem:\nmissing data group allocations \\(\\bk = (k_1, \\ldots, k_n)^T\\) belonging sample \\(\\bx_1, \\ldots, \\bx_n\\).know sample comes group estimation \nparameters \\(\\btheta\\) indeed straightforward using -called complete data log-likelihood\nbased joint distribution \\(f(\\bx, k) = \\pi_k f_k(\\bx)\\)\n\\[\n\\log L(\\btheta | \\bX, \\bk) = \\sum_{=1}^n  \\log \\left(\\pi_{k_i} f_{k_i}(\\bx_i) \\right) \n\\]idea EM algorithm (Dempster et al. 1977) exploit simplicity complete data likelihood\nobtain estimates \\(\\btheta\\) first finding probability distribution \\(z_{ik}\\) latent variable \\(k_i\\), using distribution compute optimise corresponding expected complete-data log-likelihood. Specifically, \\(z_{ik}\\) contain probabilities class sample \\(\\) thus provide soft assignment classes rather 0/1 hard assignment (\\(K\\)-means algorithm generative\nlatent variable view mixture models).EM algorithm iterate theestimation probabilistic distribution \\(z_{ik}\\) group\nallocation latent parameters using current estimate parameters \\(\\btheta\\) (obtained step 2)maximisation expected complete data log-likelihood estimate parameters \\(\\btheta\\). expectation taken regard distribution \\(z_{ik}\\) (obtained step 1).Specifically, EM algorithm applied model-based clustering proceeds follows:Initialisation: Start guess parameters \\(\\btheta^{(1)}\\), continue “E” Step, Part .\nAlternatively, start guess \\(z_{ik}^{(1)}\\), continue\n“E” Step, Part B. initialisation may derived prior information, e.g., running \\(K\\)-means, simply random.Initialisation: Start guess parameters \\(\\btheta^{(1)}\\), continue “E” Step, Part .\nAlternatively, start guess \\(z_{ik}^{(1)}\\), continue\n“E” Step, Part B. initialisation may derived prior information, e.g., running \\(K\\)-means, simply random.E “expectation” step — Part : Use Bayes’ theorem compute new probabilities allocation samples \\(\\bx_i\\):\n\\[\nz_{ik}^{(b+1)} \\leftarrow \\frac{ \\pi_k f_k(\\bx_i) }{  f(\\bx_i)  }\n\\]\nNote obtain \\(z_{ik}^{(b+1)}\\) current value\n\\(\\btheta^{(b)}\\) parameters required.\n— Part B: Construct expected complete data log-likelihood function using weights \\(z_{ik}^{(b+1)}\\):\n\\[\nQ^{(b+1)}(\\btheta | \\bX ) = \\sum_{=1}^n \\sum_{k=1}^K z_{ik}^{(b+1)}  \\log \\left( \\pi_k f_k(\\bx_i) \\right)\n\\]E “expectation” step — Part : Use Bayes’ theorem compute new probabilities allocation samples \\(\\bx_i\\):\n\\[\nz_{ik}^{(b+1)} \\leftarrow \\frac{ \\pi_k f_k(\\bx_i) }{  f(\\bx_i)  }\n\\]\nNote obtain \\(z_{ik}^{(b+1)}\\) current value\n\\(\\btheta^{(b)}\\) parameters required.\n— Part B: Construct expected complete data log-likelihood function using weights \\(z_{ik}^{(b+1)}\\):\n\\[\nQ^{(b+1)}(\\btheta | \\bX ) = \\sum_{=1}^n \\sum_{k=1}^K z_{ik}^{(b+1)}  \\log \\left( \\pi_k f_k(\\bx_i) \\right)\n\\]M “maximisation” step — Maximise expected complete data log-likelihood update mixture model parameters \\(\\btheta\\):\n\\[\n\\btheta^{(b+1)} \\leftarrow \\arg \\max_{\\btheta}  Q^{(b+1)}(\\btheta | \\bX )\n\\]M “maximisation” step — Maximise expected complete data log-likelihood update mixture model parameters \\(\\btheta\\):\n\\[\n\\btheta^{(b+1)} \\leftarrow \\arg \\max_{\\btheta}  Q^{(b+1)}(\\btheta | \\bX )\n\\]Repeat “E” Step convergence parameters \\(\\btheta^{(b)}\\) mixture model.Repeat “E” Step convergence parameters \\(\\btheta^{(b)}\\) mixture model.can shown output \\(\\btheta^{(1)}, \\btheta^{(2)}, \\btheta^{(3)}, \\ldots\\) EM algorithm converges estimate \\(\\hat{\\btheta}\\) found maximising\nmarginal log-likelihood. Since maximisation expected complete data log-likelihood often much easier (analytically tractable) maximisation observed data log-likelihood function EM algorithm preferred approach\ncase.avoid singularities expected log-likelihood function \nmay wish adopt Bayesian approach (use regularised/penalised ML) estimating parameters M-step.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"em-algorithm-for-multivariate-normal-mixture-model","chapter":"3 Unsupervised learning and clustering","heading":"3.5.3 EM algorithm for multivariate normal mixture model","text":"GMM EM algorithm can written analytically:E-step:\\[\nz_{ik} = \\frac{ \\hat{\\pi}_k N(\\bx_i | \\hat{\\bmu}_k, \\hat{\\bSigma}_k) }{  f(\\bx_i)  }\n\\]M-step:\\[\n\\hat{n}_k = \\sum_{=1}^n z_{ik}\n\\]\n\\[\n\\hat{\\pi}_k = \\frac{\\hat{n}_k}{n}\n\\]\\[\n\\hat{\\bmu}_k = \\frac{1}{\\hat{n}_k} \\sum_{=1}^n z_{ik} \\bx_i\n\\]\n\\[\n\\hat{\\bSigma}_k =  \\frac{1}{\\hat{n}_k} \\sum_{=1}^n z_{ik} ( \\bx_i -\\bmu_k)   ( \\bx_i -\\bmu_k)^T\n\\]Note estimators \\(\\hat{\\bmu}_k\\) \\(\\hat{\\bSigma}_k\\) weighted versions \nusual empirical estimators (weights \\(z_{ik}\\) soft assignment classes resulting\nBayesian updating).Worksheet 7 can find simple R implementation EM algorithm \nunivariate normal mixtures.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"connection-with-k-means-clustering-method","chapter":"3 Unsupervised learning and clustering","heading":"3.5.4 Connection with \\(K\\)-means clustering method","text":"\\(K\\)-means algorithm closely related probabilistic clustering Gaussian mixture models.Specifically, class assigment \\(K\\)-means \n\\[C(\\bx_i) = \\underset{k}{\\arg \\min} \\, (\\bx_i-\\hat{\\bmu}_k)^T (\\bx_i-\\hat{\\bmu}_k)\\]Gaussian mixture model probabilities \\(\\pi_k\\) classes asssumed identical (.e. \\(\\pi_k=\\frac{1}{K}\\))\ncovariances \\(\\bSigma_k\\) spherical form \\(\\sigma^2 \\bI\\), .e. dependence groups, correlation identical variance variables,\nsoft assignment class allocation becomes\n\\[\\log( z_{ik} ) = -\\frac{1}{2 \\sigma^2} (\\bx_i-\\hat{\\bmu}_k)^T (\\bx_i-\\hat{\\bmu}_k) +  C \n\\]\n\\(C\\) constant depending \\(\\bx_i\\) \\(k\\). order select hard class allocation based \\(z_{ik}\\) may use rule\n\\[\n\\begin{split}\nC(\\bx_i) &= \\underset{k}{\\arg \\max} \\log( z_{ik} ) \\\\\n          & = \\underset{k}{\\arg \\min}  (\\bx_i-\\hat{\\bmu}_k)^T (\\bx_i-\\hat{\\bmu}_k)\\\\\n\\end{split}\n\\]\nThus, \\(K\\)-means can viewed algorithm provide hard classifications\nsimple restricted Gaussian mixture model.","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"choosing-the-number-of-classes","chapter":"3 Unsupervised learning and clustering","heading":"3.5.5 Choosing the number of classes","text":"Since GMMs operate likelihood framework can use penalised likelihood model selection criteria choose among different models (.e. GMMs different numbers classes).popular choices AIC (Akaike Information Criterion) BIC (Bayesian Information criterion) defined follows:\n\\[\\aic = -2 \\log L + 2 K \\]\n\\[\\bic = - 2 \\log L +K \\log(n)\\]Instead maximising log-likehood minimise \\(\\aic\\) \\(\\bic\\).Note criteria complex models parameters (case groups) penalised\nsimpler models order prevent overfitting.\\(\\Longrightarrow\\) find optimal number groups \\(K\\).Another way choosing optimal numbers clusters cross-validation (see later chapter supervised learning).","code":""},{"path":"unsupervised-learning-and-clustering.html","id":"application-of-gmms-to-iris-flower-data","chapter":"3 Unsupervised learning and clustering","heading":"3.5.6 Application of GMMs to Iris flower data","text":"now explore application Gaussian mixture models Iris flower data set also investigated PCA\nK-means.First, fit GMM 3 clusters, using R software “mclust.”5The “mclust” software used following model fitting mixture:“VVV” name used “mclust” software model\nallowing individual\nunrestricted covariance matrix \\(\\bSigma_k\\) class \\(k\\).GMM substantially lower misclassification error compared \\(K\\)-means number clusters:Note “mclust” BIC criterion defined opposite sign (\\(\\bic_{\\text{mclust}} = 2 \\log L -K \\log(n)\\)), thus need find maximum value rather smallest value.compute BIC various numbers groups find model best \\(\\bic_{\\text{mclust}}\\) model 2 clusters model 3 cluster nearly good BIC:","code":"\nlibrary(\"mclust\")\ngmm3 = Mclust(X.iris, G=3, verbose=FALSE)\nplot(gmm3, what=\"classification\")\ngmm3$modelName## [1] \"VVV\"\ntable(gmm3$classification, L.iris)##    L.iris\n##     setosa versicolor virginica\n##   1     50          0         0\n##   2      0         45         0\n##   3      0          5        50"},{"path":"supervised-learning-and-classification.html","id":"supervised-learning-and-classification","chapter":"4 Supervised learning and classification","heading":"4 Supervised learning and classification","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"introduction","chapter":"4 Supervised learning and classification","heading":"4.1 Introduction","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"supervised-learning-vs.-unsupervised-learning","chapter":"4 Supervised learning and classification","heading":"4.1.1 Supervised learning vs. unsupervised learning","text":"Unsupervised learning:Starting point:unlabeled data \\(\\bx_1, \\ldots, \\bx_n\\).Aim: find labels \\(y_1, \\ldots, y_n\\) attach sample \\(\\bx_i\\).discrete labels \\(y\\) unsupervised learning called clustering.Supervised learning:Starting point:labeled training data: \\(\\{\\bx_1^{\\train}, y_1^{\\train}\\}\\),\n\\(\\ldots\\), \\(\\{\\bx_n^{\\train}, y_n^{\\train} \\}\\)addition, unlabeled test data: \\(\\bx^{\\test}\\)Aim: use training data learn function, say \\(h(\\bx)\\),\npredict label corresponding test data.\npredictor function may provide soft (probabilistic) assignment\nhard assignment class label test sample.\\(y\\) discrete supervised learning called classification.\ncontinuous \\(y\\) label called response supervised learning becomes regression.Thus, supervised learning two-step procedure:Learn predictor function \\(h(\\bx)\\) using training data \\(\\bx_i^{\\train}\\) plus labels \\(y_i^{\\train}\\).Predict label \\(y^{\\test}\\) test data \\(\\bx^{\\test}\\) using estimated classifier function:\n\\(\\hat{y}^{\\test} = \\hat{h}(\\bx^{\\test})\\).","code":""},{"path":"supervised-learning-and-classification.html","id":"terminology","chapter":"4 Supervised learning and classification","heading":"4.1.2 Terminology","text":"function \\(h(\\bx)\\) predicts class \\(y\\) called classifier.many types classifiers, focus primarily probabilistic classifiers\n(.e. output probabilities possible class/label).challenge find classifier thatexplains current training data well andthat also generalises well future unseen data.Note relatively easy find predictor explains training data especially high dimensions (.e. many predictors) often overfitting predictor generalise well!decision boundary classes defined set \\(\\bx\\) \nclass assignment predictor \\(h(\\bx)\\) switches one class another.general, simple decision boundaries preferred complex decision boundaries avoid overfitting.commonly used probabilistic methods classifications:QDA (quadratic discriminant analysis)LDA (linear discriminant analysis)DDA (diagonal discriminant analysis),Naive Bayes classificationlogistic regressionCommon non-probabilistic methods include:SVM (support vector machine),random forestneural networksDepending classifiers trainined many variations\nmethods, e.g. Fisher discriminant analysis, regularised LDA, shrinkage disciminant analysis etc.","code":""},{"path":"supervised-learning-and-classification.html","id":"bayesian-discriminant-rule-or-bayes-classifier","chapter":"4 Supervised learning and classification","heading":"4.2 Bayesian discriminant rule or Bayes classifier","text":"setup mixture models:\\(K\\) groups \\(K\\) prespecifiedeach group distribution \\(F_k\\) parameters \\(\\btheta_k\\)density class \\(f_k(\\bx ) = f(\\bx | k)\\).prior probability group \\(k\\) \\(\\prob(k) = \\pi_k\\) \\(\\sum_{k=1}^K \\pi_k = 1\\)marginal density mixture \\(f(\\bx) = \\sum_{k=1}^K \\pi_k f_k(\\bx)\\)posterior probability group \\(k\\) \n\\[\n\\prob(k | \\bx) = \\frac{\\pi_k f_k(\\bx ) }{ f(\\bx)}\n\\]already provides “soft” classification\n\\[\\bh(\\bx^{\\test}) = (\\prob(k=1 | \\bx^{\\test}),\\ldots, \\prob(k=K | \\bx^{\\test})   )^T\\]\npossible class \\(k \\\\{ 1, \\ldots, K\\}\\) assigned probability label \ntest sample \\(\\bx\\).discriminant function logarithm posterior probability:\n\\[\nd_k(\\bx) = \\log \\prob(k | \\bx) = \\log \\pi_k  + \\log f_k(\\bx )  - \\log f(\\bx) \n\\]\nSince use \\(d_k\\) compare different classes \\(k\\) can\nsimplify discriminant function dropping constant terms depend \\(k\\) — term \\(\\log f(\\bx)\\). Hence get Bayes discriminant function\n\\[\nd_k(\\bx) = \\log \\pi_k + \\log f_k(\\bx ) \\,.\n\\]subsequent “hard” classification \\(h(\\bx^{\\test})\\) select group/label value discriminant function maximised:\n\\[\n\\hat{y}^{\\test} = h(\\bx^{\\test}) = \\arg \\max_k d_k(\\bx^{\\test}) \\,.\n\\]discriminant functions \\(d_k(\\bx)\\) can mapped back probabilistic class assignment using softargmax function (also known softmax function):\n\\[\n\\prob(k | \\bx) = \n\\frac{\\exp( d_k(\\bx) )}{\\sum_{c=1}^K \\exp( d_c(\\bx) ) } = \n\\frac{\\exp( d_k(\\bx) - d_{\\max} ) }{\\sum_{c=1}^K \\exp( d_c(\\bx) - d_{\\max} ) } \\,.\n\\]\nNote subtracting \\(d_{\\max} = \\max\\{ d_1(\\bx), \\ldots, d_K(\\bx) \\}\\) avoids numerical overflow problems computing exponential\nstandardising maximum discriminant functions zero.already encountered Bayes classifier EM algorithm predict state\nlatent variables (soft assignment) \\(K\\)-means algorithm (hard assignment), see previous Chapter also Worksheet 7.","code":""},{"path":"supervised-learning-and-classification.html","id":"normal-bayes-classifier","chapter":"4 Supervised learning and classification","heading":"4.3 Normal Bayes classifier","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"quadratic-discriminant-analysis-qda-and-gaussian-assumption","chapter":"4 Supervised learning and classification","heading":"4.3.1 Quadratic discriminant analysis (QDA) and Gaussian assumption","text":"Quadratic discriminant analysis (QDA) special case Bayes classifier densities multivariate normal \\(f_k(\\bx) = N(\\bx | \\bmu_k, \\bSigma_k)\\).leads discriminant function QDA:\n\\[\nd_k^{QDA}(\\bx) = -\\frac{1}{2} (\\bx-\\bmu_k)^T \\bSigma_k^{-1} (\\bx-\\bmu_k) -\\frac{1}{2} \\log \\det(\\bSigma_k) +\\log(\\pi_k)\n\\]number noteworthy things :terms dropped depend \\(k\\), \\(-\\frac{d}{2}\\log( 2\\pi)\\).Note appearance Mahalanobis distance \\(\\bx\\) \\(\\bmu_k\\)\nlast term — recall \\(d^{Mahalanobis}(\\bx, \\bmu | \\bSigma) = (\\bx-\\bmu)^T \\bSigma^{-1} (\\bx-\\bmu)\\).QDA discriminant function quadratic \\(\\bx\\) - hence name!\nimplies decision boundaries QDA classification quadratic (.e. parabolas two dimensional settings).Gaussian models specifically can useful multiply discriminant function -2 get rid factor \\(-\\frac{1}{2}\\), note case need find minimum discriminant function rather maximum:\n\\[\nd_k^{QDA (v2)}(\\bx) =  (\\bx-\\bmu_k)^T \\bSigma_k^{-1} (\\bx-\\bmu_k) + \\log \\det(\\bSigma_k)  -2 \\log(\\pi_k)\n\\]\nliterature find versions Gaussian discriminant functions need check carefully convention used.\nfollowing use first version .Decision boundaries QDA classifier can either linear nonlinear (quadratic curve).\ndecision boundary two classes \\(\\) \\(j\\)\nrequire \\(d^{QDA}_i(\\bx) = d^{QDA}_j(\\bx)\\), equivalently\n\\(d^{QDA}_i(\\bx) - d^{QDA}_j(\\bx) = 0\\), quadratic equation.","code":""},{"path":"supervised-learning-and-classification.html","id":"linear-discriminant-analysis-lda","chapter":"4 Supervised learning and classification","heading":"4.3.2 Linear discriminant analysis (LDA)","text":"LDA special case QDA, assumption common overall covariance across groups: \\(\\bSigma_k = \\bSigma\\).leads simplified discriminant function:\n\\[\nd_k^{LDA}(\\bx) = -\\frac{1}{2} (\\bx-\\bmu_k)^T \\bSigma^{-1} (\\bx-\\bmu_k) +\\log(\\pi_k)\n\\]\nNote term containing log-determinant now gone, LDA essentially now method tries minimize Mahalanobis distance\n(taking also account prior class probabilities).function can simplified, noting quadratic term \\(\\bx^T \\bSigma^{-1} \\bx\\) depend \\(k\\) hence can dropped:\n\\[\n\\begin{split}\nd_k^{LDA}(\\bx) &=  \\bmu_k^T \\bSigma^{-1} \\bx - \\frac{1}{2}\\bmu_k^T \\bSigma^{-1} \\bmu_k + \\log(\\pi_k) \\\\\n  &= \\bb^T \\bx + \n\\end{split}\n\\]\nThus, LDA discriminant function linear \\(\\bx\\), hence \nresulting decision boundaries linear well (.e. straight lines two-dimensional settings).Comparison linear decision boundaries LDA (left) compared QDA (right):Note logistic regression (cf. GLM module) takes exactly linear form indeed closely linked LDA classifier.","code":""},{"path":"supervised-learning-and-classification.html","id":"diagonal-discriminant-analysis-dda","chapter":"4 Supervised learning and classification","heading":"4.3.3 Diagonal discriminant analysis (DDA)","text":"DDA start setting LDA, now simplify model even additionally requiring diagonal covariance containing variances (thus assume correlations among predictors \\(x_1, \\ldots, x_d\\) zero):\n\\[\n\\bSigma = \\bV = \\begin{pmatrix}\n    \\sigma^2_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma^2_{d}\n\\end{pmatrix}\n\\]\nsimplifies inversion \\(\\bSigma\\) \n\\[\n\\bSigma^{-1} = \\bV^{-1} = \\begin{pmatrix}\n    \\sigma^{-2}_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma^{-2}_{d}\n\\end{pmatrix}\n\\]\nleads discriminant function\n\\[\n\\begin{split}\nd_k^{DDA}(\\bx) &=  \\bmu_k^T \\bV^{-1} \\bx - \\frac{1}{2}\\bmu_k^T \\bV^{-1} \\bmu_k + \\log(\\pi_k) \\\\\n  &= \\sum_{j=}^d \\frac{\\mu_{k,j} x_j - \\mu_{k,j}^2/2}{\\sigma_d^2} + \\log(\\pi_k)\n\\end{split}\n\\]\nspecial case LDA, DDA classifier linear classifier thus linear decision boundaries.Bayes classifier (using distribution) assuming uncorrelated predictors\nalso known naive Bayes classifier.Hence, DDA naive Bayes classifier assuming underlying Gaussian distributions.However, don’t let misguide name “naive”: fact DDA “naive” Bayes classifier often effective classifiers, especially high-dimensional settings!","code":""},{"path":"supervised-learning-and-classification.html","id":"the-training-step-learning-qda-lda-and-dda-classifiers-from-data","chapter":"4 Supervised learning and classification","heading":"4.4 The training step — learning QDA, LDA and DDA classifiers from data","text":"","code":""},{"path":"supervised-learning-and-classification.html","id":"number-of-model-parameters","chapter":"4 Supervised learning and classification","heading":"4.4.1 Number of model parameters","text":"order predict class new data using discriminant functions need first learn underlying parameters training data \\(\\bx_i^{\\train}\\) \\(y_i^{\\train}\\):QDA, LDA DDA need learn \\(\\pi_1, \\ldots, \\pi_K\\) \\(\\sum_{k=1}^K \\pi_k = 1\\) mean vectors \\(\\bmu_1, \\ldots, \\bmu_K\\)QDA additionally require \\(\\bSigma_1, \\ldots, \\bSigma_K\\)LDA need \\(\\bSigma\\)DDA estimate \\(\\sigma^2_1, \\ldots, \\sigma^2_d\\).Overall, total number parameters estimated learning discriminant functions\ntraining data follows:QDA: \\(K-1+ K d + K \\frac{d(d+1)}{2}\\)LDA: \\(K-1+ K d + \\frac{d(d+1)}{2}\\)DDA: \\(K-1+ K d + d\\)","code":""},{"path":"supervised-learning-and-classification.html","id":"estimating-the-discriminant-predictor-function","chapter":"4 Supervised learning and classification","heading":"4.4.2 Estimating the discriminant / predictor function","text":"QDA, LDA DDA learn predictor estimating \nparameters discriminant function training data.","code":""},{"path":"supervised-learning-and-classification.html","id":"large-sample-size","chapter":"4 Supervised learning and classification","heading":"4.4.2.1 Large sample size","text":"sample size training data set sufficiently large compared model dimensions can use maximum likelihood (ML) estimate model parameters. able use ML need larger sample size QDA LDA (full covariances need estimated) DDA relatively small sample size can sufficient (explains “naive” Bayes methods popular practise).obtain parameters estimates use known labels \\(y_i^{\\train}\\) sort \nsamples \\(\\bx_i^{\\train}\\) corresponding classes, apply standard ML estimators.\nLet \\(g_k =\\{: y_i^{\\train}=k \\}\\) set indices training sample belonging group \\(k\\), \\(n_k\\) sample size group \\(k\\)ML estimates class probabilities frequencies\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n}\n\\]\nML estimate group means \\(k=1, \\ldots, K\\) \n\\[\n\\hat{\\bmu}_k = \\frac{1}{n_k} \\sum_{\\g_k} \\bx_i^{\\train} \\, .\n\\]\nML estimate global mean \\(\\bmu_0\\) (.e. assume single class ignore group labels) \n\\[\n\\hat{\\bmu}_0 = \\frac{1}{n} \\sum_{=1}^n \\bx_i^{\\train} = \\sum_{k=1}^K \\hat{\\pi}_k \\hat{\\bmu}_k\n\\]\nNote global mean identical pooled mean (.e. weighted average \nindividual group means).ML estimates covariances \\(\\bSigma_k\\) QDA \n\\[\n\\widehat{\\bSigma}_k = \\frac{1}{n_k} \\sum_{\\g_k} ( \\bx_i^{\\train} -\\hat{\\bmu}_k) ( \\bx_i^{\\train} -\\hat{\\bmu}_k)^T\n\\]order get ML estimate pooled variance \\(\\bSigma\\) use LDA compute\n\\[\n\\widehat{\\bSigma} = \\frac{1}{n} \\sum_{k=1}^K \\sum_{\\g_k} ( \\bx_i^{\\train} -\\hat{\\bmu}_k) ( \\bx_i^{\\train} -\\hat{\\bmu}_k)^T =  \\sum_{k=1}^K \\hat{\\pi}_k \\widehat{\\bSigma}_k \n\\]Note pooled variance \\(\\bSigma\\) differs (substantially!) global variance \\(\\Sigma_0\\) results simply\nignoring class labels computed \n\\[\n\\widehat{\\bSigma}_0^{ML} = \\frac{1}{n} \\sum_{=1}^n ( \\bx_i^{\\train} -\\hat{\\bmu}_0) ( \\bx_i^{\\train} -\\hat{\\bmu}_0)^T\n\\]\nrecognise variance decomposition mixture models, \\(\\bSigma_0\\) total variance\npooled \\(\\bSigma\\) unexplained/-group variance.","code":""},{"path":"supervised-learning-and-classification.html","id":"small-sample-size","chapter":"4 Supervised learning and classification","heading":"4.4.2.2 Small sample size","text":"dimension \\(d\\) large compared sample size number parameters predictor function grows fast. Especially QDA also LDA data hungry ML estimation becomes ill-posed problem.discussed Section 1.5 instance need use regularised estimator covariance(s) estimators derived framework penalised ML, Bayesian learning, shrinkage estimation etc.\nalso ensures estimated covariance matrices positive definite (\nautomatically guaranteed DDA variances positive).Furthermore, small sample setting advised reduce number parameters model. Thus using LDA DDA preferred QDA. can also prevent overfitting lead predictor generalises better.analyse high-dimensional data worksheets employ regularised version LDA DDA using Stein-type shrinkage estimation discussed Section 1.5 implemented R package “sda”.","code":""},{"path":"supervised-learning-and-classification.html","id":"comparison-of-estimated-decision-boundaries-lda-vs.-qda","chapter":"4 Supervised learning and classification","heading":"4.4.3 Comparison of estimated decision boundaries: LDA vs. QDA","text":"compare two simple scenarios using simulated data.Non-nested case (\\(K=4\\)):Nested case (\\(K=2\\)):nested case LDA fails separate two classes \nway separate two nested classes \nsimple linear boundary.","code":""},{"path":"supervised-learning-and-classification.html","id":"goodness-of-fit-and-variable-ranking","chapter":"4 Supervised learning and classification","heading":"4.5 Goodness of fit and variable ranking","text":"linear regression (cf. “Statistical Methods” module) interested finding \nwhether fitted mixture model appropriate model, \nparticular predictor(s) \\(x_j\\) \\(\\bx=(x_1, \\ldots, x_d)^T\\)\nresponsible prediction outcome, .e. categorizing sample group \\(k\\).order study problem helpful rewrite discriminant function highlight influence (importance) predictor.focus linear methods (LDA DDA) first look simple case \\(K=2\\) generalise two groups.","code":""},{"path":"supervised-learning-and-classification.html","id":"lda-with-k2-classes","chapter":"4 Supervised learning and classification","heading":"4.5.1 LDA with \\(K=2\\) classes","text":"two classes using LDA discriminant rule choose group \\(k=1\\)\n\\(d_1^{LDA}(\\bx) > d_2^{LDA}(\\bx)\\), equivalently, \n\\[\n\\Delta_{12}^{LDA} = d_1^{LDA}(\\bx) - d_2^{LDA}(\\bx) > 0\n\\]\nSince \\(d_k(\\bx)\\) log-posterior (plus/minus identical constants)\n\\(\\Delta^{LDA}\\) fact log-posterior odds class 1 versus class 2 (see Statistical Methods, Bayesian inference).difference \\(\\Delta_{12}^{LDA}\\) \n\\[\n\\underbrace{ \\Delta_{12}^{LDA}}_{\\text{log posterior odds}} = \n\\underbrace{(\\bmu_1 -\\bmu_2)^T \\bSigma^{-1} \\left(\\bx - \\frac{\\bmu_1+\\bmu_2}{2}\\right)}_{\\text{log Bayes factor } \\log B_{12}} + \\underbrace{\\log\\left( \\frac{\\pi_1}{\\pi_2} \\right)}_{\\text{log prior odds}}\n\\]\nNote since consider simple non-composite models log-Bayes factor identical\nlog-likelihood ratio!log Bayes factor \\(\\log B_{12}\\) known weight evidence favour\n\\(F_1\\) given \\(\\bx\\). expected weight evidence assuming \\(\\bx\\) indeed \\(F_1\\)\nKullback-Leibler discrimination information favour group 1,\n.e. KL divergence distribution \\(F_2\\) \\(F_1\\):\n\\[\n\\expect_{F_1} ( \\log B_{12} ) = \\ikl(F_1,  F_2) = \\frac{1}{2} (\\bmu_1 -\\bmu_2)^T \\bSigma^{-1} (\\bmu_1 -\\bmu_2) = \\frac{1}{2} \\Omega^2\n\\]\nyields, apart scale factor, population version \nHotelling \\(T^2\\)\nstatistic defined \n\\[T^2 =  c^2 (\\hat{\\bmu}_1 -\\hat{\\bmu}_2)^T \\hat{\\bSigma}^{-1} (\\hat{\\bmu}_1 -\\hat{\\bmu}_2)\\]\n\n\\(c = (\\frac{1}{n_1} + \\frac{1}{n_2})^{-1/2} = \\sqrt{n \\pi_1 \\pi_2}\\)\nsample size dependent factor (\\(\\sd(\\hat{\\bmu}_1 - \\hat{\\bmu}_2)\\)).\n\\(T^2\\) measure fit underlying two-component mixture.Using whitening transformation \\(\\bz = \\bW \\bx\\) \\(\\bW^T \\bW = \\bSigma^{-1}\\)\ncan rewrite log Bayes factor \n\\[\n\\begin{split}\n\\log B_{12} &= \\left( (\\bmu_1 -\\bmu_2)^T \\bW^T \\right)\\, \\left(\\bW \\left(\\bx - \\frac{\\bmu_1+\\bmu_2}{2}\\right) \\right) \\\\\n&=\\bomega^T \\bdelta(\\bx)\n\\end{split}\n\\]\n.e. product two vectors:\\(\\bdelta(\\bx)\\) whitened \\(\\bx\\) (centered around average means)\n\\(\\bomega = (\\omega_1, \\ldots, \\omega_d)^T = \\bW (\\bmu_1 -\\bmu_2)\\) gives weight \nwhitened component \\(\\bdelta(\\bx)\\)\nlog Bayes factor.large positive negative value \\(\\omega_j\\)\nindicates corresponding whitened predictor relevant choosing class,\nwhereas small values \\(\\omega_j\\) close zero indicate corresponding ZCA whitened predictor unimportant. Furthermore,\n\\(\\bomega^T \\bomega = \\sum_{j=1}^d \\omega_j^2 = (\\bmu_1 -\\bmu_2)^T \\bSigma^{-1} (\\bmu_1 -\\bmu_2) = \\Omega^2\\),\n.e. squared \\(\\omega_j^2\\) provide component-wise decomposition overall fit \\(\\Omega^2\\).Choosing ZCA-cor whitening transformation \\(\\bW =\\bRho^{-1/2} \\bV^{-1/2}\\)\nget\n\\[\n\\bomega^{ZCA-cor} = \\bRho^{-1/2} \\bV^{-1/2} (\\bmu_1 -\\bmu_2)\n\\]\nbetter understanding \\(\\bomega^{ZCA-cor}\\) provided \ncomparing two-sample \\(t\\)-statistic\n\\[\n\\hat{\\btau} = c \\hat{\\bV}^{-1/2} (\\hat{\\bmu}_1 - \\hat{\\bmu}_2)\n\\]\n\\(\\btau\\) population version \\(\\hat{\\btau}\\) can define\n\\[\\btau^{adj} = \\bRho^{-1/2} \\btau = c \\bomega^{ZCA-cor}\\]\ncorrelation-adjusted \\(t\\)-scores (cat scores). \\(({\\hat{\\btau}}^{adj})^T {\\hat{\\btau}}^{adj} = T^2\\) can see cat scores offer component-wise decomposition Hotelling’s \\(T^2\\).Note choice ZCA-cor whitening ensure whitened components interpretable\nstay maximally correlated original variables. However, may also choose example PCA whitening\ncase \\(\\bomega^T \\bomega\\) provide variable importance PCA whitened variables.DDA, assumes correlations among predictors vanish, .e. \\(\\bRho = \\bI_d\\), get\n\\[\n\\Delta_{12}^{DDA} =\\underbrace{ \\left( (\\bmu_1 -\\bmu_2)^T \\bV^{-1/2}  \\right)}_{\\text{ } c^{-1} \\btau^T }\\, \\underbrace{ \\left( \\bV^{-1/2} \\left(\\bx - \\frac{\\bmu_1+\\bmu_2}{2}\\right) \\right) }_{\\text{centered standardised predictor}}+ \\log\\left( \\frac{\\pi_1}{\\pi_2} \\right) \\\\\n\\]\nSimilarly , \\(t\\)-score \\(\\btau\\) determines impact standardised predictor \\(\\Delta^{DDA}\\).Consequently, DDA can rank predictors squared \\(t\\)-score.\nRecall standard linear regression uncorrelated predictors can find important predictors\nranking squared marginal correlations – ranking (squared) \\(t\\)-scores DDA exact analogy discrete response.","code":""},{"path":"supervised-learning-and-classification.html","id":"multiple-classes","chapter":"4 Supervised learning and classification","heading":"4.5.2 Multiple classes","text":"two classes need refer -called pooled centroids formulation DDA LDA (introduced Tibshirani 2002).pooled centroid given \\(\\bmu_0 = \\sum_{k=1}^K \\pi_k \\bmu_k\\) — centroid\nsingle class. corresponding probability (single class) \\(\\pi_0=1\\) distribution\ncalled \\(F_0\\).LDA discriminant function “group 0” \n\\[\nd_0^{LDA}(\\bx) = \\bmu_0^T \\bSigma^{-1} \\bx - \\frac{1}{2}\\bmu_0^T \\bSigma^{-1} \\bmu_0 \n\\]\nlog posterior odds comparison group \\(k\\) pooled group \\(0\\)\n\n\\[\n\\begin{split}\n\\Delta_k^{LDA} &= d_k^{LDA}(\\bx) - d_0^{LDA}(\\bx) \\\\\n         &= \\log B_{k0} + \\log(\\pi_k) \\\\\n         &= \\bomega_k^T \\bdelta_k(\\bx) + \\log(\\pi_k)\n\\end{split}\n\\]\n\n\\[\n\\bomega_k = \\bW (\\bmu_k - \\bmu_0)  \n\\]\n\n\\[\n\\bdelta_k(\\bx) = \\bW (\\bx - \\frac{\\bmu_k +\\bmu_0}{2} )\n\\]\nexpected log Bayes factor \n\\[\n\\expect_{F_k} ( \\log B_{k0} )= KL(F_k || F_0) = \\frac{1}{2} (\\bmu_k -\\bmu_0)^T \\bSigma^{-1} (\\bmu_k -\\bmu_0) = \\frac{1}{2} \\Omega_k^2\n\\]scale factor \\(c_k = (\\frac{1}{n_k} - \\frac{1}{n})^{-1/2} = \\sqrt{n \\frac{\\pi_k}{1-\\pi_k}}\\) (\\(\\sd(\\hat{\\bmu}_k-\\hat{\\bmu}_0)\\), minus sign \\(\\frac{1}{n}\\) due correlation \n\\(\\hat{\\bmu}_k\\) pooled mean \\(\\hat{\\bmu}_0\\))\nget correlation-adjusted \\(t\\)-score comparing mean group \\(k\\) \npooled mean\n\\[\n\\btau_k^{adj} = c_k \\bomega_k^{ZCA-cor} \\,.\n\\]two class case (\\(K=2\\)) get \n\\(\\bmu_0 = \\pi_1 \\bmu_1 + \\pi_2 \\bmu_2\\) mean difference\n\\((\\bmu_1 - \\bmu_0) = \\pi_2 (\\bmu_1 - \\bmu_2)\\)\n\\(c_1 = \\sqrt{n \\frac{\\pi_1}{\\pi_2}}\\)\nyields\n\\[\n\\btau_1^{adj} = \\sqrt{n \\pi_1 \\pi_2 } \\bRho^{-1/2} \\bV^{-1/2} (\\bmu_1 - \\bmu_2) , \n\\]\n.e. exact score two-class setting.","code":""},{"path":"supervised-learning-and-classification.html","id":"variable-selection-and-cross-validation","chapter":"4 Supervised learning and classification","heading":"4.6 Variable selection and cross-validation","text":"previous saw DDA natural score\nranking features regard relevance separating classes \n(squared) \\(t\\)-score, LDA whitened version \nsquared correlation-adjusted \\(t\\)-score (based ZCA-cor whitening) may used.\nranking established question suitable cutoff arises, .e. \nmany features need () retained model.large high-dimensional models feature selection can also viewed\nform regularisation also dimension reduction. Specifically, may many variables/ features contribute class prediction. Despite \nprinciple effect outcome presence “null variables”\ncan nonetheless deterioriate (sometimes dramatically!) overall predictive accuracy trained predictor, add noise increase model dimension. Therefore, variables contribute prediction\nfiltered order able construct good prediction models classifiers.","code":""},{"path":"supervised-learning-and-classification.html","id":"choosing-a-threshold-by-multiple-testing-using-false-discovery-rates","chapter":"4 Supervised learning and classification","heading":"4.6.1 Choosing a threshold by multiple testing using false discovery rates","text":"simple way determine cutoff threshold use standard technique \nmultiple testing.predictor variable \\(x_1, \\ldots, x_d\\) corresponding test statistic\nmeasuring influence variable response, example \n\\(t\\)-scores related statistics discussed previous section.\naddition providing overall ranking set statistics can used\ndetermine suitable cutoff trying separate two populations predictor variables:“Null” variables contribute prediction“Alternative” variables linked predictionAs discussed “Statistical Methods” module last term (Part 2 - Section 8) can done follows:distribution observed test statistics \\(z_i\\) assumed follow two-component mixture \\(F_0(z)\\) \\(F_A(z)\\) distributions corresponding null alternative, \\(f_0(z)\\) \\(f_a(z)\\) densities, \\(\\pi_0\\) \\(\\pi_A=1-\\pi_0\\) weights:\n\\[\nf(z) = \\pi_0 f_0(z) + (1-\\pi_0) f_a(z)\n\\]distribution observed test statistics \\(z_i\\) assumed follow two-component mixture \\(F_0(z)\\) \\(F_A(z)\\) distributions corresponding null alternative, \\(f_0(z)\\) \\(f_a(z)\\) densities, \\(\\pi_0\\) \\(\\pi_A=1-\\pi_0\\) weights:\n\\[\nf(z) = \\pi_0 f_0(z) + (1-\\pi_0) f_a(z)\n\\]null model typically parametric family (e.g. normal around zero \nfree variance parameter) whereas alternative often modelled nonparametrically.null model typically parametric family (e.g. normal around zero \nfree variance parameter) whereas alternative often modelled nonparametrically.fitting mixture model, often assuming additional constraints make mixture identifiable, one can compute false discovery rates (FDR) follows:\nLocal FDR:\n\\[\n\\widehat{fdr}(z_i) = \\hat{\\prob}(\\text{null} | z_i) = \\frac{\\hat{\\pi}_0 \\hat{f}_0(z_i)}{\\hat{f}(z_i)}  \n\\]\nTail-area-based FDR (=\\(q\\)-value):\n\\[\n\\widehat{Fdr}(z_i) = \\hat{\\prob}(\\text{null} | Z > z_i) = \\frac{\\hat{\\pi}_0 \\hat{F}_0(z_i)}{\\hat{F}(z_i)}\n\\]\nNote essentially \\(p\\)-values adjusted multiple testing (variant Benjamini-Hochberg method).fitting mixture model, often assuming additional constraints make mixture identifiable, one can compute false discovery rates (FDR) follows:Local FDR:\n\\[\n\\widehat{fdr}(z_i) = \\hat{\\prob}(\\text{null} | z_i) = \\frac{\\hat{\\pi}_0 \\hat{f}_0(z_i)}{\\hat{f}(z_i)}  \n\\]Tail-area-based FDR (=\\(q\\)-value):\n\\[\n\\widehat{Fdr}(z_i) = \\hat{\\prob}(\\text{null} | Z > z_i) = \\frac{\\hat{\\pi}_0 \\hat{F}_0(z_i)}{\\hat{F}(z_i)}\n\\]\nNote essentially \\(p\\)-values adjusted multiple testing (variant Benjamini-Hochberg method).thresholding false discovery rates possible identify \nvariables clearly belong two groups also features\neasily discriminated fall either group:“alternative” variables low local FDR, e.g, \\(\\widehat{fdr}(z_i) \\leq 0.2\\)“null” variables high local FDR, e.g. \\(\\widehat{fdr}(z_i) \\geq 0.8\\)features easily classified null alternative, e.g. \\(0.2 < \\widehat{fdr}(z_i) < 0.8\\)feature selection prediction settings generally aim remove \nvariable clearly belong null group, leaving others model.","code":""},{"path":"supervised-learning-and-classification.html","id":"quantifying-prediction-error","chapter":"4 Supervised learning and classification","heading":"4.6.2 Quantifying prediction error","text":"Another direct way compare models compare predictive performance\nquantification prediction error. Specifically, interested relative\nperformance models diverse sets predictor. variables.measure predictor error compares predicted label \\(\\hat{y}\\) true\nlabel \\(y\\) validation data set. validation data set contains \n\\(\\bx_i\\) associated label \\(y_i\\) unlike training data \nused learning predictor function.continuous response often squared loss used:\n\\[\n\\text{err}(\\hat{y}, y) =  (\\hat{y} - y)^2\n\\]binary outcomes one often employs 0/1 loss:\n\\[\n\\text{err}(\\hat{y}, y) =\n\\begin{cases}\n    1, & \\text{ } \\hat{y}=y\\\\\n    0,  & \\text{otherwise}\n\\end{cases}\n\\]\nAlternatively, quantity derived confusion matrix\n(containing TP, TN, FP, FN) can used.mean prediction error expectation\n\\[\nPE = \\expect(\\text{err}(\\hat{y}, y))\n\\]\nthus empirical mean prediction error \n\\[\n\\widehat{PE} = \\frac{1}{m} \\sum_{=1}^m \\text{err}(\\hat{y}_i, y_i)\n \\]\n\\(m\\) sample size validation data set.generally, can also quantify prediction error framework -called proper scoring rules, whole probabilistic forecast taken account (e.g. individual probabilities class, rather just selected probable class). commonly used scoring rule negative log-probability (“surprise”), expected surprise cross-entropy (cf. Statistical Methods module). leads back entropy likelihood (see MATH20802 Statistical Methods).estimate prediction error model can use compare choose among set candiate models, selecting sufficiently low prediction\nerror.","code":""},{"path":"supervised-learning-and-classification.html","id":"estimation-of-prediction-error-without-validation-data-using-cross-validation","chapter":"4 Supervised learning and classification","heading":"4.6.3 Estimation of prediction error without validation data using cross-validation","text":"Unfortunately, quite often separate validation data available evaluate classifier.case need rely simple algorithmic procedure called cross-validation.Outline cross-validation:split samples training data number (say \\(K\\)) parts (“folds”).use \\(K\\) folds validation data \\(K-1\\) folds training data.average resulting \\(K\\) individual estimates prediction error, get overall aggregated predictor error, along error.Note case one part data reserved validation \nused training predictor.choose \\(K\\) folds small (allow estimation \nprediction error) also large (make sure actually able train reliable classifier remaining data). typical value \\(K\\) 5 10, 80% respectively 90% samples used training 20 %\n10% validation.\\(K=n\\) many folds samples validation data set consists single data point. called “leave one ” cross-validation (LOOCV). analytic approximations prediction error obtained LOOCV\napproach computationally inexpensive standard models (including regression).number worksheets cross-validation employed evaluate classification models\ndemonstrate practise feature selection useful construct compact models small number variables nonetheless generalise predict well.reading:study technical details cross-validation: read Section 5.1 Cross-Validation James et al. (2013) introduction statistical learning applications R. Springer.","code":""},{"path":"multivariate-dependencies.html","id":"multivariate-dependencies","chapter":"5 Multivariate dependencies","heading":"5 Multivariate dependencies","text":"","code":""},{"path":"multivariate-dependencies.html","id":"measuring-the-linear-association-between-two-sets-of-random-variables","chapter":"5 Multivariate dependencies","heading":"5.1 Measuring the linear association between two sets of random variables","text":"","code":""},{"path":"multivariate-dependencies.html","id":"outline","chapter":"5 Multivariate dependencies","heading":"5.1.1 Outline","text":"section discuss measure total linear association two sets \nrandom variables \\(\\bx = (x_1, \\ldots, x_p)^T\\) \n\\(\\= (y_1, \\ldots, y_q)^T\\). assume joint correlation matrix\n\\[\n\\bRho = \n\\begin{pmatrix} \n\\bRho_{\\bx} &  \\bRho_{\\bx \\} \\\\\n\\bRho_{\\\\bx} & \\bRho_{\\} \\\\\n\\end{pmatrix} \n\\]\ncross-correlation matrix \\(\\bRho_{\\bx \\} = \\bRho_{\\\\bx}^T\\)\nwithin-group group correlations \\(\\bRho_{\\bx}\\) \\(\\bRho_{\\}\\).\ncross-correlations vanish, \\(\\bRho_{\\bx \\} =0\\), \ntwo random vectors uncorrelated, joint correlation matrix\nbecomes diagonal block matrix\n\\[\n\\bRho_{\\text{indep}} = \n\\begin{pmatrix} \n\\bRho_{\\bx} &  0 \\\\\n0 & \\bRho_{\\} \\\\\n\\end{pmatrix} \\, .\n\\]","code":""},{"path":"multivariate-dependencies.html","id":"special-cases","chapter":"5 Multivariate dependencies","heading":"5.1.2 Special cases","text":"linear regression model \nsquared multiple correlation coefficient determination\n\\[\n\\cor(\\bx, y)^2 = \\bRho_{y\\bx} \\bRho_{\\bx}^{-1} \\bRho_{\\bx y}\n\\]\nstandard measure describe strength total linear association \npredictors \\(\\bx\\) response \\(y\\). \\(\\bRho_{\\bx y} =0\\) \\(\\cor(\\bx, y)^2=0\\).single predictor \\(x\\) \\(\\bRho_{xy}=\\rho\\) \\(\\bRho_{x} = 1\\)\ntherefore squared multiple correlation reduces squared Pearson correlation\n\\[\n\\cor(x, y)^2 = \\rho^2 \\, .\n\\]","code":""},{"path":"multivariate-dependencies.html","id":"rozeboom-vector-correlation","chapter":"5 Multivariate dependencies","heading":"5.1.3 Rozeboom vector correlation","text":"general case two random vectors looking scalar\nquantity quantifies divergence general joint correlation matrix \\(\\bRho\\)\njoint correlation matrix \\(\\bRho_{\\text{indep}}\\) assuming uncorrelated\n\\(\\bx\\) \\(\\\\).One quantity Hotelling’s vector alienation\ncoefficient (given 1936 CCA paper6)\n\\[\n\\begin{split}\n(\\bx, \\) &= \\frac{\\det(\\bRho)}{\\det(\\bRho_{\\text{indep}}) } \\\\\n            & = \\frac{\\det( \\bRho) }{  \\det(\\bRho_{\\bx}) \\,  \\det(\\bRho_{\\})  }\n\\end{split}\n\\]\n\\(\\bK = \\bRho_{\\bx}^{-1/2} \\bRho_{\\bx\\} \\bRho_{\\}^{-1/2}\\) (cf. Chapter 2, Canonical correlation analysis) vector alienation coefficient can written\n(using Weinstein-Aronszajn determinant identity formula determinant\nblock-structured matrices, see Appendix) \n\\[\n\\begin{split}\n(\\bx, \\) & = \\det \\left( \\bI_p - \\bK \\bK^T \\right) \\\\\n            & = \\det \\left( \\bI_q - \\bK^T \\bK \\right) \\\\\n            &= \\prod_{=1}^m (1-\\lambda_i^2) \\\\\n\\end{split}\n\\]\n\\(\\lambda_i\\) singular values \\(\\bK\\), .e. canonical correlations\npair \\(\\bx\\) \\(\\\\).\\(\\bRho_{\\bx \\} = 0\\) und thus \\(\\bx\\) \\(\\\\) uncorrelated \\(\\bRho = \\bRho_{\\text{indep}}\\) \nthus construction vector alienation coefficient \\((\\bx, \\)=1\\).\nHence, generalisation squared multiple correlation.Instead, Rozeboom (1965) proposed use squared vector correlation\ncomplement\n\\[\n\\begin{split}\n\\cor(\\bx, \\)^2 &= \\rho^2_{\\bx \\} = 1 - (\\bx, \\) \\\\\n & = 1- \\det\\left( \\bI_p - \\bK \\bK^T \\right) \\\\\n & = 1- \\det\\left( \\bI_q - \\bK^T \\bK \\right) \\\\\n &  =1- \\prod_{=1}^m (1-\\lambda_i^2) \\\\\n\\end{split}\n\\]\n\\(\\bRho_{\\bx \\} = 0\\) \\(\\cor(\\bx, \\)^2 = 0\\).\nMoreover, either \\(p=1\\) \\(q=1\\) squared vector correlation\nreduces corresponding squared multiple correlation,\n\\(p=1\\) \\(q=1\\) becomes squared Pearson correlation.general way measure multivariate association mutual information (MI)\ncovers linear also non-linear association.\ndiscuss subsequent section Rozeboom vector\ncorrelation arises naturally MI\n\nmultivariate normal distribution.","code":""},{"path":"multivariate-dependencies.html","id":"rv-coefficient","chapter":"5 Multivariate dependencies","heading":"5.1.4 RV coefficient","text":"Another common approach measure association two random vectors RV coefficient introduced Robert Escoufier (1976)\n\n\\[\nRV(\\bx, \\) = \\frac{ \\trace(\\bSigma_{\\bx \\} \\bSigma_{\\\\bx} )}{ \\sqrt{ \\trace(\\bSigma_{\\bx}^2) \\trace(\\bSigma_{\\}^2)  } }\n\\]\neasier compute Rozeboom vector correlation since based\nmatrix trace rather matrix determinant.\\(q=p=1\\) RV coefficient reduces squared correlation.\nHowever, RV coefficient reduce multiple correlation coefficient\n\\(q=1\\) \\(p > 1\\), therefore RV coefficient considered coherent generalisation\nPearson multiple correlation case two random vectors.","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-as-generalisation-of-correlation","chapter":"5 Multivariate dependencies","heading":"5.2 Mutual information as generalisation of correlation","text":"","code":""},{"path":"multivariate-dependencies.html","id":"definition-of-mutual-information","chapter":"5 Multivariate dependencies","heading":"5.2.1 Definition of mutual information","text":"Recall definition\nKullback-Leibler divergence, relative entropy, two distributions:\n\\[\n\\ikl(F,  G) := \\expect_F \\log \\biggl( \\frac{f(\\bx)}{g(\\bx)} \\biggr) \n\\]\n\\(F\\) plays role reference distribution \\(G\\) approximating distribution,\n\\(f\\) \\(g\\) corresponding density functions\n(see MATH20802 Statistical Methods).Mutual Information (MI) two random variables \\(\\bx\\) \\(\\\\) defined \nKL divergence corresponding joint distribution product distribution:\n\\[\n\\text{MI}(\\bx, \\) = \\ikl(F_{\\bx,\\}, F_{\\bx}  F_{\\}) = \\expect_{F_{\\bx,\\}}  \\log \\biggl( \\frac{f(\\bx, \\)}{f(\\bx) \\, f(\\)} \\biggr) .\n\\]\nThus, MI measures well joint distribution can approximated product\ndistribution (appropriate joint distribution \\(\\bx\\) \\(\\\\) independent).\nSince MI application KL divergence shares properties. particular,\n\\(\\text{MI}(\\bx, \\)=0\\) implies joint distribution product distributions . Hence two random variables \\(\\bx\\) \\(\\\\) independent mutual information vanishes.","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-between-two-normal-variables","chapter":"5 Multivariate dependencies","heading":"5.2.2 Mutual information between two normal variables","text":"KL divergence two multivariate normal distributions \\(F_{\\tref}\\) \\(F\\) \n\\[\n\\ikl(F_{\\tref}, F)  = \\frac{1}{2}   \\biggl\\{\n        (\\bmu-\\bmu_{\\tref})^T \\bSigma^{-1} (\\bmu-\\bmu_{\\tref})\n      + \\trace \\biggl(\\bSigma^{-1} \\bSigma_{\\tref} \\biggr)\n    - \\log \\det \\biggl( \\bSigma^{-1} \\bSigma_{\\tref} \\biggr) \n     - d   \\biggr\\} \n\\]\nallows compute mutual information \\(\\text{MI}_{\\text{norm}}(x,y)\\) two univariate random variables \\(x\\) \\(y\\) correlated assumed jointly bivariate normal. Let \\(\\bz = (x, y)^T\\). joint bivariate normal distribution characterised mean \\(\\expect(\\bz) = \\bmu = (\\mu_x, \\mu_y)^T\\) covariance matrix\n\\[\n\\bSigma =\n\\begin{pmatrix} \n\\sigma^2_x & \\rho \\, \\sigma_x \\sigma_y \\\\\n\\rho \\, \\sigma_x  \\sigma_y & \\sigma^2_y \\\\ \n\\end{pmatrix}\n\\]\n\\(\\cor(x,y)= \\rho\\). \\(x\\) \\(y\\) independent \n\\(\\rho=0\\) \n\\[\n\\bSigma_{\\text{indep}} = \n\\begin{pmatrix} \\sigma^2_x & 0 \\\\ 0 & \\sigma^2_y \\\\ \\end{pmatrix} \\,.\n\\]\nproduct\n\\[\n\\bA = \\bSigma_{\\text{indep}}^{-1} \\bSigma = \n\\begin{pmatrix}\n1 & \\rho \\frac{\\sigma_y}{\\sigma_x} \\\\\n\\rho \\frac{\\sigma_x}{\\sigma_y} & 1 \\\\\n\\end{pmatrix}\n\\]\ntrace \\(\\trace(\\bA) = 2\\) determinant \\(\\det(\\bA) = 1-\\rho^2\\).mutual information \\(x\\) \\(y\\) can computed \n\\[\n\\begin{split}\n\\text{MI}_{\\text{norm}}(x, y) &= \\ikl(N(\\bmu, \\bSigma),  N(\\bmu,\\bSigma_{\\text{indep}} )) \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\trace \\biggl(\\bSigma^{-1}_{\\text{indep}} \\bSigma \\biggr)\n    - \\log \\det \\biggl( \\bSigma^{-1}_{\\text{indep}} \\bSigma \\biggr) \n     - 2   \\biggr\\} \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\trace( \\bA )\n    - \\log \\det( \\bA ) \n     - 2   \\biggr\\} \\\\\n&=  -\\frac{1}{2} \\log(1-\\rho^2) \\\\\n  & \\approx \\frac{\\rho^2}{2} \\\\\n\\end{split}\n\\]Thus \\(\\text{MI}_{\\text{norm}}(x,y)\\) one--one function squared correlation \\(\\rho^2\\) \\(x\\) \\(y\\):small values correlation \\(2 \\, \\text{MI}_{\\text{norm}}(x,y) \\approx \\rho^2\\).","code":""},{"path":"multivariate-dependencies.html","id":"mutual-information-between-two-normally-distributed-random-vectors","chapter":"5 Multivariate dependencies","heading":"5.2.3 Mutual information between two normally distributed random vectors","text":"mutual information \\(\\text{MI}_{\\text{norm}}(\\bx,\\)\\) two multivariate normal random vector \\(\\bx\\) \\(\\\\)\ncan computed similar fashion bivariate case.Let \\(\\bz = (\\bx, \\)^T\\) dimension \\(d=p+q\\). joint multivariate\nnormal distribution characterised mean \\(\\expect(\\bz) = \\bmu = (\\bmu_x^T, \\bmu_y^T)^T\\) covariance matrix\n\\[\n\\bSigma =\n\\begin{pmatrix} \\bSigma_{\\bx} & \\bSigma_{\\bx \\} \\\\ \n\\bSigma_{\\bx \\}^T & \\bSigma_{\\} \\\\ \n\\end{pmatrix} \\,.\n\\]\n\\(\\bx\\) \\(\\\\) independent \n\\(\\bSigma_{\\bx \\} = 0\\) \n\\[\n\\bSigma_{\\text{indep}} =\n\\begin{pmatrix}  \n\\bSigma_{\\bx} & 0 \\\\ \n0 & \\bSigma_{\\} \\\\ \n\\end{pmatrix} \\, .\n\\]\nproduct\n\\[\n\\begin{split}\n\\bA & = \n\\bSigma_{\\text{indep}}^{-1} \\bSigma = \n\\begin{pmatrix}\n\\bI_p & \\bSigma_{\\bx}^{-1} \\bSigma_{\\bx \\} \\\\\n\\bSigma_{\\}^{-1} \\bSigma_{\\\\bx}    & \\bI_q \\\\\n\\end{pmatrix} \\\\\n& = \n\\begin{pmatrix}\n\\bI_p &  \\bV_{\\bx}^{-1/2} \\bRho_{\\bx}^{-1} \\bRho_{\\bx \\} \\bV_{\\}^{1/2} \\\\\n\\bV_{\\}^{-1/2} \\bRho_{\\}^{-1} \\bRho_{\\\\bx} \\bV_{\\bx}^{1/2}   & \\bI_q \\\\\n\\end{pmatrix}\\\\\n\\end{split}\n\\]\ntrace \\(\\trace(\\bA) = d\\) determinant\n\\[\n\\begin{split}\n\\det(\\bA) & = \\det( \\bI_p - \\bK \\bK^T ) \\\\\n  &= \\det( \\bI_q - \\bK^T \\bK ) \\\\\n\\end{split}\n\\]\n\\(\\bK = \\bRho_{\\bx}^{-1/2} \\bRho_{\\bx \\} \\bRho_{\\}^{-1/2}\\).\n\\(\\lambda_1, \\ldots, \\lambda_m\\) singular values \\(\\bK\\) (.e.\ncanonical correlations \\(\\bx\\) \\(\\\\)) get\n\\[\n\\det(\\bA) =  \\prod_{=1}^m (1-\\lambda_i^2)\n\\]mutual information \\(\\bx\\) \\(\\\\) \n\\[\n\\begin{split}\n\\text{MI}_{\\text{norm}}(\\bx, \\) &= \\ikl(N(\\bmu, \\bSigma),  N(\\bmu,\\bSigma_{\\text{indep}} )) \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\trace \\biggl( \\bSigma^{-1}_{\\text{indep}} \\bSigma \\biggr)\n    - \\log \\det \\biggl( \\bSigma^{-1}_{\\text{indep}} \\bSigma \\biggr) \n     - d   \\biggr\\} \\\\\n& = \\frac{1}{2}   \\biggl\\{\n         \\trace( \\bA )\n    - \\log \\det( \\bA ) \n     - d   \\biggr\\} \\\\\n&=-\\frac{1}{2} \\sum_{=1}^m \\log(1-\\lambda_i^2)\\\\\n\\end{split}\n\\]Note \\(\\text{MI}_{\\text{norm}}(\\bx, \\)\\) sum MIs resulting \nindividual canonical correlations \\(\\lambda_i\\) functional\nform bivariate normal case.comparison squared Rozeboom vector correlation coefficient \\(\\rho^2_{\\bx \\}\\)\nrecognize \n\\[\n\\text{MI}_{\\text{norm}}(\\bx,\\) = -\\frac{1}{2} \\log(1 - \\rho^2_{\\bx \\} ) \\approx \\frac{1}{2} \\rho^2_{\\bx \\}\n\\]\nThus, multivariate case \\(\\text{MI}_{\\text{norm}}(\\bx \\)\\) exactly functional relationship \nvector correlation \\(\\rho^2_{\\bx, \\}\\) \\(\\text{MI}_{\\text{norm}}(x, y)\\)\ntwo univariate variables squared Pearson correlation \\(\\rho^2\\).Thus, Rozebooms vector correlation directly linked mutual information jointly multivariate normally distributed variables.","code":""},{"path":"multivariate-dependencies.html","id":"using-mi-for-variable-selection","chapter":"5 Multivariate dependencies","heading":"5.2.4 Using MI for variable selection","text":"principle, MI can computed distribution model thus applies normal non-normal models, linear nonlinear relationships.general way may denote \\(F_{\\|\\bx}\\) denote predictive model \\(\\\\) conditioned \\(\\bx\\) \\(F_{\\}\\) marginal distribution \\(\\\\) without predictors. Note predictive model can assume form (incl. nonlinear). Typically \\(F_{\\|\\bx}\\) complex model \\(F_{\\}\\)\nsimple model (predictors).mutual information \n\\(\\bx\\) \\(\\\\) can also understood expectated KL divergence\nconditional marginal distributions:\n\\[\n\\expect_{F_{\\bx}}\\, \\ikl(F_{\\|\\bx},  F_{\\} ) = \\text{MI}(\\bx, \\)\n\\]can shown follows.\nKL divergence \\(F_{\\|\\bx}\\) \\(F_{\\}\\)\ngiven \n\\[\n\\ikl(F_{\\|\\bx}, F_{\\} )  = \\expect_{F_{\\|\\bx}}  \\log\\biggl( \\frac{f(\\|\\bx) }{ f(\\)}  \\biggr) \\, , \n\\]\nrandom variable since depends \\(\\bx\\).\nTaking expectation regard \\(F_{\\bx}\\) (distribution \\(\\bx\\))\nget\n\\[\n\\expect_{F_{\\bx}} \\ikl(F_{\\|\\bx}, F_{\\} ) = \n\\expect_{F_{\\bx}}  \\expect_{F_{\\|\\bx}}  \\log \\biggl(\\frac{ f(\\|\\bx) f(\\bx) }{ f(\\) f(\\bx) } \\biggr) = \n\\expect_{F_{\\bx,\\}}   \\log \\biggl(\\frac{ f(\\bx,\\) }{ f(\\) f(\\bx) } \\biggr) = \\text{MI}(\\bx,\\) \\,.\n\\]link MI conditioning MI response predictor variables often used variable feature selection general models.","code":""},{"path":"multivariate-dependencies.html","id":"other-measures-of-general-dependence","chapter":"5 Multivariate dependencies","heading":"5.2.5 Other measures of general dependence","text":"Besides mutual information others measures general dependence multivariate random variables.two important ones proposed recent literatur ) distance correlation\nii) maximal information coefficient (MIC \\(\\text{MIC}_e\\)).","code":""},{"path":"multivariate-dependencies.html","id":"graphical-models","chapter":"5 Multivariate dependencies","heading":"5.3 Graphical models","text":"","code":""},{"path":"multivariate-dependencies.html","id":"purpose","chapter":"5 Multivariate dependencies","heading":"5.3.1 Purpose","text":"Graphical models combine features fromgraph theoryprobabilitystatistical inferenceThe literature graphical models huge, focus two commonly\nused models:DAGs (directed acyclic graphs), edges directed, directed loops (.e. cycles, hence “acyclic”)GGM (Gaussian graphical models), edges undirectedGraphical models provide probabilistic models trees networks, \nrandom variables represented nodes graphs, branches representing\nconditional dependencies. regard generalise tree-based clustering approaches well probabilistic non-hierarchical methods (GMMs).However, class graphical models goes much beyond simple\nunsupervised learning models. also includes regression, classification,\ntime series models etc. See e.g. reference book Murphy (2012).","code":""},{"path":"multivariate-dependencies.html","id":"basic-notions-from-graph-theory","chapter":"5 Multivariate dependencies","heading":"5.3.2 Basic notions from graph theory","text":"Mathematically, graph \\(G = (V, E)\\) consists set vertices nodes \\(V = \\{v_1, v_2, \\ldots\\}\\) set branches edges \\(E = \\{ e_1, e_2, \\ldots \\}\\).Edges can undirected directed.Graphs containing directed edges directed graphs, likewise graphs containing undirected edges called undirected graphs. Graphs containing directed undirected edges called partially directed graphs.path sequence vertices vertices edge next vertex sequence.graph connected path every pair vertices.cycle path graph connects node .connected graph cycles called tree.degree node number edges connects . edges directed degree node sum -degree -degree, counts incoming outgoing edges, respectively.External nodes nodes degree 1. tree-structed graph also called leafs.notions relevant graphs directed edges:directed graph parent node(s) vertex \\(v\\) set nodes \\(\\text{pa}(v)\\) directly connected \\(v\\) via edges directed parent node(s) towards \\(v\\).Conversely, \\(v\\) called child node \\(\\text{pa}(v)\\). Note parent node can several child nodes, \\(v\\) may child \\(\\text{pa}(v)\\).directed tree graph, node single parent, except one particular node parent (node called root node).DAG, directed acyclic graph, directed graph directed cycles. (directed) tree special version DAG.","code":""},{"path":"multivariate-dependencies.html","id":"probabilistic-graphical-models","chapter":"5 Multivariate dependencies","heading":"5.3.3 Probabilistic graphical models","text":"graphical model uses graph describe relationship random variables \\(x_1, \\ldots, x_d\\). variables assumed joint distribution density/mass function \\(\\prob(x_1, x_2, \\ldots, x_d)\\).\nrandom variable placed node graph.structure graph type edges connecting (connecting) pair nodes/variables used describe conditional dependencies, simplify joint distribution.Thus, graphical model essence visualisation joint distribution using structural information graph helping understand mutual relationship among variables.","code":""},{"path":"multivariate-dependencies.html","id":"directed-graphical-models","chapter":"5 Multivariate dependencies","heading":"5.3.4 Directed graphical models","text":"directed graphical model graph structure assumed \nDAG (directed tree, also DAG).joint probability distribution can factorised product conditional probabilities follows:\n\\[\n\\prob(x_1, x_2, \\ldots, x_d) = \\prod_i \\prob(x_i  | \\text{pa}(x_i))\n\\]\nThus, overall joint probability distribution specified local conditional distributions graph structure, directions edges providing information parent-child node relationships.Probabilistic DAGs also known “Bayesian networks”.Idea: trying possible trees/graphs fitting data using maximum likelihood (Bayesian inference) hope able identify graph structure data-generating process.Challengesin tree/network internal nodes usually known, thus \ntreated latent variables.Answer: impute states nodes may use EM algorithm GMMs\n(fact can viewed graphical models, !).treat internal nodes unknowns need marginalise \ninternal nodes, .e. need sum / integrate possible set states\ninternal nodes!Answer: can handled effectively using Viterbi algorithm essentially\napplication generalised distributive law. particular tree graphs \nmeans summations occurs locally nodes propagates recursively accross tree.order infer tree network structure space trees networks need \nexplored. possible exhaustive fashion unless number variables\ntree small.Answer: Solution: use heuristic approaches tree network search!Furthermore, exist -called “equivalence classes” graphical models, .e. sets graphical models share joint probability distribution. Thus, graphical models within equivalence class distinguished observational data, even infinite sample size!Answer: fundamental mathematical problem identifiability now way around issue. However,\npositive side, also implies search graphical models can restricted finding -called “essential graph” (e.g. https://projecteuclid.org/euclid.aos/1031833662 )Conclusion: using directed graphical models structure discovery time consuming computationally\ndemanding anything small toy data sets.also explains heuristic non-model based approaches (hierarchical clustering) popular even though full statistical modelling principle possible.","code":""},{"path":"multivariate-dependencies.html","id":"undirected-graphical-models","chapter":"5 Multivariate dependencies","heading":"5.3.5 Undirected graphical models","text":"Another class graphical models models contain undirected edges. undirected graphical models\nused represent pairwise conditional ()dependencies among variables graph, resulting model therefore also called conditional independence graph.\\(x_i\\) \\(x_j\\) two selected random variables/nodes, set \\(\\{x_k\\}\\) represents variables/nodes \\(k\\neq \\) \\(k \\neq j\\). say variables \\(x_i\\) \\(x_j\\) conditionally independent\ngiven variables \\(\\{x_k\\}\\)\n\\[\nx_i \\perp\\!\\!\\!\\perp x_j | \\{x_k\\}\n\\]\njoint probability density \\(x_i, x_j\\) \\(x_k\\)\nfactorises \n\\[\n \\prob(x_1, x_2, \\ldots, x_d) = \\prob(x_i | \\{x_k\\}) \\prob(x_j | \\{x_k\\}) \\prob(\\{x_k\\}) \\,.\n \\]\nequivalently\n\\[\n \\prob(x_i, x_j | \\{x_k\\}) = \\prob(x_i | \\{x_k\\}) \\prob(x_j | \\{x_k\\}) \\,.\n \\]corresponding conditional independence graph, edge \\(x_i\\) \\(x_j\\),\ngraph missing edges correspond conditional independence respective non-connected nodes.","code":""},{"path":"multivariate-dependencies.html","id":"gaussian-graphical-model","chapter":"5 Multivariate dependencies","heading":"5.3.5.1 Gaussian graphical model","text":"Assuming \\(x_1, \\ldots, x_d\\) jointly normally distributed, .e. \\(\\bx \\sim N(\\bmu, \\bSigma)\\),\nturns straightforward identify pairwise conditional independencies.\n\\(\\bSigma\\) first obtain precision matrix\n\\[\\bOmega = (\\omega_{ij}) = \\bSigma^{-1} \\,.\\]\nCrucially, can shown \n\\(\\omega_{ij} = 0\\) implies\n\\(x_i \\perp\\!\\!\\!\\perp x_j | \\{ x_k \\}\\)!\nHence, precision matrix \\(\\bOmega\\) can directly read pairwise conditional independencies among variables \\(x_1, x_2, \\ldots, x_d\\)!Often, covariance matrix \\(\\bSigma\\) dense (zeros) corresponding precision matrix\n\\(\\bOmega\\) sparse (many zeros).conditional independence graph computed normally distributed variables called\nGaussian graphical model, GGM. alternative name\ncovariance selection model.","code":""},{"path":"multivariate-dependencies.html","id":"related-quantity-partial-correlation","chapter":"5 Multivariate dependencies","heading":"5.3.5.2 Related quantity: partial correlation","text":"precision matrix \\(\\bOmega\\) can also compute matrix pairwise full conditional partial correlations:\\[\n\\rho_{ij|\\text{rest}}=-\\frac{\\omega_{ij}}{\\sqrt{\\omega_{ii}\\omega_{jj}}}\n\\]\nessentially standardised precision matrix (similar correlation extra minus sign!)partial correlations lie range -1 +1, \\(\\rho_{ij|\\text{rest}} \\[-1, 1]\\), just like standard correlations.\\(\\bx\\) multivariate normal \\(\\rho_{ij|\\text{rest}} = 0\\) indicates conditional independence\n\\(x_i\\) \\(x_j\\).Regression interpretation: partial correlation correlation remains \ntwo variables effect variables “regressed away”.\nwords, partial correlation exactly equivalent correlation \nresiduals remain regressing \\(x_i\\) variables \\(\\{x_k\\}\\) \\(x_j\\) \\(\\{x_k\\}\\).","code":""},{"path":"multivariate-dependencies.html","id":"algorithm-for-learning-ggms","chapter":"5 Multivariate dependencies","heading":"5.3.6 Algorithm for learning GGMs","text":"can devise simple algorithm learn Gaussian graphical model (GGM)\ndata:Estimate covariance \\(\\hat{\\bSigma}\\) (way invertible!)Compute corresponding partial correlationsIf \\(\\hat{\\rho}_{ij|\\text{rest}} \\approx 0\\) (approx). conditional\nindependence \\(x_i\\) \\(x_j\\).\npractise done statistical testing vanishing partial correlations. many edges also need\nadjustment simultaneous multiple testing since edges tested parallel.","code":""},{"path":"multivariate-dependencies.html","id":"example-exam-score-data-mardia-et-al-1979","chapter":"5 Multivariate dependencies","heading":"5.3.7 Example: exam score data (Mardia et al 1979:)","text":"Correlations (rounded 2 digits):Partial correlations (rounded 2 digits):Note zero correlations \nfour partial correlations close 0, indicating conditional independence :analysis mechanics,statistics mechanics,analysis vectors, andstatistics vectors.Thus, 10 possible edges four missing, thus\nconditional independence graph looks follows:","code":"##            mechanics vectors algebra analysis statistics\n## mechanics       1.00    0.55    0.55     0.41       0.39\n## vectors         0.55    1.00    0.61     0.49       0.44\n## algebra         0.55    0.61    1.00     0.71       0.66\n## analysis        0.41    0.49    0.71     1.00       0.61\n## statistics      0.39    0.44    0.66     0.61       1.00##            mechanics vectors algebra analysis statistics\n## mechanics       1.00    0.33    0.23     0.00       0.02\n## vectors         0.33    1.00    0.28     0.08       0.02\n## algebra         0.23    0.28    1.00     0.43       0.36\n## analysis        0.00    0.08    0.43     1.00       0.25\n## statistics      0.02    0.02    0.36     0.25       1.00Mechanics      Analysis\n   |     \\    /    |\n   |    Algebra    |\n   |     /   \\     |\n Vectors      Statistics"},{"path":"nonlinear-and-nonparametric-models.html","id":"nonlinear-and-nonparametric-models","chapter":"6 Nonlinear and nonparametric models","heading":"6 Nonlinear and nonparametric models","text":"last part module discuss methods go beyond \nlinear methods prevalent classical multivariate statistics.Relevant textbooks:lectures much part module follow selected chapters following three text books:James et al. (2013) introduction statistical learning applications R. Springer.James et al. (2013) introduction statistical learning applications R. Springer.Hastie, Tibshirani, Friedman (2009) elements statistical learning: data mining, inference, prediction. Springer.Hastie, Tibshirani, Friedman (2009) elements statistical learning: data mining, inference, prediction. Springer.Rogers Girolami (2017) first course machine learning (2nd edition). CRC Press.Rogers Girolami (2017) first course machine learning (2nd edition). CRC Press.Please study relevant section chapters indicated subsection!","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"limits-of-linear-models-and-correlation","chapter":"6 Nonlinear and nonparametric models","heading":"6.1 Limits of linear models and correlation","text":"Linear models effective tools. However, important\nrecognise limits especially modelling complex nonlinear relationships.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"correlation-only-measures-linear-dependence","chapter":"6 Nonlinear and nonparametric models","heading":"6.1.1 Correlation only measures linear dependence","text":"simple demonstration given following example. Assume \\(x\\) normally distributed\nrandom variable \\(x \\sim N(0,1)\\). \\(x\\) construct second random variable \\(y = x^2\\) — thus \\(y\\) fully depends \\(x\\) added extra noise. correlation \\(x\\) \\(y\\)?Let’s ansers question running small computer simulation:Thus, correlation (almost) zero even though \\(x\\) \\(y\\) fully dependent!\ncorrelation measures linear association!","code":"\nx=rnorm(10000)\ny = x^2\ncor(x,y)## [1] -0.02254539"},{"path":"nonlinear-and-nonparametric-models.html","id":"anscombe-data-sets","chapter":"6 Nonlinear and nonparametric models","heading":"6.1.2 Anscombe data sets","text":"Using correlation, generally linear models, blindly can thus hide underlying complexity analysed\ndata. demonstrated classic “Anscombe quartet” data sets\n(F. J. Anscombe. 1973. Graphs statistical analysis.\nAmerican Statistician 27:17-21, http://dx.doi.org/10.1080/00031305.1973.10478966 ):evident scatter plots relationship \ntwo variables \\(x\\) \\(y\\) different four cases!\nHowever, intriguingly four data sets share exactly linear characteristics summary statistics:Means \\(m_x = 9\\) \\(m_y = 7.5\\)Variances \\(s^2_x = 11\\) \\(s^2_y = 4.13\\)Correlation \\(r = 0.8162\\)Linear model fit intercept \\(=3.0\\) slope \\(b=0.5\\)Thus, actual data analysis always good idea inspect data visually get first impression whether using linear model makes sense.data “” follows linear model. Data “b” represents quadratic relationship. Data “c” linear outlier disturbs linear relationship. Finally data “d” also contains outlier also represent case \\(y\\) (apart outlier) dependent \\(x\\).Worksheet 10 recent version Anscombe quartet analysed form “datasauRus” dozen - 13 highly nonlinear datasets share \nlinear characteristics.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"alternatives","chapter":"6 Nonlinear and nonparametric models","heading":"6.1.3 Alternatives","text":"Mutual information (based relative entropy) discussed previous chapterOther measures designed capture nonlinear association, distance correlation.\nii) maximal information coefficient (MIC \\(\\text{MIC}_e\\)).","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"random-forests","chapter":"6 Nonlinear and nonparametric models","heading":"6.2 Random forests","text":"Another widely used approach prediction nonlinear settings\nmethod random forests.Relevant reading:Please read: James et al. (2013) Chapter 8 “Tree-Based Methods”Specifically:Section 8.1 Basics Decision TreesSection 8.2.1 BaggingSection 8.2.2 Random Forests","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"stochastic-vs.-algorithmic-models","chapter":"6 Nonlinear and nonparametric models","heading":"6.2.1 Stochastic vs. algorithmic models","text":"Two cultures statistical modelling: stochastic vs. algorithmic modelsClassic discussion paper Leo Breiman (2001): Statistical modeling: two cultures.\nStatistical Science 16:199–231. https://projecteuclid.org/euclid.ss/1009213726This paper recently revisited following discussion paper Efron (2020) discussants:\nPrediction, estimation, attribution. JASA 115:636–677. https://doi.org/10.1080/01621459.2020.1762613","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"random-forests-1","chapter":"6 Nonlinear and nonparametric models","heading":"6.2.2 Random forests","text":"Invented Breimann 1996.Basic idea:single decision tree unreliable unstable (weak predictor/classifier).Use boostrap generate multiple decision trees (=“forest”)Average predictions tree (=“bagging”, bootstrap aggregation)averaging procedure effect variance stabilisation.\nIntringuingly, averaging across decision trees dramatically improves \noverall prediction accuracy!Random Forests approach example ensemble method\n(since based using “ensemble” trees).Variations: boosting, XGBoost ( https://xgboost.ai/ )Random forests applied Worksheet 10.computationally expensive typically perform well!","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"comparison-of-decision-boundaries-decision-tree-vs.-random-forest","chapter":"6 Nonlinear and nonparametric models","heading":"6.2.3 Comparison of decision boundaries: decision tree vs. random forest","text":"Non-nested case:Compare also decision boundaries LDA QDA (previous chapter).","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gaussian-processes","chapter":"6 Nonlinear and nonparametric models","heading":"6.3 Gaussian processes","text":"Gaussian processes offer another nonparametric approach model\nnonlinear dependencies. provide probabilistic model \nunknown nonlinear function.Relevant reading:Please read: Rogers Girolami (2017) Chapter 8: Gaussian processes.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"main-concepts","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.1 Main concepts","text":"Gaussian processes (GPs) belong family Bayesian nonparametric modelsIdea:\nstart prior function (!),\ncondition observed data get posterior distribution (function)\nstart prior function (!),condition observed data get posterior distribution (function)GPs use infinitely dimensional multivariate normal distribution prior","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"conditional-multivariate-normal-distribution","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.2 Conditional multivariate normal distribution","text":"GPs make use fact marginal conditional distributions multivariate normal\ndistribution also multivariate normal.Multivariate normal distribution:\\[\\bz \\sim N_d(\\bmu, \\bSigma)\\]Assume:\n\\[\n\\bz =\\begin{pmatrix}\n    \\bz_1      \\\\\n    \\bz_2      \\\\\n\\end{pmatrix}\n\\]\n\n\\[\n\\bmu =\\begin{pmatrix}\n    \\bmu_1      \\\\\n    \\bmu_2      \\\\\n\\end{pmatrix}\n\\]\n\n\\[\n\\bSigma =\\begin{pmatrix}\n    \\bSigma_{1}   & \\bSigma_{12}   \\\\\n    \\bSigma_{12}^T & \\bSigma_{2}   \\\\\n\\end{pmatrix}\n\\]\ncorresponding dimensions \\(d_1\\) \\(d_2\\) \\(d_1+d_2=d\\).Marginal distributions:subset \\(\\bz\\) also multivariate normally distributed.\nSpecifically,\n\\[\n\\bz_1 \\sim N_{d_1}(\\bmu_1, \\bSigma_{1}) \n\\]\n\n\\[\n\\bz_2 \\sim N_{d_2}(\\bmu_2, \\bSigma_{2}) \n\\]Conditional multivariate normal:conditional distribution also multivariate normal:\n\\[\n\\bz_1 | \\bz_2 = \\bz_{1 | 2} \\sim N_{d_1}(\\bmu_{1|2}, \\bSigma_{1 | 2}) \n\\]\n\n\\[\\bmu_{1|2}=\\bmu_1 + \\bSigma_{12} \\bSigma_{2}^{-1} (\\bz_2 -\\bmu_2)\\]\n\n\\[\\bSigma_{1 | 2}=\\bSigma_{1} -  \\bSigma_{12} \\bSigma_{2}^{-1} \\bSigma_{12}^T\\]\\(\\bz_{1 | 2}\\) \n\\(\\bmu_{1|2}\\) dimension \\(d_1 \\times 1\\)\n\\(\\bSigma_{1 | 2}\\) dimension \\(d_1 \\times d_1\\),\n.e. dimension unconditioned variables.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"covariance-functions-and-kernels","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.3 Covariance functions and kernels","text":"GP prior infinitely dimensional multivariate normal\nmean zero covariance specified function \\(k(x, x^{\\prime})\\):widely used covariance function \n\\[\nk(x, x^{\\prime}) = \\cov(x, x^{\\prime}) = \\sigma^2 e^{-\\frac{ (x-x^{\\prime})^2}{2 l^2}}\n\\]\nknown squared-exponential kernel Radial-basis function (RBF) kernel.Note kernel implies\\(k(x, x) = \\var(x) = \\sigma^2\\) \\(\\cor(x, x^{\\prime}) = e^{-\\frac{ (x-x^{\\prime})^2}{2 l^2}}\\).parameter \\(l\\) RBF kernel length scale parameter describes\n“wigglyness” smoothness resulting function.\nSmall values \\(l\\) mean complex, wiggly functions, low autocorrelation.many kernel functions, including linear, polynomial periodic kernels.","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gp-model","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.4 GP model","text":"Nonlinear regression GP approach conceptually simple:start multivariate priorthen condition observed datathe resulting conditional multivariate normal can used predict\nfunction values unobserved valuesthe conditional variance can used compute credible intervals predictions.GP regression also provides direct link classical\nBayesian linear regression (using linear kernel).Drawbacks: computationally expensive (\\(O(n^3)\\) matrix inversion)","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"gaussian-process-example","chapter":"6 Nonlinear and nonparametric models","heading":"6.3.5 Gaussian process example","text":"now show apply Gaussian processes R justing using standard matrix calculations.aim estimate following nonlinear function number observations. Note initially assume additional noise (observations lie directly curve):can now visualise functions samples multivariate normal prior:Now compute posterior mean variance conditioning observations:Now can plot posterior mean upper lower bounds 95% credible interval:Finally, can take acount noise measured data points adding error term:Note vicinity data points CIs small away\ndata uncertain estimate underlying function becomes.","code":"\ntruefunc = function(x) sin(x)\nXLIM = c(0, 2*pi)\nYLIM = c(-2, 2)\n\nn2 = 10\nx2 = runif(n2, min=XLIM[1], max=XLIM[2])\ny2 = truefunc(x2)  # no noise\n\ncurve( truefunc(x), xlim=XLIM, ylim=YLIM, xlab=\"x\", ylab=\"y\", \n      main=\"True Function\")\npoints(x2, y2)\n# RBF kernel\nrbfkernel = function(xa, xb, s2=1, l=1/2) s2*exp(-1/2*(xa-xb)^2/l^2)\nkfun.mat = function(xavec, xbvec, FUN=rbfkernel) \n  outer(X=as.vector(xavec), Y=as.vector(xbvec), FUN=FUN)\n\n# prior mean\nmu.vec = function(x) rep(0, length(x))\n# grid of x-values \nn1 = 100\nx1 = seq(XLIM[1], XLIM[2], length.out=n1)\n\n# unconditioned covariance and mean (unobserved samples x1)\nK1 = kfun.mat(x1, x1)  \nm1 = mu.vec(x1)\n\n## sample functions from GP prior  \nB = 5\nlibrary(\"MASS\") # for mvrnorm\ny1r = t(mvrnorm(B, mu = m1, Sigma=K1))\n\nplot(x1, y1r[,1], type=\"l\", lwd=2, ylab=\"y\", xlab=\"x\", ylim=YLIM, \n  main=\"Prior Functions (RBF Kernel with l=1/2)\")\nfor(i in 2:B)\n  lines(x1, y1r[,i], col=i, lwd=2)\n# unconditioned covariance and mean (observed samples x2)\nK2 = kfun.mat(x2, x2)\nm2 = mu.vec(x2)\niK2 = solve(K2) # inverse\n\n# cross-covariance\nK12 = kfun.mat(x1, x2)\n\n# Conditioning: x1 conditioned on x2\n\n# conditional mean\nm1.2 = m1 + K12 %*% iK2 %*% (y2 - m2)\n\n# conditional variance\nK1.2 = K1 - K12 %*% iK2 %*% t(K12)\n# upper and lower CI\nupper.bound = m1.2 + 1.96*sqrt(diag(K1.2))\nlower.bound = m1.2 - 1.96*sqrt(diag(K1.2))\n\nplot(x1, m1.2, type=\"l\", xlim=XLIM, ylim=YLIM, col=\"red\", lwd=3,\n  ylab=\"y\", xlab = \"x\", main = \"Posterior\")\n\npoints(x2,y2,pch=4,lwd=4,col=\"blue\")\nlines(x1,upper.bound,lty=2,lwd=3)\nlines(x1,lower.bound,lty=2,lwd=3)\ncurve(truefunc(x), xlim=XLIM, add=TRUE, col=\"gray\")\n\nlegend(x=\"topright\", \n  legend=c(\"posterior mean\", \"posterior quantiles\", \"true function\"),\n  lty=c(1, 2, 1),lwd=c(4, 4, 1), col=c(\"red\",\"black\", \"gray\"), cex=1.0)\n# add some noise\nsdeps = 0.1\nK2 = K2 + sdeps^2*diag(1,length(x2))\n\n# update\niK2 = solve(K2) # inverse\nm1.2 = m1 + K12 %*% iK2 %*% (y2 - m2)\nK1.2 = K1 - K12 %*% iK2 %*% t(K12)\nupper.bound = m1.2 + 1.96*sqrt(diag(K1.2))\nlower.bound = m1.2 - 1.96*sqrt(diag(K1.2))\n\nplot(x1, m1.2, type=\"l\", xlim=XLIM, ylim=YLIM, col=\"red\", lwd=3, \n  ylab=\"y\", xlab = \"x\", main = \"Posterior (with noise)\")\n\npoints(x2,y2,pch=4,lwd=4,col=\"blue\")\nlines(x1,upper.bound,lty=2,lwd=3)\nlines(x1,lower.bound,lty=2,lwd=3)\ncurve(truefunc(x), xlim=XLIM, add=TRUE, col=\"gray\")\n\nlegend(x=\"topright\", \n  legend=c(\"posterior mean\", \"posterior quantiles\", \"true function\"),\n  lty=c(1, 2, 1),lwd=c(4, 4, 1), col=c(\"red\",\"black\", \"gray\"), cex=1.0)"},{"path":"nonlinear-and-nonparametric-models.html","id":"neural-networks","chapter":"6 Nonlinear and nonparametric models","heading":"6.4 Neural networks","text":"Another highly important class models\nnonlinear prediction (nonlinear function approximation) \nneural networks.Relevant reading:Please read: Hastie, Tibshirani, Friedman (2009) Chapter 11 “Neural networks”","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"history","chapter":"6 Nonlinear and nonparametric models","heading":"6.4.1 History","text":"Neural networks actually relatively old models, going back\n1950s!Three phases neural networks (NN)1950/60: replicating functions neurons brain (perceptron)1980/90: neural networks universal function approximators2010—today: deep learningThe first phase biologically inspired, second phase focused \nmathematical properties, current phase pushed forward \nadvances computer science numerical optimisation:backpropagation algorithmbackpropagation algorithmauto-differentiation,auto-differentiation,stochastic gradient descentstochastic gradient descentuse GPUs TPUs (e.g. linear algebra)use GPUs TPUs (e.g. linear algebra)availability development deep learning packages:\nTheano (University Montreal), now Theano-PyMC/Aesara (PyMC3)\nTensorFlow (Google),\nFlax / JAX (Google),\nMXNet (Amazon),\nPyTorch (Facebook),\nPaddlePaddle (Baidu) etc.\navailability development deep learning packages:Theano (University Montreal), now Theano-PyMC/Aesara (PyMC3)TensorFlow (Google),Flax / JAX (Google),MXNet (Amazon),PyTorch (Facebook),PaddlePaddle (Baidu) etc.high-level wrappers:Keras (Tensorflow, MXNet, Theano)PyTorch-Lightning (PyTorch)","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"neural-networks-1","chapter":"6 Nonlinear and nonparametric models","heading":"6.4.2 Neural networks","text":"Neural networks essentially stacked systems linear regressions,\nmapping input nodes (random variables) outputs (response nodes).\ninternal layer corresponds internal latent variables.\nlayer connected next layer non-linear activation functions.feedforward single layer NNstacked nonlinear multiple regression hidden variablesoptimise empirical risk minimisationIt can shown NN can approximate arbitrary non-linear function mapping\ninput output.“Deep” neural networks many layers, optimisation requires advanced\ntechniques (see ).Neural networks highly parameterised models require typically lot data\ntraining.statistical aspects NN well understood: particular known\nNN overfit data can still generalise well. hand, also know NN\ncan also “fooled”, .e. prediction can unstable (adversarial examples).Current statistical research NN focuses interpretability links Bayesian inference models (e.g. GPs). example:https://link.springer.com/book/10.1007/978-3-030-28954-6https://arxiv.org/abs/1910.12478","code":""},{"path":"nonlinear-and-nonparametric-models.html","id":"learning-more-about-deep-learning","chapter":"6 Nonlinear and nonparametric models","heading":"6.4.3 Learning more about deep learning","text":"good place learn deep learning actual\nimplementations computer code various platforms book “Dive deep learning” \nZhang et al. (2020) available online https://d2l.ai/","code":""},{"path":"brief-refresher-on-matrices.html","id":"brief-refresher-on-matrices","chapter":"A Brief refresher on matrices","heading":"A Brief refresher on matrices","text":"intended short recap essentials need know matrices.\nfrequently make use matrix calculations.\nMatrix notation helps make multivariate equations simpler understand\nbetter.details please consult lecture notes earlier modules (e.g. linear algebra).course mostly work real matrices, .e. assume matrix elements \nreal numbers. However, one important matrix decomposition — eigenvalues decomposition — can yield complex-valued matrices applied real matrices. point case.","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-basics","chapter":"A Brief refresher on matrices","heading":"A.1 Matrix basics","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-notation","chapter":"A Brief refresher on matrices","heading":"A.1.1 Matrix notation","text":"matrix notation distinguish scalars, vectors, matrices:Scalar: \\(x\\), \\(X\\), lower upper case, plain type.Vector: \\(\\bx\\), lower case, bold type. handwriting arrow \\(\\vec{x}\\) indicates vector.component notation write \\(\\bx = (x_1, \\ldots, x_d)^T = (x_i)^T\\). convention, vector \ncolumn vector, .e. elements arranged column index (\\(\\)) refers row\ncolumn. transpose column vector becomes row vector\n\\(\\bx^T = (x_1, \\ldots, x_d) =(x_i)\\) index now refers column.Matrix: \\(\\bX\\), upper case, bold type. handwriting underscore\n\\(\\underline{X}\\) indicates matrix.component notation write \\(\\bX = (x_{ij})\\). convention, first index (\\(\\))\nscalar elements \\(x_{ij}\\) denotes row second index (\\(j\\)) column matrix.\nAssuming \\(n\\) number rows \\(d\\) number columns\ncan also view matrix \\(\\bX = (\\bx_j) = (\\bz_i)^T\\) composed column vectors\n\\(\\bx_j = (x_{1j}, \\ldots, x_{nj})^T\\)\nrow vectors \\(\\bz_i^T = (x_{i1}, \\ldots, x_{id})\\).(column) vector matrix size \\(d\\times 1\\). row vector matrix size \\(1\\times d\\).\nscalar matrix size \\(1 \\times 1\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"random-matrix","chapter":"A Brief refresher on matrices","heading":"A.1.2 Random matrix","text":"random matrix (vector) matrix (vector) whose elements random variables.Note standard\nnotation used univariate statistics distinguish\nrandom variables realisations (.e. upper versus lower case) \nwork multivariate statistics. Therefore, need determine\ncontext whether quantity represents \nrandom variable, whether constant.","code":""},{"path":"brief-refresher-on-matrices.html","id":"special-matrices","chapter":"A Brief refresher on matrices","heading":"A.1.3 Special matrices","text":"\\(\\bI_d\\) identity matrix. square matrix size\n\\(d \\times d\\) diagonal\nfilled 1 -diagonals filled 0.\n\\[\\bI_d =\n\\begin{pmatrix}\n    1 & 0 & 0 & \\dots & 0\\\\\n    0 & 1 & 0 & \\dots & 0\\\\\n    0 & 0 & 1 &   & 0\\\\\n    \\vdots & \\vdots & & \\ddots &  \\\\\n    0 & 0 & 0 &  & 1 \\\\\n\\end{pmatrix}\\]\\(\\bOne\\) matrix contains 1s. often\nused form column vector \\(d\\) rows:\n\\[\\bOne_d =\n\\begin{pmatrix}\n    1 \\\\\n    1 \\\\\n    1 \\\\\n    \\vdots   \\\\\n    1  \\\\\n\\end{pmatrix}\\]diagonal matrix matrix -diagonal elements zero.triangular matrix square matrix whose elements either diagonal zero (upper vs. lower triangular matrix).","code":""},{"path":"brief-refresher-on-matrices.html","id":"simple-matrix-operations","chapter":"A Brief refresher on matrices","heading":"A.2 Simple matrix operations","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-addition-and-multiplication","chapter":"A Brief refresher on matrices","heading":"A.2.1 Matrix addition and multiplication","text":"Matrices behave much like common numbers. example, exist\nmatrix addition \\(\\bC = \\bA + \\bB\\)\nmatrix multiplication \\(\\bC = \\bA \\bB\\).Matrix addition simply result addition corresponding elements \\(\\bA\\) \\(\\bB\\),\n.e \\(c_{ij} = a_{ij} + b_{ij}\\). matrix addition \\(\\bA\\) \\(\\bB\\) must size, .e.\nnumber rows columns.dot product two vectors \\(\\ba \\cdot \\bb = \\ba^T \\bb = \\ba \\bb^T = \\sum_{=1}^d a_{} b_{}\\).Matrix multiplication defined \\(c_{ij} = \\sum_{k=1}^m a_{ik} b_{kj}\\) \\(m\\) \nnumber columns \\(\\bA\\) number rows \\(\\bB\\). Thus, \\(\\bC\\) contains possible\ndot products row vectors \\(\\bA\\) column vectors \\(\\bB\\).\nmatrix multiplication number columns \\(\\bA\\) must match number rows \\(\\bB\\).\nNote matrix multiplication general commutative, .e. \\(\\bA \\bB \\neq \\bB \\bA\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-transpose","chapter":"A Brief refresher on matrices","heading":"A.2.2 Matrix transpose","text":"matrix transpose \\(t(\\bA) = \\bA^T\\) interchanges rows columns. transpose\nlinear operator \\((\\bA + \\bB)^T = \\bA^T + \\bB^T\\) \napplied matrix\nproduct reverses ordering, .e. \\((\\bA \\bB)^T =\\bB^T \\bA^T\\).\\(\\bA = \\bA^T\\) \\(\\bA\\) symmetric (square).construction given rectangular \\(\\bA\\) matrices\n\\(\\bA^T \\bA\\) \\(\\bA \\bA^T\\) symmetric non-negative diagonal.","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-summaries","chapter":"A Brief refresher on matrices","heading":"A.3 Matrix summaries","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-trace","chapter":"A Brief refresher on matrices","heading":"A.3.1 Matrix trace","text":"trace matrix sum diagonal elements \\(\\trace(\\bA) = \\sum a_{ii}\\).useful identity matrix trace \n\\[\n\\trace(\\bA \\bB ) = \\trace( \\bB \\bA)  \n\\]\ntwo vectors becomes\n\\[\n\\ba^T \\bb = \\trace( \\bb \\ba^T) \\,.\n\\]squared Frobenius norm, .e. sum squares entries rectangular matrix \\(\\bA =(a_{ij})\\), can \nwritten using trace follows:\\[\n\\begin{split}\n||\\bA ||_F^2 &= \\sum_{,j} a_{ij}^2 \\\\\n &= \\trace(\\bA^T \\bA) = \\trace(\\bA \\bA^T) \\,.\n\\end{split}\n\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"determinant-of-a-matrix","chapter":"A Brief refresher on matrices","heading":"A.3.2 Determinant of a matrix","text":"\\(\\bA\\) square matrix determinant \\(\\det(\\bA)\\) scalar measuring volume spanned column vectors \\(\\bA\\) sign determined orientation vectors.\\(\\det(\\bA) \\neq 0\\) matrix \\(\\bA\\) non-singular non-degenerate. Conversely, \n\\(\\det(\\bA) =0\\) matrix \\(\\bA\\) singular degenerate.One way compute determinant matrix \\(\\bA\\) Laplace cofactor\nexpansion approach proceeds recursively based determinants submatrices \\(\\bA_{-,-j}\\) obtained deleting row \\(\\) column \\(j\\) \\(\\bA\\). Specifically, \nlevel compute thecofactor expansion either\nalong \\(\\)-th row — pick row \\(\\):\n\\[\\det(\\bA) = \\sum_{j=1}^d a_{ij} (-1)^{+j} \\det(\\bA_{-,-j})  \\text{ , }\\]\nalong \\(j\\)-th column — pick \\(j\\):\n\\[\\det(\\bA) = \\sum_{=1}^d a_{ij} (-1)^{+j} \\det(\\bA_{-,-j})\\].\nalong \\(\\)-th row — pick row \\(\\):\n\\[\\det(\\bA) = \\sum_{j=1}^d a_{ij} (-1)^{+j} \\det(\\bA_{-,-j})  \\text{ , }\\]along \\(j\\)-th column — pick \\(j\\):\n\\[\\det(\\bA) = \\sum_{=1}^d a_{ij} (-1)^{+j} \\det(\\bA_{-,-j})\\].repeat submatrix scalar \\(\\) \\(\\det()=\\,.\\)recursive nature algorithm leads complexity order \\(O(d!)\\) practical except small \\(d\\).\nTherefore, practice efficient algorithms computing determinants used still algorithmic complexity order \\(O(d^3)\\) large dimensions obtaining determinants \nexpensive.However, specially structured matrices allow fast calculation.\nparticular, turns determinant triangular matrix (includes diagonal matrices)\nsimply product diagonal elements.two-dimensional matrix \\(\\bA = \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\\\end{pmatrix}\\)\ndeterminant \\(\\det() = a_{11} a_{22} - a_{12} a_{21}\\).block-structured square matrix\n\\[\n\\bA = \\begin{pmatrix} \\bA_{11} & \\bA_{12} \\\\ \\bA_{21} & \\bA_{22} \\\\ \\end{pmatrix} \\, ,\n\\]\nmatrices diagonal \\(\\bA_{11}\\) \\(\\bA_{22}\\) square \n\\(\\bA_{21}\\) \\(\\bA_{21}\\) can shape,\ndeterminant \n\\[\n\\det(\\bA) = \\det(\\bA_{22}) \\det(\\bC_1) = \\det(\\bA_{11}) \\det(\\bC_2) \n\\]\n(Schur complement \\(\\bA_{22}\\))\n\\[\n\\bC_1 = \\bA_{11} -  \\bA_{12}  \\bA_{22}^{-1}  \\bA_{21} \n\\]\n(Schur complement \\(\\bA_{11}\\))\n\\[\n\\bC_2 = \\bA_{22} -  \\bA_{21}  \\bA_{11}^{-1}  \\bA_{12} \n\\]\nNote \\(\\bC_1\\) \\(\\bC_2\\) square matrices.block-diagonal matrix \\(\\bA\\) \\(\\bA_{12} = 0\\) \\(\\bA_{21} = 0\\)\ndeterminant \\(\\det(\\bA) = \\det(\\bA_{11}) \\det(\\bA_{22})\\).Determinants multiplicative property,\n\\[\\det(\\bA \\bB) = \\det(\\bB \\bA) = \\det(\\bA) \\det(\\bB) \\,.\\]\nscalar \\(\\) becomes\n\\(\\det(\\bB) = ^d \\det(\\bB)\\) \\(d\\) dimension \\(\\bB\\).Another important identity \n\\[\\det(\\bI_n + \\bA \\bB) = \\det(\\bI_m + \\bB \\bA)\\]\n\\(\\bA\\) \\(n \\times m\\) \\(\\bB\\) \\(m \\times n\\) matrix. called \nWeinstein-Aronszajn determinant identity (also credited Sylvester).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-inverse","chapter":"A Brief refresher on matrices","heading":"A.4 Matrix inverse","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"inversion-of-square-matrix","chapter":"A Brief refresher on matrices","heading":"A.4.1 Inversion of square matrix","text":"\\(\\bA\\) square matrix inverse matrix \\(\\bA^{-1}\\) matrix\n\n\\[\\bA^{-1} \\bA = \\bA \\bA^{-1}=  \\bI \\, .\\]\nnon-singular matrices \\(\\det(\\bA) \\neq 0\\) invertible.\\(\\det(\\bA^{-1} \\bA) = \\det(\\bI) = 1\\) \ndeterminant inverse matrix equals\ninverse determinant,\n\\[\\det(\\bA^{-1}) = \\det(\\bA)^{-1} \\,.\\]transpose inverse inverse transpose\n\n\\[\n\\begin{split}\n(\\bA^{-1})^T &= (\\bA^{-1})^T \\,  \\bA^T (\\bA^{T})^{-1}   \\\\\n &= (\\bA \\bA^{-1})^T \\, (\\bA^{T})^{-1} = (\\bA^{T})^{-1} \\,. \\\\\n\\end{split}\n\\]inverse matrix product \\((\\bA \\bB)^{-1} = \\bB^{-1} \\bA^{-1}\\)\nproduct indivdual matrix inverses reverse order.many different algorithms compute inverse matrix\n(essentially problem solving system equations).\ncomputational complexity matrix inversion order \\(O(d^3)\\)\n\\(d\\) dimension \\(\\bA\\). Therefore matrix inversion costly higher dimensions.Example .1  Inversion \\(2 \\times 2\\) matrix:inverse matrix \\(= \\begin{pmatrix} & b \\\\ c & d \\end{pmatrix}\\) \n\\(^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & \\end{pmatrix}\\)","code":""},{"path":"brief-refresher-on-matrices.html","id":"inversion-of-structured-matrices","chapter":"A Brief refresher on matrices","heading":"A.4.2 Inversion of structured matrices","text":"However, specially structured matrices inversion can done effectively:inverse diagonal matrix another diagonal matrix obtained inverting diagonal elements.generally, inverse block-diagonal matrix obtained individually inverting blocks along diagonal.Woodbury matrix identity simplifies inversion matrices can \nwritten \\(\\bA + \\bU \\bB \\bV\\) \\(\\bA\\) \\(\\bB\\) square \n\\(\\bU\\) \\(\\bV\\) suitable rectangular matrices:\n\\[\n(\\bA + \\bU \\bB \\bV)^{-1} = \\bA^{-1} - \\bA^{-1} \\bU (\\bB^{-1} + \\bV \\bA^{-1} \\bU)^{-1} \\bV \\bA^{-1}\n\\]\nTypically, inverse \\(\\bA^{-1}\\) either already known can easily obtained \ndimension \\(\\bB\\) much lower \\(\\bA\\).class matrices can easily inverted orthogonal matrices whose inverse \nobtained simply transposing matrix.","code":""},{"path":"brief-refresher-on-matrices.html","id":"orthogonal-matrices","chapter":"A Brief refresher on matrices","heading":"A.5 Orthogonal matrices","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"properties-1","chapter":"A Brief refresher on matrices","heading":"A.5.1 Properties","text":"orthogonal matrix \\(\\bQ\\) square matrix property \\(\\bQ^T = \\bQ^{-1}\\), .e.\ntranspose also inverse. implies \\(\\bQ \\bQ^T = \\bQ^T \\bQ = \\bI\\).identity matrix \\(\\bI\\) simplest example orthogonal matrix.orthogonal matrix\n\\(\\bQ\\) can interpreted geometrically operator performing\nrotation, reflection /permutation.\nMultiplication \\(\\bQ\\) vector result \nnew vector length change direction (unless \\(\\bQ=\\bI\\)).product \\(\\bQ_3 = \\bQ_1 \\bQ_2\\) two orthogonal matrices \\(\\bQ_1\\) \\(\\bQ_2\\) yields another orthogonal matrix \\(\\bQ_3 \\bQ_3^T = \\bQ_1 \\bQ_2 (\\bQ_1 \\bQ_2)^T = \\bQ_1 \\bQ_2 \\bQ_2^T \\bQ_1^T = \\bI\\).determinant \\(\\det(\\bQ)\\) orthogonal matrix either +1 -1,\n\\(\\bQ \\bQ^T = \\bI\\) thus \\(\\det(\\bQ)\\det(\\bQ^T) = \\det(\\bQ)^2 = \\det(\\bI) = 1\\).set orthogonal matrices dimension \\(d\\) together multiplication\nform group called orthogonal group \\(O(d)\\).\nsubset orthogonal matrices \\(\\det(\\bQ)=1\\) called rotation matrices form multiplication special orthogonal group \\((d)\\).\nOrthogonal matrices \\(\\det(\\bQ)=-1\\) rotation-reflection matrices.","code":""},{"path":"brief-refresher-on-matrices.html","id":"generating-orthogonal-matrices","chapter":"A Brief refresher on matrices","heading":"A.5.2 Generating orthogonal matrices","text":"two dimensions \\((d=2)\\) orthogonal matrices \\(\\bR\\) representing rotations \\(\\det(\\bR)=1\\) \ngiven \n\\[\n\\bR(\\theta) = \n\\begin{pmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta \n\\end{pmatrix}\n\\]\nrepresenting rotation-reflections \\(\\bG\\) \\(\\det(\\bG)=-1\\) \n\\[\n\\bG(\\theta) = \n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix}\\,.\n\\]\nEvery orthogonal matrix dimension \\(d=2\\)\ncan represented product two rotation-reflection\nmatrices \n\\[\n\\bR(\\theta) = \\bG(\\theta)\\, \\bG(0) =  \n\\begin{pmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{pmatrix} \n\\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}\\,.\n\\]\nThus, matrix \\(\\bG\\) generator two-dimensional orthogonal matrices.\nNote \\(\\bG(\\theta)\\) symmetric, orthogonal determinant -1.generally, applicable arbitrary dimension, role generator taken Householder reflection matrix\n\\[\n\\bQ_{HH}(\\bv) = \\bI- 2 \\bv \\bv^T\n\\]\n\\(\\bv\\) vector unit length (\\(\\bv^T \\bv=1\\)) orthogonal \nreflection hyperplane. Note \\(\\bQ_{HH}(\\bv) = \\bQ_{HH}(-\\bv)\\).\nconstruction matrix \\(\\bQ_{HH}(\\bv)\\) symmetric, orthogonal determinant -1.can shown \\(d\\)-dimensional orthogonal matrix \\(\\bQ\\) can represented product \\(d\\) Householder reflection matrices.\ntwo-dimensional generator \\(\\bG(\\theta)\\) recovered Householder matrix \\(\\bQ_{HH}(\\bv)\\)\n\\(\\bv = \\begin{pmatrix} -\\sin \\frac{\\theta}{2} \\\\ \\cos \\frac{\\theta}{2} \\end{pmatrix}\\)\n\\(\\bv = \\begin{pmatrix} \\sin \\frac{\\theta}{2} \\\\ -\\cos \\frac{\\theta}{2} \\end{pmatrix}\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"permutation-matrix","chapter":"A Brief refresher on matrices","heading":"A.5.3 Permutation matrix","text":"special type orthogonal matrix permutation matrix \\(\\bP\\) created \npermuting rows /columns identity matrix \\(\\bI\\). Thus, row column\n\\(\\bP\\) contains exactly one entry 1, necessarily diagonal.permutation matrix \\(\\bP\\) multiplied matrix \\(\\bA\\) acts operator\npermuting columns (\\(\\bA \\bP\\)) rows (\\(\\bP \\bA\\)).\nset \\(d\\) elements exist \\(d!\\) permutations. Thus, dimension \\(d\\) \n\\(d!\\) possible permutation matrices (including identity matrix).determinant permutation matrix either +1 -1.\nproduct two permutation matrices yields another permutation matrix.Symmetric permutation matrices correspond self-inverse permutations\n(.e. permutation matrix inverse), also called permutation involutions.\ncan determinant +1 -1.transposition permutation two elements exchanged.\nThus, transposition matrix \\(\\bT\\)\nexactly two rows /columns exchanged compared identity matrix \\(\\bI\\).\nTranspositions self-inverse, transposition matrices symmetric.\n\\(\\frac{d (d-1)}{2}\\) different transposition matrices.\ndeterminant transposition matrix \\(\\det(\\bT)= -1\\).Note transposition matrix instance Householder matrix \\(\\bQ_{HH}(\\bv)\\)\nvector \\(\\bv\\) filled zeros except two elements value\n\\(\\frac{\\sqrt{2}}{2}\\) \\(-\\frac{\\sqrt{2}}{2}\\).permutation \\(d\\) elements can generated series \\(d-1\\) transpositions.\nCorrespondingly, permutation matrix \\(\\bP\\) can constructed multiplication identity\nmatrix \\(d-1\\) transposition matrices. number transpositions even \\(\\det(\\bP) = 1\\) otherwise\nuneven number \\(\\det(\\bP) = -1\\). called sign signature permutation.set permutations form symmetric group \\(S_d\\), subset even permutations (positive sign \\(\\det(\\bP)=1\\)) alternating group \\(A_d\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-and-eigenvectors","chapter":"A Brief refresher on matrices","heading":"A.6 Eigenvalues and eigenvectors","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"definition","chapter":"A Brief refresher on matrices","heading":"A.6.1 Definition","text":"Assume square symmetric matrix \\(\\bA\\) size \\(d \\times d\\).\nvector \\(\\bu \\neq 0\\) called eigenvector matrix \\(\\bA\\) \\(\\lambda\\) corresponding\neigenvalue \\[\\bA \\bu = \\bu \\lambda \\, .\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"finding-eigenvalues-and-vectors","chapter":"A Brief refresher on matrices","heading":"A.6.2 Finding eigenvalues and vectors","text":"find eigenvalues eigenvector eigenequation rewritten \n\\[(\\bA -\\bI \\lambda ) \\bu  = 0 \\,.\\] solution \\(\\bu \\neq 0\\) corresponding eigenvalue\n\\(\\lambda\\) must make matrix \\(\\bA -\\bI \\lambda\\) singular, .e. determinant must vanish\n\\[\\det(\\bA -\\bI \\lambda ) =0 \\,.\\]\ncharacteristic equation matrix \\(\\bA\\), solution yields \\(d\\)\nnecessarily distinct also potentially complex eigenvalues \\(\\lambda_1, \\ldots, \\lambda_d\\).complex eigenvalues, real matrix eigenvalues come conjugate pairs.\nSpecifically, complex \\(\\lambda_1 = r e^{\\phi}\\) also corresponding complex eigenvalue \\(\\lambda_2 = r e^{-\\phi}\\).Given eigenvalues solve eigenequation corresponding non-zero eigenvectors\n\\(\\bu_1, \\ldots, \\bu_d\\). Note eigenvectors real matrices can complex components.\nAlso eigenvector defined eigenequation scalar.\nconvention eigenvectors therefore typically standardised unit length still leaves\nsign ambiguity real eigenvectors implies complex eigenvectors defined factor modulus 1.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenequation-in-matrix-notation","chapter":"A Brief refresher on matrices","heading":"A.6.3 Eigenequation in matrix notation","text":"matrix\n\\[\\bU = (\\bu_1, \\ldots, \\bu_d)\\] containing standardised eigenvectors columns diagonal matrix\n\\[\\bLambda = \\begin{pmatrix}\n    \\lambda_{1} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\lambda_{d}\n\\end{pmatrix}\\]\ncontaining eigenvalues (typically sorted order magnitude) eigenvalue equation can written \n\\[\\bA \\bU = \\bU \\bLambda \\,.\\]Note eigenvalues order, can alwas apply permutation matrix \\(\\bP\\) sort order, \\(\\bU' = \\bU \\bP\\) reorders eigenvectors \\(\\bLambda' = \\bP^T \\bLambda \\bP\\)\neigenvalues, \n\\[\\bA \\bU' =  \\bA \\bU \\bP = \\bU \\bLambda \\bP =  \\bU \\bP \\bP^T \\bLambda \\bP =  \\bU' \\bLambda' \\,.\\]","code":""},{"path":"brief-refresher-on-matrices.html","id":"defective-matrix","chapter":"A Brief refresher on matrices","heading":"A.6.4 Defective matrix","text":"cases eigenvectors \\(\\bu_i\\) linearly independent form basis span \\(d\\) dimensional space.However, case \nmatrix \\(\\bA\\) complete basis eigenvectors, matrix called defective. case\nmatrix \\(\\bU\\) containing eigenvectors singular \\(\\det(\\bU)=0\\).example defective matrix \n\\(\\begin{pmatrix} 1 &1 \\\\ 0 & 1 \\\\ \\end{pmatrix}\\)\ndeterminant 1 can inverted column vectors form complete basis\none distinct eigenvector \\((1,0)^T\\) eigenvector basis incomplete.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-of-a-diagonal-or-triangular-matrix","chapter":"A Brief refresher on matrices","heading":"A.6.5 Eigenvalues of a diagonal or triangular matrix","text":"special case \\(\\bA\\) diagonal triangular matrix eigenvalues easily determined.\nfollows simple form determinants product diagonal elements.\nHence matrices characteristic equation becomes \\(\\prod_{}^d (a_{ii} -\\lambda) = 0\\) solution\n\\(\\lambda_i=a_{ii}\\), .e. eigenvalues equal diagonal elements.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-and-vectors-of-a-symmetric-matrix","chapter":"A Brief refresher on matrices","heading":"A.6.6 Eigenvalues and vectors of a symmetric matrix","text":"\\(\\bA\\) symmetric, .e. \\(\\bA = \\bA^T\\), eigenvalues eigenvectors special properties:eigenvalues \\(\\bA\\) real,eigenvectors orthogonal, .e \\(\\bu_i^T \\bu_j = 0\\) \\(\\neq j\\), real. Thus, matrix \\(\\bU\\) containing standardised orthonormal eigenvectors orthogonal.\\(\\bA\\) never defective \\(\\bU\\) forms complete basis.","code":""},{"path":"brief-refresher-on-matrices.html","id":"eigenvalues-of-orthogonal-matrices","chapter":"A Brief refresher on matrices","heading":"A.6.7 Eigenvalues of orthogonal matrices","text":"eigenvalues orthogonal matrix \\(\\bQ\\) necessarily real \nmodulus 1 lie unit circle . Thus, eigenvalues \\(\\bQ\\)\nform \\(\\lambda = e^{\\phi} = \\cos \\phi + \\sin \\phi\\).real matrix complex eigenvalues come conjugate\npairs. Hence orthogonal matrix \\(\\bQ\\) complex eigenvalue \\(e^{\\phi}\\) also \ncomplex eigenvalue \\(e^{-\\phi} =\\cos \\phi - \\sin \\phi\\). product two conjugate\neigenvalues 1. Thus, orthogonal matrix uneven dimension least one\nreal eigenvalue (+1 -1).eigenvalues Hausholder matrix \\(\\bQ_{HH}(\\bv)\\) real (recall symmetric!).\nfact, dimension \\(d\\) eigenvalues -1 (one time) 1 ( \\(d-1\\) times).\nSince transposition matrix \\(\\bT\\) special Householder matrix eigenvalues.","code":""},{"path":"brief-refresher-on-matrices.html","id":"positive-definite-matrices","chapter":"A Brief refresher on matrices","heading":"A.6.8 Positive definite matrices","text":"eigenvalues square matrix \\(\\bA\\) real \\(\\lambda_i \\geq 0\\) \\(\\bA\\) called positive semi-definite.\neigenvalues strictly positive\n\\(\\lambda_i > 0\\) \\(\\bA\\) called positive definite.Note matrix need symmetric positive\ndefinite, e.g.\n\\(\\begin{pmatrix} 2 & 3 \\\\ 1 & 4 \\\\ \\end{pmatrix}\\)\npositive eigenvalues 5 1. also complete\nset eigenvectors diagonisable.symmetric matrix \\(\\bA\\) positive definite\nquadratic form \\(\\bx^T \\bA \\bx > 0\\) non-zero \\(\\bx\\),\npositive semi-definite \\(\\bx^T \\bA \\bx \\geq 0\\).\nholds also way around:\nsymmetric positive definite matrix (positive eigenvalues) \npositive quadratic form, symmetric positive semi-definite matrix (non-negative eigenvalues) non-negative quadratic form.symmetric positive definite matrix always positive diagonal\n(can seen setting \\(\\bx\\) unit vector 1 \nsingle position, 0 elements).\nHowever, just requiring positive diagonal weak ensure positive definiteness symmetric matrix, example \\(\\begin{pmatrix} 1 &10 \\\\ 10 & 1 \\\\ \\end{pmatrix}\\) negative eigenvalue -9.\nhand, symmetric matrix indeed positive definite strictly\ndiagonally dominant, .e. diagonal elements positive larger absolute value corresponding row column elements.\nHowever, diagonal dominance restrictive criterion \ncharacterise \nsymmetric positive definite matrices, since\nmany symmetric matrices positive definite diagonally dominant, \n\\(\\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\\\ \\end{pmatrix}\\).Finally, sum symmetric positive semi-definite matrix \\(\\bA\\)\nsymmetric positive definite matrix \\(\\bB\\) symmetric positive definite corresponding\nquadratic form \\(\\bx^T ( \\bA +\\bB) \\bx = \\bx^T \\bA \\bx + \\bx^T \\bB \\bx > 0\\) positive. Similarly, sum\ntwo symmetric positive (semi)-definite matrices symmetric positive (semi)-definite.","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-decompositions","chapter":"A Brief refresher on matrices","heading":"A.7 Matrix decompositions","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"diagonalisation-and-eigenvalue-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.1 Diagonalisation and eigenvalue decomposition","text":"\\(\\bA\\) square non-defective matrix \\(\\bU\\) invertible \ncan rewrite eigenvalue equation \n\\[\\bA  = \\bU \\bLambda \\bU^{-1} \\,.\\]\ncalled eigendecomposition, spectral decomposition, \\(\\bA\\) equivalently\n\\[\\bLambda  = \\bU^{-1} \\bA \\bU\\]\ndiagonalisation \\(\\bA\\).\nThus matrix \\(\\bA\\) defective diagonalisable using eigenvalue decomposition.","code":""},{"path":"brief-refresher-on-matrices.html","id":"orthogonal-eigenvalue-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.2 Orthogonal eigenvalue decomposition","text":"symmetric \\(\\bA\\) becomes\n\\[\\bA = \\bU \\bLambda \\bU^T\\]\nreal eigenvalues orthogonal matrix \\(\\bU\\)\n\n\\[\\bLambda = \\bU^T \\bA \\bU  \\,.\\]\nspecial case known orthogonal diagonalisation\n\\(\\bA\\).orthogonal decomposition symmetric \\(\\bA\\) \nunique apart signs\neigenvectors.\norder make fully unique one needs impose restrictions (e.g. require positive diagonal\n\\(\\bU\\)). Note can particularly important computer application sign\ncan vary depending specific implementation underlying numerical algorithms.","code":""},{"path":"brief-refresher-on-matrices.html","id":"singular-value-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.3 Singular value decomposition","text":"singular value decomposition (SVD) \ngeneralisation orthogonal eigenvalue decomposition\nsymmetric matrices.(!) rectangular matrix \\(\\bA\\) size \\(n\\times d\\) can factored\nproduct\n\\[\\bA = \\bU \\bD \\bV^T\\]\n\\(\\bU\\) \\(n \\times n\\) orthogonal matrix, \\(\\bV\\) second \\(d \\times d\\) orthogonal matrix \\(\\bD\\) diagonal rectangular matrix\nsize \\(n\\times d\\) \\(m=min(n,d)\\) real diagonal elements \\(d_1, \\ldots d_m\\). \\(d_i\\) called singular values, appear\nalong diagonal \\(\\bD\\) order magnitude.SVD unique apart \nsigns columns vectors \\(\\bU\\), \\(\\bV\\) \\(\\bD\\) (can freely specify column signs two \nthree matrices). convention \nsigns chosen singular values \\(\\bD\\) non-negative, leaves ambiguity\ncolumns signs \\(\\bU\\) \\(\\bV\\). Alternatively, one may\nfix columns signs \\(\\bU\\) \\(\\bV\\), e.g. requiring positive diagonal, determines sign singular values (thus allowing negative singular values well).\\(\\bA\\) symmetric SVD orthogonal eigenvalue decomposition coincide (apart different sign conventions singular values, eigenvalues eigenvectors).Since \\(\\bA^T \\bA = \\bV \\bD^T \\bD \\bV^T\\) \\(\\bA \\bA^T = \\bU \\bD \\bD^T \\bU^T\\) squared singular values correspond eigenvalues \\(\\bA^T \\bA\\) \\(\\bA \\bA^T\\).\nalso follows \\(\\bA^T \\bA\\) \\(\\bA \\bA^T\\) positive\nsemi-definite symmetric matrices, \\(\\bV\\) \\(\\bU\\) contain respective sets eigenvectors.","code":""},{"path":"brief-refresher-on-matrices.html","id":"polar-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.4 Polar decomposition","text":"square matrix \\(\\bA\\) can factored product\n\\[\n\\bA = \\bQ \\bB\n\\]\northogonal matrix \\(\\bQ\\) symmetric positive semi-definite matrix \\(\\bB\\).follows SVD \\(\\bA\\) given \n\\[\n\\begin{split}\n\\bA &= \\bU \\bD \\bV^T \\\\\n    &= ( \\bU  \\bV^T ) ( \\bV \\bD \\bV^T ) \\\\\n    &= \\bQ \\bB \\\\\n\\end{split}\n\\]\nnon-negative \\(\\bD\\). Note decomposition unique sign ambiguities columns \\(\\bU\\) \\(\\bV\\) cancel \\(\\bQ\\) \\(\\bB\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"cholesky-decomposition","chapter":"A Brief refresher on matrices","heading":"A.7.5 Cholesky decomposition","text":"symmetric positive definite matrix \\(\\bA\\) can decomposed product\ntriangular matrix \\(\\bL\\) transpose\n\\[\n\\bA = \\bL \\bL^T \\,.\n\\]\n, \\(\\bL\\) lower triangular matrix positive diagonal elements.decomposition unique called Cholesky factorisation. \noften used check whether symmetric matrix positive definite algorithmically\nless demanding eigenvalue decomposition.Note implementations Cholesky decomposition (e.g. R) use\nupper triangular matrices \\(\\bK\\) positive diagonal \n\\(\\bA = \\bK^T \\bK\\) \\(\\bL = \\bK^T\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-summaries-based-on-eigenvalues-and-singular-values","chapter":"A Brief refresher on matrices","heading":"A.8 Matrix summaries based on eigenvalues and singular values","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"trace-and-determinant-computed-from-eigenvalues","chapter":"A Brief refresher on matrices","heading":"A.8.1 Trace and determinant computed from eigenvalues","text":"eigendecomposition \\(\\bA=\\bU \\bLambda \\bU^{-1}\\)\nallows establish link trace determinant eigenvalues.Specifically,\n\\[\n\\begin{split}\n\\trace(\\bA) & = \\trace(\\bU \\bLambda \\bU^{-1}  ) =\n\\trace( \\bLambda \\bU^{-1} \\bU  ) \\\\\n &= \\trace( \\bLambda ) = \\sum_{=1}^d \\lambda_i \\\\\n\\end{split}\n\\]\nthus trace square matrix \\(\\bA\\) equal sum eigenvalues. Likewise,\n\\[\n\\begin{split}\n\\det(\\bA) & = \\det(\\bU) \\det(\\bLambda) \\det(\\bU^{-1}  ) \\\\\n &=\\det( \\bLambda) = \\prod_{=1}^d \\lambda_i \\\\\n\\end{split}\n\\]\ntherefore determinant \\(\\bA\\) product eigenvalues.relationship eigenvalues trace determinant\ndemonstrated diagonisable non-defective matrices.\nHowever, hold also general matrix. can shown using certain non-diagonal matrix decompositions (e.g. Jordan decomposition).result, eigenvalues equal zero \\(\\det(\\bA) = 0\\) hence \\(\\bA\\) singular invertible.trace determinant real matrix always real even though individual eigenvalues may complex.","code":""},{"path":"brief-refresher-on-matrices.html","id":"rank-and-condition-number","chapter":"A Brief refresher on matrices","heading":"A.8.2 Rank and condition number","text":"rank dimension space spanned column row vectors. rectangular matrix dimension \\(n \\times d\\) \nrank \\(m = \\min(n, d)\\), maximum indeed achieved full rank.condition number describes well- ill-conditioned\nfull rank matrix . example, square matrix large condition number implies matrix close singular\nthus ill-conditioned.\ncondition number infinite matrix full rank.rank condition matrix can determined \\(m\\) singular values \\(d_1, \\ldots, d_m\\) matrix obtained SVD:rank number non-zero singular values.condition number ratio largest singular value\ndivided smallest singular value (absolute values signs allowed).square matrix \\(\\bA\\) singular condition number infinite, full rank.\nhand, non-singular square matrix, \npositive definite matrix, full rank.","code":""},{"path":"brief-refresher-on-matrices.html","id":"functions-of-symmetric-matrices","chapter":"A Brief refresher on matrices","heading":"A.9 Functions of symmetric matrices","text":"focus symmetric square matrices \\(\\bA=\\bU \\bLambda \\bU^T\\) always diagonisable real eigenvalues \\(\\bLambda\\) orthogonal eigenvectors \\(\\bU\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"definition-of-a-matrix-function","chapter":"A Brief refresher on matrices","heading":"A.9.1 Definition of a matrix function","text":"Assume real-valued function \\(f()\\) real number \\(\\). corresponding\nmatrix function \\(f(\\bA)\\)\ndefined \n\\[\nf(\\bA) =  \\bU f(\\bLambda) \\bU^T =  \\bU \\begin{pmatrix}\n    f(\\lambda_{1}) & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & f(\\lambda_{d})\n\\end{pmatrix} \\bU^T\n\\]\nfunction \\(f()\\) applied eigenvalues \\(\\bA\\).\nconstruction \\(f(\\bA)\\) real, symmetric \nreal eigenvalues \\(f(\\lambda_i)\\).Examples:Example .2  Matrix power: \\(f() = ^p\\) (\\(p\\) real number)Special cases matrix power include :Matrix inversion: \\(f() = ^{-1}\\)\nNote matrix \\(\\bA\\) singular, .e. contains one eigenvalues \\(\\lambda_i=0\\),\n\\(\\bA^{-1}\\) defined therefore \\(\\bA\\) invertible.However, -called pseudoinverse can still computed, inverting non-zero eigenvalues, \nkeeping zero eigenvalues zero.Matrix square root: \\(f() = ^{1/2}\\)\nSince multiple solutions square root also multiple\nmatrix square roots. principal matrix square root obtained using\npositive square roots eigenvalues. Thus principal matrix square root\npositive semi-definite matrix also positive semi-definite unique.Example .3  Matrix exponential: \\(f() = \\exp()\\)\nNote \\(\\exp() \\geq 0\\) real \\(\\) matrix \\(\\exp(\\bA)\\) positive\nsemi-definite. Thus, matrix exponential can used generate positive semi-definite\nmatrices.\\(\\bA\\) \\(\\bB\\) commute, .e. \\(\\bA \\bB = \\bB \\bA\\), \n\\(\\exp(\\bA+\\bB) = \\exp(\\bA) \\exp(\\bB)\\). However, case\notherwise!Example .4  Matrix logarithm: \\(f() = \\log()\\)\nlogarithm requires \\(>0\\) matrix \\(\\bA\\) needs positive definite\n\\(\\log(\\bA)\\) defined.","code":""},{"path":"brief-refresher-on-matrices.html","id":"identities-for-the-matrix-exponential-and-logarithm","chapter":"A Brief refresher on matrices","heading":"A.9.2 Identities for the matrix exponential and logarithm","text":"give rise useful identities:symmetric matrix \\(\\bA\\) \n\\[\n\\det(\\exp(\\bA)) = \\exp(\\trace(\\bA))\n\\]\n\n\\(\\prod_i \\exp(\\lambda_i) = \\exp( \\sum_i \\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\bA\\).symmetric matrix \\(\\bA\\) \n\\[\n\\det(\\exp(\\bA)) = \\exp(\\trace(\\bA))\n\\]\n\n\\(\\prod_i \\exp(\\lambda_i) = \\exp( \\sum_i \\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\bA\\).take logarithm sides replace \\(\\exp(\\bA)=\\bB\\) get another\nidentity symmetric positive definite matrix \\(\\bB\\):\n\\[\n\\log \\det(\\bB) = \\trace(\\log(\\bB))\n\\]\n\n\\(\\log( \\prod_i \\lambda_i) = \\sum_i \\log(\\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\bB\\).take logarithm sides replace \\(\\exp(\\bA)=\\bB\\) get another\nidentity symmetric positive definite matrix \\(\\bB\\):\n\\[\n\\log \\det(\\bB) = \\trace(\\log(\\bB))\n\\]\n\n\\(\\log( \\prod_i \\lambda_i) = \\sum_i \\log(\\lambda_i)\\)\n\\(\\lambda_i\\) eigenvalues \\(\\bB\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"matrix-calculus","chapter":"A Brief refresher on matrices","heading":"A.10 Matrix calculus","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"first-order-vector-derivatives","chapter":"A Brief refresher on matrices","heading":"A.10.1 First order vector derivatives","text":"","code":""},{"path":"brief-refresher-on-matrices.html","id":"gradient","chapter":"A Brief refresher on matrices","heading":"A.10.1.1 Gradient","text":"nabla operator (also known del operator) row vector\n\\[\n\\nabla =  (\\frac{\\partial}{\\partial x_1}, \\ldots, \n\\frac{\\partial}{\\partial x_d}) = \\frac{\\partial}{\\partial \\bx}\n\\]\ncontaining\nfirst order partial derivative operators.gradient scalar-valued function\n\\(f(\\bx)\\) vector argument \\(\\bx = (x_1, \\ldots, x_d)^T\\)\nalso row vector (\\(d\\) columns) \ncan expressed using nabla operator\n\\[\n\\begin{split}\n\\nabla f(\\bx) &= \\left( \\frac{\\partial f(\\bx)}{\\partial x_1}, \\ldots, \n\\frac{\\partial f(\\bx)}{\\partial x_d} \\right) \\\\\n& = \n \\frac{\\partial f(\\bx)}{\\partial \\bx} = \\text{grad} f(\\bx) \\, .\\\\\n\\end{split}\n\\]\nNote various notations gradient.Example .5  \\(f(\\bx)=\\ba^T \\bx + b\\). \\(\\nabla f(\\bx) = \\frac{\\partial f(\\bx)}{\\partial \\bx} = \\ba^T\\).Example .6  \\(f(\\bx)=\\bx^T \\bx\\). \\(\\nabla f(\\bx) = \\frac{\\partial f(\\bx)}{\\partial \\bx} = 2 \\bx^T\\).Example .7  \\(f(\\bx)=\\bx^T \\bA \\bx\\). \\(\\nabla f(\\bx) = \\frac{\\partial f(\\bx)}{\\partial \\bx} = \\bx^T (\\bA + \\bA^T)\\).","code":""},{"path":"brief-refresher-on-matrices.html","id":"jacobian-matrix","chapter":"A Brief refresher on matrices","heading":"A.10.1.2 Jacobian matrix","text":"vector-valued function\n\\[\n\\boldf(\\bx) = ( f_1(\\bx), \\ldots, f_m(\\bx) )^T \\,.\n\\]\ncomputation gradient component yields\nJacobian matrix (\\(m\\) rows \\(d\\) columns)\n\\[\n\\begin{split}\n D \\boldf(\\bx) &= \n\\left( {\\begin{array}{c}\n \\nabla f_1(\\bx)   \\\\\n \\vdots   \\\\\n \\nabla f_m(\\bx)   \\\\\n \\end{array} } \\right) \\\\\n& = \\left(\\frac{\\partial f_i(\\bx)}{\\partial x_j}\\right) \\\\\n&= \\frac{\\partial \\boldf(\\bx)}{\\partial \\bx} \\\\\n\\end{split}\n\\]\n, note various notations Jacobian matrix!Example .8  \\(\\boldf(\\bx)=\\bA \\bx + \\bb\\). \\(D \\boldf(\\bx) = \\frac{\\partial \\boldf(\\bx)}{\\partial \\bx} = \\bA\\).\\(m=d\\) Jacobian matrix square matrix allows compute \nJacobian determinant \\[\\det  D \\boldf(\\bx) = \\det\\left(\\frac{\\partial \\boldf(\\bx)}{\\partial \\bx}\\right)\\]\\(\\= \\boldf(\\bx)\\) invertible function \\(\\bx = \\boldf^{-1}(\\)\\)\nJacobian matrix invertible inverted matrix fact \nJacobian inverse function!allows compute Jacobian determinant backtransformation \ninverse Jacobian determinant original function:\n\\[\\det  D \\boldf^{-1}(\\) = ( \\det  D \\boldf(\\bx) )^{-1}\\]\nalternative notation\n\\[\\det  D \\bx(\\) = \\frac{1}{ \\det  D \\(\\bx) }\\].","code":""},{"path":"brief-refresher-on-matrices.html","id":"second-order-vector-derivatives","chapter":"A Brief refresher on matrices","heading":"A.10.2 Second order vector derivatives","text":"matrix second order partial derivates scalar-valued\nfunction vector-valued argument called Hessian matrix\ncomputed double application nabla operator:\n\\[\n\\begin{split}\n\\nabla^T \\nabla f(\\bx) &=\n\\begin{pmatrix}\n  \\frac{\\partial^2 f(\\bx)}{\\partial x_1^2}\n     & \\frac{\\partial^2 f(\\bx)}{\\partial x_1 \\partial x_2} \n     & \\cdots \n     & \\frac{\\partial^2 f(\\bx)}{\\partial x_1 \\partial x_d} \\\\\n  \\frac{\\partial^2 f(\\bx)}{\\partial x_2 \\partial x_1} \n     & \\frac{\\partial^2 f(\\bx)}{\\partial x_2^2}\n     & \\cdots \n     & \\frac{\\partial^2 f(\\bx)}{\\partial x_2 \\partial x_d} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\frac{\\partial^2 f(\\bx)}{\\partial x_d \\partial x_1} \n     & \\frac{\\partial^2 f(\\bx)}{\\partial x_d \\partial x_2}  \n     & \\cdots \n     & \\frac{\\partial^2 f(\\bx)}{\\partial x_d^2}\n \\end{pmatrix} \\\\\n& = \\left(\\frac{\\partial f(\\bx)}{\\partial x_i \\partial x_j}\\right)  \n  = {\\left(\\frac{\\partial}{\\partial \\bx}\\right)}^T \\frac{\\partial f(\\bx)}{\\partial \\bx}\n\\,.\\\\\n\\end{split}\n\\]\nconstruction square symmetric.","code":""},{"path":"brief-refresher-on-matrices.html","id":"first-order-matrix-derivatives","chapter":"A Brief refresher on matrices","heading":"A.10.3 First order matrix derivatives","text":"derivative scalar-valued function \\(f(\\bX)\\) regard matrix argument \\(\\bX\\)\ncan also defined results matrix\ntransposed dimensions compared \\(\\bX\\).Two important specific examples :Example .9  \\(\\frac{\\partial \\trace(\\bA \\bX)}{\\partial \\bX} = \\bA\\)Example .10  \\(\\frac{\\partial \\log \\det(\\bX)}{\\partial \\bX} = \\frac{\\partial \\trace(\\log \\bX)}{\\partial \\bX} = \\bX^{-1}\\)","code":""},{"path":"further-study.html","id":"further-study","chapter":"B Further study","heading":"B Further study","text":"module can touch surface field multivariate statistics machine learning. like study \nrecommend following books starting point.","code":""},{"path":"further-study.html","id":"recommended-reading","chapter":"B Further study","heading":"B.1 Recommended reading","text":"multivariate statistics machine learning:Härdle Simar (2015) Applied multivariate statistical analysis. 4th edition. Springer.Hastie, Tibshirani, Friedman (2009) elements statistical learning: data mining, inference, prediction. Springer.James et al. (2013) introduction statistical learning applications R. Springer.Marden (2015) Multivariate Statistics: Old SchoolRogers Girolami (2017) first course machine learning (2nd Edition). Chapman Hall / CRC.","code":""},{"path":"further-study.html","id":"advanced-reading","chapter":"B Further study","heading":"B.2 Advanced reading","text":"Additional (advanced) reference books probabilistic machine learning :Murphy (2012) Machine learning: probabilistic perspective. MIT Press.Bishop (2006) Pattern recognition machine learning. Springer.","code":""},{"path":"bibliography.html","id":"bibliography","chapter":"Bibliography","heading":"Bibliography","text":"Bishop, C. M. 2006. Pattern Recognition Machine Learning. Springer. https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/.Härdle, W. K., L. Simar. 2015. Applied Multivariate Statistical Analysis. Berlin: Springer.Hastie, T., R. Tibshirani, J. Friedman. 2009. Elements Statistical Learning: Data Mining, Inference, Prediction. 2nd ed. Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.James, G., D. Witten, T. Hastie, R. Tibshirani. 2013. Introduction Statistical Learning Applications R. Springer. http://faculty.marshall.usc.edu/gareth-james/ISL/.Marden, J. . 2015. Multivariate Statistics: Old School. CreateSpace. http://stat.istics.net/Multivariate.Murphy, K. P. 2012. Machine Learning: Probabilistic Perspective. MIT Press.Rogers, S., M. Girolami. 2017. First Course Machine Learning. 2nd ed. Chapman; Hall / CRC.Zhang, ., Z. C. Lipton, M. Li, . J. Smola. 2020. Dive Deep Learning. https://d2l.ai.","code":""}]
