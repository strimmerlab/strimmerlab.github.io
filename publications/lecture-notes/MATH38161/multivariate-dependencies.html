<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>6 Multivariate dependencies | Multivariate Statistics and Machine Learning</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="6 Multivariate dependencies | Multivariate Statistics and Machine Learning">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="6 Multivariate dependencies | Multivariate Statistics and Machine Learning">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<meta name="description" content="6.1 Measuring the linear association between two sets of random variables  6.1.1 Aim The linear association between two scalar random variables \(x\) and \(y\) is measured by the correlation...">
<meta property="og:description" content="6.1 Measuring the linear association between two sets of random variables  6.1.1 Aim The linear association between two scalar random variables \(x\) and \(y\) is measured by the correlation...">
<meta name="twitter:description" content="6.1 Measuring the linear association between two sets of random variables  6.1.1 Aim The linear association between two scalar random variables \(x\) and \(y\) is measured by the correlation...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Multivariate Statistics and Machine Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="multivariate-random-variables.html"><span class="header-section-number">1</span> Multivariate random variables</a></li>
<li><a class="" href="multivariate-estimation-in-large-sample-and-small-sample-settings.html"><span class="header-section-number">2</span> Multivariate estimation in large sample and small sample settings</a></li>
<li><a class="" href="transformations-and-dimension-reduction.html"><span class="header-section-number">3</span> Transformations and dimension reduction</a></li>
<li><a class="" href="unsupervised-learning-and-clustering.html"><span class="header-section-number">4</span> Unsupervised learning and clustering</a></li>
<li><a class="" href="supervised-learning-and-classification.html"><span class="header-section-number">5</span> Supervised learning and classification</a></li>
<li><a class="active" href="multivariate-dependencies.html"><span class="header-section-number">6</span> Multivariate dependencies</a></li>
<li><a class="" href="nonlinear-and-nonparametric-models.html"><span class="header-section-number">7</span> Nonlinear and nonparametric models</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="further-study.html"><span class="header-section-number">A</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="multivariate-dependencies" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Multivariate dependencies<a class="anchor" aria-label="anchor" href="#multivariate-dependencies"><i class="fas fa-link"></i></a>
</h1>
<div id="measuring-the-linear-association-between-two-sets-of-random-variables" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Measuring the linear association between two sets of random variables<a class="anchor" aria-label="anchor" href="#measuring-the-linear-association-between-two-sets-of-random-variables"><i class="fas fa-link"></i></a>
</h2>
<div id="aim" class="section level3" number="6.1.1">
<h3>
<span class="header-section-number">6.1.1</span> Aim<a class="anchor" aria-label="anchor" href="#aim"><i class="fas fa-link"></i></a>
</h3>
<p>The linear association between two scalar random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is measured
by the correlation <span class="math inline">\(\text{Cor}(x, y) = \rho\)</span>.</p>
<p>In this chapter we now would like to explore how to generalise correlation to the case of two random vectors.
Specifically, we would like to find a scalar measure
of association between two random vectors
(or equivalently two sets of random variables) <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_p)^T\)</span> and
<span class="math inline">\(\boldsymbol y= (y_1, \ldots, y_q)^T\)</span> that contains correlation and also multiple correlation
as special case.</p>
<p>We assume a joint correlation matrix
<span class="math display">\[
\boldsymbol P=
\begin{pmatrix}
\boldsymbol P_{\boldsymbol x} &amp;  \boldsymbol P_{\boldsymbol x\boldsymbol y} \\
\boldsymbol P_{\boldsymbol y\boldsymbol x} &amp; \boldsymbol P_{\boldsymbol y} \\
\end{pmatrix}
\]</span>
with cross-correlation matrix <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol y} = \boldsymbol P_{\boldsymbol y\boldsymbol x}^T\)</span>
and the within-group group correlations <span class="math inline">\(\boldsymbol P_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol P_{\boldsymbol y}\)</span>.
If the cross-correlations vanish, <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol y} =0\)</span>, then
the two random vectors are uncorrelated, and the joint correlation matrix
becomes a diagonal block matrix
<span class="math display">\[
\boldsymbol P_{\text{indep}} =
\begin{pmatrix}
\boldsymbol P_{\boldsymbol x} &amp;  0 \\
0 &amp; \boldsymbol P_{\boldsymbol y} \\
\end{pmatrix} \, .
\]</span></p>
<p>To characterise the total association between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> we are looking for a <strong>scalar
quantity</strong> measuring the divergence of a distribution assuming the general joint correlation matrix <span class="math inline">\(\boldsymbol P\)</span>
from a distribution assuming the joint correlation matrix <span class="math inline">\(\boldsymbol P_{\text{indep}}\)</span> for uncorrelated
<span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>.</p>
</div>
<div id="known-special-cases" class="section level3" number="6.1.2">
<h3>
<span class="header-section-number">6.1.2</span> Known special cases<a class="anchor" aria-label="anchor" href="#known-special-cases"><i class="fas fa-link"></i></a>
</h3>
<p>Ideally, in case of an univariate <span class="math inline">\(y\)</span> but multivariate <span class="math inline">\(\boldsymbol x\)</span>
this measure should reduce to the
<em>squared multiple correlation</em> or <em>coefficient of determination</em>
<span class="math display">\[
\text{MCor}(\boldsymbol x, y)^2 = \boldsymbol P_{y\boldsymbol x} \boldsymbol P_{\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy}
\]</span>
This is well-known in linear regression to describe the strength of the total linear association between the
predictors <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_p)^T\)</span> and the response <span class="math inline">\(y\)</span>.</p>
<p>To derive the squared multiple correlation we may proceed as follows. First we whiten
the random vector <span class="math inline">\(\boldsymbol x\)</span> resulting in <span class="math inline">\(\boldsymbol z_{\boldsymbol x} = \boldsymbol W_{\boldsymbol x}\boldsymbol x=\boldsymbol Q_{\boldsymbol x}\boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol V_{\boldsymbol x}^{-1/2}\boldsymbol x\)</span>
where <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}\)</span> is an orthogonal matrix.
The correlations between each component in <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span> and the response <span class="math inline">\(y\)</span> are then
<span class="math display">\[
\boldsymbol \omega= \text{Cor}(\boldsymbol z_{\boldsymbol x}, y) = \boldsymbol Q_{\boldsymbol x} \boldsymbol P_{\boldsymbol x}^{-1/2} \boldsymbol P_{\boldsymbol xy}
\]</span>
As <span class="math inline">\(\text{Var}(\boldsymbol z_{\boldsymbol x}) = \boldsymbol I\)</span> and thus the components in <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span> are uncorrelated we can simply add up the squared individual correlations to get as total association measure
<span class="math display">\[
\boldsymbol \omega^T \boldsymbol \omega= \sum_i \omega_i^2 = \boldsymbol P_{y\boldsymbol x} \boldsymbol P_{\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy}
\]</span>
Note that the particular choice of the orthogonal matrix <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}\)</span> for whitening <span class="math inline">\(\boldsymbol x\)</span> is not relevant for the squared multiple correlation.</p>
<p>Note that if the cross-correlations
vanish (<span class="math inline">\(\boldsymbol P_{\boldsymbol xy} =0\)</span>) then <span class="math inline">\(\text{MCor}(\boldsymbol x, y)^2=0\)</span>. If the correlation between the predictors
vanishes (<span class="math inline">\(\boldsymbol P_{\boldsymbol x} = \boldsymbol I\)</span>) then <span class="math inline">\(\text{MCor}(\boldsymbol x, y)^2 = \sum_i \rho_{y x_i}^2\)</span>, i.e. it is the
sum of the squared cross-correlations.</p>
<p>If there is only a single predictor <span class="math inline">\(x\)</span> then <span class="math inline">\(\boldsymbol P_{xy}=\rho\)</span> and <span class="math inline">\(\boldsymbol P_{x} = 1\)</span>
and the squared multiple correlation reduces to the squared Pearson correlation
<span class="math display">\[
\text{Cor}(x, y)^2 = \rho^2 \, .
\]</span></p>
</div>
</div>
<div id="canonical-correlation-analysis-cca-aka-cca-whitening" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Canonical Correlation Analysis (CCA) aka CCA whitening<a class="anchor" aria-label="anchor" href="#canonical-correlation-analysis-cca-aka-cca-whitening"><i class="fas fa-link"></i></a>
</h2>
<p>Canonical correlation analysis was invented by Harald Hotelling in 1936 <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Hotelling, H. 1936. Relations between two sets of variates. Biometrika &lt;strong&gt;28&lt;/strong&gt;:321–377. &lt;a href="https://doi.org/10.1093/biomet/28.3-4.321" class="uri"&gt;https://doi.org/10.1093/biomet/28.3-4.321&lt;/a&gt;&lt;/p&gt;'><sup>20</sup></a>. CCA aims to characterise the linear dependence between to random vectors <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> by a set of <strong>canonical correlations</strong> <span class="math inline">\(\lambda_i\)</span>.</p>
<p>CCA works by simultaneously whitening the two random vectors <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> where the whitening matrices are chosen in such a way that the cross-correlation matrix between the resulting whitened variables
becomes diagonal, and the elements on the diagonal correspond to the <strong>canonical correlations</strong>.</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\boldsymbol x= \begin{pmatrix} x_1 \\ \vdots \\ x_p \end{pmatrix} \\
\text{Dimension } p
\end{array}
\begin{array}{ll}
\boldsymbol y= \begin{pmatrix} y_1 \\ \vdots \\ y_q \end{pmatrix} \\
\text{Dimension } q
\end{array}
\begin{array}{ll}
\text{Var}(\boldsymbol x) = \boldsymbol \Sigma_{\boldsymbol x} = \boldsymbol V_{\boldsymbol x}^{1/2}\boldsymbol P_{\boldsymbol x}\boldsymbol V_{\boldsymbol x}^{1/2} \\
\text{Var}(\boldsymbol y) = \boldsymbol \Sigma_{\boldsymbol y} = \boldsymbol V_{\boldsymbol y}^{1/2}\boldsymbol P_{\boldsymbol y}\boldsymbol V_{\boldsymbol y}^{1/2} \\
\end{array}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cc}
\text{Whitening of } \boldsymbol x\text{:} \\
\text{Whitening of } \boldsymbol y\text{:}
\end{array}
\begin{array}{cc}
\boldsymbol z_{\boldsymbol x} = \boldsymbol W_{\boldsymbol x}\boldsymbol x=\boldsymbol Q_{\boldsymbol x}\boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol V_{\boldsymbol x}^{-1/2}\boldsymbol x\\
\boldsymbol z_{\boldsymbol y} = \boldsymbol W_{\boldsymbol y}\boldsymbol y=\boldsymbol Q_{\boldsymbol y}\boldsymbol P_{\boldsymbol y}^{-1/2}\boldsymbol V_{\boldsymbol y}^{-1/2}\boldsymbol y
\end{array}
\end{align*}\]</span>
(note we use the correlation-based form of <span class="math inline">\(\boldsymbol W\)</span>)</p>
<p>Cross-correlation between <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span> and <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span>:</p>
<p><span class="math display">\[\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})=\boldsymbol Q_{\boldsymbol x}\boldsymbol K\boldsymbol Q_{\boldsymbol y}^T\]</span></p>
<p>with <span class="math inline">\(\boldsymbol K= \boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol P_{\boldsymbol x\boldsymbol y}\boldsymbol P_{\boldsymbol y}^{-1/2}\)</span>.</p>
<p><strong>Idea</strong>: we can choose suitable orthogonal matrices <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}\)</span> by putting a structural constraint on the cross-correlation matrix.</p>
<p><strong>CCA</strong>: we aim for a <em>diagonal</em> <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> so that each component in <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span> only influences one (the corresponding) component in <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span>.</p>
<p><strong>Motivation</strong>: pairs of “modules” represented by components of <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span>
and <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span> influencing each other (and not anyone other module).</p>
<p><span class="math display">\[
\begin{array}{ll}
\boldsymbol z_{\boldsymbol x} = \begin{pmatrix} z^x_1 \\ z^x_2 \\ \vdots \\ z^x_p \end{pmatrix} &amp;
\boldsymbol z_{\boldsymbol y} = \begin{pmatrix} z^y_1 \\ z^y_2 \\ \vdots \\ z^y_q \end{pmatrix} \\
\end{array}
\]</span></p>
<p><span class="math display">\[\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y}) = \begin{pmatrix} \lambda_1 &amp; \dots &amp; 0 \\ \vdots &amp;  \vdots \\ 0 &amp; \dots &amp; \lambda_m \end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(\lambda_i\)</span> are the <em>canonical correlations</em> and <span class="math inline">\(m=\min(p,q)\)</span>.</p>
<div id="how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal" class="section level3" number="6.2.1">
<h3>
<span class="header-section-number">6.2.1</span> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?<a class="anchor" aria-label="anchor" href="#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Use Singular Value Decomposition (SVD) of matrix <span class="math inline">\(\boldsymbol K\)</span>:<br><span class="math display">\[\boldsymbol K= (\boldsymbol Q_{\boldsymbol x}^{\text{CCA}})^T  \boldsymbol \Lambda\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\]</span>
where <span class="math inline">\(\boldsymbol \Lambda\)</span> is the diagonal matrix containing the singular values of <span class="math inline">\(\boldsymbol K\)</span>
</li>
<li>This yields orthogonal matrices <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> and thus the desired whitening matrices <span class="math inline">\(\boldsymbol W_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol W_{\boldsymbol y}^{\text{CCA}}\)</span>
</li>
<li>As a result <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y}) = \boldsymbol \Lambda\)</span> i.e. singular values <span class="math inline">\(\lambda_i\)</span> of <span class="math inline">\(\boldsymbol K\)</span> are the desired canonical correlations!</li>
</ul>
<p><span class="math inline">\(\longrightarrow\)</span> <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> are determined by the diagonality constraint (and note these are different to the other previously discussed whitening methods).</p>
<p>Note that the signs of corresponding in columns in <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> are not identified. Traditionally, in an SVD the
signs are chosen such that the singular values are positive. However, if we
impose positive-diagonality on <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span>,
and thus positive-diagonality on the cross-correlations <span class="math inline">\(\boldsymbol \Psi_{\boldsymbol x}\)</span> and
<span class="math inline">\(\boldsymbol \Psi_{\boldsymbol y}\)</span>, then the canonical correlations may take on both positive and
negative values.</p>
</div>
<div id="related-methods" class="section level3" number="6.2.2">
<h3>
<span class="header-section-number">6.2.2</span> Related methods<a class="anchor" aria-label="anchor" href="#related-methods"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>O2PLS: similar to CCA but using orthogonal projections rather than whitening.</p></li>
<li><p>Vector correlation: aggregates the squared canonical correlations into a single overall measure (see below).</p></li>
</ul>
</div>
</div>
<div id="vector-correlation-and-rv-coefficient" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Vector correlation and RV coefficient<a class="anchor" aria-label="anchor" href="#vector-correlation-and-rv-coefficient"><i class="fas fa-link"></i></a>
</h2>
<div id="vector-alienation-coefficient" class="section level3" number="6.3.1">
<h3>
<span class="header-section-number">6.3.1</span> Vector alienation coefficient<a class="anchor" aria-label="anchor" href="#vector-alienation-coefficient"><i class="fas fa-link"></i></a>
</h3>
<p>In his 1936 paper introducing canonical correlation analysis
Hotelling also proposed the <em>vector alienation
coefficient</em> defined as
<span class="math display">\[
\begin{split}
a(\boldsymbol x, \boldsymbol y) &amp;= \frac{\det(\boldsymbol P)}{\det(\boldsymbol P_{\text{indep}}) } \\
            &amp; = \frac{\det( \boldsymbol P) }{  \det(\boldsymbol P_{\boldsymbol x}) \,  \det(\boldsymbol P_{\boldsymbol y})  }
\end{split}
\]</span>
With <span class="math inline">\(\boldsymbol K= \boldsymbol P_{\boldsymbol x}^{-1/2} \boldsymbol P_{\boldsymbol x\boldsymbol y} \boldsymbol P_{\boldsymbol y}^{-1/2}\)</span> the vector alienation coefficient can be written
(using the Weinstein-Aronszajn determinant identity and the formula for the determinant
of block-structured matrices) as
<span class="math display">\[
\begin{split}
a(\boldsymbol x, \boldsymbol y) &amp; = \det \left( \boldsymbol I_p - \boldsymbol K\boldsymbol K^T \right) \\
            &amp; = \det \left( \boldsymbol I_q - \boldsymbol K^T \boldsymbol K\right) \\
            &amp;= \prod_{i=1}^m (1-\lambda_i^2) \\
\end{split}
\]</span>
where the <span class="math inline">\(\lambda_i\)</span> are the singular values of <span class="math inline">\(\boldsymbol K\)</span>, i.e. the canonical correlations
for the pair <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>. Therefore, the vector alienation coefficient is computed as
a summary statistic of the canonical correlations.</p>
<p>If <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol y} = 0\)</span> und thus <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are uncorrelated then <span class="math inline">\(\boldsymbol P= \boldsymbol P_{\text{indep}}\)</span> and
thus by construction the vector alienation coefficient <span class="math inline">\(a(\boldsymbol x, \boldsymbol y)=1\)</span>.
Hence, the vector alienation coefficient is itself not a generalisation of the squared multiple correlation
to the case of two random vectors as such a quantity should vanish in this case.</p>
</div>
<div id="rozeboom-vector-correlation" class="section level3" number="6.3.2">
<h3>
<span class="header-section-number">6.3.2</span> Rozeboom vector correlation<a class="anchor" aria-label="anchor" href="#rozeboom-vector-correlation"><i class="fas fa-link"></i></a>
</h3>
<p>Instead, Rozeboom (1965) <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Rozeboom, W. W. 1965. Linear correlations between sets of variables.
Psychometrika &lt;strong&gt;30&lt;/strong&gt;:57–71. &lt;a href="https://doi.org/10.1007/BF02289747" class="uri"&gt;https://doi.org/10.1007/BF02289747&lt;/a&gt;&lt;/p&gt;'><sup>21</sup></a> proposed to use as squared <em>vector correlation</em>
the complement of the vector alienation coefficient
<span class="math display">\[
\begin{split}
\text{VCor}(\boldsymbol x, \boldsymbol y)^2 &amp;= \rho^2_{\boldsymbol x\boldsymbol y} = 1 - a(\boldsymbol x, \boldsymbol y) \\
&amp; = 1- \det\left( \boldsymbol I_p - \boldsymbol K\boldsymbol K^T \right) \\
&amp; = 1- \det\left( \boldsymbol I_q - \boldsymbol K^T \boldsymbol K\right) \\
&amp;  =1- \prod_{i=1}^m (1-\lambda_i^2) \\
\end{split}
\]</span>
If <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol y} = 0\)</span> then <span class="math inline">\(\boldsymbol K=\boldsymbol 0\)</span> and hence <span class="math inline">\(\text{VCor}(\boldsymbol x, \boldsymbol y)^2 = 0\)</span>.</p>
<p>Moreover, if either <span class="math inline">\(p=1\)</span> or <span class="math inline">\(q=1\)</span> the squared vector correlation
reduces to the corresponding squared multiple correlation,
which in turn for both <span class="math inline">\(p=1\)</span> and <span class="math inline">\(q=1\)</span> becomes the squared Pearson correlation.</p>
<p>You can find the derivation in Example Sheet 10.</p>
<p>Thus, Rozeboom’s vector correlation indeed generalises both Pearson correlation
and the multiple correlation coefficient.</p>
</div>
<div id="rv-coefficient" class="section level3" number="6.3.3">
<h3>
<span class="header-section-number">6.3.3</span> RV coefficient<a class="anchor" aria-label="anchor" href="#rv-coefficient"><i class="fas fa-link"></i></a>
</h3>
<p>Another common approach to measure association between two random vectors is the <a href="https://en.wikipedia.org/wiki/RV_coefficient">RV coefficient</a> introduced by Robert and Escoufier in 1976 as
<span class="math display">\[
RV(\boldsymbol x, \boldsymbol y) = \frac{ \text{Tr}(\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} \boldsymbol \Sigma_{\boldsymbol y\boldsymbol x} )}{ \sqrt{ \text{Tr}(\boldsymbol \Sigma_{\boldsymbol x}^2) \text{Tr}(\boldsymbol \Sigma_{\boldsymbol y}^2)  } }
\]</span>
The main advantage of the RV coefficient is that it is easier to compute than the Rozeboom vector correlation as it uses the matrix trace rather than the matrix determinant.</p>
<p>For <span class="math inline">\(q=p=1\)</span> the RV coefficient reduces to the squared correlation.
However, the RV coefficient does <em>not</em> reduce to the multiple correlation coefficient
for <span class="math inline">\(q=1\)</span> and <span class="math inline">\(p &gt; 1\)</span>, and therefore the RV coefficient cannot be considered a coherent generalisation
of Pearson and multiple correlation to the case when <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are random vectors.</p>
<p>See also Worksheet 10.</p>
</div>
</div>
<div id="limits-of-linear-models-and-correlation" class="section level2" number="6.4">
<h2>
<span class="header-section-number">6.4</span> Limits of linear models and correlation<a class="anchor" aria-label="anchor" href="#limits-of-linear-models-and-correlation"><i class="fas fa-link"></i></a>
</h2>
<div id="correlation-measures-only-linear-dependence" class="section level3" number="6.4.1">
<h3>
<span class="header-section-number">6.4.1</span> Correlation measures only linear dependence<a class="anchor" aria-label="anchor" href="#correlation-measures-only-linear-dependence"><i class="fas fa-link"></i></a>
</h3>
<p>Linear models and measures of linear association (correlation) are very effective tools. However, it is important
to recognise their limits especially when modelling nonlinear relationships.</p>
<p>A very simple demonstration of this is given by the following example. Assume <span class="math inline">\(x\)</span> is a normally distributed
random variable with <span class="math inline">\(x \sim N(0, \sigma^2)\)</span> with mean zero and some variance <span class="math inline">\(\sigma^2\)</span>. From <span class="math inline">\(x\)</span> we construct a second random variable <span class="math inline">\(y = x^2\)</span>. Despite that <span class="math inline">\(y\)</span> is a function of <span class="math inline">\(x\)</span> with no extra added noise it is easy to show that <span class="math inline">\(\text{Cov}(x, y) = \text{Cov}(x, x^2)=0\)</span>.
Hence, the correlation <span class="math inline">\(\text{Cor}(x, y)= \text{Cor}(x, x^2) = 0\)</span> also vanishes.</p>
<p>This can be empirically verified by simulating data from a normal distribution (here with <span class="math inline">\(\sigma^2=4)\)</span>
and estimating the correlation:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000000</span>, mean<span class="op">=</span><span class="fl">0</span>, sd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">=</span> <span class="va">x</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">x</span>,<span class="va">y</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.001217943</code></pre>
<p>Thus, correlation is zero even though <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are fully dependent variables.
This is because correlation only measures linear association, and the relationship between
<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is nonlinear.</p>
</div>
<div id="anscombe-data-sets" class="section level3" number="6.4.2">
<h3>
<span class="header-section-number">6.4.2</span> Anscombe data sets<a class="anchor" aria-label="anchor" href="#anscombe-data-sets"><i class="fas fa-link"></i></a>
</h3>
<p>Using correlation (and more generally linear models) blindly can easily hide the underlying complexity of the analysed data. This is demonstrated by the classic “Anscombe quartet” of data sets presented in his 1973 paper <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Anscombe, F. J. 1973. Graphs in statistical analysis.
The American Statistician 27:17-21, &lt;a href="http://doi.org/10.1080/00031305.1973.10478966" class="uri"&gt;http://doi.org/10.1080/00031305.1973.10478966&lt;/a&gt;&lt;/p&gt;'><sup>22</sup></a> -</p>
<p><img src="06-dependence-a_files/figure-html/unnamed-chunk-2-1.png" width="672"><img src="06-dependence-a_files/figure-html/unnamed-chunk-2-2.png" width="672"></p>
<p>As evident from the scatter plots the relationship between the
two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is very different in the four cases!
However, intriguingly all four data sets share exactly the same linear characteristics and summary statistics:</p>
<ul>
<li>Means <span class="math inline">\(m_x = 9\)</span> and <span class="math inline">\(m_y = 7.5\)</span>
</li>
<li>Variances <span class="math inline">\(s^2_x = 11\)</span> and <span class="math inline">\(s^2_y = 4.13\)</span>
</li>
<li>Correlation <span class="math inline">\(r = 0.8162\)</span>
</li>
<li>Linear model fit with intercept <span class="math inline">\(a=3.0\)</span> and slope <span class="math inline">\(b=0.5\)</span>
</li>
</ul>
<p>Thus, in actual data analysis it is always a <strong>good idea to inspect the data visually</strong> to get a first impression whether using a linear model makes sense.</p>
<p>In the above only data set “a” follows a linear model. Data set “b” represents a quadratic relationship. Data set “c” is linear but with an outlier that disturbs the linear relationship. Finally data set “d” also contains an outlier but also represent a case where <span class="math inline">\(y\)</span> is (apart from the outlier) is not dependent on <span class="math inline">\(x\)</span>.</p>
<p>In the Worksheet 10 a more recent version of the Anscombe quartet will be analysed in the form of the “datasauRus” dozen - 13 highly nonlinear datasets that all share the
same linear characteristics.</p>
</div>
</div>
<div id="mutual-information-as-generalisation-of-correlation" class="section level2" number="6.5">
<h2>
<span class="header-section-number">6.5</span> Mutual information as generalisation of correlation<a class="anchor" aria-label="anchor" href="#mutual-information-as-generalisation-of-correlation"><i class="fas fa-link"></i></a>
</h2>
<div id="overview-2" class="section level3" number="6.5.1">
<h3>
<span class="header-section-number">6.5.1</span> Overview<a class="anchor" aria-label="anchor" href="#overview-2"><i class="fas fa-link"></i></a>
</h3>
<p>A more general way than the vector correlation to measure multivariate association is mutual information (MI)
which not only covers linear but also non-linear associations.</p>
<p>As we will see below the Rozeboom vector
correlation arises naturally when computing the MI for the multivariate normal distribution,
hence MI also recovers well-known measures of linear association (including multiple correlation and simple correlation), thus truly generalising correlation as measure of association.</p>
</div>
<div id="definition-of-mutual-information" class="section level3" number="6.5.2">
<h3>
<span class="header-section-number">6.5.2</span> Definition of mutual information<a class="anchor" aria-label="anchor" href="#definition-of-mutual-information"><i class="fas fa-link"></i></a>
</h3>
<p>Recall the definition
of Kullback-Leibler divergence, or relative entropy, between two distributions:
<span class="math display">\[
D_{\text{KL}}(F,  G) := \text{E}_F \log \biggl( \frac{f(\boldsymbol x)}{g(\boldsymbol x)} \biggr)
\]</span>
Here <span class="math inline">\(F\)</span> plays the role of the reference distribution and <span class="math inline">\(G\)</span> is an approximating distribution,
with <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> being the corresponding density functions
(see <a href="https://strimmerlab.github.io/publications/lecture-notes/MATH27720-stats2/">MATH27720 Statistics 2</a>).</p>
<p>The <em>Mutual Information</em> (MI) between two random variables <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> is defined as the
KL divergence between the corresponding joint distribution and the product distribution:
<span class="math display">\[
\text{MI}(\boldsymbol x, \boldsymbol y) = D_{\text{KL}}(F_{\boldsymbol x,\boldsymbol y}, F_{\boldsymbol x}  F_{\boldsymbol y}) = \text{E}_{F_{\boldsymbol x,\boldsymbol y}}  \log \biggl( \frac{f(\boldsymbol x, \boldsymbol y)}{f(\boldsymbol x) \, f(\boldsymbol y)} \biggr) .
\]</span>
Thus, MI measures how well the joint distribution can be approximated by the product
distribution (which would be the appropriate joint distribution if <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are independent).
Since MI is an application of KL divergence is shares all its properties. In particular,
<span class="math inline">\(\text{MI}(\boldsymbol x, \boldsymbol y)=0\)</span> implies that the joint distribution and product distributions are the same. Hence the two random variables <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are independent if the mutual information vanishes.</p>
</div>
<div id="mutual-information-between-two-normal-scalar-variables" class="section level3" number="6.5.3">
<h3>
<span class="header-section-number">6.5.3</span> Mutual information between two normal scalar variables<a class="anchor" aria-label="anchor" href="#mutual-information-between-two-normal-scalar-variables"><i class="fas fa-link"></i></a>
</h3>
<p>The KL divergence between two multivariate normal distributions <span class="math inline">\(F_{\text{ref}}\)</span> and <span class="math inline">\(F\)</span> is
<span class="math display">\[
D_{\text{KL}}(F_{\text{ref}}, F)  = \frac{1}{2}   \biggl\{
        (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})
      + \text{Tr}\biggl(\boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
    - \log \det \biggl( \boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
     - d   \biggr\}
\]</span>
This allows compute the mutual information <span class="math inline">\(\text{MI}_{\text{norm}}(x,y)\)</span> between two univariate random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> that are correlated and assumed to be jointly bivariate normal. Let <span class="math inline">\(\boldsymbol z= (x, y)^T\)</span>. The joint bivariate normal distribution is characterised by the mean <span class="math inline">\(\text{E}(\boldsymbol z) = \boldsymbol \mu= (\mu_x, \mu_y)^T\)</span> and the covariance matrix
<span class="math display">\[
\boldsymbol \Sigma=
\begin{pmatrix}
\sigma^2_x &amp; \rho \, \sigma_x \sigma_y \\
\rho \, \sigma_x  \sigma_y &amp; \sigma^2_y \\
\end{pmatrix}
\]</span>
where <span class="math inline">\(\text{Cor}(x,y)= \rho\)</span>. If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent then
<span class="math inline">\(\rho=0\)</span> and
<span class="math display">\[
\boldsymbol \Sigma_{\text{indep}} =
\begin{pmatrix} \sigma^2_x &amp; 0 \\ 0 &amp; \sigma^2_y \\ \end{pmatrix} \,.
\]</span>
The product
<span class="math display">\[
\boldsymbol A= \boldsymbol \Sigma_{\text{indep}}^{-1} \boldsymbol \Sigma=
\begin{pmatrix}
1 &amp; \rho \frac{\sigma_y}{\sigma_x} \\
\rho \frac{\sigma_x}{\sigma_y} &amp; 1 \\
\end{pmatrix}
\]</span>
has trace <span class="math inline">\(\text{Tr}(\boldsymbol A) = 2\)</span> and determinant <span class="math inline">\(\det(\boldsymbol A) = 1-\rho^2\)</span>.</p>
<p>With this the mutual information between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> can be computed as
<span class="math display">\[
\begin{split}
\text{MI}_{\text{norm}}(x, y) &amp;= D_{\text{KL}}(N(\boldsymbol \mu, \boldsymbol \Sigma),  N(\boldsymbol \mu,\boldsymbol \Sigma_{\text{indep}} )) \\
&amp; = \frac{1}{2}   \biggl\{
         \text{Tr}\biggl(\boldsymbol \Sigma^{-1}_{\text{indep}} \boldsymbol \Sigma\biggr)
    - \log \det \biggl( \boldsymbol \Sigma^{-1}_{\text{indep}} \boldsymbol \Sigma\biggr)
     - 2   \biggr\} \\
&amp; = \frac{1}{2}   \biggl\{
         \text{Tr}( \boldsymbol A)
    - \log \det( \boldsymbol A)
     - 2   \biggr\} \\
&amp;=  -\frac{1}{2} \log(1-\rho^2) \\
  &amp; \approx \frac{\rho^2}{2} \\
\end{split}
\]</span></p>
<p>Thus <span class="math inline">\(\text{MI}_{\text{norm}}(x,y)\)</span> is is a one-to-one function of the squared correlation <span class="math inline">\(\rho^2\)</span> between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>:</p>
<div class="inline-figure"><img src="06-dependence-a_files/figure-html/unnamed-chunk-3-1.png" width="384"></div>
<p>For small values of correlation <span class="math inline">\(2 \, \text{MI}_{\text{norm}}(x,y) \approx \rho^2\)</span>.</p>
</div>
<div id="mutual-information-between-two-normally-distributed-random-vectors" class="section level3" number="6.5.4">
<h3>
<span class="header-section-number">6.5.4</span> Mutual information between two normally distributed random vectors<a class="anchor" aria-label="anchor" href="#mutual-information-between-two-normally-distributed-random-vectors"><i class="fas fa-link"></i></a>
</h3>
<p>The mutual information <span class="math inline">\(\text{MI}_{\text{norm}}(\boldsymbol x,\boldsymbol y)\)</span> between two multivariate normal random vector <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>
can be computed in a similar fashion as in the bivariate case.</p>
<p>Let <span class="math inline">\(\boldsymbol z= (\boldsymbol x, \boldsymbol y)^T\)</span> with dimension <span class="math inline">\(d=p+q\)</span>. The joint multivariate
normal distribution is characterised by the mean <span class="math inline">\(\text{E}(\boldsymbol z) = \boldsymbol \mu= (\boldsymbol \mu_x^T, \boldsymbol \mu_y^T)^T\)</span> and the covariance matrix
<span class="math display">\[
\boldsymbol \Sigma=
\begin{pmatrix} \boldsymbol \Sigma_{\boldsymbol x} &amp; \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} \\
\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y}^T &amp; \boldsymbol \Sigma_{\boldsymbol y} \\
\end{pmatrix} \,.
\]</span>
If <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are independent then
<span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} = 0\)</span> and
<span class="math display">\[
\boldsymbol \Sigma_{\text{indep}} =
\begin{pmatrix}  
\boldsymbol \Sigma_{\boldsymbol x} &amp; 0 \\
0 &amp; \boldsymbol \Sigma_{\boldsymbol y} \\
\end{pmatrix} \, .
\]</span>
The product
<span class="math display">\[
\begin{split}
\boldsymbol A&amp; =
\boldsymbol \Sigma_{\text{indep}}^{-1} \boldsymbol \Sigma=
\begin{pmatrix}
\boldsymbol I_p &amp; \boldsymbol \Sigma_{\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} \\
\boldsymbol \Sigma_{\boldsymbol y}^{-1} \boldsymbol \Sigma_{\boldsymbol y\boldsymbol x}    &amp; \boldsymbol I_q \\
\end{pmatrix} \\
&amp; =
\begin{pmatrix}
\boldsymbol I_p &amp;  \boldsymbol V_{\boldsymbol x}^{-1/2} \boldsymbol P_{\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol x\boldsymbol y} \boldsymbol V_{\boldsymbol y}^{1/2} \\
\boldsymbol V_{\boldsymbol y}^{-1/2} \boldsymbol P_{\boldsymbol y}^{-1} \boldsymbol P_{\boldsymbol y\boldsymbol x} \boldsymbol V_{\boldsymbol x}^{1/2}   &amp; \boldsymbol I_q \\
\end{pmatrix}\\
\end{split}
\]</span>
has trace <span class="math inline">\(\text{Tr}(\boldsymbol A) = d\)</span> and determinant
<span class="math display">\[
\begin{split}
\det(\boldsymbol A) &amp; = \det( \boldsymbol I_p - \boldsymbol K\boldsymbol K^T ) \\
  &amp;= \det( \boldsymbol I_q - \boldsymbol K^T \boldsymbol K) \\
\end{split}
\]</span>
with <span class="math inline">\(\boldsymbol K= \boldsymbol P_{\boldsymbol x}^{-1/2} \boldsymbol P_{\boldsymbol x\boldsymbol y} \boldsymbol P_{\boldsymbol y}^{-1/2}\)</span>.
With <span class="math inline">\(\lambda_1, \ldots, \lambda_m\)</span> the singular values of <span class="math inline">\(\boldsymbol K\)</span> (i.e.
the canonical correlations between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>) we get
<span class="math display">\[
\det(\boldsymbol A) =  \prod_{i=1}^m (1-\lambda_i^2)
\]</span></p>
<p>The mutual information between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> is then
<span class="math display">\[
\begin{split}
\text{MI}_{\text{norm}}(\boldsymbol x, \boldsymbol y) &amp;= D_{\text{KL}}(N(\boldsymbol \mu, \boldsymbol \Sigma),  N(\boldsymbol \mu,\boldsymbol \Sigma_{\text{indep}} )) \\
&amp; = \frac{1}{2}   \biggl\{
         \text{Tr}\biggl( \boldsymbol \Sigma^{-1}_{\text{indep}} \boldsymbol \Sigma\biggr)
    - \log \det \biggl( \boldsymbol \Sigma^{-1}_{\text{indep}} \boldsymbol \Sigma\biggr)
     - d   \biggr\} \\
&amp; = \frac{1}{2}   \biggl\{
         \text{Tr}( \boldsymbol A)
    - \log \det( \boldsymbol A)
     - d   \biggr\} \\
&amp;=-\frac{1}{2} \sum_{i=1}^m \log(1-\lambda_i^2)\\
&amp;=-\frac{1}{2} \log \left( \prod_{i=1}^m (1-\lambda_i^2) \right)\\
&amp;=-\frac{1}{2} \log \left( 1- \text{VCor}(\boldsymbol x, \boldsymbol y)^2 \right)\\
\end{split}
\]</span></p>
<p>From the above we see that <span class="math inline">\(\text{MI}_{\text{norm}}(\boldsymbol x, \boldsymbol y)\)</span> is simply the sum
of the MIs resulting from the individual canonical correlations <span class="math inline">\(\lambda_i\)</span> with
the same functional link between the MI and the squared correlation
as in the bivariate normal case.</p>
<p>Furthermore we obtain that
<span class="math display">\[
\text{MI}_{\text{norm}}(\boldsymbol x,\boldsymbol y) = -\frac{1}{2} \log(1 - \text{VCor}(\boldsymbol x, \boldsymbol y)^2 ) \approx \frac{1}{2} \text{VCor}(\boldsymbol x, \boldsymbol y)^2
\]</span>
Thus, in the multivariate case <span class="math inline">\(\text{MI}_{\text{norm}}(\boldsymbol x\boldsymbol y)\)</span> has again exactly the same functional relationship with
the squared vector correlation <span class="math inline">\(\text{VCor}(\boldsymbol x, \boldsymbol y)^2\)</span> as the <span class="math inline">\(\text{MI}_{\text{norm}}(x, y)\)</span>
for two univariate variables with squared Pearson correlation <span class="math inline">\(\rho^2\)</span>.</p>
<p>Thus, Rozeboom’s vector correlation emerges as as a special case of mutual information
computed for jointly multivariate normally distributed variables.</p>
</div>
<div id="using-mi-for-variable-selection" class="section level3" number="6.5.5">
<h3>
<span class="header-section-number">6.5.5</span> Using MI for variable selection<a class="anchor" aria-label="anchor" href="#using-mi-for-variable-selection"><i class="fas fa-link"></i></a>
</h3>
<p>A very general way to write down a model predicting <span class="math inline">\(\boldsymbol y\)</span> by <span class="math inline">\(\boldsymbol x\)</span> is as follows:</p>
<ul>
<li>
<span class="math inline">\(F_{\boldsymbol y|\boldsymbol x}\)</span> is a conditional distribution of <span class="math inline">\(\boldsymbol y\)</span> given predictors <span class="math inline">\(\boldsymbol x\)</span> and</li>
<li>
<span class="math inline">\(F_{\boldsymbol y}\)</span> is the marginal distribution of <span class="math inline">\(\boldsymbol y\)</span> without predictors.</li>
</ul>
<p>Typically <span class="math inline">\(F_{\boldsymbol y|\boldsymbol x}\)</span> is a complex model and <span class="math inline">\(F_{\boldsymbol y}\)</span>
a simple model (no predictors). Note that the predictive model can assume any form (incl. nonlinear).</p>
<p>Intriguingly the expected KL divergence
between the conditional and the marginal distribution
<span class="math display">\[
\text{E}_{F_{\boldsymbol x}}\, D_{\text{KL}}(F_{\boldsymbol y|\boldsymbol x},  F_{\boldsymbol y} ) = \text{MI}(\boldsymbol x, \boldsymbol y)
\]</span>
is equal to mutual information between
<span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>! Thus <span class="math inline">\(\text{MI}(\boldsymbol x, \boldsymbol y)\)</span> measures the impact of conditioning. If the MI is small (i.e. close to zero) then
<span class="math inline">\(\boldsymbol x\)</span> is not useful in predicting <span class="math inline">\(\boldsymbol y\)</span>.</p>
<p>The above identity can be verified as follows.
The KL divergence between <span class="math inline">\(F_{\boldsymbol y|\boldsymbol x}\)</span> and <span class="math inline">\(F_{\boldsymbol y}\)</span>
is given by
<span class="math display">\[
D_{\text{KL}}(F_{\boldsymbol y|\boldsymbol x}, F_{\boldsymbol y} )  = \text{E}_{F_{\boldsymbol y|\boldsymbol x}}  \log\biggl( \frac{f(\boldsymbol y|\boldsymbol x) }{ f(\boldsymbol y)}  \biggr) \, ,
\]</span>
which is a random variable since it depends on <span class="math inline">\(\boldsymbol x\)</span>.
Taking the expectation with regard to <span class="math inline">\(F_{\boldsymbol x}\)</span> (the distribution of <span class="math inline">\(\boldsymbol x\)</span>)
we get
<span class="math display">\[
\begin{split}
\text{E}_{F_{\boldsymbol x}} D_{\text{KL}}(F_{\boldsymbol y|\boldsymbol x}, F_{\boldsymbol y} ) &amp;=
\text{E}_{F_{\boldsymbol x}}  \text{E}_{F_{\boldsymbol y|\boldsymbol x}}  \log \biggl(\frac{ f(\boldsymbol y|\boldsymbol x) f(\boldsymbol x) }{ f(\boldsymbol y) f(\boldsymbol x) } \biggr)\\
&amp; =
\text{E}_{F_{\boldsymbol x,\boldsymbol y}}   \log \biggl(\frac{ f(\boldsymbol x,\boldsymbol y) }{ f(\boldsymbol y) f(\boldsymbol x) } \biggr) = \text{MI}(\boldsymbol x,\boldsymbol y) \,. \\
\end{split}
\]</span></p>
<p>Because of this link of MI with conditioning the MI between response and predictor variables is often used for variable and feature selection in general models.</p>
</div>
<div id="other-measures-of-general-dependence" class="section level3" number="6.5.6">
<h3>
<span class="header-section-number">6.5.6</span> Other measures of general dependence<a class="anchor" aria-label="anchor" href="#other-measures-of-general-dependence"><i class="fas fa-link"></i></a>
</h3>
<p>In principle, MI can be computed for any distribution and model and thus applies to both normal and non-normal models,
and to both linear and nonlinear relationships.</p>
<p>Besides mutual information there are others measures of general dependence between multivariate random variables.</p>
<p>Two important measures to capture nonlinear association that have been proposed in recent literature are</p>
<ol style="list-style-type: lower-roman">
<li>
<a href="https://en.wikipedia.org/wiki/Distance_correlation">distance correlation</a> and</li>
<li>the <a href="https://en.wikipedia.org/wiki/Maximal_information_coefficient">maximal information coefficient</a> (MIC and <span class="math inline">\(\text{MIC}_e\)</span>).</li>
</ol>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="graphical-models" class="section level2" number="6.6">
<h2>
<span class="header-section-number">6.6</span> Graphical models<a class="anchor" aria-label="anchor" href="#graphical-models"><i class="fas fa-link"></i></a>
</h2>
<div id="purpose" class="section level3" number="6.6.1">
<h3>
<span class="header-section-number">6.6.1</span> Purpose<a class="anchor" aria-label="anchor" href="#purpose"><i class="fas fa-link"></i></a>
</h3>
<p>Graphical models combine features from</p>
<ul>
<li>graph theory</li>
<li>probability</li>
<li>statistical inference</li>
</ul>
<p>The literature on graphical models is huge, we focus here only on two commonly
used models:</p>
<ul>
<li>DAGs (directed acyclic graphs), all edges are directed, no directed loops (i.e. no cycles, hence “acyclic”)</li>
<li>GGM (Gaussian graphical models), all edges are undirected</li>
</ul>
<p>Graphical models provide probabilistic models for trees and for networks, with
random variables represented by nodes in the graphs, and branches representing
conditional dependencies. In this regard they generalise both the tree-based clustering approaches as well as the probabilistic non-hierarchical methods (GMMs).</p>
<p>However, the class of graphical models goes much beyond simple
unsupervised learning models. It also includes regression, classification,
time series models etc. For an overview see, e.g., the reference book by <span class="citation">Murphy (<a href="bibliography.html#ref-Murphy2023" role="doc-biblioref">2023</a>)</span>.</p>
</div>
<div id="basic-notions-from-graph-theory" class="section level3" number="6.6.2">
<h3>
<span class="header-section-number">6.6.2</span> Basic notions from graph theory<a class="anchor" aria-label="anchor" href="#basic-notions-from-graph-theory"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Mathematically, a graph <span class="math inline">\(G = (V, E)\)</span> consists of a a set of vertices or nodes <span class="math inline">\(V = \{v_1, v_2, \ldots\}\)</span> and a set of branches or edges <span class="math inline">\(E = \{ e_1, e_2, \ldots \}\)</span>.</li>
<li>Edges can be undirected or directed.</li>
<li>Graphs containing only directed edges are directed graphs, and likewise graphs containing only undirected edges are called undirected graphs. Graphs containing both directed and undirected edges are called partially directed graphs.</li>
<li>A path is a sequence of of vertices such that from each of its vertices there is an edge to the next vertex in the sequence.</li>
<li>A graph is connected when there is a path between every pair of vertices.</li>
<li>A cycle is a path in a graph that connects a node with itself.</li>
<li>A connected graph with no cycles is a called a tree.</li>
<li>The degree of a node is the number of edges it connects with. If edges are all directed the degree of a node is the sum of the in-degree and out-degree, which counts the incoming and outgoing edges, respectively.</li>
<li>External nodes are nodes with degree 1. In a tree-structured graph these are also called leaves.</li>
</ul>
<p>Some notions are only relevant for graphs with directed edges:</p>
<ul>
<li>In a directed graph the parent node(s) of vertex <span class="math inline">\(v\)</span> is the set of nodes <span class="math inline">\(\text{pa}(v)\)</span> directly connected to <span class="math inline">\(v\)</span> via edges directed from the parent node(s) towards <span class="math inline">\(v\)</span>.</li>
<li>Conversely, <span class="math inline">\(v\)</span> is called a child node of <span class="math inline">\(\text{pa}(v)\)</span>. Note that a parent node can have several child nodes, so <span class="math inline">\(v\)</span> may not be the only child of <span class="math inline">\(\text{pa}(v)\)</span>.</li>
<li>In a directed tree graph, each node has only a single parent, except for one particular node that has no parent at all (this node is called the root node).</li>
<li>A DAG, or directed acyclic graph, is a directed graph with no directed cycles. A (directed) tree is a special version of a DAG.</li>
</ul>
</div>
<div id="probabilistic-graphical-models" class="section level3" number="6.6.3">
<h3>
<span class="header-section-number">6.6.3</span> Probabilistic graphical models<a class="anchor" aria-label="anchor" href="#probabilistic-graphical-models"><i class="fas fa-link"></i></a>
</h3>
<p>A graphical model uses a graph to describe the relationship between random variables <span class="math inline">\(x_1, \ldots, x_d\)</span>. The variables are assumed to have a joint distribution with density/mass function <span class="math inline">\(p(x_1, x_2, \ldots, x_d)\)</span>.
Each random variable is placed in a node of the graph.</p>
<p>The structure of the graph and the type of the edges connecting (or not connecting) any pair of nodes/variables is used to describe the conditional dependencies, and to simplify the joint distribution.</p>
<p>Thus, a graphical model is in essence a visualisation of the joint distribution using structural information from the graph helping to understand the mutual relationship among the variables.</p>
</div>
<div id="directed-graphical-models" class="section level3" number="6.6.4">
<h3>
<span class="header-section-number">6.6.4</span> Directed graphical models<a class="anchor" aria-label="anchor" href="#directed-graphical-models"><i class="fas fa-link"></i></a>
</h3>
<p>In a <strong>directed graphical model</strong> the graph structure is assumed to be
a DAG (or a directed tree, which is also a DAG).</p>
<p>Then the joint probability distribution can be factorised into a <em>product of conditional probabilities</em> as follows:
<span class="math display">\[
p(x_1, x_2, \ldots, x_d) = \prod_i p(x_i  | \text{pa}(x_i))
\]</span>
Thus, the overall joint probability distribution is specified by local conditional distributions and the graph structure, with the directions of the edges providing the information about parent-child node relationships.</p>
<p>Probabilistic DAGs are also known as “Bayesian networks”.</p>
<p><strong>Idea:</strong> by trying out all possible trees/graphs and fitting them to the data using maximum likelihood (or Bayesian inference) we hope to be able identify the graph structure of the data-generating process.</p>
<p><strong>Challenges</strong></p>
<ol style="list-style-type: decimal">
<li>in the tree/network the internal nodes are usually not known, and thus have to
be treated as <em>latent</em> variables.</li>
</ol>
<p><strong>Answer:</strong> To impute the states at these nodes we may use the EM algorithm as in GMMs
(which in fact can be viewed as graphical models, too!).</p>
<ol start="2" style="list-style-type: decimal">
<li>If we treat the internal nodes as unknowns we need to marginalise over the
internal nodes, i.e. we need to sum / integrate over all possible set of states
of the internal nodes!</li>
</ol>
<p><strong>Answer:</strong> This can be handled very effectively using the <strong>Viterbi algorithm</strong> which is essentially
an application of the generalised distributive law. In particular for tree graphs this
means that the summations occurs locally at each node and propagates recursively across the tree.</p>
<ol start="3" style="list-style-type: decimal">
<li>In order to infer the tree or network structure the space of all trees or networks need to
be explored. This is not possible in an exhaustive fashion unless the number of variables
in the tree is very small.</li>
</ol>
<p><strong>Answer:</strong> Solution: use heuristic approaches for tree and network search!</p>
<ol start="4" style="list-style-type: decimal">
<li>Furthermore, there exist so-called “equivalence classes” of graphical models, i.e. sets of graphical models that share the same joint probability distribution. Thus, all graphical models within the same equivalence class cannot be distinguished from observational data, even with infinite sample size!</li>
</ol>
<p><strong>Answer:</strong> this is a fundamental mathematical problem of identifiability so there is now way around this issue. However,
on the positive side, this also implies that the search through all graphical models can be restricted to finding the so-called “essential graph” (e.g. <a href="https://projecteuclid.org/euclid.aos/1031833662" class="uri">https://projecteuclid.org/euclid.aos/1031833662</a> )</p>
<p><strong>Conclusion: using directed graphical models for structure discovery is very time consuming and computationally
demanding for anything but small toy data sets.</strong></p>
<p>This also explains why heuristic and non-model based approaches (such as hierarchical clustering) are so popular even though full statistical modelling is in principle possible.</p>
</div>
<div id="undirected-graphical-models" class="section level3" number="6.6.5">
<h3>
<span class="header-section-number">6.6.5</span> Undirected graphical models<a class="anchor" aria-label="anchor" href="#undirected-graphical-models"><i class="fas fa-link"></i></a>
</h3>
<p>Another class of graphical models are models that contain only undirected edges. These <strong>undirected graphical models</strong>
are used to represent the pairwise conditional (in)dependencies among the variables in the graph, and the resulting model is therefore also called <strong>conditional independence graph</strong>.</p>
<p>Suppose <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> are two random variables/nodes from <span class="math inline">\(\{x_1, \ldots, x_d\}\)</span>, and the set <span class="math inline">\(\{x_k\}\)</span> represents all other variables/nodes with <span class="math inline">\(k\neq i\)</span> and <span class="math inline">\(k \neq j\)</span>. Then the variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> are conditionally independent
given all the other variables <span class="math inline">\(\{x_k\}\)</span>
<span class="math display">\[
x_i \perp\!\!\!\perp x_j | \{x_k\}
\]</span>
if the joint probability density for all variables <span class="math inline">\(\{x_1, \ldots, x_d\}\)</span>
factorises as
<span class="math display">\[
p(x_1, x_2, \ldots, x_d) = p(x_i | \{x_k\}) \, p(x_j | \{x_k\}) \, p(\{x_k\}) \,.
\]</span>
or equivalently
<span class="math display">\[
\frac{p(x_i, x_j, \ldots, x_d)}{p(\{x_k\})}  
= p(x_i, x_j | \{x_k\})
= p(x_i | \{x_k\}) \, p(x_j | \{x_k\}) \,.
\]</span></p>
<p>In the corresponding conditional independence graph note there is no edge between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>,
as in such a graph <em>missing edges correspond to conditional independence</em> between the respective non-connected nodes.</p>
<div id="gaussian-graphical-model" class="section level4" number="6.6.5.1">
<h4>
<span class="header-section-number">6.6.5.1</span> Gaussian graphical model<a class="anchor" aria-label="anchor" href="#gaussian-graphical-model"><i class="fas fa-link"></i></a>
</h4>
<p>Assuming that <span class="math inline">\(x_1, \ldots, x_d\)</span> are jointly normally distributed, i.e. <span class="math inline">\(\boldsymbol x\sim N(\boldsymbol \mu, \boldsymbol \Sigma)\)</span>,
it turns out that it is straightforward to identify the pairwise conditional independencies.
From <span class="math inline">\(\boldsymbol \Sigma\)</span> we first obtain the precision matrix
<span class="math display">\[\boldsymbol \Omega= (\omega_{ij}) = \boldsymbol \Sigma^{-1} \,.\]</span>
Crucially, it can be shown that
<span class="math inline">\(\omega_{ij} = 0\)</span> implies
<span class="math inline">\(x_i \perp\!\!\!\perp x_j \,|\, \{ x_k \}\)</span>.
Hence, from the precision matrix <span class="math inline">\(\boldsymbol \Omega\)</span> we can directly read off all the pairwise conditional independencies among the variables <span class="math inline">\(x_1, x_2, \ldots, x_d\)</span>.</p>
<p>Often, the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> is dense (few zeros) but the corresponding precision matrix
<span class="math inline">\(\boldsymbol \Omega\)</span> is sparse (many zeros).</p>
<p>The conditional independence graph computed for normally distributed variables is called
a <strong>Gaussian graphical model</strong>, or short <strong>GGM</strong>. A further alternative name
commonly used is <strong>covariance selection model</strong>.</p>
</div>
<div id="related-quantity-partial-correlation" class="section level4" number="6.6.5.2">
<h4>
<span class="header-section-number">6.6.5.2</span> Related quantity: partial correlation<a class="anchor" aria-label="anchor" href="#related-quantity-partial-correlation"><i class="fas fa-link"></i></a>
</h4>
<p>From the precision matrix <span class="math inline">\(\boldsymbol \Omega\)</span> we can also compute the matrix of pairwise full conditional <em>partial correlations</em>:</p>
<p><span class="math display">\[
\rho_{ij|\text{rest}}=-\frac{\omega_{ij}}{\sqrt{\omega_{ii}\omega_{jj}}}
\]</span>
which is essentially the standardised precision matrix (similar to correlation but with an extra minus sign!)</p>
<p>The partial correlations lie in the range between -1 and +1, <span class="math inline">\(\rho_{ij|\text{rest}} \in [-1, 1]\)</span>, just like standard correlations.</p>
<p>If <span class="math inline">\(\boldsymbol x\)</span> is multivariate normal then <span class="math inline">\(\rho_{ij|\text{rest}} = 0\)</span> indicates conditional independence
between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>.</p>
<p><em>Regression interpretation:</em> partial correlation is the correlation that remains between
the two variables if the effect of the other variables is “regressed away”.
In other words, the partial correlation is exactly equivalent to the correlation between
the residuals that remain after regressing <span class="math inline">\(x_i\)</span> on the variables <span class="math inline">\(\{x_k\}\)</span> and <span class="math inline">\(x_j\)</span> on <span class="math inline">\(\{x_k\}\)</span>.</p>
</div>
</div>
<div id="null-distribution-of-the-empirical-correlation-coefficient" class="section level3" number="6.6.6">
<h3>
<span class="header-section-number">6.6.6</span> Null distribution of the empirical correlation coefficient<a class="anchor" aria-label="anchor" href="#null-distribution-of-the-empirical-correlation-coefficient"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we have two uncorrelated random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> with <span class="math inline">\(\rho = \text{Cor}(x, y) =0\)</span>.
After observing data <span class="math inline">\(x_1, \ldots, x_n\)</span> and <span class="math inline">\(y_1, \ldots, y_n\)</span> we compute the
the empirical covariance matrix <span class="math inline">\(\hat{\boldsymbol \Sigma}_{xy}\)</span> and from it the
empirical correlation coefficient <span class="math inline">\(r = \widehat{\text{Cor}}(x, y)\)</span>.</p>
<p>The distribution of the empirical correlation assuming <span class="math inline">\(\rho=0\)</span> is useful as null-model for testing whether the underlying correlation is in fact zero having observed empirical correlation <span class="math inline">\(r\)</span>.
If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are normally distributed with <span class="math inline">\(\rho=0\)</span> the distribution of the empirical correlation <span class="math inline">\(r\)</span> has mean <span class="math inline">\(\text{E}(r)=0\)</span> and variance <span class="math inline">\(\text{Var}(r)=\frac{1}{\kappa}\)</span>.
Here <span class="math inline">\(\kappa\)</span> is the degree of freedom of the null distribution which for standard correlation is <span class="math inline">\(\kappa=n-1\)</span>.
Furthermore, the squared empirical correlation is distributed according to a Beta distribution
<span class="math display">\[
r^2 \sim \text{Beta}\left(\frac{1}{2}, \frac{\kappa-1}{2}\right)
\]</span></p>
<p>For partial correlation the null distribution of <span class="math inline">\(r^2\)</span> has the same form but with a different degree of freedom.
Specifically, <span class="math inline">\(\kappa\)</span> is reduced by the number of variables being conditioned on.
If for <span class="math inline">\(d\)</span> dimensions we condition on <span class="math inline">\(d-2\)</span> variables the resulting degree of freedom is <span class="math inline">\(\kappa =n-1 - (d-2) = n-d+1\)</span>. For <span class="math inline">\(d=2\)</span> we get back the degree of freedom for
standard empirical correlation.</p>
</div>
<div id="algorithm-for-learning-ggms" class="section level3" number="6.6.7">
<h3>
<span class="header-section-number">6.6.7</span> Algorithm for learning GGMs<a class="anchor" aria-label="anchor" href="#algorithm-for-learning-ggms"><i class="fas fa-link"></i></a>
</h3>
<p>From the above we can devise a simple algorithm to learn Gaussian graphical model (GGM)
from data:</p>
<ol style="list-style-type: decimal">
<li>Estimate covariance <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> (in such a way that it is invertible!)</li>
<li>Compute corresponding partial correlations</li>
<li>If <span class="math inline">\(\hat{\rho}_{ij|\text{rest}} \approx 0\)</span> then there is (approx.) conditional
independence between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>.</li>
</ol>
<p>The test for conditional independence is done by statistical testing for vanishing partial correlation. Specifically, we compute the <span class="math inline">\(p\)</span>-value assuming that the true underlying partial correlation is zero and then decide whether to reject the null assumption of zero partial correlation.</p>
<p>If there are many edges tested simultaneously we may need to adjust (i.e reduce) the
test threshold, for example applying Bonferroni or FDR methods.</p>
</div>
<div id="example-exam-score-data" class="section level3" number="6.6.8">
<h3>
<span class="header-section-number">6.6.8</span> Example: exam score data<a class="anchor" aria-label="anchor" href="#example-exam-score-data"><i class="fas fa-link"></i></a>
</h3>
<p>This is a data set from Mardia et al. (1979) and features <span class="math inline">\(d=5\)</span> variables measured on
<span class="math inline">\(n=88\)</span> subjects.</p>
<p>Correlations (rounded to 2 digits):</p>
<pre><code>##            mechanics vectors algebra analysis statistics
## mechanics       1.00    0.55    0.55     0.41       0.39
## vectors         0.55    1.00    0.61     0.49       0.44
## algebra         0.55    0.61    1.00     0.71       0.66
## analysis        0.41    0.49    0.71     1.00       0.61
## statistics      0.39    0.44    0.66     0.61       1.00</code></pre>
<p>Partial correlations (rounded to 2 digits):</p>
<pre><code>##            mechanics vectors algebra analysis statistics
## mechanics       1.00    0.33    0.23     0.00       0.02
## vectors         0.33    1.00    0.28     0.08       0.02
## algebra         0.23    0.28    1.00     0.43       0.36
## analysis        0.00    0.08    0.43     1.00       0.25
## statistics      0.02    0.02    0.36     0.25       1.00</code></pre>
<p>Note that there are no zero correlations but there are
<strong>four partial correlations close to 0</strong>, indicating <strong>conditional independence</strong> between:</p>
<ul>
<li>analysis and mechanics,</li>
<li>statistics and mechanics,</li>
<li>analysis and vectors, and</li>
<li>statistics and vectors.</li>
</ul>
<p>The can be verified by computing the normal <span class="math inline">\(p\)</span>-values for the partial correlations (with <span class="math inline">\(\kappa=84\)</span> as degree of freedom):</p>
<pre><code>##            mechanics vectors algebra analysis statistics
## mechanics         NA   0.002   0.034    0.988      0.823
## vectors           NA      NA   0.009    0.477      0.854
## algebra           NA      NA      NA    0.000      0.001
## analysis          NA      NA      NA       NA      0.020
## statistics        NA      NA      NA       NA         NA</code></pre>
<p>There are six edges with small <span class="math inline">\(p\)</span>-value (smaller than say 0.05) and these correspond
to the edges for which the null assumption of zero partial correlation can be rejected
so that out of ten possible edges four are not statistically significant.
Therefore the conditional independence graph looks as follows:</p>
<pre><code>Mechanics      Analysis
   |     \    /    |
   |    Algebra    |
   |     /   \     |
 Vectors      Statistics</code></pre>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="supervised-learning-and-classification.html"><span class="header-section-number">5</span> Supervised learning and classification</a></div>
<div class="next"><a href="nonlinear-and-nonparametric-models.html"><span class="header-section-number">7</span> Nonlinear and nonparametric models</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#multivariate-dependencies"><span class="header-section-number">6</span> Multivariate dependencies</a></li>
<li>
<a class="nav-link" href="#measuring-the-linear-association-between-two-sets-of-random-variables"><span class="header-section-number">6.1</span> Measuring the linear association between two sets of random variables</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#aim"><span class="header-section-number">6.1.1</span> Aim</a></li>
<li><a class="nav-link" href="#known-special-cases"><span class="header-section-number">6.1.2</span> Known special cases</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#canonical-correlation-analysis-cca-aka-cca-whitening"><span class="header-section-number">6.2</span> Canonical Correlation Analysis (CCA) aka CCA whitening</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal"><span class="header-section-number">6.2.1</span> How to make cross-correlation matrix \(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\) diagonal?</a></li>
<li><a class="nav-link" href="#related-methods"><span class="header-section-number">6.2.2</span> Related methods</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#vector-correlation-and-rv-coefficient"><span class="header-section-number">6.3</span> Vector correlation and RV coefficient</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#vector-alienation-coefficient"><span class="header-section-number">6.3.1</span> Vector alienation coefficient</a></li>
<li><a class="nav-link" href="#rozeboom-vector-correlation"><span class="header-section-number">6.3.2</span> Rozeboom vector correlation</a></li>
<li><a class="nav-link" href="#rv-coefficient"><span class="header-section-number">6.3.3</span> RV coefficient</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#limits-of-linear-models-and-correlation"><span class="header-section-number">6.4</span> Limits of linear models and correlation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#correlation-measures-only-linear-dependence"><span class="header-section-number">6.4.1</span> Correlation measures only linear dependence</a></li>
<li><a class="nav-link" href="#anscombe-data-sets"><span class="header-section-number">6.4.2</span> Anscombe data sets</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#mutual-information-as-generalisation-of-correlation"><span class="header-section-number">6.5</span> Mutual information as generalisation of correlation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#overview-2"><span class="header-section-number">6.5.1</span> Overview</a></li>
<li><a class="nav-link" href="#definition-of-mutual-information"><span class="header-section-number">6.5.2</span> Definition of mutual information</a></li>
<li><a class="nav-link" href="#mutual-information-between-two-normal-scalar-variables"><span class="header-section-number">6.5.3</span> Mutual information between two normal scalar variables</a></li>
<li><a class="nav-link" href="#mutual-information-between-two-normally-distributed-random-vectors"><span class="header-section-number">6.5.4</span> Mutual information between two normally distributed random vectors</a></li>
<li><a class="nav-link" href="#using-mi-for-variable-selection"><span class="header-section-number">6.5.5</span> Using MI for variable selection</a></li>
<li><a class="nav-link" href="#other-measures-of-general-dependence"><span class="header-section-number">6.5.6</span> Other measures of general dependence</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#graphical-models"><span class="header-section-number">6.6</span> Graphical models</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#purpose"><span class="header-section-number">6.6.1</span> Purpose</a></li>
<li><a class="nav-link" href="#basic-notions-from-graph-theory"><span class="header-section-number">6.6.2</span> Basic notions from graph theory</a></li>
<li><a class="nav-link" href="#probabilistic-graphical-models"><span class="header-section-number">6.6.3</span> Probabilistic graphical models</a></li>
<li><a class="nav-link" href="#directed-graphical-models"><span class="header-section-number">6.6.4</span> Directed graphical models</a></li>
<li><a class="nav-link" href="#undirected-graphical-models"><span class="header-section-number">6.6.5</span> Undirected graphical models</a></li>
<li><a class="nav-link" href="#null-distribution-of-the-empirical-correlation-coefficient"><span class="header-section-number">6.6.6</span> Null distribution of the empirical correlation coefficient</a></li>
<li><a class="nav-link" href="#algorithm-for-learning-ggms"><span class="header-section-number">6.6.7</span> Algorithm for learning GGMs</a></li>
<li><a class="nav-link" href="#example-exam-score-data"><span class="header-section-number">6.6.8</span> Example: exam score data</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Multivariate Statistics and Machine Learning</strong>" was written by Korbinian Strimmer. It was last built on 22 January 2024.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
