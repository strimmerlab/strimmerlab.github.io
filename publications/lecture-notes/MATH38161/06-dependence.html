<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Multivariate dependencies – Multivariate Statistics and Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./07-nonlinear.html" rel="next">
<link href="./05-classification.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c63e616a2164fee75212f002d4510366.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./06-dependence.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multivariate dependencies</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Multivariate Statistics and Machine Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-multivariate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Multivariate random variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multivariate estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations and dimension reduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised learning and clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning and classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-dependence.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multivariate dependencies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear and nonparametric models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#measuring-the-linear-association-between-two-sets-of-random-variables" id="toc-measuring-the-linear-association-between-two-sets-of-random-variables" class="nav-link active" data-scroll-target="#measuring-the-linear-association-between-two-sets-of-random-variables"><span class="header-section-number">6.1</span> Measuring the linear association between two sets of random variables</a>
  <ul class="collapse">
  <li><a href="#aim" id="toc-aim" class="nav-link" data-scroll-target="#aim">Aim</a></li>
  <li><a href="#known-special-cases" id="toc-known-special-cases" class="nav-link" data-scroll-target="#known-special-cases">Known special cases</a></li>
  </ul></li>
  <li><a href="#canonical-correlation-analysis-cca-aka-cca-whitening" id="toc-canonical-correlation-analysis-cca-aka-cca-whitening" class="nav-link" data-scroll-target="#canonical-correlation-analysis-cca-aka-cca-whitening"><span class="header-section-number">6.2</span> Canonical Correlation Analysis (CCA) aka CCA whitening</a>
  <ul class="collapse">
  <li><a href="#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal" id="toc-how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal" class="nav-link" data-scroll-target="#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal">How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</a></li>
  <li><a href="#related-methods" id="toc-related-methods" class="nav-link" data-scroll-target="#related-methods">Related methods</a></li>
  </ul></li>
  <li><a href="#vector-correlation-and-rv-coefficient" id="toc-vector-correlation-and-rv-coefficient" class="nav-link" data-scroll-target="#vector-correlation-and-rv-coefficient"><span class="header-section-number">6.3</span> Vector correlation and RV coefficient</a>
  <ul class="collapse">
  <li><a href="#vector-alienation-coefficient" id="toc-vector-alienation-coefficient" class="nav-link" data-scroll-target="#vector-alienation-coefficient">Vector alienation coefficient</a></li>
  <li><a href="#rozeboom-vector-correlation" id="toc-rozeboom-vector-correlation" class="nav-link" data-scroll-target="#rozeboom-vector-correlation">Rozeboom vector correlation</a></li>
  <li><a href="#rv-coefficient" id="toc-rv-coefficient" class="nav-link" data-scroll-target="#rv-coefficient">RV coefficient</a></li>
  </ul></li>
  <li><a href="#limits-of-linear-models-and-correlation" id="toc-limits-of-linear-models-and-correlation" class="nav-link" data-scroll-target="#limits-of-linear-models-and-correlation"><span class="header-section-number">6.4</span> Limits of linear models and correlation</a>
  <ul class="collapse">
  <li><a href="#correlation-measures-only-linear-dependence" id="toc-correlation-measures-only-linear-dependence" class="nav-link" data-scroll-target="#correlation-measures-only-linear-dependence">Correlation measures only linear dependence</a></li>
  <li><a href="#anscombe-datasets" id="toc-anscombe-datasets" class="nav-link" data-scroll-target="#anscombe-datasets">Anscombe datasets</a></li>
  </ul></li>
  <li><a href="#mutual-information-as-generalisation-of-correlation" id="toc-mutual-information-as-generalisation-of-correlation" class="nav-link" data-scroll-target="#mutual-information-as-generalisation-of-correlation"><span class="header-section-number">6.5</span> Mutual information as generalisation of correlation</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#definition-of-mutual-information" id="toc-definition-of-mutual-information" class="nav-link" data-scroll-target="#definition-of-mutual-information">Definition of mutual information</a></li>
  <li><a href="#mutual-information-between-two-normal-scalar-variables" id="toc-mutual-information-between-two-normal-scalar-variables" class="nav-link" data-scroll-target="#mutual-information-between-two-normal-scalar-variables">Mutual information between two normal scalar variables</a></li>
  <li><a href="#mutual-information-between-two-normally-distributed-random-vectors" id="toc-mutual-information-between-two-normally-distributed-random-vectors" class="nav-link" data-scroll-target="#mutual-information-between-two-normally-distributed-random-vectors">Mutual information between two normally distributed random vectors</a></li>
  <li><a href="#using-mi-for-variable-selection" id="toc-using-mi-for-variable-selection" class="nav-link" data-scroll-target="#using-mi-for-variable-selection">Using MI for variable selection</a></li>
  <li><a href="#other-measures-of-general-dependence" id="toc-other-measures-of-general-dependence" class="nav-link" data-scroll-target="#other-measures-of-general-dependence">Other measures of general dependence</a></li>
  </ul></li>
  <li><a href="#graphical-models" id="toc-graphical-models" class="nav-link" data-scroll-target="#graphical-models"><span class="header-section-number">6.6</span> Graphical models</a>
  <ul class="collapse">
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#basic-notions-from-graph-theory" id="toc-basic-notions-from-graph-theory" class="nav-link" data-scroll-target="#basic-notions-from-graph-theory">Basic notions from graph theory</a></li>
  <li><a href="#probabilistic-graphical-models" id="toc-probabilistic-graphical-models" class="nav-link" data-scroll-target="#probabilistic-graphical-models">Probabilistic graphical models</a></li>
  <li><a href="#directed-graphical-models" id="toc-directed-graphical-models" class="nav-link" data-scroll-target="#directed-graphical-models">Directed graphical models</a></li>
  <li><a href="#undirected-graphical-models" id="toc-undirected-graphical-models" class="nav-link" data-scroll-target="#undirected-graphical-models">Undirected graphical models</a></li>
  <li><a href="#null-distribution-of-the-empirical-correlation-coefficient" id="toc-null-distribution-of-the-empirical-correlation-coefficient" class="nav-link" data-scroll-target="#null-distribution-of-the-empirical-correlation-coefficient">Null distribution of the empirical correlation coefficient</a></li>
  <li><a href="#algorithm-for-learning-ggms" id="toc-algorithm-for-learning-ggms" class="nav-link" data-scroll-target="#algorithm-for-learning-ggms">Algorithm for learning GGMs</a></li>
  <li><a href="#example-exam-score-data" id="toc-example-exam-score-data" class="nav-link" data-scroll-target="#example-exam-score-data">Example: exam score data</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multivariate dependencies</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="measuring-the-linear-association-between-two-sets-of-random-variables" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="measuring-the-linear-association-between-two-sets-of-random-variables"><span class="header-section-number">6.1</span> Measuring the linear association between two sets of random variables</h2>
<section id="aim" class="level3">
<h3 class="anchored" data-anchor-id="aim">Aim</h3>
<p>The linear association between two scalar random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is measured by the correlation <span class="math inline">\(\text{Cor}(x, y) = \rho\)</span>.</p>
<p>In this chapter we now would like to explore how to generalise correlation to the case of two random vectors. Specifically, we would like to find a scalar measure of association between two random vectors (or equivalently two sets of random variables) <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_p)^T\)</span> and <span class="math inline">\(\boldsymbol y= (y_1, \ldots, y_q)^T\)</span> that contains correlation and also multiple correlation as special case.</p>
<p>We assume a joint correlation matrix <span class="math display">\[
\boldsymbol P=
\begin{pmatrix}
\boldsymbol P_{\boldsymbol x} &amp;  \boldsymbol P_{\boldsymbol x\boldsymbol y} \\
\boldsymbol P_{\boldsymbol y\boldsymbol x} &amp; \boldsymbol P_{\boldsymbol y} \\
\end{pmatrix}
\]</span> with cross-correlation matrix <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol y} = \boldsymbol P_{\boldsymbol y\boldsymbol x}^T\)</span> and the within-group group correlations <span class="math inline">\(\boldsymbol P_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol P_{\boldsymbol y}\)</span>. If the cross-correlations vanish, <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol y} =0\)</span>, then the two random vectors are uncorrelated, and the joint correlation matrix becomes a diagonal block matrix <span class="math display">\[
\boldsymbol P_{\text{indep}} =
\begin{pmatrix}
\boldsymbol P_{\boldsymbol x} &amp;  0 \\
0 &amp; \boldsymbol P_{\boldsymbol y} \\
\end{pmatrix} \, .
\]</span></p>
<p>To characterise the total association between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> we are looking for a <strong>scalar quantity</strong> measuring the divergence of a distribution assuming the general joint correlation matrix <span class="math inline">\(\boldsymbol P\)</span> from a distribution assuming the joint correlation matrix <span class="math inline">\(\boldsymbol P_{\text{indep}}\)</span> for uncorrelated <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>.</p>
</section>
<section id="known-special-cases" class="level3">
<h3 class="anchored" data-anchor-id="known-special-cases">Known special cases</h3>
<p>Ideally, in case of an univariate <span class="math inline">\(y\)</span> but multivariate <span class="math inline">\(\boldsymbol x\)</span> this measure should reduce to the <em>squared multiple correlation</em> or <em>coefficient of determination</em> <span class="math display">\[
\text{MCor}(\boldsymbol x, y)^2 = \boldsymbol P_{y\boldsymbol x} \boldsymbol P_{\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy}
\]</span> This is well-known in linear regression to describe the strength of the total linear association between the predictors <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_p)^T\)</span> and the response <span class="math inline">\(y\)</span>.</p>
<p>To derive the squared multiple correlation we may proceed as follows. First we whiten the random vector <span class="math inline">\(\boldsymbol x\)</span> resulting in <span class="math inline">\(\boldsymbol z_{\boldsymbol x} = \boldsymbol W_{\boldsymbol x}\boldsymbol x=\boldsymbol Q_{\boldsymbol x}\boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol V_{\boldsymbol x}^{-1/2}\boldsymbol x\)</span> where <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}\)</span> is an orthogonal matrix. The correlations between each component in <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span> and the response <span class="math inline">\(y\)</span> are then <span class="math display">\[
\boldsymbol \omega= \text{Cor}(\boldsymbol z_{\boldsymbol x}, y) = \boldsymbol Q_{\boldsymbol x} \boldsymbol P_{\boldsymbol x}^{-1/2} \boldsymbol P_{\boldsymbol xy}
\]</span> As <span class="math inline">\(\text{Var}(\boldsymbol z_{\boldsymbol x}) = \boldsymbol I\)</span> and thus the components in <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span> are uncorrelated we can simply add up the squared individual correlations to get as total association measure <span class="math display">\[
\boldsymbol \omega^T \boldsymbol \omega= \sum_i \omega_i^2 = \boldsymbol P_{y\boldsymbol x} \boldsymbol P_{\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy}
\]</span> Note that the particular choice of the orthogonal matrix <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}\)</span> for whitening <span class="math inline">\(\boldsymbol x\)</span> is not relevant for the squared multiple correlation.</p>
<p>Note that if the cross-correlations vanish (<span class="math inline">\(\boldsymbol P_{\boldsymbol xy} =0\)</span>) then <span class="math inline">\(\text{MCor}(\boldsymbol x, y)^2=0\)</span>. If the correlation between the predictors vanishes (<span class="math inline">\(\boldsymbol P_{\boldsymbol x} = \boldsymbol I\)</span>) then <span class="math inline">\(\text{MCor}(\boldsymbol x, y)^2 = \sum_i \rho_{y x_i}^2\)</span>, i.e.&nbsp;it is the sum of the squared cross-correlations.</p>
<p>If there is only a single predictor <span class="math inline">\(x\)</span> then <span class="math inline">\(\boldsymbol P_{xy}=\rho\)</span> and <span class="math inline">\(\boldsymbol P_{x} = 1\)</span> and the squared multiple correlation reduces to the squared Pearson correlation <span class="math display">\[
\text{Cor}(x, y)^2 = \rho^2 \, .
\]</span></p>
</section>
</section>
<section id="canonical-correlation-analysis-cca-aka-cca-whitening" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="canonical-correlation-analysis-cca-aka-cca-whitening"><span class="header-section-number">6.2</span> Canonical Correlation Analysis (CCA) aka CCA whitening</h2>
<p>Canonical correlation analysis was invented by Harald Hotelling in 1936 <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. CCA aims to characterise the linear dependence between to random vectors <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> by a set of <strong>canonical correlations</strong> <span class="math inline">\(\lambda_i\)</span>.</p>
<p>CCA works by simultaneously whitening the two random vectors <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> where the whitening matrices are chosen in such a way that the cross-correlation matrix between the resulting whitened variables becomes diagonal, and the elements on the diagonal correspond to the <strong>canonical correlations</strong>.</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\boldsymbol x= \begin{pmatrix} x_1 \\ \vdots \\ x_p \end{pmatrix} \\
\text{Dimension } p
\end{array}
\begin{array}{ll}
\boldsymbol y= \begin{pmatrix} y_1 \\ \vdots \\ y_q \end{pmatrix} \\
\text{Dimension } q
\end{array}
\begin{array}{ll}
\text{Var}(\boldsymbol x) = \boldsymbol \Sigma_{\boldsymbol x} = \boldsymbol V_{\boldsymbol x}^{1/2}\boldsymbol P_{\boldsymbol x}\boldsymbol V_{\boldsymbol x}^{1/2} \\
\text{Var}(\boldsymbol y) = \boldsymbol \Sigma_{\boldsymbol y} = \boldsymbol V_{\boldsymbol y}^{1/2}\boldsymbol P_{\boldsymbol y}\boldsymbol V_{\boldsymbol y}^{1/2} \\
\end{array}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cc}
\text{Whitening of } \boldsymbol x\text{:} \\
\text{Whitening of } \boldsymbol y\text{:}
\end{array}
\begin{array}{cc}
\boldsymbol z_{\boldsymbol x} = \boldsymbol W_{\boldsymbol x}\boldsymbol x=\boldsymbol Q_{\boldsymbol x}\boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol V_{\boldsymbol x}^{-1/2}\boldsymbol x\\
\boldsymbol z_{\boldsymbol y} = \boldsymbol W_{\boldsymbol y}\boldsymbol y=\boldsymbol Q_{\boldsymbol y}\boldsymbol P_{\boldsymbol y}^{-1/2}\boldsymbol V_{\boldsymbol y}^{-1/2}\boldsymbol y
\end{array}
\end{align*}\]</span> (note we use the correlation-based form of <span class="math inline">\(\boldsymbol W\)</span>)</p>
<p>Cross-correlation between <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span> and <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span>:</p>
<p><span class="math display">\[\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})=\boldsymbol Q_{\boldsymbol x}\boldsymbol K\boldsymbol Q_{\boldsymbol y}^T\]</span></p>
<p>with <span class="math inline">\(\boldsymbol K= \boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol P_{\boldsymbol x\boldsymbol y}\boldsymbol P_{\boldsymbol y}^{-1/2}\)</span>.</p>
<p><strong>Idea</strong>: we can choose suitable orthogonal matrices <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}\)</span> by putting a structural constraint on the cross-correlation matrix.</p>
<p><strong>CCA</strong>: we aim for a <em>diagonal</em> <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> so that each component in <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span> only influences one (the corresponding) component in <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span>.</p>
<p><strong>Motivation</strong>: pairs of “modules” represented by components of <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span> influencing each other (and not any other module).</p>
<p><span class="math display">\[
\begin{array}{ll}
\boldsymbol z_{\boldsymbol x} = \begin{pmatrix} z^x_1 \\ z^x_2 \\ \vdots \\ z^x_p \end{pmatrix} &amp;
\boldsymbol z_{\boldsymbol y} = \begin{pmatrix} z^y_1 \\ z^y_2 \\ \vdots \\ z^y_q \end{pmatrix} \\
\end{array}
\]</span></p>
<p><span class="math display">\[\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y}) = \begin{pmatrix} \lambda_1 &amp; \dots &amp; 0 \\ \vdots &amp;  \vdots \\ 0 &amp; \dots &amp; \lambda_m \end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(\lambda_i\)</span> are the <em>canonical correlations</em> and <span class="math inline">\(m=\min(p,q)\)</span>.</p>
<section id="how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal" class="level3">
<h3 class="anchored" data-anchor-id="how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal">How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</h3>
<ul>
<li>Use Singular Value Decomposition (SVD) of matrix <span class="math inline">\(\boldsymbol K\)</span>:<br>
<span class="math display">\[\boldsymbol K= (\boldsymbol Q_{\boldsymbol x}^{\text{CCA}})^T  \boldsymbol \Lambda\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\]</span> where <span class="math inline">\(\boldsymbol \Lambda\)</span> is the diagonal matrix containing the singular values of <span class="math inline">\(\boldsymbol K\)</span></li>
<li>This yields orthogonal matrices <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> and thus the desired whitening matrices <span class="math inline">\(\boldsymbol W_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol W_{\boldsymbol y}^{\text{CCA}}\)</span></li>
<li>As a result <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y}) = \boldsymbol \Lambda\)</span> i.e.&nbsp;singular values <span class="math inline">\(\lambda_i\)</span> of <span class="math inline">\(\boldsymbol K\)</span> are the desired canonical correlations!</li>
</ul>
<p><span class="math inline">\(\longrightarrow\)</span> <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> are determined by the diagonality constraint (and note these are different to the other previously discussed whitening methods).</p>
<p>Note that the signs of corresponding in columns in <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> are not identified. Traditionally, in an SVD the signs are chosen such that the singular values are positive. However, if we impose positive-diagonality on <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span>, and thus positive-diagonality on the cross-correlations <span class="math inline">\(\boldsymbol \Psi_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol \Psi_{\boldsymbol y}\)</span>, then the canonical correlations may take on both positive and negative values.</p>
</section>
<section id="related-methods" class="level3">
<h3 class="anchored" data-anchor-id="related-methods">Related methods</h3>
<ul>
<li><p>O2PLS: similar to CCA but using orthogonal projections rather than whitening.</p></li>
<li><p>Vector correlation: aggregates the squared canonical correlations into a single overall measure (see below).</p></li>
</ul>
</section>
</section>
<section id="vector-correlation-and-rv-coefficient" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="vector-correlation-and-rv-coefficient"><span class="header-section-number">6.3</span> Vector correlation and RV coefficient</h2>
<section id="vector-alienation-coefficient" class="level3">
<h3 class="anchored" data-anchor-id="vector-alienation-coefficient">Vector alienation coefficient</h3>
<p>In his 1936 paper introducing canonical correlation analysis Hotelling also proposed the <em>vector alienation coefficient</em> defined as <span class="math display">\[
\begin{split}
a(\boldsymbol x, \boldsymbol y) &amp;= \frac{\det(\boldsymbol P)}{\det(\boldsymbol P_{\text{indep}}) } \\
            &amp; = \frac{\det( \boldsymbol P) }{  \det(\boldsymbol P_{\boldsymbol x}) \,  \det(\boldsymbol P_{\boldsymbol y})  }
\end{split}
\]</span> With <span class="math inline">\(\boldsymbol K= \boldsymbol P_{\boldsymbol x}^{-1/2} \boldsymbol P_{\boldsymbol x\boldsymbol y}  \boldsymbol P_{\boldsymbol y}^{-1/2}\)</span> the vector alienation coefficient can be written (using the Weinstein-Aronszajn determinant identity and the formula for the determinant of block-structured matrices) as <span class="math display">\[
\begin{split}
a(\boldsymbol x, \boldsymbol y) &amp; = \det \left( \boldsymbol I_p - \boldsymbol K\boldsymbol K^T \right) \\
            &amp; = \det \left( \boldsymbol I_q - \boldsymbol K^T \boldsymbol K\right) \\
            &amp;= \prod_{i=1}^m (1-\lambda_i^2) \\
\end{split}
\]</span> where the <span class="math inline">\(\lambda_i\)</span> are the singular values of <span class="math inline">\(\boldsymbol K\)</span>, i.e.&nbsp;the canonical correlations for the pair <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>. Therefore, the vector alienation coefficient is computed as a summary statistic of the canonical correlations.</p>
<p>If <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol y} = 0\)</span> und thus <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are uncorrelated then <span class="math inline">\(\boldsymbol P= \boldsymbol P_{\text{indep}}\)</span> and thus by construction the vector alienation coefficient <span class="math inline">\(a(\boldsymbol x, \boldsymbol y)=1\)</span>. Hence, the vector alienation coefficient is itself not a generalisation of the squared multiple correlation to the case of two random vectors as such a quantity should vanish in this case.</p>
</section>
<section id="rozeboom-vector-correlation" class="level3">
<h3 class="anchored" data-anchor-id="rozeboom-vector-correlation">Rozeboom vector correlation</h3>
<p>Instead, Rozeboom (1965) <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> proposed to use as squared <em>vector correlation</em> the complement of the vector alienation coefficient <span class="math display">\[
\begin{split}
\text{VCor}(\boldsymbol x, \boldsymbol y)^2 &amp;= \rho^2_{\boldsymbol x\boldsymbol y} = 1 - a(\boldsymbol x, \boldsymbol y) \\
&amp; = 1- \det\left( \boldsymbol I_p - \boldsymbol K\boldsymbol K^T \right) \\
&amp; = 1- \det\left( \boldsymbol I_q - \boldsymbol K^T \boldsymbol K\right) \\
&amp;  =1- \prod_{i=1}^m (1-\lambda_i^2) \\
\end{split}
\]</span> If <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol y} = 0\)</span> then <span class="math inline">\(\boldsymbol K=\mathbf 0\)</span> and hence <span class="math inline">\(\text{VCor}(\boldsymbol x, \boldsymbol y)^2 = 0\)</span>.</p>
<p>Moreover, if either <span class="math inline">\(p=1\)</span> or <span class="math inline">\(q=1\)</span> the squared vector correlation reduces to the corresponding squared multiple correlation, which in turn for both <span class="math inline">\(p=1\)</span> and <span class="math inline">\(q=1\)</span> becomes the squared Pearson correlation.</p>
<p>You can find the derivation in Example Sheet 10.</p>
<p>Thus, Rozeboom’s vector correlation indeed generalises both Pearson correlation and the multiple correlation coefficient.</p>
</section>
<section id="rv-coefficient" class="level3">
<h3 class="anchored" data-anchor-id="rv-coefficient">RV coefficient</h3>
<p>Another common approach to measure association between two random vectors is the <a href="https://en.wikipedia.org/wiki/RV_coefficient">RV coefficient</a> introduced by Robert and Escoufier in 1976 as <span class="math display">\[
RV(\boldsymbol x, \boldsymbol y) = \frac{ \text{Tr}(\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} \boldsymbol \Sigma_{\boldsymbol y\boldsymbol x} )}{ \sqrt{ \text{Tr}(\boldsymbol \Sigma_{\boldsymbol x}^2) \text{Tr}(\boldsymbol \Sigma_{\boldsymbol y}^2)  } }
\]</span> The main advantage of the RV coefficient is that it is easier to compute than the Rozeboom vector correlation as it uses the matrix trace rather than the matrix determinant.</p>
<p>For <span class="math inline">\(q=p=1\)</span> the RV coefficient reduces to the squared correlation. However, the RV coefficient does <em>not</em> reduce to the multiple correlation coefficient for <span class="math inline">\(q=1\)</span> and <span class="math inline">\(p &gt; 1\)</span>, and therefore the RV coefficient cannot be considered a coherent generalisation of Pearson and multiple correlation to the case when <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are random vectors.</p>
<p>See also Worksheet 10.</p>
</section>
</section>
<section id="limits-of-linear-models-and-correlation" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="limits-of-linear-models-and-correlation"><span class="header-section-number">6.4</span> Limits of linear models and correlation</h2>
<section id="correlation-measures-only-linear-dependence" class="level3">
<h3 class="anchored" data-anchor-id="correlation-measures-only-linear-dependence">Correlation measures only linear dependence</h3>
<p>Linear models and measures of linear association (correlation) are very effective tools. However, it is important to recognise their limits especially when modelling nonlinear relationships.</p>
<p>A very simple demonstration of this is given by the following example. Assume <span class="math inline">\(x\)</span> is a normally distributed random variable with <span class="math inline">\(x \sim N(0, \sigma^2)\)</span> with mean zero and some variance <span class="math inline">\(\sigma^2\)</span>. From <span class="math inline">\(x\)</span> we construct a second random variable <span class="math inline">\(y = x^2\)</span>. Despite that <span class="math inline">\(y\)</span> is a function of <span class="math inline">\(x\)</span> with no extra added noise it is easy to show that <span class="math inline">\(\text{Cov}(x, y) = \text{Cov}(x, x^2)=0\)</span>. Hence, the correlation <span class="math inline">\(\text{Cor}(x, y)= \text{Cor}(x, x^2) = 0\)</span> also vanishes.</p>
<p>This can be empirically verified by simulating data from a normal distribution (here with <span class="math inline">\(\sigma^2=4)\)</span> and estimating the correlation:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>x<span class="ot">=</span><span class="fu">rnorm</span>(<span class="dv">1000000</span>, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="dv">2</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> x<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(x,y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.001244455</code></pre>
</div>
</div>
<p>Thus, correlation is zero even though <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are fully dependent variables. This is because correlation only measures linear association, and the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is nonlinear.</p>
</section>
<section id="anscombe-datasets" class="level3">
<h3 class="anchored" data-anchor-id="anscombe-datasets">Anscombe datasets</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-anscombe" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="p">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-anscombe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-dependence_files/figure-html/fig-anscombe-1.png" class="img-fluid figure-img" data-fig-pos="p" width="432">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-anscombe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: The Anscombe (1973) quartet of datasets.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Using correlation (and more generally linear models) blindly can easily hide the underlying complexity of the analysed data. This is demonstrated by the classic “Anscombe quartet” of datasets presented in his 1973 paper <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>As evident from the scatter plots (<a href="#fig-anscombe" class="quarto-xref">Figure&nbsp;<span>6.1</span></a>) the relationship between the two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is very different in the four cases. However, intriguingly all four data sets share exactly the same linear characteristics and summary statistics:</p>
<ul>
<li>Means <span class="math inline">\(m_x = 9\)</span> and <span class="math inline">\(m_y = 7.5\)</span></li>
<li>Variances <span class="math inline">\(s^2_x = 11\)</span> and <span class="math inline">\(s^2_y = 4.13\)</span></li>
<li>Correlation <span class="math inline">\(r = 0.8162\)</span></li>
<li>Linear model fit with intercept <span class="math inline">\(a=3.0\)</span> and slope <span class="math inline">\(b=0.5\)</span></li>
</ul>
<p>Thus, in actual data analysis it is always a <strong>good idea to inspect the data visually</strong> to get a first impression whether using a linear model makes sense.</p>
<p>In the above only data set “a” follows a linear model. Data set “b” represents a quadratic relationship. Data set “c” is linear but with an outlier that disturbs the linear relationship. Finally data set “d” also contains an outlier but also represent a case where <span class="math inline">\(y\)</span> is (apart from the outlier) is not dependent on <span class="math inline">\(x\)</span>.</p>
<p>In the Worksheet 10 a more recent version of the Anscombe quartet will be analysed in the form of the “datasauRus” dozen - 13 highly nonlinear datasets that all share the same linear characteristics.</p>
</section>
</section>
<section id="mutual-information-as-generalisation-of-correlation" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="mutual-information-as-generalisation-of-correlation"><span class="header-section-number">6.5</span> Mutual information as generalisation of correlation</h2>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<p>A more general way than the vector correlation to measure multivariate association is mutual information (MI) which not only covers linear but also non-linear associations.</p>
<p>As we will see below the Rozeboom vector correlation arises naturally when computing the MI for the multivariate normal distribution, hence MI also recovers well-known measures of linear association (including multiple correlation and simple correlation), thus truly generalising correlation as measure of association.</p>
</section>
<section id="definition-of-mutual-information" class="level3">
<h3 class="anchored" data-anchor-id="definition-of-mutual-information">Definition of mutual information</h3>
<p>Recall the definition of Kullback-Leibler (KL) divergence between two distributions: <span class="math display">\[
D_{\text{KL}}(F,  G) := \text{E}_F \log \biggl( \frac{f(\boldsymbol x)}{g(\boldsymbol x)} \biggr)
\]</span> Here <span class="math inline">\(F\)</span> plays the role of the reference distribution and <span class="math inline">\(G\)</span> is an approximating distribution, with <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> being the corresponding density functions (see <a href="https://strimmerlab.github.io/publications/lecture-notes/MATH27720-stats2/">MATH27720 Statistics 2</a> for more details about the KL divergence and its properties).</p>
<p>The <em>Mutual Information</em> (MI) between two random variables <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> is defined as the KL divergence between the corresponding joint distribution and the product distribution: <span class="math display">\[
\text{MI}(\boldsymbol x, \boldsymbol y) = D_{\text{KL}}(F_{\boldsymbol x,\boldsymbol y}, F_{\boldsymbol x}  F_{\boldsymbol y}) = \text{E}_{F_{\boldsymbol x,\boldsymbol y}}  \log \biggl( \frac{f(\boldsymbol x, \boldsymbol y)}{f(\boldsymbol x) \, f(\boldsymbol y)} \biggr) .
\]</span> Thus, MI measures how well the joint distribution can be approximated by the product distribution (which would be the appropriate joint distribution if <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are independent). Since MI is an application of KL divergence is shares all its properties. In particular, <span class="math inline">\(\text{MI}(\boldsymbol x, \boldsymbol y)=0\)</span> implies that the joint distribution and product distributions are the same. Hence the two random variables <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are independent if the mutual information vanishes.</p>
</section>
<section id="mutual-information-between-two-normal-scalar-variables" class="level3">
<h3 class="anchored" data-anchor-id="mutual-information-between-two-normal-scalar-variables">Mutual information between two normal scalar variables</h3>
<p>The KL divergence between two multivariate normal distributions <span class="math inline">\(F_{\text{ref}}\)</span> and <span class="math inline">\(F\)</span> is <span class="math display">\[
D_{\text{KL}}(F_{\text{ref}}, F)  = \frac{1}{2}   \biggl\{
        (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})
      + \text{Tr}\biggl(\boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
    - \log \det \biggl( \boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
     - d   \biggr\}
\]</span> This allows compute the mutual information <span class="math inline">\(\text{MI}_{\text{norm}}(x,y)\)</span> between two univariate random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> that are correlated and assumed to be jointly bivariate normal. Let <span class="math inline">\(\boldsymbol z= (x, y)^T\)</span>. The joint bivariate normal distribution is characterised by the mean <span class="math inline">\(\text{E}(\boldsymbol z) = \boldsymbol \mu= (\mu_x, \mu_y)^T\)</span> and the covariance matrix <span class="math display">\[
\boldsymbol \Sigma=
\begin{pmatrix}
\sigma^2_x &amp; \rho \, \sigma_x \sigma_y \\
\rho \, \sigma_x  \sigma_y &amp; \sigma^2_y \\
\end{pmatrix}
\]</span> where <span class="math inline">\(\text{Cor}(x,y)= \rho\)</span>. If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent then <span class="math inline">\(\rho=0\)</span> and <span class="math display">\[
\boldsymbol \Sigma_{\text{indep}} =
\begin{pmatrix} \sigma^2_x &amp; 0 \\ 0 &amp; \sigma^2_y \\ \end{pmatrix} \,.
\]</span> The product <span class="math display">\[
\boldsymbol A= \boldsymbol \Sigma_{\text{indep}}^{-1} \boldsymbol \Sigma=
\begin{pmatrix}
1 &amp; \rho \frac{\sigma_y}{\sigma_x} \\
\rho \frac{\sigma_x}{\sigma_y} &amp; 1 \\
\end{pmatrix}
\]</span> has trace <span class="math inline">\(\text{Tr}(\boldsymbol A) = 2\)</span> and determinant <span class="math inline">\(\det(\boldsymbol A) = 1-\rho^2\)</span>.</p>
<p>With this the mutual information between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> can be computed as <span class="math display">\[
\begin{split}
\text{MI}_{\text{norm}}(x, y) &amp;= D_{\text{KL}}(N(\boldsymbol \mu, \boldsymbol \Sigma),  N(\boldsymbol \mu,\boldsymbol \Sigma_{\text{indep}} )) \\
&amp; = \frac{1}{2}   \biggl\{
         \text{Tr}\biggl(\boldsymbol \Sigma^{-1}_{\text{indep}} \boldsymbol \Sigma\biggr)
    - \log \det \biggl( \boldsymbol \Sigma^{-1}_{\text{indep}} \boldsymbol \Sigma\biggr)
     - 2   \biggr\} \\
&amp; = \frac{1}{2}   \biggl\{
         \text{Tr}( \boldsymbol A)
    - \log \det( \boldsymbol A)
     - 2   \biggr\} \\
&amp;=  -\frac{1}{2} \log(1-\rho^2) \\
  &amp; \approx \frac{\rho^2}{2} \\
\end{split}
\]</span></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-micor" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-micor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-dependence_files/figure-html/fig-micor-1.png" class="img-fluid figure-img" data-fig-pos="t" width="432">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-micor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: Relationship between correlation and mutual information.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Thus <span class="math inline">\(\text{MI}_{\text{norm}}(x,y)\)</span> is a one-to-one function of the squared correlation <span class="math inline">\(\rho^2\)</span> between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (see <a href="#fig-micor" class="quarto-xref">Figure&nbsp;<span>6.2</span></a>).</p>
<p>For small values of <span class="math inline">\(|\rho| &lt; 0.5\)</span> around zero the relationship between mutual information and correlation is quadratic, <span class="math inline">\(2 \, \text{MI}_{\text{norm}}(x,y) \approx \rho^2\)</span> (dotted line in <a href="#fig-micor" class="quarto-xref">Figure&nbsp;<span>6.2</span></a>).</p>
</section>
<section id="mutual-information-between-two-normally-distributed-random-vectors" class="level3">
<h3 class="anchored" data-anchor-id="mutual-information-between-two-normally-distributed-random-vectors">Mutual information between two normally distributed random vectors</h3>
<p>The mutual information <span class="math inline">\(\text{MI}_{\text{norm}}(\boldsymbol x,\boldsymbol y)\)</span> between two multivariate normal random vector <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> can be computed in a similar fashion as in the bivariate case.</p>
<p>Let <span class="math inline">\(\boldsymbol z= (\boldsymbol x, \boldsymbol y)^T\)</span> with dimension <span class="math inline">\(d=p+q\)</span>. The joint multivariate normal distribution is characterised by the mean <span class="math inline">\(\text{E}(\boldsymbol z) = \boldsymbol \mu= (\boldsymbol \mu_x^T, \boldsymbol \mu_y^T)^T\)</span> and the covariance matrix <span class="math display">\[
\boldsymbol \Sigma=
\begin{pmatrix} \boldsymbol \Sigma_{\boldsymbol x} &amp; \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} \\
\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y}^T &amp; \boldsymbol \Sigma_{\boldsymbol y} \\
\end{pmatrix} \,.
\]</span> If <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are independent then <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} = 0\)</span> and <span class="math display">\[
\boldsymbol \Sigma_{\text{indep}} =
\begin{pmatrix}  
\boldsymbol \Sigma_{\boldsymbol x} &amp; 0 \\
0 &amp; \boldsymbol \Sigma_{\boldsymbol y} \\
\end{pmatrix} \, .
\]</span> The product <span class="math display">\[
\begin{split}
\boldsymbol A&amp; =
\boldsymbol \Sigma_{\text{indep}}^{-1} \boldsymbol \Sigma=
\begin{pmatrix}
\boldsymbol I_p &amp; \boldsymbol \Sigma_{\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} \\
\boldsymbol \Sigma_{\boldsymbol y}^{-1} \boldsymbol \Sigma_{\boldsymbol y\boldsymbol x}    &amp; \boldsymbol I_q \\
\end{pmatrix} \\
&amp; =
\begin{pmatrix}
\boldsymbol I_p &amp;  \boldsymbol V_{\boldsymbol x}^{-1/2} \boldsymbol P_{\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol x\boldsymbol y} \boldsymbol V_{\boldsymbol y}^{1/2} \\
\boldsymbol V_{\boldsymbol y}^{-1/2} \boldsymbol P_{\boldsymbol y}^{-1} \boldsymbol P_{\boldsymbol y\boldsymbol x} \boldsymbol V_{\boldsymbol x}^{1/2}   &amp; \boldsymbol I_q \\
\end{pmatrix}\\
\end{split}
\]</span> has trace <span class="math inline">\(\text{Tr}(\boldsymbol A) = d\)</span> and determinant <span class="math display">\[
\begin{split}
\det(\boldsymbol A) &amp; = \det( \boldsymbol I_p - \boldsymbol K\boldsymbol K^T ) \\
  &amp;= \det( \boldsymbol I_q - \boldsymbol K^T \boldsymbol K) \\
\end{split}
\]</span> with <span class="math inline">\(\boldsymbol K= \boldsymbol P_{\boldsymbol x}^{-1/2} \boldsymbol P_{\boldsymbol x\boldsymbol y} \boldsymbol P_{\boldsymbol y}^{-1/2}\)</span>. With <span class="math inline">\(\lambda_1, \ldots, \lambda_m\)</span> the singular values of <span class="math inline">\(\boldsymbol K\)</span> (i.e. the canonical correlations between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>) we get <span class="math display">\[
\det(\boldsymbol A) =  \prod_{i=1}^m (1-\lambda_i^2)
\]</span></p>
<p>The mutual information between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> is then <span class="math display">\[
\begin{split}
\text{MI}_{\text{norm}}(\boldsymbol x, \boldsymbol y) &amp;= D_{\text{KL}}(N(\boldsymbol \mu, \boldsymbol \Sigma),  N(\boldsymbol \mu,\boldsymbol \Sigma_{\text{indep}} )) \\
&amp; = \frac{1}{2}   \biggl\{
         \text{Tr}\biggl( \boldsymbol \Sigma^{-1}_{\text{indep}} \boldsymbol \Sigma\biggr)
    - \log \det \biggl( \boldsymbol \Sigma^{-1}_{\text{indep}} \boldsymbol \Sigma\biggr)
     - d   \biggr\} \\
&amp; = \frac{1}{2}   \biggl\{
         \text{Tr}( \boldsymbol A)
    - \log \det( \boldsymbol A)
     - d   \biggr\} \\
&amp;=-\frac{1}{2} \sum_{i=1}^m \log(1-\lambda_i^2)\\
&amp;=-\frac{1}{2} \log \left( \prod_{i=1}^m (1-\lambda_i^2) \right)\\
&amp;=-\frac{1}{2} \log \left( 1- \text{VCor}(\boldsymbol x, \boldsymbol y)^2 \right)\\
\end{split}
\]</span></p>
<p>From the above we see that <span class="math inline">\(\text{MI}_{\text{norm}}(\boldsymbol x, \boldsymbol y)\)</span> is simply the sum of the MIs resulting from the individual canonical correlations <span class="math inline">\(\lambda_i\)</span> with the same functional link between the MI and the squared correlation as in the bivariate normal case.</p>
<p>Furthermore we obtain that <span class="math display">\[
\text{MI}_{\text{norm}}(\boldsymbol x,\boldsymbol y) = -\frac{1}{2} \log(1 - \text{VCor}(\boldsymbol x, \boldsymbol y)^2 ) \approx \frac{1}{2} \text{VCor}(\boldsymbol x, \boldsymbol y)^2
\]</span> Thus, in the multivariate case <span class="math inline">\(\text{MI}_{\text{norm}}(\boldsymbol x\boldsymbol y)\)</span> has again exactly the same functional relationship with the squared vector correlation <span class="math inline">\(\text{VCor}(\boldsymbol x, \boldsymbol y)^2\)</span> as the <span class="math inline">\(\text{MI}_{\text{norm}}(x, y)\)</span> for two univariate variables with squared Pearson correlation <span class="math inline">\(\rho^2\)</span>.</p>
<p>Thus, Rozeboom’s vector correlation emerges as a special case of mutual information computed for jointly multivariate normally distributed variables.</p>
</section>
<section id="using-mi-for-variable-selection" class="level3">
<h3 class="anchored" data-anchor-id="using-mi-for-variable-selection">Using MI for variable selection</h3>
<p>A very general way to write down a model predicting <span class="math inline">\(\boldsymbol y\)</span> by <span class="math inline">\(\boldsymbol x\)</span> is as follows:</p>
<ul>
<li><span class="math inline">\(F_{\boldsymbol y|\boldsymbol x}\)</span> is a conditional distribution of <span class="math inline">\(\boldsymbol y\)</span> given predictors <span class="math inline">\(\boldsymbol x\)</span> and</li>
<li><span class="math inline">\(F_{\boldsymbol y}\)</span> is the marginal distribution of <span class="math inline">\(\boldsymbol y\)</span> without predictors.</li>
</ul>
<p>Typically <span class="math inline">\(F_{\boldsymbol y|\boldsymbol x}\)</span> is a complex model and <span class="math inline">\(F_{\boldsymbol y}\)</span> a simple model (no predictors). Note that the predictive model can assume any form (incl.&nbsp;nonlinear).</p>
<p>Intriguingly the expected KL divergence between the conditional and the marginal distribution <span class="math display">\[
\text{E}_{F_{\boldsymbol x}}\, D_{\text{KL}}(F_{\boldsymbol y|\boldsymbol x},  F_{\boldsymbol y} ) = \text{MI}(\boldsymbol x, \boldsymbol y)
\]</span> is equal to mutual information between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>! Thus <span class="math inline">\(\text{MI}(\boldsymbol x, \boldsymbol y)\)</span> measures the impact of conditioning. If the MI is small (i.e.&nbsp;close to zero) then <span class="math inline">\(\boldsymbol x\)</span> is not useful in predicting <span class="math inline">\(\boldsymbol y\)</span>.</p>
<p>The above identity can be verified as follows. The KL divergence between <span class="math inline">\(F_{\boldsymbol y|\boldsymbol x}\)</span> and <span class="math inline">\(F_{\boldsymbol y}\)</span> is given by <span class="math display">\[
D_{\text{KL}}(F_{\boldsymbol y|\boldsymbol x}, F_{\boldsymbol y} )  = \text{E}_{F_{\boldsymbol y|\boldsymbol x}}  \log\biggl( \frac{f(\boldsymbol y|\boldsymbol x) }{ f(\boldsymbol y)}  \biggr) \, ,
\]</span> which is a random variable since it depends on <span class="math inline">\(\boldsymbol x\)</span>. Taking the expectation with regard to <span class="math inline">\(F_{\boldsymbol x}\)</span> (the distribution of <span class="math inline">\(\boldsymbol x\)</span>) we get <span class="math display">\[
\begin{split}
\text{E}_{F_{\boldsymbol x}} D_{\text{KL}}(F_{\boldsymbol y|\boldsymbol x}, F_{\boldsymbol y} ) &amp;=
\text{E}_{F_{\boldsymbol x}}  \text{E}_{F_{\boldsymbol y|\boldsymbol x}}  \log \biggl(\frac{ f(\boldsymbol y|\boldsymbol x) f(\boldsymbol x) }{ f(\boldsymbol y) f(\boldsymbol x) } \biggr)\\
&amp; =
\text{E}_{F_{\boldsymbol x,\boldsymbol y}}   \log \biggl(\frac{ f(\boldsymbol x,\boldsymbol y) }{ f(\boldsymbol y) f(\boldsymbol x) } \biggr) = \text{MI}(\boldsymbol x,\boldsymbol y) \,. \\
\end{split}
\]</span></p>
<p>Because of this link of MI with conditioning the MI between response and predictor variables is often used for variable and feature selection in general models.</p>
</section>
<section id="other-measures-of-general-dependence" class="level3">
<h3 class="anchored" data-anchor-id="other-measures-of-general-dependence">Other measures of general dependence</h3>
<p>In principle, MI can be computed for any distribution and model and thus applies to both normal and non-normal models, and to both linear and nonlinear relationships.</p>
<p>Besides mutual information there are others measures of general dependence between multivariate random variables.</p>
<p>Two important measures to capture nonlinear association that have been proposed in recent literature are</p>
<ol type="i">
<li><a href="https://en.wikipedia.org/wiki/Distance_correlation">distance correlation</a> and</li>
<li>the <a href="https://en.wikipedia.org/wiki/Maximal_information_coefficient">maximal information coefficient</a> (MIC and <span class="math inline">\(\text{MIC}_e\)</span>).</li>
</ol>
</section>
</section>
<section id="graphical-models" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="graphical-models"><span class="header-section-number">6.6</span> Graphical models</h2>
<section id="purpose" class="level3">
<h3 class="anchored" data-anchor-id="purpose">Purpose</h3>
<p>Graphical models combine features from</p>
<ul>
<li>graph theory</li>
<li>probability</li>
<li>statistical inference</li>
</ul>
<p>The literature on graphical models is huge, we focus here only on two commonly used models:</p>
<ul>
<li>DAGs (directed acyclic graphs), all edges are directed, no directed loops (i.e.&nbsp;no cycles, hence “acyclic”)</li>
<li>GGM (Gaussian graphical models), all edges are undirected</li>
</ul>
<p>Graphical models provide probabilistic models for trees and for networks, with random variables represented by nodes in the graphs, and branches representing conditional dependencies. In this regard they generalise both the tree-based clustering approaches as well as the probabilistic non-hierarchical methods (GMMs).</p>
<p>However, the class of graphical models goes much beyond simple unsupervised learning models. It also includes regression, classification, time series models etc. For an overview see, e.g., the reference book by <span class="citation" data-cites="Murphy2023">Murphy (<a href="bibliography.html#ref-Murphy2023" role="doc-biblioref">2023</a>)</span>.</p>
</section>
<section id="basic-notions-from-graph-theory" class="level3">
<h3 class="anchored" data-anchor-id="basic-notions-from-graph-theory">Basic notions from graph theory</h3>
<ul>
<li>Mathematically, a graph <span class="math inline">\(G = (V, E)\)</span> consists of a a set of vertices or nodes <span class="math inline">\(V = \{v_1, v_2, \ldots\}\)</span> and a set of branches or edges <span class="math inline">\(E = \{ e_1, e_2, \ldots \}\)</span>.</li>
<li>Edges can be undirected or directed.</li>
<li>Graphs containing only directed edges are directed graphs, and likewise graphs containing only undirected edges are called undirected graphs. Graphs containing both directed and undirected edges are called partially directed graphs.</li>
<li>A path is a sequence of of vertices such that from each of its vertices there is an edge to the next vertex in the sequence.</li>
<li>A graph is connected when there is a path between every pair of vertices.</li>
<li>A cycle is a path in a graph that connects a node with itself.</li>
<li>A connected graph with no cycles is a called a tree.</li>
<li>The degree of a node is the number of edges it connects with. If edges are all directed the degree of a node is the sum of the in-degree and out-degree, which counts the incoming and outgoing edges, respectively.</li>
<li>External nodes are nodes with degree 1. In a tree-structured graph these are also called leaves.</li>
</ul>
<p>Some notions are only relevant for graphs with directed edges:</p>
<ul>
<li>In a directed graph the parent node(s) of vertex <span class="math inline">\(v\)</span> is the set of nodes <span class="math inline">\(\text{pa}(v)\)</span> directly connected to <span class="math inline">\(v\)</span> via edges directed from the parent node(s) towards <span class="math inline">\(v\)</span>.</li>
<li>Conversely, <span class="math inline">\(v\)</span> is called a child node of <span class="math inline">\(\text{pa}(v)\)</span>. Note that a parent node can have several child nodes, so <span class="math inline">\(v\)</span> may not be the only child of <span class="math inline">\(\text{pa}(v)\)</span>.</li>
<li>In a directed tree graph, each node has only a single parent, except for one particular node that has no parent at all (this node is called the root node).</li>
<li>A DAG, or directed acyclic graph, is a directed graph with no directed cycles. A (directed) tree is a special version of a DAG.</li>
</ul>
</section>
<section id="probabilistic-graphical-models" class="level3">
<h3 class="anchored" data-anchor-id="probabilistic-graphical-models">Probabilistic graphical models</h3>
<p>A graphical model uses a graph to describe the relationship between random variables <span class="math inline">\(x_1, \ldots, x_d\)</span>. The variables are assumed to have a joint distribution with density/mass function <span class="math inline">\(p(x_1, x_2, \ldots, x_d)\)</span>. Each random variable is placed in a node of the graph.</p>
<p>The structure of the graph and the type of the edges connecting (or not connecting) any pair of nodes/variables is used to describe the conditional dependencies, and to simplify the joint distribution.</p>
<p>Thus, a graphical model is in essence a visualisation of the joint distribution using structural information from the graph helping to understand the mutual relationship among the variables.</p>
</section>
<section id="directed-graphical-models" class="level3">
<h3 class="anchored" data-anchor-id="directed-graphical-models">Directed graphical models</h3>
<p>In a <strong>directed graphical model</strong> the graph structure is assumed to be a DAG (or a directed tree, which is also a DAG).</p>
<p>Then the joint probability distribution can be factorised into a <em>product of conditional probabilities</em> as follows: <span class="math display">\[
p(x_1, x_2, \ldots, x_d) = \prod_i p(x_i  | \text{pa}(x_i))
\]</span> Thus, the overall joint probability distribution is specified by local conditional distributions and the graph structure, with the directions of the edges providing the information about parent-child node relationships.</p>
<p>Probabilistic DAGs are also known as “Bayesian networks”.</p>
<p><strong>Idea:</strong> by trying out all possible trees/graphs and fitting them to the data using maximum likelihood (or Bayesian inference) we hope to be able identify the graph structure of the data-generating process.</p>
<p><strong>Challenges</strong></p>
<ol type="1">
<li>in the tree/network the internal nodes are usually not known, and thus have to be treated as <em>latent</em> variables.</li>
</ol>
<p><strong>Answer:</strong> To impute the states at these nodes we may use the EM algorithm as in GMMs (which in fact can be viewed as graphical models, too!).</p>
<ol start="2" type="1">
<li>If we treat the internal nodes as unknowns we need to marginalise over the internal nodes, i.e.&nbsp;we need to sum / integrate over all possible set of states of the internal nodes!</li>
</ol>
<p><strong>Answer:</strong> This can be handled very effectively using the <strong>Viterbi algorithm</strong> which is essentially an application of the generalised distributive law. In particular for tree graphs this means that the summations occurs locally at each node and propagates recursively across the tree.</p>
<ol start="3" type="1">
<li>In order to infer the tree or network structure the space of all trees or networks need to be explored. This is not possible in an exhaustive fashion unless the number of variables in the tree is very small.</li>
</ol>
<p><strong>Answer:</strong> Solution: use heuristic approaches for tree and network search!</p>
<ol start="4" type="1">
<li>Furthermore, there exist so-called “equivalence classes” of graphical models, i.e.&nbsp;sets of graphical models that share the same joint probability distribution. Thus, all graphical models within the same equivalence class cannot be distinguished from observational data, even with infinite sample size!</li>
</ol>
<p><strong>Answer:</strong> this is a fundamental mathematical problem of identifiability so there is now way around this issue. However, on the positive side, this also implies that the search through all graphical models can be restricted to finding the so-called “essential graph” (e.g.&nbsp;Anderson et al.&nbsp;1997. <a href="https://projecteuclid.org/euclid.aos/1031833662" class="uri">https://projecteuclid.org/euclid.aos/1031833662</a>).</p>
<p><strong>Conclusion: using directed graphical models for structure discovery is very time consuming and computationally demanding for anything but small toy data sets.</strong></p>
<p>This also explains why heuristic and non-model based approaches (such as hierarchical clustering) are so popular even though full statistical modelling is in principle possible.</p>
</section>
<section id="undirected-graphical-models" class="level3">
<h3 class="anchored" data-anchor-id="undirected-graphical-models">Undirected graphical models</h3>
<p>Another class of graphical models are models that contain only undirected edges. These <strong>undirected graphical models</strong> are used to represent the pairwise conditional (in)dependencies among the variables in the graph, and the resulting model is therefore also called <strong>conditional independence graph</strong>.</p>
<p>Suppose <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> are two random variables/nodes from <span class="math inline">\(\{x_1, \ldots, x_d\}\)</span>, and the set <span class="math inline">\(\{x_k\}\)</span> represents all other variables/nodes with <span class="math inline">\(k\neq i\)</span> and <span class="math inline">\(k \neq j\)</span>. Then the variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> are conditionally independent given all the other variables <span class="math inline">\(\{x_k\}\)</span> <span class="math display">\[
x_i \perp\!\!\!\perp x_j | \{x_k\}
\]</span> if the joint probability density for all variables <span class="math inline">\(\{x_1, \ldots, x_d\}\)</span> factorises as <span class="math display">\[
p(x_1, x_2, \ldots, x_d) = p(x_i | \{x_k\}) \, p(x_j | \{x_k\}) \, p(\{x_k\}) \,.
\]</span> or equivalently <span class="math display">\[
\frac{p(x_i, x_j, \ldots, x_d)}{p(\{x_k\})}  
= p(x_i, x_j | \{x_k\})
= p(x_i | \{x_k\}) \, p(x_j | \{x_k\}) \,.
\]</span></p>
<p>In the corresponding conditional independence graph note there is no edge between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>, as in such a graph <em>missing edges correspond to conditional independence</em> between the respective non-connected nodes.</p>
<section id="gaussian-graphical-model" class="level4">
<h4 class="anchored" data-anchor-id="gaussian-graphical-model">Gaussian graphical model</h4>
<p>Assuming that <span class="math inline">\(x_1, \ldots, x_d\)</span> are jointly normally distributed, i.e.&nbsp;<span class="math inline">\(\boldsymbol x\sim N(\boldsymbol \mu, \boldsymbol \Sigma)\)</span>, it turns out that it is straightforward to identify the pairwise conditional independencies. From <span class="math inline">\(\boldsymbol \Sigma\)</span> we first obtain the precision matrix <span class="math display">\[\boldsymbol \Omega= (\omega_{ij}) = \boldsymbol \Sigma^{-1} \,.\]</span> Crucially, it can be shown that <span class="math inline">\(\omega_{ij} = 0\)</span> implies <span class="math inline">\(x_i \perp\!\!\!\perp x_j \,|\, \{ x_k \}\)</span>. Hence, from the precision matrix <span class="math inline">\(\boldsymbol \Omega\)</span> we can directly read off all the pairwise conditional independencies among the variables <span class="math inline">\(x_1, x_2, \ldots, x_d\)</span>.</p>
<p>Often, the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> is dense (few zeros) but the corresponding precision matrix <span class="math inline">\(\boldsymbol \Omega\)</span> is sparse (many zeros).</p>
<p>The conditional independence graph computed for normally distributed variables is called a <strong>Gaussian graphical model</strong>, or short <strong>GGM</strong>. A further alternative name commonly used is <strong>covariance selection model</strong>.</p>
</section>
<section id="related-quantity-partial-correlation" class="level4">
<h4 class="anchored" data-anchor-id="related-quantity-partial-correlation">Related quantity: partial correlation</h4>
<p>From the precision matrix <span class="math inline">\(\boldsymbol \Omega\)</span> we can also compute the matrix of pairwise full conditional <em>partial correlations</em>:</p>
<p><span class="math display">\[
\rho_{ij|\text{rest}}=-\frac{\omega_{ij}}{\sqrt{\omega_{ii}\omega_{jj}}}
\]</span> which is essentially the standardised precision matrix (similar to correlation but with an extra minus sign!)</p>
<p>The partial correlations lie in the range between -1 and +1, <span class="math inline">\(\rho_{ij|\text{rest}} \in [-1, 1]\)</span>, just like standard correlations.</p>
<p>If <span class="math inline">\(\boldsymbol x\)</span> is multivariate normal then <span class="math inline">\(\rho_{ij|\text{rest}} = 0\)</span> indicates conditional independence between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>.</p>
<p><em>Regression interpretation:</em> partial correlation is the correlation that remains between the two variables if the effect of the other variables is “regressed away”. In other words, the partial correlation is exactly equivalent to the correlation between the residuals that remain after regressing <span class="math inline">\(x_i\)</span> on the variables <span class="math inline">\(\{x_k\}\)</span> and <span class="math inline">\(x_j\)</span> on <span class="math inline">\(\{x_k\}\)</span>.</p>
</section>
</section>
<section id="null-distribution-of-the-empirical-correlation-coefficient" class="level3">
<h3 class="anchored" data-anchor-id="null-distribution-of-the-empirical-correlation-coefficient">Null distribution of the empirical correlation coefficient</h3>
<p>Suppose we have two uncorrelated random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> with <span class="math inline">\(\rho = \text{Cor}(x, y) =0\)</span>. After observing data <span class="math inline">\(x_1, \ldots, x_n\)</span> and <span class="math inline">\(y_1, \ldots, y_n\)</span> we compute the the empirical covariance matrix <span class="math inline">\(\hat{\boldsymbol \Sigma}_{xy}\)</span> and from it the empirical correlation coefficient <span class="math inline">\(r = \widehat{\text{Cor}}(x, y)\)</span>.</p>
<p>The distribution of the empirical correlation assuming <span class="math inline">\(\rho=0\)</span> is useful as null-model for testing whether the underlying correlation is in fact zero having observed empirical correlation <span class="math inline">\(r\)</span>. If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are normally distributed with <span class="math inline">\(\rho=0\)</span> the distribution of the empirical correlation <span class="math inline">\(r\)</span> has mean <span class="math inline">\(\text{E}(r)=0\)</span> and variance <span class="math inline">\(\text{Var}(r)=\frac{1}{\kappa}\)</span>. Here <span class="math inline">\(\kappa\)</span> is the degree of freedom of the null distribution which for standard correlation is <span class="math inline">\(\kappa=n-1\)</span>. Furthermore, the squared empirical correlation is distributed according to a Beta distribution <span class="math display">\[
r^2 \sim \text{Beta}\left(\frac{1}{2}, \frac{\kappa-1}{2}\right)
\]</span></p>
<p>For partial correlation the null distribution of <span class="math inline">\(r^2\)</span> has the same form but with a different degree of freedom. Specifically, <span class="math inline">\(\kappa\)</span> is reduced by the number of variables being conditioned on. If for <span class="math inline">\(d\)</span> dimensions we condition on <span class="math inline">\(d-2\)</span> variables the resulting degree of freedom is <span class="math inline">\(\kappa =n-1 - (d-2) = n-d+1\)</span>. For <span class="math inline">\(d=2\)</span> we get back the degree of freedom for standard empirical correlation.</p>
</section>
<section id="algorithm-for-learning-ggms" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-for-learning-ggms">Algorithm for learning GGMs</h3>
<p>From the above we can devise a simple algorithm to learn Gaussian graphical model (GGM) from data:</p>
<ol type="1">
<li>Estimate covariance <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> (in such a way that it is invertible!)</li>
<li>Compute corresponding partial correlations</li>
<li>If <span class="math inline">\(\hat{\rho}_{ij|\text{rest}} \approx 0\)</span> then there is (approx.) conditional independence between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>.</li>
</ol>
<p>The test for conditional independence is done by statistical testing for vanishing partial correlation. Specifically, we compute the <span class="math inline">\(p\)</span>-value assuming that the true underlying partial correlation is zero and then decide whether to reject the null assumption of zero partial correlation.</p>
<p>If there are many edges tested simultaneously we may need to adjust (i.e reduce) the test threshold, for example applying Bonferroni or FDR methods.</p>
</section>
<section id="example-exam-score-data" class="level3">
<h3 class="anchored" data-anchor-id="example-exam-score-data">Example: exam score data</h3>
<p>This is a data set from Mardia et al.&nbsp;(1979) and features <span class="math inline">\(d=5\)</span> variables measured on <span class="math inline">\(n=88\)</span> subjects.</p>
<p>Correlations (rounded to 2 digits):</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>           mechanics vectors algebra analysis statistics
mechanics       1.00    0.55    0.55     0.41       0.39
vectors         0.55    1.00    0.61     0.49       0.44
algebra         0.55    0.61    1.00     0.71       0.66
analysis        0.41    0.49    0.71     1.00       0.61
statistics      0.39    0.44    0.66     0.61       1.00</code></pre>
</div>
</div>
<p>Partial correlations (rounded to 2 digits):</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>           mechanics vectors algebra analysis statistics
mechanics       1.00    0.33    0.23     0.00       0.02
vectors         0.33    1.00    0.28     0.08       0.02
algebra         0.23    0.28    1.00     0.43       0.36
analysis        0.00    0.08    0.43     1.00       0.25
statistics      0.02    0.02    0.36     0.25       1.00</code></pre>
</div>
</div>
<p>Note that there are no zero correlations but there are <strong>four partial correlations close to 0</strong>, indicating <strong>conditional independence</strong> between:</p>
<ul>
<li>analysis and mechanics,</li>
<li>statistics and mechanics,</li>
<li>analysis and vectors, and</li>
<li>statistics and vectors.</li>
</ul>
<p>The can be verified by computing the normal <span class="math inline">\(p\)</span>-values for the partial correlations (with <span class="math inline">\(\kappa=84\)</span> as degree of freedom):</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>           mechanics vectors algebra analysis statistics
mechanics         NA   0.002   0.034    0.988      0.823
vectors           NA      NA   0.009    0.477      0.854
algebra           NA      NA      NA    0.000      0.001
analysis          NA      NA      NA       NA      0.020
statistics        NA      NA      NA       NA         NA</code></pre>
</div>
</div>
<p>There are six edges with small <span class="math inline">\(p\)</span>-value (smaller than say 0.05) and these correspond to the edges for which the null assumption of zero partial correlation can be rejected so that out of ten possible edges four are not statistically significant. Therefore the conditional independence graph looks as follows:</p>
<pre><code>Mechanics      Analysis
   |     \    /    |
   |    Algebra    |
   |     /   \     |
 Vectors      Statistics</code></pre>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Murphy2023" class="csl-entry" role="listitem">
Murphy, K. P. 2023. <em>Probabilistic Machine Learning: Advanced Topic</em>. MIT Press. <a href="https://probml.github.io/pml-book/book2.html">https://probml.github.io/pml-book/book2.html</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Hotelling, H. 1936. Relations between two sets of variates. Biometrika <strong>28</strong>:321–377. <a href="https://doi.org/10.1093/biomet/28.3-4.321" class="uri">https://doi.org/10.1093/biomet/28.3-4.321</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Rozeboom, W. W. 1965. Linear correlations between sets of variables. Psychometrika <strong>30</strong>:57–71. <a href="https://doi.org/10.1007/BF02289747" class="uri">https://doi.org/10.1007/BF02289747</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Anscombe, F. J. 1973. Graphs in statistical analysis. The American Statistician 27:17–21. <a href="http://doi.org/10.1080/00031305.1973.10478966" class="uri">http://doi.org/10.1080/00031305.1973.10478966</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH38161");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./05-classification.html" class="pagination-link" aria-label="Supervised learning and classification">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning and classification</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./07-nonlinear.html" class="pagination-link" aria-label="Nonlinear and nonparametric models">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear and nonparametric models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>