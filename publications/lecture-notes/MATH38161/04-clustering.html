<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Multivariate Statistics and Machine Learning - 4&nbsp; Unsupervised learning and clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-classification.html" rel="next">
<link href="./03-transformations.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04-clustering.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised learning and clustering</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Multivariate Statistics and Machine Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-multivariate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Multivariate random variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multivariate estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations and dimension reduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-clustering.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised learning and clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning and classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-dependence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multivariate dependencies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear and nonparametric models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#challenges-in-unsupervised-learning" id="toc-challenges-in-unsupervised-learning" class="nav-link active" data-scroll-target="#challenges-in-unsupervised-learning"><span class="header-section-number">4.1</span> Challenges in unsupervised learning</a>
  <ul class="collapse">
  <li><a href="#objective" id="toc-objective" class="nav-link" data-scroll-target="#objective"><span class="header-section-number">4.1.1</span> Objective</a></li>
  <li><a href="#questions-and-problems" id="toc-questions-and-problems" class="nav-link" data-scroll-target="#questions-and-problems"><span class="header-section-number">4.1.2</span> Questions and problems</a></li>
  <li><a href="#why-is-clustering-difficult" id="toc-why-is-clustering-difficult" class="nav-link" data-scroll-target="#why-is-clustering-difficult"><span class="header-section-number">4.1.3</span> Why is clustering difficult?</a></li>
  <li><a href="#common-types-of-clustering-methods" id="toc-common-types-of-clustering-methods" class="nav-link" data-scroll-target="#common-types-of-clustering-methods"><span class="header-section-number">4.1.4</span> Common types of clustering methods</a></li>
  </ul></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering"><span class="header-section-number">4.2</span> Hierarchical clustering</a>
  <ul class="collapse">
  <li><a href="#tree-like-structures" id="toc-tree-like-structures" class="nav-link" data-scroll-target="#tree-like-structures"><span class="header-section-number">4.2.1</span> Tree-like structures</a></li>
  <li><a href="#agglomerative-hierarchical-clustering-algorithms" id="toc-agglomerative-hierarchical-clustering-algorithms" class="nav-link" data-scroll-target="#agglomerative-hierarchical-clustering-algorithms"><span class="header-section-number">4.2.2</span> Agglomerative hierarchical clustering algorithms</a></li>
  <li><a href="#wards-clustering-method" id="toc-wards-clustering-method" class="nav-link" data-scroll-target="#wards-clustering-method"><span class="header-section-number">4.2.3</span> Ward’s clustering method</a></li>
  <li><a href="#application-to-swiss-banknote-data-set" id="toc-application-to-swiss-banknote-data-set" class="nav-link" data-scroll-target="#application-to-swiss-banknote-data-set"><span class="header-section-number">4.2.4</span> Application to Swiss banknote data set</a></li>
  <li><a href="#assessment-of-the-uncertainty-of-hierarchical-clusterings" id="toc-assessment-of-the-uncertainty-of-hierarchical-clusterings" class="nav-link" data-scroll-target="#assessment-of-the-uncertainty-of-hierarchical-clusterings"><span class="header-section-number">4.2.5</span> Assessment of the uncertainty of hierarchical clusterings</a></li>
  </ul></li>
  <li><a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link" data-scroll-target="#k-means-clustering"><span class="header-section-number">4.3</span> <span class="math inline">\(K\)</span>-means clustering</a>
  <ul class="collapse">
  <li><a href="#set-up" id="toc-set-up" class="nav-link" data-scroll-target="#set-up"><span class="header-section-number">4.3.1</span> Set-up</a></li>
  <li><a href="#algorithm" id="toc-algorithm" class="nav-link" data-scroll-target="#algorithm"><span class="header-section-number">4.3.2</span> Algorithm</a></li>
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties"><span class="header-section-number">4.3.3</span> Properties</a></li>
  <li><a href="#choosing-the-number-of-clusters" id="toc-choosing-the-number-of-clusters" class="nav-link" data-scroll-target="#choosing-the-number-of-clusters"><span class="header-section-number">4.3.4</span> Choosing the number of clusters</a></li>
  <li><a href="#k-medoids-aka-pam" id="toc-k-medoids-aka-pam" class="nav-link" data-scroll-target="#k-medoids-aka-pam"><span class="header-section-number">4.3.5</span> <span class="math inline">\(K\)</span>-medoids aka PAM</a></li>
  <li><a href="#application-of-k-means-to-iris-data" id="toc-application-of-k-means-to-iris-data" class="nav-link" data-scroll-target="#application-of-k-means-to-iris-data"><span class="header-section-number">4.3.6</span> Application of <span class="math inline">\(K\)</span>-means to Iris data</a></li>
  <li><a href="#arbitrariness-of-cluster-labels-and-label-switching" id="toc-arbitrariness-of-cluster-labels-and-label-switching" class="nav-link" data-scroll-target="#arbitrariness-of-cluster-labels-and-label-switching"><span class="header-section-number">4.3.7</span> Arbitrariness of cluster labels and label switching</a></li>
  </ul></li>
  <li><a href="#mixture-models" id="toc-mixture-models" class="nav-link" data-scroll-target="#mixture-models"><span class="header-section-number">4.4</span> Mixture models</a>
  <ul class="collapse">
  <li><a href="#finite-mixture-model" id="toc-finite-mixture-model" class="nav-link" data-scroll-target="#finite-mixture-model"><span class="header-section-number">4.4.1</span> Finite mixture model</a></li>
  <li><a href="#total-mean-and-variance-of-a-multivariate-mixture-model" id="toc-total-mean-and-variance-of-a-multivariate-mixture-model" class="nav-link" data-scroll-target="#total-mean-and-variance-of-a-multivariate-mixture-model"><span class="header-section-number">4.4.2</span> Total mean and variance of a multivariate mixture model</a></li>
  <li><a href="#total-variation" id="toc-total-variation" class="nav-link" data-scroll-target="#total-variation"><span class="header-section-number">4.4.3</span> Total variation</a></li>
  <li><a href="#univariate-mixture" id="toc-univariate-mixture" class="nav-link" data-scroll-target="#univariate-mixture"><span class="header-section-number">4.4.4</span> Univariate mixture</a></li>
  <li><a href="#example-of-mixtures" id="toc-example-of-mixtures" class="nav-link" data-scroll-target="#example-of-mixtures"><span class="header-section-number">4.4.5</span> Example of mixtures</a></li>
  <li><a href="#sampling-from-a-mixture-model" id="toc-sampling-from-a-mixture-model" class="nav-link" data-scroll-target="#sampling-from-a-mixture-model"><span class="header-section-number">4.4.6</span> Sampling from a mixture model</a></li>
  </ul></li>
  <li><a href="#fitting-mixture-models-to-data-and-inferring-the-latent-states" id="toc-fitting-mixture-models-to-data-and-inferring-the-latent-states" class="nav-link" data-scroll-target="#fitting-mixture-models-to-data-and-inferring-the-latent-states"><span class="header-section-number">4.5</span> Fitting mixture models to data and inferring the latent states</a>
  <ul class="collapse">
  <li><a href="#observed-and-latent-variables" id="toc-observed-and-latent-variables" class="nav-link" data-scroll-target="#observed-and-latent-variables"><span class="header-section-number">4.5.1</span> Observed and latent variables</a></li>
  <li><a href="#complete-data-likelihood-and-observed-data-likelihood" id="toc-complete-data-likelihood-and-observed-data-likelihood" class="nav-link" data-scroll-target="#complete-data-likelihood-and-observed-data-likelihood"><span class="header-section-number">4.5.2</span> Complete data likelihood and observed data likelihood</a></li>
  <li><a href="#fitting-the-mixture-model-to-the-observed-data" id="toc-fitting-the-mixture-model-to-the-observed-data" class="nav-link" data-scroll-target="#fitting-the-mixture-model-to-the-observed-data"><span class="header-section-number">4.5.3</span> Fitting the mixture model to the observed data</a></li>
  <li><a href="#predicting-the-group-allocation-of-a-given-sample" id="toc-predicting-the-group-allocation-of-a-given-sample" class="nav-link" data-scroll-target="#predicting-the-group-allocation-of-a-given-sample"><span class="header-section-number">4.5.4</span> Predicting the group allocation of a given sample</a></li>
  </ul></li>
  <li><a href="#application-of-gaussian-mixture-models" id="toc-application-of-gaussian-mixture-models" class="nav-link" data-scroll-target="#application-of-gaussian-mixture-models"><span class="header-section-number">4.6</span> Application of Gaussian mixture models</a>
  <ul class="collapse">
  <li><a href="#choosing-the-number-of-classes" id="toc-choosing-the-number-of-classes" class="nav-link" data-scroll-target="#choosing-the-number-of-classes"><span class="header-section-number">4.6.1</span> Choosing the number of classes</a></li>
  <li><a href="#application-of-gmms-to-iris-flower-data" id="toc-application-of-gmms-to-iris-flower-data" class="nav-link" data-scroll-target="#application-of-gmms-to-iris-flower-data"><span class="header-section-number">4.6.2</span> Application of GMMs to Iris flower data</a></li>
  </ul></li>
  <li><a href="#the-em-algorithm" id="toc-the-em-algorithm" class="nav-link" data-scroll-target="#the-em-algorithm"><span class="header-section-number">4.7</span> The EM algorithm</a>
  <ul class="collapse">
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation"><span class="header-section-number">4.7.1</span> Motivation</a></li>
  <li><a href="#the-em-algorithm-1" id="toc-the-em-algorithm-1" class="nav-link" data-scroll-target="#the-em-algorithm-1"><span class="header-section-number">4.7.2</span> The EM algorithm</a></li>
  <li><a href="#em-algorithm-for-multivariate-normal-mixture-model" id="toc-em-algorithm-for-multivariate-normal-mixture-model" class="nav-link" data-scroll-target="#em-algorithm-for-multivariate-normal-mixture-model"><span class="header-section-number">4.7.3</span> EM algorithm for multivariate normal mixture model</a></li>
  <li><a href="#convergence-and-invariant-states" id="toc-convergence-and-invariant-states" class="nav-link" data-scroll-target="#convergence-and-invariant-states"><span class="header-section-number">4.7.4</span> Convergence and invariant states</a></li>
  <li><a href="#connection-with-the-k-means-clustering-method" id="toc-connection-with-the-k-means-clustering-method" class="nav-link" data-scroll-target="#connection-with-the-k-means-clustering-method"><span class="header-section-number">4.7.5</span> Connection with the <span class="math inline">\(K\)</span>-means clustering method</a></li>
  <li><a href="#why-the-em-algorithm-works-an-entropy-point-of-view" id="toc-why-the-em-algorithm-works-an-entropy-point-of-view" class="nav-link" data-scroll-target="#why-the-em-algorithm-works-an-entropy-point-of-view"><span class="header-section-number">4.7.6</span> Why the EM algorithm works — an entropy point of view</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised learning and clustering</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="challenges-in-unsupervised-learning" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="challenges-in-unsupervised-learning"><span class="header-section-number">4.1</span> Challenges in unsupervised learning</h2>
<section id="objective" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="objective"><span class="header-section-number">4.1.1</span> Objective</h3>
<p>We observe data <span class="math inline">\(\symbfit x_1, \ldots, \symbfit x_n\)</span> for <span class="math inline">\(n\)</span> objects (or subjects). Each sample <span class="math inline">\(\symbfit x_i\)</span> is a vector of dimension <span class="math inline">\(d\)</span>. Thus, for each of the <span class="math inline">\(n\)</span> objects / subjects we have measurements on <span class="math inline">\(d\)</span> variables. The aim of unsupervised learning is to identify patters relating the objects/subjects based on the information available in <span class="math inline">\(\symbfit x_i\)</span>. Note that in unsupervised learning we use <em>only</em> the information in <span class="math inline">\(\symbfit x_i\)</span> and nothing else.</p>
<p>For illustration consider the first two principal components of the Iris flower data (see e.g.&nbsp;Worksheet 5):</p>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Clearly there is a group structure among the samples that is linked to particular patterns in the first two principal components.</p>
<p>Note that in this plot we have used additional information, the class labels (setosa, versicolor, virginica), to highlighting the true underlying structure (the three flower species).</p>
<p>In <strong>unsupervised learning</strong> the class labels are (assumed to be) unknown, and the aim is to infer the clustering and thus the classes labels. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>There are many methods for clustering and unsupervise learning, both purely algorithmic as well as probabilistic. In this chapter we will study a few of the most commonly used approaches.</p>
</section>
<section id="questions-and-problems" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="questions-and-problems"><span class="header-section-number">4.1.2</span> Questions and problems</h3>
<p>In order to implement unsupervised learning we need to address a number of questions:</p>
<ul>
<li>how do we define clusters?</li>
<li>how do we learn / infer clusters?</li>
<li>how many clusters are there? (this is surprisingly difficult!)</li>
<li>how can we assess the uncertainty of clusters?</li>
</ul>
<p>Once we know the clusters we are also interested in:</p>
<ul>
<li>which features define / separate each cluster?</li>
</ul>
<p>(note this is a feature / variable selection problem, discussed in in supervised learning).</p>
<p>Many of these problems and questions are highly specific to the data at hand. Correspondingly, there are many different types of methods and models for clustering and unsupervised learning.</p>
<p>In terms of representing the data, unsupervised learning tries to balance between the following two extremes:</p>
<ol type="1">
<li>all objects are grouped into a single cluster (low complexity model)</li>
<li>all objects are put into their own cluster (high complexity model)</li>
</ol>
<p>In practise, the aim is to find a compromise, i.e.&nbsp;a model that captures the structure in the data with appropriate complexity — not too low and not too complex.</p>
</section>
<section id="why-is-clustering-difficult" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="why-is-clustering-difficult"><span class="header-section-number">4.1.3</span> Why is clustering difficult?</h3>
<p><strong>Partioning problem</strong> (combinatorics): How many partitions of <span class="math inline">\(n\)</span> objects (say flowers) into <span class="math inline">\(K\)</span> groups (say species) exists?</p>
<p><strong>Answer:</strong></p>
<p><span class="math display">\[
S(n,K) = \left\{\begin{array}{l} n \\ K \end{array} \right\}
\]</span> this is the “Sterling number of the second type”.</p>
<p>For large n: <span class="math display">\[
S(n,K) \approx \frac{K^n }{ K!}
\]</span> Example:</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(n\)</span></th>
<th><span class="math inline">\(K\)</span></th>
<th>Number of possible partitions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>15</td>
<td>3</td>
<td><span class="math inline">\(\approx\)</span> 2.4 million (<span class="math inline">\(10^6\)</span>)</td>
</tr>
<tr class="even">
<td>20</td>
<td>4</td>
<td><span class="math inline">\(\approx\)</span> 2.4 billion (<span class="math inline">\(10^9\)</span>)</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>100</td>
<td>5</td>
<td><span class="math inline">\(\approx 6.6 \times 10^{76}\)</span></td>
</tr>
</tbody>
</table>
<p>These are enormously big numbers even for relatively small problems!</p>
<p><span class="math inline">\(\Longrightarrow\)</span> Clustering / partitioning / structure discovery is not easy!</p>
<p><span class="math inline">\(\Longrightarrow\)</span> We cannot expect perfect answers or a single “true” clustering</p>
<p>In fact, as a model of the data many differnt clusterings may fit the data equally well.</p>
<p><span class="math inline">\(\Longrightarrow\)</span> We need to assesse the uncertainty of the clustering</p>
<p>This can be done as part of probabilistic modelling or by resampling (e.g., bootstrap).</p>
</section>
<section id="common-types-of-clustering-methods" class="level3" data-number="4.1.4">
<h3 data-number="4.1.4" class="anchored" data-anchor-id="common-types-of-clustering-methods"><span class="header-section-number">4.1.4</span> Common types of clustering methods</h3>
<p>There are very many different clustering algorithms!</p>
<p>We consider the following two broad types of methods:</p>
<ol type="1">
<li><strong>Algorithmic clustering methods</strong> (these are not explicitly based on a probabilistic model)</li>
</ol>
<ul>
<li><span class="math inline">\(K\)</span>-means</li>
<li>PAM</li>
<li>hierarchical clustering (distance or similarity-based, divise and agglomerative)</li>
</ul>
<blockquote class="blockquote">
<p><strong>pros:</strong> fast, effective algorithms to find at least some grouping <strong>cons:</strong> no probabilistic interpretation, blackbox methods</p>
</blockquote>
<ol start="2" type="1">
<li><strong>Model-based clustering</strong> (based on a probabilistic model)</li>
</ol>
<ul>
<li>mixture models (e.g.&nbsp;Gaussian mixture models, GMMs, non-hierarchical)</li>
<li>graphical models (e.g.&nbsp;Bayesian networks, Gaussian graphical models GGM, trees and networks)</li>
</ul>
<blockquote class="blockquote">
<p><strong>pros:</strong> full probabilistic model with all corresponding advantages <strong>cons:</strong> computationally very expensive, sometimes impossible to compute exactly.</p>
</blockquote>
</section>
</section>
<section id="hierarchical-clustering" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="hierarchical-clustering"><span class="header-section-number">4.2</span> Hierarchical clustering</h2>
<section id="tree-like-structures" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="tree-like-structures"><span class="header-section-number">4.2.1</span> Tree-like structures</h3>
<p>Often, categorisations of objects are nested, i.e.&nbsp;there sub-categories of categories etc. These can be naturally represented by <strong>tree-like hierarchical structures</strong>.</p>
<p>In many branches of science hierarchical clusterings are widely employed, for example in evolutionary biology: see e.g.&nbsp;</p>
<ul>
<li><a href="http://tolweb.org/">Tree of Life</a> explaining the biodiversity of life</li>
<li>phylogenetic trees among species (e.g.&nbsp;vertebrata)</li>
<li>population genetic trees to describe human evolution</li>
<li>taxonomic trees for plant species</li>
<li>etc.</li>
</ul>
<p>Note that when visualising hierarchical structures typically the corresponding tree is depicted facing downwards, i.e.&nbsp;the root of the tree is shown on the top, and the tips/leaves of the tree are shown at the bottom!</p>
<p>In order to obtain such a hierarchical clustering from data two opposing strategies are commonly used:</p>
<ol type="1">
<li><strong>divisive or recursive partitioning algorithms</strong>
<ul>
<li>grow the tree from the root downwards</li>
<li>first determine the main two clusters, then recursively refine the clusters further.</li>
</ul></li>
<li><strong>agglomerative algorithms</strong>
<ul>
<li>grow the tree from the leaves upwards</li>
<li>successively form partitions by first joining individual object together, then recursively join groups of items together, until all is merged.</li>
</ul></li>
</ol>
<p>In the following we discuss a number of popular hierarchical agglomerative clustering algorithms that are based on the pairwise distances / similarities (a <span class="math inline">\(n \times n\)</span> matrix) among all data points.</p>
</section>
<section id="agglomerative-hierarchical-clustering-algorithms" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="agglomerative-hierarchical-clustering-algorithms"><span class="header-section-number">4.2.2</span> Agglomerative hierarchical clustering algorithms</h3>
<p>A general algorithm for agglomerative construction of a hierarchical clustering works as follows:</p>
<p><em>Initialisation:</em></p>
<p>Compute a dissimilarity / distance matrix between all pairs of objects where “objects” are single data points at this stage but later are also be sets of data points.</p>
<p><em>Iterative procedure:</em></p>
<ol type="1">
<li><p>identify the pair of objects with the smallest distance. These two objects are then merged together into one set. Create an internal node in the tree to represent this set.</p></li>
<li><p>update the distance matrix by computing the distances between the new set and all other objects. If the new set contains all data points the procedure terminates. The final node created is the root node.</p></li>
</ol>
<p>For actual implementation of this algorithm two key ingredients are needed:</p>
<ol type="1">
<li>a distance measure <span class="math inline">\(d(\symbfit a, \symbfit b)\)</span> between two individual elementary data points <span class="math inline">\(\symbfit a\)</span> and <span class="math inline">\(\symbfit b\)</span>.</li>
</ol>
<blockquote class="blockquote">
<p>This is typically one of the following:</p>
</blockquote>
<blockquote class="blockquote">
<ul>
<li>Euclidean distance <span class="math inline">\(d(\symbfit a, \symbfit b) = \sqrt{\sum_{i=1}^d ( a_i-b_i )^2} = \sqrt{(\symbfit a-\symbfit b)^T (\symbfit a-\symbfit b)}\)</span></li>
<li>Squared Euclidean distance <span class="math inline">\(d(\symbfit a, \symbfit b) = (\symbfit a-\symbfit b)^T (\symbfit a-\symbfit b)\)</span></li>
<li>Manhattan distance <span class="math inline">\(d(\symbfit a, \symbfit b) = \sum_{i=1}^d | a_i-b_i |\)</span></li>
<li>Maximum norm <span class="math inline">\(d(\symbfit a, \symbfit b) = \underset{i \in \{1, \ldots, d\}}{\max} | a_i-b_i |\)</span></li>
</ul>
</blockquote>
<blockquote class="blockquote">
<p>In the end, making the correct choice of distance will require subject knowledge about the data!</p>
</blockquote>
<ol start="2" type="1">
<li>a distance measure <span class="math inline">\(d(A, B)\)</span> between two sets of objects <span class="math inline">\(A=\{\symbfit a_1, \symbfit a_2, \ldots,  \symbfit a_{n_A} \}\)</span> and <span class="math inline">\(B=\{\symbfit b_1, \symbfit b_2, \ldots, \symbfit b_{n_B}\}\)</span> of size <span class="math inline">\(n_A\)</span> and <span class="math inline">\(n_B\)</span>, respectively.</li>
</ol>
<blockquote class="blockquote">
<p>To determine the distance <span class="math inline">\(d(A, B)\)</span> between these two sets the following measures are often employed:</p>
</blockquote>
<blockquote class="blockquote">
<ul>
<li><strong>complete linkage</strong> (max. distance): <span class="math inline">\(d(A, B) = \underset{\symbfit a_i \in A, \symbfit b_i \in B}{\max} d(\symbfit a_i, \symbfit b_i)\)</span></li>
<li><strong>single linkage</strong> (min. distance): <span class="math inline">\(d(A, B) = \underset{\symbfit a_i \in A, \symbfit b_i \in B}{\min} d(\symbfit a_i, \symbfit b_i)\)</span></li>
<li><strong>average linkage</strong> (avg. distance): <span class="math inline">\(d(A, B) = \frac{1}{n_A n_B} \sum_{\symbfit a_i \in A} \sum_{\symbfit b_i \in B} d(\symbfit a_i, \symbfit b_i)\)</span></li>
</ul>
</blockquote>
</section>
<section id="wards-clustering-method" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="wards-clustering-method"><span class="header-section-number">4.2.3</span> Ward’s clustering method</h3>
<p>Another agglomerative hierarchical procedure is <strong>Ward’s minimum variance approach</strong> <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> (see also <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>). In this approach in each iteration the two sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are merged that lead to the <strong>smallest increase in within-group variation</strong>. The centroids of the two sets is given by <span class="math inline">\(\symbfit \mu_A = \frac{1}{n_A} \sum_{\symbfit a_i \in A} \symbfit a_i\)</span> and <span class="math inline">\(\symbfit \mu_B = \frac{1}{n_B} \sum_{\symbfit b_i \in B} \symbfit b_i\)</span>.</p>
<p>The within-group sum of squares for group <span class="math inline">\(A\)</span> is <span class="math display">\[
w_A = \sum_{\symbfit a_i \in A} (\symbfit a_i -\symbfit \mu_A)^T (\symbfit a_i -\symbfit \mu_A)
\]</span> and is computed here on the basis of the difference of the observations <span class="math inline">\(\symbfit a_i\)</span> relative to their mean <span class="math inline">\(\symbfit \mu_A\)</span>. However, since we typically only have pairwise distances available we don’t know the group means so this formula can’t be applied. Fortunately, it is also possible to compute <span class="math inline">\(w_A\)</span> using only the pairwise differences using <span class="math display">\[
w_A = \frac{1}{n_A} \sum_{\symbfit a_i, \symbfit a_j \in A, i &lt; j} (\symbfit a_i -\symbfit a_j)^T (\symbfit a_i -\symbfit a_j)
\]</span> This trick is employed in Ward’s clustering method by constructing a distance measure between two sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> as <span class="math display">\[
d(A, B) = w_{A \cup B} - w_A -w_B \,
\]</span> and using as the distance between two elementary data points <span class="math inline">\(\symbfit a\)</span> and <span class="math inline">\(\symbfit b\)</span> the squared Euclidean distance <span class="math display">\[
d(\symbfit a, \symbfit b) = (\symbfit a- \symbfit b)^T (\symbfit a- \symbfit b) \, .
\]</span></p>
</section>
<section id="application-to-swiss-banknote-data-set" class="level3" data-number="4.2.4">
<h3 data-number="4.2.4" class="anchored" data-anchor-id="application-to-swiss-banknote-data-set"><span class="header-section-number">4.2.4</span> Application to Swiss banknote data set</h3>
<p>This data set is reports 6 pysical measurements on 200 Swiss bank notes. Of the 200 notes 100 are genuine and 100 are counterfeit. The measurements are: length, left width, right width, bottom margin, top margin, diagonal length of the bank notes.</p>
<p>Plotting the first to PCAs of this data shows that there are indeed two well defined groups, and that these groups correspond precisely to the genuine and counterfeit banknotes:</p>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We now compare the hierarchical clusterings of the Swiss bank note data using four different methods using Euclidean distance.</p>
<p>An interactive <a href="https://shiny.rstudio.com/">R Shiny web app</a> of this analysis (which also allows to explore further distance measures) is available online at <a href="https://minerva.it.manchester.ac.uk/shiny/strimmer/hclust/" class="uri">https://minerva.it.manchester.ac.uk/shiny/strimmer/hclust/</a>.</p>
<p>Ward.D2 (=Ward’s method):</p>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Average linkage:</p>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div style="page-break-after: always;"></div>
<p>Complete linkage:</p>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Single linkage:</p>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Result:</p>
<ul>
<li>All four trees / hierarchical clusterings are quite different!</li>
<li>The Ward.D2 method is the only one that finds the correct grouping (except for a single error).</li>
</ul>
</section>
<section id="assessment-of-the-uncertainty-of-hierarchical-clusterings" class="level3" data-number="4.2.5">
<h3 data-number="4.2.5" class="anchored" data-anchor-id="assessment-of-the-uncertainty-of-hierarchical-clusterings"><span class="header-section-number">4.2.5</span> Assessment of the uncertainty of hierarchical clusterings</h3>
<p>In practical application of hierarchical clustering methods is is essential to evaluate the stability and uncertainty of the obtained groupings. This is often done as follows using the “bootstrap”:</p>
<ul>
<li>Sampling with replacement is used to generate a number of so-called bootstrap data sets (say <span class="math inline">\(B=200\)</span>) similar to the original one. Specifically, we create new data matrices by repeately randomly selecting columns (variables) from the original data matrix for inclusion in the bootstrap data matrix. Note that we sample columns as our aim is to cluster the samples.</li>
<li>Subsequently, a hierarchical clustering is computed for each of the bootstrap data sets. As a result, we now have an “ensemble” of <span class="math inline">\(B\)</span> bootstrap trees.</li>
<li>Finally, analysis of the clusters (bipartions) shown in all the bootstrap trees allows to count the clusters that appear frequently, and also those that appear less frequently. These counts provide a measure of the stability of the clusterings appearing in the original tree.</li>
<li>Additionally, from the bootstrap tree we can also compute a consensus tree containing the most stable clusters. This an be viewed as an “ensemble average” of all the bootstrap trees.</li>
</ul>
<p>A disadvantage of this procedure is that bootstrapping trees is computationally very expensive, as the original procedure is already time consuming but now needs to be repeated a large number of times.</p>
</section>
</section>
<section id="k-means-clustering" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="k-means-clustering"><span class="header-section-number">4.3</span> <span class="math inline">\(K\)</span>-means clustering</h2>
<section id="set-up" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="set-up"><span class="header-section-number">4.3.1</span> Set-up</h3>
<ul>
<li>We assume that there are <span class="math inline">\(K\)</span> groups (i.e.&nbsp;<span class="math inline">\(K\)</span> is known in advance).</li>
<li>For each group <span class="math inline">\(k \in \{1, \ldots, K\}\)</span> we assume a group mean <span class="math inline">\(\symbfit \mu_k\)</span>.</li>
<li>Aim: partition the data points <span class="math inline">\(\symbfit x_1, \ldots, \symbfit x_n\)</span> into <span class="math inline">\(K\)</span> non-overlapping groups.</li>
<li>Each of the <span class="math inline">\(n\)</span> data points <span class="math inline">\(\symbfit x_i\)</span> is assigned to exactly one of the <span class="math inline">\(K\)</span> groups.</li>
<li>Maximise the homogeneity within each group (i.e.&nbsp;each group should contain similar objects).</li>
<li>Maximise the heterogeneity between the different groups (i.e each group should differ from the other groups).</li>
</ul>
</section>
<section id="algorithm" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="algorithm"><span class="header-section-number">4.3.2</span> Algorithm</h3>
<p>After running <span class="math inline">\(K\)</span>-means we will get estimates of <span class="math inline">\(\hat{\symbfit \mu}_k\)</span> of the group means, as well allocations <span class="math inline">\(y_i \in \{1, \ldots, K\}\)</span> of each data point <span class="math inline">\(\symbfit x_i\)</span> to one of the classes.</p>
<p><em>Initialisation:</em></p>
<p>At the start of the algorithm the <span class="math inline">\(n\)</span> observations <span class="math inline">\(\symbfit x_1, \ldots, \symbfit x_n\)</span> are randomly allocated with equal probability to one of the <span class="math inline">\(K\)</span> groups. The resulting assignment is <span class="math inline">\(y_1, \ldots, y_n\)</span>, with each <span class="math inline">\(y_i=\{1, \ldots, K\}\)</span>. With <span class="math inline">\(G_k = \{ i | y_i = k\}\)</span> we denote the set of indices of the data points in cluster <span class="math inline">\(k\)</span>, and with <span class="math inline">\(n_k = | G_k |\)</span> the number of samples in cluster <span class="math inline">\(k\)</span>.</p>
<p><em>Iterative refinement:</em></p>
<ol type="1">
<li>Estimate the group means by <span class="math display">\[
\hat{\symbfit \mu}_k = \frac{1}{n_k} \sum_{i \in G_k} \symbfit x_i
\]</span></li>
<li>Update the group allocations <span class="math inline">\(y_i\)</span>. Specifically, assign each data point <span class="math inline">\(\symbfit x_i\)</span> to the group <span class="math inline">\(k\)</span> with the nearest <span class="math inline">\(\hat{\symbfit \mu}_k\)</span>. The distance is measured in terms of the Euclidean norm: <span class="math display">\[
\begin{split}
y_i &amp; = \underset{k}{\arg \min} \,  \left| \symbfit x_i-\hat{\symbfit \mu}_k \right|_2 \\
      &amp; = \underset{k}{\arg \min} \, \left(\symbfit x_i-\hat{\symbfit \mu}_k\right)^T \left(\symbfit x_i-\hat{\symbfit \mu}_k\right) \\
\end{split}
\]</span></li>
</ol>
<p>Steps 1 and 2 are repeated until the algorithm converges (i.e.&nbsp;until the group allocations don’t change any more) or until a specified upper limit of iterations is reached.</p>
</section>
<section id="properties" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="properties"><span class="header-section-number">4.3.3</span> Properties</h3>
<p><span class="math inline">\(K\)</span>-means has been proposed in the 1950 to 1970s by various authors in diverse contexts <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. Despite its simplicity <span class="math inline">\(K\)</span>-means is, perhaps surprisingly, a very effective clustering algorithm. The main reason for this is the close connection of <span class="math inline">\(K\)</span>-means with probabilistic clustering based on Gaussian mixture models (for details see later section).</p>
<p>Since the clustering depends on the initialisation it is often useful to run <span class="math inline">\(K\)</span>-means several times with different starting group allocations of the data points. Furthermore, non-random or non-uniform initialisations can lead to improved and faster convergence, see the <a href="https://en.wikipedia.org/wiki/K-means%2B%2B">K-means++</a> algorithm.</p>
<p>The clusters constructed in <span class="math inline">\(K\)</span>-means have linear boundaries and thus form a <a href="https://en.wikipedia.org/wiki/Voronoi_diagram">Voronoi tessellation</a> around the cluster means. Again, this can be explained by the close link of <span class="math inline">\(K\)</span>-means with a particular Gaussian mixture model.</p>
</section>
<section id="choosing-the-number-of-clusters" class="level3" data-number="4.3.4">
<h3 data-number="4.3.4" class="anchored" data-anchor-id="choosing-the-number-of-clusters"><span class="header-section-number">4.3.4</span> Choosing the number of clusters</h3>
<p>Once the <span class="math inline">\(K\)</span>-means algorithm has run we can assess the homogeneity and heterogeneity of the resulting clusters:</p>
<ol type="a">
<li><p>the total within-group sum of squares <span class="math inline">\(SSW\)</span> (in R: <code>tot.withinss</code>), or total unexplained sum of squares: <span class="math display">\[
SSW = \sum_{k=1}^K \, \sum_{i \in G_k} (\symbfit x_i -\hat{\symbfit \mu}_k)^T (\symbfit x_i -\hat{\symbfit \mu}_k)
\]</span> This quantity decreases with <span class="math inline">\(K\)</span> and is zero for <span class="math inline">\(K=n\)</span>. The <span class="math inline">\(K\)</span>-means algorithm tries to minimise this quantity but it will typically only find a local minimum rather than the global one.</p></li>
<li><p>the between-group sum of squares <span class="math inline">\(SSB\)</span> (in R: <code>betweenss</code>), or explained sum of squares: <span class="math display">\[
SSB = \sum_{k=1}^K n_k (\hat{\symbfit \mu}_k - \hat{\symbfit \mu}_0)^T (\hat{\symbfit \mu}_k - \hat{\symbfit \mu}_0)
\]</span> where <span class="math inline">\(\hat{\symbfit \mu}_0 = \frac{1}{n} \sum_{i=1}^n \symbfit x_i = \frac{1}{n} \sum_{k=1}^K n_k \hat{\symbfit \mu}_k\)</span> is the global mean of the samples. <span class="math inline">\(SSB\)</span> increases with the number of clusters <span class="math inline">\(K\)</span> until for <span class="math inline">\(K=n\)</span> it becomes equal to the total sum of squares <span class="math inline">\(SST\)</span>.</p></li>
<li><p>the total sum of squares <span class="math display">\[
SST = \sum_{i=1}^n (\symbfit x_i - \hat{\symbfit \mu}_0)^T (\symbfit x_i - \hat{\symbfit \mu}_0) \, .
\]</span> By construction <span class="math inline">\(SST = SSB + SSW\)</span> for any <span class="math inline">\(K\)</span> (i.e.&nbsp;<span class="math inline">\(SST\)</span> is a constant independent of <span class="math inline">\(K\)</span>).</p></li>
</ol>
<p>Dividing the sum of squares by the sample size <span class="math inline">\(n\)</span> we get</p>
<ul>
<li><span class="math inline">\(T = \frac{SST}{n}\)</span> as the <em>total variation</em>,</li>
<li><span class="math inline">\(B = \frac{SSB}{n}\)</span> as the <em>explained variation</em> and</li>
<li><span class="math inline">\(W = \frac{SSW}{n}\)</span> as the total <em>unexplained variation</em> ,</li>
<li>with <span class="math inline">\(T = B + W\)</span>.</li>
</ul>
<p>In order to decide on the optimal number of clusters we run <span class="math inline">\(K\)</span>-means for different settings for <span class="math inline">\(K\)</span> and then choose the smallest <span class="math inline">\(K\)</span> for which the explained variation <span class="math inline">\(B\)</span> is not significantly worse compared to a clustering with a substantially larger <span class="math inline">\(K\)</span> (see example below).</p>
</section>
<section id="k-medoids-aka-pam" class="level3" data-number="4.3.5">
<h3 data-number="4.3.5" class="anchored" data-anchor-id="k-medoids-aka-pam"><span class="header-section-number">4.3.5</span> <span class="math inline">\(K\)</span>-medoids aka PAM</h3>
<p>A closely related clustering method is <span class="math inline">\(K\)</span>-medoids or PAM (“Partitioning Around Medoids”).</p>
<p>This works exactly like <span class="math inline">\(K\)</span>-means, only that</p>
<ul>
<li>instead of the estimated group means <span class="math inline">\(\hat{\symbfit \mu}_k\)</span> one member of each group is selected as its representative (the so-called “medoid”)</li>
<li>instead of squared Euclidean distance other dissimilarity measures are also allowed.</li>
</ul>
</section>
<section id="application-of-k-means-to-iris-data" class="level3" data-number="4.3.6">
<h3 data-number="4.3.6" class="anchored" data-anchor-id="application-of-k-means-to-iris-data"><span class="header-section-number">4.3.6</span> Application of <span class="math inline">\(K\)</span>-means to Iris data</h3>
<p>Scatter plots of Iris data:</p>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="432"></p>
</figure>
</div>
</div>
</div>
<p>The R output from a <span class="math inline">\(K\)</span>-means analysis with known true number of clusters specified (<span class="math inline">\(K=3\)</span>) is: ::: {.cell warnings=‘false’}</p>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>kmeans.out <span class="ot">=</span> <span class="fu">kmeans</span>(X.iris, <span class="dv">3</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>kmeans.out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>K-means clustering with 3 clusters of sizes 53, 50, 47

Cluster means:
  Sepal.Length Sepal.Width Petal.Length Petal.Width
1  -0.05005221 -0.88042696    0.3465767   0.2805873
2  -1.01119138  0.85041372   -1.3006301  -1.2507035
3   1.13217737  0.08812645    0.9928284   1.0141287

Clustering vector:
  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [38] 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 1 1 1 3 1 1 1 1 1 1 1 1 3 1 1 1 1 3 1 1 1
 [75] 1 3 3 3 1 1 1 1 1 1 1 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 3 3 3 3 1 3 3 3 3
[112] 3 3 1 1 3 3 3 3 1 3 1 3 1 3 3 1 3 3 3 3 3 3 1 1 3 3 3 1 3 3 3 1 3 3 3 1 3
[149] 3 1

Within cluster sum of squares by cluster:
[1] 44.08754 47.35062 47.45019
 (between_SS / total_SS =  76.7 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault"      </code></pre>
</div>
<p>::: The corresponding total within-group sum of squares (<span class="math inline">\(SSW\)</span>, <code>tot.withinss</code>) is ::: {.cell warnings=‘false’}</p>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>kmeans.out<span class="sc">$</span>tot.withinss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 138.8884</code></pre>
</div>
<p>::: and the between-group sum of squares (<span class="math inline">\(SSB\)</span>, <code>betweenss</code>) is ::: {.cell warnings=‘false’}</p>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>kmeans.out<span class="sc">$</span>betweenss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 457.1116</code></pre>
</div>
<p>::: By comparing with the known class assignments we can assess the accuracy of <span class="math inline">\(K\)</span>-means clustering:</p>
<div class="cell" data-warnings="false">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(L.iris, kmeans.out<span class="sc">$</span>cluster)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            
L.iris        1  2  3
  setosa      0 50  0
  versicolor 39  0 11
  virginica  14  0 36</code></pre>
</div>
</div>
<p>For choosing <span class="math inline">\(K\)</span> we run <span class="math inline">\(K\)</span>-means several times and compute within and between cluster variation in dependence of <span class="math inline">\(K\)</span>:</p>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="480"></p>
</figure>
</div>
</div>
</div>
<p>Thus, <span class="math inline">\(K=3\)</span> clusters seem appropriate since the the explained variation does not significantly improve (and the unexplained variation does not significantly decrease) with a further increase of the number of clusters.</p>
</section>
<section id="arbitrariness-of-cluster-labels-and-label-switching" class="level3" data-number="4.3.7">
<h3 data-number="4.3.7" class="anchored" data-anchor-id="arbitrariness-of-cluster-labels-and-label-switching"><span class="header-section-number">4.3.7</span> Arbitrariness of cluster labels and label switching</h3>
<p>It is important to realise that in unsupervised learning and clustering the labels of each group are assigned in an arbitrary fashion. Recall that for <span class="math inline">\(K\)</span> groups there are <span class="math inline">\(K!\)</span> possibilities to attach the labels, corresponding to the number of permutations of <span class="math inline">\(K\)</span> groups.</p>
<p>Thus, different runs of a clustering algorithm such as <span class="math inline">\(K\)</span>-means may return the same clustering (groupings of samples) but with different labels. This phenomenon is called “label switching” and makes it difficult to automatise comparison of clusterings. In particular, one cannot simply rely on the automatically assigned group label, instead one needs to compare the actual members of the clusters.</p>
<p>A way to resolve the problem of label switching is to relabel the clusters using additional information, such as requiring that some samples are in specific groups (e.g.: sample 1 is always in group labelled “1”), and/or linking labels to orderings or constraints on the group characteristics (e.g.: the group with label “1” has always a smaller mean that group with label “2”).</p>
</section>
</section>
<section id="mixture-models" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="mixture-models"><span class="header-section-number">4.4</span> Mixture models</h2>
<section id="finite-mixture-model" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="finite-mixture-model"><span class="header-section-number">4.4.1</span> Finite mixture model</h3>
<ul>
<li><span class="math inline">\(K\)</span> groups / classes / categories, with finite <span class="math inline">\(K\)</span> known in advance.</li>
<li>Probability of class <span class="math inline">\(k\)</span>: <span class="math inline">\(\text{Pr}(k) = \pi_k\)</span> with <span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span>.</li>
<li>Each class <span class="math inline">\(k \in C= \{1, \ldots, K\}\)</span> is modelled by its own distribution <span class="math inline">\(F_k\)</span> with own parameters <span class="math inline">\(\symbfit \theta_k\)</span>.</li>
<li>Density of class <span class="math inline">\(k\)</span>: <span class="math inline">\(f_k(\symbfit x) = f(\symbfit x| k)\)</span>.</li>
<li>The conditional means and variances for each class <span class="math inline">\(k \in C\)</span> are <span class="math inline">\(\text{E}(\symbfit x| k) = \symbfit \mu_k\)</span> and <span class="math inline">\(\text{Var}(\symbfit x| k) = \symbfit \Sigma_k\)</span>.</li>
<li>The resulting mixture density for the observed variable <span class="math inline">\(\symbfit x\)</span> is <span class="math display">\[
f_{\text{mix}}(\symbfit x) = \sum_{k=1}^K \pi_k f_k(\symbfit x)
\]</span></li>
</ul>
<p>Very often one uses <strong>multivariate normal components</strong> <span class="math inline">\(f_k(\symbfit x) = N(\symbfit x| \symbfit \mu_k, \symbfit \Sigma_k)\)</span> <span class="math inline">\(\\ \Longrightarrow\)</span> <strong>Gaussian mixture model</strong> (GMM)</p>
<p>Mixture models are fundamental not just in clustering but for many other applications (e.g.&nbsp;classification).</p>
<p>Note: don’t confuse <em>mixture model</em> with <em>mixed model</em> (= terminology for a <em>random effects</em> regression model).</p>
</section>
<section id="total-mean-and-variance-of-a-multivariate-mixture-model" class="level3" data-number="4.4.2">
<h3 data-number="4.4.2" class="anchored" data-anchor-id="total-mean-and-variance-of-a-multivariate-mixture-model"><span class="header-section-number">4.4.2</span> Total mean and variance of a multivariate mixture model</h3>
<p>Using the <strong>law of total expectation</strong> we obtain the mean of the mixture density with multivariate <span class="math inline">\(\symbfit x\)</span> as follows: <span class="math display">\[
\begin{split}
\text{E}(\symbfit x) &amp; = \text{E}(\text{E}(\symbfit x| k)) \\
            &amp; = \text{E}( \symbfit \mu_k ) \\
            &amp;= \sum_{k=1}^K \pi_k \symbfit \mu_k \\
            &amp;= \symbfit \mu_0 \\
\end{split}
\]</span> Note that we treat both <span class="math inline">\(\symbfit x\)</span> as well as <span class="math inline">\(k\)</span> as random variables.</p>
<p>Similarly, using the <strong>law of total variance</strong> we compute the marginal variance: <span class="math display">\[
\begin{split}
\underbrace{\text{Var}(\symbfit x)}_{\text{total}} &amp; =  \underbrace{ \text{Var}( \text{E}(\symbfit x| k )  )}_{\text{explained / between-group}} + \underbrace{\text{E}(\text{Var}(\symbfit x|k))}_{\text{unexplained / expected within-group / pooled}} \\
\symbfit \Sigma_0 &amp; =  \text{Var}( \symbfit \mu_k  ) + \text{E}( \symbfit \Sigma_k )  \\
               &amp; =  \sum_{k=1}^K \pi_k (\symbfit \mu_k - \symbfit \mu_0) (\symbfit \mu_k - \symbfit \mu_0)^T + \sum_{k=1}^K \pi_k \symbfit \Sigma_k  \\
&amp; =  \symbfit \Sigma_{\text{explained}} +  \symbfit \Sigma_{\text{unexplained}} \\
\end{split}
\]</span></p>
<p>Thus, the <strong>total variance</strong> decomposes into the <strong>explained (between group) variance</strong> and the <strong>unexplained (expected within group, pooled) variance</strong>.</p>
</section>
<section id="total-variation" class="level3" data-number="4.4.3">
<h3 data-number="4.4.3" class="anchored" data-anchor-id="total-variation"><span class="header-section-number">4.4.3</span> Total variation</h3>
<p>The <strong>total variation</strong> is given by the <strong>trace of the covariance matrix</strong>. The above decomposition for the total variation is <span class="math display">\[
\begin{split}
\text{Tr}(\symbfit \Sigma_0) &amp; =  \text{Tr}( \symbfit \Sigma_{\text{explained}} )  + \text{Tr}( \symbfit \Sigma_{\text{unexplained}} )  \\
&amp; =  \sum_{k=1}^K \pi_k \text{Tr}((\symbfit \mu_k - \symbfit \mu_0) (\symbfit \mu_k - \symbfit \mu_0)^T) + \sum_{k=1}^K \pi_k \text{Tr}(\symbfit \Sigma_k)  \\
&amp; =  \sum_{k=1}^K \pi_k (\symbfit \mu_k - \symbfit \mu_0)^T (\symbfit \mu_k - \symbfit \mu_0) + \sum_{k=1}^K \pi_k \text{Tr}(\symbfit \Sigma_k)\\
\end{split}
\]</span> If the covariances are replaced by their empirical estimates we obtain the <span class="math inline">\(T=B+W\)</span> decomposition of total variation familiar from <span class="math inline">\(K\)</span>-means: <span class="math display">\[T = \text{Tr}\left( \hat{\symbfit \Sigma}_0 \right)  =
\frac{1}{n} \sum_{i=1}^n (\symbfit x_i - \hat{\symbfit \mu}_0)^T (\symbfit x_i - \hat{\symbfit \mu}_0)\]</span> <span class="math display">\[B = \text{Tr}( \hat{\symbfit \Sigma}_{\text{explained}} ) =  \frac{1}{n} \sum_{k=1}^K n_k (\hat{\symbfit \mu}_k - \hat{\symbfit \mu}_0)^T (\hat{\symbfit \mu}_k - \hat{\symbfit \mu}_0)\]</span> <span class="math display">\[W = \text{Tr}( \hat{\symbfit \Sigma}_{\text{unexplained}} ) = \frac{1}{n}  \sum_{k=1}^K \, \sum_{i \in G_k} (\symbfit x_i -\hat{\symbfit \mu}_k)^T (\symbfit x_i -\hat{\symbfit \mu}_k)
\]</span></p>
</section>
<section id="univariate-mixture" class="level3" data-number="4.4.4">
<h3 data-number="4.4.4" class="anchored" data-anchor-id="univariate-mixture"><span class="header-section-number">4.4.4</span> Univariate mixture</h3>
<p>For a univariate mixture (<span class="math inline">\(d=1\)</span>) with <span class="math inline">\(K=2\)</span> components we get <span class="math display">\[
\mu_0 = \pi_1 \mu_1+ \pi_2 \mu_2 \, ,
\]</span> <span class="math display">\[
\sigma^2_{\text{within}} = \pi_1 \sigma^2_1 + \pi_2 \sigma^2_2 = \sigma^2_{\text{pooled}}\,,
\]</span> also known as pooled variance, and <span class="math display">\[
\begin{split}
\sigma^2_{\text{between}} &amp;= \pi_1 (\mu_1 - \mu_0)^2 + \pi_2 (\mu_2 - \mu_0)^2 \\
&amp; =\pi_1 \pi_2^2 (\mu_1 - \mu_2)^2 + \pi_2 \pi_1^2 (\mu_1 - \mu_2)^2\\
&amp; = \pi_1 \pi_2 (\mu_1 - \mu_2)^2  \\
&amp; = \left( \frac{1}{\pi_1} + \frac{1}{\pi_2}   \right)^{-1} (\mu_1 - \mu_2)^2 \\
\end{split} \,.
\]</span> The ratio of the between-group variance and the within-group variance is proportional (by factor of <span class="math inline">\(n\)</span>) to the squared pooled-variance <span class="math inline">\(t\)</span>-score: <span class="math display">\[
\frac{\sigma^2_{\text{between}}}{\sigma^2_{\text{within}}} =
  \frac{ (\mu_1 - \mu_2)^2}{ \left(\frac{1}{\pi_1} + \frac{1}{\pi_2}   \right)  \sigma^2_{\text{pooled}} }= \frac{t_{\text{pooled}}^2}{n}
\]</span> If you are familiar with ANOVA (e.g.&nbsp;linear models course) you will recognise this ratio as the <span class="math inline">\(F\)</span>-score.</p>
</section>
<section id="example-of-mixtures" class="level3" data-number="4.4.5">
<h3 data-number="4.4.5" class="anchored" data-anchor-id="example-of-mixtures"><span class="header-section-number">4.4.5</span> Example of mixtures</h3>
<p>Mixtures can take on many different shapes and forms, so it is instructive to study a few examples. An interactive tool to visualise two component normal mixture is available online as <a href="https://shiny.rstudio.com/">R Shiny web app</a> at <a href="https://minerva.it.manchester.ac.uk/shiny/strimmer/mixture/" class="uri">https://minerva.it.manchester.ac.uk/shiny/strimmer/mixture/</a>.</p>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The first plot shows the bimodal density of a mixture distribution consisting of two normals with <span class="math inline">\(\pi_1=0.7\)</span>, <span class="math inline">\(\mu_1=-1\)</span>, <span class="math inline">\(\mu_2=2\)</span> and the two variances equal to 1 (<span class="math inline">\(\sigma^2_1 = 1\)</span> and <span class="math inline">\(\sigma^2_2 = 1\)</span>). Because the two components are well-separated there are two clear modes. The plot also shows the density of a normal distribution with the same total mean (<span class="math inline">\(\mu_0=-0.1\)</span>) and variance (<span class="math inline">\(\sigma_0^2=2.89\)</span>) as the mixture distribution. Clearly the total normal and the mixture density are very different.</p>
<p>However, a two-component mixtures can also be unimodal. For example, if the mean of the second component is adjusted to <span class="math inline">\(\mu_2=0\)</span> then there is only a single mode and the total normal density with <span class="math inline">\(\mu_0=-0.7\)</span> and <span class="math inline">\(\sigma_0^2=1.21\)</span> is now almost inistinguishable in form from the mixture density. Thus, in this case it will be very hard (or even impossible) to identify the two peaks from data.</p>
<p>Most mixtures we consider in this course are multivariate. For illustration, here is a plot of a mixture of two bivariate normals, with <span class="math inline">\(\pi_1=0.7\)</span>, <span class="math inline">\(\symbfit \mu_1 = \begin{pmatrix}-1 \\1 \\ \end{pmatrix}\)</span>, <span class="math inline">\(\symbfit \Sigma_1 = \begin{pmatrix} 1 &amp; 0.7 \\ 0.7 &amp; 1  \\ \end{pmatrix}\)</span>, <span class="math inline">\(\symbfit \mu_2 = \begin{pmatrix}2.5 \\0.5 \\ \end{pmatrix}\)</span> and <span class="math inline">\(\symbfit \Sigma_2 = \begin{pmatrix} 1 &amp; -0.7 \\ -0.7 &amp; 1  \\ \end{pmatrix}\)</span>:</p>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/fig2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sampling-from-a-mixture-model" class="level3" data-number="4.4.6">
<h3 data-number="4.4.6" class="anchored" data-anchor-id="sampling-from-a-mixture-model"><span class="header-section-number">4.4.6</span> Sampling from a mixture model</h3>
<p>Assuming we know how to sample from the component densities <span class="math inline">\(f_k(\symbfit x)\)</span> of the mixture model it is straightforward to set up a procedure for sampling from the mixture <span class="math inline">\(f_{\text{mix}}(\symbfit x) = \sum_{k=1}^K \pi_k f_k(\symbfit x)\)</span> itself.</p>
<p>This is done in a two-step process:</p>
<ol type="1">
<li><p>Draw from categorical distribution with parameter <span class="math inline">\(\symbfit \pi=(\pi_1, \ldots, \pi_K)^T\)</span>: <span class="math display">\[\symbfit z\sim \text{Cat}(\symbfit \pi)\]</span> Here the vector <span class="math inline">\(\symbfit z= (z_1, \ldots, z_K)^T\)</span> indicates a hard group 0/1 allocation, with all components <span class="math inline">\(z_{\neq k}=0\)</span> except for a single entry <span class="math inline">\(z_k=1\)</span>.</p></li>
<li><p>Subsequently, sample from the component <span class="math inline">\(k\)</span> selected in step 1: <span class="math display">\[
\symbfit x\sim F_k
\]</span></p></li>
</ol>
<p>This two-stage sampling approach is also known as hierarchical generative model for a mixture distribution. This generative view is not only useful for simulating data from a mixture model but also highlights the role of the latent variable (the class allocation).</p>
</section>
</section>
<section id="fitting-mixture-models-to-data-and-inferring-the-latent-states" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="fitting-mixture-models-to-data-and-inferring-the-latent-states"><span class="header-section-number">4.5</span> Fitting mixture models to data and inferring the latent states</h2>
<p>In the following we denote by</p>
<ul>
<li><span class="math inline">\(\symbfit X= (\symbfit x_1, \ldots, \symbfit x_n)^T\)</span> the data matrix containing the observations of <span class="math inline">\(n\)</span> independent and identically distributed samples <span class="math inline">\(\symbfit x_1, \ldots, \symbfit x_n\)</span>, and</li>
<li><span class="math inline">\(\symbfit y= (y_1, \ldots, y_n)^T\)</span> the associated group memberships, as well as</li>
<li>the parameters <span class="math inline">\(\symbfit \theta\)</span> which for a Gaussian mixture model are <span class="math inline">\(\symbfit \theta= \{\symbfit \pi, \symbfit \mu_1, \ldots, \symbfit \mu_K, \symbfit \Sigma_1, \ldots, \symbfit \Sigma_K\}\)</span>.</li>
</ul>
<section id="observed-and-latent-variables" class="level3" data-number="4.5.1">
<h3 data-number="4.5.1" class="anchored" data-anchor-id="observed-and-latent-variables"><span class="header-section-number">4.5.1</span> Observed and latent variables</h3>
<p>When we observe data from a mixture model we collect samples <span class="math inline">\(\symbfit x_1, \ldots, \symbfit x_n\)</span>. Associated with each observed <span class="math inline">\(\symbfit x_i\)</span> is the corresponding underlying class allocation <span class="math inline">\(y_1, \ldots, y_n\)</span> where each <span class="math inline">\(y_i\)</span> takes on a value from <span class="math inline">\(C = \{1, \ldots, K\}\)</span>. Crucially, the class allocations <span class="math inline">\(y_i\)</span> are unknown and cannot be directly observed, thus are latent.</p>
<ul>
<li>The <strong>joint density</strong> for observed and unobserved variables: <span class="math display">\[f(\symbfit x, y) = f(\symbfit x| y) \text{Pr}(y) = f_y(\symbfit x) \pi_y\]</span></li>
</ul>
<p>The mixture density is therefore a <strong>marginal density</strong> as it arises from the joint density <span class="math inline">\(f(\symbfit x, y)\)</span> by marginalising over the discrete variable <span class="math inline">\(y\)</span>.</p>
<ul>
<li>Marginalisation: <span class="math inline">\(f(\symbfit x) =  \sum_{y \in C} f(\symbfit x, y)\)</span></li>
</ul>
</section>
<section id="complete-data-likelihood-and-observed-data-likelihood" class="level3" data-number="4.5.2">
<h3 data-number="4.5.2" class="anchored" data-anchor-id="complete-data-likelihood-and-observed-data-likelihood"><span class="header-section-number">4.5.2</span> Complete data likelihood and observed data likelihood</h3>
<p>If we know <span class="math inline">\(\symbfit y\)</span> in advance, i.e.&nbsp;if we know which sample belongs to a particular group, we can construct a <em>complete data log-likelihood</em> based on the joint distribution <span class="math inline">\(f(\symbfit x, y) = \pi_y f_y(\symbfit x)\)</span>. The log-likelihood for <span class="math inline">\(\symbfit \theta\)</span> given the both <span class="math inline">\(\symbfit X\)</span> and <span class="math inline">\(\symbfit y\)</span> is <span class="math display">\[
\log L(\symbfit \theta| \symbfit X, \symbfit y) = \sum_{i=1}^n \log f(\symbfit x_i, y_i)  =  \sum_{i=1}^n  \log \left(\pi_{y_i} f_{y_i}(\symbfit x_i) \right)
\]</span></p>
<p>On the other hand, typically we do not know <span class="math inline">\(\symbfit y\)</span> and therefore use the marginal or mixture density <span class="math inline">\(f(\symbfit x)\)</span> to construct the <em>observed data log-likelihood</em> (sometimes also called <em>incomplete data log-likelihood</em>) <span class="math inline">\(f(\symbfit x| \symbfit \theta)\)</span> as <span class="math display">\[
\begin{split}
\log L(\symbfit \theta| \symbfit X) &amp; =\sum_{i=1}^n \log f(\symbfit x_i | \symbfit \theta)\\
&amp; = \sum_{i=1}^n \log \left( \sum_{k=1}^K \pi_k f_k(\symbfit x_i)  \right)\\
\end{split}
\]</span></p>
<p>The <em>observed data log-likelihood</em> can also be computed from the complete data likelihood function by marginalising over <span class="math inline">\(\symbfit y\)</span> <span class="math display">\[
\begin{split}
\log L(\symbfit \theta| \symbfit X) &amp;= \log \sum_{\symbfit y}   L(\symbfit \theta| \symbfit X, \symbfit y)\\
&amp;= \log \sum_{y_1, \ldots, y_K}  \prod_{i=1}^n f(\symbfit x_i, y_i)\\
&amp;= \log \prod_{i=1}^n  \sum_{k=1}^K f(\symbfit x_i, k)\\
&amp; = \sum_{i=1}^n \log \left(  \sum_{k=1}^K f(\symbfit x_i, k)     \right)
\end{split}
\]</span></p>
<p>Clustering with a mixture model can be viewed as an <em>incomplete</em> or <em>missing</em> data problem (see also <a href="https://strimmerlab.github.io/publications/lecture-notes/MATH27720-stats2/">MATH27720 Statistics 2</a>).</p>
<p>Specifically, we face the problem of</p>
<ul>
<li>fitting the model using only the observed data <span class="math inline">\(\symbfit X\)</span> and</li>
<li>simultaneously inferring the class allocations <span class="math inline">\(\symbfit y\)</span>, i.e.&nbsp;states of the latent variable.</li>
</ul>
</section>
<section id="fitting-the-mixture-model-to-the-observed-data" class="level3" data-number="4.5.3">
<h3 data-number="4.5.3" class="anchored" data-anchor-id="fitting-the-mixture-model-to-the-observed-data"><span class="header-section-number">4.5.3</span> Fitting the mixture model to the observed data</h3>
<p>For large sample size <span class="math inline">\(n\)</span> the standard way to fit a mixture model is to employ maximum likelihood to find the MLEs of the parameters of the mixture model.</p>
<p>The direct way to fit a mixture model by maximum likelihood is to <strong>maximise the observed data log-likelihood function</strong> with regard to <span class="math inline">\(\symbfit \theta\)</span>: <span class="math display">\[
\hat{\symbfit \theta}^{ML} = \underset{\symbfit \theta}{\arg \max}\,\, \log L(\symbfit \theta| \symbfit X)
\]</span></p>
<p>Unfortunately, in practise evaluation and optimisation of the log-likelihood function can be difficult due to a number of reasons:</p>
<ul>
<li>The form of the observed data log-likelihood function prevents analytic simplifications (note the sum inside the logarithm) and thus can be difficult to compute.</li>
<li>Because of the symmetries due to exchangeability of cluster labels the likelihood function is multimodal and thus hard to optimise. Note this is also linked to the general problem of label switching and non-identifiability of cluster labels — see the discussion for <span class="math inline">\(K\)</span>-means clustering.</li>
<li>Further identifiability issues can arise if (for instance) two neighboring components of the mixture model are largely overlapping and thus are too close to each other to be discriminated as two different modes. In other words, it is difficult to determine the number of classes.</li>
<li>Furthermore, the likelihood in Gaussian mixture models is singular if one of the fitted covariance matrices becomes singular. However, this can be easily adressed by using some form of regularisation (Bayes, penalised ML, etc.) or simply by requiring sufficient sample size per group.</li>
</ul>
</section>
<section id="predicting-the-group-allocation-of-a-given-sample" class="level3" data-number="4.5.4">
<h3 data-number="4.5.4" class="anchored" data-anchor-id="predicting-the-group-allocation-of-a-given-sample"><span class="header-section-number">4.5.4</span> Predicting the group allocation of a given sample</h3>
<p>In probabilistic clustering the aim is to infer the latent states <span class="math inline">\(y_1, \ldots, y_n\)</span> for all observed samples <span class="math inline">\(\symbfit x_1, \ldots, \symbfit x_n\)</span>.</p>
<p>Assuming that the mixture model is known (either in advance or after fitting it) Bayes’ theorem allows predict the probability that an observation <span class="math inline">\(\symbfit x_i\)</span> falls in group <span class="math inline">\(k \in \{1, \ldots, K\}\)</span>: <span class="math display">\[
q_i(k) = \text{Pr}(k | \symbfit x_i) = \frac{\pi_k f_k(\symbfit x_i ) }{ f(\symbfit x_i)}
\]</span> Thus, for each of the <span class="math inline">\(n\)</span> samples we get a probability mass function over the <span class="math inline">\(K\)</span> classes with <span class="math inline">\(\sum_{k=1}^K q_i(k)=1\)</span>.</p>
<p>The posterior probabilities in <span class="math inline">\(q_i(k)\)</span> provide a so-called <em>soft assignment</em> of the sample <span class="math inline">\(\symbfit x_i\)</span> to all classes rather than a 0/1 <em>hard assignment</em> to a specific class (as for example in the <span class="math inline">\(K\)</span>-means algorithm).</p>
<p>To obtain at a hard clustering and to infer the most probable latent state we select the class with the highest probability <span class="math display">\[
y_i =\underset{k}{\arg \max}\,\,q_i(k)
\]</span></p>
<p>Thus, in probabilistic clustering we directly obtain an assessment of the uncertainty of the class assignment for a sample <span class="math inline">\(\symbfit x_i\)</span> (which is not the case in simple algorithmic clustering such <span class="math inline">\(K\)</span>-means). We can use this information to check whether there are several classes with equal or similar probability. This will be the case, e.g., if <span class="math inline">\(\symbfit x_i\)</span> lies near the boundary between two neighbouring classes.</p>
<p>Using the interactive Shiny app for the univariate normal component mixture (online at <a href="https://minerva.it.manchester.ac.uk/shiny/strimmer/mixture/" class="uri">https://minerva.it.manchester.ac.uk/shiny/strimmer/mixture/</a>) you can explore the posterior probabilities of each class.</p>
</section>
</section>
<section id="application-of-gaussian-mixture-models" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="application-of-gaussian-mixture-models"><span class="header-section-number">4.6</span> Application of Gaussian mixture models</h2>
<section id="choosing-the-number-of-classes" class="level3" data-number="4.6.1">
<h3 data-number="4.6.1" class="anchored" data-anchor-id="choosing-the-number-of-classes"><span class="header-section-number">4.6.1</span> Choosing the number of classes</h3>
<p>In an application of a GMM we need to select a suitable value for <span class="math inline">\(K\)</span>, i.e.&nbsp;the number of classes.</p>
<p>Since GMMs operate in a likelihood framework we can use penalised likelihood model selection criteria to choose among different models (i.e.&nbsp;GMMs with different numbers of classes).</p>
<p>The most popular choices are AIC (Akaike Information Criterion) and BIC (Bayesian Information criterion) defined as follows: <span class="math display">\[\text{AIC}= -2 \log L + 2 K \]</span> <span class="math display">\[\text{BIC}= - 2 \log L +K \log(n)\]</span></p>
<p>In order to choose a suitable model we evaluate different models with different <span class="math inline">\(K\)</span> and then choose the model that minimises <span class="math inline">\(\text{AIC}\)</span> or <span class="math inline">\(\text{BIC}\)</span></p>
<p>Note that in both criteria more complex models with more parameters (in this case groups) are penalised over simpler models in order to prevent overfitting.</p>
<p>Another way of choosing optimal numbers of clusters is by cross-validation (see later chapter on supervised learning).</p>
</section>
<section id="application-of-gmms-to-iris-flower-data" class="level3" data-number="4.6.2">
<h3 data-number="4.6.2" class="anchored" data-anchor-id="application-of-gmms-to-iris-flower-data"><span class="header-section-number">4.6.2</span> Application of GMMs to Iris flower data</h3>
<p>We now explore the application of Gaussian mixture models to the Iris flower data set we also investigated with PCA and K-means.</p>
<p>First, we fit a GMM with 3 clusters, using the R software “mclust” <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(iris)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>X.iris <span class="ot">=</span> <span class="fu">scale</span>((iris[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]), <span class="at">scale=</span><span class="cn">TRUE</span>) <span class="co"># center and standardise</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>L.iris <span class="ot">=</span> iris[, <span class="dv">5</span>]</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"mclust"</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>gmm3 <span class="ot">=</span> <span class="fu">Mclust</span>(X.iris, <span class="at">G=</span><span class="dv">3</span>, <span class="at">verbose=</span><span class="cn">FALSE</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gmm3, <span class="at">what=</span><span class="st">"classification"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="432"></p>
</figure>
</div>
</div>
</div>
<p>The “mclust” software has used the following model when fitting the mixture: ::: {.cell}</p>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>gmm3<span class="sc">$</span>modelName</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "VVV"</code></pre>
</div>
<p>::: Here “VVV” is the name used by the “mclust” software for a model allowing for an individual unrestricted covariance matrix <span class="math inline">\(\symbfit \Sigma_k\)</span> for each class <span class="math inline">\(k\)</span>.</p>
<p>This GMM has a substantially lower misclassification error compared to <span class="math inline">\(K\)</span>-means with the same number of clusters:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(gmm3<span class="sc">$</span>classification, L.iris)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   L.iris
    setosa versicolor virginica
  1     50          0         0
  2      0         45         0
  3      0          5        50</code></pre>
</div>
</div>
<p>Note that in “mclust” the BIC criterion is defined with the opposite sign (<span class="math inline">\(\text{BIC}_{\text{mclust}} = 2 \log L -K \log(n)\)</span>), thus we need to find the <em>maximum</em> value rather than the smallest value.</p>
<p>If we compute BIC for various numbers of groups we find that the model with the best <span class="math inline">\(\text{BIC}_{\text{mclust}}\)</span> is a model with 2 clusters but the model with 3 cluster has nearly as good a BIC:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="04-clustering_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="the-em-algorithm" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="the-em-algorithm"><span class="header-section-number">4.7</span> The EM algorithm</h2>
<section id="motivation" class="level3" data-number="4.7.1">
<h3 data-number="4.7.1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">4.7.1</span> Motivation</h3>
<p>As discussed above, the observed data log-likelihood can be difficult to maximise directly due to its form as a log marginal likelihood. Intriguingly, it is possible to optimise it <em>indirectly</em> using the complete data log-likelihood, and what’s more this also allows in many cases for an analytic expression of the maximisation step.</p>
<p>This method is called the EM algorithm and has been formally proposed and described by <a href="https://en.wikipedia.org/wiki/Arthur_P._Dempster">Arthur Dempster (1929–)</a> and others in 1977<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> but the algorithm was already know prior. It iteratively estimates both the parameters of the mixture model parameters and the latent states. The key idea behind the EM algorithm is to capitalise on the simplicity of the complete data likelihood and to obtain estimates of <span class="math inline">\(\symbfit \theta\)</span> by imputing the missing group allocations and then subsequently iteratively refining both the imputations and the estimates of <span class="math inline">\(\symbfit \theta\)</span>.</p>
<p>More precisely, in the EM (=expectation-maximisation) algorithm we alternate between</p>
<ul>
<li><p>Step 1) updating the soft allocations of each sample using the current estimate of the parameters <span class="math inline">\(\symbfit \theta\)</span> (obtained in step 2)</p></li>
<li><p>Step 2) updating the parameter estimates by maximising the <em>expected</em> complete data log-likelihood. The expectation is taken with regard to the distribution over the latent states (obtained in step 1). Thus the complete data log-likelihood is averaged over the soft class assignments. For an exponential family (e.g.&nbsp;when the distributions for each group are normal) maximisation of the expected complete data log-likelihood can even be done analytically.</p></li>
</ul>
</section>
<section id="the-em-algorithm-1" class="level3" data-number="4.7.2">
<h3 data-number="4.7.2" class="anchored" data-anchor-id="the-em-algorithm-1"><span class="header-section-number">4.7.2</span> The EM algorithm</h3>
<p>Specifically, the EM algorithm proceeds as follows:</p>
<ol type="1">
<li>Initialisation:</li>
</ol>
<ul>
<li>Start with a guess of the parameters <span class="math inline">\(\hat{\symbfit \theta}^{(1)}\)</span>, then continue with “E” Step, Part A.</li>
<li>Alternatively, start with a guess of the soft allocations for each sample <span class="math inline">\(q_i(k)^{(1)}\)</span>, collected in the matrix <span class="math inline">\(\symbfit Q^{(1)}\)</span>, then continue with “E” Step, Part B.<br>
This may be derived from some prior information, e.g., from running <span class="math inline">\(K\)</span>-means. Caveat: some particular initialisations correspond to invariant states and hence should be avoided (see further below).</li>
</ul>
<ol start="2" type="1">
<li><strong>E “expectation” step</strong></li>
</ol>
<ul>
<li><p>Part A: Use Bayes’ theorem to compute new probabilities of allocation to class <span class="math inline">\(k\)</span> for all the samples <span class="math inline">\(\symbfit x_i\)</span>: <span class="math display">\[
q_i(k)^{(b+1)} \leftarrow \frac{ \hat{\pi}_k^{(b)} f_k(\symbfit x_i | \hat{\symbfit \theta}^{(b)})    }{  f(\symbfit x_i |\hat{\symbfit \theta}^{(b)} )  }
\]</span> Note that to obtain <span class="math inline">\(q_i(k)^{(b+1)}\)</span> the current estimate <span class="math inline">\(\hat{\symbfit \theta}^{(b)}\)</span> of the parameters of the mixture model is required.</p></li>
<li><p>Part B: Construct the <em>expected</em> complete data log-likelihood function for <span class="math inline">\(\symbfit \theta\)</span> using the soft allocations <span class="math inline">\(q_i(k)^{(b+1)}\)</span> collected in the matrix <span class="math inline">\(\symbfit Q^{(b+1)}\)</span>: <span class="math display">\[
G(\symbfit \theta| \symbfit X, \symbfit Q^{(b+1)} ) = \sum_{i=1}^n \sum_{k=1}^K q_i(k)^{(b+1)}  \log \left( \pi_k f_k(\symbfit x_i) \right)
\]</span> Note that in the case that the soft allocations <span class="math inline">\(\symbfit Q^{(b+1)}\)</span> turn into hard 0/1 allocations then <span class="math inline">\(G(\symbfit \theta| \symbfit X, \symbfit Q^{(b+1)})\)</span> becomes equivalent to the complete data log-likelihood.</p></li>
</ul>
<ol start="3" type="1">
<li><p><strong>M “maximisation” step</strong> — Maximise the expected complete data log-likelihood to update the estimates of mixture model parameters: <span class="math display">\[
\hat{\symbfit \theta}^{(b+1)} \leftarrow \arg \max_{\symbfit \theta}  G(\symbfit \theta| \symbfit X, \symbfit Q^{(b+1)} )
\]</span></p></li>
<li><p>Continue with 2) “E” Step until the series <span class="math inline">\(\hat{\symbfit \theta}^{(1)}, \hat{\symbfit \theta}^{(2)}, \hat{\symbfit \theta}^{(3)}, \ldots\)</span> has converged.</p></li>
</ol>
<p>Crucially, maximisation of the expected complete data log-likelihood is typically much easier than maximisation of the observed data log-likelihood, and in many cases even analytically tractable, which is why in this case the EM algorithm is often preferred over direct maximisation of the observed data log-likelihood.</p>
<p>Note that to avoid singularities in the expected log-likelihood function we may need to adopt regularisation (i.e.&nbsp;penalised maximum likelihood or Bayesian learning) for estimating the parameters in the M-step.</p>
</section>
<section id="em-algorithm-for-multivariate-normal-mixture-model" class="level3" data-number="4.7.3">
<h3 data-number="4.7.3" class="anchored" data-anchor-id="em-algorithm-for-multivariate-normal-mixture-model"><span class="header-section-number">4.7.3</span> EM algorithm for multivariate normal mixture model</h3>
<p>We now consider the EM algorithm applied to the case when the conditional group distributions are normal, i.e.&nbsp;when applied to the Gaussian mixture model (GMM). In this case the two iterative steps in the EM algorithm can be expressed as follows:</p>
<p><strong>E-step:</strong></p>
<p>Update the soft allocations: <span class="math display">\[
q_i(k)^{(b+1)} = \frac{ \hat{\pi}_k^{(b)} N(\symbfit x_i | \hat{\symbfit \mu}_k^{(b)}, \hat{\symbfit \Sigma}_k^{(b)}) }{
\hat{f}^{(b)} \left(\symbfit x_i |  
\hat{\pi}_1^{(b)}, \ldots, \hat{\pi}_K^{(b)},
\hat{\symbfit \mu}_1^{(b)}, \ldots, \hat{\symbfit \mu}_K^{(b)},
\hat{\symbfit \Sigma}_1^{(b)}, \ldots, \hat{\symbfit \Sigma}_K^{(b)} \right)  }
\]</span></p>
<p>Correspondingly, the number of samples assigned to class <span class="math inline">\(k\)</span> in the current step is <span class="math display">\[
n_k^{(b+1)} = \sum_{i=1}^n q_i(k)^{(b+1)}
\]</span> Note this is not necessarily an integer because of the soft allocations of samples to groups.</p>
<p>The expected complete data log-likelihood becomes: <span class="math display">\[
G(\pi_1, \ldots \pi_K,\symbfit \mu_1, \ldots, \symbfit \mu_K,  \symbfit \Sigma_1, \ldots, \symbfit \Sigma_K | \symbfit X, \symbfit Q^{(b+1)} ) =
\sum_{i=1}^n \sum_{k=1}^K q_i(k)^{(b+1)}  \log \left( \pi_k f_k(\symbfit x_i) \right)
\]</span> with <span class="math display">\[
\log \left( \pi_k f_k(\symbfit x_i) \right) =  -\frac{1}{2} (\symbfit x-\symbfit \mu_k)^T \symbfit \Sigma_k^{-1} (\symbfit x-\symbfit \mu_k) -\frac{1}{2} \log \det(\symbfit \Sigma_k) +\log(\pi_k)
\]</span> (Remark: we will encounter this expression again in the next chapter when discussing quadratic discriminant analysis).</p>
<p><strong>M-step:</strong></p>
<p>The maximisation of the expected complete data log-likelihood can be done analytically as it is a weighted version of the conventional single group multivariate normal log-likelihood. The resulting estimators are also weighted variants of the usual MLEs.</p>
<p>The updated estimates of the group probabilities are <span class="math display">\[
\hat{\pi}_k^{(b+1)} = \frac{n_k^{(b+1)}}{n}
\]</span> The updated estimates of the means are <span class="math display">\[
\hat{\symbfit \mu}_k^{(b+1)} = \frac{1}{n_k^{(b+1)}} \sum_{i=1}^n q_i(k)^{(b+1)} \symbfit x_i
\]</span> and the updated covariance estimates are <span class="math display">\[
\hat{\symbfit \Sigma}_k^{(b+1)} =  \frac{1}{n_k^{(b+1)}} \sum_{i=1}^n q_i(k)^{(b+1)} \left( \symbfit x_i -\symbfit \mu_k^{(b+1)}\right)   \left( \symbfit x_i -\symbfit \mu_k^{(b+1)}\right)^T
\]</span></p>
<p>Note that if <span class="math inline">\(q_i(k)\)</span> is a hard allocation (so that for any <span class="math inline">\(i\)</span> only one class has weight 1 and all others weight 0) then all estimators above reduce to the usual MLEs.</p>
<p>In Worksheet 8 you can find a simple R implementation of the EM algorithm for univariate normal mixtures.</p>
<p>Similar analytical expressions can also be found in more general mixtures where the components are exponential families. As mentioned above this is one of the advantages of using the EM algorithm.</p>
</section>
<section id="convergence-and-invariant-states" class="level3" data-number="4.7.4">
<h3 data-number="4.7.4" class="anchored" data-anchor-id="convergence-and-invariant-states"><span class="header-section-number">4.7.4</span> Convergence and invariant states</h3>
<p>Under mild assumptions the EM algorithm is guaranteed to monotonically converge to local optima of the observed data log-likelihood <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. Thus the series <span class="math inline">\(\hat{\symbfit \theta}^{(1)}, \hat{\symbfit \theta}^{(2)}, \hat{\symbfit \theta}^{(3)}, \ldots\)</span> converges to the estimate <span class="math inline">\(\hat{\symbfit \theta}\)</span> found when maximising the observed data log-likelihood. However, the speed of convergence in the EM algorithm can sometimes be slow, and there are also situations in which there is no convergence at all to <span class="math inline">\(\hat{\symbfit \theta}\)</span> because the EM algorithm remains in an invariant state.</p>
<p>An example of such an invariant state for a Gaussian mixture model is uniform initialisation of the latent variables <span class="math inline">\(q_i(k) = \frac{1}{K}\)</span>, where <span class="math inline">\(K\)</span> is the number of classes.<br>
With this we get in the M step <span class="math inline">\(n_k = \frac{n}{K}\)</span> and as parameter estimates <span class="math display">\[
\hat{\pi}_k = \frac{1}{K}
\]</span> <span class="math display">\[
\hat{\symbfit \mu}_k = \frac{1}{n} \sum_{i=1}^n \symbfit x_i = \bar{\symbfit x}
\]</span> <span class="math display">\[
\hat{\symbfit \Sigma}_k = \frac{1}{n}  \sum_{i=1}^n ( \symbfit x_i -\bar{\symbfit x})   ( \symbfit x_i -\bar{\symbfit x})^T = \hat{\symbfit \Sigma}
\]</span> Crucially, none of these actually depend on the group <span class="math inline">\(k\)</span>! Thus, in the E step when the next soft allocations are determined this leads to <span class="math display">\[
q_i(k) = \frac{ \frac{1}{K} N(\symbfit x_i | \bar{\symbfit x}, \hat{\symbfit \Sigma} ) }{ \sum_{j=1}^K  \frac{1}{K} N(\symbfit x_i | \bar{\symbfit x}, \hat{\symbfit \Sigma} )  } = \frac{1}{K}
\]</span> After one cycle in the EM algorithm we arrive at the same soft allocation that we started with, and the algorithm is trapped in an invariant state! Therefore uniform initialisation should clearly be avoided!</p>
<p>You will explore this effect in practise in Worksheet 8.</p>
</section>
<section id="connection-with-the-k-means-clustering-method" class="level3" data-number="4.7.5">
<h3 data-number="4.7.5" class="anchored" data-anchor-id="connection-with-the-k-means-clustering-method"><span class="header-section-number">4.7.5</span> Connection with the <span class="math inline">\(K\)</span>-means clustering method</h3>
<p>The <span class="math inline">\(K\)</span>-means algorithm is very closely related to the EM algorithm and probabilistic clustering with a specific Gaussian mixture models.</p>
<p>Specifically, we assume a simplified model where the probabilities <span class="math inline">\(\pi_k\)</span> of all classes are equal (i.e.&nbsp;<span class="math inline">\(\pi_k=\frac{1}{K}\)</span>) and where the covariances <span class="math inline">\(\symbfit \Sigma_k\)</span> are all of the same spherical form <span class="math inline">\(\sigma^2 \symbfit I\)</span>. Thus, the covariance does not depend on the group, there is no correlation between the variables and the variance of all variables is the same.</p>
<p>First, we consider the “E” step. Using the mixture model above the soft assignment for the class allocation becomes <span class="math display">\[
\log( q_i(k) ) = -\frac{1}{2 \sigma^2} (\symbfit x_i-\hat{\symbfit \mu}_k)^T (\symbfit x_i-\hat{\symbfit \mu}_k) +  \text{const}
\]</span> where <span class="math inline">\(\text{const}\)</span> does not depend on <span class="math inline">\(k\)</span>. This can turned into a hard class allocation by <span class="math display">\[
\begin{split}
y_i &amp;= \underset{k}{\arg \max} \log( q_i(k) ) \\
          &amp; = \underset{k}{\arg \min}  (\symbfit x_i-\hat{\symbfit \mu}_k)^T (\symbfit x_i-\hat{\symbfit \mu}_k)\\
\end{split}
\]</span> which is exactly the <span class="math inline">\(K\)</span>-means rule to dallocate of samples to groups.</p>
<p>Second, in the “M” step we compute the parameters of the model. If the class allocations are hard the expected log-likelihood becomes the observed data likelihood and the MLE of the group mean is the average of samples in that group.</p>
<p>Thus, <span class="math inline">\(K\)</span>-means can be viewed as an EM type algorithm to provide hard classification based on a simple restricted Gaussian mixture model.</p>
</section>
<section id="why-the-em-algorithm-works-an-entropy-point-of-view" class="level3" data-number="4.7.6">
<h3 data-number="4.7.6" class="anchored" data-anchor-id="why-the-em-algorithm-works-an-entropy-point-of-view"><span class="header-section-number">4.7.6</span> Why the EM algorithm works — an entropy point of view</h3>
<p>The iterative (soft) imputation of the latent states in the EM algorithm is intuitively clear.</p>
<p>However, in order to get a better understanding of EM we need to demonstrate</p>
<ol type="i">
<li>why the expected observed log-likelihood needs to be maximised rather than, e.g., the observed log-likelihood with hard allocations, and</li>
<li>that applying the EM algorithm versus directly maximising the marginal likelihood both lead to the same fitted mixture model.</li>
</ol>
<p>Intriguingly, both these aspects of the EM algorithm are easiest to understand from an entropy point of view, i.e.&nbsp;considering the entropy foundations of maximum likelihood and Bayesian learning — for details see <a href="https://strimmerlab.github.io/publications/lecture-notes/MATH27720-stats2/">MATH27720 Statistics 2</a>. Specifically, the reason for the need of using expectation is the link between likelihood and cross-entropy (which also is defined as expectation). Furthermore, the EM algorithm is an example of using an ELBO (“evidence lower bound”) to successively approximate the maximised marginal log-likelihood, with the bound only getting better in each step.</p>
<p>First, recall that the method of maximum likelihood results from minimising the KL divergence between an empirical distribution <span class="math inline">\(Q_{\symbfit x}\)</span> representing the observations <span class="math inline">\(\symbfit x_1, \ldots, \symbfit x_n\)</span> and the model family <span class="math inline">\(F_{\symbfit x}^{\symbfit \theta}\)</span> with parameters <span class="math inline">\(\symbfit \theta\)</span>: <span class="math display">\[
\hat{\symbfit \theta}^{ML} =  \underset{\symbfit \theta}{\arg \min}\,\, D_{\text{KL}}(Q_{\symbfit x}, F_{\symbfit x}^{\symbfit \theta})
\]</span> The KL divergence decomposes into a cross-entropy and an entropy part <span class="math display">\[
D_{\text{KL}}(Q_{\symbfit x}, F_{\symbfit \theta}) = H(Q_{\symbfit x}, F_{\symbfit x}^{\symbfit \theta})- H(Q_{\symbfit x})
\]</span> hence minimising the KL divergence with regard to <span class="math inline">\(\symbfit \theta\)</span> is the same as maximising the function <span class="math display">\[
\begin{split}
-n H(Q_{\symbfit x}, F_{\symbfit x}^{\symbfit \theta}) &amp;= n \text{E}_{Q_{\symbfit x}}( \log f(\symbfit x| \symbfit \theta)  ) \\
&amp;= \sum_{i=1}^n  \log f(\symbfit x_i | \symbfit \theta)\\
&amp;= \log L(\symbfit \theta| \symbfit X)\\
\end{split}
\]</span> which is indeed the observed data log-likelihood for <span class="math inline">\(\symbfit \theta\)</span>.</p>
<p>Second, we recall the chain rule for the KL divergence. Specifically, the KL divergence for the joint model forms an upper bound of the KL divergence for the marginal model: <span class="math display">\[
\begin{split}
D_{\text{KL}}(Q_{\symbfit x,y} , F_{\symbfit x, y}^{\symbfit \theta}) &amp;= D_{\text{KL}}(Q_{\symbfit x} , F_{\symbfit x}^{\symbfit \theta}) + \underbrace{  D_{\text{KL}}(Q_{y| \symbfit x} , F_{y|\symbfit x}^{\symbfit \theta})   }_{\geq 0}\\
&amp;\geq D_{\text{KL}}(Q_{\symbfit x} , F_{\symbfit x}^{\symbfit \theta})
\end{split}
\]</span> Unlike for <span class="math inline">\(\symbfit x\)</span> we do not have observations about the latent state <span class="math inline">\(y\)</span>. Nonetheless, we can model the joint distribution <span class="math inline">\(Q_{\symbfit x, y} =Q_{\symbfit x}  Q_{y|\symbfit x}\)</span> by assuming a distribution <span class="math inline">\(Q_{y|\symbfit x}\)</span> over the latent variable.</p>
<p>The EM algorithm arises from iteratively decreasing the joint KL divergence <span class="math inline">\(D_{\text{KL}}(Q_{\symbfit x}  Q_{y|\symbfit x} , F_{\symbfit x, y}^{\symbfit \theta})\)</span> with regard to both <span class="math inline">\(Q_{y|\symbfit x}\)</span> and <span class="math inline">\(\symbfit \theta\)</span>:</p>
<ol type="1">
<li><p>“E” Step: While keeping <span class="math inline">\(\symbfit \theta\)</span> fixed we vary <span class="math inline">\(Q_{y|\symbfit x}\)</span> to minimise the joint KL divergence. The minimum is reached at <span class="math inline">\(D_{\text{KL}}(Q_{y| \symbfit x} , F_{y|\symbfit x}^{\symbfit \theta}) = 0\)</span>. This is the case for <span class="math inline">\(Q_{y| \symbfit x} = F_{y|\symbfit x}^{\symbfit \theta}\)</span>, i.e.&nbsp;when the latent distribution <span class="math inline">\(Q_{y| \symbfit x}\)</span> representing the soft allocations is computed by conditioning, i.e.&nbsp;using Bayes’ theorem.</p></li>
<li><p>“M” Step: While keeping <span class="math inline">\(Q_{y| \symbfit x}\)</span> fixed the joint KL divergence is further minimised with regard to <span class="math inline">\(\symbfit \theta\)</span>. This is equivalent to maximising the function <span class="math inline">\(\sum_{k=1}^K \sum_{i=1}^n q(k | \symbfit x_i)  \log f(\symbfit x_i, k| \symbfit \theta)\)</span> which is indeed the expected complete data log-likelihood.</p></li>
</ol>
<p>Thus in the “E” step the first argument in the KL divergence is optimised (“I” projection) and in the “M” step it is the second argument that is optimised (“M” projection). In both steps the joint KL divergence always decreases and never increases. Furthermore, at the end of “E” step the joint KL divergence equals the marginal KL divergence. Thus, this procedure implicitly minimises the marginal KL divergence as well, and hence EM maximises the marginal log-likelihood.</p>
<p>An alternative way to look at the EM algorithm is in terms of cross-entropy. Using <span class="math inline">\(H( Q_{\symbfit x,y}) = H(Q_{\symbfit x}) + H(Q_{y| \symbfit x} )\)</span> we can rewrite the above upper bound for the joint KL divergence as an equivalent lower bound for <span class="math inline">\(n\)</span> times the negative marginal cross-entropy: <span class="math display">\[
\begin{split}
- n H(Q_{\symbfit x}, F_{\symbfit x}^{\symbfit \theta}) &amp;= \underbrace{ -n  H(Q_{\symbfit x} Q_{y| \symbfit x} , F_{\symbfit x, y}^{\symbfit \theta})  + n H(Q_{y| \symbfit x} )}_{\text{lower bound, ELBO}}  + \underbrace{ n D_{\text{KL}}(Q_{y| \symbfit x} , F_{y|\symbfit x}^{\symbfit \theta})}_{\geq 0}\\
&amp; \geq {\cal F}\left( Q_{\symbfit x}, Q_{y| \symbfit x},  F_{\symbfit x, y}^{\symbfit \theta}\right)\\
\end{split}
\]</span> The lower bound is known as the “ELBO” (“evidence lower bound”). Then the EM algorithm arises by iteratively maximising the ELBO <span class="math inline">\(\cal F\)</span> with regard to <span class="math inline">\(Q_{y| \symbfit x}\)</span> (“E” step”) and <span class="math inline">\(\symbfit \theta\)</span> (“M” step).</p>
<p>The entropy interpretation of the EM algorithm is due to Csiszàr and Tusnàdy (1984)<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> and the ELBO interpretation was introduced by Neal and Hinton (1998)<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>In contrast, in <strong>supervised learning</strong> (to be discussed in a subsequent chapter) the class labels are known for a subset of the data (the training data set) and are required to learn a prediction function.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Ward, J.H. 1963. Hierarchical grouping to optimize an objective function. JASA <strong>58</strong>:236–244.<br>
<a href="https://doi.org/10.1080/01621459.1963.10500845" class="uri">https://doi.org/10.1080/01621459.1963.10500845</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>F. Murtagh and P Legendre. 2014. Ward’s hierarchical agglomerative clustering method: which algorithms implement Ward’s criterion? J. Classif. <em>31</em>:274–295. <a href="https://doi.org/10.1007/s00357-014-9161-z" class="uri">https://doi.org/10.1007/s00357-014-9161-z</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>H.-H. Bock. 2008. Origins and extensions of the <span class="math inline">\(k\)</span>-means algorithm in cluster analysis. JEHPS <strong>4</strong>, no. 2. <a href="https://www.jehps.net/Decembre2008/Bock.pdf" class="uri">https://www.jehps.net/Decembre2008/Bock.pdf</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>L. Scrucca L. et. al.&nbsp;2016. mclust 5: Clustering, classification and density estimation using Gaussian finite mixture models. The R Journal 8:205–233. See <a href="https://journal.r-project.org/archive/2016/RJ-2016-021/" class="uri">https://journal.r-project.org/archive/2016/RJ-2016-021/</a> and <a href="https://mclust-org.github.io/mclust/" class="uri">https://mclust-org.github.io/mclust/</a>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Dempster, A. P, N. M. Laird and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. JRSS B <strong>39</strong>:1–38. <a href="https://doi.org/10.1111/j.2517-6161.1977.tb01600.x" class="uri">https://doi.org/10.1111/j.2517-6161.1977.tb01600.x</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Wu, C.F. 1983. On the convergence properties of the EM algorithm. The Annals of Statistics <em>11</em>:95–103. <a href="https://doi.org/10.1214/aos/1176346060" class="uri">https://doi.org/10.1214/aos/1176346060</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Csiszàr, I., and G, Tusnàdy. 1984. Information geometry and alternating minimization procedures. In Dudewicz, E. J. et al.&nbsp;(eds.) Recent Results in Estimation Theory and Related Topics Statistics and Decisions, Supplement Issue No.&nbsp;1. pp.&nbsp;205–237.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Neal, R. M., and G. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Jordan, M.I. (eds.). Learning in Graphical Models. pp.&nbsp;355–368. <a href="https://doi.org/10.1007/978-94-011-5014-9_12" class="uri">https://doi.org/10.1007/978-94-011-5014-9_12</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH38161");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-transformations.html" class="pagination-link" aria-label="Transformations and dimension reduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations and dimension reduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-classification.html" class="pagination-link" aria-label="Supervised learning and classification">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning and classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>These notes were written by <a href="https://strimmerlab.github.io/korbinian.html">Korbinian Strimmer</a> using <a href="https://quarto.org">Quarto</a>,</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>