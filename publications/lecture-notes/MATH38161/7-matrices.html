<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>A Brief refresher on matrices | HTML</title>
  <meta name="description" content="Multivariate Statistics and Machine Learning" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="A Brief refresher on matrices | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Brief refresher on matrices | HTML" />
  
  
  



<meta name="date" content="2020-12-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="6-nonlinear.html"/>
<link rel="next" href="8-further-study.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH38161/index.html">MATH38161 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-multivariate.html"><a href="1-multivariate.html"><i class="fa fa-check"></i><b>1</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="1.1" data-path="1-multivariate.html"><a href="1-multivariate.html#why-multivariate-statistics"><i class="fa fa-check"></i><b>1.1</b> Why multivariate statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="1-multivariate.html"><a href="1-multivariate.html#essentials-in-multivariate-statistics"><i class="fa fa-check"></i><b>1.2</b> Essentials in multivariate statistics</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-vs.multivariate-random-variables"><i class="fa fa-check"></i><b>1.2.1</b> Univariate vs.Â multivariate random variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-multivariate.html"><a href="1-multivariate.html#mean-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.2</b> Mean of a random vector</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-multivariate.html"><a href="1-multivariate.html#variance-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.3</b> Variance of a random vector</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-multivariate.html"><a href="1-multivariate.html#properties-of-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.4</b> Properties of the covariance matrix</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-multivariate.html"><a href="1-multivariate.html#eigenvalue-decomposition-of-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.2.5</b> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.2.6" data-path="1-multivariate.html"><a href="1-multivariate.html#quantities-related-to-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.6</b> Quantities related to the covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>1.3</b> Multivariate normal distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-normal-distribution"><i class="fa fa-check"></i><b>1.3.1</b> Univariate normal distribution:</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-model"><i class="fa fa-check"></i><b>1.3.2</b> Multivariate normal model</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-multivariate.html"><a href="1-multivariate.html#shape-of-the-multivariate-normal-density"><i class="fa fa-check"></i><b>1.3.3</b> Shape of the multivariate normal density</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-multivariate.html"><a href="1-multivariate.html#three-types-of-covariances"><i class="fa fa-check"></i><b>1.3.4</b> Three types of covariances</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-in-large-sample-and-small-sample-settings"><i class="fa fa-check"></i><b>1.4</b> Estimation in large sample and small sample settings</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-data"><i class="fa fa-check"></i><b>1.4.1</b> Multivariate data</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-multivariate.html"><a href="1-multivariate.html#strategies-for-large-sample-estimation"><i class="fa fa-check"></i><b>1.4.2</b> Strategies for large sample estimation</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-multivariate.html"><a href="1-multivariate.html#large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.4.3</b> Large sample estimates of mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.4.4" data-path="1-multivariate.html"><a href="1-multivariate.html#problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions"><i class="fa fa-check"></i><b>1.4.4</b> Problems with maximum likelihood in small sample settings and high dimensions</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-of-covariance-matrix-in-small-sample-settings"><i class="fa fa-check"></i><b>1.4.5</b> Estimation of covariance matrix in small sample settings</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-multivariate.html"><a href="1-multivariate.html#discrete-multivariate-distributions"><i class="fa fa-check"></i><b>1.5</b> Discrete multivariate distributions</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-multivariate.html"><a href="1-multivariate.html#categorical-distribution"><i class="fa fa-check"></i><b>1.5.1</b> Categorical distribution</a></li>
<li class="chapter" data-level="1.5.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multinomial-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Multinomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="1-multivariate.html"><a href="1-multivariate.html#continuous-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Continuous multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-multivariate.html"><a href="1-multivariate.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.6.1</b> Dirichlet distribution</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-multivariate.html"><a href="1-multivariate.html#wishart-distribution"><i class="fa fa-check"></i><b>1.6.2</b> Wishart distribution</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-multivariate.html"><a href="1-multivariate.html#inverse-wishart-distribution"><i class="fa fa-check"></i><b>1.6.3</b> Inverse Wishart distribution</a></li>
<li class="chapter" data-level="1.6.4" data-path="1-multivariate.html"><a href="1-multivariate.html#further-distributions"><i class="fa fa-check"></i><b>1.6.4</b> Further distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-transformations.html"><a href="2-transformations.html"><i class="fa fa-check"></i><b>2</b> Transformations and dimension reduction</a><ul>
<li class="chapter" data-level="2.1" data-path="2-transformations.html"><a href="2-transformations.html#linear-transformations"><i class="fa fa-check"></i><b>2.1</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-transformations.html"><a href="2-transformations.html#location-scale-transformation"><i class="fa fa-check"></i><b>2.1.1</b> Location-scale transformation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-transformations.html"><a href="2-transformations.html#invertible-location-scale-transformation"><i class="fa fa-check"></i><b>2.1.2</b> Invertible location-scale transformation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-transformations.html"><a href="2-transformations.html#nonlinear-transformations"><i class="fa fa-check"></i><b>2.2</b> Nonlinear transformations</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-transformations.html"><a href="2-transformations.html#general-transformation"><i class="fa fa-check"></i><b>2.2.1</b> General transformation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-transformations.html"><a href="2-transformations.html#delta-method"><i class="fa fa-check"></i><b>2.2.2</b> Delta method</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-transformations.html"><a href="2-transformations.html#transformation-of-densities-under-general-invertible-transformation"><i class="fa fa-check"></i><b>2.2.3</b> Transformation of densities under general invertible transformation</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-transformations.html"><a href="2-transformations.html#normalising-flows"><i class="fa fa-check"></i><b>2.2.4</b> Normalising flows</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-transformations.html"><a href="2-transformations.html#whitening-transformations"><i class="fa fa-check"></i><b>2.3</b> Whitening transformations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-transformations.html"><a href="2-transformations.html#overview"><i class="fa fa-check"></i><b>2.3.1</b> Overview</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-transformations.html"><a href="2-transformations.html#general-whitening-transformation"><i class="fa fa-check"></i><b>2.3.2</b> General whitening transformation</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-transformations.html"><a href="2-transformations.html#solution-of-whitening-constraint-covariance-based"><i class="fa fa-check"></i><b>2.3.3</b> Solution of whitening constraint (covariance-based)</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-transformations.html"><a href="2-transformations.html#another-solution-correlation-based"><i class="fa fa-check"></i><b>2.3.4</b> Another solution (correlation-based)</a></li>
<li class="chapter" data-level="2.3.5" data-path="2-transformations.html"><a href="2-transformations.html#cross-covariance-and-cross-correlation"><i class="fa fa-check"></i><b>2.3.5</b> Cross-covariance and cross-correlation</a></li>
<li class="chapter" data-level="2.3.6" data-path="2-transformations.html"><a href="2-transformations.html#inverse-whitening-transformation-loadings-and-multiple-correlation"><i class="fa fa-check"></i><b>2.3.6</b> Inverse whitening transformation, loadings, and multiple correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-transformations.html"><a href="2-transformations.html#natural-whitening-procedures"><i class="fa fa-check"></i><b>2.4</b> Natural whitening procedures</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-transformations.html"><a href="2-transformations.html#zca-whitening"><i class="fa fa-check"></i><b>2.4.1</b> ZCA whitening</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-transformations.html"><a href="2-transformations.html#zca-cor-whitening"><i class="fa fa-check"></i><b>2.4.2</b> ZCA-Cor whitening</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-transformations.html"><a href="2-transformations.html#pca-whitening"><i class="fa fa-check"></i><b>2.4.3</b> PCA whitening</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-transformations.html"><a href="2-transformations.html#pca-cor-whitening"><i class="fa fa-check"></i><b>2.4.4</b> PCA-cor whitening</a></li>
<li class="chapter" data-level="2.4.5" data-path="2-transformations.html"><a href="2-transformations.html#cholesky-whitening"><i class="fa fa-check"></i><b>2.4.5</b> Cholesky whitening</a></li>
<li class="chapter" data-level="2.4.6" data-path="2-transformations.html"><a href="2-transformations.html#comparison-of-zca-pca-and-cholesky-whitening"><i class="fa fa-check"></i><b>2.4.6</b> Comparison of ZCA, PCA and Cholesky whitening</a></li>
<li class="chapter" data-level="2.4.7" data-path="2-transformations.html"><a href="2-transformations.html#recap"><i class="fa fa-check"></i><b>2.4.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-transformations.html"><a href="2-transformations.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>2.5</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-transformations.html"><a href="2-transformations.html#pca-transformation"><i class="fa fa-check"></i><b>2.5.1</b> PCA transformation</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-transformations.html"><a href="2-transformations.html#application-to-data"><i class="fa fa-check"></i><b>2.5.2</b> Application to data</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-transformations.html"><a href="2-transformations.html#iris-data-example"><i class="fa fa-check"></i><b>2.5.3</b> Iris data example</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-transformations.html"><a href="2-transformations.html#correlation-loadings-plot-to-interpret-pca-components"><i class="fa fa-check"></i><b>2.6</b> Correlation loadings plot to interpret PCA components</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-transformations.html"><a href="2-transformations.html#pca-correlation-loadings"><i class="fa fa-check"></i><b>2.6.1</b> PCA correlation loadings</a></li>
<li class="chapter" data-level="2.6.2" data-path="2-transformations.html"><a href="2-transformations.html#pca-correlation-loadings-plot"><i class="fa fa-check"></i><b>2.6.2</b> PCA correlation loadings plot</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-transformations.html"><a href="2-transformations.html#cca-whitening-canonical-correlation-analysis"><i class="fa fa-check"></i><b>2.7</b> CCA whitening (Canonical Correlation Analysis)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-transformations.html"><a href="2-transformations.html#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal"><i class="fa fa-check"></i><b>2.7.1</b> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</a></li>
<li class="chapter" data-level="2.7.2" data-path="2-transformations.html"><a href="2-transformations.html#related-methods"><i class="fa fa-check"></i><b>2.7.2</b> Related methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-clustering.html"><a href="3-clustering.html"><i class="fa fa-check"></i><b>3</b> Unsupervised learning and clustering</a><ul>
<li class="chapter" data-level="3.1" data-path="3-clustering.html"><a href="3-clustering.html#challenges-in-supervised-learning"><i class="fa fa-check"></i><b>3.1</b> Challenges in supervised learning</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-clustering.html"><a href="3-clustering.html#objective"><i class="fa fa-check"></i><b>3.1.1</b> Objective</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-clustering.html"><a href="3-clustering.html#questions-and-problems"><i class="fa fa-check"></i><b>3.1.2</b> Questions and problems</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-clustering.html"><a href="3-clustering.html#why-is-clustering-difficult"><i class="fa fa-check"></i><b>3.1.3</b> Why is clustering difficult?</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-clustering.html"><a href="3-clustering.html#common-types-of-clustering-methods"><i class="fa fa-check"></i><b>3.1.4</b> Common types of clustering methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-clustering.html"><a href="3-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>3.2</b> Hierarchical clustering</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-clustering.html"><a href="3-clustering.html#tree-like-structures"><i class="fa fa-check"></i><b>3.2.1</b> Tree-like structures</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-clustering.html"><a href="3-clustering.html#agglomerative-hierarchical-clustering-algorithms"><i class="fa fa-check"></i><b>3.2.2</b> Agglomerative hierarchical clustering algorithms</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-clustering.html"><a href="3-clustering.html#wards-clustering-method"><i class="fa fa-check"></i><b>3.2.3</b> Wardâs clustering method</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-clustering.html"><a href="3-clustering.html#application-to-swiss-banknote-data-set"><i class="fa fa-check"></i><b>3.2.4</b> Application to Swiss banknote data set</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-clustering.html"><a href="3-clustering.html#assessment-of-the-uncertainty-of-hierarchical-clusterings"><i class="fa fa-check"></i><b>3.2.5</b> Assessment of the uncertainty of hierarchical clusterings</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-clustering.html"><a href="3-clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>3.3</b> <span class="math inline">\(K\)</span>-means clustering</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-clustering.html"><a href="3-clustering.html#general-aims"><i class="fa fa-check"></i><b>3.3.1</b> General aims</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-clustering.html"><a href="3-clustering.html#algorithm"><i class="fa fa-check"></i><b>3.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-clustering.html"><a href="3-clustering.html#properties"><i class="fa fa-check"></i><b>3.3.3</b> Properties</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>3.3.4</b> Choosing the number of clusters</a></li>
<li class="chapter" data-level="3.3.5" data-path="3-clustering.html"><a href="3-clustering.html#k-medoids-aka-pam"><i class="fa fa-check"></i><b>3.3.5</b> <span class="math inline">\(K\)</span>-medoids aka PAM</a></li>
<li class="chapter" data-level="3.3.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-k-means-to-iris-data"><i class="fa fa-check"></i><b>3.3.6</b> Application of <span class="math inline">\(K\)</span>-means to Iris data</a></li>
<li class="chapter" data-level="3.3.7" data-path="3-clustering.html"><a href="3-clustering.html#arbitrariness-of-cluster-labels-and-label-switching"><i class="fa fa-check"></i><b>3.3.7</b> Arbitrariness of cluster labels and label switching</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-clustering.html"><a href="3-clustering.html#mixture-models"><i class="fa fa-check"></i><b>3.4</b> Mixture models</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-clustering.html"><a href="3-clustering.html#finite-mixture-model"><i class="fa fa-check"></i><b>3.4.1</b> Finite mixture model</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-clustering.html"><a href="3-clustering.html#total-variance-and-variation-of-mixture-model"><i class="fa fa-check"></i><b>3.4.2</b> Total variance and variation of mixture model</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-clustering.html"><a href="3-clustering.html#example-of-mixtures"><i class="fa fa-check"></i><b>3.4.3</b> Example of mixtures</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-clustering.html"><a href="3-clustering.html#generative-view-sampling-from-a-mixture-model"><i class="fa fa-check"></i><b>3.4.4</b> Generative view: sampling from a mixture model</a></li>
<li class="chapter" data-level="3.4.5" data-path="3-clustering.html"><a href="3-clustering.html#predicting-the-group-allocation-of-a-given-sample"><i class="fa fa-check"></i><b>3.4.5</b> Predicting the group allocation of a given sample</a></li>
<li class="chapter" data-level="3.4.6" data-path="3-clustering.html"><a href="3-clustering.html#variation-1-infinite-mixture-model"><i class="fa fa-check"></i><b>3.4.6</b> Variation 1: Infinite mixture model</a></li>
<li class="chapter" data-level="3.4.7" data-path="3-clustering.html"><a href="3-clustering.html#variation-2-semiparametric-mixture-model-with-two-classes"><i class="fa fa-check"></i><b>3.4.7</b> Variation 2: Semiparametric mixture model with two classes</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-clustering.html"><a href="3-clustering.html#fitting-mixture-models-to-data"><i class="fa fa-check"></i><b>3.5</b> Fitting mixture models to data</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-clustering.html"><a href="3-clustering.html#direct-estimation-of-mixture-model-parameters"><i class="fa fa-check"></i><b>3.5.1</b> Direct estimation of mixture model parameters</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-clustering.html"><a href="3-clustering.html#estimating-mixture-model-parameters-using-the-em-algorithm"><i class="fa fa-check"></i><b>3.5.2</b> Estimating mixture model parameters using the EM algorithm</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-clustering.html"><a href="3-clustering.html#em-algorithm-for-multivariate-normal-mixture-model"><i class="fa fa-check"></i><b>3.5.3</b> EM algorithm for multivariate normal mixture model</a></li>
<li class="chapter" data-level="3.5.4" data-path="3-clustering.html"><a href="3-clustering.html#connection-with-k-means-clustering-method"><i class="fa fa-check"></i><b>3.5.4</b> Connection with <span class="math inline">\(K\)</span>-means clustering method</a></li>
<li class="chapter" data-level="3.5.5" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-classes"><i class="fa fa-check"></i><b>3.5.5</b> Choosing the number of classes</a></li>
<li class="chapter" data-level="3.5.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-gmms-to-iris-flower-data"><i class="fa fa-check"></i><b>3.5.6</b> Application of GMMs to Iris flower data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-classification.html"><a href="4-classification.html"><i class="fa fa-check"></i><b>4</b> Supervised learning and classification</a><ul>
<li class="chapter" data-level="4.1" data-path="4-classification.html"><a href="4-classification.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-classification.html"><a href="4-classification.html#supervised-learning-vs.unsupervised-learning"><i class="fa fa-check"></i><b>4.1.1</b> Supervised learning vs.Â unsupervised learning</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-classification.html"><a href="4-classification.html#terminology"><i class="fa fa-check"></i><b>4.1.2</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-classification.html"><a href="4-classification.html#bayesian-discriminant-rule-or-bayes-classifier"><i class="fa fa-check"></i><b>4.2</b> Bayesian discriminant rule or Bayes classifier</a></li>
<li class="chapter" data-level="4.3" data-path="4-classification.html"><a href="4-classification.html#normal-bayes-classifier"><i class="fa fa-check"></i><b>4.3</b> Normal Bayes classifier</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-classification.html"><a href="4-classification.html#quadratic-discriminant-analysis-qda-and-gaussian-assumption"><i class="fa fa-check"></i><b>4.3.1</b> Quadratic discriminant analysis (QDA) and Gaussian assumption</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-classification.html"><a href="4-classification.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>4.3.2</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-classification.html"><a href="4-classification.html#diagonal-discriminant-analysis-dda"><i class="fa fa-check"></i><b>4.3.3</b> Diagonal discriminant analysis (DDA)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-classification.html"><a href="4-classification.html#the-training-step-learning-qda-lda-and-dda-classifiers-from-data"><i class="fa fa-check"></i><b>4.4</b> The training step â learning QDA, LDA and DDA classifiers from data</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-classification.html"><a href="4-classification.html#number-of-model-parameters"><i class="fa fa-check"></i><b>4.4.1</b> Number of model parameters</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-classification.html"><a href="4-classification.html#estimating-the-discriminant-predictor-function"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the discriminant / predictor function</a></li>
<li class="chapter" data-level="4.4.3" data-path="4-classification.html"><a href="4-classification.html#comparison-of-estimated-decision-boundaries-lda-vs.qda"><i class="fa fa-check"></i><b>4.4.3</b> Comparison of estimated decision boundaries: LDA vs.Â QDA</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-classification.html"><a href="4-classification.html#goodness-of-fit-and-variable-ranking"><i class="fa fa-check"></i><b>4.5</b> Goodness of fit and variable ranking</a><ul>
<li class="chapter" data-level="4.5.1" data-path="4-classification.html"><a href="4-classification.html#lda-with-k2-classes"><i class="fa fa-check"></i><b>4.5.1</b> LDA with <span class="math inline">\(K=2\)</span> classes</a></li>
<li class="chapter" data-level="4.5.2" data-path="4-classification.html"><a href="4-classification.html#multiple-classes"><i class="fa fa-check"></i><b>4.5.2</b> Multiple classes</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4-classification.html"><a href="4-classification.html#variable-selection-and-cross-validation"><i class="fa fa-check"></i><b>4.6</b> Variable selection and cross-validation</a><ul>
<li class="chapter" data-level="4.6.1" data-path="4-classification.html"><a href="4-classification.html#choosing-a-threshold-by-multiple-testing-using-false-discovery-rates"><i class="fa fa-check"></i><b>4.6.1</b> Choosing a threshold by multiple testing using false discovery rates</a></li>
<li class="chapter" data-level="4.6.2" data-path="4-classification.html"><a href="4-classification.html#quantifying-prediction-error"><i class="fa fa-check"></i><b>4.6.2</b> Quantifying prediction error</a></li>
<li class="chapter" data-level="4.6.3" data-path="4-classification.html"><a href="4-classification.html#estimation-of-prediction-error-without-validation-data-using-cross-validation"><i class="fa fa-check"></i><b>4.6.3</b> Estimation of prediction error without validation data using cross-validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-dependence.html"><a href="5-dependence.html"><i class="fa fa-check"></i><b>5</b> Multivariate dependencies</a><ul>
<li class="chapter" data-level="5.1" data-path="5-dependence.html"><a href="5-dependence.html#measuring-the-linear-association-between-two-sets-of-random-variables"><i class="fa fa-check"></i><b>5.1</b> Measuring the linear association between two sets of random variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-dependence.html"><a href="5-dependence.html#outline"><i class="fa fa-check"></i><b>5.1.1</b> Outline</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-dependence.html"><a href="5-dependence.html#special-cases"><i class="fa fa-check"></i><b>5.1.2</b> Special cases</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-dependence.html"><a href="5-dependence.html#rozeboom-vector-correlation"><i class="fa fa-check"></i><b>5.1.3</b> Rozeboom vector correlation</a></li>
<li class="chapter" data-level="5.1.4" data-path="5-dependence.html"><a href="5-dependence.html#rv-coefficient"><i class="fa fa-check"></i><b>5.1.4</b> RV coefficient</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-as-generalisation-of-correlation"><i class="fa fa-check"></i><b>5.2</b> Mutual information as generalisation of correlation</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-dependence.html"><a href="5-dependence.html#definition-of-mutual-information"><i class="fa fa-check"></i><b>5.2.1</b> Definition of mutual information</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normal-variables"><i class="fa fa-check"></i><b>5.2.2</b> Mutual information between two normal variables</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normally-distributed-random-vectors"><i class="fa fa-check"></i><b>5.2.3</b> Mutual information between two normally distributed random vectors</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-dependence.html"><a href="5-dependence.html#using-mi-for-variable-selection"><i class="fa fa-check"></i><b>5.2.4</b> Using MI for variable selection</a></li>
<li class="chapter" data-level="5.2.5" data-path="5-dependence.html"><a href="5-dependence.html#other-measures-of-general-dependence"><i class="fa fa-check"></i><b>5.2.5</b> Other measures of general dependence</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-dependence.html"><a href="5-dependence.html#graphical-models"><i class="fa fa-check"></i><b>5.3</b> Graphical models</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-dependence.html"><a href="5-dependence.html#purpose"><i class="fa fa-check"></i><b>5.3.1</b> Purpose</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-dependence.html"><a href="5-dependence.html#basic-notions-from-graph-theory"><i class="fa fa-check"></i><b>5.3.2</b> Basic notions from graph theory</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-dependence.html"><a href="5-dependence.html#probabilistic-graphical-models"><i class="fa fa-check"></i><b>5.3.3</b> Probabilistic graphical models</a></li>
<li class="chapter" data-level="5.3.4" data-path="5-dependence.html"><a href="5-dependence.html#directed-graphical-models"><i class="fa fa-check"></i><b>5.3.4</b> Directed graphical models</a></li>
<li class="chapter" data-level="5.3.5" data-path="5-dependence.html"><a href="5-dependence.html#undirected-graphical-models"><i class="fa fa-check"></i><b>5.3.5</b> Undirected graphical models</a></li>
<li class="chapter" data-level="5.3.6" data-path="5-dependence.html"><a href="5-dependence.html#algorithm-for-learning-ggms"><i class="fa fa-check"></i><b>5.3.6</b> Algorithm for learning GGMs</a></li>
<li class="chapter" data-level="5.3.7" data-path="5-dependence.html"><a href="5-dependence.html#example-exam-score-data-mardia-et-al-1979"><i class="fa fa-check"></i><b>5.3.7</b> Example: exam score data (Mardia et al 1979:)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-nonlinear.html"><a href="6-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Nonlinear and nonparametric models</a><ul>
<li class="chapter" data-level="6.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#limits-of-linear-models-and-correlation"><i class="fa fa-check"></i><b>6.1</b> Limits of linear models and correlation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#correlation-only-measures-linear-dependence"><i class="fa fa-check"></i><b>6.1.1</b> Correlation only measures linear dependence</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#anscombe-data-sets"><i class="fa fa-check"></i><b>6.1.2</b> Anscombe data sets</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#alternatives"><i class="fa fa-check"></i><b>6.1.3</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests"><i class="fa fa-check"></i><b>6.2</b> Random forests</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#stochastic-vs.algorithmic-models"><i class="fa fa-check"></i><b>6.2.1</b> Stochastic vs.Â algorithmic models</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests-1"><i class="fa fa-check"></i><b>6.2.2</b> Random forests</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#comparison-of-decision-boundaries-decision-tree-vs.random-forest"><i class="fa fa-check"></i><b>6.2.3</b> Comparison of decision boundaries: decision tree vs.Â random forest</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gaussian-processes"><i class="fa fa-check"></i><b>6.3</b> Gaussian processes</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#main-concepts"><i class="fa fa-check"></i><b>6.3.1</b> Main concepts</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#conditional-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.3.2</b> Conditional multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#covariance-functions-and-kernels"><i class="fa fa-check"></i><b>6.3.3</b> Covariance functions and kernels</a></li>
<li class="chapter" data-level="6.3.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gp-model"><i class="fa fa-check"></i><b>6.3.4</b> GP model</a></li>
<li class="chapter" data-level="6.3.5" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gaussian-process-example"><i class="fa fa-check"></i><b>6.3.5</b> Gaussian process example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks"><i class="fa fa-check"></i><b>6.4</b> Neural networks</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#history"><i class="fa fa-check"></i><b>6.4.1</b> History</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks-1"><i class="fa fa-check"></i><b>6.4.2</b> Neural networks</a></li>
<li class="chapter" data-level="6.4.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#learning-more-about-deep-learning"><i class="fa fa-check"></i><b>6.4.3</b> Learning more about deep learning</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="7-matrices.html"><a href="7-matrices.html"><i class="fa fa-check"></i><b>A</b> Brief refresher on matrices</a><ul>
<li class="chapter" data-level="A.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-basics"><i class="fa fa-check"></i><b>A.1</b> Matrix basics</a><ul>
<li class="chapter" data-level="A.1.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-notation"><i class="fa fa-check"></i><b>A.1.1</b> Matrix notation</a></li>
<li class="chapter" data-level="A.1.2" data-path="7-matrices.html"><a href="7-matrices.html#random-matrix"><i class="fa fa-check"></i><b>A.1.2</b> Random matrix</a></li>
<li class="chapter" data-level="A.1.3" data-path="7-matrices.html"><a href="7-matrices.html#special-matrices"><i class="fa fa-check"></i><b>A.1.3</b> Special matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="7-matrices.html"><a href="7-matrices.html#simple-matrix-operations"><i class="fa fa-check"></i><b>A.2</b> Simple matrix operations</a><ul>
<li class="chapter" data-level="A.2.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-addition-and-multiplication"><i class="fa fa-check"></i><b>A.2.1</b> Matrix addition and multiplication</a></li>
<li class="chapter" data-level="A.2.2" data-path="7-matrices.html"><a href="7-matrices.html#matrix-transpose"><i class="fa fa-check"></i><b>A.2.2</b> Matrix transpose</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="7-matrices.html"><a href="7-matrices.html#matrix-summaries"><i class="fa fa-check"></i><b>A.3</b> Matrix summaries</a><ul>
<li class="chapter" data-level="A.3.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-trace"><i class="fa fa-check"></i><b>A.3.1</b> Matrix trace</a></li>
<li class="chapter" data-level="A.3.2" data-path="7-matrices.html"><a href="7-matrices.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>A.3.2</b> Determinant of a matrix</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="7-matrices.html"><a href="7-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.4</b> Matrix inverse</a><ul>
<li class="chapter" data-level="A.4.1" data-path="7-matrices.html"><a href="7-matrices.html#inversion-of-square-matrix"><i class="fa fa-check"></i><b>A.4.1</b> Inversion of square matrix</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.5</b> Orthogonal matrices</a><ul>
<li class="chapter" data-level="A.5.1" data-path="7-matrices.html"><a href="7-matrices.html#properties-1"><i class="fa fa-check"></i><b>A.5.1</b> Properties</a></li>
<li class="chapter" data-level="A.5.2" data-path="7-matrices.html"><a href="7-matrices.html#generating-orthogonal-matrices"><i class="fa fa-check"></i><b>A.5.2</b> Generating orthogonal matrices</a></li>
<li class="chapter" data-level="A.5.3" data-path="7-matrices.html"><a href="7-matrices.html#permutation-matrix"><i class="fa fa-check"></i><b>A.5.3</b> Permutation matrix</a></li>
</ul></li>
<li class="chapter" data-level="A.6" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>A.6</b> Eigenvalues and eigenvectors</a><ul>
<li class="chapter" data-level="A.6.1" data-path="7-matrices.html"><a href="7-matrices.html#definition"><i class="fa fa-check"></i><b>A.6.1</b> Definition</a></li>
<li class="chapter" data-level="A.6.2" data-path="7-matrices.html"><a href="7-matrices.html#finding-eigenvalues-and-vectors"><i class="fa fa-check"></i><b>A.6.2</b> Finding eigenvalues and vectors</a></li>
<li class="chapter" data-level="A.6.3" data-path="7-matrices.html"><a href="7-matrices.html#eigenequation-in-matrix-notation"><i class="fa fa-check"></i><b>A.6.3</b> Eigenequation in matrix notation</a></li>
<li class="chapter" data-level="A.6.4" data-path="7-matrices.html"><a href="7-matrices.html#defective-matrix"><i class="fa fa-check"></i><b>A.6.4</b> Defective matrix</a></li>
<li class="chapter" data-level="A.6.5" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-of-a-diagonal-or-triangular-matrix"><i class="fa fa-check"></i><b>A.6.5</b> Eigenvalues of a diagonal or triangular matrix</a></li>
<li class="chapter" data-level="A.6.6" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-vectors-of-a-symmetric-matrix"><i class="fa fa-check"></i><b>A.6.6</b> Eigenvalues and vectors of a symmetric matrix</a></li>
<li class="chapter" data-level="A.6.7" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-of-orthogonal-matrices"><i class="fa fa-check"></i><b>A.6.7</b> Eigenvalues of orthogonal matrices</a></li>
<li class="chapter" data-level="A.6.8" data-path="7-matrices.html"><a href="7-matrices.html#positive-definite-matrices"><i class="fa fa-check"></i><b>A.6.8</b> Positive definite matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.7" data-path="7-matrices.html"><a href="7-matrices.html#matrix-decompositions"><i class="fa fa-check"></i><b>A.7</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="A.7.1" data-path="7-matrices.html"><a href="7-matrices.html#diagonalisation-and-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.7.1</b> Diagonalisation and eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.7.2" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.7.2</b> Orthogonal eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.7.3" data-path="7-matrices.html"><a href="7-matrices.html#singular-value-decomposition"><i class="fa fa-check"></i><b>A.7.3</b> Singular value decomposition</a></li>
<li class="chapter" data-level="A.7.4" data-path="7-matrices.html"><a href="7-matrices.html#polar-decomposition"><i class="fa fa-check"></i><b>A.7.4</b> Polar decomposition</a></li>
<li class="chapter" data-level="A.7.5" data-path="7-matrices.html"><a href="7-matrices.html#cholesky-decomposition"><i class="fa fa-check"></i><b>A.7.5</b> Cholesky decomposition</a></li>
</ul></li>
<li class="chapter" data-level="A.8" data-path="7-matrices.html"><a href="7-matrices.html#matrix-summaries-based-on-eigenvalues-and-singular-values"><i class="fa fa-check"></i><b>A.8</b> Matrix summaries based on eigenvalues and singular values</a><ul>
<li class="chapter" data-level="A.8.1" data-path="7-matrices.html"><a href="7-matrices.html#trace-and-determinant-computed-from-eigenvalues"><i class="fa fa-check"></i><b>A.8.1</b> Trace and determinant computed from eigenvalues</a></li>
<li class="chapter" data-level="A.8.2" data-path="7-matrices.html"><a href="7-matrices.html#rank-and-condition-number"><i class="fa fa-check"></i><b>A.8.2</b> Rank and condition number</a></li>
</ul></li>
<li class="chapter" data-level="A.9" data-path="7-matrices.html"><a href="7-matrices.html#functions-of-symmetric-matrices"><i class="fa fa-check"></i><b>A.9</b> Functions of symmetric matrices</a><ul>
<li class="chapter" data-level="A.9.1" data-path="7-matrices.html"><a href="7-matrices.html#definition-of-a-matrix-function"><i class="fa fa-check"></i><b>A.9.1</b> Definition of a matrix function</a></li>
<li class="chapter" data-level="A.9.2" data-path="7-matrices.html"><a href="7-matrices.html#identities-for-the-matrix-exponential-and-logarithm"><i class="fa fa-check"></i><b>A.9.2</b> Identities for the matrix exponential and logarithm</a></li>
</ul></li>
<li class="chapter" data-level="A.10" data-path="7-matrices.html"><a href="7-matrices.html#matrix-calculus"><i class="fa fa-check"></i><b>A.10</b> Matrix calculus</a><ul>
<li class="chapter" data-level="A.10.1" data-path="7-matrices.html"><a href="7-matrices.html#first-order-vector-derivatives"><i class="fa fa-check"></i><b>A.10.1</b> First order vector derivatives</a></li>
<li class="chapter" data-level="A.10.2" data-path="7-matrices.html"><a href="7-matrices.html#second-order-vector-derivatives"><i class="fa fa-check"></i><b>A.10.2</b> Second order vector derivatives</a></li>
<li class="chapter" data-level="A.10.3" data-path="7-matrices.html"><a href="7-matrices.html#first-order-matrix-derivatives"><i class="fa fa-check"></i><b>A.10.3</b> First order matrix derivatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="8-further-study.html"><a href="8-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="8-further-study.html"><a href="8-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="8-further-study.html"><a href="8-further-study.html#advanced-reading"><i class="fa fa-check"></i><b>B.2</b> Advanced reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="9-references.html"><a href="9-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Multivariate Statistics and Machine Learning</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="brief-refresher-on-matrices" class="section level1">
<h1><span class="header-section-number">A</span> Brief refresher on matrices</h1>
<p>This is intended a very short recap of some essentials you need to know about matrices.
We will frequently make use of matrix calculations.
Matrix notation helps to make multivariate equations simpler and to understand
them better.</p>
<p>For more details please consult the lecture notes of earlier modules (e.g.Â linear algebra).</p>
<p>In this course we mostly work with <strong>real matrices</strong>, i.e.Â we assume all matrix elements are
real numbers. However, one important matrix decomposition â the eigenvalues decomposition â can yield complex-valued matrices when applied to real matrices. Below we will point out when this is the case.</p>
<div id="matrix-basics" class="section level2">
<h2><span class="header-section-number">A.1</span> Matrix basics</h2>
<div id="matrix-notation" class="section level3">
<h3><span class="header-section-number">A.1.1</span> Matrix notation</h3>
<p>In matrix notation we distinguish between scalars, vectors, and matrices:</p>
<p><strong>Scalar</strong>: <span class="math inline">\(x\)</span>, <span class="math inline">\(X\)</span>, lower or upper case, plain type.</p>
<p><strong>Vector</strong>: <span class="math inline">\(\boldsymbol x\)</span>, lower case, bold type. In handwriting an arrow <span class="math inline">\(\vec{x}\)</span> indicates a vector.</p>
<p>In component notation we write <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_d)^T = (x_i)^T\)</span>. By convention, a vector is a
column vector, i.e.Â the elements are arranged in a column and the index (here <span class="math inline">\(i\)</span>) refers to the row
of the column. If you transpose a column vector it becomes a row vector
<span class="math inline">\(\boldsymbol x^T = (x_1, \ldots, x_d) =(x_i)\)</span> and the index now refers to the column.</p>
<p><strong>Matrix</strong>: <span class="math inline">\(\boldsymbol X\)</span>, upper case, bold type. In handwriting an underscore
<span class="math inline">\(\underline{X}\)</span> indicates a matrix.</p>
<p>In component notation we write <span class="math inline">\(\boldsymbol X= (x_{ij})\)</span>. By convention, the first index (here <span class="math inline">\(i\)</span>)
of the scalar elements <span class="math inline">\(x_{ij}\)</span> denotes the row and the second index (here <span class="math inline">\(j\)</span>) the column of the matrix.
Assuming that <span class="math inline">\(n\)</span> is the number of rows and <span class="math inline">\(d\)</span> is the number of columns
you can also view the matrix <span class="math inline">\(\boldsymbol X= (\boldsymbol x_j) = (\boldsymbol z_i)^T\)</span> as being composed of column vectors
<span class="math inline">\(\boldsymbol x_j = (x_{1j}, \ldots, x_{nj})^T\)</span>
or of row vectors <span class="math inline">\(\boldsymbol z_i^T = (x_{i1}, \ldots, x_{id})\)</span>.</p>
<p>A (column) vector is a matrix of size <span class="math inline">\(d\times 1\)</span>. A row vector is a matrix of size <span class="math inline">\(1\times d\)</span>.
A scalar is a matrix of size <span class="math inline">\(1 \times 1\)</span>.</p>
</div>
<div id="random-matrix" class="section level3">
<h3><span class="header-section-number">A.1.2</span> Random matrix</h3>
<p>A <strong>random matrix</strong> (vector) is a matrix (vector) whose elements are random variables.</p>
<p>Note that the standard
notation used in univariate statistics to distinguish
random variables and their realisations (i.e.Â upper versus lower case) does not
work in multivariate statistics. Therefore, you need to determine
from the context whether a quantity represents a
random variable, or whether it is a constant.</p>
</div>
<div id="special-matrices" class="section level3">
<h3><span class="header-section-number">A.1.3</span> Special matrices</h3>
<p><span class="math inline">\(\boldsymbol I_d\)</span> is the identity matrix. It is a square matrix of size
<span class="math inline">\(d \times d\)</span> with the diagonal
filled with 1 and off-diagonals filled with 0.
<span class="math display">\[\boldsymbol I_d =
\begin{pmatrix}
    1 &amp; 0 &amp; 0 &amp; \dots &amp; 0\\
    0 &amp; 1 &amp; 0 &amp; \dots &amp; 0\\
    0 &amp; 0 &amp; 1 &amp;   &amp; 0\\
    \vdots &amp; \vdots &amp; &amp; \ddots &amp;  \\
    0 &amp; 0 &amp; 0 &amp;  &amp; 1 \\
\end{pmatrix}\]</span></p>
<p><span class="math inline">\(\boldsymbol 1\)</span> is a matrix that contains only 1s. Most often
it is used in the form of a column vector with <span class="math inline">\(d\)</span> rows:
<span class="math display">\[\boldsymbol 1_d =
\begin{pmatrix}
    1 \\
    1 \\
    1 \\
    \vdots   \\
    1  \\
\end{pmatrix}\]</span></p>
<p>A diagonal matrix is a matrix where all off-diagonal elements are zero.</p>
<p>A triangular matrix is a square matrix whose elements either below or above the diagonal are all zero (upper vs.Â lower triangular matrix).</p>
</div>
</div>
<div id="simple-matrix-operations" class="section level2">
<h2><span class="header-section-number">A.2</span> Simple matrix operations</h2>
<div id="matrix-addition-and-multiplication" class="section level3">
<h3><span class="header-section-number">A.2.1</span> Matrix addition and multiplication</h3>
<p>Matrices behave much like common numbers. For example, there exist
matrix addition <span class="math inline">\(\boldsymbol C= \boldsymbol A+ \boldsymbol B\)</span>
and matrix multiplication <span class="math inline">\(\boldsymbol C= \boldsymbol A\boldsymbol B\)</span>.</p>
<p>Matrix addition is simply the result of the addition of the corresponding elements in <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span>,
i.e <span class="math inline">\(c_{ij} = a_{ij} + b_{ij}\)</span>. For matrix addition <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> must have the same size,
i.e.Â the same number of rows and columns.</p>
<p>The dot product between two vectors is <span class="math inline">\(\boldsymbol a\cdot \boldsymbol b= \boldsymbol a^T \boldsymbol b= \boldsymbol a\boldsymbol b^T = \sum_{i=1}^d a_{i} b_{i}\)</span>.</p>
<p>Matrix multiplication is defined as <span class="math inline">\(c_{ij} = \sum_{k=1}^m a_{ik} b_{kj}\)</span> where <span class="math inline">\(m\)</span> is the
number of columns of <span class="math inline">\(\boldsymbol A\)</span> and the number of rows in <span class="math inline">\(\boldsymbol B\)</span>. Thus, <span class="math inline">\(\boldsymbol C\)</span> contains all possible
dot products of the row vectors in <span class="math inline">\(\boldsymbol A\)</span> with the column vectors in <span class="math inline">\(\boldsymbol B\)</span>.
For matrix multiplication the number of columns in <span class="math inline">\(\boldsymbol A\)</span> must match the number of rows in <span class="math inline">\(\boldsymbol B\)</span>.
Note that matrix multiplication is in general not commutative, i.e. <span class="math inline">\(\boldsymbol A\boldsymbol B\neq \boldsymbol B\boldsymbol A\)</span>.</p>
</div>
<div id="matrix-transpose" class="section level3">
<h3><span class="header-section-number">A.2.2</span> Matrix transpose</h3>
<p>The matrix transpose <span class="math inline">\(t(\boldsymbol A) = \boldsymbol A^T\)</span> interchanges rows and columns. The transpose
is a linear operator <span class="math inline">\((\boldsymbol A+ \boldsymbol B)^T = \boldsymbol A^T + \boldsymbol B^T\)</span> and
applied to a matrix
product it reverses the ordering, i.e. <span class="math inline">\((\boldsymbol A\boldsymbol B)^T =\boldsymbol B^T \boldsymbol A^T\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol A= \boldsymbol A^T\)</span> then <span class="math inline">\(\boldsymbol A\)</span> is symmetric (and square).</p>
<p>By construction given a rectangular <span class="math inline">\(\boldsymbol A\)</span> the matrices
<span class="math inline">\(\boldsymbol A^T \boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T\)</span> are symmetric with non-negative diagonal.</p>
</div>
</div>
<div id="matrix-summaries" class="section level2">
<h2><span class="header-section-number">A.3</span> Matrix summaries</h2>
<div id="matrix-trace" class="section level3">
<h3><span class="header-section-number">A.3.1</span> Matrix trace</h3>
<p>The trace of the matrix is the sum of the diagonal elements <span class="math inline">\(\text{Tr}(\boldsymbol A) = \sum a_{ii}\)</span>.</p>
<p>A useful identity for the matrix trace is
<span class="math display">\[
\text{Tr}(\boldsymbol A\boldsymbol B) = \text{Tr}( \boldsymbol B\boldsymbol A)  
\]</span>
with for two vectors becomes
<span class="math display">\[
\boldsymbol a^T \boldsymbol b= \text{Tr}( \boldsymbol b\boldsymbol a^T) \,.
\]</span></p>
<p>The squared Frobenius norm, i.e.Â the sum of the squares of all entries of a rectangular matrix <span class="math inline">\(\boldsymbol A=(a_{ij})\)</span>, can be
written using the trace as follows:<br />
<span class="math display">\[
\begin{split}
||\boldsymbol A||_F^2 &amp;= \sum_{i,j} a_{ij}^2 \\
 &amp;= \text{Tr}(\boldsymbol A^T \boldsymbol A) = \text{Tr}(\boldsymbol A\boldsymbol A^T) \,.
\end{split}
\]</span></p>
</div>
<div id="determinant-of-a-matrix" class="section level3">
<h3><span class="header-section-number">A.3.2</span> Determinant of a matrix</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is a square matrix the determinant <span class="math inline">\(\det(\boldsymbol A)\)</span> is a scalar measuring the volume spanned by the column vectors in <span class="math inline">\(\boldsymbol A\)</span> with the sign determined by the orientation of the vectors.</p>
<p>If <span class="math inline">\(\det(\boldsymbol A) \neq 0\)</span> the matrix <span class="math inline">\(\boldsymbol A\)</span> is non-singular or non-degenerate. Conversely, if
<span class="math inline">\(\det(\boldsymbol A) =0\)</span> the matrix <span class="math inline">\(\boldsymbol A\)</span> is singular or degenerate.</p>
<p>One way to compute the determinant of a matrix <span class="math inline">\(\boldsymbol A\)</span> is the Laplace cofactor
expansion approach that proceeds recursively based on the determinants of the submatrices <span class="math inline">\(\boldsymbol A_{-i,-j}\)</span> obtained by deleting row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> from <span class="math inline">\(\boldsymbol A\)</span>. Specifically, at each
level we compute the</p>
<ol style="list-style-type: decimal">
<li>cofactor expansion either
<ol style="list-style-type: lower-alpha">
<li>along the <span class="math inline">\(i\)</span>-th row â pick any row <span class="math inline">\(i\)</span>:
<span class="math display">\[\det(\boldsymbol A) = \sum_{j=1}^d a_{ij} (-1)^{i+j} \det(\boldsymbol A_{-i,-j})  \text{ , or}\]</span></li>
<li>along the <span class="math inline">\(j\)</span>-th column â pick any <span class="math inline">\(j\)</span>:
<span class="math display">\[\det(\boldsymbol A) = \sum_{i=1}^d a_{ij} (-1)^{i+j} \det(\boldsymbol A_{-i,-j})\]</span>.</li>
</ol></li>
<li>Then repeat until the submatrix is a scalar <span class="math inline">\(a\)</span> and <span class="math inline">\(\det(a)=a \,.\)</span></li>
</ol>
<p>The recursive nature of this algorithm leads to a complexity of order <span class="math inline">\(O(d!)\)</span> so it is not practical except for very small <span class="math inline">\(d\)</span>.
Therefore, in practice other more efficient algorithms for computing determinants are used but these still have algorithmic complexity in the order of <span class="math inline">\(O(d^3)\)</span> so for large dimensions obtaining determinants is
very expensive.</p>
<p>However, some specially structured matrices do allow for very fast calculation.
In particular, it turns out that the determinant of a triangular matrix (which includes diagonal matrices)
is simply the product of the diagonal elements.</p>
<p>For a two-dimensional matrix <span class="math inline">\(\boldsymbol A= \begin{pmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \\\end{pmatrix}\)</span>
the determinant is <span class="math inline">\(\det(A) = a_{11} a_{22} - a_{12} a_{21}\)</span>.</p>
<p>The determinant of a block-structured matrix
<span class="math display">\[
\boldsymbol A= \begin{pmatrix} \boldsymbol A_{11} &amp; \boldsymbol A_{12} \\ \boldsymbol A_{21} &amp; \boldsymbol A_{22} \\ \end{pmatrix}
\]</span>
is
<span class="math display">\[
\det(\boldsymbol A) = \det(\boldsymbol A_{22}) \det(\boldsymbol C_1) = \det(\boldsymbol A_{11}) \det(\boldsymbol C_2) 
\]</span>
with (Schur complement of <span class="math inline">\(\boldsymbol A_{22}\)</span>)
<span class="math display">\[
\boldsymbol C_1 = \boldsymbol A_{11} -  \boldsymbol A_{12}  \boldsymbol A_{22}^{-1}  \boldsymbol A_{21} 
\]</span>
and (Schur complement of <span class="math inline">\(\boldsymbol A_{11}\)</span>)
<span class="math display">\[
\boldsymbol C_2 = \boldsymbol A_{22} -  \boldsymbol A_{21}  \boldsymbol A_{11}^{-1}  \boldsymbol A_{12} 
\]</span></p>
<p>For a block-diagonal matrix <span class="math inline">\(\boldsymbol A\)</span> with <span class="math inline">\(\boldsymbol A_{12} = 0\)</span> and <span class="math inline">\(\boldsymbol A_{21} = 0\)</span>
the determinant is <span class="math inline">\(\det(\boldsymbol A) = \det(\boldsymbol A_{11}) \det(\boldsymbol A_{22})\)</span>.</p>
<p>Determinants have a multiplicative property,
<span class="math display">\[\det(\boldsymbol A\boldsymbol B) = \det(\boldsymbol B\boldsymbol A) = \det(\boldsymbol A) \det(\boldsymbol B) \,.\]</span>
For scalar <span class="math inline">\(a\)</span> this becomes
<span class="math inline">\(\det(a \boldsymbol B) = a^d \det(\boldsymbol B)\)</span> where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(\boldsymbol B\)</span>.</p>
<p>Another important identity is
<span class="math display">\[\det(\boldsymbol I_n + \boldsymbol A\boldsymbol B) = \det(\boldsymbol I_m + \boldsymbol B\boldsymbol A)\]</span>
where <span class="math inline">\(\boldsymbol A\)</span> is a <span class="math inline">\(n \times m\)</span> and <span class="math inline">\(\boldsymbol B\)</span> is a <span class="math inline">\(m \times n\)</span> matrix. This is called the
Weinstein-Aronszajn determinant identity (also credited to Sylvester).</p>
</div>
</div>
<div id="matrix-inverse" class="section level2">
<h2><span class="header-section-number">A.4</span> Matrix inverse</h2>
<div id="inversion-of-square-matrix" class="section level3">
<h3><span class="header-section-number">A.4.1</span> Inversion of square matrix</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is a square matrix then the inverse matrix <span class="math inline">\(\boldsymbol A^{-1}\)</span> is a matrix
such that
<span class="math display">\[\boldsymbol A^{-1} \boldsymbol A= \boldsymbol A\boldsymbol A^{-1}=  \boldsymbol I\, .\]</span>
Only non-singular matrices with <span class="math inline">\(\det(\boldsymbol A) \neq 0\)</span> are invertible.</p>
<p>As <span class="math inline">\(\det(\boldsymbol A^{-1} \boldsymbol A) = \det(\boldsymbol I) = 1\)</span> the
determinant of the inverse matrix equals
the inverse determinant,
<span class="math display">\[\det(\boldsymbol A^{-1}) = \det(\boldsymbol A)^{-1} \,.\]</span></p>
<p>The transpose of the inverse is the inverse of the transpose
as
<span class="math display">\[
\begin{split}
(\boldsymbol A^{-1})^T &amp;= (\boldsymbol A^{-1})^T \,  \boldsymbol A^T (\boldsymbol A^{T})^{-1}   \\
 &amp;= (\boldsymbol A\boldsymbol A^{-1})^T \, (\boldsymbol A^{T})^{-1} = (\boldsymbol A^{T})^{-1} \,. \\
\end{split}
\]</span></p>
<p>The inverse of a matrix product <span class="math inline">\((\boldsymbol A\boldsymbol B)^{-1} = \boldsymbol B^{-1} \boldsymbol A^{-1}\)</span>
is the product of the indivdual matrix inverses in reverse order.</p>
<p>There are many different algorithms to compute the inverse of a matrix
(which is essentially a problem of solving a system of equations).
The computational complexity of matrix inversion is of the order <span class="math inline">\(O(d^3)\)</span>
where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(\boldsymbol A\)</span>. Therefore matrix inversion is very costly in higher dimensions.</p>
<p>However, for specially structured matrices inversion can be done effectively regardless of dimension.
For example, the inverse of a diagonal matrix is another diagonal matrix obtained by inverting the diagonal elements.
Another example are orthogonal matrices.</p>
</div>
</div>
<div id="orthogonal-matrices" class="section level2">
<h2><span class="header-section-number">A.5</span> Orthogonal matrices</h2>
<div id="properties-1" class="section level3">
<h3><span class="header-section-number">A.5.1</span> Properties</h3>
<p>An orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> is a square matrix with the property that <span class="math inline">\(\boldsymbol Q^T = \boldsymbol Q^{-1}\)</span>,
i.e.Â the transpose is also the inverse. This implies that <span class="math inline">\(\boldsymbol Q\boldsymbol Q^T = \boldsymbol Q^T \boldsymbol Q= \boldsymbol I\)</span>.</p>
<p>The identity matrix <span class="math inline">\(\boldsymbol I\)</span> is the simplest example of an orthogonal matrix.</p>
<p>An orthogonal matrix
<span class="math inline">\(\boldsymbol Q\)</span> can be interpreted geometrically as an operator performing
rotation, reflection and/or permutation.
Multiplication of <span class="math inline">\(\boldsymbol Q\)</span> with a vector will result in
a new vector of the same length but with a change in direction (unless <span class="math inline">\(\boldsymbol Q=\boldsymbol I\)</span>).</p>
<p>The product <span class="math inline">\(\boldsymbol Q_3 = \boldsymbol Q_1 \boldsymbol Q_2\)</span> of two orthogonal matrices <span class="math inline">\(\boldsymbol Q_1\)</span> and <span class="math inline">\(\boldsymbol Q_2\)</span> yields another orthogonal matrix as <span class="math inline">\(\boldsymbol Q_3 \boldsymbol Q_3^T = \boldsymbol Q_1 \boldsymbol Q_2 (\boldsymbol Q_1 \boldsymbol Q_2)^T = \boldsymbol Q_1 \boldsymbol Q_2 \boldsymbol Q_2^T \boldsymbol Q_1^T = \boldsymbol I\)</span>.</p>
<p>The determinant <span class="math inline">\(\det(\boldsymbol Q)\)</span> of an orthogonal matrix is either +1 or -1,
because <span class="math inline">\(\boldsymbol Q\boldsymbol Q^T = \boldsymbol I\)</span> and thus <span class="math inline">\(\det(\boldsymbol Q)\det(\boldsymbol Q^T) = \det(\boldsymbol Q)^2 = \det(\boldsymbol I) = 1\)</span>.</p>
<p>The set of all orthogonal matrices of dimension <span class="math inline">\(d\)</span> together with multiplication
form a group called the orthogonal group <span class="math inline">\(O(d)\)</span>.
The subset of orthogonal matrices with <span class="math inline">\(\det(\boldsymbol Q)=1\)</span> are called rotation matrices and form with multiplication the special orthogonal group <span class="math inline">\(SO(d)\)</span>.
Orthogonal matrices with <span class="math inline">\(\det(\boldsymbol Q)=-1\)</span> are rotation-reflection matrices.</p>
</div>
<div id="generating-orthogonal-matrices" class="section level3">
<h3><span class="header-section-number">A.5.2</span> Generating orthogonal matrices</h3>
<p>In two dimensions <span class="math inline">\((d=2)\)</span> all orthogonal matrices <span class="math inline">\(\boldsymbol R\)</span> representing rotations with <span class="math inline">\(\det(\boldsymbol R)=1\)</span> are
given by
<span class="math display">\[
\boldsymbol R(\theta) = 
\begin{pmatrix}
\cos \theta &amp; -\sin \theta \\
\sin \theta &amp; \cos \theta 
\end{pmatrix}
\]</span>
and those representing rotation-reflections <span class="math inline">\(\boldsymbol G\)</span> with <span class="math inline">\(\det(\boldsymbol G)=-1\)</span> by
<span class="math display">\[
\boldsymbol G(\theta) = 
\begin{pmatrix}
\cos \theta &amp; \sin \theta \\
\sin \theta &amp; -\cos \theta
\end{pmatrix}\,.
\]</span>
Every orthogonal matrix of dimension <span class="math inline">\(d=2\)</span>
can be represented as the product of at most two rotation-reflection
matrices because
<span class="math display">\[
\boldsymbol R(\boldsymbol \theta) = \boldsymbol G(\theta)\, \boldsymbol G(0) =  
\begin{pmatrix}
\cos \theta &amp; \sin \theta \\
\sin \theta &amp; -\cos \theta
\end{pmatrix} 
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; -1
\end{pmatrix}\,.
\]</span>
Thus, the matrix <span class="math inline">\(\boldsymbol G\)</span> is a generator of two-dimensional orthogonal matrices.
Note that <span class="math inline">\(\boldsymbol G(\theta)\)</span> is symmetric, orthogonal and has determinant -1.</p>
<p>More generally, and applicable in arbitrary dimension, the role of generator is taken by the Householder reflection matrix
<span class="math display">\[
\boldsymbol Q_{HH}(\boldsymbol v) = \boldsymbol I- 2 \boldsymbol v\boldsymbol v^T
\]</span>
where <span class="math inline">\(\boldsymbol v\)</span> is a vector of unit length (with <span class="math inline">\(\boldsymbol v^T \boldsymbol v=1\)</span>) orthogonal to
the reflection hyperplane. Note that <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v) = \boldsymbol Q_{HH}(-\boldsymbol v)\)</span>.
By construction the matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span> is symmetric, orthogonal and has determinant -1.</p>
<p>It can be shown that any <span class="math inline">\(d\)</span>-dimensional orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> can be represented as the product of at most <span class="math inline">\(d\)</span> Householder reflection matrices.
The two-dimensional generator <span class="math inline">\(\boldsymbol G(\theta)\)</span> is recovered as the Householder matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span>
with <span class="math inline">\(\boldsymbol v= \begin{pmatrix} -\sin \frac{\theta}{2} \\ \cos \frac{\theta}{2} \end{pmatrix}\)</span>
or <span class="math inline">\(\boldsymbol v= \begin{pmatrix} \sin \frac{\theta}{2} \\ -\cos \frac{\theta}{2} \end{pmatrix}\)</span>.</p>
</div>
<div id="permutation-matrix" class="section level3">
<h3><span class="header-section-number">A.5.3</span> Permutation matrix</h3>
<p>A special type of an orthogonal matrix is a permutation matrix <span class="math inline">\(\boldsymbol P\)</span> created by
permuting rows and/or columns of the identity matrix <span class="math inline">\(\boldsymbol I\)</span>. Thus, each row and column
of <span class="math inline">\(\boldsymbol P\)</span> contains exactly one entry of 1, but not necessarily on the diagonal.</p>
<p>If a permutation matrix <span class="math inline">\(\boldsymbol P\)</span> is multiplied with a matrix <span class="math inline">\(\boldsymbol A\)</span> it acts as an operator
permuting the columns (<span class="math inline">\(\boldsymbol A\boldsymbol P\)</span>) or the rows (<span class="math inline">\(\boldsymbol P\boldsymbol A\)</span>).
For a set of <span class="math inline">\(d\)</span> elements there exist <span class="math inline">\(d!\)</span> permutations. Thus, for dimension <span class="math inline">\(d\)</span> there
are <span class="math inline">\(d!\)</span> possible permutation matrices (including the identity matrix).</p>
<p>The determinant of a permutation matrix is either +1 or -1.
The product of two permutation matrices yields another permutation matrix.</p>
<p>Symmetric permutation matrices correspond to self-inverse permutations
(i.e.Â the permutation matrix is its own inverse), and are also called permutation involutions.
They can have determinant +1 and -1.</p>
<p>A transposition is a permutation where only two elements are exchanged.
Thus, in a transposition matrix <span class="math inline">\(\boldsymbol T\)</span>
exactly two rows and/or columns are exchanged compared to identity matrix <span class="math inline">\(\boldsymbol I\)</span>.
Transpositions are self-inverse, and transposition matrices are symmetric.
There are <span class="math inline">\(\frac{d (d-1)}{2}\)</span> different transposition matrices.
The determinant of a transposition matrix is <span class="math inline">\(\det(\boldsymbol T)= -1\)</span>.</p>
<p>Note that the transposition matrix is an instance of a Householder matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span>
with vector <span class="math inline">\(\boldsymbol v\)</span> filled with zeros except for two elements that have value
<span class="math inline">\(\frac{\sqrt{2}}{2}\)</span> and <span class="math inline">\(-\frac{\sqrt{2}}{2}\)</span>.</p>
<p>Any permutation of <span class="math inline">\(d\)</span> elements can be generated by a series of at most <span class="math inline">\(d-1\)</span> transpositions.
Correspondingly, any permutation matrix <span class="math inline">\(\boldsymbol P\)</span> can be constructed by multiplication of the identity
matrix with at most <span class="math inline">\(d-1\)</span> transposition matrices. If the number of transpositions is even then <span class="math inline">\(\det(\boldsymbol P) = 1\)</span> otherwise
for an uneven number <span class="math inline">\(\det(\boldsymbol P) = -1\)</span>. This is called the sign or signature of the permutation.</p>
<p>The set of all permutations form the symmetric group <span class="math inline">\(S_d\)</span>, the subset of even permutations (with positive sign and <span class="math inline">\(\det(\boldsymbol P)=1\)</span>) the alternating group <span class="math inline">\(A_d\)</span>.</p>
</div>
</div>
<div id="eigenvalues-and-eigenvectors" class="section level2">
<h2><span class="header-section-number">A.6</span> Eigenvalues and eigenvectors</h2>
<div id="definition" class="section level3">
<h3><span class="header-section-number">A.6.1</span> Definition</h3>
<p>Assume a square symmetric matrix <span class="math inline">\(\boldsymbol A\)</span> of size <span class="math inline">\(d \times d\)</span>.
A vector <span class="math inline">\(\boldsymbol u\neq 0\)</span> is called an eigenvector of the matrix <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\lambda\)</span> the corresponding
eigenvalue if<br />
<span class="math display">\[\boldsymbol A\boldsymbol u= \boldsymbol u\lambda \, .\]</span></p>
</div>
<div id="finding-eigenvalues-and-vectors" class="section level3">
<h3><span class="header-section-number">A.6.2</span> Finding eigenvalues and vectors</h3>
<p>To find the eigenvalues and eigenvector the <em>eigenequation</em> is rewritten as
<span class="math display">\[(\boldsymbol A-\boldsymbol I\lambda ) \boldsymbol u= 0 \,.\]</span> For any solution <span class="math inline">\(\boldsymbol u\neq 0\)</span> the corresponding eigenvalue
<span class="math inline">\(\lambda\)</span> must make the matrix <span class="math inline">\(\boldsymbol A-\boldsymbol I\lambda\)</span> singular, i.e.Â the determinant must vanish
<span class="math display">\[\det(\boldsymbol A-\boldsymbol I\lambda ) =0 \,.\]</span>
This is the <em>characteristic equation</em> of the matrix <span class="math inline">\(\boldsymbol A\)</span>, and its solution yields <span class="math inline">\(d\)</span>
not necessarily distinct and also potentially complex eigenvalues <span class="math inline">\(\lambda_1, \ldots, \lambda_d\)</span>.</p>
<p>If there are complex eigenvalues, for a real matrix those eigenvalues come in conjugate pairs.
Specifically, for a complex <span class="math inline">\(\lambda_1 = r e^{i \phi}\)</span> there will also be a corresponding complex eigenvalue <span class="math inline">\(\lambda_2 = r e^{-i \phi}\)</span>.</p>
<p>Given the eigenvalues we then solve the eigenequation for the corresponding non-zero eigenvectors
<span class="math inline">\(\boldsymbol u_1, \ldots, \boldsymbol u_d\)</span>. Note that eigenvectors of real matrices can have complex components.
Also the eigenvector is only defined by the eigenequation up to a scalar.
By convention eigenvectors are therefore typically standardised to unit length but this still leaves
a sign ambiguity for real eigenvectors and implies that complex eigenvectors are defined only up to a factor with modulus 1.</p>
</div>
<div id="eigenequation-in-matrix-notation" class="section level3">
<h3><span class="header-section-number">A.6.3</span> Eigenequation in matrix notation</h3>
<p>With the matrix
<span class="math display">\[\boldsymbol U= (\boldsymbol u_1, \ldots, \boldsymbol u_d)\]</span> containing the standardised eigenvectors in the columns and the diagonal matrix
<span class="math display">\[\boldsymbol \Lambda= \begin{pmatrix}
    \lambda_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}
\end{pmatrix}\]</span>
containing the eigenvalues (typically sorted in order of magnitude) the eigenvalue equation can be written as
<span class="math display">\[\boldsymbol A\boldsymbol U= \boldsymbol U\boldsymbol \Lambda\,.\]</span></p>
<p>Note that if eigenvalues are not in order, we can alwas apply a permutation matrix <span class="math inline">\(\boldsymbol P\)</span> to sort them in order, such that <span class="math inline">\(\boldsymbol U&#39; = \boldsymbol U\boldsymbol P\)</span> reorders the eigenvectors and <span class="math inline">\(\boldsymbol \Lambda&#39; = \boldsymbol P^T \boldsymbol \Lambda\boldsymbol P\)</span>
the eigenvalues, with
<span class="math display">\[\boldsymbol A\boldsymbol U&#39; =  \boldsymbol A\boldsymbol U\boldsymbol P= \boldsymbol U\boldsymbol \Lambda\boldsymbol P=  \boldsymbol U\boldsymbol P\boldsymbol P^T \boldsymbol \Lambda\boldsymbol P=  \boldsymbol U&#39; \boldsymbol \Lambda&#39; \,.\]</span></p>
</div>
<div id="defective-matrix" class="section level3">
<h3><span class="header-section-number">A.6.4</span> Defective matrix</h3>
<p>In most cases the eigenvectors <span class="math inline">\(\boldsymbol u_i\)</span> will be linearly independent so that they form a basis to span a <span class="math inline">\(d\)</span> dimensional space.</p>
<p>However, if this is not the case and
the matrix <span class="math inline">\(\boldsymbol A\)</span> does not have a complete basis of eigenvectors, then the matrix is called defective. In this case
the matrix <span class="math inline">\(\boldsymbol U\)</span> containing the eigenvectors is singular and <span class="math inline">\(\det(\boldsymbol U)=0\)</span>.</p>
<p>An example of a defective matrix is
<span class="math inline">\(\begin{pmatrix} 1 &amp;1 \\ 0 &amp; 1 \\ \end{pmatrix}\)</span>
which has determinant 1 so that it can be inverted and its column vectors do form a complete basis
but has only one distinct eigenvector <span class="math inline">\((1,0)^T\)</span> so that the eigenvector basis is incomplete.</p>
</div>
<div id="eigenvalues-of-a-diagonal-or-triangular-matrix" class="section level3">
<h3><span class="header-section-number">A.6.5</span> Eigenvalues of a diagonal or triangular matrix</h3>
<p>In the special case that <span class="math inline">\(\boldsymbol A\)</span> is diagonal or a triangular matrix the eigenvalues are easily determined.
This follows from the simple form of their determinants as the product of the diagonal elements.
Hence for these matrices the characteristic equation becomes <span class="math inline">\(\prod_{i}^d (a_{ii} -\lambda) = 0\)</span> and has solution
<span class="math inline">\(\lambda_i=a_{ii}\)</span>, i.e.Â the eigenvalues are equal to the diagonal elements.</p>
</div>
<div id="eigenvalues-and-vectors-of-a-symmetric-matrix" class="section level3">
<h3><span class="header-section-number">A.6.6</span> Eigenvalues and vectors of a symmetric matrix</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is symmetric, i.e. <span class="math inline">\(\boldsymbol A= \boldsymbol A^T\)</span>, then its eigenvalues and eigenvectors have special properties:</p>
<ol style="list-style-type: lower-roman">
<li>all eigenvalues of <span class="math inline">\(\boldsymbol A\)</span> are real,</li>
<li>the eigenvectors are orthogonal, i.e <span class="math inline">\(\boldsymbol u_i^T \boldsymbol u_j = 0\)</span> for <span class="math inline">\(i \neq j\)</span>, and real. Thus, the matrix <span class="math inline">\(\boldsymbol U\)</span> containing the standardised orthonormal eigenvectors is orthogonal.</li>
<li><span class="math inline">\(\boldsymbol A\)</span> is never defective as <span class="math inline">\(\boldsymbol U\)</span> forms a complete basis.</li>
</ol>
</div>
<div id="eigenvalues-of-orthogonal-matrices" class="section level3">
<h3><span class="header-section-number">A.6.7</span> Eigenvalues of orthogonal matrices</h3>
<p>The eigenvalues of an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> are not necessarily real but
they all have modulus 1 and lie on the unit circle . Thus, the eigenvalues of <span class="math inline">\(\boldsymbol Q\)</span>
all have the form <span class="math inline">\(\lambda = e^{i \phi} = \cos \phi + i \sin \phi\)</span>.</p>
<p>In any real matrix complex eigenvalues come in conjugate
pairs. Hence if an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> has the complex eigenvalue <span class="math inline">\(e^{i \phi}\)</span> it also has an
complex eigenvalue <span class="math inline">\(e^{-i \phi} =\cos \phi - i \sin \phi\)</span>. The product of these two conjugate
eigenvalues is 1. Thus, an orthogonal matrix of uneven dimension has at least one
real eigenvalue (+1 or -1).</p>
<p>The eigenvalues of a Hausholder matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span> are all real (recall that it is symmetric!).
In fact, in dimension <span class="math inline">\(d\)</span> its eigenvalues are -1 (one time) and 1 ( <span class="math inline">\(d-1\)</span> times).
Since a transposition matrix <span class="math inline">\(\boldsymbol T\)</span> is a special Householder matrix they have the same eigenvalues.</p>
</div>
<div id="positive-definite-matrices" class="section level3">
<h3><span class="header-section-number">A.6.8</span> Positive definite matrices</h3>
<p>If all eigenvalues of a square matrix <span class="math inline">\(\boldsymbol A\)</span> are real and <span class="math inline">\(\lambda_i \geq 0\)</span> then <span class="math inline">\(\boldsymbol A\)</span> is called <em>positive semi-definite</em>.
If all eigenvalues are strictly positive
<span class="math inline">\(\lambda_i &gt; 0\)</span> then <span class="math inline">\(\boldsymbol A\)</span> is called <em>positive definite</em>.</p>
<p>Note that a matrix does not need to be symmetric to be positive
definite, e.g.
<span class="math inline">\(\begin{pmatrix} 2 &amp; 3 \\ 1 &amp; 4 \\ \end{pmatrix}\)</span>
has positive eigenvalues 5 and 1. It also has a complete
set of eigenvectors and is diagonisable.</p>
<p>A symmetric matrix <span class="math inline">\(\boldsymbol A\)</span> is positive definite
if the quadratic form <span class="math inline">\(\boldsymbol x^T \boldsymbol A\boldsymbol x&gt; 0\)</span> for any non-zero <span class="math inline">\(\boldsymbol x\)</span>,
and it is positive semi-definite if <span class="math inline">\(\boldsymbol x^T \boldsymbol A\boldsymbol x\geq 0\)</span>.
This holds also the other way around:
a symmetric positive definite matrix (with positive eigenvalues) has a
positive quadratic form, and a symmetric positive semi-definite matrix (with non-negative eigenvalues) a non-negative quadratic form.</p>
<p>A symmetric positive definite matrix always has a positive diagonal
(this can be seen by setting <span class="math inline">\(\boldsymbol x\)</span> above to a unit vector with 1 at
a single position, and 0 at all other elements).
However, just requiring a positive diagonal is too weak to ensure positive definiteness of a symmetric matrix, for example <span class="math inline">\(\begin{pmatrix} 1 &amp;10 \\ 10 &amp; 1 \\ \end{pmatrix}\)</span> has a negative eigenvalue of -9.
On the other hand, a symmetric matrix is indeed positive definite if it is strictly
diagonally dominant, i.e.Â if all its diagonal elements are positive and are larger than the absolute value of any of the corresponding row or column elements.
However, diagonal dominance is too restrictive as criterion to
characterise all
symmetric positive definite matrices, since
there are many symmetric matrices that are positive definite but not diagonally dominant, such as
<span class="math inline">\(\begin{pmatrix} 1 &amp; 2 \\ 2 &amp; 5 \\ \end{pmatrix}\)</span>.</p>
<p>Finally, the sum of a symmetric positive semi-definite matrix <span class="math inline">\(\boldsymbol A\)</span>
and a symmetric positive definite matrix <span class="math inline">\(\boldsymbol B\)</span> is itself symmetric positive definite because the corresponding
quadratic form <span class="math inline">\(\boldsymbol x^T ( \boldsymbol A+\boldsymbol B) \boldsymbol x= \boldsymbol x^T \boldsymbol A\boldsymbol x+ \boldsymbol x^T \boldsymbol B\boldsymbol x&gt; 0\)</span> is positive. Similarly, the sum
of two symmetric positive (semi)-definite matrices is itself symmetric positive (semi)-definite.</p>
</div>
</div>
<div id="matrix-decompositions" class="section level2">
<h2><span class="header-section-number">A.7</span> Matrix decompositions</h2>
<div id="diagonalisation-and-eigenvalue-decomposition" class="section level3">
<h3><span class="header-section-number">A.7.1</span> Diagonalisation and eigenvalue decomposition</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is a square non-defective matrix then <span class="math inline">\(\boldsymbol U\)</span> is invertible and
we can rewrite the eigenvalue equation to
<span class="math display">\[\boldsymbol A= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1} \,.\]</span>
This is called the eigendecomposition, or spectral decomposition, of <span class="math inline">\(\boldsymbol A\)</span> and equivalently
<span class="math display">\[\boldsymbol \Lambda= \boldsymbol U^{-1} \boldsymbol A\boldsymbol U\]</span>
is the diagonalisation of <span class="math inline">\(\boldsymbol A\)</span>.
Thus any matrix <span class="math inline">\(\boldsymbol A\)</span> that is not defective is diagonalisable using eigenvalue decomposition.</p>
</div>
<div id="orthogonal-eigenvalue-decomposition" class="section level3">
<h3><span class="header-section-number">A.7.2</span> Orthogonal eigenvalue decomposition</h3>
<p>For symmetric <span class="math inline">\(\boldsymbol A\)</span> this becomes
<span class="math display">\[\boldsymbol A= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\]</span>
with real eigenvalues and orthogonal matrix <span class="math inline">\(\boldsymbol U\)</span>
and
<span class="math display">\[\boldsymbol \Lambda= \boldsymbol U^T \boldsymbol A\boldsymbol U\,.\]</span>
This special case is known as the orthogonal diagonalisation
of <span class="math inline">\(\boldsymbol A\)</span>.</p>
<p>The orthogonal decomposition for symmetric <span class="math inline">\(\boldsymbol A\)</span> is
unique apart from the signs
of the eigenvectors.
In order to make it fully unique one needs to impose further restrictions (e.g.Â require a positive diagonal
of <span class="math inline">\(\boldsymbol U\)</span>). Note that this can be particularly important in computer application where the sign
can vary depending on the specific implementation of the underlying numerical algorithms.</p>
</div>
<div id="singular-value-decomposition" class="section level3">
<h3><span class="header-section-number">A.7.3</span> Singular value decomposition</h3>
<p>The <strong>singular value decomposition</strong> (SVD) is
a generalisation of the orthogonal eigenvalue decomposition
for symmetric matrices.</p>
<p>Any (!) rectangular matrix <span class="math inline">\(\boldsymbol A\)</span> of size <span class="math inline">\(n\times d\)</span> can be factored
into the product
<span class="math display">\[\boldsymbol A= \boldsymbol U\boldsymbol D\boldsymbol V^T\]</span>
where <span class="math inline">\(\boldsymbol U\)</span> is a <span class="math inline">\(n \times n\)</span> orthogonal matrix, <span class="math inline">\(\boldsymbol V\)</span> is a second <span class="math inline">\(d \times d\)</span> orthogonal matrix and <span class="math inline">\(\boldsymbol D\)</span> is a diagonal but rectangular matrix
of size <span class="math inline">\(n\times d\)</span> with <span class="math inline">\(m=min(n,d)\)</span> real diagonal elements <span class="math inline">\(d_1, \ldots d_m\)</span>. The <span class="math inline">\(d_i\)</span> are called singular values, and appear
along the diagonal in <span class="math inline">\(\boldsymbol D\)</span> by order of magnitude.</p>
<p>The SVD is unique apart from the
signs of the columns vectors in <span class="math inline">\(\boldsymbol U\)</span>, <span class="math inline">\(\boldsymbol V\)</span> and <span class="math inline">\(\boldsymbol D\)</span> (you can freely specify the column signs of any two of the
three matrices). By convention the
signs are chosen such that the singular values in <span class="math inline">\(\boldsymbol D\)</span> are all non-negative, which leaves ambiguity
in columns signs of <span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span>. Alternatively, one may
fix the columns signs of <span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span>, e.g.Â by requiring a positive diagonal, which then determines the sign of the singular values (thus allowing for negative singular values as well).</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is symmetric then the SVD and the orthogonal eigenvalue decomposition coincide (apart from different sign conventions for singular values, eigenvalues and eigenvectors).</p>
<p>Since <span class="math inline">\(\boldsymbol A^T \boldsymbol A= \boldsymbol V\boldsymbol D^T \boldsymbol D\boldsymbol V^T\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T = \boldsymbol U\boldsymbol D\boldsymbol D^T \boldsymbol U^T\)</span> the squared singular values correspond to the eigenvalues of <span class="math inline">\(\boldsymbol A^T \boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T\)</span>.
It also follows that <span class="math inline">\(\boldsymbol A^T \boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T\)</span> are both positive
semi-definite symmetric matrices, and that <span class="math inline">\(\boldsymbol V\)</span> and <span class="math inline">\(\boldsymbol U\)</span> contain the respective sets of eigenvectors.</p>
</div>
<div id="polar-decomposition" class="section level3">
<h3><span class="header-section-number">A.7.4</span> Polar decomposition</h3>
<p>Any square matrix <span class="math inline">\(\boldsymbol A\)</span> can be factored into the product
<span class="math display">\[
\boldsymbol A= \boldsymbol Q\boldsymbol B
\]</span>
of an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> and a symmetric positive semi-definite matrix <span class="math inline">\(\boldsymbol B\)</span>.</p>
<p>This follows from the SVD of <span class="math inline">\(\boldsymbol A\)</span> given as
<span class="math display">\[
\begin{split}
\boldsymbol A&amp;= \boldsymbol U\boldsymbol D\boldsymbol V^T \\
    &amp;= ( \boldsymbol U\boldsymbol V^T ) ( \boldsymbol V\boldsymbol D\boldsymbol V^T ) \\
    &amp;= \boldsymbol Q\boldsymbol B\\
\end{split}
\]</span>
with non-negative <span class="math inline">\(\boldsymbol D\)</span>. Note that this decomposition is unique as the sign ambiguities in the columns of <span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span> cancel out in <span class="math inline">\(\boldsymbol Q\)</span> and <span class="math inline">\(\boldsymbol B\)</span>.</p>
</div>
<div id="cholesky-decomposition" class="section level3">
<h3><span class="header-section-number">A.7.5</span> Cholesky decomposition</h3>
<p>A symmetric positive definite matrix <span class="math inline">\(\boldsymbol A\)</span> can be decomposed into a product
of a triangular matrix <span class="math inline">\(\boldsymbol L\)</span> with its transpose
<span class="math display">\[
\boldsymbol A= \boldsymbol L\boldsymbol L^T \,.
\]</span>
Here, <span class="math inline">\(\boldsymbol L\)</span> is a lower triangular matrix with positive diagonal elements.</p>
<p>This decomposition is unique and is called <strong>Cholesky factorisation</strong>. It is
often used to check whether a symmetric matrix is positive definite as it is algorithmically
less demanding than eigenvalue decomposition.</p>
<p>Note that some implementations of the Cholesky decomposition (e.g.Â R) use
upper triangular matrices <span class="math inline">\(\boldsymbol K\)</span> with positive diagonal so that
<span class="math inline">\(\boldsymbol A= \boldsymbol K^T \boldsymbol K\)</span> and <span class="math inline">\(\boldsymbol L= \boldsymbol K^T\)</span>.</p>
</div>
</div>
<div id="matrix-summaries-based-on-eigenvalues-and-singular-values" class="section level2">
<h2><span class="header-section-number">A.8</span> Matrix summaries based on eigenvalues and singular values</h2>
<div id="trace-and-determinant-computed-from-eigenvalues" class="section level3">
<h3><span class="header-section-number">A.8.1</span> Trace and determinant computed from eigenvalues</h3>
<p>The eigendecomposition <span class="math inline">\(\boldsymbol A=\boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1}\)</span>
allows to establish a link between trace and determinant and eigenvalues.</p>
<p>Specifically,
<span class="math display">\[
\begin{split}
\text{Tr}(\boldsymbol A) &amp; = \text{Tr}(\boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1}  ) =
\text{Tr}( \boldsymbol \Lambda\boldsymbol U^{-1} \boldsymbol U) \\
 &amp;= \text{Tr}( \boldsymbol \Lambda) = \sum_{i=1}^d \lambda_i \\
\end{split}
\]</span>
thus the trace of a square matrix <span class="math inline">\(\boldsymbol A\)</span> is equal to the <em>sum</em> of its eigenvalues. Likewise,
<span class="math display">\[
\begin{split}
\det(\boldsymbol A) &amp; = \det(\boldsymbol U) \det(\boldsymbol \Lambda) \det(\boldsymbol U^{-1}  ) \\
 &amp;=\det( \boldsymbol \Lambda) = \prod_{i=1}^d \lambda_i \\
\end{split}
\]</span>
therefore the determinant of <span class="math inline">\(\boldsymbol A\)</span> is the <em>product</em> of the eigenvalues.</p>
<p>The relationship between eigenvalues and the trace and the determinant
is demonstrated here for diagonisable non-defective matrices.
However, it does hold also in general for any matrix. This can be shown by using certain non-diagonal matrix decompositions (e.g.Â Jordan decomposition).</p>
<p>As a result, if any of the eigenvalues is equal to zero then <span class="math inline">\(\det(\boldsymbol A) = 0\)</span> and as hence <span class="math inline">\(\boldsymbol A\)</span> is singular and not invertible.</p>
<p>The trace and determinant of a real matrix are always real even though the individual eigenvalues may be complex.</p>
</div>
<div id="rank-and-condition-number" class="section level3">
<h3><span class="header-section-number">A.8.2</span> Rank and condition number</h3>
<p>The rank is the dimension of the space spanned by both the column and row vectors. A rectangular matrix of dimension <span class="math inline">\(n \times d\)</span> will have
rank of at most <span class="math inline">\(m = \min(n, d)\)</span>, and if the maximum is indeed achieved then it has full rank.</p>
<p>The condition number describes how well- or ill-conditioned
a full rank matrix is. For example, for a square matrix a large condition number implies that the matrix is close to being singular
and thus ill-conditioned.
If the condition number is infinite then the matrix is not full rank.</p>
<p>The rank and condition of a matrix can both be determined from the <span class="math inline">\(m\)</span> singular values <span class="math inline">\(d_1, \ldots, d_m\)</span> of a matrix obtained by SVD:</p>
<ol style="list-style-type: lower-roman">
<li>The rank is number of non-zero singular values.</li>
<li>The condition number is the ratio of the largest singular value
divided by the smallest singular value (absolute values if signs are allowed).</li>
</ol>
<p>If a square matrix <span class="math inline">\(\boldsymbol A\)</span> is singular then the condition number is infinite, and it will not have full rank.
On the other hand, a non-singular square matrix, such as
a positive definite matrix, has full rank.</p>
</div>
</div>
<div id="functions-of-symmetric-matrices" class="section level2">
<h2><span class="header-section-number">A.9</span> Functions of symmetric matrices</h2>
<p>We focus on <em>symmetric</em> square matrices <span class="math inline">\(\boldsymbol A=\boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span> which are always diagonisable with real eigenvalues <span class="math inline">\(\boldsymbol \Lambda\)</span> and orthogonal eigenvectors <span class="math inline">\(\boldsymbol U\)</span>.</p>
<div id="definition-of-a-matrix-function" class="section level3">
<h3><span class="header-section-number">A.9.1</span> Definition of a matrix function</h3>
<p>Assume a real-valued function <span class="math inline">\(f(a)\)</span> of a real number <span class="math inline">\(a\)</span>. Then the corresponding
matrix function <span class="math inline">\(f(\boldsymbol A)\)</span>
is defined as
<span class="math display">\[
f(\boldsymbol A) =  \boldsymbol Uf(\boldsymbol \Lambda) \boldsymbol U^T =  \boldsymbol U\begin{pmatrix}
    f(\lambda_{1}) &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; f(\lambda_{d})
\end{pmatrix} \boldsymbol U^T
\]</span>
where the function <span class="math inline">\(f(a)\)</span> is applied to the eigenvalues of <span class="math inline">\(\boldsymbol A\)</span>.
By construction <span class="math inline">\(f(\boldsymbol A)\)</span> is real, symmetric and has
real eigenvalues <span class="math inline">\(f(\lambda_i)\)</span>.</p>
<p>Examples:</p>

<div class="example">
<span id="exm:unnamed-chunk-1" class="example"><strong>Example A.1  </strong></span>Matrix power: <span class="math inline">\(f(a) = a^p\)</span> (with <span class="math inline">\(p\)</span> a real number)
</div>

<p>Special cases of matrix power include :</p>
<ul>
<li>Matrix inversion: <span class="math inline">\(f(a) = a^{-1}\)</span><br />
Note that if the matrix <span class="math inline">\(\boldsymbol A\)</span> is singular, i.e.Â contains one or more eigenvalues <span class="math inline">\(\lambda_i=0\)</span>,
then <span class="math inline">\(\boldsymbol A^{-1}\)</span> is not defined and therefore <span class="math inline">\(\boldsymbol A\)</span> is not invertible.</li>
</ul>
<p>However, a so-called <em>pseudoinverse</em> can still be computed, by inverting the non-zero eigenvalues, and
keeping the zero eigenvalues as zero.</p>
<ul>
<li>Matrix square root: <span class="math inline">\(f(a) = a^{1/2}\)</span><br />
Since there are multiple solutions to the square root there are also multiple
matrix square roots. The principal matrix square root is obtained by using
the positive square roots of all the eigenvalues. Thus the <strong>principal matrix square root</strong>
of a positive semi-definite matrix is also positive semi-definite and it is unique.</li>
</ul>

<div class="example">
<p><span id="exm:unnamed-chunk-2" class="example"><strong>Example A.2  </strong></span>Matrix exponential: <span class="math inline">\(f(a) = \exp(a)\)</span><br />
Note that because <span class="math inline">\(\exp(a) \geq 0\)</span> for all real <span class="math inline">\(a\)</span> the matrix <span class="math inline">\(\exp(\boldsymbol A)\)</span> is positive
semi-definite. Thus, the matrix exponential can be used to generate positive semi-definite
matrices.</p>
If <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> commute, i.e.Â if <span class="math inline">\(\boldsymbol A\boldsymbol B= \boldsymbol B\boldsymbol A\)</span>, then
<span class="math inline">\(\exp(\boldsymbol A+\boldsymbol B) = \exp(\boldsymbol A) \exp(\boldsymbol B)\)</span>. However, this is not the case
otherwise!
</div>


<div class="example">
<span id="exm:unnamed-chunk-3" class="example"><strong>Example A.3  </strong></span>Matrix logarithm: <span class="math inline">\(f(a) = \log(a)\)</span><br />
As the logarithm requires <span class="math inline">\(a &gt;0\)</span> the matrix <span class="math inline">\(\boldsymbol A\)</span> needs to be positive definite
for <span class="math inline">\(\log(\boldsymbol A)\)</span> to be defined.
</div>

</div>
<div id="identities-for-the-matrix-exponential-and-logarithm" class="section level3">
<h3><span class="header-section-number">A.9.2</span> Identities for the matrix exponential and logarithm</h3>
<p>The above give rise to useful identities:</p>
<ol style="list-style-type: decimal">
<li><p>For any symmetric matrix <span class="math inline">\(\boldsymbol A\)</span> we have
<span class="math display">\[
\det(\exp(\boldsymbol A)) = \exp(\text{Tr}(\boldsymbol A))
\]</span>
because
<span class="math inline">\(\prod_i \exp(\lambda_i) = \exp( \sum_i \lambda_i)\)</span>
where <span class="math inline">\(\lambda_i\)</span> are the eigenvalues of <span class="math inline">\(\boldsymbol A\)</span>.</p></li>
<li><p>If we take the logarithm on both sides and replace <span class="math inline">\(\exp(\boldsymbol A)=\boldsymbol B\)</span> we get another
identity for a symmetric positive definite matrix <span class="math inline">\(\boldsymbol B\)</span>:
<span class="math display">\[
\log \det(\boldsymbol B) = \text{Tr}(\log(\boldsymbol B))
\]</span>
because
<span class="math inline">\(\log( \prod_i \lambda_i) = \sum_i \log(\lambda_i)\)</span>
where <span class="math inline">\(\lambda_i\)</span> are the eigenvalues of <span class="math inline">\(\boldsymbol B\)</span>.</p></li>
</ol>
</div>
</div>
<div id="matrix-calculus" class="section level2">
<h2><span class="header-section-number">A.10</span> Matrix calculus</h2>
<div id="first-order-vector-derivatives" class="section level3">
<h3><span class="header-section-number">A.10.1</span> First order vector derivatives</h3>
<div id="gradient" class="section level4">
<h4><span class="header-section-number">A.10.1.1</span> Gradient</h4>
<p>The <strong>nabla operator</strong> (also known as <strong>del operator</strong>) is the <em>row</em> vector
<span class="math display">\[
\nabla =  (\frac{\partial}{\partial x_1}, \ldots, 
\frac{\partial}{\partial x_d}) = \frac{\partial}{\partial \boldsymbol x}
\]</span>
containing
the first order partial derivative operators.</p>
<p>The <strong>gradient</strong> of a scalar-valued function
<span class="math inline">\(f(\boldsymbol x)\)</span> with vector argument <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_d)^T\)</span>
is also a <em>row</em> vector (with <span class="math inline">\(d\)</span> columns) and
can be expressed using the nabla operator
<span class="math display">\[
\begin{split}
\nabla f(\boldsymbol x) &amp;= \left( \frac{\partial f(\boldsymbol x)}{\partial x_1}, \ldots, 
\frac{\partial f(\boldsymbol x)}{\partial x_d} \right) \\
&amp; = 
 \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = \text{grad} f(\boldsymbol x) \, .\\
\end{split}
\]</span>
Note the various notations for the gradient.</p>

<div class="example">
<span id="exm:unnamed-chunk-4" class="example"><strong>Example A.4  </strong></span><span class="math inline">\(f(\boldsymbol x)=\boldsymbol a^T \boldsymbol x+ b\)</span>. Then <span class="math inline">\(\nabla f(\boldsymbol x) = \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol a^T\)</span>.
</div>


<div class="example">
<span id="exm:unnamed-chunk-5" class="example"><strong>Example A.5  </strong></span><span class="math inline">\(f(\boldsymbol x)=\boldsymbol x^T \boldsymbol x\)</span>. Then <span class="math inline">\(\nabla f(\boldsymbol x) = \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = 2 \boldsymbol x^T\)</span>.
</div>


<div class="example">
<span id="exm:unnamed-chunk-6" class="example"><strong>Example A.6  </strong></span><span class="math inline">\(f(\boldsymbol x)=\boldsymbol x^T \boldsymbol A\boldsymbol x\)</span>. Then <span class="math inline">\(\nabla f(\boldsymbol x) = \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol x^T (\boldsymbol A+ \boldsymbol A^T)\)</span>.
</div>

</div>
<div id="jacobian-matrix" class="section level4">
<h4><span class="header-section-number">A.10.1.2</span> Jacobian matrix</h4>
<p>For a vector-valued function
<span class="math display">\[
\boldsymbol f(\boldsymbol x) = ( f_1(\boldsymbol x), \ldots, f_m(\boldsymbol x) )^T \,.
\]</span>
the computation of the gradient of each component yields
the <strong>Jacobian matrix</strong> (with <span class="math inline">\(m\)</span> rows and <span class="math inline">\(d\)</span> columns)
<span class="math display">\[
\begin{split}
\boldsymbol J_{\boldsymbol f}(\boldsymbol x) &amp;= 
\left( {\begin{array}{c}
 \nabla f_1(\boldsymbol x)   \\
 \vdots   \\
 \nabla f_m(\boldsymbol x)   \\
 \end{array} } \right) 
= \left(\frac{\partial f_i(\boldsymbol x)}{\partial x_j}\right) \\
&amp;= \frac{\partial \boldsymbol f(\boldsymbol x)}{\partial \boldsymbol x} = D \boldsymbol f(\boldsymbol x)\, \\
\end{split}
\]</span>
Again, note the various notations for the Jacobian matrix!</p>

<div class="example">
<span id="exm:unnamed-chunk-7" class="example"><strong>Example A.7  </strong></span><span class="math inline">\(\boldsymbol f(\boldsymbol x)=\boldsymbol A\boldsymbol x+ \boldsymbol b\)</span>. Then <span class="math inline">\(\boldsymbol J_{\boldsymbol f}(\boldsymbol x) = \frac{\partial \boldsymbol f(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol A\)</span>.
</div>

<p>If <span class="math inline">\(m=d\)</span> then the Jacobian matrix is a square matrix and this allows to compute the
<strong>Jacobian determinant</strong> <span class="math display">\[\det  \boldsymbol J_{\boldsymbol f}(\boldsymbol x) = \det\left(\frac{\partial \boldsymbol f(\boldsymbol x)}{\partial \boldsymbol x}\right)\]</span></p>
<p>If <span class="math inline">\(\boldsymbol y= \boldsymbol f(\boldsymbol x)\)</span> is an invertible function with <span class="math inline">\(\boldsymbol x= \boldsymbol f^{-1}(\boldsymbol y)\)</span>
then the Jacobian matrix is invertible and the inverted matrix is in fact the
Jacobian of the inverse function!</p>
<p>This allows to compute the Jacobian determinant of the backtransformation as
as the inverse of the Jacobian determinant the original function:
<span class="math display">\[\det  D \boldsymbol f^{-1}(\boldsymbol y) = ( \det  D \boldsymbol f(\boldsymbol x) )^{-1}\]</span>
or in alternative notation
<span class="math display">\[\det  D \boldsymbol x(\boldsymbol y) = \frac{1}{ \det  D \boldsymbol y(\boldsymbol x) }\]</span>.</p>
</div>
</div>
<div id="second-order-vector-derivatives" class="section level3">
<h3><span class="header-section-number">A.10.2</span> Second order vector derivatives</h3>
<p>The matrix of all second order partial derivates of scalar-valued
function with vector-valued argument is called the <strong>Hessian matrix</strong>
and is computed by double application of the nabla operator:
<span class="math display">\[
\begin{split}
\nabla^T \nabla f(\boldsymbol x) &amp;=
\begin{pmatrix}
  \frac{\partial^2 f(\boldsymbol x)}{\partial x_1^2}
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_1 \partial x_2} 
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_1 \partial x_d} \\
  \frac{\partial^2 f(\boldsymbol x)}{\partial x_2 \partial x_1} 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_2^2}
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_2 \partial x_d} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  \frac{\partial^2 f(\boldsymbol x)}{\partial x_d \partial x_1} 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_d \partial x_2}  
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_d^2}
 \end{pmatrix} \\
&amp; = \left(\frac{\partial f(\boldsymbol x)}{\partial x_i \partial x_j}\right)  
  = {\left(\frac{\partial}{\partial \boldsymbol x}\right)}^T \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x}
\,.\\
\end{split}
\]</span>
By construction it is square and symmetric.</p>
</div>
<div id="first-order-matrix-derivatives" class="section level3">
<h3><span class="header-section-number">A.10.3</span> First order matrix derivatives</h3>
<p>The derivative of a scalar-valued function <span class="math inline">\(f(\boldsymbol X)\)</span> with regard to a matrix argument <span class="math inline">\(\boldsymbol X\)</span>
can also be defined and results in a matrix
with transposed dimensions compared to <span class="math inline">\(\boldsymbol X\)</span>.</p>
Two important specific examples are:

<div class="example">
<span id="exm:unnamed-chunk-8" class="example"><strong>Example A.8  </strong></span><span class="math inline">\(\frac{\partial \text{Tr}(\boldsymbol A\boldsymbol X)}{\partial \boldsymbol X} = \boldsymbol A\)</span>
</div>


<div class="example">
<span id="exm:unnamed-chunk-9" class="example"><strong>Example A.9  </strong></span><span class="math inline">\(\frac{\partial \log \det(\boldsymbol X)}{\partial \boldsymbol X} = \frac{\partial \text{Tr}(\log \boldsymbol X)}{\partial \boldsymbol X} = \boldsymbol X^{-1}\)</span>
</div>


</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="6-nonlinear.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="8-further-study.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math38161-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
