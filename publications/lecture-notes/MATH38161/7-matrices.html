<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>A Brief refresher on matrices | index.split</title>
  <meta name="description" content="Multivariate Statistics and Machine Learning<br />
<br />
Lecture Notes<br />
MATH38161</div>" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="A Brief refresher on matrices | index.split" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Brief refresher on matrices | index.split" />
  
  
  

<meta name="author" content="Korbinian Strimmer" />


<meta name="date" content="2020-10-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="6-nonlinear.html"/>
<link rel="next" href="8-further-study.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH38161/index.html">MATH38161 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-multivariate.html"><a href="1-multivariate.html"><i class="fa fa-check"></i><b>1</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="1.1" data-path="1-multivariate.html"><a href="1-multivariate.html#why-multivariate-statistics"><i class="fa fa-check"></i><b>1.1</b> Why multivariate statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="1-multivariate.html"><a href="1-multivariate.html#basics"><i class="fa fa-check"></i><b>1.2</b> Basics</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-vs.multivariate-random-variables"><i class="fa fa-check"></i><b>1.2.1</b> Univariate vs.Â multivariate random variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-multivariate.html"><a href="1-multivariate.html#mean-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.2</b> Mean of a random vector</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-multivariate.html"><a href="1-multivariate.html#variance-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.3</b> Variance of a random vector</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-multivariate.html"><a href="1-multivariate.html#properties-of-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.4</b> Properties of the covariance matrix</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-multivariate.html"><a href="1-multivariate.html#eigenvalue-decomposition-of-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.2.5</b> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.2.6" data-path="1-multivariate.html"><a href="1-multivariate.html#quantities-related-to-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.6</b> Quantities related to the covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>1.3</b> Multivariate normal distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-normal-distribution"><i class="fa fa-check"></i><b>1.3.1</b> Univariate normal distribution:</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-model"><i class="fa fa-check"></i><b>1.3.2</b> Multivariate normal model</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-multivariate.html"><a href="1-multivariate.html#shape-of-the-contour-lines-of-the-multivariate-normal-density"><i class="fa fa-check"></i><b>1.3.3</b> Shape of the contour lines of the multivariate normal density</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-in-large-sample-and-small-sample-settings"><i class="fa fa-check"></i><b>1.4</b> Estimation in large sample and small sample settings</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-data"><i class="fa fa-check"></i><b>1.4.1</b> Multivariate data</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-multivariate.html"><a href="1-multivariate.html#strategies-for-large-sample-estimation"><i class="fa fa-check"></i><b>1.4.2</b> Strategies for large sample estimation</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-multivariate.html"><a href="1-multivariate.html#large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.4.3</b> Large sample estimates of mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.4.4" data-path="1-multivariate.html"><a href="1-multivariate.html#problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions"><i class="fa fa-check"></i><b>1.4.4</b> Problems with maximum likelihood in small sample settings and high dimensions</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-of-covariance-matrix-in-small-sample-settings"><i class="fa fa-check"></i><b>1.4.5</b> Estimation of covariance matrix in small sample settings</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-multivariate.html"><a href="1-multivariate.html#discrete-multivariate-distributions"><i class="fa fa-check"></i><b>1.5</b> Discrete multivariate distributions</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-multivariate.html"><a href="1-multivariate.html#categorical-distribution"><i class="fa fa-check"></i><b>1.5.1</b> Categorical distribution</a></li>
<li class="chapter" data-level="1.5.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multinomial-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Multinomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="1-multivariate.html"><a href="1-multivariate.html#continuous-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Continuous multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-multivariate.html"><a href="1-multivariate.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.6.1</b> Dirichlet distribution</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-multivariate.html"><a href="1-multivariate.html#wishart-distribution"><i class="fa fa-check"></i><b>1.6.2</b> Wishart distribution</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-multivariate.html"><a href="1-multivariate.html#inverse-wishart-distribution"><i class="fa fa-check"></i><b>1.6.3</b> Inverse Wishart distribution</a></li>
<li class="chapter" data-level="1.6.4" data-path="1-multivariate.html"><a href="1-multivariate.html#further-distributions"><i class="fa fa-check"></i><b>1.6.4</b> Further distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-transformations.html"><a href="2-transformations.html"><i class="fa fa-check"></i><b>2</b> Transformations and dimension reduction</a><ul>
<li class="chapter" data-level="2.1" data-path="2-transformations.html"><a href="2-transformations.html#linear-transformations"><i class="fa fa-check"></i><b>2.1</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-transformations.html"><a href="2-transformations.html#location-scale-transformation"><i class="fa fa-check"></i><b>2.1.1</b> Location-scale transformation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-transformations.html"><a href="2-transformations.html#invertible-location-scale-transformation"><i class="fa fa-check"></i><b>2.1.2</b> Invertible location-scale transformation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-transformations.html"><a href="2-transformations.html#nonlinear-transformations"><i class="fa fa-check"></i><b>2.2</b> Nonlinear transformations</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-transformations.html"><a href="2-transformations.html#general-transformation"><i class="fa fa-check"></i><b>2.2.1</b> General transformation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-transformations.html"><a href="2-transformations.html#delta-method"><i class="fa fa-check"></i><b>2.2.2</b> Delta method</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-transformations.html"><a href="2-transformations.html#transformation-of-densities-under-general-invertible-transformation"><i class="fa fa-check"></i><b>2.2.3</b> Transformation of densities under general invertible transformation</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-transformations.html"><a href="2-transformations.html#normalising-flows"><i class="fa fa-check"></i><b>2.2.4</b> Normalising flows</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-transformations.html"><a href="2-transformations.html#whitening-transformations"><i class="fa fa-check"></i><b>2.3</b> Whitening transformations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-transformations.html"><a href="2-transformations.html#overview"><i class="fa fa-check"></i><b>2.3.1</b> Overview</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-transformations.html"><a href="2-transformations.html#general-whitening-transformation"><i class="fa fa-check"></i><b>2.3.2</b> General whitening transformation</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-transformations.html"><a href="2-transformations.html#general-solution-of-whitening-constraint-covariance-based"><i class="fa fa-check"></i><b>2.3.3</b> General solution of whitening constraint (covariance-based)</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-transformations.html"><a href="2-transformations.html#another-solution-correlation-based"><i class="fa fa-check"></i><b>2.3.4</b> Another solution (correlation-based)</a></li>
<li class="chapter" data-level="2.3.5" data-path="2-transformations.html"><a href="2-transformations.html#objective-criteria-for-choosing-among-boldsymbol-w-using-boldsymbol-q_1-or-boldsymbol-q_2"><i class="fa fa-check"></i><b>2.3.5</b> Objective criteria for choosing among <span class="math inline">\(\boldsymbol W\)</span> using <span class="math inline">\(\boldsymbol Q_1\)</span> or <span class="math inline">\(\boldsymbol Q_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-transformations.html"><a href="2-transformations.html#natural-whitening-procedures"><i class="fa fa-check"></i><b>2.4</b> Natural whitening procedures</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-transformations.html"><a href="2-transformations.html#zca-whitening"><i class="fa fa-check"></i><b>2.4.1</b> ZCA Whitening</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-transformations.html"><a href="2-transformations.html#zca-cor-whitening"><i class="fa fa-check"></i><b>2.4.2</b> ZCA-Cor Whitening</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-transformations.html"><a href="2-transformations.html#pca-whitening"><i class="fa fa-check"></i><b>2.4.3</b> PCA Whitening</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-transformations.html"><a href="2-transformations.html#pca-cor-whitening"><i class="fa fa-check"></i><b>2.4.4</b> PCA-cor Whitening</a></li>
<li class="chapter" data-level="2.4.5" data-path="2-transformations.html"><a href="2-transformations.html#cholesky-whitening"><i class="fa fa-check"></i><b>2.4.5</b> Cholesky Whitening</a></li>
<li class="chapter" data-level="2.4.6" data-path="2-transformations.html"><a href="2-transformations.html#comparison-of-zca-pca-and-chol-whitening"><i class="fa fa-check"></i><b>2.4.6</b> Comparison of ZCA, PCA and Chol whitening</a></li>
<li class="chapter" data-level="2.4.7" data-path="2-transformations.html"><a href="2-transformations.html#recap"><i class="fa fa-check"></i><b>2.4.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-transformations.html"><a href="2-transformations.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>2.5</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-transformations.html"><a href="2-transformations.html#application-to-data"><i class="fa fa-check"></i><b>2.5.1</b> Application to data</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-transformations.html"><a href="2-transformations.html#pca-correlation-loadings-and-plot"><i class="fa fa-check"></i><b>2.6</b> PCA correlation loadings and plot</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-transformations.html"><a href="2-transformations.html#iris-data-example"><i class="fa fa-check"></i><b>2.6.1</b> Iris data example</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-transformations.html"><a href="2-transformations.html#cca-whitening-canonical-correlation-analysis"><i class="fa fa-check"></i><b>2.7</b> CCA whitening (Canonical Correlation Analysis)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-transformations.html"><a href="2-transformations.html#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal"><i class="fa fa-check"></i><b>2.7.1</b> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</a></li>
<li class="chapter" data-level="2.7.2" data-path="2-transformations.html"><a href="2-transformations.html#related-methods"><i class="fa fa-check"></i><b>2.7.2</b> Related methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-clustering.html"><a href="3-clustering.html"><i class="fa fa-check"></i><b>3</b> Unsupervised learning and clustering</a><ul>
<li class="chapter" data-level="3.1" data-path="3-clustering.html"><a href="3-clustering.html#challenges-in-supervised-learning"><i class="fa fa-check"></i><b>3.1</b> Challenges in supervised learning</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-clustering.html"><a href="3-clustering.html#objective"><i class="fa fa-check"></i><b>3.1.1</b> Objective</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-clustering.html"><a href="3-clustering.html#questions-and-problems"><i class="fa fa-check"></i><b>3.1.2</b> Questions and problems</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-clustering.html"><a href="3-clustering.html#why-is-clustering-difficult"><i class="fa fa-check"></i><b>3.1.3</b> Why is clustering difficult?</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-clustering.html"><a href="3-clustering.html#common-types-of-clustering-methods"><i class="fa fa-check"></i><b>3.1.4</b> Common types of clustering methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-clustering.html"><a href="3-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>3.2</b> Hierarchical clustering</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-clustering.html"><a href="3-clustering.html#tree-like-structures"><i class="fa fa-check"></i><b>3.2.1</b> Tree-like structures</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-clustering.html"><a href="3-clustering.html#agglomerative-hierarchical-clustering-algorithms"><i class="fa fa-check"></i><b>3.2.2</b> Agglomerative hierarchical clustering algorithms</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-clustering.html"><a href="3-clustering.html#application-to-swiss-banknote-data-set"><i class="fa fa-check"></i><b>3.2.3</b> Application to Swiss banknote data set</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-clustering.html"><a href="3-clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>3.3</b> <span class="math inline">\(K\)</span>-means clustering</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-clustering.html"><a href="3-clustering.html#general-aims"><i class="fa fa-check"></i><b>3.3.1</b> General aims</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-clustering.html"><a href="3-clustering.html#algorithm"><i class="fa fa-check"></i><b>3.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-clustering.html"><a href="3-clustering.html#properties"><i class="fa fa-check"></i><b>3.3.3</b> Properties</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>3.3.4</b> Choosing the number of clusters</a></li>
<li class="chapter" data-level="3.3.5" data-path="3-clustering.html"><a href="3-clustering.html#k-medoids-aka-pam"><i class="fa fa-check"></i><b>3.3.5</b> <span class="math inline">\(K\)</span>-medoids aka PAM</a></li>
<li class="chapter" data-level="3.3.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-k-means-to-iris-data"><i class="fa fa-check"></i><b>3.3.6</b> Application of <span class="math inline">\(K\)</span>-means to Iris data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-clustering.html"><a href="3-clustering.html#mixture-models"><i class="fa fa-check"></i><b>3.4</b> Mixture models</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-clustering.html"><a href="3-clustering.html#finite-mixture-model"><i class="fa fa-check"></i><b>3.4.1</b> Finite mixture model</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-clustering.html"><a href="3-clustering.html#example-of-mixture-of-three-univariate-normal-densities"><i class="fa fa-check"></i><b>3.4.2</b> Example of mixture of three univariate normal densities:</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-clustering.html"><a href="3-clustering.html#example-of-a-mixture-of-two-bivariate-normal-densities"><i class="fa fa-check"></i><b>3.4.3</b> Example of a mixture of two bivariate normal densities</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-clustering.html"><a href="3-clustering.html#sampling-from-a-mixture-model-and-latent-allocation-variable-formulation"><i class="fa fa-check"></i><b>3.4.4</b> Sampling from a mixture model and latent allocation variable formulation</a></li>
<li class="chapter" data-level="3.4.5" data-path="3-clustering.html"><a href="3-clustering.html#predicting-the-group-allocation-of-a-given-sample"><i class="fa fa-check"></i><b>3.4.5</b> Predicting the group allocation of a given sample</a></li>
<li class="chapter" data-level="3.4.6" data-path="3-clustering.html"><a href="3-clustering.html#decomposition-of-covariance-and-total-variation"><i class="fa fa-check"></i><b>3.4.6</b> Decomposition of covariance and total variation</a></li>
<li class="chapter" data-level="3.4.7" data-path="3-clustering.html"><a href="3-clustering.html#variation-1-infinite-mixture-model"><i class="fa fa-check"></i><b>3.4.7</b> Variation 1: Infinite mixture model</a></li>
<li class="chapter" data-level="3.4.8" data-path="3-clustering.html"><a href="3-clustering.html#variation-2-semiparametric-mixture-model-with-two-classes"><i class="fa fa-check"></i><b>3.4.8</b> Variation 2: Semiparametric mixture model with two classes</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-clustering.html"><a href="3-clustering.html#fitting-mixture-models-to-data"><i class="fa fa-check"></i><b>3.5</b> Fitting mixture models to data</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-clustering.html"><a href="3-clustering.html#direct-estimation-of-mixture-model-parameters"><i class="fa fa-check"></i><b>3.5.1</b> Direct estimation of mixture model parameters</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-clustering.html"><a href="3-clustering.html#estimate-mixture-model-parameters-using-the-em-algorithm"><i class="fa fa-check"></i><b>3.5.2</b> Estimate mixture model parameters using the EM algorithm</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-clustering.html"><a href="3-clustering.html#em-algorithm-for-multivariate-normal-mixture-model"><i class="fa fa-check"></i><b>3.5.3</b> EM algorithm for multivariate normal mixture model</a></li>
<li class="chapter" data-level="3.5.4" data-path="3-clustering.html"><a href="3-clustering.html#connection-with-k-means-clustering-method"><i class="fa fa-check"></i><b>3.5.4</b> Connection with <span class="math inline">\(K\)</span>-means clustering method</a></li>
<li class="chapter" data-level="3.5.5" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-classes"><i class="fa fa-check"></i><b>3.5.5</b> Choosing the number of classes</a></li>
<li class="chapter" data-level="3.5.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-gmms-to-iris-flower-data"><i class="fa fa-check"></i><b>3.5.6</b> Application of GMMs to Iris flower data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-classification.html"><a href="4-classification.html"><i class="fa fa-check"></i><b>4</b> Supervised learning and classification</a><ul>
<li class="chapter" data-level="4.1" data-path="4-classification.html"><a href="4-classification.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-classification.html"><a href="4-classification.html#supervised-learning-vs.unsupervised-learning"><i class="fa fa-check"></i><b>4.1.1</b> Supervised learning vs.Â unsupervised learning</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-classification.html"><a href="4-classification.html#terminology"><i class="fa fa-check"></i><b>4.1.2</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-classification.html"><a href="4-classification.html#bayesian-discriminant-rule-or-bayes-classifier"><i class="fa fa-check"></i><b>4.2</b> Bayesian discriminant rule or Bayes classifier</a></li>
<li class="chapter" data-level="4.3" data-path="4-classification.html"><a href="4-classification.html#normal-bayes-classifier"><i class="fa fa-check"></i><b>4.3</b> Normal Bayes classifier</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-classification.html"><a href="4-classification.html#quadratic-discriminant-analysis-qda-and-gaussian-assumption"><i class="fa fa-check"></i><b>4.3.1</b> Quadratic discriminant analysis (QDA) and Gaussian assumption</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-classification.html"><a href="4-classification.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>4.3.2</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-classification.html"><a href="4-classification.html#diagonal-discriminant-analysis-dda"><i class="fa fa-check"></i><b>4.3.3</b> Diagonal discriminant analysis (DDA)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-classification.html"><a href="4-classification.html#the-training-step-learning-qda-lda-and-dda-classifiers-from-data"><i class="fa fa-check"></i><b>4.4</b> The training step â learning QDA, LDA and DDA classifiers from data</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-classification.html"><a href="4-classification.html#number-of-model-parameters"><i class="fa fa-check"></i><b>4.4.1</b> Number of model parameters</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-classification.html"><a href="4-classification.html#estimating-the-discriminant-predictor-function"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the discriminant / predictor function</a></li>
<li class="chapter" data-level="4.4.3" data-path="4-classification.html"><a href="4-classification.html#comparison-of-estimated-decision-boundaries-lda-vs.qda"><i class="fa fa-check"></i><b>4.4.3</b> Comparison of estimated decision boundaries: LDA vs.Â QDA</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-classification.html"><a href="4-classification.html#goodness-of-fit-and-variable-ranking"><i class="fa fa-check"></i><b>4.5</b> Goodness of fit and variable ranking</a><ul>
<li class="chapter" data-level="4.5.1" data-path="4-classification.html"><a href="4-classification.html#lda-with-k2-classes"><i class="fa fa-check"></i><b>4.5.1</b> LDA with <span class="math inline">\(K=2\)</span> classes</a></li>
<li class="chapter" data-level="4.5.2" data-path="4-classification.html"><a href="4-classification.html#multiple-classes"><i class="fa fa-check"></i><b>4.5.2</b> Multiple classes</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4-classification.html"><a href="4-classification.html#variable-selection-and-cross-validation"><i class="fa fa-check"></i><b>4.6</b> Variable selection and cross-validation</a><ul>
<li class="chapter" data-level="4.6.1" data-path="4-classification.html"><a href="4-classification.html#choosing-a-threshold-by-multiple-testing-using-false-discovery-rates"><i class="fa fa-check"></i><b>4.6.1</b> Choosing a threshold by multiple testing using false discovery rates</a></li>
<li class="chapter" data-level="4.6.2" data-path="4-classification.html"><a href="4-classification.html#quantifying-prediction-error"><i class="fa fa-check"></i><b>4.6.2</b> Quantifying prediction error</a></li>
<li class="chapter" data-level="4.6.3" data-path="4-classification.html"><a href="4-classification.html#estimation-of-prediction-error-without-validation-data-using-cross-validation"><i class="fa fa-check"></i><b>4.6.3</b> Estimation of prediction error without validation data using cross-validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-dependence.html"><a href="5-dependence.html"><i class="fa fa-check"></i><b>5</b> Multivariate dependencies</a><ul>
<li class="chapter" data-level="5.1" data-path="5-dependence.html"><a href="5-dependence.html#measuring-the-linear-association-between-two-sets-of-random-variables"><i class="fa fa-check"></i><b>5.1</b> Measuring the linear association between two sets of random variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-dependence.html"><a href="5-dependence.html#outline"><i class="fa fa-check"></i><b>5.1.1</b> Outline</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-dependence.html"><a href="5-dependence.html#special-cases"><i class="fa fa-check"></i><b>5.1.2</b> Special cases</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-dependence.html"><a href="5-dependence.html#rozeboom-vector-correlation"><i class="fa fa-check"></i><b>5.1.3</b> Rozeboom vector correlation</a></li>
<li class="chapter" data-level="5.1.4" data-path="5-dependence.html"><a href="5-dependence.html#rv-coefficient"><i class="fa fa-check"></i><b>5.1.4</b> RV coefficient</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-as-generalisation-of-correlation"><i class="fa fa-check"></i><b>5.2</b> Mutual information as generalisation of correlation</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-dependence.html"><a href="5-dependence.html#definition-of-mutual-information"><i class="fa fa-check"></i><b>5.2.1</b> Definition of mutual information</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normal-variables"><i class="fa fa-check"></i><b>5.2.2</b> Mutual information between two normal variables</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normally-distributed-random-vectors"><i class="fa fa-check"></i><b>5.2.3</b> Mutual information between two normally distributed random vectors</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-dependence.html"><a href="5-dependence.html#using-mi-for-variable-selection"><i class="fa fa-check"></i><b>5.2.4</b> Using MI for variable selection</a></li>
<li class="chapter" data-level="5.2.5" data-path="5-dependence.html"><a href="5-dependence.html#other-measures-of-general-dependence"><i class="fa fa-check"></i><b>5.2.5</b> Other measures of general dependence</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-dependence.html"><a href="5-dependence.html#graphical-models"><i class="fa fa-check"></i><b>5.3</b> Graphical models</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-dependence.html"><a href="5-dependence.html#purpose"><i class="fa fa-check"></i><b>5.3.1</b> Purpose</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-dependence.html"><a href="5-dependence.html#basic-notions-from-graph-theory"><i class="fa fa-check"></i><b>5.3.2</b> Basic notions from graph theory</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-dependence.html"><a href="5-dependence.html#probabilistic-graphical-models"><i class="fa fa-check"></i><b>5.3.3</b> Probabilistic graphical models</a></li>
<li class="chapter" data-level="5.3.4" data-path="5-dependence.html"><a href="5-dependence.html#directed-graphical-models"><i class="fa fa-check"></i><b>5.3.4</b> Directed graphical models</a></li>
<li class="chapter" data-level="5.3.5" data-path="5-dependence.html"><a href="5-dependence.html#undirected-graphical-models"><i class="fa fa-check"></i><b>5.3.5</b> Undirected graphical models</a></li>
<li class="chapter" data-level="5.3.6" data-path="5-dependence.html"><a href="5-dependence.html#algorithm-for-learning-ggms"><i class="fa fa-check"></i><b>5.3.6</b> Algorithm for learning GGMs</a></li>
<li class="chapter" data-level="5.3.7" data-path="5-dependence.html"><a href="5-dependence.html#example-exam-score-data-mardia-et-al-1979"><i class="fa fa-check"></i><b>5.3.7</b> Example: exam score data (Mardia et al 1979:)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-nonlinear.html"><a href="6-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Nonlinear and nonparametric models</a><ul>
<li class="chapter" data-level="6.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#limits-of-linear-models-and-correlation"><i class="fa fa-check"></i><b>6.1</b> Limits of linear models and correlation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#correlation-only-measures-linear-dependence"><i class="fa fa-check"></i><b>6.1.1</b> Correlation only measures linear dependence</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#anscombe-data-sets"><i class="fa fa-check"></i><b>6.1.2</b> Anscombe data sets</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests"><i class="fa fa-check"></i><b>6.2</b> Random forests</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#stochastic-vs.algorithmic-models"><i class="fa fa-check"></i><b>6.2.1</b> Stochastic vs.Â algorithmic models</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests-1"><i class="fa fa-check"></i><b>6.2.2</b> Random forests</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#comparison-of-decision-boundaries-decision-tree-vs.random-forest"><i class="fa fa-check"></i><b>6.2.3</b> Comparison of decision boundaries: decision tree vs.Â random forest</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gaussian-processes"><i class="fa fa-check"></i><b>6.3</b> Gaussian processes</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#main-concepts"><i class="fa fa-check"></i><b>6.3.1</b> Main concepts</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#technical-background"><i class="fa fa-check"></i><b>6.3.2</b> Technical background:</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#covariance-functions-and-kernel"><i class="fa fa-check"></i><b>6.3.3</b> Covariance functions and kernel</a></li>
<li class="chapter" data-level="6.3.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gp-model"><i class="fa fa-check"></i><b>6.3.4</b> GP model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks"><i class="fa fa-check"></i><b>6.4</b> Neural networks</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#history"><i class="fa fa-check"></i><b>6.4.1</b> History</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks-1"><i class="fa fa-check"></i><b>6.4.2</b> Neural networks</a></li>
<li class="chapter" data-level="6.4.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#learning-more-about-deep-learning"><i class="fa fa-check"></i><b>6.4.3</b> Learning more about deep learning</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="7-matrices.html"><a href="7-matrices.html"><i class="fa fa-check"></i><b>A</b> Brief refresher on matrices</a><ul>
<li class="chapter" data-level="A.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-notation"><i class="fa fa-check"></i><b>A.1</b> Matrix notation</a></li>
<li class="chapter" data-level="A.2" data-path="7-matrices.html"><a href="7-matrices.html#simple-special-matrices"><i class="fa fa-check"></i><b>A.2</b> Simple special matrices</a></li>
<li class="chapter" data-level="A.3" data-path="7-matrices.html"><a href="7-matrices.html#simple-matrix-operations"><i class="fa fa-check"></i><b>A.3</b> Simple matrix operations</a></li>
<li class="chapter" data-level="A.4" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.4</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="A.5" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.5</b> Eigenvalues and eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.6" data-path="7-matrices.html"><a href="7-matrices.html#singular-value-decomposition"><i class="fa fa-check"></i><b>A.6</b> Singular value decomposition</a></li>
<li class="chapter" data-level="A.7" data-path="7-matrices.html"><a href="7-matrices.html#positive-semi-definiteness-rank-condition"><i class="fa fa-check"></i><b>A.7</b> Positive (semi-)definiteness, rank, condition</a></li>
<li class="chapter" data-level="A.8" data-path="7-matrices.html"><a href="7-matrices.html#trace-and-determinant-of-a-matrix-and-eigenvalues"><i class="fa fa-check"></i><b>A.8</b> Trace and determinant of a matrix and eigenvalues</a></li>
<li class="chapter" data-level="A.9" data-path="7-matrices.html"><a href="7-matrices.html#useful-identities-for-determinants"><i class="fa fa-check"></i><b>A.9</b> Useful identities for determinants</a></li>
<li class="chapter" data-level="A.10" data-path="7-matrices.html"><a href="7-matrices.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.10</b> Functions of matrices</a></li>
<li class="chapter" data-level="A.11" data-path="7-matrices.html"><a href="7-matrices.html#matrix-calculus"><i class="fa fa-check"></i><b>A.11</b> Matrix calculus</a><ul>
<li class="chapter" data-level="A.11.1" data-path="7-matrices.html"><a href="7-matrices.html#first-order-vector-derivatives"><i class="fa fa-check"></i><b>A.11.1</b> First order vector derivatives</a></li>
<li class="chapter" data-level="A.11.2" data-path="7-matrices.html"><a href="7-matrices.html#second-order-vector-derivatives"><i class="fa fa-check"></i><b>A.11.2</b> Second order vector derivatives</a></li>
<li class="chapter" data-level="A.11.3" data-path="7-matrices.html"><a href="7-matrices.html#first-order-matrix-derivatives"><i class="fa fa-check"></i><b>A.11.3</b> First order matrix derivatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="8-further-study.html"><a href="8-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="8-further-study.html"><a href="8-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="8-further-study.html"><a href="8-further-study.html#advanced-reading"><i class="fa fa-check"></i><b>B.2</b> Advanced reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="9-references.html"><a href="9-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Multivariate Statistics and Machine Learning<br />
<br />
Lecture Notes<br />
MATH38161</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="brief-refresher-on-matrices" class="section level1">
<h1><span class="header-section-number">A</span> Brief refresher on matrices</h1>
<p>This is intended a very short recap of some essentials you need to know about matrices
For more details please consult the lecture notes of earlier modules (e.g.Â linear algebra).</p>
<div id="matrix-notation" class="section level2">
<h2><span class="header-section-number">A.1</span> Matrix notation</h2>
<p>We will frequently make use of matrix calculations.
Matrix notation helps to make the equations simpler and enables to understand
them better.</p>
<p>In matrix notion we distinguish between scalars, vectors, and matrices:</p>
<p><strong>Scalar</strong>: <span class="math inline">\(x\)</span>, <span class="math inline">\(X\)</span>, lower or upper case, plain type.</p>
<p><strong>Vector</strong>: <span class="math inline">\(\boldsymbol x\)</span>, lower case, bold type. In handwriting one uses an arrow <span class="math inline">\(\vec{x}\)</span> to indicate a vector.</p>
<p><strong>Matrix</strong>: <span class="math inline">\(\boldsymbol X\)</span>, upper case, bold type. In handwriting you use an underscore
<span class="math inline">\(\underline{X}\)</span> to indicate a matrix.</p>
<p>Note that a vector may be viewed as a matrix (with only one column or only one row).
Likewise, a scalar may also be considered as as special case of a matrix.</p>
<p>Note on <strong>random</strong> matrices and vectors:</p>
<p>In the above notation you need to determine
from the context whether a quantity represents a
random variable, or whether it is a constant. You cannot read this
this from the case (upper vs.Â lower case) as in the standard notation
commonly used in univariate statistics.</p>
</div>
<div id="simple-special-matrices" class="section level2">
<h2><span class="header-section-number">A.2</span> Simple special matrices</h2>
<p><span class="math inline">\(\boldsymbol I_d\)</span> is the identity matrix. It is a square matrix of dimension
<span class="math inline">\(d \times d\)</span> with the diagonal
filled with 1 and off-diagonals filled with 0.
<span class="math display">\[\boldsymbol I_d =
\begin{pmatrix}
    1 &amp; 0 &amp; 0 &amp; \dots &amp; 0\\
    0 &amp; 1 &amp; 0 &amp; \dots &amp; 0\\
    0 &amp; 0 &amp; 1 &amp;   &amp; 0\\
    \vdots &amp; \vdots &amp; &amp; \ddots &amp;  \\
    0 &amp; 0 &amp; 0 &amp;  &amp; 1 \\
\end{pmatrix}\]</span></p>
<p><span class="math inline">\(\boldsymbol 1\)</span> is a matrix that contains only 1s. Most often
it is used in the form of a column vector with <span class="math inline">\(d\)</span> rows:
<span class="math display">\[\boldsymbol 1_d =
\begin{pmatrix}
    1 \\
    1 \\
    1 \\
    \vdots   \\
    1  \\
\end{pmatrix}\]</span></p>
</div>
<div id="simple-matrix-operations" class="section level2">
<h2><span class="header-section-number">A.3</span> Simple matrix operations</h2>
<p>Matrices behave much like ordinary numbers. For example,
you can apply operations such as matrix addition <span class="math inline">\(\boldsymbol A+ \boldsymbol B\)</span>
and matrix multiplication <span class="math inline">\(\boldsymbol A\boldsymbol B\)</span>. In order to conduct these operations the
matrices need to be compatible: for addition the matrices need to have the same
dimension, and for multiplication the number of columns of <span class="math inline">\(\boldsymbol A\)</span> must match
the number of rows of <span class="math inline">\(\boldsymbol B\)</span>. Note that <span class="math inline">\(\boldsymbol A\boldsymbol B\neq \boldsymbol B\boldsymbol A\)</span>, i.e.Â matrix multiplication is in general not commutative.</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is a squared matrix and is nonsingular (i.e.Â it has no zero eigenvalues) then you can define an
inverse <span class="math inline">\(\boldsymbol A^{-1}\)</span> such that <span class="math inline">\(\boldsymbol A^{-1} \boldsymbol A= \boldsymbol A\boldsymbol A^{-1}= \boldsymbol I\)</span>.</p>
<p>The matrix transpose <span class="math inline">\(t(\boldsymbol A) = \boldsymbol A^T\)</span> interchanges rows and columns.</p>
<p>The trace of the matrix is the sum of the diagonal entries <span class="math inline">\(\text{Tr}(\boldsymbol A) = \sum a_{ii}\)</span>.</p>
<p>The squared Frobenius norm, i.e.Â the sum of the squares of all entries of a rectangular matrix <span class="math inline">\(\boldsymbol A=(a_{ij})\)</span>, can be
written using the trace as follows: <span class="math inline">\(||\boldsymbol A||_F^2 = \sum_{i,j} a_{ij}^2 = \text{Tr}(\boldsymbol A^T \boldsymbol A) = \text{Tr}(\boldsymbol A\boldsymbol A^T)\)</span>.</p>
</div>
<div id="orthogonal-matrices" class="section level2">
<h2><span class="header-section-number">A.4</span> Orthogonal matrices</h2>
<p>An orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> has the property that <span class="math inline">\(\boldsymbol Q^T = \boldsymbol Q^{-1}\)</span>.
This implies that <span class="math inline">\(\boldsymbol Q\boldsymbol Q^T = \boldsymbol Q^T \boldsymbol Q= \boldsymbol I\)</span>. An orthogonal matrix
<span class="math inline">\(\boldsymbol Q\)</span> can be interpreted geometrically as a rotation-reflection
operator since multiplication of <span class="math inline">\(\boldsymbol Q\)</span> with a vector will result in
a new vector of the same length but with a change in direction.
The identity matrix <span class="math inline">\(\boldsymbol I\)</span> is the simplest example of an orthogonal matrix
but in general there are infinitely many rotation-reflection matrices.</p>
</div>
<div id="eigenvalues-and-eigenvalue-decomposition" class="section level2">
<h2><span class="header-section-number">A.5</span> Eigenvalues and eigenvalue decomposition</h2>
<p>A vector <span class="math inline">\(\boldsymbol u_i\)</span> is called an eigenvector of a square matrix <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\lambda_i\)</span> the corresponding
eigenvalue if <span class="math inline">\(\boldsymbol A\boldsymbol u_i = \boldsymbol u_i \lambda_i\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is a symmetric matrix (i.e. <span class="math inline">\(\boldsymbol A= \boldsymbol A^T\)</span>) with real entries (as assumed for the rest of
this section) then it can be shown
that all eigenvalues are real, and that the corresponding eigenvectors are
all orthogonal.</p>
<p>The eigenvalue decomposition of a symmetric real-valued <span class="math inline">\(\boldsymbol A\)</span> is given by
<span class="math display">\[\boldsymbol A= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\]</span>
with
<span class="math display">\[\boldsymbol \Lambda= \begin{pmatrix}
    \lambda_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}
\end{pmatrix}\]</span>
being a diagonal matrix containing all eigenvalues <span class="math inline">\(\lambda_i\)</span>
of <span class="math inline">\(\boldsymbol A\)</span>. <span class="math inline">\(\boldsymbol U\)</span> is an orthogonal matrix containing the corresponding
eigensystem (i.e.Â all eigenvectors <span class="math inline">\(\boldsymbol u_i\)</span>) in the columns of <span class="math inline">\(\boldsymbol U\)</span>.
The eigenvectors in <span class="math inline">\(\boldsymbol U\)</span> are orthonormal, i.e.Â they
are orthogonal to each other and have length 1.</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is multiplied with <span class="math inline">\(\boldsymbol U\)</span> we immediately see
that <span class="math inline">\(\boldsymbol \Lambda\)</span> contains the eigenvalues since <span class="math inline">\(\boldsymbol A\boldsymbol U= \boldsymbol U\boldsymbol \Lambda\)</span>.</p>
<p>Furthermore, it can be shown that the above eigenvalue decomposition of <span class="math inline">\(\boldsymbol A\)</span> is <strong>unique apart from the signs
of the eigenvectors</strong> (i.e.Â individual column signs of <span class="math inline">\(\boldsymbol U\)</span> can be changed and the
eigenvalue decomposition is still valid). In order to make the eigenvalue
decomposition fully unique you need to impose further restrictions (e.g.Â require a positive diagonal
of <span class="math inline">\(\boldsymbol U\)</span>). Note that this may be particularly important in computer applications where the sign
can vary depending on the specific implementation of the underlying numerical algorithms.</p>
<p>Eigenvalue decomposition is also known as spectral decomposition.</p>
</div>
<div id="singular-value-decomposition" class="section level2">
<h2><span class="header-section-number">A.6</span> Singular value decomposition</h2>
<p>A generalisation of the eigenvalue decomposition to non-square
matrices is the <strong>singular value decomposition</strong> (SVD).</p>
<p>Let <span class="math inline">\(\boldsymbol A\)</span> be a <span class="math inline">\(n\times m\)</span> matrix. The SVD decomposition of
<span class="math inline">\(\boldsymbol A\)</span> is <span class="math inline">\(\boldsymbol A= \boldsymbol U\boldsymbol D\boldsymbol V^T\)</span>.</p>
<p><span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span> are orthogonal matrices,
<span class="math inline">\(\boldsymbol D\)</span> contains the singular values.</p>
<p>Like the eigenvalue decomposition, the SVD is unique apart from the
signs of the column vectors in <span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span>.</p>
<p>Because <span class="math inline">\(\boldsymbol A^T \boldsymbol A= \boldsymbol V\boldsymbol D^T \boldsymbol D\boldsymbol V^T\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T = \boldsymbol U\boldsymbol D\boldsymbol D^T \boldsymbol U^T\)</span>
the squared singular values correspond to the eigenvalues of <span class="math inline">\(\boldsymbol A^T \boldsymbol A\)</span>
and <span class="math inline">\(\boldsymbol A\boldsymbol A^T\)</span>.</p>
</div>
<div id="positive-semi-definiteness-rank-condition" class="section level2">
<h2><span class="header-section-number">A.7</span> Positive (semi-)definiteness, rank, condition</h2>
<p>If all <span class="math inline">\(\lambda_i \geq 0\)</span> then <span class="math inline">\(\boldsymbol A\)</span> is is called positive semi-definite.<br />
If all eigenvalues are strictly positive
<span class="math inline">\(\lambda_i &gt; 0\)</span> then <span class="math inline">\(\boldsymbol A\)</span> is called positive definite.</p>
<p>If one or more of the eigenvalues equal zero then <span class="math inline">\(\boldsymbol A\)</span> is said to be singular.</p>
<p>The rank of <span class="math inline">\(\boldsymbol A\)</span> counts how many eigenvalues are non-zero.<br />
<span class="math inline">\(\boldsymbol A\)</span> is full rank if all eigenvalues are non-zero.</p>
<p>The condition number of <span class="math inline">\(\boldsymbol A\)</span> is the ratio between the largest and smallest absolute eigenvalue.<br />
If <span class="math inline">\(\boldsymbol A\)</span> is singular then the condition number is infinite.</p>
</div>
<div id="trace-and-determinant-of-a-matrix-and-eigenvalues" class="section level2">
<h2><span class="header-section-number">A.8</span> Trace and determinant of a matrix and eigenvalues</h2>
<p>The trace of the matrix <span class="math inline">\(\boldsymbol A\)</span> can be also obtained as the <em>sum</em> of its eigenvalues:
<span class="math inline">\(\text{Tr}(\boldsymbol A) = \sum \lambda_i\)</span>.</p>
<p>The determinant of <span class="math inline">\(\boldsymbol A\)</span> is the <em>product</em> of the eigenvalues, <span class="math inline">\(\det(\boldsymbol A) = \prod \lambda_i\)</span>.
Therefore, if <span class="math inline">\(\boldsymbol A\)</span> is singular then <span class="math inline">\(\det(\boldsymbol A) = 0\)</span>.</p>
</div>
<div id="useful-identities-for-determinants" class="section level2">
<h2><span class="header-section-number">A.9</span> Useful identities for determinants</h2>
<p>Determinants have a multiplicative property, <span class="math inline">\(\det(\boldsymbol A\boldsymbol B) = \det(\boldsymbol A) \det(\boldsymbol B)\)</span>.</p>
<p>The determinant of a block-structured matrix
<span class="math display">\[
\boldsymbol A= \begin{pmatrix} \boldsymbol A_{11} &amp; \boldsymbol A_{12} \\ \boldsymbol A_{21} &amp; \boldsymbol A_{22} \\ \end{pmatrix}
\]</span>
is
<span class="math display">\[
\det(\boldsymbol A) = \det(\boldsymbol A_{22}) \det(\boldsymbol C_1) = \det(\boldsymbol A_{11}) \det(\boldsymbol C_2) 
\]</span>
with (Schur complement of <span class="math inline">\(\boldsymbol A_{22}\)</span>)
<span class="math display">\[
\boldsymbol C_1 = \boldsymbol A_{11} -  \boldsymbol A_{12}  \boldsymbol A_{22}^{-1}  \boldsymbol A_{21} 
\]</span>
and (Schur complement of <span class="math inline">\(\boldsymbol A_{11}\)</span>)
<span class="math display">\[
\boldsymbol C_2 = \boldsymbol A_{22} -  \boldsymbol A_{21}  \boldsymbol A_{11}^{-1}  \boldsymbol A_{12} 
\]</span>
For a block-diagonal matrix <span class="math inline">\(\boldsymbol A\)</span> with <span class="math inline">\(\boldsymbol A_{12} = 0\)</span> and <span class="math inline">\(\boldsymbol A_{21} = 0\)</span>
computation of the determinant simplifies to <span class="math inline">\(\det(\boldsymbol A) = \det(\boldsymbol A_{11}) \det(\boldsymbol A_{22})\)</span>.<br />
If all entries are scalars so that
<span class="math inline">\(\boldsymbol A= \begin{pmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{21} \\ \end{pmatrix}\)</span> the determinant is computed as
<span class="math inline">\(\det(\boldsymbol A) = a_{11} a_{22} - a_{12} a_{21}\)</span>.</p>
<p>Another important identity is
<span class="math display">\[\det(\boldsymbol I_n + \boldsymbol A\boldsymbol B) = \det(\boldsymbol I_m + \boldsymbol B\boldsymbol A)\]</span>
where <span class="math inline">\(\boldsymbol A\)</span> is a <span class="math inline">\(n \times m\)</span> and <span class="math inline">\(\boldsymbol B\)</span> is a <span class="math inline">\(m \times n\)</span> matrix. This is called the
Weinstein-Aronszajn determinant identity (also credited to Sylvester).</p>
</div>
<div id="functions-of-matrices" class="section level2">
<h2><span class="header-section-number">A.10</span> Functions of matrices</h2>
<p>Again, we focus on symmetric, real-valued squared matrices <span class="math inline">\(\boldsymbol A\)</span>.</p>
<p>A matrix function <span class="math inline">\(f(\boldsymbol A)\)</span>, generalising from a simple function <span class="math inline">\(f(a)\)</span> can then
be defined via the eigenvalue decomposition as
<span class="math display">\[
f(\boldsymbol A) =  \boldsymbol Uf(\boldsymbol \Lambda) \boldsymbol U^T =  \boldsymbol U\begin{pmatrix}
    f(\lambda_{1}) &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; f(\lambda_{d})
\end{pmatrix} \boldsymbol U^T
\]</span>
Therefore, in order to obtain the corresponding matrix function,
the function is simply applied on the level of eigenvalues.
By construction <span class="math inline">\(f(\boldsymbol A)\)</span> will also be symmetric and and real-valued as long as
the transformed eigenvalues <span class="math inline">\(f(\lambda_i)\)</span> are real.</p>
<p>Examples:</p>

<div class="example">
<span id="exm:unnamed-chunk-1" class="example"><strong>Example A.1  </strong></span>matrix power: <span class="math inline">\(f(a) = a^p\)</span> (with <span class="math inline">\(p\)</span> real number)
</div>

<p>Special cases of matrix power include :</p>
<ul>
<li>matrix inversion: <span class="math inline">\(f(a) = a^{-1}\)</span></li>
<li>the matrix square root: <span class="math inline">\(f(a) = a^{1/2}\)</span><br />
(since there are multiple solutions to the square root there are also multiple
matrix square roots. The principal matrix square root is given by using
the positive square roots of all the eigenvalues. Thus the <strong>principal matrix square root</strong>
of a positive semidefinite matrix is also positive semidefinite and it is unique).</li>
</ul>

<div class="example">
<span id="exm:unnamed-chunk-2" class="example"><strong>Example A.2  </strong></span>matrix exponential: <span class="math inline">\(f(a) = \exp(a)\)</span>
</div>


<div class="example">
<span id="exm:unnamed-chunk-3" class="example"><strong>Example A.3  </strong></span>matrix logarithm: <span class="math inline">\(f(a) = \log(a)\)</span>
</div>

<p>For a positive definite matrix <span class="math inline">\(\boldsymbol A\)</span> computing the trace of the matrix logarithm of <span class="math inline">\(\boldsymbol A\)</span> is
the same as taking the log of the determinant of <span class="math inline">\(\boldsymbol A\)</span>:
<span class="math display">\[
\text{Tr}(\log(\boldsymbol A)) = \log \det(\boldsymbol A)
\]</span>
because
<span class="math inline">\(\sum \log(\lambda_i) = \log( \prod \lambda_i )\)</span>.</p>
</div>
<div id="matrix-calculus" class="section level2">
<h2><span class="header-section-number">A.11</span> Matrix calculus</h2>
<div id="first-order-vector-derivatives" class="section level3">
<h3><span class="header-section-number">A.11.1</span> First order vector derivatives</h3>
<div id="gradient" class="section level4">
<h4><span class="header-section-number">A.11.1.1</span> Gradient</h4>
<p>The <strong>nabla operator</strong> (also known as <strong>del operator</strong>) is the <em>row</em> vector
<span class="math display">\[
\nabla =  (\frac{\partial}{\partial x_1}, \ldots, 
\frac{\partial}{\partial x_d}) = \frac{\partial}{\partial \boldsymbol x}
\]</span>
containing
the first order partial derivative operators.</p>
<p>The <strong>gradient</strong> of a scalar-valued function
<span class="math inline">\(f(\boldsymbol x)\)</span> with vector argument <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_d)^T\)</span>
is also a <em>row</em> vector (with <span class="math inline">\(d\)</span> columns) and
can be expressed using the nabla operator
<span class="math display">\[
\nabla f(\boldsymbol x) = \left( \frac{\partial f(\boldsymbol x)}{\partial x_1}, \ldots, 
\frac{\partial f(\boldsymbol x)}{\partial x_d} \right) = 
 \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = \text{grad} f(\boldsymbol x) \, .
\]</span>
Note the various notations for the gradient.</p>

<div class="example">
<span id="exm:unnamed-chunk-4" class="example"><strong>Example A.4  </strong></span><span class="math inline">\(f(\boldsymbol x)=\boldsymbol a^T \boldsymbol x+ b\)</span>. Then <span class="math inline">\(\nabla f(\boldsymbol x) = \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol a^T\)</span>.
</div>


<div class="example">
<span id="exm:unnamed-chunk-5" class="example"><strong>Example A.5  </strong></span><span class="math inline">\(f(\boldsymbol x)=\boldsymbol x^T \boldsymbol x\)</span>. Then <span class="math inline">\(\nabla f(\boldsymbol x) = \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = 2 \boldsymbol x^T\)</span>.
</div>


<div class="example">
<span id="exm:unnamed-chunk-6" class="example"><strong>Example A.6  </strong></span><span class="math inline">\(f(\boldsymbol x)=\boldsymbol x^T \boldsymbol A\boldsymbol x\)</span>. Then <span class="math inline">\(\nabla f(\boldsymbol x) = \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol x^T (\boldsymbol A+ \boldsymbol A^T)\)</span>.
</div>

</div>
<div id="jacobian-matrix" class="section level4">
<h4><span class="header-section-number">A.11.1.2</span> Jacobian matrix</h4>
<p>For a vector-valued function
<span class="math display">\[
\boldsymbol f(\boldsymbol x) = ( f_1(\boldsymbol x), \ldots, f_m(\boldsymbol x) )^T \,.
\]</span>
the computation of the gradient of each component yields
the <strong>Jacobian matrix</strong> (with <span class="math inline">\(m\)</span> rows and <span class="math inline">\(d\)</span> columns)
<span class="math display">\[
\boldsymbol J_{\boldsymbol f}(\boldsymbol x) = 
\left( {\begin{array}{c}
 \nabla f_1(\boldsymbol x)   \\
 \vdots   \\
 \nabla f_m(\boldsymbol x)   \\
 \end{array} } \right) 
= \left(\frac{\partial f_i(\boldsymbol x)}{\partial x_j}\right) = 
\frac{\partial \boldsymbol f(\boldsymbol x)}{\partial \boldsymbol x} = D \boldsymbol f(\boldsymbol x)\, 
\]</span>
Again, note the various notations for the Jacobian matrix!</p>

<div class="example">
<span id="exm:unnamed-chunk-7" class="example"><strong>Example A.7  </strong></span><span class="math inline">\(\boldsymbol f(\boldsymbol x)=\boldsymbol A\boldsymbol x+ \boldsymbol b\)</span>. Then <span class="math inline">\(\boldsymbol J_{\boldsymbol f}(\boldsymbol x) = \frac{\partial \boldsymbol f(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol A\)</span>.
</div>

<p>If <span class="math inline">\(m=d\)</span> then the Jacobian matrix is a square matrix and this allows to compute the
<strong>Jacobian determinant</strong> <span class="math display">\[\det  \boldsymbol J_{\boldsymbol f}(\boldsymbol x) = \det\left(\frac{\partial \boldsymbol f(\boldsymbol x)}{\partial \boldsymbol x}\right)\]</span></p>
<p>If <span class="math inline">\(\boldsymbol y= \boldsymbol f(\boldsymbol x)\)</span> is an invertible function with <span class="math inline">\(\boldsymbol x= \boldsymbol f^{-1}(\boldsymbol y)\)</span>
then the Jacobian matrix is invertible and the inverted matrix is in fact the
Jacobian of the inverse function!</p>
<p>This allows to compute the Jacobian determinant of the backtransformation as
as the inverse of the Jacobian determinant the original function:
<span class="math display">\[\det  D \boldsymbol f^{-1}(\boldsymbol y) = ( \det  D \boldsymbol f(\boldsymbol x) )^{-1}\]</span>
or in alternative notation
<span class="math display">\[\det  D \boldsymbol x(\boldsymbol y) = \frac{1}{ \det  D \boldsymbol y(\boldsymbol x) }\]</span>.</p>
</div>
</div>
<div id="second-order-vector-derivatives" class="section level3">
<h3><span class="header-section-number">A.11.2</span> Second order vector derivatives</h3>
<p>The matrix of all second order partial derivates of scalar-valued
function with vector-valued argument is called the <strong>Hessian matrix</strong>
and is computed by double application of the nabla operator:
<span class="math display">\[
\nabla^T \nabla f(\boldsymbol x) =
\begin{pmatrix}
  \frac{\partial^2 f(\boldsymbol x)}{\partial x_1^2}
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_1 \partial x_2} 
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_1 \partial x_d} \\
  \frac{\partial^2 f(\boldsymbol x)}{\partial x_2 \partial x_1} 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_2^2}
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_2 \partial x_d} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  \frac{\partial^2 f(\boldsymbol x)}{\partial x_d \partial x_1} 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_d \partial x_2}  
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_d^2}
 \end{pmatrix} = \left(\frac{\partial f(\boldsymbol x)}{\partial x_i \partial x_j}\right)  
= {\left(\frac{\partial}{\partial \boldsymbol x}\right)}^T \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x}
\,.
\]</span>
By construction it is square and symmetric.</p>
</div>
<div id="first-order-matrix-derivatives" class="section level3">
<h3><span class="header-section-number">A.11.3</span> First order matrix derivatives</h3>
<p>The derivative of a scalar-valued function <span class="math inline">\(f(\boldsymbol X)\)</span> with regard to a matrix argument <span class="math inline">\(\boldsymbol X\)</span>
can also be defined and results in a matrix
with transposed dimensions compared to <span class="math inline">\(\boldsymbol X\)</span>.</p>
Two important specific examples are:

<div class="example">
<span id="exm:unnamed-chunk-8" class="example"><strong>Example A.8  </strong></span><span class="math inline">\(\frac{\partial \text{Tr}(\boldsymbol A\boldsymbol X)}{\partial \boldsymbol X} = \boldsymbol A\)</span>
</div>


<div class="example">
<span id="exm:unnamed-chunk-9" class="example"><strong>Example A.9  </strong></span><span class="math inline">\(\frac{\partial \log \det(\boldsymbol X)}{\partial \boldsymbol X} = \frac{\partial \text{Tr}(\log \boldsymbol X)}{\partial \boldsymbol X} = \boldsymbol X^{-1}\)</span>
</div>


</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="6-nonlinear.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="8-further-study.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math38161-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
