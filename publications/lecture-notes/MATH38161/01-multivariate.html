<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Multivariate random variables – Multivariate Statistics and Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02-estimation.html" rel="next">
<link href="./00-preface.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3c04d35918bfbae480bb424d60ad250e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-multivariate.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Multivariate random variables</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Multivariate Statistics and Machine Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-multivariate.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Multivariate random variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multivariate estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations and dimension reduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised learning and clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning and classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-dependence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multivariate dependencies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear and nonparametric models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#essentials-in-multivariate-statistics" id="toc-essentials-in-multivariate-statistics" class="nav-link active" data-scroll-target="#essentials-in-multivariate-statistics"><span class="header-section-number">1.1</span> Essentials in multivariate statistics</a>
  <ul class="collapse">
  <li><a href="#why-multivariate-statistics" id="toc-why-multivariate-statistics" class="nav-link" data-scroll-target="#why-multivariate-statistics">Why multivariate statistics?</a></li>
  <li><a href="#univariate-vs.-multivariate-random-variables" id="toc-univariate-vs.-multivariate-random-variables" class="nav-link" data-scroll-target="#univariate-vs.-multivariate-random-variables">Univariate vs.&nbsp;multivariate random variables</a></li>
  <li><a href="#multivariate-data" id="toc-multivariate-data" class="nav-link" data-scroll-target="#multivariate-data">Multivariate data</a></li>
  <li><a href="#mean-of-a-random-vector" id="toc-mean-of-a-random-vector" class="nav-link" data-scroll-target="#mean-of-a-random-vector">Mean of a random vector</a></li>
  <li><a href="#variance-of-a-random-vector" id="toc-variance-of-a-random-vector" class="nav-link" data-scroll-target="#variance-of-a-random-vector">Variance of a random vector</a></li>
  <li><a href="#properties-of-the-covariance-matrix" id="toc-properties-of-the-covariance-matrix" class="nav-link" data-scroll-target="#properties-of-the-covariance-matrix">Properties of the covariance matrix</a></li>
  <li><a href="#eigenvalue-decomposition-of-boldsymbol-sigma" id="toc-eigenvalue-decomposition-of-boldsymbol-sigma" class="nav-link" data-scroll-target="#eigenvalue-decomposition-of-boldsymbol-sigma">Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
  <li><a href="#joint-covariance-matrix" id="toc-joint-covariance-matrix" class="nav-link" data-scroll-target="#joint-covariance-matrix">Joint covariance matrix</a></li>
  <li><a href="#quantities-related-to-the-covariance-matrix" id="toc-quantities-related-to-the-covariance-matrix" class="nav-link" data-scroll-target="#quantities-related-to-the-covariance-matrix">Quantities related to the covariance matrix</a></li>
  </ul></li>
  <li><a href="#multivariate-distributions" id="toc-multivariate-distributions" class="nav-link" data-scroll-target="#multivariate-distributions"><span class="header-section-number">1.2</span> Multivariate distributions</a>
  <ul class="collapse">
  <li><a href="#common-distributions" id="toc-common-distributions" class="nav-link" data-scroll-target="#common-distributions">Common distributions</a></li>
  <li><a href="#further-multivariate-distributions" id="toc-further-multivariate-distributions" class="nav-link" data-scroll-target="#further-multivariate-distributions">Further multivariate distributions</a></li>
  </ul></li>
  <li><a href="#multivariate-normal-distribution" id="toc-multivariate-normal-distribution" class="nav-link" data-scroll-target="#multivariate-normal-distribution"><span class="header-section-number">1.3</span> Multivariate normal distribution</a>
  <ul class="collapse">
  <li><a href="#univariate-normal-distribution" id="toc-univariate-normal-distribution" class="nav-link" data-scroll-target="#univariate-normal-distribution">Univariate normal distribution:</a></li>
  <li><a href="#multivariate-normal-model" id="toc-multivariate-normal-model" class="nav-link" data-scroll-target="#multivariate-normal-model">Multivariate normal model</a></li>
  <li><a href="#shape-of-the-multivariate-normal-density" id="toc-shape-of-the-multivariate-normal-density" class="nav-link" data-scroll-target="#shape-of-the-multivariate-normal-density">Shape of the multivariate normal density</a></li>
  <li><a href="#three-types-of-covariances" id="toc-three-types-of-covariances" class="nav-link" data-scroll-target="#three-types-of-covariances">Three types of covariances</a></li>
  <li><a href="#concentration-of-probability-mass-for-small-and-large-dimension" id="toc-concentration-of-probability-mass-for-small-and-large-dimension" class="nav-link" data-scroll-target="#concentration-of-probability-mass-for-small-and-large-dimension">Concentration of probability mass for small and large dimension</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Multivariate random variables</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="essentials-in-multivariate-statistics" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="essentials-in-multivariate-statistics"><span class="header-section-number">1.1</span> Essentials in multivariate statistics</h2>
<section id="why-multivariate-statistics" class="level3">
<h3 class="anchored" data-anchor-id="why-multivariate-statistics">Why multivariate statistics?</h3>
<p>In science we use experiments to learn about underlying mechanisms of interest, both deterministic and stochastic, to compare different models and to verify or reject hypotheses about the world. Statistics provides tools to quantify this procedure and offers methods to link data (experiments) with probabilistic models (hypotheses).</p>
<p>In univariate statistics with we use relatively simple approaches based on a single random variable or single parameter. However, in practise we often have to consider multiple random variables and multiple parameters, so we need more complex models and also be able to deal with more complex data. Hence, the need for multivariate statistical approaches and models.</p>
<p>Specifically, multivariate statistics is concerned with methods and models for <strong>random vectors</strong> and <strong>random matrices</strong>, rather than just random univariate (scalar) variables. Therefore, in multivariate statistics we will frequently make use of matrix notation.</p>
<p>Closely related to multivariate statistics (traditionally a subfield of statistics) is machine learning (ML) which is traditionally a subfield of computer science. ML used to focus more on algorithms rather on probabilistic modelling but nowadays most machine learning methods are fully based on statistical multivariate approaches, so the two fields are converging.</p>
<p>Multivariate models provide a means to learn dependencies and interactions among the components of the random variables which in turn allow us to draw conclusion about underlying mechanisms of interest (e.g.&nbsp;in biological or medical problems).</p>
<p>Two main tasks:</p>
<ul>
<li>unsupervised learning (finding structure, clustering)</li>
<li>supervised learning (training from labelled data, followed by prediction)</li>
</ul>
<p>Challenges:</p>
<ul>
<li>complexity of model needs to be appropriate for problem and available data</li>
<li>high dimensions make estimation and inference difficult</li>
<li>computational issues</li>
</ul>
</section>
<section id="univariate-vs.-multivariate-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="univariate-vs.-multivariate-random-variables">Univariate vs.&nbsp;multivariate random variables</h3>
<p>Univariate random variable (dimension <span class="math inline">\(d=1\)</span>): <span class="math display">\[x \sim F\]</span> where <span class="math inline">\(x\)</span> is a <strong>scalar</strong> and <span class="math inline">\(F\)</span> is the distribution. <span class="math inline">\(\text{E}(x) = \mu\)</span> denotes the mean and <span class="math inline">\(\text{Var}(x) = \sigma^2\)</span> the variance of <span class="math inline">\(x\)</span>.</p>
<p>Multivariate random <strong>vector</strong> of dimension <span class="math inline">\(d\)</span>: <span class="math display">\[\boldsymbol x= (x_1, x_2,...,x_d)^T  \sim F\]</span></p>
<p><span class="math inline">\(\boldsymbol x\)</span> is <strong>vector</strong> valued random variable.</p>
<p>The vector <span class="math inline">\(\boldsymbol x\)</span> is column vector (=matrix of size <span class="math inline">\(d \times 1\)</span>). Its components <span class="math inline">\(x_1, x_2,...,x_d\)</span> are univariate random variables. The dimension <span class="math inline">\(d\)</span> is also often denoted by <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span>.</p>
<p>Note that for simplicity of notation we use the same symbol to denote the random variable and its elementary outcomes (in particular we don’t use capitalisation to indicate a random variable). This convention greatly facilitates working with random vectors and matrices and follows, e.g., the classic multivariate statistics textbook by <span class="citation" data-cites="MKB1979">Mardia, Kent, and Bibby (<a href="bibliography.html#ref-MKB1979" role="doc-biblioref">1979</a>)</span>. If a quantity is random we will always specify this explicitly in the context.</p>
</section>
<section id="multivariate-data" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-data">Multivariate data</h3>
<p><strong>Vector notation:</strong></p>
<p>Samples from a multivariate distribution are <em>vectors</em> (not scalars as for univariate normal): <span class="math display">\[\boldsymbol x_1,\boldsymbol x_2,...,\boldsymbol x_n \stackrel{\text{iid}}\sim F\]</span></p>
<p><strong>Matrix and component notation:</strong></p>
<p>All the data points are commonly collected into a matrix <span class="math inline">\(\boldsymbol X\)</span>.</p>
<p>In statistics the convention is to store each data vector in the rows of the data matrix <span class="math inline">\(\boldsymbol X\)</span>:</p>
<p><span class="math display">\[\boldsymbol X= (\boldsymbol x_1,\boldsymbol x_2,...,\boldsymbol x_n)^T = \begin{pmatrix}
    x_{11}  &amp; x_{12} &amp; \dots &amp; x_{1d}   \\
    x_{21}  &amp; x_{22} &amp; \dots &amp; x_{2d}   \\
    \vdots \\
    x_{n1}  &amp; x_{n2} &amp; \dots &amp; x_{nd}
\end{pmatrix}\]</span></p>
<p>Therefore, <span class="math display">\[\boldsymbol x_1=\begin{pmatrix}
    x_{11}       \\
    \vdots \\
    x_{1d}
\end{pmatrix} , \space \boldsymbol x_2=\begin{pmatrix}
    x_{21}       \\
    \vdots \\
    x_{2d}
\end{pmatrix} , \ldots , \boldsymbol x_n=\begin{pmatrix}
    x_{n1}       \\
    \vdots \\
    x_{nd}
\end{pmatrix}\]</span></p>
<p>Thus, in statistics the first index runs over <span class="math inline">\((1,...,n)\)</span> and denotes the samples while the second index runs over <span class="math inline">\((1,...,d)\)</span> and refers to the variables.</p>
<p>The <strong>statistics convention on data matrices</strong> is <em>not</em> universal! In fact, in most of the machine learning literature in engineering and computer science the data samples are stored in the columns so that the variables appear in the rows (thus in the engineering convention the data matrix is transposed compared to the statistics convention).</p>
<p>In order to avoid confusion and ambiguity it is recommended to prefer vector notation to describe data over matrix or component notation (see also the section below on estimating covariance matrices for examples).</p>
</section>
<section id="mean-of-a-random-vector" class="level3">
<h3 class="anchored" data-anchor-id="mean-of-a-random-vector">Mean of a random vector</h3>
<p>The mean / expectation of a random vector with dimensions <span class="math inline">\(d\)</span> is also a vector with dimensions <span class="math inline">\(d\)</span>: <span class="math display">\[\text{E}(\boldsymbol x) = \boldsymbol \mu= \begin{pmatrix}
    \text{E}(x_1)       \\
    \text{E}(x_2)       \\
    \vdots \\
    \text{E}(x_d)
\end{pmatrix} = \left( \begin{array}{l}
    \mu_1       \\
    \mu_2       \\
    \vdots \\
    \mu_d
\end{array}\right)\]</span></p>
</section>
<section id="variance-of-a-random-vector" class="level3">
<h3 class="anchored" data-anchor-id="variance-of-a-random-vector">Variance of a random vector</h3>
<p>Recall the definition of mean and variance for a univariate random variable:</p>
<p><span class="math display">\[\text{E}(x) = \mu\]</span></p>
<p><span class="math display">\[\text{Var}(x) = \sigma^2 = \text{E}( (x-\mu)^2 )=\text{E}( (x-\mu)(x-\mu) ) = \text{E}(x^2)-\mu^2\]</span></p>
<p>Definition of <strong>variance of a random vector:</strong></p>
<p><span class="math display">\[\text{Var}(\boldsymbol x) = \underbrace{\boldsymbol \Sigma}_{d\times d} =
\text{E}\left(\underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d\times 1} \underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1\times d} \right)
= \text{E}(\boldsymbol x\boldsymbol x^T)-\boldsymbol \mu\boldsymbol \mu^T\]</span></p>
<p>The variance of a random vector is, therefore, <strong>not</strong> a vector but a <strong>matrix</strong>!</p>
<p><span class="math display">\[\boldsymbol \Sigma= (\sigma_{ij}) = \underbrace{\begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}}_{d\times d}\]</span></p>
</section>
<section id="properties-of-the-covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-the-covariance-matrix">Properties of the covariance matrix</h3>
<ol type="1">
<li><span class="math inline">\(\boldsymbol \Sigma\)</span> is real valued: <span class="math inline">\(\sigma_{ij} \in \mathbb{R}\)</span></li>
<li><span class="math inline">\(\boldsymbol \Sigma\)</span> is symmetric: <span class="math inline">\(\sigma_{ij} = \sigma_{ji}\)</span></li>
<li>The diagonal of <span class="math inline">\(\boldsymbol \Sigma\)</span> contains <span class="math inline">\(\sigma_{ii} = \text{Var}(x_i) = \sigma_i^2\)</span>, i.e.&nbsp;the variances of the components of <span class="math inline">\(\boldsymbol x\)</span>.</li>
<li>Off-diagonal elements <span class="math inline">\(\sigma_{ij} = \text{Cov}(x_i,x_j)\)</span> represent linear dependencies among the <span class="math inline">\(x_i\)</span>. <span class="math inline">\(\Longrightarrow\)</span> linear regression, correlation</li>
</ol>
<div id="tbl-numelemcovmat" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-numelemcovmat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1.1: Number of distinct elements in a covariance matrix.
</figcaption>
<div aria-describedby="tbl-numelemcovmat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(d\)</span></th>
<th># entries</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>10</td>
<td>55</td>
</tr>
<tr class="odd">
<td>100</td>
<td>5050</td>
</tr>
<tr class="even">
<td>1000</td>
<td>500500</td>
</tr>
<tr class="odd">
<td>10000</td>
<td>50005000</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>How many distinct elements does <span class="math inline">\(\boldsymbol \Sigma\)</span> have? <span class="math display">\[
\frac{d(d+1)}{2}
\]</span> This grows with the square of the dimension <span class="math inline">\(d\)</span>, i.e.&nbsp;it grows with order <span class="math inline">\(O(d^2)\)</span> (<a href="#tbl-numelemcovmat" class="quarto-xref">Table&nbsp;<span>1.1</span></a>).</p>
<p>For large dimension <span class="math inline">\(d\)</span> the covariance matrix has many components!</p>
<p>–&gt; computationally expensive (both for storage and in handling) –&gt; very challenging to estimate <span class="math inline">\(\boldsymbol \Sigma\)</span> in high dimensions <span class="math inline">\(d\)</span>.</p>
<p>Note: matrix inversion requires <span class="math inline">\(O(d^3)\)</span> operations using standard algorithms such as <a href="https://en.wikipedia.org/wiki/Gaussian_elimination">Gauss Jordan elimination</a>. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Hence, computing <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> is computationally expensive for large <span class="math inline">\(d\)</span>!</p>
</section>
<section id="eigenvalue-decomposition-of-boldsymbol-sigma" class="level3">
<h3 class="anchored" data-anchor-id="eigenvalue-decomposition-of-boldsymbol-sigma">Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></h3>
<p>Recall from linear matrix algebra that any real symmetric matrix has real eigenvalues and a complete set of orthogonal eigenvectors. These can be obtained by orthogonal eigendecomposition. <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>Applying eigenvalue decomposition to the covariance matrix yields <span class="math display">\[
\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T
\]</span> where <span class="math inline">\(\boldsymbol U\)</span> is an <strong>orthogonal matrix</strong> <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> containing the eigenvectors of the covariance matrix and <span class="math display">\[\boldsymbol \Lambda= \begin{pmatrix}
    \lambda_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}
\end{pmatrix}\]</span> contains the corresponding eigenvalues <span class="math inline">\(\lambda_i\)</span>.</p>
<p>Importantly, the eigenvalues of a covariance matrix are not only real-valued but are by construction further constrained to be non-negative. This can be seen by computing the quadratic form <span class="math inline">\(\boldsymbol z^T  \boldsymbol \Sigma\boldsymbol z\)</span> where <span class="math inline">\(\boldsymbol z\)</span> is a non-random vector. For any non-zero <span class="math inline">\(\boldsymbol z\)</span> <span class="math display">\[
\begin{split}
\boldsymbol z^T  \boldsymbol \Sigma\boldsymbol z&amp; = \boldsymbol z^T \text{E}\left(  (\boldsymbol x-\boldsymbol \mu) (\boldsymbol x-\boldsymbol \mu)^T  \right) \boldsymbol z\\
&amp; =  \text{E}\left( \boldsymbol z^T (\boldsymbol x-\boldsymbol \mu) (\boldsymbol x-\boldsymbol \mu)^T \boldsymbol z\right) \\
&amp; =  \text{E}\left( \left( \boldsymbol z^T (\boldsymbol x-\boldsymbol \mu) \right)^2 \right) \geq 0 \, .\\
\end{split}
\]</span> Furthermore, with <span class="math inline">\(\boldsymbol y= \boldsymbol U^T \boldsymbol z\)</span> we get <span class="math display">\[
\begin{split}
\boldsymbol z^T  \boldsymbol \Sigma\boldsymbol z&amp; =  \boldsymbol z^T\boldsymbol U\boldsymbol \Lambda\boldsymbol U^T \boldsymbol z\\
                      &amp; =  \boldsymbol y^T \boldsymbol \Lambda\boldsymbol y= \sum_{i=1}^d  y_i^2 \lambda_i \\
\end{split}
\]</span> and hence all the <span class="math inline">\(\lambda_i \geq 0\)</span>. Therefore the <strong>covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> is always positive semi-definite</strong>.</p>
<p>In fact, <strong>unless there is collinearity</strong> ( i.e.&nbsp;a variable is a linear function the other variables) all eigenvalues will be positive and <strong><span class="math inline">\(\boldsymbol \Sigma\)</span> is positive definite</strong>.</p>
</section>
<section id="joint-covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="joint-covariance-matrix">Joint covariance matrix</h3>
<p>Assume we have random vector <span class="math inline">\(\boldsymbol z\)</span> with mean <span class="math inline">\(\text{E}(\boldsymbol z) = \boldsymbol \mu_{\boldsymbol z}\)</span> and covariance matrix <span class="math inline">\(\text{Var}(\boldsymbol z) = \boldsymbol \Sigma_{\boldsymbol z}\)</span>.</p>
<p>Often it makes sense to partion the components of <span class="math inline">\(\boldsymbol z\)</span> into two groups <span class="math display">\[
\boldsymbol z= \begin{pmatrix} \boldsymbol x\\ \boldsymbol y\end{pmatrix}
\]</span> This induces a corresponding partition in the expectation <span class="math display">\[
\boldsymbol \mu_{\boldsymbol z} =  \begin{pmatrix} \boldsymbol \mu_{\boldsymbol x} \\ \boldsymbol \mu_{\boldsymbol y} \end{pmatrix}
\]</span> where <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \mu_{\boldsymbol x}\)</span> and <span class="math inline">\(\text{E}(\boldsymbol y) = \boldsymbol \mu_{\boldsymbol y}\)</span>.</p>
<p>Furthermore, the joint covariance matrix for <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> can then be written as <span class="math display">\[
\boldsymbol \Sigma_{\boldsymbol z} =
\begin{pmatrix}
\boldsymbol \Sigma_{\boldsymbol x} &amp;  \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} \\
\boldsymbol \Sigma_{\boldsymbol y\boldsymbol x} &amp; \boldsymbol \Sigma_{\boldsymbol y} \\
\end{pmatrix}
\]</span> It contains the within-group group covariance matrices <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol y}\)</span> as diagonal elements and the cross-covariance matrix <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} = \boldsymbol \Sigma_{\boldsymbol y\boldsymbol x}^T\)</span> as off-diagonal element.</p>
<p>Note that the cross-covariance matrix <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y}\)</span> is rectangular and not symmetric. We also write <span class="math inline">\(\text{Cov}(\boldsymbol x, \boldsymbol y) = \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y}\)</span> and we can define cross-covariance directly by <span class="math display">\[
\text{Cov}(\boldsymbol x, \boldsymbol y) = \text{E}\left( (\boldsymbol x- \boldsymbol \mu_{\boldsymbol x}) ( \boldsymbol y- \boldsymbol \mu_{\boldsymbol y} )^T \right) = \text{E}(\boldsymbol x\boldsymbol y^T)-\boldsymbol \mu_{\boldsymbol x} \boldsymbol \mu_{\boldsymbol y}^T
\]</span></p>
</section>
<section id="quantities-related-to-the-covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="quantities-related-to-the-covariance-matrix">Quantities related to the covariance matrix</h3>
<section id="correlation-matrix-boldsymbol-p" class="level4">
<h4 class="anchored" data-anchor-id="correlation-matrix-boldsymbol-p">Correlation matrix <span class="math inline">\(\boldsymbol P\)</span></h4>
<p>The correlation matrix <span class="math inline">\(\boldsymbol P\)</span> (= upper case greek “rho”) is the standardised covariance matrix</p>
<p><span class="math display">\[\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}=\text{Cor}(x_i,x_j)\]</span></p>
<p><span class="math display">\[\rho_{ii} = 1 = \text{Cor}(x_i,x_i)\]</span></p>
<p><span class="math display">\[ \boldsymbol P= (\rho_{ij}) = \begin{pmatrix}
    1 &amp; \dots &amp; \rho_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \rho_{d1} &amp; \dots &amp; 1
\end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol P\)</span> (“upper case rho”) is a symmetric matrix (<span class="math inline">\(\rho_{ij}=\rho_{ji}\)</span>).</p>
<p>Note the <strong>variance-correlation decomposition</strong></p>
<p><span class="math display">\[\boldsymbol \Sigma= \boldsymbol V^{\frac{1}{2}} \boldsymbol P\boldsymbol V^{\frac{1}{2}}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol V\)</span> is a diagonal matrix containing the variances:</p>
<p><span class="math display">\[ \boldsymbol V= \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}\]</span></p>
<p><span class="math display">\[\boldsymbol P= \boldsymbol V^{-\frac{1}{2}}\boldsymbol \Sigma\boldsymbol V^{-\frac{1}{2}}\]</span></p>
<p>This is the definition of correlation written in matrix notation.</p>
<p>As with the covariance matrix, in many applications it makes sense to partition a joint correlation matrix <span class="math display">\[
\boldsymbol P_{\boldsymbol z} =
\begin{pmatrix}
\boldsymbol P_{\boldsymbol x} &amp;  \boldsymbol P_{\boldsymbol x\boldsymbol y} \\
\boldsymbol P_{\boldsymbol y\boldsymbol x} &amp; \boldsymbol P_{\boldsymbol y} \\
\end{pmatrix}
\]</span> into within-group group correlation matrices <span class="math display">\[
\boldsymbol P_{\boldsymbol x} = \boldsymbol V_{\boldsymbol x}^{-\frac{1}{2}} \boldsymbol \Sigma_{\boldsymbol x} \boldsymbol V_{\boldsymbol x}^{-\frac{1}{2}}
\]</span> and <span class="math display">\[
\boldsymbol P_{\boldsymbol y} = \boldsymbol V_{\boldsymbol y}^{-\frac{1}{2}} \boldsymbol \Sigma_{\boldsymbol y} \boldsymbol V_{\boldsymbol y}^{-\frac{1}{2}}
\]</span> and the cross-correlation matrix <span class="math display">\[
\boldsymbol P_{\boldsymbol x\boldsymbol y} = \boldsymbol V_{\boldsymbol x}^{-\frac{1}{2}} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} \boldsymbol V_{\boldsymbol y}^{-\frac{1}{2}}
\]</span> with <span class="math display">\[
\boldsymbol P_{\boldsymbol y\boldsymbol x} = \boldsymbol P_{\boldsymbol x\boldsymbol y}^T
= \boldsymbol V_{\boldsymbol y}^{-\frac{1}{2}} \boldsymbol \Sigma_{\boldsymbol y\boldsymbol x} \boldsymbol V_{\boldsymbol x}^{-\frac{1}{2}} \,.
\]</span></p>
</section>
<section id="precision-matrix-or-concentration-matrix" class="level4">
<h4 class="anchored" data-anchor-id="precision-matrix-or-concentration-matrix">Precision matrix or concentration matrix</h4>
<p><span class="math display">\[\boldsymbol \Omega= (\omega_{ij}) = \boldsymbol \Sigma^{-1}\]</span></p>
<p><span class="math inline">\(\boldsymbol \Omega\)</span> (“Omega”) is the inverse of the covariance matrix.</p>
<p>The inverse of the covariance matrix can be obtained via the spectral decomposition, followed by inverting the eigenvalues <span class="math inline">\(\lambda_i\)</span>: <span class="math display">\[\boldsymbol \Sigma^{-1} = \boldsymbol U\boldsymbol \Lambda^{-1} \boldsymbol U^T =
\boldsymbol U\begin{pmatrix}
    \lambda_{1}^{-1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}^{-1}
\end{pmatrix} \boldsymbol U^T \]</span></p>
<p>Note that <strong>all eigenvalues <span class="math inline">\(\lambda_i\)</span> need to be positive so that <span class="math inline">\(\boldsymbol \Sigma\)</span> can be inverted.</strong> (i.e., <span class="math inline">\(\boldsymbol \Sigma\)</span> needs to be positive definite).<br>
If any <span class="math inline">\(\lambda_i = 0\)</span> then <span class="math inline">\(\boldsymbol \Sigma\)</span> is singular and not invertible.</p>
<p>Importance of <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span>:</p>
<ul>
<li>Many expressions in multivariate statistics contain <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> and not <span class="math inline">\(\boldsymbol \Sigma\)</span>.</li>
<li><span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> has close connection with graphical models (e.g.&nbsp;conditional independence graph, partial correlations).</li>
<li><span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> is a natural parameter from an exponential family perspective.</li>
</ul>
</section>
<section id="partial-correlation-matrix" class="level4">
<h4 class="anchored" data-anchor-id="partial-correlation-matrix">Partial correlation matrix</h4>
<p>This is a standardised version of the precision matrix, see later chapter on graphical models.</p>
</section>
<section id="total-variation-and-generalised-variance" class="level4">
<h4 class="anchored" data-anchor-id="total-variation-and-generalised-variance">Total variation and generalised variance</h4>
<p>To summarise the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> in a single scalar value there are two commonly used measures:</p>
<ul>
<li><strong>total variation</strong>: <span class="math inline">\(\text{Tr}(\boldsymbol \Sigma) = \sum_{i=1}^d \lambda_i\)</span></li>
<li><strong>generalised variance</strong>: <span class="math inline">\(\det(\boldsymbol \Sigma) = \prod_{i=1}^d \lambda_i\)</span></li>
</ul>
<p>The generalised variance <span class="math inline">\(\det(\boldsymbol \Sigma)\)</span> is also known as the volume of <span class="math inline">\(\boldsymbol \Sigma\)</span>.</p>
</section>
</section>
</section>
<section id="multivariate-distributions" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="multivariate-distributions"><span class="header-section-number">1.2</span> Multivariate distributions</h2>
<section id="common-distributions" class="level3">
<h3 class="anchored" data-anchor-id="common-distributions">Common distributions</h3>
<p>In multivariate statistics we make use of multivariate distributions. These are typically generalisations of corresponding univariate distribution.</p>
<p>Among the most commonly used multivariate distributions are:</p>
<ul>
<li>The <strong>multivariate normal distribution</strong> <span class="math inline">\(N_d(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> as a generalisation of univariate normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span></li>
<li>The <strong>categorical distribution</strong> <span class="math inline">\(\text{Cat}(\boldsymbol \pi)\)</span> as a generalisation of the Bernoulli distribution <span class="math inline">\(\text{Ber}(\theta)\)</span></li>
<li>The <strong>multinomial distribution</strong> <span class="math inline">\(\text{Mult}(n, \boldsymbol \pi)\)</span> as a generalisation of binomial distribution <span class="math inline">\(\text{Bin}(n, \theta)\)</span></li>
</ul>
<p>The above distribution have already been introduced earlier in <a href="https://strimmerlab.github.io/publications/lecture-notes/MATH27720-stats2/">MATH27720 Statistics 2</a>.</p>
<p>Conceptually, these multivariate generalisation work behave exactly the same as their univariate counterparts and are employed in the same settings.</p>
</section>
<section id="further-multivariate-distributions" class="level3">
<h3 class="anchored" data-anchor-id="further-multivariate-distributions">Further multivariate distributions</h3>
<p>For multivariate Bayesian analyis we also need to consider a number of further multivariate distributions:</p>
<ul>
<li>The <strong>Dirichlet distribution</strong> <span class="math inline">\(\text{Dir}(\boldsymbol \alpha)\)</span> as the generalisation of the beta distribution <span class="math inline">\(\text{Beta}(\alpha, \beta)\)</span>,</li>
<li>The <strong>Wishart distribution</strong> as the generalisation of the gamma distribution <span class="math inline">\(\text{Gam}(\alpha, \theta)\)</span>,</li>
<li>The <strong>inverse Wishart distribution</strong> as the generalisation of the inverse gamma distribution <span class="math inline">\(\text{IG}(\alpha, \beta)\)</span>.</li>
</ul>
<p>For technical details of the densities etc. of the multivariate distribution families we refer to the supplementary <a href="https://strimmerlab.github.io/publications/lecture-notes/probability-distribution-refresher/index.html">Probability and Distribution refresher notes</a>.</p>
</section>
</section>
<section id="multivariate-normal-distribution" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="multivariate-normal-distribution"><span class="header-section-number">1.3</span> Multivariate normal distribution</h2>
<p>The multivariate normal disribution is ubiquitous in multivariate statistics and hence it is important to discuss it in more detail.</p>
<p>The multivariate normal model is a generalisation of the univariate normal distribution from dimension 1 to dimension <span class="math inline">\(d\)</span>.</p>
<section id="univariate-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="univariate-normal-distribution">Univariate normal distribution:</h3>
<p><span class="math display">\[\text{Dimension } d = 1\]</span> <span class="math display">\[x \sim N(\mu, \sigma^2)\]</span> <span class="math display">\[\text{E}(x) = \mu \space , \space  \text{Var}(x) = \sigma^2\]</span></p>
<p><strong>Probability Density Function</strong>:</p>
<p><span class="math display">\[f(x |\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right) \]</span></p>
<p><strong>Plot of univariate normal density </strong></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-normaldens" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normaldens-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-multivariate_files/figure-html/fig-normaldens-1.png" class="img-fluid figure-img" data-fig-pos="t" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normaldens-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Illustration of the density of the normal distribution.
</figcaption>
</figure>
</div>
</div>
</div>
<p>See <a href="#fig-normaldens" class="quarto-xref">Figure&nbsp;<span>1.1</span></a>. The density is unimodal with a mode at <span class="math inline">\(\mu\)</span> and width determined by <span class="math inline">\(\sigma\)</span> (in this plot: <span class="math inline">\(\mu=2, \sigma^2=1\)</span> )</p>
<p>Special case: <strong>standard normal</strong> with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=1\)</span>:</p>
<p><span class="math display">\[f(x |\mu=0,\sigma^2=1)=\frac{1}{\sqrt{2\pi}} \exp\left( {-\frac{x^2}{2}} \right) \]</span></p>
<p><strong>Differential entropy</strong>:<br>
<span class="math display">\[
H(F) = \frac{1}{2} (\log(2 \pi \sigma^2) + 1)
\]</span></p>
<p><strong>Cross-entropy</strong>:<br>
<span class="math display">\[
H(F_{\text{ref}}, F) = \frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 }
+\frac{\sigma^2_{\text{ref}}}{\sigma^2}  +\log(2 \pi \sigma^2) \right)
\]</span> <strong>KL divergence</strong>:<br>
<span class="math display">\[
D_{\text{KL}}(F_{\text{ref}}, F) = H(F_{\text{ref}}, F) - H(F_{\text{ref}}) =
\frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 }
+\frac{\sigma^2_{\text{ref}}}{\sigma^2}  -\log\left(\frac{\sigma^2_{\text{ref}}}{ \sigma^2}\right) -1
\right)
\]</span></p>
<p><strong>Maximum entropy characterisation:</strong> the normal distribution is the unique distribution that has the highest (differential) entropy over all continuous distributions with support from minus infinity to plus infinity with a given mean and variance.</p>
<p>This is in fact one of the reasons why the normal distribution is so important (und useful) – if we only know that a random variable has a mean and variance, and not much else, then using the normal distribution will be a reasonable and well justified model.</p>
</section>
<section id="multivariate-normal-model" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-normal-model">Multivariate normal model</h3>
<p><span class="math display">\[\text{Dimension } d\]</span> <span class="math display">\[\boldsymbol x\sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\]</span> <span class="math display">\[\text{E}(\boldsymbol x) = \boldsymbol \mu\space , \space  \text{Var}(\boldsymbol x) = \boldsymbol \Sigma\]</span></p>
<p><strong>Density</strong>:</p>
<p><span class="math display">\[f(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma) = \det(2 \pi \boldsymbol \Sigma)^{-\frac{1}{2}} \exp\left({{-\frac{1}{2}} \underbrace{\underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1 \times d} \underbrace{\boldsymbol \Sigma^{-1}}_{d \times d} \underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d \times 1} }_{1 \times 1 = \text{scalar!}}}\right)\]</span></p>
<ul>
<li>the density contains the precision matrix <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span></li>
<li>to invert the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> we need to invert its eigenvalues <span class="math inline">\(\lambda_i\)</span> (hence we require that all <span class="math inline">\(\lambda_i &gt; 0\)</span>)</li>
<li>the density also contains <span class="math inline">\(\det(\boldsymbol \Sigma) = \prod\limits_{i=1}^d \lambda_i\)</span> <span class="math inline">\(\equiv\)</span> product of the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span></li>
<li>note that <span class="math inline">\(\det(2 \pi \boldsymbol \Sigma)^{-\frac{1}{2}} = \det(2 \pi \boldsymbol I_d)^{-\frac{1}{2}} \det(\boldsymbol \Sigma)^{-\frac{1}{2}} =  (2 \pi)^{-d/2} \det(\boldsymbol \Sigma)^{-\frac{1}{2}}\)</span></li>
</ul>
<p>Special case: <strong>standard multivariate normal</strong> with <span class="math display">\[\boldsymbol \mu=\mathbf 0, \boldsymbol \Sigma=\boldsymbol I=\begin{pmatrix}
    1 &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; 1
\end{pmatrix}\]</span></p>
<p><span class="math display">\[f(\boldsymbol x| \boldsymbol \mu=\mathbf 0,\boldsymbol \Sigma=\boldsymbol I)=(2\pi)^{-d/2}\exp\left( -\frac{1}{2} \boldsymbol x^T \boldsymbol x\right) = \prod\limits_{i=1}^d \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x_i^2}{2}\right)\]</span> which is equivalent to the product of <span class="math inline">\(d\)</span> univariate standard normals!</p>
<p><strong>Misc:</strong></p>
<ul>
<li>for <span class="math inline">\(d=1\)</span>, the multivariate normal density reduces to the univariate normal density.</li>
<li>for <span class="math inline">\(\boldsymbol \Sigma\)</span> diagonal (i.e.&nbsp;<span class="math inline">\(\boldsymbol P= \boldsymbol I\)</span>, no correlation), the multivariate normal density is the product of univariate normal densities (see Worksheet 2).</li>
</ul>
<p><strong>Plot of the multivariate normal density</strong>:</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-bivarnormal" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="p">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bivarnormal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-multivariate_files/figure-html/fig-bivarnormal-1.png" class="img-fluid figure-img" data-fig-pos="p" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bivarnormal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.2: Illustration of the density of the bivariate normal distribution.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-bivarnormal" class="quarto-xref">Figure&nbsp;<span>1.2</span></a> illustrates the bivariate normal distribution, with <strong>location</strong> determined by <span class="math inline">\(\boldsymbol \mu\)</span>, <strong>shape</strong> determined by <span class="math inline">\(\boldsymbol \Sigma\)</span> and a single mode. The support ranges from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span> in each dimension.</p>
<p>An interactive <a href="https://shiny.rstudio.com/">R Shiny web app</a> of the bivariate normal density plot is available online at <a href="https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/" class="uri">https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/</a>.</p>
<p><strong>Differential entropy</strong>:<br>
<span class="math display">\[
H = \frac{1}{2} (\log \det(2 \pi \boldsymbol \Sigma) + d)
\]</span></p>
<p><strong>Cross-entropy</strong>:<br>
<span class="math display">\[
H(F_{\text{ref}}, F) = \frac{1}{2}   \biggl\{
        (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})
      + \text{Tr}\biggl(\boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
    + \log \det \biggl( 2 \pi \boldsymbol \Sigma\biggr)    \biggr\}
\]</span> <strong>KL divergence</strong>:<br>
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\text{ref}}, F) &amp;= H(F_{\text{ref}}, F) - H(F_{\text{ref}}) \\
&amp;= \frac{1}{2}   \biggl\{
        (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})
      + \text{Tr}\biggl(\boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
    - \log \det \biggl( \boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
     - d   \biggr\} \\
\end{split}
\]</span></p>
</section>
<section id="shape-of-the-multivariate-normal-density" class="level3">
<h3 class="anchored" data-anchor-id="shape-of-the-multivariate-normal-density">Shape of the multivariate normal density</h3>
<p>Now we show that the contour lines of the multivariate normal density always take on the form of an ellipse, and that the radii and orientation of the ellipse is determined by the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span>.</p>
<p>We start by observing that a circle with radius <span class="math inline">\(r\)</span> around the origin can be described as the set of points <span class="math inline">\((x_1,x_2)\)</span> satisfying <span class="math inline">\(x_1^2+x_2^2 = r^2\)</span>, or equivalently, <span class="math inline">\(\frac{x_1^2}{r^2} + \frac{x_2^2}{r^2} = 1\)</span>. This is generalised to the shape of an ellipse by allowing (in two dimensions) for two radii <span class="math inline">\(r_1\)</span> and <span class="math inline">\(r_2\)</span> with <span class="math inline">\(\frac{x_1^2}{r_1^2} + \frac{x_2^2}{r_2^2} = 1\)</span>, or in vector notation <span class="math inline">\(\boldsymbol x^T \text{Diag}(r_1^2, r_2^2)^{-1} \boldsymbol x= 1\)</span>. Here two axes of the ellipse are parallel to the two coordinate axes.</p>
<p>In <span class="math inline">\(d\)</span> dimensions and allowing for rotation of the axes and a shift of the origin from 0 to <span class="math inline">\(\boldsymbol \mu\)</span> the condition for an ellipse is <span class="math display">\[(\boldsymbol x-\boldsymbol \mu)^T \boldsymbol Q\, \text{Diag}(r_1^2, \ldots , r_d^2)^{-1} \boldsymbol Q^T (\boldsymbol x-\boldsymbol \mu) = 1\]</span> where <span class="math inline">\(\boldsymbol Q\)</span> is an orthogonal matrix whose column vectors indicate the direction of the axes. These are also called the <strong>principal axes</strong> of the ellipse, and by construction all <span class="math inline">\(d\)</span> principal axes are <strong>perpendicular</strong> to each other.</p>
<p>A contour line of a probability density function is a set of connected points where the density assumes the same constant value. In the case of the multivariate normal distribution keeping the density <span class="math inline">\(f(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma)\)</span> at some fixed value implies that <span class="math inline">\((\boldsymbol x-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu) = c\)</span> where <span class="math inline">\(c\)</span> is a constant. Using the eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span> we can rewrite this condition as <span class="math display">\[
(\boldsymbol x-\boldsymbol \mu)^T \boldsymbol U\boldsymbol \Lambda^{-1} \boldsymbol U^T (\boldsymbol x-\boldsymbol \mu) = c \,.
\]</span> This implies that</p>
<ol type="i">
<li>the contour lines of the multivariate normal density are indeed ellipses,</li>
<li>the direction of the principal axes of the ellipse are given correspond to the colum vectors in <span class="math inline">\(\boldsymbol U\)</span> (i.e.&nbsp;the eigenvectors of <span class="math inline">\(\boldsymbol \Sigma\)</span>), and</li>
<li>the squared radii of the ellipse are proportional to the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span> Equivalently, the positive square roots of the eigenvalues are proportional to the radii of the ellipse. Hence, for a singular covariance matrix with one or more <span class="math inline">\(\lambda_i=0\)</span> the corresponding radii are zero.</li>
</ol>
<p>An interactive <a href="https://shiny.rstudio.com/">R Shiny web app</a> to play with the contour lines of the bivariate normal distribution is available online at <a href="https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/" class="uri">https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/</a>.</p>
</section>
<section id="three-types-of-covariances" class="level3">
<h3 class="anchored" data-anchor-id="three-types-of-covariances">Three types of covariances</h3>
<p>Following the above we can parametrise a covariance matrix in terms of its i) volume, ii) shape and iii) orientation by writing <span class="math display">\[
\boldsymbol \Sigma= \kappa \, \boldsymbol U\boldsymbol A\boldsymbol U^T = \boldsymbol U\; \left(\kappa \boldsymbol A\right) \; \boldsymbol U^T
\]</span> with <span class="math inline">\(\boldsymbol A=\text{Diag}(a_1, \ldots, a_d)\)</span> and <span class="math inline">\(\det(\boldsymbol A) = \prod_{i=1}^d a_i = 1\)</span>. Note that in this parametrisation the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span> are <span class="math inline">\(\lambda_i = \kappa a_i\)</span>.</p>
<ol type="i">
<li>The <strong>volume</strong> is <span class="math inline">\(\det(\boldsymbol \Sigma) = \kappa^d\)</span>, determined by a single parameter <span class="math inline">\(\kappa\)</span>. This parameter can be interpreted as the length of the side of a <span class="math inline">\(d\)</span>-dimensional hypercube.</li>
<li>The <strong>shape</strong> is determined by the diagonal matrix <span class="math inline">\(\boldsymbol A\)</span> with <span class="math inline">\(d-1\)</span> free parameters. Note that there are only <span class="math inline">\(d-1\)</span> and not <span class="math inline">\(d\)</span> free parameters because of the constraint <span class="math inline">\(\det(\boldsymbol A) = 1\)</span>.</li>
<li>The <strong>orientation</strong> is given by the orthogonal matrix <span class="math inline">\(\boldsymbol U\)</span>, with <span class="math inline">\(d (d-1)/2\)</span> free parameters.</li>
</ol>
<p>This leads to classification of covariances into three varieties:</p>
<p><strong>Type 1:</strong> <strong>spherical covariance</strong> <span class="math inline">\(\boldsymbol \Sigma=\kappa \boldsymbol I\)</span>, with spherical contour lines, 1 free parameter (<span class="math inline">\(\boldsymbol A=\boldsymbol I\)</span>, <span class="math inline">\(\boldsymbol U=\boldsymbol I\)</span>).</p>
<p>Example (<a href="#fig-sphcov" class="quarto-xref">Figure&nbsp;<span>1.3</span></a>): <span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix}
2 &amp; 0 \\
0 &amp; 2
\end{pmatrix}\)</span> with <span class="math inline">\(\sqrt{\lambda_1/ \lambda_2} = 1\)</span>:</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-sphcov" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="h">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sphcov-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-multivariate_files/figure-html/fig-sphcov-1.png" class="img-fluid figure-img" data-fig-pos="h" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sphcov-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.3: Contor lines for a spherical covariance matrix.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Type 2</strong>: <strong>diagonal covariance</strong> <span class="math inline">\(\boldsymbol \Sigma= \kappa \boldsymbol A\)</span>, with elliptical contour lines and the principal axes of the ellipse oriented parallel to the coordinate axes, <span class="math inline">\(d\)</span> free parameters (<span class="math inline">\(\boldsymbol U=\boldsymbol I\)</span>).</p>
<p>Example (<a href="#fig-diagcov" class="quarto-xref">Figure&nbsp;<span>1.4</span></a>): <span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix}
1 &amp; 0 \\
0 &amp; 2
\end{pmatrix}\)</span> with <span class="math inline">\(\sqrt{\lambda_1 / \lambda_2} \approx 1.41\)</span>:</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-diagcov" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="h">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diagcov-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-multivariate_files/figure-html/fig-diagcov-1.png" class="img-fluid figure-img" data-fig-pos="h" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diagcov-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.4: Contour lines for a diagonal covariance matrix.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Type 3</strong>: <strong>general unrestricted covariance</strong> <span class="math inline">\(\boldsymbol \Sigma\)</span>, with elliptical contour lines, with the principal axes of the ellipse oriented according to the column vectors in <span class="math inline">\(\boldsymbol U\)</span>, <span class="math inline">\(d (d+1)/2\)</span> free parameters.</p>
<p>Example (<a href="#fig-gencov" class="quarto-xref">Figure&nbsp;<span>1.5</span></a>): <span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix}
2 &amp; 0.6 \\
0.6 &amp; 1
\end{pmatrix}\)</span> with <span class="math inline">\(\sqrt{\lambda_1 / \lambda_2} \approx 2.20\)</span>:</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-gencov" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="h">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gencov-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-multivariate_files/figure-html/fig-gencov-1.png" class="img-fluid figure-img" data-fig-pos="h" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gencov-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.5: Contour lines for a general covariance matrix.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="concentration-of-probability-mass-for-small-and-large-dimension" class="level3">
<h3 class="anchored" data-anchor-id="concentration-of-probability-mass-for-small-and-large-dimension">Concentration of probability mass for small and large dimension</h3>
<p>The density of the multivariate normal distribution has a bell shape with a single mode. Intuitively, we may assume that most of the probability mass is always concentrated around this mode, as it is in the univariate case (<span class="math inline">\(d=1\)</span>). While this is still true for small dimensions (small <span class="math inline">\(d\)</span>) we now show that this intuition is incorrect for high dimensions (large <span class="math inline">\(d\)</span>).</p>
<p>For simplicity we consider the standard multivariate normal distribution with dimension <span class="math inline">\(d\)</span> <span class="math display">\[\boldsymbol x\sim N_d(\mathbf 0, \boldsymbol I_d)\]</span> with a spherical covariance <span class="math inline">\(\boldsymbol I_d\)</span> and sample <span class="math inline">\(\boldsymbol x\)</span>. The squared Euclidean length of <span class="math inline">\(\boldsymbol x\)</span> is <span class="math inline">\(r^2= || \boldsymbol x||^2 = \boldsymbol x^T \boldsymbol x= \sum_{i=1}^d x_i^2\)</span>. The corresponding density of the <span class="math inline">\(d\)</span>-dimensional standard multivariate normal distribution is <span class="math display">\[
g_d(\boldsymbol x) = (2\pi)^{-d/2} e^{-\boldsymbol x^T \boldsymbol x/2}
\]</span> A natural way to define the main part of the “bell” of the standard multivariate normal as the set of all <span class="math inline">\(\boldsymbol x\)</span> for which the density is larger than a specified fraction <span class="math inline">\(\eta\)</span> (say 0.001) of the maximum value of the density <span class="math inline">\(g_d(0)\)</span> at the peak at zero. To formalise <span class="math display">\[
B = \left\{ \boldsymbol x: \frac{g_d(\boldsymbol x)}{ g_d(0)} &gt; \eta    \right\}
\]</span> which can be equivalently written as the set <span class="math display">\[
B = \{ \boldsymbol x: \boldsymbol x^T \boldsymbol x= r^2 &lt; -2 \log(\eta) = r^2_{\max} \}
\]</span></p>
<p>Each individual component in the sample <span class="math inline">\(\boldsymbol x\)</span> is independently distributed as <span class="math inline">\(x_i \sim N(0,1)\)</span>, hence <span class="math inline">\(r^2 \sim \text{$\chi^2_{d}$}\)</span> is chi-squared distributed with degree of freedom <span class="math inline">\(d\)</span>. The probability <span class="math inline">\(\text{Pr}(\boldsymbol x\in B)\)</span> can thus be obtained as the value of the cumulative density function of a chi-squared distribution with <span class="math inline">\(d\)</span> degrees of freedom at <span class="math inline">\(r^2_{\max}\)</span>. Computing this probability for fixed <span class="math inline">\(\eta\)</span> as a function of the dimension <span class="math inline">\(d\)</span> we obtain the curve shown in <a href="#fig-probmass" class="quarto-xref">Figure&nbsp;<span>1.6</span></a>. In this plot we have used <span class="math inline">\(\eta=0.001\)</span>. You can see that for dimensions up to around <span class="math inline">\(d=10\)</span> the probability mass is indeed concentrated in the center of the distribution but from <span class="math inline">\(d=30\)</span> onwards it has moved completely to the tails.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-probmass" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="b">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-probmass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-multivariate_files/figure-html/fig-probmass-1.png" class="img-fluid figure-img" data-fig-pos="b" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-probmass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.6: Concentration of probability mass in the center of a multivariate normal distribution.
</figcaption>
</figure>
</div>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-MKB1979" class="csl-entry" role="listitem">
Mardia, K. V., J. T. Kent, and J. M. Bibby. 1979. <em>Multivariate Analysis</em>. Academic Press.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra">Specialised matrix algorithms improve this</a> to about <span class="math inline">\(O(d^{2.373})\)</span>. Matrices with special symmetries (e.g.&nbsp;diagonal and block diagonal matrices) or particular properties (e.g.&nbsp;orthogonal matrix) can also be inverted much easier.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>A brief summary of eigenvalue decompositon is found in the supplementary <a href="https://strimmerlab.github.io/publications/lecture-notes/matrix-calculus-refresher/index.html">Matrix and Calculus Refresher notes</a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>An orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> satisfies <span class="math inline">\(\boldsymbol Q^T \boldsymbol Q= \boldsymbol I\)</span>, <span class="math inline">\(\boldsymbol Q\boldsymbol Q^T = \boldsymbol I\)</span> and <span class="math inline">\(\boldsymbol Q^{-1} = \boldsymbol Q^T\)</span> and is also called rotation-reflection matrix. We will make frequent use of orthogonal matrices so this might be a good time to revisit their properties, see e.g.&nbsp;the <a href="https://strimmerlab.github.io/publications/lecture-notes/matrix-calculus-refresher/index.html">Matrix and Calculus Refresher notes</a>..<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH38161");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./00-preface.html" class="pagination-link" aria-label="Preface">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./02-estimation.html" class="pagination-link" aria-label="Multivariate estimation">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multivariate estimation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>