<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>1 Multivariate random variables | Multivariate Statistics and Machine Learning</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.38 with bs4_book()">
<meta property="og:title" content="1 Multivariate random variables | Multivariate Statistics and Machine Learning">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Multivariate random variables | Multivariate Statistics and Machine Learning">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<meta name="description" content="1.1 Essentials in multivariate statistics  1.1.1 Why multivariate statistics? In science we use experiments to learn about underlying mechanisms of interest, both deterministic and stochastic, to...">
<meta property="og:description" content="1.1 Essentials in multivariate statistics  1.1.1 Why multivariate statistics? In science we use experiments to learn about underlying mechanisms of interest, both deterministic and stochastic, to...">
<meta name="twitter:description" content="1.1 Essentials in multivariate statistics  1.1.1 Why multivariate statistics? In science we use experiments to learn about underlying mechanisms of interest, both deterministic and stochastic, to...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Multivariate Statistics and Machine Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="active" href="multivariate-random-variables.html"><span class="header-section-number">1</span> Multivariate random variables</a></li>
<li><a class="" href="multivariate-estimation-in-large-sample-and-small-sample-settings.html"><span class="header-section-number">2</span> Multivariate estimation in large sample and small sample settings</a></li>
<li><a class="" href="transformations-and-dimension-reduction.html"><span class="header-section-number">3</span> Transformations and dimension reduction</a></li>
<li><a class="" href="unsupervised-learning-and-clustering.html"><span class="header-section-number">4</span> Unsupervised learning and clustering</a></li>
<li><a class="" href="supervised-learning-and-classification.html"><span class="header-section-number">5</span> Supervised learning and classification</a></li>
<li><a class="" href="multivariate-dependencies.html"><span class="header-section-number">6</span> Multivariate dependencies</a></li>
<li><a class="" href="nonlinear-and-nonparametric-models.html"><span class="header-section-number">7</span> Nonlinear and nonparametric models</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="further-study.html"><span class="header-section-number">A</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="multivariate-random-variables" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Multivariate random variables<a class="anchor" aria-label="anchor" href="#multivariate-random-variables"><i class="fas fa-link"></i></a>
</h1>
<div id="essentials-in-multivariate-statistics" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Essentials in multivariate statistics<a class="anchor" aria-label="anchor" href="#essentials-in-multivariate-statistics"><i class="fas fa-link"></i></a>
</h2>
<div id="why-multivariate-statistics" class="section level3" number="1.1.1">
<h3>
<span class="header-section-number">1.1.1</span> Why multivariate statistics?<a class="anchor" aria-label="anchor" href="#why-multivariate-statistics"><i class="fas fa-link"></i></a>
</h3>
<p>In science we use experiments to learn about underlying mechanisms of interest, both deterministic and stochastic, to compare different models and to verify or reject hypotheses about the world.
Statistics provides tools to quantify this procedure and offers methods to
link data (experiments) with probabilistic models (hypotheses).</p>
<p>In univariate statistics with we use relatively simple approaches based on a single random variable
or single parameter. However, in practise we often have to consider multiple random variables and multiple parameters, so we need more complex models and also be able to deal with more complex data. Hence, the need for multivariate statistical approaches and models.</p>
<p>Specifically, multivariate statistics is concerned with methods and models for <strong>random vectors</strong> and <strong>random matrices</strong>, rather than just random univariate (scalar) variables. Therefore, in multivariate statistics we will frequently make use of matrix notation.</p>
<p>Closely related to multivariate statistics (traditionally a subfield of statistics) is machine learning (ML) which is traditionally a subfield of computer science. ML used to focus more on algorithms rather on probabilistic modelling but nowadays most machine learning methods are fully based on statistical multivariate approaches, so the two fields are converging.</p>
<p>Multivariate models provide a means to learn dependencies and interactions among the
components of the random variables which in turn allow us to draw conclusion about underlying mechanisms of interest (e.g. in biological or medical problems).</p>
<p>Two main tasks:</p>
<ul>
<li>unsupervised learning (finding structure, clustering)</li>
<li>supervised learning (training from labelled data, followed by prediction)</li>
</ul>
<p>Challenges:</p>
<ul>
<li>complexity of model needs to be appropriate for problem and available data</li>
<li>high dimensions make estimation and inference difficult</li>
<li>computational issues</li>
</ul>
</div>
<div id="univariate-vs.-multivariate-random-variables" class="section level3" number="1.1.2">
<h3>
<span class="header-section-number">1.1.2</span> Univariate vs. multivariate random variables<a class="anchor" aria-label="anchor" href="#univariate-vs.-multivariate-random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>Univariate random variable (dimension <span class="math inline">\(d=1\)</span>):
<span class="math display">\[x \sim F\]</span>
where <span class="math inline">\(x\)</span> is a <strong>scalar</strong> and <span class="math inline">\(F\)</span> is the distribution.
<span class="math inline">\(\text{E}(x) = \mu\)</span> denotes the mean and <span class="math inline">\(\text{Var}(x) = \sigma^2\)</span> the variance of <span class="math inline">\(x\)</span>.</p>
<p>Multivariate random <strong>vector</strong> of dimension <span class="math inline">\(d\)</span>:
<span class="math display">\[\boldsymbol x= (x_1, x_2,...,x_d)^T  \sim F\]</span></p>
<p><span class="math inline">\(\boldsymbol x\)</span> is <strong>vector</strong> valued random variable.</p>
<p>The vector <span class="math inline">\(\boldsymbol x\)</span> is column vector (=matrix of size <span class="math inline">\(d \times 1\)</span>).
Its components <span class="math inline">\(x_1, x_2,...,x_d\)</span> are univariate random variables.
The dimension <span class="math inline">\(d\)</span> is also often denoted by <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span>.</p>
</div>
<div id="multivariate-data" class="section level3" number="1.1.3">
<h3>
<span class="header-section-number">1.1.3</span> Multivariate data<a class="anchor" aria-label="anchor" href="#multivariate-data"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Vector notation:</strong></p>
<p>Samples from a multivariate distribution are <em>vectors</em> (not scalars as for univariate normal):
<span class="math display">\[\boldsymbol x_1,\boldsymbol x_2,...,\boldsymbol x_n \stackrel{\text{iid}}\sim F\]</span></p>
<p><strong>Matrix and component notation:</strong></p>
<p>All the data points are commonly collected into a matrix <span class="math inline">\(\boldsymbol X\)</span>.</p>
<p>In statistics the convention is to store each data vector in the rows of the data matrix <span class="math inline">\(\boldsymbol X\)</span>:</p>
<p><span class="math display">\[\boldsymbol X= (\boldsymbol x_1,\boldsymbol x_2,...,\boldsymbol x_n)^T = \begin{pmatrix}
    x_{11}  &amp; x_{12} &amp; \dots &amp; x_{1d}   \\
    x_{21}  &amp; x_{22} &amp; \dots &amp; x_{2d}   \\
    \vdots \\
    x_{n1}  &amp; x_{n2} &amp; \dots &amp; x_{nd}
\end{pmatrix}\]</span></p>
<p>Therefore,
<span class="math display">\[\boldsymbol x_1=\begin{pmatrix}
    x_{11}       \\
    \vdots \\
    x_{1d}
\end{pmatrix} , \space \boldsymbol x_2=\begin{pmatrix}
    x_{21}       \\
    \vdots \\
    x_{2d}
\end{pmatrix} , \ldots , \boldsymbol x_n=\begin{pmatrix}
    x_{n1}       \\
    \vdots \\
    x_{nd}
\end{pmatrix}\]</span></p>
<p>Thus, in statistics the first index runs over <span class="math inline">\((1,...,n)\)</span> and denotes the samples while the second index runs over <span class="math inline">\((1,...,d)\)</span> and refers to the variables.</p>
<p>The <strong>statistics convention on data matrices</strong> is <em>not</em> universal! In fact, in most of the machine learning literature in engineering and computer science the data samples are stored in the columns so that the variables appear in the rows (thus in the engineering convention the data matrix is transposed compared to the statistics convention).</p>
<p>In order to avoid confusion and ambiguity it is recommended to prefer vector notation to describe data over matrix or component notation (see also the section below on estimating covariance matrices for examples).</p>
</div>
<div id="mean-of-a-random-vector" class="section level3" number="1.1.4">
<h3>
<span class="header-section-number">1.1.4</span> Mean of a random vector<a class="anchor" aria-label="anchor" href="#mean-of-a-random-vector"><i class="fas fa-link"></i></a>
</h3>
<p>The mean / expectation of a random vector with dimensions <span class="math inline">\(d\)</span> is also a vector with dimensions <span class="math inline">\(d\)</span>:
<span class="math display">\[\text{E}(\boldsymbol x) = \boldsymbol \mu= \begin{pmatrix}
    \text{E}(x_1)       \\
    \text{E}(x_2)       \\
    \vdots \\
    \text{E}(x_d)
\end{pmatrix} = \left( \begin{array}{l}
    \mu_1       \\
    \mu_2       \\
    \vdots \\
    \mu_d
\end{array}\right)\]</span></p>
</div>
<div id="variance-of-a-random-vector" class="section level3" number="1.1.5">
<h3>
<span class="header-section-number">1.1.5</span> Variance of a random vector<a class="anchor" aria-label="anchor" href="#variance-of-a-random-vector"><i class="fas fa-link"></i></a>
</h3>
<p>Recall the definition of mean and variance for a univariate random variable:</p>
<p><span class="math display">\[\text{E}(x) = \mu\]</span></p>
<p><span class="math display">\[\text{Var}(x) = \sigma^2 = \text{E}( (x-\mu)^2 )=\text{E}( (x-\mu)(x-\mu) ) = \text{E}(x^2)-\mu^2\]</span></p>
<p>Definition of <strong>variance of a random vector:</strong></p>
<p><span class="math display">\[\text{Var}(\boldsymbol x) = \underbrace{\boldsymbol \Sigma}_{d\times d} =
\text{E}\left(\underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d\times 1} \underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1\times d} \right)
= \text{E}(\boldsymbol x\boldsymbol x^T)-\boldsymbol \mu\boldsymbol \mu^T\]</span></p>
<p>The variance of a random vector is, therefore, <strong>not</strong> a vector but a <strong>matrix</strong>!</p>
<p><span class="math display">\[\boldsymbol \Sigma= (\sigma_{ij}) = \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}\]</span></p>
<p>This matrix is called the <strong>Covariance Matrix</strong>, with off-diagonal elements <span class="math inline">\(\sigma_{ij}= \text{Cov}(x_i,x_j)\)</span> and the diagonal <span class="math inline">\(\sigma_{ii}= \text{Var}(X_i) = \sigma_i^2\)</span>.</p>
</div>
<div id="properties-of-the-covariance-matrix" class="section level3" number="1.1.6">
<h3>
<span class="header-section-number">1.1.6</span> Properties of the covariance matrix<a class="anchor" aria-label="anchor" href="#properties-of-the-covariance-matrix"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\boldsymbol \Sigma\)</span> is real valued: <span class="math inline">\(\sigma_{ij} \in \mathbb{R}\)</span>
</li>
<li>
<span class="math inline">\(\boldsymbol \Sigma\)</span> is symmetric: <span class="math inline">\(\sigma_{ij} = \sigma_{ji}\)</span>
</li>
<li>The diagonal of <span class="math inline">\(\boldsymbol \Sigma\)</span> contains <span class="math inline">\(\sigma_{ii} = \text{Var}(x_i) = \sigma_i^2\)</span>, i.e. the
variances of the components of <span class="math inline">\(\boldsymbol x\)</span>.</li>
<li>Off-diagonal elements <span class="math inline">\(\sigma_{ij} = \text{Cov}(x_i,x_j)\)</span> represent linear dependencies among the <span class="math inline">\(x_i\)</span>. <span class="math inline">\(\Longrightarrow\)</span> linear regression, correlation</li>
</ol>
<p>How many separate entries does <span class="math inline">\(\boldsymbol \Sigma\)</span> have?</p>
<p><span class="math display">\[\boldsymbol \Sigma= (\sigma_{ij}) = \underbrace{\begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}}_{d\times d}\]</span>
with <span class="math inline">\(\sigma_{ij} = \sigma_{ji}\)</span>.</p>
<p>Number of separate entries: <span class="math inline">\(\frac{d(d+1)}{2}\)</span>.</p>
<p>This numbers grows with the square of the dimension <span class="math inline">\(d\)</span>, i.e. is of order <span class="math inline">\(O(d^2)\)</span>:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th><span class="math inline">\(d\)</span></th>
<th># entries</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>10</td>
<td>55</td>
</tr>
<tr class="odd">
<td>100</td>
<td>5050</td>
</tr>
<tr class="even">
<td>1000</td>
<td>500500</td>
</tr>
<tr class="odd">
<td>10000</td>
<td>50005000</td>
</tr>
</tbody>
</table></div>
<p>For large dimension <span class="math inline">\(d\)</span> the covariance matrix has many components!</p>
<p>–&gt; computationally expensive (both for storage and in handling)
–&gt; very challenging to estimate in high dimensions <span class="math inline">\(d\)</span>.</p>
<p>Note: matrix inversion requires <span class="math inline">\(O(d^3)\)</span> operations using standard algorithms such as <a href="https://en.wikipedia.org/wiki/Gaussian_elimination">Gauss Jordan elimination</a>. <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra"&gt;Specialised matrix algorithms improve this&lt;/a&gt; to about &lt;span class="math inline"&gt;\(O(d^{2.373})\)&lt;/span&gt;.
Matrices with special symmetries (e.g. diagonal and block diagonal matrices) or particular properties (e.g. orthogonal matrix) can also be inverted much easier.&lt;/p&gt;'><sup>1</sup></a> Hence, computing <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> is computationally expensive for large <span class="math inline">\(d\)</span>!</p>
</div>
<div id="eigenvalue-decomposition-of-boldsymbol-sigma" class="section level3" number="1.1.7">
<h3>
<span class="header-section-number">1.1.7</span> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span><a class="anchor" aria-label="anchor" href="#eigenvalue-decomposition-of-boldsymbol-sigma"><i class="fas fa-link"></i></a>
</h3>
<p>Recall from linear matrix algebra that any real symmetric matrix
has real eigenvalues and a complete set of orthogonal eigenvectors.
These can be obtained by orthogonal eigendecomposition. <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;A brief summary of eigenvalue decompositon is found in the supplementary
&lt;a href="https://strimmerlab.github.io/publications/lecture-notes/matrix-calculus-refresher/index.html"&gt;Matrix and Calculus Refresher notes&lt;/a&gt;.&lt;/p&gt;'><sup>2</sup></a></p>
<p>Applying eigenvalue decomposition to the covariance matrix yields
<span class="math display">\[
\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T
\]</span>
where <span class="math inline">\(\boldsymbol U\)</span> is an <strong>orthogonal matrix</strong> <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;An orthogonal matrix &lt;span class="math inline"&gt;\(\boldsymbol Q\)&lt;/span&gt; satisfies &lt;span class="math inline"&gt;\(\boldsymbol Q^T \boldsymbol Q= \boldsymbol I\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\boldsymbol Q\boldsymbol Q^T = \boldsymbol I\)&lt;/span&gt;
and &lt;span class="math inline"&gt;\(\boldsymbol Q^{-1} = \boldsymbol Q^T\)&lt;/span&gt; and is also called rotation-reflection matrix.
We will make frequent use of orthogonal matrices so this might be a good time to
revisit their properties, see e.g. the &lt;a href="https://strimmerlab.github.io/publications/lecture-notes/matrix-calculus-refresher/index.html"&gt;Matrix and Calculus Refresher notes&lt;/a&gt;..&lt;/p&gt;'><sup>3</sup></a> containing the eigenvectors of the covariance matrix
and
<span class="math display">\[\boldsymbol \Lambda= \begin{pmatrix}
    \lambda_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}
\end{pmatrix}\]</span>
contains the corresponding eigenvalues <span class="math inline">\(\lambda_i\)</span>.</p>
<p>Importantly, the eigenvalues of a covariance matrix are not only real-valued
but are by construction further constrained to be non-negative.
This can be seen by computing the quadratic form <span class="math inline">\(\boldsymbol z^T \boldsymbol \Sigma\boldsymbol z\)</span>
where <span class="math inline">\(\boldsymbol z\)</span> is a non-random vector. For any non-zero <span class="math inline">\(\boldsymbol z\)</span>
<span class="math display">\[
\begin{split}
\boldsymbol z^T  \boldsymbol \Sigma\boldsymbol z&amp; = \boldsymbol z^T \text{E}\left(  (\boldsymbol x-\boldsymbol \mu) (\boldsymbol x-\boldsymbol \mu)^T  \right) \boldsymbol z\\
&amp; =  \text{E}\left( \boldsymbol z^T (\boldsymbol x-\boldsymbol \mu) (\boldsymbol x-\boldsymbol \mu)^T \boldsymbol z\right) \\
&amp; =  \text{E}\left( \left( \boldsymbol z^T (\boldsymbol x-\boldsymbol \mu) \right)^2 \right) \geq 0 \, .\\
\end{split}
\]</span>
Furthermore, with <span class="math inline">\(\boldsymbol y= \boldsymbol U^T \boldsymbol z\)</span> we get
<span class="math display">\[
\begin{split}
\boldsymbol z^T  \boldsymbol \Sigma\boldsymbol z&amp; =  \boldsymbol z^T\boldsymbol U\boldsymbol \Lambda\boldsymbol U^T \boldsymbol z\\
                      &amp; =  \boldsymbol y^T \boldsymbol \Lambda\boldsymbol y= \sum_{i=1}^d  y_i^2 \lambda_i \\
\end{split}
\]</span>
and hence all the <span class="math inline">\(\lambda_i \geq 0\)</span>.
Therefore the <strong>covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> is always
positive semi-definite</strong>.</p>
<p>In fact, <strong>unless there is collinearity</strong> ( i.e. a variable is a linear function the other variables) all eigenvalues will be positive and <strong><span class="math inline">\(\boldsymbol \Sigma\)</span> is positive definite</strong>.</p>
</div>
<div id="joint-covariance-matrix" class="section level3" number="1.1.8">
<h3>
<span class="header-section-number">1.1.8</span> Joint covariance matrix<a class="anchor" aria-label="anchor" href="#joint-covariance-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>Assume we have random vector <span class="math inline">\(\boldsymbol z\)</span> with mean <span class="math inline">\(\text{E}(\boldsymbol z) = \boldsymbol \mu_{\boldsymbol z}\)</span>
and covariance matrix <span class="math inline">\(\text{Var}(\boldsymbol z) = \boldsymbol \Sigma_{\boldsymbol z}\)</span>.</p>
<p>Often it makes sense to partion the components of <span class="math inline">\(\boldsymbol z\)</span> into two groups
<span class="math display">\[
\boldsymbol z= \begin{pmatrix} \boldsymbol x\\ \boldsymbol y\end{pmatrix}
\]</span>
This induces a corresponding partition
in the expectation
<span class="math display">\[
\boldsymbol \mu_{\boldsymbol z} =  \begin{pmatrix} \boldsymbol \mu_{\boldsymbol x} \\ \boldsymbol \mu_{\boldsymbol y} \end{pmatrix}
\]</span>
where <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \mu_{\boldsymbol x}\)</span> and <span class="math inline">\(\text{E}(\boldsymbol y) = \boldsymbol \mu_{\boldsymbol y}\)</span>.</p>
<p>Furthermore, the joint covariance matrix for <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> can
then be written as
<span class="math display">\[
\boldsymbol \Sigma_{\boldsymbol z} =
\begin{pmatrix}
\boldsymbol \Sigma_{\boldsymbol x} &amp;  \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} \\
\boldsymbol \Sigma_{\boldsymbol y\boldsymbol x} &amp; \boldsymbol \Sigma_{\boldsymbol y} \\
\end{pmatrix}
\]</span>
It contains the within-group group covariance matrices
<span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol y}\)</span> as diagonal elements and the
cross-covariance matrix <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} = \boldsymbol \Sigma_{\boldsymbol y\boldsymbol x}^T\)</span>
as off-diagonal element.</p>
<p>Note that the cross-covariance matrix <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y}\)</span> is rectangular
and not symmetric. We also write
<span class="math inline">\(\text{Cov}(\boldsymbol x, \boldsymbol y) = \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y}\)</span>
and we can define cross-covariance directly by
<span class="math display">\[
\text{Cov}(\boldsymbol x, \boldsymbol y) = \text{E}\left( (\boldsymbol x- \boldsymbol \mu_{\boldsymbol x}) ( \boldsymbol y- \boldsymbol \mu_{\boldsymbol y} )^T \right) = \text{E}(\boldsymbol x\boldsymbol y^T)-\boldsymbol \mu_{\boldsymbol x} \boldsymbol \mu_{\boldsymbol y}^T
\]</span></p>
</div>
<div id="quantities-related-to-the-covariance-matrix" class="section level3" number="1.1.9">
<h3>
<span class="header-section-number">1.1.9</span> Quantities related to the covariance matrix<a class="anchor" aria-label="anchor" href="#quantities-related-to-the-covariance-matrix"><i class="fas fa-link"></i></a>
</h3>
<div id="correlation-matrix-boldsymbol-p" class="section level4" number="1.1.9.1">
<h4>
<span class="header-section-number">1.1.9.1</span> Correlation matrix <span class="math inline">\(\boldsymbol P\)</span><a class="anchor" aria-label="anchor" href="#correlation-matrix-boldsymbol-p"><i class="fas fa-link"></i></a>
</h4>
<p>The correlation matrix <span class="math inline">\(\boldsymbol P\)</span> (= upper case greek “rho”) is the standardised covariance matrix</p>
<p><span class="math display">\[\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}=\text{Cor}(x_i,x_j)\]</span></p>
<p><span class="math display">\[\rho_{ii} = 1 = \text{Cor}(x_i,x_i)\]</span></p>
<p><span class="math display">\[ \boldsymbol P= (\rho_{ij}) = \begin{pmatrix}
    1 &amp; \dots &amp; \rho_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \rho_{d1} &amp; \dots &amp; 1
\end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol P\)</span> (“upper case rho”) is a symmetric matrix (<span class="math inline">\(\rho_{ij}=\rho_{ji}\)</span>).</p>
<p>Note the <strong>variance-correlation decomposition</strong></p>
<p><span class="math display">\[\boldsymbol \Sigma= \boldsymbol V^{\frac{1}{2}} \boldsymbol P\boldsymbol V^{\frac{1}{2}}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol V\)</span> is a diagonal matrix containing the variances:</p>
<p><span class="math display">\[ \boldsymbol V= \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}\]</span></p>
<p><span class="math display">\[\boldsymbol P= \boldsymbol V^{-\frac{1}{2}}\boldsymbol \Sigma\boldsymbol V^{-\frac{1}{2}}\]</span></p>
<p>This is the definition of correlation written in matrix notation.</p>
<p>As with the covariance matrix, in many applications it makes sense to
partition a joint correlation matrix
<span class="math display">\[
\boldsymbol P_{\boldsymbol z} =
\begin{pmatrix}
\boldsymbol P_{\boldsymbol x} &amp;  \boldsymbol P_{\boldsymbol x\boldsymbol y} \\
\boldsymbol P_{\boldsymbol y\boldsymbol x} &amp; \boldsymbol P_{\boldsymbol y} \\
\end{pmatrix}
\]</span>
into within-group group correlation matrices
<span class="math display">\[
\boldsymbol P_{\boldsymbol x} = \boldsymbol V_{\boldsymbol x}^{-\frac{1}{2}} \boldsymbol \Sigma_{\boldsymbol x} \boldsymbol V_{\boldsymbol x}^{-\frac{1}{2}}
\]</span>
and
<span class="math display">\[
\boldsymbol P_{\boldsymbol y} = \boldsymbol V_{\boldsymbol y}^{-\frac{1}{2}} \boldsymbol \Sigma_{\boldsymbol y} \boldsymbol V_{\boldsymbol y}^{-\frac{1}{2}}
\]</span>
and the cross-correlation matrix
<span class="math display">\[
\boldsymbol P_{\boldsymbol x\boldsymbol y} = \boldsymbol V_{\boldsymbol x}^{-\frac{1}{2}} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y} \boldsymbol V_{\boldsymbol y}^{-\frac{1}{2}}
\]</span>
with
<span class="math display">\[
\boldsymbol P_{\boldsymbol y\boldsymbol x} = \boldsymbol P_{\boldsymbol x\boldsymbol y}^T
= \boldsymbol V_{\boldsymbol y}^{-\frac{1}{2}} \boldsymbol \Sigma_{\boldsymbol y\boldsymbol x} \boldsymbol V_{\boldsymbol x}^{-\frac{1}{2}} \,.
\]</span></p>
</div>
<div id="precision-matrix-or-concentration-matrix" class="section level4" number="1.1.9.2">
<h4>
<span class="header-section-number">1.1.9.2</span> Precision matrix or concentration matrix<a class="anchor" aria-label="anchor" href="#precision-matrix-or-concentration-matrix"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[\boldsymbol \Omega= (\omega_{ij}) = \boldsymbol \Sigma^{-1}\]</span></p>
<p><span class="math inline">\(\boldsymbol \Omega\)</span> (“Omega”) is the inverse of the covariance matrix.</p>
<p>The inverse of the covariance matrix can be obtained via
the spectral decomposition, followed by inverting the eigenvalues <span class="math inline">\(\lambda_i\)</span>:
<span class="math display">\[\boldsymbol \Sigma^{-1} = \boldsymbol U\boldsymbol \Lambda^{-1} \boldsymbol U^T =
\boldsymbol U\begin{pmatrix}
    \lambda_{1}^{-1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}^{-1}
\end{pmatrix} \boldsymbol U^T \]</span></p>
<p>Note that <strong>all eigenvalues <span class="math inline">\(\lambda_i\)</span> need to be positive so that <span class="math inline">\(\boldsymbol \Sigma\)</span> can be inverted.</strong> (i.e., <span class="math inline">\(\boldsymbol \Sigma\)</span> needs to be positive definite).<br>
If any <span class="math inline">\(\lambda_i = 0\)</span> then <span class="math inline">\(\boldsymbol \Sigma\)</span> is singular and not invertible.</p>
<p>Importance of <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span>:</p>
<ul>
<li>Many expressions in multivariate statistics contain <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> and not <span class="math inline">\(\boldsymbol \Sigma\)</span>.</li>
<li>
<span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> has close connection with graphical models
(e.g. conditional independence graph, partial correlations).</li>
<li>
<span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> is a natural parameter from an exponential family perspective.</li>
</ul>
</div>
<div id="partial-correlation-matrix" class="section level4" number="1.1.9.3">
<h4>
<span class="header-section-number">1.1.9.3</span> Partial correlation matrix<a class="anchor" aria-label="anchor" href="#partial-correlation-matrix"><i class="fas fa-link"></i></a>
</h4>
<p>This is a standardised version of the precision matrix, see later chapter on graphical models.</p>
</div>
<div id="total-variation-and-generalised-variance" class="section level4" number="1.1.9.4">
<h4>
<span class="header-section-number">1.1.9.4</span> Total variation and generalised variance<a class="anchor" aria-label="anchor" href="#total-variation-and-generalised-variance"><i class="fas fa-link"></i></a>
</h4>
<p>To summarise the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> in a single scalar value there are two commonly used
measures:</p>
<ul>
<li>
<strong>total variation</strong>: <span class="math inline">\(\text{Tr}(\boldsymbol \Sigma) = \sum_{i=1}^d \lambda_i\)</span>
</li>
<li>
<strong>generalised variance</strong>: <span class="math inline">\(\det(\boldsymbol \Sigma) = \prod_{i=1}^d \lambda_i\)</span>
</li>
</ul>
<p>The generalised variance <span class="math inline">\(\det(\boldsymbol \Sigma)\)</span> is also known as the volume of <span class="math inline">\(\boldsymbol \Sigma\)</span>.</p>

</div>
</div>
</div>
<div id="multivariate-distributions" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Multivariate distributions<a class="anchor" aria-label="anchor" href="#multivariate-distributions"><i class="fas fa-link"></i></a>
</h2>
<div id="common-distributions" class="section level3" number="1.2.1">
<h3>
<span class="header-section-number">1.2.1</span> Common distributions<a class="anchor" aria-label="anchor" href="#common-distributions"><i class="fas fa-link"></i></a>
</h3>
<p>In multivariate statistics we make use of multivariate distributions. These are typically generalisations of corresponding univariate distribution.</p>
<p>Among the most commonly used multivariate distributions are:</p>
<ul>
<li>The <strong>multivariate normal distribution</strong> <span class="math inline">\(N_d(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> as a generalisation of univariate normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span>
</li>
<li>The <strong>categorical distribution</strong> <span class="math inline">\(\text{Cat}(\boldsymbol \pi)\)</span> as a generalisation of the Bernoulli distribution <span class="math inline">\(\text{Ber}(\theta)\)</span>
</li>
<li>The <strong>multinomial distribution</strong> <span class="math inline">\(\text{Mult}(n, \boldsymbol \pi)\)</span> as a generalisation of binomial distribution <span class="math inline">\(\text{Bin}(n, \theta)\)</span>
</li>
</ul>
<p>The above distribution have already been introduced earlier in
<a href="https://strimmerlab.github.io/publications/lecture-notes/MATH27720-stats2/">MATH27720 Statistics 2</a>.</p>
<p>Conceptually, these multivariate generalisation work behave exactly
the same as their univariate counterparts and are employed in the same settings.</p>
</div>
<div id="further-multivariate-distributions" class="section level3" number="1.2.2">
<h3>
<span class="header-section-number">1.2.2</span> Further multivariate distributions<a class="anchor" aria-label="anchor" href="#further-multivariate-distributions"><i class="fas fa-link"></i></a>
</h3>
<p>For multivariate Bayesian analyis we also need to consider a number of further multivariate distributions:</p>
<ul>
<li>The <strong>Dirichlet distribution</strong> <span class="math inline">\(\text{Dir}(\boldsymbol \alpha)\)</span> as the generalisation of the beta distribution <span class="math inline">\(\text{Beta}(\alpha, \beta)\)</span>,</li>
<li>The <strong>Wishart distribution</strong> as the generalisation of the gamma distribution <span class="math inline">\(\text{Gam}(\alpha, \theta)\)</span>,</li>
<li>The <strong>inverse Wishart distribution</strong> as the generalisation of the inverse gamma distribution <span class="math inline">\(\text{Inv-Gam}(\alpha, \beta)\)</span>.</li>
</ul>
<p>For technical details of the densities etc. of the multivariate distribution families we refer to the supplementary <a href="https://strimmerlab.github.io/publications/lecture-notes/probability-distribution-refresher/index.html">Probability and Distribution refresher notes</a>.</p>
</div>
</div>
<div id="multivariate-normal-distribution" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Multivariate normal distribution<a class="anchor" aria-label="anchor" href="#multivariate-normal-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>The multivariate normal disribution is ubiquitous in multivariate statistics and hence it is important to discuss it in more detail.</p>
<p>The multivariate normal model is a generalisation of the univariate normal distribution
from dimension 1 to dimension <span class="math inline">\(d\)</span>.</p>
<div id="univariate-normal-distribution" class="section level3" number="1.3.1">
<h3>
<span class="header-section-number">1.3.1</span> Univariate normal distribution:<a class="anchor" aria-label="anchor" href="#univariate-normal-distribution"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[\text{Dimension } d = 1\]</span>
<span class="math display">\[x \sim N(\mu, \sigma^2)\]</span>
<span class="math display">\[\text{E}(x) = \mu \space , \space  \text{Var}(x) = \sigma^2\]</span></p>
<p><strong>Probability Density Function</strong>:</p>
<p><span class="math display">\[f(x |\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right) \]</span></p>
<p><strong>Plot of univariate normal density </strong>:</p>
<p>Unimodal with peak at <span class="math inline">\(\mu\)</span>, width determined by <span class="math inline">\(\sigma\)</span> (in this plot: <span class="math inline">\(\mu=2, \sigma^2=1\)</span> )</p>
<div class="inline-figure"><img src="01-multivariate-b-distributions_files/figure-html/unnamed-chunk-1-1.png" width="672"></div>
<p>Special case: <strong>standard normal</strong> with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=1\)</span>:</p>
<p><span class="math display">\[f(x |\mu=0,\sigma^2=1)=\frac{1}{\sqrt{2\pi}} \exp\left( {-\frac{x^2}{2}} \right) \]</span></p>
<p><strong>Differential entropy</strong>:<br><span class="math display">\[
H(F) = \frac{1}{2} (\log(2 \pi \sigma^2) + 1)
\]</span></p>
<p><strong>Cross-entropy</strong>:<br><span class="math display">\[
H(F_{\text{ref}}, F) = \frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 }
+\frac{\sigma^2_{\text{ref}}}{\sigma^2}  +\log(2 \pi \sigma^2) \right)
\]</span>
<strong>KL divergence</strong>:<br><span class="math display">\[
D_{\text{KL}}(F_{\text{ref}}, F) = H(F_{\text{ref}}, F) - H(F_{\text{ref}}) =
\frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 }
+\frac{\sigma^2_{\text{ref}}}{\sigma^2}  -\log\left(\frac{\sigma^2_{\text{ref}}}{ \sigma^2}\right) -1
\right)
\]</span></p>
<p><strong>Maximum entropy characterisation:</strong> the normal distribution is the unique distribution
that has the
highest (differential) entropy over all continuous distributions with support from minus infinity to plus infinity with a given mean and variance.</p>
<p>This is in fact one of the reasons why the normal distribution is so important (und useful) –
if we only know that a random variable has a mean and variance, and not much else, then using the
normal distribution will be a reasonable and well justified model.</p>
</div>
<div id="multivariate-normal-model" class="section level3" number="1.3.2">
<h3>
<span class="header-section-number">1.3.2</span> Multivariate normal model<a class="anchor" aria-label="anchor" href="#multivariate-normal-model"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[\text{Dimension } d\]</span>
<span class="math display">\[\boldsymbol x\sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\]</span>
<span class="math display">\[\text{E}(\boldsymbol x) = \boldsymbol \mu\space , \space  \text{Var}(\boldsymbol x) = \boldsymbol \Sigma\]</span></p>
<p><strong>Density</strong>:</p>
<p><span class="math display">\[f(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma) = \det(2 \pi \boldsymbol \Sigma)^{-\frac{1}{2}} \exp\left({{-\frac{1}{2}} \underbrace{\underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1 \times d} \underbrace{\boldsymbol \Sigma^{-1}}_{d \times d} \underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d \times 1} }_{1 \times 1 = \text{scalar!}}}\right)\]</span></p>
<ul>
<li>the density contains the precision matrix <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span>
</li>
<li>to invert the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> we need to invert its eigenvalues <span class="math inline">\(\lambda_i\)</span> (hence we require that all <span class="math inline">\(\lambda_i &gt; 0\)</span>)</li>
<li>the density also contains <span class="math inline">\(\det(\boldsymbol \Sigma) = \prod\limits_{i=1}^d \lambda_i\)</span> <span class="math inline">\(\equiv\)</span> product of the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span>
</li>
<li>note that <span class="math inline">\(\det(2 \pi \boldsymbol \Sigma)^{-\frac{1}{2}} = \det(2 \pi \boldsymbol I_d)^{-\frac{1}{2}} \det(\boldsymbol \Sigma)^{-\frac{1}{2}} = (2 \pi)^{-d/2} \det(\boldsymbol \Sigma)^{-\frac{1}{2}}\)</span>
</li>
</ul>
<p>Special case: <strong>standard multivariate normal</strong> with <span class="math display">\[\boldsymbol \mu=\boldsymbol 0, \boldsymbol \Sigma=\boldsymbol I=\begin{pmatrix}
    1 &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; 1
\end{pmatrix}\]</span></p>
<p><span class="math display">\[f(\boldsymbol x| \boldsymbol \mu=\boldsymbol 0,\boldsymbol \Sigma=\boldsymbol I)=(2\pi)^{-d/2}\exp\left( -\frac{1}{2} \boldsymbol x^T \boldsymbol x\right) = \prod\limits_{i=1}^d \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x_i^2}{2}\right)\]</span>
which is equivalent to the product of <span class="math inline">\(d\)</span> univariate standard normals!</p>
<p><strong>Misc:</strong></p>
<ul>
<li>for <span class="math inline">\(d=1\)</span>, the multivariate normal density reduces to the univariate normal density.</li>
<li>for <span class="math inline">\(\boldsymbol \Sigma\)</span> diagonal (i.e. <span class="math inline">\(\boldsymbol P= \boldsymbol I\)</span>, no correlation), the multivariate normal density is the product of univariate normal densities (see Worksheet 2).</li>
</ul>
<p><strong>Plot of the multivariate normal density</strong>:</p>
<div class="inline-figure"><img src="01-multivariate-b-distributions_files/figure-html/fig1-1.png" width="672"></div>
<ul>
<li>Location: <span class="math inline">\(\boldsymbol \mu\)</span><br>
</li>
<li>Shape: <span class="math inline">\(\boldsymbol \Sigma\)</span><br>
</li>
<li>Unimodal: <strong>one</strong> peak<br>
</li>
<li>Support from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span> in each dimension</li>
</ul>
<p>An interactive <a href="https://shiny.rstudio.com/">R Shiny web app</a> of the bivariate normal density plot
is available online at <a href="https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/" class="uri">https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/</a> .</p>
<p><strong>Differential entropy</strong>:<br><span class="math display">\[
H = \frac{1}{2} (\log \det(2 \pi \boldsymbol \Sigma) + d)
\]</span></p>
<p><strong>Cross-entropy</strong>:<br><span class="math display">\[
H(F_{\text{ref}}, F) = \frac{1}{2}   \biggl\{
        (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})
      + \text{Tr}\biggl(\boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
    + \log \det \biggl( 2 \pi \boldsymbol \Sigma\biggr)    \biggr\}
\]</span>
<strong>KL divergence</strong>:<br><span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\text{ref}}, F) &amp;= H(F_{\text{ref}}, F) - H(F_{\text{ref}}) \\
&amp;= \frac{1}{2}   \biggl\{
        (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})
      + \text{Tr}\biggl(\boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
    - \log \det \biggl( \boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
     - d   \biggr\} \\
\end{split}
\]</span></p>
</div>
<div id="shape-of-the-multivariate-normal-density" class="section level3" number="1.3.3">
<h3>
<span class="header-section-number">1.3.3</span> Shape of the multivariate normal density<a class="anchor" aria-label="anchor" href="#shape-of-the-multivariate-normal-density"><i class="fas fa-link"></i></a>
</h3>
<p>Now we show that the contour lines of the multivariate normal density always take on the form of an ellipse, and that the radii and orientation of the ellipse is determined by the eigenvalues of
<span class="math inline">\(\boldsymbol \Sigma\)</span>.</p>
<p>We start by observing that a circle with radius <span class="math inline">\(r\)</span> around the origin can be described as the set of points <span class="math inline">\((x_1,x_2)\)</span> satisfying
<span class="math inline">\(x_1^2+x_2^2 = r^2\)</span>, or equivalently, <span class="math inline">\(\frac{x_1^2}{r^2} + \frac{x_2^2}{r^2} = 1\)</span>.
This is generalised to the shape of an ellipse by allowing (in two dimensions) for two radii
<span class="math inline">\(r_1\)</span> and <span class="math inline">\(r_2\)</span> with
<span class="math inline">\(\frac{x_1^2}{r_1^2} + \frac{x_2^2}{r_2^2} = 1\)</span>, or in vector notation
<span class="math inline">\(\boldsymbol x^T \text{Diag}(r_1^2, r_2^2)^{-1} \boldsymbol x= 1\)</span>. Here two axes of the ellipse are parallel
to the two coordinate axes.</p>
<p>In <span class="math inline">\(d\)</span> dimensions and allowing for rotation of
the axes and a shift of the origin from 0 to <span class="math inline">\(\boldsymbol \mu\)</span> the condition for an ellipse is
<span class="math display">\[(\boldsymbol x-\boldsymbol \mu)^T \boldsymbol Q\, \text{Diag}(r_1^2, \ldots , r_d^2)^{-1} \boldsymbol Q^T (\boldsymbol x-\boldsymbol \mu) = 1\]</span>
where <span class="math inline">\(\boldsymbol Q\)</span> is an orthogonal matrix whose column vectors indicate the direction of the axes. These are also called the <strong>principal axes</strong> of the ellipse, and by construction
all <span class="math inline">\(d\)</span> principal axes are <strong>perpendicular</strong> to each other.</p>
<p>A contour line of a probability density function is a set of connected points where the density assumes the same constant value. In the case of the multivariate normal distribution keeping the density <span class="math inline">\(f(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma)\)</span> at some fixed value implies that <span class="math inline">\((\boldsymbol x-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu) = c\)</span> where <span class="math inline">\(c\)</span> is a constant. Using the eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span> we can rewrite this condition as
<span class="math display">\[
(\boldsymbol x-\boldsymbol \mu)^T \boldsymbol U\boldsymbol \Lambda^{-1} \boldsymbol U^T (\boldsymbol x-\boldsymbol \mu) = c \,.
\]</span>
This implies that</p>
<ol style="list-style-type: lower-roman">
<li>the contour lines of the multivariate normal density are indeed ellipses,</li>
<li>the direction of the principal axes of the ellipse are given
correspond to the colum vectors in <span class="math inline">\(\boldsymbol U\)</span> (i.e. the eigenvectors of <span class="math inline">\(\boldsymbol \Sigma\)</span>), and</li>
<li>the squared radii of the ellipse are proportional to the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span>
Equivalently, the positive square roots of the eigenvalues are proportional to the radii of the ellipse. Hence, for a singular covariance matrix with one or more <span class="math inline">\(\lambda_i=0\)</span> the corresponding radii are zero.</li>
</ol>
<p>An interactive <a href="https://shiny.rstudio.com/">R Shiny web app</a> to play with the contour lines of the
bivariate normal distribution is available online at <a href="https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/" class="uri">https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/</a> .</p>
</div>
<div id="three-types-of-covariances" class="section level3" number="1.3.4">
<h3>
<span class="header-section-number">1.3.4</span> Three types of covariances<a class="anchor" aria-label="anchor" href="#three-types-of-covariances"><i class="fas fa-link"></i></a>
</h3>
<p>Following the above we can parameterise a covariance matrix in terms of its i) volume, ii) shape and iii) orientation by writing
<span class="math display">\[
\boldsymbol \Sigma= \kappa \, \boldsymbol U\boldsymbol A\boldsymbol U^T = \boldsymbol U\; \left(\kappa \boldsymbol A\right) \; \boldsymbol U^T
\]</span>
with <span class="math inline">\(\boldsymbol A=\text{Diag}(a_1, \ldots, a_d)\)</span> and <span class="math inline">\(\det(\boldsymbol A) = \prod_{i=1}^d a_i = 1\)</span>.
Note that in this parameterisation the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span> are <span class="math inline">\(\lambda_i = \kappa a_i\)</span>.</p>
<ol style="list-style-type: lower-roman">
<li>The <strong>volume</strong> is <span class="math inline">\(\det(\boldsymbol \Sigma) = \kappa^d\)</span>, determined by a single parameter <span class="math inline">\(\kappa\)</span>. This parameter can be interpreted as the length of the side of a <span class="math inline">\(d\)</span>-dimensional hypercube.</li>
<li>The <strong>shape</strong> is determined by the diagonal matrix <span class="math inline">\(\boldsymbol A\)</span> with <span class="math inline">\(d-1\)</span> free parameters. Note that there are only <span class="math inline">\(d-1\)</span> and not <span class="math inline">\(d\)</span> free parameters because of the constraint <span class="math inline">\(\det(\boldsymbol A) = 1\)</span>.</li>
<li>The <strong>orientation</strong> is given by the orthogonal matrix <span class="math inline">\(\boldsymbol U\)</span>, with <span class="math inline">\(d (d-1)/2\)</span> free parameters.</li>
</ol>
<p>This leads to classification of covariances into three varieties:</p>
<p><strong>Type 1:</strong> <strong>spherical covariance</strong> <span class="math inline">\(\boldsymbol \Sigma=\kappa \boldsymbol I\)</span>,
with spherical contour lines, 1 free parameter (<span class="math inline">\(\boldsymbol A=\boldsymbol I\)</span>, <span class="math inline">\(\boldsymbol U=\boldsymbol I\)</span>).</p>
<p>Example:
<span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\)</span>
with <span class="math inline">\(\sqrt{\lambda_1/ \lambda_2} = 1\)</span>:</p>
<div class="inline-figure"><img src="01-multivariate-b-distributions_files/figure-html/fig2-1.png" width="672"></div>
<p><strong>Type 2</strong>: <strong>diagonal covariance</strong> <span class="math inline">\(\boldsymbol \Sigma= \kappa \boldsymbol A\)</span>, with elliptical contour lines and the principal axes of the ellipse oriented parallel to the coordinate axes, <span class="math inline">\(d\)</span> free parameters (<span class="math inline">\(\boldsymbol U=\boldsymbol I\)</span>).</p>
<p>Example:
<span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\)</span>
with <span class="math inline">\(\sqrt{\lambda_1 / \lambda_2} \approx 1.41\)</span>:</p>
<div class="inline-figure"><img src="01-multivariate-b-distributions_files/figure-html/fig3-1.png" width="672"></div>
<p><strong>Type 3</strong>: <strong>general unrestricted covariance</strong> <span class="math inline">\(\boldsymbol \Sigma\)</span>,
with elliptical contour lines, with the principal axes of the ellipse oriented according to the column vectors in <span class="math inline">\(\boldsymbol U\)</span>,
<span class="math inline">\(d (d+1)/2\)</span> free parameters.</p>
<p>Example:
<span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix} 2 &amp; 0.6 \\ 0.6 &amp; 1 \end{pmatrix}\)</span>
with <span class="math inline">\(\sqrt{\lambda_1 / \lambda_2} \approx 2.20\)</span>:</p>
<div class="inline-figure"><img src="01-multivariate-b-distributions_files/figure-html/fig4-1.png" width="672"></div>
</div>
<div id="concentration-of-probability-mass-for-small-and-large-dimension" class="section level3" number="1.3.5">
<h3>
<span class="header-section-number">1.3.5</span> Concentration of probability mass for small and large dimension<a class="anchor" aria-label="anchor" href="#concentration-of-probability-mass-for-small-and-large-dimension"><i class="fas fa-link"></i></a>
</h3>
<p>The density of the multivariate normal distribution has a bell shape with a single mode. Intuitively, we may assume that most of the probability mass is always concentrated around this mode, as it is in the univariate case (<span class="math inline">\(d=1\)</span>). While this is still true for small dimensions (small <span class="math inline">\(d\)</span>) we now show that this intuition is incorrect for high dimensions (large <span class="math inline">\(d\)</span>).</p>
<p>For simplicity we consider the standard multivariate normal distribution with dimension <span class="math inline">\(d\)</span>
<span class="math display">\[\boldsymbol x\sim N_d(\boldsymbol 0, \boldsymbol I_d)\]</span>
with a spherical covariance <span class="math inline">\(\boldsymbol I_d\)</span> and sample <span class="math inline">\(\boldsymbol x\)</span>. The squared Euclidean length of <span class="math inline">\(\boldsymbol x\)</span> is
<span class="math inline">\(r^2= || \boldsymbol x||^2 = \boldsymbol x^T \boldsymbol x= \sum_{i=1}^d x_i^2\)</span>. The corresponding density of the <span class="math inline">\(d\)</span>-dimensional standard multivariate normal distribution is
<span class="math display">\[
g_d(\boldsymbol x) = (2\pi)^{-d/2} e^{-\boldsymbol x^T \boldsymbol x/2}
\]</span>
A natural way to define the main part of the “bell” of the standard multivariate normal as the set of
all <span class="math inline">\(\boldsymbol x\)</span> for which the density is larger than a specified fraction <span class="math inline">\(\eta\)</span> (say 0.001) of the maximum value of the density <span class="math inline">\(g_d(0)\)</span> at the peak at zero.
To formalise
<span class="math display">\[
B = \left\{ \boldsymbol x: \frac{g_d(\boldsymbol x)}{ g_d(0)} &gt; \eta    \right\}
\]</span>
which can be equivalently written as the set
<span class="math display">\[
B = \{ \boldsymbol x: \boldsymbol x^T \boldsymbol x= r^2 &lt; -2 \log(\eta) = r^2_{\max} \}
\]</span></p>
<p>Each individual component in the sample <span class="math inline">\(\boldsymbol x\)</span> is independently distributed as <span class="math inline">\(x_i \sim N(0,1)\)</span>, hence <span class="math inline">\(r^2 \sim \text{$\chi^2_{d}$}\)</span> is chi-squared distributed with degree of freedom <span class="math inline">\(d\)</span>.
The probability <span class="math inline">\(\text{Pr}(\boldsymbol x\in B)\)</span> can thus be obtained as the value of the
cumulative density function of a chi-squared distribution with <span class="math inline">\(d\)</span> degrees
of freedom at <span class="math inline">\(r^2_{\max}\)</span>. Computing this probability for fixed <span class="math inline">\(\eta\)</span> as a function of the dimension <span class="math inline">\(d\)</span> we obtain the following curve:</p>
<p><img src="01-multivariate-b-distributions_files/figure-html/unnamed-chunk-3-1.png" width="672">
The above plot is for <span class="math inline">\(\eta=0.001\)</span>. You can see that for dimensions up to around <span class="math inline">\(d=10\)</span> the probability mass is indeed concentrated in the center of the distribution
but from <span class="math inline">\(d=30\)</span> onwards it has moved completely to the tails.</p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="preface.html">Preface</a></div>
<div class="next"><a href="multivariate-estimation-in-large-sample-and-small-sample-settings.html"><span class="header-section-number">2</span> Multivariate estimation in large sample and small sample settings</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#multivariate-random-variables"><span class="header-section-number">1</span> Multivariate random variables</a></li>
<li>
<a class="nav-link" href="#essentials-in-multivariate-statistics"><span class="header-section-number">1.1</span> Essentials in multivariate statistics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#why-multivariate-statistics"><span class="header-section-number">1.1.1</span> Why multivariate statistics?</a></li>
<li><a class="nav-link" href="#univariate-vs.-multivariate-random-variables"><span class="header-section-number">1.1.2</span> Univariate vs. multivariate random variables</a></li>
<li><a class="nav-link" href="#multivariate-data"><span class="header-section-number">1.1.3</span> Multivariate data</a></li>
<li><a class="nav-link" href="#mean-of-a-random-vector"><span class="header-section-number">1.1.4</span> Mean of a random vector</a></li>
<li><a class="nav-link" href="#variance-of-a-random-vector"><span class="header-section-number">1.1.5</span> Variance of a random vector</a></li>
<li><a class="nav-link" href="#properties-of-the-covariance-matrix"><span class="header-section-number">1.1.6</span> Properties of the covariance matrix</a></li>
<li><a class="nav-link" href="#eigenvalue-decomposition-of-boldsymbol-sigma"><span class="header-section-number">1.1.7</span> Eigenvalue decomposition of \(\boldsymbol \Sigma\)</a></li>
<li><a class="nav-link" href="#joint-covariance-matrix"><span class="header-section-number">1.1.8</span> Joint covariance matrix</a></li>
<li><a class="nav-link" href="#quantities-related-to-the-covariance-matrix"><span class="header-section-number">1.1.9</span> Quantities related to the covariance matrix</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#multivariate-distributions"><span class="header-section-number">1.2</span> Multivariate distributions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#common-distributions"><span class="header-section-number">1.2.1</span> Common distributions</a></li>
<li><a class="nav-link" href="#further-multivariate-distributions"><span class="header-section-number">1.2.2</span> Further multivariate distributions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#multivariate-normal-distribution"><span class="header-section-number">1.3</span> Multivariate normal distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#univariate-normal-distribution"><span class="header-section-number">1.3.1</span> Univariate normal distribution:</a></li>
<li><a class="nav-link" href="#multivariate-normal-model"><span class="header-section-number">1.3.2</span> Multivariate normal model</a></li>
<li><a class="nav-link" href="#shape-of-the-multivariate-normal-density"><span class="header-section-number">1.3.3</span> Shape of the multivariate normal density</a></li>
<li><a class="nav-link" href="#three-types-of-covariances"><span class="header-section-number">1.3.4</span> Three types of covariances</a></li>
<li><a class="nav-link" href="#concentration-of-probability-mass-for-small-and-large-dimension"><span class="header-section-number">1.3.5</span> Concentration of probability mass for small and large dimension</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Multivariate Statistics and Machine Learning</strong>" was written by Korbinian Strimmer. It was last built on 1 March 2024.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
