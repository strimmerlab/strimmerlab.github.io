<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>1 Multivariate random variables | Multivariate Statistics and Machine Learning</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="1 Multivariate random variables | Multivariate Statistics and Machine Learning">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Multivariate random variables | Multivariate Statistics and Machine Learning">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="1.1 Why multivariate statistics? Science uses experiments to verify hypotheses about the world. Statistics provides tools to quantify this procedure and offers methods to link data (experiments)...">
<meta property="og:description" content="1.1 Why multivariate statistics? Science uses experiments to verify hypotheses about the world. Statistics provides tools to quantify this procedure and offers methods to link data (experiments)...">
<meta name="twitter:description" content="1.1 Why multivariate statistics? Science uses experiments to verify hypotheses about the world. Statistics provides tools to quantify this procedure and offers methods to link data (experiments)...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Multivariate Statistics and Machine Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="active" href="multivariate-random-variables.html"><span class="header-section-number">1</span> Multivariate random variables</a></li>
<li><a class="" href="transformations-and-dimension-reduction.html"><span class="header-section-number">2</span> Transformations and dimension reduction</a></li>
<li><a class="" href="unsupervised-learning-and-clustering.html"><span class="header-section-number">3</span> Unsupervised learning and clustering</a></li>
<li><a class="" href="supervised-learning-and-classification.html"><span class="header-section-number">4</span> Supervised learning and classification</a></li>
<li><a class="" href="multivariate-dependencies.html"><span class="header-section-number">5</span> Multivariate dependencies</a></li>
<li><a class="" href="nonlinear-and-nonparametric-models.html"><span class="header-section-number">6</span> Nonlinear and nonparametric models</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="brief-refresher-on-matrices.html"><span class="header-section-number">A</span> Brief refresher on matrices</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="multivariate-random-variables" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Multivariate random variables<a class="anchor" aria-label="anchor" href="#multivariate-random-variables"><i class="fas fa-link"></i></a>
</h1>
<div id="why-multivariate-statistics" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Why multivariate statistics?<a class="anchor" aria-label="anchor" href="#why-multivariate-statistics"><i class="fas fa-link"></i></a>
</h2>
<p>Science uses experiments to verify hypotheses about the world.
Statistics provides tools to quantify this procedure and offers methods to
link data (experiments) with probabilistic models (hypotheses).
Since the world is complex we need complex models and complex data, hence
the need for multivariate statistics and machine learning.</p>
<p>Specifically, multivariate statistics (as opposed to univariate statistics) is concerned with methods and models for <strong>random vectors</strong> and <strong>random matrices</strong>, rather than just random univariate (scalar) variables. Therefore, in multivariate statistics we will frequently make use of matrix notation.</p>
<p>Closely related to multivariate statistics (traditionally a subfield of statistics) is machine learning (ML) which is traditionally a subfield of computer science. ML used to focus more on algorithms rather on probabilistic modelling but nowadays most machine learning methods are fully based on statistical multivariate approaches, so the two fields are converging.</p>
<p>Multivariate models provide a means to learn dependencies and interactions among the
components of the random variables which in turn allow us to draw conclusion about underlying mechanisms of interest (e.g. biological or medical).</p>
<p>Two main tasks:</p>
<ul>
<li>unsupervised learning (finding structure, clustering)</li>
<li>supervised learning (training from labelled data, followed by prediction)</li>
</ul>
<p>Challenges:</p>
<ul>
<li>complexity of model needs to be appropriate for problem and available data,</li>
<li>high dimensions make estimation and inference difficult</li>
<li>computational issues.</li>
</ul>
</div>
<div id="essentials-in-multivariate-statistics" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Essentials in multivariate statistics<a class="anchor" aria-label="anchor" href="#essentials-in-multivariate-statistics"><i class="fas fa-link"></i></a>
</h2>
<div id="univariate-vs.-multivariate-random-variables" class="section level3" number="1.2.1">
<h3>
<span class="header-section-number">1.2.1</span> Univariate vs. multivariate random variables<a class="anchor" aria-label="anchor" href="#univariate-vs.-multivariate-random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>Univariate random variable (dimension <span class="math inline">\(d=1\)</span>):
<span class="math display">\[x \sim F\]</span>
where <span class="math inline">\(x\)</span> is a <strong>scalar</strong> and <span class="math inline">\(F\)</span> is the distribution.
<span class="math inline">\(\text{E}(x) = \mu\)</span> denotes the mean and <span class="math inline">\(\text{Var}(x) = \sigma^2\)</span> the variance of <span class="math inline">\(x\)</span>.</p>
<p>Multivariate random <strong>vector</strong> of dimension <span class="math inline">\(d\)</span>:
<span class="math display">\[\boldsymbol x= (x_1, x_2,...,x_d)^T  \sim F\]</span></p>
<p><span class="math inline">\(\boldsymbol x\)</span> is <strong>vector</strong> valued random variable.</p>
<p>The vector <span class="math inline">\(\boldsymbol x\)</span> is column vector (=matrix of size <span class="math inline">\(d \times 1\)</span>).
Its components <span class="math inline">\(x_1, x_2,...,x_d\)</span> are univariate random variables.
The dimension <span class="math inline">\(d\)</span> is also often denoted by <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span>.</p>
</div>
<div id="mean-of-a-random-vector" class="section level3" number="1.2.2">
<h3>
<span class="header-section-number">1.2.2</span> Mean of a random vector<a class="anchor" aria-label="anchor" href="#mean-of-a-random-vector"><i class="fas fa-link"></i></a>
</h3>
<p>The mean / expectation of a random vector with dimensions <span class="math inline">\(d\)</span> is also a vector with dimensions <span class="math inline">\(d\)</span>:
<span class="math display">\[\text{E}(\boldsymbol x) = \boldsymbol \mu= \begin{pmatrix}
    \text{E}(x_1)       \\
    \text{E}(x_2)       \\
    \vdots \\
    \text{E}(x_d)
\end{pmatrix} = \left( \begin{array}{l}
    \mu_1       \\
    \mu_2       \\
    \vdots \\
    \mu_d
\end{array}\right)\]</span></p>
</div>
<div id="variance-of-a-random-vector" class="section level3" number="1.2.3">
<h3>
<span class="header-section-number">1.2.3</span> Variance of a random vector<a class="anchor" aria-label="anchor" href="#variance-of-a-random-vector"><i class="fas fa-link"></i></a>
</h3>
<p>Recall the definition of mean and variance for a univariate random variable:</p>
<p><span class="math display">\[\text{E}(x) = \mu\]</span></p>
<p><span class="math display">\[\text{Var}(x) = \sigma^2 = \text{E}( (x-\mu)^2 )=\text{E}( (x-\mu)(x-\mu) ) = \text{E}(x^2)-\mu^2\]</span></p>
<p>Definition of <strong>variance of a random vector:</strong></p>
<p><span class="math display">\[\text{Var}(\boldsymbol x) = \underbrace{\boldsymbol \Sigma}_{d\times d} = 
\text{E}\left(\underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d\times 1} \underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1\times d} \right) 
 = \text{E}(\boldsymbol x\boldsymbol x^T)-\boldsymbol \mu\boldsymbol \mu^T\]</span></p>
<p>The variance of a random vector is, therefore, <strong>not</strong> a vector but a <strong>matrix</strong>!</p>
<p><span class="math display">\[\boldsymbol \Sigma= (\sigma_{ij}) = \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}\]</span></p>
<p>This matrix is called the <strong>Covariance Matrix</strong>, with off-diagonal elements <span class="math inline">\(\sigma_{ij}= \text{Cov}(x_i,x_j)\)</span> and the diagonal <span class="math inline">\(\sigma_{ii}= \text{Var}(X_i) = \sigma_i^2\)</span>.</p>
</div>
<div id="properties-of-the-covariance-matrix" class="section level3" number="1.2.4">
<h3>
<span class="header-section-number">1.2.4</span> Properties of the covariance matrix<a class="anchor" aria-label="anchor" href="#properties-of-the-covariance-matrix"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\boldsymbol \Sigma\)</span> is real valued: <span class="math inline">\(\sigma_{ij} \in \mathbb{R}\)</span>
</li>
<li>
<span class="math inline">\(\boldsymbol \Sigma\)</span> is symmetric: <span class="math inline">\(\sigma_{ij} = \sigma_{ji}\)</span>
</li>
<li>The diagonal of <span class="math inline">\(\boldsymbol \Sigma\)</span> contains <span class="math inline">\(\sigma_{ii} = \text{Var}(x_i) = \sigma_i^2\)</span>, i.e. the
variances of the components of <span class="math inline">\(\boldsymbol x\)</span>.</li>
<li>Off-diagonal elements <span class="math inline">\(\sigma_{ij} = \text{Cov}(x_i,x_j)\)</span> represent linear dependencies among the <span class="math inline">\(x_i\)</span>. <span class="math inline">\(\Longrightarrow\)</span> linear regression, correlation</li>
</ol>
<p>How many separate entries does <span class="math inline">\(\boldsymbol \Sigma\)</span> have?</p>
<p><span class="math display">\[\boldsymbol \Sigma= (\sigma_{ij}) = \underbrace{\begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}}_{d\times d}\]</span>
with <span class="math inline">\(\sigma_{ij} = \sigma_{ji}\)</span>.</p>
<p>Number of separate entries: <span class="math inline">\(\frac{d(d+1)}{2}\)</span>.</p>
<p>This numbers grows with the square of the dimension <span class="math inline">\(d\)</span>, i.e. is of order <span class="math inline">\(O(d^2)\)</span>:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th><span class="math inline">\(d\)</span></th>
<th># entries</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>10</td>
<td>55</td>
</tr>
<tr class="odd">
<td>100</td>
<td>5050</td>
</tr>
<tr class="even">
<td>1000</td>
<td>500500</td>
</tr>
<tr class="odd">
<td>10000</td>
<td>50005000</td>
</tr>
</tbody>
</table></div>
<p>For large dimension <span class="math inline">\(d\)</span> the covariance matrix has many components!</p>
<p>–&gt; computationally expensive (both for storage and in handling)
–&gt; very challenging to estimate in high dimensions <span class="math inline">\(d\)</span>.</p>
<p>Note: matrix inversion requires <span class="math inline">\(O(d^3)\)</span> operations using standard algorithms such as <a href="https://en.wikipedia.org/wiki/Gaussian_elimination">Gauss Jordan elimination</a>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra"&gt;Specialised matrix algorithms improve this&lt;/a&gt; to about &lt;span class="math inline"&gt;\(O(d^{2.373})\)&lt;/span&gt;.
Matrices with special symmetries (e.g. diagonal and block diagonal matrices) or properties (e.g. orthogonal matrix) can also be inverted much easier (see Appendix A).&lt;/p&gt;'><sup>1</sup></a> Hence, computing <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> is computationally expensive for large <span class="math inline">\(d\)</span>!</p>
</div>
<div id="eigenvalue-decomposition-of-boldsymbol-sigma" class="section level3" number="1.2.5">
<h3>
<span class="header-section-number">1.2.5</span> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span><a class="anchor" aria-label="anchor" href="#eigenvalue-decomposition-of-boldsymbol-sigma"><i class="fas fa-link"></i></a>
</h3>
<p>Recall the orthogonal eigendecomposition from matrix analysis and linear algebra: A symmetric matrix with real entries has real eigenvalues and a complete set of orthogonal eigenvectors.</p>
<p>Applying eigenvalue decomposition to the covariance matrix yields
<span class="math display">\[
\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T
\]</span>
where <span class="math inline">\(\boldsymbol U\)</span> is an orthogonal matrix containing the eigenvectors
and
<span class="math display">\[\boldsymbol \Lambda= \begin{pmatrix}
    \lambda_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}
\end{pmatrix}\]</span>
contains the eigenvalues <span class="math inline">\(\lambda_i\)</span>.</p>
<p>Importantly, the eigenvalues of the covariance matrix are not only real valued
but they are further constrained to be non-negative.
This can be seen by computing the quadratic form <span class="math inline">\(\boldsymbol z^T \boldsymbol \Sigma\boldsymbol z\)</span>
for a non-zero non-random vector <span class="math inline">\(\boldsymbol z\)</span> which yields
<span class="math display">\[
\begin{split}
\boldsymbol z^T  \boldsymbol \Sigma\boldsymbol z&amp; = \boldsymbol z^T \text{E}\left(  (\boldsymbol x-\boldsymbol \mu) (\boldsymbol x-\boldsymbol \mu)^T  \right) \boldsymbol z\\
 &amp; =  \text{E}\left( \boldsymbol z^T (\boldsymbol x-\boldsymbol \mu) (\boldsymbol x-\boldsymbol \mu)^T \boldsymbol z\right) \\
 &amp; =  \text{E}\left( \left( \boldsymbol z^T (\boldsymbol x-\boldsymbol \mu) \right)^2 \right) \geq 0 \, .\\
\end{split}
\]</span>
Therefore the <strong>covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> is always
positive semi-definite</strong>.</p>
<p>In fact, <strong>unless there is collinearity</strong> ( i.e. a variable is a linear function the other variables) all eigenvalues will be positive and <strong><span class="math inline">\(\boldsymbol \Sigma\)</span> is positive definite</strong>.</p>
</div>
<div id="quantities-related-to-the-covariance-matrix" class="section level3" number="1.2.6">
<h3>
<span class="header-section-number">1.2.6</span> Quantities related to the covariance matrix<a class="anchor" aria-label="anchor" href="#quantities-related-to-the-covariance-matrix"><i class="fas fa-link"></i></a>
</h3>
<div id="correlation-matrix-boldsymbol-p" class="section level4" number="1.2.6.1">
<h4>
<span class="header-section-number">1.2.6.1</span> Correlation matrix <span class="math inline">\(\boldsymbol P\)</span><a class="anchor" aria-label="anchor" href="#correlation-matrix-boldsymbol-p"><i class="fas fa-link"></i></a>
</h4>
<p>The correlation matrix <span class="math inline">\(\boldsymbol P\)</span> (= upper case greek “rho”) is the standardised covariance matrix</p>
<p><span class="math display">\[\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}=\text{Cor}(x_i,x_j)\]</span></p>
<p><span class="math display">\[\rho_{ii} = 1 = \text{Cor}(x_i,x_i)\]</span></p>
<p><span class="math display">\[ \boldsymbol P= (\rho_{ij}) = \begin{pmatrix}
    1 &amp; \dots &amp; \rho_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \rho_{d1} &amp; \dots &amp; 1
\end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol P\)</span> (“upper case rho”) is a symmetric matrix (<span class="math inline">\(\rho_{ij}=\rho_{ji}\)</span>).</p>
<p>Note the <strong>variance-correlation decomposition</strong></p>
<p><span class="math display">\[\boldsymbol \Sigma= \boldsymbol V^{\frac{1}{2}} \boldsymbol P\boldsymbol V^{\frac{1}{2}}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol V\)</span> is a diagonal matrix containing the variances:</p>
<p><span class="math display">\[ \boldsymbol V= \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}\]</span></p>
<p><span class="math display">\[\boldsymbol P= \boldsymbol V^{-\frac{1}{2}}\boldsymbol \Sigma\boldsymbol V^{-\frac{1}{2}}\]</span></p>
<p>This is the definition of correlation written in matrix notation.</p>
</div>
<div id="precision-matrix-or-concentration-matrix" class="section level4" number="1.2.6.2">
<h4>
<span class="header-section-number">1.2.6.2</span> Precision matrix or concentration matrix<a class="anchor" aria-label="anchor" href="#precision-matrix-or-concentration-matrix"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[\boldsymbol \Omega= (\omega_{ij}) = \boldsymbol \Sigma^{-1}\]</span></p>
<p><span class="math inline">\(\boldsymbol \Omega\)</span> (“Omega”) is the inverse of the covariance matrix.</p>
<p>The inverse of the covariance matrix can be obtained via
the spectral decomposition, followed by inverting the eigenvalues <span class="math inline">\(\lambda_i\)</span>:
<span class="math display">\[\boldsymbol \Sigma^{-1} = \boldsymbol U\boldsymbol \Lambda^{-1} \boldsymbol U^T = 
 \boldsymbol U\begin{pmatrix}
    \lambda_{1}^{-1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}^{-1}
\end{pmatrix} \boldsymbol U^T \]</span></p>
<p>Note that <strong>all eigenvalues <span class="math inline">\(\lambda_i\)</span> need to be positive so that <span class="math inline">\(\boldsymbol \Sigma\)</span> can be inverted.</strong> (i.e., <span class="math inline">\(\boldsymbol \Sigma\)</span> needs to be positive definite).<br>
If any <span class="math inline">\(\lambda_i = 0\)</span> then <span class="math inline">\(\boldsymbol \Sigma\)</span> is singular and not invertible.</p>
<p>Importance of <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span>:</p>
<ul>
<li>Many expressions in multivariate statistics contain <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> and not <span class="math inline">\(\boldsymbol \Sigma\)</span>.</li>
<li>
<span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> has close connection with graphical models
(e.g. conditional independence graph, partial correlations).</li>
<li>
<span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> is a natural parameter from an exponential family perspective.</li>
</ul>
</div>
<div id="partial-correlation-matrix" class="section level4" number="1.2.6.3">
<h4>
<span class="header-section-number">1.2.6.3</span> Partial correlation matrix<a class="anchor" aria-label="anchor" href="#partial-correlation-matrix"><i class="fas fa-link"></i></a>
</h4>
<p>This is a standardised version of the precision matrix, see later chapter on graphical models.</p>
</div>
<div id="total-variation-and-generalised-variance" class="section level4" number="1.2.6.4">
<h4>
<span class="header-section-number">1.2.6.4</span> Total variation and generalised variance<a class="anchor" aria-label="anchor" href="#total-variation-and-generalised-variance"><i class="fas fa-link"></i></a>
</h4>
<p>To summarise the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> in a single scalar value there are two commonly used
measures:</p>
<ul>
<li>
<strong>total variation</strong>: <span class="math inline">\(\text{Tr}(\boldsymbol \Sigma) = \sum_{i=1}^d \lambda_i\)</span>
</li>
<li>
<strong>generalised variance</strong>: <span class="math inline">\(\det(\boldsymbol \Sigma) = \prod_{i=1}^d \lambda_i\)</span>
</li>
</ul>
<p>The generalised variance <span class="math inline">\(\det(\boldsymbol \Sigma)\)</span> is also known as the volume of <span class="math inline">\(\boldsymbol \Sigma\)</span>.</p>
</div>
</div>
</div>
<div id="multivariate-normal-distribution" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Multivariate normal distribution<a class="anchor" aria-label="anchor" href="#multivariate-normal-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>The multivariate normal model is a generalisation of the univariate normal distribution
from dimension 1 to dimension <span class="math inline">\(d\)</span>.</p>
<div id="univariate-normal-distribution" class="section level3" number="1.3.1">
<h3>
<span class="header-section-number">1.3.1</span> Univariate normal distribution:<a class="anchor" aria-label="anchor" href="#univariate-normal-distribution"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[\text{Dimension } d = 1\]</span>
<span class="math display">\[x \sim N(\mu, \sigma^2)\]</span>
<span class="math display">\[\text{E}(x) = \mu \space , \space  \text{Var}(x) = \sigma^2\]</span></p>
<p><strong>Density</strong>:</p>
<p><span class="math display">\[f(x |\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right) \]</span></p>
<p><strong>Plot of univariate normal density </strong>:</p>
<p>Unimodal with peak at <span class="math inline">\(\mu\)</span>, width determined by <span class="math inline">\(\sigma\)</span> (in this plot: <span class="math inline">\(\mu=2, \sigma^2=1\)</span> )</p>
<div class="inline-figure"><img src="1-multivariate_files/figure-html/unnamed-chunk-1-1.png" width="672"></div>
<p>Special case: <strong>standard normal</strong> with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=1\)</span>:</p>
<p><span class="math display">\[f(x |\mu=0,\sigma^2=1)=\frac{1}{\sqrt{2\pi}} \exp\left( {-\frac{x^2}{2}} \right) \]</span></p>
<p><strong>Differential entropy</strong>:<br><span class="math display">\[
H(F) = \frac{1}{2} (\log(2 \pi \sigma^2) + 1) 
\]</span></p>
<p><strong>Cross-entropy</strong>:<br><span class="math display">\[
H(F_{\text{ref}}, F) = \frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 } 
 +\frac{\sigma^2_{\text{ref}}}{\sigma^2}  +\log(2 \pi \sigma^2) \right)
\]</span>
<strong>KL divergence</strong>:<br><span class="math display">\[
D_{\text{KL}}(F_{\text{ref}}, F) = H(F_{\text{ref}}, F) - H(F_{\text{ref}}) = 
\frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 } 
 +\frac{\sigma^2_{\text{ref}}}{\sigma^2}  -\log\left(\frac{\sigma^2_{\text{ref}}}{ \sigma^2}\right) -1
\right)
\]</span></p>
<p><strong>Maximum entropy characterisation:</strong> the normal distribution is the unique distribution
that has the
highest (differential) entropy over all continuous distributions with support from minus infinity to plus infinity with a given mean and variance.</p>
<p>This is in fact one of the reasons why the normal distribution is so important (und useful) –
if we only know that a random variable has a mean and variance, and not much else, then using the
normal distribution will be a reasonable and well justified working assumption!</p>
</div>
<div id="multivariate-normal-model" class="section level3" number="1.3.2">
<h3>
<span class="header-section-number">1.3.2</span> Multivariate normal model<a class="anchor" aria-label="anchor" href="#multivariate-normal-model"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[\text{Dimension } d\]</span>
<span class="math display">\[\boldsymbol x\sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\]</span>
<span class="math display">\[\boldsymbol x\sim \text{MVN}(\boldsymbol \mu,\boldsymbol \Sigma) \]</span>
<span class="math display">\[\text{E}(\boldsymbol x) = \boldsymbol \mu\space , \space  \text{Var}(\boldsymbol x) = \boldsymbol \Sigma\]</span></p>
<p><strong>Density</strong>:</p>
<p><span class="math display">\[f(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma) = \det(2 \pi \boldsymbol \Sigma)^{-\frac{1}{2}} \exp\left({{-\frac{1}{2}} \underbrace{\underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1 \times d} \underbrace{\boldsymbol \Sigma^{-1}}_{d \times d} \underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d \times 1} }_{1 \times 1 = \text{scalar!}}}\right)\]</span></p>
<ul>
<li>note that density contains precision matrix <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span>
</li>
<li>inverting <span class="math inline">\(\boldsymbol \Sigma\)</span> implies inverting the eigenvalues <span class="math inline">\(\lambda_i\)</span> of <span class="math inline">\(\boldsymbol \Sigma\)</span>
(thus we need <span class="math inline">\(\lambda_i &gt; 0\)</span>)</li>
<li>density also contains <span class="math inline">\(\det(\boldsymbol \Sigma) = \prod\limits_{i=1}^d \lambda_i\)</span> <span class="math inline">\(\equiv\)</span> product of eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span>
</li>
</ul>
<p>Special case: <strong>standard multivariate normal</strong> with <span class="math display">\[\boldsymbol \mu=\boldsymbol 0, \boldsymbol \Sigma=\boldsymbol I=\begin{pmatrix}
    1 &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; 1
\end{pmatrix}\]</span></p>
<p><span class="math display">\[f(\boldsymbol x| \boldsymbol \mu=\boldsymbol 0,\boldsymbol \Sigma=\boldsymbol I)=(2\pi)^{-d/2}\exp\left( -\frac{1}{2} \boldsymbol x^T \boldsymbol x\right) = \prod\limits_{i=1}^d \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x_i^2}{2}\right)\]</span>
which is equivalent to the product of <span class="math inline">\(d\)</span> univariate standard normals!</p>
<p><strong>Misc:</strong></p>
<ul>
<li>for <span class="math inline">\(d=1\)</span>, multivariate normal reduces to normal.</li>
<li>for <span class="math inline">\(\boldsymbol \Sigma\)</span> diagonal (i.e. <span class="math inline">\(\boldsymbol P= \boldsymbol I\)</span>, no correlation), MVN is the product of univariate normals (see Worksheet 2).</li>
</ul>
<p><strong>Plot of MVN density</strong>:</p>
<div class="inline-figure"><img src="1-multivariate_files/figure-html/fig1-1.png" width="672"></div>
<ul>
<li>Location: <span class="math inline">\(\boldsymbol \mu\)</span><br>
</li>
<li>Shape: <span class="math inline">\(\boldsymbol \Sigma\)</span><br>
</li>
<li>Unimodal: <strong>one</strong> peak<br>
</li>
<li>Support from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span> in each dimension</li>
</ul>
<p>An interactive <a href="https://shiny.rstudio.com/">R Shiny web app</a> of the bivariate normal density plot
is available online at <a href="https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/" class="uri">https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/</a> .</p>
<p><strong>Differential entropy</strong>:<br><span class="math display">\[
H = \frac{1}{2} (\log \det(2 \pi \boldsymbol \Sigma) + d) 
\]</span></p>
<p><strong>Cross-entropy</strong>:<br><span class="math display">\[
H(F_{\text{ref}}, F) = \frac{1}{2}   \biggl\{
        (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})
      + \text{Tr}\biggl(\boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
    + \log \det \biggl( 2 \pi \boldsymbol \Sigma\biggr)    \biggr\} 
\]</span>
<strong>KL divergence</strong>:<br><span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\text{ref}}, F) &amp;= H(F_{\text{ref}}, F) - H(F_{\text{ref}}) \\
&amp;= \frac{1}{2}   \biggl\{
        (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})
      + \text{Tr}\biggl(\boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
    - \log \det \biggl( \boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr) 
     - d   \biggr\} \\
\end{split}
\]</span></p>
</div>
<div id="shape-of-the-multivariate-normal-density" class="section level3" number="1.3.3">
<h3>
<span class="header-section-number">1.3.3</span> Shape of the multivariate normal density<a class="anchor" aria-label="anchor" href="#shape-of-the-multivariate-normal-density"><i class="fas fa-link"></i></a>
</h3>
<p>Now we show that the contour lines of the multivariate normal density always take on the form of an ellipse, and that the radii of the ellipse is determined by the eigenvalues of
<span class="math inline">\(\boldsymbol \Sigma\)</span>.</p>
<p>We start by observing that a circle with radius <span class="math inline">\(r\)</span> around the origin can be described as the set of points <span class="math inline">\((x_1,x_2)\)</span> satisfying
<span class="math inline">\(x_1^2+x_2^2 = r^2\)</span>, or equivalently, <span class="math inline">\(\frac{x_1^2}{r^2} + \frac{x_2^2}{r^2} = 1\)</span>.
This is generalised to the shape of an ellipse by allowing (in two dimensions) for two radii
<span class="math inline">\(r_1\)</span> and <span class="math inline">\(r_2\)</span> with
<span class="math inline">\(\frac{x_1^2}{r_1^2} + \frac{x_2^2}{r_2^2} = 1\)</span>, or in vector notation
<span class="math inline">\(\boldsymbol x^T \text{Diag}(r_1^2, r_2^2)^{-1} \boldsymbol x= 1\)</span>. In <span class="math inline">\(d\)</span> dimensions and allowing for rotation of
the axes and a shift of the origin from 0 to <span class="math inline">\(\boldsymbol \mu\)</span> the condition for an ellipse is
<span class="math display">\[(\boldsymbol x-\boldsymbol \mu)^T \boldsymbol Q\, \text{Diag}(r_1^2, \ldots , r_d^2)^{-1} \boldsymbol Q^T (\boldsymbol x-\boldsymbol \mu) = 1\]</span>
where <span class="math inline">\(\boldsymbol Q\)</span> is an orthogonal matrix whose column vectors indicate the direction of the axes.</p>
<p>A contour line of a probability density function is a set of connected points where the density assumes the same constant value. In the case of the multivariate normal distribution keeping the density at some fixed value implies that <span class="math inline">\((\boldsymbol x-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu) = c\)</span> where <span class="math inline">\(c\)</span> is a constant. Using the eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span> we can rewrite this condition as
<span class="math display">\[
(\boldsymbol x-\boldsymbol \mu)^T \boldsymbol U\boldsymbol \Lambda^{-1} \boldsymbol U^T (\boldsymbol x-\boldsymbol \mu) = c \,.
\]</span>
This implies that</p>
<ul>
<li>the contour lines of the multivariate normal density are indeed ellipses,</li>
<li>the squared radii are proportional to the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span> and</li>
<li>the direction of the axes correspond to the eigenvectors in <span class="math inline">\(\boldsymbol U\)</span>.</li>
</ul>
<p>Equivalently, the positive square roots of the eigenvalues are proportional to the radii of the ellipse. Hence, for a singular covariance matrix with one or more <span class="math inline">\(\lambda_i=0\)</span> the corresponding radii are zero.</p>
<p>An interactive <a href="https://shiny.rstudio.com/">R Shiny web app</a> to play with the contour lines of the
bivariate normal distribution is online at <a href="https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/" class="uri">https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/</a> .</p>
</div>
<div id="three-types-of-covariances" class="section level3" number="1.3.4">
<h3>
<span class="header-section-number">1.3.4</span> Three types of covariances<a class="anchor" aria-label="anchor" href="#three-types-of-covariances"><i class="fas fa-link"></i></a>
</h3>
<p>Following the above we can parameterise a covariance matrix in terms of its
i) volume, ii) shape, and iii) orientation by writing
<span class="math display">\[
\boldsymbol \Sigma= \kappa \, \boldsymbol U\boldsymbol A\boldsymbol U^T
\]</span>
with <span class="math inline">\(\boldsymbol A=\text{Diag}(a_1, \ldots, a_d)\)</span> and <span class="math inline">\(\prod_{i=1}^d a_i = 1\)</span>.</p>
<p>Note that the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span> are <span class="math inline">\(\lambda_i = \kappa a_i\)</span>.</p>
<ol style="list-style-type: lower-roman">
<li>The <strong>volume</strong> is <span class="math inline">\(\det(\boldsymbol \Sigma) = \kappa^d\)</span>, determined by a single parameter <span class="math inline">\(\kappa\)</span>.</li>
<li>The <strong>shape</strong> is determined by <span class="math inline">\(\boldsymbol A\)</span>, with <span class="math inline">\(d-1\)</span> free parameters.</li>
<li>The <strong>orientation</strong> is given by the orthogonal matrix <span class="math inline">\(\boldsymbol U\)</span>, with <span class="math inline">\(d (d-1)/2\)</span> free parameters.</li>
</ol>
<p>This leads to classification of covariances into three varieties:</p>
<p><strong>Type 1:</strong> <strong>spherical covariance</strong> <span class="math inline">\(\boldsymbol \Sigma=\kappa \boldsymbol I\)</span>,
with spherical contour lines, 1 free parameter (<span class="math inline">\(\boldsymbol A=\boldsymbol I\)</span>, <span class="math inline">\(\boldsymbol U=\boldsymbol I\)</span>).</p>
<p>Example:
<span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\)</span>
with <span class="math inline">\(\sqrt{\lambda_1/ \lambda_2} = 1\)</span>:</p>
<div class="inline-figure"><img src="1-multivariate_files/figure-html/fig2-1.png" width="672"></div>
<p><strong>Type 2</strong>: <strong>diagonal covariance</strong> <span class="math inline">\(\boldsymbol \Sigma= \kappa \boldsymbol A\)</span>, with elliptical contour lines and axes of the ellipse oriented parallel to the coordinates, <span class="math inline">\(d\)</span> free parameters (<span class="math inline">\(\boldsymbol U=\boldsymbol I\)</span>).</p>
<p>Example:
<span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\)</span>
with <span class="math inline">\(\sqrt{\lambda_1 / \lambda_2} \approx 1.41\)</span>:</p>
<div class="inline-figure"><img src="1-multivariate_files/figure-html/fig3-1.png" width="672"></div>
<p><strong>Type 3</strong>: <strong>general unrestricted covariance</strong> <span class="math inline">\(\boldsymbol \Sigma\)</span>,
with elliptical contour lines, with axes of the ellipse oriented according to the
column vectors in <span class="math inline">\(\boldsymbol U\)</span>,
<span class="math inline">\(d (d+1)/2\)</span> free parameters.</p>
<p>Example:
<span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix} 2 &amp; 0.6 \\ 0.6 &amp; 1 \end{pmatrix}\)</span>
with <span class="math inline">\(\sqrt{\lambda_1 / \lambda_2} \approx 2.20\)</span>:</p>
<div class="inline-figure"><img src="1-multivariate_files/figure-html/fig4-1.png" width="672"></div>
</div>
<div id="concentration-of-probability-mass-for-small-and-large-dimension" class="section level3" number="1.3.5">
<h3>
<span class="header-section-number">1.3.5</span> Concentration of probability mass for small and large dimension<a class="anchor" aria-label="anchor" href="#concentration-of-probability-mass-for-small-and-large-dimension"><i class="fas fa-link"></i></a>
</h3>
<p>The density of the multivariate normal distribution is of bell shape with a single mode. Thus, intuitively one may believe that all probability mass must be concentrated around this peak. While this is true for small dimensions we now show that this is completely incorrect for high dimensions.</p>
<p>For simplicity we consider the standard multivariate normal distribution <span class="math inline">\(\boldsymbol x\sim N_d(\boldsymbol 0, \boldsymbol I_d)\)</span> with a spherical covariance <span class="math inline">\(\boldsymbol I_d\)</span> and sample <span class="math inline">\(\boldsymbol x\)</span>.
The squared Euclidean length of <span class="math inline">\(\boldsymbol x\)</span> is given by <span class="math inline">\(a= || \boldsymbol x||^2 = \boldsymbol x^T \boldsymbol x= \sum_{i=1}^d x_i^2\)</span>. We note that <span class="math inline">\(a \sim \text{$\chi^2_{d}$}\)</span> is chi-squared distributed
with degree of freedom <span class="math inline">\(d\)</span> as each individual component is distributed as <span class="math inline">\(x_i \sim N(0,1)\)</span>.
The density of a <span class="math inline">\(d\)</span>-dimensional multivariate normal as a function of <span class="math inline">\(a\)</span> is <span class="math inline">\(g_d(a) = (2\pi)^{-d/2} e^{-a/2}\)</span>.</p>
<p>A natural way to define the main part of the “bell” of the multivariate normal as the
the set of all values <span class="math inline">\(a\)</span> for which the density is larger than a given small fraction <span class="math inline">\(\eta\)</span> (say 0.001) of the value of the density at the peak.
To formalise
<span class="math display">\[
B = \{ a: g_d(a) &gt; \eta \, g_d(0)   \}
\]</span>
Equivalently, <span class="math inline">\(B = \{ a: \frac{g_d(a)}{ g_d(0)} =e^{-a/2} &gt; \eta \}\)</span> or
<span class="math inline">\(B = \{ a: a &lt; -2 \log(\eta) \}\)</span>.</p>
<p>Since we know that <span class="math inline">\(a\)</span> is chi-squared distributed
the probability <span class="math inline">\(\text{Pr}(a \in B)\)</span> of a random <span class="math inline">\(a\)</span> to lie within the bell <span class="math inline">\(B\)</span> is now easily computed using the cumulative density function of the chi-squared distribution. There is no analytic formula but we can compute this probability numerically in dependence of the dimension <span class="math inline">\(d\)</span>:</p>
<p><img src="1-multivariate_files/figure-html/unnamed-chunk-3-1.png" width="672">
The above plot is for <span class="math inline">\(\eta=0.001\)</span>. You can see that only for dimensions up to around <span class="math inline">\(d=10\)</span>
is the probability mass concentrated the bell in the center but from <span class="math inline">\(d=30\)</span> it has moved completely to the tail of the distribution outside the bell!</p>
</div>
</div>
<div id="estimation-in-large-sample-and-small-sample-settings" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> Estimation in large sample and small sample settings<a class="anchor" aria-label="anchor" href="#estimation-in-large-sample-and-small-sample-settings"><i class="fas fa-link"></i></a>
</h2>
<p>In practical application of multivariate normal model we need to
learn its parameters from data. We first consider the case when
there are many measurements available, and then second the case when
the number of data points is small compared to the number of parameters.</p>
<p>In a previous course in year 2
(see <a href="https://strimmerlab.github.io/publications/lecture-notes/MATH20802/">MATH20802 Statistical Methods</a>)
the method of maximum likelihood as well as essential of Bayesian statistics
were introduced. Here we apply these approaches in the setting of the
multivariate normal distribution.</p>
<div id="multivariate-data" class="section level3" number="1.4.1">
<h3>
<span class="header-section-number">1.4.1</span> Multivariate data<a class="anchor" aria-label="anchor" href="#multivariate-data"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Vector notation:</strong></p>
<p>Samples from a multivariate normal distribution are <em>vectors</em> (not scalars as for univariate normal):
<span class="math display">\[\boldsymbol x_1,\boldsymbol x_2,...,\boldsymbol x_n \stackrel{\text{iid}}\sim N_d\left(\boldsymbol \mu,\boldsymbol \Sigma\right)\]</span></p>
<p><strong>Matrix and component notation:</strong></p>
<p>All the data points are commonly collected into a matrix <span class="math inline">\(\boldsymbol X\)</span>.</p>
<p>In statistics the convention is to store each data vector in the rows of <span class="math inline">\(\boldsymbol X\)</span>:</p>
<p><span class="math display">\[\boldsymbol X= (\boldsymbol x_1,\boldsymbol x_2,...,\boldsymbol x_n)^T = \begin{pmatrix}
    x_{11}  &amp; x_{12} &amp; \dots &amp; x_{1d}   \\
    x_{21}  &amp; x_{22} &amp; \dots &amp; x_{2d}   \\
    \vdots \\
    x_{n1}  &amp; x_{n2} &amp; \dots &amp; x_{nd}
\end{pmatrix}\]</span></p>
<p>Therefore,
<span class="math display">\[\boldsymbol x_1=\begin{pmatrix}
    x_{11}       \\
    \vdots \\
    x_{1d}
\end{pmatrix} , \space \boldsymbol x_2=\begin{pmatrix}
    x_{21}       \\
    \vdots \\
    x_{2d}
\end{pmatrix} , \ldots , \boldsymbol x_n=\begin{pmatrix}
    x_{n1}       \\
    \vdots \\
    x_{nd}
\end{pmatrix}\]</span></p>
<p>Thus, in statistics the first index runs over <span class="math inline">\((1,...,n)\)</span> and denotes the samples while the second index runs over <span class="math inline">\((1,...,d)\)</span> and refers to the variables.</p>
<p>The statistics convention on data matrices is <em>not</em> universal! In fact, in most of the machine learning literature in engineering and computer science the data samples are stored in the columns so that the variables appear in the rows (thus in the engineering convention the data matrix is transposed compared to the statistics convention).</p>
<p>In order to avoid confusion it is recommended to use vector notation for data instead of
matrix notation because this is not ambiguous.</p>
</div>
<div id="strategies-for-large-sample-estimation" class="section level3" number="1.4.2">
<h3>
<span class="header-section-number">1.4.2</span> Strategies for large sample estimation<a class="anchor" aria-label="anchor" href="#strategies-for-large-sample-estimation"><i class="fas fa-link"></i></a>
</h3>
<div id="empirical-estimators-outline" class="section level4" number="1.4.2.1">
<h4>
<span class="header-section-number">1.4.2.1</span> Empirical estimators (outline)<a class="anchor" aria-label="anchor" href="#empirical-estimators-outline"><i class="fas fa-link"></i></a>
</h4>
<p>For large <span class="math inline">\(n\)</span> we have thanks to the law of large numbers:
<span class="math display">\[\underbrace{F}_{\text{true}} \approx \underbrace{\widehat{F}}_{\text{empirical}}\]</span></p>
<p>We now would like to estimate <span class="math inline">\(A\)</span> which is a functional of <span class="math inline">\(F\)</span>, i.e. <span class="math inline">\(A=m(F)\)</span>.
For example the mean, the median or some other quantity.</p>
<p>The <em>empirical estimate</em> is obtained by replacing the unknown true distribution
<span class="math inline">\(F\)</span> with the observed empirical distribution: <span class="math inline">\(\hat{A} = m(\widehat{F})\)</span>.</p>
<p>For example, the expectation of a random variable is approximated/estimated
as the average over the observation:
<span class="math display">\[\text{E}_F(\boldsymbol x) \approx \text{E}_{\widehat{F}}(\boldsymbol x) = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k\]</span>
<span class="math display">\[\text{E}_F(g(\boldsymbol x)) \approx  \text{E}_{\widehat{F}}(g(\boldsymbol x)) = \frac{1}{n}\sum^{n}_{k=1} g(\boldsymbol x_k)\]</span></p>
<p><strong>Simple recipe to obtain an empirical estimator</strong>: simply replace the expectation operator
by the sample average in the quantity of interest.</p>
<p><strong>What does this work:</strong> the empirical distribution <span class="math inline">\(\widehat{F}\)</span> is actually the nonparametric maximum likelihood estimate of <span class="math inline">\(F\)</span> (see below for likelihood estimation).</p>
<p>Note: the approximation of <span class="math inline">\(F\)</span> by <span class="math inline">\(\widehat{F}\)</span> also the basis other approaches such as Efron’s bootstrap method.</p>
</div>
<div id="maximum-likelihood-estimation-outline" class="section level4" number="1.4.2.2">
<h4>
<span class="header-section-number">1.4.2.2</span> Maximum likelihood estimation (outline)<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimation-outline"><i class="fas fa-link"></i></a>
</h4>
<p>R.A. Fisher (1922): model-based estimators using the density or probability mass function</p>
<p><strong>log-likelihood function</strong>:
<span class="math display">\[\log L(\boldsymbol \theta) = \sum^{n}_{k=1}  \underbrace{\log f}_{\text{log-density}}(\underbrace{x_i}_{\text{data}} |\underbrace{\boldsymbol \theta}_{\text{parameters}})\]</span>
likelihood = probability to observe data given the model parameters</p>
<p><strong>Maximum likelihood estimate:</strong>
<span class="math display">\[\hat{\boldsymbol \theta}^{\text{ML}}=\underset{\boldsymbol \theta}{\arg\,\max} \log L(\boldsymbol \theta)\]</span></p>
<p>Maximum likelihood (ML) finds the parameters that make the observed data most likely (it does <em>not</em> find the most probable model!)</p>
<p>Recall from <a href="https://strimmerlab.github.io/publications/lecture-notes/MATH20802/">MATH20802 Statistical Methods</a>
that maximum likelihood is closely linked to minimising the relative entropy (KL divergence)
<span class="math inline">\(D_{\text{KL}}(F, F_{\boldsymbol \theta})\)</span> between the unknown true model <span class="math inline">\(F\)</span> to the specified model <span class="math inline">\(F_{\boldsymbol \theta}\)</span>. Specifically, for large
sample size <span class="math inline">\(n\)</span> the model <span class="math inline">\(F_{\hat{\boldsymbol \theta}}\)</span> fit by maximum likelihood is indeed the model that is closest to <span class="math inline">\(F\)</span>.</p>
<p>Correspondingly, the great appeal of <strong>maximum likelihood estimates</strong> (MLEs) is that they <strong>are optimal for large</strong> <span class="math inline">\(\mathbf{n}\)</span>, i.e. so that <strong>for large sample size no estimator can be constructed that outperforms the MLE</strong> (note the emphasis on “for large <span class="math inline">\(n\)</span>”!).
A further advantage of the method of maximum likelihood is that it does not only provide a point estimate but also the asymptotic error (via the Fisher information which is related to the curvature of the log-likelihood function).</p>
</div>
</div>
<div id="large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma" class="section level3" number="1.4.3">
<h3>
<span class="header-section-number">1.4.3</span> Large sample estimates of mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance <span class="math inline">\(\boldsymbol \Sigma\)</span><a class="anchor" aria-label="anchor" href="#large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma"><i class="fas fa-link"></i></a>
</h3>
<div id="empirical-estimates" class="section level4" number="1.4.3.1">
<h4>
<span class="header-section-number">1.4.3.1</span> Empirical estimates:<a class="anchor" aria-label="anchor" href="#empirical-estimates"><i class="fas fa-link"></i></a>
</h4>
<p>Recall the definitions:
<span class="math display">\[
\boldsymbol \mu= \text{E}(\boldsymbol x)
\]</span>
and
<span class="math display">\[
\boldsymbol \Sigma= \text{E}\left(   (\boldsymbol x-\boldsymbol \mu) (\boldsymbol x-\boldsymbol \mu)^T \right)
\]</span></p>
<p>For the empirical estimate we replace the expectations by the
corresponding sample averages.</p>
<p>These resulting estimators can be written in three different ways:</p>
<p><strong>Vector notation:</strong></p>
<p><span class="math display">\[\hat{\boldsymbol \mu} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k\]</span></p>
<p><span class="math display">\[
\widehat{\boldsymbol \Sigma} = \frac{1}{n}\sum^{n}_{k=1} (\boldsymbol x_k-\hat{\boldsymbol \mu})  (\boldsymbol x_k-\hat{\boldsymbol \mu})^T
= \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k  \boldsymbol x_k^T - \hat{\boldsymbol \mu} \hat{\boldsymbol \mu}^T
\]</span></p>
<p><strong>Data matrix notation:</strong></p>
<p>The empirical mean and covariance can also be written in terms of the data matrix <span class="math inline">\(\boldsymbol X\)</span> (using statistics convention):</p>
<p><span class="math display">\[\hat{\boldsymbol \mu} = \frac{1}{n} \boldsymbol X^T \boldsymbol 1_n\]</span></p>
<p><span class="math display">\[\widehat{\boldsymbol \Sigma} = \frac{1}{n} \boldsymbol X^T \boldsymbol X- \hat{\boldsymbol \mu} \hat{\boldsymbol \mu}^T\]</span></p>
<p>See Worksheet 2 for details.</p>
<p><strong>Component notation:</strong></p>
<p>The corresponding component notation with <span class="math inline">\(\boldsymbol X= (x_{ki})\)</span> is:</p>
<p><span class="math display">\[\hat{\mu}_i = \frac{1}{n}\sum^{n}_{k=1} x_{ki}\]</span></p>
<p><span class="math display">\[\hat{\sigma}_{ij} = \frac{1}{n}\sum^{n}_{k=1} (x_{ki}-\hat{\mu}_i) ( 
x_{kj}-\hat{\mu}_j )\]</span></p>
<p><span class="math display">\[\hat{\boldsymbol \mu}=\begin{pmatrix}
    \hat{\mu}_{1}       \\
    \vdots \\
    \hat{\mu}_{d}
\end{pmatrix}, \widehat{\boldsymbol \Sigma} = (\hat{\sigma}_{ij})\]</span></p>
<p>Variance estimate:<br><span class="math display">\[\hat{\sigma}_{ii} = \frac{1}{n}\sum^{n}_{k=1} \left(x_{ki}-\hat{\mu}_i\right)^2\]</span>
Note the factor <span class="math inline">\(\frac{1}{n}\)</span> (not <span class="math inline">\(\frac{1}{n-1}\)</span>)</p>
<p><strong>Engineering and machine learning convention:</strong></p>
<p>Using the engineering and machine learning convention for the data matrix <span class="math inline">\(\boldsymbol X\)</span> the estimators are written as</p>
<p><span class="math display">\[\hat{\boldsymbol \mu} = \frac{1}{n} \boldsymbol X\boldsymbol 1_n\]</span></p>
<p><span class="math display">\[\widehat{\boldsymbol \Sigma} = \frac{1}{n} \boldsymbol X\boldsymbol X^T - \hat{\boldsymbol \mu} \hat{\boldsymbol \mu}^T\]</span></p>
<p>In the corresponding component notation the two indices for the columns and rowns are interchanged.</p>
<p>To avoid confusion when using matrix or component notation you need to always state which
convention is used! In these notes we strictly follow the statistics convention.</p>
</div>
<div id="maximum-likelihood-estimates" class="section level4" number="1.4.3.2">
<h4>
<span class="header-section-number">1.4.3.2</span> Maximum likelihood estimates<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimates"><i class="fas fa-link"></i></a>
</h4>
<p>We now derive the MLE of the parameters <span class="math inline">\(\boldsymbol \mu\)</span> and <span class="math inline">\(\boldsymbol \Sigma\)</span> of the multivariate normal distribution.
The corresponding log-likelihood function is
<span class="math display">\[
\begin{split}
\log L(\boldsymbol \mu, \boldsymbol \Sigma) &amp; = \sum_{k=1}^n \log f( \boldsymbol x_k | \boldsymbol \mu, \boldsymbol \Sigma) \\
  &amp; = -\frac{n d}{2} \log(2\pi) -\frac{n}{2} \log \det(\boldsymbol \Sigma)  
   - \frac{1}{2}  \sum_{k=1}^n  (\boldsymbol x_k-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x_k-\boldsymbol \mu) \,.\\
\end{split}
\]</span>
Written in terms of the precision matrix <span class="math inline">\(\boldsymbol \Omega= \boldsymbol \Sigma^{-1}\)</span> this becomes
<span class="math display">\[
\log L(\boldsymbol \mu, \boldsymbol \Omega) = -\frac{n d}{2} \log(2\pi) +\frac{n}{2} \log \det(\boldsymbol \Omega)  - \frac{1}{2}  \sum_{k=1}^n  (\boldsymbol x_k-\boldsymbol \mu)^T \boldsymbol \Omega(\boldsymbol x_k-\boldsymbol \mu) \,.
\]</span>
First, to find the MLE for <span class="math inline">\(\boldsymbol \mu\)</span> we compute
<span class="math display">\[\nabla_{\boldsymbol \mu} \log L(\boldsymbol \mu, \boldsymbol \Omega) =  \frac{\partial \log L(\boldsymbol \mu, \boldsymbol \Omega) }{\partial \boldsymbol \mu}= \sum_{k=1}^n  \boldsymbol \Omega(\boldsymbol x_k-\boldsymbol \mu)\]</span>
noting that <span class="math inline">\(\boldsymbol \Omega\)</span> is symmetric (see the Appendix for rules in vector calculus).
Setting this equal to zero we get <span class="math inline">\(\sum_{k=1}^n \boldsymbol x_k = n \hat{\boldsymbol \mu}_{ML}\)</span> and thus
<span class="math display">\[\hat{\boldsymbol \mu}_{ML} = \frac{1}{n} \sum_{k=1}^n \boldsymbol x_k\,.\]</span></p>
<p>Next, to obtain the MLE for <span class="math inline">\(\boldsymbol \Omega\)</span> we compute
<span class="math display">\[\frac{\partial \log L(\boldsymbol \mu, \boldsymbol \Omega) }{\partial \boldsymbol \Omega}=\frac{n}{2}\boldsymbol \Omega^{-1} - \frac{1}{2}  \sum_{k=1}^n (\boldsymbol x_k-\boldsymbol \mu) (\boldsymbol x_k-\boldsymbol \mu)^T\]</span>
(see the Appendix for rules in matrix calculus).
Setting this equal to zero and substituting the MLE for <span class="math inline">\(\boldsymbol \mu\)</span> we get
<span class="math display">\[\widehat{\boldsymbol \Omega}^{-1}_{ML}=  \frac{1}{n} \sum_{k=1}^n  (\boldsymbol x_k-\hat{\boldsymbol \mu}) (\boldsymbol x_k-\hat{\boldsymbol \mu})^T=\widehat{\boldsymbol \Sigma}_{ML}\,.\]</span></p>
<p>Therefore, the MLEs are identical to the empirical estimates.</p>
<p>Note the factor <span class="math inline">\(\frac{1}{n}\)</span> in the MLE of the covariance matrix.</p>
</div>
<div id="distribution-of-the-empirical-maximum-likelihood-estimates" class="section level4" number="1.4.3.3">
<h4>
<span class="header-section-number">1.4.3.3</span> Distribution of the empirical / maximum likelihood estimates<a class="anchor" aria-label="anchor" href="#distribution-of-the-empirical-maximum-likelihood-estimates"><i class="fas fa-link"></i></a>
</h4>
<p>With <span class="math inline">\(\boldsymbol x_1,...,\boldsymbol x_n \sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> one can find the exact distributions
of the estimators.</p>
<p><strong>1. Distribution of the estimate of the mean:</strong></p>
<p><span class="math display">\[\hat{\boldsymbol \mu}_{ML} \sim N_d\left(\boldsymbol \mu, \frac{\boldsymbol \Sigma}{n}\right)\]</span>
Since
<span class="math inline">\(\text{E}(\hat{\boldsymbol \mu}_{ML}) = \boldsymbol \mu\Longrightarrow \hat{\boldsymbol \mu}_{ML}\)</span> is unbiased.</p>
<p><strong>2. Distribution of the covariance estimate:</strong></p>
<p><span class="math display">\[\widehat{\boldsymbol \Sigma}_{ML} \sim \text{Wishart}\left(\frac{\boldsymbol \Sigma}{n}, n-1\right)\]</span>
Since
<span class="math inline">\(\text{E}(\widehat{\boldsymbol \Sigma}_{ML}) = \frac{n-1}{n}\boldsymbol \Sigma\)</span> <span class="math inline">\(\Longrightarrow \widehat{\boldsymbol \Sigma}_{ML}\)</span> is biased, with <span class="math inline">\(\text{Bias}(\widehat{\boldsymbol \Sigma}_{ML} ) = \boldsymbol \Sigma- \text{E}(\widehat{\boldsymbol \Sigma}_{ML}) = -\frac{\boldsymbol \Sigma}{n}\)</span>.</p>
<p>Easy to make unbiased:
<span class="math inline">\(\widehat{\boldsymbol \Sigma}_{UB} = \frac{n}{n-1}\widehat{\boldsymbol \Sigma}_{ML}=\frac{1}{n-1}\sum^n_{k=1}\left(\boldsymbol x_k-\hat{\boldsymbol \mu}\right)\left(\boldsymbol x_k-\hat{\boldsymbol \mu}\right)^T\)</span> is unbiased.</p>
<p>But unbiasedness of an estimator is <strong>not</strong> a very relevant criterion in multivariate statistics as we will see in the next section.</p>
</div>
</div>
<div id="problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions" class="section level3" number="1.4.4">
<h3>
<span class="header-section-number">1.4.4</span> Problems with maximum likelihood in small sample settings and high dimensions<a class="anchor" aria-label="anchor" href="#problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Modern data is high dimensional!</strong></p>
<p>Data sets with <span class="math inline">\(n&lt;d\)</span>, i.e. high dimension <span class="math inline">\(d\)</span> and small sample size <span class="math inline">\(n\)</span> are now common in
many fields, e.g., medicine, biology but also finance and business analytics.</p>
<p><span class="math display">\[n = 100 \, \text{(e.g, patients/samples)}\]</span>
<span class="math display">\[d = 20000 \, \text{(e.g., genes/SNPs/proteins/variables)}\]</span>
Reasons:</p>
<ul>
<li>the number of measured variables is increasing quickly with technological advances (e.g. genomics)</li>
<li>but the number of samples cannot be similary increased (for cost and ethical reasons)</li>
</ul>
<p><strong>General problems of MLEs:</strong></p>
<ol style="list-style-type: decimal">
<li>ML estimators are optimal only if <strong>sample size is large</strong> compared to the number of parameters. However, this optimality is not any more valid if sample size is moderate or smaller than the number of parameters.</li>
<li>If there is not enough data the <strong>ML estimate overfits</strong>. This means ML fits the current data perfectly but the resulting model does not generalise well (i.e. model will perform poorly in prediction)</li>
<li>If there is a choice between different models with different complexity <strong>ML will always select the model with the largest number of parameters</strong>.</li>
</ol>
<p><strong>-&gt; for high-dimensional data with small sample size maximum likelihood estimation does not work!!!</strong></p>
<p><strong>History of Statistics:</strong></p>
<p>Much of modern statistics (from 1960 onwards) is devoted to the development of inference and estimation techniques that work with complex, high-dimensional data.</p>
<div class="inline-figure"><img src="fig/fig1-history.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>Maximum likelihood is a method from classical statistics (time up to about 1960).</li>
<li>From 1960 modern (computational) statistics emerges, starting with
<strong>“Stein Paradox” (1956):</strong> Charles Stein showed that in a <strong>multivariate setting</strong> ML estimators are <strong>dominated by</strong> (= are always worse than) shrinkage estimators!</li>
<li>For example, there is a shrinkage estimator for the mean that is better (in terms of MSE) than the average (which is the MLE)!</li>
</ul>
<p>Modern statistics has developed many different (but related) methods for use in high-dimensional, small sample settings:</p>
<ul>
<li>regularised estimators</li>
<li>shrinkage estimators</li>
<li>penalised maximum likelihood estimators</li>
<li>Bayesian estimators</li>
<li>Empirical Bayes estimators</li>
<li>KL / entropy-based estimators</li>
</ul>
<p>Most of this is out of scope for our class, but will be covered in advanced statistical courses.</p>
<p>Next, we describe a <strong>simple regularised estimator for the estimation of the covariance</strong>
that we will use later (i.e. in classification).</p>
<div style="page-break-after: always;"></div>
</div>
<div id="estimation-of-covariance-matrix-in-small-sample-settings" class="section level3" number="1.4.5">
<h3>
<span class="header-section-number">1.4.5</span> Estimation of covariance matrix in small sample settings<a class="anchor" aria-label="anchor" href="#estimation-of-covariance-matrix-in-small-sample-settings"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Problems with ML estimate of <span class="math inline">\(\boldsymbol \Sigma\)</span></strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\Sigma\)</span> has O(<span class="math inline">\(d^2\)</span>) number of parameters!
<span class="math inline">\(\Longrightarrow \hat{\boldsymbol \Sigma}^{\text{MLE}}\)</span> requires <em>a lot</em> of data! <span class="math inline">\(n\gg d \text{ or } d^2\)</span></p></li>
<li><p>if <span class="math inline">\(n &lt; d\)</span> then <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> is positive <strong>semi</strong>-definite (even if <span class="math inline">\(\Sigma\)</span> is p.d.f.!)<br><span class="math inline">\(\Longrightarrow \hat{\boldsymbol \Sigma}\)</span> will have <strong>vanishing eigenvalues</strong> (some <span class="math inline">\(\lambda_i=0\)</span>) and thus <strong>cannot be inverted</strong> and is singular!</p></li>
</ol>
<p><strong>Simple regularised estimate of <span class="math inline">\(\boldsymbol \Sigma\)</span></strong></p>
<p>Regularised estimator <span class="math inline">\(\boldsymbol S^\ast\)</span> = convex combination of <span class="math inline">\(\boldsymbol S= \hat{\boldsymbol \Sigma}^\text{MLE}\)</span> and <span class="math inline">\(\boldsymbol I_d\)</span> (identity matrix) to get</p>
<p>Regularisation:
<span class="math display">\[\underbrace{\boldsymbol S^\ast}_{\text{regularised estimate}} = \underbrace{\lambda}_{\text{shrinkage intensity}} \, \underbrace{\boldsymbol I_d}_{\text{target}} + (1-\lambda)\underbrace{\boldsymbol S}_{\text{ML estimate}}\]</span>
Next, choose <span class="math inline">\(\lambda \in [0,1]\)</span> such that <span class="math inline">\(\boldsymbol S^\ast\)</span> is better (in terms of MSE) than both <span class="math inline">\(\boldsymbol S\)</span> and <span class="math inline">\(\boldsymbol I_d\)</span>.</p>
<p><strong>Bias-variance trade-off</strong></p>
<p><span class="math inline">\(\text{MSE}\)</span> is the Mean Squared Error, composed of squared bias and variance.</p>
<p><span class="math display">\[\text{MSE}(\theta) = \text{E}((\hat{\theta}-\theta)^2) = \text{Bias}(\hat{\theta})^2 + \text{Var}(\hat{\theta})\]</span>
with <span class="math inline">\(\text{Bias}(\hat{\theta}) = \text{E}(\hat{\theta})-\theta\)</span></p>
<p><span class="math inline">\(\boldsymbol S\)</span>: ML estimate, many parameters, low bias, high variance<br><span class="math inline">\(\boldsymbol I_d\)</span>: “target”, no parameters, high bias, low variance<br><span class="math inline">\(\Longrightarrow\)</span> <strong>reduce high variance of <span class="math inline">\(\boldsymbol S\)</span> by <em>introducing</em> a bit of bias through <span class="math inline">\(\boldsymbol I_d\)</span></strong>!<br><span class="math inline">\(\Longrightarrow\)</span> overall, <span class="math inline">\(\text{MSE}\)</span> is decreased</p>
<div class="inline-figure"><img src="fig/fig1-biasvariance.png" width="90%" style="display: block; margin: auto;"></div>
<p>How to find optimal shrinkage / regularisation parameter <span class="math inline">\(\lambda\)</span>? Minimise <span class="math inline">\(\text{MSE}\)</span>!</p>
<p>Challenge: since we don’t know the true <span class="math inline">\(\boldsymbol \Sigma\)</span> we cannot actually compute the <span class="math inline">\(\text{MSE}\)</span> directly but have to estimate it! How is this done in practise?</p>
<ul>
<li>by cross-validation (=resampling procedure)</li>
<li>by using some analytic approximation (e.g. Stein’s formula)</li>
</ul>
<p><strong>Why does regularisation of <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> work?</strong></p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\boldsymbol S^\ast\)</span> is <strong>positive definite</strong>:<br>
Matrix Theory:<br><span class="math display">\[\underbrace{\boldsymbol M_1}_{ \text{symmetric positive definite, } \lambda \boldsymbol I} + \underbrace{\boldsymbol M_2}_{\text{symmetric positive semi-definite, } (1-\lambda) \boldsymbol S} = \underbrace{\boldsymbol M_3}_{\text{symmetric positive definite, } \boldsymbol S^\ast} \]</span><br><span class="math inline">\(\Longrightarrow \boldsymbol S^\ast\)</span> can be inverted even if <span class="math inline">\(n&lt;d\)</span>
</li>
</ol>
<p>(see Appendix A for details).</p>
<ol start="2" style="list-style-type: decimal">
<li>It’s <strong>Bayesian</strong> in disguise!
<span class="math display">\[\underbrace{\boldsymbol S^\ast}_{\text{posterior mean}} = \underbrace{\lambda \boldsymbol I_d}_{\text{prior information}}  + (1-\lambda)\underbrace{\boldsymbol S}_{\text{data summarised by maximum likelihood}}\]</span>
<ul>
<li>Prior information helps to infer <span class="math inline">\(\boldsymbol \Sigma\)</span> even in small samples</li>
<li>Since <span class="math inline">\(\lambda\)</span> is chosen from data, it is actually an empirical Bayes.</li>
<li>also called shrinkage estimator since the off-diagonal entries are shrunk towards zero</li>
<li>this type of linear shrinkage/regularisation is natural for exponential family models (Diaconis and Ylvisaker, 1979)</li>
</ul>
</li>
</ol>
<p>In Worksheet 2 the empirical estimator of covariance is compared with the covariance estimator implemented in the R package
“corpcor”. This uses a regularisation similar as above (but for the correlation rather than
covariance matrix) and it
employs an analytic data-adaptive estimate of the shrinkage intensity <span class="math inline">\(\lambda\)</span>.
This estimator is a variant of an empirical Bayes / James-Stein estimator (see <a href="https://strimmerlab.github.io/publications/lecture-notes/MATH20802/">MATH20802 Statistical Methods</a>).
.</p>
<p><strong>Summary</strong></p>
<ul>
<li>In multivariate statistics, it is useful (and often necessary) to utilise prior information!</li>
<li>Regularisation introduces bias and reduces variance, minimising overall MSE</li>
<li>Unbiased estimation (a highly valued property in classical statistics!) is not a good idea in multivariate settings and often leads to poor estimators.</li>
</ul>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="categorical-and-multinomial-distribution" class="section level2" number="1.5">
<h2>
<span class="header-section-number">1.5</span> Categorical and multinomial distribution<a class="anchor" aria-label="anchor" href="#categorical-and-multinomial-distribution"><i class="fas fa-link"></i></a>
</h2>
<div id="categorical-distribution" class="section level3" number="1.5.1">
<h3>
<span class="header-section-number">1.5.1</span> Categorical distribution<a class="anchor" aria-label="anchor" href="#categorical-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>categorical distribution</strong> is a generalisation of the Bernoulli distribution
and is correspondingly also known as <strong>Multinoulli</strong> distribution.</p>
<p>Assume we have <span class="math inline">\(K\)</span> classes labelled “class 1”, “class 2”, …, “class K”.
A <em>discrete</em> random variable with a state space consisting of these <span class="math inline">\(K\)</span> classes
has a categorical distribution <span class="math inline">\(\text{Cat}(\boldsymbol \pi)\)</span>.
The parameter vector
<span class="math inline">\(\boldsymbol \pi= (\pi_1, \ldots, \pi_K)^T\)</span> specifies
the probabilities of each of the <span class="math inline">\(K\)</span> classes with <span class="math inline">\(\text{Pr}(\text{"class k"}) = \pi_k\)</span>.
The parameters satisfy <span class="math inline">\(\pi_k \in [0,1]\)</span> and
<span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span>, hence there are <span class="math inline">\(K-1\)</span> independent parameters in a categorical distribution (and not <span class="math inline">\(K\)</span>).</p>
<p>Sampling from a categorical distributions <span class="math inline">\(\text{Cat}(\boldsymbol \pi)\)</span> yields one of <span class="math inline">\(K\)</span> classes.
There are several ways to numerically
represent “class k”, for example simply by the corresponding number <span class="math inline">\(k\)</span>. However, instead
of this “integer encoding” it is often
more convenient to use the so-called “one hot encoding” where the class
is represented by an indicator vector
<span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_K)^T = (0, 0, \ldots, 1, \ldots, 0)^T\)</span> containing zeros everywhere except for
the element <span class="math inline">\(x_k=1\)</span> at position <span class="math inline">\(k\)</span>. Thus all <span class="math inline">\(x_k \in \{ 0, 1\}\)</span> and <span class="math inline">\(\sum_{k=1}^K x_k = 1\)</span>.</p>
<p>The expectation of <span class="math inline">\(\boldsymbol x\sim \text{Cat}(\boldsymbol \pi)\)</span> is <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \pi\)</span>, with
<span class="math inline">\(\text{E}(x_k) = \pi_k\)</span>.
The covariance matrix is <span class="math inline">\(\text{Var}(\boldsymbol x) = \text{Diag}(\boldsymbol \pi) - \boldsymbol \pi\boldsymbol \pi^T\)</span>.
In component notation <span class="math inline">\(\text{Var}(x_i) = \pi_i (1-\pi_i)\)</span> and <span class="math inline">\(\text{Cov}(x_i, x_j) = -\pi_i \pi_j\)</span>.
This follows directly from the definition of the variance <span class="math inline">\(\text{Var}(\boldsymbol x) = \text{E}( \boldsymbol x\boldsymbol x^T) - \text{E}( \boldsymbol x) \text{E}( \boldsymbol x)^T\)</span>
and noting that <span class="math inline">\(x_i^2 = x_i\)</span> and <span class="math inline">\(x_i x_j = 0\)</span> if <span class="math inline">\(i \neq j\)</span>.
Note that the variance matrix <span class="math inline">\(\text{Var}(\boldsymbol x)\)</span> is singular by construction, as the <span class="math inline">\(K\)</span> random variables
<span class="math inline">\(x_1, \ldots, x_K\)</span> are dependent through the constraint <span class="math inline">\(\sum_{k=1}^K x_k = 1\)</span>.</p>
<p>The corresponding probability mass function (pmf)
can be written conveniently in terms of <span class="math inline">\(x_k\)</span> as
<span class="math display">\[
f(\boldsymbol x) = \prod_{k=1}^K \pi_k^{x_k} = 
\begin{cases} 
   \pi_k  &amp; \text{if } x_k = 1 \\
\end{cases}
\]</span>
and the log pmf as
<span class="math display">\[
\log f(\boldsymbol x) = \sum_{k=1}^K x_k \log \pi_k   =
\begin{cases} 
   \log \pi_k  &amp; \text{if } x_k = 1 \\
\end{cases}
\]</span></p>
<p>In order to be more explicit that the categorical distribution has <span class="math inline">\(K-1\)</span> and not <span class="math inline">\(K\)</span> parameters
we rewrite the log-density with
<span class="math inline">\(\pi_K = 1 - \sum_{k=1}^{K-1} \pi_k\)</span> and <span class="math inline">\(x_K = 1 - \sum_{k=1}^{K-1} x_k\)</span> as
<span class="math display">\[
\begin{split}
\log f(\boldsymbol x) &amp; =\sum_{k=1}^{K-1}  x_k \log \pi_k    + x_K \log \pi_K \\
 &amp; =\sum_{k=1}^{K-1}  x_k \log \pi_k    + \left( 1 - \sum_{k=1}^{K-1} x_k  \right) \log \left( 1 - \sum_{k=1}^{K-1} \pi_k \right) \\
\end{split}
\]</span>
Note that there is no particular reason to choose <span class="math inline">\(\pi_K\)</span> as derived, in its place any other of the
<span class="math inline">\(\pi_k\)</span> may be selected.</p>
<p>For <span class="math inline">\(K=2\)</span> the categorical distribution reduces to the Bernoulli <span class="math inline">\(\text{Ber}(p)\)</span> distribution,
with <span class="math inline">\(\pi_1=p\)</span> and <span class="math inline">\(\pi_2=1-p\)</span>.</p>
</div>
<div id="multinomial-distribution" class="section level3" number="1.5.2">
<h3>
<span class="header-section-number">1.5.2</span> Multinomial distribution<a class="anchor" aria-label="anchor" href="#multinomial-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The multinomial distribution arises from repeated categorical sampling,
just like the Binomial distribution arises from repeated Bernoulli sampling.</p>
<div id="univariate-case" class="section level4" number="1.5.2.1">
<h4>
<span class="header-section-number">1.5.2.1</span> Univariate case<a class="anchor" aria-label="anchor" href="#univariate-case"><i class="fas fa-link"></i></a>
</h4>
<p>Binomial Distribution:</p>
<p>Repeat Bernoulli <span class="math inline">\(\text{Ber}(\pi)\)</span> experiment <span class="math inline">\(n\)</span> times:</p>
<p><span class="math display">\[x \sim \text{Bin}(n, \pi)\]</span>
<span class="math display">\[ x \in \{0,...,n\}\]</span>
<span class="math display">\[\text{E}(x) = n \, \pi\]</span>
<span class="math display">\[\text{Var}(x)=n \, \pi(1-\pi)\]</span></p>
<p>Standardised to unit interval:
<span class="math display">\[\frac{x}{n} \in \left\{0,\frac{1}{n},...,1\right\}\]</span>
<span class="math display">\[\text{E}\left(\frac{x}{n}\right) = \pi\]</span>
<span class="math display">\[\text{Var}\left(\frac{x}{n}\right)=\frac{\pi(1-\pi)}{n}\]</span></p>
<p><span class="math display">\[\textbf{Urn model:}\]</span></p>
<p>distribute <span class="math inline">\(n\)</span> balls into two bins</p>
<div class="inline-figure"><img src="fig/fig1-binom.png" width="50%" style="display: block; margin: auto;"></div>
</div>
<div id="multivariate-case" class="section level4" number="1.5.2.2">
<h4>
<span class="header-section-number">1.5.2.2</span> Multivariate case<a class="anchor" aria-label="anchor" href="#multivariate-case"><i class="fas fa-link"></i></a>
</h4>
<p>Multinomial distribution:</p>
<p>Draw <span class="math inline">\(n\)</span> times from categorical distribution <span class="math inline">\(\text{Cat}(\boldsymbol \pi)\)</span>:</p>
<p><span class="math display">\[\boldsymbol x\sim \text{Mult}(n, \boldsymbol \pi)  \]</span>
<span class="math display">\[ x_i \in \{0,1,...,n\}; \, \sum^{K}_{i=1}x_i = n\]</span>
<span class="math display">\[\text{E}(\boldsymbol x) = n \,\boldsymbol \pi\]</span>
<span class="math display">\[\text{Var}(x_i)=n\, \pi_i(1-\pi_i)\]</span>
<span class="math display">\[\text{Cov}(x_i,x_j)=-n\, \pi_i\pi_j\]</span></p>
<p>Standardised to unit interval:
<span class="math display">\[\frac{x_i}{n} \in \left\{0,\frac{1}{n},\frac{2}{n},...,1\right\}\]</span>
<span class="math display">\[\text{E}\left(\frac{\boldsymbol x}{n}\right) = \boldsymbol \pi\]</span>
<span class="math display">\[\text{Var}\left(\frac{x_i}{n}\right)=\frac{\pi_i(1-\pi_i)}{n}\]</span>
<span class="math display">\[\text{Cov}\left(\frac{x_i}{n},\frac{x_j}{n}\right)=-\frac{\pi_i\pi_j}{n} \]</span>
<span class="math display">\[\textbf{Urn model:}\]</span></p>
<p>distribute <span class="math inline">\(n\)</span> balls into <span class="math inline">\(K\)</span> bins:</p>
<div class="inline-figure"><img src="fig/fig1-multinom.png" width="60%" style="display: block; margin: auto;"></div>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="entropy-and-maximum-likelihood-analysis-for-the-categorical-distribution" class="section level3" number="1.5.3">
<h3>
<span class="header-section-number">1.5.3</span> Entropy and maximum likelihood analysis for the categorical distribution<a class="anchor" aria-label="anchor" href="#entropy-and-maximum-likelihood-analysis-for-the-categorical-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>In the following we compute the KL divergence, the MLE and other related quantities for the categorical distribution.</p>
<p>This generalises the same calculations for the Bernoulli distribution discussed in the earlier module <a href="https://strimmerlab.github.io/publications/lecture-notes/MATH20802/">MATH20802 Statistical Methods</a>.</p>
<div class="example">
<p><span id="exm:catkl" class="example"><strong>Example 1.1  </strong></span>KL divergence between two categorical distributions with <span class="math inline">\(K\)</span> classes:</p>
<p>With <span class="math inline">\(P=\text{Cat}(\boldsymbol p)\)</span> and <span class="math inline">\(Q=\text{Cat}(\boldsymbol q)\)</span> and corresponding
probabilities <span class="math inline">\(p_1,\dots,p_K\)</span> and <span class="math inline">\(q_1,\dots,q_K\)</span> satisfying <span class="math inline">\(\sum_{i=1}^K p_i =1\)</span> and <span class="math inline">\(\sum_{i=1}^K q_i = 1\)</span> we get:</p>
<p><span class="math display">\[\begin{equation*}
D_{\text{KL}}(P, Q)=\sum_{i=1}^K p_i\log\left(\frac{p_i}{q_i}\right) 
\end{equation*}\]</span></p>
<p>To be explicit that there are only <span class="math inline">\(K-1\)</span> parameters in a categorical distribution we can also write
<span class="math display">\[\begin{equation*}
D_{\text{KL}}(P, Q)=\sum_{i=1}^{K-1} p_i\log\left(\frac{p_i}{q_i}\right)  + p_K\log\left(\frac{p_K}{q_K}\right)
\end{equation*}\]</span>
with <span class="math inline">\(p_K=\left(1- \sum_{i=1}^{K-1} p_i\right)\)</span> and
<span class="math inline">\(q_K=\left(1- \sum_{i=1}^{K-1} q_i\right)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:catexpectfisher" class="example"><strong>Example 1.2  </strong></span>Expected Fisher information of the categorical distribution:</p>
<p>We first compute the Hessian matrix
<span class="math inline">\(\nabla \nabla^T \log f(\boldsymbol x)\)</span> of the log-probability mass function, where the
differentiation is with regard to <span class="math inline">\(\pi_1, \ldots, \pi_{K-1}\)</span>.</p>
<p>The diagonal entries of the Hessian matrix (with <span class="math inline">\(i=1, \ldots, K-1\)</span>) are
<span class="math display">\[
\frac{\partial^2}{\partial \pi_i^2} \log f(\boldsymbol x) =
  -\frac{x_i}{\pi_i^2}-\frac{x_K}{\pi_K^2}
\]</span>
and its off-diagonal entries are (with <span class="math inline">\(j=1, \ldots, K-1\)</span>)
<span class="math display">\[
\frac{\partial^2}{\partial \pi_i \partial \pi_j} \log f(\boldsymbol x) =
 -\frac{ x_K}{\pi_K^2}
\]</span>
Recalling that <span class="math inline">\(\text{E}(x_i) = \pi_i\)</span> we can compute the expected Fisher information matrix for a categorical distribution as
<span class="math display">\[
\begin{split}
\boldsymbol I^{\text{Fisher}}\left( \pi_1, \ldots, \pi_{K-1}  \right) &amp;= -\text{E}\left( \nabla \nabla^T \log f(\boldsymbol x) \right) \\
&amp; =
\begin{pmatrix}
 \frac{1}{\pi_1} + \frac{1}{\pi_K} &amp; \cdots &amp; \frac{1}{\pi_K} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{1}{\pi_K} &amp; \cdots &amp; \frac{1}{\pi_{K-1}} + \frac{1}{\pi_K} \\
\end{pmatrix}\\
&amp; = \text{Diag}\left( \frac{1}{\pi_1} , \ldots,  \frac{1}{\pi_{K-1}}   \right) + \frac{1}{\pi_K} \boldsymbol 1\\
\end{split}
\]</span></p>
<p>For <span class="math inline">\(K=2\)</span> and <span class="math inline">\(\pi_1=p\)</span> this reduces to the expected Fisher information of a Bernoulli variable
<span class="math display">\[
\begin{split}
I^{\text{Fisher}}(p) &amp; =  \left(\frac{1}{p} + \frac{1}{1-p} \right) \\
  &amp;= \frac{1}{p (1-p)} \\
\end{split}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:catquadapproxkl" class="example"><strong>Example 1.3  </strong></span>Quadratic approximation of KL divergence of the categorical distribution:</p>
<p>The expected Fisher information arises from a local quadratic approximation of the KL divergence:
<span class="math display">\[
D_{\text{KL}}(F_{\boldsymbol \theta}, F_{\boldsymbol \theta+\boldsymbol \varepsilon})  \approx  \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)  \boldsymbol \varepsilon
\]</span>
and
<span class="math display">\[
D_{\text{KL}}(F_{\boldsymbol \theta+\boldsymbol \varepsilon}, F_{\boldsymbol \theta}) \approx  \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)  \boldsymbol \varepsilon
\]</span></p>
<p>We now consider the KL divergence <span class="math inline">\(D_{\text{KL}}(P, Q)\)</span> between the categorical distribution <span class="math inline">\(P=\text{Cat}(\boldsymbol p)\)</span> with probabilities <span class="math inline">\(\boldsymbol p=(p_1, \ldots, p_K)^T\)</span> with the categorical distribution <span class="math inline">\(Q=\text{Cat}(\boldsymbol q)\)</span> with probabilities <span class="math inline">\(\boldsymbol q= (q_1, \ldots, q_K)^T\)</span>.</p>
<p>First, we keep <span class="math inline">\(P\)</span> fixed and assume that <span class="math inline">\(Q\)</span> is a perturbed version of <span class="math inline">\(P\)</span> with <span class="math inline">\(\boldsymbol q= \boldsymbol p+\boldsymbol \varepsilon\)</span>.
Note that the perturbations <span class="math inline">\(\boldsymbol \varepsilon=(\varepsilon_1, \ldots, \varepsilon_K)^T\)</span> satisfy
<span class="math inline">\(\sum_{k=1}^K \varepsilon_k = 0\)</span> because <span class="math inline">\(\sum_{k=1}^K p_i=1\)</span> and <span class="math inline">\(\sum_{k=1}^K q_i=1\)</span>.
Thus <span class="math inline">\(\varepsilon_K = -\sum_{k=1}^{K-1} \varepsilon_k\)</span>. Then
<span class="math display">\[
\begin{split}
D_{\text{KL}}(P, Q=P+\varepsilon) &amp;  = D_{\text{KL}}(\text{Cat}(\boldsymbol p), \text{Cat}(\boldsymbol p+\boldsymbol \varepsilon)) \\
&amp;  \approx \frac{1}{2} (\varepsilon_1, \ldots,  \varepsilon_{K-1}) \,
\boldsymbol I^{\text{Fisher}}\left( p_1, \ldots, p_{K-1}  \right) 
\begin{pmatrix} \varepsilon_1 \\ \vdots \\  \varepsilon_{K-1}\\
\end{pmatrix} \\
&amp;= \frac{1}{2} \left( \sum_{k=1}^{K-1} \frac{\varepsilon_k^2}{p_k}   + \frac{ \left(\sum_{k=1}^{K-1} \varepsilon_k\right)^2}{p_K} \right)  \\
&amp;= \frac{1}{2}  \sum_{k=1}^{K} \frac{\varepsilon_k^2}{p_k}\\
&amp;= \frac{1}{2}  \sum_{k=1}^{K} \frac{(p_k-q_k)^2}{p_k}\\
&amp; = \frac{1}{2} D_{\text{Neyman}}(P, Q)\\
\end{split} 
\]</span>
Similarly, if we keep <span class="math inline">\(Q\)</span> fixed and consider <span class="math inline">\(P\)</span> as a disturbed version of <span class="math inline">\(Q\)</span> we get
<span class="math display">\[
\begin{split}
D_{\text{KL}}(P=Q+\varepsilon, Q) &amp;  =D_{\text{KL}}(\text{Cat}(\boldsymbol q+\boldsymbol \varepsilon), \text{Cat}(\boldsymbol q)) \\
&amp;\approx \frac{1}{2}  \sum_{k=1}^{K} \frac{(p_k-q_k)^2}{q_k}\\
&amp;= \frac{1}{2} D_{\text{Pearson}}(P, Q)
\end{split}
\]</span>
Note that in both approximations we divide by the probabilities of the distribution that
is kept fixed.</p>
<p>Note the appearance of the <em>Pearson <span class="math inline">\(\chi^2\)</span> divergence</em> and the <em>Neyman <span class="math inline">\(\chi^2\)</span> divergence</em> in the above. Both are, like the KL divergence, part of the family of <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergences</a>. The Neyman <span class="math inline">\(\chi^2\)</span>
divergence is also known as the reverse Pearson divergence as <span class="math inline">\(D_{\text{Neyman}}(P, Q) = D_{\text{Pearson}}(Q, P)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:catmle" class="example"><strong>Example 1.4  </strong></span>Maximum likelihood estimation of the parameters of the categorical distribution:</p>
<p>Maximum likelihood estimation seems trivial at first sight but it is in fact a bit more complicated since there are only <span class="math inline">\(K-1\)</span> free parameters, and not <span class="math inline">\(K\)</span>. So we either need to optimise with regard to a specific set of <span class="math inline">\(K-1\)</span> parameters (which is what we do below) or use a constrained optimisation procedure to enforce that <span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span> (for example by using a Lagrange multiplier).</p>
<ul>
<li><p>The data: We observe <span class="math inline">\(n\)</span> samples <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n\)</span>.
The data matrix of dimension <span class="math inline">\(n \times K\)</span> is
<span class="math inline">\(\boldsymbol X= (\boldsymbol x_1, \ldots, \boldsymbol x_n)^T = (x_{ik})\)</span>.
It contains each <span class="math inline">\(\boldsymbol x_i = (x_{i1}, \ldots, x_{iK})^T\)</span>. The corresponding summary
(minimal sufficient) statistics are
<span class="math inline">\(\bar{\boldsymbol x} = \frac{1}{n} \sum_{i=1}^n \boldsymbol x_i = (\bar{x}_1, \ldots, \bar{x}_K)^T\)</span>
with
<span class="math inline">\(\bar{x}_k = \frac{1}{n} \sum_{i=1}^n x_{ik}\)</span>. We can also write
<span class="math inline">\(\bar{x}_{K} = 1 - \sum_{k=1}^{K-1} \bar{x}_{k}\)</span>.
The number of samples for class <span class="math inline">\(k\)</span> is <span class="math inline">\(n_k = n \bar{x}_k\)</span> with
<span class="math inline">\(\sum_{k=1}^K n_k = n\)</span>.</p></li>
<li><p>The log-likelihood is
<span class="math display">\[
\begin{split}
l_n(\pi_1, \ldots, \pi_{K-1}) &amp; = \sum_{i=1}^n \log f(\boldsymbol x_i) \\
&amp; =\sum_{i=1}^n \left( \sum_{k=1}^{K-1}  x_{ik} \log \pi_k    + \left( 1 - \sum_{k=1}^{K-1} x_{ik}  \right) \log \left( 1 - \sum_{k=1}^{K-1} \pi_k \right) \right)\\
&amp; = n \left(  \sum_{k=1}^{K-1}  \bar{x}_k \log \pi_k    + \left(
1 - \sum_{k=1}^{K-1} \bar{x}_{k} \right) \log\left(1 - \sum_{k=1}^{K-1} \pi_k\right) \right) \\ 
&amp; = n \left(  \sum_{k=1}^{K-1}  \bar{x}_k \log \pi_k    + \bar{x}_K \log \pi_K \right) \\  
\end{split}
\]</span></p></li>
<li><p>Score function (gradient)
<span class="math display">\[
\begin{split}
\boldsymbol S_n(\pi_1, \ldots, \pi_{K-1}) &amp;=  \nabla l_n(\pi_1, \ldots, \pi_{K-1} ) \\
&amp; = 
\begin{pmatrix}
 \frac{\partial}{\partial \pi_1} l_n(\pi_1, \ldots, \pi_{K-1} )  \\
\vdots\\
\frac{\partial}{\partial \pi_{K-1}} l_n(\pi_1, \ldots, \pi_{K-1} )  \\
\end{pmatrix}\\
&amp; = n
\begin{pmatrix}
\frac{\bar{x}_1}{\pi_1}-\frac{\bar{x}_K}{\pi_K}  \\
\vdots\\
\frac{\bar{x}_{K-1}}{\pi_{K-1}}-\frac{\bar{x}_K}{\pi_K}  \\
\end{pmatrix}\\
\end{split}
\]</span>
Note in particular the need for the second term that arises because <span class="math inline">\(\pi_K\)</span> depends on all
the <span class="math inline">\(\pi_1, \ldots, \pi_{K-1}\)</span>.</p></li>
<li><p>Maximum likelihood estimate: Setting <span class="math inline">\(\boldsymbol S_n(\hat{\pi}_1^{ML}, \ldots, \hat{\pi}_{K-1}^{ML})=0\)</span> yields
<span class="math inline">\(K-1\)</span> equations
<span class="math display">\[
\bar{x}_i \left(1 - \sum_{k=1}^{K-1} \hat{\pi}_k^{ML}\right)  = \hat{\pi}_i^{ML} \left(
1 - \sum_{k=1}^{K-1} \bar{x}_{k} \right)
\]</span>
for <span class="math inline">\(i=1, \ldots, K-1\)</span> and with solution
<span class="math display">\[
\hat{\pi}_i^{ML} = \bar{x}_i
\]</span>
It also follows that
<span class="math display">\[ 
\hat{\pi}_K^{ML} = 1 - \sum_{k=1}^{K-1} \hat{\pi}_k^{ML} = 1 - \sum_{k=1}^{K-1} \bar{x}_{k} = \bar{x}_K
\]</span>
The maximum likelihood estimator is therefore the frequency
of of the occurance of a class among the <span class="math inline">\(n\)</span> samples.</p></li>
</ul>
</div>
<div class="example">
<p><span id="exm:catobsfisher" class="example"><strong>Example 1.5  </strong></span>Observed Fisher information of the categorical distribution:</p>
<p>We first need to compute the negative Hessian matrix of the log likelihood function
<span class="math inline">\(- \nabla \nabla^T l_n(\pi_1, \ldots, \pi_{K-1} )\)</span> and then evaluate it at the
MLEs <span class="math inline">\(\hat{\pi}_1^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}\)</span>.</p>
<p>The diagonal entries of the Hessian matrix (with <span class="math inline">\(i=1, \ldots, K-1\)</span>) are
<span class="math display">\[
\frac{\partial^2}{\partial \pi_i^2} l_n(\pi_1, \ldots, \pi_{K-1} ) =
 -n \left( \frac{\bar{x}_i}{\pi_i^2} +\frac{\bar{x}_K}{\pi_K^2}\right)
\]</span>
and its off-diagonal entries are (with <span class="math inline">\(j=1, \ldots, K-1\)</span>)
<span class="math display">\[
\frac{\partial^2}{\partial \pi_i \partial \pi_j} l_n(\pi_1, \ldots, \pi_{K-1} ) =
 -\frac{n \bar{x}_K}{\pi_K^2}
\]</span>
Thus, the observed Fisher information matrix at the MLE for a categorical distribution is
<span class="math display">\[
\boldsymbol J_n(\hat{\pi}_{1}^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  ) =
n 
\begin{pmatrix}
 \frac{1}{\hat{\pi}_1^{ML}} + \frac{1}{\hat{\pi}_K^{ML}} &amp; \cdots &amp; \frac{1}{\hat{\pi}_K^{ML}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{1}{\hat{\pi}_K^{ML}} &amp; \cdots &amp; \frac{1}{\hat{\pi}_{K-1}^{ML}} + \frac{1}{\hat{\pi}_K^{ML}} \\
\end{pmatrix} 
\]</span></p>
<p>For <span class="math inline">\(K=2\)</span> this reduces to the observed Fisher information of a Bernoulli variable
<span class="math display">\[
\begin{split}
J_n(\hat{p}_{ML}) &amp; = n \left(\frac{1}{\hat{p}_{ML}} + \frac{1}{1-\hat{p}_{ML}} \right) \\
  &amp;= \frac{n}{\hat{p}_{ML} (1-\hat{p}_{ML})} \\
\end{split}
\]</span></p>
<p>The inverse of the observed Fisher information is:
<span class="math display">\[
\boldsymbol J_n(\hat{\pi}_{1}^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  )^{-1} =
\frac{1}{n} 
\begin{pmatrix}
\hat{\pi}_1^{ML} (1- \hat{\pi}_1^{ML} )  &amp; \cdots &amp; -  \hat{\pi}_{1}^{ML} \hat{\pi}_{K-1}^{ML}   \\
\vdots &amp; \ddots &amp; \vdots \\
-  \hat{\pi}_{K-1}^{ML} \hat{\pi}_{1}^{ML} &amp; \cdots &amp; \hat{\pi}_{K-1}^{ML} (1- \hat{\pi}_{K-1}^{ML} )  \\
\end{pmatrix}
\]</span></p>
<p>To show that this is indeed the inverse we use the Woodbury matrix identity (see Appendix)
<span class="math display">\[
(\boldsymbol A+ \boldsymbol U\boldsymbol B\boldsymbol V)^{-1} = \boldsymbol A^{-1} - \boldsymbol A^{-1} \boldsymbol U(\boldsymbol B^{-1} + \boldsymbol V\boldsymbol A^{-1} \boldsymbol U)^{-1} \boldsymbol V\boldsymbol A^{-1}
\]</span>
with</p>
<ul>
<li>
<span class="math inline">\(B=1\)</span>,</li>
<li>
<span class="math inline">\(\boldsymbol u= (\pi_1, \ldots, \pi_{K-1})^T\)</span>,</li>
<li>
<span class="math inline">\(\boldsymbol v=-\boldsymbol u^T\)</span>,</li>
<li>
<span class="math inline">\(\boldsymbol A= \text{Diag}(\boldsymbol u)\)</span> and its inverse <span class="math inline">\(\boldsymbol A^{-1} = \text{Diag}(\pi_1^{-1}, \ldots, \pi_{K-1}^{-1})\)</span>.</li>
</ul>
<p>Then <span class="math inline">\(\boldsymbol A^{-1} \boldsymbol u= \boldsymbol 1_{K-1}\)</span> and <span class="math inline">\(1-\boldsymbol u^T \boldsymbol A^{-1} \boldsymbol u= \pi_K\)</span>.
With this
<span class="math display">\[
\boldsymbol J_n(\hat{\pi}_{1}^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  )^{-1} = \frac{1}{n}
\left( \boldsymbol A- \boldsymbol u\boldsymbol u^T \right)
\]</span>
and
<span class="math display">\[
\boldsymbol J_n(\hat{\pi}_{1}^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  ) = n \left( \boldsymbol A^{-1} + \frac{1}{\pi_K} \boldsymbol 1_{K-1 \times K-1}  \right)
\]</span></p>
<p>For <span class="math inline">\(K=2\)</span> the inverse observed Fisher information of the categorical distribution reduces to that of the Bernoulli distribution
<span class="math display">\[
J_n(\hat{p}_{ML})^{-1}=\frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n}
\]</span></p>
<p>The inverse observed Fisher information is useful, e.g., as
the asymptotic variance of the maximum likelihood estimate.</p>
</div>
<div class="example">
<p><span id="exm:catwald" class="example"><strong>Example 1.6  </strong></span>Wald statistic for the categorical distribution:</p>
<p>The squared Wald statistic is
<span class="math display">\[
\begin{split}
t(\boldsymbol p_0)^2 &amp;= 
(\hat{\pi}_{1}^{ML}-p_1^0, \ldots,  \hat{\pi}_{K-1}^{ML}-p_{K-1}^0)   \boldsymbol J_n(\hat{\pi}_{1}^{ML}, \ldots, \hat{\pi}_{K-1}^{ML} ) \begin{pmatrix} \hat{\pi}_{1}^{ML}-p_1^0 \\
\vdots \\
\hat{\pi}_{K-1}^{ML}-p_{K-1}^0\\
\end{pmatrix}\\
&amp;= n  \left( \sum_{k=1}^{K-1} \frac{(\hat{\pi}_{k}^{ML}-p_{k}^0)^2}{\hat{\pi}_{k}^{ML}}   + \frac{ \left(\sum_{k=1}^{K-1} (\hat{\pi}_{k}^{ML}-p_{k}^0)\right)^2}{\hat{\pi}_{K}^{ML}} \right)  \\
&amp;= n  \left( \sum_{k=1}^{K} \frac{(\hat{\pi}_{k}^{ML}-p_{k}^0)^2}{\hat{\pi}_{k}^{ML}}    \right)  \\
&amp; = n D_{\text{Neyman}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) )
\end{split}
\]</span></p>
<p>With <span class="math inline">\(n_1, \ldots, n_K\)</span> the observed counts with <span class="math inline">\(n = \sum_{k=1}^K n_k\)</span>
and <span class="math inline">\(\hat{\pi}_k^{ML} = \frac{n_k}{n} = \bar{x}_k\)</span>,
and <span class="math inline">\(n_1^{\text{expect}}, \ldots, n_K^{\text{expect}}\)</span> the
expected counts <span class="math inline">\(n_k^{\text{expect}} = n p_k^{0}\)</span> under <span class="math inline">\(\boldsymbol p_0\)</span>
we can write the squared Wald statistic
as follows:
<span class="math display">\[
t(\boldsymbol p_0)^2 = \sum_{k=1}^K \frac{(n_k-n_k^{\text{expect}} )^2}{n_k} =  \chi^2_{\text{Neyman}}
\]</span>
This is known as the Neyman chi-squared statistic (note the <em>observed</em> counts in its denominator) and it is asymptotically distributed as <span class="math inline">\(\chi^2_{K-1}\)</span> because there
are <span class="math inline">\(K-1\)</span> free parameters in <span class="math inline">\(\boldsymbol p_0\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:catwilks" class="example"><strong>Example 1.7  </strong></span>Wilks log-likelihood ratio statistic for the categorical distribution:</p>
<p>The Wilks log-likelihood ratio is
<span class="math display">\[
W(\boldsymbol p_0) = 2 (l_n(\hat{\pi}_1^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  ) - l_n(p_1^{0}, \ldots, p_{K-1}^{0}    ))
\]</span>
with <span class="math inline">\(\boldsymbol p_0 = c(p_1^{0}, \ldots, p_{K}^{0} )^T\)</span>.
As the probabilities sum up to 1 there are only <span class="math inline">\(K-1\)</span> free parameters.</p>
<p>The log-likelihood at the MLE is
<span class="math display">\[
l_n(\hat{\pi}_1^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  ) =  n   \sum_{k=1}^{K}  \bar{x}_k \log \hat{\pi}_k^{ML}  =  n   \sum_{k=1}^{K}  \bar{x}_k \log \bar{x}_k 
\]</span>
with <span class="math inline">\(\hat{\pi}_k^{ML} = \frac{n_k}{n} = \bar{x}_k\)</span>.
Note that here and in the following the sums run from <span class="math inline">\(1\)</span> to <span class="math inline">\(K\)</span> where the <span class="math inline">\(K\)</span>-th component is always computed from the components <span class="math inline">\(1\)</span> to <span class="math inline">\(K-1\)</span>, as in the previous section.
The log-likelihood at <span class="math inline">\(\boldsymbol p_0\)</span> is
<span class="math display">\[l_n( p_1^{0}, \ldots, p_{K-1}^{0}    ) =  n   \sum_{k=1}^{K}  \bar{x}_k \log p_k^{0} 
\]</span>
so that the Wilks statistic becomes
<span class="math display">\[
W(\boldsymbol p_0) = 2 n   \sum_{k=1}^{K}  \bar{x}_k \log\left(\frac{\bar{x}_k}{ p_k^{0}} \right) 
\]</span>
It is asymptotically chi-squared distributed with <span class="math inline">\(K-1\)</span> degrees of freedom.</p>
<p>Note that for this model the Wilks statistic is equal to the KL Divergence
<span class="math display">\[
W(\boldsymbol p_0) = 2 n D_{\text{KL}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) )
\]</span></p>
<p>The Wilks log-likelihood ratio statistic for the categorical distribution is also known as the <a href="https://en.wikipedia.org/wiki/G-test"><span class="math inline">\(G\)</span> test statistic</a> where <span class="math inline">\(\hat{\boldsymbol \pi}_{ML}\)</span> corresponds to the observed frequencies (as observed in data) and <span class="math inline">\(\boldsymbol p_0\)</span> are the expected frequencies (i.e. hypothesised to be the true frequencies).</p>
<p>Using observed counts <span class="math inline">\(n_k\)</span> and expected counts <span class="math inline">\(n_k^{\text{expect}} = n p_k^{0}\)</span>
we can write the Wilks statistic respectively the <span class="math inline">\(G\)</span>-statistic
as follows:
<span class="math display">\[
W(\boldsymbol p_0) = 2   \sum_{k=1}^{K}  n_k \log\left(\frac{  n_k }{  n_k^{\text{expect}}   } \right) 
\]</span></p>
</div>
<div class="example">
<p><span id="exm:catwilksquadapprox" class="example"><strong>Example 1.8  </strong></span>Quadratic approximation of the Wilks log-likelihood ratio statistic for the categorical distribution:</p>
<p>Developing the Wilks statistic <span class="math inline">\(W(\boldsymbol p_0)\)</span> around the MLE <span class="math inline">\(\hat{\boldsymbol \pi}_{ML}\)</span> yields the squared Wald statistic which for the categorical distribution is the Neyman chi-squared statistic:
<span class="math display">\[
\begin{split}
W(\boldsymbol p_0)&amp; = 2 n D_{\text{KL}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) ) \\
&amp; \approx n D_{\text{Neyman}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) ) \\
&amp; = \sum_{k=1}^K \frac{(n_k-n_k^{\text{expect}} )^2}{n_k} \\
&amp; =  \chi^2_{\text{Neyman}}\\
\end{split}
\]</span></p>
<p>If instead we approximate the KL divergence assuming <span class="math inline">\(\boldsymbol p_0\)</span> as fixed we arrive at
<span class="math display">\[ 
\begin{split}
2 n D_{\text{KL}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) ) &amp;\approx n D_{\text{Pearson}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}),  \text{Cat}(\boldsymbol p_0 ) )\\
&amp; = \sum_{k=1}^K \frac{(n_k-n_k^{\text{expect}})^2}{n_k^{\text{expect}}} \\
&amp; = \chi^2_{\text{Pearson}}
\end{split}
\]</span>
which is the well-known Pearson chi-squared statistic (note the <em>expected</em> counts in its denominator).</p>
</div>
</div>
</div>
<div id="further-multivariate-distributions" class="section level2" number="1.6">
<h2>
<span class="header-section-number">1.6</span> Further multivariate distributions<a class="anchor" aria-label="anchor" href="#further-multivariate-distributions"><i class="fas fa-link"></i></a>
</h2>
<p>For most univariate distributions there are multivariate versions.</p>
<p>In the following we describe the multivariate versions of the Beta distribution,
the Gamma distribution (also known as scaled <span class="math inline">\(\chi^2\)</span> distribution)
and of the inverse Gamma distribution.</p>
<div id="dirichlet-distribution" class="section level3" number="1.6.1">
<h3>
<span class="header-section-number">1.6.1</span> Dirichlet distribution<a class="anchor" aria-label="anchor" href="#dirichlet-distribution"><i class="fas fa-link"></i></a>
</h3>
<div id="univariate-case-1" class="section level4" number="1.6.1.1">
<h4>
<span class="header-section-number">1.6.1.1</span> Univariate case<a class="anchor" aria-label="anchor" href="#univariate-case-1"><i class="fas fa-link"></i></a>
</h4>
<p>Beta distribution</p>
<p><span class="math display">\[x \sim \text{Beta}(\alpha,\beta)\]</span>
<span class="math display">\[x \in [0,1]\]</span>
<span class="math display">\[\alpha &gt; 0; \beta &gt; 0\]</span>
<span class="math display">\[m = \alpha + \beta \]</span>
<span class="math display">\[\mu = \frac{\alpha}{m} \in \left[0,1\right]\]</span>
<span class="math display">\[\text{E}(x) = \mu\]</span>
<span class="math display">\[\text{Var}(x)=\frac{\mu(1-\mu)}{m+1}\]</span>
<span class="math inline">\(\text{compare with unit standardised binomial!}\)</span></p>
<p><span class="math inline">\(\textbf{Different shapes}\)</span></p>
<div class="inline-figure"><img src="fig/fig1-betashapes.png" width="60%" style="display: block; margin: auto;"></div>
<p><span class="math display">\[\text{Useful as distribution for a proportion } \pi\]</span></p>
<p><span class="math display">\[ \text{ Bayesian Model:}\]</span><br><span class="math display">\[\text{Beta prior:} \; \pi \sim  \text{Beta}(\alpha,\beta)\]</span>
<span class="math display">\[\text{Binomial likelihood:} \; x|\pi \sim \text{Bin}(n, \pi)\]</span></p>
</div>
<div id="multivariate-case-1" class="section level4" number="1.6.1.2">
<h4>
<span class="header-section-number">1.6.1.2</span> Multivariate case<a class="anchor" aria-label="anchor" href="#multivariate-case-1"><i class="fas fa-link"></i></a>
</h4>
<p>Dirichlet distribution</p>
<p><span class="math display">\[\boldsymbol x\sim \text{Dir}(\boldsymbol \alpha)\]</span>
<span class="math display">\[x_i \in [0,1]; \, \sum^{d}_{i=1} x_i = 1\]</span>
<span class="math display">\[\boldsymbol \alpha= (\alpha_1,...,\alpha_d)^T &gt;0\]</span>
<span class="math display">\[m = \sum^{d}_{i=1}\alpha_i\]</span>
<span class="math display">\[\mu_i = \frac{\alpha_i}{m} \in \left[0,1\right]\]</span>
<span class="math display">\[\text{E}(x_i) = \mu_i\]</span>
<span class="math display">\[\text{Var}(x_i)=\frac{\mu_i(1-\mu_i)}{m+1}\]</span>
<span class="math display">\[\text{Cov}(x_i,x_j)=-\frac{\mu_i \mu_j}{m+1}\]</span>
<span class="math inline">\(\text{compare with unit standardised multinomial!}\)</span></p>
<p>Stick breaking" model</p>
<div class="inline-figure"><img src="fig/fig1-stickbreaking.png" width="60%" style="display: block; margin: auto;"></div>
<p><span class="math display">\[\text{Useful as distribution for a proportion } \boldsymbol \pi\]</span></p>
<p><span class="math display">\[\text{ Bayesian Model:}\]</span><br><span class="math display">\[\text{Dirichlet prior:} \,  \boldsymbol \pi\sim \text{Dir}(\boldsymbol \alpha)\]</span>
<span class="math display">\[\text{Multinomial likelihood:} \, \boldsymbol x|\boldsymbol \pi\sim \text{Mult}(n, \boldsymbol \pi)\]</span></p>
</div>
</div>
<div id="wishart-distribution" class="section level3" number="1.6.2">
<h3>
<span class="header-section-number">1.6.2</span> Wishart distribution<a class="anchor" aria-label="anchor" href="#wishart-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The Wishart distribution is a multivariate generalisation of the gamma distribution,
also known as univariate Wishart distribution and scaled chi-squared distribution. The exponential and chi-squared distribution are special cases.</p>
<div id="univariate-case-2" class="section level4" number="1.6.2.1">
<h4>
<span class="header-section-number">1.6.2.1</span> Univariate case<a class="anchor" aria-label="anchor" href="#univariate-case-2"><i class="fas fa-link"></i></a>
</h4>
<p>Gamma distribution:</p>
<p><span class="math display">\[z_1,z_2,\ldots,z_m \stackrel{\text{iid}}\sim N(0,\sigma^2_z)\]</span>
<span class="math display">\[x = \sum^{m}_{i=1}z_i^2\]</span>
<span class="math display">\[\mu_x = m \sigma^2_z\]</span>
Then <span class="math inline">\(x\)</span> is distributed as:
<span class="math display">\[
x \sim  \text{W}_1\left(\frac{\mu_x}{m}, m\right) =
        \text{Gam}\left(\alpha=\frac{1}{2} m, \beta=2 \frac{\mu_x}{m}\right) = 
        \frac{\mu_x}{m} \chi^2_m
\]</span>
where <span class="math inline">\(\alpha\)</span> is the shape and <span class="math inline">\(\beta\)</span> the scale parameter of the gamma distribution.</p>
<p>The mean and variance of <span class="math inline">\(x\)</span> are:
<span class="math display">\[\text{E}(x) = \mu_x\]</span>
<span class="math display">\[\text{Var}(x) = \frac{2 \mu^2_x}{m}\]</span></p>
<p>Useful as the distribution of sample variance:
<span class="math display">\[y_1, \ldots, y_n \sim N(\mu_y, \sigma^2)\]</span>
Known mean <span class="math inline">\(\mu_y\)</span>:
<span class="math display">\[\frac{1}{n}\sum_{i=1}^n(y_i -\mu_y)^2 \sim \text{W}_1\left(\frac{\sigma^2}{n}, n\right)\]</span>
Unknown mean <span class="math inline">\(\mu_y\)</span> (estimated by <span class="math inline">\(\bar{y}\)</span>):
<span class="math display">\[\widehat{\sigma^2}_{ML} = \frac{1}{n}\sum_{i=1}^n(y_i -\bar{y})^2 \sim \text{W}_1\left(\frac{\sigma^2}{n}, n-1\right)\]</span>
<span class="math display">\[\widehat{\sigma^2}_{UB} = \frac{1}{n-1}\sum_{i=1}^n(y_i -\bar{y})^2 \sim \text{W}_1\left(\frac{\sigma^2}{n-1}, n-1\right)\]</span></p>
</div>
<div id="multivariate-case-2" class="section level4" number="1.6.2.2">
<h4>
<span class="header-section-number">1.6.2.2</span> Multivariate case<a class="anchor" aria-label="anchor" href="#multivariate-case-2"><i class="fas fa-link"></i></a>
</h4>
<p>Wishart distribution:</p>
<p><span class="math display">\[\boldsymbol z_1,\boldsymbol z_2,\ldots,\boldsymbol z_m \stackrel{\text{iid}}\sim N_d(0,\boldsymbol \Sigma_{\boldsymbol z})\]</span>
<span class="math display">\[\underbrace{\boldsymbol X}_{d\times d}=\sum^{m}_{i=1}\underbrace{\boldsymbol z_i\boldsymbol z_i^T}_{d\times d}\]</span><br><span class="math display">\[\boldsymbol M= (\mu_{ij}) = m \boldsymbol \Sigma_{\boldsymbol z}\]</span></p>
<p>Then <span class="math inline">\(\boldsymbol X\)</span> (a random matrix!) is distributed as:
<span class="math display">\[\boldsymbol X\sim  \text{W}_d\left(\frac{\boldsymbol M}{m}, m\right) \]</span>
with mean and variances:
<span class="math display">\[\text{E}(\boldsymbol X) = \boldsymbol M\]</span>
<span class="math display">\[\text{Var}(x_{ij})= \frac{ \mu^2_{ij}+\mu_{ii}\mu_{jj} }{m} \]</span></p>
<p>Useful as distribution of sample covariance:
<span class="math display">\[\boldsymbol y_1, \ldots, \boldsymbol y_n \sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\]</span>
<span class="math display">\[\frac{1}{n}\sum_{i=1}^n (\boldsymbol y_i -\boldsymbol \mu)(\boldsymbol y_i -\boldsymbol \mu)^T \sim \text{W}_d\left(\boldsymbol \Sigma/n, n\right)\]</span>
<span class="math display">\[\widehat{\boldsymbol \Sigma}_{ML} = \frac{1}{n}\sum_{i=1}^n (\boldsymbol y_i -\bar{\boldsymbol y})(\boldsymbol y_i -\bar{\boldsymbol y})^T \sim \text{W}_d\left(\boldsymbol \Sigma/n, n-1\right)\]</span>
<span class="math display">\[\widehat{\boldsymbol \Sigma}_{UB} = \frac{1}{n-1}\sum_{i=1}^n (\boldsymbol y_i -\bar{\boldsymbol y})(\boldsymbol y_i -\bar{\boldsymbol y})^T \sim \text{W}_d\left(\boldsymbol \Sigma/(n-1), n-1\right)\]</span></p>
</div>
</div>
<div id="inverse-wishart-distribution" class="section level3" number="1.6.3">
<h3>
<span class="header-section-number">1.6.3</span> Inverse Wishart distribution<a class="anchor" aria-label="anchor" href="#inverse-wishart-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The inverse Wishart distribution is a multivariate generalisation of the inverse gamma
distribution (also known as inverse scaled chi-squared distribution).</p>
<div id="univariate-case-3" class="section level4" number="1.6.3.1">
<h4>
<span class="header-section-number">1.6.3.1</span> Univariate case<a class="anchor" aria-label="anchor" href="#univariate-case-3"><i class="fas fa-link"></i></a>
</h4>
<p>Inverse gamma distribution (with <span class="math inline">\(\alpha\)</span> shape
and <span class="math inline">\(\beta\)</span> scale parameter):</p>
<p><span class="math display">\[x \sim \text{W}^{-1}_1(k \mu, k+2) = \text{Inv-Gam}\left( \alpha = \frac{k+2}{2}, \beta=\frac{k \mu}{2} \right) = k \mu\,\text{Inv-$\chi^2_{k+2}$}\]</span></p>
<p>Then <span class="math inline">\(x\)</span> has mean and variance</p>
<p><span class="math display">\[\text{E}(x) = \mu\]</span>
<span class="math display">\[\text{Var}(x)= \frac{2\mu^2}{k-2}\]</span></p>
<p>Relationship to gamma distribution:
<span class="math display">\[
\frac{1}{x} \sim W_1\left(\frac{1}{k \mu}, k+2\right) 
=  \text{Gam}\left(\frac{k+2}{2}, \frac{2}{k \mu}\right) = \frac{1}{k \mu} \, \chi^2_{k+2}
\]</span></p>
</div>
<div id="multivariate-case-3" class="section level4" number="1.6.3.2">
<h4>
<span class="header-section-number">1.6.3.2</span> Multivariate case<a class="anchor" aria-label="anchor" href="#multivariate-case-3"><i class="fas fa-link"></i></a>
</h4>
<p>Inverse Wishart distribution:</p>
<p><span class="math display">\[\underbrace{\boldsymbol X}_{d\times d} \sim \text{W}^{-1}_d\left( k \underbrace{ \boldsymbol M}_{d\times d} \, , \, k+d+1\right)\]</span>
<span class="math display">\[\text{E}(\boldsymbol X) =\boldsymbol M\]</span>
<span class="math display">\[\text{Var}(x_{ij})= \frac{2 }{k-2} \frac{(k+2) \mu_{ij}^2 + k \, \mu_{ii} \mu_{jj} }{2 k + 2}\]</span></p>
<p>Relationship to Wishart:
<span class="math display">\[\boldsymbol X^{-1} \sim \text{W}_d\left( \frac{ \boldsymbol M^{-1}}{k}  \, , k+d+1\right)\]</span></p>
<hr>
<p>The inverse Wishart distribution is useful as prior and posterior distribution
of the variance where <span class="math inline">\(k\)</span> is the sample size parameter and <span class="math inline">\(\boldsymbol M\)</span> resp. <span class="math inline">\(\mu\)</span> is
mean of the distribution for <span class="math inline">\(\boldsymbol \Sigma\)</span> and <span class="math inline">\(\sigma^2\)</span> (in the univariate case).</p>
<hr>
</div>
</div>
<div id="further-distributions" class="section level3" number="1.6.4">
<h3>
<span class="header-section-number">1.6.4</span> Further distributions<a class="anchor" aria-label="anchor" href="#further-distributions"><i class="fas fa-link"></i></a>
</h3>
<p><a href="https://en.wikipedia.org/wiki/List_of_probability_distributions" class="uri">https://en.wikipedia.org/wiki/List_of_probability_distributions</a></p>
<p>Wikipedia is a quite good source for information on distributions!</p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="preface.html">Preface</a></div>
<div class="next"><a href="transformations-and-dimension-reduction.html"><span class="header-section-number">2</span> Transformations and dimension reduction</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#multivariate-random-variables"><span class="header-section-number">1</span> Multivariate random variables</a></li>
<li><a class="nav-link" href="#why-multivariate-statistics"><span class="header-section-number">1.1</span> Why multivariate statistics?</a></li>
<li>
<a class="nav-link" href="#essentials-in-multivariate-statistics"><span class="header-section-number">1.2</span> Essentials in multivariate statistics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#univariate-vs.-multivariate-random-variables"><span class="header-section-number">1.2.1</span> Univariate vs. multivariate random variables</a></li>
<li><a class="nav-link" href="#mean-of-a-random-vector"><span class="header-section-number">1.2.2</span> Mean of a random vector</a></li>
<li><a class="nav-link" href="#variance-of-a-random-vector"><span class="header-section-number">1.2.3</span> Variance of a random vector</a></li>
<li><a class="nav-link" href="#properties-of-the-covariance-matrix"><span class="header-section-number">1.2.4</span> Properties of the covariance matrix</a></li>
<li><a class="nav-link" href="#eigenvalue-decomposition-of-boldsymbol-sigma"><span class="header-section-number">1.2.5</span> Eigenvalue decomposition of \(\boldsymbol \Sigma\)</a></li>
<li><a class="nav-link" href="#quantities-related-to-the-covariance-matrix"><span class="header-section-number">1.2.6</span> Quantities related to the covariance matrix</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#multivariate-normal-distribution"><span class="header-section-number">1.3</span> Multivariate normal distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#univariate-normal-distribution"><span class="header-section-number">1.3.1</span> Univariate normal distribution:</a></li>
<li><a class="nav-link" href="#multivariate-normal-model"><span class="header-section-number">1.3.2</span> Multivariate normal model</a></li>
<li><a class="nav-link" href="#shape-of-the-multivariate-normal-density"><span class="header-section-number">1.3.3</span> Shape of the multivariate normal density</a></li>
<li><a class="nav-link" href="#three-types-of-covariances"><span class="header-section-number">1.3.4</span> Three types of covariances</a></li>
<li><a class="nav-link" href="#concentration-of-probability-mass-for-small-and-large-dimension"><span class="header-section-number">1.3.5</span> Concentration of probability mass for small and large dimension</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#estimation-in-large-sample-and-small-sample-settings"><span class="header-section-number">1.4</span> Estimation in large sample and small sample settings</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#multivariate-data"><span class="header-section-number">1.4.1</span> Multivariate data</a></li>
<li><a class="nav-link" href="#strategies-for-large-sample-estimation"><span class="header-section-number">1.4.2</span> Strategies for large sample estimation</a></li>
<li><a class="nav-link" href="#large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma"><span class="header-section-number">1.4.3</span> Large sample estimates of mean \(\boldsymbol \mu\) and covariance \(\boldsymbol \Sigma\)</a></li>
<li><a class="nav-link" href="#problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions"><span class="header-section-number">1.4.4</span> Problems with maximum likelihood in small sample settings and high dimensions</a></li>
<li><a class="nav-link" href="#estimation-of-covariance-matrix-in-small-sample-settings"><span class="header-section-number">1.4.5</span> Estimation of covariance matrix in small sample settings</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#categorical-and-multinomial-distribution"><span class="header-section-number">1.5</span> Categorical and multinomial distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#categorical-distribution"><span class="header-section-number">1.5.1</span> Categorical distribution</a></li>
<li><a class="nav-link" href="#multinomial-distribution"><span class="header-section-number">1.5.2</span> Multinomial distribution</a></li>
<li><a class="nav-link" href="#entropy-and-maximum-likelihood-analysis-for-the-categorical-distribution"><span class="header-section-number">1.5.3</span> Entropy and maximum likelihood analysis for the categorical distribution</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#further-multivariate-distributions"><span class="header-section-number">1.6</span> Further multivariate distributions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#dirichlet-distribution"><span class="header-section-number">1.6.1</span> Dirichlet distribution</a></li>
<li><a class="nav-link" href="#wishart-distribution"><span class="header-section-number">1.6.2</span> Wishart distribution</a></li>
<li><a class="nav-link" href="#inverse-wishart-distribution"><span class="header-section-number">1.6.3</span> Inverse Wishart distribution</a></li>
<li><a class="nav-link" href="#further-distributions"><span class="header-section-number">1.6.4</span> Further distributions</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Multivariate Statistics and Machine Learning</strong>" was written by Korbinian Strimmer. It was last built on 2 June 2022.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
