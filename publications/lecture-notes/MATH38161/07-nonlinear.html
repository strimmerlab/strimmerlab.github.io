<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Nonlinear and nonparametric models – Multivariate Statistics and Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./bibliography.html" rel="next">
<link href="./06-dependence.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3c04d35918bfbae480bb424d60ad250e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./07-nonlinear.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear and nonparametric models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Multivariate Statistics and Machine Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-multivariate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Multivariate random variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multivariate estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations and dimension reduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised learning and clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning and classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-dependence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multivariate dependencies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-nonlinear.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear and nonparametric models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#random-forests" id="toc-random-forests" class="nav-link active" data-scroll-target="#random-forests"><span class="header-section-number">7.1</span> Random forests</a>
  <ul class="collapse">
  <li><a href="#stochastic-vs.-algorithmic-models" id="toc-stochastic-vs.-algorithmic-models" class="nav-link" data-scroll-target="#stochastic-vs.-algorithmic-models">Stochastic vs.&nbsp;algorithmic models</a></li>
  <li><a href="#random-forests-1" id="toc-random-forests-1" class="nav-link" data-scroll-target="#random-forests-1">Random forests</a></li>
  <li><a href="#comparison-of-decision-boundaries-decision-tree-vs.-random-forest" id="toc-comparison-of-decision-boundaries-decision-tree-vs.-random-forest" class="nav-link" data-scroll-target="#comparison-of-decision-boundaries-decision-tree-vs.-random-forest">Comparison of decision boundaries: decision tree vs.&nbsp;random forest</a></li>
  </ul></li>
  <li><a href="#gaussian-processes" id="toc-gaussian-processes" class="nav-link" data-scroll-target="#gaussian-processes"><span class="header-section-number">7.2</span> Gaussian processes</a>
  <ul class="collapse">
  <li><a href="#main-concepts" id="toc-main-concepts" class="nav-link" data-scroll-target="#main-concepts">Main concepts</a></li>
  <li><a href="#conditional-multivariate-normal-distribution" id="toc-conditional-multivariate-normal-distribution" class="nav-link" data-scroll-target="#conditional-multivariate-normal-distribution">Conditional multivariate normal distribution</a></li>
  <li><a href="#covariance-functions-and-kernels" id="toc-covariance-functions-and-kernels" class="nav-link" data-scroll-target="#covariance-functions-and-kernels">Covariance functions and kernels</a></li>
  <li><a href="#gp-model" id="toc-gp-model" class="nav-link" data-scroll-target="#gp-model">GP model</a></li>
  <li><a href="#gaussian-process-example" id="toc-gaussian-process-example" class="nav-link" data-scroll-target="#gaussian-process-example">Gaussian process example</a></li>
  </ul></li>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link" data-scroll-target="#neural-networks"><span class="header-section-number">7.3</span> Neural networks</a>
  <ul class="collapse">
  <li><a href="#history" id="toc-history" class="nav-link" data-scroll-target="#history">History</a></li>
  <li><a href="#neural-networks-1" id="toc-neural-networks-1" class="nav-link" data-scroll-target="#neural-networks-1">Neural networks</a></li>
  <li><a href="#learning-more-about-deep-learning" id="toc-learning-more-about-deep-learning" class="nav-link" data-scroll-target="#learning-more-about-deep-learning">Learning more about deep learning</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear and nonparametric models</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In the last part of the module we discuss methods that go beyond the linear parametric methods prevalent in classical multivariate statistics.</p>
<p><strong>Relevant textbooks:</strong></p>
<p>The lectures for much of this part of the module follow selected chapters from the following text books:</p>
<ul>
<li><p><span class="citation" data-cites="JWHT2021">James et al. (<a href="bibliography.html#ref-JWHT2021" role="doc-biblioref">2021</a>)</span> <a href="https://www.statlearning.com"><em>An introduction to statistical learning with applications in R (2nd edition)</em></a>. Springer.</p></li>
<li><p><span class="citation" data-cites="RG2017">Rogers and Girolami (<a href="bibliography.html#ref-RG2017" role="doc-biblioref">2017</a>)</span> <a href="https://www.crcpress.com/A-First-Course-in-Machine-Learning-Second-Edition/Rogers-Girolami/p/book/9781498738484"><em>A first course in machine learning (2nd edition)</em></a>. CRC Press.</p></li>
</ul>
<p>Please study the relevant section and chapters as indicated below in each subsection!</p>
<p>The first book is also available in a version whith examples in Python:</p>
<ul>
<li><span class="citation" data-cites="JWHTT2023">James et al. (<a href="bibliography.html#ref-JWHTT2023" role="doc-biblioref">2023</a>)</span> <a href="https://www.statlearning.com"><em>An introduction to statistical learning with applications in Python</em></a>. Springer.</li>
</ul>
<div style="page-break-after: always;"></div>
<section id="random-forests" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="random-forests"><span class="header-section-number">7.1</span> Random forests</h2>
<p>Another widely used approach for prediction in nonlinear settings is the method of random forests.</p>
<p><strong>Relevant reading:</strong></p>
<p>Please read: <span class="citation" data-cites="JWHT2021">James et al. (<a href="bibliography.html#ref-JWHT2021" role="doc-biblioref">2021</a>)</span> or <span class="citation" data-cites="JWHTT2023">James et al. (<a href="bibliography.html#ref-JWHTT2023" role="doc-biblioref">2023</a>)</span> <strong>Chapter 8 “Tree-Based Methods”</strong></p>
<p>Specifically:</p>
<ul>
<li>Section 8.1 The Basics of Decision Trees</li>
<li>Section 8.2.1 Bagging</li>
<li>Section 8.2.2 Random Forests</li>
</ul>
<section id="stochastic-vs.-algorithmic-models" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-vs.-algorithmic-models">Stochastic vs.&nbsp;algorithmic models</h3>
<p>Two cultures in statistical modelling: stochastic vs.&nbsp;algorithmic models</p>
<p>Classic discussion paper by Leo Breiman (2001): Statistical modeling: the two cultures. Statistical Science <strong>16</strong>:199–231. <a href="https://doi.org/10.1214/ss/1009213726" class="uri">https://doi.org/10.1214/ss/1009213726</a></p>
<p>This paper has recently be revisited in the following discussion paper by Efron (2020) and discussants: Prediction, estimation, and attribution. JASA <strong>115</strong>:636–677. <a href="https://doi.org/10.1080/01621459.2020.1762613" class="uri">https://doi.org/10.1080/01621459.2020.1762613</a></p>
</section>
<section id="random-forests-1" class="level3">
<h3 class="anchored" data-anchor-id="random-forests-1">Random forests</h3>
<p>Proposed by Leo Breimann in 2001 as application of “bagging” (Breiman 1996) to decision trees.</p>
<p>Basic idea:</p>
<ul>
<li>A single decision tree is unreliable and unstable (weak predictor/classifier).</li>
<li>Use boostrap to generate multiple decision trees (=“forest”)</li>
<li>Average over predictions from all tree (=“bagging”, bootstrap aggregation)</li>
</ul>
<p>The averaging procedure has the effect of variance stabilisation. Intringuingly, averaging across all decision trees dramatically improves the overall prediction accuracy!</p>
<p>The Random Forests approach is an example of an <strong>ensemble method</strong> (since it is based on using an “ensemble” of trees).</p>
<p>Variations: boosting, XGBoost ( <a href="https://xgboost.ai" class="uri">https://xgboost.ai</a> )</p>
<p>Random forests will be applied in Worksheet 11.</p>
<p>They are computationally expensive but typically perform very well!</p>
</section>
<section id="comparison-of-decision-boundaries-decision-tree-vs.-random-forest" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-decision-boundaries-decision-tree-vs.-random-forest">Comparison of decision boundaries: decision tree vs.&nbsp;random forest</h3>
<div id="fig-dtrfnonnested" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dtrfnonnested-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/dtrf-nonnested.png" class="img-fluid figure-img" style="width:90.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dtrfnonnested-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.1: Decision boundaries for decision trees and random forests in the non-nested case.
</figcaption>
</figure>
</div>
<div id="fig-dtrfnested" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dtrfnested-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/dtrf-nested.png" class="img-fluid figure-img" style="width:90.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dtrfnested-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.2: Decision boundaries for decision trees and random forests in the nested case.
</figcaption>
</figure>
</div>
<p>Compare the non-nested case (<a href="#fig-dtrfnonnested" class="quarto-xref">Figure&nbsp;<span>7.1</span></a>) and the nested case (<a href="#fig-dtrfnested" class="quarto-xref">Figure&nbsp;<span>7.2</span></a>).</p>
<p>Compare also with the decision boundaries for LDA and QDA in <a href="05-classification.html#fig-ldaqdanonnested" class="quarto-xref">Figure&nbsp;<span>5.4</span></a> and <a href="05-classification.html#fig-ldaqdanested" class="quarto-xref">Figure&nbsp;<span>5.5</span></a>.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="gaussian-processes" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="gaussian-processes"><span class="header-section-number">7.2</span> Gaussian processes</h2>
<p>Gaussian processes offer a nonparametric probabilistic approach to model nonlinear dependencies.</p>
<p><strong>Relevant reading:</strong></p>
<p>Please read: <span class="citation" data-cites="RG2017">Rogers and Girolami (<a href="bibliography.html#ref-RG2017" role="doc-biblioref">2017</a>)</span> <strong>Chapter 8: Gaussian processes.</strong></p>
<section id="main-concepts" class="level3">
<h3 class="anchored" data-anchor-id="main-concepts">Main concepts</h3>
<ul>
<li>Gaussian processes (GPs) belong the the family of <strong>Bayesian nonparametric models</strong></li>
<li>Idea:
<ul>
<li>start with prior over a function (!),</li>
<li>then condition on observed data to get posterior distribution (again over a function)</li>
</ul></li>
<li>GPs use an infinitely dimensional multivariate normal distribution as prior</li>
</ul>
</section>
<section id="conditional-multivariate-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="conditional-multivariate-normal-distribution">Conditional multivariate normal distribution</h3>
<p>GPs make use of the fact that marginal and conditional distributions of a multivariate normal distribution are also multivariate normal.</p>
<p><strong>Multivariate normal distribution:</strong></p>
<p><span class="math display">\[\boldsymbol z\sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\]</span></p>
<p>Assume: <span class="math display">\[
\boldsymbol z=\begin{pmatrix}
    \boldsymbol z_1      \\
    \boldsymbol z_2      \\
\end{pmatrix}
\]</span> with <span class="math display">\[
\boldsymbol \mu=\begin{pmatrix}
    \boldsymbol \mu_1      \\
    \boldsymbol \mu_2      \\
\end{pmatrix}
\]</span> and <span class="math display">\[
\boldsymbol \Sigma=\begin{pmatrix}
    \boldsymbol \Sigma_{1}   &amp; \boldsymbol \Sigma_{12}   \\
    \boldsymbol \Sigma_{12}^T &amp; \boldsymbol \Sigma_{2}   \\
\end{pmatrix}
\]</span> with corresponding dimensions <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span> and <span class="math inline">\(d_1+d_2=d\)</span>.</p>
<p><strong>Marginal distributions:</strong></p>
<p>Any subset of <span class="math inline">\(\boldsymbol z\)</span> is also multivariate normally distributed. Specifically, <span class="math display">\[
\boldsymbol z_1 \sim N_{d_1}(\boldsymbol \mu_1, \boldsymbol \Sigma_{1})
\]</span> and <span class="math display">\[
\boldsymbol z_2 \sim N_{d_2}(\boldsymbol \mu_2, \boldsymbol \Sigma_{2})
\]</span></p>
<p><strong>Conditional multivariate normal:</strong></p>
<p>The conditional distribution is also multivariate normal: <span class="math display">\[
\boldsymbol z_1 | \boldsymbol z_2 = \boldsymbol z_{1 | 2} \sim N_{d_1}(\boldsymbol \mu_{1|2}, \boldsymbol \Sigma_{1 | 2})
\]</span> with <span class="math display">\[\boldsymbol \mu_{1|2}=\boldsymbol \mu_1 + \boldsymbol \Sigma_{12} \boldsymbol \Sigma_{2}^{-1} (\boldsymbol z_2 -\boldsymbol \mu_2)\]</span> and <span class="math display">\[\boldsymbol \Sigma_{1 | 2}=\boldsymbol \Sigma_{1} -  \boldsymbol \Sigma_{12} \boldsymbol \Sigma_{2}^{-1} \boldsymbol \Sigma_{12}^T\]</span></p>
<p><span class="math inline">\(\boldsymbol z_{1 | 2}\)</span> and <span class="math inline">\(\boldsymbol \mu_{1|2}\)</span> have dimension <span class="math inline">\(d_1 \times 1\)</span> and <span class="math inline">\(\boldsymbol \Sigma_{1 | 2}\)</span> has dimension <span class="math inline">\(d_1 \times d_1\)</span>, i.e.&nbsp;the same dimension as the unconditioned variables.</p>
<p>You may recall the above formula in the context of linear regression, with <span class="math inline">\(y = z_1\)</span> and <span class="math inline">\(\boldsymbol x= \boldsymbol z_2\)</span> so that the conditional mean becomes <span class="math display">\[
\begin{split}
\text{E}(y|\boldsymbol x) &amp;=\boldsymbol \mu_y + \boldsymbol \Sigma_{y\boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x}^{-1} (\boldsymbol x-\boldsymbol \mu_{\boldsymbol x})\\
&amp;= \beta_0+ \boldsymbol \beta^T \boldsymbol x\\
\end{split}
\]</span> with <span class="math inline">\(\boldsymbol \beta= \boldsymbol \Sigma_{\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}\)</span> and <span class="math inline">\(\beta_0 = \boldsymbol \mu_y-\boldsymbol \beta^T \boldsymbol \mu_{\boldsymbol x}\)</span>, and the corresponding conditional variance is <span class="math display">\[
\text{Var}(y|\boldsymbol x) = \sigma^2_y -  \boldsymbol \Sigma_{y\boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy} \,.
\]</span></p>
</section>
<section id="covariance-functions-and-kernels" class="level3">
<h3 class="anchored" data-anchor-id="covariance-functions-and-kernels">Covariance functions and kernels</h3>
<p>The GP prior is an infinitely dimensional multivariate normal distribution with mean zero and the <strong>covariance specified by a function</strong> <span class="math inline">\(k(x, x^{\prime})\)</span>:</p>
<p>A widely used covariance function is <span class="math display">\[
k(x, x^{\prime}) = \text{Cov}(x, x^{\prime}) = \sigma^2 e^{-\frac{ (x-x^{\prime})^2}{2 l^2}}
\]</span> This is known as the <strong>squared-exponential kernel</strong> or <strong>Radial-basis function (RBF) kernel</strong>.</p>
<p>Note that this kernel implies</p>
<ul>
<li><span class="math inline">\(k(x, x) = \text{Var}(x) = \sigma^2\)</span> and</li>
<li><span class="math inline">\(\text{Cor}(x, x^{\prime}) =  e^{-\frac{ (x-x^{\prime})^2}{2 l^2}}\)</span>.</li>
</ul>
<p>The parameter <span class="math inline">\(l\)</span> in the RBF kernel is the length scale parameter and describes the “wigglyness” or “stiffness” of the resulting function. Small values of <span class="math inline">\(l\)</span> correspond to more complex, more wiggly functions, and to low spatial correlation, as the correlation decreases quicker with distance, and large values correspond to more rigid, stiffer functions, with longer range spatial correlation (note that in a time series context this would be called autocorrelation).</p>
<p>There are many other kernel functions, including linear, polynomial or periodic kernels.</p>
</section>
<section id="gp-model" class="level3">
<h3 class="anchored" data-anchor-id="gp-model">GP model</h3>
<p>Nonlinear regression in the GP approach is conceptually very simple:</p>
<ul>
<li>start with multivariate prior</li>
<li>then condition on the observed data</li>
<li>the resulting conditional multivariate normal can used to predict the function values at any unobserved values</li>
<li>the conditional variance can be used to compute credible intervals for predictions.</li>
</ul>
<p>GP regression also provides a direct link with classical Bayesian linear regression (when using a linear kernel). Furthermore, GPs are also linked with neural networks as their limit in the case of an infinitely wide network (see section on neural networks).</p>
<p>Drawbacks of GPs: computationally expensive, typically <span class="math inline">\(O(n^3)\)</span> because of the matrix inversion. However, there are now variations of GPs that help to overcome this issue (e.g.&nbsp;sparse GPs).</p>
</section>
<section id="gaussian-process-example" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-process-example">Gaussian process example</h3>
<p>We now show how to apply Gaussian processes in R justing using standard matrix calculations.</p>
<p>Our aim is to estimate the following nonlinear function from a number of observations. Note that initially we assume that there is no additional noise (so the observations lie directly on the curve):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>truefunc <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">sin</span>(x)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>XLIM <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>YLIM <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>n2 <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> <span class="fu">runif</span>(n2, <span class="at">min=</span>XLIM[<span class="dv">1</span>], <span class="at">max=</span>XLIM[<span class="dv">2</span>])</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>y2 <span class="ot">=</span> <span class="fu">truefunc</span>(x2)  <span class="co"># no noise</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>( <span class="fu">truefunc</span>(x), <span class="at">xlim=</span>XLIM, <span class="at">ylim=</span>YLIM, <span class="at">xlab=</span><span class="st">"x"</span>, <span class="at">ylab=</span><span class="st">"y"</span>, </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">main=</span><span class="st">"True Function"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x2, y2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="07-nonlinear_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Use the RFB kernel as the prior covariance and assume that the prior has mean zero:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># RBF kernel</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>rbfkernel <span class="ot">=</span> <span class="cf">function</span>(xa, xb, <span class="at">s2=</span><span class="dv">1</span>, <span class="at">l=</span><span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>) s2<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span><span class="dv">1</span><span class="sc">/</span><span class="dv">2</span><span class="sc">*</span>(xa<span class="sc">-</span>xb)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>l<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>kfun.mat <span class="ot">=</span> <span class="cf">function</span>(xavec, xbvec, <span class="at">FUN=</span>rbfkernel) </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">outer</span>(<span class="at">X=</span><span class="fu">as.vector</span>(xavec), <span class="at">Y=</span><span class="fu">as.vector</span>(xbvec), <span class="at">FUN=</span>FUN)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># prior mean</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>mu.vec <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Visualise the functions sampled from the multivariate normal prior:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># grid of x-values </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>n1 <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">seq</span>(XLIM[<span class="dv">1</span>], XLIM[<span class="dv">2</span>], <span class="at">length.out=</span>n1)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># unconditioned covariance and mean (unobserved samples x1)</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>K1 <span class="ot">=</span> <span class="fu">kfun.mat</span>(x1, x1)  </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">=</span> <span class="fu">mu.vec</span>(x1)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="do">## sample functions from GP prior  </span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>B <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"MASS"</span>) <span class="co"># for mvrnorm</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>y1r <span class="ot">=</span> <span class="fu">t</span>(<span class="fu">mvrnorm</span>(B, <span class="at">mu =</span> m1, <span class="at">Sigma=</span>K1))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1, y1r[,<span class="dv">1</span>], <span class="at">type=</span><span class="st">"l"</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">ylab=</span><span class="st">"y"</span>, <span class="at">xlab=</span><span class="st">"x"</span>, <span class="at">ylim=</span>YLIM, </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">main=</span><span class="st">"Prior Functions (RBF Kernel with l=1/2)"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>B)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(x1, y1r[,i], <span class="at">col=</span>i, <span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="07-nonlinear_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Compute the posterior mean and variance by conditioning on the observations:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># unconditioned covariance and mean (observed samples x2)</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>K2 <span class="ot">=</span> <span class="fu">kfun.mat</span>(x2, x2)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">=</span> <span class="fu">mu.vec</span>(x2)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>iK2 <span class="ot">=</span> <span class="fu">solve</span>(K2) <span class="co"># inverse</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># cross-covariance</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>K12 <span class="ot">=</span> <span class="fu">kfun.mat</span>(x1, x2)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Conditioning: x1 conditioned on x2</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># conditional mean</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>m1<span class="fl">.2</span> <span class="ot">=</span> m1 <span class="sc">+</span> K12 <span class="sc">%*%</span> iK2 <span class="sc">%*%</span> (y2 <span class="sc">-</span> m2)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># conditional variance</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>K1<span class="fl">.2</span> <span class="ot">=</span> K1 <span class="sc">-</span> K12 <span class="sc">%*%</span> iK2 <span class="sc">%*%</span> <span class="fu">t</span>(K12)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Plot the posterior mean and upper and lower bounds of a 95% credible interval:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># upper and lower CI</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>upper.bound <span class="ot">=</span> m1<span class="fl">.2</span> <span class="sc">+</span> <span class="fl">1.96</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">diag</span>(K1<span class="fl">.2</span>))</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>lower.bound <span class="ot">=</span> m1<span class="fl">.2</span> <span class="sc">-</span> <span class="fl">1.96</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">diag</span>(K1<span class="fl">.2</span>))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1, m1<span class="fl">.2</span>, <span class="at">type=</span><span class="st">"l"</span>, <span class="at">xlim=</span>XLIM, <span class="at">ylim=</span>YLIM, <span class="at">col=</span><span class="st">"red"</span>, <span class="at">lwd=</span><span class="dv">3</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylab=</span><span class="st">"y"</span>, <span class="at">xlab =</span> <span class="st">"x"</span>, <span class="at">main =</span> <span class="st">"Posterior"</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x2,y2,<span class="at">pch=</span><span class="dv">4</span>,<span class="at">lwd=</span><span class="dv">4</span>,<span class="at">col=</span><span class="st">"blue"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1,upper.bound,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1,lower.bound,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">truefunc</span>(x), <span class="at">xlim=</span>XLIM, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">"gray"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x=</span><span class="st">"topright"</span>, </span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"posterior mean"</span>, <span class="st">"posterior quantiles"</span>, <span class="st">"true function"</span>),</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>),<span class="at">lwd=</span><span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">col=</span><span class="fu">c</span>(<span class="st">"red"</span>,<span class="st">"black"</span>, <span class="st">"gray"</span>), <span class="at">cex=</span><span class="fl">1.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="07-nonlinear_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Finally, we can take into acount noise at the measured data points by adding an error term:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add some noise</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>sdeps <span class="ot">=</span> <span class="fl">0.1</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>K2 <span class="ot">=</span> K2 <span class="sc">+</span> sdeps<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span><span class="fu">diag</span>(<span class="dv">1</span>,<span class="fu">length</span>(x2))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># update</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>iK2 <span class="ot">=</span> <span class="fu">solve</span>(K2) <span class="co"># inverse</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>m1<span class="fl">.2</span> <span class="ot">=</span> m1 <span class="sc">+</span> K12 <span class="sc">%*%</span> iK2 <span class="sc">%*%</span> (y2 <span class="sc">-</span> m2)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>K1<span class="fl">.2</span> <span class="ot">=</span> K1 <span class="sc">-</span> K12 <span class="sc">%*%</span> iK2 <span class="sc">%*%</span> <span class="fu">t</span>(K12)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>upper.bound <span class="ot">=</span> m1<span class="fl">.2</span> <span class="sc">+</span> <span class="fl">1.96</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">diag</span>(K1<span class="fl">.2</span>))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>lower.bound <span class="ot">=</span> m1<span class="fl">.2</span> <span class="sc">-</span> <span class="fl">1.96</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">diag</span>(K1<span class="fl">.2</span>))</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1, m1<span class="fl">.2</span>, <span class="at">type=</span><span class="st">"l"</span>, <span class="at">xlim=</span>XLIM, <span class="at">ylim=</span>YLIM, <span class="at">col=</span><span class="st">"red"</span>, <span class="at">lwd=</span><span class="dv">3</span>, </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylab=</span><span class="st">"y"</span>, <span class="at">xlab =</span> <span class="st">"x"</span>, <span class="at">main =</span> <span class="st">"Posterior (with noise)"</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x2,y2,<span class="at">pch=</span><span class="dv">4</span>,<span class="at">lwd=</span><span class="dv">4</span>,<span class="at">col=</span><span class="st">"blue"</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1,upper.bound,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1,lower.bound,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">truefunc</span>(x), <span class="at">xlim=</span>XLIM, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">"gray"</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x=</span><span class="st">"topright"</span>, </span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"posterior mean"</span>, <span class="st">"posterior quantiles"</span>, <span class="st">"true function"</span>),</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>),<span class="at">lwd=</span><span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">col=</span><span class="fu">c</span>(<span class="st">"red"</span>,<span class="st">"black"</span>, <span class="st">"gray"</span>), <span class="at">cex=</span><span class="fl">1.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="07-nonlinear_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Note that in the vicinity of data points the CIs are small and the further away from data the more uncertain the estimate of the underlying function becomes.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="neural-networks" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="neural-networks"><span class="header-section-number">7.3</span> Neural networks</h2>
<p>Another highly important class of models for nonlinear prediction (and nonlinear function approximation) are neural networks.</p>
<p><strong>Relevant reading:</strong></p>
<p>Please read: <span class="citation" data-cites="JWHT2021">James et al. (<a href="bibliography.html#ref-JWHT2021" role="doc-biblioref">2021</a>)</span> or <span class="citation" data-cites="JWHTT2023">James et al. (<a href="bibliography.html#ref-JWHTT2023" role="doc-biblioref">2023</a>)</span> <strong>Chapter 10 “Deep Learning”</strong></p>
<section id="history" class="level3">
<h3 class="anchored" data-anchor-id="history">History</h3>
<p>Neural networks are actually relatively old models, going back to the 1950s.</p>
<p>Three phases of neural networks (NN)</p>
<ul>
<li>1950/60: replicating functions of neurons in the brain (e.g.&nbsp;perceptron)</li>
<li>1980/90: neural networks as <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal function approximators</a></li>
<li>2010—today: deep learning</li>
</ul>
<p>The first phase was biologically inspired, the second phase focused on mathematical properties, and the current phase is pushed forward by advances in computer science and numerical optimisation:</p>
<ul>
<li>backpropagation algorithm</li>
<li>efficient automatic symbolic differentiation (e.g.&nbsp;autograd)</li>
<li>stochastic gradient descent algorithms (e.g.&nbsp;Adam)</li>
<li>use of GPUs and TPUs (e.g.&nbsp;for linear algebra)</li>
<li>availability of packages for symbolic tensor computations and deep learning.</li>
</ul>
<p>Currently the most popular frameworks are:</p>
<ul>
<li><a href="https://pytorch.org">PyTorch</a> (PyTorch Foundation, formerly Meta/Facebook)</li>
<li><a href="https://www.tensorflow.org">TensorFlow</a> (Google Research)</li>
<li><a href="https://flax.readthedocs.io/en/latest/">Flax</a> / <a href="https://jax.readthedocs.io/en/latest/">JAX</a> (Google Research)</li>
</ul>
<p>and high-level wrappers:</p>
<ul>
<li><a href="https://skorch.readthedocs.io/en/latest/">skorch</a> (scikit-learn wrapper for PyTorch)</li>
<li><a href="https://keras.io/keras_3/">Keras 3</a> (for TensorFlow, JAX, and PyTorch)</li>
</ul>
</section>
<section id="neural-networks-1" class="level3">
<h3 class="anchored" data-anchor-id="neural-networks-1">Neural networks</h3>
<p>Neural networks are essentially stacked systems of linear regressions, with nonlinear mappings between each layer, mapping the input to output via one or more layers of internal hidden nodes corresponding to internal latent variables:</p>
<ul>
<li>Each internal node is a nonlinear function of all or some of nodes in the previous layer</li>
<li>Typically, the output of a node is computed using a <strong>non-linear activation function</strong>, such as the sigmoid function or a piecewise linear function (ReLU), from a linear combination of the input variables of that node.</li>
</ul>
<p>A simple architecture is a feedforward network with a single hidden layer. More complex models are multilayer perceptrons and convolutional neural networks.</p>
<p>It can be shown that even simple network architectures can (with sufficient number of nodes) approximate any arbitrary non-linear function. This is called the <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"><strong>universal function approximation</strong></a> property.</p>
<p>“Deep” neural networks have many layers, and the optimisation of their parameters requires advanced techniques (see above), with the objective function typically an empirical risk based on, e.g., squared error loss or cross-entropy loss. Neural networks are very highly parameterised models and therefore require lots of data for training, and typically also some form of regularisation (e.g.&nbsp;dropout).</p>
<p>As an extreme example, the neural network behind the <a href="https://en.wikipedia.org/wiki/GPT-4">ChatGPT 4</a> language model that is trained on essentially the whole freely accessible text available on the internet has an estimated 1.76 trillion (!) parameters (<span class="math inline">\(1.76 \times 10^{12}\)</span>).</p>
<p><strong>In the limit of an infinite width a single layer fully connected neural network becomes equivalent to a Gaussian process</strong>. This was first shown by R. M. Neal (1996)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. More recently, this equivalence has also been demonstrated for other types of neural networks (with the kernel function of the GP being determined by the neural network architecture). This is formalised in the “neural tangent kernel” (NTK) framework.</p>
<p>Some of the statistical aspects of neural networks are not well understood. For example, there is the <strong>paradox that neural networks typically overfit the training data but still generalise well</strong> - this clearly violates the traditional understanding of bias-variance tradeoff for classical modelling in statistics and machine learning — see for example Belkin et al.&nbsp;(2019)<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Some researchers argue that this contradiction can be resolved by better understanding the effective dimension of complex models. There is a lot of current research to explain this phenomenon of “multiple descent”, i.e.&nbsp;the decrease of prediction error for models with very many parameters. A further topic is robustness of the predictions, which is also caused by overfitting. It is well known that neural networks can sometimes be “fooled” by so-called adversarial examples, e.g., the classification of a sample may change if a small amount of noise is added to the test data.</p>
</section>
<section id="learning-more-about-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="learning-more-about-deep-learning">Learning more about deep learning</h3>
<p>A good place to learn more about the concepts of deep learning is the book “Understanding Deep Learning” by <span class="citation" data-cites="Prince2023">Prince (<a href="bibliography.html#ref-Prince2023" role="doc-biblioref">2023</a>)</span> available online at <a href="https://udlbook.github.io/udlbook/" class="uri">https://udlbook.github.io/udlbook/</a>. For actual application in computer code using various software frameworks the book “Dive Into Deep Learning” by <span class="citation" data-cites="ZLLS2023">Zhang et al. (<a href="bibliography.html#ref-ZLLS2023" role="doc-biblioref">2023</a>)</span> available online at <a href="https://d2l.ai" class="uri">https://d2l.ai</a> is recommended.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-JWHT2021" class="csl-entry" role="listitem">
James, G., D. Witten, T. Hastie, and R. Tibshirani. 2021. <em>An Introduction to Statistical Learning with Applications in <span>R</span></em>. 2nd ed. Springer. <a href="https://doi.org/10.1007/978-1-0716-1418-1">https://doi.org/10.1007/978-1-0716-1418-1</a>.
</div>
<div id="ref-JWHTT2023" class="csl-entry" role="listitem">
James, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023. <em>An Introduction to Statistical Learning with Applications in <span>Python</span></em>. Springer. <a href="https://doi.org/10.1007/978-3-031-38747-0">https://doi.org/10.1007/978-3-031-38747-0</a>.
</div>
<div id="ref-Prince2023" class="csl-entry" role="listitem">
Prince, S. J. D. 2023. <em>Understanding Deep Learning</em>. MIT Press. <a href="https://mitpress.mit.edu/9780262048644/understanding-deep-learning/">https://mitpress.mit.edu/9780262048644/understanding-deep-learning/</a>.
</div>
<div id="ref-RG2017" class="csl-entry" role="listitem">
Rogers, S., and M. Girolami. 2017. <em>A First Course in Machine Learning</em>. 2nd ed. Chapman; Hall / CRC. <a href="https://doi.org/10.1201/9781315382159">https://doi.org/10.1201/9781315382159</a>.
</div>
<div id="ref-ZLLS2023" class="csl-entry" role="listitem">
Zhang, A., Z. C. Lipton, M. Li, and A. J. Smola. 2023. <em>Dive into Deep Learning</em>. <a href="https://d2l.ai">https://d2l.ai</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Neal, R. M. 1996. Bayesian Learning for Neural Networks. Springer. <a href="https://doi.org/10.1007/978-1-4612-0745-0" class="uri">https://doi.org/10.1007/978-1-4612-0745-0</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Belkin, M. et al.&nbsp;2019. Reconciling modern machine-learning practice and the classical bias–variance trade-off. PNAS <strong>116</strong>: 15849–15854. <a href="https://doi.org/10.1073/pnas.1903070116" class="uri">https://doi.org/10.1073/pnas.1903070116</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH38161");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./06-dependence.html" class="pagination-link" aria-label="Multivariate dependencies">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multivariate dependencies</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./bibliography.html" class="pagination-link" aria-label="Bibliography">
        <span class="nav-page-text">Bibliography</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>