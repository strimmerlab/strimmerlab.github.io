<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Transformations and dimension reduction | Multivariate Statistics and Machine Learning MATH38161</title>
  <meta name="description" content="2 Transformations and dimension reduction | Multivariate Statistics and Machine Learning MATH38161" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Transformations and dimension reduction | Multivariate Statistics and Machine Learning MATH38161" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Transformations and dimension reduction | Multivariate Statistics and Machine Learning MATH38161" />
  
  
  

<meta name="author" content="Korbinian Strimmer" />


<meta name="date" content="2020-09-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-multivariate.html"/>
<link rel="next" href="3-clustering.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH38161/index.html">MATH38161 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-multivariate.html"><a href="1-multivariate.html"><i class="fa fa-check"></i><b>1</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="1.1" data-path="1-multivariate.html"><a href="1-multivariate.html#why-multivariate-statistics"><i class="fa fa-check"></i><b>1.1</b> Why multivariate statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="1-multivariate.html"><a href="1-multivariate.html#basics"><i class="fa fa-check"></i><b>1.2</b> Basics</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-vs.multivariate-random-variables"><i class="fa fa-check"></i><b>1.2.1</b> Univariate vs.Â multivariate random variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-multivariate.html"><a href="1-multivariate.html#mean-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.2</b> Mean of a random vector</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-multivariate.html"><a href="1-multivariate.html#variance-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.3</b> Variance of a random vector</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-multivariate.html"><a href="1-multivariate.html#properties-of-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.4</b> Properties of the covariance matrix</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-multivariate.html"><a href="1-multivariate.html#eigenvalue-decomposition-of-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.2.5</b> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.2.6" data-path="1-multivariate.html"><a href="1-multivariate.html#quantities-related-to-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.6</b> Quantities related to the covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>1.3</b> Multivariate normal distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-normal-distribution"><i class="fa fa-check"></i><b>1.3.1</b> Univariate normal distribution:</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-model"><i class="fa fa-check"></i><b>1.3.2</b> Multivariate normal model</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-multivariate.html"><a href="1-multivariate.html#shape-of-the-contour-lines-of-the-multivariate-normal-density"><i class="fa fa-check"></i><b>1.3.3</b> Shape of the contour lines of the multivariate normal density</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-in-large-sample-and-small-sample-settings"><i class="fa fa-check"></i><b>1.4</b> Estimation in large sample and small sample settings</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-multivariate.html"><a href="1-multivariate.html#data-matrix"><i class="fa fa-check"></i><b>1.4.1</b> Data matrix</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-multivariate.html"><a href="1-multivariate.html#strategies-for-large-sample-estimation"><i class="fa fa-check"></i><b>1.4.2</b> Strategies for large sample estimation</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-multivariate.html"><a href="1-multivariate.html#large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.4.3</b> Large sample estimates of mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.4.4" data-path="1-multivariate.html"><a href="1-multivariate.html#problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions"><i class="fa fa-check"></i><b>1.4.4</b> Problems with maximum likelihood in small sample settings and high dimensions</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-of-covariance-matrix-in-small-sample-settings"><i class="fa fa-check"></i><b>1.4.5</b> Estimation of covariance matrix in small sample settings</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-multivariate.html"><a href="1-multivariate.html#discrete-multivariate-distributions"><i class="fa fa-check"></i><b>1.5</b> Discrete multivariate distributions</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-multivariate.html"><a href="1-multivariate.html#categorical-distribution"><i class="fa fa-check"></i><b>1.5.1</b> Categorical distribution</a></li>
<li class="chapter" data-level="1.5.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multinomial-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Multinomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="1-multivariate.html"><a href="1-multivariate.html#continuous-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Continuous multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-multivariate.html"><a href="1-multivariate.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.6.1</b> Dirichlet distribution</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-multivariate.html"><a href="1-multivariate.html#wishart-distribution"><i class="fa fa-check"></i><b>1.6.2</b> Wishart distribution</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-multivariate.html"><a href="1-multivariate.html#inverse-wishart-distribution"><i class="fa fa-check"></i><b>1.6.3</b> Inverse Wishart distribution</a></li>
<li class="chapter" data-level="1.6.4" data-path="1-multivariate.html"><a href="1-multivariate.html#further-distributions"><i class="fa fa-check"></i><b>1.6.4</b> Further distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-transformations.html"><a href="2-transformations.html"><i class="fa fa-check"></i><b>2</b> Transformations and dimension reduction</a><ul>
<li class="chapter" data-level="2.1" data-path="2-transformations.html"><a href="2-transformations.html#linear-transformations"><i class="fa fa-check"></i><b>2.1</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-transformations.html"><a href="2-transformations.html#location-scale-transformation"><i class="fa fa-check"></i><b>2.1.1</b> Location-scale transformation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-transformations.html"><a href="2-transformations.html#invertible-location-scale-transformation"><i class="fa fa-check"></i><b>2.1.2</b> Invertible location-scale transformation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-transformations.html"><a href="2-transformations.html#nonlinear-transformations"><i class="fa fa-check"></i><b>2.2</b> Nonlinear transformations</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-transformations.html"><a href="2-transformations.html#general-transformation"><i class="fa fa-check"></i><b>2.2.1</b> General transformation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-transformations.html"><a href="2-transformations.html#linearisation-of-boldsymbol-hboldsymbol-x"><i class="fa fa-check"></i><b>2.2.2</b> Linearisation of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="2-transformations.html"><a href="2-transformations.html#delta-method"><i class="fa fa-check"></i><b>2.2.3</b> Delta method</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-transformations.html"><a href="2-transformations.html#transformation-of-densities-under-general-invertible-transformation"><i class="fa fa-check"></i><b>2.2.4</b> Transformation of densities under general invertible transformation</a></li>
<li class="chapter" data-level="2.2.5" data-path="2-transformations.html"><a href="2-transformations.html#normalising-flows"><i class="fa fa-check"></i><b>2.2.5</b> Normalising flows</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-transformations.html"><a href="2-transformations.html#whitening-transformations"><i class="fa fa-check"></i><b>2.3</b> Whitening transformations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-transformations.html"><a href="2-transformations.html#overview"><i class="fa fa-check"></i><b>2.3.1</b> Overview</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-transformations.html"><a href="2-transformations.html#general-whitening-transformation"><i class="fa fa-check"></i><b>2.3.2</b> General whitening transformation</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-transformations.html"><a href="2-transformations.html#general-solution-of-whitening-constraint-covariance-based"><i class="fa fa-check"></i><b>2.3.3</b> General solution of whitening constraint (covariance-based)</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-transformations.html"><a href="2-transformations.html#another-solution-correlation-based"><i class="fa fa-check"></i><b>2.3.4</b> Another solution (correlation-based)</a></li>
<li class="chapter" data-level="2.3.5" data-path="2-transformations.html"><a href="2-transformations.html#objective-criteria-for-choosing-among-boldsymbol-w-using-boldsymbol-q_1-or-boldsymbol-q_2"><i class="fa fa-check"></i><b>2.3.5</b> Objective criteria for choosing among <span class="math inline">\(\boldsymbol W\)</span> using <span class="math inline">\(\boldsymbol Q_1\)</span> or <span class="math inline">\(\boldsymbol Q_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-transformations.html"><a href="2-transformations.html#natural-whitening-procedures"><i class="fa fa-check"></i><b>2.4</b> Natural whitening procedures</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-transformations.html"><a href="2-transformations.html#zca-whitening"><i class="fa fa-check"></i><b>2.4.1</b> ZCA Whitening</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-transformations.html"><a href="2-transformations.html#zca-cor-whitening"><i class="fa fa-check"></i><b>2.4.2</b> ZCA-Cor Whitening</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-transformations.html"><a href="2-transformations.html#cholesky-whitening"><i class="fa fa-check"></i><b>2.4.3</b> Cholesky Whitening</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-transformations.html"><a href="2-transformations.html#pca-whitening"><i class="fa fa-check"></i><b>2.4.4</b> PCA Whitening</a></li>
<li class="chapter" data-level="2.4.5" data-path="2-transformations.html"><a href="2-transformations.html#pca-cor-whitening"><i class="fa fa-check"></i><b>2.4.5</b> PCA-cor Whitening</a></li>
<li class="chapter" data-level="2.4.6" data-path="2-transformations.html"><a href="2-transformations.html#comparison-of-zca-pca-and-chol-whitening"><i class="fa fa-check"></i><b>2.4.6</b> Comparison of ZCA, PCA and Chol whitening</a></li>
<li class="chapter" data-level="2.4.7" data-path="2-transformations.html"><a href="2-transformations.html#recap"><i class="fa fa-check"></i><b>2.4.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-transformations.html"><a href="2-transformations.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>2.5</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-transformations.html"><a href="2-transformations.html#application-to-data"><i class="fa fa-check"></i><b>2.5.1</b> Application to data</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-transformations.html"><a href="2-transformations.html#pca-correlation-loadings-and-plot"><i class="fa fa-check"></i><b>2.6</b> PCA correlation loadings and plot</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-transformations.html"><a href="2-transformations.html#iris-data-example"><i class="fa fa-check"></i><b>2.6.1</b> Iris data example</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-transformations.html"><a href="2-transformations.html#cca-whitening-canonical-correlation-analysis"><i class="fa fa-check"></i><b>2.7</b> CCA whitening (Canonical Correlation Analysis)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-transformations.html"><a href="2-transformations.html#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal"><i class="fa fa-check"></i><b>2.7.1</b> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</a></li>
<li class="chapter" data-level="2.7.2" data-path="2-transformations.html"><a href="2-transformations.html#related-methods"><i class="fa fa-check"></i><b>2.7.2</b> Related methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-clustering.html"><a href="3-clustering.html"><i class="fa fa-check"></i><b>3</b> Clustering / unsupervised Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="3-clustering.html"><a href="3-clustering.html#overview-of-clustering"><i class="fa fa-check"></i><b>3.1</b> Overview of clustering</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-clustering.html"><a href="3-clustering.html#general-aim"><i class="fa fa-check"></i><b>3.1.1</b> General aim</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-clustering.html"><a href="3-clustering.html#why-is-clustering-difficult"><i class="fa fa-check"></i><b>3.1.2</b> Why is clustering difficult?</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-clustering.html"><a href="3-clustering.html#common-types-of-clustering-methods"><i class="fa fa-check"></i><b>3.1.3</b> Common types of clustering methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-clustering.html"><a href="3-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>3.2</b> Hierarchical clustering</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-clustering.html"><a href="3-clustering.html#tree-like-structures"><i class="fa fa-check"></i><b>3.2.1</b> Tree-like structures</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-clustering.html"><a href="3-clustering.html#agglomerative-hierarchical-clustering-algorithms"><i class="fa fa-check"></i><b>3.2.2</b> Agglomerative hierarchical clustering algorithms</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-clustering.html"><a href="3-clustering.html#application-to-swiss-banknote-data-set"><i class="fa fa-check"></i><b>3.2.3</b> Application to Swiss banknote data set</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-clustering.html"><a href="3-clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>3.3</b> <span class="math inline">\(K\)</span>-means clustering</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-clustering.html"><a href="3-clustering.html#general-aims"><i class="fa fa-check"></i><b>3.3.1</b> General aims</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-clustering.html"><a href="3-clustering.html#algorithm"><i class="fa fa-check"></i><b>3.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-clustering.html"><a href="3-clustering.html#properties"><i class="fa fa-check"></i><b>3.3.3</b> Properties</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>3.3.4</b> Choosing the number of clusters</a></li>
<li class="chapter" data-level="3.3.5" data-path="3-clustering.html"><a href="3-clustering.html#k-medoids-aka-pam"><i class="fa fa-check"></i><b>3.3.5</b> <span class="math inline">\(K\)</span>-medoids aka PAM</a></li>
<li class="chapter" data-level="3.3.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-k-means-to-iris-data"><i class="fa fa-check"></i><b>3.3.6</b> Application of <span class="math inline">\(K\)</span>-means to Iris data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-clustering.html"><a href="3-clustering.html#mixture-models"><i class="fa fa-check"></i><b>3.4</b> Mixture models</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-clustering.html"><a href="3-clustering.html#finite-mixture-model"><i class="fa fa-check"></i><b>3.4.1</b> Finite mixture model</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-clustering.html"><a href="3-clustering.html#decomposition-of-covariance-and-total-variation"><i class="fa fa-check"></i><b>3.4.2</b> Decomposition of covariance and total variation</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-clustering.html"><a href="3-clustering.html#example-of-mixture-of-three-univariate-normal-densities"><i class="fa fa-check"></i><b>3.4.3</b> Example of mixture of three univariate normal densities:</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-clustering.html"><a href="3-clustering.html#example-of-a-mixture-of-two-bivariate-normal-densities"><i class="fa fa-check"></i><b>3.4.4</b> Example of a mixture of two bivariate normal densities</a></li>
<li class="chapter" data-level="3.4.5" data-path="3-clustering.html"><a href="3-clustering.html#sampling-from-a-mixture-model-and-latent-allocation-variable-formulation"><i class="fa fa-check"></i><b>3.4.5</b> Sampling from a mixture model and latent allocation variable formulation</a></li>
<li class="chapter" data-level="3.4.6" data-path="3-clustering.html"><a href="3-clustering.html#predicting-the-group-allocation-of-a-given-sample"><i class="fa fa-check"></i><b>3.4.6</b> Predicting the group allocation of a given sample</a></li>
<li class="chapter" data-level="3.4.7" data-path="3-clustering.html"><a href="3-clustering.html#variation-1-infinite-mixture-model"><i class="fa fa-check"></i><b>3.4.7</b> Variation 1: Infinite mixture model</a></li>
<li class="chapter" data-level="3.4.8" data-path="3-clustering.html"><a href="3-clustering.html#variation-2-semiparametric-mixture-model-with-two-classes"><i class="fa fa-check"></i><b>3.4.8</b> Variation 2: Semiparametric mixture model with two classes</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-clustering.html"><a href="3-clustering.html#fitting-mixture-models-to-data"><i class="fa fa-check"></i><b>3.5</b> Fitting mixture models to data</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-clustering.html"><a href="3-clustering.html#direct-estimation-of-mixture-model-parameters"><i class="fa fa-check"></i><b>3.5.1</b> Direct estimation of mixture model parameters</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-clustering.html"><a href="3-clustering.html#estimate-mixture-model-parameters-using-the-em-algorithm"><i class="fa fa-check"></i><b>3.5.2</b> Estimate mixture model parameters using the EM algorithm</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-clustering.html"><a href="3-clustering.html#em-algorithm-for-multivariate-normal-mixture-model"><i class="fa fa-check"></i><b>3.5.3</b> EM algorithm for multivariate normal mixture model</a></li>
<li class="chapter" data-level="3.5.4" data-path="3-clustering.html"><a href="3-clustering.html#connection-with-k-means-clustering-method"><i class="fa fa-check"></i><b>3.5.4</b> Connection with <span class="math inline">\(K\)</span>-means clustering method</a></li>
<li class="chapter" data-level="3.5.5" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-classes"><i class="fa fa-check"></i><b>3.5.5</b> Choosing the number of classes</a></li>
<li class="chapter" data-level="3.5.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-gmms-to-iris-flower-data"><i class="fa fa-check"></i><b>3.5.6</b> Application of GMMs to Iris flower data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-classification.html"><a href="4-classification.html"><i class="fa fa-check"></i><b>4</b> Classification / supervised learning</a><ul>
<li class="chapter" data-level="4.1" data-path="4-classification.html"><a href="4-classification.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-classification.html"><a href="4-classification.html#supervised-learning-vs.unsupervised-learning"><i class="fa fa-check"></i><b>4.1.1</b> Supervised learning vs.Â unsupervised learning</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-classification.html"><a href="4-classification.html#terminology"><i class="fa fa-check"></i><b>4.1.2</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-classification.html"><a href="4-classification.html#bayesian-discriminant-rule-or-bayes-classifier"><i class="fa fa-check"></i><b>4.2</b> Bayesian discriminant rule or Bayes classifier</a></li>
<li class="chapter" data-level="4.3" data-path="4-classification.html"><a href="4-classification.html#normal-bayes-classifier"><i class="fa fa-check"></i><b>4.3</b> Normal Bayes classifier</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-classification.html"><a href="4-classification.html#quadratic-discriminant-analysis-qda-and-gaussian-assumption"><i class="fa fa-check"></i><b>4.3.1</b> Quadratic discriminant analysis (QDA) and Gaussian assumption</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-classification.html"><a href="4-classification.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>4.3.2</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-classification.html"><a href="4-classification.html#diagonal-discriminant-analysis-dda"><i class="fa fa-check"></i><b>4.3.3</b> Diagonal discriminant analysis (DDA)</a></li>
<li class="chapter" data-level="4.3.4" data-path="4-classification.html"><a href="4-classification.html#comparison-of-decision-boundaries-lda-vs.qda"><i class="fa fa-check"></i><b>4.3.4</b> Comparison of decision boundaries: LDA vs.Â QDA</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-classification.html"><a href="4-classification.html#the-training-step-learning-qda-lda-and-dda-classifiers-from-data"><i class="fa fa-check"></i><b>4.4</b> The training step â learning QDA, LDA and DDA classifiers from data</a></li>
<li class="chapter" data-level="4.5" data-path="4-classification.html"><a href="4-classification.html#goodness-of-fit-and-variable-selection"><i class="fa fa-check"></i><b>4.5</b> Goodness of fit and variable selection</a><ul>
<li class="chapter" data-level="4.5.1" data-path="4-classification.html"><a href="4-classification.html#lda-with-k2-classes"><i class="fa fa-check"></i><b>4.5.1</b> LDA with <span class="math inline">\(K=2\)</span> classes</a></li>
<li class="chapter" data-level="4.5.2" data-path="4-classification.html"><a href="4-classification.html#multiple-classes"><i class="fa fa-check"></i><b>4.5.2</b> Multiple classes</a></li>
<li class="chapter" data-level="4.5.3" data-path="4-classification.html"><a href="4-classification.html#choosing-a-threshold"><i class="fa fa-check"></i><b>4.5.3</b> Choosing a threshold</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4-classification.html"><a href="4-classification.html#estimating-prediction-error"><i class="fa fa-check"></i><b>4.6</b> Estimating prediction error</a><ul>
<li class="chapter" data-level="4.6.1" data-path="4-classification.html"><a href="4-classification.html#quantifying-prediction-error"><i class="fa fa-check"></i><b>4.6.1</b> Quantifying prediction error</a></li>
<li class="chapter" data-level="4.6.2" data-path="4-classification.html"><a href="4-classification.html#estimation-of-prediction-error-without-test-data"><i class="fa fa-check"></i><b>4.6.2</b> Estimation of prediction error without test data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-dependence.html"><a href="5-dependence.html"><i class="fa fa-check"></i><b>5</b> Multivariate dependencies</a><ul>
<li class="chapter" data-level="5.1" data-path="5-dependence.html"><a href="5-dependence.html#measuring-the-association-between-two-sets-of-random-variables"><i class="fa fa-check"></i><b>5.1</b> Measuring the association between two sets of random variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-dependence.html"><a href="5-dependence.html#rozeboom-vector-correlation"><i class="fa fa-check"></i><b>5.1.1</b> Rozeboom vector correlation</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-dependence.html"><a href="5-dependence.html#other-common-approaches"><i class="fa fa-check"></i><b>5.1.2</b> Other common approaches</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-dependence.html"><a href="5-dependence.html#graphical-models"><i class="fa fa-check"></i><b>5.2</b> Graphical models</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-dependence.html"><a href="5-dependence.html#purpose"><i class="fa fa-check"></i><b>5.2.1</b> Purpose</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-dependence.html"><a href="5-dependence.html#basic-notions-from-graph-theory"><i class="fa fa-check"></i><b>5.2.2</b> Basic notions from graph theory</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-dependence.html"><a href="5-dependence.html#probabilistic-graphical-models"><i class="fa fa-check"></i><b>5.2.3</b> Probabilistic graphical models</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-dependence.html"><a href="5-dependence.html#directed-graphical-models"><i class="fa fa-check"></i><b>5.2.4</b> Directed graphical models</a></li>
<li class="chapter" data-level="5.2.5" data-path="5-dependence.html"><a href="5-dependence.html#undirected-graphical-models"><i class="fa fa-check"></i><b>5.2.5</b> Undirected graphical models</a></li>
<li class="chapter" data-level="5.2.6" data-path="5-dependence.html"><a href="5-dependence.html#algorithm-for-learning-ggms"><i class="fa fa-check"></i><b>5.2.6</b> Algorithm for learning GGMs</a></li>
<li class="chapter" data-level="5.2.7" data-path="5-dependence.html"><a href="5-dependence.html#example-exam-score-data-mardia-et-al-1979"><i class="fa fa-check"></i><b>5.2.7</b> Example: exam score data (Mardia et al 1979:)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-nonlinear.html"><a href="6-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Nonlinear and nonparametric models</a><ul>
<li class="chapter" data-level="6.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#limits-of-linear-models-and-correlation"><i class="fa fa-check"></i><b>6.1</b> Limits of linear models and correlation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#correlation-only-measures-linear-dependence"><i class="fa fa-check"></i><b>6.1.1</b> Correlation only measures linear dependence</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#anscombe-data-sets"><i class="fa fa-check"></i><b>6.1.2</b> Anscombe data sets</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#mutual-information-as-generalised-correlation"><i class="fa fa-check"></i><b>6.2</b> Mutual information as generalised correlation</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#definition-of-mutual-information"><i class="fa fa-check"></i><b>6.2.1</b> Definition of mutual information</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#mutual-information-between-two-normal-variables"><i class="fa fa-check"></i><b>6.2.2</b> Mutual information between two normal variables</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#mutual-information-between-two-normally-distributed-random-vectors"><i class="fa fa-check"></i><b>6.2.3</b> Mutual information between two normally distributed random vectors</a></li>
<li class="chapter" data-level="6.2.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#using-mi-for-variable-selection"><i class="fa fa-check"></i><b>6.2.4</b> Using MI for variable selection</a></li>
<li class="chapter" data-level="6.2.5" data-path="6-nonlinear.html"><a href="6-nonlinear.html#other-measures-of-general-dependence"><i class="fa fa-check"></i><b>6.2.5</b> Other measures of general dependence</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#nonlinear-regression-models"><i class="fa fa-check"></i><b>6.3</b> Nonlinear regression models</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#scatterplot-smoothing"><i class="fa fa-check"></i><b>6.3.1</b> Scatterplot smoothing</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#polynomial-regression-model"><i class="fa fa-check"></i><b>6.3.2</b> Polynomial regression model</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#piecewise-polyomial-regression"><i class="fa fa-check"></i><b>6.3.3</b> Piecewise polyomial regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests"><i class="fa fa-check"></i><b>6.4</b> Random forests</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#stochastic-vs.algorithmic-models"><i class="fa fa-check"></i><b>6.4.1</b> Stochastic vs.Â algorithmic models</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests-1"><i class="fa fa-check"></i><b>6.4.2</b> Random forests</a></li>
<li class="chapter" data-level="6.4.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#comparison-of-decision-boundaries-decision-tree-vs.random-forest"><i class="fa fa-check"></i><b>6.4.3</b> Comparison of decision boundaries: decision tree vs.Â random forest</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gaussian-processes"><i class="fa fa-check"></i><b>6.5</b> Gaussian processes</a><ul>
<li class="chapter" data-level="6.5.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#main-concepts"><i class="fa fa-check"></i><b>6.5.1</b> Main concepts</a></li>
<li class="chapter" data-level="6.5.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#technical-background"><i class="fa fa-check"></i><b>6.5.2</b> Technical background:</a></li>
<li class="chapter" data-level="6.5.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#covariance-functions-and-kernel"><i class="fa fa-check"></i><b>6.5.3</b> Covariance functions and kernel</a></li>
<li class="chapter" data-level="6.5.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gp-model"><i class="fa fa-check"></i><b>6.5.4</b> GP model</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks"><i class="fa fa-check"></i><b>6.6</b> Neural networks</a><ul>
<li class="chapter" data-level="6.6.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#history"><i class="fa fa-check"></i><b>6.6.1</b> History</a></li>
<li class="chapter" data-level="6.6.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks-1"><i class="fa fa-check"></i><b>6.6.2</b> Neural networks</a></li>
<li class="chapter" data-level="6.6.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#learning-more-about-deep-learning"><i class="fa fa-check"></i><b>6.6.3</b> Learning more about deep learning</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="7-matrices.html"><a href="7-matrices.html"><i class="fa fa-check"></i><b>A</b> Brief refresher on matrices</a><ul>
<li class="chapter" data-level="A.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-notation"><i class="fa fa-check"></i><b>A.1</b> Matrix notation</a></li>
<li class="chapter" data-level="A.2" data-path="7-matrices.html"><a href="7-matrices.html#simple-special-matrices"><i class="fa fa-check"></i><b>A.2</b> Simple special matrices</a></li>
<li class="chapter" data-level="A.3" data-path="7-matrices.html"><a href="7-matrices.html#simple-matrix-operations"><i class="fa fa-check"></i><b>A.3</b> Simple matrix operations</a></li>
<li class="chapter" data-level="A.4" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.4</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="A.5" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.5</b> Eigenvalues and eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.6" data-path="7-matrices.html"><a href="7-matrices.html#singular-value-decomposition"><i class="fa fa-check"></i><b>A.6</b> Singular value decomposition</a></li>
<li class="chapter" data-level="A.7" data-path="7-matrices.html"><a href="7-matrices.html#positive-semi-definiteness-rank-condition"><i class="fa fa-check"></i><b>A.7</b> Positive (semi-)definiteness, rank, condition</a></li>
<li class="chapter" data-level="A.8" data-path="7-matrices.html"><a href="7-matrices.html#trace-and-determinant-of-a-matrix-and-eigenvalues"><i class="fa fa-check"></i><b>A.8</b> Trace and determinant of a matrix and eigenvalues</a></li>
<li class="chapter" data-level="A.9" data-path="7-matrices.html"><a href="7-matrices.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.9</b> Functions of matrices</a></li>
<li class="chapter" data-level="A.10" data-path="7-matrices.html"><a href="7-matrices.html#matrix-calculus"><i class="fa fa-check"></i><b>A.10</b> Matrix calculus</a><ul>
<li class="chapter" data-level="A.10.1" data-path="7-matrices.html"><a href="7-matrices.html#first-order-vector-derivatives"><i class="fa fa-check"></i><b>A.10.1</b> First order vector derivatives</a></li>
<li class="chapter" data-level="A.10.2" data-path="7-matrices.html"><a href="7-matrices.html#second-order-vector-derivatives"><i class="fa fa-check"></i><b>A.10.2</b> Second order vector derivatives</a></li>
<li class="chapter" data-level="A.10.3" data-path="7-matrices.html"><a href="7-matrices.html#first-order-matrix-derivatives"><i class="fa fa-check"></i><b>A.10.3</b> First order matrix derivatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="8-further-study.html"><a href="8-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="8-further-study.html"><a href="8-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="8-further-study.html"><a href="8-further-study.html#advanced-reading"><i class="fa fa-check"></i><b>B.2</b> Advanced reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="9-references.html"><a href="9-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics and Machine Learning MATH38161</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="transformations-and-dimension-reduction" class="section level1">
<h1><span class="header-section-number">2</span> Transformations and dimension reduction</h1>
<p>Motivation:
In the following we study transformations of random vectors and their distributions.
These transformation are very important
since they either transform simple distributions into more complex distributions or allow to simplify
complex models. In machine learning invertible mappings of transformations
for probability distributions are known as ânormalising flowsâ (these play a key role
e.g.Â in neural networks).</p>
<div id="linear-transformations" class="section level2">
<h2><span class="header-section-number">2.1</span> Linear Transformations</h2>
<div id="location-scale-transformation" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Location-scale transformation</h3>
<p>Also known as affine transformation.</p>
<p><span class="math display">\[\boldsymbol y= \underbrace{\boldsymbol a}_{\text{location parameter}}+\underbrace{\boldsymbol B}_{\text{scale parameter}} \boldsymbol x\space\]</span>
<span class="math display">\[\boldsymbol y: m \times 1 \text{ random vector}\]</span>
<span class="math display">\[\boldsymbol a: m \times 1 \text{ vector, location parameter}\]</span>
<span class="math display">\[\boldsymbol B: m \times d \text{ matrix, scale parameter },  m \geq 1\]</span>
<span class="math display">\[\boldsymbol x: d \times 1 \text{ random vector}\]</span></p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\text{E}(\boldsymbol x)=\boldsymbol \mu\\
\text{Var}(\boldsymbol x)=\boldsymbol \Sigma\\
\end{array}
\Longrightarrow
\begin{array}{ll}
\text{E}(\boldsymbol y)=\boldsymbol a+ \boldsymbol B\boldsymbol \mu\\
\text{Var}(\boldsymbol y)= \boldsymbol B\boldsymbol \Sigma\boldsymbol B^T \\
\end{array}
\end{align*}\]</span></p>
<p>Special cases/examples:</p>
<ol style="list-style-type: decimal">
<li>Univariate case (<span class="math inline">\(d=1, m=1\)</span>)
<ul>
<li><span class="math inline">\(\text{E}(y)=a+b\mu\)</span></li>
<li><span class="math inline">\(\text{Var}(y)=b^2\sigma^2\)</span></li>
</ul></li>
<li>Sum of two random univariate variables<br />
<span class="math inline">\(y = x_1 + x_2\)</span>, i.e. <span class="math inline">\(a=0\)</span> and <span class="math inline">\(\boldsymbol B=(1,1)\)</span>
<ul>
<li><span class="math inline">\(\text{E}(x_1+x_2)=\mu_1+\mu_2\)</span><br />
</li>
<li><span class="math inline">\(\text{Var}(x_1+x_2) = (1,1)\begin{pmatrix}  \sigma^2_1 &amp; \sigma_{12}\\  \sigma_{21} &amp; \sigma^2_2  \end{pmatrix} \begin{pmatrix}  1\\  1  \end{pmatrix} = \sigma^2_1+\sigma^2_2+2\sigma_{12} = \text{Var}(x_1)+\text{Var}(x_2)+2\text{Cov}(x_1,x_2)\)</span></li>
</ul></li>
<li><span class="math inline">\(y_1=a_1+b_1 x_1\)</span> and <span class="math inline">\(y_2=a_2+b_2 x_2\)</span>, i.e. <span class="math inline">\(\boldsymbol a= \begin{pmatrix} a_1\\ a_2 \end{pmatrix}\)</span> and <span class="math inline">\(\boldsymbol B= \begin{pmatrix}b_1 &amp; 0\\ 0 &amp; b_2\end{pmatrix}\)</span>
<ul>
<li><span class="math inline">\(\text{E}(\boldsymbol y)=\begin{pmatrix} a_1+b_1 \mu_1\\ a_2+b_2 \mu_2 \end{pmatrix}\)</span><br />
</li>
<li><span class="math inline">\(\text{Var}(\boldsymbol y) = \begin{pmatrix} b_1 &amp; 0\\ 0 &amp; b_2 \end{pmatrix} \begin{pmatrix}  \sigma^2_1 &amp; \sigma_{12}\\  \sigma_{21} &amp; \sigma^2_2  \end{pmatrix} \begin{pmatrix} b_1 &amp; 0\\ 0 &amp; b_2 \end{pmatrix} = \begin{pmatrix}  b^2_1\sigma^2_1 &amp; b_1b_2\sigma_{12}\\  b_1b_2\sigma_{21} &amp; b^2_2\sigma^2_2  \end{pmatrix}\)</span><br />
i.e. <span class="math inline">\(\text{Cov}(a_1+b_1 x_1,a_2+b_2 x_2) = b_1 b_2\text{Cov}(x_1,x_2)\)</span></li>
</ul></li>
</ol>
</div>
<div id="invertible-location-scale-transformation" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Invertible location-scale transformation</h3>
<p>If <span class="math inline">\(m=d\)</span> and <span class="math inline">\(\det(\boldsymbol B) \neq 0\)</span> then we get an <strong>invertible</strong> transformation:
<span class="math display">\[\boldsymbol y= \boldsymbol a+ \boldsymbol B\boldsymbol x\]</span>
<span class="math display">\[\boldsymbol x= \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\]</span></p>
<p>Transformation of density:
<span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> with density <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span></p>
<p><span class="math inline">\(\Longrightarrow\)</span> <span class="math inline">\(\boldsymbol y\sim F_{\boldsymbol y}\)</span> with density
<span class="math display">\[ f_{\boldsymbol y}(\boldsymbol y)=|\det(\boldsymbol B)|^{-1} f_{\boldsymbol x} \left( \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\right)\]</span></p>

<div class="example">
<span id="exm:coltrans" class="example"><strong>Example 2.1  </strong></span><strong>coloring transformation</strong><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> <span class="math inline">\(\boldsymbol y= \boldsymbol \mu+\boldsymbol \Sigma^{1/2} \boldsymbol x\)</span>
</div>

<p><span class="math display">\[\boldsymbol a=\boldsymbol \mu\]</span>
<span class="math display">\[\boldsymbol B=\boldsymbol \Sigma^{1/2}\]</span>
where <span class="math inline">\(\boldsymbol \Sigma^{1/2} = \boldsymbol U\boldsymbol \Lambda^{1/2} \boldsymbol U^T\)</span> is the principal matrix square root obtained by eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span>.
Note that <span class="math inline">\(\boldsymbol \Sigma^{1/2}\)</span> is unique, symmetric, and positive definite (not
just positive semi-definite because this is an invertible transformation).</p>
<p><span class="math display">\[\text{E}(\boldsymbol x)=\boldsymbol 0\text{ and } \text{Var}(\boldsymbol x)=\boldsymbol I_d\]</span>
<span class="math display">\[\Longrightarrow\text{E}(\boldsymbol y) = \boldsymbol \mu\text{ and } \text{Var}(\boldsymbol y) = \boldsymbol \Sigma\]</span>
Assume <span class="math inline">\(\boldsymbol x\)</span> is multivariate standard normal <span class="math inline">\(\boldsymbol x\sim N_d(\boldsymbol 0,\boldsymbol I_d)\)</span> with density
<span class="math display">\[f_{\boldsymbol x}(\boldsymbol x) = (2\pi)^{-d/2}\exp\left( -\frac{1}{2} \boldsymbol x^T \boldsymbol x\right)\]</span>
Then the density after applying this coloring transform is</p>
<p><span class="math display">\[f_{\boldsymbol y}(\boldsymbol y) = |\det(\boldsymbol \Sigma^{1/2})|^{-1} (2\pi)^{-d/2} \exp\left(-\frac{1}{2}(\boldsymbol y-\boldsymbol \mu)^T\boldsymbol \Sigma^{-1/2} \,\boldsymbol \Sigma^{-1/2}(\boldsymbol y-\boldsymbol \mu)\right)\]</span></p>
<p><span class="math display">\[= (2\pi)^{-d/2} \det(\boldsymbol \Sigma)^{-1/2} \exp\left(-\frac{1}{2}(\boldsymbol y-\boldsymbol \mu)^T\boldsymbol \Sigma^{-1}(\boldsymbol y-\boldsymbol \mu)\right)\]</span>
<span class="math inline">\(\Longrightarrow\)</span> <span class="math inline">\(\boldsymbol y\)</span> has multivariate normal density!!</p>
<p><em>Application:</em> e.g.Â random number generation: draw from <span class="math inline">\(N_d(\boldsymbol 0,\boldsymbol I_d)\)</span> (easy!) then convert to multivariate normal by coloring transformation <span class="math inline">\(\boldsymbol x\rightarrow \boldsymbol y\)</span></p>

<div class="example">
<span id="exm:mahatrans" class="example"><strong>Example 2.2  </strong></span><strong>Mahalanobis transform</strong> <span class="math inline">\(\boldsymbol y=\boldsymbol \Sigma^{-1/2}(\boldsymbol x-\boldsymbol \mu)\)</span>
</div>

<p>This is the <strong>inverse</strong> of the above coloring transformation</p>
<p><span class="math display">\[\boldsymbol a= - \boldsymbol \Sigma^{-1/2} \boldsymbol \mu\]</span>
<span class="math display">\[\boldsymbol B= \boldsymbol \Sigma^{-1/2}\]</span>
<span class="math display">\[\text{E}(\boldsymbol x)=\boldsymbol \mu\text{ and } \text{Var}(\boldsymbol x)=\boldsymbol \Sigma\]</span>
<span class="math display">\[\Longrightarrow\text{E}(\boldsymbol y) = \boldsymbol 0\text{ and } \text{Var}(\boldsymbol y) = \boldsymbol I_d\]</span>
Mahalanobis transformation performs three functions:</p>
<ol style="list-style-type: decimal">
<li>Centering (<span class="math inline">\(-\boldsymbol \mu\)</span>)</li>
<li>Standardisation <span class="math inline">\(\text{Var}(y_i)=1\)</span></li>
<li>Decorrelation <span class="math inline">\(\text{Cor}(y_i,y_j)=0\)</span> for <span class="math inline">\(i \neq j\)</span></li>
</ol>
<p><strong>Univariate case (<span class="math inline">\(d=1\)</span>)</strong></p>
<p><span class="math display">\[y = \frac{x-\mu}{\sigma}\]</span>
As there is no correlation for <span class="math inline">\(d=1\)</span>, itâs standardisation + centering.</p>
<p><strong>Mahalanobis transformation</strong> appears implicitly in many places in multivariate statistics,
e.g.Â in the multivariate normal density.</p>
</div>
</div>
<div id="nonlinear-transformations" class="section level2">
<h2><span class="header-section-number">2.2</span> Nonlinear transformations</h2>
<div id="general-transformation" class="section level3">
<h3><span class="header-section-number">2.2.1</span> General transformation</h3>
<p><span class="math display">\[\boldsymbol y= \boldsymbol h(\boldsymbol x)\]</span>
with <span class="math inline">\(\boldsymbol h\)</span> an arbitrary vector-valued function</p>
<ul>
<li>linear case: <span class="math inline">\(\boldsymbol h(\boldsymbol x) = \boldsymbol a+\boldsymbol B\boldsymbol x\)</span></li>
</ul>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\text{E}(\boldsymbol y)=? \\
\text{Var}(\boldsymbol y)=? \\
\end{array}
\end{align*}\]</span></p>
<p>For a general transformation <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> the mean and variance of the transformed variable cannot be
easily or analytically calculated.</p>
<p>However, we can find a <strong>linear approximation</strong> and then compute the mean and variance.
This is called the âDelta Methodâ.</p>
</div>
<div id="linearisation-of-boldsymbol-hboldsymbol-x" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Linearisation of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span></h3>
<p>Taylor series approximation (first order) of <span class="math inline">\(\boldsymbol h(\boldsymbol x\)</span> around <span class="math inline">\(\boldsymbol \mu\)</span>:
<span class="math display">\[\boldsymbol h(\boldsymbol x) \approx \boldsymbol h(\boldsymbol \mu) + \underbrace{\boldsymbol J_{\boldsymbol h}(\boldsymbol \mu)}_{\text{Jacobi matrix}}(\boldsymbol x-\boldsymbol \mu)\]</span></p>
<p><span class="math inline">\(\nabla\)</span>, the nabla operator, is the row vector <span class="math inline">\((\frac{\partial}{\partial x_1},...,\frac{\partial}{\partial x_d})\)</span>, which when applied to univariate <span class="math inline">\(h\)</span> gives the gradient:</p>
<p><span class="math display">\[\nabla h(\boldsymbol x) = \left(\frac{\partial h}{\partial x_1},...,\frac{\partial h}{\partial x_d}\right)\]</span></p>
<p>The <strong>Jacobi matrix</strong> is the <strong>generalisation of gradient</strong> if <span class="math inline">\(\boldsymbol h\)</span> is vector-valued:</p>
<p><span class="math display">\[\boldsymbol J_{\boldsymbol h}(\boldsymbol x) = \begin{pmatrix}\nabla h_1(\boldsymbol x)\\ \nabla h_2(\boldsymbol x) \\ \vdots \\ \nabla h_m(\boldsymbol x) \end{pmatrix} = \begin{pmatrix}
    \frac{\partial h_1}{\partial x_1} &amp; \dots &amp; \frac{\partial h_1}{\partial x_d}\\
    \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial h_m}{\partial x_1} &amp; \dots &amp; \frac{\partial h_m}{\partial x_d}
    \end{pmatrix}\]</span></p>
</div>
<div id="delta-method" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Delta method</h3>
<p><span class="math display">\[\boldsymbol y= \boldsymbol h(\boldsymbol x) \approx  \boldsymbol a+ \boldsymbol B\boldsymbol x\]</span>
with
<span class="math inline">\(\boldsymbol a= \boldsymbol h(\boldsymbol \mu) -\boldsymbol \mu\boldsymbol J_{\boldsymbol h}(\boldsymbol \mu)\)</span> and
<span class="math inline">\(\boldsymbol B= \boldsymbol J_{\boldsymbol h}(\boldsymbol \mu)\)</span></p>
<p><span class="math inline">\(\Longrightarrow\)</span>
<span class="math display">\[\text{E}(\boldsymbol y)\approx\boldsymbol h(\boldsymbol \mu)\]</span>
<span class="math display">\[\text{Var}(\boldsymbol y)\approx \boldsymbol J_{\boldsymbol h}(\boldsymbol \mu) \, \boldsymbol \Sigma\, \boldsymbol J_{\boldsymbol h}(\boldsymbol \mu)^T\]</span>
Special case: univariate Delta method
<span class="math display">\[\text{E}(y)= h(\mu)\]</span>
<span class="math display">\[\text{Var}(y)= \sigma^2 h&#39;(\mu)^2\]</span></p>
</div>
<div id="transformation-of-densities-under-general-invertible-transformation" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Transformation of densities under general invertible transformation</h3>
<p>Assume <span class="math inline">\(\boldsymbol h(\boldsymbol x) = \boldsymbol y(\boldsymbol x)\)</span> is invertible: <span class="math inline">\(\boldsymbol h^{-1}(\boldsymbol y)=\boldsymbol x(\boldsymbol y)\)</span></p>
<p><span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> with probability density function <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span></p>
<p>The density <span class="math inline">\(f_{\boldsymbol y}(\boldsymbol y)\)</span> of the transformed random vector <span class="math inline">\(\boldsymbol y\)</span> is then given by<br />
<span class="math display">\[f_{\boldsymbol y}(\boldsymbol y) = |\det\left( \boldsymbol J_{\boldsymbol x}(\boldsymbol y) \right)| \,\,\,  f_{\boldsymbol x}\left( \boldsymbol x(\boldsymbol y) \right)\]</span></p>
<p>where <span class="math inline">\(\boldsymbol J_{\boldsymbol x}(\boldsymbol y)\)</span> is the Jacobi matrix of the inverse transformation</p>
<p>Special cases:</p>
<ul>
<li>Univariate version: <span class="math inline">\(f_y(y) = |\frac{dx}{dy}| \, f_x\left(x(y)\right)\)</span></li>
<li>Linear transformation <span class="math inline">\(\boldsymbol h(\boldsymbol x) = \boldsymbol a+ \boldsymbol B\boldsymbol x\)</span>, with <span class="math inline">\(\boldsymbol x(\boldsymbol y) = \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\)</span>
and <span class="math inline">\(\boldsymbol J_{\boldsymbol x}(\boldsymbol y) = \boldsymbol B^{-1}\)</span>:<br />
<span class="math inline">\(f_{\boldsymbol y}(\boldsymbol y)=|\det(\boldsymbol B)|^{-1} f_{\boldsymbol x} \left( \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\right)\)</span></li>
</ul>
</div>
<div id="normalising-flows" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Normalising flows</h3>
<p>In machine learning (sequences of) invertible nonlinear transformations are known as ânormalising flowsâ. They are used both in a generative way (building complex models from
simple models) and also in a simplification and dimension reduction context.</p>
<p>In this module we will focus mostly on linear transformations as these underpin
much of classical multivariate statistics, but it is important to keep in mind for later study
the importance of nonlinear transformations âsee, e.g, the review paper by Kobyzev et al. âNormalizing Flows: Introduction and Ideasâ, available from <a href="https://arxiv.org/abs/1908.09257" class="uri">https://arxiv.org/abs/1908.09257</a> .</p>
</div>
</div>
<div id="whitening-transformations" class="section level2">
<h2><span class="header-section-number">2.3</span> Whitening transformations</h2>
<div id="overview" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Overview</h3>
<p>The <em>Mahalanobis</em> transform (also know as ZCA transform in machine learning) is a specific example of a <strong>whitening transformation</strong>. These constitute an important and widely used class of invertible location-scale transformations.</p>
<p><em>Terminology:</em> whitening refers to the fact that after the transformation the covariance matrix is spherical, isotrop, white (<span class="math inline">\(\boldsymbol I_d\)</span>)</p>
<p>Whitening is <strong>useful in preprocessing</strong>, to <strong>turn multivariate problems into simple univariate models</strong> and some <strong>reduce the dimension in an optimal way</strong>.</p>
<p>In so-called latent variable models whitening procedures link observed and latent variables (which usually are uncorrelated and standardised random variables):</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cl}
\text{Whitening} \\
\downarrow
\end{array}
\begin{array}{ll}
\boldsymbol x\\
\uparrow \\
\boldsymbol z\\
\end{array}
\begin{array}{ll}
\text{Observed external variable (can be measured), typically correlated} \\
\space \\
\text{Unobserved &quot;latent&quot; variable internal, typically not correlated} \\
\end{array}
\end{align*}\]</span></p>
</div>
<div id="general-whitening-transformation" class="section level3">
<h3><span class="header-section-number">2.3.2</span> General whitening transformation</h3>
<p><span class="math display">\[\boldsymbol x\text{ random vector} \sim F_{\boldsymbol x} \text{ (not necessarily from multivariate normal)}\]</span></p>
<p><span class="math display">\[\underbrace{\boldsymbol z}_{d \times 1 \text{ vector }} = \underbrace{\boldsymbol W}_{d \times d \text{ whitening matrix }} \underbrace{\boldsymbol x}_{d \times 1 \text{ vector }}\]</span>
<strong>Objective</strong>: choose <span class="math inline">\(\boldsymbol W\)</span> so that <span class="math inline">\(\text{Var}(\boldsymbol z)=\boldsymbol I_d\)</span></p>
<p>Note: we do not care about <span class="math inline">\(\text{E}(\boldsymbol z)\)</span> since we can always centre!</p>
<p>For Mahalanobis/ZCA whitening we already know that <span class="math inline">\(\boldsymbol W^{\text{ZCA}}=\boldsymbol \Sigma^{-1/2}\)</span>.</p>
<p>In general, <span class="math inline">\(\boldsymbol W\)</span> needs to satisfy a constraint:
<span class="math display">\[
\begin{array}{lll}
                &amp; \text{Var}(\boldsymbol z) &amp; = \boldsymbol I_d \\
\Longrightarrow &amp; \text{Var}(\boldsymbol W\boldsymbol x) &amp;= \boldsymbol W\boldsymbol \Sigma\boldsymbol W^T = \boldsymbol I_d \\
\Longrightarrow &amp;  \boldsymbol W\, \boldsymbol \Sigma\, \boldsymbol W^T \boldsymbol W= \boldsymbol W&amp; \\
\end{array}
\]</span>
<span class="math display">\[\Longrightarrow \text{constraint on whitening matrix: } \boldsymbol W^T \boldsymbol W= \boldsymbol \Sigma^{-1}\]</span></p>
<p>Clearly, the ZCA whitening matrix satisfies this constraint: <span class="math inline">\((\boldsymbol W^{ZCA})^T \boldsymbol W^{ZCA} = \boldsymbol \Sigma^{-1/2}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1}\)</span></p>
</div>
<div id="general-solution-of-whitening-constraint-covariance-based" class="section level3">
<h3><span class="header-section-number">2.3.3</span> General solution of whitening constraint (covariance-based)</h3>
<p><span class="math display">\[\boldsymbol W= \boldsymbol Q_1 \boldsymbol \Sigma^{-1/2}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol Q_1\)</span> is a rotation (orthogonal) matrix (with properties <span class="math inline">\(\boldsymbol Q^T \boldsymbol Q= \boldsymbol Q\boldsymbol Q^T = \boldsymbol I\)</span> and <span class="math inline">\(\boldsymbol Q^T = \boldsymbol Q^{-1}\)</span>).</p>
<p>Easy to see that this <span class="math inline">\(\boldsymbol W\)</span> satisfies the whitening constraint:</p>
<p><span class="math display">\[\boldsymbol W^T \boldsymbol W= \boldsymbol \Sigma^{-1/2}\underbrace{\boldsymbol Q_1^T \boldsymbol Q_1}_{\boldsymbol I}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1}\]</span></p>
<p><span class="math inline">\(\Longrightarrow\)</span> instead of choosing <span class="math inline">\(\boldsymbol W\)</span>, <strong>we choose the rotation matrix</strong> <span class="math inline">\(\boldsymbol Q_1\)</span>!</p>
<ul>
<li>it is now clear that there are infinitely many whitening procedures, because there are infinitely many rotations! This also means we need to find ways to choose/select among whitening procedures.</li>
<li>for the Mahalanobis/ZCA transformation <span class="math inline">\(\boldsymbol Q_1^{\text{ZCA}}=\boldsymbol I\)</span></li>
<li><strong>whitening</strong> can be interpreted as <strong>Mahalanobis transform</strong> followed by <strong>rotation</strong></li>
</ul>
</div>
<div id="another-solution-correlation-based" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Another solution (correlation-based)</h3>
<p>Instead of working with <span class="math inline">\(\boldsymbol \Sigma\)</span>, we can express <span class="math inline">\(\boldsymbol W\)</span> also in terms of correlation matrix <span class="math inline">\(\boldsymbol P= (\rho_{ij})\)</span>.
<span class="math display">\[\boldsymbol W= \boldsymbol Q_2 \boldsymbol P^{-1/2} \boldsymbol V^{-1/2}\]</span>
where <span class="math inline">\(\boldsymbol V^{1/2}\)</span> is the diagonal matrix containing the variances.</p>
<p>It is easy to verify that this <span class="math inline">\(\boldsymbol W\)</span> also satisfies the whitening constraint:
<span class="math display">\[
\begin{array}{ll}
\boldsymbol W^T \boldsymbol W&amp; = \boldsymbol V^{-1/2}\boldsymbol P^{-1/2}\underbrace{\boldsymbol Q_2^T \boldsymbol Q_2}_{\boldsymbol I}\boldsymbol P^{-1/2} \boldsymbol V^{-1/2} \\
&amp; = \boldsymbol V^{-1/2} \boldsymbol P^{-1} \boldsymbol V^{-1/2} = \boldsymbol \Sigma^{-1} \\
\end{array}
\]</span></p>
<ul>
<li>for Mahalanobis/ZCA transformation <span class="math inline">\(\boldsymbol Q_2^{\text{ZCA}} = \boldsymbol \Sigma^{-1/2} \boldsymbol V^{1/2} \boldsymbol P^{1/2}\)</span></li>
<li><strong>Another interpretation of whitening</strong>: first <strong>standardising</strong> (<span class="math inline">\(\boldsymbol V^{-1/2}\)</span>), then <strong>decorrelation</strong> (<span class="math inline">\(\boldsymbol P^{-1/2}\)</span>), followed by <strong>rotation</strong> (<span class="math inline">\(\boldsymbol Q_2\)</span>)</li>
</ul>
<p><strong>Both forms to write <span class="math inline">\(\boldsymbol W\)</span> are equally valid (and interchangeable).</strong></p>
<p>Note that for the same <span class="math inline">\(\boldsymbol W\)</span>
<span class="math display">\[\boldsymbol Q_1\neq\boldsymbol Q_2 \text{  Two different rotation matrices!}\]</span>
and also
<span class="math display">\[\underbrace{\boldsymbol \Sigma^{-1/2}}_{\text{Symmetric}}\neq\underbrace{\boldsymbol P^{-1/2}\boldsymbol V^{-1/2}}_{\text{Not Symmetric}}\]</span>
even though<br />
<span class="math display">\[\boldsymbol \Sigma^{-1/2}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1} = \boldsymbol V^{-1/2}\boldsymbol P^{-1/2}\boldsymbol P^{-1/2}\boldsymbol V^{-1/2}\]</span></p>
</div>
<div id="objective-criteria-for-choosing-among-boldsymbol-w-using-boldsymbol-q_1-or-boldsymbol-q_2" class="section level3">
<h3><span class="header-section-number">2.3.5</span> Objective criteria for choosing among <span class="math inline">\(\boldsymbol W\)</span> using <span class="math inline">\(\boldsymbol Q_1\)</span> or <span class="math inline">\(\boldsymbol Q_2\)</span></h3>
<ol style="list-style-type: decimal">
<li><strong>Cross-covariance</strong> <span class="math inline">\(\boldsymbol \Phi= \Sigma_{\boldsymbol z,\boldsymbol x}\)</span> between <span class="math inline">\(\boldsymbol z\)</span> and <span class="math inline">\(\boldsymbol x\)</span><br />
<span class="math display">\[
\begin{array}{ll}
\boldsymbol \Phi= \text{Cov}(\boldsymbol z,\boldsymbol x) &amp; = \text{Cov}(\boldsymbol W\boldsymbol x,\boldsymbol x) = \boldsymbol W\boldsymbol \Sigma\\
&amp;= \boldsymbol Q_1 \boldsymbol \Sigma^{-1/2} \boldsymbol \Sigma= \boldsymbol Q_1\boldsymbol \Sigma^{1/2}
\end{array}
\]</span></li>
</ol>
<ul>
<li><strong>Cross-covariance linked with</strong> <span class="math inline">\(\boldsymbol Q_1\)</span>!</li>
<li>Choosing suitable cross-covariance allows the selection of a âgoodâ <span class="math inline">\(\boldsymbol Q_1\)</span> (and hence <span class="math inline">\(\boldsymbol W\)</span>)</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Cross-correlation</strong> <span class="math inline">\(\boldsymbol \Psi= \boldsymbol P_{\boldsymbol z,\boldsymbol x}\)</span> between <span class="math inline">\(\boldsymbol z\)</span> and <span class="math inline">\(\boldsymbol x\)</span><br />
<span class="math display">\[
\begin{array}{ll}
\boldsymbol \Psi= \text{Cor}(\boldsymbol z,\boldsymbol x) &amp; = \boldsymbol \Phi\boldsymbol V^{-1/2} = \boldsymbol W\boldsymbol \Sigma\boldsymbol V^{-1/2}\\
&amp;=\boldsymbol Q_2 \boldsymbol P^{-1/2} \boldsymbol V^{-1/2} \boldsymbol \Sigma\boldsymbol V^{-1/2} =  \boldsymbol Q_2\boldsymbol P^{1/2}
\end{array}
\]</span></li>
</ol>
<ul>
<li><strong>Cross-correlation linked with</strong> <span class="math inline">\(\boldsymbol Q_2\)</span>!</li>
<li>Choosing suitable cross-correlation allows the selection of a âgoodâ <span class="math inline">\(\boldsymbol Q_2\)</span> (and hence <span class="math inline">\(\boldsymbol W\)</span>)</li>
</ul>
<p>Properties:
<span class="math display">\[\boldsymbol \Psi= (\psi_{ij})\]</span>
<span class="math display">\[\boldsymbol \Psi^T \boldsymbol \Psi= \boldsymbol P\]</span>
<span class="math inline">\(\Longrightarrow \text{Diag}(\boldsymbol \Psi^T \boldsymbol \Psi) = (1,1, \ldots, 1)\)</span> and <span class="math inline">\(\text{Tr}(\boldsymbol \Psi^T \boldsymbol \Psi)=d\)</span></p>
<p><span class="math display">\[\Longrightarrow \sum_{i=1}^d \psi_{ij}^2 = 1\]</span>
i.e.Â the <em>column</em> sums of <span class="math inline">\(\psi_{ij}^2\)</span> are equal to 1.</p>
<p><strong>Interpretation:</strong> this is the quared multiple correlation coefficient <span class="math inline">\(R^2=1\)</span> between <span class="math inline">\(z_1, ..., z_d\)</span> and <span class="math inline">\(x_j\)</span>!<br />
(as the <span class="math inline">\(z_i\)</span> are all uncorrelated you can simply sum the squared correlations to get <span class="math inline">\(R^2\)</span>; it is equal to 1 because whitening is an invertible transformation)</p>
<p>The <span class="math inline">\(R^2\)</span>-s going from <span class="math inline">\(x_1, \ldots, x_d\)</span> to the <span class="math inline">\(z_i\)</span> also equal 1, but because the <span class="math inline">\(x_i\)</span> are correlated they need to be computed as <span class="math inline">\(\text{Diag}(\boldsymbol \Psi\boldsymbol P^{-1} \boldsymbol \Psi^T) = \text{Diag}(\boldsymbol P_{\boldsymbol z,\boldsymbol x} \boldsymbol P^{-1} \boldsymbol P_{\boldsymbol x,\boldsymbol z}) = (1,1, \ldots, 1)\)</span>.</p>
</div>
</div>
<div id="natural-whitening-procedures" class="section level2">
<h2><span class="header-section-number">2.4</span> Natural whitening procedures</h2>
<p>Now we discuss several strategies (maximise correlation between individual components, maximise compression, etc.) to arrive at optimal whitening transformation.</p>
<p>This leads to the following ânaturalâ whitening transformations:</p>
<ul>
<li><strong>Mahalanobis</strong> whitening (also known as <strong>ZCA</strong> whitening in machine learning)</li>
<li><strong>ZCA-cor</strong> whitening</li>
<li><strong>Cholesky</strong> whitening</li>
<li><strong>PCA</strong> whitening</li>
<li><strong>PCA-cor</strong> whitening</li>
<li><strong>CCA</strong> whitening (Canonical Correlation Analysis)</li>
</ul>
<p>In the following <span class="math inline">\(\boldsymbol x_c = \boldsymbol x-\boldsymbol \mu_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol z_c = \boldsymbol z-\boldsymbol \mu_{\boldsymbol z}\)</span> denote the mean-centered variables.</p>
<div id="zca-whitening" class="section level3">
<h3><span class="header-section-number">2.4.1</span> ZCA Whitening</h3>
<p><em>Aim</em>: remove correlations but otherwise keep <span class="math inline">\(\boldsymbol z\)</span> as similar as possible to <span class="math inline">\(\boldsymbol x\)</span> (componentwise!)</p>
<p><span class="math display">\[
\begin{array}{cc}
z_1\leftrightarrow x_1 \\
z_2\leftrightarrow x_2\\
z_3\leftrightarrow x_3 \\
\vdots
\end{array}
\]</span></p>
<p><em>Objective function</em>: minimise <span class="math inline">\(\text{E}\left((\boldsymbol z_c-\boldsymbol x_c)^T(\boldsymbol z_c-\boldsymbol x_c)\right) = d - 2\text{Tr}(\boldsymbol \Phi)+\text{Tr}(\boldsymbol V)\)</span></p>
<p><em>Equivalent objective</em>: maximise <span class="math inline">\(\text{Tr}(\boldsymbol \Phi) = \text{Tr}(\boldsymbol Q_1\boldsymbol \Sigma^{1/2})\)</span> to find optimal <span class="math inline">\(\boldsymbol Q_1\)</span><br />
<em>Optinal solution</em>:
<span class="math display">\[\boldsymbol Q_1^{\text{ZCA}}=\boldsymbol I\]</span>
<span class="math display">\[\Longrightarrow \boldsymbol W^{\text{ZCA}} = \boldsymbol \Sigma^{-1/2}\]</span></p>
<ul>
<li>ZCA/Mahalanobis transform is the unique transformation that minimises the expected squared component-wise difference between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>.</li>
<li>Use ZCA and Mahalanobis whitening if you want to âjustâ remove correlations.</li>
</ul>
</div>
<div id="zca-cor-whitening" class="section level3">
<h3><span class="header-section-number">2.4.2</span> ZCA-Cor Whitening</h3>
<p><em>Aim</em>: same as above but remove scale in <span class="math inline">\(\boldsymbol x\)</span> first before comparing to <span class="math inline">\(\boldsymbol z\)</span><br />
<em>Objective function</em>: minimise <span class="math inline">\(\text{E}\left((\boldsymbol z_c-\boldsymbol V^{-1/2}\boldsymbol x_c)^T(\boldsymbol z_c-\boldsymbol V^{-1/2}\boldsymbol x_c)\right) = 2d - 2\text{Tr}(\boldsymbol \Psi)\)</span><br />
<em>Equivalent objective</em>: maximise <span class="math inline">\(\text{Tr}(\boldsymbol \Psi)=\text{Tr}(\boldsymbol Q_2\boldsymbol P^{1/2})\)</span> to find optimal <span class="math inline">\(\boldsymbol Q_2\)</span><br />
<em>Optimal solution</em>: <span class="math display">\[\boldsymbol Q_2^{\text{ZCA-Cor}}=\boldsymbol I\]</span>
<span class="math display">\[\Longrightarrow \boldsymbol W^{\text{ZCA-Cor}} = \boldsymbol P^{-1/2}\boldsymbol V^{-1/2}\]</span></p>
<ul>
<li>ZCA-cor whitening is the unique whitening transformation if you aim to maximise correlation between corresponding components in <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>.</li>
<li>Only if <span class="math inline">\(\boldsymbol x\)</span> is standardised to <span class="math inline">\(\text{Var}(x_i)=1\)</span> then ZCA and ZCA-cor are identical</li>
<li>ZCA and ZCA-cor: lead to interpretable <span class="math inline">\(\boldsymbol z\)</span></li>
</ul>
</div>
<div id="cholesky-whitening" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Cholesky Whitening</h3>
<p><em>Aim</em>: find a whitening transformation such that the cross-covariance and cross-correlation have triangular structure. This is useful in some models (such as time course data) to ensure that the
future cannot influence the past.</p>
<p><em>Solution</em>: Cholesky decomposition of <span class="math inline">\(\boldsymbol \Sigma^{-1} = \boldsymbol L\boldsymbol L^T\)</span></p>
<blockquote>
<p><span class="math inline">\(\boldsymbol L\)</span> is a lower triangular matrix with positive diagonal elements<br />
<span class="math inline">\(\Longrightarrow \boldsymbol W^{\text{Chol}}=\boldsymbol L^T\)</span><br />
By construction, <span class="math inline">\(\boldsymbol W^{\text{Chol}}\)</span> satisfies the whitening constraint since <span class="math inline">\((\boldsymbol W^{\text{Chol}})^T\boldsymbol W^{\text{Chol}} = \boldsymbol \Sigma^{-1}\)</span>.</p>
</blockquote>
<blockquote>
<p>The corresponding rotation matrices are <span class="math inline">\(\boldsymbol Q_1^{\text{Chol}} = \boldsymbol L^T \boldsymbol \Sigma^{1/2}\)</span> and <span class="math inline">\(\boldsymbol Q_2^{\text{Chol}} = \boldsymbol L^T \boldsymbol V^{1/2} \boldsymbol P^{1/2}\)</span><br />
which results in <span class="math inline">\(\boldsymbol \Phi^{\text{Chol}} = \boldsymbol L^T\boldsymbol \Sigma\)</span> and <span class="math inline">\(\boldsymbol \Psi^{\text{Chol}} = \boldsymbol L^T \boldsymbol \Sigma\boldsymbol V^{-1/2}\)</span></p>
</blockquote>
</div>
<div id="pca-whitening" class="section level3">
<h3><span class="header-section-number">2.4.4</span> PCA Whitening</h3>
<p><em>Aim</em>: remove correlations and compress information in <span class="math inline">\(\boldsymbol x\)</span> (each component <span class="math inline">\(z_i\)</span>
maximally linked with all variables in <span class="math inline">\(\boldsymbol x\)</span>):
<span class="math display">\[
\begin{array}{ccccccc}
z_1 &amp; \leftarrow x_1 &amp; &amp; z_2 &amp; \leftarrow x_1  &amp;&amp; \ldots \\
z_1 &amp; \leftarrow x_2 &amp; &amp; z_2 &amp; \leftarrow x_2  \\
\vdots\\
z_1 &amp; \leftarrow x_d &amp; &amp; z_2 &amp; \leftarrow x_d  \\
\end{array}
\]</span>
<em>Objective</em>: maximise <span class="math inline">\(\sum^d_{j=1}\text{Cov}(z_i,x_j)^2 = \sum^d_{j=1} \phi_{ij}^2\)</span> for all <span class="math inline">\(i\)</span><br />
<em>Equivalent objective</em>: maximise <span class="math inline">\((\phi_1,...,\phi_d)^T=\text{Diag}(\boldsymbol \Phi\boldsymbol \Phi^T)=\text{Diag}(\boldsymbol Q_1\boldsymbol \Sigma\boldsymbol Q_1^T)\)</span>, with <span class="math inline">\(\phi_1&gt;\phi_2&gt;\dots &gt;\phi_d\)</span></p>
<p><em>Optimal solution</em>:
<span class="math display">\[\boldsymbol Q_1^{\text{PCA}}=\boldsymbol U^T\]</span>
<span class="math display">\[\Longrightarrow \boldsymbol W^{\text{PCA}} = \boldsymbol U^T\boldsymbol \Sigma^{-1/2}=\boldsymbol \Lambda^{-1/2}\boldsymbol U^T\]</span></p>
<ul>
<li>Optimum value: <span class="math inline">\(\text{Diag}(\boldsymbol Q_1^{\text{PCA}}\boldsymbol \Sigma(\boldsymbol Q_1^{\text{PCA}})^T) = \text{Diag}(\boldsymbol \Lambda)=(\lambda_1, \ldots ,\lambda_d)^T\)</span></li>
<li>PCA whitening is the unique whitening transformation that maximises compression with the sum of squared cross-covariances as underlying optimality criteron</li>
<li>One may use <span class="math inline">\(\frac{\lambda_i}{\sum^d_{j=1}\lambda_j}\)</span> as measure of âproportion of explained variationâ for each component in <span class="math inline">\(\boldsymbol z\)</span><br />
<span class="math inline">\(\rightarrow\)</span> scree plots</li>
<li>This allows the reduction of dimension by discarding low ranking components in <span class="math inline">\(\boldsymbol z\)</span> that do not carry much information</li>
</ul>
</div>
<div id="pca-cor-whitening" class="section level3">
<h3><span class="header-section-number">2.4.5</span> PCA-cor Whitening</h3>
<p><em>Aim</em>: as for PCA whitening but remove scale in <span class="math inline">\(\boldsymbol x\)</span> first (i.e.Â use squared correlations rather squared covariances to measure compression)<br />
<em>Objective</em>: maximise <span class="math inline">\(\sum^d_{j=1}\text{Cor}(z_i, x_j)^2 = \sum^d_{i=1} \psi_{ij}^2\)</span> for all <span class="math inline">\(i\)</span><br />
<em>Equivalent objective</em>: maximise <span class="math inline">\((\psi_1,...,\psi_d)^T=\text{Diag}(\boldsymbol \Psi\boldsymbol \Psi^T)=\text{Diag}(\boldsymbol Q_2\boldsymbol P\boldsymbol Q_2^T)\)</span>
with <span class="math inline">\(\psi_1&gt;\psi_2&gt;\dots &gt;\psi_d\)</span>.</p>
<p><em>Optimal solution</em>: <span class="math display">\[\boldsymbol Q_2^{\text{PCA-Cor}}=\boldsymbol G^T\]</span><br />
(with eigendecomposition of <span class="math inline">\(\boldsymbol P= \boldsymbol G\boldsymbol \Theta\boldsymbol G^T\)</span>)<br />
<span class="math display">\[\Longrightarrow \boldsymbol W^{\text{PCA-Cor}} = \boldsymbol \Theta^{-1/2} \boldsymbol G^T \boldsymbol V^{-1/2}\]</span></p>
<ul>
<li>Optimum value: <span class="math inline">\(\text{Diag}(\boldsymbol Q_2^{\text{PCA-Cor}}\boldsymbol P(\boldsymbol Q_2^{\text{PCA-Cor}})^T) = \text{Diag}(\boldsymbol \Theta) = (\theta_1, \ldots, \theta_d)^T\)</span></li>
<li>PCA-cor whitening is the unique whitening procedure that maximises compression with the sum of squared cross-correlation as optimality criterion</li>
<li>If <span class="math inline">\(\boldsymbol x\)</span> is standardised to <span class="math inline">\(\text{Var}(x_i)=1\)</span>, then PCA and PCA-cor are identical.</li>
<li><span class="math inline">\(\frac{\theta_i}{\sum^d_{j=1} \theta_j} = \frac{\theta_j}{d}\)</span> indicates relative importance of each whitened variable <span class="math inline">\(z_j\)</span> to predict <span class="math inline">\(\boldsymbol x\)</span><br />
(note that <span class="math inline">\(\sum_{j=1}^d \theta_j = \text{Tr}(\boldsymbol P) = d\)</span>)</li>
<li>Useful for reduction of dimension by discarding low-ranking elements in <span class="math inline">\(\boldsymbol z\)</span></li>
</ul>
<p>Criterion for relative contributions is linked to multivariate regression <span class="math inline">\(R^2\)</span>: <span class="math inline">\(\text{Tr}(\boldsymbol P_{yx}\boldsymbol P_{x}^{-1}\boldsymbol P_{xy})\)</span> which for whitening transformations is <span class="math inline">\(d\)</span> in both directions.</p>
</div>
<div id="comparison-of-zca-pca-and-chol-whitening" class="section level3">
<h3><span class="header-section-number">2.4.6</span> Comparison of ZCA, PCA and Chol whitening</h3>
<p><img src="2-transformations_files/figure-html/fig1-1.png" width="672" /></p>
<p>In the above comparison you see ZCA, PCA and Cholesky whitening applied to a simulated bivariate normal data set with correlation <span class="math inline">\(\rho=0.8\)</span> (column 1). All approaches equally succeed in whitening (column 2) but they differ in the cross-correlations. Columns 3 and 4 shows the cross-correlations between the first two pairs corresponding components for ZCA, PCA and Cholesky whitening. As expected, in ZCA both components show strong correlation, but this is not the case for PCA and Cholesky whitening.</p>
</div>
<div id="recap" class="section level3">
<h3><span class="header-section-number">2.4.7</span> Recap</h3>
<table>
<colgroup>
<col width="31%" />
<col width="68%" />
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Type of usage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ZCA, ZCA-cor:</td>
<td>pure decorrelate, maintain similarity to original data set, interpretability</td>
</tr>
<tr class="even">
<td>PCA, PCA-cor:</td>
<td>compression, find effective dimension, reduce dimensionality, feature identification</td>
</tr>
<tr class="odd">
<td>Chol:</td>
<td>time course data</td>
</tr>
</tbody>
</table>
<p><strong>Related models not discussed in this course:</strong></p>
<ul>
<li><p>Factor models: essentially whitening plus an additional error term, factors have rotational
freedom just like in whitening</p></li>
<li><p>PLS: similar to PCA but in regression setting (with the choice of
latent variables depending on the response)</p></li>
</ul>
</div>
</div>
<div id="principal-component-analysis-pca" class="section level2">
<h2><span class="header-section-number">2.5</span> Principal Component Analysis (PCA)</h2>
<ul>
<li>Traditional PCA (invented 1901 by Pearson) is very closely related to <strong>PCA whitening</strong>.</li>
<li>But PCA itself is <strong>not</strong> a whitening procedure!</li>
</ul>
<p><strong>PCA transformation:</strong>
<span class="math display">\[\underbrace{\boldsymbol t}_{\text{Principal Components}} = \underbrace{\boldsymbol U^T\boldsymbol x}_{\text{Orthogonal projection}}\]</span>
<span class="math display">\[\Longrightarrow \text{Var}(\boldsymbol t) = \boldsymbol \Lambda= \begin{pmatrix} \lambda_1 &amp; \dots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \dots &amp; \lambda_d\end{pmatrix}\]</span></p>
<ul>
<li>Principal components are <strong>orthogonal</strong> but do <em>not</em> have unit variance!</li>
<li>The variances of the principal components are equal to the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span>: <span class="math inline">\(\text{Var}(\boldsymbol t^{\text{PCA}}) = \boldsymbol \Lambda\)</span>.</li>
<li>You arrive at PCA whitening by standardising the PCA components: <span class="math inline">\(\boldsymbol z^{\text{PCA}} = \boldsymbol \Lambda^{-1/2} \boldsymbol t^{\text{PCA}}\)</span></li>
<li>Same compression / optimality properties as PCA whitening.</li>
</ul>
<div id="application-to-data" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Application to data</h3>
<p>Written in terms of a data matrix <span class="math inline">\(\boldsymbol X\)</span> instead of a random vector <span class="math inline">\(\boldsymbol x\)</span> PCA becomes:
<span class="math display">\[\underbrace{\boldsymbol T}_{\text{Sample version of principal components}}=\underbrace{\boldsymbol X}_{\text{Data matrix}}\boldsymbol U\]</span>
There are now two ways to obtain <span class="math inline">\(\boldsymbol U\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the covariance matrix, e.g.Â by <span class="math inline">\(\hat{\boldsymbol \Sigma} = \frac{1}{n}\boldsymbol X_c^T\boldsymbol X_c\)</span> where <span class="math inline">\(\boldsymbol X_c\)</span> is the column-centred data matrix; then apply the eigenvalue decomposition on <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> to get <span class="math inline">\(\boldsymbol U\)</span>.</p></li>
<li><p>Compute the singular value decomposition of <span class="math inline">\(\boldsymbol X_c = \boldsymbol V\boldsymbol D\boldsymbol U^T\)</span>. As <span class="math inline">\(\hat{\boldsymbol \Sigma} = \frac{1}{n}\boldsymbol X_c^T\boldsymbol X= \boldsymbol U(\frac{1}{n}\boldsymbol D^2)\boldsymbol U^T\)</span> you can just use <span class="math inline">\(\boldsymbol U\)</span> from the SVD of <span class="math inline">\(\boldsymbol X_c\)</span> and there is no need to compute the covariance.</p></li>
</ol>
</div>
</div>
<div id="pca-correlation-loadings-and-plot" class="section level2">
<h2><span class="header-section-number">2.6</span> PCA correlation loadings and plot</h2>
<p>A useful quantity to evaluate the PCA whitened components <span class="math inline">\(\boldsymbol z^{\text{PCA}}\)</span> and the related principle components <span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span> is the cross-correlation matrix <span class="math inline">\(\boldsymbol \Psi\)</span>. Specifically,
<span class="math inline">\(\boldsymbol \Psi= \text{Cor}(\boldsymbol z^{\text{PCA}}, \boldsymbol x) = \text{Cor}(\boldsymbol t^{\text{PCA}}, \boldsymbol x) = \boldsymbol \Lambda^{1/2} \boldsymbol U^T \boldsymbol V^{-1/2}\)</span>.</p>
<p>We now consider the back-transformation of the (standardised) PCA components to the standardised original components.
The inverse PCA transformation is <span class="math inline">\(\boldsymbol x= \boldsymbol U\boldsymbol t^{\text{PCA}}\)</span> (note that <span class="math inline">\(\boldsymbol U^{-1} = \boldsymbol U^T\)</span>). If one standardises <span class="math inline">\(\boldsymbol x\)</span> this equation becomes <span class="math inline">\(\boldsymbol V^{-1/2} \boldsymbol x= \boldsymbol V^{-1/2} \boldsymbol U\boldsymbol t^{\text{PCA}}\)</span> and expressed in terms of the standardised <span class="math inline">\(\boldsymbol z^{\text{PCA}}\)</span> it becomes <span class="math inline">\(\boldsymbol V^{-1/2} \boldsymbol x= ( \boldsymbol V^{-1/2} \boldsymbol U\boldsymbol \Lambda^{1/2}) \boldsymbol \Lambda^{-1/2} \boldsymbol t^{\text{PCA}}\)</span> which is equivalent to <span class="math inline">\(\boldsymbol V^{-1/2} \boldsymbol x= \boldsymbol \Psi^T \boldsymbol z^{\text{PCA}}\)</span>. Thus the cross-correlation matrix <span class="math inline">\(\boldsymbol \Psi\)</span> plays the role of the <em>correlation loadings</em>, i.e.Â the
coefficients linking the (standardised) PCA components with the standardised original components.</p>
<p>Recall that in a linear regression model with <em>uncorrelated predictors</em> the (squared) correlations between each predictor and the response can be used as measure of variable importance. Since PCA (whitened) components are uncorrelated by construction, the correlation loadings <span class="math inline">\(\boldsymbol \Psi\)</span>
measure the capability of each principal component to predict the original variables.</p>
<p>For visualisation, a correlation loadings plot is often constructed as follows.
The loadings <span class="math inline">\(\boldsymbol \Psi\)</span> for the first two whitened PCA components <span class="math inline">\(z_1\)</span>, <span class="math inline">\(z_2\)</span> (or equivalently for
<span class="math inline">\(t_1, t_2\)</span>) to all original variables are computed. Then a point for each orginal
variable is drawn in a plane with the two correlation loadings acting as its coordinates. By construction, all points
have to lie within a unit circle around the origin . The orginal variables most strongly influenced
by the two latent variables will have strong correlation and thus lie near the outer circle, whereas
variables that are not influenced by the two latent variables will lie near the origin.
(see next section for an example).</p>
<div id="iris-data-example" class="section level3">
<h3><span class="header-section-number">2.6.1</span> Iris data example</h3>
<p>A plot of the the first two components after PCA-whitening is applied reveals the group structure among iris flowers:</p>
<p><img src="2-transformations_files/figure-html/fig2-1.png" width="364.8" /></p>
<p>Here is the corresponding loadings plot:</p>
<p><img src="2-transformations_files/figure-html/unnamed-chunk-1-1.png" width="384" /></p>
</div>
</div>
<div id="cca-whitening-canonical-correlation-analysis" class="section level2">
<h2><span class="header-section-number">2.7</span> CCA whitening (Canonical Correlation Analysis)</h2>
<p>So far, we have looked only into whitening as a <strong>single</strong> vector <span class="math inline">\(\boldsymbol x\)</span>. In CCA whitening we consider <strong>two vectors</strong> <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> simultaneously:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\boldsymbol x= \begin{pmatrix} x_1 \\ \vdots \\ x_p \end{pmatrix} \\
\text{Dimension } p
\end{array}
\begin{array}{ll}
\boldsymbol y= \begin{pmatrix} y_1 \\ \vdots \\ y_q \end{pmatrix} \\
\text{Dimension } q
\end{array}
\begin{array}{ll}
\text{Var}(\boldsymbol x) = \boldsymbol \Sigma_{\boldsymbol x} = \boldsymbol V_{\boldsymbol x}^{1/2}\boldsymbol P_{\boldsymbol x}\boldsymbol V_{\boldsymbol x}^{1/2} \\
\text{Var}(\boldsymbol y) = \boldsymbol \Sigma_{\boldsymbol y} = \boldsymbol V_{\boldsymbol y}^{1/2}\boldsymbol P_{\boldsymbol y}\boldsymbol V_{\boldsymbol y}^{1/2} \\
\end{array}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cc}
\text{Whitening of } \boldsymbol x\text{:} \\
\text{Whitening of } \boldsymbol y\text{:}
\end{array}
\begin{array}{cc}
\boldsymbol z_{\boldsymbol x} = \boldsymbol W_{\boldsymbol x}\boldsymbol x=\boldsymbol Q_{\boldsymbol x}\boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol V_{\boldsymbol x}^{-1/2}\boldsymbol x\\
\boldsymbol z_{\boldsymbol y} = \boldsymbol W_{\boldsymbol y}\boldsymbol y=\boldsymbol Q_{\boldsymbol y}\boldsymbol P_{\boldsymbol y}^{-1/2}\boldsymbol V_{\boldsymbol y}^{-1/2}\boldsymbol y
\end{array}
\end{align*}\]</span>
(note we use the correlation-based form of <span class="math inline">\(\boldsymbol W\)</span>)</p>
<p>Cross-correlation between <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span> and <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span>:</p>
<p><span class="math display">\[\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})=\boldsymbol Q_{\boldsymbol x}\boldsymbol K\boldsymbol Q_{\boldsymbol y}^T\]</span></p>
<p>with <span class="math inline">\(\boldsymbol K= \boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol P_{\boldsymbol x\boldsymbol y}\boldsymbol P_{\boldsymbol y}^{-1/2}\)</span>.</p>
<p><strong>Idea</strong>: we can choose a suitable <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}\)</span> rotation matrix by putting constraints on the cross-correlation.</p>
<p><strong>CCA</strong>: we aim for a <em>diagonal</em> <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> so that each component in <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span> only influences one (the corresponding) component in <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\begin{array}{ll}
\boldsymbol z_{\boldsymbol x} = \begin{pmatrix} z^x_1 \\ z^x_2 \\ \vdots \\ z^x_p \end{pmatrix} \\
\end{array}
\begin{array}{ll}
\boldsymbol z_{\boldsymbol y} = \begin{pmatrix} z^y_1 \\ z^y_2 \\ \vdots \\ z^y_q \end{pmatrix} \\
\end{array}
\begin{array}{ll}
\text{\bf Motivation} \\
\text{pairs of &quot;modules&quot; represented by components of } \boldsymbol z_{\boldsymbol x} \text{ and } \boldsymbol z_{\boldsymbol y} \\
\text{influencing each other (and not anyone else).}
\end{array}
\end{align}\]</span></p>
<p><span class="math display">\[\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y}) = \begin{pmatrix} d_1 &amp; \dots &amp; 0 \\ \vdots &amp;  \vdots \\ 0 &amp; \dots &amp; d_m \end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(d_i\)</span> are the <em>canonical correlations</em> and <span class="math inline">\(m=\min(p,q)\)</span>.</p>
<div id="how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal" class="section level3">
<h3><span class="header-section-number">2.7.1</span> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</h3>
<ul>
<li>Use Singular Value Decomposition (SVD) of matrix <span class="math inline">\(\boldsymbol K\)</span>:<br />
<span class="math display">\[\boldsymbol K= (\boldsymbol Q_{\boldsymbol x}^{\text{CCA}})^T  \boldsymbol D\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\]</span>
where <span class="math inline">\(\boldsymbol D\)</span> is the diagonal matrix containing the singular values of <span class="math inline">\(\boldsymbol K\)</span></li>
<li>This yields rotation matrices <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> and thus the desired whitened matrices <span class="math inline">\(\boldsymbol W_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol W_{\boldsymbol y}^{\text{CCA}}\)</span></li>
<li>As a result <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y}) = \boldsymbol D\)</span> i.e.Â singular values of <span class="math inline">\(\boldsymbol K\)</span> are identical to canonical correlations <span class="math inline">\(d_i\)</span>!</li>
</ul>
<p><span class="math inline">\(\longrightarrow\)</span> <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> are uniquely determined by the diagonality constraint (and different to the other previously discussed whitening methods)</p>
</div>
<div id="related-methods" class="section level3">
<h3><span class="header-section-number">2.7.2</span> Related methods</h3>
<ul>
<li>O2PLS: similar to CCA but using orthogonal projections
(thus in O2PLS the latent variables underlying <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are not orthogonal)</li>
</ul>


</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>this is just one particular example of a coloring tranformation. There are in fact infinitely many, see the discussion on whitening transformations.<a href="2-transformations.html#fnref1" class="footnote-back">â©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-multivariate.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-clustering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math38161-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
