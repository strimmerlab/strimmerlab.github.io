<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Transformations and dimension reduction | _main.utf8</title>
  <meta name="description" content="Multivariate Statistics and Machine Learning<br />
<br />
Lecture Notes<br />
MATH38161</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Transformations and dimension reduction | _main.utf8" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Transformations and dimension reduction | _main.utf8" />
  
  
  

<meta name="author" content="Korbinian Strimmer" />


<meta name="date" content="2020-10-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-multivariate.html"/>
<link rel="next" href="3-clustering.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH38161/index.html">MATH38161 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-multivariate.html"><a href="1-multivariate.html"><i class="fa fa-check"></i><b>1</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="1.1" data-path="1-multivariate.html"><a href="1-multivariate.html#why-multivariate-statistics"><i class="fa fa-check"></i><b>1.1</b> Why multivariate statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="1-multivariate.html"><a href="1-multivariate.html#basics"><i class="fa fa-check"></i><b>1.2</b> Basics</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-vs.multivariate-random-variables"><i class="fa fa-check"></i><b>1.2.1</b> Univariate vs. multivariate random variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-multivariate.html"><a href="1-multivariate.html#mean-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.2</b> Mean of a random vector</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-multivariate.html"><a href="1-multivariate.html#variance-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.3</b> Variance of a random vector</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-multivariate.html"><a href="1-multivariate.html#properties-of-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.4</b> Properties of the covariance matrix</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-multivariate.html"><a href="1-multivariate.html#eigenvalue-decomposition-of-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.2.5</b> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.2.6" data-path="1-multivariate.html"><a href="1-multivariate.html#quantities-related-to-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.6</b> Quantities related to the covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>1.3</b> Multivariate normal distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-normal-distribution"><i class="fa fa-check"></i><b>1.3.1</b> Univariate normal distribution:</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-model"><i class="fa fa-check"></i><b>1.3.2</b> Multivariate normal model</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-multivariate.html"><a href="1-multivariate.html#shape-of-the-contour-lines-of-the-multivariate-normal-density"><i class="fa fa-check"></i><b>1.3.3</b> Shape of the contour lines of the multivariate normal density</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-in-large-sample-and-small-sample-settings"><i class="fa fa-check"></i><b>1.4</b> Estimation in large sample and small sample settings</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-data"><i class="fa fa-check"></i><b>1.4.1</b> Multivariate data</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-multivariate.html"><a href="1-multivariate.html#strategies-for-large-sample-estimation"><i class="fa fa-check"></i><b>1.4.2</b> Strategies for large sample estimation</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-multivariate.html"><a href="1-multivariate.html#large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.4.3</b> Large sample estimates of mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.4.4" data-path="1-multivariate.html"><a href="1-multivariate.html#problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions"><i class="fa fa-check"></i><b>1.4.4</b> Problems with maximum likelihood in small sample settings and high dimensions</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-of-covariance-matrix-in-small-sample-settings"><i class="fa fa-check"></i><b>1.4.5</b> Estimation of covariance matrix in small sample settings</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-multivariate.html"><a href="1-multivariate.html#discrete-multivariate-distributions"><i class="fa fa-check"></i><b>1.5</b> Discrete multivariate distributions</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-multivariate.html"><a href="1-multivariate.html#categorical-distribution"><i class="fa fa-check"></i><b>1.5.1</b> Categorical distribution</a></li>
<li class="chapter" data-level="1.5.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multinomial-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Multinomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="1-multivariate.html"><a href="1-multivariate.html#continuous-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Continuous multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-multivariate.html"><a href="1-multivariate.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.6.1</b> Dirichlet distribution</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-multivariate.html"><a href="1-multivariate.html#wishart-distribution"><i class="fa fa-check"></i><b>1.6.2</b> Wishart distribution</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-multivariate.html"><a href="1-multivariate.html#inverse-wishart-distribution"><i class="fa fa-check"></i><b>1.6.3</b> Inverse Wishart distribution</a></li>
<li class="chapter" data-level="1.6.4" data-path="1-multivariate.html"><a href="1-multivariate.html#further-distributions"><i class="fa fa-check"></i><b>1.6.4</b> Further distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-transformations.html"><a href="2-transformations.html"><i class="fa fa-check"></i><b>2</b> Transformations and dimension reduction</a><ul>
<li class="chapter" data-level="2.1" data-path="2-transformations.html"><a href="2-transformations.html#linear-transformations"><i class="fa fa-check"></i><b>2.1</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-transformations.html"><a href="2-transformations.html#location-scale-transformation"><i class="fa fa-check"></i><b>2.1.1</b> Location-scale transformation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-transformations.html"><a href="2-transformations.html#invertible-location-scale-transformation"><i class="fa fa-check"></i><b>2.1.2</b> Invertible location-scale transformation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-transformations.html"><a href="2-transformations.html#nonlinear-transformations"><i class="fa fa-check"></i><b>2.2</b> Nonlinear transformations</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-transformations.html"><a href="2-transformations.html#general-transformation"><i class="fa fa-check"></i><b>2.2.1</b> General transformation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-transformations.html"><a href="2-transformations.html#delta-method"><i class="fa fa-check"></i><b>2.2.2</b> Delta method</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-transformations.html"><a href="2-transformations.html#transformation-of-densities-under-general-invertible-transformation"><i class="fa fa-check"></i><b>2.2.3</b> Transformation of densities under general invertible transformation</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-transformations.html"><a href="2-transformations.html#normalising-flows"><i class="fa fa-check"></i><b>2.2.4</b> Normalising flows</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-transformations.html"><a href="2-transformations.html#whitening-transformations"><i class="fa fa-check"></i><b>2.3</b> Whitening transformations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-transformations.html"><a href="2-transformations.html#overview"><i class="fa fa-check"></i><b>2.3.1</b> Overview</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-transformations.html"><a href="2-transformations.html#general-whitening-transformation"><i class="fa fa-check"></i><b>2.3.2</b> General whitening transformation</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-transformations.html"><a href="2-transformations.html#general-solution-of-whitening-constraint-covariance-based"><i class="fa fa-check"></i><b>2.3.3</b> General solution of whitening constraint (covariance-based)</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-transformations.html"><a href="2-transformations.html#another-solution-correlation-based"><i class="fa fa-check"></i><b>2.3.4</b> Another solution (correlation-based)</a></li>
<li class="chapter" data-level="2.3.5" data-path="2-transformations.html"><a href="2-transformations.html#objective-criteria-for-choosing-among-boldsymbol-w-using-boldsymbol-q_1-or-boldsymbol-q_2"><i class="fa fa-check"></i><b>2.3.5</b> Objective criteria for choosing among <span class="math inline">\(\boldsymbol W\)</span> using <span class="math inline">\(\boldsymbol Q_1\)</span> or <span class="math inline">\(\boldsymbol Q_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-transformations.html"><a href="2-transformations.html#natural-whitening-procedures"><i class="fa fa-check"></i><b>2.4</b> Natural whitening procedures</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-transformations.html"><a href="2-transformations.html#zca-whitening"><i class="fa fa-check"></i><b>2.4.1</b> ZCA Whitening</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-transformations.html"><a href="2-transformations.html#zca-cor-whitening"><i class="fa fa-check"></i><b>2.4.2</b> ZCA-Cor Whitening</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-transformations.html"><a href="2-transformations.html#pca-whitening"><i class="fa fa-check"></i><b>2.4.3</b> PCA Whitening</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-transformations.html"><a href="2-transformations.html#pca-cor-whitening"><i class="fa fa-check"></i><b>2.4.4</b> PCA-cor Whitening</a></li>
<li class="chapter" data-level="2.4.5" data-path="2-transformations.html"><a href="2-transformations.html#cholesky-whitening"><i class="fa fa-check"></i><b>2.4.5</b> Cholesky Whitening</a></li>
<li class="chapter" data-level="2.4.6" data-path="2-transformations.html"><a href="2-transformations.html#comparison-of-zca-pca-and-chol-whitening"><i class="fa fa-check"></i><b>2.4.6</b> Comparison of ZCA, PCA and Chol whitening</a></li>
<li class="chapter" data-level="2.4.7" data-path="2-transformations.html"><a href="2-transformations.html#recap"><i class="fa fa-check"></i><b>2.4.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-transformations.html"><a href="2-transformations.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>2.5</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-transformations.html"><a href="2-transformations.html#application-to-data"><i class="fa fa-check"></i><b>2.5.1</b> Application to data</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-transformations.html"><a href="2-transformations.html#pca-correlation-loadings-and-plot"><i class="fa fa-check"></i><b>2.6</b> PCA correlation loadings and plot</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-transformations.html"><a href="2-transformations.html#iris-data-example"><i class="fa fa-check"></i><b>2.6.1</b> Iris data example</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-transformations.html"><a href="2-transformations.html#cca-whitening-canonical-correlation-analysis"><i class="fa fa-check"></i><b>2.7</b> CCA whitening (Canonical Correlation Analysis)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-transformations.html"><a href="2-transformations.html#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal"><i class="fa fa-check"></i><b>2.7.1</b> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</a></li>
<li class="chapter" data-level="2.7.2" data-path="2-transformations.html"><a href="2-transformations.html#related-methods"><i class="fa fa-check"></i><b>2.7.2</b> Related methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-clustering.html"><a href="3-clustering.html"><i class="fa fa-check"></i><b>3</b> Unsupervised learning and clustering</a><ul>
<li class="chapter" data-level="3.1" data-path="3-clustering.html"><a href="3-clustering.html#challenges-in-supervised-learning"><i class="fa fa-check"></i><b>3.1</b> Challenges in supervised learning</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-clustering.html"><a href="3-clustering.html#objective"><i class="fa fa-check"></i><b>3.1.1</b> Objective</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-clustering.html"><a href="3-clustering.html#questions-and-problems"><i class="fa fa-check"></i><b>3.1.2</b> Questions and problems</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-clustering.html"><a href="3-clustering.html#why-is-clustering-difficult"><i class="fa fa-check"></i><b>3.1.3</b> Why is clustering difficult?</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-clustering.html"><a href="3-clustering.html#common-types-of-clustering-methods"><i class="fa fa-check"></i><b>3.1.4</b> Common types of clustering methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-clustering.html"><a href="3-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>3.2</b> Hierarchical clustering</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-clustering.html"><a href="3-clustering.html#tree-like-structures"><i class="fa fa-check"></i><b>3.2.1</b> Tree-like structures</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-clustering.html"><a href="3-clustering.html#agglomerative-hierarchical-clustering-algorithms"><i class="fa fa-check"></i><b>3.2.2</b> Agglomerative hierarchical clustering algorithms</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-clustering.html"><a href="3-clustering.html#application-to-swiss-banknote-data-set"><i class="fa fa-check"></i><b>3.2.3</b> Application to Swiss banknote data set</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-clustering.html"><a href="3-clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>3.3</b> <span class="math inline">\(K\)</span>-means clustering</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-clustering.html"><a href="3-clustering.html#general-aims"><i class="fa fa-check"></i><b>3.3.1</b> General aims</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-clustering.html"><a href="3-clustering.html#algorithm"><i class="fa fa-check"></i><b>3.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-clustering.html"><a href="3-clustering.html#properties"><i class="fa fa-check"></i><b>3.3.3</b> Properties</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>3.3.4</b> Choosing the number of clusters</a></li>
<li class="chapter" data-level="3.3.5" data-path="3-clustering.html"><a href="3-clustering.html#k-medoids-aka-pam"><i class="fa fa-check"></i><b>3.3.5</b> <span class="math inline">\(K\)</span>-medoids aka PAM</a></li>
<li class="chapter" data-level="3.3.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-k-means-to-iris-data"><i class="fa fa-check"></i><b>3.3.6</b> Application of <span class="math inline">\(K\)</span>-means to Iris data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-clustering.html"><a href="3-clustering.html#mixture-models"><i class="fa fa-check"></i><b>3.4</b> Mixture models</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-clustering.html"><a href="3-clustering.html#finite-mixture-model"><i class="fa fa-check"></i><b>3.4.1</b> Finite mixture model</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-clustering.html"><a href="3-clustering.html#example-of-mixture-of-three-univariate-normal-densities"><i class="fa fa-check"></i><b>3.4.2</b> Example of mixture of three univariate normal densities:</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-clustering.html"><a href="3-clustering.html#example-of-a-mixture-of-two-bivariate-normal-densities"><i class="fa fa-check"></i><b>3.4.3</b> Example of a mixture of two bivariate normal densities</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-clustering.html"><a href="3-clustering.html#sampling-from-a-mixture-model-and-latent-allocation-variable-formulation"><i class="fa fa-check"></i><b>3.4.4</b> Sampling from a mixture model and latent allocation variable formulation</a></li>
<li class="chapter" data-level="3.4.5" data-path="3-clustering.html"><a href="3-clustering.html#predicting-the-group-allocation-of-a-given-sample"><i class="fa fa-check"></i><b>3.4.5</b> Predicting the group allocation of a given sample</a></li>
<li class="chapter" data-level="3.4.6" data-path="3-clustering.html"><a href="3-clustering.html#decomposition-of-covariance-and-total-variation"><i class="fa fa-check"></i><b>3.4.6</b> Decomposition of covariance and total variation</a></li>
<li class="chapter" data-level="3.4.7" data-path="3-clustering.html"><a href="3-clustering.html#variation-1-infinite-mixture-model"><i class="fa fa-check"></i><b>3.4.7</b> Variation 1: Infinite mixture model</a></li>
<li class="chapter" data-level="3.4.8" data-path="3-clustering.html"><a href="3-clustering.html#variation-2-semiparametric-mixture-model-with-two-classes"><i class="fa fa-check"></i><b>3.4.8</b> Variation 2: Semiparametric mixture model with two classes</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-clustering.html"><a href="3-clustering.html#fitting-mixture-models-to-data"><i class="fa fa-check"></i><b>3.5</b> Fitting mixture models to data</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-clustering.html"><a href="3-clustering.html#direct-estimation-of-mixture-model-parameters"><i class="fa fa-check"></i><b>3.5.1</b> Direct estimation of mixture model parameters</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-clustering.html"><a href="3-clustering.html#estimate-mixture-model-parameters-using-the-em-algorithm"><i class="fa fa-check"></i><b>3.5.2</b> Estimate mixture model parameters using the EM algorithm</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-clustering.html"><a href="3-clustering.html#em-algorithm-for-multivariate-normal-mixture-model"><i class="fa fa-check"></i><b>3.5.3</b> EM algorithm for multivariate normal mixture model</a></li>
<li class="chapter" data-level="3.5.4" data-path="3-clustering.html"><a href="3-clustering.html#connection-with-k-means-clustering-method"><i class="fa fa-check"></i><b>3.5.4</b> Connection with <span class="math inline">\(K\)</span>-means clustering method</a></li>
<li class="chapter" data-level="3.5.5" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-classes"><i class="fa fa-check"></i><b>3.5.5</b> Choosing the number of classes</a></li>
<li class="chapter" data-level="3.5.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-gmms-to-iris-flower-data"><i class="fa fa-check"></i><b>3.5.6</b> Application of GMMs to Iris flower data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-classification.html"><a href="4-classification.html"><i class="fa fa-check"></i><b>4</b> Supervised learning and classification</a><ul>
<li class="chapter" data-level="4.1" data-path="4-classification.html"><a href="4-classification.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-classification.html"><a href="4-classification.html#supervised-learning-vs.unsupervised-learning"><i class="fa fa-check"></i><b>4.1.1</b> Supervised learning vs. unsupervised learning</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-classification.html"><a href="4-classification.html#terminology"><i class="fa fa-check"></i><b>4.1.2</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-classification.html"><a href="4-classification.html#bayesian-discriminant-rule-or-bayes-classifier"><i class="fa fa-check"></i><b>4.2</b> Bayesian discriminant rule or Bayes classifier</a></li>
<li class="chapter" data-level="4.3" data-path="4-classification.html"><a href="4-classification.html#normal-bayes-classifier"><i class="fa fa-check"></i><b>4.3</b> Normal Bayes classifier</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-classification.html"><a href="4-classification.html#quadratic-discriminant-analysis-qda-and-gaussian-assumption"><i class="fa fa-check"></i><b>4.3.1</b> Quadratic discriminant analysis (QDA) and Gaussian assumption</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-classification.html"><a href="4-classification.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>4.3.2</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-classification.html"><a href="4-classification.html#diagonal-discriminant-analysis-dda"><i class="fa fa-check"></i><b>4.3.3</b> Diagonal discriminant analysis (DDA)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-classification.html"><a href="4-classification.html#the-training-step-learning-qda-lda-and-dda-classifiers-from-data"><i class="fa fa-check"></i><b>4.4</b> The training step — learning QDA, LDA and DDA classifiers from data</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-classification.html"><a href="4-classification.html#number-of-model-parameters"><i class="fa fa-check"></i><b>4.4.1</b> Number of model parameters</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-classification.html"><a href="4-classification.html#estimating-the-discriminant-predictor-function"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the discriminant / predictor function</a></li>
<li class="chapter" data-level="4.4.3" data-path="4-classification.html"><a href="4-classification.html#comparison-of-estimated-decision-boundaries-lda-vs.qda"><i class="fa fa-check"></i><b>4.4.3</b> Comparison of estimated decision boundaries: LDA vs. QDA</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-classification.html"><a href="4-classification.html#goodness-of-fit-and-variable-ranking"><i class="fa fa-check"></i><b>4.5</b> Goodness of fit and variable ranking</a><ul>
<li class="chapter" data-level="4.5.1" data-path="4-classification.html"><a href="4-classification.html#lda-with-k2-classes"><i class="fa fa-check"></i><b>4.5.1</b> LDA with <span class="math inline">\(K=2\)</span> classes</a></li>
<li class="chapter" data-level="4.5.2" data-path="4-classification.html"><a href="4-classification.html#multiple-classes"><i class="fa fa-check"></i><b>4.5.2</b> Multiple classes</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4-classification.html"><a href="4-classification.html#variable-selection-and-cross-validation"><i class="fa fa-check"></i><b>4.6</b> Variable selection and cross-validation</a><ul>
<li class="chapter" data-level="4.6.1" data-path="4-classification.html"><a href="4-classification.html#choosing-a-threshold-by-multiple-testing-using-false-discovery-rates"><i class="fa fa-check"></i><b>4.6.1</b> Choosing a threshold by multiple testing using false discovery rates</a></li>
<li class="chapter" data-level="4.6.2" data-path="4-classification.html"><a href="4-classification.html#quantifying-prediction-error"><i class="fa fa-check"></i><b>4.6.2</b> Quantifying prediction error</a></li>
<li class="chapter" data-level="4.6.3" data-path="4-classification.html"><a href="4-classification.html#estimation-of-prediction-error-without-validation-data-using-cross-validation"><i class="fa fa-check"></i><b>4.6.3</b> Estimation of prediction error without validation data using cross-validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-dependence.html"><a href="5-dependence.html"><i class="fa fa-check"></i><b>5</b> Multivariate dependencies</a><ul>
<li class="chapter" data-level="5.1" data-path="5-dependence.html"><a href="5-dependence.html#measuring-the-linear-association-between-two-sets-of-random-variables"><i class="fa fa-check"></i><b>5.1</b> Measuring the linear association between two sets of random variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-dependence.html"><a href="5-dependence.html#outline"><i class="fa fa-check"></i><b>5.1.1</b> Outline</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-dependence.html"><a href="5-dependence.html#special-cases"><i class="fa fa-check"></i><b>5.1.2</b> Special cases</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-dependence.html"><a href="5-dependence.html#rozeboom-vector-correlation"><i class="fa fa-check"></i><b>5.1.3</b> Rozeboom vector correlation</a></li>
<li class="chapter" data-level="5.1.4" data-path="5-dependence.html"><a href="5-dependence.html#rv-coefficient"><i class="fa fa-check"></i><b>5.1.4</b> RV coefficient</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-as-generalisation-of-correlation"><i class="fa fa-check"></i><b>5.2</b> Mutual information as generalisation of correlation</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-dependence.html"><a href="5-dependence.html#definition-of-mutual-information"><i class="fa fa-check"></i><b>5.2.1</b> Definition of mutual information</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normal-variables"><i class="fa fa-check"></i><b>5.2.2</b> Mutual information between two normal variables</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normally-distributed-random-vectors"><i class="fa fa-check"></i><b>5.2.3</b> Mutual information between two normally distributed random vectors</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-dependence.html"><a href="5-dependence.html#using-mi-for-variable-selection"><i class="fa fa-check"></i><b>5.2.4</b> Using MI for variable selection</a></li>
<li class="chapter" data-level="5.2.5" data-path="5-dependence.html"><a href="5-dependence.html#other-measures-of-general-dependence"><i class="fa fa-check"></i><b>5.2.5</b> Other measures of general dependence</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-dependence.html"><a href="5-dependence.html#graphical-models"><i class="fa fa-check"></i><b>5.3</b> Graphical models</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-dependence.html"><a href="5-dependence.html#purpose"><i class="fa fa-check"></i><b>5.3.1</b> Purpose</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-dependence.html"><a href="5-dependence.html#basic-notions-from-graph-theory"><i class="fa fa-check"></i><b>5.3.2</b> Basic notions from graph theory</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-dependence.html"><a href="5-dependence.html#probabilistic-graphical-models"><i class="fa fa-check"></i><b>5.3.3</b> Probabilistic graphical models</a></li>
<li class="chapter" data-level="5.3.4" data-path="5-dependence.html"><a href="5-dependence.html#directed-graphical-models"><i class="fa fa-check"></i><b>5.3.4</b> Directed graphical models</a></li>
<li class="chapter" data-level="5.3.5" data-path="5-dependence.html"><a href="5-dependence.html#undirected-graphical-models"><i class="fa fa-check"></i><b>5.3.5</b> Undirected graphical models</a></li>
<li class="chapter" data-level="5.3.6" data-path="5-dependence.html"><a href="5-dependence.html#algorithm-for-learning-ggms"><i class="fa fa-check"></i><b>5.3.6</b> Algorithm for learning GGMs</a></li>
<li class="chapter" data-level="5.3.7" data-path="5-dependence.html"><a href="5-dependence.html#example-exam-score-data-mardia-et-al-1979"><i class="fa fa-check"></i><b>5.3.7</b> Example: exam score data (Mardia et al 1979:)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-nonlinear.html"><a href="6-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Nonlinear and nonparametric models</a><ul>
<li class="chapter" data-level="6.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#limits-of-linear-models-and-correlation"><i class="fa fa-check"></i><b>6.1</b> Limits of linear models and correlation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#correlation-only-measures-linear-dependence"><i class="fa fa-check"></i><b>6.1.1</b> Correlation only measures linear dependence</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#anscombe-data-sets"><i class="fa fa-check"></i><b>6.1.2</b> Anscombe data sets</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests"><i class="fa fa-check"></i><b>6.2</b> Random forests</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#stochastic-vs.algorithmic-models"><i class="fa fa-check"></i><b>6.2.1</b> Stochastic vs. algorithmic models</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests-1"><i class="fa fa-check"></i><b>6.2.2</b> Random forests</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#comparison-of-decision-boundaries-decision-tree-vs.random-forest"><i class="fa fa-check"></i><b>6.2.3</b> Comparison of decision boundaries: decision tree vs. random forest</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gaussian-processes"><i class="fa fa-check"></i><b>6.3</b> Gaussian processes</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#main-concepts"><i class="fa fa-check"></i><b>6.3.1</b> Main concepts</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#technical-background"><i class="fa fa-check"></i><b>6.3.2</b> Technical background:</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#covariance-functions-and-kernel"><i class="fa fa-check"></i><b>6.3.3</b> Covariance functions and kernel</a></li>
<li class="chapter" data-level="6.3.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gp-model"><i class="fa fa-check"></i><b>6.3.4</b> GP model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks"><i class="fa fa-check"></i><b>6.4</b> Neural networks</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#history"><i class="fa fa-check"></i><b>6.4.1</b> History</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks-1"><i class="fa fa-check"></i><b>6.4.2</b> Neural networks</a></li>
<li class="chapter" data-level="6.4.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#learning-more-about-deep-learning"><i class="fa fa-check"></i><b>6.4.3</b> Learning more about deep learning</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="7-matrices.html"><a href="7-matrices.html"><i class="fa fa-check"></i><b>A</b> Brief refresher on matrices</a><ul>
<li class="chapter" data-level="A.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-basics"><i class="fa fa-check"></i><b>A.1</b> Matrix basics</a><ul>
<li class="chapter" data-level="A.1.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-notation"><i class="fa fa-check"></i><b>A.1.1</b> Matrix notation</a></li>
<li class="chapter" data-level="A.1.2" data-path="7-matrices.html"><a href="7-matrices.html#random-matrix"><i class="fa fa-check"></i><b>A.1.2</b> Random matrix</a></li>
<li class="chapter" data-level="A.1.3" data-path="7-matrices.html"><a href="7-matrices.html#special-matrices"><i class="fa fa-check"></i><b>A.1.3</b> Special matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="7-matrices.html"><a href="7-matrices.html#simple-matrix-operations"><i class="fa fa-check"></i><b>A.2</b> Simple matrix operations</a><ul>
<li class="chapter" data-level="A.2.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-addition-and-multiplication"><i class="fa fa-check"></i><b>A.2.1</b> Matrix addition and multiplication</a></li>
<li class="chapter" data-level="A.2.2" data-path="7-matrices.html"><a href="7-matrices.html#matrix-transpose"><i class="fa fa-check"></i><b>A.2.2</b> Matrix transpose</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="7-matrices.html"><a href="7-matrices.html#matrix-summaries"><i class="fa fa-check"></i><b>A.3</b> Matrix summaries</a><ul>
<li class="chapter" data-level="A.3.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-trace"><i class="fa fa-check"></i><b>A.3.1</b> Matrix trace</a></li>
<li class="chapter" data-level="A.3.2" data-path="7-matrices.html"><a href="7-matrices.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>A.3.2</b> Determinant of a matrix</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="7-matrices.html"><a href="7-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.4</b> Matrix inverse</a><ul>
<li class="chapter" data-level="A.4.1" data-path="7-matrices.html"><a href="7-matrices.html#inversion-of-square-matrix"><i class="fa fa-check"></i><b>A.4.1</b> Inversion of square matrix</a></li>
<li class="chapter" data-level="A.4.2" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.4.2</b> Orthogonal matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>A.5</b> Eigenvalues and eigenvectors</a><ul>
<li class="chapter" data-level="A.5.1" data-path="7-matrices.html"><a href="7-matrices.html#definition"><i class="fa fa-check"></i><b>A.5.1</b> Definition</a></li>
<li class="chapter" data-level="A.5.2" data-path="7-matrices.html"><a href="7-matrices.html#finding-eigenvalues-and-vectors"><i class="fa fa-check"></i><b>A.5.2</b> Finding eigenvalues and vectors</a></li>
<li class="chapter" data-level="A.5.3" data-path="7-matrices.html"><a href="7-matrices.html#eigenequation-in-matrix-notation"><i class="fa fa-check"></i><b>A.5.3</b> Eigenequation in matrix notation</a></li>
<li class="chapter" data-level="A.5.4" data-path="7-matrices.html"><a href="7-matrices.html#defective-matrix"><i class="fa fa-check"></i><b>A.5.4</b> Defective matrix</a></li>
<li class="chapter" data-level="A.5.5" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-of-a-diagonal-or-triangular-matrix"><i class="fa fa-check"></i><b>A.5.5</b> Eigenvalues of a diagonal or triangular matrix</a></li>
<li class="chapter" data-level="A.5.6" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-vectors-of-a-symmetric-matrix"><i class="fa fa-check"></i><b>A.5.6</b> Eigenvalues and vectors of a symmetric matrix</a></li>
<li class="chapter" data-level="A.5.7" data-path="7-matrices.html"><a href="7-matrices.html#positive-definite-matrices"><i class="fa fa-check"></i><b>A.5.7</b> Positive definite matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.6" data-path="7-matrices.html"><a href="7-matrices.html#matrix-decompositions"><i class="fa fa-check"></i><b>A.6</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="A.6.1" data-path="7-matrices.html"><a href="7-matrices.html#diagonalisation-and-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.6.1</b> Diagonalisation and eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.6.2" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.6.2</b> Orthogonal eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.6.3" data-path="7-matrices.html"><a href="7-matrices.html#singular-value-decomposition"><i class="fa fa-check"></i><b>A.6.3</b> Singular value decomposition</a></li>
<li class="chapter" data-level="A.6.4" data-path="7-matrices.html"><a href="7-matrices.html#cholesky-decomposition"><i class="fa fa-check"></i><b>A.6.4</b> Cholesky decomposition</a></li>
</ul></li>
<li class="chapter" data-level="A.7" data-path="7-matrices.html"><a href="7-matrices.html#matrix-summaries-based-on-eigenvalues-and-singular-values"><i class="fa fa-check"></i><b>A.7</b> Matrix summaries based on eigenvalues and singular values</a><ul>
<li class="chapter" data-level="A.7.1" data-path="7-matrices.html"><a href="7-matrices.html#trace-and-determinant-computed-from-eigenvalues"><i class="fa fa-check"></i><b>A.7.1</b> Trace and determinant computed from eigenvalues</a></li>
<li class="chapter" data-level="A.7.2" data-path="7-matrices.html"><a href="7-matrices.html#rank-and-condition-number"><i class="fa fa-check"></i><b>A.7.2</b> Rank and condition number</a></li>
</ul></li>
<li class="chapter" data-level="A.8" data-path="7-matrices.html"><a href="7-matrices.html#functions-of-symmetric-matrices"><i class="fa fa-check"></i><b>A.8</b> Functions of symmetric matrices</a></li>
<li class="chapter" data-level="A.9" data-path="7-matrices.html"><a href="7-matrices.html#matrix-calculus"><i class="fa fa-check"></i><b>A.9</b> Matrix calculus</a><ul>
<li class="chapter" data-level="A.9.1" data-path="7-matrices.html"><a href="7-matrices.html#first-order-vector-derivatives"><i class="fa fa-check"></i><b>A.9.1</b> First order vector derivatives</a></li>
<li class="chapter" data-level="A.9.2" data-path="7-matrices.html"><a href="7-matrices.html#second-order-vector-derivatives"><i class="fa fa-check"></i><b>A.9.2</b> Second order vector derivatives</a></li>
<li class="chapter" data-level="A.9.3" data-path="7-matrices.html"><a href="7-matrices.html#first-order-matrix-derivatives"><i class="fa fa-check"></i><b>A.9.3</b> First order matrix derivatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="8-further-study.html"><a href="8-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="8-further-study.html"><a href="8-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="8-further-study.html"><a href="8-further-study.html#advanced-reading"><i class="fa fa-check"></i><b>B.2</b> Advanced reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="9-references.html"><a href="9-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Multivariate Statistics and Machine Learning<br />
<br />
Lecture Notes<br />
MATH38161</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="transformations-and-dimension-reduction" class="section level1">
<h1><span class="header-section-number">2</span> Transformations and dimension reduction</h1>
<p>Motivation:
In the following we study transformations of random vectors and their distributions.
These transformation are very important
since they either transform simple distributions into more complex distributions or allow to simplify
complex models. In machine learning invertible mappings of transformations
for probability distributions are known as “normalising flows” (these play a key role
e.g. in neural networks).</p>
<div id="linear-transformations" class="section level2">
<h2><span class="header-section-number">2.1</span> Linear Transformations</h2>
<div id="location-scale-transformation" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Location-scale transformation</h3>
<p>Also known as affine transformation.</p>
<p><span class="math display">\[\boldsymbol y= \underbrace{\boldsymbol a}_{\text{location parameter}}+\underbrace{\boldsymbol B}_{\text{scale parameter}} \boldsymbol x\space\]</span>
<span class="math display">\[\boldsymbol y: m \times 1 \text{ random vector}\]</span>
<span class="math display">\[\boldsymbol a: m \times 1 \text{ vector, location parameter}\]</span>
<span class="math display">\[\boldsymbol B: m \times d \text{ matrix, scale parameter },  m \geq 1\]</span>
<span class="math display">\[\boldsymbol x: d \times 1 \text{ random vector}\]</span></p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\text{E}(\boldsymbol x)=\boldsymbol \mu\\
\text{Var}(\boldsymbol x)=\boldsymbol \Sigma\\
\end{array}
\Longrightarrow
\begin{array}{ll}
\text{E}(\boldsymbol y)=\boldsymbol a+ \boldsymbol B\boldsymbol \mu\\
\text{Var}(\boldsymbol y)= \boldsymbol B\boldsymbol \Sigma\boldsymbol B^T \\
\end{array}
\end{align*}\]</span></p>
<p>Special cases/examples:</p>
<ol style="list-style-type: decimal">
<li>Univariate case (<span class="math inline">\(d=1, m=1\)</span>)
<ul>
<li><span class="math inline">\(\text{E}(y)=a+b\mu\)</span></li>
<li><span class="math inline">\(\text{Var}(y)=b^2\sigma^2\)</span></li>
</ul></li>
<li>Sum of two random univariate variables<br />
<span class="math inline">\(y = x_1 + x_2\)</span>, i.e. <span class="math inline">\(a=0\)</span> and <span class="math inline">\(\boldsymbol B=(1,1)\)</span>
<ul>
<li><span class="math inline">\(\text{E}(x_1+x_2)=\mu_1+\mu_2\)</span><br />
</li>
<li><span class="math inline">\(\text{Var}(x_1+x_2) = (1,1)\begin{pmatrix}  \sigma^2_1 &amp; \sigma_{12}\\  \sigma_{21} &amp; \sigma^2_2  \end{pmatrix} \begin{pmatrix}  1\\  1  \end{pmatrix} = \sigma^2_1+\sigma^2_2+2\sigma_{12} = \text{Var}(x_1)+\text{Var}(x_2)+2\text{Cov}(x_1,x_2)\)</span></li>
</ul></li>
<li><span class="math inline">\(y_1=a_1+b_1 x_1\)</span> and <span class="math inline">\(y_2=a_2+b_2 x_2\)</span>, i.e. <span class="math inline">\(\boldsymbol a= \begin{pmatrix} a_1\\ a_2 \end{pmatrix}\)</span> and <span class="math inline">\(\boldsymbol B= \begin{pmatrix}b_1 &amp; 0\\ 0 &amp; b_2\end{pmatrix}\)</span>
<ul>
<li><span class="math inline">\(\text{E}(\boldsymbol y)=\begin{pmatrix} a_1+b_1 \mu_1\\ a_2+b_2 \mu_2 \end{pmatrix}\)</span><br />
</li>
<li><span class="math inline">\(\text{Var}(\boldsymbol y) = \begin{pmatrix} b_1 &amp; 0\\ 0 &amp; b_2 \end{pmatrix} \begin{pmatrix}  \sigma^2_1 &amp; \sigma_{12}\\  \sigma_{21} &amp; \sigma^2_2  \end{pmatrix} \begin{pmatrix} b_1 &amp; 0\\ 0 &amp; b_2 \end{pmatrix} = \begin{pmatrix}  b^2_1\sigma^2_1 &amp; b_1b_2\sigma_{12}\\  b_1b_2\sigma_{21} &amp; b^2_2\sigma^2_2  \end{pmatrix}\)</span><br />
i.e. <span class="math inline">\(\text{Cov}(a_1+b_1 x_1,a_2+b_2 x_2) = b_1 b_2\text{Cov}(x_1,x_2)\)</span></li>
</ul></li>
</ol>
</div>
<div id="invertible-location-scale-transformation" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Invertible location-scale transformation</h3>
<p>If <span class="math inline">\(m=d\)</span> and <span class="math inline">\(\det(\boldsymbol B) \neq 0\)</span> then we get an <strong>invertible</strong> transformation:
<span class="math display">\[\boldsymbol y= \boldsymbol a+ \boldsymbol B\boldsymbol x\]</span>
<span class="math display">\[\boldsymbol x= \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\]</span></p>
<p>Transformation of density:
<span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> with density <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span></p>
<p><span class="math inline">\(\Longrightarrow\)</span> <span class="math inline">\(\boldsymbol y\sim F_{\boldsymbol y}\)</span> with density
<span class="math display">\[ f_{\boldsymbol y}(\boldsymbol y)=|\det(\boldsymbol B)|^{-1} f_{\boldsymbol x} \left( \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\right)\]</span></p>

<div class="example">
<span id="exm:mahatrans" class="example"><strong>Example 2.1  </strong></span><strong>Mahalanobis transform</strong> <span class="math inline">\(\boldsymbol y=\boldsymbol \Sigma^{-1/2}(\boldsymbol x-\boldsymbol \mu)\)</span>
</div>

<p>We assume a positive definite and thus invertible <span class="math inline">\(\boldsymbol \Sigma\)</span>, so
that the inverse principal matrix square root <span class="math inline">\(\boldsymbol \Sigma^{-1/2}\)</span> can be computed,
and the transformation itself is invertible.</p>
<p><span class="math display">\[\boldsymbol a= - \boldsymbol \Sigma^{-1/2} \boldsymbol \mu\]</span>
<span class="math display">\[\boldsymbol B= \boldsymbol \Sigma^{-1/2}\]</span>
<span class="math display">\[\text{E}(\boldsymbol x)=\boldsymbol \mu\text{ and } \text{Var}(\boldsymbol x)=\boldsymbol \Sigma\]</span>
<span class="math display">\[\Longrightarrow\text{E}(\boldsymbol y) = \boldsymbol 0\text{ and } \text{Var}(\boldsymbol y) = \boldsymbol I_d\]</span>
Mahalanobis transformation performs three functions:</p>
<ol style="list-style-type: decimal">
<li>Centering (<span class="math inline">\(-\boldsymbol \mu\)</span>)</li>
<li>Standardisation <span class="math inline">\(\text{Var}(y_i)=1\)</span></li>
<li>Decorrelation <span class="math inline">\(\text{Cor}(y_i,y_j)=0\)</span> for <span class="math inline">\(i \neq j\)</span></li>
</ol>
<p><strong>Univariate case (<span class="math inline">\(d=1\)</span>)</strong></p>
<p><span class="math display">\[y = \frac{x-\mu}{\sigma}\]</span></p>
<p>= centering + standardisation</p>
<p>The <strong>Mahalanobis transformation</strong> appears implicitly in many places in multivariate statistics,
e.g. in the multivariate normal density.</p>
<p>It is a particular example of a whitening transformation (of which there
are infinitely many, see later chapters).</p>

<div class="example">
<span id="exm:coltrans" class="example"><strong>Example 2.2  </strong></span><strong>Inverse Mahalanobis transformation</strong> <span class="math inline">\(\boldsymbol y= \boldsymbol \mu+\boldsymbol \Sigma^{1/2} \boldsymbol x\)</span>
</div>

<p>This transformation is the <strong>inverse</strong> of the Mahalanobis transformation.
As the Mahalanobis transform is a particular whitening transform the inverse
transform is sometimes called the Mahalanobis colouring transformation.</p>
<p><span class="math display">\[\boldsymbol a=\boldsymbol \mu\]</span>
<span class="math display">\[\boldsymbol B=\boldsymbol \Sigma^{1/2}\]</span></p>
<p><span class="math display">\[\text{E}(\boldsymbol x)=\boldsymbol 0\text{ and } \text{Var}(\boldsymbol x)=\boldsymbol I_d\]</span>
<span class="math display">\[\Longrightarrow\text{E}(\boldsymbol y) = \boldsymbol \mu\text{ and } \text{Var}(\boldsymbol y) = \boldsymbol \Sigma\]</span>
Assume <span class="math inline">\(\boldsymbol x\)</span> is multivariate standard normal <span class="math inline">\(\boldsymbol x\sim N_d(\boldsymbol 0,\boldsymbol I_d)\)</span> with density
<span class="math display">\[f_{\boldsymbol x}(\boldsymbol x) = (2\pi)^{-d/2}\exp\left( -\frac{1}{2} \boldsymbol x^T \boldsymbol x\right)\]</span>
Then the density after applying this inverse Mahalanobis transform is</p>
<p><span class="math display">\[f_{\boldsymbol y}(\boldsymbol y) = |\det(\boldsymbol \Sigma^{1/2})|^{-1} (2\pi)^{-d/2} \exp\left(-\frac{1}{2}(\boldsymbol y-\boldsymbol \mu)^T\boldsymbol \Sigma^{-1/2} \,\boldsymbol \Sigma^{-1/2}(\boldsymbol y-\boldsymbol \mu)\right)\]</span></p>
<p><span class="math display">\[= (2\pi)^{-d/2} \det(\boldsymbol \Sigma)^{-1/2} \exp\left(-\frac{1}{2}(\boldsymbol y-\boldsymbol \mu)^T\boldsymbol \Sigma^{-1}(\boldsymbol y-\boldsymbol \mu)\right)\]</span>
<span class="math inline">\(\Longrightarrow\)</span> <span class="math inline">\(\boldsymbol y\)</span> has multivariate normal density!!</p>
<p><em>Application:</em> e.g. random number generation: draw from <span class="math inline">\(N_d(\boldsymbol 0,\boldsymbol I_d)\)</span> (easy!) then convert to multivariate normal by tranformation.</p>
</div>
</div>
<div id="nonlinear-transformations" class="section level2">
<h2><span class="header-section-number">2.2</span> Nonlinear transformations</h2>
<div id="general-transformation" class="section level3">
<h3><span class="header-section-number">2.2.1</span> General transformation</h3>
<p><span class="math display">\[\boldsymbol y= \boldsymbol h(\boldsymbol x)\]</span>
with <span class="math inline">\(\boldsymbol h\)</span> an arbitrary vector-valued function</p>
<ul>
<li>linear case: <span class="math inline">\(\boldsymbol h(\boldsymbol x) = \boldsymbol a+\boldsymbol B\boldsymbol x\)</span></li>
</ul>
</div>
<div id="delta-method" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Delta method</h3>
<p>Assume that we know the mean <span class="math inline">\(\text{E}(\boldsymbol x)=\boldsymbol \mu\)</span> and variance <span class="math inline">\(\text{Var}(\boldsymbol x)=\boldsymbol \Sigma\)</span> of <span class="math inline">\(\boldsymbol x\)</span>.
Is it possible to say something about the mean and variance of the transformed
random variable <span class="math inline">\(\boldsymbol y\)</span>?
<span class="math display">\[
\text{E}(\boldsymbol y)= \text{E}(\boldsymbol h(\boldsymbol x))= ?
\]</span>
<span class="math display">\[
\text{Var}(\boldsymbol y) = \text{Var}(\boldsymbol h(\boldsymbol x))= ? \\
\]</span></p>
<p>In general, for a transformation <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> the exact mean and variance of the transformed variable cannot be obtained analytically.</p>
<p>However, we can find a <strong>linear approximation</strong> and then compute its mean and variance.
This approximation is called the “Delta Method”, or the “law of propagation of errors”, and is credited to Gauss.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>Linearisation of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> is achieved by a Taylor series approximation of first order
of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> around <span class="math inline">\(\boldsymbol x_0\)</span>:
<span class="math display">\[\boldsymbol h(\boldsymbol x) \approx \boldsymbol h(\boldsymbol x_0) + \underbrace{\boldsymbol J_{\boldsymbol h}(\boldsymbol x_0)}_{\text{Jacobi matrix}}(\boldsymbol x-\boldsymbol x_0)  = 
\underbrace{\boldsymbol h(\boldsymbol x_0) -\boldsymbol J_{\boldsymbol h}(\boldsymbol x_0)\, \boldsymbol x_0}_{\boldsymbol a} + \underbrace{\boldsymbol J_{\boldsymbol h}(\boldsymbol x_0)}_{\boldsymbol B} \boldsymbol x\]</span></p>
<p><span class="math inline">\(\nabla\)</span>, the nabla operator, is the row vector <span class="math inline">\((\frac{\partial}{\partial x_1},...,\frac{\partial}{\partial x_d})\)</span>, which when applied to univariate <span class="math inline">\(h\)</span> gives the gradient:</p>
<p><span class="math display">\[\nabla h(\boldsymbol x) = \left(\frac{\partial h}{\partial x_1},...,\frac{\partial h}{\partial x_d}\right)\]</span></p>
<p>The <strong>Jacobi matrix</strong> is the <strong>generalisation of the gradient</strong> if <span class="math inline">\(\boldsymbol h\)</span> is vector-valued:</p>
<p><span class="math display">\[\boldsymbol J_{\boldsymbol h}(\boldsymbol x) = \begin{pmatrix}\nabla h_1(\boldsymbol x)\\ \nabla h_2(\boldsymbol x) \\ \vdots \\ \nabla h_m(\boldsymbol x) \end{pmatrix} = \begin{pmatrix}
    \frac{\partial h_1}{\partial x_1} &amp; \dots &amp; \frac{\partial h_1}{\partial x_d}\\
    \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial h_m}{\partial x_1} &amp; \dots &amp; \frac{\partial h_m}{\partial x_d}
    \end{pmatrix}\]</span></p>
<p>First order approximation of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> around <span class="math inline">\(\boldsymbol x_0=\boldsymbol \mu\)</span> yields
<span class="math inline">\(\boldsymbol a= \boldsymbol h(\boldsymbol \mu) - \boldsymbol J_{\boldsymbol h}(\boldsymbol \mu)\, \boldsymbol \mu\)</span>
<span class="math inline">\(\boldsymbol B= \boldsymbol J_{\boldsymbol h}(\boldsymbol \mu)\)</span> and leads directly to the <strong>multivariate Delta method</strong>:</p>
<p><span class="math display">\[\text{E}(\boldsymbol y)\approx\boldsymbol h(\boldsymbol \mu)\]</span>
<span class="math display">\[\text{Var}(\boldsymbol y)\approx \boldsymbol J_{\boldsymbol h}(\boldsymbol \mu) \, \boldsymbol \Sigma\, \boldsymbol J_{\boldsymbol h}(\boldsymbol \mu)^T\]</span></p>
<p>The <strong>univariate Delta method</strong> is a special case:
<span class="math display">\[\text{E}(y) \approx h(\mu)\]</span>
<span class="math display">\[\text{Var}(y)\approx \sigma^2 h&#39;(\mu)^2\]</span></p>
<p>Note that the Delta approximation breaks down if <span class="math inline">\(\text{Var}(\boldsymbol y)\)</span> is singular,
for example if the first derivative (or gradient or Jacobi matrix) at <span class="math inline">\(\boldsymbol \mu\)</span> is zero.</p>

<div class="example">
<p><span id="exm:varoddsration" class="example"><strong>Example 2.3  </strong></span><strong>Variance of the odds ratio</strong></p>
<p>The proportion <span class="math inline">\(\hat{p} = \frac{n_1}{n}\)</span> resulting from
<span class="math inline">\(n\)</span> repeats of a Bernoulli experiment has expectation <span class="math inline">\(\text{E}(\hat{p})=p\)</span>
and variance <span class="math inline">\(\text{Var}(\hat{p}) = \frac{p (1-p)}{n}\)</span>.
What are the (approximate) mean and the variance of the corresponding odds ratio <span class="math inline">\(\widehat{OR}=\frac{\hat{p}}{1-\hat{p}}\)</span>?</p>
With <span class="math inline">\(h(x) = \frac{x}{1-x}\)</span>,
<span class="math inline">\(\widehat{OR} = h(\hat{p})\)</span> and <span class="math inline">\(h&#39;(x) = \frac{1}{(1-x)^2}\)</span> we get using the
Delta method
<span class="math inline">\(\text{E}( \widehat{OR} ) \approx h(p) = \frac{p}{1-p}\)</span> and
<span class="math inline">\(\text{Var}( \widehat{OR} )\approx h&#39;(p)^2 \text{Var}( \hat{p} ) = \frac{p}{n (1-p)^3}\)</span>.
</div>


<div class="example">
<p><span id="exm:logtransform" class="example"><strong>Example 2.4  </strong></span><strong>Log-transform as variance stabilisation</strong></p>
<p>Assume <span class="math inline">\(x\)</span> has some mean <span class="math inline">\(\text{E}(x)=\mu\)</span> and variance <span class="math inline">\(\text{Var}(x) = \sigma^2 \mu^2\)</span>,
i.e. the standard deviation <span class="math inline">\(\text{SD}(x)\)</span> is proportional to the mean <span class="math inline">\(\mu\)</span>.
What are the (approximate) mean and the variance of the log-transformed variable <span class="math inline">\(\log(x)\)</span>?</p>
With <span class="math inline">\(h(x) = \log(x)\)</span> and <span class="math inline">\(h&#39;(x) = \frac{1}{x}\)</span> we get using the
Delta method
<span class="math inline">\(\text{E}( \log(x) ) \approx h(\mu) = \log(\mu)\)</span> and
<span class="math inline">\(\text{Var}( \log(x) )\approx h&#39;(\mu)^2 \text{Var}( x ) = \left(\frac{1}{\mu} \right)^2 \sigma^2 \mu^2 = \sigma^2\)</span>. Thus, after applying the log-transform the variance does not depend any more on the mean!
</div>

</div>
<div id="transformation-of-densities-under-general-invertible-transformation" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Transformation of densities under general invertible transformation</h3>
<p>Assume <span class="math inline">\(\boldsymbol h(\boldsymbol x) = \boldsymbol y(\boldsymbol x)\)</span> is invertible: <span class="math inline">\(\boldsymbol h^{-1}(\boldsymbol y)=\boldsymbol x(\boldsymbol y)\)</span></p>
<p><span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> with probability density function <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span></p>
<p>The density <span class="math inline">\(f_{\boldsymbol y}(\boldsymbol y)\)</span> of the transformed random vector <span class="math inline">\(\boldsymbol y\)</span> is then given by<br />
<span class="math display">\[f_{\boldsymbol y}(\boldsymbol y) = |\det\left( \boldsymbol J_{\boldsymbol x}(\boldsymbol y) \right)| \,\,\,  f_{\boldsymbol x}\left( \boldsymbol x(\boldsymbol y) \right)\]</span></p>
<p>where <span class="math inline">\(\boldsymbol J_{\boldsymbol x}(\boldsymbol y)\)</span> is the Jacobi matrix of the inverse transformation</p>
<p>Special cases:</p>
<ul>
<li>Univariate version: <span class="math inline">\(f_y(y) = |\frac{dx}{dy}| \, f_x\left(x(y)\right)\)</span></li>
<li>Linear transformation <span class="math inline">\(\boldsymbol h(\boldsymbol x) = \boldsymbol a+ \boldsymbol B\boldsymbol x\)</span>, with <span class="math inline">\(\boldsymbol x(\boldsymbol y) = \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\)</span>
and <span class="math inline">\(\boldsymbol J_{\boldsymbol x}(\boldsymbol y) = \boldsymbol B^{-1}\)</span>:<br />
<span class="math inline">\(f_{\boldsymbol y}(\boldsymbol y)=|\det(\boldsymbol B)|^{-1} f_{\boldsymbol x} \left( \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\right)\)</span></li>
</ul>
</div>
<div id="normalising-flows" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Normalising flows</h3>
<p>In machine learning (sequences of) invertible nonlinear transformations are known as “normalising flows”. They are used both in a generative way (building complex models from
simple models) and also in a simplification and dimension reduction context.</p>
<p>In this module we will focus mostly on linear transformations as these underpin
much of classical multivariate statistics, but it is important to keep in mind for later study
the importance of nonlinear transformations —see, e.g, the review paper by Kobyzev et al. “Normalizing Flows: Introduction and Ideas”, available from <a href="https://arxiv.org/abs/1908.09257" class="uri">https://arxiv.org/abs/1908.09257</a> .</p>
</div>
</div>
<div id="whitening-transformations" class="section level2">
<h2><span class="header-section-number">2.3</span> Whitening transformations</h2>
<div id="overview" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Overview</h3>
<p>The <em>Mahalanobis</em> transform (also know as “zero-phase component analysis” or short ZCA transform in machine learning) is a specific example of a <strong>whitening transformation</strong>. These constitute an important and widely used class of invertible location-scale transformations.</p>
<p><em>Terminology:</em> whitening refers to the fact that after the transformation the covariance matrix is spherical, isotrop, white (<span class="math inline">\(\boldsymbol I_d\)</span>)</p>
<p>Whitening is <strong>useful in preprocessing</strong>, to <strong>turn multivariate problems into simple univariate models</strong> and some <strong>reduce the dimension in an optimal way</strong>.</p>
<p>In so-called latent variable models whitening procedures link observed and latent variables (which usually are uncorrelated and standardised random variables):</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cl}
\text{Whitening} \\
\downarrow
\end{array}
\begin{array}{ll}
\boldsymbol x\\
\uparrow \\
\boldsymbol z\\
\end{array}
\begin{array}{ll}
\text{Observed external variable (can be measured), typically correlated} \\
\space \\
\text{Unobserved &quot;latent&quot; variable internal, typically not correlated} \\
\end{array}
\end{align*}\]</span></p>
</div>
<div id="general-whitening-transformation" class="section level3">
<h3><span class="header-section-number">2.3.2</span> General whitening transformation</h3>
<p><span class="math display">\[\boldsymbol x\text{ random vector} \sim F_{\boldsymbol x} \text{ (not necessarily from multivariate normal)}\]</span></p>
<p><span class="math display">\[\underbrace{\boldsymbol z}_{d \times 1 \text{ vector }} = \underbrace{\boldsymbol W}_{d \times d \text{ whitening matrix }} \underbrace{\boldsymbol x}_{d \times 1 \text{ vector }}\]</span>
<strong>Objective</strong>: choose <span class="math inline">\(\boldsymbol W\)</span> so that <span class="math inline">\(\text{Var}(\boldsymbol z)=\boldsymbol I_d\)</span></p>
<p>Note: we do not care about <span class="math inline">\(\text{E}(\boldsymbol z)\)</span> since we can always centre!</p>
<p>For Mahalanobis/ZCA whitening we already know that <span class="math inline">\(\boldsymbol W^{\text{ZCA}}=\boldsymbol \Sigma^{-1/2}\)</span>.</p>
<p>In general, <span class="math inline">\(\boldsymbol W\)</span> needs to satisfy a constraint:
<span class="math display">\[
\begin{array}{lll}
                &amp; \text{Var}(\boldsymbol z) &amp; = \boldsymbol I_d \\
\Longrightarrow &amp; \text{Var}(\boldsymbol W\boldsymbol x) &amp;= \boldsymbol W\boldsymbol \Sigma\boldsymbol W^T = \boldsymbol I_d \\
\Longrightarrow &amp;  \boldsymbol W\, \boldsymbol \Sigma\, \boldsymbol W^T \boldsymbol W= \boldsymbol W&amp; \\
\end{array}
\]</span>
<span class="math display">\[\Longrightarrow \text{constraint on whitening matrix: } \boldsymbol W^T \boldsymbol W= \boldsymbol \Sigma^{-1}\]</span></p>
<p>Clearly, the ZCA whitening matrix satisfies this constraint: <span class="math inline">\((\boldsymbol W^{ZCA})^T \boldsymbol W^{ZCA} = \boldsymbol \Sigma^{-1/2}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1}\)</span></p>
</div>
<div id="general-solution-of-whitening-constraint-covariance-based" class="section level3">
<h3><span class="header-section-number">2.3.3</span> General solution of whitening constraint (covariance-based)</h3>
<p><span class="math display">\[\boldsymbol W= \boldsymbol Q_1 \boldsymbol \Sigma^{-1/2}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol Q_1\)</span> is a rotation (orthogonal) matrix (with properties <span class="math inline">\(\boldsymbol Q^T \boldsymbol Q= \boldsymbol Q\boldsymbol Q^T = \boldsymbol I\)</span> and <span class="math inline">\(\boldsymbol Q^T = \boldsymbol Q^{-1}\)</span>).</p>
<p>Easy to see that this <span class="math inline">\(\boldsymbol W\)</span> satisfies the whitening constraint:</p>
<p><span class="math display">\[\boldsymbol W^T \boldsymbol W= \boldsymbol \Sigma^{-1/2}\underbrace{\boldsymbol Q_1^T \boldsymbol Q_1}_{\boldsymbol I}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1}\]</span></p>
<p>Note the converse is also true: any whitening whitening matrix, i.e. any <span class="math inline">\(\boldsymbol W\)</span> satisfying the whitening constraint, can be written in the above form as
<span class="math inline">\(\boldsymbol Q_1 = \boldsymbol W\boldsymbol \Sigma^{1/2}\)</span> is orthogonal by construction.</p>
<p><span class="math inline">\(\Longrightarrow\)</span> instead of choosing <span class="math inline">\(\boldsymbol W\)</span>, <strong>we choose the rotation matrix</strong> <span class="math inline">\(\boldsymbol Q_1\)</span>!</p>
<ul>
<li>it is now clear that there are infinitely many whitening procedures, because there are infinitely many rotations! This also means we need to find ways to choose/select among whitening procedures.</li>
<li>for the Mahalanobis/ZCA transformation <span class="math inline">\(\boldsymbol Q_1^{\text{ZCA}}=\boldsymbol I\)</span></li>
<li><strong>whitening</strong> can be interpreted as <strong>Mahalanobis transform</strong> followed by <strong>rotation</strong></li>
</ul>
</div>
<div id="another-solution-correlation-based" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Another solution (correlation-based)</h3>
<p>Instead of working with <span class="math inline">\(\boldsymbol \Sigma\)</span>, we can express <span class="math inline">\(\boldsymbol W\)</span> also in terms of correlation matrix <span class="math inline">\(\boldsymbol P= (\rho_{ij})\)</span>.
<span class="math display">\[\boldsymbol W= \boldsymbol Q_2 \boldsymbol P^{-1/2} \boldsymbol V^{-1/2}\]</span>
where <span class="math inline">\(\boldsymbol V^{1/2}\)</span> is the diagonal matrix containing the variances.</p>
<p>It is easy to verify that this <span class="math inline">\(\boldsymbol W\)</span> also satisfies the whitening constraint:
<span class="math display">\[
\begin{array}{ll}
\boldsymbol W^T \boldsymbol W&amp; = \boldsymbol V^{-1/2}\boldsymbol P^{-1/2}\underbrace{\boldsymbol Q_2^T \boldsymbol Q_2}_{\boldsymbol I}\boldsymbol P^{-1/2} \boldsymbol V^{-1/2} \\
&amp; = \boldsymbol V^{-1/2} \boldsymbol P^{-1} \boldsymbol V^{-1/2} = \boldsymbol \Sigma^{-1} \\
\end{array}
\]</span>
Conversely, any whitening matrix <span class="math inline">\(\boldsymbol W\)</span> can also be written in this form as
<span class="math inline">\(\boldsymbol Q_2 = \boldsymbol W\boldsymbol V^{1/2} \boldsymbol P^{1/2}\)</span> is orthogonal by construction.</p>
<ul>
<li>for Mahalanobis/ZCA transformation <span class="math inline">\(\boldsymbol Q_2^{\text{ZCA}} = \boldsymbol \Sigma^{-1/2} \boldsymbol V^{1/2} \boldsymbol P^{1/2}\)</span></li>
<li><strong>Another interpretation of whitening</strong>: first <strong>standardising</strong> (<span class="math inline">\(\boldsymbol V^{-1/2}\)</span>), then <strong>decorrelation</strong> (<span class="math inline">\(\boldsymbol P^{-1/2}\)</span>), followed by <strong>rotation</strong> (<span class="math inline">\(\boldsymbol Q_2\)</span>)</li>
</ul>
<p><strong>Both forms to write <span class="math inline">\(\boldsymbol W\)</span> are equally valid (and interchangeable).</strong></p>
<p>Note that for the same <span class="math inline">\(\boldsymbol W\)</span>
<span class="math display">\[\boldsymbol Q_1\neq\boldsymbol Q_2 \text{  Two different rotation matrices!}\]</span>
and also
<span class="math display">\[\underbrace{\boldsymbol \Sigma^{-1/2}}_{\text{Symmetric}}\neq\underbrace{\boldsymbol P^{-1/2}\boldsymbol V^{-1/2}}_{\text{Not Symmetric}}\]</span>
even though<br />
<span class="math display">\[\boldsymbol \Sigma^{-1/2}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1} = \boldsymbol V^{-1/2}\boldsymbol P^{-1/2}\boldsymbol P^{-1/2}\boldsymbol V^{-1/2}\]</span></p>
</div>
<div id="objective-criteria-for-choosing-among-boldsymbol-w-using-boldsymbol-q_1-or-boldsymbol-q_2" class="section level3">
<h3><span class="header-section-number">2.3.5</span> Objective criteria for choosing among <span class="math inline">\(\boldsymbol W\)</span> using <span class="math inline">\(\boldsymbol Q_1\)</span> or <span class="math inline">\(\boldsymbol Q_2\)</span></h3>
<ol style="list-style-type: decimal">
<li><strong>Cross-covariance</strong> <span class="math inline">\(\boldsymbol \Phi= \Sigma_{\boldsymbol z\boldsymbol x}\)</span> between <span class="math inline">\(\boldsymbol z\)</span> and <span class="math inline">\(\boldsymbol x\)</span><br />
<span class="math display">\[
\begin{array}{ll}
\boldsymbol \Phi= \text{Cov}(\boldsymbol z,\boldsymbol x) &amp; = \text{Cov}(\boldsymbol W\boldsymbol x,\boldsymbol x) = \boldsymbol W\boldsymbol \Sigma\\
&amp;= \boldsymbol Q_1 \boldsymbol \Sigma^{-1/2} \boldsymbol \Sigma= \boldsymbol Q_1\boldsymbol \Sigma^{1/2}
\end{array}
\]</span></li>
</ol>
<ul>
<li><strong>Cross-covariance linked with</strong> <span class="math inline">\(\boldsymbol Q_1\)</span>!</li>
<li>Choosing suitable cross-covariance allows the selection of a “good” <span class="math inline">\(\boldsymbol Q_1\)</span> (and hence <span class="math inline">\(\boldsymbol W\)</span>)</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Cross-correlation</strong> <span class="math inline">\(\boldsymbol \Psi= \boldsymbol P_{\boldsymbol z\boldsymbol x}\)</span> between <span class="math inline">\(\boldsymbol z\)</span> and <span class="math inline">\(\boldsymbol x\)</span><br />
<span class="math display">\[
\begin{array}{ll}
\boldsymbol \Psi= \text{Cor}(\boldsymbol z,\boldsymbol x) &amp; = \boldsymbol \Phi\boldsymbol V^{-1/2} = \boldsymbol W\boldsymbol \Sigma\boldsymbol V^{-1/2}\\
&amp;=\boldsymbol Q_2 \boldsymbol P^{-1/2} \boldsymbol V^{-1/2} \boldsymbol \Sigma\boldsymbol V^{-1/2} =  \boldsymbol Q_2\boldsymbol P^{1/2}
\end{array}
\]</span></li>
</ol>
<ul>
<li><strong>Cross-correlation linked with</strong> <span class="math inline">\(\boldsymbol Q_2\)</span>!</li>
<li>Choosing suitable cross-correlation allows the selection of a “good” <span class="math inline">\(\boldsymbol Q_2\)</span> (and hence <span class="math inline">\(\boldsymbol W\)</span>)</li>
</ul>
<p>Properties:
<span class="math display">\[\boldsymbol \Psi= (\psi_{ij})\]</span>
<span class="math display">\[\boldsymbol \Psi^T \boldsymbol \Psi= \boldsymbol P\]</span>
<span class="math inline">\(\Longrightarrow \text{Diag}(\boldsymbol \Psi^T \boldsymbol \Psi) = (1,1, \ldots, 1)\)</span> and <span class="math inline">\(\text{Tr}(\boldsymbol \Psi^T \boldsymbol \Psi)=d\)</span></p>
<p><span class="math display">\[\Longrightarrow \sum_{i=1}^d \psi_{ij}^2 = 1\]</span>
i.e. the <em>column</em> sums of <span class="math inline">\(\psi_{ij}^2\)</span> are equal to 1.</p>
<p><strong>Interpretation:</strong> this is the quared multiple correlation coefficient <span class="math inline">\(R^2=1\)</span> between <span class="math inline">\(z_1, ..., z_d\)</span> and <span class="math inline">\(x_j\)</span>!<br />
(as the <span class="math inline">\(z_i\)</span> are all uncorrelated you can simply sum the squared correlations to get <span class="math inline">\(R^2\)</span>; it is equal to 1 because whitening is an invertible linear transformation)</p>
<p>The <span class="math inline">\(R^2\)</span>-s going from <span class="math inline">\(x_1, \ldots, x_d\)</span> to the <span class="math inline">\(z_i\)</span> also equal 1, but because the <span class="math inline">\(x_i\)</span> are correlated they need to be computed as <span class="math inline">\(\text{Diag}(\boldsymbol \Psi\boldsymbol P^{-1} \boldsymbol \Psi^T) = \text{Diag}(\boldsymbol P_{\boldsymbol z\boldsymbol x} \boldsymbol P^{-1}_{\boldsymbol x} \boldsymbol P_{\boldsymbol x\boldsymbol z}) = (1,1, \ldots, 1)\)</span>.</p>
</div>
</div>
<div id="natural-whitening-procedures" class="section level2">
<h2><span class="header-section-number">2.4</span> Natural whitening procedures</h2>
<p>Now we discuss several strategies (maximise correlation between individual components, maximise compression, etc.) to arrive at optimal whitening transformation.</p>
<p>This leads to the following “natural” whitening transformations:</p>
<ul>
<li><strong>Mahalanobis</strong> whitening, also known as <strong>ZCA</strong> (zero-phase component analysis) whitening in machine learning</li>
<li><strong>ZCA-cor</strong> whitening</li>
<li><strong>PCA</strong> whitening</li>
<li><strong>PCA-cor</strong> whitening</li>
<li><strong>Cholesky</strong> whitening</li>
</ul>
<p>In the following <span class="math inline">\(\boldsymbol x_c = \boldsymbol x-\boldsymbol \mu_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol z_c = \boldsymbol z-\boldsymbol \mu_{\boldsymbol z}\)</span> denote the mean-centered variables.</p>
<div id="zca-whitening" class="section level3">
<h3><span class="header-section-number">2.4.1</span> ZCA Whitening</h3>
<p><em>Aim</em>: remove correlations but otherwise keep <span class="math inline">\(\boldsymbol z\)</span> as similar as possible to <span class="math inline">\(\boldsymbol x\)</span> (componentwise!)</p>
<p><span class="math display">\[
\begin{array}{cc}
z_1\leftrightarrow x_1 \\
z_2\leftrightarrow x_2\\
z_3\leftrightarrow x_3 \\
\vdots
\end{array}
\]</span></p>
<p><em>Objective function</em>: minimise <span class="math inline">\(\text{E}\left((\boldsymbol z_c-\boldsymbol x_c)^T(\boldsymbol z_c-\boldsymbol x_c)\right) = d - 2\text{Tr}(\boldsymbol \Phi)+\text{Tr}(\boldsymbol V)\)</span></p>
<p><em>Equivalent objective</em>: maximise <span class="math inline">\(\text{Tr}(\boldsymbol \Phi) = \text{Tr}(\boldsymbol Q_1\boldsymbol \Sigma^{1/2})\)</span> to find optimal <span class="math inline">\(\boldsymbol Q_1\)</span><br />
<em>Optimal solution</em>: Using the eigendecomposition <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span> we have <span class="math inline">\(\text{Tr}(\boldsymbol Q_1\boldsymbol \Sigma^{1/2}) = \text{Tr}(\boldsymbol \Lambda^{1/2} \boldsymbol U^T \boldsymbol Q_1 \boldsymbol U)\)</span> which is maximised for <span class="math inline">\(\boldsymbol U^T \boldsymbol Q_1 \boldsymbol U=\boldsymbol I\)</span>. Thus, the optimal rotation matrix is
<span class="math display">\[\boldsymbol Q_1^{\text{ZCA}}=\boldsymbol I\]</span>
<span class="math display">\[\Longrightarrow \boldsymbol W^{\text{ZCA}} = \boldsymbol \Sigma^{-1/2}\]</span></p>
<ul>
<li>ZCA/Mahalanobis transform is the unique transformation that minimises the expected squared component-wise difference between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>.</li>
<li>Use ZCA and Mahalanobis whitening if you want to “just” remove correlations.</li>
</ul>
</div>
<div id="zca-cor-whitening" class="section level3">
<h3><span class="header-section-number">2.4.2</span> ZCA-Cor Whitening</h3>
<p><em>Aim</em>: same as above but remove scale in <span class="math inline">\(\boldsymbol x\)</span> first before comparing to <span class="math inline">\(\boldsymbol z\)</span><br />
<em>Objective function</em>: minimise <span class="math inline">\(\text{E}\left((\boldsymbol z_c-\boldsymbol V^{-1/2}\boldsymbol x_c)^T(\boldsymbol z_c-\boldsymbol V^{-1/2}\boldsymbol x_c)\right) = 2d - 2\text{Tr}(\boldsymbol \Psi)\)</span><br />
<em>Equivalent objective</em>: maximise <span class="math inline">\(\text{Tr}(\boldsymbol \Psi)=\text{Tr}(\boldsymbol Q_2\boldsymbol P^{1/2})\)</span> to find optimal <span class="math inline">\(\boldsymbol Q_2\)</span><br />
<em>Optimal solution</em>: same as above for ZCA but using correlation matrix instead of covariance: using the eigendecomposition of <span class="math inline">\(\boldsymbol P= \boldsymbol G\boldsymbol \Theta\boldsymbol G^T\)</span> we
get <span class="math inline">\(\text{Tr}(\boldsymbol Q_2\boldsymbol P^{1/2}) ) = \text{Tr}( \boldsymbol \Theta^{1/2} \boldsymbol G^T \boldsymbol Q_2 \boldsymbol G)\)</span>
which is maximised for
<span class="math display">\[\boldsymbol Q_2^{\text{ZCA-Cor}}=\boldsymbol I\]</span>
<span class="math display">\[\Longrightarrow \boldsymbol W^{\text{ZCA-Cor}} = \boldsymbol P^{-1/2}\boldsymbol V^{-1/2}\]</span></p>
<ul>
<li>ZCA-cor whitening is the unique whitening transformation if you aim to maximise correlation between corresponding components in <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>.</li>
<li>Only if <span class="math inline">\(\boldsymbol x\)</span> is standardised to <span class="math inline">\(\text{Var}(x_i)=1\)</span> then ZCA and ZCA-cor are identical</li>
<li>ZCA and ZCA-cor: lead to interpretable <span class="math inline">\(\boldsymbol z\)</span></li>
</ul>
</div>
<div id="pca-whitening" class="section level3">
<h3><span class="header-section-number">2.4.3</span> PCA Whitening</h3>
<p><em>Aim</em>: remove correlations and compress information in <span class="math inline">\(\boldsymbol x\)</span> (each component <span class="math inline">\(z_i\)</span>
maximally linked with all variables in <span class="math inline">\(\boldsymbol x\)</span>):
<span class="math display">\[
\begin{array}{ccccccc}
z_1 &amp; \leftarrow x_1 &amp; &amp; z_2 &amp; \leftarrow x_1  &amp;&amp; \ldots \\
z_1 &amp; \leftarrow x_2 &amp; &amp; z_2 &amp; \leftarrow x_2  \\
\vdots\\
z_1 &amp; \leftarrow x_d &amp; &amp; z_2 &amp; \leftarrow x_d  \\
\end{array}
\]</span>
<em>Objective</em>: maximise <span class="math inline">\(\sum^d_{j=1}\text{Cov}(z_i,x_j)^2 = \sum^d_{j=1} \phi_{ij}^2\)</span> for all <span class="math inline">\(i\)</span><br />
<em>Equivalent objective</em>: maximise <span class="math inline">\((\phi_1,...,\phi_d)^T=\text{Diag}(\boldsymbol \Phi\boldsymbol \Phi^T)=\text{Diag}(\boldsymbol Q_1\boldsymbol \Sigma\boldsymbol Q_1^T) = \text{Diag}( (\boldsymbol Q_1 \boldsymbol U) \boldsymbol \Lambda(\boldsymbol Q_1 \boldsymbol U) ^T )\)</span>, with <span class="math inline">\(\phi_1&gt;\phi_2&gt;\dots &gt;\phi_d\)</span></p>
<p><em>Optimal solution</em>: this is optimised for <span class="math inline">\(\boldsymbol Q_1 \boldsymbol U= \boldsymbol I\)</span> hence
<span class="math display">\[\boldsymbol Q_1^{\text{PCA}}=\boldsymbol U^T\]</span>
<span class="math display">\[\Longrightarrow \boldsymbol W^{\text{PCA}} = \boldsymbol U^T\boldsymbol \Sigma^{-1/2}=\boldsymbol \Lambda^{-1/2}\boldsymbol U^T\]</span></p>
<ul>
<li>Optimum value: <span class="math inline">\(\text{Diag}(\boldsymbol Q_1^{\text{PCA}}\boldsymbol \Sigma(\boldsymbol Q_1^{\text{PCA}})^T) = \text{Diag}(\boldsymbol \Lambda)=(\lambda_1, \ldots ,\lambda_d)^T\)</span>, i.e. the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span> are equal to
the sum of the squared covariances from each latent component to all the original variables.</li>
<li>PCA whitening is a whitening transformation that maximises compression with the sum of squared cross-covariances as underlying optimality criteron</li>
</ul>
<p>The fraction <span class="math inline">\(\frac{\lambda_i}{\sum^d_{j=1}\lambda_j}\)</span> can be used as a measure
of relative importance of each component in <span class="math inline">\(\boldsymbol z\)</span> to explain the original variables.
Thus, low ranking components in <span class="math inline">\(\boldsymbol z\)</span> with low total squared
covariance with <span class="math inline">\(\boldsymbol x\)</span> may be discarded, thus leading to a reduction in dimension.</p>
<p>Note that the column signs in <span class="math inline">\(\boldsymbol U\)</span> are not identified in an eigenvalue decomposition. Therefore, it is useful to impose a further constraint on the rotation
matrix. A useful condition is to assume a positive diagonal, i.e. <span class="math inline">\(\text{Diag}(\boldsymbol Q_1^{\text{PCA}}) &gt; 0\)</span> and thus <span class="math inline">\(\text{Diag}(\boldsymbol U) &gt; 0\)</span> and <span class="math inline">\(\text{Diag}(\boldsymbol \Phi) &gt; 0\)</span>.</p>
</div>
<div id="pca-cor-whitening" class="section level3">
<h3><span class="header-section-number">2.4.4</span> PCA-cor Whitening</h3>
<p><em>Aim</em>: same as for PCA whitening but remove scale in <span class="math inline">\(\boldsymbol x\)</span> first (i.e. use squared correlations rather squared covariances to measure compression)<br />
<em>Objective</em>: maximise <span class="math inline">\(\sum^d_{j=1}\text{Cor}(z_i, x_j)^2 = \sum^d_{i=1} \psi_{ij}^2\)</span> for all <span class="math inline">\(i\)</span><br />
<em>Equivalent objective</em>: maximise <span class="math inline">\((\psi_1,...,\psi_d)^T=\text{Diag}(\boldsymbol \Psi\boldsymbol \Psi^T)=\text{Diag}(\boldsymbol Q_2\boldsymbol P\boldsymbol Q_2^T) =\text{Diag}(( \boldsymbol Q_2 \boldsymbol G) \boldsymbol \Theta( \boldsymbol Q_2 \boldsymbol G)^T\)</span>
with <span class="math inline">\(\psi_1&gt;\psi_2&gt;\dots &gt;\psi_d\)</span>.</p>
<p><em>Optimal solution</em>: this is optimised for <span class="math inline">\(\boldsymbol Q_2 \boldsymbol G= \boldsymbol I\)</span> hence
<span class="math display">\[\boldsymbol Q_2^{\text{PCA-Cor}}=\boldsymbol G^T\]</span><br />
<span class="math display">\[\Longrightarrow \boldsymbol W^{\text{PCA-Cor}} = \boldsymbol \Theta^{-1/2} \boldsymbol G^T \boldsymbol V^{-1/2}\]</span></p>
<ul>
<li>Optimum value: <span class="math inline">\(\text{Diag}(\boldsymbol Q_2^{\text{PCA-Cor}}\boldsymbol P(\boldsymbol Q_2^{\text{PCA-Cor}})^T) = \text{Diag}(\boldsymbol \Theta) = (\theta_1, \ldots, \theta_d)^T\)</span>, i.e. the eigenvalues of <span class="math inline">\(\boldsymbol P\)</span> are equal to
the sum of the squared correlations from each latent component to all the original variables.</li>
<li>PCA-cor whitening is a whitening procedure that maximises compression with the sum of squared cross-correlation as optimality criterion</li>
<li>If <span class="math inline">\(\boldsymbol x\)</span> is standardised to <span class="math inline">\(\text{Var}(x_i)=1\)</span>, then PCA and PCA-cor are identical.</li>
</ul>
<p>Note that <span class="math inline">\(\sum_{j=1}^d \theta_j = \text{Tr}(\boldsymbol P) = d\)</span>. Therefore the fraction
<span class="math inline">\(\frac{\theta_i}{\sum^d_{j=1} \theta_j} = \frac{\theta_j}{d}\)</span>.</p>
<p>This can be used as a measure
of relative importance of each component in <span class="math inline">\(\boldsymbol z\)</span> to explain the original variables.
Thus, low ranking components in <span class="math inline">\(\boldsymbol z\)</span> with low total squared
correlation with <span class="math inline">\(\boldsymbol x\)</span> may be discarded, thus leading to a reduction in dimension.</p>
<p>Note that the individual correlations <span class="math inline">\(cor(z_i, x_j)\)</span> between each component in
<span class="math inline">\(\boldsymbol z\)</span> and the original variables in <span class="math inline">\(\boldsymbol x\)</span> are the correlation loadings (see section for PCA below).</p>
<p>As with PCA whitening for identifiability we need to impose a further constraint on the rotation matrix <span class="math inline">\(\boldsymbol Q_2\)</span>. A useful condition is
to assume a positive diagonal, i.e. <span class="math inline">\(\text{Diag}(\boldsymbol Q_2^{\text{PCA-Cor}}) &gt; 0\)</span> and thus
<span class="math inline">\(\text{Diag}(\boldsymbol G) &gt; 0\)</span> and <span class="math inline">\(\text{Diag}(\boldsymbol \Psi) &gt; 0\)</span>.</p>
</div>
<div id="cholesky-whitening" class="section level3">
<h3><span class="header-section-number">2.4.5</span> Cholesky Whitening</h3>
<p><em>Aim</em>: find a whitening transformation such that the cross-covariance and cross-correlation have triangular structure. This is useful in some models (such as time course data) to ensure that the
future cannot influence the past.</p>
<p><em>Solution</em>: Cholesky decomposition of <span class="math inline">\(\boldsymbol \Sigma^{-1} = \boldsymbol L\boldsymbol L^T\)</span></p>
<p><span class="math inline">\(\boldsymbol L\)</span> is a lower triangular matrix with positive diagonal elements<br />
<span class="math inline">\(\Longrightarrow \boldsymbol W^{\text{Chol}}=\boldsymbol L^T\)</span></p>
<p>By construction, <span class="math inline">\(\boldsymbol W^{\text{Chol}}\)</span> satisfies the whitening constraint since <span class="math inline">\((\boldsymbol W^{\text{Chol}})^T\boldsymbol W^{\text{Chol}} = \boldsymbol \Sigma^{-1}\)</span>.</p>
<p>The corresponding rotation matrices are:</p>
<ul>
<li><span class="math inline">\(\boldsymbol Q_1^{\text{Chol}} = \boldsymbol L^T \boldsymbol \Sigma^{1/2}\)</span> and</li>
<li><span class="math inline">\(\boldsymbol Q_2^{\text{Chol}} = \boldsymbol L^T \boldsymbol V^{1/2} \boldsymbol P^{1/2}\)</span></li>
</ul>
<p>This results in:</p>
<ul>
<li><span class="math inline">\(\boldsymbol \Phi^{\text{Chol}} = \boldsymbol L^T\boldsymbol \Sigma\)</span> and</li>
<li><span class="math inline">\(\boldsymbol \Psi^{\text{Chol}} = \boldsymbol L^T \boldsymbol \Sigma\boldsymbol V^{-1/2}\)</span></li>
</ul>
<p>Note that the cross-covariance matrix <span class="math inline">\(\boldsymbol \Phi^{\text{Chol}}\)</span> and
the cross-correlation matrix <span class="math inline">\(\boldsymbol \Psi^{\text{Chol}}\)</span> by construction are also
lower triangular matrices with positive diagonal elements!</p>
</div>
<div id="comparison-of-zca-pca-and-chol-whitening" class="section level3">
<h3><span class="header-section-number">2.4.6</span> Comparison of ZCA, PCA and Chol whitening</h3>
<p><img src="2-transformations_files/figure-html/fig1-1.png" width="672" /></p>
<p>In the above comparison you see ZCA, PCA and Cholesky whitening applied to a simulated bivariate normal data set with correlation <span class="math inline">\(\rho=0.8\)</span> (column 1). All approaches equally succeed in whitening (column 2) but they differ in the cross-correlations. Columns 3 and 4 shows the cross-correlations between the first two pairs corresponding components for ZCA, PCA and Cholesky whitening. As expected, in ZCA both components show strong correlation, but this is not the case for PCA and Cholesky whitening.</p>
</div>
<div id="recap" class="section level3">
<h3><span class="header-section-number">2.4.7</span> Recap</h3>
<table>
<colgroup>
<col width="31%" />
<col width="68%" />
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Type of usage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ZCA, ZCA-cor:</td>
<td>pure decorrelate, maintain similarity to original data set, interpretability</td>
</tr>
<tr class="even">
<td>PCA, PCA-cor:</td>
<td>compression, find effective dimension, reduce dimensionality, feature identification</td>
</tr>
<tr class="odd">
<td>Chol:</td>
<td>time course data</td>
</tr>
</tbody>
</table>
<p><strong>Related models not discussed in this course:</strong></p>
<ul>
<li><p>Factor models: essentially whitening plus an additional error term, factors have rotational
freedom just like in whitening</p></li>
<li><p>PLS: similar to PCA but in regression setting (with the choice of
latent variables depending on the response)</p></li>
</ul>
</div>
</div>
<div id="principal-component-analysis-pca" class="section level2">
<h2><span class="header-section-number">2.5</span> Principal Component Analysis (PCA)</h2>
<ul>
<li>Traditional PCA (invented 1901 by Pearson) is very closely related to <strong>PCA whitening</strong>.</li>
<li>But PCA itself is <strong>not</strong> a whitening procedure!</li>
</ul>
<p><strong>PCA transformation:</strong>
<span class="math display">\[\underbrace{\boldsymbol t}_{\text{Principal Components}} = \underbrace{\boldsymbol U^T\boldsymbol x}_{\text{Orthogonal projection}}\]</span>
<span class="math display">\[\Longrightarrow \text{Var}(\boldsymbol t) = \boldsymbol \Lambda= \begin{pmatrix} \lambda_1 &amp; \dots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \dots &amp; \lambda_d\end{pmatrix}\]</span></p>
<ul>
<li>Principal components are <strong>orthogonal</strong> but do <em>not</em> have unit variance!</li>
<li>The variances of the principal components are equal to the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span>: <span class="math inline">\(\text{Var}(\boldsymbol t^{\text{PCA}}) = \boldsymbol \Lambda\)</span>.</li>
<li>You arrive at PCA whitening by standardising the PCA components: <span class="math inline">\(\boldsymbol z^{\text{PCA}} = \boldsymbol \Lambda^{-1/2} \boldsymbol t^{\text{PCA}}\)</span></li>
<li>Same compression / optimality properties as PCA whitening.</li>
</ul>
<p>With principle components the fraction <span class="math inline">\(\frac{\lambda_i}{\sum^d_{j=1}\lambda_j}\)</span> can be interpreted as the proportion of variation contributed by
each component in <span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span> to the total variation.
Thus, low ranking components in <span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span> with low variation may be discarded, thus leading to a reduction in dimension.</p>
<div id="application-to-data" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Application to data</h3>
<p>Written in terms of a data matrix <span class="math inline">\(\boldsymbol X\)</span> instead of a random vector <span class="math inline">\(\boldsymbol x\)</span> PCA becomes:
<span class="math display">\[\underbrace{\boldsymbol T}_{\text{Sample version of principal components}}=\underbrace{\boldsymbol X}_{\text{Data matrix}}\boldsymbol U\]</span>
There are now two ways to obtain <span class="math inline">\(\boldsymbol U\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the covariance matrix, e.g. by <span class="math inline">\(\hat{\boldsymbol \Sigma} = \frac{1}{n}\boldsymbol X_c^T\boldsymbol X_c\)</span> where <span class="math inline">\(\boldsymbol X_c\)</span> is the column-centred data matrix; then apply the eigenvalue decomposition on <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> to get <span class="math inline">\(\boldsymbol U\)</span>.</p></li>
<li><p>Compute the singular value decomposition of <span class="math inline">\(\boldsymbol X_c = \boldsymbol V\boldsymbol D\boldsymbol U^T\)</span>. As <span class="math inline">\(\hat{\boldsymbol \Sigma} = \frac{1}{n}\boldsymbol X_c^T\boldsymbol X= \boldsymbol U(\frac{1}{n}\boldsymbol D^2)\boldsymbol U^T\)</span> you can just use <span class="math inline">\(\boldsymbol U\)</span> from the SVD of <span class="math inline">\(\boldsymbol X_c\)</span> and there is no need to compute the covariance.</p></li>
</ol>
</div>
</div>
<div id="pca-correlation-loadings-and-plot" class="section level2">
<h2><span class="header-section-number">2.6</span> PCA correlation loadings and plot</h2>
<p>A useful quantity to evaluate the PCA whitened components <span class="math inline">\(\boldsymbol z^{\text{PCA}}\)</span> and the related principle components <span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span> is the cross-correlation matrix <span class="math inline">\(\boldsymbol \Psi\)</span>. Specifically,
<span class="math inline">\(\boldsymbol \Psi= \text{Cor}(\boldsymbol z^{\text{PCA}}, \boldsymbol x) = \text{Cor}(\boldsymbol t^{\text{PCA}}, \boldsymbol x) = \boldsymbol \Lambda^{1/2} \boldsymbol U^T \boldsymbol V^{-1/2}\)</span>.</p>
<p>We now consider the back-transformation of the (standardised) PCA components to the standardised original components:</p>
<ul>
<li>The inverse PCA transformation is <span class="math inline">\(\boldsymbol x= \boldsymbol U\boldsymbol t^{\text{PCA}}\)</span> (note that <span class="math inline">\(\boldsymbol U^{-1} = \boldsymbol U^T\)</span>).</li>
<li>If one standardises <span class="math inline">\(\boldsymbol x\)</span> this equation becomes <span class="math inline">\(\boldsymbol V^{-1/2} \boldsymbol x= \boldsymbol V^{-1/2} \boldsymbol U\boldsymbol t^{\text{PCA}}\)</span>.</li>
<li>expressed in terms of the standardised <span class="math inline">\(\boldsymbol z^{\text{PCA}}\)</span> it becomes<br />
<span class="math inline">\(\boldsymbol V^{-1/2} \boldsymbol x= ( \boldsymbol V^{-1/2} \boldsymbol U\boldsymbol \Lambda^{1/2}) \boldsymbol \Lambda^{-1/2} \boldsymbol t^{\text{PCA}}\)</span>,
or equivalently,
<span class="math inline">\(\boldsymbol V^{-1/2} \boldsymbol x= \boldsymbol \Psi^T \boldsymbol z^{\text{PCA}}\)</span>.</li>
</ul>
<p>Thus the cross-correlation matrix <span class="math inline">\(\boldsymbol \Psi\)</span> plays the role of <em>correlation loadings</em>, i.e. they are the
coefficients linking the (standardised) PCA components with the standardised original components.</p>
<p>Recall that in a linear regression model with <em>uncorrelated predictors</em> the (squared) correlations between each predictor and the response can be used as measure of variable importance. Since PCA (whitened) components are uncorrelated by construction, the correlation loadings <span class="math inline">\(\boldsymbol \Psi\)</span>
measure the capability of each principal component to predict the original variables. Note that this is explicitly maximised in PCA-cor whitening — but not in PCA and in PCA whitening because those methods use squared covariance (rather than correlation) in the objective function.</p>
<p>For visualisation, a correlation loadings plot is often constructed as follows.
The loadings <span class="math inline">\(\boldsymbol \Psi\)</span> for the first two whitened PCA components <span class="math inline">\(z_1\)</span>, <span class="math inline">\(z_2\)</span> (or equivalently for
<span class="math inline">\(t_1, t_2\)</span>) to all original variables are computed. Then a point for each orginal
variable is drawn in a plane with the two correlation loadings acting as its coordinates. By construction, all points
have to lie within a unit circle around the origin . The orginal variables most strongly influenced
by the two latent variables will have strong correlation and thus lie near the outer circle, whereas
variables that are not influenced by the two latent variables will lie near the origin.
(see next section for an example).</p>
<div id="iris-data-example" class="section level3">
<h3><span class="header-section-number">2.6.1</span> Iris data example</h3>
<p>A plot of the the first two components after PCA-whitening is applied reveals the group structure among iris flowers:</p>
<p><img src="2-transformations_files/figure-html/fig2-1.png" width="364.8" /></p>
<p>Here is the corresponding loadings plot:</p>
<p><img src="2-transformations_files/figure-html/unnamed-chunk-1-1.png" width="384" /></p>
</div>
</div>
<div id="cca-whitening-canonical-correlation-analysis" class="section level2">
<h2><span class="header-section-number">2.7</span> CCA whitening (Canonical Correlation Analysis)</h2>
<p>So far, we have looked only into whitening as a <strong>single</strong> vector <span class="math inline">\(\boldsymbol x\)</span>. In CCA whitening we consider <strong>two vectors</strong> <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> simultaneously:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\boldsymbol x= \begin{pmatrix} x_1 \\ \vdots \\ x_p \end{pmatrix} \\
\text{Dimension } p
\end{array}
\begin{array}{ll}
\boldsymbol y= \begin{pmatrix} y_1 \\ \vdots \\ y_q \end{pmatrix} \\
\text{Dimension } q
\end{array}
\begin{array}{ll}
\text{Var}(\boldsymbol x) = \boldsymbol \Sigma_{\boldsymbol x} = \boldsymbol V_{\boldsymbol x}^{1/2}\boldsymbol P_{\boldsymbol x}\boldsymbol V_{\boldsymbol x}^{1/2} \\
\text{Var}(\boldsymbol y) = \boldsymbol \Sigma_{\boldsymbol y} = \boldsymbol V_{\boldsymbol y}^{1/2}\boldsymbol P_{\boldsymbol y}\boldsymbol V_{\boldsymbol y}^{1/2} \\
\end{array}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cc}
\text{Whitening of } \boldsymbol x\text{:} \\
\text{Whitening of } \boldsymbol y\text{:}
\end{array}
\begin{array}{cc}
\boldsymbol z_{\boldsymbol x} = \boldsymbol W_{\boldsymbol x}\boldsymbol x=\boldsymbol Q_{\boldsymbol x}\boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol V_{\boldsymbol x}^{-1/2}\boldsymbol x\\
\boldsymbol z_{\boldsymbol y} = \boldsymbol W_{\boldsymbol y}\boldsymbol y=\boldsymbol Q_{\boldsymbol y}\boldsymbol P_{\boldsymbol y}^{-1/2}\boldsymbol V_{\boldsymbol y}^{-1/2}\boldsymbol y
\end{array}
\end{align*}\]</span>
(note we use the correlation-based form of <span class="math inline">\(\boldsymbol W\)</span>)</p>
<p>Cross-correlation between <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span> and <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span>:</p>
<p><span class="math display">\[\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})=\boldsymbol Q_{\boldsymbol x}\boldsymbol K\boldsymbol Q_{\boldsymbol y}^T\]</span></p>
<p>with <span class="math inline">\(\boldsymbol K= \boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol P_{\boldsymbol x\boldsymbol y}\boldsymbol P_{\boldsymbol y}^{-1/2}\)</span>.</p>
<p><strong>Idea</strong>: we can choose a suitable <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}\)</span> rotation matrix by putting constraints on the cross-correlation.</p>
<p><strong>CCA</strong>: we aim for a <em>diagonal</em> <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> so that each component in <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span> only influences one (the corresponding) component in <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span>.</p>
<p><strong>Motivation</strong>: pairs of “modules” represented by components of <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span>
and <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span> influencing each other (and not anyone other module).</p>
<p><span class="math display">\[
\begin{array}{ll}
\boldsymbol z_{\boldsymbol x} = \begin{pmatrix} z^x_1 \\ z^x_2 \\ \vdots \\ z^x_p \end{pmatrix} &amp;
\boldsymbol z_{\boldsymbol y} = \begin{pmatrix} z^y_1 \\ z^y_2 \\ \vdots \\ z^y_q \end{pmatrix} \\
\end{array}
\]</span></p>
<p>\end{align}</p>
<p><span class="math display">\[\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y}) = \begin{pmatrix} d_1 &amp; \dots &amp; 0 \\ \vdots &amp;  \vdots \\ 0 &amp; \dots &amp; d_m \end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(d_i\)</span> are the <em>canonical correlations</em> and <span class="math inline">\(m=\min(p,q)\)</span>.</p>
<div id="how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal" class="section level3">
<h3><span class="header-section-number">2.7.1</span> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</h3>
<ul>
<li>Use Singular Value Decomposition (SVD) of matrix <span class="math inline">\(\boldsymbol K\)</span>:<br />
<span class="math display">\[\boldsymbol K= (\boldsymbol Q_{\boldsymbol x}^{\text{CCA}})^T  \boldsymbol D\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\]</span>
where <span class="math inline">\(\boldsymbol D\)</span> is the diagonal matrix containing the singular values of <span class="math inline">\(\boldsymbol K\)</span></li>
<li>This yields rotation matrices <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> and thus the desired whitened matrices <span class="math inline">\(\boldsymbol W_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol W_{\boldsymbol y}^{\text{CCA}}\)</span></li>
<li>As a result <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y}) = \boldsymbol D\)</span> i.e. singular values of <span class="math inline">\(\boldsymbol K\)</span> are identical to canonical correlations <span class="math inline">\(d_i\)</span>!</li>
</ul>
<p><span class="math inline">\(\longrightarrow\)</span> <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> are determined by the diagonality constraint (and are different to the other previously discussed whitening methods).</p>
<p>Note that the signs of corresponding in columns in <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> are not identified. Traditionally, in an SVD the
signs are chosen such that the singular values are positive. However, if we
impose positive-diagonality on <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span>,
and thus positive-diagonality on the cross-correlations <span class="math inline">\(\boldsymbol \Psi_{\boldsymbol x}\)</span> and
<span class="math inline">\(\boldsymbol \Psi_{\boldsymbol y}\)</span>, then the canonical correlations may take on both positive and
negative values.</p>
</div>
<div id="related-methods" class="section level3">
<h3><span class="header-section-number">2.7.2</span> Related methods</h3>
<ul>
<li><p>O2PLS: similar to CCA but using orthogonal projections
(thus in O2PLS the latent variables underlying <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are not orthogonal)</p></li>
<li><p>Vector correlation: aggregates the squared canonical correlations into a single overall measure
of association between two random vectors <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> (see Chapter 5
on multivariate dependencies).</p></li>
</ul>

<p></p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>P. Gorroochurn. 2020. Who Invented the Delta Method, Really? The Mathematical Intelligencer <strong>42</strong>:46–49. <a href="https://doi.org/10.1007/s00283-020-09982-0" class="uri">https://doi.org/10.1007/s00283-020-09982-0</a><a href="2-transformations.html#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-multivariate.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-clustering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math38161-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
