<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Transformations and dimension reduction | HTML</title>
  <meta name="description" content="Multivariate Statistics and Machine Learning" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Transformations and dimension reduction | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Transformations and dimension reduction | HTML" />
  
  
  



<meta name="date" content="2020-11-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-multivariate.html"/>
<link rel="next" href="3-clustering.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH38161/index.html">MATH38161 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-multivariate.html"><a href="1-multivariate.html"><i class="fa fa-check"></i><b>1</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="1.1" data-path="1-multivariate.html"><a href="1-multivariate.html#why-multivariate-statistics"><i class="fa fa-check"></i><b>1.1</b> Why multivariate statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="1-multivariate.html"><a href="1-multivariate.html#essentials-in-multivariate-statistics"><i class="fa fa-check"></i><b>1.2</b> Essentials in multivariate statistics</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-vs.multivariate-random-variables"><i class="fa fa-check"></i><b>1.2.1</b> Univariate vs. multivariate random variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-multivariate.html"><a href="1-multivariate.html#mean-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.2</b> Mean of a random vector</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-multivariate.html"><a href="1-multivariate.html#variance-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.3</b> Variance of a random vector</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-multivariate.html"><a href="1-multivariate.html#properties-of-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.4</b> Properties of the covariance matrix</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-multivariate.html"><a href="1-multivariate.html#eigenvalue-decomposition-of-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.2.5</b> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.2.6" data-path="1-multivariate.html"><a href="1-multivariate.html#quantities-related-to-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.6</b> Quantities related to the covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>1.3</b> Multivariate normal distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-normal-distribution"><i class="fa fa-check"></i><b>1.3.1</b> Univariate normal distribution:</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-model"><i class="fa fa-check"></i><b>1.3.2</b> Multivariate normal model</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-multivariate.html"><a href="1-multivariate.html#shape-of-the-contour-lines-of-the-multivariate-normal-density"><i class="fa fa-check"></i><b>1.3.3</b> Shape of the contour lines of the multivariate normal density</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-in-large-sample-and-small-sample-settings"><i class="fa fa-check"></i><b>1.4</b> Estimation in large sample and small sample settings</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-data"><i class="fa fa-check"></i><b>1.4.1</b> Multivariate data</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-multivariate.html"><a href="1-multivariate.html#strategies-for-large-sample-estimation"><i class="fa fa-check"></i><b>1.4.2</b> Strategies for large sample estimation</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-multivariate.html"><a href="1-multivariate.html#large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.4.3</b> Large sample estimates of mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.4.4" data-path="1-multivariate.html"><a href="1-multivariate.html#problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions"><i class="fa fa-check"></i><b>1.4.4</b> Problems with maximum likelihood in small sample settings and high dimensions</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-of-covariance-matrix-in-small-sample-settings"><i class="fa fa-check"></i><b>1.4.5</b> Estimation of covariance matrix in small sample settings</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-multivariate.html"><a href="1-multivariate.html#discrete-multivariate-distributions"><i class="fa fa-check"></i><b>1.5</b> Discrete multivariate distributions</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-multivariate.html"><a href="1-multivariate.html#categorical-distribution"><i class="fa fa-check"></i><b>1.5.1</b> Categorical distribution</a></li>
<li class="chapter" data-level="1.5.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multinomial-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Multinomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="1-multivariate.html"><a href="1-multivariate.html#continuous-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Continuous multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-multivariate.html"><a href="1-multivariate.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.6.1</b> Dirichlet distribution</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-multivariate.html"><a href="1-multivariate.html#wishart-distribution"><i class="fa fa-check"></i><b>1.6.2</b> Wishart distribution</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-multivariate.html"><a href="1-multivariate.html#inverse-wishart-distribution"><i class="fa fa-check"></i><b>1.6.3</b> Inverse Wishart distribution</a></li>
<li class="chapter" data-level="1.6.4" data-path="1-multivariate.html"><a href="1-multivariate.html#further-distributions"><i class="fa fa-check"></i><b>1.6.4</b> Further distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-transformations.html"><a href="2-transformations.html"><i class="fa fa-check"></i><b>2</b> Transformations and dimension reduction</a><ul>
<li class="chapter" data-level="2.1" data-path="2-transformations.html"><a href="2-transformations.html#linear-transformations"><i class="fa fa-check"></i><b>2.1</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-transformations.html"><a href="2-transformations.html#location-scale-transformation"><i class="fa fa-check"></i><b>2.1.1</b> Location-scale transformation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-transformations.html"><a href="2-transformations.html#invertible-location-scale-transformation"><i class="fa fa-check"></i><b>2.1.2</b> Invertible location-scale transformation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-transformations.html"><a href="2-transformations.html#nonlinear-transformations"><i class="fa fa-check"></i><b>2.2</b> Nonlinear transformations</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-transformations.html"><a href="2-transformations.html#general-transformation"><i class="fa fa-check"></i><b>2.2.1</b> General transformation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-transformations.html"><a href="2-transformations.html#delta-method"><i class="fa fa-check"></i><b>2.2.2</b> Delta method</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-transformations.html"><a href="2-transformations.html#transformation-of-densities-under-general-invertible-transformation"><i class="fa fa-check"></i><b>2.2.3</b> Transformation of densities under general invertible transformation</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-transformations.html"><a href="2-transformations.html#normalising-flows"><i class="fa fa-check"></i><b>2.2.4</b> Normalising flows</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-transformations.html"><a href="2-transformations.html#whitening-transformations"><i class="fa fa-check"></i><b>2.3</b> Whitening transformations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-transformations.html"><a href="2-transformations.html#overview"><i class="fa fa-check"></i><b>2.3.1</b> Overview</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-transformations.html"><a href="2-transformations.html#general-whitening-transformation"><i class="fa fa-check"></i><b>2.3.2</b> General whitening transformation</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-transformations.html"><a href="2-transformations.html#solution-of-whitening-constraint-covariance-based"><i class="fa fa-check"></i><b>2.3.3</b> Solution of whitening constraint (covariance-based)</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-transformations.html"><a href="2-transformations.html#another-solution-correlation-based"><i class="fa fa-check"></i><b>2.3.4</b> Another solution (correlation-based)</a></li>
<li class="chapter" data-level="2.3.5" data-path="2-transformations.html"><a href="2-transformations.html#cross-covariance-and-cross-correlation"><i class="fa fa-check"></i><b>2.3.5</b> Cross-covariance and cross-correlation</a></li>
<li class="chapter" data-level="2.3.6" data-path="2-transformations.html"><a href="2-transformations.html#inverse-whitening-transformation-loadings-and-multiple-correlation"><i class="fa fa-check"></i><b>2.3.6</b> Inverse whitening transformation, loadings, and multiple correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-transformations.html"><a href="2-transformations.html#natural-whitening-procedures"><i class="fa fa-check"></i><b>2.4</b> Natural whitening procedures</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-transformations.html"><a href="2-transformations.html#zca-whitening"><i class="fa fa-check"></i><b>2.4.1</b> ZCA whitening</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-transformations.html"><a href="2-transformations.html#zca-cor-whitening"><i class="fa fa-check"></i><b>2.4.2</b> ZCA-Cor whitening</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-transformations.html"><a href="2-transformations.html#pca-whitening"><i class="fa fa-check"></i><b>2.4.3</b> PCA whitening</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-transformations.html"><a href="2-transformations.html#pca-cor-whitening"><i class="fa fa-check"></i><b>2.4.4</b> PCA-cor whitening</a></li>
<li class="chapter" data-level="2.4.5" data-path="2-transformations.html"><a href="2-transformations.html#cholesky-whitening"><i class="fa fa-check"></i><b>2.4.5</b> Cholesky whitening</a></li>
<li class="chapter" data-level="2.4.6" data-path="2-transformations.html"><a href="2-transformations.html#comparison-of-zca-pca-and-cholesky-whitening"><i class="fa fa-check"></i><b>2.4.6</b> Comparison of ZCA, PCA and Cholesky whitening</a></li>
<li class="chapter" data-level="2.4.7" data-path="2-transformations.html"><a href="2-transformations.html#recap"><i class="fa fa-check"></i><b>2.4.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-transformations.html"><a href="2-transformations.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>2.5</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-transformations.html"><a href="2-transformations.html#pca-transformation"><i class="fa fa-check"></i><b>2.5.1</b> PCA transformation</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-transformations.html"><a href="2-transformations.html#application-to-data"><i class="fa fa-check"></i><b>2.5.2</b> Application to data</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-transformations.html"><a href="2-transformations.html#iris-data-example"><i class="fa fa-check"></i><b>2.5.3</b> Iris data example</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-transformations.html"><a href="2-transformations.html#correlation-loadings-plot-to-interpret-pca-components"><i class="fa fa-check"></i><b>2.6</b> Correlation loadings plot to interpret PCA components</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-transformations.html"><a href="2-transformations.html#pca-correlation-loadings"><i class="fa fa-check"></i><b>2.6.1</b> PCA correlation loadings</a></li>
<li class="chapter" data-level="2.6.2" data-path="2-transformations.html"><a href="2-transformations.html#pca-correlation-loadings-plot"><i class="fa fa-check"></i><b>2.6.2</b> PCA correlation loadings plot</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-transformations.html"><a href="2-transformations.html#cca-whitening-canonical-correlation-analysis"><i class="fa fa-check"></i><b>2.7</b> CCA whitening (Canonical Correlation Analysis)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-transformations.html"><a href="2-transformations.html#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal"><i class="fa fa-check"></i><b>2.7.1</b> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</a></li>
<li class="chapter" data-level="2.7.2" data-path="2-transformations.html"><a href="2-transformations.html#related-methods"><i class="fa fa-check"></i><b>2.7.2</b> Related methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-clustering.html"><a href="3-clustering.html"><i class="fa fa-check"></i><b>3</b> Unsupervised learning and clustering</a><ul>
<li class="chapter" data-level="3.1" data-path="3-clustering.html"><a href="3-clustering.html#challenges-in-supervised-learning"><i class="fa fa-check"></i><b>3.1</b> Challenges in supervised learning</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-clustering.html"><a href="3-clustering.html#objective"><i class="fa fa-check"></i><b>3.1.1</b> Objective</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-clustering.html"><a href="3-clustering.html#questions-and-problems"><i class="fa fa-check"></i><b>3.1.2</b> Questions and problems</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-clustering.html"><a href="3-clustering.html#why-is-clustering-difficult"><i class="fa fa-check"></i><b>3.1.3</b> Why is clustering difficult?</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-clustering.html"><a href="3-clustering.html#common-types-of-clustering-methods"><i class="fa fa-check"></i><b>3.1.4</b> Common types of clustering methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-clustering.html"><a href="3-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>3.2</b> Hierarchical clustering</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-clustering.html"><a href="3-clustering.html#tree-like-structures"><i class="fa fa-check"></i><b>3.2.1</b> Tree-like structures</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-clustering.html"><a href="3-clustering.html#agglomerative-hierarchical-clustering-algorithms"><i class="fa fa-check"></i><b>3.2.2</b> Agglomerative hierarchical clustering algorithms</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-clustering.html"><a href="3-clustering.html#wards-clustering-method"><i class="fa fa-check"></i><b>3.2.3</b> Ward’s clustering method</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-clustering.html"><a href="3-clustering.html#application-to-swiss-banknote-data-set"><i class="fa fa-check"></i><b>3.2.4</b> Application to Swiss banknote data set</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-clustering.html"><a href="3-clustering.html#assessment-of-the-uncertainty-of-hierarchical-clusterings"><i class="fa fa-check"></i><b>3.2.5</b> Assessment of the uncertainty of hierarchical clusterings</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-clustering.html"><a href="3-clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>3.3</b> <span class="math inline">\(K\)</span>-means clustering</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-clustering.html"><a href="3-clustering.html#general-aims"><i class="fa fa-check"></i><b>3.3.1</b> General aims</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-clustering.html"><a href="3-clustering.html#algorithm"><i class="fa fa-check"></i><b>3.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-clustering.html"><a href="3-clustering.html#properties"><i class="fa fa-check"></i><b>3.3.3</b> Properties</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>3.3.4</b> Choosing the number of clusters</a></li>
<li class="chapter" data-level="3.3.5" data-path="3-clustering.html"><a href="3-clustering.html#k-medoids-aka-pam"><i class="fa fa-check"></i><b>3.3.5</b> <span class="math inline">\(K\)</span>-medoids aka PAM</a></li>
<li class="chapter" data-level="3.3.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-k-means-to-iris-data"><i class="fa fa-check"></i><b>3.3.6</b> Application of <span class="math inline">\(K\)</span>-means to Iris data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-clustering.html"><a href="3-clustering.html#mixture-models"><i class="fa fa-check"></i><b>3.4</b> Mixture models</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-clustering.html"><a href="3-clustering.html#finite-mixture-model"><i class="fa fa-check"></i><b>3.4.1</b> Finite mixture model</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-clustering.html"><a href="3-clustering.html#total-variance-and-variation-of-mixture-model"><i class="fa fa-check"></i><b>3.4.2</b> Total variance and variation of mixture model</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-clustering.html"><a href="3-clustering.html#example-of-mixtures"><i class="fa fa-check"></i><b>3.4.3</b> Example of mixtures</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-clustering.html"><a href="3-clustering.html#generative-view-sampling-from-a-mixture-model"><i class="fa fa-check"></i><b>3.4.4</b> Generative view: sampling from a mixture model</a></li>
<li class="chapter" data-level="3.4.5" data-path="3-clustering.html"><a href="3-clustering.html#predicting-the-group-allocation-of-a-given-sample"><i class="fa fa-check"></i><b>3.4.5</b> Predicting the group allocation of a given sample</a></li>
<li class="chapter" data-level="3.4.6" data-path="3-clustering.html"><a href="3-clustering.html#variation-1-infinite-mixture-model"><i class="fa fa-check"></i><b>3.4.6</b> Variation 1: Infinite mixture model</a></li>
<li class="chapter" data-level="3.4.7" data-path="3-clustering.html"><a href="3-clustering.html#variation-2-semiparametric-mixture-model-with-two-classes"><i class="fa fa-check"></i><b>3.4.7</b> Variation 2: Semiparametric mixture model with two classes</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-clustering.html"><a href="3-clustering.html#fitting-mixture-models-to-data"><i class="fa fa-check"></i><b>3.5</b> Fitting mixture models to data</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-clustering.html"><a href="3-clustering.html#direct-estimation-of-mixture-model-parameters"><i class="fa fa-check"></i><b>3.5.1</b> Direct estimation of mixture model parameters</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-clustering.html"><a href="3-clustering.html#estimating-mixture-model-parameters-using-the-em-algorithm"><i class="fa fa-check"></i><b>3.5.2</b> Estimating mixture model parameters using the EM algorithm</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-clustering.html"><a href="3-clustering.html#em-algorithm-for-multivariate-normal-mixture-model"><i class="fa fa-check"></i><b>3.5.3</b> EM algorithm for multivariate normal mixture model</a></li>
<li class="chapter" data-level="3.5.4" data-path="3-clustering.html"><a href="3-clustering.html#connection-with-k-means-clustering-method"><i class="fa fa-check"></i><b>3.5.4</b> Connection with <span class="math inline">\(K\)</span>-means clustering method</a></li>
<li class="chapter" data-level="3.5.5" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-classes"><i class="fa fa-check"></i><b>3.5.5</b> Choosing the number of classes</a></li>
<li class="chapter" data-level="3.5.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-gmms-to-iris-flower-data"><i class="fa fa-check"></i><b>3.5.6</b> Application of GMMs to Iris flower data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-classification.html"><a href="4-classification.html"><i class="fa fa-check"></i><b>4</b> Supervised learning and classification</a><ul>
<li class="chapter" data-level="4.1" data-path="4-classification.html"><a href="4-classification.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-classification.html"><a href="4-classification.html#supervised-learning-vs.unsupervised-learning"><i class="fa fa-check"></i><b>4.1.1</b> Supervised learning vs. unsupervised learning</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-classification.html"><a href="4-classification.html#terminology"><i class="fa fa-check"></i><b>4.1.2</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-classification.html"><a href="4-classification.html#bayesian-discriminant-rule-or-bayes-classifier"><i class="fa fa-check"></i><b>4.2</b> Bayesian discriminant rule or Bayes classifier</a></li>
<li class="chapter" data-level="4.3" data-path="4-classification.html"><a href="4-classification.html#normal-bayes-classifier"><i class="fa fa-check"></i><b>4.3</b> Normal Bayes classifier</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-classification.html"><a href="4-classification.html#quadratic-discriminant-analysis-qda-and-gaussian-assumption"><i class="fa fa-check"></i><b>4.3.1</b> Quadratic discriminant analysis (QDA) and Gaussian assumption</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-classification.html"><a href="4-classification.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>4.3.2</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-classification.html"><a href="4-classification.html#diagonal-discriminant-analysis-dda"><i class="fa fa-check"></i><b>4.3.3</b> Diagonal discriminant analysis (DDA)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-classification.html"><a href="4-classification.html#the-training-step-learning-qda-lda-and-dda-classifiers-from-data"><i class="fa fa-check"></i><b>4.4</b> The training step — learning QDA, LDA and DDA classifiers from data</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-classification.html"><a href="4-classification.html#number-of-model-parameters"><i class="fa fa-check"></i><b>4.4.1</b> Number of model parameters</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-classification.html"><a href="4-classification.html#estimating-the-discriminant-predictor-function"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the discriminant / predictor function</a></li>
<li class="chapter" data-level="4.4.3" data-path="4-classification.html"><a href="4-classification.html#comparison-of-estimated-decision-boundaries-lda-vs.qda"><i class="fa fa-check"></i><b>4.4.3</b> Comparison of estimated decision boundaries: LDA vs. QDA</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-classification.html"><a href="4-classification.html#goodness-of-fit-and-variable-ranking"><i class="fa fa-check"></i><b>4.5</b> Goodness of fit and variable ranking</a><ul>
<li class="chapter" data-level="4.5.1" data-path="4-classification.html"><a href="4-classification.html#lda-with-k2-classes"><i class="fa fa-check"></i><b>4.5.1</b> LDA with <span class="math inline">\(K=2\)</span> classes</a></li>
<li class="chapter" data-level="4.5.2" data-path="4-classification.html"><a href="4-classification.html#multiple-classes"><i class="fa fa-check"></i><b>4.5.2</b> Multiple classes</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4-classification.html"><a href="4-classification.html#variable-selection-and-cross-validation"><i class="fa fa-check"></i><b>4.6</b> Variable selection and cross-validation</a><ul>
<li class="chapter" data-level="4.6.1" data-path="4-classification.html"><a href="4-classification.html#choosing-a-threshold-by-multiple-testing-using-false-discovery-rates"><i class="fa fa-check"></i><b>4.6.1</b> Choosing a threshold by multiple testing using false discovery rates</a></li>
<li class="chapter" data-level="4.6.2" data-path="4-classification.html"><a href="4-classification.html#quantifying-prediction-error"><i class="fa fa-check"></i><b>4.6.2</b> Quantifying prediction error</a></li>
<li class="chapter" data-level="4.6.3" data-path="4-classification.html"><a href="4-classification.html#estimation-of-prediction-error-without-validation-data-using-cross-validation"><i class="fa fa-check"></i><b>4.6.3</b> Estimation of prediction error without validation data using cross-validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-dependence.html"><a href="5-dependence.html"><i class="fa fa-check"></i><b>5</b> Multivariate dependencies</a><ul>
<li class="chapter" data-level="5.1" data-path="5-dependence.html"><a href="5-dependence.html#measuring-the-linear-association-between-two-sets-of-random-variables"><i class="fa fa-check"></i><b>5.1</b> Measuring the linear association between two sets of random variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-dependence.html"><a href="5-dependence.html#outline"><i class="fa fa-check"></i><b>5.1.1</b> Outline</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-dependence.html"><a href="5-dependence.html#special-cases"><i class="fa fa-check"></i><b>5.1.2</b> Special cases</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-dependence.html"><a href="5-dependence.html#rozeboom-vector-correlation"><i class="fa fa-check"></i><b>5.1.3</b> Rozeboom vector correlation</a></li>
<li class="chapter" data-level="5.1.4" data-path="5-dependence.html"><a href="5-dependence.html#rv-coefficient"><i class="fa fa-check"></i><b>5.1.4</b> RV coefficient</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-as-generalisation-of-correlation"><i class="fa fa-check"></i><b>5.2</b> Mutual information as generalisation of correlation</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-dependence.html"><a href="5-dependence.html#definition-of-mutual-information"><i class="fa fa-check"></i><b>5.2.1</b> Definition of mutual information</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normal-variables"><i class="fa fa-check"></i><b>5.2.2</b> Mutual information between two normal variables</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normally-distributed-random-vectors"><i class="fa fa-check"></i><b>5.2.3</b> Mutual information between two normally distributed random vectors</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-dependence.html"><a href="5-dependence.html#using-mi-for-variable-selection"><i class="fa fa-check"></i><b>5.2.4</b> Using MI for variable selection</a></li>
<li class="chapter" data-level="5.2.5" data-path="5-dependence.html"><a href="5-dependence.html#other-measures-of-general-dependence"><i class="fa fa-check"></i><b>5.2.5</b> Other measures of general dependence</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-dependence.html"><a href="5-dependence.html#graphical-models"><i class="fa fa-check"></i><b>5.3</b> Graphical models</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-dependence.html"><a href="5-dependence.html#purpose"><i class="fa fa-check"></i><b>5.3.1</b> Purpose</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-dependence.html"><a href="5-dependence.html#basic-notions-from-graph-theory"><i class="fa fa-check"></i><b>5.3.2</b> Basic notions from graph theory</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-dependence.html"><a href="5-dependence.html#probabilistic-graphical-models"><i class="fa fa-check"></i><b>5.3.3</b> Probabilistic graphical models</a></li>
<li class="chapter" data-level="5.3.4" data-path="5-dependence.html"><a href="5-dependence.html#directed-graphical-models"><i class="fa fa-check"></i><b>5.3.4</b> Directed graphical models</a></li>
<li class="chapter" data-level="5.3.5" data-path="5-dependence.html"><a href="5-dependence.html#undirected-graphical-models"><i class="fa fa-check"></i><b>5.3.5</b> Undirected graphical models</a></li>
<li class="chapter" data-level="5.3.6" data-path="5-dependence.html"><a href="5-dependence.html#algorithm-for-learning-ggms"><i class="fa fa-check"></i><b>5.3.6</b> Algorithm for learning GGMs</a></li>
<li class="chapter" data-level="5.3.7" data-path="5-dependence.html"><a href="5-dependence.html#example-exam-score-data-mardia-et-al-1979"><i class="fa fa-check"></i><b>5.3.7</b> Example: exam score data (Mardia et al 1979:)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-nonlinear.html"><a href="6-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Nonlinear and nonparametric models</a><ul>
<li class="chapter" data-level="6.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#limits-of-linear-models-and-correlation"><i class="fa fa-check"></i><b>6.1</b> Limits of linear models and correlation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#correlation-only-measures-linear-dependence"><i class="fa fa-check"></i><b>6.1.1</b> Correlation only measures linear dependence</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#anscombe-data-sets"><i class="fa fa-check"></i><b>6.1.2</b> Anscombe data sets</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests"><i class="fa fa-check"></i><b>6.2</b> Random forests</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#stochastic-vs.algorithmic-models"><i class="fa fa-check"></i><b>6.2.1</b> Stochastic vs. algorithmic models</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests-1"><i class="fa fa-check"></i><b>6.2.2</b> Random forests</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#comparison-of-decision-boundaries-decision-tree-vs.random-forest"><i class="fa fa-check"></i><b>6.2.3</b> Comparison of decision boundaries: decision tree vs. random forest</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gaussian-processes"><i class="fa fa-check"></i><b>6.3</b> Gaussian processes</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#main-concepts"><i class="fa fa-check"></i><b>6.3.1</b> Main concepts</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#technical-background"><i class="fa fa-check"></i><b>6.3.2</b> Technical background:</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#covariance-functions-and-kernel"><i class="fa fa-check"></i><b>6.3.3</b> Covariance functions and kernel</a></li>
<li class="chapter" data-level="6.3.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gp-model"><i class="fa fa-check"></i><b>6.3.4</b> GP model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks"><i class="fa fa-check"></i><b>6.4</b> Neural networks</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#history"><i class="fa fa-check"></i><b>6.4.1</b> History</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks-1"><i class="fa fa-check"></i><b>6.4.2</b> Neural networks</a></li>
<li class="chapter" data-level="6.4.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#learning-more-about-deep-learning"><i class="fa fa-check"></i><b>6.4.3</b> Learning more about deep learning</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="7-matrices.html"><a href="7-matrices.html"><i class="fa fa-check"></i><b>A</b> Brief refresher on matrices</a><ul>
<li class="chapter" data-level="A.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-basics"><i class="fa fa-check"></i><b>A.1</b> Matrix basics</a><ul>
<li class="chapter" data-level="A.1.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-notation"><i class="fa fa-check"></i><b>A.1.1</b> Matrix notation</a></li>
<li class="chapter" data-level="A.1.2" data-path="7-matrices.html"><a href="7-matrices.html#random-matrix"><i class="fa fa-check"></i><b>A.1.2</b> Random matrix</a></li>
<li class="chapter" data-level="A.1.3" data-path="7-matrices.html"><a href="7-matrices.html#special-matrices"><i class="fa fa-check"></i><b>A.1.3</b> Special matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="7-matrices.html"><a href="7-matrices.html#simple-matrix-operations"><i class="fa fa-check"></i><b>A.2</b> Simple matrix operations</a><ul>
<li class="chapter" data-level="A.2.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-addition-and-multiplication"><i class="fa fa-check"></i><b>A.2.1</b> Matrix addition and multiplication</a></li>
<li class="chapter" data-level="A.2.2" data-path="7-matrices.html"><a href="7-matrices.html#matrix-transpose"><i class="fa fa-check"></i><b>A.2.2</b> Matrix transpose</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="7-matrices.html"><a href="7-matrices.html#matrix-summaries"><i class="fa fa-check"></i><b>A.3</b> Matrix summaries</a><ul>
<li class="chapter" data-level="A.3.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-trace"><i class="fa fa-check"></i><b>A.3.1</b> Matrix trace</a></li>
<li class="chapter" data-level="A.3.2" data-path="7-matrices.html"><a href="7-matrices.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>A.3.2</b> Determinant of a matrix</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="7-matrices.html"><a href="7-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.4</b> Matrix inverse</a><ul>
<li class="chapter" data-level="A.4.1" data-path="7-matrices.html"><a href="7-matrices.html#inversion-of-square-matrix"><i class="fa fa-check"></i><b>A.4.1</b> Inversion of square matrix</a></li>
<li class="chapter" data-level="A.4.2" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.4.2</b> Orthogonal matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>A.5</b> Eigenvalues and eigenvectors</a><ul>
<li class="chapter" data-level="A.5.1" data-path="7-matrices.html"><a href="7-matrices.html#definition"><i class="fa fa-check"></i><b>A.5.1</b> Definition</a></li>
<li class="chapter" data-level="A.5.2" data-path="7-matrices.html"><a href="7-matrices.html#finding-eigenvalues-and-vectors"><i class="fa fa-check"></i><b>A.5.2</b> Finding eigenvalues and vectors</a></li>
<li class="chapter" data-level="A.5.3" data-path="7-matrices.html"><a href="7-matrices.html#eigenequation-in-matrix-notation"><i class="fa fa-check"></i><b>A.5.3</b> Eigenequation in matrix notation</a></li>
<li class="chapter" data-level="A.5.4" data-path="7-matrices.html"><a href="7-matrices.html#defective-matrix"><i class="fa fa-check"></i><b>A.5.4</b> Defective matrix</a></li>
<li class="chapter" data-level="A.5.5" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-of-a-diagonal-or-triangular-matrix"><i class="fa fa-check"></i><b>A.5.5</b> Eigenvalues of a diagonal or triangular matrix</a></li>
<li class="chapter" data-level="A.5.6" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-vectors-of-a-symmetric-matrix"><i class="fa fa-check"></i><b>A.5.6</b> Eigenvalues and vectors of a symmetric matrix</a></li>
<li class="chapter" data-level="A.5.7" data-path="7-matrices.html"><a href="7-matrices.html#positive-definite-matrices"><i class="fa fa-check"></i><b>A.5.7</b> Positive definite matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.6" data-path="7-matrices.html"><a href="7-matrices.html#matrix-decompositions"><i class="fa fa-check"></i><b>A.6</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="A.6.1" data-path="7-matrices.html"><a href="7-matrices.html#diagonalisation-and-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.6.1</b> Diagonalisation and eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.6.2" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.6.2</b> Orthogonal eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.6.3" data-path="7-matrices.html"><a href="7-matrices.html#singular-value-decomposition"><i class="fa fa-check"></i><b>A.6.3</b> Singular value decomposition</a></li>
<li class="chapter" data-level="A.6.4" data-path="7-matrices.html"><a href="7-matrices.html#polar-decomposition"><i class="fa fa-check"></i><b>A.6.4</b> Polar decomposition</a></li>
<li class="chapter" data-level="A.6.5" data-path="7-matrices.html"><a href="7-matrices.html#cholesky-decomposition"><i class="fa fa-check"></i><b>A.6.5</b> Cholesky decomposition</a></li>
</ul></li>
<li class="chapter" data-level="A.7" data-path="7-matrices.html"><a href="7-matrices.html#matrix-summaries-based-on-eigenvalues-and-singular-values"><i class="fa fa-check"></i><b>A.7</b> Matrix summaries based on eigenvalues and singular values</a><ul>
<li class="chapter" data-level="A.7.1" data-path="7-matrices.html"><a href="7-matrices.html#trace-and-determinant-computed-from-eigenvalues"><i class="fa fa-check"></i><b>A.7.1</b> Trace and determinant computed from eigenvalues</a></li>
<li class="chapter" data-level="A.7.2" data-path="7-matrices.html"><a href="7-matrices.html#rank-and-condition-number"><i class="fa fa-check"></i><b>A.7.2</b> Rank and condition number</a></li>
</ul></li>
<li class="chapter" data-level="A.8" data-path="7-matrices.html"><a href="7-matrices.html#functions-of-symmetric-matrices"><i class="fa fa-check"></i><b>A.8</b> Functions of symmetric matrices</a><ul>
<li class="chapter" data-level="A.8.1" data-path="7-matrices.html"><a href="7-matrices.html#definition-of-a-matrix-function"><i class="fa fa-check"></i><b>A.8.1</b> Definition of a matrix function</a></li>
<li class="chapter" data-level="A.8.2" data-path="7-matrices.html"><a href="7-matrices.html#identities-for-the-matrix-exponential-and-logarithm"><i class="fa fa-check"></i><b>A.8.2</b> Identities for the matrix exponential and logarithm</a></li>
</ul></li>
<li class="chapter" data-level="A.9" data-path="7-matrices.html"><a href="7-matrices.html#matrix-calculus"><i class="fa fa-check"></i><b>A.9</b> Matrix calculus</a><ul>
<li class="chapter" data-level="A.9.1" data-path="7-matrices.html"><a href="7-matrices.html#first-order-vector-derivatives"><i class="fa fa-check"></i><b>A.9.1</b> First order vector derivatives</a></li>
<li class="chapter" data-level="A.9.2" data-path="7-matrices.html"><a href="7-matrices.html#second-order-vector-derivatives"><i class="fa fa-check"></i><b>A.9.2</b> Second order vector derivatives</a></li>
<li class="chapter" data-level="A.9.3" data-path="7-matrices.html"><a href="7-matrices.html#first-order-matrix-derivatives"><i class="fa fa-check"></i><b>A.9.3</b> First order matrix derivatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="8-further-study.html"><a href="8-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="8-further-study.html"><a href="8-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="8-further-study.html"><a href="8-further-study.html#advanced-reading"><i class="fa fa-check"></i><b>B.2</b> Advanced reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="9-references.html"><a href="9-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Multivariate Statistics and Machine Learning</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="transformations-and-dimension-reduction" class="section level1">
<h1><span class="header-section-number">2</span> Transformations and dimension reduction</h1>
<p>Motivation:
In the following we study transformations of random vectors and their distributions.
These transformation are very important
since they either transform simple distributions into more complex distributions or allow to simplify
complex models. In machine learning invertible mappings of transformations
for probability distributions are known as “normalising flows” (these play a key role
e.g. in neural networks).</p>
<div id="linear-transformations" class="section level2">
<h2><span class="header-section-number">2.1</span> Linear Transformations</h2>
<div id="location-scale-transformation" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Location-scale transformation</h3>
<p>Also known as affine transformation.</p>
<p><span class="math display">\[\boldsymbol y= \underbrace{\boldsymbol a}_{\text{location parameter}}+\underbrace{\boldsymbol B}_{\text{scale parameter}} \boldsymbol x\space\]</span>
<span class="math display">\[\boldsymbol y: m \times 1 \text{ random vector}\]</span>
<span class="math display">\[\boldsymbol a: m \times 1 \text{ vector, location parameter}\]</span>
<span class="math display">\[\boldsymbol B: m \times d \text{ matrix, scale parameter },  m \geq 1\]</span>
<span class="math display">\[\boldsymbol x: d \times 1 \text{ random vector}\]</span></p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\text{E}(\boldsymbol x)=\boldsymbol \mu\\
\text{Var}(\boldsymbol x)=\boldsymbol \Sigma\\
\end{array}
\Longrightarrow
\begin{array}{ll}
\text{E}(\boldsymbol y)=\boldsymbol a+ \boldsymbol B\boldsymbol \mu\\
\text{Var}(\boldsymbol y)= \boldsymbol B\boldsymbol \Sigma\boldsymbol B^T \\
\end{array}
\end{align*}\]</span></p>
<p>Special cases/examples:</p>
<ol style="list-style-type: decimal">
<li>Univariate case (<span class="math inline">\(d=1, m=1\)</span>)
<ul>
<li><span class="math inline">\(\text{E}(y)=a+b\mu\)</span></li>
<li><span class="math inline">\(\text{Var}(y)=b^2\sigma^2\)</span></li>
</ul></li>
<li>Sum of two random univariate variables<br />
<span class="math inline">\(y = x_1 + x_2\)</span>, i.e. <span class="math inline">\(a=0\)</span> and <span class="math inline">\(\boldsymbol B=(1,1)\)</span>
<ul>
<li><span class="math inline">\(\text{E}(x_1+x_2)=\mu_1+\mu_2\)</span><br />
</li>
<li><span class="math inline">\(\text{Var}(x_1+x_2) = (1,1)\begin{pmatrix}  \sigma^2_1 &amp; \sigma_{12}\\  \sigma_{21} &amp; \sigma^2_2  \end{pmatrix} \begin{pmatrix}  1\\  1  \end{pmatrix} = \sigma^2_1+\sigma^2_2+2\sigma_{12} = \text{Var}(x_1)+\text{Var}(x_2)+2\text{Cov}(x_1,x_2)\)</span></li>
</ul></li>
<li><span class="math inline">\(y_1=a_1+b_1 x_1\)</span> and <span class="math inline">\(y_2=a_2+b_2 x_2\)</span>, i.e. <span class="math inline">\(\boldsymbol a= \begin{pmatrix} a_1\\ a_2 \end{pmatrix}\)</span> and <span class="math inline">\(\boldsymbol B= \begin{pmatrix}b_1 &amp; 0\\ 0 &amp; b_2\end{pmatrix}\)</span>
<ul>
<li><span class="math inline">\(\text{E}(\boldsymbol y)=\begin{pmatrix} a_1+b_1 \mu_1\\ a_2+b_2 \mu_2 \end{pmatrix}\)</span><br />
</li>
<li><span class="math inline">\(\text{Var}(\boldsymbol y) = \begin{pmatrix} b_1 &amp; 0\\ 0 &amp; b_2 \end{pmatrix} \begin{pmatrix}  \sigma^2_1 &amp; \sigma_{12}\\  \sigma_{21} &amp; \sigma^2_2  \end{pmatrix} \begin{pmatrix} b_1 &amp; 0\\ 0 &amp; b_2 \end{pmatrix} = \begin{pmatrix}  b^2_1\sigma^2_1 &amp; b_1b_2\sigma_{12}\\  b_1b_2\sigma_{21} &amp; b^2_2\sigma^2_2  \end{pmatrix}\)</span><br />
i.e. <span class="math inline">\(\text{Cov}(a_1+b_1 x_1,a_2+b_2 x_2) = b_1 b_2\text{Cov}(x_1,x_2)\)</span></li>
</ul></li>
</ol>
</div>
<div id="invertible-location-scale-transformation" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Invertible location-scale transformation</h3>
<p>If <span class="math inline">\(m=d\)</span> and <span class="math inline">\(\det(\boldsymbol B) \neq 0\)</span> then we get an <strong>invertible</strong> transformation:
<span class="math display">\[\boldsymbol y= \boldsymbol a+ \boldsymbol B\boldsymbol x\]</span>
<span class="math display">\[\boldsymbol x= \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\]</span></p>
<p>Transformation of density:
<span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> with density <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span></p>
<p><span class="math inline">\(\Longrightarrow\)</span> <span class="math inline">\(\boldsymbol y\sim F_{\boldsymbol y}\)</span> with density
<span class="math display">\[ f_{\boldsymbol y}(\boldsymbol y)=|\det(\boldsymbol B)|^{-1} f_{\boldsymbol x} \left( \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\right)\]</span></p>

<div class="example">
<span id="exm:mahatrans" class="example"><strong>Example 2.1  </strong></span><strong>Mahalanobis transform</strong> <span class="math inline">\(\boldsymbol y=\boldsymbol \Sigma^{-1/2}(\boldsymbol x-\boldsymbol \mu)\)</span>
</div>

<p>We assume a positive definite and thus invertible <span class="math inline">\(\boldsymbol \Sigma\)</span>, so
that the inverse principal matrix square root <span class="math inline">\(\boldsymbol \Sigma^{-1/2}\)</span> can be computed,
and the transformation itself is invertible.</p>
<p><span class="math display">\[\boldsymbol a= - \boldsymbol \Sigma^{-1/2} \boldsymbol \mu\]</span>
<span class="math display">\[\boldsymbol B= \boldsymbol \Sigma^{-1/2}\]</span>
<span class="math display">\[\text{E}(\boldsymbol x)=\boldsymbol \mu\text{ and } \text{Var}(\boldsymbol x)=\boldsymbol \Sigma\]</span>
<span class="math display">\[\Longrightarrow\text{E}(\boldsymbol y) = \boldsymbol 0\text{ and } \text{Var}(\boldsymbol y) = \boldsymbol I_d\]</span>
Mahalanobis transformation performs three functions:</p>
<ol style="list-style-type: decimal">
<li>Centering (<span class="math inline">\(-\boldsymbol \mu\)</span>)</li>
<li>Standardisation <span class="math inline">\(\text{Var}(y_i)=1\)</span></li>
<li>Decorrelation <span class="math inline">\(\text{Cor}(y_i,y_j)=0\)</span> for <span class="math inline">\(i \neq j\)</span></li>
</ol>
<p><strong>Univariate case (<span class="math inline">\(d=1\)</span>)</strong></p>
<p><span class="math display">\[y = \frac{x-\mu}{\sigma}\]</span></p>
<p>= centering + standardisation</p>
<p>The <strong>Mahalanobis transformation</strong> appears implicitly in many places in multivariate statistics,
e.g. in the multivariate normal density.</p>
<p>It is a particular example of a whitening transformation (of which there
are infinitely many, see later chapters).</p>

<div class="example">
<span id="exm:coltrans" class="example"><strong>Example 2.2  </strong></span><strong>Inverse Mahalanobis transformation</strong> <span class="math inline">\(\boldsymbol y= \boldsymbol \mu+\boldsymbol \Sigma^{1/2} \boldsymbol x\)</span>
</div>

<p>This transformation is the <strong>inverse</strong> of the Mahalanobis transformation.
As the Mahalanobis transform is a particular whitening transform the inverse
transform is sometimes called the Mahalanobis colouring transformation.</p>
<p><span class="math display">\[\boldsymbol a=\boldsymbol \mu\]</span>
<span class="math display">\[\boldsymbol B=\boldsymbol \Sigma^{1/2}\]</span></p>
<p><span class="math display">\[\text{E}(\boldsymbol x)=\boldsymbol 0\text{ and } \text{Var}(\boldsymbol x)=\boldsymbol I_d\]</span>
<span class="math display">\[\Longrightarrow\text{E}(\boldsymbol y) = \boldsymbol \mu\text{ and } \text{Var}(\boldsymbol y) = \boldsymbol \Sigma\]</span>
Assume <span class="math inline">\(\boldsymbol x\)</span> is multivariate standard normal <span class="math inline">\(\boldsymbol x\sim N_d(\boldsymbol 0,\boldsymbol I_d)\)</span> with density
<span class="math display">\[f_{\boldsymbol x}(\boldsymbol x) = (2\pi)^{-d/2}\exp\left( -\frac{1}{2} \boldsymbol x^T \boldsymbol x\right)\]</span>
Then the density after applying this inverse Mahalanobis transform is</p>
<p><span class="math display">\[f_{\boldsymbol y}(\boldsymbol y) = |\det(\boldsymbol \Sigma^{1/2})|^{-1} (2\pi)^{-d/2} \exp\left(-\frac{1}{2}(\boldsymbol y-\boldsymbol \mu)^T\boldsymbol \Sigma^{-1/2} \,\boldsymbol \Sigma^{-1/2}(\boldsymbol y-\boldsymbol \mu)\right)\]</span></p>
<p><span class="math display">\[= (2\pi)^{-d/2} \det(\boldsymbol \Sigma)^{-1/2} \exp\left(-\frac{1}{2}(\boldsymbol y-\boldsymbol \mu)^T\boldsymbol \Sigma^{-1}(\boldsymbol y-\boldsymbol \mu)\right)\]</span>
<span class="math inline">\(\Longrightarrow\)</span> <span class="math inline">\(\boldsymbol y\)</span> has multivariate normal density!!</p>
<p><em>Application:</em> e.g. random number generation: draw from <span class="math inline">\(N_d(\boldsymbol 0,\boldsymbol I_d)\)</span> (easy!) then convert to multivariate normal by tranformation.</p>
</div>
</div>
<div id="nonlinear-transformations" class="section level2">
<h2><span class="header-section-number">2.2</span> Nonlinear transformations</h2>
<div id="general-transformation" class="section level3">
<h3><span class="header-section-number">2.2.1</span> General transformation</h3>
<p><span class="math display">\[\boldsymbol y= \boldsymbol h(\boldsymbol x)\]</span>
with <span class="math inline">\(\boldsymbol h\)</span> an arbitrary vector-valued function</p>
<ul>
<li>linear case: <span class="math inline">\(\boldsymbol h(\boldsymbol x) = \boldsymbol a+\boldsymbol B\boldsymbol x\)</span></li>
</ul>
</div>
<div id="delta-method" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Delta method</h3>
<p>Assume that we know the mean <span class="math inline">\(\text{E}(\boldsymbol x)=\boldsymbol \mu\)</span> and variance <span class="math inline">\(\text{Var}(\boldsymbol x)=\boldsymbol \Sigma\)</span> of <span class="math inline">\(\boldsymbol x\)</span>.
Is it possible to say something about the mean and variance of the transformed
random variable <span class="math inline">\(\boldsymbol y\)</span>?
<span class="math display">\[
\text{E}(\boldsymbol y)= \text{E}(\boldsymbol h(\boldsymbol x))= ?
\]</span>
<span class="math display">\[
\text{Var}(\boldsymbol y) = \text{Var}(\boldsymbol h(\boldsymbol x))= ? \\
\]</span></p>
<p>In general, for a transformation <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> the exact mean and variance of the transformed variable cannot be obtained analytically.</p>
<p>However, we can find a <strong>linear approximation</strong> and then compute its mean and variance.
This approximation is called the “Delta Method”, or the “law of propagation of errors”, and is credited to Gauss.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>Linearisation of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> is achieved by a Taylor series approximation of first order
of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> around <span class="math inline">\(\boldsymbol x_0\)</span>:
<span class="math display">\[\boldsymbol h(\boldsymbol x) \approx \boldsymbol h(\boldsymbol x_0) + \underbrace{\boldsymbol J_{\boldsymbol h}(\boldsymbol x_0)}_{\text{Jacobi matrix}}(\boldsymbol x-\boldsymbol x_0)  = 
\underbrace{\boldsymbol h(\boldsymbol x_0) -\boldsymbol J_{\boldsymbol h}(\boldsymbol x_0)\, \boldsymbol x_0}_{\boldsymbol a} + \underbrace{\boldsymbol J_{\boldsymbol h}(\boldsymbol x_0)}_{\boldsymbol B} \boldsymbol x\]</span></p>
<p><span class="math inline">\(\nabla\)</span>, the nabla operator, is the row vector <span class="math inline">\((\frac{\partial}{\partial x_1},...,\frac{\partial}{\partial x_d})\)</span>, which when applied to univariate <span class="math inline">\(h\)</span> gives the gradient:</p>
<p><span class="math display">\[\nabla h(\boldsymbol x) = \left(\frac{\partial h}{\partial x_1},...,\frac{\partial h}{\partial x_d}\right)\]</span></p>
<p>The <strong>Jacobi matrix</strong> is the <strong>generalisation of the gradient</strong> if <span class="math inline">\(\boldsymbol h\)</span> is vector-valued:</p>
<p><span class="math display">\[\boldsymbol J_{\boldsymbol h}(\boldsymbol x) = \begin{pmatrix}\nabla h_1(\boldsymbol x)\\ \nabla h_2(\boldsymbol x) \\ \vdots \\ \nabla h_m(\boldsymbol x) \end{pmatrix} = \begin{pmatrix}
    \frac{\partial h_1}{\partial x_1} &amp; \dots &amp; \frac{\partial h_1}{\partial x_d}\\
    \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial h_m}{\partial x_1} &amp; \dots &amp; \frac{\partial h_m}{\partial x_d}
    \end{pmatrix}\]</span></p>
<p>First order approximation of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> around <span class="math inline">\(\boldsymbol x_0=\boldsymbol \mu\)</span> yields
<span class="math inline">\(\boldsymbol a= \boldsymbol h(\boldsymbol \mu) - \boldsymbol J_{\boldsymbol h}(\boldsymbol \mu)\, \boldsymbol \mu\)</span>
<span class="math inline">\(\boldsymbol B= \boldsymbol J_{\boldsymbol h}(\boldsymbol \mu)\)</span> and leads directly to the <strong>multivariate Delta method</strong>:</p>
<p><span class="math display">\[\text{E}(\boldsymbol y)\approx\boldsymbol h(\boldsymbol \mu)\]</span>
<span class="math display">\[\text{Var}(\boldsymbol y)\approx \boldsymbol J_{\boldsymbol h}(\boldsymbol \mu) \, \boldsymbol \Sigma\, \boldsymbol J_{\boldsymbol h}(\boldsymbol \mu)^T\]</span></p>
<p>The <strong>univariate Delta method</strong> is a special case:
<span class="math display">\[\text{E}(y) \approx h(\mu)\]</span>
<span class="math display">\[\text{Var}(y)\approx \sigma^2 h&#39;(\mu)^2\]</span></p>
<p>Note that the Delta approximation breaks down if <span class="math inline">\(\text{Var}(\boldsymbol y)\)</span> is singular,
for example if the first derivative (or gradient or Jacobi matrix) at <span class="math inline">\(\boldsymbol \mu\)</span> is zero.</p>

<div class="example">
<p><span id="exm:varoddsration" class="example"><strong>Example 2.3  </strong></span><strong>Variance of the odds ratio</strong></p>
<p>The proportion <span class="math inline">\(\hat{p} = \frac{n_1}{n}\)</span> resulting from
<span class="math inline">\(n\)</span> repeats of a Bernoulli experiment has expectation <span class="math inline">\(\text{E}(\hat{p})=p\)</span>
and variance <span class="math inline">\(\text{Var}(\hat{p}) = \frac{p (1-p)}{n}\)</span>.
What are the (approximate) mean and the variance of the corresponding odds ratio <span class="math inline">\(\widehat{OR}=\frac{\hat{p}}{1-\hat{p}}\)</span>?</p>
With <span class="math inline">\(h(x) = \frac{x}{1-x}\)</span>,
<span class="math inline">\(\widehat{OR} = h(\hat{p})\)</span> and <span class="math inline">\(h&#39;(x) = \frac{1}{(1-x)^2}\)</span> we get using the
Delta method
<span class="math inline">\(\text{E}( \widehat{OR} ) \approx h(p) = \frac{p}{1-p}\)</span> and
<span class="math inline">\(\text{Var}( \widehat{OR} )\approx h&#39;(p)^2 \text{Var}( \hat{p} ) = \frac{p}{n (1-p)^3}\)</span>.
</div>


<div class="example">
<p><span id="exm:logtransform" class="example"><strong>Example 2.4  </strong></span><strong>Log-transform as variance stabilisation</strong></p>
<p>Assume <span class="math inline">\(x\)</span> has some mean <span class="math inline">\(\text{E}(x)=\mu\)</span> and variance <span class="math inline">\(\text{Var}(x) = \sigma^2 \mu^2\)</span>,
i.e. the standard deviation <span class="math inline">\(\text{SD}(x)\)</span> is proportional to the mean <span class="math inline">\(\mu\)</span>.
What are the (approximate) mean and the variance of the log-transformed variable <span class="math inline">\(\log(x)\)</span>?</p>
With <span class="math inline">\(h(x) = \log(x)\)</span> and <span class="math inline">\(h&#39;(x) = \frac{1}{x}\)</span> we get using the
Delta method
<span class="math inline">\(\text{E}( \log(x) ) \approx h(\mu) = \log(\mu)\)</span> and
<span class="math inline">\(\text{Var}( \log(x) )\approx h&#39;(\mu)^2 \text{Var}( x ) = \left(\frac{1}{\mu} \right)^2 \sigma^2 \mu^2 = \sigma^2\)</span>. Thus, after applying the log-transform the variance does not depend any more on the mean!
</div>

</div>
<div id="transformation-of-densities-under-general-invertible-transformation" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Transformation of densities under general invertible transformation</h3>
<p>Assume <span class="math inline">\(\boldsymbol h(\boldsymbol x) = \boldsymbol y(\boldsymbol x)\)</span> is invertible: <span class="math inline">\(\boldsymbol h^{-1}(\boldsymbol y)=\boldsymbol x(\boldsymbol y)\)</span></p>
<p><span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> with probability density function <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span></p>
<p>The density <span class="math inline">\(f_{\boldsymbol y}(\boldsymbol y)\)</span> of the transformed random vector <span class="math inline">\(\boldsymbol y\)</span> is then given by<br />
<span class="math display">\[f_{\boldsymbol y}(\boldsymbol y) = |\det\left( \boldsymbol J_{\boldsymbol x}(\boldsymbol y) \right)| \,\,\,  f_{\boldsymbol x}\left( \boldsymbol x(\boldsymbol y) \right)\]</span></p>
<p>where <span class="math inline">\(\boldsymbol J_{\boldsymbol x}(\boldsymbol y)\)</span> is the Jacobi matrix of the inverse transformation</p>
<p>Special cases:</p>
<ul>
<li>Univariate version: <span class="math inline">\(f_y(y) = |\frac{dx}{dy}| \, f_x\left(x(y)\right)\)</span></li>
<li>Linear transformation <span class="math inline">\(\boldsymbol h(\boldsymbol x) = \boldsymbol a+ \boldsymbol B\boldsymbol x\)</span>, with <span class="math inline">\(\boldsymbol x(\boldsymbol y) = \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\)</span>
and <span class="math inline">\(\boldsymbol J_{\boldsymbol x}(\boldsymbol y) = \boldsymbol B^{-1}\)</span>:<br />
<span class="math inline">\(f_{\boldsymbol y}(\boldsymbol y)=|\det(\boldsymbol B)|^{-1} f_{\boldsymbol x} \left( \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\right)\)</span></li>
</ul>
</div>
<div id="normalising-flows" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Normalising flows</h3>
<p>In machine learning (sequences of) invertible nonlinear transformations are known as “normalising flows”. They are used both in a generative way (building complex models from
simple models) and also in a simplification and dimension reduction context.</p>
<p>In this module we will focus mostly on linear transformations as these underpin
much of classical multivariate statistics, but it is important to keep in mind for later study
the importance of nonlinear transformations —see, e.g, the review paper by Kobyzev et al. “Normalizing Flows: Introduction and Ideas”, available from <a href="https://arxiv.org/abs/1908.09257" class="uri">https://arxiv.org/abs/1908.09257</a> .</p>
</div>
</div>
<div id="whitening-transformations" class="section level2">
<h2><span class="header-section-number">2.3</span> Whitening transformations</h2>
<div id="overview" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Overview</h3>
<p>The <em>Mahalanobis</em> transform (also know as “zero-phase component analysis” or short ZCA transform in machine learning) is a specific example of a <strong>whitening transformation</strong>. These constitute an important and widely used class of invertible location-scale transformations.</p>
<p><em>Terminology:</em> whitening refers to the fact that after the transformation the covariance matrix is spherical, isotrop, white (<span class="math inline">\(\boldsymbol I_d\)</span>)</p>
<p>Whitening is <strong>useful in preprocessing</strong>, to <strong>turn multivariate problems into simple univariate models</strong> and some <strong>reduce the dimension in an optimal way</strong>.</p>
<p>In so-called latent variable models whitening procedures link observed and latent variables (which usually are uncorrelated and standardised random variables):</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cl}
\text{Whitening} \\
\downarrow
\end{array}
\begin{array}{ll}
\boldsymbol x\\
\uparrow \\
\boldsymbol z\\
\end{array}
\begin{array}{ll}
\text{Observed variable (can be measured), external, typically correlated} \\
\space \\
\text{Unobserved &quot;latent&quot; variable, internal, typically not correlated} \\
\end{array}
\end{align*}\]</span></p>
</div>
<div id="general-whitening-transformation" class="section level3">
<h3><span class="header-section-number">2.3.2</span> General whitening transformation</h3>
<p><strong>Starting point:</strong></p>
<p>Random vector <span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> (not necessarily from multivariate normal).</p>
<p>The random variance <span class="math inline">\(\boldsymbol x\)</span> has some mean <span class="math inline">\(\text{E}(\boldsymbol z)=\boldsymbol \mu\)</span> and a positive definite (invertible) covariance matrix <span class="math inline">\(\text{Var}(\boldsymbol x) = \boldsymbol \Sigma\)</span>.
The covariance can be split into positive variances <span class="math inline">\(\boldsymbol V\)</span> and a
positive definite invertible correlation matrix <span class="math inline">\(\boldsymbol P\)</span> so that <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol V^{1/2} \boldsymbol P\boldsymbol V^{1/2}\)</span>.</p>
<p><strong>Whitening transformation:</strong></p>
<p><span class="math display">\[\underbrace{\boldsymbol z}_{d \times 1 \text{ vector }} = \underbrace{\boldsymbol W}_{d \times d \text{ whitening matrix }} \underbrace{\boldsymbol x}_{d \times 1 \text{ vector }}\]</span>
<strong>Objective</strong>: choose <span class="math inline">\(\boldsymbol W\)</span> so that <span class="math inline">\(\text{Var}(\boldsymbol z)=\boldsymbol I_d\)</span></p>
<p>For Mahalanobis/ZCA whitening we already know that <span class="math inline">\(\boldsymbol W^{\text{ZCA}}=\boldsymbol \Sigma^{-1/2}\)</span>.</p>
<p>In general, the whitening matrix <span class="math inline">\(\boldsymbol W\)</span> needs to satisfy a constraint:
<span class="math display">\[
\begin{array}{lll}
                &amp; \text{Var}(\boldsymbol z) &amp; = \boldsymbol I_d \\
\Longrightarrow &amp; \text{Var}(\boldsymbol W\boldsymbol x) &amp;= \boldsymbol W\boldsymbol \Sigma\boldsymbol W^T = \boldsymbol I_d \\
\Longrightarrow &amp;  \boldsymbol W\, \boldsymbol \Sigma\, \boldsymbol W^T \boldsymbol W= \boldsymbol W&amp; \\
\end{array}
\]</span>
<span class="math display">\[\Longrightarrow \text{constraint on whitening matrix: } \boldsymbol W^T \boldsymbol W= \boldsymbol \Sigma^{-1}\]</span></p>
<p>Clearly, the ZCA whitening matrix satisfies this constraint: <span class="math inline">\((\boldsymbol W^{ZCA})^T \boldsymbol W^{ZCA} = \boldsymbol \Sigma^{-1/2}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1}\)</span></p>
</div>
<div id="solution-of-whitening-constraint-covariance-based" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Solution of whitening constraint (covariance-based)</h3>
<p>A general way to specify a valid whitening matrix is
<span class="math display">\[
\boldsymbol W= \boldsymbol Q_1 \boldsymbol \Sigma^{-1/2}
\]</span>
where <span class="math inline">\(\boldsymbol Q_1\)</span> is an orthogonal matrix</p>
<p>Recall that an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> has the property that <span class="math inline">\(\boldsymbol Q^{-1} = \boldsymbol Q^T\)</span> and
and as a consequence <span class="math inline">\(\boldsymbol Q^T \boldsymbol Q= \boldsymbol Q\boldsymbol Q^T = \boldsymbol I\)</span>.</p>
<p>As a result, the above <span class="math inline">\(\boldsymbol W\)</span> satisfies the whitening constraint:</p>
<p><span class="math display">\[\boldsymbol W^T \boldsymbol W= \boldsymbol \Sigma^{-1/2}\underbrace{\boldsymbol Q_1^T \boldsymbol Q_1}_{\boldsymbol I}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1}\]</span></p>
<p>Note the converse is also true: any whitening whitening matrix, i.e. any <span class="math inline">\(\boldsymbol W\)</span> satisfying the whitening constraint, can be written in the above form as
<span class="math inline">\(\boldsymbol Q_1 = \boldsymbol W\boldsymbol \Sigma^{1/2}\)</span> is orthogonal by construction.</p>
<p><span class="math inline">\(\Longrightarrow\)</span> instead of choosing <span class="math inline">\(\boldsymbol W\)</span>, <strong>we choose the orthogonal matrix</strong> <span class="math inline">\(\boldsymbol Q_1\)</span>!</p>
<ul>
<li>recall that orthogonal matrices geometrically represent rotations (plus reflections).</li>
<li>it is now clear that there are infinitely many whitening procedures, because there are infinitely many rotations! This also means we need to find ways to choose/select among whitening procedures.</li>
<li>for the Mahalanobis/ZCA transformation <span class="math inline">\(\boldsymbol Q_1^{\text{ZCA}}=\boldsymbol I\)</span></li>
<li><strong>whitening</strong> can be interpreted as <strong>Mahalanobis transform</strong> followed by <strong>rotation</strong></li>
</ul>
</div>
<div id="another-solution-correlation-based" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Another solution (correlation-based)</h3>
<p>Instead of working with the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span>, we can express <span class="math inline">\(\boldsymbol W\)</span> also in terms of the corresponding correlation matrix <span class="math inline">\(\boldsymbol P= (\rho_{ij}) = \boldsymbol V^{-1/2} \boldsymbol \Sigma\boldsymbol V^{-1/2}\)</span>
where <span class="math inline">\(\boldsymbol V^{1/2}\)</span> is the diagonal matrix containing the variances.</p>
<p>Specifically we can specify the whitening matrix as
<span class="math display">\[\boldsymbol W= \boldsymbol Q_2 \boldsymbol P^{-1/2} \boldsymbol V^{-1/2}\]</span></p>
<p>It is easy to verify that this <span class="math inline">\(\boldsymbol W\)</span> also satisfies the whitening constraint:
<span class="math display">\[
\begin{split}
\boldsymbol W^T \boldsymbol W&amp; = \boldsymbol V^{-1/2}\boldsymbol P^{-1/2}\underbrace{\boldsymbol Q_2^T \boldsymbol Q_2}_{\boldsymbol I}\boldsymbol P^{-1/2} \boldsymbol V^{-1/2} \\
&amp; = \boldsymbol V^{-1/2} \boldsymbol P^{-1} \boldsymbol V^{-1/2} = \boldsymbol \Sigma^{-1} \\
\end{split}
\]</span>
Conversely, any whitening matrix <span class="math inline">\(\boldsymbol W\)</span> can also be written in this form as
<span class="math inline">\(\boldsymbol Q_2 = \boldsymbol W\boldsymbol V^{1/2} \boldsymbol P^{1/2}\)</span> is orthogonal by construction.</p>
<ul>
<li><strong>Another interpretation of whitening</strong>: first <strong>standardising</strong> (<span class="math inline">\(\boldsymbol V^{-1/2}\)</span>), then <strong>decorrelation</strong> (<span class="math inline">\(\boldsymbol P^{-1/2}\)</span>), followed by <strong>rotation</strong> (<span class="math inline">\(\boldsymbol Q_2\)</span>)</li>
<li>for Mahalanobis/ZCA transformation <span class="math inline">\(\boldsymbol Q_2^{\text{ZCA}} = \boldsymbol \Sigma^{-1/2} \boldsymbol V^{1/2} \boldsymbol P^{1/2}\)</span></li>
</ul>
<p><strong>Both forms to write <span class="math inline">\(\boldsymbol W\)</span> using <span class="math inline">\(\boldsymbol Q_1\)</span> and <span class="math inline">\(\boldsymbol Q_2\)</span> are equally valid (and interchangeable).</strong></p>
<p>Note that for the same <span class="math inline">\(\boldsymbol W\)</span>
<span class="math display">\[\boldsymbol Q_1\neq\boldsymbol Q_2 \text{  Two different orthogonal matrices!}\]</span>
and also
<span class="math display">\[\underbrace{\boldsymbol \Sigma^{-1/2}}_{\text{Symmetric}}\neq\underbrace{\boldsymbol P^{-1/2}\boldsymbol V^{-1/2}}_{\text{Not Symmetric}}\]</span>
even though<br />
<span class="math display">\[\boldsymbol \Sigma^{-1/2}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1} = \boldsymbol V^{-1/2}\boldsymbol P^{-1/2}\boldsymbol P^{-1/2}\boldsymbol V^{-1/2}\]</span></p>
</div>
<div id="cross-covariance-and-cross-correlation" class="section level3">
<h3><span class="header-section-number">2.3.5</span> Cross-covariance and cross-correlation</h3>
<p>A useful criterion to distinguish whitening transformation is to consider the
cross-covariance and cross-correlation:</p>
<ol style="list-style-type: lower-alpha">
<li><p><strong>Cross-covariance</strong> <span class="math inline">\(\boldsymbol \Phi= \Sigma_{\boldsymbol z\boldsymbol x}\)</span> between <span class="math inline">\(\boldsymbol z\)</span> and <span class="math inline">\(\boldsymbol x\)</span>:
<span class="math display">\[
\begin{split}
\boldsymbol \Phi= \text{Cov}(\boldsymbol z,\boldsymbol x) &amp; = \text{Cov}(\boldsymbol W\boldsymbol x,\boldsymbol x)\\
&amp; = \boldsymbol W\boldsymbol \Sigma\\
&amp;= \boldsymbol Q_1 \boldsymbol \Sigma^{-1/2} \boldsymbol \Sigma\\
&amp;= \boldsymbol Q_1\boldsymbol \Sigma^{1/2} \\
\end{split}
\]</span>
<strong>Cross-covariance is linked with</strong> <span class="math inline">\(\boldsymbol Q_1\)</span>!
Thus, choosing cross-covariance determines <span class="math inline">\(\boldsymbol Q_1\)</span> (and vice versa).
The whitening matrix expressed in terms of cross-covariance is <span class="math inline">\(\boldsymbol W= \boldsymbol \Phi\boldsymbol \Sigma^{-1}\)</span>.</p></li>
<li><p><strong>Cross-correlation</strong> <span class="math inline">\(\boldsymbol \Psi= \boldsymbol P_{\boldsymbol z\boldsymbol x}\)</span> between <span class="math inline">\(\boldsymbol z\)</span> and <span class="math inline">\(\boldsymbol x\)</span>:
<span class="math display">\[
\begin{split}
\boldsymbol \Psi= \text{Cor}(\boldsymbol z,\boldsymbol x) &amp; = \boldsymbol \Phi\boldsymbol V^{-1/2}\\
&amp; = \boldsymbol W\boldsymbol \Sigma\boldsymbol V^{-1/2}\\
&amp;=\boldsymbol Q_2 \boldsymbol P^{-1/2} \boldsymbol V^{-1/2} \boldsymbol \Sigma\boldsymbol V^{-1/2} \\
&amp; =  \boldsymbol Q_2\boldsymbol P^{1/2}\\
\end{split}
\]</span>
<strong>Cross-correlation is linked with</strong> <span class="math inline">\(\boldsymbol Q_2\)</span>!
Hence, choosing cross-correlation determines <span class="math inline">\(\boldsymbol Q_2\)</span> (and vice versa). The whitening
matrix expressed in terms of cross-correlation is
<span class="math inline">\(\boldsymbol W= \boldsymbol \Psi\boldsymbol P^{-1} \boldsymbol V^{-1/2}\)</span>.</p></li>
</ol>
<p>Note that the factorisation of the cross-covariance <span class="math inline">\(\boldsymbol \Phi=\boldsymbol Q_1\boldsymbol \Sigma^{1/2}\)</span> and
the cross-correlation <span class="math inline">\(\boldsymbol \Psi=\boldsymbol Q_2\boldsymbol P^{1/2}\)</span> into the product of an orthogonal matrix
and a positive semi-definite symmetric matrix is the called <em>polar decomposition</em>.</p>
</div>
<div id="inverse-whitening-transformation-loadings-and-multiple-correlation" class="section level3">
<h3><span class="header-section-number">2.3.6</span> Inverse whitening transformation, loadings, and multiple correlation</h3>
<p><strong>Reverse transformation:</strong></p>
<p>Recall that <span class="math inline">\(\boldsymbol z= \boldsymbol W\boldsymbol x\)</span>. Therefore, the reverse transformation going from the whitened
to the original variable is <span class="math inline">\(\boldsymbol x= \boldsymbol W^{-1} \boldsymbol z\)</span>.
This can be expressed also in terms of cross-covariance and cross-correlation.
With <span class="math inline">\(\boldsymbol W= \boldsymbol Q_1 \boldsymbol \Sigma^{-1/2}\)</span> we get for the inverse
<span class="math inline">\(\boldsymbol W^{-1} = \boldsymbol \Sigma^{1/2} \boldsymbol Q_1^{-1} = \boldsymbol \Sigma^{1/2} \boldsymbol Q_1^{-T} = \boldsymbol \Phi^T\)</span> so
that
<span class="math display">\[
\boldsymbol x= \boldsymbol \Phi^T \boldsymbol z\, .
\]</span>
Furthermore, since <span class="math inline">\(\boldsymbol \Psi= \boldsymbol \Phi\boldsymbol V^{-1/2}\)</span> we have
<span class="math inline">\(\boldsymbol W^{-1} = \boldsymbol V^{1/2} \boldsymbol \Psi^T\)</span> and
<span class="math display">\[
\boldsymbol V^{-1/2} \boldsymbol x=   \boldsymbol \Psi^T \boldsymbol z\, .
\]</span></p>
<p><strong>Definition of loadings:</strong></p>
<p><em>Loadings</em> are the coefficients of the linear transformation from the latent variable back to the observed variable. If the variables are standardised to unit variance then the loadings are also called <em>correlation loadings</em>.</p>
<p>Hence, the cross-covariance matrix plays the role of <em>loadings</em> linking the latent variable <span class="math inline">\(\boldsymbol z\)</span>
with the original <span class="math inline">\(\boldsymbol x\)</span>. Similarly, the cross-correlation matrix are the <em>correlation loadings</em>
linking the (already standardised) latent variable <span class="math inline">\(\boldsymbol z\)</span> with the standardised <span class="math inline">\(\boldsymbol x\)</span>.</p>
<p><strong>Multiple correlation coefficients from <span class="math inline">\(\boldsymbol z\)</span> to <span class="math inline">\(\boldsymbol x\)</span>:</strong></p>
<p>Consider the backtransformation from <span class="math inline">\(\boldsymbol z\)</span> to <span class="math inline">\(\boldsymbol x\)</span>.
The components of <span class="math inline">\(\boldsymbol z\)</span> are all uncorrelated. Therefore, we can compute the squared multiple correlation coefficient between each <span class="math inline">\(x_j\)</span> and <span class="math inline">\(\boldsymbol z\)</span> as the sum of the squared correlations
<span class="math inline">\(\text{Cor}(z_i, x_j)^2\)</span>:
<span class="math display">\[
\text{Cor}(\boldsymbol z, x_j)^2 = \sum_{i=1}^d  \text{Cor}(z_i, x_j)^2    = \sum_{i=1}^d \psi_{ij}^2
\]</span>
In vector notation with <span class="math inline">\(\boldsymbol \Psi= (\psi_{ij})\)</span> we get
<span class="math display">\[
\begin{split}
(\text{Cor}(\boldsymbol z, x_j)^2 )^T &amp;= \text{Diag}(\boldsymbol \Psi^T \boldsymbol \Psi) \\
&amp;= \text{Diag}(\boldsymbol P^{1/2} \boldsymbol Q_2^T \boldsymbol Q_2\boldsymbol P^{1/2}) \\
&amp;= \text{Diag}(\boldsymbol P) \\
&amp;= (1, \ldots, 1)^T\\
\end{split}
\]</span>
Therefore the column sums of the matrix <span class="math inline">\((\psi_{ij}^2)\)</span> are all 1 regardless of the choice of <span class="math inline">\(\boldsymbol Q_2\)</span>:
<span class="math display">\[
\sum_{i=1}^d \psi_{ij}^2 = 1 \text{ for all } j
\]</span></p>
<p>It is easy to understand why we get multiple squared correlations of value 1 — because <span class="math inline">\(x_j\)</span> is a linear function of the <span class="math inline">\(z_1, \ldots, z_d\)</span> with no noise term, which means <span class="math inline">\(x_j\)</span> can be predicted perfectly from <span class="math inline">\(\boldsymbol z\)</span> with no error.</p>
<p><strong>Multiple correlation coefficients from <span class="math inline">\(\boldsymbol x\)</span> to <span class="math inline">\(\boldsymbol z\)</span>:</strong></p>
<p>For the original direction going from <span class="math inline">\(x_1, \ldots, x_d\)</span> to the <span class="math inline">\(z_i\)</span> the corresponding squared multiple correlations <span class="math inline">\(\text{Cor}(z_i, \boldsymbol x)^2\)</span> are also 1, but because the <span class="math inline">\(x_j\)</span> are correlated we cannot simply sum the squared correlations to get <span class="math inline">\(\text{Cor}(z_i, \boldsymbol x)^2\)</span> but we also need take account of the correlations among the <span class="math inline">\(\boldsymbol x_j\)</span> (i.e. <span class="math inline">\(\boldsymbol P\)</span>).
In vector notation:
<span class="math display">\[
\begin{split}
(\text{Cor}(z_i, \boldsymbol x)^2 )^T &amp;= \text{Diag}( \boldsymbol \Psi\boldsymbol P^{-1} \boldsymbol \Psi^T) \\
 &amp;= \text{Diag}( \boldsymbol Q_2 \boldsymbol P^{1/2} \boldsymbol P^{-1} \boldsymbol P^{1/2} \boldsymbol Q_2^T ) \\
 &amp; = \text{Diag}(\boldsymbol I) \\
 &amp; = (1, \ldots, 1)^T\\
\end{split}
\]</span></p>
</div>
</div>
<div id="natural-whitening-procedures" class="section level2">
<h2><span class="header-section-number">2.4</span> Natural whitening procedures</h2>
<p>Now we discuss several strategies (maximise correlation between individual components, maximise compression, etc.) to arrive at optimal whitening transformation.</p>
<p>This leads to the following “natural” whitening transformations:</p>
<ul>
<li><strong>Mahalanobis</strong> whitening, also known as <strong>ZCA</strong> (zero-phase component analysis) whitening in machine learning</li>
<li><strong>ZCA-cor</strong> whitening</li>
<li><strong>PCA</strong> whitening</li>
<li><strong>PCA-cor</strong> whitening</li>
<li><strong>Cholesky</strong> whitening</li>
</ul>
<p>In the following <span class="math inline">\(\boldsymbol x_c = \boldsymbol x-\boldsymbol \mu_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol z_c = \boldsymbol z-\boldsymbol \mu_{\boldsymbol z}\)</span> denote the mean-centered variables.</p>
<div id="zca-whitening" class="section level3">
<h3><span class="header-section-number">2.4.1</span> ZCA whitening</h3>
<p><em>Aim</em>: remove correlations but otherwise make sure that after whitening <span class="math inline">\(\boldsymbol z\)</span> does not differ too much from <span class="math inline">\(\boldsymbol x\)</span>. Specifically, each element <span class="math inline">\(z_i\)</span> should be as close as as possible to the corresponding element <span class="math inline">\(x_i\)</span>:
<span class="math display">\[
\begin{array}{cc}
z_1\leftrightarrow x_1 \\
z_2\leftrightarrow x_2\\
z_3\leftrightarrow x_3 \\
\vdots
\end{array}
\]</span>
One possible way to implement this is to compute the expected squared difference between the two centered random vectors <span class="math inline">\(\boldsymbol z_c\)</span> and <span class="math inline">\(\boldsymbol x_c\)</span>.</p>
<p><em>ZCA objective function</em>: minimise <span class="math inline">\(\text{E}\left((\boldsymbol z_c-\boldsymbol x_c)^T(\boldsymbol z_c-\boldsymbol x_c)\right)\)</span> to find an optimal whitening procedure.</p>
<p>The ZCA objective function can be simplified as follows:
<span class="math display">\[
\begin{split}
&amp; = \text{E}( \boldsymbol z_c^T \boldsymbol z_c ) - 2 \text{E}( \boldsymbol x_c^T \boldsymbol z_c ) + \text{E}(\boldsymbol x_c^T \boldsymbol x_c)  \\
&amp; = \text{E}( \text{Tr}( \boldsymbol z_c \boldsymbol z_c^T ) ) - 2 \text{E}( \text{Tr}( \boldsymbol z_c \boldsymbol x_c^T ) ) +
 \text{E}(  \text{Tr}( \boldsymbol x_c \boldsymbol x_c^T ) )  \\
&amp; = \text{Tr}( \text{E}( \boldsymbol z_c \boldsymbol z_c^T ) ) - 2 \text{Tr}( \text{E}(  \boldsymbol z_c \boldsymbol x_c^T ) ) +
 \text{Tr}(  \text{E}( \boldsymbol x_c \boldsymbol x_c^T ) )  \\
&amp; = \text{Tr}( \text{Var}(\boldsymbol z) ) - 2 \text{Tr}( \text{Cov}(\boldsymbol z, \boldsymbol x) ) + \text{Tr}( \text{Var}(\boldsymbol x) )   \\
&amp; = d - 2\text{Tr}(\boldsymbol \Phi)+\text{Tr}(\boldsymbol V) \\
\end{split}
\]</span>
The only term that depends on the whitening transformation is <span class="math inline">\(-2 \text{Tr}(\boldsymbol \Phi)\)</span> as it is a function
of <span class="math inline">\(\boldsymbol Q_1\)</span>. Therefore we can use the following
alternative objective:</p>
<p><em>ZCA equivalent objective</em>: maximise <span class="math inline">\(\text{Tr}(\boldsymbol \Phi) = \text{Tr}(\boldsymbol Q_1\boldsymbol \Sigma^{1/2})\)</span> to find optimal <span class="math inline">\(\boldsymbol Q_1\)</span></p>
<p>This is the sum <span class="math inline">\(\sum_{i=1}^d \text{Cov}(z_i, x_i)\)</span> of all covariances between corresponding
elements in <span class="math inline">\(\boldsymbol z\)</span> and <span class="math inline">\(\boldsymbol x\)</span>.</p>
<p><em>Solution</em>:</p>
<ol style="list-style-type: lower-roman">
<li>Apply eigendecomposition to <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span>. Note that <span class="math inline">\(\boldsymbol \Lambda\)</span> is diagonal with positive entries <span class="math inline">\(\lambda_i &gt; 0\)</span> as <span class="math inline">\(\boldsymbol \Sigma\)</span> is positive definite.</li>
<li>The objective function becomes
<span class="math display">\[
\begin{split}
\text{Tr}(\boldsymbol Q_1\boldsymbol \Sigma^{1/2}) &amp;= \text{Tr}(\boldsymbol Q_1 \boldsymbol U\boldsymbol \Lambda^{1/2} \boldsymbol U^T  ) \\
&amp;= \text{Tr}(\boldsymbol \Lambda^{1/2} \, \boldsymbol U^T \boldsymbol Q_1 \boldsymbol U) \\
&amp; = \text{Tr}(\boldsymbol \Lambda^{1/2} \, \boldsymbol B) \\
&amp; = \sum_{i=1}^d \lambda_i b_{ii}.
\end{split} 
\]</span>
Note that the product of the orthogonal matrices <span class="math inline">\(\boldsymbol B= \boldsymbol U^T \boldsymbol Q_1 \boldsymbol U\)</span> is itself an orthogonal matrix, and
<span class="math inline">\(\boldsymbol Q_1 = \boldsymbol U\boldsymbol B\boldsymbol U^T\)</span>.</li>
<li>As <span class="math inline">\(\lambda_i &gt; 0\)</span> the objective function is maximised for the orthogonal matrix <span class="math inline">\(\boldsymbol B=\boldsymbol I\)</span>.</li>
<li>Thus, the optimal <span class="math inline">\(\boldsymbol Q_1\)</span> matrix is <span class="math display">\[\boldsymbol Q_1^{\text{ZCA}}=\boldsymbol I\]</span></li>
</ol>
<p>The corresponding whitening matrix for ZCA is
<span class="math display">\[
\boldsymbol W^{\text{ZCA}} = \boldsymbol \Sigma^{-1/2}
\]</span>
and the cross-covariance matrix is
<span class="math display">\[
\boldsymbol \Phi^{\text{ZCA}} = \boldsymbol \Sigma^{1/2}
\]</span>
and the cross-correlation matrix
<span class="math display">\[
\boldsymbol \Psi^{\text{ZCA}} = \boldsymbol \Sigma^{1/2} \boldsymbol V^{-1/2}
\]</span></p>
<p>Note that for ZCA
<span class="math inline">\(\text{Cov}(z_i, x_i) &gt; 0\)</span> and <span class="math inline">\(\text{Cor}(z_i, x_i) &gt; 0\)</span>
so two corresponding components <span class="math inline">\(z_i\)</span> and <span class="math inline">\(x_i\)</span> are always positively correlated!</p>
<p><em>Summary</em>:</p>
<ul>
<li>ZCA/Mahalanobis transform is the unique transformation that minimises the expected total squared component-wise difference between <span class="math inline">\(\boldsymbol x_c\)</span> and <span class="math inline">\(\boldsymbol z_c\)</span>.</li>
<li>As corresponding components in the whitened and original variables are always positively correlated this facilitates interpretation of the whitening variables.</li>
<li>Use ZCA aka Mahalanobis whitening if you want to “just” remove correlations.</li>
</ul>
</div>
<div id="zca-cor-whitening" class="section level3">
<h3><span class="header-section-number">2.4.2</span> ZCA-Cor whitening</h3>
<p><em>Aim</em>: same as above but remove scale in <span class="math inline">\(\boldsymbol x\)</span> first before comparing to <span class="math inline">\(\boldsymbol z\)</span></p>
<p><em>ZCA-cor objective function</em>: minimise <span class="math inline">\(\text{E}\left((\boldsymbol z_c-\boldsymbol V^{-1/2}\boldsymbol x_c)^T(\boldsymbol z_c-\boldsymbol V^{-1/2}\boldsymbol x_c)\right)\)</span> to find an optimal whitening procedure.</p>
<p>This can be simplified as follows:
<span class="math display">\[
\begin{split}
&amp; = \text{E}( \boldsymbol z_c^T \boldsymbol z_c ) - 2 \text{E}( \boldsymbol x_c^T \boldsymbol V^{-1/2} \boldsymbol z_c ) + \text{E}(\boldsymbol x_c^T \boldsymbol V^{-1} \boldsymbol x_c)  \\
&amp; = \text{Tr}( \text{Var}(\boldsymbol z) ) - 2 \text{Tr}( \text{Cor}(\boldsymbol z, \boldsymbol x) ) + \text{Tr}(  \text{Cor}(\boldsymbol x, \boldsymbol x) )   \\
&amp; = d - 2\text{Tr}(\boldsymbol \Psi)+ d \\
&amp; = 2d - 2\text{Tr}(\boldsymbol \Psi)
\end{split}
\]</span>
The only term that depends on the whitening transformation via <span class="math inline">\(\boldsymbol Q_2\)</span> is <span class="math inline">\(-2 \text{Tr}(\boldsymbol \Psi)\)</span> so we can use the following alternative objective instead:</p>
<p><em>ZCA-cor equivalent objective</em>: maximise <span class="math inline">\(\text{Tr}(\boldsymbol \Psi)=\text{Tr}(\boldsymbol Q_2\boldsymbol P^{1/2})\)</span> to find optimal <span class="math inline">\(\boldsymbol Q_2\)</span></p>
<p>This is the sum <span class="math inline">\(\sum_{i=1}^d \text{Cor}(z_i, x_i)\)</span> of all correlations between corresponding
elements in <span class="math inline">\(\boldsymbol z\)</span> and <span class="math inline">\(\boldsymbol x\)</span>.</p>
<p><em>Solution</em>: same as above for ZCA but using correlation instead of covariance:</p>
<ol style="list-style-type: lower-roman">
<li>Apply eigendecomposition to <span class="math inline">\(\boldsymbol P= \boldsymbol G\boldsymbol \Theta\boldsymbol G^T\)</span> with positive diagonal <span class="math inline">\(\boldsymbol \Theta\)</span>.</li>
<li>The objective function becomes <span class="math inline">\(\text{Tr}(\boldsymbol Q_2\boldsymbol P^{1/2}) ) = \text{Tr}( \boldsymbol \Theta^{1/2} \boldsymbol G^T \boldsymbol Q_2 \boldsymbol G)\)</span></li>
<li>This is maximised for
<span class="math display">\[\boldsymbol Q_2^{\text{ZCA-Cor}}=\boldsymbol I\]</span></li>
</ol>
<p>The corresponding whitening matrix for ZCA-cor is
<span class="math display">\[\boldsymbol W^{\text{ZCA-Cor}} = \boldsymbol P^{-1/2}\boldsymbol V^{-1/2}\]</span>
and the cross-covariance matrix is
<span class="math display">\[
\boldsymbol \Phi^{\text{ZCA-Cor}} = \boldsymbol P^{1/2} \boldsymbol V^{1/2}
\]</span>
and the cross-correlation matrix is
<span class="math display">\[
\boldsymbol \Psi^{\text{ZCA-Cor}} = \boldsymbol P^{1/2}
\]</span>
As a result for ZCA-cor
<span class="math inline">\(\text{Cov}(z_i, x_i) &gt; 0\)</span> and <span class="math inline">\(\text{Cor}(z_i, x_i) &gt; 0\)</span>
so two corresponding components <span class="math inline">\(z_i\)</span> and <span class="math inline">\(x_i\)</span> are always positively correlated!</p>
<p><em>Summary</em>:</p>
<ul>
<li>ZCA-cor whitening is the unique whitening transformation maximising the
total correlation between corresponding elements in <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>.</li>
<li>ZCA-cor leads to interpretable <span class="math inline">\(\boldsymbol z\)</span> because each individual element in <span class="math inline">\(\boldsymbol z\)</span>
is (typically strongly) positively correlated with the corresponding element in the original <span class="math inline">\(\boldsymbol x\)</span>.</li>
<li>As ZCA-cor is explicitly constructed to maximise the total
pairwise correlations it achieves the higher correlation than ZCA.</li>
<li>If <span class="math inline">\(\boldsymbol x\)</span> is standardised to <span class="math inline">\(\text{Var}(x_i)=1\)</span> then ZCA and ZCA-cor are identical but otherwise they are different whitening transformations.</li>
</ul>
</div>
<div id="pca-whitening" class="section level3">
<h3><span class="header-section-number">2.4.3</span> PCA whitening</h3>
<p><em>Aim</em>: remove correlations and at the same compress information into a few latent variables.
Specfically, we would like that the first latent component <span class="math inline">\(z_1\)</span> is
maximally linked with all variables in <span class="math inline">\(\boldsymbol x\)</span>, followed by
the second component <span class="math inline">\(z_2\)</span> and so on:
<span class="math display">\[
\begin{array}{ccccccc}
z_1 &amp; \leftarrow x_1 &amp; &amp; z_2 &amp; \leftarrow x_1  &amp;&amp; \ldots \\
z_1 &amp; \leftarrow x_2 &amp; &amp; z_2 &amp; \leftarrow x_2  \\
\vdots\\
z_1 &amp; \leftarrow x_d &amp; &amp; z_2 &amp; \leftarrow x_d  \\
\end{array}
\]</span>
One way to measure the total link of each <span class="math inline">\(z_i\)</span> with all <span class="math inline">\(x_j\)</span> is the sum of the
corresponding squared covariances
<span class="math display">\[
p_i = \sum^d_{j=1}\text{Cov}(z_i,x_j)^2 = \sum^d_{j=1} \phi_{ij}^2
\]</span>
In vector notation we write
<span class="math display">\[
\boldsymbol p= (p_1,...,p_d)^T = \text{Diag}(\boldsymbol \Phi\boldsymbol \Phi^T)
\]</span>
With <span class="math inline">\(\boldsymbol \Phi= \boldsymbol Q_1 \boldsymbol \Sigma^{1/2}\)</span> this can be written <span class="math inline">\(\boldsymbol p=\text{Diag}(\boldsymbol Q_1\boldsymbol \Sigma\boldsymbol Q_1^T)\)</span>
as a function of <span class="math inline">\(\boldsymbol Q_1\)</span>.</p>
<p><em>PCA objective functions</em>: maximise <span class="math inline">\(p_1, \ldots, p_{d-1}\)</span> in <span class="math inline">\(\boldsymbol p= \text{Diag}(\boldsymbol Q_1\boldsymbol \Sigma\boldsymbol Q_1^T)\)</span> such that <span class="math inline">\(p_1 \geq p_2 \geq \dots \geq p_d\)</span>
to find an optimal optimal <span class="math inline">\(\boldsymbol Q_1\)</span> and the corresponding whitening transformation.</p>
<p>Note that <span class="math inline">\(\sum_{i=1}^d p_i = \text{Tr}( \boldsymbol Q_1 \boldsymbol \Sigma\boldsymbol Q_1 ) = \text{Tr}(\boldsymbol \Sigma)\)</span>
is constant regardless of the choice <span class="math inline">\(\boldsymbol Q_1\)</span> so there are only <span class="math inline">\(d-1\)</span> independent <span class="math inline">\(p_i\)</span>.</p>
<p><em>Solution:</em></p>
<ol style="list-style-type: lower-roman">
<li><p>Apply eigendecomposition to <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span>. Note that <span class="math inline">\(\boldsymbol \Lambda\)</span> is diagonal with positive entries <span class="math inline">\(\lambda_1 \geq \lambda_2 \ldots \geq \lambda_d &gt; 0\)</span> as <span class="math inline">\(\boldsymbol \Sigma\)</span> is positive definite and that the eigenvalues are already arranged non-increasing order.
Also recall that <span class="math inline">\(\boldsymbol U\)</span> is not uniquely defined — you are free to change the columns signs.</p></li>
<li><p>The objective functions become
<span class="math inline">\(\boldsymbol p= \text{Diag}( (\boldsymbol Q_1 \boldsymbol U) \boldsymbol \Lambda(\boldsymbol Q_1 \boldsymbol U)^T ) = \text{Diag}( \boldsymbol B\boldsymbol \Lambda\boldsymbol B^T)\)</span> where <span class="math inline">\(\boldsymbol B\)</span> is an orthogonal matrix, and <span class="math inline">\(\boldsymbol Q_1= \boldsymbol B\boldsymbol U^T\)</span>.</p></li>
<li><p>The optimal (maximum) values are achieved for <span class="math inline">\(\boldsymbol B= \boldsymbol I\)</span>, with <span class="math inline">\(p_i^{\text{PCA}} = \lambda_i\)</span>.
However, this is not the only solution — you can arbitrarily change
the column signs of the matrix <span class="math inline">\(\boldsymbol B\)</span> to arrive at the same maximum!</p></li>
<li><p>The corresponding optimal value for the <span class="math inline">\(\boldsymbol Q_1\)</span> matrix is
<span class="math display">\[
\boldsymbol Q_1^{\text{PCA}}=\boldsymbol U^T
\]</span>
The corresponding whitening matrix is
<span class="math display">\[
\boldsymbol W^{\text{PCA}} = \boldsymbol U^T\boldsymbol \Sigma^{-1/2}=\boldsymbol \Lambda^{-1/2}\boldsymbol U^T
\]</span><br />
and the cross-covariance matrix is
<span class="math display">\[
\boldsymbol \Phi^{\text{PCA}} = \boldsymbol \Lambda^{1/2} \boldsymbol U^T
\]</span>
and the cross-correlation matrix is
<span class="math display">\[
\boldsymbol \Psi^{\text{PCA}} = \boldsymbol \Lambda^{1/2} \boldsymbol U^T \boldsymbol V^{-1/2}
\]</span></p></li>
</ol>
<p>Note that all of the above (i.e. <span class="math inline">\(\boldsymbol Q_1^{\text{PCA}}, \boldsymbol W^{\text{PCA}}, \boldsymbol \Phi^{\text{PCA}}, \boldsymbol \Psi^{\text{PCA}}\)</span>) is not unique
as we still have the sign ambiguity in the columns
of <span class="math inline">\(\boldsymbol U\)</span> (which also has absorbed the sign ambiguity of <span class="math inline">\(\boldsymbol B\)</span>)!</p>
<p><em>Identifiability:</em></p>
<p>Therefore, for identifiability reasons we need to impose a further constraint on <span class="math inline">\(\boldsymbol Q_1^{\text{PCA}}\)</span>.
A useful condition is to require a positive
diagonal, i.e. <span class="math inline">\(\text{Diag}(\boldsymbol Q_1^{\text{PCA}}) &gt; 0\)</span> and also <span class="math inline">\(\text{Diag}(\boldsymbol U) &gt; 0\)</span>.
As a result, <span class="math inline">\(\text{Diag}(\boldsymbol \Phi^{\text{PCA}}) &gt; 0\)</span> and <span class="math inline">\(\text{Diag}(\boldsymbol \Psi^{\text{PCA}}) &gt; 0\)</span>. With
this constraint in place all pairs of <span class="math inline">\(x_i\)</span> and <span class="math inline">\(z_i\)</span> are positively correlated.</p>
<p>It is particularly important to pay attention to the sign ambiguity
if different computer implementations of PCA whitening (and the related PCA approach)
are used.</p>
<p><em>Proportion of total variation:</em></p>
<p>The sum of the maximised squared covariances for each latent component <span class="math inline">\(z_i\)</span> is
<span class="math inline">\(\sum_{i=1}^d p_i^{\text{PCA}} = \sum_{i=1}^d \lambda_i\)</span>
and equals the total variation <span class="math inline">\(\text{Tr}(\boldsymbol \Sigma)\)</span>.</p>
<p>The fraction <span class="math inline">\(\frac{\lambda_i}{\sum^d_{j=1}\lambda_j}\)</span> is the proportional
contribution of each element in <span class="math inline">\(\boldsymbol z\)</span> to explain the total variation.
Thus, low ranking components <span class="math inline">\(z_i\)</span> with small <span class="math inline">\(p_i^{\text{PCA}}=\lambda_i\)</span> may be discarded.
In fact, the aim of PCA whitening is to achieve this kind of compression and the resuling
reduction in dimension in the latent space.</p>
<p><em>Summary:</em></p>
<ul>
<li>PCA whitening is a whitening transformation that maximises compression with the sum of squared cross-covariances as underlying optimality criterion.</li>
<li>There are sign ambiguities in the PCA whitened variables which are inherited from the sign ambiguities in eigenvectors.</li>
<li>If a positive-diagonal condition on the orthogonal matrices is imposed then these sign ambiguities are fully resolved and corresponding components <span class="math inline">\(z_i\)</span> and <span class="math inline">\(x_i\)</span> are always positively correlated.</li>
</ul>
</div>
<div id="pca-cor-whitening" class="section level3">
<h3><span class="header-section-number">2.4.4</span> PCA-cor whitening</h3>
<p><em>Aim</em>: same as for PCA whitening but remove scale in <span class="math inline">\(\boldsymbol x\)</span> first. This means we use squared correlations rather than squared covariances to measure compression, i.e.<br />
<span class="math display">\[
p_i = \sum^d_{j=1}\text{Cor}(z_i, x_j)^2 = \sum^d_{j=1} \psi_{ij}^2\]</span>
In vector notation this becomes
<span class="math display">\[
\boldsymbol p= \text{Diag}(\boldsymbol \Psi\boldsymbol \Psi^T)=\text{Diag}(\boldsymbol Q_2\boldsymbol P\boldsymbol Q_2^T)
\]</span></p>
<p><em>PCA-cor objective functions:</em>
maximise <span class="math inline">\(p_1, \ldots, p_{d-1}\)</span> in <span class="math inline">\(\boldsymbol p= \text{Diag}(\boldsymbol Q_2\boldsymbol P\boldsymbol Q_2^T)\)</span> such that <span class="math inline">\(p_1 \geq p_2 \geq \dots \geq p_d\)</span>
to find an optimal optimal <span class="math inline">\(\boldsymbol Q_2\)</span> and the corresponding whitening transformation.</p>
<p>Note that <span class="math inline">\(\sum_{i=1}^d p_i = \text{Tr}( \boldsymbol Q_2 \boldsymbol P\boldsymbol Q_2 ) = \text{Tr}(\boldsymbol P) = d\)</span>
is constant regardless of the choice <span class="math inline">\(\boldsymbol Q_2\)</span> so there are only <span class="math inline">\(d-1\)</span> independent <span class="math inline">\(p_i\)</span>.</p>
<p><em>Solution:</em></p>
<ol style="list-style-type: lower-roman">
<li><p>Apply eigendecomposition to <span class="math inline">\(\boldsymbol P= \boldsymbol G\boldsymbol \Theta\boldsymbol G^T\)</span>. Note that <span class="math inline">\(\boldsymbol \Theta\)</span> is diagonal with positive entries <span class="math inline">\(\theta_1 \geq \theta_2 \ldots \geq \theta_d &gt; 0\)</span> as <span class="math inline">\(\boldsymbol P\)</span> is positive definite and that the eigenvalues are already arranged non-increasing order. Also recall that <span class="math inline">\(\boldsymbol G\)</span> is not uniquely defined — you are free to change the columns signs.</p></li>
<li><p>The objective functions become
<span class="math inline">\(\boldsymbol p= \text{Diag}( (\boldsymbol Q_2 \boldsymbol G) \boldsymbol \Theta(\boldsymbol Q_2 \boldsymbol G)^T ) = \text{Diag}( \boldsymbol B\boldsymbol \Theta\boldsymbol B^T)\)</span> where <span class="math inline">\(\boldsymbol B\)</span> is an orthogonal matrix, and <span class="math inline">\(\boldsymbol Q_2= \boldsymbol B\boldsymbol G^T\)</span>.</p></li>
<li><p>The optimal (maximum) values are achieved for <span class="math inline">\(\boldsymbol B= \boldsymbol I\)</span>, with <span class="math inline">\(p_i^{\text{PCA-Cor}} = \theta_i\)</span>.
However, this is not the only solution — you can arbitrarily change
the column signs of the matrix <span class="math inline">\(\boldsymbol B\)</span> to arrive at the same maximum!</p></li>
<li><p>The corresponding optimal value for the <span class="math inline">\(\boldsymbol Q_2\)</span> matrix is
<span class="math display">\[
\boldsymbol Q_2^{\text{PCA-Cor}}=\boldsymbol G^T
\]</span>
The corresponding whitening matrix is<br />
<span class="math display">\[
\boldsymbol W^{\text{PCA-Cor}} = \boldsymbol \Theta^{-1/2} \boldsymbol G^T \boldsymbol V^{-1/2}
\]</span><br />
and the cross-covariance matrix is
<span class="math display">\[
\boldsymbol \Phi^{\text{PCA-Cor}} = \boldsymbol \Theta^{1/2} \boldsymbol G^T \boldsymbol V^{1/2}
\]</span>
and the cross-correlation matrix is
<span class="math display">\[
\boldsymbol \Psi^{\text{PCA-Cor}} = \boldsymbol \Theta^{1/2} \boldsymbol G^T
\]</span>
As with PCA whitening, there are sign ambiguities in the above because the column signs of <span class="math inline">\(\boldsymbol G\)</span>
can be freely chosen.</p></li>
</ol>
<p><em>Identifiability:</em></p>
<p>For identifiability we choose to set <span class="math inline">\(\text{Diag}(\boldsymbol Q_2^{\text{PCA-Cor}}) &gt; 0\)</span> and also
<span class="math inline">\(\text{Diag}(\boldsymbol G) &gt; 0\)</span> so that <span class="math inline">\(\text{Diag}(\boldsymbol \Phi^{\text{PCA-Cor}}) &gt; 0\)</span> and <span class="math inline">\(\text{Diag}(\boldsymbol \Psi^{\text{PCA-Cor}}) &gt; 0\)</span>.</p>
<p><em>Proportion of total variation:</em></p>
<p>The sum of the maximised squared correlations for each latent component <span class="math inline">\(z_i\)</span> is
<span class="math inline">\(\sum_{i=1}^d p_i^{\text{PCA-Cor}} = \sum_{i=1}^d \theta_i = d\)</span>
and equals the total variation <span class="math inline">\(\text{Tr}(\boldsymbol P)\)</span>.
Therefore the fraction
<span class="math inline">\(\frac{\theta_i}{\sum^d_{j=1} \theta_j} = \frac{\theta_j}{d}\)</span> is the
proportional
contribution of each element in <span class="math inline">\(\boldsymbol z\)</span> to explain the total variation.</p>
<p><em>Summary:</em></p>
<ul>
<li>PCA-cor whitening is a whitening transformation that maximises compression with the sum of squared cross-correlations as underlying optimality criterion.</li>
<li>There are sign ambiguities in the PCA-cor whitened variables which are inherited from the sign ambiguities in eigenvectors.</li>
<li>If a positive-diagonal condition on the orthogonal matrices is imposed then these sign ambiguities are fully resolved and corresponding components <span class="math inline">\(z_i\)</span> and <span class="math inline">\(x_i\)</span> are always positively correlated.</li>
<li>If <span class="math inline">\(\boldsymbol x\)</span> is standardised to <span class="math inline">\(\text{Var}(x_i)=1\)</span>, then PCA and PCA-cor whitening are identical.</li>
</ul>
</div>
<div id="cholesky-whitening" class="section level3">
<h3><span class="header-section-number">2.4.5</span> Cholesky whitening</h3>
<p><em>Aim</em>: find a whitening transformation such that the cross-covariance <span class="math inline">\(\boldsymbol \Phi\)</span> and cross-correlation <span class="math inline">\(\boldsymbol \Psi\)</span> have triangular structure. This is useful in some models such as time course data, e.g. to ensure that the
future cannot influence the past.</p>
<p><em>Solution</em>: Apply a Cholesky decomposition to <span class="math inline">\(\boldsymbol \Sigma^{-1} = \boldsymbol L\boldsymbol L^T\)</span></p>
<p>The Cholesky descomposition requires positive definite <span class="math inline">\(\boldsymbol \Sigma\)</span> and is unique.
<span class="math inline">\(\boldsymbol L\)</span> is a lower triangular matrix with positive diagonal elements.
Its inverse <span class="math inline">\(\boldsymbol L^{-1}\)</span> is also lower triangular with positive diagonal elements.</p>
<p>The resulting whitening matrix is
<span class="math display">\[
\boldsymbol W^{\text{Chol}}=\boldsymbol L^T
\]</span><br />
By construction, <span class="math inline">\(\boldsymbol W^{\text{Chol}}\)</span> satisfies the whitening constraint since <span class="math inline">\((\boldsymbol W^{\text{Chol}})^T\boldsymbol W^{\text{Chol}} = \boldsymbol \Sigma^{-1}\)</span>.</p>
<p>The cross-covariance matrix is (with <span class="math inline">\(\boldsymbol \Sigma= (\boldsymbol L^{-1})^T \boldsymbol L^{-1}\)</span>)
<span class="math display">\[
\boldsymbol \Phi^{\text{Chol}} = \boldsymbol L^T\boldsymbol \Sigma=  \boldsymbol L^{-1}
\]</span>
and the cross-correlation matrix is
<span class="math display">\[
\boldsymbol \Psi^{\text{Chol}} = \boldsymbol L^T \boldsymbol \Sigma\boldsymbol V^{-1/2} =  \boldsymbol L^{-1} \boldsymbol V^{-1/2}
\]</span>
Note that the both <span class="math inline">\(\boldsymbol \Phi^{\text{Chol}}\)</span> and
<span class="math inline">\(\boldsymbol \Psi^{\text{Chol}}\)</span> are also
lower triangular matrices with positive diagonal elements!</p>
<p>Finally, the corresponding orthogonal matrices are
<span class="math display">\[
\boldsymbol Q_1^{\text{Chol}} = \boldsymbol L^T \boldsymbol \Sigma^{1/2}
\]</span>
and
<span class="math display">\[
\boldsymbol Q_2^{\text{Chol}} = \boldsymbol L^T \boldsymbol V^{1/2} \boldsymbol P^{1/2}
\]</span></p>
</div>
<div id="comparison-of-zca-pca-and-cholesky-whitening" class="section level3">
<h3><span class="header-section-number">2.4.6</span> Comparison of ZCA, PCA and Cholesky whitening</h3>
<p>For comparison, here are the results of ZCA, PCA and Cholesky whitening applied to a simulated bivariate normal data set with correlation <span class="math inline">\(\rho=0.8\)</span>.</p>
<p><img src="2-transformations_files/figure-html/fig1-1.png" width="672" /></p>
<p>In column 1 you can see the simulated data as scatter plot.</p>
<p>Column 2 shows the scatter plots of the whitened data — as expect all three methods removed correlation produce isotropic covariance.</p>
<p>The three approached differ differ in the cross-correlations. Columns 3 and 4 show the cross-correlations between the first two corresponding components (<span class="math inline">\(x_1\)</span> and <span class="math inline">\(z_1\)</span>, and <span class="math inline">\(x_2\)</span> and <span class="math inline">\(z_2\)</span>) for ZCA, PCA and Cholesky whitening. As expected, in ZCA both pairs show strong correlation, but this is not the case for PCA and Cholesky whitening.</p>
</div>
<div id="recap" class="section level3">
<h3><span class="header-section-number">2.4.7</span> Recap</h3>
<table>
<colgroup>
<col width="31%" />
<col width="68%" />
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Type of usage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ZCA, ZCA-cor:</td>
<td>pure decorrelate, maintain similarity to original data set, interpretability</td>
</tr>
<tr class="even">
<td>PCA, PCA-cor:</td>
<td>compression, find effective dimension, reduce dimensionality, feature identification</td>
</tr>
<tr class="odd">
<td>Cholesk y</td>
<td>time course data, triangular <span class="math inline">\(W\)</span>, <span class="math inline">\(\boldsymbol \Phi\)</span> and <span class="math inline">\(\boldsymbol P\)</span></td>
</tr>
</tbody>
</table>
<p><strong>Related models not discussed in this course:</strong></p>
<ul>
<li><p>Factor models: essentially whitening plus an additional error term, factors have rotational
freedom just like in whitening</p></li>
<li><p>PLS: similar to PCA but in regression setting (with the choice of
latent variables depending on the response)</p></li>
</ul>
</div>
</div>
<div id="principal-component-analysis-pca" class="section level2">
<h2><span class="header-section-number">2.5</span> Principal Component Analysis (PCA)</h2>
<div id="pca-transformation" class="section level3">
<h3><span class="header-section-number">2.5.1</span> PCA transformation</h3>
<p>Traditional PCA was invented 1901 by Karl Pearson<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> and is very closely related to <strong>PCA whitening</strong>.</p>
<p>Assume random vector <span class="math inline">\(\boldsymbol x\)</span> with <span class="math inline">\(\text{Var}(\boldsymbol x) = \boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span>.
PCA is a particular orthogonal transformation of the original <span class="math inline">\(\boldsymbol x\)</span>
such that the resulting components are orthogonal:
<span class="math display">\[
\underbrace{\boldsymbol t^{\text{PCA}}}_{\text{Principal components}} = \underbrace{\boldsymbol U^T}_{\text{Orthogonal matrix}}   \boldsymbol x
\]</span>
<span class="math display">\[\text{Var}(\boldsymbol t^{\text{PCA}}) = \boldsymbol \Lambda= \begin{pmatrix} \lambda_1 &amp; \dots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \dots &amp; \lambda_d\end{pmatrix}\]</span>
Note that while principal components are <em>orthogonal</em> they do <em>not</em> have unit variance but the variance of principal components <span class="math inline">\(t_i\)</span> equals the eigenvalues <span class="math inline">\(\lambda_i\)</span>.</p>
<p>Thus PCA itself is <em>not</em> a whitening procedure. However, you arrive at PCA whitening by simply by standardising the PCA components: <span class="math inline">\(\boldsymbol z^{\text{PCA}} = \boldsymbol \Lambda^{-1/2} \boldsymbol t^{\text{PCA}}\)</span></p>
<p><strong>Compression properties:</strong></p>
<p>The total variation is <span class="math inline">\(\text{Tr}(\text{Var}(\boldsymbol t^{\text{PCA}})) = \text{Tr}( \boldsymbol \Lambda) = \sum^d_{j=1}\lambda_j\)</span>.
With principle components the fraction <span class="math inline">\(\frac{\lambda_i}{\sum^d_{j=1}\lambda_j}\)</span> can be interpreted as the proportion of variation contributed by
each component in <span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span> to the total variation. Thus, low ranking components in <span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span> with low variation may be discarded, thus leading to a reduction in dimension.</p>
</div>
<div id="application-to-data" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Application to data</h3>
<p>Written in terms of a data matrix <span class="math inline">\(\boldsymbol X\)</span> instead of a random vector <span class="math inline">\(\boldsymbol x\)</span> PCA becomes:
<span class="math display">\[\underbrace{\boldsymbol T}_{\text{Sample version of principal components}}=\underbrace{\boldsymbol X}_{\text{Data matrix}}\boldsymbol U\]</span>
There are now two ways to obtain <span class="math inline">\(\boldsymbol U\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the covariance matrix, e.g. by <span class="math inline">\(\hat{\boldsymbol \Sigma} = \frac{1}{n}\boldsymbol X_c^T\boldsymbol X_c\)</span> where <span class="math inline">\(\boldsymbol X_c\)</span> is the column-centred data matrix; then apply the eigenvalue decomposition on <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> to get <span class="math inline">\(\boldsymbol U\)</span>.</p></li>
<li><p>Compute the singular value decomposition of <span class="math inline">\(\boldsymbol X_c = \boldsymbol V\boldsymbol D\boldsymbol U^T\)</span>. As <span class="math inline">\(\hat{\boldsymbol \Sigma} = \frac{1}{n}\boldsymbol X_c^T\boldsymbol X_c = \boldsymbol U(\frac{1}{n}\boldsymbol D^2)\boldsymbol U^T\)</span> you can just use <span class="math inline">\(\boldsymbol U\)</span> from the SVD of <span class="math inline">\(\boldsymbol X_c\)</span> and there is no need to compute the covariance.</p></li>
</ol>
</div>
<div id="iris-data-example" class="section level3">
<h3><span class="header-section-number">2.5.3</span> Iris data example</h3>
<p>As an example we consider the famous <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">iris flower data set</a>. It consists of data for for botanical variables (sepal length, sepal width,
petal length and petal width) measured on 150 flowers from
three iris species (setosa, versicolr, virginica). Thus for this data set <span class="math inline">\(d=4\)</span> and <span class="math inline">\(n=150\)</span>.</p>
<p>We first standardise the data, then compute PCA components and plot the proportion of total variance contributed by each component.
This shows that only two PCA components are needed to achieve 95% of the total variation:</p>
<p><img src="2-transformations_files/figure-html/fig2-1.png" width="432" /></p>
<p>A scatter plot plot of the the first two principal components is also informative:</p>
<p><img src="2-transformations_files/figure-html/fig3-1.png" width="432" /></p>
<p>This shows that there groupings among the
150 flowers, corresponding to the species, and that these groups can be characterised
by the the principal components.</p>
</div>
</div>
<div id="correlation-loadings-plot-to-interpret-pca-components" class="section level2">
<h2><span class="header-section-number">2.6</span> Correlation loadings plot to interpret PCA components</h2>
<div id="pca-correlation-loadings" class="section level3">
<h3><span class="header-section-number">2.6.1</span> PCA correlation loadings</h3>
<p>In an earlier section we have learned that for a general whitening transformation the cross-correlations <span class="math inline">\(\boldsymbol \Psi=\text{Cor}(\boldsymbol z, \boldsymbol x)\)</span> play the role of correlation loadings in the inverse transformation:
<span class="math display">\[
\boldsymbol V^{-1/2} \boldsymbol x= \boldsymbol \Psi^T  \boldsymbol z\, , 
\]</span>
i.e. they are the coefficients linking the whitening variable <span class="math inline">\(\boldsymbol z\)</span> with the standardised <span class="math inline">\(\boldsymbol x\)</span>.
This relationship holds therefore also for PCA whitening
with <span class="math inline">\(\boldsymbol z^{\text{PCA}}= \boldsymbol \Lambda^{-1/2} \boldsymbol U^T \boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol \Psi^{\text{PCA}} = \boldsymbol \Lambda^{1/2} \boldsymbol U^T \boldsymbol V^{-1/2}\)</span>.</p>
<p>The classical PCA is not a whitening approach because <span class="math inline">\(\text{Var}(\boldsymbol t^{\text{PCA}}) \neq \boldsymbol I\)</span>. However, we can still compute cross-correlations between the principal components <span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span> and <span class="math inline">\(\boldsymbol x\)</span>, resulting in
<span class="math display">\[
\text{Cor}(\boldsymbol t^{\text{PCA}}, \boldsymbol x) = \boldsymbol \Lambda^{1/2} \boldsymbol U^T \boldsymbol V^{-1/2} = \boldsymbol \Psi^{\text{PCA}}
\]</span>
Note these are the same as the cross-correlations for PCA-whitening since
<span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span> and <span class="math inline">\(\boldsymbol z^{\text{PCA}}\)</span> only differ in scale.</p>
<p>The inverse PCA transformation is
<span class="math display">\[
\boldsymbol x= \boldsymbol U\boldsymbol t^{\text{PCA}}
\]</span>
In terms of standardised PCA components and standardised original components it becomes
<span class="math display">\[
\boldsymbol V^{-1/2} \boldsymbol x= \boldsymbol \Psi^T  \boldsymbol \Lambda^{-1/2} \boldsymbol t^{\text{PCA}}
\]</span>
Thus the cross-correlation matrix <span class="math inline">\(\boldsymbol \Psi\)</span> plays the role of <em>correlation loadings</em>
also in classical PCA, i.e. they are the
coefficients linking the standardised PCA components with the standardised original components.</p>
</div>
<div id="pca-correlation-loadings-plot" class="section level3">
<h3><span class="header-section-number">2.6.2</span> PCA correlation loadings plot</h3>
<p>In PCA and PCA-cor whitening as well as in classical PCA the aim is compression,
i.e. to find latent variables such that most of the total variation is contributed by
a small number of components.</p>
<p>In order to be able to better interpret the top ranking PCA component we can use a visual device called <em>correlation loadings plot</em>. For this we compute the correlation between the PCA components 1 and 2 (<span class="math inline">\(t_1^{\text{PCA}}\)</span> and <span class="math inline">\(t_2^{\text{PCA}})\)</span> with all original variables <span class="math inline">\(x_1, \ldots, x_d\)</span>.</p>
<p>For each original variable <span class="math inline">\(x_j\)</span> we therefore have two numbers betweem -1 and 1, the correlationa
<span class="math inline">\(\text{Cor}(t_1^{\text{PCA}}, x_j)\)</span> and <span class="math inline">\(cor(t_2^{\text{PCA}}, x_j)\)</span> that we use as coordinates to draw a point in a plane. By construction, all points
have to lie within a unit circle around the origin. As the sum of the squared correlation loadings from all latents component to one specific original variable sum up to one, the sum of the squared loadings from just the first two components is also at most 1.
The orginal variables most strongly influenced
by the two latent variables will have strong correlation and thus lie near the outer circle, whereas variables that are not influenced by the two latent variables will lie near the origin.</p>
<p>As an example, here is the correlation loadings plot showing the cross-correlation between the first two
PCA components and all four variables of the iris flower data set discussed earlier.</p>
<p><img src="2-transformations_files/figure-html/fig4-1.png" width="432" /></p>
<p>The interpretation of this plot is discussed in Worksheet 4.</p>
</div>
</div>
<div id="cca-whitening-canonical-correlation-analysis" class="section level2">
<h2><span class="header-section-number">2.7</span> CCA whitening (Canonical Correlation Analysis)</h2>
<p>Canonical correlation analysis was invented by Harald Hotelling in 1936.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>So far, we have looked only into whitening as a <strong>single</strong> vector <span class="math inline">\(\boldsymbol x\)</span>. In CCA whitening we consider <strong>two vectors</strong> <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> simultaneously:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\boldsymbol x= \begin{pmatrix} x_1 \\ \vdots \\ x_p \end{pmatrix} \\
\text{Dimension } p
\end{array}
\begin{array}{ll}
\boldsymbol y= \begin{pmatrix} y_1 \\ \vdots \\ y_q \end{pmatrix} \\
\text{Dimension } q
\end{array}
\begin{array}{ll}
\text{Var}(\boldsymbol x) = \boldsymbol \Sigma_{\boldsymbol x} = \boldsymbol V_{\boldsymbol x}^{1/2}\boldsymbol P_{\boldsymbol x}\boldsymbol V_{\boldsymbol x}^{1/2} \\
\text{Var}(\boldsymbol y) = \boldsymbol \Sigma_{\boldsymbol y} = \boldsymbol V_{\boldsymbol y}^{1/2}\boldsymbol P_{\boldsymbol y}\boldsymbol V_{\boldsymbol y}^{1/2} \\
\end{array}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cc}
\text{Whitening of } \boldsymbol x\text{:} \\
\text{Whitening of } \boldsymbol y\text{:}
\end{array}
\begin{array}{cc}
\boldsymbol z_{\boldsymbol x} = \boldsymbol W_{\boldsymbol x}\boldsymbol x=\boldsymbol Q_{\boldsymbol x}\boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol V_{\boldsymbol x}^{-1/2}\boldsymbol x\\
\boldsymbol z_{\boldsymbol y} = \boldsymbol W_{\boldsymbol y}\boldsymbol y=\boldsymbol Q_{\boldsymbol y}\boldsymbol P_{\boldsymbol y}^{-1/2}\boldsymbol V_{\boldsymbol y}^{-1/2}\boldsymbol y
\end{array}
\end{align*}\]</span>
(note we use the correlation-based form of <span class="math inline">\(\boldsymbol W\)</span>)</p>
<p>Cross-correlation between <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span> and <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span>:</p>
<p><span class="math display">\[\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})=\boldsymbol Q_{\boldsymbol x}\boldsymbol K\boldsymbol Q_{\boldsymbol y}^T\]</span></p>
<p>with <span class="math inline">\(\boldsymbol K= \boldsymbol P_{\boldsymbol x}^{-1/2}\boldsymbol P_{\boldsymbol x\boldsymbol y}\boldsymbol P_{\boldsymbol y}^{-1/2}\)</span>.</p>
<p><strong>Idea</strong>: we can choose suitable orthogonal matrices <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}\)</span> by putting constraints on the cross-correlation.</p>
<p><strong>CCA</strong>: we aim for a <em>diagonal</em> <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> so that each component in <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span> only influences one (the corresponding) component in <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span>.</p>
<p><strong>Motivation</strong>: pairs of “modules” represented by components of <span class="math inline">\(\boldsymbol z_{\boldsymbol x}\)</span>
and <span class="math inline">\(\boldsymbol z_{\boldsymbol y}\)</span> influencing each other (and not anyone other module).</p>
<p><span class="math display">\[
\begin{array}{ll}
\boldsymbol z_{\boldsymbol x} = \begin{pmatrix} z^x_1 \\ z^x_2 \\ \vdots \\ z^x_p \end{pmatrix} &amp;
\boldsymbol z_{\boldsymbol y} = \begin{pmatrix} z^y_1 \\ z^y_2 \\ \vdots \\ z^y_q \end{pmatrix} \\
\end{array}
\]</span></p>
<p>\end{align}</p>
<p><span class="math display">\[\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y}) = \begin{pmatrix} d_1 &amp; \dots &amp; 0 \\ \vdots &amp;  \vdots \\ 0 &amp; \dots &amp; d_m \end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(d_i\)</span> are the <em>canonical correlations</em> and <span class="math inline">\(m=\min(p,q)\)</span>.</p>
<div id="how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal" class="section level3">
<h3><span class="header-section-number">2.7.1</span> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</h3>
<ul>
<li>Use Singular Value Decomposition (SVD) of matrix <span class="math inline">\(\boldsymbol K\)</span>:<br />
<span class="math display">\[\boldsymbol K= (\boldsymbol Q_{\boldsymbol x}^{\text{CCA}})^T  \boldsymbol D\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\]</span>
where <span class="math inline">\(\boldsymbol D\)</span> is the diagonal matrix containing the singular values of <span class="math inline">\(\boldsymbol K\)</span></li>
<li>This yields orthogonal matrices <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> and thus the desired whitened matrices <span class="math inline">\(\boldsymbol W_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol W_{\boldsymbol y}^{\text{CCA}}\)</span></li>
<li>As a result <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y}) = \boldsymbol D\)</span> i.e. singular values of <span class="math inline">\(\boldsymbol K\)</span> are identical to canonical correlations <span class="math inline">\(d_i\)</span>!</li>
</ul>
<p><span class="math inline">\(\longrightarrow\)</span> <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> are determined by the diagonality constraint (and are different to the other previously discussed whitening methods).</p>
<p>Note that the signs of corresponding in columns in <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span> are not identified. Traditionally, in an SVD the
signs are chosen such that the singular values are positive. However, if we
impose positive-diagonality on <span class="math inline">\(\boldsymbol Q_{\boldsymbol x}^{\text{CCA}}\)</span> and <span class="math inline">\(\boldsymbol Q_{\boldsymbol y}^{\text{CCA}}\)</span>,
and thus positive-diagonality on the cross-correlations <span class="math inline">\(\boldsymbol \Psi_{\boldsymbol x}\)</span> and
<span class="math inline">\(\boldsymbol \Psi_{\boldsymbol y}\)</span>, then the canonical correlations may take on both positive and
negative values.</p>
</div>
<div id="related-methods" class="section level3">
<h3><span class="header-section-number">2.7.2</span> Related methods</h3>
<ul>
<li><p>O2PLS: similar to CCA but using orthogonal projections
(thus in O2PLS the latent variables underlying <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are not orthogonal)</p></li>
<li><p>Vector correlation: aggregates the squared canonical correlations into a single overall measure
of association between two random vectors <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> (see Chapter 5
on multivariate dependencies).</p></li>
</ul>

<p></p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Gorroochurn, P. 2020. Who Invented the Delta Method, Really? The Mathematical Intelligencer <strong>42</strong>:46–49. <a href="https://doi.org/10.1007/s00283-020-09982-0" class="uri">https://doi.org/10.1007/s00283-020-09982-0</a><a href="2-transformations.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Pearson, K. 1901. On lines and planes of closest fit to systems of
points in space. Philosophical Magazine <strong>2</strong>:559–572.<a href="2-transformations.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>Hotelling, H. 1936. Relations between two sets of variates.
Biometrika <strong>28</strong>:321–377.<a href="2-transformations.html#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-multivariate.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-clustering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math38161-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
