<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Supervised learning and classification – Multivariate Statistics and Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06-dependence.html" rel="next">
<link href="./04-clustering.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3c04d35918bfbae480bb424d60ad250e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./05-classification.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning and classification</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Multivariate Statistics and Machine Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-multivariate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Multivariate random variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multivariate estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations and dimension reduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised learning and clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-classification.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning and classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-dependence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multivariate dependencies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear and nonparametric models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#aims-of-supervised-learning" id="toc-aims-of-supervised-learning" class="nav-link active" data-scroll-target="#aims-of-supervised-learning"><span class="header-section-number">5.1</span> Aims of supervised learning</a>
  <ul class="collapse">
  <li><a href="#supervised-learning-vs.-unsupervised-learning" id="toc-supervised-learning-vs.-unsupervised-learning" class="nav-link" data-scroll-target="#supervised-learning-vs.-unsupervised-learning">Supervised learning vs.&nbsp;unsupervised learning</a></li>
  <li><a href="#terminology" id="toc-terminology" class="nav-link" data-scroll-target="#terminology">Terminology</a></li>
  </ul></li>
  <li><a href="#bayesian-discriminant-rule-or-bayes-classifier" id="toc-bayesian-discriminant-rule-or-bayes-classifier" class="nav-link" data-scroll-target="#bayesian-discriminant-rule-or-bayes-classifier"><span class="header-section-number">5.2</span> Bayesian discriminant rule or Bayes classifier</a></li>
  <li><a href="#normal-bayes-classifier" id="toc-normal-bayes-classifier" class="nav-link" data-scroll-target="#normal-bayes-classifier"><span class="header-section-number">5.3</span> Normal Bayes classifier</a>
  <ul class="collapse">
  <li><a href="#quadratic-discriminant-analysis-qda-and-gaussian-assumption" id="toc-quadratic-discriminant-analysis-qda-and-gaussian-assumption" class="nav-link" data-scroll-target="#quadratic-discriminant-analysis-qda-and-gaussian-assumption">Quadratic discriminant analysis (QDA) and Gaussian assumption</a></li>
  <li><a href="#linear-discriminant-analysis-lda" id="toc-linear-discriminant-analysis-lda" class="nav-link" data-scroll-target="#linear-discriminant-analysis-lda">Linear discriminant analysis (LDA)</a></li>
  <li><a href="#diagonal-discriminant-analysis-dda" id="toc-diagonal-discriminant-analysis-dda" class="nav-link" data-scroll-target="#diagonal-discriminant-analysis-dda">Diagonal discriminant analysis (DDA)</a></li>
  </ul></li>
  <li><a href="#the-training-step-learning-qda-lda-and-dda-classifiers-from-data" id="toc-the-training-step-learning-qda-lda-and-dda-classifiers-from-data" class="nav-link" data-scroll-target="#the-training-step-learning-qda-lda-and-dda-classifiers-from-data"><span class="header-section-number">5.4</span> The training step — learning QDA, LDA and DDA classifiers from data</a>
  <ul class="collapse">
  <li><a href="#number-of-model-parameters" id="toc-number-of-model-parameters" class="nav-link" data-scroll-target="#number-of-model-parameters">Number of model parameters</a></li>
  <li><a href="#estimating-the-discriminant-predictor-function" id="toc-estimating-the-discriminant-predictor-function" class="nav-link" data-scroll-target="#estimating-the-discriminant-predictor-function">Estimating the discriminant / predictor function</a></li>
  <li><a href="#comparison-of-estimated-decision-boundaries-lda-vs.-qda" id="toc-comparison-of-estimated-decision-boundaries-lda-vs.-qda" class="nav-link" data-scroll-target="#comparison-of-estimated-decision-boundaries-lda-vs.-qda">Comparison of estimated decision boundaries: LDA vs.&nbsp;QDA</a></li>
  </ul></li>
  <li><a href="#quantifying-prediction-error" id="toc-quantifying-prediction-error" class="nav-link" data-scroll-target="#quantifying-prediction-error"><span class="header-section-number">5.5</span> Quantifying prediction error</a>
  <ul class="collapse">
  <li><a href="#quantification-of-prediction-error-based-on-validation-data" id="toc-quantification-of-prediction-error-based-on-validation-data" class="nav-link" data-scroll-target="#quantification-of-prediction-error-based-on-validation-data">Quantification of prediction error based on validation data</a></li>
  <li><a href="#estimation-of-prediction-error-using-cross-validation" id="toc-estimation-of-prediction-error-using-cross-validation" class="nav-link" data-scroll-target="#estimation-of-prediction-error-using-cross-validation">Estimation of prediction error using cross-validation</a></li>
  </ul></li>
  <li><a href="#goodness-of-fit-and-variable-ranking" id="toc-goodness-of-fit-and-variable-ranking" class="nav-link" data-scroll-target="#goodness-of-fit-and-variable-ranking"><span class="header-section-number">5.6</span> Goodness of fit and variable ranking</a>
  <ul class="collapse">
  <li><a href="#lda-with-k2-classes" id="toc-lda-with-k2-classes" class="nav-link" data-scroll-target="#lda-with-k2-classes">LDA with <span class="math inline">\(K=2\)</span> classes</a></li>
  <li><a href="#multiple-classes" id="toc-multiple-classes" class="nav-link" data-scroll-target="#multiple-classes">Multiple classes</a></li>
  </ul></li>
  <li><a href="#variable-selection" id="toc-variable-selection" class="nav-link" data-scroll-target="#variable-selection"><span class="header-section-number">5.7</span> Variable selection</a>
  <ul class="collapse">
  <li><a href="#choosing-a-threshold-by-multiple-testing-using-false-discovery-rates" id="toc-choosing-a-threshold-by-multiple-testing-using-false-discovery-rates" class="nav-link" data-scroll-target="#choosing-a-threshold-by-multiple-testing-using-false-discovery-rates">Choosing a threshold by multiple testing using false discovery rates</a></li>
  <li><a href="#variable-selection-using-cross-validation" id="toc-variable-selection-using-cross-validation" class="nav-link" data-scroll-target="#variable-selection-using-cross-validation">Variable selection using cross-validation</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning and classification</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="aims-of-supervised-learning" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="aims-of-supervised-learning"><span class="header-section-number">5.1</span> Aims of supervised learning</h2>
<section id="supervised-learning-vs.-unsupervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="supervised-learning-vs.-unsupervised-learning">Supervised learning vs.&nbsp;unsupervised learning</h3>
<p><strong>Unsupervised learning:</strong></p>
<p>Starting point:</p>
<ul>
<li>unlabelled data <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n\)</span>.</li>
</ul>
<p>Aim: find labels <span class="math inline">\(y_1, \ldots, y_n\)</span> to attach to each sample <span class="math inline">\(\boldsymbol x_i\)</span>.</p>
<p>For discrete labels <span class="math inline">\(y\)</span> unsupervised learning is called <em>clustering</em>.</p>
<p><strong>Supervised learning:</strong></p>
<p>Starting point:</p>
<ul>
<li>labelled <em>training data</em>: <span class="math inline">\(\{\boldsymbol x_1^{\text{train}}, y_1^{\text{train}}\}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\{\boldsymbol x_n^{\text{train}}, y_n^{\text{train}} \}\)</span></li>
<li>In addition, we have unlabelled <em>test data</em>: <span class="math inline">\(\boldsymbol x^{\text{test}}\)</span></li>
</ul>
<p>Aim: use training data to learn a function, say <span class="math inline">\(h(\boldsymbol x)\)</span>, to predict the label corresponding to the test data. The predictor function may provide a soft (probabilistic) assignment or a hard assignment of a class label to a test sample.</p>
<p>For <span class="math inline">\(y\)</span> discrete supervised learning is called <em>classification</em>. For continuous <span class="math inline">\(y\)</span> the label is called response and supervised learning becomes <em>regression</em>.</p>
<p>Thus, supervised learning is a two-step procedure:</p>
<ol type="1">
<li>Learn predictor function <span class="math inline">\(h(\boldsymbol x)\)</span> using the training data <span class="math inline">\(\boldsymbol x_i^{\text{train}}\)</span> plus labels <span class="math inline">\(y_i^{\text{train}}\)</span>.</li>
<li>Predict the label <span class="math inline">\(y^{\text{test}}\)</span> for the test data <span class="math inline">\(\boldsymbol x^{\text{test}}\)</span> using the estimated classifier function: <span class="math inline">\(\hat{y}^{\text{test}} = \hat{h}(\boldsymbol x^{\text{test}})\)</span>.</li>
</ol>
</section>
<section id="terminology" class="level3">
<h3 class="anchored" data-anchor-id="terminology">Terminology</h3>
<p>The function <span class="math inline">\(h(\boldsymbol x)\)</span> that predicts the class <span class="math inline">\(y\)</span> is called a <em>classifier</em>.</p>
<p>There are many types of classifiers, we focus here primarily on probabilistic classifiers (i.e.&nbsp;those that output probabilities for each possible class/label).</p>
<p>The challenge is to find a classifier that</p>
<ul>
<li>explains the current training data well <em>and</em></li>
<li>that also generalises well to future unseen data.</li>
</ul>
<p>Note that it is relatively easy to find a predictor that explains the training data but especially in high dimensions (i.e.&nbsp;with many predictors) there is often overfitting and then the predictor does not generalise well!</p>
<div id="fig-decbound" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decbound-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/decbound.jpg" class="img-fluid figure-img" style="width:90.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decbound-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: Illustration of a decision boundary in a two-group classification problem.
</figcaption>
</figure>
</div>
<p>The <em>decision boundary</em> between the classes is defined as the set of all <span class="math inline">\(\boldsymbol x\)</span> for which the class assignment by the predictor <span class="math inline">\(h(\boldsymbol x)\)</span> switches from one class to another (cf. <a href="#fig-decbound" class="quarto-xref">Figure&nbsp;<span>5.1</span></a>).</p>
<p>In general, simple decision boundaries are preferred over complex decision boundaries to avoid overfitting.</p>
<p>Some commonly used probabilistic methods for classifications:</p>
<ul>
<li>QDA (quadratic discriminant analysis)</li>
<li>LDA (linear discriminant analysis)</li>
<li>DDA (diagonal discriminant analysis),</li>
<li>Naive Bayes classification</li>
<li>logistic regression</li>
</ul>
<p>Depending on how the classifiers are trainined there are many variations of the above methods, e.g.&nbsp;Fisher discriminant analysis, regularised LDA, shrinkage disciminant analysis etc.</p>
<p>Common non-probabilistic methods for classification include:</p>
<ul>
<li><span class="math inline">\(k\)</span>-NN (Nearest Neigbors)</li>
<li>SVM (support vector machine),</li>
<li>random forest</li>
<li>neural networks</li>
</ul>
<p>Neural networks may in fact also be counted under probabilistic models as they essentially are high-dimensional complex nonlinear regression models. Just like a linear regression model can be fitted by “least squares” only without assuming an explicit probabilistic model neural networks are also most often trained by optimising a loss function.</p>
</section>
</section>
<section id="bayesian-discriminant-rule-or-bayes-classifier" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="bayesian-discriminant-rule-or-bayes-classifier"><span class="header-section-number">5.2</span> Bayesian discriminant rule or Bayes classifier</h2>
<p>Bayes classifiers are based on mixture models:</p>
<ul>
<li><span class="math inline">\(K\)</span> groups with <span class="math inline">\(K\)</span> prespecified</li>
<li>each group has its own distribution <span class="math inline">\(F_k\)</span> with own parameters <span class="math inline">\(\boldsymbol \theta_k\)</span></li>
<li>the density of each class is <span class="math inline">\(f_k(\boldsymbol x) = f(\boldsymbol x| k)\)</span>.</li>
<li>prior probability of group <span class="math inline">\(k\)</span> is <span class="math inline">\(\text{Pr}(k) = \pi_k\)</span> with <span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span></li>
<li>marginal density is the mixture <span class="math inline">\(f(\boldsymbol x) = \sum_{k=1}^K \pi_k f_k(\boldsymbol x)\)</span></li>
</ul>
<p>For the moment we do not assume anything particular about the conditional densities <span class="math inline">\(f_k(\boldsymbol x)\)</span> but later (see the following section) we will focus on normal densities and hence normal classifiers.</p>
<p>The posterior probability of group <span class="math inline">\(k\)</span> is given by <span class="math display">\[
\text{Pr}(k | \boldsymbol x) = \frac{\pi_k f_k(\boldsymbol x) }{ f(\boldsymbol x)}
\]</span></p>
<p>This provides a “soft” classification <span class="math display">\[\boldsymbol h(\boldsymbol x^{\text{test}}) = (\text{Pr}(k=1 | \boldsymbol x^{\text{test}}),\ldots, \text{Pr}(k=K | \boldsymbol x^{\text{test}})   )^T\]</span> where each possible class <span class="math inline">\(k \in \{ 1, \ldots, K\}\)</span> is assigned a probability to be the label for the test sample <span class="math inline">\(\boldsymbol x\)</span>.</p>
<p>The <em>discriminant function</em> is defined as the logarithm of the posterior probability: <span class="math display">\[
d_k(\boldsymbol x) = \log \text{Pr}(k | \boldsymbol x) = \log \pi_k  + \log f_k(\boldsymbol x)  - \log f(\boldsymbol x)
\]</span> Since we use <span class="math inline">\(d_k\)</span> to compare the different classes <span class="math inline">\(k\)</span> we can simplify the discriminant function by dropping all constant terms that do not depend on <span class="math inline">\(k\)</span> — in the above this is the term <span class="math inline">\(\log f(\boldsymbol x)\)</span>. Hence we get for the Bayes discriminant function <span class="math display">\[
d_k(\boldsymbol x) = \log \pi_k + \log f_k(\boldsymbol x) \,.
\]</span></p>
<p>For subsequent “hard” classification <span class="math inline">\(h(\boldsymbol x^{\text{test}})\)</span> we then select the group/label for which the value of the discriminant function is maximised: <span class="math display">\[
\hat{y}^{\text{test}} = h(\boldsymbol x^{\text{test}}) = \arg \max_k d_k(\boldsymbol x^{\text{test}}) \,.
\]</span></p>
<p>We have already encountered the Bayes classifier in the EM algorithm to predict the state of the latent variables (soft assignment) and in the <span class="math inline">\(K\)</span>-means algorithm (hard assignment).</p>
<p>The discriminant functions <span class="math inline">\(d_k(\boldsymbol x)\)</span> can be mapped back to the probabilistic class assignment by using the softargmax function (also known as softmax function): <span class="math display">\[
\text{Pr}(k | \boldsymbol x) =
\frac{\exp( d_k(\boldsymbol x) )}{\sum_{c=1}^K \exp( d_c(\boldsymbol x) ) }  \,.
\]</span> In practise this will be calculated as <span class="math display">\[
\text{Pr}(k | \boldsymbol x) =
\frac{\exp( d_k(\boldsymbol x) - d_{\max}(\boldsymbol x) ) }{\sum_{c=1}^K \exp( d_c(\boldsymbol x) - d_{\max}(\boldsymbol x) ) } \,.
\]</span> because subtracting <span class="math inline">\(d_{\max}(\boldsymbol x) = \max\{ d_1(\boldsymbol x), \ldots,  d_K(\boldsymbol x) \}\)</span> from all discriminant functions, and thus standardising the maximum of the discriminant functions to zero, avoids numerical overflow problems when computing the exponential function on a computer.</p>
</section>
<section id="normal-bayes-classifier" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="normal-bayes-classifier"><span class="header-section-number">5.3</span> Normal Bayes classifier</h2>
<section id="quadratic-discriminant-analysis-qda-and-gaussian-assumption" class="level3">
<h3 class="anchored" data-anchor-id="quadratic-discriminant-analysis-qda-and-gaussian-assumption">Quadratic discriminant analysis (QDA) and Gaussian assumption</h3>
<p>Quadratic discriminant analysis (QDA) is a special case of the Bayes classifier when all group-specific densities are multivariate normal with <span class="math inline">\(f_k(\boldsymbol x) = N(\boldsymbol x| \boldsymbol \mu_k, \boldsymbol \Sigma_k)\)</span>. Note in particular that each group <span class="math inline">\(k \in \{1, \ldots, K\}\)</span> has its own covariance matrix <span class="math inline">\(\boldsymbol \Sigma_k\)</span>.</p>
<p>Some calculation leads to the discriminant function for QDA: <span class="math display">\[
d_k^{QDA}(\boldsymbol x) = -\frac{1}{2} (\boldsymbol x-\boldsymbol \mu_k)^T \boldsymbol \Sigma_k^{-1} (\boldsymbol x-\boldsymbol \mu_k) -\frac{1}{2} \log \det(\boldsymbol \Sigma_k) +\log(\pi_k)
\]</span></p>
<p>There are a number of noteworthy things here:</p>
<ul>
<li>Again terms are dropped that do not depend on <span class="math inline">\(k\)</span>, such as <span class="math inline">\(-\frac{d}{2}\log( 2\pi)\)</span>.</li>
<li>Note the appearance of the squared <strong>Mahalanobis distance</strong> between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol \mu_k\)</span>: <span class="math inline">\(d^{\text{Mahalanobis}}(\boldsymbol x, \boldsymbol \mu| \boldsymbol \Sigma)^2 = (\boldsymbol x-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu)\)</span>.</li>
<li>The <strong>QDA discriminant function is quadratic in <span class="math inline">\(\boldsymbol x\)</span></strong> - hence its name!<br>
This implies that the <strong>decision boundaries for QDA classification are quadratic</strong> (i.e.&nbsp;parabolas in two dimensional settings).</li>
</ul>
<p>For Gaussian models specifically it can useful be to multiply the discriminant function by -2 to get rid of the factor <span class="math inline">\(-\frac{1}{2}\)</span>, but note that in that case we then need to find the minimum of the discriminant function rather than the maximum: <span class="math display">\[
d_k^{QDA (v2)}(\boldsymbol x) =  (\boldsymbol x-\boldsymbol \mu_k)^T \boldsymbol \Sigma_k^{-1} (\boldsymbol x-\boldsymbol \mu_k) + \log \det(\boldsymbol \Sigma_k)  -2 \log(\pi_k)
\]</span> In the literature you will find both versions of Gaussian discriminant functions so you need to check carefully which convention is used. In the following we will use the first version only.</p>
<p>Decision boundaries for the QDA classifier can be either linear or nonlinear (quadratic curve). The decision boundary between any two classes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> require that <span class="math inline">\(d^{QDA}_i(\boldsymbol x) = d^{QDA}_j(\boldsymbol x)\)</span>, or equivalently &lt;<span class="math inline">\(d^{QDA}_i(\boldsymbol x) - d^{QDA}_j(\boldsymbol x) = 0\)</span>, which is a quadratic equation.</p>
</section>
<section id="linear-discriminant-analysis-lda" class="level3">
<h3 class="anchored" data-anchor-id="linear-discriminant-analysis-lda">Linear discriminant analysis (LDA)</h3>
<p>LDA is a special case of QDA, with the assumption of common overall covariance across all groups: <span class="math inline">\(\boldsymbol \Sigma_k = \boldsymbol \Sigma\)</span>.</p>
<p>This leads to a simplified discriminant function: <span class="math display">\[
d_k^{LDA}(\boldsymbol x) = -\frac{1}{2} (\boldsymbol x-\boldsymbol \mu_k)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu_k) +\log(\pi_k)
\]</span> Note that term containing the log-determinant is now gone, and that LDA is essentially now a method that tries to minimize the Mahalanobis distance (while taking also into account the prior class probabilities).</p>
<p>The above function can be further simplified, by noting that the quadratic term <span class="math inline">\(\boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol x\)</span> does not depend on <span class="math inline">\(k\)</span> and hence can be dropped: <span class="math display">\[
\begin{split}
d_k^{LDA}(\boldsymbol x) &amp;=  \boldsymbol \mu_k^T \boldsymbol \Sigma^{-1} \boldsymbol x- \frac{1}{2}\boldsymbol \mu_k^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_k + \log(\pi_k) \\
  &amp;= \boldsymbol b^T \boldsymbol x+ a
\end{split}
\]</span> Thus, the <strong>LDA discriminant function is linear in <span class="math inline">\(\boldsymbol x\)</span>, and hence the resulting decision boundaries are linear</strong> as well (i.e.&nbsp;straight lines in two-dimensional settings)</p>
<div id="fig-ldaqdaboundary" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ldaqdaboundary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/ldaqda-boundary.jpg" class="img-fluid figure-img" style="width:90.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ldaqdaboundary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: Comparison of the linear decision boundary for LDA (left) compared with the nonlinear boundary for QDA (right.
</figcaption>
</figure>
</div>
<p><a href="#fig-ldaqdaboundary" class="quarto-xref">Figure&nbsp;<span>5.2</span></a> shows an comparison of the linear decision boundaries of LDA compared with the nonlinear boundaries for QDA.</p>
<p>Note that logistic regression (cf.&nbsp;GLM module) takes on exactly the above linear form and is indeed closely linked with the LDA classifier.</p>
</section>
<section id="diagonal-discriminant-analysis-dda" class="level3">
<h3 class="anchored" data-anchor-id="diagonal-discriminant-analysis-dda">Diagonal discriminant analysis (DDA)</h3>
<p>For DDA we start with the same setting as for LDA, but now we simplify the model even further by additionally requiring a <strong>diagonal covariance</strong> containing only the variances (thus we assume that all correlations among the predictors <span class="math inline">\(x_1, \ldots, x_d\)</span> are zero): <span class="math display">\[
\boldsymbol \Sigma= \boldsymbol V= \begin{pmatrix}
    \sigma^2_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma^2_{d}
\end{pmatrix}
\]</span> This simplifies the inversion of <span class="math inline">\(\boldsymbol \Sigma\)</span> as <span class="math display">\[
\boldsymbol \Sigma^{-1} = \boldsymbol V^{-1} = \begin{pmatrix}
    \sigma^{-2}_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma^{-2}_{d}
\end{pmatrix}
\]</span> and leads to the discriminant function <span class="math display">\[
\begin{split}
d_k^{DDA}(\boldsymbol x) &amp;=  \boldsymbol \mu_k^T \boldsymbol V^{-1} \boldsymbol x- \frac{1}{2}\boldsymbol \mu_k^T \boldsymbol V^{-1} \boldsymbol \mu_k + \log(\pi_k) \\
  &amp;= \sum_{j=i}^d \frac{\mu_{k,j} x_j - \mu_{k,j}^2/2}{\sigma_d^2} + \log(\pi_k)
\end{split}
\]</span> As special case of LDA, the <strong>DDA classifier is a linear classifier</strong> and thus has linear decision boundaries.</p>
<p>The <strong>Bayes classifier</strong> (using any distribution) <strong>assuming uncorrelated predictors</strong> is also known as the <strong>naive Bayes classifier</strong>.</p>
<p>Hence, <strong>DDA is a naive Bayes classifier</strong> assuming that the underlying densities are normal.</p>
<p>However, don’t let the label “naive” mislead you as DDA and other “naive” Bayes classifiers are often very effective methods for classification and prediction, especially in high-dimensional settings!</p>
</section>
</section>
<section id="the-training-step-learning-qda-lda-and-dda-classifiers-from-data" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="the-training-step-learning-qda-lda-and-dda-classifiers-from-data"><span class="header-section-number">5.4</span> The training step — learning QDA, LDA and DDA classifiers from data</h2>
<section id="number-of-model-parameters" class="level3">
<h3 class="anchored" data-anchor-id="number-of-model-parameters">Number of model parameters</h3>
<p>In order to predict the class for new data using any of the above discriminant functions we need to first learn the underlying parameters from the training data <span class="math inline">\(\boldsymbol x_i^{\text{train}}\)</span> and <span class="math inline">\(y_i^{\text{train}}\)</span>:</p>
<ul>
<li>For QDA, LDA and DDA we need to learn <span class="math inline">\(\pi_1, \ldots, \pi_K\)</span> with <span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span> and the mean vectors <span class="math inline">\(\boldsymbol \mu_1, \ldots, \boldsymbol \mu_K\)</span></li>
<li>For QDA we additionally require <span class="math inline">\(\boldsymbol \Sigma_1, \ldots, \boldsymbol \Sigma_K\)</span></li>
<li>For LDA we need <span class="math inline">\(\boldsymbol \Sigma\)</span></li>
<li>For DDA we estimate <span class="math inline">\(\sigma^2_1, \ldots, \sigma^2_d\)</span>.</li>
</ul>
<p>Overall, the total number of parameters to be estimated when learning the discriminant functions from training data is as follows (see also <a href="#fig-modelcomplexity" class="quarto-xref">Figure&nbsp;<span>5.3</span></a>):</p>
<ul>
<li>QDA: <span class="math inline">\(K-1+ K d + K \frac{d(d+1)}{2}\)</span></li>
<li>LDA: <span class="math inline">\(K-1+ K d + \frac{d(d+1)}{2}\)</span></li>
<li>DDA: <span class="math inline">\(K-1+ K d + d\)</span></li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div id="fig-modelcomplexity" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-modelcomplexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05-classification_files/figure-html/fig-modelcomplexity-1.png" class="img-fluid figure-img" data-fig-pos="t" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-modelcomplexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.3: Comparison of model complexity of QDA, LDA and DDA.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="estimating-the-discriminant-predictor-function" class="level3">
<h3 class="anchored" data-anchor-id="estimating-the-discriminant-predictor-function">Estimating the discriminant / predictor function</h3>
<p>For QDA, LDA and DDA we learn the predictor by estimating the parameters of the discriminant function from the training data.</p>
<section id="large-sample-size" class="level4">
<h4 class="anchored" data-anchor-id="large-sample-size">Large sample size</h4>
<p>If the sample size of the training data set is sufficiently large compared to the model dimensions we can use maximum likelihood (ML) to estimate the model parameters. To be able to use ML we need a larger sample size for QDA and LDA (because full covariances need to be estimated) but for DDA a comparatively small sample size can be sufficient (which is one aspect why “naive” Bayes methods are very popular in practise).</p>
<p>To obtain the parameters estimates we use the known labels <span class="math inline">\(y_i^{\text{train}}\)</span> to sort the samples <span class="math inline">\(\boldsymbol x_i^{\text{train}}\)</span> into the corresponding classes, and then apply the standard ML estimators. Let <span class="math inline">\(g_k =\{i: y_i^{\text{train}}=k  \}\)</span> be the set of all indices of training sample belonging to group <span class="math inline">\(k\)</span>, <span class="math inline">\(n_k\)</span> the sample size in group <span class="math inline">\(k\)</span></p>
<p>The ML estimates of the class probabilities are the frequencies <span class="math display">\[
\hat{\pi}_k = \frac{n_k}{n}
\]</span> and the ML estimate of the group means <span class="math inline">\(k=1, \ldots, K\)</span> are <span class="math display">\[
\hat{\boldsymbol \mu}_k = \frac{1}{n_k} \sum_{i \in g_k} \boldsymbol x_i^{\text{train}} \, .
\]</span> The ML estimate of the global mean <span class="math inline">\(\boldsymbol \mu_0\)</span> (i.e.&nbsp;if we assume there is only a single class and ignore the group labels) is <span class="math display">\[
\hat{\boldsymbol \mu}_0 = \frac{1}{n} \sum_{i=1}^n \boldsymbol x_i^{\text{train}} = \sum_{k=1}^K \hat{\pi}_k \hat{\boldsymbol \mu}_k
\]</span> Note the global mean is identical to the pooled mean (i.e.&nbsp;weighted average of the individual group means).</p>
<p>The ML estimates for the covariances <span class="math inline">\(\boldsymbol \Sigma_k\)</span> for QDA are <span class="math display">\[
\widehat{\boldsymbol \Sigma}_k = \frac{1}{n_k} \sum_{i \in g_k} ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_k) ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_k)^T
\]</span></p>
<p>In order to get the ML estimate of the pooled variance <span class="math inline">\(\boldsymbol \Sigma\)</span> for use with LDA we compute <span class="math display">\[
\widehat{\boldsymbol \Sigma} = \frac{1}{n} \sum_{k=1}^K \sum_{i \in g_k} ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_k) ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_k)^T =  \sum_{k=1}^K \hat{\pi}_k \widehat{\boldsymbol \Sigma}_k
\]</span></p>
<p>Note that the pooled variance <span class="math inline">\(\boldsymbol \Sigma\)</span> differs (substantially!) from the global variance <span class="math inline">\(\Sigma_0\)</span> that results from simply ignoring class labels and that is computed as <span class="math display">\[
\widehat{\boldsymbol \Sigma}_0^{ML} = \frac{1}{n} \sum_{i =1}^n ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_0) ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_0)^T
\]</span> You will recognise the above from the variance decomposition in mixture models, with <span class="math inline">\(\boldsymbol \Sigma_0\)</span> being the total variance and the pooled <span class="math inline">\(\boldsymbol \Sigma\)</span> the unexplained/with-in group variance.</p>
</section>
<section id="small-sample-size" class="level4">
<h4 class="anchored" data-anchor-id="small-sample-size">Small sample size</h4>
<p>If the dimension <span class="math inline">\(d\)</span> is large compared to the sample size then the number of parameters in the predictor function grows fast. Especially QDA but also LDA is data hungry and ML estimation becomes an ill-posed problem.</p>
<p>As discussed in Section 1.3 in this instance we need to use a regularised estimator for the covariance(s) such as estimators derived in the framework of penalised ML, Bayesian learning, shrinkage estimation etc. This also ensures that the estimated covariance matrices are positive definite (which is automatically guaranteed only for DDA if all variances are positive).</p>
<p>Furthermore, in small sample setting it is advised to reduce the number of parameters of the model. Thus using LDA or DDA is preferred over QDA. This can also prevent overfitting and lead to a predictor that generalises better.</p>
<p>To analyse high-dimensional data in the worksheets we will employ a regularised version of LDA and DDA using Stein-type shrinkage estimation as discussed in Section 1.3 and implemented in the R package “sda”.</p>
</section>
</section>
<section id="comparison-of-estimated-decision-boundaries-lda-vs.-qda" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-estimated-decision-boundaries-lda-vs.-qda">Comparison of estimated decision boundaries: LDA vs.&nbsp;QDA</h3>
<div id="fig-ldaqdanonnested" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ldaqdanonnested-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/ldaqda-nonnested.png" class="img-fluid figure-img" style="width:90.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ldaqdanonnested-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.4: Decision boundaries for LDA and QDA in the non-nested case.
</figcaption>
</figure>
</div>
<div id="fig-ldaqdanested" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ldaqdanested-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/ldaqda-nested.png" class="img-fluid figure-img" style="width:90.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ldaqdanested-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.5: Decision boundaries for LDA and QDA in the nested case.
</figcaption>
</figure>
</div>
<p>We compare two simple scenarios using simulated data.</p>
<p><strong>Non-nested case (<span class="math inline">\(K=4\)</span>):</strong> See <a href="#fig-ldaqdanonnested" class="quarto-xref">Figure&nbsp;<span>5.4</span></a>. Both LDA and QDA clearly separate the 4 classes. Note the curved decision boundaries for QDA and the linear decision boundaries for LDA.</p>
<p><strong>Nested case (<span class="math inline">\(K=2\)</span>):</strong> See <a href="#fig-ldaqdanested" class="quarto-xref">Figure&nbsp;<span>5.5</span></a>. In the nested case LDA fails to separate the two classes because there is no way to separate two nested classes with a simple linear boundary.</p>
<p>Later we will investigate the decision boundaries for further methods (see <a href="07-nonlinear.html#fig-dtrfnonnested" class="quarto-xref">Figure&nbsp;<span>7.1</span></a> and <a href="07-nonlinear.html#fig-dtrfnested" class="quarto-xref">Figure&nbsp;<span>7.2</span></a>).</p>
</section>
</section>
<section id="quantifying-prediction-error" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="quantifying-prediction-error"><span class="header-section-number">5.5</span> Quantifying prediction error</h2>
<p>Once a classifier has been trained we are naturally interested in its performance to correctly classify previously unseen data points. This is useful for comparing different types of classifiers and also for comparing the same type of classifier using different sets of predictor variables.</p>
<section id="quantification-of-prediction-error-based-on-validation-data" class="level3">
<h3 class="anchored" data-anchor-id="quantification-of-prediction-error-based-on-validation-data">Quantification of prediction error based on validation data</h3>
<p>A measure of predictor error compares the predicted label <span class="math inline">\(\hat{y}\)</span> with the true label <span class="math inline">\(y\)</span> for validation data. A validation data set contains both the <span class="math inline">\(\boldsymbol x_i\)</span> and the associated label <span class="math inline">\(y_i\)</span> but unlike the training data it has not been used for learning the predictor function.</p>
<p>For continuous response often the squared loss is used: <span class="math display">\[
\text{err}(\hat{y}, y) =  (\hat{y} - y)^2
\]</span></p>
<p>For binary outcomes one often employs the 0/1 loss: <span class="math display">\[
\text{err}(\hat{y}, y) =
\begin{cases}
    0, &amp; \text{if  } \hat{y}=y\\
    1,  &amp; \text{otherwise}
\end{cases}
\]</span> Alternatively, any other quantity derived from the confusion matrix (containing TP, TN, FP, FN) can be used.</p>
<p>The mean prediction error is the expectation <span class="math display">\[
PE = \text{E}(\text{err}(\hat{y}, y))
\]</span> and thus the empirical mean prediction error is <span class="math display">\[
\widehat{PE} = \frac{1}{m} \sum_{i=1}^m \text{err}(\hat{y}_i, y_i)
\]</span> where <span class="math inline">\(m\)</span> is the sample size of the validation data set.</p>
<p>More generally, we can also quantify prediction error in the framework of so-called <strong>proper scoring rules</strong>, where the whole probabilistic forecast is taken into account (e.g.&nbsp;the individual probabilities for each class, rather than just the selected most probable class). A commonly used scoring rule is the negative log-probability (“surprise”), and the expected surprise is the cross-entropy. So this leads back to entropy and likelihood (see <a href="https://strimmerlab.github.io/publications/lecture-notes/MATH27720-stats2/">MATH27720 Statistics 2</a>).</p>
<p>Once we have an estimate of the prediction error of a model we can use it to compare and choose among a set of candidate models, selecting those with a sufficiently low prediction error.</p>
</section>
<section id="estimation-of-prediction-error-using-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="estimation-of-prediction-error-using-cross-validation">Estimation of prediction error using cross-validation</h3>
<p>Unfortunately, quite often we do not have separate validation data available to evaluate a classifier.</p>
<p>In this case we need to rely on a simple algorithmic procedure called <strong>cross-validation</strong>.</p>
<p>Outline of cross-validation:</p>
<ol type="1">
<li>split the samples in the training data into a number (say <span class="math inline">\(K\)</span>) parts (“folds”).</li>
<li>use each of the <span class="math inline">\(K\)</span> folds as validation data and the other <span class="math inline">\(K-1\)</span> folds as training data.</li>
<li>average over the resulting <span class="math inline">\(K\)</span> individual estimates of prediction error, to get an overall aggregated predictor error, along with an error.</li>
</ol>
<p>Note that in each case one part of the data is reserved for validation and <em>not</em> used for training the predictor.</p>
<p>We choose <span class="math inline">\(K\)</span> such that the folds are not too small (to allow estimation of prediction error) but also not too large (to make sure that we actually are able to train a reliable classifier from the remaining data). A typical value for <span class="math inline">\(K\)</span> is 5 or 10, so that 80% respectively 90% of the samples are used for training and the other 20 % or 10% for validation.</p>
<p>If <span class="math inline">\(K=n\)</span> there are as many folds as there are samples and the validation data set consists only of a single data point. This is called “leave one out” cross-validation (LOOCV). There are analytic approximations for the prediction error obtained by LOOCV so that this approach is computationally inexpensive for some standard models (including regression).</p>
<p><strong>Further reading:</strong></p>
<p>To study the technical details of cross-validation: read <strong>Section 5.1 Cross-Validation</strong> in <span class="citation" data-cites="JWHT2021">James et al. (<a href="bibliography.html#ref-JWHT2021" role="doc-biblioref">2021</a>)</span> (R version) or <span class="citation" data-cites="JWHTT2023">James et al. (<a href="bibliography.html#ref-JWHTT2023" role="doc-biblioref">2023</a>)</span> (Python version).</p>
</section>
</section>
<section id="goodness-of-fit-and-variable-ranking" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="goodness-of-fit-and-variable-ranking"><span class="header-section-number">5.6</span> Goodness of fit and variable ranking</h2>
<p>As in linear regression we are interested in finding out whether the fitted mixture model is an appropriate model, and which particular predictor(s) <span class="math inline">\(x_j\)</span> from <span class="math inline">\(\boldsymbol x=(x_1, \ldots, x_d)^T\)</span> are responsible prediction the outcome, i.e.&nbsp;for categorising a sample into group <span class="math inline">\(k\)</span>.</p>
<p>In order to study these problem it is helpful to rewrite the discriminant function to highlight the influence (or importance) of each predictor.</p>
<p>We focus on linear methods (LDA and DDA) and first look at the simple case <span class="math inline">\(K=2\)</span> and then generalise to more than two groups.</p>
<section id="lda-with-k2-classes" class="level3">
<h3 class="anchored" data-anchor-id="lda-with-k2-classes">LDA with <span class="math inline">\(K=2\)</span> classes</h3>
<p>For two classes using the LDA discriminant rule will choose group <span class="math inline">\(k=1\)</span> if <span class="math inline">\(d_1^{LDA}(\boldsymbol x) &gt; d_2^{LDA}(\boldsymbol x)\)</span>, or equivalently, if <span class="math display">\[
\Delta_{12}^{LDA} = d_1^{LDA}(\boldsymbol x) - d_2^{LDA}(\boldsymbol x) &gt; 0
\]</span> Since <span class="math inline">\(d_k(\boldsymbol x)\)</span> is the log-posterior (plus/minus identical constants) <span class="math inline">\(\Delta^{LDA}\)</span> is in fact the <strong>log-posterior odds of class 1 versus class 2</strong>.</p>
<p>The difference <span class="math inline">\(\Delta_{12}^{LDA}\)</span> is <span class="math display">\[
\underbrace{ \Delta_{12}^{LDA}}_{\text{log posterior odds}} =
\underbrace{(\boldsymbol \mu_1 -\boldsymbol \mu_2)^T \boldsymbol \Sigma^{-1} \left(\boldsymbol x- \frac{\boldsymbol \mu_1+\boldsymbol \mu_2}{2}\right)}_{\text{log Bayes factor } \log B_{12}} + \underbrace{\log\left( \frac{\pi_1}{\pi_2} \right)}_{\text{log prior odds}}
\]</span> Note that since we only consider simple non-composite models here the log-Bayes factor is identical with the log-likelihood ratio!</p>
<p>The log Bayes factor <span class="math inline">\(\log B_{12}\)</span> is known as the <em>weight of evidence</em> in favour of <span class="math inline">\(F_1\)</span> given <span class="math inline">\(\boldsymbol x\)</span>. The <em>expected weight of evidence</em> assuming <span class="math inline">\(\boldsymbol x\)</span> is indeed from <span class="math inline">\(F_1\)</span> is the Kullback-Leibler discrimination information in favour of group&nbsp;1, i.e.&nbsp;the KL divergence of from distribution <span class="math inline">\(F_2\)</span> to <span class="math inline">\(F_1\)</span>: <span class="math display">\[
\text{E}_{F_1} ( \log B_{12} ) = D_{\text{KL}}(F_1,  F_2) = \frac{1}{2} (\boldsymbol \mu_1 -\boldsymbol \mu_2)^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu_1 -\boldsymbol \mu_2) = \frac{1}{2} \Omega^2
\]</span> This yields, apart of a scale factor, a population version of the Hotelling <span class="math inline">\(T^2\)</span> statistic defined as <span class="math display">\[T^2 =  c^2 (\hat{\boldsymbol \mu}_1 -\hat{\boldsymbol \mu}_2)^T \hat{\boldsymbol \Sigma}^{-1} (\hat{\boldsymbol \mu}_1 -\hat{\boldsymbol \mu}_2)\]</span> where <span class="math inline">\(c = (\frac{1}{n_1} + \frac{1}{n_2})^{-1/2} = \sqrt{n \pi_1 \pi_2}\)</span> is a sample size dependent factor (for <span class="math inline">\(\text{SD}(\hat{\boldsymbol \mu}_1 - \hat{\boldsymbol \mu}_2)\)</span>). <span class="math inline">\(T^2\)</span> is a measure of fit of the underlying two-component mixture.</p>
<p>Using the whitening transformation with <span class="math inline">\(\boldsymbol z= \boldsymbol W\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol W^T \boldsymbol W= \boldsymbol \Sigma^{-1}\)</span> we can rewrite the log Bayes factor as <span class="math display">\[
\begin{split}
\log B_{12} &amp;= \left( (\boldsymbol \mu_1 -\boldsymbol \mu_2)^T \boldsymbol W^T \right)\, \left(\boldsymbol W\left(\boldsymbol x- \frac{\boldsymbol \mu_1+\boldsymbol \mu_2}{2}\right) \right) \\
&amp;=\boldsymbol \omega^T \boldsymbol \delta(\boldsymbol x)
\end{split}
\]</span> i.e.&nbsp;as the product of two vectors:</p>
<ul>
<li><span class="math inline">\(\boldsymbol \delta(\boldsymbol x)\)</span> is the whitened <span class="math inline">\(\boldsymbol x\)</span> (centered around average means) and</li>
<li><span class="math inline">\(\boldsymbol \omega= (\omega_1, \ldots, \omega_d)^T = \boldsymbol W(\boldsymbol \mu_1 -\boldsymbol \mu_2)\)</span> gives the weight of each whitened component <span class="math inline">\(\boldsymbol \delta(\boldsymbol x)\)</span> in the log Bayes factor.</li>
</ul>
<p>A large positive or negative value of <span class="math inline">\(\omega_j\)</span> indicates that the corresponding whitened predictor is relevant for choosing a class, whereas small values of <span class="math inline">\(\omega_j\)</span> close to zero indicate that the corresponding ZCA whitened predictor is unimportant. Furthermore, <span class="math inline">\(\boldsymbol \omega^T \boldsymbol \omega= \sum_{j=1}^d \omega_j^2 = (\boldsymbol \mu_1 -\boldsymbol \mu_2)^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu_1 -\boldsymbol \mu_2) = \Omega^2\)</span>, i.e.&nbsp;the squared <span class="math inline">\(\omega_j^2\)</span> provide a component-wise decomposition of the overall fit <span class="math inline">\(\Omega^2\)</span>.</p>
<p>Choosing ZCA-cor as whitening transformation with <span class="math inline">\(\boldsymbol W=\boldsymbol P^{-1/2} \boldsymbol V^{-1/2}\)</span> we get <span class="math display">\[
\boldsymbol \omega^{ZCA-cor} = \boldsymbol P^{-1/2} \boldsymbol V^{-1/2} (\boldsymbol \mu_1 -\boldsymbol \mu_2)
\]</span> A better understanding of <span class="math inline">\(\boldsymbol \omega^{ZCA-cor}\)</span> is provided by comparing with the two-sample <span class="math inline">\(t\)</span>-statistic <span class="math display">\[
\hat{\boldsymbol \tau} = c \hat{\boldsymbol V}^{-1/2} (\hat{\boldsymbol \mu}_1 - \hat{\boldsymbol \mu}_2)
\]</span> With <span class="math inline">\(\boldsymbol \tau\)</span> the population version of <span class="math inline">\(\hat{\boldsymbol \tau}\)</span> we can define <span class="math display">\[\boldsymbol \tau^{adj} = \boldsymbol P^{-1/2} \boldsymbol \tau= c \boldsymbol \omega^{ZCA-cor}\]</span> as correlation-adjusted <span class="math inline">\(t\)</span>-scores (cat scores). With <span class="math inline">\(({\hat{\boldsymbol \tau}}^{adj})^T {\hat{\boldsymbol \tau}}^{adj} = T^2\)</span> we can see that the cat scores offer a component-wise decomposition of Hotelling’s <span class="math inline">\(T^2\)</span>.</p>
<p>Note the choice of ZCA-cor whitening is to ensure that the whitened components are interpretable and stay maximally correlated to the original variables. However, you may also choose for example PCA whitening in which case the <span class="math inline">\(\boldsymbol \omega^T \boldsymbol \omega\)</span> provide the variable importance for the PCA whitened variables.</p>
<p>For DDA, which assumes that correlations among predictors vanish, i.e.&nbsp;<span class="math inline">\(\boldsymbol P= \boldsymbol I_d\)</span>, we get <span class="math display">\[
\Delta_{12}^{DDA} =\underbrace{ \left( (\boldsymbol \mu_1 -\boldsymbol \mu_2)^T \boldsymbol V^{-1/2}  \right)}_{\text{ } c^{-1} \boldsymbol \tau^T }\, \underbrace{ \left( \boldsymbol V^{-1/2} \left(\boldsymbol x- \frac{\boldsymbol \mu_1+\boldsymbol \mu_2}{2}\right) \right) }_{\text{centered standardised predictor}}+ \log\left( \frac{\pi_1}{\pi_2} \right) \\
\]</span> Similarly as above, the <span class="math inline">\(t\)</span>-score <span class="math inline">\(\boldsymbol \tau\)</span> determines the impact of the standardised predictor in <span class="math inline">\(\Delta^{DDA}\)</span>.</p>
<p>Consequently, in DDA we can rank predictors by the squared <span class="math inline">\(t\)</span>-score. Recall that in standard linear regression with uncorrelated predictors we can find the most important predictors by ranking the squared marginal correlations – ranking by (squared) <span class="math inline">\(t\)</span>-scores in DDA is the exact analogy but for discrete response.</p>
</section>
<section id="multiple-classes" class="level3">
<h3 class="anchored" data-anchor-id="multiple-classes">Multiple classes</h3>
<p>For more than two classes we need to refer to the so-called <strong>pooled centroids formulation</strong> of DDA and LDA (introduced by Tibshirani 2002).</p>
<p>The pooled centroid is given by <span class="math inline">\(\boldsymbol \mu_0 = \sum_{k=1}^K \pi_k \boldsymbol \mu_k\)</span> — this is the centroid if there would be only a single class. The corresponding probability (for a single class) is <span class="math inline">\(\pi_0=1\)</span> and the distribution is called <span class="math inline">\(F_0\)</span>.</p>
<p>The LDA discriminant function for this “group 0” is <span class="math display">\[
d_0^{LDA}(\boldsymbol x) = \boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol x- \frac{1}{2}\boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0
\]</span> and the log posterior odds for comparison of group <span class="math inline">\(k\)</span> with the pooled group <span class="math inline">\(0\)</span> is <span class="math display">\[
\begin{split}
\Delta_k^{LDA} &amp;= d_k^{LDA}(\boldsymbol x) - d_0^{LDA}(\boldsymbol x) \\
         &amp;= \log B_{k0} + \log(\pi_k) \\
         &amp;= \boldsymbol \omega_k^T \boldsymbol \delta_k(\boldsymbol x) + \log(\pi_k)
\end{split}
\]</span> with <span class="math display">\[
\boldsymbol \omega_k = \boldsymbol W(\boldsymbol \mu_k - \boldsymbol \mu_0)  
\]</span> and <span class="math display">\[
\boldsymbol \delta_k(\boldsymbol x) = \boldsymbol W(\boldsymbol x- \frac{\boldsymbol \mu_k +\boldsymbol \mu_0}{2} )
\]</span> The expected log Bayes factor is <span class="math display">\[
\text{E}_{F_k} ( \log B_{k0} )= KL(F_k || F_0) = \frac{1}{2} (\boldsymbol \mu_k -\boldsymbol \mu_0)^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu_k -\boldsymbol \mu_0) = \frac{1}{2} \Omega_k^2
\]</span></p>
<p>With scale factor <span class="math inline">\(c_k  = (\frac{1}{n_k} - \frac{1}{n})^{-1/2} = \sqrt{n \frac{\pi_k}{1-\pi_k}}\)</span> (for <span class="math inline">\(\text{SD}(\hat{\boldsymbol \mu}_k-\hat{\boldsymbol \mu}_0)\)</span>, with the minus sign before <span class="math inline">\(\frac{1}{n}\)</span> due to correlation between <span class="math inline">\(\hat{\boldsymbol \mu}_k\)</span> and pooled mean <span class="math inline">\(\hat{\boldsymbol \mu}_0\)</span>) we get as correlation-adjusted <span class="math inline">\(t\)</span>-score for comparing mean of group <span class="math inline">\(k\)</span> with the pooled mean <span class="math display">\[
\boldsymbol \tau_k^{adj} = c_k \boldsymbol \omega_k^{ZCA-cor} \,.
\]</span></p>
<p>For the two class case (<span class="math inline">\(K=2\)</span>) we get with <span class="math inline">\(\boldsymbol \mu_0 = \pi_1 \boldsymbol \mu_1 + \pi_2 \boldsymbol \mu_2\)</span> for the mean difference <span class="math inline">\((\boldsymbol \mu_1 - \boldsymbol \mu_0) = \pi_2 (\boldsymbol \mu_1 - \boldsymbol \mu_2)\)</span> and with <span class="math inline">\(c_1 =  \sqrt{n \frac{\pi_1}{\pi_2}}\)</span> this yields <span class="math display">\[
\boldsymbol \tau_1^{adj} = \sqrt{n \pi_1 \pi_2 } \boldsymbol P^{-1/2} \boldsymbol V^{-1/2} (\boldsymbol \mu_1 - \boldsymbol \mu_2) ,
\]</span> i.e.&nbsp;the exact same score as in the two-class setting.</p>
</section>
</section>
<section id="variable-selection" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="variable-selection"><span class="header-section-number">5.7</span> Variable selection</h2>
<p>In the previous we saw that in DDA the natural score for <strong>ranking features</strong> with regard to their relevance in separating the classes is the (squared) <span class="math inline">\(t\)</span>-score, and for LDA a whitened version such as the squared correlation-adjusted <span class="math inline">\(t\)</span>-score (based on ZCA-cor whitening) may be used. Once such a ranking has been established the question of a <strong>suitable cutoff</strong> arises, i.e.&nbsp; how many features need (or should) be retained in a model.</p>
<p>For large and high-dimensional models <strong>feature selection can also be viewed as a form of regularisation and also dimension reduction</strong>. Specifically, there may be many variables/ features that do no contribute to the class prediction. Despite having in principle no effect on the outcome the presence of these “null variables” can nonetheless deterioriate (sometimes dramatically!) the overall predictive accuracy of a trained predictor, because they add noise and increase the model dimension. Therefore, variables that do not contribute to prediction should be filtered out in order to be able to construct good prediction models and classifiers.</p>
<section id="choosing-a-threshold-by-multiple-testing-using-false-discovery-rates" class="level3">
<h3 class="anchored" data-anchor-id="choosing-a-threshold-by-multiple-testing-using-false-discovery-rates">Choosing a threshold by multiple testing using false discovery rates</h3>
<p>The most simple way to determine a cutoff threshold is to use a standard technique for multiple testing.</p>
<p>For each predictor variable <span class="math inline">\(x_1, \ldots, x_d\)</span> we have a corresponding test statistic measuring the influence of this variable on the response, for example the the <span class="math inline">\(t\)</span>-scores and related statistics discussed in the previous section. In addition to providing an overall ranking the set of all these statistics can be used to determine a suitable cutoff by trying to separate two populations of predictor variables:</p>
<ul>
<li>“Null” variables that do not contribute to prediction</li>
<li>“Alternative” variables that are linked to prediction</li>
</ul>
<p>This can be done as follows:</p>
<ul>
<li><p>The distribution of the observed test statistics <span class="math inline">\(z_i\)</span> is assumed to follow a two-component mixture where <span class="math inline">\(F_0(z)\)</span> and <span class="math inline">\(F_A(z)\)</span> are the distributions corresponding to the null and the alternative, <span class="math inline">\(f_0(z)\)</span> and <span class="math inline">\(f_a(z)\)</span> the densities, and <span class="math inline">\(\pi_0\)</span> and <span class="math inline">\(\pi_A=1-\pi_0\)</span> are the weights: <span class="math display">\[
f(z) = \pi_0 f_0(z) + (1-\pi_0) f_a(z)
\]</span></p></li>
<li><p>The null model is typically from a parametric family (e.g.&nbsp;normal around zero and with a free variance parameter) whereas the alternative is often modelled nonparametrically.</p></li>
<li><p>After fitting the mixture model, often assuming some additional constraints to make the mixture identifiable, one can compute false discovery rates (FDR) as follows:</p>
<p>Local FDR: <span class="math display">\[
\widehat{\text{fdr}}(z_i) = \widehat{\text{Pr}}(\text{null} | z_i) = \frac{\hat{\pi}_0 \hat{f}_0(z_i)}{\hat{f}(z_i)}  
\]</span></p>
<p>Tail-area-based FDR (=<span class="math inline">\(q\)</span>-value): <span class="math display">\[
\widehat{\text{Fdr}}(z_i) = \widehat{\text{Pr}}(\text{null} | Z &gt; z_i) = \frac{\hat{\pi}_0 \hat{F}_0(z_i)}{\hat{F}(z_i)}
\]</span> Note these are essentially <span class="math inline">\(p\)</span>-values adjusted for multiple testing (by a variant of the Benjamini-Hochberg method).</p></li>
</ul>
<p>By thresholding false discovery rates it is possible to identify those variables that clearly belong to each of the two groups but also those features that cannot easily be discriminated to fall into either group:</p>
<ul>
<li>“alternative” variables have low local FDR, e.g., <span class="math inline">\(\widehat{\text{fdr}}(z_i) \leq 0.2\)</span></li>
<li>“null” variables have high local FDR, e.g.&nbsp;<span class="math inline">\(\widehat{\text{fdr}}(z_i) \geq 0.8\)</span></li>
<li>features that cannot easily classified as null or alternative, e.g.&nbsp;<span class="math inline">\(0.2 &lt; \widehat{\text{fdr}}(z_i) &lt; 0.8\)</span></li>
</ul>
<p>For feature selection in prediction settings we generally aim to remove only those variable that clearly belong to the null group, leaving all others in the model.</p>
</section>
<section id="variable-selection-using-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="variable-selection-using-cross-validation">Variable selection using cross-validation</h3>
<p>A conceptually simple but computationally more expensive approach to variable selection is to estimate the predicion error of the same type of predictor but with different sets of predictors using cross-validation, and then choosing a predictor that achieves good prediction accuracy while using only a small number of featurs.</p>
<p>This is a method that works very well in practise as is demonstrated in a number of problems in the worksheets.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-JWHT2021" class="csl-entry" role="listitem">
James, G., D. Witten, T. Hastie, and R. Tibshirani. 2021. <em>An Introduction to Statistical Learning with Applications in <span>R</span></em>. 2nd ed. Springer. <a href="https://doi.org/10.1007/978-1-0716-1418-1">https://doi.org/10.1007/978-1-0716-1418-1</a>.
</div>
<div id="ref-JWHTT2023" class="csl-entry" role="listitem">
James, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023. <em>An Introduction to Statistical Learning with Applications in <span>Python</span></em>. Springer. <a href="https://doi.org/10.1007/978-3-031-38747-0">https://doi.org/10.1007/978-3-031-38747-0</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH38161");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04-clustering.html" class="pagination-link" aria-label="Unsupervised learning and clustering">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised learning and clustering</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./06-dependence.html" class="pagination-link" aria-label="Multivariate dependencies">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multivariate dependencies</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>