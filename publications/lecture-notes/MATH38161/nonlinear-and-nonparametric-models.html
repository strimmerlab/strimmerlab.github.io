<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>7 Nonlinear and nonparametric models | Multivariate Statistics and Machine Learning</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="7 Nonlinear and nonparametric models | Multivariate Statistics and Machine Learning">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="7 Nonlinear and nonparametric models | Multivariate Statistics and Machine Learning">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<meta name="description" content="In the last part of the module we discuss methods that go beyond the linear parametric methods prevalent in classical multivariate statistics. Relevant textbooks: The lectures for much of this...">
<meta property="og:description" content="In the last part of the module we discuss methods that go beyond the linear parametric methods prevalent in classical multivariate statistics. Relevant textbooks: The lectures for much of this...">
<meta name="twitter:description" content="In the last part of the module we discuss methods that go beyond the linear parametric methods prevalent in classical multivariate statistics. Relevant textbooks: The lectures for much of this...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Multivariate Statistics and Machine Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="multivariate-random-variables.html"><span class="header-section-number">1</span> Multivariate random variables</a></li>
<li><a class="" href="multivariate-estimation-in-large-sample-and-small-sample-settings.html"><span class="header-section-number">2</span> Multivariate estimation in large sample and small sample settings</a></li>
<li><a class="" href="transformations-and-dimension-reduction.html"><span class="header-section-number">3</span> Transformations and dimension reduction</a></li>
<li><a class="" href="unsupervised-learning-and-clustering.html"><span class="header-section-number">4</span> Unsupervised learning and clustering</a></li>
<li><a class="" href="supervised-learning-and-classification.html"><span class="header-section-number">5</span> Supervised learning and classification</a></li>
<li><a class="" href="multivariate-dependencies.html"><span class="header-section-number">6</span> Multivariate dependencies</a></li>
<li><a class="active" href="nonlinear-and-nonparametric-models.html"><span class="header-section-number">7</span> Nonlinear and nonparametric models</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="further-study.html"><span class="header-section-number">A</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="nonlinear-and-nonparametric-models" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Nonlinear and nonparametric models<a class="anchor" aria-label="anchor" href="#nonlinear-and-nonparametric-models"><i class="fas fa-link"></i></a>
</h1>
<p>In the last part of the module we discuss methods that go beyond the
linear parametric methods prevalent in classical multivariate statistics.</p>
<p><strong>Relevant textbooks:</strong></p>
<p>The lectures for much of this part of the module follow selected chapters from the following text books:</p>
<ul>
<li><p><span class="citation">James et al. (<a href="bibliography.html#ref-JWHT2021" role="doc-biblioref">2021</a>)</span> <a href="https://www.statlearning.com"><em>An introduction to statistical learning with applications in R (2nd edition)</em></a>. Springer.</p></li>
<li><p><span class="citation">Rogers and Girolami (<a href="bibliography.html#ref-RG2017" role="doc-biblioref">2017</a>)</span> <a href="https://www.crcpress.com/A-First-Course-in-Machine-Learning-Second-Edition/Rogers-Girolami/p/book/9781498738484"><em>A first course in machine learning (2nd edition)</em></a>. CRC Press.</p></li>
</ul>
<p>Please study the relevant section and chapters as indicated below in each subsection!</p>
<p>The first book is also available in a version whith examples in Python:</p>
<ul>
<li>
<span class="citation">James et al. (<a href="bibliography.html#ref-JWHTT2023" role="doc-biblioref">2023</a>)</span> <a href="https://www.statlearning.com"><em>An introduction to statistical learning with applications in Python</em></a>. Springer.</li>
</ul>
<div style="page-break-after: always;"></div>
<div id="random-forests" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> Random forests<a class="anchor" aria-label="anchor" href="#random-forests"><i class="fas fa-link"></i></a>
</h2>
<p>Another widely used approach for prediction in nonlinear settings
is the method of random forests.</p>
<p><strong>Relevant reading:</strong></p>
<p>Please read: <span class="citation">James et al. (<a href="bibliography.html#ref-JWHT2021" role="doc-biblioref">2021</a>)</span> or <span class="citation">James et al. (<a href="bibliography.html#ref-JWHTT2023" role="doc-biblioref">2023</a>)</span> <strong>Chapter 8 “Tree-Based Methods”</strong></p>
<p>Specifically:</p>
<ul>
<li>Section 8.1 The Basics of Decision Trees</li>
<li>Section 8.2.1 Bagging</li>
<li>Section 8.2.2 Random Forests</li>
</ul>
<div id="stochastic-vs.-algorithmic-models" class="section level3" number="7.1.1">
<h3>
<span class="header-section-number">7.1.1</span> Stochastic vs. algorithmic models<a class="anchor" aria-label="anchor" href="#stochastic-vs.-algorithmic-models"><i class="fas fa-link"></i></a>
</h3>
<p>Two cultures in statistical modelling: stochastic vs. algorithmic models</p>
<p>Classic discussion paper by Leo Breiman (2001): Statistical modeling: the two cultures.
Statistical Science <strong>16</strong>:199–231. <a href="https://doi.org/10.1214/ss/1009213726" class="uri">https://doi.org/10.1214/ss/1009213726</a></p>
<p>This paper has recently be revisited in the following discussion paper by Efron (2020) and discussants:
Prediction, estimation, and attribution. JASA <strong>115</strong>:636–677. <a href="https://doi.org/10.1080/01621459.2020.1762613" class="uri">https://doi.org/10.1080/01621459.2020.1762613</a></p>
</div>
<div id="random-forests-1" class="section level3" number="7.1.2">
<h3>
<span class="header-section-number">7.1.2</span> Random forests<a class="anchor" aria-label="anchor" href="#random-forests-1"><i class="fas fa-link"></i></a>
</h3>
<p>Proposed by Leo Breimann in 2001 as application of “bagging” (Breiman 1996) to decision trees.</p>
<p>Basic idea:</p>
<ul>
<li>A single decision tree is unreliable and unstable (weak predictor/classifier).</li>
<li>Use boostrap to generate multiple decision trees (=“forest”)</li>
<li>Average over predictions from all tree (=“bagging”, bootstrap aggregation)</li>
</ul>
<p>The averaging procedure has the effect of variance stabilisation.
Intringuingly, averaging across all decision trees dramatically improves the
overall prediction accuracy!</p>
<p>The Random Forests approach is an example of an <strong>ensemble method</strong>
(since it is based on using an “ensemble” of trees).</p>
<p>Variations: boosting, XGBoost ( <a href="https://xgboost.ai/" class="uri">https://xgboost.ai/</a> )</p>
<p>Random forests will be applied in Worksheet 11.</p>
<p>They are computationally expensive but typically perform very well!</p>
<div style="page-break-after: always;"></div>
</div>
<div id="comparison-of-decision-boundaries-decision-tree-vs.-random-forest" class="section level3" number="7.1.3">
<h3>
<span class="header-section-number">7.1.3</span> Comparison of decision boundaries: decision tree vs. random forest<a class="anchor" aria-label="anchor" href="#comparison-of-decision-boundaries-decision-tree-vs.-random-forest"><i class="fas fa-link"></i></a>
</h3>
<p>Non-nested case:</p>
<div class="inline-figure">
<img src="fig/fig6-nonnested.png" width="90%" style="display: block; margin: auto;">
Nested case:</div>
<div class="inline-figure"><img src="fig/fig6-nested.png" width="90%" style="display: block; margin: auto;"></div>
<p>Compare also with the decision boundaries for LDA and QDA (Chapter 4).</p>

<div style="page-break-after: always;"></div>
</div>
</div>
<div id="gaussian-processes" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> Gaussian processes<a class="anchor" aria-label="anchor" href="#gaussian-processes"><i class="fas fa-link"></i></a>
</h2>
<p>Gaussian processes offer another nonparametric approach to model
nonlinear dependencies. They provide a probabilistic model for
the unknown nonlinear function.</p>
<p><strong>Relevant reading:</strong></p>
<p>Please read: <span class="citation">Rogers and Girolami (<a href="bibliography.html#ref-RG2017" role="doc-biblioref">2017</a>)</span> <strong>Chapter 8: Gaussian processes.</strong></p>
<div id="main-concepts" class="section level3" number="7.2.1">
<h3>
<span class="header-section-number">7.2.1</span> Main concepts<a class="anchor" aria-label="anchor" href="#main-concepts"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Gaussian processes (GPs) belong the the family of <strong>Bayesian nonparametric models</strong>
</li>
<li>Idea:
<ul>
<li>start with prior over a function (!),</li>
<li>then condition on observed data to get posterior distribution (again over a function)</li>
</ul>
</li>
<li>GPs use an infinitely dimensional multivariate normal distribution as prior</li>
</ul>
</div>
<div id="conditional-multivariate-normal-distribution" class="section level3" number="7.2.2">
<h3>
<span class="header-section-number">7.2.2</span> Conditional multivariate normal distribution<a class="anchor" aria-label="anchor" href="#conditional-multivariate-normal-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>GPs make use of the fact that marginal and conditional distributions of a multivariate normal
distribution are also multivariate normal.</p>
<p><strong>Multivariate normal distribution:</strong></p>
<p><span class="math display">\[\boldsymbol z\sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\]</span></p>
<p>Assume:
<span class="math display">\[
\boldsymbol z=\begin{pmatrix}
    \boldsymbol z_1      \\
    \boldsymbol z_2      \\
\end{pmatrix}
\]</span>
with
<span class="math display">\[
\boldsymbol \mu=\begin{pmatrix}
    \boldsymbol \mu_1      \\
    \boldsymbol \mu_2      \\
\end{pmatrix}
\]</span>
and
<span class="math display">\[
\boldsymbol \Sigma=\begin{pmatrix}
    \boldsymbol \Sigma_{1}   &amp; \boldsymbol \Sigma_{12}   \\
    \boldsymbol \Sigma_{12}^T &amp; \boldsymbol \Sigma_{2}   \\
\end{pmatrix}
\]</span>
with corresponding dimensions <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span> and <span class="math inline">\(d_1+d_2=d\)</span>.</p>
<p><strong>Marginal distributions:</strong></p>
<p>Any subset of <span class="math inline">\(\boldsymbol z\)</span> is also multivariate normally distributed.
Specifically,
<span class="math display">\[
\boldsymbol z_1 \sim N_{d_1}(\boldsymbol \mu_1, \boldsymbol \Sigma_{1})
\]</span>
and
<span class="math display">\[
\boldsymbol z_2 \sim N_{d_2}(\boldsymbol \mu_2, \boldsymbol \Sigma_{2})
\]</span></p>
<p><strong>Conditional multivariate normal:</strong></p>
<p>The conditional distribution is also multivariate normal:
<span class="math display">\[
\boldsymbol z_1 | \boldsymbol z_2 = \boldsymbol z_{1 | 2} \sim N_{d_1}(\boldsymbol \mu_{1|2}, \boldsymbol \Sigma_{1 | 2})
\]</span>
with
<span class="math display">\[\boldsymbol \mu_{1|2}=\boldsymbol \mu_1 + \boldsymbol \Sigma_{12} \boldsymbol \Sigma_{2}^{-1} (\boldsymbol z_2 -\boldsymbol \mu_2)\]</span>
and
<span class="math display">\[\boldsymbol \Sigma_{1 | 2}=\boldsymbol \Sigma_{1} -  \boldsymbol \Sigma_{12} \boldsymbol \Sigma_{2}^{-1} \boldsymbol \Sigma_{12}^T\]</span></p>
<p><span class="math inline">\(\boldsymbol z_{1 | 2}\)</span> and
<span class="math inline">\(\boldsymbol \mu_{1|2}\)</span> have dimension <span class="math inline">\(d_1 \times 1\)</span>
and <span class="math inline">\(\boldsymbol \Sigma_{1 | 2}\)</span> has dimension <span class="math inline">\(d_1 \times d_1\)</span>,
i.e. the same dimension as the unconditioned variables.</p>
<p>You may recall the above formula in the context of linear regression, with
<span class="math inline">\(y = z_1\)</span> and <span class="math inline">\(\boldsymbol x= \boldsymbol z_2\)</span> so that the conditional mean becomes
<span class="math display">\[
\begin{split}
\text{E}(y|\boldsymbol x) &amp;=\boldsymbol \mu_y + \boldsymbol \Sigma_{y\boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x}^{-1} (\boldsymbol x-\boldsymbol \mu_{\boldsymbol x})\\
&amp;= \beta_0+ \boldsymbol \beta^T \boldsymbol x\\
\end{split}
\]</span>
with <span class="math inline">\(\boldsymbol \beta= \boldsymbol \Sigma_{\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}\)</span>
and <span class="math inline">\(\beta_0 = \boldsymbol \mu_y-\boldsymbol \beta^T \boldsymbol \mu_{\boldsymbol x}\)</span>, and the
corresponding conditional variance is
<span class="math display">\[
\text{Var}(y|\boldsymbol x) = \sigma^2_y -  \boldsymbol \Sigma_{y\boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy} \,.
\]</span></p>
</div>
<div id="covariance-functions-and-kernels" class="section level3" number="7.2.3">
<h3>
<span class="header-section-number">7.2.3</span> Covariance functions and kernels<a class="anchor" aria-label="anchor" href="#covariance-functions-and-kernels"><i class="fas fa-link"></i></a>
</h3>
<p>The GP prior is an infinitely dimensional multivariate normal distribution
with mean zero and the <strong>covariance specified by a function</strong> <span class="math inline">\(k(x, x^{\prime})\)</span>:</p>
<p>A widely used covariance function is
<span class="math display">\[
k(x, x^{\prime}) = \text{Cov}(x, x^{\prime}) = \sigma^2 e^{-\frac{ (x-x^{\prime})^2}{2 l^2}}
\]</span>
This is known as the <strong>squared-exponential kernel</strong> or <strong>Radial-basis function (RBF) kernel</strong>.</p>
<p>Note that this kernel implies</p>
<ul>
<li>
<span class="math inline">\(k(x, x) = \text{Var}(x) = \sigma^2\)</span> and</li>
<li>
<span class="math inline">\(\text{Cor}(x, x^{\prime}) = e^{-\frac{ (x-x^{\prime})^2}{2 l^2}}\)</span>.</li>
</ul>
<p>The parameter <span class="math inline">\(l\)</span> in the RBF kernel is the length scale parameter and describes
the “wigglyness” or “stiffness” of the resulting function.
Small values of <span class="math inline">\(l\)</span> correspond to more complex, more wiggly functions, and to low spatial correlation,
as the correlation decreases quicker with distance, and large values correspond to more rigid, stiffer
functions, with longer range spatial correlation (note that in a time series context this would be
called autocorrelation).</p>
<p>There are many other kernel functions, including linear, polynomial or periodic kernels.</p>
</div>
<div id="gp-model" class="section level3" number="7.2.4">
<h3>
<span class="header-section-number">7.2.4</span> GP model<a class="anchor" aria-label="anchor" href="#gp-model"><i class="fas fa-link"></i></a>
</h3>
<p>Nonlinear regression in the GP approach is conceptually very simple:</p>
<ul>
<li>start with multivariate prior</li>
<li>then condition on the observed data</li>
<li>the resulting conditional multivariate normal can used to predict
the function values at any unobserved values</li>
<li>the conditional variance can be used to compute credible intervals for predictions.</li>
</ul>
<p>GP regression also provides a direct link with classical
Bayesian linear regression (when using a linear kernel).
Furthermore, GPs are also linked with neural networks as their limit
in the case of an infinitely wide network (see section on neural networks).</p>
<p>Drawbacks of GPs: computationally expensive, typically <span class="math inline">\(O(n^3)\)</span> because of the matrix inversion.
However, there are now variations of GPs that help to overcome this issue (e.g. sparse GPs).</p>
</div>
<div id="gaussian-process-example" class="section level3" number="7.2.5">
<h3>
<span class="header-section-number">7.2.5</span> Gaussian process example<a class="anchor" aria-label="anchor" href="#gaussian-process-example"><i class="fas fa-link"></i></a>
</h3>
<p>We now show how to apply Gaussian processes in R justing using standard matrix calculations.</p>
<p>Our aim is to estimate the following nonlinear function from a number of observations. Note that initially we assume that there is no additional noise (so the observations lie directly on the curve):</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">truefunc</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">XLIM</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">2</span><span class="op">*</span><span class="va">pi</span><span class="op">)</span></span>
<span><span class="va">YLIM</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">n2</span> <span class="op">=</span> <span class="fl">10</span></span>
<span><span class="va">x2</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n2</span>, min<span class="op">=</span><span class="va">XLIM</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, max<span class="op">=</span><span class="va">XLIM</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">y2</span> <span class="op">=</span> <span class="fu">truefunc</span><span class="op">(</span><span class="va">x2</span><span class="op">)</span>  <span class="co"># no noise</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span> <span class="fu">truefunc</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, xlim<span class="op">=</span><span class="va">XLIM</span>, ylim<span class="op">=</span><span class="va">YLIM</span>, xlab<span class="op">=</span><span class="st">"x"</span>, ylab<span class="op">=</span><span class="st">"y"</span>, </span>
<span>      main<span class="op">=</span><span class="st">"True Function"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">x2</span>, <span class="va">y2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure">
<img src="07-nonlinear-b_files/figure-html/unnamed-chunk-1-1.png" width="672">
Next we use the RFB kernel as the prior covariance and also assume that the prior has mean zero:</div>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># RBF kernel</span></span>
<span><span class="va">rbfkernel</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xa</span>, <span class="va">xb</span>, <span class="va">s2</span><span class="op">=</span><span class="fl">1</span>, <span class="va">l</span><span class="op">=</span><span class="fl">1</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span> <span class="va">s2</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span><span class="op">/</span><span class="fl">2</span><span class="op">*</span><span class="op">(</span><span class="va">xa</span><span class="op">-</span><span class="va">xb</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="va">l</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">kfun.mat</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xavec</span>, <span class="va">xbvec</span>, <span class="va">FUN</span><span class="op">=</span><span class="va">rbfkernel</span><span class="op">)</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/outer.html">outer</a></span><span class="op">(</span>X<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="va">xavec</span><span class="op">)</span>, Y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="va">xbvec</span><span class="op">)</span>, FUN<span class="op">=</span><span class="va">FUN</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># prior mean</span></span>
<span><span class="va">mu.vec</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>We can now visualise the functions sampled from the multivariate normal prior:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># grid of x-values </span></span>
<span><span class="va">n1</span> <span class="op">=</span> <span class="fl">100</span></span>
<span><span class="va">x1</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="va">XLIM</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="va">XLIM</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, length.out<span class="op">=</span><span class="va">n1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># unconditioned covariance and mean (unobserved samples x1)</span></span>
<span><span class="va">K1</span> <span class="op">=</span> <span class="fu">kfun.mat</span><span class="op">(</span><span class="va">x1</span>, <span class="va">x1</span><span class="op">)</span>  </span>
<span><span class="va">m1</span> <span class="op">=</span> <span class="fu">mu.vec</span><span class="op">(</span><span class="va">x1</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## sample functions from GP prior  </span></span>
<span><span class="va">B</span> <span class="op">=</span> <span class="fl">5</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">"MASS"</a></span><span class="op">)</span> <span class="co"># for mvrnorm</span></span>
<span><span class="va">y1r</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html">mvrnorm</a></span><span class="op">(</span><span class="va">B</span>, mu <span class="op">=</span> <span class="va">m1</span>, Sigma<span class="op">=</span><span class="va">K1</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">y1r</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, type<span class="op">=</span><span class="st">"l"</span>, lwd<span class="op">=</span><span class="fl">2</span>, ylab<span class="op">=</span><span class="st">"y"</span>, xlab<span class="op">=</span><span class="st">"x"</span>, ylim<span class="op">=</span><span class="va">YLIM</span>, </span>
<span>  main<span class="op">=</span><span class="st">"Prior Functions (RBF Kernel with l=1/2)"</span><span class="op">)</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="va">B</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">y1r</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span>, col<span class="op">=</span><span class="va">i</span>, lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-nonlinear-b_files/figure-html/unnamed-chunk-3-1.png" width="672"></div>
<p>Now we compute the posterior mean and variance by conditioning on the observations:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># unconditioned covariance and mean (observed samples x2)</span></span>
<span><span class="va">K2</span> <span class="op">=</span> <span class="fu">kfun.mat</span><span class="op">(</span><span class="va">x2</span>, <span class="va">x2</span><span class="op">)</span></span>
<span><span class="va">m2</span> <span class="op">=</span> <span class="fu">mu.vec</span><span class="op">(</span><span class="va">x2</span><span class="op">)</span></span>
<span><span class="va">iK2</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">K2</span><span class="op">)</span> <span class="co"># inverse</span></span>
<span></span>
<span><span class="co"># cross-covariance</span></span>
<span><span class="va">K12</span> <span class="op">=</span> <span class="fu">kfun.mat</span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Conditioning: x1 conditioned on x2</span></span>
<span></span>
<span><span class="co"># conditional mean</span></span>
<span><span class="va">m1.2</span> <span class="op">=</span> <span class="va">m1</span> <span class="op">+</span> <span class="va">K12</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">iK2</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="op">(</span><span class="va">y2</span> <span class="op">-</span> <span class="va">m2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># conditional variance</span></span>
<span><span class="va">K1.2</span> <span class="op">=</span> <span class="va">K1</span> <span class="op">-</span> <span class="va">K12</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">iK2</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">K12</span><span class="op">)</span></span></code></pre></div>
<p>Now we can plot the posterior mean and upper and lower bounds of a 95% credible interval:</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># upper and lower CI</span></span>
<span><span class="va">upper.bound</span> <span class="op">=</span> <span class="va">m1.2</span> <span class="op">+</span> <span class="fl">1.96</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">K1.2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">lower.bound</span> <span class="op">=</span> <span class="va">m1.2</span> <span class="op">-</span> <span class="fl">1.96</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">K1.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">m1.2</span>, type<span class="op">=</span><span class="st">"l"</span>, xlim<span class="op">=</span><span class="va">XLIM</span>, ylim<span class="op">=</span><span class="va">YLIM</span>, col<span class="op">=</span><span class="st">"red"</span>, lwd<span class="op">=</span><span class="fl">3</span>,</span>
<span>  ylab<span class="op">=</span><span class="st">"y"</span>, xlab <span class="op">=</span> <span class="st">"x"</span>, main <span class="op">=</span> <span class="st">"Posterior"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">x2</span>,<span class="va">y2</span>,pch<span class="op">=</span><span class="fl">4</span>,lwd<span class="op">=</span><span class="fl">4</span>,col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">x1</span>,<span class="va">upper.bound</span>,lty<span class="op">=</span><span class="fl">2</span>,lwd<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">x1</span>,<span class="va">lower.bound</span>,lty<span class="op">=</span><span class="fl">2</span>,lwd<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu">truefunc</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, xlim<span class="op">=</span><span class="va">XLIM</span>, add<span class="op">=</span><span class="cn">TRUE</span>, col<span class="op">=</span><span class="st">"gray"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"topright"</span>, </span>
<span>  legend<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"posterior mean"</span>, <span class="st">"posterior quantiles"</span>, <span class="st">"true function"</span><span class="op">)</span>,</span>
<span>  lty<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">1</span><span class="op">)</span>,lwd<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">4</span>, <span class="fl">1</span><span class="op">)</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"red"</span>,<span class="st">"black"</span>, <span class="st">"gray"</span><span class="op">)</span>, cex<span class="op">=</span><span class="fl">1.0</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-nonlinear-b_files/figure-html/unnamed-chunk-5-1.png" width="672"></div>
<p>Finally, we can take into acount noise at the measured data points by adding an error term:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># add some noise</span></span>
<span><span class="va">sdeps</span> <span class="op">=</span> <span class="fl">0.1</span></span>
<span><span class="va">K2</span> <span class="op">=</span> <span class="va">K2</span> <span class="op">+</span> <span class="va">sdeps</span><span class="op">^</span><span class="fl">2</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># update</span></span>
<span><span class="va">iK2</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">K2</span><span class="op">)</span> <span class="co"># inverse</span></span>
<span><span class="va">m1.2</span> <span class="op">=</span> <span class="va">m1</span> <span class="op">+</span> <span class="va">K12</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">iK2</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="op">(</span><span class="va">y2</span> <span class="op">-</span> <span class="va">m2</span><span class="op">)</span></span>
<span><span class="va">K1.2</span> <span class="op">=</span> <span class="va">K1</span> <span class="op">-</span> <span class="va">K12</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">iK2</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">K12</span><span class="op">)</span></span>
<span><span class="va">upper.bound</span> <span class="op">=</span> <span class="va">m1.2</span> <span class="op">+</span> <span class="fl">1.96</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">K1.2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">lower.bound</span> <span class="op">=</span> <span class="va">m1.2</span> <span class="op">-</span> <span class="fl">1.96</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">K1.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">m1.2</span>, type<span class="op">=</span><span class="st">"l"</span>, xlim<span class="op">=</span><span class="va">XLIM</span>, ylim<span class="op">=</span><span class="va">YLIM</span>, col<span class="op">=</span><span class="st">"red"</span>, lwd<span class="op">=</span><span class="fl">3</span>, </span>
<span>  ylab<span class="op">=</span><span class="st">"y"</span>, xlab <span class="op">=</span> <span class="st">"x"</span>, main <span class="op">=</span> <span class="st">"Posterior (with noise)"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">x2</span>,<span class="va">y2</span>,pch<span class="op">=</span><span class="fl">4</span>,lwd<span class="op">=</span><span class="fl">4</span>,col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">x1</span>,<span class="va">upper.bound</span>,lty<span class="op">=</span><span class="fl">2</span>,lwd<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">x1</span>,<span class="va">lower.bound</span>,lty<span class="op">=</span><span class="fl">2</span>,lwd<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu">truefunc</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, xlim<span class="op">=</span><span class="va">XLIM</span>, add<span class="op">=</span><span class="cn">TRUE</span>, col<span class="op">=</span><span class="st">"gray"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"topright"</span>, </span>
<span>  legend<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"posterior mean"</span>, <span class="st">"posterior quantiles"</span>, <span class="st">"true function"</span><span class="op">)</span>,</span>
<span>  lty<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">1</span><span class="op">)</span>,lwd<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">4</span>, <span class="fl">1</span><span class="op">)</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"red"</span>,<span class="st">"black"</span>, <span class="st">"gray"</span><span class="op">)</span>, cex<span class="op">=</span><span class="fl">1.0</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-nonlinear-b_files/figure-html/unnamed-chunk-6-1.png" width="672"></div>
<p>Note that in the vicinity of data points the CIs are small and the further away
from data the more uncertain the estimate of the underlying function becomes.</p>

<div style="page-break-after: always;"></div>
</div>
</div>
<div id="neural-networks" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> Neural networks<a class="anchor" aria-label="anchor" href="#neural-networks"><i class="fas fa-link"></i></a>
</h2>
<p>Another highly important class of models
for nonlinear prediction (and nonlinear function approximation) are
neural networks.</p>
<p><strong>Relevant reading:</strong></p>
<p>Please read: <span class="citation">James et al. (<a href="bibliography.html#ref-JWHT2021" role="doc-biblioref">2021</a>)</span> or <span class="citation">James et al. (<a href="bibliography.html#ref-JWHTT2023" role="doc-biblioref">2023</a>)</span> <strong>Chapter 10 “Deep Learning”</strong></p>
<div id="history" class="section level3" number="7.3.1">
<h3>
<span class="header-section-number">7.3.1</span> History<a class="anchor" aria-label="anchor" href="#history"><i class="fas fa-link"></i></a>
</h3>
<p>Neural networks are actually relatively old models, going back
to the 1950s.</p>
<p>Three phases of neural networks (NN)</p>
<ul>
<li>1950/60: replicating functions of neurons in the brain (e.g. perceptron)</li>
<li>1980/90: neural networks as <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal function approximators</a>
</li>
<li>2010—today: deep learning</li>
</ul>
<p>The first phase was biologically inspired, the second phase focused on
mathematical properties, and the current phase is pushed forward by
advances in computer science and numerical optimisation:</p>
<ul>
<li>backpropagation algorithm</li>
<li>efficient automatic symbolic differentiation (e.g. autograd)</li>
<li>stochastic gradient descent algorithms (e.g. Adam)</li>
<li>use of GPUs and TPUs (e.g. for linear algebra)</li>
<li>availability of packages for
symbolic tensor computations and deep learning.</li>
</ul>
<p>Currently the most popular frameworks are:</p>
<ul>
<li>
<a href="https://pytorch.org/PyTorch">PyTorch</a> (PyTorch Foundation, formerly Meta/Facebook)</li>
<li>
<a href="https://www.tensorflow.org/">TensorFlow</a> (Google Research)</li>
<li>
<a href="https://flax.readthedocs.io/en/latest/">Flax</a> / <a href="https://jax.readthedocs.io/en/latest/">JAX</a> (Google Research)</li>
</ul>
<p>and high-level wrappers:</p>
<ul>
<li>
<a href="https://skorch.readthedocs.io/en/latest/">skorch</a> (scikit-learn wrapper for PyTorch)</li>
<li>
<a href="https://keras.io/keras_3/">Keras 3</a> (for TensorFlow, JAX, and PyTorch)</li>
</ul>
</div>
<div id="neural-networks-1" class="section level3" number="7.3.2">
<h3>
<span class="header-section-number">7.3.2</span> Neural networks<a class="anchor" aria-label="anchor" href="#neural-networks-1"><i class="fas fa-link"></i></a>
</h3>
<p>Neural networks are essentially stacked systems of linear regressions,
with nonlinear mappings between each layer,
mapping the input to output via one or more
layers of internal hidden nodes corresponding to internal latent variables:</p>
<ul>
<li>Each internal node is a nonlinear function of all or some of nodes in the previous
layer</li>
<li>Typically, the output of a node is computed using a <strong>non-linear activation function</strong>,
such as the sigmoid function or a piecewise linear function (ReLU), from a
linear combination of the input variables of that node.</li>
</ul>
<p>A simple architecture is a feedforward network with a single hidden layer.
More complex models are multilayer perceptrons and convolutional neural networks.</p>
<p>It can be shown that even simple network architectures can (with sufficient number of nodes)
approximate any arbitrary non-linear function. This is called the <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"><strong>universal function approximation</strong></a> property.</p>
<p>“Deep” neural networks have many layers, and the optimisation of their parameters requires advanced
techniques (see above), with the objective function typically an empirical risk based on, e.g.,
squared error loss or cross-entropy loss. Neural networks are very highly parameterised models and
therefore require lots of data for training, and typically also some form of regularisation (e.g. dropout).</p>
<p>As an extreme example, the neural network behind the <a href="https://en.wikipedia.org/wiki/GPT-4">ChatGPT 4</a> language model
that is trained on essentially the whole freely accessible text available on the internet has
an estimated 1.76 trillion (!) parameters (<span class="math inline">\(1.76 \times 10^{12}\)</span>).</p>
<p><strong>In the limit of an infinite width a single layer fully connected neural network becomes
equivalent to a Gaussian process</strong>. This was first shown by R. M. Neal (1996)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Neal, R. M. 1996. Bayesian Learning for Neural Networks. Springer. &lt;a href="https://doi.org/10.1007/978-1-4612-0745-0" class="uri"&gt;https://doi.org/10.1007/978-1-4612-0745-0&lt;/a&gt;&lt;/p&gt;'><sup>21</sup></a>. More recently,
this equivalence has also been demonstrated for other types of neural networks (with the kernel function of
the GP being determined by the neural network architecture). This is formalised in the “neural tangent kernel” (NTK)
framework.</p>
<p>Some of the statistical aspects of neural networks are not well understood. For example, there is the <strong>paradox that
neural networks typically overfit the training data but still generalise well</strong> - this clearly violates the traditional understanding of bias-variance tradeoff for classical modelling in statistics and machine learning — see for example Belkin et al. (2019)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Belkin, M. et al. 2019. Reconciling modern machine-learning practice and
the classical bias–variance trade-off. PNAS &lt;strong&gt;116&lt;/strong&gt;: 15849–15854. &lt;a href="https://doi.org/10.1073/pnas.1903070116" class="uri"&gt;https://doi.org/10.1073/pnas.1903070116&lt;/a&gt;&lt;/p&gt;'><sup>22</sup></a>. Some researchers argue that this contradiction can be resolved by better understanding the effective dimension of complex models.
There is a lot of current research to explain this phenomenon of “multiple descent”, i.e. the decrease of prediction
error for models with very many parameters. A further topic is robustness of the predictions, which is also caused by overfitting. It is well known that neural networks can sometimes be “fooled” by so-called adversarial examples, e.g., the classification of a sample may change if a small amount of noise is added to the test data.</p>
</div>
<div id="learning-more-about-deep-learning" class="section level3" number="7.3.3">
<h3>
<span class="header-section-number">7.3.3</span> Learning more about deep learning<a class="anchor" aria-label="anchor" href="#learning-more-about-deep-learning"><i class="fas fa-link"></i></a>
</h3>
<p>A good place to to learn more about deep learning and actual application in computer code using various
deep learning software frameworks is the online book “Dive into deep learning” by
<span class="citation">Zhang et al. (<a href="bibliography.html#ref-ZLLS2023" role="doc-biblioref">2023</a>)</span> available online at <a href="https://d2l.ai/" class="uri">https://d2l.ai/</a></p>

</div>
</div>
</div>



<div class="chapter-nav">
<div class="prev"><a href="multivariate-dependencies.html"><span class="header-section-number">6</span> Multivariate dependencies</a></div>
<div class="next"><a href="further-study.html"><span class="header-section-number">A</span> Further study</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#nonlinear-and-nonparametric-models"><span class="header-section-number">7</span> Nonlinear and nonparametric models</a></li>
<li>
<a class="nav-link" href="#random-forests"><span class="header-section-number">7.1</span> Random forests</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#stochastic-vs.-algorithmic-models"><span class="header-section-number">7.1.1</span> Stochastic vs. algorithmic models</a></li>
<li><a class="nav-link" href="#random-forests-1"><span class="header-section-number">7.1.2</span> Random forests</a></li>
<li><a class="nav-link" href="#comparison-of-decision-boundaries-decision-tree-vs.-random-forest"><span class="header-section-number">7.1.3</span> Comparison of decision boundaries: decision tree vs. random forest</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#gaussian-processes"><span class="header-section-number">7.2</span> Gaussian processes</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#main-concepts"><span class="header-section-number">7.2.1</span> Main concepts</a></li>
<li><a class="nav-link" href="#conditional-multivariate-normal-distribution"><span class="header-section-number">7.2.2</span> Conditional multivariate normal distribution</a></li>
<li><a class="nav-link" href="#covariance-functions-and-kernels"><span class="header-section-number">7.2.3</span> Covariance functions and kernels</a></li>
<li><a class="nav-link" href="#gp-model"><span class="header-section-number">7.2.4</span> GP model</a></li>
<li><a class="nav-link" href="#gaussian-process-example"><span class="header-section-number">7.2.5</span> Gaussian process example</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#neural-networks"><span class="header-section-number">7.3</span> Neural networks</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#history"><span class="header-section-number">7.3.1</span> History</a></li>
<li><a class="nav-link" href="#neural-networks-1"><span class="header-section-number">7.3.2</span> Neural networks</a></li>
<li><a class="nav-link" href="#learning-more-about-deep-learning"><span class="header-section-number">7.3.3</span> Learning more about deep learning</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>
</div>

  

  

</div>
 <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Multivariate Statistics and Machine Learning</strong>" was written by Korbinian Strimmer. It was last built on 18 January 2024.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
