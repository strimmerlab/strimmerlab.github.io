<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Supervised learning and classification | _main.utf8</title>
  <meta name="description" content="Multivariate Statistics and Machine Learning<br />
<br />
Lecture Notes<br />
MATH38161</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Supervised learning and classification | _main.utf8" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Supervised learning and classification | _main.utf8" />
  
  
  

<meta name="author" content="Korbinian Strimmer" />


<meta name="date" content="2020-10-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3-clustering.html"/>
<link rel="next" href="5-dependence.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH38161/index.html">MATH38161 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-multivariate.html"><a href="1-multivariate.html"><i class="fa fa-check"></i><b>1</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="1.1" data-path="1-multivariate.html"><a href="1-multivariate.html#why-multivariate-statistics"><i class="fa fa-check"></i><b>1.1</b> Why multivariate statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="1-multivariate.html"><a href="1-multivariate.html#basics"><i class="fa fa-check"></i><b>1.2</b> Basics</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-vs.multivariate-random-variables"><i class="fa fa-check"></i><b>1.2.1</b> Univariate vs.Â multivariate random variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-multivariate.html"><a href="1-multivariate.html#mean-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.2</b> Mean of a random vector</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-multivariate.html"><a href="1-multivariate.html#variance-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.3</b> Variance of a random vector</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-multivariate.html"><a href="1-multivariate.html#properties-of-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.4</b> Properties of the covariance matrix</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-multivariate.html"><a href="1-multivariate.html#eigenvalue-decomposition-of-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.2.5</b> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.2.6" data-path="1-multivariate.html"><a href="1-multivariate.html#quantities-related-to-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.6</b> Quantities related to the covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>1.3</b> Multivariate normal distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-normal-distribution"><i class="fa fa-check"></i><b>1.3.1</b> Univariate normal distribution:</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-model"><i class="fa fa-check"></i><b>1.3.2</b> Multivariate normal model</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-multivariate.html"><a href="1-multivariate.html#shape-of-the-contour-lines-of-the-multivariate-normal-density"><i class="fa fa-check"></i><b>1.3.3</b> Shape of the contour lines of the multivariate normal density</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-in-large-sample-and-small-sample-settings"><i class="fa fa-check"></i><b>1.4</b> Estimation in large sample and small sample settings</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-data"><i class="fa fa-check"></i><b>1.4.1</b> Multivariate data</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-multivariate.html"><a href="1-multivariate.html#strategies-for-large-sample-estimation"><i class="fa fa-check"></i><b>1.4.2</b> Strategies for large sample estimation</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-multivariate.html"><a href="1-multivariate.html#large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.4.3</b> Large sample estimates of mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.4.4" data-path="1-multivariate.html"><a href="1-multivariate.html#problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions"><i class="fa fa-check"></i><b>1.4.4</b> Problems with maximum likelihood in small sample settings and high dimensions</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-of-covariance-matrix-in-small-sample-settings"><i class="fa fa-check"></i><b>1.4.5</b> Estimation of covariance matrix in small sample settings</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-multivariate.html"><a href="1-multivariate.html#discrete-multivariate-distributions"><i class="fa fa-check"></i><b>1.5</b> Discrete multivariate distributions</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-multivariate.html"><a href="1-multivariate.html#categorical-distribution"><i class="fa fa-check"></i><b>1.5.1</b> Categorical distribution</a></li>
<li class="chapter" data-level="1.5.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multinomial-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Multinomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="1-multivariate.html"><a href="1-multivariate.html#continuous-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Continuous multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-multivariate.html"><a href="1-multivariate.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.6.1</b> Dirichlet distribution</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-multivariate.html"><a href="1-multivariate.html#wishart-distribution"><i class="fa fa-check"></i><b>1.6.2</b> Wishart distribution</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-multivariate.html"><a href="1-multivariate.html#inverse-wishart-distribution"><i class="fa fa-check"></i><b>1.6.3</b> Inverse Wishart distribution</a></li>
<li class="chapter" data-level="1.6.4" data-path="1-multivariate.html"><a href="1-multivariate.html#further-distributions"><i class="fa fa-check"></i><b>1.6.4</b> Further distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-transformations.html"><a href="2-transformations.html"><i class="fa fa-check"></i><b>2</b> Transformations and dimension reduction</a><ul>
<li class="chapter" data-level="2.1" data-path="2-transformations.html"><a href="2-transformations.html#linear-transformations"><i class="fa fa-check"></i><b>2.1</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-transformations.html"><a href="2-transformations.html#location-scale-transformation"><i class="fa fa-check"></i><b>2.1.1</b> Location-scale transformation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-transformations.html"><a href="2-transformations.html#invertible-location-scale-transformation"><i class="fa fa-check"></i><b>2.1.2</b> Invertible location-scale transformation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-transformations.html"><a href="2-transformations.html#nonlinear-transformations"><i class="fa fa-check"></i><b>2.2</b> Nonlinear transformations</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-transformations.html"><a href="2-transformations.html#general-transformation"><i class="fa fa-check"></i><b>2.2.1</b> General transformation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-transformations.html"><a href="2-transformations.html#delta-method"><i class="fa fa-check"></i><b>2.2.2</b> Delta method</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-transformations.html"><a href="2-transformations.html#transformation-of-densities-under-general-invertible-transformation"><i class="fa fa-check"></i><b>2.2.3</b> Transformation of densities under general invertible transformation</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-transformations.html"><a href="2-transformations.html#normalising-flows"><i class="fa fa-check"></i><b>2.2.4</b> Normalising flows</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-transformations.html"><a href="2-transformations.html#whitening-transformations"><i class="fa fa-check"></i><b>2.3</b> Whitening transformations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-transformations.html"><a href="2-transformations.html#overview"><i class="fa fa-check"></i><b>2.3.1</b> Overview</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-transformations.html"><a href="2-transformations.html#general-whitening-transformation"><i class="fa fa-check"></i><b>2.3.2</b> General whitening transformation</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-transformations.html"><a href="2-transformations.html#general-solution-of-whitening-constraint-covariance-based"><i class="fa fa-check"></i><b>2.3.3</b> General solution of whitening constraint (covariance-based)</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-transformations.html"><a href="2-transformations.html#another-solution-correlation-based"><i class="fa fa-check"></i><b>2.3.4</b> Another solution (correlation-based)</a></li>
<li class="chapter" data-level="2.3.5" data-path="2-transformations.html"><a href="2-transformations.html#objective-criteria-for-choosing-among-boldsymbol-w-using-boldsymbol-q_1-or-boldsymbol-q_2"><i class="fa fa-check"></i><b>2.3.5</b> Objective criteria for choosing among <span class="math inline">\(\boldsymbol W\)</span> using <span class="math inline">\(\boldsymbol Q_1\)</span> or <span class="math inline">\(\boldsymbol Q_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-transformations.html"><a href="2-transformations.html#natural-whitening-procedures"><i class="fa fa-check"></i><b>2.4</b> Natural whitening procedures</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-transformations.html"><a href="2-transformations.html#zca-whitening"><i class="fa fa-check"></i><b>2.4.1</b> ZCA Whitening</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-transformations.html"><a href="2-transformations.html#zca-cor-whitening"><i class="fa fa-check"></i><b>2.4.2</b> ZCA-Cor Whitening</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-transformations.html"><a href="2-transformations.html#pca-whitening"><i class="fa fa-check"></i><b>2.4.3</b> PCA Whitening</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-transformations.html"><a href="2-transformations.html#pca-cor-whitening"><i class="fa fa-check"></i><b>2.4.4</b> PCA-cor Whitening</a></li>
<li class="chapter" data-level="2.4.5" data-path="2-transformations.html"><a href="2-transformations.html#cholesky-whitening"><i class="fa fa-check"></i><b>2.4.5</b> Cholesky Whitening</a></li>
<li class="chapter" data-level="2.4.6" data-path="2-transformations.html"><a href="2-transformations.html#comparison-of-zca-pca-and-chol-whitening"><i class="fa fa-check"></i><b>2.4.6</b> Comparison of ZCA, PCA and Chol whitening</a></li>
<li class="chapter" data-level="2.4.7" data-path="2-transformations.html"><a href="2-transformations.html#recap"><i class="fa fa-check"></i><b>2.4.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-transformations.html"><a href="2-transformations.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>2.5</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-transformations.html"><a href="2-transformations.html#application-to-data"><i class="fa fa-check"></i><b>2.5.1</b> Application to data</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-transformations.html"><a href="2-transformations.html#pca-correlation-loadings-and-plot"><i class="fa fa-check"></i><b>2.6</b> PCA correlation loadings and plot</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-transformations.html"><a href="2-transformations.html#iris-data-example"><i class="fa fa-check"></i><b>2.6.1</b> Iris data example</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-transformations.html"><a href="2-transformations.html#cca-whitening-canonical-correlation-analysis"><i class="fa fa-check"></i><b>2.7</b> CCA whitening (Canonical Correlation Analysis)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-transformations.html"><a href="2-transformations.html#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal"><i class="fa fa-check"></i><b>2.7.1</b> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</a></li>
<li class="chapter" data-level="2.7.2" data-path="2-transformations.html"><a href="2-transformations.html#related-methods"><i class="fa fa-check"></i><b>2.7.2</b> Related methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-clustering.html"><a href="3-clustering.html"><i class="fa fa-check"></i><b>3</b> Unsupervised learning and clustering</a><ul>
<li class="chapter" data-level="3.1" data-path="3-clustering.html"><a href="3-clustering.html#challenges-in-supervised-learning"><i class="fa fa-check"></i><b>3.1</b> Challenges in supervised learning</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-clustering.html"><a href="3-clustering.html#objective"><i class="fa fa-check"></i><b>3.1.1</b> Objective</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-clustering.html"><a href="3-clustering.html#questions-and-problems"><i class="fa fa-check"></i><b>3.1.2</b> Questions and problems</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-clustering.html"><a href="3-clustering.html#why-is-clustering-difficult"><i class="fa fa-check"></i><b>3.1.3</b> Why is clustering difficult?</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-clustering.html"><a href="3-clustering.html#common-types-of-clustering-methods"><i class="fa fa-check"></i><b>3.1.4</b> Common types of clustering methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-clustering.html"><a href="3-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>3.2</b> Hierarchical clustering</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-clustering.html"><a href="3-clustering.html#tree-like-structures"><i class="fa fa-check"></i><b>3.2.1</b> Tree-like structures</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-clustering.html"><a href="3-clustering.html#agglomerative-hierarchical-clustering-algorithms"><i class="fa fa-check"></i><b>3.2.2</b> Agglomerative hierarchical clustering algorithms</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-clustering.html"><a href="3-clustering.html#application-to-swiss-banknote-data-set"><i class="fa fa-check"></i><b>3.2.3</b> Application to Swiss banknote data set</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-clustering.html"><a href="3-clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>3.3</b> <span class="math inline">\(K\)</span>-means clustering</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-clustering.html"><a href="3-clustering.html#general-aims"><i class="fa fa-check"></i><b>3.3.1</b> General aims</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-clustering.html"><a href="3-clustering.html#algorithm"><i class="fa fa-check"></i><b>3.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-clustering.html"><a href="3-clustering.html#properties"><i class="fa fa-check"></i><b>3.3.3</b> Properties</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>3.3.4</b> Choosing the number of clusters</a></li>
<li class="chapter" data-level="3.3.5" data-path="3-clustering.html"><a href="3-clustering.html#k-medoids-aka-pam"><i class="fa fa-check"></i><b>3.3.5</b> <span class="math inline">\(K\)</span>-medoids aka PAM</a></li>
<li class="chapter" data-level="3.3.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-k-means-to-iris-data"><i class="fa fa-check"></i><b>3.3.6</b> Application of <span class="math inline">\(K\)</span>-means to Iris data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-clustering.html"><a href="3-clustering.html#mixture-models"><i class="fa fa-check"></i><b>3.4</b> Mixture models</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-clustering.html"><a href="3-clustering.html#finite-mixture-model"><i class="fa fa-check"></i><b>3.4.1</b> Finite mixture model</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-clustering.html"><a href="3-clustering.html#example-of-mixture-of-three-univariate-normal-densities"><i class="fa fa-check"></i><b>3.4.2</b> Example of mixture of three univariate normal densities:</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-clustering.html"><a href="3-clustering.html#example-of-a-mixture-of-two-bivariate-normal-densities"><i class="fa fa-check"></i><b>3.4.3</b> Example of a mixture of two bivariate normal densities</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-clustering.html"><a href="3-clustering.html#sampling-from-a-mixture-model-and-latent-allocation-variable-formulation"><i class="fa fa-check"></i><b>3.4.4</b> Sampling from a mixture model and latent allocation variable formulation</a></li>
<li class="chapter" data-level="3.4.5" data-path="3-clustering.html"><a href="3-clustering.html#predicting-the-group-allocation-of-a-given-sample"><i class="fa fa-check"></i><b>3.4.5</b> Predicting the group allocation of a given sample</a></li>
<li class="chapter" data-level="3.4.6" data-path="3-clustering.html"><a href="3-clustering.html#decomposition-of-covariance-and-total-variation"><i class="fa fa-check"></i><b>3.4.6</b> Decomposition of covariance and total variation</a></li>
<li class="chapter" data-level="3.4.7" data-path="3-clustering.html"><a href="3-clustering.html#variation-1-infinite-mixture-model"><i class="fa fa-check"></i><b>3.4.7</b> Variation 1: Infinite mixture model</a></li>
<li class="chapter" data-level="3.4.8" data-path="3-clustering.html"><a href="3-clustering.html#variation-2-semiparametric-mixture-model-with-two-classes"><i class="fa fa-check"></i><b>3.4.8</b> Variation 2: Semiparametric mixture model with two classes</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-clustering.html"><a href="3-clustering.html#fitting-mixture-models-to-data"><i class="fa fa-check"></i><b>3.5</b> Fitting mixture models to data</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-clustering.html"><a href="3-clustering.html#direct-estimation-of-mixture-model-parameters"><i class="fa fa-check"></i><b>3.5.1</b> Direct estimation of mixture model parameters</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-clustering.html"><a href="3-clustering.html#estimate-mixture-model-parameters-using-the-em-algorithm"><i class="fa fa-check"></i><b>3.5.2</b> Estimate mixture model parameters using the EM algorithm</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-clustering.html"><a href="3-clustering.html#em-algorithm-for-multivariate-normal-mixture-model"><i class="fa fa-check"></i><b>3.5.3</b> EM algorithm for multivariate normal mixture model</a></li>
<li class="chapter" data-level="3.5.4" data-path="3-clustering.html"><a href="3-clustering.html#connection-with-k-means-clustering-method"><i class="fa fa-check"></i><b>3.5.4</b> Connection with <span class="math inline">\(K\)</span>-means clustering method</a></li>
<li class="chapter" data-level="3.5.5" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-classes"><i class="fa fa-check"></i><b>3.5.5</b> Choosing the number of classes</a></li>
<li class="chapter" data-level="3.5.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-gmms-to-iris-flower-data"><i class="fa fa-check"></i><b>3.5.6</b> Application of GMMs to Iris flower data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-classification.html"><a href="4-classification.html"><i class="fa fa-check"></i><b>4</b> Supervised learning and classification</a><ul>
<li class="chapter" data-level="4.1" data-path="4-classification.html"><a href="4-classification.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-classification.html"><a href="4-classification.html#supervised-learning-vs.unsupervised-learning"><i class="fa fa-check"></i><b>4.1.1</b> Supervised learning vs.Â unsupervised learning</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-classification.html"><a href="4-classification.html#terminology"><i class="fa fa-check"></i><b>4.1.2</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-classification.html"><a href="4-classification.html#bayesian-discriminant-rule-or-bayes-classifier"><i class="fa fa-check"></i><b>4.2</b> Bayesian discriminant rule or Bayes classifier</a></li>
<li class="chapter" data-level="4.3" data-path="4-classification.html"><a href="4-classification.html#normal-bayes-classifier"><i class="fa fa-check"></i><b>4.3</b> Normal Bayes classifier</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-classification.html"><a href="4-classification.html#quadratic-discriminant-analysis-qda-and-gaussian-assumption"><i class="fa fa-check"></i><b>4.3.1</b> Quadratic discriminant analysis (QDA) and Gaussian assumption</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-classification.html"><a href="4-classification.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>4.3.2</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-classification.html"><a href="4-classification.html#diagonal-discriminant-analysis-dda"><i class="fa fa-check"></i><b>4.3.3</b> Diagonal discriminant analysis (DDA)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-classification.html"><a href="4-classification.html#the-training-step-learning-qda-lda-and-dda-classifiers-from-data"><i class="fa fa-check"></i><b>4.4</b> The training step â learning QDA, LDA and DDA classifiers from data</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-classification.html"><a href="4-classification.html#number-of-model-parameters"><i class="fa fa-check"></i><b>4.4.1</b> Number of model parameters</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-classification.html"><a href="4-classification.html#estimating-the-discriminant-predictor-function"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the discriminant / predictor function</a></li>
<li class="chapter" data-level="4.4.3" data-path="4-classification.html"><a href="4-classification.html#comparison-of-estimated-decision-boundaries-lda-vs.qda"><i class="fa fa-check"></i><b>4.4.3</b> Comparison of estimated decision boundaries: LDA vs.Â QDA</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-classification.html"><a href="4-classification.html#goodness-of-fit-and-variable-ranking"><i class="fa fa-check"></i><b>4.5</b> Goodness of fit and variable ranking</a><ul>
<li class="chapter" data-level="4.5.1" data-path="4-classification.html"><a href="4-classification.html#lda-with-k2-classes"><i class="fa fa-check"></i><b>4.5.1</b> LDA with <span class="math inline">\(K=2\)</span> classes</a></li>
<li class="chapter" data-level="4.5.2" data-path="4-classification.html"><a href="4-classification.html#multiple-classes"><i class="fa fa-check"></i><b>4.5.2</b> Multiple classes</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4-classification.html"><a href="4-classification.html#variable-selection-and-cross-validation"><i class="fa fa-check"></i><b>4.6</b> Variable selection and cross-validation</a><ul>
<li class="chapter" data-level="4.6.1" data-path="4-classification.html"><a href="4-classification.html#choosing-a-threshold-by-multiple-testing-using-false-discovery-rates"><i class="fa fa-check"></i><b>4.6.1</b> Choosing a threshold by multiple testing using false discovery rates</a></li>
<li class="chapter" data-level="4.6.2" data-path="4-classification.html"><a href="4-classification.html#quantifying-prediction-error"><i class="fa fa-check"></i><b>4.6.2</b> Quantifying prediction error</a></li>
<li class="chapter" data-level="4.6.3" data-path="4-classification.html"><a href="4-classification.html#estimation-of-prediction-error-without-validation-data-using-cross-validation"><i class="fa fa-check"></i><b>4.6.3</b> Estimation of prediction error without validation data using cross-validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-dependence.html"><a href="5-dependence.html"><i class="fa fa-check"></i><b>5</b> Multivariate dependencies</a><ul>
<li class="chapter" data-level="5.1" data-path="5-dependence.html"><a href="5-dependence.html#measuring-the-linear-association-between-two-sets-of-random-variables"><i class="fa fa-check"></i><b>5.1</b> Measuring the linear association between two sets of random variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-dependence.html"><a href="5-dependence.html#outline"><i class="fa fa-check"></i><b>5.1.1</b> Outline</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-dependence.html"><a href="5-dependence.html#special-cases"><i class="fa fa-check"></i><b>5.1.2</b> Special cases</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-dependence.html"><a href="5-dependence.html#rozeboom-vector-correlation"><i class="fa fa-check"></i><b>5.1.3</b> Rozeboom vector correlation</a></li>
<li class="chapter" data-level="5.1.4" data-path="5-dependence.html"><a href="5-dependence.html#rv-coefficient"><i class="fa fa-check"></i><b>5.1.4</b> RV coefficient</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-as-generalisation-of-correlation"><i class="fa fa-check"></i><b>5.2</b> Mutual information as generalisation of correlation</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-dependence.html"><a href="5-dependence.html#definition-of-mutual-information"><i class="fa fa-check"></i><b>5.2.1</b> Definition of mutual information</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normal-variables"><i class="fa fa-check"></i><b>5.2.2</b> Mutual information between two normal variables</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normally-distributed-random-vectors"><i class="fa fa-check"></i><b>5.2.3</b> Mutual information between two normally distributed random vectors</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-dependence.html"><a href="5-dependence.html#using-mi-for-variable-selection"><i class="fa fa-check"></i><b>5.2.4</b> Using MI for variable selection</a></li>
<li class="chapter" data-level="5.2.5" data-path="5-dependence.html"><a href="5-dependence.html#other-measures-of-general-dependence"><i class="fa fa-check"></i><b>5.2.5</b> Other measures of general dependence</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-dependence.html"><a href="5-dependence.html#graphical-models"><i class="fa fa-check"></i><b>5.3</b> Graphical models</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-dependence.html"><a href="5-dependence.html#purpose"><i class="fa fa-check"></i><b>5.3.1</b> Purpose</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-dependence.html"><a href="5-dependence.html#basic-notions-from-graph-theory"><i class="fa fa-check"></i><b>5.3.2</b> Basic notions from graph theory</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-dependence.html"><a href="5-dependence.html#probabilistic-graphical-models"><i class="fa fa-check"></i><b>5.3.3</b> Probabilistic graphical models</a></li>
<li class="chapter" data-level="5.3.4" data-path="5-dependence.html"><a href="5-dependence.html#directed-graphical-models"><i class="fa fa-check"></i><b>5.3.4</b> Directed graphical models</a></li>
<li class="chapter" data-level="5.3.5" data-path="5-dependence.html"><a href="5-dependence.html#undirected-graphical-models"><i class="fa fa-check"></i><b>5.3.5</b> Undirected graphical models</a></li>
<li class="chapter" data-level="5.3.6" data-path="5-dependence.html"><a href="5-dependence.html#algorithm-for-learning-ggms"><i class="fa fa-check"></i><b>5.3.6</b> Algorithm for learning GGMs</a></li>
<li class="chapter" data-level="5.3.7" data-path="5-dependence.html"><a href="5-dependence.html#example-exam-score-data-mardia-et-al-1979"><i class="fa fa-check"></i><b>5.3.7</b> Example: exam score data (Mardia et al 1979:)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-nonlinear.html"><a href="6-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Nonlinear and nonparametric models</a><ul>
<li class="chapter" data-level="6.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#limits-of-linear-models-and-correlation"><i class="fa fa-check"></i><b>6.1</b> Limits of linear models and correlation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#correlation-only-measures-linear-dependence"><i class="fa fa-check"></i><b>6.1.1</b> Correlation only measures linear dependence</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#anscombe-data-sets"><i class="fa fa-check"></i><b>6.1.2</b> Anscombe data sets</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests"><i class="fa fa-check"></i><b>6.2</b> Random forests</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#stochastic-vs.algorithmic-models"><i class="fa fa-check"></i><b>6.2.1</b> Stochastic vs.Â algorithmic models</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests-1"><i class="fa fa-check"></i><b>6.2.2</b> Random forests</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#comparison-of-decision-boundaries-decision-tree-vs.random-forest"><i class="fa fa-check"></i><b>6.2.3</b> Comparison of decision boundaries: decision tree vs.Â random forest</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gaussian-processes"><i class="fa fa-check"></i><b>6.3</b> Gaussian processes</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#main-concepts"><i class="fa fa-check"></i><b>6.3.1</b> Main concepts</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#technical-background"><i class="fa fa-check"></i><b>6.3.2</b> Technical background:</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#covariance-functions-and-kernel"><i class="fa fa-check"></i><b>6.3.3</b> Covariance functions and kernel</a></li>
<li class="chapter" data-level="6.3.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gp-model"><i class="fa fa-check"></i><b>6.3.4</b> GP model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks"><i class="fa fa-check"></i><b>6.4</b> Neural networks</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#history"><i class="fa fa-check"></i><b>6.4.1</b> History</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks-1"><i class="fa fa-check"></i><b>6.4.2</b> Neural networks</a></li>
<li class="chapter" data-level="6.4.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#learning-more-about-deep-learning"><i class="fa fa-check"></i><b>6.4.3</b> Learning more about deep learning</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="7-matrices.html"><a href="7-matrices.html"><i class="fa fa-check"></i><b>A</b> Brief refresher on matrices</a><ul>
<li class="chapter" data-level="A.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-basics"><i class="fa fa-check"></i><b>A.1</b> Matrix basics</a><ul>
<li class="chapter" data-level="A.1.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-notation"><i class="fa fa-check"></i><b>A.1.1</b> Matrix notation</a></li>
<li class="chapter" data-level="A.1.2" data-path="7-matrices.html"><a href="7-matrices.html#random-matrix"><i class="fa fa-check"></i><b>A.1.2</b> Random matrix</a></li>
<li class="chapter" data-level="A.1.3" data-path="7-matrices.html"><a href="7-matrices.html#special-matrices"><i class="fa fa-check"></i><b>A.1.3</b> Special matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="7-matrices.html"><a href="7-matrices.html#simple-matrix-operations"><i class="fa fa-check"></i><b>A.2</b> Simple matrix operations</a><ul>
<li class="chapter" data-level="A.2.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-addition-and-multiplication"><i class="fa fa-check"></i><b>A.2.1</b> Matrix addition and multiplication</a></li>
<li class="chapter" data-level="A.2.2" data-path="7-matrices.html"><a href="7-matrices.html#matrix-transpose"><i class="fa fa-check"></i><b>A.2.2</b> Matrix transpose</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="7-matrices.html"><a href="7-matrices.html#matrix-summaries"><i class="fa fa-check"></i><b>A.3</b> Matrix summaries</a><ul>
<li class="chapter" data-level="A.3.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-trace"><i class="fa fa-check"></i><b>A.3.1</b> Matrix trace</a></li>
<li class="chapter" data-level="A.3.2" data-path="7-matrices.html"><a href="7-matrices.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>A.3.2</b> Determinant of a matrix</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="7-matrices.html"><a href="7-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.4</b> Matrix inverse</a><ul>
<li class="chapter" data-level="A.4.1" data-path="7-matrices.html"><a href="7-matrices.html#inversion-of-square-matrix"><i class="fa fa-check"></i><b>A.4.1</b> Inversion of square matrix</a></li>
<li class="chapter" data-level="A.4.2" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.4.2</b> Orthogonal matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>A.5</b> Eigenvalues and eigenvectors</a><ul>
<li class="chapter" data-level="A.5.1" data-path="7-matrices.html"><a href="7-matrices.html#definition"><i class="fa fa-check"></i><b>A.5.1</b> Definition</a></li>
<li class="chapter" data-level="A.5.2" data-path="7-matrices.html"><a href="7-matrices.html#finding-eigenvalues-and-vectors"><i class="fa fa-check"></i><b>A.5.2</b> Finding eigenvalues and vectors</a></li>
<li class="chapter" data-level="A.5.3" data-path="7-matrices.html"><a href="7-matrices.html#eigenequation-in-matrix-notation"><i class="fa fa-check"></i><b>A.5.3</b> Eigenequation in matrix notation</a></li>
<li class="chapter" data-level="A.5.4" data-path="7-matrices.html"><a href="7-matrices.html#defective-matrix"><i class="fa fa-check"></i><b>A.5.4</b> Defective matrix</a></li>
<li class="chapter" data-level="A.5.5" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-of-a-diagonal-or-triangular-matrix"><i class="fa fa-check"></i><b>A.5.5</b> Eigenvalues of a diagonal or triangular matrix</a></li>
<li class="chapter" data-level="A.5.6" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-vectors-of-a-symmetric-matrix"><i class="fa fa-check"></i><b>A.5.6</b> Eigenvalues and vectors of a symmetric matrix</a></li>
<li class="chapter" data-level="A.5.7" data-path="7-matrices.html"><a href="7-matrices.html#positive-definite-matrices"><i class="fa fa-check"></i><b>A.5.7</b> Positive definite matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.6" data-path="7-matrices.html"><a href="7-matrices.html#matrix-decompositions"><i class="fa fa-check"></i><b>A.6</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="A.6.1" data-path="7-matrices.html"><a href="7-matrices.html#diagonalisation-and-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.6.1</b> Diagonalisation and eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.6.2" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.6.2</b> Orthogonal eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.6.3" data-path="7-matrices.html"><a href="7-matrices.html#singular-value-decomposition"><i class="fa fa-check"></i><b>A.6.3</b> Singular value decomposition</a></li>
<li class="chapter" data-level="A.6.4" data-path="7-matrices.html"><a href="7-matrices.html#cholesky-decomposition"><i class="fa fa-check"></i><b>A.6.4</b> Cholesky decomposition</a></li>
</ul></li>
<li class="chapter" data-level="A.7" data-path="7-matrices.html"><a href="7-matrices.html#matrix-summaries-based-on-eigenvalues-and-singular-values"><i class="fa fa-check"></i><b>A.7</b> Matrix summaries based on eigenvalues and singular values</a><ul>
<li class="chapter" data-level="A.7.1" data-path="7-matrices.html"><a href="7-matrices.html#trace-and-determinant-computed-from-eigenvalues"><i class="fa fa-check"></i><b>A.7.1</b> Trace and determinant computed from eigenvalues</a></li>
<li class="chapter" data-level="A.7.2" data-path="7-matrices.html"><a href="7-matrices.html#rank-and-condition-number"><i class="fa fa-check"></i><b>A.7.2</b> Rank and condition number</a></li>
</ul></li>
<li class="chapter" data-level="A.8" data-path="7-matrices.html"><a href="7-matrices.html#functions-of-symmetric-matrices"><i class="fa fa-check"></i><b>A.8</b> Functions of symmetric matrices</a></li>
<li class="chapter" data-level="A.9" data-path="7-matrices.html"><a href="7-matrices.html#matrix-calculus"><i class="fa fa-check"></i><b>A.9</b> Matrix calculus</a><ul>
<li class="chapter" data-level="A.9.1" data-path="7-matrices.html"><a href="7-matrices.html#first-order-vector-derivatives"><i class="fa fa-check"></i><b>A.9.1</b> First order vector derivatives</a></li>
<li class="chapter" data-level="A.9.2" data-path="7-matrices.html"><a href="7-matrices.html#second-order-vector-derivatives"><i class="fa fa-check"></i><b>A.9.2</b> Second order vector derivatives</a></li>
<li class="chapter" data-level="A.9.3" data-path="7-matrices.html"><a href="7-matrices.html#first-order-matrix-derivatives"><i class="fa fa-check"></i><b>A.9.3</b> First order matrix derivatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="8-further-study.html"><a href="8-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="8-further-study.html"><a href="8-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="8-further-study.html"><a href="8-further-study.html#advanced-reading"><i class="fa fa-check"></i><b>B.2</b> Advanced reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="9-references.html"><a href="9-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Multivariate Statistics and Machine Learning<br />
<br />
Lecture Notes<br />
MATH38161</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised-learning-and-classification" class="section level1">
<h1><span class="header-section-number">4</span> Supervised learning and classification</h1>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<div id="supervised-learning-vs.unsupervised-learning" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Supervised learning vs.Â unsupervised learning</h3>
<p><strong>Unsupervised learning:</strong></p>
<p>Starting point:</p>
<ul>
<li>unlabeled data <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n\)</span>.</li>
</ul>
<p>Aim: find labels <span class="math inline">\(y_1, \ldots, y_n\)</span> to attach to each sample <span class="math inline">\(\boldsymbol x_i\)</span>.</p>
<p>For discrete labels <span class="math inline">\(y\)</span> unsupervised learning is called <em>clustering</em>.</p>
<p><strong>Supervised learning:</strong></p>
<p>Starting point:</p>
<ul>
<li>labeled <em>training data</em>: <span class="math inline">\(\{\boldsymbol x_1^{train}, y_1^{train}\}\)</span>,
<span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\{\boldsymbol x_n^{train}, y_n^{train} \}\)</span></li>
<li>In addition, we have unlabeled <em>test data</em>: <span class="math inline">\(\boldsymbol x^{test}\)</span></li>
</ul>
<p>Aim: use training data to learn a function <span class="math inline">\(f(\boldsymbol x)\)</span>
to predict the label corresponding to the test data.
The predictor function may provide a soft (probabilistic) assignment
or a hard assignment.</p>
<p>For <span class="math inline">\(y\)</span> discrete supervised learning is called <em>classification</em>.
For continuous <span class="math inline">\(y\)</span> the label is called response and supervised learning becomes <em>regression</em>.</p>
<p>Thus, supervised learning is a two-step procedure:</p>
<ol style="list-style-type: decimal">
<li>learn predictor function using only the training data</li>
<li>predict the label <span class="math inline">\(y^{test}\)</span> for the test data <span class="math inline">\(\boldsymbol x^{test}\)</span></li>
</ol>
</div>
<div id="terminology" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Terminology</h3>
<p>The function <span class="math inline">\(f(\boldsymbol x)\)</span> that predicts the class <span class="math inline">\(y\)</span> is called a <em>classifier</em>.</p>
<p>There are many types of classifiers, we focus here primarily on probabilistic classifiers (i.e.Â those that output the predicted class along with a probability).</p>
<p>The challenge is to find a classifier that explains the current training data well <em>and</em> that also generalises well to future unseen data. Note that it is relatively easy to find a predictor that explains the training data but especially in high dimensions (i.e.Â with many predictors) there is often overfitting and then the predictor does not generalise well!</p>
<p>The classifier function describes the decision boundary between the classes:</p>
<p><img src="fig/fig4-qda.jpg" width="80%" style="display: block; margin: auto;" /></p>
<p>In general, simple decision boundaries are preferred over complex decision boundaries to avoid overfitting!</p>
<p>Some commonly used probabilistic methods for classifications:</p>
<ul>
<li>QDA (quadratic discriminant analysis)</li>
<li>LDA (linear discriminant analysis)</li>
<li>DDA (diagonal discriminant analysis),</li>
<li>Naive Bayes classification</li>
<li>logistic regression</li>
</ul>
<p>Common non-probabilistic methods include:</p>
<ul>
<li>SVM (support vector machine),</li>
<li>random forest</li>
<li>neural networks</li>
</ul>
<p>Depending on how the classifiers are trainined there are many variations
of the above methods, e.g.Â Fisher discriminant analysis, regularised LDA, shrinkage disciminant analysis etc.</p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="bayesian-discriminant-rule-or-bayes-classifier" class="section level2">
<h2><span class="header-section-number">4.2</span> Bayesian discriminant rule or Bayes classifier</h2>
<p>Same setup as with mixture models:</p>
<ul>
<li><span class="math inline">\(K\)</span> groups with <span class="math inline">\(K\)</span> prespecified</li>
<li>each group has its own distribution <span class="math inline">\(F_k\)</span> with own parameters <span class="math inline">\(\boldsymbol \theta_k\)</span></li>
<li>the density of each class is <span class="math inline">\(f_k(\boldsymbol x) = f(\boldsymbol x| k)\)</span>.</li>
<li>prior probability of group <span class="math inline">\(k\)</span> is <span class="math inline">\(\text{Pr}(k) = \pi_k\)</span> with <span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span></li>
<li>marginal density is the mixture <span class="math inline">\(f(\boldsymbol x) = \sum_{k=1}^K \pi_k f_k(\boldsymbol x)\)</span></li>
</ul>
<p>The posterior probability of group <span class="math inline">\(k\)</span> is then
<span class="math display">\[
\text{Pr}(k | \boldsymbol x) = \frac{\pi_k f_k(\boldsymbol x) }{ f(\boldsymbol x)}
\]</span></p>
<p>The <em>discriminant function</em> is the logarithm of the posterior probability:
<span class="math display">\[
d_k(\boldsymbol x) = \log \text{Pr}(k | \boldsymbol x) = \log(\pi_k) + \log(f_k(\boldsymbol x) ) - \log(f(\boldsymbol x)) 
\]</span>
Since we use <span class="math inline">\(d_k\)</span> to compare the different classes <span class="math inline">\(k\)</span> we can
simplify the discriminant function by dropping all constant terms that do not depend on <span class="math inline">\(k\)</span> â in the above this is the term <span class="math inline">\(\log(f(\boldsymbol x))\)</span>. Hence we get for the Bayes discriminant function
<span class="math display">\[
d_k(\boldsymbol x) = \log(\pi_k) + \log(f_k(\boldsymbol x) ) 
\]</span></p>
<p>This provides us with the probability of each class given the test data <span class="math inline">\(\boldsymbol x\)</span>. For subsequent âhardâ classification we need to use a decision rule,
such as selecting the group <span class="math inline">\(\hat{k}\)</span> for that which the group probability
/ value of discriminant function is maximised
<span class="math display">\[
\hat{k} = \arg \max_k d_k(\boldsymbol x) \,.
\]</span>
with <span class="math inline">\(d_{\max} = d_{\hat{k}}(\boldsymbol x)\)</span>.</p>
<p>The discriminant functions <span class="math inline">\(d_k(\boldsymbol x)\)</span> can be mapped back to the probabilistic class assignment by using the softargmax function (also known as softmax function):
<span class="math display">\[
\text{Pr}(k | \boldsymbol x) = 
\frac{\exp( d_k(\boldsymbol x) )}{\sum_{c=1}^K \exp( d_c(\boldsymbol x) ) } = 
\frac{\exp( d_k(\boldsymbol x) - d_{\max} ) }{\sum_{c=1}^K \exp( d_c(\boldsymbol x) - d_{\max} ) }
\]</span>
Note the second form avoids numerical overflow problems when computing the exponential
by standardising the maximum of the discriminant functions to zero.</p>
<p>You have already encountered the Bayes classifier in the EM algorithm to predict the state
of the latent variables. In a simplied versions it also plays a role in the <span class="math inline">\(K\)</span>-means algorithm (see previous Chapter) and in the likelihood classifier (cf.Â Worksheet 7).</p>
</div>
<div id="normal-bayes-classifier" class="section level2">
<h2><span class="header-section-number">4.3</span> Normal Bayes classifier</h2>
<div id="quadratic-discriminant-analysis-qda-and-gaussian-assumption" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Quadratic discriminant analysis (QDA) and Gaussian assumption</h3>
<p>Quadratic discriminant analysis (QDA) is a special case of the Bayes classifier when all densities are multivariate normal with <span class="math inline">\(f_k(\boldsymbol x) = N(\boldsymbol x| \boldsymbol \mu_k, \boldsymbol \Sigma_k)\)</span>.</p>
<p>This leads to the discriminant function for QDA:
<span class="math display">\[
d_k^{QDA}(\boldsymbol x) = -\frac{1}{2} (\boldsymbol x-\boldsymbol \mu_k)^T \boldsymbol \Sigma_k^{-1} (\boldsymbol x-\boldsymbol \mu_k) -\frac{1}{2} \log \det(\boldsymbol \Sigma_k) +\log(\pi_k)
\]</span></p>
<p>There are a number of noteworthy things here:</p>
<ul>
<li>Again terms are dropped that do not depend on <span class="math inline">\(k\)</span>, such as <span class="math inline">\(-\frac{d}{2}\log( 2\pi)\)</span>.</li>
<li>Note the appearance of the Mahalanobis distance between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol \mu_k\)</span>
in the last term â recall <span class="math inline">\(d^{Mahalanobis}(\boldsymbol x, \boldsymbol \mu| \boldsymbol \Sigma) = (\boldsymbol x-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu)\)</span>.</li>
<li>The <strong>QDA discriminant function is quadratic in <span class="math inline">\(\boldsymbol x\)</span></strong> - hence its name!<br />
This implies that the <strong>decision boundaries for QDA classification are quadratic</strong> (i.e.Â parabolas in two dimensional settings).</li>
</ul>
<p>For Gaussian models specifically it can useful be to multiply the discriminant function by -2 to get rid of the factor <span class="math inline">\(-\frac{1}{2}\)</span>, but note that in that case we then need to find the minimum of the discriminant function rather than the maximum:
<span class="math display">\[
d_k^{QDA (v2)}(\boldsymbol x) =  (\boldsymbol x-\boldsymbol \mu_k)^T \boldsymbol \Sigma_k^{-1} (\boldsymbol x-\boldsymbol \mu_k) + \log \det(\boldsymbol \Sigma_k)  -2 \log(\pi_k)
\]</span>
In the literature you will find both versions of Gaussian discriminant functions so you need to check carefully which convention is used.
In the following we will use the first version only.</p>
</div>
<div id="linear-discriminant-analysis-lda" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Linear discriminant analysis (LDA)</h3>
<p>LDA is a special case of QDA, with the assumption of common overall covariance across all groups: <span class="math inline">\(\boldsymbol \Sigma_k = \boldsymbol \Sigma\)</span>.</p>
<p>This leads to a simplified discriminant function:
<span class="math display">\[
d_k^{LDA}(\boldsymbol x) = -\frac{1}{2} (\boldsymbol x-\boldsymbol \mu_k)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu_k) +\log(\pi_k)
\]</span>
Note that term containing the log-determinant is now gone, and that LDA is essentially now a method that tries to minimize the Mahalanobis distance
(while taking also into account the prior class probabilities).</p>
<p>The above function can be further simplified, by noting that the quadratic term <span class="math inline">\(\boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol x\)</span> does not depend on <span class="math inline">\(k\)</span> and hence can be dropped:
<span class="math display">\[
\begin{split}
d_k^{LDA}(\boldsymbol x) &amp;=  \boldsymbol \mu_k^T \boldsymbol \Sigma^{-1} \boldsymbol x- \frac{1}{2}\boldsymbol \mu_k^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_k + \log(\pi_k) \\
  &amp;= \boldsymbol b^T \boldsymbol x+ a
\end{split}
\]</span>
Thus, the <strong>LDA discriminant function is linear in <span class="math inline">\(\boldsymbol x\)</span>, and hence the
resulting decision boundaries are linear</strong> as well (i.e.Â straight lines in two-dimensional settings).</p>
<p>Comparison of decision boundary of LDA (left) compared with QDA (right):</p>
<p><img src="fig/fig4-ldaqda.jpg" width="90%" style="display: block; margin: auto;" /></p>
<p>Note that logistic regression (cf.Â GLM module) takes on exactly the above linear form and is indeed closely linked with the LDA classifier.</p>
</div>
<div id="diagonal-discriminant-analysis-dda" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Diagonal discriminant analysis (DDA)</h3>
<p>In DDA we assume the same setting as LDA, but now we simplify even further by assuming a <strong>diagonal covariance</strong> containing only the variances:
<span class="math display">\[
\boldsymbol \Sigma= \boldsymbol V= \begin{pmatrix}
    \sigma^2_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma^2_{d}
\end{pmatrix}
\]</span>
This simplifies the inversion of <span class="math inline">\(\boldsymbol \Sigma\)</span> as
<span class="math display">\[
\boldsymbol \Sigma^{-1} = \boldsymbol V^{-1} = \begin{pmatrix}
    \sigma^{-2}_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma^{-2}_{d}
\end{pmatrix}
\]</span>
and leads to the discriminant function
<span class="math display">\[
\begin{split}
d_k^{DDA}(\boldsymbol x) &amp;=  \boldsymbol \mu_k^T \boldsymbol V^{-1} \boldsymbol x- \frac{1}{2}\boldsymbol \mu_k^T \boldsymbol V^{-1} \boldsymbol \mu_k + \log(\pi_k) \\
  &amp;= \sum_{j=i}^d \frac{\mu_{k,j} x_j - \mu_{k,j}^2/2}{\sigma_d^2} + \log(\pi_k)
\end{split}
\]</span>
As special case of LDA, the <strong>DDA classifier is a linear classifier</strong>.</p>
<p>The <strong>Bayes classifier</strong> (using any distribution) <strong>assuming uncorrelated predictors</strong>
is also known as the <strong>naive Bayes classifier</strong>.</p>
<p>Hence, <strong>DDA is a naive Bayes classifier</strong> assuming underlying Gaussian distributions.</p>
<p>However, donât let you misguide because of the name ânaiveâ: in fact DDA and other ânaiveâ Bayes classfier are often very effective classifiers, especially in high-dimensional settings!</p>
</div>
</div>
<div id="the-training-step-learning-qda-lda-and-dda-classifiers-from-data" class="section level2">
<h2><span class="header-section-number">4.4</span> The training step â learning QDA, LDA and DDA classifiers from data</h2>
<div id="number-of-model-parameters" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Number of model parameters</h3>
<p>In order to predict the class for new data using any of the above discriminant functions we need to first learn the underlying parameters from the training data <span class="math inline">\(\boldsymbol x_i^{\text{train}}\)</span> and <span class="math inline">\(y_i^{\text{train}}\)</span>:</p>
<ul>
<li>For QDA, LDA and DDA we need to learn <span class="math inline">\(\pi_1, \ldots, \pi_K\)</span> with <span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span> and the mean vectors <span class="math inline">\(\boldsymbol \mu_1, \ldots, \boldsymbol \mu_K\)</span></li>
<li>For QDA we additionally require <span class="math inline">\(\boldsymbol \Sigma_1, \ldots, \boldsymbol \Sigma_K\)</span></li>
<li>For LDA we need <span class="math inline">\(\boldsymbol \Sigma\)</span></li>
<li>For DDA we estimate <span class="math inline">\(\sigma^2_1, \ldots, \sigma^2_d\)</span>.</li>
</ul>
<p>Overall, the total number of parameters to be estimated when learning the discriminant functions
from training data is as follows:</p>
<ul>
<li>QDA: <span class="math inline">\(K-1+ K d + K \frac{d(d-1)}{2}\)</span></li>
<li>LDA: <span class="math inline">\(K-1+ K d + \frac{d(d-1)}{2}\)</span></li>
<li>DDA: <span class="math inline">\(K-1+ K d + d\)</span></li>
</ul>
<p><img src="4-classification_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="estimating-the-discriminant-predictor-function" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Estimating the discriminant / predictor function</h3>
<p>For QDA, LDA and DDA we learn the predictor by estimating the
parameters of the discriminant function from the training data.</p>
<div id="large-sample-size" class="section level4">
<h4><span class="header-section-number">4.4.2.1</span> Large sample size</h4>
<p>If the sample size of the training data set is sufficiently large compared to the model dimensions we can use maximum likelihood to estimate the model parameters. To be able use ML we need a larger sample size for QDA and LDA (because full covariances need to be estimated) but for DDA relatively small sample size can be sufficient (which explains why ânaiveâ Bayes methods are very popular in practise).</p>
<p>To obtain the parameters estimates we use the known labels <span class="math inline">\(y_i^{\text{train}}\)</span> to sort the
samples <span class="math inline">\(\boldsymbol x_i^{\text{train}}\)</span> into the corresponding classes, and then apply the standard ML estimators.
Let <span class="math inline">\(G_k =\{i: y_i^{\text{train}}=k \}\)</span> be the set of all indices of training sample belonging to group <span class="math inline">\(k\)</span>, <span class="math inline">\(n_k\)</span> the sample size in group <span class="math inline">\(k\)</span></p>
<p>The ML estimates of the class probabilities are the frequencies
<span class="math display">\[
\hat{\pi}_k = \frac{n_k}{n}
\]</span>
and the ML estimate of the group means <span class="math inline">\(k=1, \ldots, K\)</span> are
<span class="math display">\[
\hat{\boldsymbol \mu}_k = \frac{1}{n_k} \sum_{i \in g_k} \boldsymbol x_i^{\text{train}} \, .
\]</span>
The ML estimate of the global mean <span class="math inline">\(\boldsymbol \mu_0\)</span> (i.e.Â if we assume there is only a single class and ignore the group labels) is
<span class="math display">\[
\hat{\boldsymbol \mu}_0 = \frac{1}{n} \sum_{i=1}^n \boldsymbol x_i^{\text{train}} = \sum_{k=1}^K \hat{\pi}_k \hat{\boldsymbol \mu}_k
\]</span>
Note the global mean is identical to the pooled mean (i.e.Â weighted average of
the individual group means).</p>
<p>The ML estimates for the covariances <span class="math inline">\(\boldsymbol \Sigma_k\)</span> for QDA are
<span class="math display">\[
\widehat{\boldsymbol \Sigma}_k = \frac{1}{n_k} \sum_{i \in g_k} ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_k) ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_k)^T
\]</span></p>
<p>In order to get the ML estimate of the pooled variance <span class="math inline">\(\boldsymbol \Sigma\)</span> for use with LDA we compute
<span class="math display">\[
\widehat{\boldsymbol \Sigma} = \frac{1}{n} \sum_{k=1}^K \sum_{i \in g_k} ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_k) ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_k)^T =  \sum_{k=1}^K \hat{\pi}_k \widehat{\boldsymbol \Sigma}_k 
\]</span></p>
<p>Note that the pooled variance <span class="math inline">\(\boldsymbol \Sigma\)</span> differs (substantially!) from the global variance <span class="math inline">\(\Sigma_0\)</span> that results from simply
ignoring class labels and that is computed as
<span class="math display">\[
\widehat{\boldsymbol \Sigma}_0^{ML} = \frac{1}{n} \sum_{i =1}^n ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_0) ( \boldsymbol x_i^{\text{train}} -\hat{\boldsymbol \mu}_0)^T
\]</span>
You will recognise the above from the variance decomposion in mixture models, with <span class="math inline">\(\boldsymbol \Sigma_0\)</span> being the total variance
and the pooled <span class="math inline">\(\boldsymbol \Sigma\)</span> the unexplained/with-in group variance.</p>
</div>
<div id="small-sample-size" class="section level4">
<h4><span class="header-section-number">4.4.2.2</span> Small sample size</h4>
<p>If the dimension <span class="math inline">\(d\)</span> is large compared to the sample size then the number of parameters in the predictor function grows fast. Especially QDA but also LDA is data hungry and ML estimation becomes an ill-posed problem. As discussed in Section 1.5 in this instance we need to use a regularised estimator for the covariance(s) based, e.g., on penalised ML, Bayesian learning, shrinkage estimation.
This also ensures that the estimated covariance matrices are positive definite (which is
automatically guaranteed only for DDA if all variances are positive).
Furthermore, in small sample setting it is advised to reduce the number of parameters, therefore using LDA or DDA is preferred over QDA. This can also prevent overfitting and lead to a predictor that generalises better.</p>
<p>To analyse high-dimensional data in the worksheets we will employ a regularised version of LDA and DDA using Stein-type shrinkage estimation as discussed in Section 1.5 and implemented in the R package âsdaâ.</p>
</div>
</div>
<div id="comparison-of-estimated-decision-boundaries-lda-vs.qda" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Comparison of estimated decision boundaries: LDA vs.Â QDA</h3>
<p>We compare two simple scenarios using simulated data.</p>
<p><strong>Non-nested case (<span class="math inline">\(K=4\)</span>):</strong></p>
<p><img src="fig/fig4-nonnested.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Note the linear decision boundaries for LDA!</p>
<p><strong>Nested case (<span class="math inline">\(K=2\)</span>):</strong></p>
<p><img src="fig/fig4-nested.png" width="90%" style="display: block; margin: auto;" /></p>
<p>There is no linear classifier that can seperate two nested classes!</p>
</div>
</div>
<div id="goodness-of-fit-and-variable-ranking" class="section level2">
<h2><span class="header-section-number">4.5</span> Goodness of fit and variable ranking</h2>
<p>As in linear regression (cf. âStatistical Methodsâ module) we are interested in finding out
whether the fitted mixture model is an appropriate model, and
which particular predictor(s) <span class="math inline">\(x_j\)</span> from <span class="math inline">\(\boldsymbol x=(x_1, \ldots, x_d)^T\)</span>
are responsible prediction the outcome, i.e.Â for categorizing a sample into group <span class="math inline">\(k\)</span>.</p>
<p>In order to study these problem it is helpful to rewrite the discriminant function to highlight the influence (or importance) of each predictor.</p>
<p>We focus on linear methods (LDA and DDA) and first look at the simple case <span class="math inline">\(K=2\)</span> and then generalise to more than two groups.</p>
<div id="lda-with-k2-classes" class="section level3">
<h3><span class="header-section-number">4.5.1</span> LDA with <span class="math inline">\(K=2\)</span> classes</h3>
<p>For two classes using the LDA discriminant rule will choose group <span class="math inline">\(k=1\)</span>
if <span class="math inline">\(d_1^{LDA}(\boldsymbol x) &gt; d_2^{LDA}(\boldsymbol x)\)</span>, or equivalently, if
<span class="math display">\[
\Delta_{12}^{LDA} = d_1^{LDA}(\boldsymbol x) - d_2^{LDA}(\boldsymbol x) &gt; 0
\]</span>
Since <span class="math inline">\(d_k(\boldsymbol x)\)</span> is the log-posterior (plus/minus identical constants)
<span class="math inline">\(\Delta^{LDA}\)</span> is in fact the <strong>log-posterior odds of class 1 versus class 2</strong> (see Statistical Methods, Bayesian inference).</p>
<p>The difference <span class="math inline">\(\Delta_{12}^{LDA}\)</span> is
<span class="math display">\[
\underbrace{ \Delta_{12}^{LDA}}_{\text{log posterior odds}} = 
\underbrace{(\boldsymbol \mu_1 -\boldsymbol \mu_2)^T \boldsymbol \Sigma^{-1} \left(\boldsymbol x- \frac{\boldsymbol \mu_1+\boldsymbol \mu_2}{2}\right)}_{\text{log Bayes factor } \log B_{12}} + \underbrace{\log\left( \frac{\pi_1}{\pi_2} \right)}_{\text{log prior odds}}
\]</span>
Note that since we only consider simple non-composite models here the log-Bayes factor is identical
with the log-likelihood ratio!</p>
<p>The log Bayes factor <span class="math inline">\(\log B_{12}\)</span> is known as the <em>weight of evidence</em> in favour
of <span class="math inline">\(F_1\)</span> given <span class="math inline">\(\boldsymbol x\)</span>. The <em>expected weight of evidence</em> assuming <span class="math inline">\(\boldsymbol x\)</span> is indeed from <span class="math inline">\(F_1\)</span>
is the Kullback-Leibler discrimination information in favour of groupÂ 1,
i.e.Â the KL divergence of from distribution <span class="math inline">\(F_2\)</span> to <span class="math inline">\(F_1\)</span>:
<span class="math display">\[
\text{E}_{F_1} ( \log B_{12} ) = KL(F_1 || F_2) = \frac{1}{2} (\boldsymbol \mu_1 -\boldsymbol \mu_2)^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu_1 -\boldsymbol \mu_2) = \frac{1}{2} \Omega^2
\]</span>
This yields, apart of a scale factor, a population version of
the Hotelling <span class="math inline">\(T^2\)</span>
statistic defined as
<span class="math display">\[T^2 =  c^2 (\hat{\boldsymbol \mu}_1 -\hat{\boldsymbol \mu}_2)^T \hat{\boldsymbol \Sigma}^{-1} (\hat{\boldsymbol \mu}_1 -\hat{\boldsymbol \mu}_2)\]</span>
where
<span class="math inline">\(c = (\frac{1}{n_1} + \frac{1}{n_2})^{-1/2} = \sqrt{n \pi_1 \pi_2}\)</span>
is a sample size dependent factor (for <span class="math inline">\(\text{SD}(\hat{\boldsymbol \mu}_1 - \hat{\boldsymbol \mu}_2)\)</span>).
<span class="math inline">\(T^2\)</span> is a measure of fit of the underlying two-component mixture.</p>
<p>Using the whitening transformation with <span class="math inline">\(\boldsymbol z= \boldsymbol W\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol W^T \boldsymbol W= \boldsymbol \Sigma^{-1}\)</span>
we can rewrite the log Bayes factor as
<span class="math display">\[
\begin{split}
\log B_{12} &amp;= \left( (\boldsymbol \mu_1 -\boldsymbol \mu_2)^T \boldsymbol W^T \right)\, \left(\boldsymbol W\left(\boldsymbol x- \frac{\boldsymbol \mu_1+\boldsymbol \mu_2}{2}\right) \right) \\
&amp;=\boldsymbol \omega^T \boldsymbol \delta(\boldsymbol x)
\end{split}
\]</span>
i.e.Â as the product of two vectors:</p>
<ul>
<li><span class="math inline">\(\boldsymbol \delta(\boldsymbol x)\)</span> is the whitened <span class="math inline">\(\boldsymbol x\)</span> (centered around average means)
and</li>
<li><span class="math inline">\(\boldsymbol \omega= (\omega_1, \ldots, \omega_d)^T = \boldsymbol W(\boldsymbol \mu_1 -\boldsymbol \mu_2)\)</span> gives the weight of each
whitened component <span class="math inline">\(\boldsymbol \delta(\boldsymbol x)\)</span>
in the log Bayes factor.</li>
</ul>
<p>A large positive or negative value of <span class="math inline">\(\omega_j\)</span>
indicates that the corresponding whitened predictor is relevant for choosing a class,
whereas small values of <span class="math inline">\(\omega_j\)</span> close to zero indicate that the corresponding ZCA whitened predictor is unimportant. Furthermore,
<span class="math inline">\(\boldsymbol \omega^T \boldsymbol \omega= \sum_{j=1}^d \omega_j^2 = (\boldsymbol \mu_1 -\boldsymbol \mu_2)^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu_1 -\boldsymbol \mu_2) = \Omega^2\)</span>,
i.e.Â the squared <span class="math inline">\(\omega_j^2\)</span> provide a component-wise decomposition of the overall fit <span class="math inline">\(\Omega^2\)</span>.</p>
<p>Choosing ZCA-cor as whitening transformation with <span class="math inline">\(\boldsymbol W=\boldsymbol P^{-1/2} \boldsymbol V^{-1/2}\)</span>
we get
<span class="math display">\[
\boldsymbol \omega^{ZCA-cor} = \boldsymbol P^{-1/2} \boldsymbol V^{-1/2} (\boldsymbol \mu_1 -\boldsymbol \mu_2)
\]</span>
A better understanding of <span class="math inline">\(\boldsymbol \omega^{ZCA-cor}\)</span> is provided by
comparing with the two-sample <span class="math inline">\(t\)</span>-statistic
<span class="math display">\[
\hat{\boldsymbol \tau} = c \hat{\boldsymbol V}^{-1/2} (\hat{\boldsymbol \mu}_1 - \hat{\boldsymbol \mu}_2)
\]</span>
With <span class="math inline">\(\boldsymbol \tau\)</span> the population version of <span class="math inline">\(\hat{\boldsymbol \tau}\)</span> we can define
<span class="math display">\[\boldsymbol \tau^{adj} = \boldsymbol P^{-1/2} \boldsymbol \tau= c \boldsymbol \omega^{ZCA-cor}\]</span>
as correlation-adjusted <span class="math inline">\(t\)</span>-scores (cat scores). With <span class="math inline">\(({\hat{\boldsymbol \tau}}^{adj})^T {\hat{\boldsymbol \tau}}^{adj} = T^2\)</span> we can see that the cat scores offer a component-wise decomposition of Hotellingâs <span class="math inline">\(T^2\)</span>.</p>
<p>Note the choice of ZCA-cor whitening is to ensure that the whitened components are interpretable
and stay maximally correlated to the original variables. However, you may also choose for example PCA whitening
in which case the <span class="math inline">\(\boldsymbol \omega^T \boldsymbol \omega\)</span> provide the variable importance for the PCA whitened variables.</p>
<p>For DDA, which assumes that correlations among predictors vanish, i.e. <span class="math inline">\(\boldsymbol P= \boldsymbol I_d\)</span>, we get
<span class="math display">\[
\Delta_{12}^{DDA} =\underbrace{ \left( (\boldsymbol \mu_1 -\boldsymbol \mu_2)^T \boldsymbol V^{-1/2}  \right)}_{\text{ } c^{-1} \boldsymbol \tau^T }\, \underbrace{ \left( \boldsymbol V^{-1/2} \left(\boldsymbol x- \frac{\boldsymbol \mu_1+\boldsymbol \mu_2}{2}\right) \right) }_{\text{centered standardised predictor}}+ \log\left( \frac{\pi_1}{\pi_2} \right) \\
\]</span>
Similarly as above, the <span class="math inline">\(t\)</span>-score <span class="math inline">\(\boldsymbol \tau\)</span> determines the impact of the standardised predictor in <span class="math inline">\(\Delta^{DDA}\)</span>.</p>
<p>Consequently, in DDA we can rank predictors by the squared <span class="math inline">\(t\)</span>-score.
Recall that in standard linear regression with uncorrelated predictors we can find the most important predictors
by ranking the squared marginal correlations â ranking by (squared) <span class="math inline">\(t\)</span>-scores in DDA is the exact analogy but for discrete response.</p>
</div>
<div id="multiple-classes" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Multiple classes</h3>
<p>For more than two classes we need to refer to the so-called <strong>pooled centroids formulation</strong> of DDA and LDA (introduced by Tibshirani 2002).</p>
<p>The pooled centroid is given by <span class="math inline">\(\boldsymbol \mu_0 = \sum_{k=1}^K \pi_k \boldsymbol \mu_k\)</span> â this is the centroid
if there would be only a single class. The corresponding probability (for a single class) is <span class="math inline">\(\pi_0=1\)</span> and the distribution
is called <span class="math inline">\(F_0\)</span>.</p>
<p>The LDA discriminant function for this âgroup 0â is
<span class="math display">\[
d_0^{LDA}(\boldsymbol x) = \boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol x- \frac{1}{2}\boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 
\]</span>
and the log posterior odds for comparison of group <span class="math inline">\(k\)</span> with the pooled group <span class="math inline">\(0\)</span>
is
<span class="math display">\[
\begin{split}
\Delta_k^{LDA} &amp;= d_k^{LDA}(\boldsymbol x) - d_0^{LDA}(\boldsymbol x) \\
         &amp;= \log B_{k0} + \log(\pi_k) \\
         &amp;= \boldsymbol \omega_k^T \boldsymbol \delta_k(\boldsymbol x) + \log(\pi_k)
\end{split}
\]</span>
with
<span class="math display">\[
\boldsymbol \omega_k = \boldsymbol W(\boldsymbol \mu_k - \boldsymbol \mu_0)  
\]</span>
and
<span class="math display">\[
\boldsymbol \delta_k(\boldsymbol x) = \boldsymbol W(\boldsymbol x- \frac{\boldsymbol \mu_k +\boldsymbol \mu_0}{2} )
\]</span>
The expected log Bayes factor is
<span class="math display">\[
\text{E}_{F_k} ( \log B_{k0} )= KL(F_k || F_0) = \frac{1}{2} (\boldsymbol \mu_k -\boldsymbol \mu_0)^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu_k -\boldsymbol \mu_0) = \frac{1}{2} \Omega_k^2
\]</span></p>
<p>With scale factor <span class="math inline">\(c_k = (\frac{1}{n_k} - \frac{1}{n})^{-1/2} = \sqrt{n \frac{\pi_k}{1-\pi_k}}\)</span> (for <span class="math inline">\(\text{SD}(\hat{\boldsymbol \mu}_k-\hat{\boldsymbol \mu}_0)\)</span>, with the minus sign before <span class="math inline">\(\frac{1}{n}\)</span> due to correlation between
<span class="math inline">\(\hat{\boldsymbol \mu}_k\)</span> and pooled mean <span class="math inline">\(\hat{\boldsymbol \mu}_0\)</span>)
we get as correlation-adjusted <span class="math inline">\(t\)</span>-score for comparing mean of group <span class="math inline">\(k\)</span> with the
pooled mean
<span class="math display">\[
\boldsymbol \tau_k^{adj} = c_k \boldsymbol \omega_k^{ZCA-cor} \,.
\]</span></p>
<p>For the two class case (<span class="math inline">\(K=2\)</span>) we get with
<span class="math inline">\(\boldsymbol \mu_0 = \pi_1 \boldsymbol \mu_1 + \pi_2 \boldsymbol \mu_2\)</span> for the mean difference
<span class="math inline">\((\boldsymbol \mu_1 - \boldsymbol \mu_0) = \pi_2 (\boldsymbol \mu_1 - \boldsymbol \mu_2)\)</span>
and with <span class="math inline">\(c_1 = \sqrt{n \frac{\pi_1}{\pi_2}}\)</span>
this yields
<span class="math display">\[
\boldsymbol \tau_1^{adj} = \sqrt{n \pi_1 \pi_2 } \boldsymbol P^{-1/2} \boldsymbol V^{-1/2} (\boldsymbol \mu_1 - \boldsymbol \mu_2) , 
\]</span>
i.e.Â the exact same score as in the two-class setting.</p>
</div>
</div>
<div id="variable-selection-and-cross-validation" class="section level2">
<h2><span class="header-section-number">4.6</span> Variable selection and cross-validation</h2>
<p>In the previous we saw that in DDA the natural score
for <strong>ranking features</strong> with regard to their relevance in separating the classes is
the (squared) <span class="math inline">\(t\)</span>-score, and for LDA a whitened version such as the
squared correlation-adjusted <span class="math inline">\(t\)</span>-score (based on ZCA-cor whitening) may be used.
Once such a ranking has been established the question of a <strong>suitable cutoff</strong> arises, i.e.
how many features need (or should) be retained in a model.</p>
<p>For large and high-dimensional models <strong>feature selection can also be viewed
as a form of regularisation and also dimension reduction</strong>. Specifically, there may be many variables/ features that do no contribute to the class prediction. Despite having
in principle no effect on the outcome the presence of these ânull variablesâ
can nonetheless deterioriate (sometimes dramatically!) the overall predictive accuracy of a trained predictor, because they add noise and increase the model dimension. Therefore, variables that do not contribute to prediction
should be filtered out in order to be able to construct good prediction models and classifiers.</p>
<div id="choosing-a-threshold-by-multiple-testing-using-false-discovery-rates" class="section level3">
<h3><span class="header-section-number">4.6.1</span> Choosing a threshold by multiple testing using false discovery rates</h3>
<p>The most simple way to determine a cutoff threshold is to use a standard technique for
multiple testing.</p>
<p>For each predictor variable <span class="math inline">\(x_1, \ldots, x_d\)</span> we have a corresponding test statistic
measuring the influence of this variable on the response, for example the
the <span class="math inline">\(t\)</span>-scores and related statistics discussed in the previous section.
In addition to providing an overall ranking the set of all these statistics can be used
to determine a suitable cutoff by trying to separate two populations of predictor variables:</p>
<ul>
<li>âNullâ variables that do not contribute to prediction</li>
<li>âAlternativeâ variables that are linked to prediction</li>
</ul>
<p>As discussed in the âStatistical Methodsâ module last term (Part 2 - Section 8) this can be done as follows:</p>
<ul>
<li><p>The distribution of the observed test statistics <span class="math inline">\(z_i\)</span> is assumed to follow a two-component mixture where <span class="math inline">\(F_0(z)\)</span> and <span class="math inline">\(F_A(z)\)</span> are the distributions corresponding to the null and the alternative, <span class="math inline">\(f_0(z)\)</span> and <span class="math inline">\(f_a(z)\)</span> the densities, and <span class="math inline">\(\pi_0\)</span> and <span class="math inline">\(\pi_A=1-\pi_0\)</span> are the weights:
<span class="math display">\[
f(z) = \pi_0 f_0(z) + (1-\pi_0) f_a(z)
\]</span></p></li>
<li>The null model is typically from a parametric family (e.g.Â normal around zero and with
a free variance parameter) whereas the alternative is often modelled nonparametrically.</li>
<li><p>After fitting the mixture model, often assuming some additional constraints to make the mixture identifiable, one can compute false discovery rates (FDR) as follows:</p>
<p>Local FDR:
<span class="math display">\[
\widehat{fdr}(z_i) = \hat{\text{Pr}}(\text{null} | z_i) = \frac{\hat{\pi}_0 \hat{f}_0(z_i)}{\hat{f}(z_i)}  
\]</span></p>
<p>Tail-area-based FDR (=<span class="math inline">\(q\)</span>-value):
<span class="math display">\[
\widehat{Fdr}(z_i) = \hat{\text{Pr}}(\text{null} | Z &gt; z_i) = \frac{\hat{\pi}_0 \hat{F}_0(z_i)}{\hat{F}(z_i)}
\]</span>
Note these are essentially <span class="math inline">\(p\)</span>-values adjusted for multiple testing (by a variant of the Benjamini-Hochberg method).</p></li>
</ul>
<p>By thresholding false discovery rates it is possible to identify those
variables that clearly belong to each of the two groups but also those features
that cannot easily be discriminated to fall into either group:</p>
<ul>
<li>âalternativeâ variables have low local FDR, e.g, <span class="math inline">\(\widehat{fdr}(z_i) \leq 0.2\)</span></li>
<li>ânullâ variables have high local FDR, e.g. <span class="math inline">\(\widehat{fdr}(z_i) \geq 0.8\)</span></li>
<li>features that cannot easily classified as null or alternative, e.g. <span class="math inline">\(0.2 &lt; \widehat{fdr}(z_i) &lt; 0.8\)</span></li>
</ul>
<p>For feature selection in prediction settings we generally aim to remove only those
variable that clearly belong to the null group, leaving all others in the model.</p>
</div>
<div id="quantifying-prediction-error" class="section level3">
<h3><span class="header-section-number">4.6.2</span> Quantifying prediction error</h3>
<p>Another and more direct way to compare models is to compare their predictive performance
by quantification of prediction error. Specifically, we are interested in the relative
performance of models with diverse sets of predictor. variables.</p>
<p>A measure of predictor error compares the predicted label <span class="math inline">\(\hat{y}\)</span> with the true
label <span class="math inline">\(y\)</span> for a validation data set. A validation data set contains both the
<span class="math inline">\(\boldsymbol x_i\)</span> and the associated label <span class="math inline">\(y_i\)</span> but unlike the training data it has
not been used for learning the predictor function.</p>
<p>For continuous response often the squared loss is used:
<span class="math display">\[
\text{err}(\hat{y}, y) =  (\hat{y} - y)^2
\]</span></p>
<p>For binary outcomes one often employs the 0/1 loss:
<span class="math display">\[
\text{err}(\hat{y}, y) =
\begin{cases}
    1, &amp; \text{if  } \hat{y}=y\\
    0,  &amp; \text{otherwise}
\end{cases}
\]</span>
Alternatively, any other quantity derived from the confusion matrix
(containing TP, TN, FP, FN) can be used.</p>
<p>The mean prediction error is the expectation
<span class="math display">\[
PE = \text{E}(\text{err}(\hat{y}, y))
\]</span>
and thus the empirical mean prediction error is
<span class="math display">\[
\widehat{PE} = \frac{1}{m} \sum_{i=1}^m \text{err}(\hat{y}_i, y_i)
 \]</span>
where <span class="math inline">\(m\)</span> is the sample size of the validation data set.</p>
<p>More generally, we can also quantify prediction error in the framework of so-called <strong>proper scoring rules</strong>, where the whole probabilistic forecast is taken into account (e.g.Â the individual probabilities for each class, rather than just the selected most probable class). A commonly used scoring rule is the negative log-probability (âsurpriseâ), and the expected surprise is the cross-entropy (cf.Â Statistical Methods module). So this leads back to entropy and likelihood (cf.Â e.g.Â MATH20802 lecture notes).</p>
<p>Once we have an estimate of the prediction error of a model we can use it to compare and choose among a set of candiate models, selecting those with a sufficiently low prediction
error.</p>
</div>
<div id="estimation-of-prediction-error-without-validation-data-using-cross-validation" class="section level3">
<h3><span class="header-section-number">4.6.3</span> Estimation of prediction error without validation data using cross-validation</h3>
<p>Unfortunately, quite often we do not have separate validation data available to evaluate a classifier.</p>
<p>In this case we need to rely on a simple algorithmic procedure called <strong>cross-validation</strong>.</p>
<p>Outline of cross-validation:</p>
<ol style="list-style-type: decimal">
<li>split the samples in the training data into a number (say <span class="math inline">\(K\)</span>) parts (âfoldsâ).</li>
<li>use each of the <span class="math inline">\(K\)</span> folds as validation data and the other <span class="math inline">\(K-1\)</span> folds as training data.</li>
<li>average over the resulting <span class="math inline">\(K\)</span> individual estimates of prediction error, to get an overall aggregated predictor error, along with an error.</li>
</ol>
<p>Note that in each case one part of the data is reserved for validation and
<em>not</em> used for training the predictor.</p>
<p>We choose <span class="math inline">\(K\)</span> such that the folds are not too small (to allow estimation of
prediction error) but also not too large (to make sure that we actually are able to train a reliable classifier from the remaining data). A typical value for <span class="math inline">\(K\)</span> is 5 or 10, so that 80% respectively 90% of the samples are used for training and the other 20 %
or 10% for validation.</p>
<p>If <span class="math inline">\(K=n\)</span> there are as many folds as there are samples and the validation data set consists only of a single data point. This is called âleave one outâ cross-validation (LOOCV). There are analytic approximations for the prediction error obtained by LOOCV
so that this approach is computationally inexpensive for some standard models (including regression).</p>
<p>In a number of worksheets cross-validation is employed to evaluate classification models
to demonstrate in practise that feature selection is useful to construct compact models with only a small number of variables that nonetheless generalise and predict well.</p>
<p><strong>Further reading:</strong></p>
<p>To study the technical details of cross-validation: read <strong>Section 5.1 Cross-Validation</strong> in <span class="citation">James et al. (<a href="9-references.html#ref-JWHT2013">2013</a>)</span> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/"><em>An introduction to statistical learning with applications in R</em></a>. Springer.</p>

<p></p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-clustering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="5-dependence.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math38161-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
