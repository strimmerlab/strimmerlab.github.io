<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>A Brief refresher on matrices | Multivariate Statistics and Machine Learning</title>
<meta name="author" content="Korbinian Strimmer">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.9/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Multivariate Statistics and Machine Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="multivariate-random-variables.html"><span class="header-section-number">1</span> Multivariate random variables</a></li>
<li><a class="" href="transformations-and-dimension-reduction.html"><span class="header-section-number">2</span> Transformations and dimension reduction</a></li>
<li><a class="" href="unsupervised-learning-and-clustering.html"><span class="header-section-number">3</span> Unsupervised learning and clustering</a></li>
<li><a class="" href="supervised-learning-and-classification.html"><span class="header-section-number">4</span> Supervised learning and classification</a></li>
<li><a class="" href="multivariate-dependencies.html"><span class="header-section-number">5</span> Multivariate dependencies</a></li>
<li><a class="" href="nonlinear-and-nonparametric-models.html"><span class="header-section-number">6</span> Nonlinear and nonparametric models</a></li>
<li class="book-part">Appendix</li>
<li><a class="active" href="brief-refresher-on-matrices.html"><span class="header-section-number">A</span> Brief refresher on matrices</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="brief-refresher-on-matrices" class="section level1" number="7">
<h1>
<span class="header-section-number">A</span> Brief refresher on matrices<a class="anchor" aria-label="anchor" href="#brief-refresher-on-matrices"><i class="fas fa-link"></i></a>
</h1>
<p>This is intended a very short recap of some essentials you need to know about matrices.
We will frequently make use of matrix calculations.
Matrix notation helps to make multivariate equations simpler and to understand
them better.</p>
<p>For more details please consult the lecture notes of earlier modules (e.g. linear algebra).</p>
<p>In this course we mostly work with <strong>real matrices</strong>, i.e. we assume all matrix elements are
real numbers. However, one important matrix decomposition — the eigenvalues decomposition — can yield complex-valued matrices when applied to real matrices. Below we will point out when this is the case.</p>
<div id="matrix-basics" class="section level2" number="7.1">
<h2>
<span class="header-section-number">A.1</span> Matrix basics<a class="anchor" aria-label="anchor" href="#matrix-basics"><i class="fas fa-link"></i></a>
</h2>
<div id="matrix-notation" class="section level3" number="7.1.1">
<h3>
<span class="header-section-number">A.1.1</span> Matrix notation<a class="anchor" aria-label="anchor" href="#matrix-notation"><i class="fas fa-link"></i></a>
</h3>
<p>In matrix notation we distinguish between scalars, vectors, and matrices:</p>
<p><strong>Scalar</strong>: <span class="math inline">\(x\)</span>, <span class="math inline">\(X\)</span>, lower or upper case, plain type.</p>
<p><strong>Vector</strong>: <span class="math inline">\(\bx\)</span>, lower case, bold type. In handwriting an arrow <span class="math inline">\(\vec{x}\)</span> indicates a vector.</p>
<p>In component notation we write <span class="math inline">\(\bx = (x_1, \ldots, x_d)^T = (x_i)^T\)</span>. By convention, a vector is a
column vector, i.e. the elements are arranged in a column and the index (here <span class="math inline">\(i\)</span>) refers to the row
of the column. If you transpose a column vector it becomes a row vector
<span class="math inline">\(\bx^T = (x_1, \ldots, x_d) =(x_i)\)</span> and the index now refers to the column.</p>
<p><strong>Matrix</strong>: <span class="math inline">\(\bX\)</span>, upper case, bold type. In handwriting an underscore
<span class="math inline">\(\underline{X}\)</span> indicates a matrix.</p>
<p>In component notation we write <span class="math inline">\(\bX = (x_{ij})\)</span>. By convention, the first index (here <span class="math inline">\(i\)</span>)
of the scalar elements <span class="math inline">\(x_{ij}\)</span> denotes the row and the second index (here <span class="math inline">\(j\)</span>) the column of the matrix.
Assuming that <span class="math inline">\(n\)</span> is the number of rows and <span class="math inline">\(d\)</span> is the number of columns
you can also view the matrix <span class="math inline">\(\bX = (\bx_j) = (\bz_i)^T\)</span> as being composed of column vectors
<span class="math inline">\(\bx_j = (x_{1j}, \ldots, x_{nj})^T\)</span>
or of row vectors <span class="math inline">\(\bz_i^T = (x_{i1}, \ldots, x_{id})\)</span>.</p>
<p>A (column) vector is a matrix of size <span class="math inline">\(d\times 1\)</span>. A row vector is a matrix of size <span class="math inline">\(1\times d\)</span>.
A scalar is a matrix of size <span class="math inline">\(1 \times 1\)</span>.</p>
</div>
<div id="random-matrix" class="section level3" number="7.1.2">
<h3>
<span class="header-section-number">A.1.2</span> Random matrix<a class="anchor" aria-label="anchor" href="#random-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>A <strong>random matrix</strong> (vector) is a matrix (vector) whose elements are random variables.</p>
<p>Note that the standard
notation used in univariate statistics to distinguish
random variables and their realisations (i.e. upper versus lower case) does not
work in multivariate statistics. Therefore, you need to determine
from the context whether a quantity represents a
random variable, or whether it is a constant.</p>
</div>
<div id="special-matrices" class="section level3" number="7.1.3">
<h3>
<span class="header-section-number">A.1.3</span> Special matrices<a class="anchor" aria-label="anchor" href="#special-matrices"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math inline">\(\bI_d\)</span> is the identity matrix. It is a square matrix of size
<span class="math inline">\(d \times d\)</span> with the diagonal
filled with 1 and off-diagonals filled with 0.
<span class="math display">\[\bI_d =
\begin{pmatrix}
    1 &amp; 0 &amp; 0 &amp; \dots &amp; 0\\
    0 &amp; 1 &amp; 0 &amp; \dots &amp; 0\\
    0 &amp; 0 &amp; 1 &amp;   &amp; 0\\
    \vdots &amp; \vdots &amp; &amp; \ddots &amp;  \\
    0 &amp; 0 &amp; 0 &amp;  &amp; 1 \\
\end{pmatrix}\]</span></p>
<p><span class="math inline">\(\bOne\)</span> is a matrix that contains only 1s. Most often
it is used in the form of a column vector with <span class="math inline">\(d\)</span> rows:
<span class="math display">\[\bOne_d =
\begin{pmatrix}
    1 \\
    1 \\
    1 \\
    \vdots   \\
    1  \\
\end{pmatrix}\]</span></p>
<p>A diagonal matrix is a matrix where all off-diagonal elements are zero.</p>
<p>A triangular matrix is a square matrix whose elements either below or above the diagonal are all zero (upper vs. lower triangular matrix).</p>
</div>
</div>
<div id="simple-matrix-operations" class="section level2" number="7.2">
<h2>
<span class="header-section-number">A.2</span> Simple matrix operations<a class="anchor" aria-label="anchor" href="#simple-matrix-operations"><i class="fas fa-link"></i></a>
</h2>
<div id="matrix-addition-and-multiplication" class="section level3" number="7.2.1">
<h3>
<span class="header-section-number">A.2.1</span> Matrix addition and multiplication<a class="anchor" aria-label="anchor" href="#matrix-addition-and-multiplication"><i class="fas fa-link"></i></a>
</h3>
<p>Matrices behave much like common numbers. For example, there exist
matrix addition <span class="math inline">\(\bC = \bA + \bB\)</span>
and matrix multiplication <span class="math inline">\(\bC = \bA \bB\)</span>.</p>
<p>Matrix addition is simply the result of the addition of the corresponding elements in <span class="math inline">\(\bA\)</span> and <span class="math inline">\(\bB\)</span>,
i.e <span class="math inline">\(c_{ij} = a_{ij} + b_{ij}\)</span>. For matrix addition <span class="math inline">\(\bA\)</span> and <span class="math inline">\(\bB\)</span> must have the same size, i.e.
the same number of rows and columns.</p>
<p>The dot product between two vectors is <span class="math inline">\(\ba \cdot \bb = \ba^T \bb = \ba \bb^T = \sum_{i=1}^d a_{i} b_{i}\)</span>.</p>
<p>Matrix multiplication is defined as <span class="math inline">\(c_{ij} = \sum_{k=1}^m a_{ik} b_{kj}\)</span> where <span class="math inline">\(m\)</span> is the
number of columns of <span class="math inline">\(\bA\)</span> and the number of rows in <span class="math inline">\(\bB\)</span>. Thus, <span class="math inline">\(\bC\)</span> contains all possible
dot products of the row vectors in <span class="math inline">\(\bA\)</span> with the column vectors in <span class="math inline">\(\bB\)</span>.
For matrix multiplication the number of columns in <span class="math inline">\(\bA\)</span> must match the number of rows in <span class="math inline">\(\bB\)</span>.
Note that matrix multiplication is in general not commutative, i.e. <span class="math inline">\(\bA \bB \neq \bB \bA\)</span>.</p>
</div>
<div id="matrix-transpose" class="section level3" number="7.2.2">
<h3>
<span class="header-section-number">A.2.2</span> Matrix transpose<a class="anchor" aria-label="anchor" href="#matrix-transpose"><i class="fas fa-link"></i></a>
</h3>
<p>The matrix transpose <span class="math inline">\(t(\bA) = \bA^T\)</span> interchanges rows and columns. The transpose
is a linear operator <span class="math inline">\((\bA + \bB)^T = \bA^T + \bB^T\)</span> and
applied to a matrix
product it reverses the ordering, i.e. <span class="math inline">\((\bA \bB)^T =\bB^T \bA^T\)</span>.</p>
<p>If <span class="math inline">\(\bA = \bA^T\)</span> then <span class="math inline">\(\bA\)</span> is symmetric (and square).</p>
<p>By construction given a rectangular <span class="math inline">\(\bA\)</span> the matrices
<span class="math inline">\(\bA^T \bA\)</span> and <span class="math inline">\(\bA \bA^T\)</span> are symmetric with non-negative diagonal.</p>
</div>
</div>
<div id="matrix-summaries" class="section level2" number="7.3">
<h2>
<span class="header-section-number">A.3</span> Matrix summaries<a class="anchor" aria-label="anchor" href="#matrix-summaries"><i class="fas fa-link"></i></a>
</h2>
<div id="matrix-trace" class="section level3" number="7.3.1">
<h3>
<span class="header-section-number">A.3.1</span> Matrix trace<a class="anchor" aria-label="anchor" href="#matrix-trace"><i class="fas fa-link"></i></a>
</h3>
<p>The trace of the matrix is the sum of the diagonal elements <span class="math inline">\(\trace(\bA) = \sum a_{ii}\)</span>.</p>
<p>A useful identity for the matrix trace is
<span class="math display">\[
\trace(\bA \bB ) = \trace( \bB \bA)  
\]</span>
with for two vectors becomes
<span class="math display">\[
\ba^T \bb = \trace( \bb \ba^T) \,.
\]</span></p>
<p>The squared Frobenius norm, i.e. the sum of the squares of all entries of a rectangular matrix <span class="math inline">\(\bA =(a_{ij})\)</span>, can be
written using the trace as follows:<br><span class="math display">\[
\begin{split}
||\bA ||_F^2 &amp;= \sum_{i,j} a_{ij}^2 \\
 &amp;= \trace(\bA^T \bA) = \trace(\bA \bA^T) \,.
\end{split}
\]</span></p>
</div>
<div id="determinant-of-a-matrix" class="section level3" number="7.3.2">
<h3>
<span class="header-section-number">A.3.2</span> Determinant of a matrix<a class="anchor" aria-label="anchor" href="#determinant-of-a-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(\bA\)</span> is a square matrix the determinant <span class="math inline">\(\det(\bA)\)</span> is a scalar measuring the volume spanned by the column vectors in <span class="math inline">\(\bA\)</span> with the sign determined by the orientation of the vectors.</p>
<p>If <span class="math inline">\(\det(\bA) \neq 0\)</span> the matrix <span class="math inline">\(\bA\)</span> is non-singular or non-degenerate. Conversely, if
<span class="math inline">\(\det(\bA) =0\)</span> the matrix <span class="math inline">\(\bA\)</span> is singular or degenerate.</p>
<p>One way to compute the determinant of a matrix <span class="math inline">\(\bA\)</span> is the Laplace cofactor
expansion approach that proceeds recursively based on the determinants of the submatrices <span class="math inline">\(\bA_{-i,-j}\)</span> obtained by deleting row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> from <span class="math inline">\(\bA\)</span>. Specifically, at each
level we compute the</p>
<ol style="list-style-type: decimal">
<li>cofactor expansion either
<ol style="list-style-type: lower-alpha">
<li>along the <span class="math inline">\(i\)</span>-th row — pick any row <span class="math inline">\(i\)</span>:
<span class="math display">\[\det(\bA) = \sum_{j=1}^d a_{ij} (-1)^{i+j} \det(\bA_{-i,-j})  \text{ , or}\]</span>
</li>
<li>along the <span class="math inline">\(j\)</span>-th column — pick any <span class="math inline">\(j\)</span>:
<span class="math display">\[\det(\bA) = \sum_{i=1}^d a_{ij} (-1)^{i+j} \det(\bA_{-i,-j})\]</span>.</li>
</ol>
</li>
<li>Then repeat until the submatrix is a scalar <span class="math inline">\(a\)</span> and <span class="math inline">\(\det(a)=a \,.\)</span>
</li>
</ol>
<p>The recursive nature of this algorithm leads to a complexity of order <span class="math inline">\(O(d!)\)</span> so it is not practical except for very small <span class="math inline">\(d\)</span>.
Therefore, in practice other more efficient algorithms for computing determinants are used but these still have algorithmic complexity in the order of <span class="math inline">\(O(d^3)\)</span> so for large dimensions obtaining determinants is
very expensive.</p>
<p>However, some specially structured matrices do allow for very fast calculation.
In particular, it turns out that the determinant of a triangular matrix (which includes diagonal matrices)
is simply the product of the diagonal elements.</p>
<p>For a two-dimensional matrix <span class="math inline">\(\bA = \begin{pmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \\\end{pmatrix}\)</span>
the determinant is <span class="math inline">\(\det(A) = a_{11} a_{22} - a_{12} a_{21}\)</span>.</p>
<p>For a block-structured square matrix
<span class="math display">\[
\bA = \begin{pmatrix} \bA_{11} &amp; \bA_{12} \\ \bA_{21} &amp; \bA_{22} \\ \end{pmatrix} \, ,
\]</span>
where the matrices on the diagonal <span class="math inline">\(\bA_{11}\)</span> and <span class="math inline">\(\bA_{22}\)</span> are themselves square but
<span class="math inline">\(\bA_{21}\)</span> and <span class="math inline">\(\bA_{21}\)</span> can have any shape,
the determinant is
<span class="math display">\[
\det(\bA) = \det(\bA_{22}) \det(\bC_1) = \det(\bA_{11}) \det(\bC_2) 
\]</span>
with the (Schur complement of <span class="math inline">\(\bA_{22}\)</span>)
<span class="math display">\[
\bC_1 = \bA_{11} -  \bA_{12}  \bA_{22}^{-1}  \bA_{21} 
\]</span>
and (Schur complement of <span class="math inline">\(\bA_{11}\)</span>)
<span class="math display">\[
\bC_2 = \bA_{22} -  \bA_{21}  \bA_{11}^{-1}  \bA_{12} 
\]</span>
Note that <span class="math inline">\(\bC_1\)</span> and <span class="math inline">\(\bC_2\)</span> are square matrices.</p>
<p>For a block-diagonal matrix <span class="math inline">\(\bA\)</span> with <span class="math inline">\(\bA_{12} = 0\)</span> and <span class="math inline">\(\bA_{21} = 0\)</span>
the determinant is <span class="math inline">\(\det(\bA) = \det(\bA_{11}) \det(\bA_{22})\)</span>.</p>
<p>Determinants have a multiplicative property,
<span class="math display">\[\det(\bA \bB) = \det(\bB \bA) = \det(\bA) \det(\bB) \,.\]</span>
For scalar <span class="math inline">\(a\)</span> this becomes
<span class="math inline">\(\det(a \bB) = a^d \det(\bB)\)</span> where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(\bB\)</span>.</p>
<p>Another important identity is
<span class="math display">\[\det(\bI_n + \bA \bB) = \det(\bI_m + \bB \bA)\]</span>
where <span class="math inline">\(\bA\)</span> is a <span class="math inline">\(n \times m\)</span> and <span class="math inline">\(\bB\)</span> is a <span class="math inline">\(m \times n\)</span> matrix. This is called the
Weinstein-Aronszajn determinant identity (also credited to Sylvester).</p>
</div>
</div>
<div id="matrix-inverse" class="section level2" number="7.4">
<h2>
<span class="header-section-number">A.4</span> Matrix inverse<a class="anchor" aria-label="anchor" href="#matrix-inverse"><i class="fas fa-link"></i></a>
</h2>
<div id="inversion-of-square-matrix" class="section level3" number="7.4.1">
<h3>
<span class="header-section-number">A.4.1</span> Inversion of square matrix<a class="anchor" aria-label="anchor" href="#inversion-of-square-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(\bA\)</span> is a square matrix then the inverse matrix <span class="math inline">\(\bA^{-1}\)</span> is a matrix
such that
<span class="math display">\[\bA^{-1} \bA = \bA \bA^{-1}=  \bI \, .\]</span>
Only non-singular matrices with <span class="math inline">\(\det(\bA) \neq 0\)</span> are invertible.</p>
<p>As <span class="math inline">\(\det(\bA^{-1} \bA) = \det(\bI) = 1\)</span> the
determinant of the inverse matrix equals
the inverse determinant,
<span class="math display">\[\det(\bA^{-1}) = \det(\bA)^{-1} \,.\]</span></p>
<p>The transpose of the inverse is the inverse of the transpose
as
<span class="math display">\[
\begin{split}
(\bA^{-1})^T &amp;= (\bA^{-1})^T \,  \bA^T (\bA^{T})^{-1}   \\
 &amp;= (\bA \bA^{-1})^T \, (\bA^{T})^{-1} = (\bA^{T})^{-1} \,. \\
\end{split}
\]</span></p>
<p>The inverse of a matrix product <span class="math inline">\((\bA \bB)^{-1} = \bB^{-1} \bA^{-1}\)</span>
is the product of the indivdual matrix inverses in reverse order.</p>
<p>There are many different algorithms to compute the inverse of a matrix
(which is essentially a problem of solving a system of equations).
The computational complexity of matrix inversion is of the order <span class="math inline">\(O(d^3)\)</span>
where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(\bA\)</span>. Therefore matrix inversion is very costly in higher dimensions.</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example A.1  </strong></span>Inversion of a <span class="math inline">\(2 \times 2\)</span> matrix:</p>
<p>The inverse of the matrix <span class="math inline">\(A = \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}\)</span> is
<span class="math inline">\(A^{-1} = \frac{1}{ad-bc} \begin{pmatrix} d &amp; -b \\ -c &amp; a \end{pmatrix}\)</span></p>
</div>
</div>
<div id="inversion-of-structured-matrices" class="section level3" number="7.4.2">
<h3>
<span class="header-section-number">A.4.2</span> Inversion of structured matrices<a class="anchor" aria-label="anchor" href="#inversion-of-structured-matrices"><i class="fas fa-link"></i></a>
</h3>
<p>However, for specially structured matrices inversion can be done effectively:</p>
<ul>
<li>The inverse of a <strong>diagonal matrix</strong> is another diagonal matrix obtained by inverting the diagonal elements.</li>
<li>More generally, the inverse of a <strong>block-diagonal matrix</strong> is obtained by individually inverting the blocks along the diagonal.</li>
</ul>
<p>The <strong>Woodbury matrix identity</strong> simplifies the inversion of matrices that can be
written as <span class="math inline">\(\bA + \bU \bB \bV\)</span> where <span class="math inline">\(\bA\)</span> and <span class="math inline">\(\bB\)</span> are both square and
<span class="math inline">\(\bU\)</span> and <span class="math inline">\(\bV\)</span> are suitable rectangular matrices:
<span class="math display">\[
(\bA + \bU \bB \bV)^{-1} = \bA^{-1} - \bA^{-1} \bU (\bB^{-1} + \bV \bA^{-1} \bU)^{-1} \bV \bA^{-1}
\]</span>
Typically, the inverse <span class="math inline">\(\bA^{-1}\)</span> is either already known or can be easily obtained and
the dimension of <span class="math inline">\(\bB\)</span> is much lower than that of <span class="math inline">\(\bA\)</span>.</p>
<p>The class of matrices that can be most easily inverted are <strong>orthogonal matrices</strong> whose inverse is
obtained simply by transposing the matrix.</p>
</div>
</div>
<div id="orthogonal-matrices" class="section level2" number="7.5">
<h2>
<span class="header-section-number">A.5</span> Orthogonal matrices<a class="anchor" aria-label="anchor" href="#orthogonal-matrices"><i class="fas fa-link"></i></a>
</h2>
<div id="properties-1" class="section level3" number="7.5.1">
<h3>
<span class="header-section-number">A.5.1</span> Properties<a class="anchor" aria-label="anchor" href="#properties-1"><i class="fas fa-link"></i></a>
</h3>
<p>An orthogonal matrix <span class="math inline">\(\bQ\)</span> is a square matrix with the property that <span class="math inline">\(\bQ^T = \bQ^{-1}\)</span>, i.e.
the transpose is also the inverse. This implies that <span class="math inline">\(\bQ \bQ^T = \bQ^T \bQ = \bI\)</span>.</p>
<p>The identity matrix <span class="math inline">\(\bI\)</span> is the simplest example of an orthogonal matrix.</p>
<p>An orthogonal matrix
<span class="math inline">\(\bQ\)</span> can be interpreted geometrically as an operator performing
rotation, reflection and/or permutation.
Multiplication of <span class="math inline">\(\bQ\)</span> with a vector will result in
a new vector of the same length but with a change in direction (unless <span class="math inline">\(\bQ=\bI\)</span>).</p>
<p>The product <span class="math inline">\(\bQ_3 = \bQ_1 \bQ_2\)</span> of two orthogonal matrices <span class="math inline">\(\bQ_1\)</span> and <span class="math inline">\(\bQ_2\)</span> yields another orthogonal matrix as <span class="math inline">\(\bQ_3 \bQ_3^T = \bQ_1 \bQ_2 (\bQ_1 \bQ_2)^T = \bQ_1 \bQ_2 \bQ_2^T \bQ_1^T = \bI\)</span>.</p>
<p>The determinant <span class="math inline">\(\det(\bQ)\)</span> of an orthogonal matrix is either +1 or -1,
because <span class="math inline">\(\bQ \bQ^T = \bI\)</span> and thus <span class="math inline">\(\det(\bQ)\det(\bQ^T) = \det(\bQ)^2 = \det(\bI) = 1\)</span>.</p>
<p>The set of all orthogonal matrices of dimension <span class="math inline">\(d\)</span> together with multiplication
form a group called the orthogonal group <span class="math inline">\(O(d)\)</span>.
The subset of orthogonal matrices with <span class="math inline">\(\det(\bQ)=1\)</span> are called rotation matrices and form with multiplication the special orthogonal group <span class="math inline">\(SO(d)\)</span>.
Orthogonal matrices with <span class="math inline">\(\det(\bQ)=-1\)</span> are rotation-reflection matrices.</p>
</div>
<div id="generating-orthogonal-matrices" class="section level3" number="7.5.2">
<h3>
<span class="header-section-number">A.5.2</span> Generating orthogonal matrices<a class="anchor" aria-label="anchor" href="#generating-orthogonal-matrices"><i class="fas fa-link"></i></a>
</h3>
<p>In two dimensions <span class="math inline">\((d=2)\)</span> all orthogonal matrices <span class="math inline">\(\bR\)</span> representing rotations with <span class="math inline">\(\det(\bR)=1\)</span> are
given by
<span class="math display">\[
\bR(\theta) = 
\begin{pmatrix}
\cos \theta &amp; -\sin \theta \\
\sin \theta &amp; \cos \theta 
\end{pmatrix}
\]</span>
and those representing rotation-reflections <span class="math inline">\(\bG\)</span> with <span class="math inline">\(\det(\bG)=-1\)</span> by
<span class="math display">\[
\bG(\theta) = 
\begin{pmatrix}
\cos \theta &amp; \sin \theta \\
\sin \theta &amp; -\cos \theta
\end{pmatrix}\,.
\]</span>
Every orthogonal matrix of dimension <span class="math inline">\(d=2\)</span>
can be represented as the product of at most two rotation-reflection
matrices because
<span class="math display">\[
\bR(\theta) = \bG(\theta)\, \bG(0) =  
\begin{pmatrix}
\cos \theta &amp; \sin \theta \\
\sin \theta &amp; -\cos \theta
\end{pmatrix} 
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; -1
\end{pmatrix}\,.
\]</span>
Thus, the matrix <span class="math inline">\(\bG\)</span> is a generator of two-dimensional orthogonal matrices.
Note that <span class="math inline">\(\bG(\theta)\)</span> is symmetric, orthogonal and has determinant -1.</p>
<p>More generally, and applicable in arbitrary dimension, the role of generator is taken by the Householder reflection matrix
<span class="math display">\[
\bQ_{HH}(\bv) = \bI- 2 \bv \bv^T
\]</span>
where <span class="math inline">\(\bv\)</span> is a vector of unit length (with <span class="math inline">\(\bv^T \bv=1\)</span>) orthogonal to
the reflection hyperplane. Note that <span class="math inline">\(\bQ_{HH}(\bv) = \bQ_{HH}(-\bv)\)</span>.
By construction the matrix <span class="math inline">\(\bQ_{HH}(\bv)\)</span> is symmetric, orthogonal and has determinant -1.</p>
<p>It can be shown that any <span class="math inline">\(d\)</span>-dimensional orthogonal matrix <span class="math inline">\(\bQ\)</span> can be represented as the product of at most <span class="math inline">\(d\)</span> Householder reflection matrices.
The two-dimensional generator <span class="math inline">\(\bG(\theta)\)</span> is recovered as the Householder matrix <span class="math inline">\(\bQ_{HH}(\bv)\)</span>
with <span class="math inline">\(\bv = \begin{pmatrix} -\sin \frac{\theta}{2} \\ \cos \frac{\theta}{2} \end{pmatrix}\)</span>
or <span class="math inline">\(\bv = \begin{pmatrix} \sin \frac{\theta}{2} \\ -\cos \frac{\theta}{2} \end{pmatrix}\)</span>.</p>
</div>
<div id="permutation-matrix" class="section level3" number="7.5.3">
<h3>
<span class="header-section-number">A.5.3</span> Permutation matrix<a class="anchor" aria-label="anchor" href="#permutation-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>A special type of an orthogonal matrix is a permutation matrix <span class="math inline">\(\bP\)</span> created by
permuting rows and/or columns of the identity matrix <span class="math inline">\(\bI\)</span>. Thus, each row and column
of <span class="math inline">\(\bP\)</span> contains exactly one entry of 1, but not necessarily on the diagonal.</p>
<p>If a permutation matrix <span class="math inline">\(\bP\)</span> is multiplied with a matrix <span class="math inline">\(\bA\)</span> it acts as an operator
permuting the columns (<span class="math inline">\(\bA \bP\)</span>) or the rows (<span class="math inline">\(\bP \bA\)</span>).
For a set of <span class="math inline">\(d\)</span> elements there exist <span class="math inline">\(d!\)</span> permutations. Thus, for dimension <span class="math inline">\(d\)</span> there
are <span class="math inline">\(d!\)</span> possible permutation matrices (including the identity matrix).</p>
<p>The determinant of a permutation matrix is either +1 or -1.
The product of two permutation matrices yields another permutation matrix.</p>
<p>Symmetric permutation matrices correspond to self-inverse permutations
(i.e. the permutation matrix is its own inverse), and are also called permutation involutions.
They can have determinant +1 and -1.</p>
<p>A transposition is a permutation where only two elements are exchanged.
Thus, in a transposition matrix <span class="math inline">\(\bT\)</span>
exactly two rows and/or columns are exchanged compared to identity matrix <span class="math inline">\(\bI\)</span>.
Transpositions are self-inverse, and transposition matrices are symmetric.
There are <span class="math inline">\(\frac{d (d-1)}{2}\)</span> different transposition matrices.
The determinant of a transposition matrix is <span class="math inline">\(\det(\bT)= -1\)</span>.</p>
<p>Note that the transposition matrix is an instance of a Householder matrix <span class="math inline">\(\bQ_{HH}(\bv)\)</span>
with vector <span class="math inline">\(\bv\)</span> filled with zeros except for two elements that have value
<span class="math inline">\(\frac{\sqrt{2}}{2}\)</span> and <span class="math inline">\(-\frac{\sqrt{2}}{2}\)</span>.</p>
<p>Any permutation of <span class="math inline">\(d\)</span> elements can be generated by a series of at most <span class="math inline">\(d-1\)</span> transpositions.
Correspondingly, any permutation matrix <span class="math inline">\(\bP\)</span> can be constructed by multiplication of the identity
matrix with at most <span class="math inline">\(d-1\)</span> transposition matrices. If the number of transpositions is even then <span class="math inline">\(\det(\bP) = 1\)</span> otherwise
for an uneven number <span class="math inline">\(\det(\bP) = -1\)</span>. This is called the sign or signature of the permutation.</p>
<p>The set of all permutations form the symmetric group <span class="math inline">\(S_d\)</span>, the subset of even permutations (with positive sign and <span class="math inline">\(\det(\bP)=1\)</span>) the alternating group <span class="math inline">\(A_d\)</span>.</p>
</div>
</div>
<div id="eigenvalues-and-eigenvectors" class="section level2" number="7.6">
<h2>
<span class="header-section-number">A.6</span> Eigenvalues and eigenvectors<a class="anchor" aria-label="anchor" href="#eigenvalues-and-eigenvectors"><i class="fas fa-link"></i></a>
</h2>
<div id="definition" class="section level3" number="7.6.1">
<h3>
<span class="header-section-number">A.6.1</span> Definition<a class="anchor" aria-label="anchor" href="#definition"><i class="fas fa-link"></i></a>
</h3>
<p>Assume a square symmetric matrix <span class="math inline">\(\bA\)</span> of size <span class="math inline">\(d \times d\)</span>.
A vector <span class="math inline">\(\bu \neq 0\)</span> is called an eigenvector of the matrix <span class="math inline">\(\bA\)</span> and <span class="math inline">\(\lambda\)</span> the corresponding
eigenvalue if<br><span class="math display">\[\bA \bu = \bu \lambda \, .\]</span></p>
</div>
<div id="finding-eigenvalues-and-vectors" class="section level3" number="7.6.2">
<h3>
<span class="header-section-number">A.6.2</span> Finding eigenvalues and vectors<a class="anchor" aria-label="anchor" href="#finding-eigenvalues-and-vectors"><i class="fas fa-link"></i></a>
</h3>
<p>To find the eigenvalues and eigenvector the <em>eigenequation</em> is rewritten as
<span class="math display">\[(\bA -\bI \lambda ) \bu  = 0 \,.\]</span> For any solution <span class="math inline">\(\bu \neq 0\)</span> the corresponding eigenvalue
<span class="math inline">\(\lambda\)</span> must make the matrix <span class="math inline">\(\bA -\bI \lambda\)</span> singular, i.e. the determinant must vanish
<span class="math display">\[\det(\bA -\bI \lambda ) =0 \,.\]</span>
This is the <em>characteristic equation</em> of the matrix <span class="math inline">\(\bA\)</span>, and its solution yields <span class="math inline">\(d\)</span>
not necessarily distinct and also potentially complex eigenvalues <span class="math inline">\(\lambda_1, \ldots, \lambda_d\)</span>.</p>
<p>If there are complex eigenvalues, for a real matrix those eigenvalues come in conjugate pairs.
Specifically, for a complex <span class="math inline">\(\lambda_1 = r e^{i \phi}\)</span> there will also be a corresponding complex eigenvalue <span class="math inline">\(\lambda_2 = r e^{-i \phi}\)</span>.</p>
<p>Given the eigenvalues we then solve the eigenequation for the corresponding non-zero eigenvectors
<span class="math inline">\(\bu_1, \ldots, \bu_d\)</span>. Note that eigenvectors of real matrices can have complex components.
Also the eigenvector is only defined by the eigenequation up to a scalar.
By convention eigenvectors are therefore typically standardised to unit length but this still leaves
a sign ambiguity for real eigenvectors and implies that complex eigenvectors are defined only up to a factor with modulus 1.</p>
</div>
<div id="eigenequation-in-matrix-notation" class="section level3" number="7.6.3">
<h3>
<span class="header-section-number">A.6.3</span> Eigenequation in matrix notation<a class="anchor" aria-label="anchor" href="#eigenequation-in-matrix-notation"><i class="fas fa-link"></i></a>
</h3>
<p>With the matrix
<span class="math display">\[\bU = (\bu_1, \ldots, \bu_d)\]</span> containing the standardised eigenvectors in the columns and the diagonal matrix
<span class="math display">\[\bLambda = \begin{pmatrix}
    \lambda_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}
\end{pmatrix}\]</span>
containing the eigenvalues (typically sorted in order of magnitude) the eigenvalue equation can be written as
<span class="math display">\[\bA \bU = \bU \bLambda \,.\]</span></p>
<p>Note that if eigenvalues are not in order, we can alwas apply a permutation matrix <span class="math inline">\(\bP\)</span> to sort them in order, such that <span class="math inline">\(\bU' = \bU \bP\)</span> reorders the eigenvectors and <span class="math inline">\(\bLambda' = \bP^T \bLambda \bP\)</span>
the eigenvalues, with
<span class="math display">\[\bA \bU' =  \bA \bU \bP = \bU \bLambda \bP =  \bU \bP \bP^T \bLambda \bP =  \bU' \bLambda' \,.\]</span></p>
</div>
<div id="defective-matrix" class="section level3" number="7.6.4">
<h3>
<span class="header-section-number">A.6.4</span> Defective matrix<a class="anchor" aria-label="anchor" href="#defective-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>In most cases the eigenvectors <span class="math inline">\(\bu_i\)</span> will be linearly independent so that they form a basis to span a <span class="math inline">\(d\)</span> dimensional space.</p>
<p>However, if this is not the case and
the matrix <span class="math inline">\(\bA\)</span> does not have a complete basis of eigenvectors, then the matrix is called defective. In this case
the matrix <span class="math inline">\(\bU\)</span> containing the eigenvectors is singular and <span class="math inline">\(\det(\bU)=0\)</span>.</p>
<p>An example of a defective matrix is
<span class="math inline">\(\begin{pmatrix} 1 &amp;1 \\ 0 &amp; 1 \\ \end{pmatrix}\)</span>
which has determinant 1 so that it can be inverted and its column vectors do form a complete basis
but has only one distinct eigenvector <span class="math inline">\((1,0)^T\)</span> so that the eigenvector basis is incomplete.</p>
</div>
<div id="eigenvalues-of-a-diagonal-or-triangular-matrix" class="section level3" number="7.6.5">
<h3>
<span class="header-section-number">A.6.5</span> Eigenvalues of a diagonal or triangular matrix<a class="anchor" aria-label="anchor" href="#eigenvalues-of-a-diagonal-or-triangular-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>In the special case that <span class="math inline">\(\bA\)</span> is diagonal or a triangular matrix the eigenvalues are easily determined.
This follows from the simple form of their determinants as the product of the diagonal elements.
Hence for these matrices the characteristic equation becomes <span class="math inline">\(\prod_{i}^d (a_{ii} -\lambda) = 0\)</span> and has solution
<span class="math inline">\(\lambda_i=a_{ii}\)</span>, i.e. the eigenvalues are equal to the diagonal elements.</p>
</div>
<div id="eigenvalues-and-vectors-of-a-symmetric-matrix" class="section level3" number="7.6.6">
<h3>
<span class="header-section-number">A.6.6</span> Eigenvalues and vectors of a symmetric matrix<a class="anchor" aria-label="anchor" href="#eigenvalues-and-vectors-of-a-symmetric-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(\bA\)</span> is symmetric, i.e. <span class="math inline">\(\bA = \bA^T\)</span>, then its eigenvalues and eigenvectors have special properties:</p>
<ol style="list-style-type: lower-roman">
<li>all eigenvalues of <span class="math inline">\(\bA\)</span> are real,</li>
<li>the eigenvectors are orthogonal, i.e <span class="math inline">\(\bu_i^T \bu_j = 0\)</span> for <span class="math inline">\(i \neq j\)</span>, and real. Thus, the matrix <span class="math inline">\(\bU\)</span> containing the standardised orthonormal eigenvectors is orthogonal.</li>
<li>
<span class="math inline">\(\bA\)</span> is never defective as <span class="math inline">\(\bU\)</span> forms a complete basis.</li>
</ol>
</div>
<div id="eigenvalues-of-orthogonal-matrices" class="section level3" number="7.6.7">
<h3>
<span class="header-section-number">A.6.7</span> Eigenvalues of orthogonal matrices<a class="anchor" aria-label="anchor" href="#eigenvalues-of-orthogonal-matrices"><i class="fas fa-link"></i></a>
</h3>
<p>The eigenvalues of an orthogonal matrix <span class="math inline">\(\bQ\)</span> are not necessarily real but
they all have modulus 1 and lie on the unit circle . Thus, the eigenvalues of <span class="math inline">\(\bQ\)</span>
all have the form <span class="math inline">\(\lambda = e^{i \phi} = \cos \phi + i \sin \phi\)</span>.</p>
<p>In any real matrix complex eigenvalues come in conjugate
pairs. Hence if an orthogonal matrix <span class="math inline">\(\bQ\)</span> has the complex eigenvalue <span class="math inline">\(e^{i \phi}\)</span> it also has an
complex eigenvalue <span class="math inline">\(e^{-i \phi} =\cos \phi - i \sin \phi\)</span>. The product of these two conjugate
eigenvalues is 1. Thus, an orthogonal matrix of uneven dimension has at least one
real eigenvalue (+1 or -1).</p>
<p>The eigenvalues of a Hausholder matrix <span class="math inline">\(\bQ_{HH}(\bv)\)</span> are all real (recall that it is symmetric!).
In fact, in dimension <span class="math inline">\(d\)</span> its eigenvalues are -1 (one time) and 1 ( <span class="math inline">\(d-1\)</span> times).
Since a transposition matrix <span class="math inline">\(\bT\)</span> is a special Householder matrix they have the same eigenvalues.</p>
</div>
<div id="positive-definite-matrices" class="section level3" number="7.6.8">
<h3>
<span class="header-section-number">A.6.8</span> Positive definite matrices<a class="anchor" aria-label="anchor" href="#positive-definite-matrices"><i class="fas fa-link"></i></a>
</h3>
<p>If all eigenvalues of a square matrix <span class="math inline">\(\bA\)</span> are real and <span class="math inline">\(\lambda_i \geq 0\)</span> then <span class="math inline">\(\bA\)</span> is called <em>positive semi-definite</em>.
If all eigenvalues are strictly positive
<span class="math inline">\(\lambda_i &gt; 0\)</span> then <span class="math inline">\(\bA\)</span> is called <em>positive definite</em>.</p>
<p>Note that a matrix does not need to be symmetric to be positive
definite, e.g.
<span class="math inline">\(\begin{pmatrix} 2 &amp; 3 \\ 1 &amp; 4 \\ \end{pmatrix}\)</span>
has positive eigenvalues 5 and 1. It also has a complete
set of eigenvectors and is diagonisable.</p>
<p>A symmetric matrix <span class="math inline">\(\bA\)</span> is positive definite
if the quadratic form <span class="math inline">\(\bx^T \bA \bx &gt; 0\)</span> for any non-zero <span class="math inline">\(\bx\)</span>,
and it is positive semi-definite if <span class="math inline">\(\bx^T \bA \bx \geq 0\)</span>.
This holds also the other way around:
a symmetric positive definite matrix (with positive eigenvalues) has a
positive quadratic form, and a symmetric positive semi-definite matrix (with non-negative eigenvalues) a non-negative quadratic form.</p>
<p>A symmetric positive definite matrix always has a positive diagonal
(this can be seen by setting <span class="math inline">\(\bx\)</span> above to a unit vector with 1 at
a single position, and 0 at all other elements).
However, just requiring a positive diagonal is too weak to ensure positive definiteness of a symmetric matrix, for example <span class="math inline">\(\begin{pmatrix} 1 &amp;10 \\ 10 &amp; 1 \\ \end{pmatrix}\)</span> has a negative eigenvalue of -9.
On the other hand, a symmetric matrix is indeed positive definite if it is strictly
diagonally dominant, i.e. if all its diagonal elements are positive and are larger than the absolute value of any of the corresponding row or column elements.
However, diagonal dominance is too restrictive as criterion to
characterise all
symmetric positive definite matrices, since
there are many symmetric matrices that are positive definite but not diagonally dominant, such as
<span class="math inline">\(\begin{pmatrix} 1 &amp; 2 \\ 2 &amp; 5 \\ \end{pmatrix}\)</span>.</p>
<p>Finally, the sum of a symmetric positive semi-definite matrix <span class="math inline">\(\bA\)</span>
and a symmetric positive definite matrix <span class="math inline">\(\bB\)</span> is itself symmetric positive definite because the corresponding
quadratic form <span class="math inline">\(\bx^T ( \bA +\bB) \bx = \bx^T \bA \bx + \bx^T \bB \bx &gt; 0\)</span> is positive. Similarly, the sum
of two symmetric positive (semi)-definite matrices is itself symmetric positive (semi)-definite.</p>
</div>
</div>
<div id="matrix-decompositions" class="section level2" number="7.7">
<h2>
<span class="header-section-number">A.7</span> Matrix decompositions<a class="anchor" aria-label="anchor" href="#matrix-decompositions"><i class="fas fa-link"></i></a>
</h2>
<div id="diagonalisation-and-eigenvalue-decomposition" class="section level3" number="7.7.1">
<h3>
<span class="header-section-number">A.7.1</span> Diagonalisation and eigenvalue decomposition<a class="anchor" aria-label="anchor" href="#diagonalisation-and-eigenvalue-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(\bA\)</span> is a square non-defective matrix then <span class="math inline">\(\bU\)</span> is invertible and
we can rewrite the eigenvalue equation to
<span class="math display">\[\bA  = \bU \bLambda \bU^{-1} \,.\]</span>
This is called the eigendecomposition, or spectral decomposition, of <span class="math inline">\(\bA\)</span> and equivalently
<span class="math display">\[\bLambda  = \bU^{-1} \bA \bU\]</span>
is the diagonalisation of <span class="math inline">\(\bA\)</span>.
Thus any matrix <span class="math inline">\(\bA\)</span> that is not defective is diagonalisable using eigenvalue decomposition.</p>
</div>
<div id="orthogonal-eigenvalue-decomposition" class="section level3" number="7.7.2">
<h3>
<span class="header-section-number">A.7.2</span> Orthogonal eigenvalue decomposition<a class="anchor" aria-label="anchor" href="#orthogonal-eigenvalue-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>For symmetric <span class="math inline">\(\bA\)</span> this becomes
<span class="math display">\[\bA = \bU \bLambda \bU^T\]</span>
with real eigenvalues and orthogonal matrix <span class="math inline">\(\bU\)</span>
and
<span class="math display">\[\bLambda = \bU^T \bA \bU  \,.\]</span>
This special case is known as the orthogonal diagonalisation
of <span class="math inline">\(\bA\)</span>.</p>
<p>The orthogonal decomposition for symmetric <span class="math inline">\(\bA\)</span> is
unique apart from the signs
of the eigenvectors.
In order to make it fully unique one needs to impose further restrictions (e.g. require a positive diagonal
of <span class="math inline">\(\bU\)</span>). Note that this can be particularly important in computer application where the sign
can vary depending on the specific implementation of the underlying numerical algorithms.</p>
</div>
<div id="singular-value-decomposition" class="section level3" number="7.7.3">
<h3>
<span class="header-section-number">A.7.3</span> Singular value decomposition<a class="anchor" aria-label="anchor" href="#singular-value-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>singular value decomposition</strong> (SVD) is
a generalisation of the orthogonal eigenvalue decomposition
for symmetric matrices.</p>
<p>Any (!) rectangular matrix <span class="math inline">\(\bA\)</span> of size <span class="math inline">\(n\times d\)</span> can be factored
into the product
<span class="math display">\[\bA = \bU \bD \bV^T\]</span>
where <span class="math inline">\(\bU\)</span> is a <span class="math inline">\(n \times n\)</span> orthogonal matrix, <span class="math inline">\(\bV\)</span> is a second <span class="math inline">\(d \times d\)</span> orthogonal matrix and <span class="math inline">\(\bD\)</span> is a diagonal but rectangular matrix
of size <span class="math inline">\(n\times d\)</span> with <span class="math inline">\(m=min(n,d)\)</span> real diagonal elements <span class="math inline">\(d_1, \ldots d_m\)</span>. The <span class="math inline">\(d_i\)</span> are called singular values, and appear
along the diagonal in <span class="math inline">\(\bD\)</span> by order of magnitude.</p>
<p>The SVD is unique apart from the
signs of the columns vectors in <span class="math inline">\(\bU\)</span>, <span class="math inline">\(\bV\)</span> and <span class="math inline">\(\bD\)</span> (you can freely specify the column signs of any two of the
three matrices). By convention the
signs are chosen such that the singular values in <span class="math inline">\(\bD\)</span> are all non-negative, which leaves ambiguity
in columns signs of <span class="math inline">\(\bU\)</span> and <span class="math inline">\(\bV\)</span>. Alternatively, one may
fix the columns signs of <span class="math inline">\(\bU\)</span> and <span class="math inline">\(\bV\)</span>, e.g. by requiring a positive diagonal, which then determines the sign of the singular values (thus allowing for negative singular values as well).</p>
<p>If <span class="math inline">\(\bA\)</span> is symmetric then the SVD and the orthogonal eigenvalue decomposition coincide (apart from different sign conventions for singular values, eigenvalues and eigenvectors).</p>
<p>Since <span class="math inline">\(\bA^T \bA = \bV \bD^T \bD \bV^T\)</span> and <span class="math inline">\(\bA \bA^T = \bU \bD \bD^T \bU^T\)</span> the squared singular values correspond to the eigenvalues of <span class="math inline">\(\bA^T \bA\)</span> and <span class="math inline">\(\bA \bA^T\)</span>.
It also follows that <span class="math inline">\(\bA^T \bA\)</span> and <span class="math inline">\(\bA \bA^T\)</span> are both positive
semi-definite symmetric matrices, and that <span class="math inline">\(\bV\)</span> and <span class="math inline">\(\bU\)</span> contain the respective sets of eigenvectors.</p>
</div>
<div id="polar-decomposition" class="section level3" number="7.7.4">
<h3>
<span class="header-section-number">A.7.4</span> Polar decomposition<a class="anchor" aria-label="anchor" href="#polar-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>Any square matrix <span class="math inline">\(\bA\)</span> can be factored into the product
<span class="math display">\[
\bA = \bQ \bB
\]</span>
of an orthogonal matrix <span class="math inline">\(\bQ\)</span> and a symmetric positive semi-definite matrix <span class="math inline">\(\bB\)</span>.</p>
<p>This follows from the SVD of <span class="math inline">\(\bA\)</span> given as
<span class="math display">\[
\begin{split}
\bA &amp;= \bU \bD \bV^T \\
    &amp;= ( \bU  \bV^T ) ( \bV \bD \bV^T ) \\
    &amp;= \bQ \bB \\
\end{split}
\]</span>
with non-negative <span class="math inline">\(\bD\)</span>. Note that this decomposition is unique as the sign ambiguities in the columns of <span class="math inline">\(\bU\)</span> and <span class="math inline">\(\bV\)</span> cancel out in <span class="math inline">\(\bQ\)</span> and <span class="math inline">\(\bB\)</span>.</p>
</div>
<div id="cholesky-decomposition" class="section level3" number="7.7.5">
<h3>
<span class="header-section-number">A.7.5</span> Cholesky decomposition<a class="anchor" aria-label="anchor" href="#cholesky-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>A symmetric positive definite matrix <span class="math inline">\(\bA\)</span> can be decomposed into a product
of a triangular matrix <span class="math inline">\(\bL\)</span> with its transpose
<span class="math display">\[
\bA = \bL \bL^T \,.
\]</span>
Here, <span class="math inline">\(\bL\)</span> is a lower triangular matrix with positive diagonal elements.</p>
<p>This decomposition is unique and is called <strong>Cholesky factorisation</strong>. It is
often used to check whether a symmetric matrix is positive definite as it is algorithmically
less demanding than eigenvalue decomposition.</p>
<p>Note that some implementations of the Cholesky decomposition (e.g. R) use
upper triangular matrices <span class="math inline">\(\bK\)</span> with positive diagonal so that
<span class="math inline">\(\bA = \bK^T \bK\)</span> and <span class="math inline">\(\bL = \bK^T\)</span>.</p>
</div>
</div>
<div id="matrix-summaries-based-on-eigenvalues-and-singular-values" class="section level2" number="7.8">
<h2>
<span class="header-section-number">A.8</span> Matrix summaries based on eigenvalues and singular values<a class="anchor" aria-label="anchor" href="#matrix-summaries-based-on-eigenvalues-and-singular-values"><i class="fas fa-link"></i></a>
</h2>
<div id="trace-and-determinant-computed-from-eigenvalues" class="section level3" number="7.8.1">
<h3>
<span class="header-section-number">A.8.1</span> Trace and determinant computed from eigenvalues<a class="anchor" aria-label="anchor" href="#trace-and-determinant-computed-from-eigenvalues"><i class="fas fa-link"></i></a>
</h3>
<p>The eigendecomposition <span class="math inline">\(\bA=\bU \bLambda \bU^{-1}\)</span>
allows to establish a link between trace and determinant and eigenvalues.</p>
<p>Specifically,
<span class="math display">\[
\begin{split}
\trace(\bA) &amp; = \trace(\bU \bLambda \bU^{-1}  ) =
\trace( \bLambda \bU^{-1} \bU  ) \\
 &amp;= \trace( \bLambda ) = \sum_{i=1}^d \lambda_i \\
\end{split}
\]</span>
thus the trace of a square matrix <span class="math inline">\(\bA\)</span> is equal to the <em>sum</em> of its eigenvalues. Likewise,
<span class="math display">\[
\begin{split}
\det(\bA) &amp; = \det(\bU) \det(\bLambda) \det(\bU^{-1}  ) \\
 &amp;=\det( \bLambda) = \prod_{i=1}^d \lambda_i \\
\end{split}
\]</span>
therefore the determinant of <span class="math inline">\(\bA\)</span> is the <em>product</em> of the eigenvalues.</p>
<p>The relationship between eigenvalues and the trace and the determinant
is demonstrated here for diagonisable non-defective matrices.
However, it does hold also in general for any matrix. This can be shown by using certain non-diagonal matrix decompositions (e.g. Jordan decomposition).</p>
<p>As a result, if any of the eigenvalues is equal to zero then <span class="math inline">\(\det(\bA) = 0\)</span> and as hence <span class="math inline">\(\bA\)</span> is singular and not invertible.</p>
<p>The trace and determinant of a real matrix are always real even though the individual eigenvalues may be complex.</p>
</div>
<div id="rank-and-condition-number" class="section level3" number="7.8.2">
<h3>
<span class="header-section-number">A.8.2</span> Rank and condition number<a class="anchor" aria-label="anchor" href="#rank-and-condition-number"><i class="fas fa-link"></i></a>
</h3>
<p>The rank is the dimension of the space spanned by both the column and row vectors. A rectangular matrix of dimension <span class="math inline">\(n \times d\)</span> will have
rank of at most <span class="math inline">\(m = \min(n, d)\)</span>, and if the maximum is indeed achieved then it has full rank.</p>
<p>The condition number describes how well- or ill-conditioned
a full rank matrix is. For example, for a square matrix a large condition number implies that the matrix is close to being singular
and thus ill-conditioned.
If the condition number is infinite then the matrix is not full rank.</p>
<p>The rank and condition of a matrix can both be determined from the <span class="math inline">\(m\)</span> singular values <span class="math inline">\(d_1, \ldots, d_m\)</span> of a matrix obtained by SVD:</p>
<ol style="list-style-type: lower-roman">
<li>The rank is number of non-zero singular values.</li>
<li>The condition number is the ratio of the largest singular value
divided by the smallest singular value (absolute values if signs are allowed).</li>
</ol>
<p>If a square matrix <span class="math inline">\(\bA\)</span> is singular then the condition number is infinite, and it will not have full rank.
On the other hand, a non-singular square matrix, such as
a positive definite matrix, has full rank.</p>
</div>
</div>
<div id="functions-of-symmetric-matrices" class="section level2" number="7.9">
<h2>
<span class="header-section-number">A.9</span> Functions of symmetric matrices<a class="anchor" aria-label="anchor" href="#functions-of-symmetric-matrices"><i class="fas fa-link"></i></a>
</h2>
<p>We focus on <em>symmetric</em> square matrices <span class="math inline">\(\bA=\bU \bLambda \bU^T\)</span> which are always diagonisable with real eigenvalues <span class="math inline">\(\bLambda\)</span> and orthogonal eigenvectors <span class="math inline">\(\bU\)</span>.</p>
<div id="definition-of-a-matrix-function" class="section level3" number="7.9.1">
<h3>
<span class="header-section-number">A.9.1</span> Definition of a matrix function<a class="anchor" aria-label="anchor" href="#definition-of-a-matrix-function"><i class="fas fa-link"></i></a>
</h3>
<p>Assume a real-valued function <span class="math inline">\(f(a)\)</span> of a real number <span class="math inline">\(a\)</span>. Then the corresponding
matrix function <span class="math inline">\(f(\bA)\)</span>
is defined as
<span class="math display">\[
f(\bA) =  \bU f(\bLambda) \bU^T =  \bU \begin{pmatrix}
    f(\lambda_{1}) &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; f(\lambda_{d})
\end{pmatrix} \bU^T
\]</span>
where the function <span class="math inline">\(f(a)\)</span> is applied to the eigenvalues of <span class="math inline">\(\bA\)</span>.
By construction <span class="math inline">\(f(\bA)\)</span> is real, symmetric and has
real eigenvalues <span class="math inline">\(f(\lambda_i)\)</span>.</p>
<p>Examples:</p>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example A.2  </strong></span>Matrix power: <span class="math inline">\(f(a) = a^p\)</span> (with <span class="math inline">\(p\)</span> a real number)</p>
</div>
<p>Special cases of matrix power include :</p>
<ul>
<li>Matrix inversion: <span class="math inline">\(f(a) = a^{-1}\)</span><br>
Note that if the matrix <span class="math inline">\(\bA\)</span> is singular, i.e. contains one or more eigenvalues <span class="math inline">\(\lambda_i=0\)</span>,
then <span class="math inline">\(\bA^{-1}\)</span> is not defined and therefore <span class="math inline">\(\bA\)</span> is not invertible.</li>
</ul>
<p>However, a so-called <em>pseudoinverse</em> can still be computed, by inverting the non-zero eigenvalues, and
keeping the zero eigenvalues as zero.</p>
<ul>
<li>Matrix square root: <span class="math inline">\(f(a) = a^{1/2}\)</span><br>
Since there are multiple solutions to the square root there are also multiple
matrix square roots. The principal matrix square root is obtained by using
the positive square roots of all the eigenvalues. Thus the <strong>principal matrix square root</strong>
of a positive semi-definite matrix is also positive semi-definite and it is unique.</li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example A.3  </strong></span>Matrix exponential: <span class="math inline">\(f(a) = \exp(a)\)</span><br>
Note that because <span class="math inline">\(\exp(a) \geq 0\)</span> for all real <span class="math inline">\(a\)</span> the matrix <span class="math inline">\(\exp(\bA)\)</span> is positive
semi-definite. Thus, the matrix exponential can be used to generate positive semi-definite
matrices.</p>
<p>If <span class="math inline">\(\bA\)</span> and <span class="math inline">\(\bB\)</span> commute, i.e. if <span class="math inline">\(\bA \bB = \bB \bA\)</span>, then
<span class="math inline">\(\exp(\bA+\bB) = \exp(\bA) \exp(\bB)\)</span>. However, this is not the case
otherwise!</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-4" class="example"><strong>Example A.4  </strong></span>Matrix logarithm: <span class="math inline">\(f(a) = \log(a)\)</span><br>
As the logarithm requires <span class="math inline">\(a &gt;0\)</span> the matrix <span class="math inline">\(\bA\)</span> needs to be positive definite
for <span class="math inline">\(\log(\bA)\)</span> to be defined.</p>
</div>
</div>
<div id="identities-for-the-matrix-exponential-and-logarithm" class="section level3" number="7.9.2">
<h3>
<span class="header-section-number">A.9.2</span> Identities for the matrix exponential and logarithm<a class="anchor" aria-label="anchor" href="#identities-for-the-matrix-exponential-and-logarithm"><i class="fas fa-link"></i></a>
</h3>
<p>The above give rise to useful identities:</p>
<ol style="list-style-type: decimal">
<li><p>For any symmetric matrix <span class="math inline">\(\bA\)</span> we have
<span class="math display">\[
\det(\exp(\bA)) = \exp(\trace(\bA))
\]</span>
because
<span class="math inline">\(\prod_i \exp(\lambda_i) = \exp( \sum_i \lambda_i)\)</span>
where <span class="math inline">\(\lambda_i\)</span> are the eigenvalues of <span class="math inline">\(\bA\)</span>.</p></li>
<li><p>If we take the logarithm on both sides and replace <span class="math inline">\(\exp(\bA)=\bB\)</span> we get another
identity for a symmetric positive definite matrix <span class="math inline">\(\bB\)</span>:
<span class="math display">\[
\log \det(\bB) = \trace(\log(\bB))
\]</span>
because
<span class="math inline">\(\log( \prod_i \lambda_i) = \sum_i \log(\lambda_i)\)</span>
where <span class="math inline">\(\lambda_i\)</span> are the eigenvalues of <span class="math inline">\(\bB\)</span>.</p></li>
</ol>
</div>
</div>
<div id="matrix-calculus" class="section level2" number="7.10">
<h2>
<span class="header-section-number">A.10</span> Matrix calculus<a class="anchor" aria-label="anchor" href="#matrix-calculus"><i class="fas fa-link"></i></a>
</h2>
<div id="first-order-vector-derivatives" class="section level3" number="7.10.1">
<h3>
<span class="header-section-number">A.10.1</span> First order vector derivatives<a class="anchor" aria-label="anchor" href="#first-order-vector-derivatives"><i class="fas fa-link"></i></a>
</h3>
<div id="gradient" class="section level4" number="7.10.1.1">
<h4>
<span class="header-section-number">A.10.1.1</span> Gradient<a class="anchor" aria-label="anchor" href="#gradient"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>nabla operator</strong> (also known as <strong>del operator</strong>) is the <em>row</em> vector
<span class="math display">\[
\nabla =  (\frac{\partial}{\partial x_1}, \ldots, 
\frac{\partial}{\partial x_d}) = \frac{\partial}{\partial \bx}
\]</span>
containing
the first order partial derivative operators.</p>
<p>The <strong>gradient</strong> of a scalar-valued function
<span class="math inline">\(f(\bx)\)</span> with vector argument <span class="math inline">\(\bx = (x_1, \ldots, x_d)^T\)</span>
is also a <em>row</em> vector (with <span class="math inline">\(d\)</span> columns) and
can be expressed using the nabla operator
<span class="math display">\[
\begin{split}
\nabla f(\bx) &amp;= \left( \frac{\partial f(\bx)}{\partial x_1}, \ldots, 
\frac{\partial f(\bx)}{\partial x_d} \right) \\
&amp; = 
 \frac{\partial f(\bx)}{\partial \bx} = \text{grad} f(\bx) \, .\\
\end{split}
\]</span>
Note the various notations for the gradient.</p>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example A.5  </strong></span><span class="math inline">\(f(\bx)=\ba^T \bx + b\)</span>. Then <span class="math inline">\(\nabla f(\bx) = \frac{\partial f(\bx)}{\partial \bx} = \ba^T\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example A.6  </strong></span><span class="math inline">\(f(\bx)=\bx^T \bx\)</span>. Then <span class="math inline">\(\nabla f(\bx) = \frac{\partial f(\bx)}{\partial \bx} = 2 \bx^T\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example A.7  </strong></span><span class="math inline">\(f(\bx)=\bx^T \bA \bx\)</span>. Then <span class="math inline">\(\nabla f(\bx) = \frac{\partial f(\bx)}{\partial \bx} = \bx^T (\bA + \bA^T)\)</span>.</p>
</div>
</div>
<div id="jacobian-matrix" class="section level4" number="7.10.1.2">
<h4>
<span class="header-section-number">A.10.1.2</span> Jacobian matrix<a class="anchor" aria-label="anchor" href="#jacobian-matrix"><i class="fas fa-link"></i></a>
</h4>
<p>For a vector-valued function
<span class="math display">\[
\boldf(\bx) = ( f_1(\bx), \ldots, f_m(\bx) )^T \,.
\]</span>
the computation of the gradient of each component yields
the <strong>Jacobian matrix</strong> (with <span class="math inline">\(m\)</span> rows and <span class="math inline">\(d\)</span> columns)
<span class="math display">\[
\begin{split}
 D \boldf(\bx) &amp;= 
\left( {\begin{array}{c}
 \nabla f_1(\bx)   \\
 \vdots   \\
 \nabla f_m(\bx)   \\
 \end{array} } \right) \\
&amp; = \left(\frac{\partial f_i(\bx)}{\partial x_j}\right) \\
&amp;= \frac{\partial \boldf(\bx)}{\partial \bx} \\
\end{split}
\]</span>
Again, note the various notations for the Jacobian matrix!</p>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example A.8  </strong></span><span class="math inline">\(\boldf(\bx)=\bA \bx + \bb\)</span>. Then <span class="math inline">\(D \boldf(\bx) = \frac{\partial \boldf(\bx)}{\partial \bx} = \bA\)</span>.</p>
</div>
<p>If <span class="math inline">\(m=d\)</span> then the Jacobian matrix is a square matrix and this allows to compute the
<strong>Jacobian determinant</strong> <span class="math display">\[\det  D \boldf(\bx) = \det\left(\frac{\partial \boldf(\bx)}{\partial \bx}\right)\]</span></p>
<p>If <span class="math inline">\(\by = \boldf(\bx)\)</span> is an invertible function with <span class="math inline">\(\bx = \boldf^{-1}(\by)\)</span>
then the Jacobian matrix is invertible and the inverted matrix is in fact the
Jacobian of the inverse function!</p>
<p>This allows to compute the Jacobian determinant of the backtransformation as
as the inverse of the Jacobian determinant the original function:
<span class="math display">\[\det  D \boldf^{-1}(\by) = ( \det  D \boldf(\bx) )^{-1}\]</span>
or in alternative notation
<span class="math display">\[\det  D \bx(\by) = \frac{1}{ \det  D \by(\bx) }\]</span>.</p>
</div>
</div>
<div id="second-order-vector-derivatives" class="section level3" number="7.10.2">
<h3>
<span class="header-section-number">A.10.2</span> Second order vector derivatives<a class="anchor" aria-label="anchor" href="#second-order-vector-derivatives"><i class="fas fa-link"></i></a>
</h3>
<p>The matrix of all second order partial derivates of scalar-valued
function with vector-valued argument is called the <strong>Hessian matrix</strong>
and is computed by double application of the nabla operator:
<span class="math display">\[
\begin{split}
\nabla^T \nabla f(\bx) &amp;=
\begin{pmatrix}
  \frac{\partial^2 f(\bx)}{\partial x_1^2}
     &amp; \frac{\partial^2 f(\bx)}{\partial x_1 \partial x_2} 
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\bx)}{\partial x_1 \partial x_d} \\
  \frac{\partial^2 f(\bx)}{\partial x_2 \partial x_1} 
     &amp; \frac{\partial^2 f(\bx)}{\partial x_2^2}
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\bx)}{\partial x_2 \partial x_d} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  \frac{\partial^2 f(\bx)}{\partial x_d \partial x_1} 
     &amp; \frac{\partial^2 f(\bx)}{\partial x_d \partial x_2}  
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\bx)}{\partial x_d^2}
 \end{pmatrix} \\
&amp; = \left(\frac{\partial f(\bx)}{\partial x_i \partial x_j}\right)  
  = {\left(\frac{\partial}{\partial \bx}\right)}^T \frac{\partial f(\bx)}{\partial \bx}
\,.\\
\end{split}
\]</span>
By construction it is square and symmetric.</p>
</div>
<div id="first-order-matrix-derivatives" class="section level3" number="7.10.3">
<h3>
<span class="header-section-number">A.10.3</span> First order matrix derivatives<a class="anchor" aria-label="anchor" href="#first-order-matrix-derivatives"><i class="fas fa-link"></i></a>
</h3>
<p>The derivative of a scalar-valued function <span class="math inline">\(f(\bX)\)</span> with regard to a matrix argument <span class="math inline">\(\bX\)</span>
can also be defined and results in a matrix
with transposed dimensions compared to <span class="math inline">\(\bX\)</span>.</p>
<p>Two important specific examples are:</p>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example A.9  </strong></span><span class="math inline">\(\frac{\partial \trace(\bA \bX)}{\partial \bX} = \bA\)</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example A.10  </strong></span><span class="math inline">\(\frac{\partial \log \det(\bX)}{\partial \bX} = \frac{\partial \trace(\log \bX)}{\partial \bX} = \bX^{-1}\)</span></p>
</div>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="nonlinear-and-nonparametric-models.html"><span class="header-section-number">6</span> Nonlinear and nonparametric models</a></div>
<div class="next"><a href="further-study.html"><span class="header-section-number">B</span> Further study</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#brief-refresher-on-matrices"><span class="header-section-number">A</span> Brief refresher on matrices</a></li>
<li>
<a class="nav-link" href="#matrix-basics"><span class="header-section-number">A.1</span> Matrix basics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#matrix-notation"><span class="header-section-number">A.1.1</span> Matrix notation</a></li>
<li><a class="nav-link" href="#random-matrix"><span class="header-section-number">A.1.2</span> Random matrix</a></li>
<li><a class="nav-link" href="#special-matrices"><span class="header-section-number">A.1.3</span> Special matrices</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#simple-matrix-operations"><span class="header-section-number">A.2</span> Simple matrix operations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#matrix-addition-and-multiplication"><span class="header-section-number">A.2.1</span> Matrix addition and multiplication</a></li>
<li><a class="nav-link" href="#matrix-transpose"><span class="header-section-number">A.2.2</span> Matrix transpose</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#matrix-summaries"><span class="header-section-number">A.3</span> Matrix summaries</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#matrix-trace"><span class="header-section-number">A.3.1</span> Matrix trace</a></li>
<li><a class="nav-link" href="#determinant-of-a-matrix"><span class="header-section-number">A.3.2</span> Determinant of a matrix</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#matrix-inverse"><span class="header-section-number">A.4</span> Matrix inverse</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#inversion-of-square-matrix"><span class="header-section-number">A.4.1</span> Inversion of square matrix</a></li>
<li><a class="nav-link" href="#inversion-of-structured-matrices"><span class="header-section-number">A.4.2</span> Inversion of structured matrices</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#orthogonal-matrices"><span class="header-section-number">A.5</span> Orthogonal matrices</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#properties-1"><span class="header-section-number">A.5.1</span> Properties</a></li>
<li><a class="nav-link" href="#generating-orthogonal-matrices"><span class="header-section-number">A.5.2</span> Generating orthogonal matrices</a></li>
<li><a class="nav-link" href="#permutation-matrix"><span class="header-section-number">A.5.3</span> Permutation matrix</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#eigenvalues-and-eigenvectors"><span class="header-section-number">A.6</span> Eigenvalues and eigenvectors</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#definition"><span class="header-section-number">A.6.1</span> Definition</a></li>
<li><a class="nav-link" href="#finding-eigenvalues-and-vectors"><span class="header-section-number">A.6.2</span> Finding eigenvalues and vectors</a></li>
<li><a class="nav-link" href="#eigenequation-in-matrix-notation"><span class="header-section-number">A.6.3</span> Eigenequation in matrix notation</a></li>
<li><a class="nav-link" href="#defective-matrix"><span class="header-section-number">A.6.4</span> Defective matrix</a></li>
<li><a class="nav-link" href="#eigenvalues-of-a-diagonal-or-triangular-matrix"><span class="header-section-number">A.6.5</span> Eigenvalues of a diagonal or triangular matrix</a></li>
<li><a class="nav-link" href="#eigenvalues-and-vectors-of-a-symmetric-matrix"><span class="header-section-number">A.6.6</span> Eigenvalues and vectors of a symmetric matrix</a></li>
<li><a class="nav-link" href="#eigenvalues-of-orthogonal-matrices"><span class="header-section-number">A.6.7</span> Eigenvalues of orthogonal matrices</a></li>
<li><a class="nav-link" href="#positive-definite-matrices"><span class="header-section-number">A.6.8</span> Positive definite matrices</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#matrix-decompositions"><span class="header-section-number">A.7</span> Matrix decompositions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#diagonalisation-and-eigenvalue-decomposition"><span class="header-section-number">A.7.1</span> Diagonalisation and eigenvalue decomposition</a></li>
<li><a class="nav-link" href="#orthogonal-eigenvalue-decomposition"><span class="header-section-number">A.7.2</span> Orthogonal eigenvalue decomposition</a></li>
<li><a class="nav-link" href="#singular-value-decomposition"><span class="header-section-number">A.7.3</span> Singular value decomposition</a></li>
<li><a class="nav-link" href="#polar-decomposition"><span class="header-section-number">A.7.4</span> Polar decomposition</a></li>
<li><a class="nav-link" href="#cholesky-decomposition"><span class="header-section-number">A.7.5</span> Cholesky decomposition</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#matrix-summaries-based-on-eigenvalues-and-singular-values"><span class="header-section-number">A.8</span> Matrix summaries based on eigenvalues and singular values</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#trace-and-determinant-computed-from-eigenvalues"><span class="header-section-number">A.8.1</span> Trace and determinant computed from eigenvalues</a></li>
<li><a class="nav-link" href="#rank-and-condition-number"><span class="header-section-number">A.8.2</span> Rank and condition number</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#functions-of-symmetric-matrices"><span class="header-section-number">A.9</span> Functions of symmetric matrices</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#definition-of-a-matrix-function"><span class="header-section-number">A.9.1</span> Definition of a matrix function</a></li>
<li><a class="nav-link" href="#identities-for-the-matrix-exponential-and-logarithm"><span class="header-section-number">A.9.2</span> Identities for the matrix exponential and logarithm</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#matrix-calculus"><span class="header-section-number">A.10</span> Matrix calculus</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#first-order-vector-derivatives"><span class="header-section-number">A.10.1</span> First order vector derivatives</a></li>
<li><a class="nav-link" href="#second-order-vector-derivatives"><span class="header-section-number">A.10.2</span> Second order vector derivatives</a></li>
<li><a class="nav-link" href="#first-order-matrix-derivatives"><span class="header-section-number">A.10.3</span> First order matrix derivatives</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Multivariate Statistics and Machine Learning</strong>" was written by Korbinian Strimmer. It was last built on 11 May 2021.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
