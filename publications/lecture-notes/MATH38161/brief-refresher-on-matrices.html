<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>A Brief refresher on matrices | Multivariate Statistics and Machine Learning</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.34 with bs4_book()">
<meta property="og:title" content="A Brief refresher on matrices | Multivariate Statistics and Machine Learning">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Brief refresher on matrices | Multivariate Statistics and Machine Learning">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.0/transition.js"></script><script src="libs/bs3compat-0.5.0/tabs.js"></script><script src="libs/bs3compat-0.5.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<meta name="description" content="In multivariate statistics we will frequently make use of matrix calculations and matrix notation. This helps to make multivariate equations simpler and enables a better understanding of the...">
<meta property="og:description" content="In multivariate statistics we will frequently make use of matrix calculations and matrix notation. This helps to make multivariate equations simpler and enables a better understanding of the...">
<meta name="twitter:description" content="In multivariate statistics we will frequently make use of matrix calculations and matrix notation. This helps to make multivariate equations simpler and enables a better understanding of the...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Multivariate Statistics and Machine Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="multivariate-random-variables.html"><span class="header-section-number">1</span> Multivariate random variables</a></li>
<li><a class="" href="transformations-and-dimension-reduction.html"><span class="header-section-number">2</span> Transformations and dimension reduction</a></li>
<li><a class="" href="unsupervised-learning-and-clustering.html"><span class="header-section-number">3</span> Unsupervised learning and clustering</a></li>
<li><a class="" href="supervised-learning-and-classification.html"><span class="header-section-number">4</span> Supervised learning and classification</a></li>
<li><a class="" href="multivariate-dependencies.html"><span class="header-section-number">5</span> Multivariate dependencies</a></li>
<li><a class="" href="nonlinear-and-nonparametric-models.html"><span class="header-section-number">6</span> Nonlinear and nonparametric models</a></li>
<li class="book-part">Appendix</li>
<li><a class="active" href="brief-refresher-on-matrices.html"><span class="header-section-number">A</span> Brief refresher on matrices</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="brief-refresher-on-matrices" class="section level1" number="7">
<h1>
<span class="header-section-number">A</span> Brief refresher on matrices<a class="anchor" aria-label="anchor" href="#brief-refresher-on-matrices"><i class="fas fa-link"></i></a>
</h1>
<p>In multivariate statistics we will frequently make use of matrix calculations and matrix notation.
This helps to make multivariate equations simpler and enables a better understanding of
the underlying concepts.</p>
<p>Throughout the module we mostly work with <strong>real matrices</strong>, i.e. we assume all matrix elements are
real numbers. However, one important matrix decomposition — the eigenvalue decomposition — can yield complex-valued matrices even when applied to real matrices. Thus occasionally we will need to deal also with complex numbers.</p>
<p>For further details on matrix theory please consult the lecture notes of related modules (e.g. linear algebra).</p>
<div id="matrix-basics" class="section level2" number="7.1">
<h2>
<span class="header-section-number">A.1</span> Matrix basics<a class="anchor" aria-label="anchor" href="#matrix-basics"><i class="fas fa-link"></i></a>
</h2>
<div id="matrix-notation" class="section level3" number="7.1.1">
<h3>
<span class="header-section-number">A.1.1</span> Matrix notation<a class="anchor" aria-label="anchor" href="#matrix-notation"><i class="fas fa-link"></i></a>
</h3>
<p>In matrix notation we distinguish between scalars, vectors, and matrices:</p>
<p><strong>Scalar</strong>: <span class="math inline">\(x\)</span>, <span class="math inline">\(X\)</span>, lower or upper case, plain type.</p>
<p><strong>Vector</strong>: <span class="math inline">\(\boldsymbol x\)</span>, lower case, bold type. In handwriting an arrow <span class="math inline">\(\vec{x}\)</span> indicates a vector.</p>
<p>In component notation we write <span class="math inline">\(\boldsymbol x= \begin{pmatrix} x_1 \\ \vdots\\ x_d\end{pmatrix}\)</span>. By default, a vector is a
column vector, i.e. the elements are arranged in a column and index of the components <span class="math inline">\(x_i\)</span> refers to the row.</p>
<p>The <strong>transpose</strong> of a vector (indicated by the superscript <span class="math inline">\(T\)</span>) turns it into a row vector. To save space we can write the column vector <span class="math inline">\(\boldsymbol x\)</span>
as <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_d)^T\)</span> so that <span class="math inline">\(\boldsymbol x^T\)</span> is a row vector.</p>
<p><strong>Matrix</strong>: <span class="math inline">\(\boldsymbol X\)</span>, upper case, bold type. In handwriting an underscore
<span class="math inline">\(\underline{X}\)</span> indicates a matrix.</p>
<p>In component notation we write <span class="math inline">\(\boldsymbol X= (x_{ij})\)</span>. By convention, the first index (here <span class="math inline">\(i\)</span>)
of the scalar elements <span class="math inline">\(x_{ij}\)</span> denotes the row and the second index (here <span class="math inline">\(j\)</span>) the column of the matrix.
For <span class="math inline">\(n\)</span> the number of rows and <span class="math inline">\(d\)</span> the number of columns
we can view the matrix
<span class="math inline">\(\boldsymbol X= (\boldsymbol x_1, \ldots, \boldsymbol x_d)\)</span>
either as being composed of <span class="math inline">\(d\)</span> column vectors
<span class="math inline">\(\boldsymbol x_j = \begin{pmatrix} x_{1j} \\ \vdots \\ x_{nj}\\ \end{pmatrix}\)</span>
or <span class="math inline">\(\boldsymbol X= \begin{pmatrix} \boldsymbol z_1^T \\ \vdots \\ \boldsymbol z_n^T\end{pmatrix}\)</span>
being
composed of <span class="math inline">\(n\)</span> row vectors <span class="math inline">\(\boldsymbol z_i^T = (x_{i1}, \ldots, x_{id})\)</span>.</p>
<p>A (column) vector of dimension <span class="math inline">\(d\)</span> is a matrix of size <span class="math inline">\(d\times 1\)</span>. A row vector of dimension <span class="math inline">\(d\)</span> is a matrix of size <span class="math inline">\(1\times d\)</span>.
A scalar is of dimension <span class="math inline">\(1\)</span> and is a matrix of size <span class="math inline">\(1 \times 1\)</span>.</p>
</div>
<div id="random-matrix" class="section level3" number="7.1.2">
<h3>
<span class="header-section-number">A.1.2</span> Random matrix<a class="anchor" aria-label="anchor" href="#random-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>A <strong>random matrix</strong> (vector) is a matrix (vector) whose elements are random variables.</p>
<p>Note that the standard
notation used in univariate statistics to distinguish
random variables and their realisations (i.e. upper versus lower case) does not
work in multivariate statistics. Therefore, you need to determine
from the context whether a quantity represents a
random variable, or whether it is a constant.</p>
</div>
<div id="special-matrices" class="section level3" number="7.1.3">
<h3>
<span class="header-section-number">A.1.3</span> Special matrices<a class="anchor" aria-label="anchor" href="#special-matrices"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math inline">\(\boldsymbol I_d\)</span> is the identity matrix. It is a square matrix of size
<span class="math inline">\(d \times d\)</span> with the diagonal
filled with 1 and off-diagonals filled with 0.
<span class="math display">\[\boldsymbol I_d =
\begin{pmatrix}
    1 &amp; 0 &amp; 0 &amp; \dots &amp; 0\\
    0 &amp; 1 &amp; 0 &amp; \dots &amp; 0\\
    0 &amp; 0 &amp; 1 &amp;   &amp; 0\\
    \vdots &amp; \vdots &amp; &amp; \ddots &amp;  \\
    0 &amp; 0 &amp; 0 &amp;  &amp; 1 \\
\end{pmatrix}\]</span></p>
<p><span class="math inline">\(\boldsymbol 1\)</span> is a matrix that contains only ones. Most often
it is used in the form of a column vector with <span class="math inline">\(d\)</span> rows:
<span class="math display">\[\boldsymbol 1_d =
\begin{pmatrix}
    1 \\
    1 \\
    1 \\
    \vdots   \\
    1  \\
\end{pmatrix}\]</span></p>
<p>Similarly, <span class="math inline">\(\boldsymbol 0\)</span> is a matrix that contains only zeros. Most often
it is used in the form of a column vector with <span class="math inline">\(d\)</span> rows:
<span class="math display">\[\boldsymbol 0_d =
\begin{pmatrix}
    0 \\
    0 \\
    0 \\
    \vdots   \\
    0  \\
\end{pmatrix}\]</span></p>
<p>A diagonal matrix is a matrix where all off-diagonal elements are zero.
By <span class="math inline">\(\text{Diag}(\boldsymbol A)\)</span> we access the diagonal elements of a matrix as vector and by
<span class="math inline">\(\text{Diag}(a_1, \ldots, a_d)\)</span> we specify a diagonal matrix by listing the diagonal elements.</p>
<p>A triangular matrix is a square matrix whose elements either below or above the diagonal are all zero (upper vs. lower triangular matrix).</p>
</div>
</div>
<div id="simple-matrix-operations" class="section level2" number="7.2">
<h2>
<span class="header-section-number">A.2</span> Simple matrix operations<a class="anchor" aria-label="anchor" href="#simple-matrix-operations"><i class="fas fa-link"></i></a>
</h2>
<div id="matrix-addition-and-multiplication" class="section level3" number="7.2.1">
<h3>
<span class="header-section-number">A.2.1</span> Matrix addition and multiplication<a class="anchor" aria-label="anchor" href="#matrix-addition-and-multiplication"><i class="fas fa-link"></i></a>
</h3>
<p>Matrices behave much like common numbers. For example, we can add matrices
<span class="math inline">\(\boldsymbol C= \boldsymbol A+ \boldsymbol B\)</span>
and multiply matrices <span class="math inline">\(\boldsymbol C= \boldsymbol A\boldsymbol B\)</span>.</p>
<p>For <strong>matrix addition</strong> <span class="math inline">\(\boldsymbol C= \boldsymbol A+ \boldsymbol B\)</span> we add the corresponding elements <span class="math inline">\(c_{ij} = a_{ij} + b_{ij}\)</span>. For matrix addition <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> must have the same dimensions, i.e.
the same number of rows and columns.</p>
<p>The <strong>dot product</strong>, or <strong>scalar product</strong>, of two vectors <span class="math inline">\(\boldsymbol a\)</span> and <span class="math inline">\(\boldsymbol b\)</span> is a scalar given by <span class="math inline">\(\boldsymbol a\cdot \boldsymbol b= \langle \boldsymbol a, \boldsymbol b\rangle = \boldsymbol a^T \boldsymbol b= \boldsymbol b^T \boldsymbol a= \sum_{i=1}^d a_{i} b_{i}\)</span>.</p>
<p><strong>Matrix multiplication</strong> <span class="math inline">\(\boldsymbol C= \boldsymbol A\boldsymbol B\)</span> is obtained by setting <span class="math inline">\(c_{ij} = \sum_{k=1}^m a_{ik} b_{kj}\)</span> where <span class="math inline">\(m\)</span> is the
number of columns of <span class="math inline">\(\boldsymbol A\)</span> and the number of rows in <span class="math inline">\(\boldsymbol B\)</span>. Thus, <span class="math inline">\(\boldsymbol C\)</span> contains all possible
dot products of the row vectors in <span class="math inline">\(\boldsymbol A\)</span> with the column vectors in <span class="math inline">\(\boldsymbol B\)</span>.
For matrix multiplication the number of columns in <span class="math inline">\(\boldsymbol A\)</span> must match the number of rows in <span class="math inline">\(\boldsymbol B\)</span>.
Note that matrix multiplication in general (for <span class="math inline">\(m &gt; 1\)</span>) does not commute, i.e. <span class="math inline">\(\boldsymbol A\boldsymbol B\neq \boldsymbol B\boldsymbol A\)</span>.</p>
</div>
<div id="matrix-transpose" class="section level3" number="7.2.2">
<h3>
<span class="header-section-number">A.2.2</span> Matrix transpose<a class="anchor" aria-label="anchor" href="#matrix-transpose"><i class="fas fa-link"></i></a>
</h3>
<p>The matrix transpose <span class="math inline">\(\boldsymbol A^T\)</span> indicate by the superscript <span class="math inline">\(T\)</span> interchanges rows and columns of a matrix. The transpose
is a linear operator <span class="math inline">\((\boldsymbol A+ \boldsymbol B)^T = \boldsymbol A^T + \boldsymbol B^T\)</span> and
applied to a matrix
product it reverses the ordering, i.e. <span class="math inline">\((\boldsymbol A\boldsymbol B)^T =\boldsymbol B^T \boldsymbol A^T\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol A= \boldsymbol A^T\)</span> then <span class="math inline">\(\boldsymbol A\)</span> is symmetric (and square).</p>
<p>By construction given a rectangular <span class="math inline">\(\boldsymbol A\)</span> the matrices
<span class="math inline">\(\boldsymbol A^T \boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T\)</span> are symmetric with non-negative diagonal.</p>
</div>
</div>
<div id="matrix-summaries" class="section level2" number="7.3">
<h2>
<span class="header-section-number">A.3</span> Matrix summaries<a class="anchor" aria-label="anchor" href="#matrix-summaries"><i class="fas fa-link"></i></a>
</h2>
<div id="row-column-and-grand-sum" class="section level3" number="7.3.1">
<h3>
<span class="header-section-number">A.3.1</span> Row, column and grand sum<a class="anchor" aria-label="anchor" href="#row-column-and-grand-sum"><i class="fas fa-link"></i></a>
</h3>
<p>Assume a matrix <span class="math inline">\(\boldsymbol A\)</span> of size <span class="math inline">\(n \times m\)</span>.</p>
<p>The sum over the <span class="math inline">\(m\)</span> entries of row <span class="math inline">\(i\)</span> is <span class="math inline">\(\sum_{j=1}^m a_{ij}\)</span>.
In matrix notation the <span class="math inline">\(n\)</span> row sums are given by
<span class="math inline">\(\boldsymbol A\, \boldsymbol 1_m\)</span>.</p>
<p>The sum over the <span class="math inline">\(n\)</span> entries of column <span class="math inline">\(j\)</span> is <span class="math inline">\(\sum_{i=1}^n a_{ij}\)</span>.
In matrix notation the <span class="math inline">\(m\)</span> column sums are <span class="math inline">\(\boldsymbol A^T \boldsymbol 1_n\)</span>.</p>
<p>The grand sum of all matrix entries of <span class="math inline">\(\boldsymbol A\)</span> is obtained by
<span class="math display">\[
\sum_{i=1}^n \sum_{j=1}^m a_{ij} = \boldsymbol 1_n^T \, \boldsymbol A\, \boldsymbol 1_m
\]</span></p>
</div>
<div id="matrix-trace" class="section level3" number="7.3.2">
<h3>
<span class="header-section-number">A.3.2</span> Matrix trace<a class="anchor" aria-label="anchor" href="#matrix-trace"><i class="fas fa-link"></i></a>
</h3>
<p>The trace of the matrix is the sum of the diagonal elements <span class="math inline">\(\text{Tr}(\boldsymbol A) = \sum a_{ii}\)</span>.</p>
<p>The trace is invariant against transposition, i.e.
<span class="math display">\[
\text{Tr}(\boldsymbol A) = \text{Tr}(\boldsymbol A^T )
\]</span></p>
<p>A useful identity for the matrix trace of the product of two matrices is
<span class="math display">\[
\text{Tr}(\boldsymbol A\boldsymbol B) = \text{Tr}( \boldsymbol B\boldsymbol A)  
\]</span></p>
<p>Intriguingly, the trace of a matrix equals the sum of the eigenvalues of the matrix (see further below).</p>
</div>
<div id="row-column-and-grand-sum-of-squares" class="section level3" number="7.3.3">
<h3>
<span class="header-section-number">A.3.3</span> Row, column and grand sum of squares<a class="anchor" aria-label="anchor" href="#row-column-and-grand-sum-of-squares"><i class="fas fa-link"></i></a>
</h3>
<p>The sum over the <span class="math inline">\(m\)</span> squared entries of row <span class="math inline">\(i\)</span> is <span class="math inline">\(\sum_{j=1}^m a_{ij}^2\)</span>.
In matrix notation the <span class="math inline">\(n\)</span> row sums of squares are given by
<span class="math inline">\(\text{Diag}( \boldsymbol A\boldsymbol A^T)\)</span>.</p>
<p>The sum over the <span class="math inline">\(n\)</span> squared entries of column <span class="math inline">\(j\)</span> is <span class="math inline">\(\sum_{i=1}^n a_{ij}^2\)</span>.
In matrix notation the <span class="math inline">\(m\)</span> column sums of squares are <span class="math inline">\(\text{Diag}( \boldsymbol A^T \boldsymbol A)\)</span>.</p>
<p>The grand sum of all squared elements of <span class="math inline">\(\boldsymbol A\)</span> is obtained by
<span class="math display">\[
\sum_{i=1}^n \sum_{j=1}^m a_{ij}^2 = \text{Tr}(\boldsymbol A^T \boldsymbol A) = \text{Tr}(\boldsymbol A\boldsymbol A^T)
\]</span>
This is also known as the squared Frobenius norm of <span class="math inline">\(\boldsymbol A\)</span> (see below).</p>
</div>
<div id="sum-of-squared-diagonal-entries" class="section level3" number="7.3.4">
<h3>
<span class="header-section-number">A.3.4</span> Sum of squared diagonal entries<a class="anchor" aria-label="anchor" href="#sum-of-squared-diagonal-entries"><i class="fas fa-link"></i></a>
</h3>
<p>The sum of the squared entries on the diagonal is in matrix notation
<span class="math display">\[
\text{Diag}(\boldsymbol A)^T \text{Diag}(\boldsymbol A) = \sum_{i=1}^{\min(n,m)} a_{ii}^2
\]</span></p>
</div>
<div id="frobenius-inner-product" class="section level3" number="7.3.5">
<h3>
<span class="header-section-number">A.3.5</span> Frobenius inner product<a class="anchor" aria-label="anchor" href="#frobenius-inner-product"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Frobenius inner product</strong> between two rectangular matrices of the same dimension is the scalar
<span class="math display">\[
\begin{split}
\langle \boldsymbol A, \boldsymbol B\rangle &amp;=  \text{Tr}(\boldsymbol A\boldsymbol B^T) = \text{Tr}(\boldsymbol B\boldsymbol A^T)\\
&amp;=  \text{Tr}(\boldsymbol A^T \boldsymbol B) = \text{Tr}(\boldsymbol B^T \boldsymbol A)\\
&amp;= \sum_{i,j} a_{ij} b_{ij} \,.
\end{split}
\]</span>
This generalises the dot product between two vectors.
Note that the dot product can therefore also be written as the trace of a matrix
<span class="math display">\[
\langle \boldsymbol a, \boldsymbol b\rangle = \text{Tr}( \boldsymbol a\boldsymbol b^T ) = \text{Tr}( \boldsymbol b\boldsymbol a^T) \,.
\]</span></p>
</div>
<div id="euclidean-norm" class="section level3" number="7.3.6">
<h3>
<span class="header-section-number">A.3.6</span> Euclidean norm<a class="anchor" aria-label="anchor" href="#euclidean-norm"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>squared Euclidean norm</strong> or the <strong>squared length</strong> of the vector <span class="math inline">\(\boldsymbol a\)</span>
is the
dot product <span class="math inline">\(||\boldsymbol a||^2_2 = \boldsymbol a\cdot \boldsymbol a= \langle \boldsymbol a, \boldsymbol a\rangle = \boldsymbol a^T \boldsymbol a= \boldsymbol a\boldsymbol a^T = \sum_{i=1}^d a_i^2\)</span>.</p>
<p>The <strong>squared Frobenius norm</strong> is a generalisation of the squared Euclidean vector norm to a rectangular matrix and is
the sum of the squares of all its elements.
Using the trace it can be written as
<span class="math display">\[
\begin{split}
||\boldsymbol A||_F^2 &amp;= \langle \boldsymbol A, \boldsymbol A\rangle \\
&amp;= \text{Tr}(\boldsymbol A^T \boldsymbol A) = \text{Tr}(\boldsymbol A\boldsymbol A^T) \\
&amp;= \sum_{i,j} a_{ij}^2 \,.
\end{split}
\]</span></p>
<p>A useful identity for the <strong>squared Frobenius norm of the difference of two matrices</strong>
is
<span class="math display">\[
\begin{split}
||\boldsymbol A- \boldsymbol B||_F^2 &amp;= ||\boldsymbol A||_F^2 + ||\boldsymbol B||_F^2  - 2 \langle \boldsymbol A, \boldsymbol B\rangle \\
&amp;= \text{Tr}(\boldsymbol A^T \boldsymbol A) +\text{Tr}(\boldsymbol B^T \boldsymbol B) - 2 \text{Tr}(\boldsymbol A^T \boldsymbol B) \\
&amp;= \sum_{i,j} (a_{ij}-b_{ij})^2 \,.
\end{split}
\]</span></p>
<p>The Frobenius norm of a matrix <span class="math inline">\(||\boldsymbol A||_F\)</span> is not to be confused with the induced <span class="math inline">\(2\)</span>-norm of a matrix <span class="math inline">\(||\boldsymbol A||_2\)</span>. The latter equals the maximum
absolute eigenvalue of the matrix, with <span class="math inline">\(||\boldsymbol A||_2 \leq ||\boldsymbol A||_F\)</span>.</p>
</div>
<div id="determinant-of-a-matrix" class="section level3" number="7.3.7">
<h3>
<span class="header-section-number">A.3.7</span> Determinant of a matrix<a class="anchor" aria-label="anchor" href="#determinant-of-a-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is a square matrix the determinant <span class="math inline">\(\det(\boldsymbol A)\)</span> is a scalar measuring the volume spanned by the column vectors in <span class="math inline">\(\boldsymbol A\)</span> with the sign determined by the orientation of the vectors.</p>
<p>If <span class="math inline">\(\det(\boldsymbol A) \neq 0\)</span> the matrix <span class="math inline">\(\boldsymbol A\)</span> is non-singular or non-degenerate. Conversely, if
<span class="math inline">\(\det(\boldsymbol A) =0\)</span> the matrix <span class="math inline">\(\boldsymbol A\)</span> is singular or degenerate.</p>
<p>Intriguingly, the determinant of <span class="math inline">\(\boldsymbol A\)</span> is the product of the eigenvalues of <span class="math inline">\(\boldsymbol A\)</span> (see further below).</p>
<p>One way to compute the determinant of a matrix <span class="math inline">\(\boldsymbol A\)</span> is the Laplace cofactor
expansion approach that proceeds recursively based on the determinants of the submatrices <span class="math inline">\(\boldsymbol A_{-i,-j}\)</span> obtained by deleting row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> from <span class="math inline">\(\boldsymbol A\)</span>. Specifically, at each
level we compute the</p>
<ol style="list-style-type: decimal">
<li>cofactor expansion either
<ol style="list-style-type: lower-alpha">
<li>along the <span class="math inline">\(i\)</span>-th row — pick any row <span class="math inline">\(i\)</span>:
<span class="math display">\[\det(\boldsymbol A) = \sum_{j=1}^d a_{ij} (-1)^{i+j} \det(\boldsymbol A_{-i,-j})  \text{ , or}\]</span>
</li>
<li>along the <span class="math inline">\(j\)</span>-th column — pick any <span class="math inline">\(j\)</span>:
<span class="math display">\[\det(\boldsymbol A) = \sum_{i=1}^d a_{ij} (-1)^{i+j} \det(\boldsymbol A_{-i,-j})\]</span>.</li>
</ol>
</li>
<li>Then repeat until the submatrix is a scalar <span class="math inline">\(a\)</span> and <span class="math inline">\(\det(a)=a \,.\)</span>
</li>
</ol>
<p>The recursive nature of this algorithm leads to a complexity of order <span class="math inline">\(O(d!)\)</span> so it is not practical except for very small <span class="math inline">\(d\)</span>.
Therefore, in practice other more efficient algorithms for computing determinants are used but these still have algorithmic complexity in the order of <span class="math inline">\(O(d^3)\)</span> so for large dimensions obtaining determinants is
very expensive.</p>
<p>However, some specially structured matrices do allow for very fast calculation.</p>
<p>The determinant of a <strong>triangular matrix</strong> (and thus also of a <strong>diagonal matrix</strong>)
<span class="math display">\[
\boldsymbol A= \begin{pmatrix}
a_{11} &amp; 0       &amp; \cdots &amp; 0\\
a_{21} &amp; a_{22}  &amp; \cdots &amp; 0\\
\vdots  &amp; \vdots &amp; \ddots &amp; 0 \\
a_{d1} &amp; a_{d2} &amp; \cdots &amp; a_{dd} \\
\end{pmatrix}
\]</span>
is the product of its diagonal elements, i.e. <span class="math inline">\(\det(\boldsymbol A) = \prod_{i=1}^d a_{ii}\)</span>.</p>
<p>For a two-dimensional matrix <span class="math inline">\(\boldsymbol A= \begin{pmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \\\end{pmatrix}\)</span>
the determinant is <span class="math inline">\(\det(A) = a_{11} a_{22} - a_{12} a_{21}\)</span>.</p>
<p>For a block-structured square matrix
<span class="math display">\[
\boldsymbol A= \begin{pmatrix} \boldsymbol A_{11} &amp; \boldsymbol A_{12} \\ \boldsymbol A_{21} &amp; \boldsymbol A_{22} \\ \end{pmatrix} \, ,
\]</span>
where the matrices on the diagonal <span class="math inline">\(\boldsymbol A_{11}\)</span> and <span class="math inline">\(\boldsymbol A_{22}\)</span> are themselves square but
<span class="math inline">\(\boldsymbol A_{21}\)</span> and <span class="math inline">\(\boldsymbol A_{21}\)</span> can have any shape,
the determinant is
<span class="math display">\[
\det(\boldsymbol A) = \det(\boldsymbol A_{22}) \det(\boldsymbol C_1) = \det(\boldsymbol A_{11}) \det(\boldsymbol C_2)
\]</span>
with the (Schur complement of <span class="math inline">\(\boldsymbol A_{22}\)</span>)
<span class="math display">\[
\boldsymbol C_1 = \boldsymbol A_{11} -  \boldsymbol A_{12}  \boldsymbol A_{22}^{-1}  \boldsymbol A_{21}
\]</span>
and (Schur complement of <span class="math inline">\(\boldsymbol A_{11}\)</span>)
<span class="math display">\[
\boldsymbol C_2 = \boldsymbol A_{22} -  \boldsymbol A_{21}  \boldsymbol A_{11}^{-1}  \boldsymbol A_{12}
\]</span>
Note that <span class="math inline">\(\boldsymbol C_1\)</span> and <span class="math inline">\(\boldsymbol C_2\)</span> are square matrices.</p>
<p>For a block-diagonal matrix <span class="math inline">\(\boldsymbol A\)</span> with <span class="math inline">\(\boldsymbol A_{12} = 0\)</span> and <span class="math inline">\(\boldsymbol A_{21} = 0\)</span>
the determinant is <span class="math inline">\(\det(\boldsymbol A) = \det(\boldsymbol A_{11}) \det(\boldsymbol A_{22})\)</span>, i.e. 
the product of the determinants of the submatrices along the diagonal.</p>
<p>Determinants have a multiplicative property,
<span class="math display">\[\det(\boldsymbol A\boldsymbol B) = \det(\boldsymbol B\boldsymbol A) = \det(\boldsymbol A) \det(\boldsymbol B) \,.\]</span>
In the above <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> are both square and of the same dimension.</p>
<p>For rectangular <span class="math inline">\(\boldsymbol A\)</span> (<span class="math inline">\(n \times m\)</span>) and rectangular <span class="math inline">\(\boldsymbol B\)</span> (<span class="math inline">\(m \times n\)</span>)
with <span class="math inline">\(m \geq n\)</span>
this generalises to the Cauchy-Binet formula
<span class="math display">\[
\det(\boldsymbol A\boldsymbol B) = \sum_{w} \det(\boldsymbol A_{,w}) \det(\boldsymbol B_{w,})
\]</span>
where the summation is over all <span class="math inline">\(\binom{m}{n}\)</span> index subsets <span class="math inline">\(w\)</span> of size <span class="math inline">\(n\)</span> taken from <span class="math inline">\(\{1, \ldots, m\}\)</span> keeping the ordering
and <span class="math inline">\(\boldsymbol A_{,w}\)</span> and <span class="math inline">\(\boldsymbol B_{w,}\)</span> are the corresponding square <span class="math inline">\(n \times n\)</span> submatrices. If <span class="math inline">\(m &lt; n\)</span> then <span class="math inline">\(\det(\boldsymbol A\boldsymbol B) = 0\)</span>.</p>
<p>For scalar <span class="math inline">\(a\)</span>
<span class="math inline">\(\det(a \boldsymbol B) = a^d \det(\boldsymbol B)\)</span> where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(\boldsymbol B\)</span>.</p>
<p>Another important identity is
<span class="math display">\[\det(\boldsymbol I_n + \boldsymbol A\boldsymbol B) = \det(\boldsymbol I_m + \boldsymbol B\boldsymbol A)\]</span>
where <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> are rectangular matrices. This is called the Weinstein-Aronszajn determinant identity (also credited to Sylvester).</p>
</div>
</div>
<div id="matrix-inverse" class="section level2" number="7.4">
<h2>
<span class="header-section-number">A.4</span> Matrix inverse<a class="anchor" aria-label="anchor" href="#matrix-inverse"><i class="fas fa-link"></i></a>
</h2>
<div id="inversion-of-square-matrix" class="section level3" number="7.4.1">
<h3>
<span class="header-section-number">A.4.1</span> Inversion of square matrix<a class="anchor" aria-label="anchor" href="#inversion-of-square-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is a square matrix then the inverse matrix <span class="math inline">\(\boldsymbol A^{-1}\)</span> is a matrix
such that
<span class="math display">\[\boldsymbol A^{-1} \boldsymbol A= \boldsymbol A\boldsymbol A^{-1}=  \boldsymbol I\, .\]</span>
Only non-singular matrices with <span class="math inline">\(\det(\boldsymbol A) \neq 0\)</span> are invertible.</p>
<p>As <span class="math inline">\(\det(\boldsymbol A^{-1} \boldsymbol A) = \det(\boldsymbol I) = 1\)</span> the
determinant of the inverse matrix equals
the inverse determinant,
<span class="math display">\[\det(\boldsymbol A^{-1}) = \det(\boldsymbol A)^{-1} \,.\]</span></p>
<p>The transpose of the inverse is the inverse of the transpose
as
<span class="math display">\[
\begin{split}
(\boldsymbol A^{-1})^T &amp;= (\boldsymbol A^{-1})^T \,  \boldsymbol A^T (\boldsymbol A^{T})^{-1}   \\
&amp;= (\boldsymbol A\boldsymbol A^{-1})^T \, (\boldsymbol A^{T})^{-1} = (\boldsymbol A^{T})^{-1} \,. \\
\end{split}
\]</span></p>
<p>The inverse of a matrix product <span class="math inline">\((\boldsymbol A\boldsymbol B)^{-1} = \boldsymbol B^{-1} \boldsymbol A^{-1}\)</span>
is the product of the indivdual matrix inverses in reverse order.</p>
<p>There are many different algorithms to compute the inverse of a matrix
(which is essentially a problem of solving a system of equations).
The computational complexity of matrix inversion is of the order <span class="math inline">\(O(d^3)\)</span>
where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(\boldsymbol A\)</span>. Therefore matrix inversion is very costly in higher dimensions.</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example A.1  </strong></span>Inversion of a <span class="math inline">\(2 \times 2\)</span> matrix:</p>
<p>The inverse of the matrix <span class="math inline">\(A = \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}\)</span> is
<span class="math inline">\(A^{-1} = \frac{1}{ad-bc} \begin{pmatrix} d &amp; -b \\ -c &amp; a \end{pmatrix}\)</span></p>
</div>
</div>
<div id="inversion-of-structured-matrices" class="section level3" number="7.4.2">
<h3>
<span class="header-section-number">A.4.2</span> Inversion of structured matrices<a class="anchor" aria-label="anchor" href="#inversion-of-structured-matrices"><i class="fas fa-link"></i></a>
</h3>
<p>However, for specially structured matrices inversion can be done effectively:</p>
<ul>
<li>The inverse of a <strong>diagonal matrix</strong> is another diagonal matrix obtained by inverting the diagonal elements.</li>
<li>More generally, the inverse of a <strong>block-diagonal matrix</strong> is obtained by individually inverting the blocks along the diagonal.</li>
</ul>
<p>The <strong>Woodbury matrix identity</strong> simplifies the inversion of matrices that can be
written as <span class="math inline">\(\boldsymbol A+ \boldsymbol U\boldsymbol B\boldsymbol V\)</span> where <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> are both square and
<span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span> are suitable rectangular matrices:
<span class="math display">\[
(\boldsymbol A+ \boldsymbol U\boldsymbol B\boldsymbol V)^{-1} = \boldsymbol A^{-1} - \boldsymbol A^{-1} \boldsymbol U(\boldsymbol B^{-1} + \boldsymbol V\boldsymbol A^{-1} \boldsymbol U)^{-1} \boldsymbol V\boldsymbol A^{-1}
\]</span>
Typically, the inverse <span class="math inline">\(\boldsymbol A^{-1}\)</span> is either already known or can be easily obtained and
the dimension of <span class="math inline">\(\boldsymbol B\)</span> is much lower than that of <span class="math inline">\(\boldsymbol A\)</span>.</p>
<p>The class of matrices that can be most easily inverted are <strong>orthogonal matrices</strong> whose inverse is
obtained simply by transposing the matrix.</p>
</div>
</div>
<div id="orthogonal-matrices" class="section level2" number="7.5">
<h2>
<span class="header-section-number">A.5</span> Orthogonal matrices<a class="anchor" aria-label="anchor" href="#orthogonal-matrices"><i class="fas fa-link"></i></a>
</h2>
<div id="properties-1" class="section level3" number="7.5.1">
<h3>
<span class="header-section-number">A.5.1</span> Properties<a class="anchor" aria-label="anchor" href="#properties-1"><i class="fas fa-link"></i></a>
</h3>
<p>An orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> is a square matrix with the property that <span class="math inline">\(\boldsymbol Q^T = \boldsymbol Q^{-1}\)</span>, i.e.
the transpose is also the inverse. This implies that <span class="math inline">\(\boldsymbol Q\boldsymbol Q^T = \boldsymbol Q^T \boldsymbol Q= \boldsymbol I\)</span>.</p>
<p>Both the column and the row vectors in <span class="math inline">\(\boldsymbol Q\)</span> all have length 1. This implies that
each element <span class="math inline">\(q_{ij}\)</span> of <span class="math inline">\(\boldsymbol Q\)</span> can only take a value in the interval <span class="math inline">\([-1, 1]\)</span>.</p>
<p>The identity matrix <span class="math inline">\(\boldsymbol I\)</span> is the simplest example of an orthogonal matrix.</p>
<p>The squared Euclidean and Frobenius norm is preserved when a vector <span class="math inline">\(\boldsymbol a\)</span> or matrix <span class="math inline">\(\boldsymbol A\)</span> is multiplied with an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span>:
<span class="math display">\[
|| \boldsymbol Q\boldsymbol a||^2_2 = (\boldsymbol Q\boldsymbol a)^T \boldsymbol Q\boldsymbol a= \boldsymbol a^T \boldsymbol a= || \boldsymbol a||^2_2
\]</span>
and
<span class="math display">\[
|| \boldsymbol Q\boldsymbol A||^2_F = \text{Tr}\left((\boldsymbol Q\boldsymbol A)^T \boldsymbol Q\boldsymbol A\right) = \text{Tr}\left(\boldsymbol A^T \boldsymbol A\right) = || \boldsymbol A||^2_F
\]</span></p>
<p>Multiplication of <span class="math inline">\(\boldsymbol Q\)</span> with a vector results in
a new vector of the same length but with a change in direction (unless <span class="math inline">\(\boldsymbol Q=\boldsymbol I\)</span>).
An orthogonal matrix
<span class="math inline">\(\boldsymbol Q\)</span> can thus be interpreted geometrically as an operator performing
rotation, reflection and/or permutation.</p>
<p>The product <span class="math inline">\(\boldsymbol Q_3 = \boldsymbol Q_1 \boldsymbol Q_2\)</span> of two orthogonal matrices <span class="math inline">\(\boldsymbol Q_1\)</span> and <span class="math inline">\(\boldsymbol Q_2\)</span> yields another orthogonal matrix as <span class="math inline">\(\boldsymbol Q_3 \boldsymbol Q_3^T = \boldsymbol Q_1 \boldsymbol Q_2 (\boldsymbol Q_1 \boldsymbol Q_2)^T = \boldsymbol Q_1 \boldsymbol Q_2 \boldsymbol Q_2^T \boldsymbol Q_1^T = \boldsymbol I\)</span>.</p>
<p>The determinant <span class="math inline">\(\det(\boldsymbol Q)\)</span> of an orthogonal matrix is either +1 or -1,
because <span class="math inline">\(\boldsymbol Q\boldsymbol Q^T = \boldsymbol I\)</span> and thus <span class="math inline">\(\det(\boldsymbol Q)\det(\boldsymbol Q^T) = \det(\boldsymbol Q)^2 = \det(\boldsymbol I) = 1\)</span>.</p>
<p>The set of all orthogonal matrices of dimension <span class="math inline">\(d\)</span> together with multiplication
form a group called the orthogonal group <span class="math inline">\(O(d)\)</span>.
The subset of orthogonal matrices with <span class="math inline">\(\det(\boldsymbol Q)=1\)</span> are called rotation matrices and form with multiplication the special orthogonal group <span class="math inline">\(SO(d)\)</span>.
Orthogonal matrices with <span class="math inline">\(\det(\boldsymbol Q)=-1\)</span> are rotation-reflection matrices.</p>
</div>
<div id="semi-orthogonal-matrices" class="section level3" number="7.5.2">
<h3>
<span class="header-section-number">A.5.2</span> Semi-orthogonal matrices<a class="anchor" aria-label="anchor" href="#semi-orthogonal-matrices"><i class="fas fa-link"></i></a>
</h3>
<p>A rectangular <span class="math inline">\(d \times k\)</span> matrix <span class="math inline">\(\boldsymbol Q\)</span> is semi-orthogonal
if for <span class="math inline">\(k &lt; d\)</span> the <span class="math inline">\(k\)</span> column vectors are orthonormal and hence <span class="math inline">\(\boldsymbol Q^T \boldsymbol Q= \boldsymbol I_k\)</span>, or if for <span class="math inline">\(k &gt; d\)</span> the <span class="math inline">\(d\)</span> row vectors are orthonormal with
<span class="math inline">\(\boldsymbol Q\boldsymbol Q^T = \boldsymbol I_d\)</span>.</p>
<p>The set of all (semi)-orthogonal matrices <span class="math inline">\(\boldsymbol Q\)</span> with <span class="math inline">\(k \leq d\)</span> column vectors is known as the Stiefel manifold <span class="math inline">\(\text{St}(d, k)\)</span>.</p>
</div>
<div id="generating-orthogonal-matrices" class="section level3" number="7.5.3">
<h3>
<span class="header-section-number">A.5.3</span> Generating orthogonal matrices<a class="anchor" aria-label="anchor" href="#generating-orthogonal-matrices"><i class="fas fa-link"></i></a>
</h3>
<p>In two dimensions <span class="math inline">\((d=2)\)</span> all orthogonal matrices <span class="math inline">\(\boldsymbol R\)</span> representing rotations with <span class="math inline">\(\det(\boldsymbol R)=1\)</span> are
given by
<span class="math display">\[
\boldsymbol R(\theta) =
\begin{pmatrix}
\cos \theta &amp; -\sin \theta \\
\sin \theta &amp; \cos \theta
\end{pmatrix}
\]</span>
and those representing rotation-reflections <span class="math inline">\(\boldsymbol G\)</span> with <span class="math inline">\(\det(\boldsymbol G)=-1\)</span> by
<span class="math display">\[
\boldsymbol G(\theta) =
\begin{pmatrix}
\cos \theta &amp; \sin \theta \\
\sin \theta &amp; -\cos \theta
\end{pmatrix}\,.
\]</span>
Every orthogonal matrix of dimension <span class="math inline">\(d=2\)</span>
can be represented as the product of at most two rotation-reflection
matrices because
<span class="math display">\[
\boldsymbol R(\theta) = \boldsymbol G(\theta)\, \boldsymbol G(0) =  
\begin{pmatrix}
\cos \theta &amp; \sin \theta \\
\sin \theta &amp; -\cos \theta
\end{pmatrix}
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; -1
\end{pmatrix}\,.
\]</span>
Thus, the matrix <span class="math inline">\(\boldsymbol G\)</span> is a generator of two-dimensional orthogonal matrices.
Note that <span class="math inline">\(\boldsymbol G(\theta)\)</span> is symmetric, orthogonal and has determinant -1.</p>
<p>More generally, and applicable in arbitrary dimension, the role of generator is taken by the Householder reflection matrix
<span class="math display">\[
\boldsymbol Q_{HH}(\boldsymbol v) = \boldsymbol I- 2 \boldsymbol v\boldsymbol v^T
\]</span>
where <span class="math inline">\(\boldsymbol v\)</span> is a vector of unit length (with <span class="math inline">\(\boldsymbol v^T \boldsymbol v=1\)</span>) orthogonal to
the reflection hyperplane. Note that <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v) = \boldsymbol Q_{HH}(-\boldsymbol v)\)</span>.
By construction the matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span> is symmetric, orthogonal and has determinant -1.</p>
<p>It can be shown that any <span class="math inline">\(d\)</span>-dimensional orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> can be represented as the product of at most <span class="math inline">\(d\)</span> Householder reflection matrices.
The two-dimensional generator <span class="math inline">\(\boldsymbol G(\theta)\)</span> is recovered as the Householder matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span>
with <span class="math inline">\(\boldsymbol v= \begin{pmatrix} -\sin \frac{\theta}{2} \\ \cos \frac{\theta}{2} \end{pmatrix}\)</span>
or <span class="math inline">\(\boldsymbol v= \begin{pmatrix} \sin \frac{\theta}{2} \\ -\cos \frac{\theta}{2} \end{pmatrix}\)</span>.</p>
</div>
<div id="permutation-matrix" class="section level3" number="7.5.4">
<h3>
<span class="header-section-number">A.5.4</span> Permutation matrix<a class="anchor" aria-label="anchor" href="#permutation-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>A special type of an orthogonal matrix is a permutation matrix <span class="math inline">\(\boldsymbol P\)</span> created by
permuting rows and/or columns of the identity matrix <span class="math inline">\(\boldsymbol I\)</span>. Thus, each row and column
of <span class="math inline">\(\boldsymbol P\)</span> contains exactly one entry of 1, but not necessarily on the diagonal.</p>
<p>If a permutation matrix <span class="math inline">\(\boldsymbol P\)</span> is multiplied with a matrix <span class="math inline">\(\boldsymbol A\)</span> it acts as an operator
permuting the columns (<span class="math inline">\(\boldsymbol A\boldsymbol P\)</span>) or the rows (<span class="math inline">\(\boldsymbol P\boldsymbol A\)</span>).
For a set of <span class="math inline">\(d\)</span> elements there exist <span class="math inline">\(d!\)</span> permutations. Thus, for dimension <span class="math inline">\(d\)</span> there
are <span class="math inline">\(d!\)</span> possible permutation matrices (including the identity matrix).</p>
<p>The determinant of a permutation matrix is either +1 or -1.
The product of two permutation matrices yields another permutation matrix.</p>
<p>Symmetric permutation matrices correspond to self-inverse permutations
(i.e. the permutation matrix is its own inverse), and are also called permutation involutions.
They can have determinant +1 and -1.</p>
<p>A transposition is a permutation where only two elements are exchanged.
Thus, in a transposition matrix <span class="math inline">\(\boldsymbol T\)</span>
exactly two rows and/or columns are exchanged compared to identity matrix <span class="math inline">\(\boldsymbol I\)</span>.
Transpositions are self-inverse, and transposition matrices are symmetric.
There are <span class="math inline">\(\frac{d (d-1)}{2}\)</span> different transposition matrices.
The determinant of a transposition matrix is <span class="math inline">\(\det(\boldsymbol T)= -1\)</span>.</p>
<p>Note that the transposition matrix is an instance of a Householder matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span>
with vector <span class="math inline">\(\boldsymbol v\)</span> filled with zeros except for two elements that have value
<span class="math inline">\(\frac{\sqrt{2}}{2}\)</span> and <span class="math inline">\(-\frac{\sqrt{2}}{2}\)</span>.</p>
<p>Any permutation of <span class="math inline">\(d\)</span> elements can be generated by a series of at most <span class="math inline">\(d-1\)</span> transpositions.
Correspondingly, any permutation matrix <span class="math inline">\(\boldsymbol P\)</span> can be constructed by multiplication of the identity
matrix with at most <span class="math inline">\(d-1\)</span> transposition matrices. If the number of transpositions is even then <span class="math inline">\(\det(\boldsymbol P) = 1\)</span> otherwise
for an uneven number <span class="math inline">\(\det(\boldsymbol P) = -1\)</span>. This is called the sign or signature of the permutation.</p>
<p>The set of all permutations form the symmetric group <span class="math inline">\(S_d\)</span>, the subset of even permutations (with positive sign and <span class="math inline">\(\det(\boldsymbol P)=1\)</span>) the alternating group <span class="math inline">\(A_d\)</span>.</p>
</div>
</div>
<div id="eigenvalues-and-eigenvectors" class="section level2" number="7.6">
<h2>
<span class="header-section-number">A.6</span> Eigenvalues and eigenvectors<a class="anchor" aria-label="anchor" href="#eigenvalues-and-eigenvectors"><i class="fas fa-link"></i></a>
</h2>
<div id="definition" class="section level3" number="7.6.1">
<h3>
<span class="header-section-number">A.6.1</span> Definition<a class="anchor" aria-label="anchor" href="#definition"><i class="fas fa-link"></i></a>
</h3>
<p>Assume a square matrix <span class="math inline">\(\boldsymbol A\)</span> of size <span class="math inline">\(d \times d\)</span>.
A vector <span class="math inline">\(\boldsymbol u\neq 0\)</span> is called an eigenvector of the matrix <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\lambda\)</span> the corresponding
eigenvalue if<br><span class="math display">\[\boldsymbol A\boldsymbol u= \boldsymbol u\lambda \, .\]</span>
This is called <strong>eigenvalue equation</strong> or <strong>eigenequation</strong>.</p>
</div>
<div id="finding-eigenvalues-and-vectors" class="section level3" number="7.6.2">
<h3>
<span class="header-section-number">A.6.2</span> Finding eigenvalues and vectors<a class="anchor" aria-label="anchor" href="#finding-eigenvalues-and-vectors"><i class="fas fa-link"></i></a>
</h3>
<p>To find the eigenvalues and eigenvectors the eigenequation is rewritten as
<span class="math display">\[(\boldsymbol A-\boldsymbol I\lambda ) \; \boldsymbol u= \boldsymbol 0\,.\]</span>
For this equation to hold for an eigenvector <span class="math inline">\(\boldsymbol u\neq 0\)</span> with eigenvalue
<span class="math inline">\(\lambda\)</span> implies that the matrix <span class="math inline">\(\boldsymbol A-\boldsymbol I\lambda\)</span> is singular.
Correspondingly, its determinant must vanish
<span class="math display">\[\det(\boldsymbol A-\boldsymbol I\lambda ) =0 \,.\]</span>
This is called the <em>characteristic equation</em> of the matrix <span class="math inline">\(\boldsymbol A\)</span>, and its solution yields the <span class="math inline">\(d\)</span>
eigenvalues <span class="math inline">\(\lambda_1, \ldots, \lambda_d\)</span>. Note the eigenvalues need not be
distinct and they may be complex even if the matrix <span class="math inline">\(\boldsymbol A\)</span> is real.</p>
<p>If there are complex eigenvalues, for a real matrix those eigenvalues come in conjugate pairs.
Hence, for a complex <span class="math inline">\(\lambda_1 = r e^{i \phi}\)</span> there will also be a corresponding complex eigenvalue <span class="math inline">\(\lambda_2 = r e^{-i \phi}\)</span>.</p>
<p>Given the eigenvalues we then solve the eigenequation for the corresponding non-zero eigenvectors
<span class="math inline">\(\boldsymbol u_1, \ldots, \boldsymbol u_d\)</span>. Note that eigenvectors of real matrices can have complex components.
Also the eigenvector is only defined by the eigenequation up to a scalar.
By convention eigenvectors are therefore typically standardised to unit length but this still leaves
a sign ambiguity for real eigenvectors and implies that complex eigenvectors are defined only up to a factor with modulus 1.</p>
</div>
<div id="eigenequation-in-matrix-notation" class="section level3" number="7.6.3">
<h3>
<span class="header-section-number">A.6.3</span> Eigenequation in matrix notation<a class="anchor" aria-label="anchor" href="#eigenequation-in-matrix-notation"><i class="fas fa-link"></i></a>
</h3>
<p>With the matrix
<span class="math display">\[\boldsymbol U= (\boldsymbol u_1, \ldots, \boldsymbol u_d)\]</span> containing the standardised eigenvectors in the columns and the diagonal matrix
<span class="math display">\[\boldsymbol \Lambda= \begin{pmatrix}
    \lambda_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}
\end{pmatrix}\]</span>
containing the eigenvalues (typically sorted in order of magnitude) the eigenvalue equation can be written as
<span class="math display">\[\boldsymbol A\boldsymbol U= \boldsymbol U\boldsymbol \Lambda\,.\]</span></p>
</div>
<div id="permutation-of-eigenvalues" class="section level3" number="7.6.4">
<h3>
<span class="header-section-number">A.6.4</span> Permutation of eigenvalues<a class="anchor" aria-label="anchor" href="#permutation-of-eigenvalues"><i class="fas fa-link"></i></a>
</h3>
<p>If eigenvalues are not in order, we may apply a permutation matrix <span class="math inline">\(\boldsymbol P\)</span> to arrange them in order.
With <span class="math inline">\(\boldsymbol \Lambda^{\text{sort}} = \boldsymbol P^T \boldsymbol \Lambda\boldsymbol P\)</span> as the sorted eigenvalues
and <span class="math inline">\(\boldsymbol U^{\text{sort}} = \boldsymbol U\boldsymbol P\)</span> as the corresponding eigenvectors the eigenequation becomes
<span class="math display">\[\boldsymbol A\boldsymbol U^{\text{sort}} =  \boldsymbol A\boldsymbol U\boldsymbol P= \boldsymbol U\boldsymbol \Lambda\boldsymbol P=  \boldsymbol U\boldsymbol P\boldsymbol P^T \boldsymbol \Lambda\boldsymbol P=  \boldsymbol U^{\text{sort}} \boldsymbol \Lambda^{\text{sort}} \,.\]</span></p>
</div>
<div id="similar-matrices" class="section level3" number="7.6.5">
<h3>
<span class="header-section-number">A.6.5</span> Similar matrices<a class="anchor" aria-label="anchor" href="#similar-matrices"><i class="fas fa-link"></i></a>
</h3>
<p>Two matrices <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> are called <strong>similar</strong> if they share the same eigenvalues.</p>
<p>From <span class="math inline">\(\boldsymbol A\)</span> with eigenvalues <span class="math inline">\(\boldsymbol \Lambda\)</span> and eigenvectors <span class="math inline">\(\boldsymbol U\)</span> we can construct a similar <span class="math inline">\(\boldsymbol B\)</span> via the <strong>similarity transformation</strong> <span class="math inline">\(\boldsymbol B= \boldsymbol M\boldsymbol A\boldsymbol M^{-1}\)</span> where <span class="math inline">\(\boldsymbol M\)</span> is an invertible matrix.</p>
<p>Then <span class="math inline">\(\boldsymbol \Lambda\)</span> are the eigenvalues of <span class="math inline">\(\boldsymbol B\)</span> and <span class="math inline">\(\boldsymbol V= \boldsymbol M\boldsymbol U\)</span> its eigenvectors as
<span class="math display">\[
\boldsymbol B\boldsymbol V= \boldsymbol M\boldsymbol A\boldsymbol M^{-1} \boldsymbol M\boldsymbol U= \boldsymbol M\boldsymbol A\boldsymbol U= \boldsymbol M\boldsymbol U\boldsymbol \Lambda= \boldsymbol V\boldsymbol \Lambda\,.
\]</span></p>
</div>
<div id="defective-matrix" class="section level3" number="7.6.6">
<h3>
<span class="header-section-number">A.6.6</span> Defective matrix<a class="anchor" aria-label="anchor" href="#defective-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>In most cases the eigenvectors <span class="math inline">\(\boldsymbol u_i\)</span> will be linearly independent so that they form a basis to span a <span class="math inline">\(d\)</span> dimensional space.</p>
<p>However, if this is not the case and
the matrix <span class="math inline">\(\boldsymbol A\)</span> does not have a complete basis of eigenvectors, then the matrix is called defective. In this case
the matrix <span class="math inline">\(\boldsymbol U\)</span> containing the eigenvectors is singular and <span class="math inline">\(\det(\boldsymbol U)=0\)</span>.</p>
<p>An example of a defective matrix is
<span class="math inline">\(\begin{pmatrix} 1 &amp;1 \\ 0 &amp; 1 \\ \end{pmatrix}\)</span>
which has determinant 1 so that it can be inverted and its column vectors do form a complete basis
but has only one distinct eigenvector <span class="math inline">\((1,0)^T\)</span> so that the eigenvector basis is incomplete.</p>
</div>
<div id="eigenvalues-of-a-diagonal-or-triangular-matrix" class="section level3" number="7.6.7">
<h3>
<span class="header-section-number">A.6.7</span> Eigenvalues of a diagonal or triangular matrix<a class="anchor" aria-label="anchor" href="#eigenvalues-of-a-diagonal-or-triangular-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>In the special case that <span class="math inline">\(\boldsymbol A\)</span> is diagonal or a triangular matrix the eigenvalues are easily determined.
This follows from the simple form of their determinants as the product of the diagonal elements.
Hence for these matrices the characteristic equation becomes <span class="math inline">\(\prod_{i}^d (a_{ii} -\lambda) = 0\)</span> and has solution
<span class="math inline">\(\lambda_i=a_{ii}\)</span>, i.e. the eigenvalues are equal to the diagonal elements.</p>
</div>
<div id="eigenvalues-and-vectors-of-a-symmetric-matrix" class="section level3" number="7.6.8">
<h3>
<span class="header-section-number">A.6.8</span> Eigenvalues and vectors of a symmetric matrix<a class="anchor" aria-label="anchor" href="#eigenvalues-and-vectors-of-a-symmetric-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is symmetric, i.e. <span class="math inline">\(\boldsymbol A= \boldsymbol A^T\)</span>, then its eigenvalues and eigenvectors have special properties:</p>
<ol style="list-style-type: lower-roman">
<li>all eigenvalues of <span class="math inline">\(\boldsymbol A\)</span> are real,</li>
<li>the eigenvectors are orthogonal, i.e <span class="math inline">\(\boldsymbol u_i^T \boldsymbol u_j = 0\)</span> for <span class="math inline">\(i \neq j\)</span>, and real. Thus, the matrix <span class="math inline">\(\boldsymbol U\)</span> containing the standardised orthonormal eigenvectors is orthogonal.</li>
<li>
<span class="math inline">\(\boldsymbol A\)</span> is never defective as <span class="math inline">\(\boldsymbol U\)</span> forms a complete basis.</li>
</ol>
<p>Furthermore, for a symmetric matrix <span class="math inline">\(\boldsymbol A\)</span> with diagonal elements <span class="math inline">\(p_1 \geq \ldots \geq p_d\)</span>
and eigenvalues <span class="math inline">\(\lambda_1 \geq \ldots \geq \lambda_d\)</span> (note both written in decreasing order) the sum of the
largest <span class="math inline">\(k\)</span> eigenvalues forms an upper bound of the sum of the largest <span class="math inline">\(k\)</span> diagonal elements:
<span class="math display">\[
\sum_{i}^k \lambda_i \geq \sum_{i}^k p_i
\]</span>
This theorem is due to Schur (1923) <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Schur, I. 1923. Über eine Klasse von Mittelbildungen mit Anwendungen auf die Determinantentheorie.
Sitzungsber. Berl. Math. Ges. &lt;strong&gt;22&lt;/strong&gt;:9–29.&lt;/p&gt;"><sup>22</sup></a>. The equality holds for <span class="math inline">\(k=d\)</span> (as the trace of <span class="math inline">\(\boldsymbol A\)</span> equals the sum
of its eigenvalues) and for any <span class="math inline">\(k\)</span> if <span class="math inline">\(\boldsymbol A\)</span> is diagonal (as in this case of the diagonal elements equal the eigenvalues).</p>
</div>
<div id="eigenvalues-of-orthogonal-matrices" class="section level3" number="7.6.9">
<h3>
<span class="header-section-number">A.6.9</span> Eigenvalues of orthogonal matrices<a class="anchor" aria-label="anchor" href="#eigenvalues-of-orthogonal-matrices"><i class="fas fa-link"></i></a>
</h3>
<p>The eigenvalues of an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> are not necessarily real but
they all have modulus 1 and lie on the unit circle . Thus, the eigenvalues of <span class="math inline">\(\boldsymbol Q\)</span>
all have the form <span class="math inline">\(\lambda = e^{i \phi} = \cos \phi + i \sin \phi\)</span>.</p>
<p>In any real matrix complex eigenvalues come in conjugate
pairs. Hence if an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> has the complex eigenvalue <span class="math inline">\(e^{i \phi}\)</span> it also has an
complex eigenvalue <span class="math inline">\(e^{-i \phi} =\cos \phi - i \sin \phi\)</span>. The product of these two conjugate
eigenvalues is 1. Thus, an orthogonal matrix of uneven dimension has at least one
real eigenvalue (+1 or -1).</p>
<p>The eigenvalues of a Hausholder matrix <span class="math inline">\(\boldsymbol Q_{HH}(\boldsymbol v)\)</span> are all real (recall that it is symmetric!).
In fact, in dimension <span class="math inline">\(d\)</span> its eigenvalues are -1 (one time) and 1 ( <span class="math inline">\(d-1\)</span> times).
Since a transposition matrix <span class="math inline">\(\boldsymbol T\)</span> is a special Householder matrix they have the same eigenvalues.</p>
</div>
<div id="positive-definite-matrices" class="section level3" number="7.6.10">
<h3>
<span class="header-section-number">A.6.10</span> Positive definite matrices<a class="anchor" aria-label="anchor" href="#positive-definite-matrices"><i class="fas fa-link"></i></a>
</h3>
<p>If all eigenvalues of a square matrix <span class="math inline">\(\boldsymbol A\)</span> are real and <span class="math inline">\(\lambda_i \geq 0\)</span> then <span class="math inline">\(\boldsymbol A\)</span> is called <em>positive semi-definite</em>.
If all eigenvalues are strictly positive
<span class="math inline">\(\lambda_i &gt; 0\)</span> then <span class="math inline">\(\boldsymbol A\)</span> is called <em>positive definite</em>.</p>
<p>Note that a matrix does not need to be symmetric to be positive
definite, e.g.
<span class="math inline">\(\begin{pmatrix} 2 &amp; 3 \\ 1 &amp; 4 \\ \end{pmatrix}\)</span>
has positive eigenvalues 5 and 1. It also has a complete
set of eigenvectors and is diagonisable.</p>
<p>A symmetric matrix <span class="math inline">\(\boldsymbol A\)</span> is positive definite
if the quadratic form <span class="math inline">\(\boldsymbol x^T \boldsymbol A\boldsymbol x&gt; 0\)</span> for any non-zero <span class="math inline">\(\boldsymbol x\)</span>,
and it is positive semi-definite if <span class="math inline">\(\boldsymbol x^T \boldsymbol A\boldsymbol x\geq 0\)</span>.
This holds also the other way around:
a symmetric positive definite matrix (with positive eigenvalues) has a
positive quadratic form, and a symmetric positive semi-definite matrix (with non-negative eigenvalues) a non-negative quadratic form.</p>
<p>A symmetric positive definite matrix always has a positive diagonal
(this can be seen by setting <span class="math inline">\(\boldsymbol x\)</span> above to a unit vector with 1 at
a single position, and 0 at all other elements).
However, just requiring a positive diagonal is too weak to ensure positive definiteness of a symmetric matrix, for example <span class="math inline">\(\begin{pmatrix} 1 &amp;10 \\ 10 &amp; 1 \\ \end{pmatrix}\)</span> has a negative eigenvalue of -9.
On the other hand, a symmetric matrix is indeed positive definite if it is strictly
diagonally dominant, i.e. if all its diagonal elements are positive and are larger than the absolute value of any of the corresponding row or column elements.
However, diagonal dominance is too restrictive as criterion to
characterise all
symmetric positive definite matrices, since
there are many symmetric matrices that are positive definite but not diagonally dominant, such as
<span class="math inline">\(\begin{pmatrix} 1 &amp; 2 \\ 2 &amp; 5 \\ \end{pmatrix}\)</span>.</p>
<p>Finally, the sum of a symmetric positive semi-definite matrix <span class="math inline">\(\boldsymbol A\)</span>
and a symmetric positive definite matrix <span class="math inline">\(\boldsymbol B\)</span> is itself symmetric positive definite because the corresponding
quadratic form <span class="math inline">\(\boldsymbol x^T ( \boldsymbol A+\boldsymbol B) \boldsymbol x= \boldsymbol x^T \boldsymbol A\boldsymbol x+ \boldsymbol x^T \boldsymbol B\boldsymbol x&gt; 0\)</span> is positive. Similarly, the sum
of two symmetric positive (semi)-definite matrices is itself symmetric positive (semi)-definite.</p>
</div>
</div>
<div id="matrix-decompositions" class="section level2" number="7.7">
<h2>
<span class="header-section-number">A.7</span> Matrix decompositions<a class="anchor" aria-label="anchor" href="#matrix-decompositions"><i class="fas fa-link"></i></a>
</h2>
<div id="diagonalisation-and-eigenvalue-decomposition" class="section level3" number="7.7.1">
<h3>
<span class="header-section-number">A.7.1</span> Diagonalisation and eigenvalue decomposition<a class="anchor" aria-label="anchor" href="#diagonalisation-and-eigenvalue-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is a square non-defective matrix then the eigensystem <span class="math inline">\(\boldsymbol U\)</span> is invertible and
we can rewrite the eigenvalue equation to
<span class="math display">\[\boldsymbol A= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1} \,.\]</span>
This is called the <strong>eigendecomposition</strong>, or <strong>spectral decomposition</strong>, of <span class="math inline">\(\boldsymbol A\)</span> and equivalently
<span class="math display">\[\boldsymbol \Lambda= \boldsymbol U^{-1} \boldsymbol A\boldsymbol U\]</span>
is the diagonalisation of <span class="math inline">\(\boldsymbol A\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is defective (i.e. <span class="math inline">\(\boldsymbol U\)</span> is singular) one can still <em>approximately</em> diagonalise <span class="math inline">\(\boldsymbol A\)</span> as there always exists a similarity transformation to
<span class="math inline">\(\boldsymbol J= \boldsymbol M\boldsymbol A\boldsymbol M^{-1}\)</span> where <span class="math inline">\(\boldsymbol M\)</span> is a invertible matrix
and <span class="math inline">\(\boldsymbol J\)</span> has Jordan canonical form, i.e. <span class="math inline">\(\boldsymbol J\)</span> is upper triangular with
the (potentially complex) eigenvalues on the diagonal and
some non-zero entries equal to 1 immediately above the main diagonal.</p>
</div>
<div id="orthogonal-eigenvalue-decomposition" class="section level3" number="7.7.2">
<h3>
<span class="header-section-number">A.7.2</span> Orthogonal eigenvalue decomposition<a class="anchor" aria-label="anchor" href="#orthogonal-eigenvalue-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>For symmetric <span class="math inline">\(\boldsymbol A\)</span> with real eigenvalues and orthogonal matrix <span class="math inline">\(\boldsymbol U\)</span> the spectral decomposition
becomes
<span class="math display">\[\boldsymbol A= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\]</span>
and
<span class="math display">\[\boldsymbol \Lambda= \boldsymbol U^T \boldsymbol A\boldsymbol U\,.\]</span>
This special case is known as the orthogonal diagonalisation
of <span class="math inline">\(\boldsymbol A\)</span>.</p>
<p>The orthogonal decomposition for symmetric <span class="math inline">\(\boldsymbol A\)</span> is
unique apart from the signs
of the eigenvectors (columns of <span class="math inline">\(\boldsymbol U\)</span>). Thus, in a computer application
depending on the specific implementation of a numerical algorithm for eigenvalue
decomposition
the signs may vary.</p>
</div>
<div id="singular-value-decomposition" class="section level3" number="7.7.3">
<h3>
<span class="header-section-number">A.7.3</span> Singular value decomposition<a class="anchor" aria-label="anchor" href="#singular-value-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>singular value decomposition</strong> (SVD) is
a generalisation of the orthogonal eigenvalue decomposition
for symmetric matrices.</p>
<p>Any (!) rectangular matrix <span class="math inline">\(\boldsymbol A\)</span> of size <span class="math inline">\(n\times d\)</span> can be factored
into the product
<span class="math display">\[\boldsymbol A= \boldsymbol U\boldsymbol D\boldsymbol V^T\]</span>
where <span class="math inline">\(\boldsymbol U\)</span> is a <span class="math inline">\(n \times n\)</span> orthogonal matrix, <span class="math inline">\(\boldsymbol V\)</span> is a second <span class="math inline">\(d \times d\)</span> orthogonal matrix and <span class="math inline">\(\boldsymbol D\)</span> is a diagonal but rectangular matrix
of size <span class="math inline">\(n\times d\)</span> with <span class="math inline">\(m=min(n,d)\)</span> real diagonal elements <span class="math inline">\(d_1, \ldots d_m\)</span>. The <span class="math inline">\(d_i\)</span> are called singular values, and appear
along the diagonal in <span class="math inline">\(\boldsymbol D\)</span> by order of magnitude.</p>
<p>The SVD is unique apart from the
signs of the columns vectors in <span class="math inline">\(\boldsymbol U\)</span>, <span class="math inline">\(\boldsymbol V\)</span> and <span class="math inline">\(\boldsymbol D\)</span> (you can freely specify the column signs of any two of the
three matrices). By convention the
signs are chosen such that the singular values in <span class="math inline">\(\boldsymbol D\)</span> are all non-negative, which leaves ambiguity
in columns signs of <span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span>. Alternatively, one may
fix the columns signs of <span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span>, e.g. by requiring a positive diagonal, which then determines the sign of the singular values (thus allowing for negative singular values as well).</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is symmetric then the SVD and the orthogonal eigenvalue decomposition coincide (apart from different sign conventions for singular values, eigenvalues and eigenvectors).</p>
<p>Since <span class="math inline">\(\boldsymbol A^T \boldsymbol A= \boldsymbol V\boldsymbol D^T \boldsymbol D\boldsymbol V^T\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T = \boldsymbol U\boldsymbol D\boldsymbol D^T \boldsymbol U^T\)</span> the squared singular values correspond to the eigenvalues of <span class="math inline">\(\boldsymbol A^T \boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T\)</span>.
It also follows that <span class="math inline">\(\boldsymbol A^T \boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol A\boldsymbol A^T\)</span> are both positive
semi-definite symmetric matrices, and that <span class="math inline">\(\boldsymbol V\)</span> and <span class="math inline">\(\boldsymbol U\)</span> contain the respective sets of eigenvectors.</p>
</div>
<div id="polar-decomposition" class="section level3" number="7.7.4">
<h3>
<span class="header-section-number">A.7.4</span> Polar decomposition<a class="anchor" aria-label="anchor" href="#polar-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>Any square matrix <span class="math inline">\(\boldsymbol A\)</span> can be factored into the product
<span class="math display">\[
\boldsymbol A= \boldsymbol Q\boldsymbol B
\]</span>
of an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> and a symmetric positive semi-definite matrix <span class="math inline">\(\boldsymbol B\)</span>.</p>
<p>This follows from the SVD of <span class="math inline">\(\boldsymbol A\)</span> given as
<span class="math display">\[
\begin{split}
\boldsymbol A&amp;= \boldsymbol U\boldsymbol D\boldsymbol V^T \\
    &amp;= ( \boldsymbol U\boldsymbol V^T ) ( \boldsymbol V\boldsymbol D\boldsymbol V^T ) \\
    &amp;= \boldsymbol Q\boldsymbol B\\
\end{split}
\]</span>
with non-negative <span class="math inline">\(\boldsymbol D\)</span>. Note that this decomposition is unique as the sign ambiguities in the columns of <span class="math inline">\(\boldsymbol U\)</span> and <span class="math inline">\(\boldsymbol V\)</span> cancel out in <span class="math inline">\(\boldsymbol Q\)</span> and <span class="math inline">\(\boldsymbol B\)</span>.</p>
</div>
<div id="cholesky-decomposition" class="section level3" number="7.7.5">
<h3>
<span class="header-section-number">A.7.5</span> Cholesky decomposition<a class="anchor" aria-label="anchor" href="#cholesky-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>A symmetric positive definite matrix <span class="math inline">\(\boldsymbol A\)</span> can be decomposed into a product
of a triangular matrix <span class="math inline">\(\boldsymbol L\)</span> with its transpose
<span class="math display">\[
\boldsymbol A= \boldsymbol L\boldsymbol L^T \,.
\]</span>
Here, <span class="math inline">\(\boldsymbol L\)</span> is a lower triangular matrix with positive diagonal elements.</p>
<p>This decomposition is unique and is called <strong>Cholesky factorisation</strong>. It is
often used to check whether a symmetric matrix is positive definite as it is algorithmically
less demanding than eigenvalue decomposition.</p>
<p>Note that some implementations of the Cholesky decomposition (e.g. in R) use
upper triangular matrices <span class="math inline">\(\boldsymbol K\)</span> with positive diagonal so that
<span class="math inline">\(\boldsymbol A= \boldsymbol K^T \boldsymbol K\)</span> and <span class="math inline">\(\boldsymbol L= \boldsymbol K^T\)</span>.</p>
</div>
</div>
<div id="matrix-summaries-based-on-eigenvalues-and-singular-values" class="section level2" number="7.8">
<h2>
<span class="header-section-number">A.8</span> Matrix summaries based on eigenvalues and singular values<a class="anchor" aria-label="anchor" href="#matrix-summaries-based-on-eigenvalues-and-singular-values"><i class="fas fa-link"></i></a>
</h2>
<div id="trace-and-determinant-computed-from-eigenvalues" class="section level3" number="7.8.1">
<h3>
<span class="header-section-number">A.8.1</span> Trace and determinant computed from eigenvalues<a class="anchor" aria-label="anchor" href="#trace-and-determinant-computed-from-eigenvalues"><i class="fas fa-link"></i></a>
</h3>
<p>The eigendecomposition <span class="math inline">\(\boldsymbol A=\boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1}\)</span>
allows to establish a link between the trace and the determinant and the eigenvalues
of a matrix.</p>
<p>Specifically,
<span class="math display">\[
\begin{split}
\text{Tr}(\boldsymbol A) &amp; = \text{Tr}(\boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1}  ) =
\text{Tr}( \boldsymbol \Lambda\boldsymbol U^{-1} \boldsymbol U) \\
&amp;= \text{Tr}( \boldsymbol \Lambda) = \sum_{i=1}^d \lambda_i \\
\end{split}
\]</span>
thus the trace of a square matrix <span class="math inline">\(\boldsymbol A\)</span> is equal to the <em>sum</em> of its eigenvalues. Likewise,
<span class="math display">\[
\begin{split}
\det(\boldsymbol A) &amp; = \det(\boldsymbol U) \det(\boldsymbol \Lambda) \det(\boldsymbol U^{-1}  ) \\
&amp;=\det( \boldsymbol \Lambda) = \prod_{i=1}^d \lambda_i \\
\end{split}
\]</span>
therefore the determinant of <span class="math inline">\(\boldsymbol A\)</span> is the <em>product</em> of the eigenvalues.</p>
<p>The relationship between the eigenvalues of a square matrix and the trace and the determinant
of that matrix is shown above for diagonisable matrices.
However, it holds more generally for any square matrix, i.e. also for defective matrices.
For the latter the Jordan canonical form <span class="math inline">\(\boldsymbol J\)</span> replaces <span class="math inline">\(\boldsymbol \Lambda\)</span> (in both cases the eigenvalues are simply the entries on the diagonal).</p>
<p>If any of the eigenvalues are equal to zero then <span class="math inline">\(\det(\boldsymbol A) = 0\)</span> and as hence <span class="math inline">\(\boldsymbol A\)</span> is singular and not invertible.</p>
<p>The trace and determinant of a real matrix are always real even though the individual eigenvalues may be complex.</p>
</div>
<div id="eigenvalues-of-a-squared-matrix" class="section level3" number="7.8.2">
<h3>
<span class="header-section-number">A.8.2</span> Eigenvalues of a squared matrix<a class="anchor" aria-label="anchor" href="#eigenvalues-of-a-squared-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>From the eigendecomposition <span class="math inline">\(\boldsymbol A=\boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1}\)</span>
it is easy to see that the eigenvalues of <span class="math inline">\(\boldsymbol A^2\)</span> are simply the
squared eigenvalues of <span class="math inline">\(\boldsymbol A\)</span> as
<span class="math display">\[
\boldsymbol A^2 = \boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1} \boldsymbol U\boldsymbol \Lambda\boldsymbol U^{-1} = \boldsymbol U\boldsymbol \Lambda^2 \boldsymbol U^{-1}
\]</span>
As a result we can compute the trace of <span class="math inline">\(\boldsymbol A^2\)</span> as the sum of the squared
eigenvalues of <span class="math inline">\(\boldsymbol A\)</span>, i.e. <span class="math inline">\(\text{Tr}(\boldsymbol A^2) = \sum_{i=1}^d \lambda_i^2\)</span>,
and the determinant as the product of squared eigenvalues, i.e
<span class="math inline">\(\det(\boldsymbol A^2) = \prod_{i=1}^d \lambda_i^2\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> is symmetric then <span class="math inline">\(\text{Tr}(\boldsymbol A^2) = \text{Tr}(\boldsymbol A\boldsymbol A^T) = || A ||^2_F = \sum_{i=1}^d \sum_{j=1}^d a_{ij}^2\)</span>.
This leads to the identity
<span class="math display">\[
\sum_{i=1}^d \lambda_i^2 =  \sum_{i=1}^d \sum_{j=1}^d a_{ij}^2
\]</span>
between the sum of the squared eigenvalues and the sum of all squared entries of a symmetric matrix <span class="math inline">\(\boldsymbol A\)</span>.</p>
</div>
<div id="rank-and-condition-number" class="section level3" number="7.8.3">
<h3>
<span class="header-section-number">A.8.3</span> Rank and condition number<a class="anchor" aria-label="anchor" href="#rank-and-condition-number"><i class="fas fa-link"></i></a>
</h3>
<p>The rank is the dimension of the space spanned by both the column and row vectors. A rectangular matrix of dimension <span class="math inline">\(n \times d\)</span> will have
rank of at most <span class="math inline">\(m = \min(n, d)\)</span>, and if the maximum is indeed achieved then it has full rank.</p>
<p>The condition number describes how well- or ill-conditioned
a full rank matrix is. For example, for a square matrix a large condition number implies that the matrix is close to being singular
and thus ill-conditioned.
If the condition number is infinite then the matrix is not full rank.</p>
<p>The rank and condition of a matrix can both be determined from the <span class="math inline">\(m\)</span> singular values <span class="math inline">\(d_1, \ldots, d_m\)</span> of a matrix obtained by SVD:</p>
<ol style="list-style-type: lower-roman">
<li>The rank is number of non-zero singular values.</li>
<li>The condition number is the ratio of the largest singular value
divided by the smallest singular value (absolute values if signs are allowed).</li>
</ol>
<p>If a square matrix <span class="math inline">\(\boldsymbol A\)</span> is singular then the condition number is infinite, and it will not have full rank.
On the other hand, a non-singular square matrix, such as
a positive definite matrix, has full rank.</p>
</div>
</div>
<div id="functions-of-symmetric-matrices" class="section level2" number="7.9">
<h2>
<span class="header-section-number">A.9</span> Functions of symmetric matrices<a class="anchor" aria-label="anchor" href="#functions-of-symmetric-matrices"><i class="fas fa-link"></i></a>
</h2>
<p>We focus on <em>symmetric</em> square matrices <span class="math inline">\(\boldsymbol A=\boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span> which are always diagonisable with real eigenvalues <span class="math inline">\(\boldsymbol \Lambda\)</span> and orthogonal eigenvectors <span class="math inline">\(\boldsymbol U\)</span>.</p>
<div id="definition-of-a-matrix-function" class="section level3" number="7.9.1">
<h3>
<span class="header-section-number">A.9.1</span> Definition of a matrix function<a class="anchor" aria-label="anchor" href="#definition-of-a-matrix-function"><i class="fas fa-link"></i></a>
</h3>
<p>Assume a real-valued function <span class="math inline">\(f(a)\)</span> of a real number <span class="math inline">\(a\)</span>. Then the corresponding
matrix function <span class="math inline">\(f(\boldsymbol A)\)</span>
is defined as
<span class="math display">\[
f(\boldsymbol A) =  \boldsymbol Uf(\boldsymbol \Lambda) \boldsymbol U^T =  \boldsymbol U\begin{pmatrix}
    f(\lambda_{1}) &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; f(\lambda_{d})
\end{pmatrix} \boldsymbol U^T
\]</span>
where the function <span class="math inline">\(f(a)\)</span> is applied to the eigenvalues of <span class="math inline">\(\boldsymbol A\)</span>.
By construction <span class="math inline">\(f(\boldsymbol A)\)</span> is real, symmetric and has
real eigenvalues <span class="math inline">\(f(\lambda_i)\)</span>.</p>
<p>Examples:</p>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example A.2  </strong></span>Matrix power: <span class="math inline">\(f(a) = a^p\)</span> (with <span class="math inline">\(p\)</span> a real number)</p>
</div>
<p>Special cases of matrix power include :</p>
<ul>
<li>Matrix inversion: <span class="math inline">\(f(a) = a^{-1}\)</span><br>
Note that if the matrix <span class="math inline">\(\boldsymbol A\)</span> is singular, i.e. contains one or more eigenvalues <span class="math inline">\(\lambda_i=0\)</span>,
then <span class="math inline">\(\boldsymbol A^{-1}\)</span> is not defined and therefore <span class="math inline">\(\boldsymbol A\)</span> is not invertible.</li>
</ul>
<p>However, a so-called <em>pseudoinverse</em> can still be computed, by inverting the non-zero eigenvalues, and
keeping the zero eigenvalues as zero.</p>
<ul>
<li>Matrix square root: <span class="math inline">\(f(a) = a^{1/2}\)</span><br>
Since there are multiple solutions to the square root there are also multiple
matrix square roots. The principal matrix square root is obtained by using
the positive square roots of all the eigenvalues. Thus the <strong>principal matrix square root</strong>
of a positive semi-definite matrix is also positive semi-definite and it is unique.</li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example A.3  </strong></span>Matrix exponential: <span class="math inline">\(f(a) = \exp(a)\)</span><br>
Note that because <span class="math inline">\(\exp(a) \geq 0\)</span> for all real <span class="math inline">\(a\)</span> the matrix <span class="math inline">\(\exp(\boldsymbol A)\)</span> is positive
semi-definite. Thus, the matrix exponential can be used to generate positive semi-definite
matrices.</p>
<p>If <span class="math inline">\(\boldsymbol A\)</span> and <span class="math inline">\(\boldsymbol B\)</span> commute, i.e. if <span class="math inline">\(\boldsymbol A\boldsymbol B= \boldsymbol B\boldsymbol A\)</span>, then
<span class="math inline">\(\exp(\boldsymbol A+\boldsymbol B) = \exp(\boldsymbol A) \exp(\boldsymbol B)\)</span>. However, this is not the case
otherwise!</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-4" class="example"><strong>Example A.4  </strong></span>Matrix logarithm: <span class="math inline">\(f(a) = \log(a)\)</span><br>
As the logarithm requires <span class="math inline">\(a &gt;0\)</span> the matrix <span class="math inline">\(\boldsymbol A\)</span> needs to be positive definite
for <span class="math inline">\(\log(\boldsymbol A)\)</span> to be defined.</p>
</div>
</div>
<div id="identities-for-the-matrix-exponential-and-logarithm" class="section level3" number="7.9.2">
<h3>
<span class="header-section-number">A.9.2</span> Identities for the matrix exponential and logarithm<a class="anchor" aria-label="anchor" href="#identities-for-the-matrix-exponential-and-logarithm"><i class="fas fa-link"></i></a>
</h3>
<p>The above give rise to useful identities:</p>
<ol style="list-style-type: decimal">
<li><p>For any symmetric matrix <span class="math inline">\(\boldsymbol A\)</span> we have
<span class="math display">\[
\det(\exp(\boldsymbol A)) = \exp(\text{Tr}(\boldsymbol A))
\]</span>
because
<span class="math inline">\(\prod_i \exp(\lambda_i) = \exp( \sum_i \lambda_i)\)</span>
where <span class="math inline">\(\lambda_i\)</span> are the eigenvalues of <span class="math inline">\(\boldsymbol A\)</span>.</p></li>
<li><p>If we take the logarithm on both sides and replace <span class="math inline">\(\exp(\boldsymbol A)=\boldsymbol B\)</span> we get another
identity for a symmetric positive definite matrix <span class="math inline">\(\boldsymbol B\)</span>:
<span class="math display">\[
\log \det(\boldsymbol B) = \text{Tr}(\log(\boldsymbol B))
\]</span>
because
<span class="math inline">\(\log( \prod_i \lambda_i) = \sum_i \log(\lambda_i)\)</span>
where <span class="math inline">\(\lambda_i\)</span> are the eigenvalues of <span class="math inline">\(\boldsymbol B\)</span>.</p></li>
</ol>
</div>
</div>
<div id="matrix-calculus" class="section level2" number="7.10">
<h2>
<span class="header-section-number">A.10</span> Matrix calculus<a class="anchor" aria-label="anchor" href="#matrix-calculus"><i class="fas fa-link"></i></a>
</h2>
<div id="first-order-vector-derivatives" class="section level3" number="7.10.1">
<h3>
<span class="header-section-number">A.10.1</span> First order vector derivatives<a class="anchor" aria-label="anchor" href="#first-order-vector-derivatives"><i class="fas fa-link"></i></a>
</h3>
<div id="gradient" class="section level4" number="7.10.1.1">
<h4>
<span class="header-section-number">A.10.1.1</span> Gradient<a class="anchor" aria-label="anchor" href="#gradient"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>gradient</strong> of a scalar-valued function
<span class="math inline">\(h(\boldsymbol x)\)</span> with vector argument <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_d)^T\)</span>
is the vector containing the first order partial derivatives
of <span class="math inline">\(h(\boldsymbol x)\)</span> with regard to each <span class="math inline">\(x_1, \ldots, x_d\)</span>:
<span class="math display">\[
\begin{split}
\nabla h(\boldsymbol x) &amp;= \begin{pmatrix}
\frac{\partial h(\boldsymbol x)}{\partial x_1} \\
\vdots\\
\frac{\partial h(\boldsymbol x)}{\partial x_d}
\end{pmatrix}\\
&amp;=  \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} \\
&amp; = \text{grad } h(\boldsymbol x) \\
\end{split}
\]</span>
The symbol <span class="math inline">\(\nabla\)</span> is called the <strong>nabla operator</strong> (also known as <strong>del operator</strong>).</p>
<p>Note that we write the gradient as a <strong>column vector</strong>. This is called the
<strong>denominator layout</strong> convention, see <a href="https://en.wikipedia.org/wiki/Matrix_calculus" class="uri">https://en.wikipedia.org/wiki/Matrix_calculus</a> for details.
In contrast, many textbooks (and also earlier versions of these lecture notes) assume that gradients are row vectors, following the so-called numerator layout convention.</p>
<div class="example">
<p><span id="exm:gradientexamples" class="example"><strong>Example A.5  </strong></span>Examples for the gradient:</p>
<ul>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol a^T \boldsymbol x+ b\)</span>. Then <span class="math inline">\(\nabla h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol a\)</span>.</li>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol x^T \boldsymbol x\)</span>. Then <span class="math inline">\(\nabla h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = 2 \boldsymbol x\)</span>.</li>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol x^T \boldsymbol A\boldsymbol x\)</span>. Then <span class="math inline">\(\nabla h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = (\boldsymbol A+ \boldsymbol A^T) \boldsymbol x\)</span>.</li>
</ul>
</div>
</div>
<div id="jacobian-matrix" class="section level4" number="7.10.1.2">
<h4>
<span class="header-section-number">A.10.1.2</span> Jacobian matrix<a class="anchor" aria-label="anchor" href="#jacobian-matrix"><i class="fas fa-link"></i></a>
</h4>
<p>For a vector-valued function
<span class="math display">\[
\boldsymbol h(\boldsymbol x) = ( h_1(\boldsymbol x), \ldots, h_m(\boldsymbol x) )^T
\]</span>
we can also compute the vector derivative
<span class="math display">\[
\begin{split}
\frac{\partial \boldsymbol h(\boldsymbol x)}{\partial \boldsymbol x}  &amp;=
\begin{pmatrix}
\frac{\partial h_1(\boldsymbol x)}{\partial x_1} &amp; \cdots  &amp; \frac{\partial h_m(\boldsymbol x)}{\partial x_1} \\
\vdots &amp;\ddots &amp; \vdots \\
\frac{\partial h_1(\boldsymbol x)}{\partial x_d} &amp; \cdots &amp; \frac{\partial h_m(\boldsymbol x)}{\partial x_d} \\
\end{pmatrix} \\
&amp;=\left(\frac{\partial h_j(\boldsymbol x)}{\partial x_i}\right) \\
&amp; = \left(\nabla h_1(\boldsymbol x), \ldots, \nabla h_m(\boldsymbol x)  \right)
\end{split}
\]</span>
which yields a matrix whose columns contain the
gradient vectors of the component of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span>.</p>
<p>The transpose of this matrix is called the <strong>Jacobian matrix</strong> (of size <span class="math inline">\(m\)</span> rows and <span class="math inline">\(d\)</span> columns):
<span class="math display">\[
\begin{split}
J_{\boldsymbol h}(\boldsymbol x) &amp;=
\left( {\begin{array}{c}
\nabla h_1(\boldsymbol x)^T   \\
\vdots   \\
\nabla h_m(\boldsymbol x)^T  \\
\end{array} } \right) \\
&amp; = \left(\frac{\partial h_i(\boldsymbol x)}{\partial x_j}\right) \\
&amp; = \left( \frac{\partial \boldsymbol h(\boldsymbol x)}{\partial \boldsymbol x}   \right)^T
\end{split}
\]</span>
Note that by convention the Jacobian matrix contains the gradients in its rows.</p>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example A.6  </strong></span><span class="math inline">\(\boldsymbol h(\boldsymbol x)=\boldsymbol A^T \boldsymbol x+ \boldsymbol b\)</span>. Then <span class="math inline">\(\frac{\partial \boldsymbol h(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol A\)</span>
and <span class="math inline">\(J_{\boldsymbol h}(\boldsymbol x) =\boldsymbol A^T\)</span>.</p>
</div>
<p>If <span class="math inline">\(m=d\)</span> then the Jacobian matrix is a square matrix and this allows to compute the
<strong>Jacobian determinant</strong> <span class="math inline">\(\det J_{\boldsymbol h}(\boldsymbol x)\)</span>.</p>
<p>Both the Jacobian matrix and the Jacobian determinant are often called simply “the Jacobian”.</p>
<p>If <span class="math inline">\(\boldsymbol y= \boldsymbol h(\boldsymbol x)\)</span> is an invertible function with <span class="math inline">\(\boldsymbol x= \boldsymbol h^{-1}(\boldsymbol y)\)</span>
then the Jacobian matrix is invertible and the inverted matrix is in fact the
Jacobian of the inverse function!</p>
<p>This allows to compute the Jacobian determinant of the backtransformation as
as the inverse of the Jacobian determinant the original function:
<span class="math display">\[\det  J_{\boldsymbol h^{-1}}(\boldsymbol y) = ( \det  J_{\boldsymbol h}(\boldsymbol x) )^{-1}\]</span>
or in alternative notation
<span class="math display">\[\det  J_{\boldsymbol x}(\boldsymbol y) = \frac{1}{ \det J_{\boldsymbol y}(\boldsymbol x) }\]</span></p>
</div>
</div>
<div id="second-order-vector-derivatives" class="section level3" number="7.10.2">
<h3>
<span class="header-section-number">A.10.2</span> Second order vector derivatives<a class="anchor" aria-label="anchor" href="#second-order-vector-derivatives"><i class="fas fa-link"></i></a>
</h3>
<p>The matrix of all second order partial derivates of scalar-valued
function with vector-valued argument is called the <strong>Hessian matrix</strong>:
<span class="math display">\[
\begin{split}
\nabla \nabla^T h(\boldsymbol x) &amp;=
\begin{pmatrix}
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_1^2}
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_1 \partial x_2}
     &amp; \cdots
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_1 \partial x_d} \\
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_2 \partial x_1}
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_2^2}
     &amp; \cdots
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_2 \partial x_d} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_d \partial x_1}
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_d \partial x_2}  
     &amp; \cdots
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_d^2}
\end{pmatrix} \\
&amp;= \left(\frac{\partial h(\boldsymbol x)}{\partial x_i \partial x_j}\right) \\
&amp; = \frac{\partial^2 h(\boldsymbol x)}{\partial \boldsymbol x\partial \boldsymbol x^T} \\
\end{split}
\]</span>
By construction the Hessian matrix is square and symmetric.</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example A.7  </strong></span><span class="math inline">\(h(\boldsymbol x)=\boldsymbol x^T \boldsymbol A\boldsymbol x\)</span>. Then <span class="math inline">\(\nabla \nabla^T h(\boldsymbol x) = \frac{\partial^2 h(\boldsymbol x)}{\partial \boldsymbol x\partial \boldsymbol x^T} = (\boldsymbol A+ \boldsymbol A^T)\)</span>.</p>
</div>
</div>
<div id="first-order-matrix-derivatives" class="section level3" number="7.10.3">
<h3>
<span class="header-section-number">A.10.3</span> First order matrix derivatives<a class="anchor" aria-label="anchor" href="#first-order-matrix-derivatives"><i class="fas fa-link"></i></a>
</h3>
<p>The derivative of a scalar-valued function <span class="math inline">\(f(\boldsymbol X)\)</span> with regard to a matrix argument <span class="math inline">\(\boldsymbol X\)</span>
can also be defined and results in a matrix. Below you find some matrix calculus rules (written in
<strong>denominator layout</strong> convention). See
<a href="https://en.wikipedia.org/wiki/Matrix_calculus" class="uri">https://en.wikipedia.org/wiki/Matrix_calculus</a> for further examples.</p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example A.8  </strong></span><span class="math inline">\(\frac{\partial \text{Tr}(\boldsymbol A^T \boldsymbol X)}{\partial \boldsymbol X} = \boldsymbol A\)</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example A.9  </strong></span><span class="math inline">\(\frac{\partial \text{Tr}(\boldsymbol A^T \boldsymbol X\boldsymbol B)}{\partial \boldsymbol X} = \boldsymbol A\boldsymbol B^T\)</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example A.10  </strong></span><span class="math inline">\(\frac{\partial \text{Tr}(\boldsymbol X^T \boldsymbol A\boldsymbol X)}{\partial \boldsymbol X} = (\boldsymbol A+ \boldsymbol A^T) \boldsymbol X\)</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example A.11  </strong></span><span class="math inline">\(\frac{\partial \log \det(\boldsymbol X)}{\partial \boldsymbol X} = \frac{\partial \text{Tr}(\log \boldsymbol X)}{\partial \boldsymbol X} = (\boldsymbol X^{-1})^T\)</span></p>
</div>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="nonlinear-and-nonparametric-models.html"><span class="header-section-number">6</span> Nonlinear and nonparametric models</a></div>
<div class="next"><a href="further-study.html"><span class="header-section-number">B</span> Further study</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#brief-refresher-on-matrices"><span class="header-section-number">A</span> Brief refresher on matrices</a></li>
<li>
<a class="nav-link" href="#matrix-basics"><span class="header-section-number">A.1</span> Matrix basics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#matrix-notation"><span class="header-section-number">A.1.1</span> Matrix notation</a></li>
<li><a class="nav-link" href="#random-matrix"><span class="header-section-number">A.1.2</span> Random matrix</a></li>
<li><a class="nav-link" href="#special-matrices"><span class="header-section-number">A.1.3</span> Special matrices</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#simple-matrix-operations"><span class="header-section-number">A.2</span> Simple matrix operations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#matrix-addition-and-multiplication"><span class="header-section-number">A.2.1</span> Matrix addition and multiplication</a></li>
<li><a class="nav-link" href="#matrix-transpose"><span class="header-section-number">A.2.2</span> Matrix transpose</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#matrix-summaries"><span class="header-section-number">A.3</span> Matrix summaries</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#row-column-and-grand-sum"><span class="header-section-number">A.3.1</span> Row, column and grand sum</a></li>
<li><a class="nav-link" href="#matrix-trace"><span class="header-section-number">A.3.2</span> Matrix trace</a></li>
<li><a class="nav-link" href="#row-column-and-grand-sum-of-squares"><span class="header-section-number">A.3.3</span> Row, column and grand sum of squares</a></li>
<li><a class="nav-link" href="#sum-of-squared-diagonal-entries"><span class="header-section-number">A.3.4</span> Sum of squared diagonal entries</a></li>
<li><a class="nav-link" href="#frobenius-inner-product"><span class="header-section-number">A.3.5</span> Frobenius inner product</a></li>
<li><a class="nav-link" href="#euclidean-norm"><span class="header-section-number">A.3.6</span> Euclidean norm</a></li>
<li><a class="nav-link" href="#determinant-of-a-matrix"><span class="header-section-number">A.3.7</span> Determinant of a matrix</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#matrix-inverse"><span class="header-section-number">A.4</span> Matrix inverse</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#inversion-of-square-matrix"><span class="header-section-number">A.4.1</span> Inversion of square matrix</a></li>
<li><a class="nav-link" href="#inversion-of-structured-matrices"><span class="header-section-number">A.4.2</span> Inversion of structured matrices</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#orthogonal-matrices"><span class="header-section-number">A.5</span> Orthogonal matrices</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#properties-1"><span class="header-section-number">A.5.1</span> Properties</a></li>
<li><a class="nav-link" href="#semi-orthogonal-matrices"><span class="header-section-number">A.5.2</span> Semi-orthogonal matrices</a></li>
<li><a class="nav-link" href="#generating-orthogonal-matrices"><span class="header-section-number">A.5.3</span> Generating orthogonal matrices</a></li>
<li><a class="nav-link" href="#permutation-matrix"><span class="header-section-number">A.5.4</span> Permutation matrix</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#eigenvalues-and-eigenvectors"><span class="header-section-number">A.6</span> Eigenvalues and eigenvectors</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#definition"><span class="header-section-number">A.6.1</span> Definition</a></li>
<li><a class="nav-link" href="#finding-eigenvalues-and-vectors"><span class="header-section-number">A.6.2</span> Finding eigenvalues and vectors</a></li>
<li><a class="nav-link" href="#eigenequation-in-matrix-notation"><span class="header-section-number">A.6.3</span> Eigenequation in matrix notation</a></li>
<li><a class="nav-link" href="#permutation-of-eigenvalues"><span class="header-section-number">A.6.4</span> Permutation of eigenvalues</a></li>
<li><a class="nav-link" href="#similar-matrices"><span class="header-section-number">A.6.5</span> Similar matrices</a></li>
<li><a class="nav-link" href="#defective-matrix"><span class="header-section-number">A.6.6</span> Defective matrix</a></li>
<li><a class="nav-link" href="#eigenvalues-of-a-diagonal-or-triangular-matrix"><span class="header-section-number">A.6.7</span> Eigenvalues of a diagonal or triangular matrix</a></li>
<li><a class="nav-link" href="#eigenvalues-and-vectors-of-a-symmetric-matrix"><span class="header-section-number">A.6.8</span> Eigenvalues and vectors of a symmetric matrix</a></li>
<li><a class="nav-link" href="#eigenvalues-of-orthogonal-matrices"><span class="header-section-number">A.6.9</span> Eigenvalues of orthogonal matrices</a></li>
<li><a class="nav-link" href="#positive-definite-matrices"><span class="header-section-number">A.6.10</span> Positive definite matrices</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#matrix-decompositions"><span class="header-section-number">A.7</span> Matrix decompositions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#diagonalisation-and-eigenvalue-decomposition"><span class="header-section-number">A.7.1</span> Diagonalisation and eigenvalue decomposition</a></li>
<li><a class="nav-link" href="#orthogonal-eigenvalue-decomposition"><span class="header-section-number">A.7.2</span> Orthogonal eigenvalue decomposition</a></li>
<li><a class="nav-link" href="#singular-value-decomposition"><span class="header-section-number">A.7.3</span> Singular value decomposition</a></li>
<li><a class="nav-link" href="#polar-decomposition"><span class="header-section-number">A.7.4</span> Polar decomposition</a></li>
<li><a class="nav-link" href="#cholesky-decomposition"><span class="header-section-number">A.7.5</span> Cholesky decomposition</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#matrix-summaries-based-on-eigenvalues-and-singular-values"><span class="header-section-number">A.8</span> Matrix summaries based on eigenvalues and singular values</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#trace-and-determinant-computed-from-eigenvalues"><span class="header-section-number">A.8.1</span> Trace and determinant computed from eigenvalues</a></li>
<li><a class="nav-link" href="#eigenvalues-of-a-squared-matrix"><span class="header-section-number">A.8.2</span> Eigenvalues of a squared matrix</a></li>
<li><a class="nav-link" href="#rank-and-condition-number"><span class="header-section-number">A.8.3</span> Rank and condition number</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#functions-of-symmetric-matrices"><span class="header-section-number">A.9</span> Functions of symmetric matrices</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#definition-of-a-matrix-function"><span class="header-section-number">A.9.1</span> Definition of a matrix function</a></li>
<li><a class="nav-link" href="#identities-for-the-matrix-exponential-and-logarithm"><span class="header-section-number">A.9.2</span> Identities for the matrix exponential and logarithm</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#matrix-calculus"><span class="header-section-number">A.10</span> Matrix calculus</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#first-order-vector-derivatives"><span class="header-section-number">A.10.1</span> First order vector derivatives</a></li>
<li><a class="nav-link" href="#second-order-vector-derivatives"><span class="header-section-number">A.10.2</span> Second order vector derivatives</a></li>
<li><a class="nav-link" href="#first-order-matrix-derivatives"><span class="header-section-number">A.10.3</span> First order matrix derivatives</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Multivariate Statistics and Machine Learning</strong>" was written by Korbinian Strimmer. It was last built on 1 August 2023.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
