<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Transformations and dimension reduction – Multivariate Statistics and Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04-clustering.html" rel="next">
<link href="./02-estimation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-transformations.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations and dimension reduction</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Multivariate Statistics and Machine Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-multivariate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Multivariate random variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multivariate estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-transformations.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations and dimension reduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised learning and clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning and classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-dependence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multivariate dependencies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear and nonparametric models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-further-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Further study</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#linear-transformations" id="toc-linear-transformations" class="nav-link active" data-scroll-target="#linear-transformations"><span class="header-section-number">3.1</span> Linear Transformations</a>
  <ul class="collapse">
  <li><a href="#location-scale-transformation" id="toc-location-scale-transformation" class="nav-link" data-scroll-target="#location-scale-transformation">Location-scale transformation</a></li>
  <li><a href="#squared-multiple-correlation" id="toc-squared-multiple-correlation" class="nav-link" data-scroll-target="#squared-multiple-correlation">Squared multiple correlation</a></li>
  <li><a href="#invertible-location-scale-transformation" id="toc-invertible-location-scale-transformation" class="nav-link" data-scroll-target="#invertible-location-scale-transformation">Invertible location-scale transformation</a></li>
  <li><a href="#transformation-of-a-density-under-an-invertible-location-scale-transformation" id="toc-transformation-of-a-density-under-an-invertible-location-scale-transformation" class="nav-link" data-scroll-target="#transformation-of-a-density-under-an-invertible-location-scale-transformation">Transformation of a density under an invertible location-scale transformation:</a></li>
  </ul></li>
  <li><a href="#nonlinear-transformations" id="toc-nonlinear-transformations" class="nav-link" data-scroll-target="#nonlinear-transformations"><span class="header-section-number">3.2</span> Nonlinear transformations</a>
  <ul class="collapse">
  <li><a href="#general-transformation" id="toc-general-transformation" class="nav-link" data-scroll-target="#general-transformation">General transformation</a></li>
  <li><a href="#delta-method" id="toc-delta-method" class="nav-link" data-scroll-target="#delta-method">Delta method</a></li>
  <li><a href="#transformation-of-a-probability-density-function-under-a-general-invertible-transformation" id="toc-transformation-of-a-probability-density-function-under-a-general-invertible-transformation" class="nav-link" data-scroll-target="#transformation-of-a-probability-density-function-under-a-general-invertible-transformation">Transformation of a probability density function under a general invertible transformation</a></li>
  <li><a href="#normalising-flows" id="toc-normalising-flows" class="nav-link" data-scroll-target="#normalising-flows">Normalising flows</a></li>
  </ul></li>
  <li><a href="#general-whitening-transformations" id="toc-general-whitening-transformations" class="nav-link" data-scroll-target="#general-whitening-transformations"><span class="header-section-number">3.3</span> General whitening transformations</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#whitening-transformation-and-whitening-constraint" id="toc-whitening-transformation-and-whitening-constraint" class="nav-link" data-scroll-target="#whitening-transformation-and-whitening-constraint">Whitening transformation and whitening constraint</a></li>
  <li><a href="#parameterisation-of-whitening-matrix" id="toc-parameterisation-of-whitening-matrix" class="nav-link" data-scroll-target="#parameterisation-of-whitening-matrix">Parameterisation of whitening matrix</a></li>
  <li><a href="#cross-covariance-and-cross-correlation-for-general-whitening-transformations" id="toc-cross-covariance-and-cross-correlation-for-general-whitening-transformations" class="nav-link" data-scroll-target="#cross-covariance-and-cross-correlation-for-general-whitening-transformations">Cross-covariance and cross-correlation for general whitening transformations</a></li>
  <li><a href="#inverse-whitening-transformation-and-loadings" id="toc-inverse-whitening-transformation-and-loadings" class="nav-link" data-scroll-target="#inverse-whitening-transformation-and-loadings">Inverse whitening transformation and loadings</a></li>
  <li><a href="#summaries-of-cross-covariance-boldsymbol-phi-and-cross-correlation-boldsymbol-psi-resulting-from-whitening-transformations" id="toc-summaries-of-cross-covariance-boldsymbol-phi-and-cross-correlation-boldsymbol-psi-resulting-from-whitening-transformations" class="nav-link" data-scroll-target="#summaries-of-cross-covariance-boldsymbol-phi-and-cross-correlation-boldsymbol-psi-resulting-from-whitening-transformations">Summaries of cross-covariance <span class="math inline">\(\boldsymbol \Phi\)</span> and cross-correlation <span class="math inline">\(\boldsymbol \Psi\)</span> resulting from whitening transformations</a></li>
  </ul></li>
  <li><a href="#natural-whitening-procedures" id="toc-natural-whitening-procedures" class="nav-link" data-scroll-target="#natural-whitening-procedures"><span class="header-section-number">3.4</span> Natural whitening procedures</a>
  <ul class="collapse">
  <li><a href="#zca-whitening" id="toc-zca-whitening" class="nav-link" data-scroll-target="#zca-whitening">ZCA whitening</a></li>
  <li><a href="#zca-cor-whitening" id="toc-zca-cor-whitening" class="nav-link" data-scroll-target="#zca-cor-whitening">ZCA-Cor whitening</a></li>
  <li><a href="#pca-whitening" id="toc-pca-whitening" class="nav-link" data-scroll-target="#pca-whitening">PCA whitening</a></li>
  <li><a href="#pca-cor-whitening" id="toc-pca-cor-whitening" class="nav-link" data-scroll-target="#pca-cor-whitening">PCA-cor whitening</a></li>
  <li><a href="#cholesky-whitening" id="toc-cholesky-whitening" class="nav-link" data-scroll-target="#cholesky-whitening">Cholesky whitening</a></li>
  <li><a href="#comparison-of-whitening-procedures---simulated-data" id="toc-comparison-of-whitening-procedures---simulated-data" class="nav-link" data-scroll-target="#comparison-of-whitening-procedures---simulated-data">Comparison of whitening procedures - simulated data</a></li>
  <li><a href="#comparison-of-whitening-procedures---iris-flowers" id="toc-comparison-of-whitening-procedures---iris-flowers" class="nav-link" data-scroll-target="#comparison-of-whitening-procedures---iris-flowers">Comparison of whitening procedures - iris flowers</a></li>
  <li><a href="#recap" id="toc-recap" class="nav-link" data-scroll-target="#recap">Recap</a></li>
  </ul></li>
  <li><a href="#principal-component-analysis-pca" id="toc-principal-component-analysis-pca" class="nav-link" data-scroll-target="#principal-component-analysis-pca"><span class="header-section-number">3.5</span> Principal Component Analysis (PCA)</a>
  <ul class="collapse">
  <li><a href="#pca-transformation" id="toc-pca-transformation" class="nav-link" data-scroll-target="#pca-transformation">PCA transformation</a></li>
  <li><a href="#application-to-data" id="toc-application-to-data" class="nav-link" data-scroll-target="#application-to-data">Application to data</a></li>
  <li><a href="#iris-flower-data-example" id="toc-iris-flower-data-example" class="nav-link" data-scroll-target="#iris-flower-data-example">Iris flower data example</a></li>
  <li><a href="#pca-correlation-loadings" id="toc-pca-correlation-loadings" class="nav-link" data-scroll-target="#pca-correlation-loadings">PCA correlation loadings</a></li>
  <li><a href="#pca-correlation-loadings-plot" id="toc-pca-correlation-loadings-plot" class="nav-link" data-scroll-target="#pca-correlation-loadings-plot">PCA correlation loadings plot</a></li>
  <li><a href="#outlook" id="toc-outlook" class="nav-link" data-scroll-target="#outlook">Outlook</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformations and dimension reduction</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In the following we study transformations of random vectors and their distributions. These transformation are very important since they either transform simple distributions into more complex distributions or allow to simplify complex models. Futhermore, they enable dimension reduction. We first consider affine transformation, and then also nonlinear transformations.</p>
<section id="linear-transformations" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="linear-transformations"><span class="header-section-number">3.1</span> Linear Transformations</h2>
<section id="location-scale-transformation" class="level3">
<h3 class="anchored" data-anchor-id="location-scale-transformation">Location-scale transformation</h3>
<p>Also known as affine transformation.</p>
<p><span class="math display">\[\boldsymbol y= \underbrace{\boldsymbol a}_{\text{location parameter}}+\underbrace{\boldsymbol B}_{\text{scale parameter}} \boldsymbol x\space\]</span> <span class="math display">\[\boldsymbol y: m \times 1 \text{ random vector}\]</span> <span class="math display">\[\boldsymbol a: m \times 1 \text{ vector, location parameter}\]</span> <span class="math display">\[\boldsymbol B: m \times d \text{ matrix, scale parameter },  m \geq 1\]</span> <span class="math display">\[\boldsymbol x: d \times 1 \text{ random vector}\]</span></p>
<p><strong>Mean and variance:</strong></p>
<p>Mean and variance of the original vector <span class="math inline">\(\boldsymbol x\)</span>:</p>
<p><span class="math display">\[\text{E}(\boldsymbol x)=\boldsymbol \mu_{\boldsymbol x}\]</span> <span class="math display">\[\text{Var}(\boldsymbol x)=\boldsymbol \Sigma_{\boldsymbol x}\]</span></p>
<p>Mean and variance of the transformed random vector <span class="math inline">\(\boldsymbol y\)</span>:</p>
<p><span class="math display">\[\text{E}(\boldsymbol y)=\boldsymbol a+ \boldsymbol B\boldsymbol \mu_{\boldsymbol x}\]</span> <span class="math display">\[\text{Var}(\boldsymbol y)= \boldsymbol B\boldsymbol \Sigma_{\boldsymbol x} \boldsymbol B^T\]</span></p>
<p><strong>Cross-covariance and cross-correlation:</strong></p>
<p>Cross-covariance <span class="math inline">\(\boldsymbol \Phi= \Sigma_{\boldsymbol x\boldsymbol y} =  \text{Cov}(\boldsymbol x, \boldsymbol y)\)</span> between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>: <span class="math display">\[
\boldsymbol \Phi= \text{Cov}(\boldsymbol x, \boldsymbol B\boldsymbol x) = \boldsymbol \Sigma_{\boldsymbol x}  \boldsymbol B^T
\]</span> Note that <span class="math inline">\(\boldsymbol \Phi\)</span> is a matrix of dimensions <span class="math inline">\(d \times m\)</span> as the dimension of <span class="math inline">\(\boldsymbol x\)</span> is <span class="math inline">\(d\)</span> and the dimension of <span class="math inline">\(\boldsymbol y\)</span> is <span class="math inline">\(m\)</span>.</p>
<p>Cross-correlation <span class="math inline">\(\boldsymbol \Psi= \boldsymbol P_{\boldsymbol x\boldsymbol y} = \text{Cor}(\boldsymbol x, \boldsymbol y)\)</span> between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>: <span class="math display">\[
\boldsymbol \Psi= \boldsymbol V_{\boldsymbol x}^{-1/2}  \boldsymbol \Phi\boldsymbol V_{\boldsymbol y}^{-1/2}
\]</span> where <span class="math inline">\(\boldsymbol V_{\boldsymbol x} = \text{Diag}(\boldsymbol \Sigma_{\boldsymbol x})\)</span> and <span class="math inline">\(\boldsymbol V_{\boldsymbol y} = \text{Diag}(\boldsymbol B\boldsymbol \Sigma_{\boldsymbol x}  \boldsymbol B^T)\)</span> are diagonal matrices containing the variances for the components of <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>. The dimensions of the matrix <span class="math inline">\(\boldsymbol \Psi\)</span> are also <span class="math inline">\(d \times m\)</span>.</p>
<p>Special cases/examples:</p>
<div id="exm-univartrans" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1</strong></span> Univariate case (<span class="math inline">\(d=1, m=1\)</span>): <span class="math inline">\(y=a + b x\)</span></p>
<ul>
<li><span class="math inline">\(\text{E}(y)=a+b\mu\)</span></li>
<li><span class="math inline">\(\text{Var}(y)=b^2\sigma^2\)</span></li>
<li><span class="math inline">\(\text{Cov}(y, x) = b\sigma^2\)</span></li>
<li><span class="math inline">\(\text{Cor}(y, x) = \frac{b \sigma^2}{\sqrt{b^2\sigma^2} \sqrt{\sigma^2}  } =1\)</span></li>
</ul>
<p>Note that <span class="math inline">\(y\)</span> can predicted perfectly from <span class="math inline">\(x\)</span> as <span class="math inline">\(\text{Cor}(y, x)=1\)</span>. This is because there is no error term in the transformation. See also the more general case with multiple correlation further below.</p>
</div>
<div id="exm-sumunivariate" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2</strong></span> Sum of two random univariate variables: <span class="math inline">\(y = x_1 + x_2\)</span>, i.e.&nbsp;<span class="math inline">\(a=0\)</span> and <span class="math inline">\(\boldsymbol B=(1,1)\)</span></p>
<ul>
<li><span class="math inline">\(\text{E}(y) = \text{E}(x_1+x_2)=\mu_1+\mu_2\)</span></li>
<li><span class="math inline">\(\text{Var}(y) = \text{Var}(x_1+x_2) = (1,1)\begin{pmatrix}
\sigma^2_1 &amp; \sigma_{12}\\
\sigma_{12} &amp; \sigma^2_2
\end{pmatrix} \begin{pmatrix}
1\\
1
\end{pmatrix} = \sigma^2_1+\sigma^2_2+2\sigma_{12} = \text{Var}(x_1)+\text{Var}(x_2)+2\,\text{Cov}(x_1,x_2)\)</span></li>
</ul>
</div>
<div id="exm-transformcov" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3</strong></span> <span class="math inline">\(y_1=a_1+b_1 x_1\)</span> and <span class="math inline">\(y_2=a_2+b_2 x_2\)</span>, i.e.&nbsp;<span class="math inline">\(\boldsymbol a= \begin{pmatrix} a_1\\ a_2 \end{pmatrix}\)</span> and<br>
<span class="math inline">\(\boldsymbol B= \begin{pmatrix}b_1 &amp; 0\\ 0 &amp; b_2\end{pmatrix}\)</span></p>
<ul>
<li><span class="math inline">\(\text{E}(\boldsymbol y)= \begin{pmatrix}  a_1\\ a_2 \end{pmatrix} +  \begin{pmatrix}b_1 &amp; 0\\ 0 &amp; b_2\end{pmatrix}
\begin{pmatrix} \mu_1 \\ \mu_2\end{pmatrix}
  = \begin{pmatrix} a_1+b_1 \mu_1\\ a_2+b_2 \mu_2 \end{pmatrix}\)</span><br>
</li>
<li><span class="math inline">\(\text{Var}(\boldsymbol y) = \begin{pmatrix} b_1 &amp; 0\\ 0 &amp; b_2 \end{pmatrix}
   \begin{pmatrix}
\sigma^2_1 &amp; \sigma_{12}\\
\sigma_{12} &amp; \sigma^2_2
\end{pmatrix}
\begin{pmatrix} b_1 &amp; 0\\ 0 &amp; b_2 \end{pmatrix} =
\begin{pmatrix}
b^2_1\sigma^2_1 &amp; b_1b_2\sigma_{12}\\
b_1b_2\sigma_{12} &amp; b^2_2\sigma^2_2
\end{pmatrix}\)</span><br>
note that <span class="math inline">\(\text{Cov}(y_1, y_2) = b_1 b_2\text{Cov}(x_1,x_2)\)</span></li>
</ul>
</div>
</section>
<section id="squared-multiple-correlation" class="level3">
<h3 class="anchored" data-anchor-id="squared-multiple-correlation">Squared multiple correlation</h3>
<p>Squared multiple correlation <span class="math inline">\(\text{MCor}(y, \boldsymbol x)^2\)</span> is a scalar measure summarising the linear association between a scalar response variable <span class="math inline">\(y\)</span> and a set of predictors <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_d)^T\)</span>. It is defined as <span class="math display">\[
\begin{split}
\text{MCor}(y, \boldsymbol x)^2 &amp;= \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy} / \sigma^2_y\\
&amp;=\boldsymbol P_{y \boldsymbol x} \boldsymbol P_{ \boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy}\\
\end{split}
\]</span> If <span class="math inline">\(y\)</span> can be perfectly linearly predicted by <span class="math inline">\(\boldsymbol x\)</span> then <span class="math inline">\(\text{MCor}(y, \boldsymbol x)^2 = 1\)</span>.</p>
<p>The empirical estimate of <span class="math inline">\(\text{MCor}(y, \boldsymbol x)^2\)</span> is the <span class="math inline">\(R^2\)</span> coefficient that you will find in any software for linear regression.</p>
<div id="exm-mcoraffine" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.4</strong></span> Squared multiple correlation for an affine transformation.</p>
<p>Since we linearly transform <span class="math inline">\(\boldsymbol x\)</span> into <span class="math inline">\(\boldsymbol y\)</span> with no additional error involved we expect that for each component <span class="math inline">\(y_i\)</span> in <span class="math inline">\(\boldsymbol y\)</span> we have <span class="math inline">\(\text{MCor}(y_i, \boldsymbol x)^2=1\)</span>. This can be shown directly by computing <span class="math display">\[
\begin{split}
\left(\text{MCor}(y_1, \boldsymbol x)^2, \ldots, \text{MCor}(y_m, \boldsymbol x)^2 \right)^T
&amp;=\text{Diag}\left(\boldsymbol \Sigma_{\boldsymbol y\boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y}  \right) / \text{Diag}\left( \boldsymbol \Sigma_{\boldsymbol y} \right) \\
&amp;= \text{Diag}\left(\boldsymbol B\boldsymbol \Sigma_{\boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol x} \boldsymbol B^T \right) / \text{Diag}\left( \boldsymbol B\boldsymbol \Sigma_{\boldsymbol x} \boldsymbol B^T \right) \\
&amp;= \text{Diag}\left(\boldsymbol B\boldsymbol \Sigma_{\boldsymbol x} \boldsymbol B^T \right) / \text{Diag}\left( \boldsymbol B\boldsymbol \Sigma_{\boldsymbol x} \boldsymbol B^T \right) \\
&amp;=\left(1, \ldots, 1 \right)^T\\
\end{split}
\]</span></p>
</div>
</section>
<section id="invertible-location-scale-transformation" class="level3">
<h3 class="anchored" data-anchor-id="invertible-location-scale-transformation">Invertible location-scale transformation</h3>
<p>If <span class="math inline">\(m=d\)</span> (square <span class="math inline">\(\boldsymbol B\)</span>) and <span class="math inline">\(\det(\boldsymbol B) \neq 0\)</span> then the affine transformation is <strong>invertible</strong>.</p>
<p>Forward transformation: <span class="math display">\[\boldsymbol y= \boldsymbol a+ \boldsymbol B\boldsymbol x\]</span></p>
<p>Back transformation: <span class="math display">\[\boldsymbol x= \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\]</span></p>
<p>Invertible transformations thus provide a one-to-one map between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>.</p>
<div id="exm-orthotrans" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.5</strong></span> <strong>Orthogonal transformation</strong></p>
<p>Setting <span class="math inline">\(\boldsymbol a=0\)</span> and <span class="math inline">\(\boldsymbol B=\boldsymbol Q\)</span> to an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> yields an orthogonal transformation. The inverse transformation is given by setting <span class="math inline">\(\boldsymbol B^{-1} = \boldsymbol Q^T\)</span>.</p>
<p>Assume that <span class="math inline">\(\boldsymbol x\)</span> has a positive definite covariance matrix <span class="math inline">\(\text{Var}(\boldsymbol x)=\boldsymbol \Sigma_{\boldsymbol x}\)</span> with eigenvalue decomposition <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x} = \boldsymbol U_1 \boldsymbol \Lambda\boldsymbol U_1^T\)</span>. After orthogonal transformation <span class="math inline">\(\boldsymbol y= \boldsymbol Q\boldsymbol x\)</span> the covariance matrix for <span class="math inline">\(\boldsymbol y\)</span> is <span class="math inline">\(\text{Var}(\boldsymbol y)= \boldsymbol \Sigma_{\boldsymbol y} = \boldsymbol Q\boldsymbol \Sigma_{\boldsymbol x} \boldsymbol Q^T = \boldsymbol Q\boldsymbol U_1 \boldsymbol \Lambda\boldsymbol U_1^T\boldsymbol Q^T = \boldsymbol U_2 \boldsymbol \Lambda\boldsymbol U_2^T\)</span> where <span class="math inline">\(\boldsymbol U_2 = \boldsymbol Q\boldsymbol U_1\)</span> is another orthogonal matrix. This shows that an orthogonal transformation reorientates the principal axes of the ellipse corresponding to covariance matrix, without changing the shape of the ellipse itself as the eigenvalues stay the same.</p>
<p>If you set <span class="math inline">\(\boldsymbol Q=\boldsymbol U_1^T\)</span> then <span class="math inline">\(\boldsymbol U_2= \boldsymbol I\)</span> and the reoriented principal axes are now parallel to the coordinate axes. This special type of orthogonal transformation is called <strong>principal component analysis</strong> (PCA) transformation. We revisit PCA in a later chapter.</p>
</div>
<div id="exm-whitetrans" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.6</strong></span> <strong>Whitening transformation</strong></p>
<p>Assume that <span class="math inline">\(\boldsymbol x\)</span> has a positive definite covariance matrix <span class="math inline">\(\text{Var}(\boldsymbol x)=\boldsymbol \Sigma_{\boldsymbol x}\)</span>. The <strong>inverse principal matrix square root</strong> is denoted by <span class="math inline">\(\boldsymbol \Sigma^{-1/2}_{\boldsymbol x}\)</span>. This can be obtained by eigendecomposition of <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x} = \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span> so that <span class="math inline">\(\boldsymbol \Sigma^{-1/2}_{\boldsymbol x} =\boldsymbol U\boldsymbol \Lambda^{-1/2} \boldsymbol U^T\)</span>.</p>
<p>Setting <span class="math inline">\(\boldsymbol a=0\)</span> and <span class="math inline">\(\boldsymbol B=\boldsymbol Q\boldsymbol \Sigma_{\boldsymbol x}^{-1/2}\)</span> where <span class="math inline">\(\boldsymbol Q\)</span> is an orthogonal matrix yields the covariance-based parameterisation of the general whitening transformation. The matrix <span class="math inline">\(\boldsymbol B\)</span> is called the <em>whitening matrix</em> and is also often denoted by <span class="math inline">\(\boldsymbol W\)</span>.<br>
The inverse transformation is given by setting <span class="math inline">\(\boldsymbol B^{-1} = \boldsymbol \Sigma_{\boldsymbol x}^{1/2} \boldsymbol Q^T\)</span>.</p>
<p>After transformation <span class="math inline">\(\boldsymbol y= \boldsymbol Q\boldsymbol \Sigma_{\boldsymbol x}^{-1/2} \boldsymbol x\)</span> the covariance matrix for <span class="math inline">\(\boldsymbol y\)</span> is <span class="math inline">\(\text{Var}(\boldsymbol y)= \boldsymbol \Sigma_{\boldsymbol y} = \boldsymbol Q\boldsymbol \Sigma_{\boldsymbol x}^{-1/2} \boldsymbol \Sigma_{\boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x}^{-1/2} \boldsymbol Q^T = \boldsymbol Q\boldsymbol Q^T = \boldsymbol I\)</span>, hence the name of the transformation. Whitening transformations are discussed in detail later.</p>
</div>
<div id="exm-mahatrans" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.7</strong></span> <strong>Mahalanobis transform</strong></p>
</div>
<p>We assume <span class="math inline">\(\text{E}(\boldsymbol x)=\boldsymbol \mu_{\boldsymbol x}\)</span> and a positive definite covariance matrix <span class="math inline">\(\text{Var}(\boldsymbol x)=\boldsymbol \Sigma_{\boldsymbol x}\)</span> with <span class="math inline">\(\det(\boldsymbol \Sigma_{\boldsymbol x}) &gt; 0\)</span>.</p>
<p>The Mahalanobis transformation is given by <span class="math display">\[
\boldsymbol y=\boldsymbol \Sigma^{-1/2}_{\boldsymbol x}(\boldsymbol x-\boldsymbol \mu_{\boldsymbol x})
\]</span> This corresponds to an affine transformation with <span class="math inline">\(\boldsymbol a= - \boldsymbol \Sigma^{-1/2}_{\boldsymbol x} \boldsymbol \mu_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol B= \boldsymbol \Sigma^{-1/2}_{\boldsymbol x}\)</span>.</p>
<p>The mean and the variance of <span class="math inline">\(\boldsymbol y\)</span> becomes <span class="math display">\[
\text{E}(\boldsymbol y) = \mathbf 0\]</span> and <span class="math display">\[\text{Var}(\boldsymbol y) = \boldsymbol I_d\]</span>.</p>
<p>The Mahalanobis transforms performs three functions:</p>
<ol type="1">
<li>Centering (<span class="math inline">\(-\boldsymbol \mu\)</span>)</li>
<li>Standardisation <span class="math inline">\(\text{Var}(y_i)=1\)</span></li>
<li>Decorrelation <span class="math inline">\(\text{Cor}(y_i,y_j)=0\)</span> for <span class="math inline">\(i \neq j\)</span></li>
</ol>
<p>In the <strong>univariate case (<span class="math inline">\(d=1\)</span>)</strong> the coefficients reduce to <span class="math inline">\(a = - \frac{\mu_x}{\sigma_x}\)</span> and <span class="math inline">\(B = \frac{1}{\sigma_x}\)</span> and the Mahalanobis transform becomes <span class="math display">\[y = \frac{x-\mu_x}{\sigma_x}\]</span> i.e.&nbsp;it applies centering + standardisation.</p>
<p>The <strong>Mahalanobis transformation</strong> appears implicitly in many places in multivariate statistics, e.g.&nbsp;in the multivariate normal density. It is a particular example of a whitening transformation (plus centering).</p>
<div id="exm-coltrans" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.8</strong></span> <strong>Inverse Mahalanobis transformation</strong></p>
</div>
<p>The inverse of the Mahalanobis transform is given by <span class="math display">\[
\boldsymbol y= \boldsymbol \mu_{\boldsymbol y}+\boldsymbol \Sigma^{1/2}_{\boldsymbol y} \boldsymbol x
\]</span> As the Mahalanobis transform is a whitening transform the inverse Mahalonobis transform is sometimes called the Mahalanobis colouring transformation. The coefficients in the affine transformation are <span class="math inline">\(\boldsymbol a=\boldsymbol \mu_{\boldsymbol y}\)</span> and <span class="math inline">\(\boldsymbol B=\boldsymbol \Sigma^{1/2}_{\boldsymbol y}\)</span>.</p>
<p>Starting with <span class="math inline">\(\text{E}(\boldsymbol x)=\mathbf 0\)</span> and <span class="math inline">\(\text{Var}(\boldsymbol x)=\boldsymbol I_d\)</span> the mean and variance of the transformed variable are <span class="math display">\[\text{E}(\boldsymbol y) = \boldsymbol \mu_{\boldsymbol y}
\]</span> and <span class="math display">\[\text{Var}(\boldsymbol y) = \boldsymbol \Sigma_{\boldsymbol y}
\]</span></p>
</section>
<section id="transformation-of-a-density-under-an-invertible-location-scale-transformation" class="level3">
<h3 class="anchored" data-anchor-id="transformation-of-a-density-under-an-invertible-location-scale-transformation">Transformation of a density under an invertible location-scale transformation:</h3>
<p>Assume <span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> with density <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span>.</p>
<p>After linear transformation <span class="math inline">\(\boldsymbol y= \boldsymbol a+ \boldsymbol B\boldsymbol x\)</span> we get <span class="math inline">\(\boldsymbol y\sim F_{\boldsymbol y}\)</span> with density <span class="math display">\[f_{\boldsymbol y}(\boldsymbol y)=|\det(\boldsymbol B)|^{-1} f_{\boldsymbol x} \left( \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\right)\]</span></p>
<div id="exm-invmahanorm" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.9</strong></span> Transformation of standard normal with inverse Mahalanobis transform</p>
<p>Assume <span class="math inline">\(\boldsymbol x\)</span> is multivariate standard normal <span class="math inline">\(\boldsymbol x\sim N_d(\mathbf 0,\boldsymbol I_d)\)</span> with density <span class="math display">\[f_{\boldsymbol x}(\boldsymbol x) = (2\pi)^{-d/2}\exp\left( -\frac{1}{2} \boldsymbol x^T \boldsymbol x\right)\]</span> Then the density after applying the inverse Mahalanobis transform<br>
<span class="math inline">\(\boldsymbol y= \boldsymbol \mu_{\boldsymbol y}+\boldsymbol \Sigma^{1/2}_{\boldsymbol y} \boldsymbol x\)</span> is <span class="math display">\[
\begin{split}
f_{\boldsymbol y}(\boldsymbol y) &amp;= |\det(\boldsymbol \Sigma^{1/2}_{\boldsymbol y})|^{-1} (2\pi)^{-d/2} \exp\left(-\frac{1}{2}(\boldsymbol y-\boldsymbol \mu_{\boldsymbol y})^T\boldsymbol \Sigma^{-1/2}_{\boldsymbol y} \,\boldsymbol \Sigma^{-1/2}_{\boldsymbol y}(\boldsymbol y-\boldsymbol \mu_{\boldsymbol y})\right)\\
&amp; = (2\pi)^{-d/2} \det(\boldsymbol \Sigma_{\boldsymbol y})^{-1/2} \exp\left(-\frac{1}{2}(\boldsymbol y-\boldsymbol \mu_{\boldsymbol y})^T\boldsymbol \Sigma^{-1}_{\boldsymbol y}(\boldsymbol y-\boldsymbol \mu_{\boldsymbol y})\right) \\
\end{split}
\]</span> <span class="math inline">\(\Longrightarrow\)</span> <span class="math inline">\(\boldsymbol y\)</span> has multivariate normal density <span class="math inline">\(N_d(\boldsymbol \mu_{\boldsymbol y}, \boldsymbol \Sigma_{\boldsymbol y})\)</span></p>
<p><em>Application:</em> e.g.&nbsp;random number generation: draw from <span class="math inline">\(N_d(\mathbf 0,\boldsymbol I_d)\)</span> (easy!) then convert to multivariate normal by tranformation (see Worksheet 4).</p>
</div>
</section>
</section>
<section id="nonlinear-transformations" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="nonlinear-transformations"><span class="header-section-number">3.2</span> Nonlinear transformations</h2>
<section id="general-transformation" class="level3">
<h3 class="anchored" data-anchor-id="general-transformation">General transformation</h3>
<p><span class="math display">\[\boldsymbol y= \boldsymbol h(\boldsymbol x)\]</span> with <span class="math inline">\(\boldsymbol h\)</span> an arbitrary vector-valued function</p>
<ul>
<li>linear case: <span class="math inline">\(\boldsymbol h(\boldsymbol x) = \boldsymbol a+\boldsymbol B\boldsymbol x\)</span></li>
</ul>
</section>
<section id="delta-method" class="level3">
<h3 class="anchored" data-anchor-id="delta-method">Delta method</h3>
<p>Assume that we know the mean <span class="math inline">\(\text{E}(\boldsymbol x)=\boldsymbol \mu_{\boldsymbol x}\)</span> and variance <span class="math inline">\(\text{Var}(\boldsymbol x)=\boldsymbol \Sigma_{\boldsymbol x}\)</span> of <span class="math inline">\(\boldsymbol x\)</span>. Is it possible to say something about the mean and variance of the transformed random variable <span class="math inline">\(\boldsymbol y\)</span>? <span class="math display">\[
\text{E}(\boldsymbol y)= \text{E}(\boldsymbol h(\boldsymbol x))= ?
\]</span> <span class="math display">\[
\text{Var}(\boldsymbol y) = \text{Var}(\boldsymbol h(\boldsymbol x))= ? \\
\]</span></p>
<p>In general, for a transformation <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> the exact mean and variance of the transformed variable cannot be obtained analytically.</p>
<p>However, we can find a <strong>linear approximation</strong> and then compute its mean and variance. This approximation is called the “Delta Method”, or the “law of propagation of errors”, and is credited to Gauss <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>Linearisation of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> is achieved by a Taylor series approximation of first order of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> around <span class="math inline">\(\boldsymbol x_0\)</span>: <span class="math display">\[\boldsymbol h(\boldsymbol x) \approx \boldsymbol h(\boldsymbol x_0) + \underbrace{D \boldsymbol h(\boldsymbol x_0)}_{\text{Jacobian matrix}}(\boldsymbol x-\boldsymbol x_0)  =
\underbrace{\boldsymbol h(\boldsymbol x_0) - D \boldsymbol h(\boldsymbol x_0)\, \boldsymbol x_0}_{\boldsymbol a} + \underbrace{D \boldsymbol h(\boldsymbol x_0)}_{\boldsymbol B} \boldsymbol x\]</span></p>
<p>If <span class="math inline">\(h(\boldsymbol x)\)</span> is scalar-valued then <strong>gradient</strong> <span class="math inline">\(\nabla h(\boldsymbol x)\)</span> is given by the vector of partial correlations <span class="math display">\[
\nabla h(\boldsymbol x) =
\begin{pmatrix}
\frac{\partial h(\boldsymbol x)}{\partial x_1}  \\
\vdots\\
\frac{\partial h(\boldsymbol x)}{\partial x_d} \\
\end{pmatrix}
\]</span> where <span class="math inline">\(\nabla\)</span> is the nabla operator.</p>
<p>The <strong>Jacobian matrix</strong> is used if <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> is vector-valued:</p>
<p><span class="math display">\[D \boldsymbol h(\boldsymbol x) = \begin{pmatrix}\nabla^T h_1(\boldsymbol x)\\ \nabla h_2(\boldsymbol x)^T \\ \vdots \\ \nabla^T h_m(\boldsymbol x) \end{pmatrix} = \begin{pmatrix}
    \frac{\partial h_1(\boldsymbol x)}{\partial x_1} &amp; \dots &amp; \frac{\partial h_1(\boldsymbol x)}{\partial x_d}\\
    \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial h_m(\boldsymbol x)}{\partial x_1} &amp; \dots &amp; \frac{\partial h_m(\boldsymbol x)}{\partial x_d}
    \end{pmatrix}\]</span> Note that in the Jacobian matrix by convention the gradient for each individual component of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> is contained in the <em>row</em> of the matrix so the number of rows corresponds to the dimension of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> and the number of columns to the dimension of <span class="math inline">\(\boldsymbol x\)</span>.</p>
<p>First order approximation of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span> around <span class="math inline">\(\boldsymbol x_0=\boldsymbol \mu_{\boldsymbol x}\)</span> yields <span class="math inline">\(\boldsymbol a= \boldsymbol h(\boldsymbol \mu_{\boldsymbol x}) - D \boldsymbol h(\boldsymbol \mu_{\boldsymbol x})\, \boldsymbol \mu_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol B= D \boldsymbol h(\boldsymbol \mu_{\boldsymbol x})\)</span> and leads directly to the <strong>multivariate Delta method</strong>:</p>
<p><span class="math display">\[\text{E}(\boldsymbol y)\approx\boldsymbol h(\boldsymbol \mu_{\boldsymbol x})\]</span> <span class="math display">\[\text{Var}(\boldsymbol y)\approx D \boldsymbol h(\boldsymbol \mu_{\boldsymbol x}) \, \boldsymbol \Sigma_{\boldsymbol x} \, D \boldsymbol h(\boldsymbol \mu_{\boldsymbol x})^T\]</span></p>
<p>The <strong>univariate Delta method</strong> is a special case: <span class="math display">\[\text{E}(y) \approx h(\mu_x)\]</span> <span class="math display">\[\text{Var}(y)\approx \sigma^2_x \, h'(\mu_x)^2\]</span></p>
<p>Note that the Delta approximation breaks down if <span class="math inline">\(\text{Var}(\boldsymbol y)\)</span> is singular, for example if the first derivative (or gradient or Jacobian matrix) at <span class="math inline">\(\boldsymbol \mu_{\boldsymbol x}\)</span> is zero.</p>
<div id="exm-varoddsration" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.10</strong></span> <strong>Variance of the odds ratio</strong></p>
<p>The proportion <span class="math inline">\(\hat{p} = \frac{n_1}{n}\)</span> resulting from <span class="math inline">\(n\)</span> repeats of a Bernoulli experiment has expectation <span class="math inline">\(\text{E}(\hat{p})=p\)</span> and variance <span class="math inline">\(\text{Var}(\hat{p}) = \frac{p (1-p)}{n}\)</span>. What are the (approximate) mean and the variance of the corresponding odds ratio <span class="math inline">\(\widehat{OR}=\frac{\hat{p}}{1-\hat{p}}\)</span>?</p>
<p>With <span class="math inline">\(h(x) = \frac{x}{1-x}\)</span>, <span class="math inline">\(\widehat{OR} = h(\hat{p})\)</span> and <span class="math inline">\(h'(x) = \frac{1}{(1-x)^2}\)</span> we get using the Delta method <span class="math inline">\(\text{E}( \widehat{OR} ) \approx h(p) = \frac{p}{1-p}\)</span> and <span class="math inline">\(\text{Var}( \widehat{OR} )\approx h'(p)^2 \text{Var}( \hat{p} ) = \frac{p}{n (1-p)^3}\)</span>.</p>
</div>
<div id="exm-logtransform" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.11</strong></span> <strong>Log-transform as variance stabilisation</strong></p>
<p>Assume <span class="math inline">\(x\)</span> has some mean <span class="math inline">\(\text{E}(x)=\mu\)</span> and variance <span class="math inline">\(\text{Var}(x) = \sigma^2 \mu^2\)</span>, i.e.&nbsp;the standard deviation <span class="math inline">\(\text{SD}(x)\)</span> is proportional to the mean <span class="math inline">\(\mu\)</span>. What are the (approximate) mean and the variance of the log-transformed variable <span class="math inline">\(\log(x)\)</span>?</p>
<p>With <span class="math inline">\(h(x) = \log(x)\)</span> and <span class="math inline">\(h'(x) = \frac{1}{x}\)</span> we get using the Delta method <span class="math inline">\(\text{E}( \log(x) ) \approx h(\mu) = \log(\mu)\)</span> and <span class="math inline">\(\text{Var}( \log(x) )\approx h'(\mu)^2 \text{Var}( x ) = \left(\frac{1}{\mu} \right)^2 \sigma^2 \mu^2 = \sigma^2\)</span>. Thus, after applying the log-transform the variance does not depend any more on the mean!</p>
</div>
</section>
<section id="transformation-of-a-probability-density-function-under-a-general-invertible-transformation" class="level3">
<h3 class="anchored" data-anchor-id="transformation-of-a-probability-density-function-under-a-general-invertible-transformation">Transformation of a probability density function under a general invertible transformation</h3>
<p>Assume <span class="math inline">\(\boldsymbol y(\boldsymbol x) = \boldsymbol h(\boldsymbol x)\)</span> is invertible: <span class="math inline">\(\boldsymbol x(\boldsymbol y) = \boldsymbol h^{-1}(\boldsymbol y)\)</span></p>
<p><span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> with probability density function <span class="math inline">\(f_{\boldsymbol x}(\boldsymbol x)\)</span></p>
<p>The density <span class="math inline">\(f_{\boldsymbol y}(\boldsymbol y)\)</span> of the transformed random vector <span class="math inline">\(\boldsymbol y\)</span> is then given by<br>
<span class="math display">\[f_{\boldsymbol y}(\boldsymbol y) = |\det\left( D\boldsymbol x(\boldsymbol y) \right)| \,\,\,  f_{\boldsymbol x}\left( \boldsymbol x(\boldsymbol y) \right)\]</span></p>
<p>where <span class="math inline">\(D\boldsymbol x(\boldsymbol y)\)</span> is the Jacobian matrix of the inverse transformation.</p>
<p>Special cases:</p>
<ul>
<li>Univariate version: <span class="math inline">\(f_y(y) = \left|Dx(y) \right| \, f_x\left(x(y)\right)\)</span> with <span class="math inline">\(Dx(y) = \frac{dx(y)}{dy}\)</span></li>
<li>Linear transformation <span class="math inline">\(\boldsymbol h(\boldsymbol x) = \boldsymbol a+ \boldsymbol B\boldsymbol x\)</span>, with <span class="math inline">\(\boldsymbol x(\boldsymbol y) = \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\)</span> and <span class="math inline">\(D\boldsymbol x(\boldsymbol y) = \boldsymbol B^{-1}\)</span>: <span class="math display">\[f_{\boldsymbol y}(\boldsymbol y)=\left|\det(\boldsymbol B)\right|^{-1} f_{\boldsymbol x} \left( \boldsymbol B^{-1}(\boldsymbol y-\boldsymbol a)\right)\]</span></li>
</ul>
</section>
<section id="normalising-flows" class="level3">
<h3 class="anchored" data-anchor-id="normalising-flows">Normalising flows</h3>
<p>In this module we will focus mostly on linear transformations as these underpin much of classical multivariate statistics, but it is important to keep in mind for later study the importance of nonlinear transformations</p>
<p>In machine learning (sequences of) invertible nonlinear transformations are known as “normalising flows”. They are used both in a generative way (building complex models from simple models) and for simplification and dimension reduction.</p>
<p>If you are interested in normalising flows then a good start to learn more are the review papers by Kobyzev et al (2021 )<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> and Papamakarios et al.&nbsp;(2021) <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
</section>
</section>
<section id="general-whitening-transformations" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="general-whitening-transformations"><span class="header-section-number">3.3</span> General whitening transformations</h2>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<p>Whitening transformations are a special and widely used class of invertible location-scale transformations (<a href="#exm-whitetrans" class="quarto-xref">Example&nbsp;<span>3.6</span></a>).</p>
<p><em>Terminology:</em> whitening refers to the fact that after the transformation the covariance matrix is spherical, isotropic, white (<span class="math inline">\(\boldsymbol I_d\)</span>)</p>
<p>Whitening is <strong>useful in preprocessing</strong>, as they allow to <strong>turn multivariate models into uncorrelated univariate models</strong> (via decorrelation property). Some whitening transformations <strong>reduce the dimension in an optimal way</strong> (via compression property).</p>
<p>The <em>Mahalanobis</em> transform is a specific example of a <strong>whitening transformation</strong>. It is also know as “zero-phase component analysis” or short ZCA transform.</p>
<p>In so-called latent variable models whitening procedures are implicitly used in linear models to link observed (correlated) variables and latent variables (which typically are uncorrelated and standardised):</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cl}
\text{Whitening} \\
\downarrow
\end{array}
\begin{array}{ll}
\boldsymbol x\\
\uparrow \\
\boldsymbol z\\
\end{array}
\begin{array}{ll}
\text{Observed variable (can be measured)} \\
\text{external, typically correlated} \\
\space \\
\text{Unobserved "latent" variable (cannot be directly measured)} \\
\text{internal, typically chosen to be uncorrelated} \\
\end{array}
\end{align*}\]</span></p>
</section>
<section id="whitening-transformation-and-whitening-constraint" class="level3">
<h3 class="anchored" data-anchor-id="whitening-transformation-and-whitening-constraint">Whitening transformation and whitening constraint</h3>
<p><strong>Starting point:</strong></p>
<p>Random vector <span class="math inline">\(\boldsymbol x\sim F_{\boldsymbol x}\)</span> <strong>not necessarily from multivariate normal</strong>.</p>
<p><span class="math inline">\(\boldsymbol x\)</span> has mean <span class="math inline">\(\text{E}(\boldsymbol x)=\boldsymbol \mu\)</span> and a positive definite (invertible) covariance matrix <span class="math inline">\(\text{Var}(\boldsymbol x) = \boldsymbol \Sigma\)</span>.</p>
<p>Note that in the following we leave out the subscript <span class="math inline">\(\boldsymbol x\)</span> for the covariance of <span class="math inline">\(\boldsymbol x\)</span> unless it is needed for clarification.</p>
<p>The covariance can be split into positive variances <span class="math inline">\(\boldsymbol V\)</span> and a positive definite invertible correlation matrix <span class="math inline">\(\boldsymbol P\)</span> so that <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol V^{1/2} \boldsymbol P\boldsymbol V^{1/2}\)</span>.</p>
<p><strong>Whitening transformation:</strong></p>
<p><span class="math display">\[\underbrace{\boldsymbol z}_{d \times 1 \text{ vector }} = \underbrace{\boldsymbol W}_{d \times d \text{ whitening matrix }} \underbrace{\boldsymbol x}_{d \times 1 \text{ vector }}\]</span> <strong>Objective</strong>: choose <span class="math inline">\(\boldsymbol W\)</span> so that <span class="math inline">\(\text{Var}(\boldsymbol z)=\boldsymbol I_d\)</span></p>
<p>For Mahalanobis/ZCA whitening we already know from <a href="#exm-mahatrans" class="quarto-xref">Example&nbsp;<span>3.7</span></a> that <span class="math inline">\(\boldsymbol W^{\text{ZCA}}=\boldsymbol \Sigma^{-1/2}\)</span>.</p>
<p>In general, the whitening matrix <span class="math inline">\(\boldsymbol W\)</span> needs to satisfy a constraint: <span class="math display">\[
\begin{array}{lll}
                &amp; \text{Var}(\boldsymbol z) &amp; = \boldsymbol I_d \\
\Longrightarrow &amp; \text{Var}(\boldsymbol W\boldsymbol x) &amp;= \boldsymbol W\boldsymbol \Sigma\boldsymbol W^T = \boldsymbol I_d \\
\Longrightarrow &amp;  \boldsymbol W\, \boldsymbol \Sigma\, \boldsymbol W^T \boldsymbol W= \boldsymbol W&amp; \\
\end{array}
\]</span> <span class="math display">\[\Longrightarrow \text{constraint on whitening matrix: } \boldsymbol W^T \boldsymbol W= \boldsymbol \Sigma^{-1}\]</span></p>
<p>Clearly, the ZCA whitening matrix satisfies this constraint: <span class="math inline">\((\boldsymbol W^{ZCA})^T \boldsymbol W^{ZCA} = \boldsymbol \Sigma^{-1/2}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1}\)</span></p>
</section>
<section id="parameterisation-of-whitening-matrix" class="level3">
<h3 class="anchored" data-anchor-id="parameterisation-of-whitening-matrix">Parameterisation of whitening matrix</h3>
<p><strong>Covariance-based parameterisation of whitening matrix:</strong></p>
<p>A general way to specify a valid whitening matrix is <span class="math display">\[
\boldsymbol W= \boldsymbol Q_1 \boldsymbol \Sigma^{-1/2}
\]</span> where <span class="math inline">\(\boldsymbol Q_1\)</span> is an orthogonal matrix.</p>
<p>Recall that an orthogonal matrix <span class="math inline">\(\boldsymbol Q\)</span> has the property that <span class="math inline">\(\boldsymbol Q^{-1} = \boldsymbol Q^T\)</span> and and as a consequence <span class="math inline">\(\boldsymbol Q^T \boldsymbol Q= \boldsymbol Q\boldsymbol Q^T = \boldsymbol I\)</span>.</p>
<p>As a result, the above <span class="math inline">\(\boldsymbol W\)</span> satisfies the whitening constraint:</p>
<p><span class="math display">\[\boldsymbol W^T \boldsymbol W= \boldsymbol \Sigma^{-1/2}\underbrace{\boldsymbol Q_1^T \boldsymbol Q_1}_{\boldsymbol I}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1}\]</span></p>
<p>Note the converse is also true: any whitening whitening matrix, i.e.&nbsp;any <span class="math inline">\(\boldsymbol W\)</span> satisfying the whitening constraint, can be written in the above form as <span class="math inline">\(\boldsymbol Q_1 = \boldsymbol W\boldsymbol \Sigma^{1/2}\)</span> is orthogonal by construction.</p>
<p><span class="math inline">\(\Longrightarrow\)</span> instead of choosing <span class="math inline">\(\boldsymbol W\)</span>, <strong>we choose the orthogonal matrix</strong> <span class="math inline">\(\boldsymbol Q_1\)</span>!</p>
<ul>
<li>recall that orthogonal matrices geometrically represent rotations (plus reflections).</li>
<li>it is now clear that there are infinitely many whitening procedures, because there are infinitely many rotations! This also means we need to find ways to choose/select among whitening procedures.</li>
<li>for the Mahalanobis/ZCA transformation <span class="math inline">\(\boldsymbol Q_1^{\text{ZCA}}=\boldsymbol I\)</span></li>
<li><strong>whitening</strong> can be interpreted as <strong>Mahalanobis transformation</strong> followed by further <strong>rotation-reflection</strong></li>
</ul>
<p><strong>Correlation-based parameterisation of whitening matrix:</strong></p>
<p>Instead of working with the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span>, we can express <span class="math inline">\(\boldsymbol W\)</span> also in terms of the corresponding correlation matrix <span class="math inline">\(\boldsymbol P= (\rho_{ij}) = \boldsymbol V^{-1/2} \boldsymbol \Sigma\boldsymbol V^{-1/2}\)</span> where <span class="math inline">\(\boldsymbol V\)</span> is the diagonal matrix containing the variances.</p>
<p>Specifically, we can specify the whitening matrix as <span class="math display">\[\boldsymbol W= \boldsymbol Q_2 \boldsymbol P^{-1/2} \boldsymbol V^{-1/2}\]</span></p>
<p>It is easy to verify that this <span class="math inline">\(\boldsymbol W\)</span> also satisfies the whitening constraint: <span class="math display">\[
\begin{split}
\boldsymbol W^T \boldsymbol W&amp; = \boldsymbol V^{-1/2}\boldsymbol P^{-1/2}\underbrace{\boldsymbol Q_2^T \boldsymbol Q_2}_{\boldsymbol I}\boldsymbol P^{-1/2} \boldsymbol V^{-1/2} \\
&amp; = \boldsymbol V^{-1/2} \boldsymbol P^{-1} \boldsymbol V^{-1/2} = \boldsymbol \Sigma^{-1} \\
\end{split}
\]</span> Conversely, any whitening matrix <span class="math inline">\(\boldsymbol W\)</span> can also be written in this form as <span class="math inline">\(\boldsymbol Q_2 = \boldsymbol W\boldsymbol V^{1/2} \boldsymbol P^{1/2}\)</span> is orthogonal by construction.</p>
<ul>
<li><strong>Another interpretation of whitening</strong>: first <strong>standardising</strong> (<span class="math inline">\(\boldsymbol V^{-1/2}\)</span>), then <strong>decorrelation</strong> (<span class="math inline">\(\boldsymbol P^{-1/2}\)</span>), followed by <strong>rotation-reflection</strong> (<span class="math inline">\(\boldsymbol Q_2\)</span>)</li>
<li>for Mahalanobis/ZCA transformation <span class="math inline">\(\boldsymbol Q_2^{\text{ZCA}} = \boldsymbol \Sigma^{-1/2} \boldsymbol V^{1/2} \boldsymbol P^{1/2}\)</span></li>
</ul>
<p><strong>Both forms to write <span class="math inline">\(\boldsymbol W\)</span> using <span class="math inline">\(\boldsymbol Q_1\)</span> and <span class="math inline">\(\boldsymbol Q_2\)</span> are equally valid (and interchangeable).</strong></p>
<p>Note that for the same <span class="math inline">\(\boldsymbol W\)</span> <span class="math display">\[\boldsymbol Q_1\neq\boldsymbol Q_2 \text{  Two different orthogonal matrices!}\]</span> and also <span class="math display">\[\underbrace{\boldsymbol \Sigma^{-1/2}}_{\text{Symmetric}}\neq\underbrace{\boldsymbol P^{-1/2}\boldsymbol V^{-1/2}}_{\text{Not Symmetric}}\]</span> even though<br>
<span class="math display">\[\boldsymbol \Sigma^{-1/2}\boldsymbol \Sigma^{-1/2}=\boldsymbol \Sigma^{-1} = \boldsymbol V^{-1/2}\boldsymbol P^{-1/2}\boldsymbol P^{-1/2}\boldsymbol V^{-1/2}\]</span></p>
</section>
<section id="cross-covariance-and-cross-correlation-for-general-whitening-transformations" class="level3">
<h3 class="anchored" data-anchor-id="cross-covariance-and-cross-correlation-for-general-whitening-transformations">Cross-covariance and cross-correlation for general whitening transformations</h3>
<p>A useful criterion to characterise and to distinguish among whitening transformations is the cross-covariance and cross-correlation matrix between the original variable <span class="math inline">\(\boldsymbol x\)</span> and the whitened variable <span class="math inline">\(\boldsymbol z\)</span>:</p>
<ol type="a">
<li><p><strong>Cross-covariance</strong> <span class="math inline">\(\boldsymbol \Phi= \Sigma_{\boldsymbol x\boldsymbol z}\)</span> between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>: <span class="math display">\[
\begin{split}
\boldsymbol \Phi= \text{Cov}(\boldsymbol x, \boldsymbol z) &amp; = \text{Cov}( \boldsymbol x,\boldsymbol W\boldsymbol x)\\
&amp; = \boldsymbol \Sigma\boldsymbol W^T \\
&amp;= \boldsymbol \Sigma\, \boldsymbol \Sigma^{-1/2} \boldsymbol Q_1^T \\
&amp;= \boldsymbol \Sigma^{1/2} \boldsymbol Q_1^T\\
\end{split}
\]</span> In component notation we write <span class="math inline">\(\boldsymbol \Phi= (\phi_{ij})\)</span> where the row index <span class="math inline">\(i\)</span> refers to <span class="math inline">\(\boldsymbol x\)</span> and the column index <span class="math inline">\(j\)</span> to <span class="math inline">\(\boldsymbol z\)</span>.</p>
<p><strong>Cross-covariance is linked with</strong> <span class="math inline">\(\boldsymbol Q_1\)</span>! Thus, choosing cross-covariance determines <span class="math inline">\(\boldsymbol Q_1\)</span> (and vice versa).</p>
<p>Note that the above cross-covariance matrix <span class="math inline">\(\boldsymbol \Phi\)</span> satisfies the condition <span class="math inline">\(\boldsymbol \Phi\boldsymbol \Phi^T = \boldsymbol \Sigma\)</span>.</p>
<p>The whitening matrix expressed in terms of cross-covariance is <span class="math inline">\(\boldsymbol W= \boldsymbol \Phi^T \boldsymbol \Sigma^{-1}\)</span>, so as required <span class="math inline">\(\boldsymbol W^T \boldsymbol W= \boldsymbol \Sigma^{-1} \boldsymbol \Phi\boldsymbol \Phi^T \boldsymbol \Sigma^{-1}
=\boldsymbol \Sigma^{-1}\)</span>. Furthermore, <span class="math inline">\(\boldsymbol \Phi\)</span> is the <em>inverse</em> of the whitening matrix, as <span class="math inline">\(\boldsymbol W^{-1} = \left( \boldsymbol Q_1 \boldsymbol \Sigma^{-1/2}  \right)^{-1} =
\boldsymbol \Sigma^{1/2} \boldsymbol Q_1^{-1} =
\boldsymbol \Sigma^{1/2} \boldsymbol Q_1^{T}   = \boldsymbol \Phi\)</span>.</p></li>
<li><p><strong>Cross-correlation</strong> <span class="math inline">\(\boldsymbol \Psi= \boldsymbol P_{\boldsymbol x\boldsymbol z}\)</span> between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>: <span class="math display">\[
\begin{split}
\boldsymbol \Psi= \text{Cor}(\boldsymbol x, \boldsymbol z) &amp; = \boldsymbol V^{-1/2} \boldsymbol \Phi\\
&amp; =  \boldsymbol V^{-1/2} \boldsymbol \Sigma\boldsymbol W^T \\
&amp;=\boldsymbol V^{-1/2} \boldsymbol \Sigma\boldsymbol V^{-1/2} \boldsymbol P^{-1/2} \boldsymbol Q_2^T\\
&amp; =  \boldsymbol P^{1/2}  \boldsymbol Q_2^T\\
\end{split}
\]</span></p>
<p>In component notation we write <span class="math inline">\(\boldsymbol \Psi= (\psi_{ij})\)</span> where the row index <span class="math inline">\(i\)</span> refers to <span class="math inline">\(\boldsymbol x\)</span> and the column index <span class="math inline">\(j\)</span> to <span class="math inline">\(\boldsymbol z\)</span>.</p>
<p><strong>Cross-correlation is linked with</strong> <span class="math inline">\(\boldsymbol Q_2\)</span>! Hence, choosing cross-correlation determines <span class="math inline">\(\boldsymbol Q_2\)</span> (and vice versa). The whitening matrix expressed in terms of cross-correlation is <span class="math inline">\(\boldsymbol W= \boldsymbol \Psi^T \boldsymbol P^{-1} \boldsymbol V^{-1/2}\)</span>.</p></li>
</ol>
<p>Note that the factorisation of the cross-covariance <span class="math inline">\(\boldsymbol \Phi=\boldsymbol \Sigma^{1/2}\boldsymbol Q_1^T\)</span> and the cross-correlation <span class="math inline">\(\boldsymbol \Psi=\boldsymbol P^{1/2}\boldsymbol Q_2^T\)</span> into the product of a positive definite symmetric matrix and an orthogonal matrix are examples of a <strong>polar decomposition</strong>.</p>
</section>
<section id="inverse-whitening-transformation-and-loadings" class="level3">
<h3 class="anchored" data-anchor-id="inverse-whitening-transformation-and-loadings">Inverse whitening transformation and loadings</h3>
<p><strong>Inverse transformation:</strong></p>
<p>Recall that <span class="math inline">\(\boldsymbol z= \boldsymbol W\boldsymbol x\)</span>. Therefore, the reverse transformation going from the whitened to the original variable is <span class="math inline">\(\boldsymbol x= \boldsymbol W^{-1} \boldsymbol z\)</span>. This can be expressed also in terms of cross-covariance and cross-correlation. With <span class="math inline">\(\boldsymbol W^{-1} = \boldsymbol \Phi\)</span> we get <span class="math display">\[
\boldsymbol x= \boldsymbol \Phi\boldsymbol z\, .
\]</span> Furthermore, since <span class="math inline">\(\boldsymbol \Psi= \boldsymbol V^{-1/2} \boldsymbol \Phi\)</span> we have <span class="math inline">\(\boldsymbol W^{-1}  = \boldsymbol V^{1/2}  \boldsymbol \Psi\)</span> and hence <span class="math display">\[
\boldsymbol V^{-1/2} \boldsymbol x=   \boldsymbol \Psi\boldsymbol z\, .
\]</span></p>
<p>The reverse whitening transformation is also known as colouring transformation (the previously discussed inverse Mahalanobis transform is one example).</p>
<p><strong>Definition of loadings:</strong></p>
<p><em>Loadings</em> are the coefficients of the linear transformation from the latent variable back to the observed variable. If the variables are standardised to unit variance then the loadings are also called <em>correlation loadings</em>.</p>
<p>Hence, the cross-covariance matrix <span class="math inline">\(\boldsymbol \Phi\)</span> plays the role of <em>loadings</em> linking the latent variable <span class="math inline">\(\boldsymbol z\)</span> with the original <span class="math inline">\(\boldsymbol x\)</span>. Similarly, the cross-correlation matrix <span class="math inline">\(\boldsymbol \Psi\)</span> contains the <em>correlation loadings</em> linking the (already standardised) latent variable <span class="math inline">\(\boldsymbol z\)</span> with the standardised <span class="math inline">\(\boldsymbol x\)</span>.</p>
<p>In the convention we use here the rows correspond to the original variables and the columns to the whitened latent variables.</p>
<p><strong>Multiple correlation coefficients from <span class="math inline">\(\boldsymbol z\)</span> back to <span class="math inline">\(\boldsymbol x\)</span>:</strong></p>
<p>We consider the backtransformation from the whitened variable <span class="math inline">\(\boldsymbol z\)</span> to the original variables <span class="math inline">\(\boldsymbol x\)</span> and note that the components of <span class="math inline">\(\boldsymbol z\)</span> are all uncorrelated witth <span class="math inline">\(\boldsymbol P_{\boldsymbol z} = \boldsymbol I\)</span>. The squared multiple correlation coefficient <span class="math inline">\(\text{MCor}(x_i, \boldsymbol z)\)</span> between each <span class="math inline">\(x_i\)</span> and all <span class="math inline">\(\boldsymbol z\)</span> is therefore just the sum of the corresponding squared correlations <span class="math inline">\(\text{Cor}(x_i, z_j)^2\)</span>: <span class="math display">\[
\begin{split}
\text{MCor}(x_i, \boldsymbol z)^2 &amp;=  \boldsymbol P_{x_i \boldsymbol z} \boldsymbol P_{\boldsymbol z}^{-1} \boldsymbol P_{\boldsymbol zx_i} = \\
             &amp; \sum_{j=1}^d  \text{Cor}(x_i, z_j)^2  \\
&amp;  = \sum_{j=1}^d \psi_{ij}^2 = 1
\end{split}
\]</span> As shown earlier for a general linear one-to-one- transformation (which includes whitening as special case) the squared multiple correlation must be 1 because there is no error. We can confirm this by computing the <strong>row sums of squares</strong> of the cross-correlation matrix <span class="math inline">\(\boldsymbol \Psi\)</span> in matrix notation <span class="math display">\[
\begin{split}
\text{Diag}\left(\boldsymbol \Psi\boldsymbol \Psi^T\right) &amp;= \text{Diag}\left(\boldsymbol P^{1/2} \boldsymbol Q_2^T \boldsymbol Q_2\boldsymbol P^{1/2}\right) \\
&amp;= \text{Diag}(\boldsymbol P) \\
&amp;= (1, \ldots, 1)^T\\
\end{split}
\]</span> from which it is clear that the choice of <span class="math inline">\(\boldsymbol Q_2\)</span> is not relevant.</p>
<p>Similarly, the <strong>row sums of squares</strong> of the cross-covariance matrix <span class="math inline">\(\boldsymbol \Phi\)</span> equal the variances of the original variables, regardless of <span class="math inline">\(\boldsymbol Q_1\)</span>: <span class="math display">\[
\sum_{j=1}^d \phi_{ij}^2 = \text{Var}(x_i)
\]</span> or in matrix notation <span class="math display">\[
\begin{split}
\text{Diag}\left(\boldsymbol \Phi\boldsymbol \Phi^T\right) &amp;= \text{Diag}\left(\boldsymbol \Sigma^{1/2} \boldsymbol Q_1^T \boldsymbol Q_1 \boldsymbol \Sigma^{1/2}\right) \\
&amp;= \text{Diag}(\boldsymbol \Sigma) \\
&amp;= (\text{Var}(x_1), \ldots, \text{Var}(x_d)^T\\
\end{split}
\]</span></p>
</section>
<section id="summaries-of-cross-covariance-boldsymbol-phi-and-cross-correlation-boldsymbol-psi-resulting-from-whitening-transformations" class="level3">
<h3 class="anchored" data-anchor-id="summaries-of-cross-covariance-boldsymbol-phi-and-cross-correlation-boldsymbol-psi-resulting-from-whitening-transformations">Summaries of cross-covariance <span class="math inline">\(\boldsymbol \Phi\)</span> and cross-correlation <span class="math inline">\(\boldsymbol \Psi\)</span> resulting from whitening transformations</h3>
<p><strong>Matrix trace:</strong></p>
<p>A simply summary of a matrix is its trace. For the cross-covariance matrix <span class="math inline">\(\boldsymbol \Phi\)</span> the trace is the sum of all covariances between corresponding elements in <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>: <span class="math display">\[
\text{Tr}(\boldsymbol \Phi) =  \sum_{i=1}^d \text{Cov}(x_i, z_i) =  \sum_{i=1}^d  \phi_{ii} = \text{Tr}\left(\boldsymbol \Sigma^{1/2} \boldsymbol Q_1^T\right)
\]</span> Likewise, for the cross-correlation matrix <span class="math inline">\(\boldsymbol \Psi\)</span> the trace is the sum of all correlations between corresponding elements in <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>: <span class="math display">\[
\text{Tr}(\boldsymbol \Psi) =  \sum_{i=1}^d \text{Cor}(x_i, z_i) =  \sum_{i=1}^d  \psi_{ii} = \text{Tr}\left(\boldsymbol P^{1/2} \boldsymbol Q_2^T\right)
\]</span></p>
<p>In both cases the value of the trace depends on <span class="math inline">\(\boldsymbol Q_1\)</span> and <span class="math inline">\(\boldsymbol Q_2\)</span>. Interestingly, there is unique choice such that the trace is maximised.</p>
<p>Specifically, to maximise <span class="math inline">\(\text{Tr}(\boldsymbol \Phi)\)</span> we conduct the following steps:</p>
<ol type="i">
<li>Apply eigendecomposition to <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span>. Note that <span class="math inline">\(\boldsymbol \Lambda\)</span> is diagonal with positive eigenvalues <span class="math inline">\(\lambda_i &gt; 0\)</span> as <span class="math inline">\(\boldsymbol \Sigma\)</span> is positive definite and <span class="math inline">\(\boldsymbol U\)</span> is an orthogonal matrix.</li>
<li>The objective function becomes <span class="math display">\[
\begin{split}
\text{Tr}(\boldsymbol \Phi) &amp;= \text{Tr}\left(\boldsymbol \Sigma^{1/2} \boldsymbol Q_1^T\right)\\
&amp;= \text{Tr}\left( \boldsymbol U\boldsymbol \Lambda^{1/2} \boldsymbol U^T \boldsymbol Q_1^T \right) \\
&amp;= \text{Tr}\left(\boldsymbol \Lambda^{1/2} \, \boldsymbol U^T \boldsymbol Q_1^T \boldsymbol U\right) \\
&amp; = \text{Tr}\left(\boldsymbol \Lambda^{1/2} \, \boldsymbol B\right) \\
&amp; = \sum_{i=1}^d \lambda_i^{1/2} b_{ii}.
\end{split}
\]</span> Note that the product of two orthogonal matrices is itself an orthogonal matrix. Therefore, <span class="math inline">\(\boldsymbol B= \boldsymbol U^T \boldsymbol Q_1^T \boldsymbol U\)</span> is an orthogonal matrix and <span class="math inline">\(\boldsymbol Q_1 = \boldsymbol U\boldsymbol B^T \boldsymbol U^T\)</span>.</li>
<li>As <span class="math inline">\(\lambda_i &gt; 0\)</span> and all <span class="math inline">\(b_{ii} \in [-1, 1]\)</span> the objective function is maximised for <span class="math inline">\(b_{ii}=1\)</span>, i.e.&nbsp;for <span class="math inline">\(\boldsymbol B=\boldsymbol I\)</span>.</li>
<li>In turn this implies that <span class="math inline">\(\text{Tr}(\boldsymbol \Phi)\)</span> is maximised for <span class="math inline">\(\boldsymbol Q_1=\boldsymbol I\)</span>.</li>
</ol>
<p>Similarly, to maximise <span class="math inline">\(\text{Tr}(\boldsymbol \Psi)\)</span> we</p>
<ul>
<li>decompose <span class="math inline">\(\boldsymbol P= \boldsymbol G\boldsymbol \Theta\boldsymbol G^T\)</span> and then, following the above,<br>
</li>
<li>find that <span class="math inline">\(\text{Tr}(\boldsymbol \Psi) = \text{Tr}\left(\boldsymbol \Theta^{1/2} \, \boldsymbol G^T \boldsymbol Q_2^T \boldsymbol G\right)\)</span> is maximised for <span class="math inline">\(\boldsymbol Q_2=\boldsymbol I\)</span>.</li>
</ul>
<p><strong>Squared Frobenius norm and total variation:</strong></p>
<p>Another way to summarise and dissect the association between <span class="math inline">\(\boldsymbol x\)</span> and the corresponding whitened <span class="math inline">\(\boldsymbol z\)</span> is the squared Frobenius norm and the total variation based on <span class="math inline">\(\boldsymbol \Phi\)</span> and <span class="math inline">\(\boldsymbol \Psi\)</span>.</p>
<p>The squared Frobenius norm (Euclidean) norm is the sum of squared elements of a matrix.</p>
<p>If we consider the squared Frobenius norm of the cross-covariance matrix, i.e.&nbsp;the sum of squared covariances between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>, <span class="math display">\[
|| \boldsymbol \Phi||_F^2 = \sum_{i=1}^d \sum_{j=1}^d \phi_{ij}^2 =  \text{Tr}(\boldsymbol \Phi^T \boldsymbol \Phi) = \text{Tr}( \boldsymbol \Sigma)
\]</span> we find that this equals the <strong>total variation</strong> of <span class="math inline">\(\boldsymbol \Sigma\)</span> and that it does not depend on <span class="math inline">\(\boldsymbol Q_1\)</span>. Likewise, computing the squared Frobenius norm of the cross-correlation matrix, i.e.&nbsp;the sum of squared correlations between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>, <span class="math display">\[
|| \boldsymbol \Psi||_F^2  = \sum_{i=1}^d \sum_{j=1}^d \psi_{ij}^2= \text{Tr}\left(\boldsymbol \Psi^T  \boldsymbol \Psi\right) =\text{Tr}\left( \boldsymbol P\right) = d
\]</span> yields the total variation of <span class="math inline">\(\boldsymbol P\)</span> which also does not depend on <span class="math inline">\(\boldsymbol Q_2\)</span>. Note this is because the squared Frobenius norm is invariant against rotations and reflections.</p>
<p><strong>Proportion of total variation:</strong></p>
<p>We can now compute the contribution of each whitened component <span class="math inline">\(z_j\)</span> to the total variation. The sum of squared covariances of each <span class="math inline">\(z_j\)</span> with all <span class="math inline">\(x_1, \ldots, x_d\)</span> is <span class="math display">\[
h_j = \sum^d_{i=1}\text{Cov}(x_i,z_j)^2 = \sum^d_{i=1} \phi_{ij}^2
\]</span> with <span class="math inline">\(\sum_{j=1}^d h_j = \text{Tr}\left(\boldsymbol \Sigma\right)\)</span> the total variation. In vector notation the contributions are written as the <strong>column sums of squares</strong> of <span class="math inline">\(\boldsymbol \Phi\)</span> <span class="math display">\[
\boldsymbol h= (h_1,\ldots,h_d)^T = \text{Diag}(\boldsymbol \Phi^T\boldsymbol \Phi) = \text{Diag}\left(\boldsymbol Q_1\boldsymbol \Sigma\boldsymbol Q_1^T\right)\,.
\]</span> The relative contribution of <span class="math inline">\(z_j\)</span> versus the total variation is <span class="math display">\[
\frac{ h_j }{\text{Tr}\left( \boldsymbol \Sigma\right)} \,.
\]</span> Crucially, in contrast to total variation, the contributions <span class="math inline">\(h_j\)</span> depend on the choice of <span class="math inline">\(\boldsymbol Q_1\)</span>.</p>
<p>Similarly, the sum of squared correlations of each <span class="math inline">\(z_j\)</span> with all <span class="math inline">\(x_1, \ldots, x_d\)</span> is <span class="math display">\[
k_j = \sum^d_{i=1}\text{Cor}(x_i,z_j)^2 = \sum^d_{i=1} \psi_{ij}^2
\]</span> with <span class="math inline">\(\sum_{i=j}^d k_j = \text{Tr}( \boldsymbol P) = d\)</span>. In vector notation this correspoinds to the <strong>column sums of squares</strong> of <span class="math inline">\(\boldsymbol \Psi\)</span> <span class="math display">\[
\boldsymbol k= (k_1,\ldots,k_d)^T = \text{Diag}\left(\boldsymbol \Psi^T\boldsymbol \Psi\right)=\text{Diag}\left(\boldsymbol Q_2\boldsymbol P\boldsymbol Q_2^T\right)\,.
\]</span> The relative contribution of <span class="math inline">\(z_j\)</span> with regard to the total variation of the correlation <span class="math inline">\(\boldsymbol P\)</span> is <span class="math display">\[
\frac{ k_j  }{\text{Tr}( \boldsymbol P)} = \frac{ k_j  }{d} \,.
\]</span> As above, the contributions <span class="math inline">\(k_j\)</span> depend on the choice of <span class="math inline">\(\boldsymbol Q_2\)</span>.</p>
<p><strong>Maximising the proportion of total variation:</strong></p>
<p>Interestingly, it is possible to choose a unique whitening transformation such that the contributions are maximised, i.e.&nbsp;that the sum of the <span class="math inline">\(m\)</span> largest contributions of <span class="math inline">\(h_j\)</span> and <span class="math inline">\(k_j\)</span> is as large as possible.</p>
<p>Specifically, we note that <span class="math inline">\(\boldsymbol \Phi^T\boldsymbol \Phi\)</span> and <span class="math inline">\(\boldsymbol \Psi^T\boldsymbol \Psi\)</span> are symmetric real matrices. For these type of matrices we know from Schur’s theorem (1923) that the eigenvalues <span class="math inline">\(\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_d\)</span> <strong>majorise</strong> the diagonal elements <span class="math inline">\(p_1 \geq p_2 \geq \ldots \geq p_d\)</span>. More precisely, <span class="math display">\[
\sum_{i=1}^m \lambda_i \geq \sum_{i=1}^m p_i \, ,
\]</span> i.e.&nbsp;the sum of the largest <span class="math inline">\(m\)</span> eigenvalues is larger than or equal to the sum of the <span class="math inline">\(m\)</span> largest diagonal elements. The maximum (and equality) is only achieved if the matrix is diagonal, as in this case the diagonal elements are equal to the eigenvalues.</p>
<p>Therefore, the optimal solution to problem of maximising the relative contributions is obtained by computing the eigendecompositions <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span> and <span class="math inline">\(\boldsymbol P= \boldsymbol G\boldsymbol \Theta\boldsymbol G^T\)</span> and diagonalise <span class="math inline">\(\boldsymbol \Phi^T\boldsymbol \Phi=\boldsymbol Q_1\boldsymbol \Sigma\boldsymbol Q_1^T\)</span> and <span class="math inline">\(\boldsymbol \Psi^T \boldsymbol \Psi=\boldsymbol Q_2\boldsymbol P\boldsymbol Q_2^T\)</span> by setting <span class="math inline">\(\boldsymbol Q_1= \boldsymbol U^T\)</span> and <span class="math inline">\(\boldsymbol Q_2= \boldsymbol G^T\)</span>, respectively. This yields for the maximised contributions <span class="math display">\[
\boldsymbol h= \text{Diag}\left(\boldsymbol \Lambda\right)= (\lambda_1, \ldots, \lambda_d)^T
\]</span> and <span class="math display">\[
\boldsymbol k= \text{Diag}\left(\boldsymbol \Theta\right) = (\theta_1, \ldots, \theta_d)^T
\]</span> with eigenvalues <span class="math inline">\(\lambda_i\)</span> and <span class="math inline">\(\theta_i\)</span> arranged in decreasing order.</p>
</section>
</section>
<section id="natural-whitening-procedures" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="natural-whitening-procedures"><span class="header-section-number">3.4</span> Natural whitening procedures</h2>
<p>We now introduce several strategies (maximise correlation between individual components, maximise compression, structural constraints) to select an optimal whitening procedure.</p>
<p>Specifically, we discuss the following whitening transformations:</p>
<ul>
<li><strong>Mahalanobis</strong> whitening, also known as <strong>ZCA</strong> (zero-phase component analysis) whitening in machine learning (based on covariance)</li>
<li><strong>ZCA-cor</strong> whitening (based on correlation)</li>
<li><strong>PCA</strong> whitening (based on covariance)</li>
<li><strong>PCA-cor</strong> whitening (based on correlation)</li>
<li><strong>Cholesky</strong> whitening</li>
</ul>
<p>Thus, in the following we consider three main types (ZCA, PCA, Cholesky) of whitening.</p>
<p>In the following <span class="math inline">\(\boldsymbol x_c = \boldsymbol x-\boldsymbol \mu_{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol z_c = \boldsymbol z-\boldsymbol \mu_{\boldsymbol z}\)</span> denote the mean-centered variables.</p>
<section id="zca-whitening" class="level3">
<h3 class="anchored" data-anchor-id="zca-whitening">ZCA whitening</h3>
<p><em>Aim</em>: remove correlations and standardise but otherwise make sure that the whitened vector <span class="math inline">\(\boldsymbol z\)</span> does not differ too much from the original vector <span class="math inline">\(\boldsymbol x\)</span>. Specifically, each latent component <span class="math inline">\(z_i\)</span> should be as close as as possible to the corresponding original variable <span class="math inline">\(x_i\)</span>: <span class="math display">\[
\begin{array}{cc}
z_1\leftrightarrow x_1 \\
z_2\leftrightarrow x_2\\
z_3\leftrightarrow x_3 \\
\ldots\\
z_d\leftrightarrow x_d \\
\end{array}
\]</span> One possible way to implement this is to compute the expected squared difference between the two centered random vectors <span class="math inline">\(\boldsymbol z_c\)</span> and <span class="math inline">\(\boldsymbol x_c\)</span>.</p>
<p><em>ZCA objective function</em>: <strong>minimise</strong> <span class="math inline">\(\text{E}\left( || \boldsymbol x_c - \boldsymbol z_c ||^2_F   \right)\)</span> to find an optimal whitening procedure.</p>
<p>The ZCA objective function can be simplified as follows: <span class="math display">\[
\begin{split}
\text{E}\left( || \boldsymbol x_c-\boldsymbol z_c ||^2_F   \right)&amp;=\text{E}\left( || \boldsymbol x_c ||^2_F   \right)  -2 \text{E}\left(  \text{Tr}\left( \boldsymbol x_c \boldsymbol z_c^T \right) \right)  + \text{E}\left( || \boldsymbol z_c ||^2_F   \right) \\
&amp; = \text{Tr}(  \text{E}( \boldsymbol x_c \boldsymbol x_c^T ) )  - 2 \text{Tr}( \text{E}(  \boldsymbol x_c \boldsymbol z_c^T ) ) + \text{Tr}( \text{E}( \boldsymbol z_c \boldsymbol z_c^T ) )
   \\
&amp; = \text{Tr}( \text{Var}(\boldsymbol x) ) - 2 \text{Tr}( \text{Cov}(\boldsymbol x, \boldsymbol z) ) +  \text{Tr}( \text{Var}(\boldsymbol z) )  \\
&amp; = \text{Tr}(\boldsymbol \Sigma) - 2\text{Tr}(\boldsymbol \Phi)+ d \\
\end{split}
\]</span> The same objective function can also be obtained by putting a diagonal constraint on the cross-covariance <span class="math inline">\(\boldsymbol \Phi\)</span>. Specifically, we are looking for the <span class="math inline">\(\boldsymbol \Phi\)</span> that is closest to the diagonal matrix <span class="math inline">\(\boldsymbol I\)</span> by <strong>minimising</strong> <span class="math display">\[
\begin{split}
|| \boldsymbol \Phi- \boldsymbol I||^2_F &amp;= || \boldsymbol \Phi||^2_F  - 2 \text{Tr}(\boldsymbol \Phi^T \boldsymbol I)  + || \boldsymbol I||^2_F \\
&amp;= \text{Tr}(\boldsymbol \Sigma) - 2 \text{Tr}(\boldsymbol \Phi) + d \\
\end{split}
\]</span> This will force the off-diagonal elements of <span class="math inline">\(\boldsymbol \Phi\)</span> to be close to zero and thus leads to sparsity in the cross-covariance matrix.</p>
<p>The only term in the above that depends on the whitening transformation is <span class="math inline">\(-2 \text{Tr}(\boldsymbol \Phi)\)</span> as <span class="math inline">\(\boldsymbol \Phi\)</span> is a function of <span class="math inline">\(\boldsymbol Q_1\)</span>. Therefore we can use the following alternative objective:</p>
<p><em>ZCA equivalent objective</em>: <strong>maximise</strong> <span class="math inline">\(\text{Tr}(\boldsymbol \Phi) = \text{Tr}(\boldsymbol \Sigma^{1/2} \boldsymbol Q_1^T)\)</span> to find the optimal <span class="math inline">\(\boldsymbol Q_1\)</span></p>
<p><em>Solution</em>:</p>
<p>From the earlier discussion we know that the optimal matrix is <span class="math display">\[
\boldsymbol Q_1^{\text{ZCA}}=\boldsymbol I
\]</span> The corresponding whitening matrix for ZCA is therefore <span class="math display">\[
\boldsymbol W^{\text{ZCA}} = \boldsymbol \Sigma^{-1/2}
\]</span> and the cross-covariance matrix is <span class="math display">\[
\boldsymbol \Phi^{\text{ZCA}} = \boldsymbol \Sigma^{1/2}
\]</span> and the cross-correlation matrix <span class="math display">\[
\boldsymbol \Psi^{\text{ZCA}} = \boldsymbol V^{-1/2} \boldsymbol \Sigma^{1/2}
\]</span></p>
<p>Note that <span class="math inline">\(\boldsymbol \Sigma^{1/2}\)</span> is a symmetric positive definite matrix, hence its diagonal elements are all positive. As a result, the diagonals of <span class="math inline">\(\boldsymbol \Phi^{\text{ZCA}}\)</span> and <span class="math inline">\(\boldsymbol \Psi^{\text{ZCA}}\)</span> are positive, i.e.&nbsp;<span class="math inline">\(\text{Cov}(x_i, z_i) &gt; 0\)</span> and <span class="math inline">\(\text{Cor}(x_i, z_i) &gt; 0\)</span>. Hence, for ZCA two corresponding components <span class="math inline">\(x_i\)</span> and <span class="math inline">\(z_i\)</span> are always positively correlated!</p>
<p><em>Proportion of total variation</em>:</p>
<p>For ZCA with <span class="math inline">\(\boldsymbol Q_1=\boldsymbol I\)</span> we find that <span class="math inline">\(\boldsymbol h=\text{Diag}(\boldsymbol \Sigma) = \sum_{j=1}^d \text{Var}(x_j)\)</span> with <span class="math inline">\(h_i=\text{Var}(x_i)\)</span> hence for ZCA the proportion of total variation contributed by the latent component <span class="math inline">\(z_i\)</span> is the ratio <span class="math inline">\(\frac{\text{Var}(x_i)}{\sum_{j=1}^d \text{Var}(x_j)}\)</span>.</p>
<p><em>Summary</em>:</p>
<ul>
<li>ZCA/Mahalanobis transform is the unique transformation that minimises the expected total squared component-wise difference between <span class="math inline">\(\boldsymbol x_c\)</span> and <span class="math inline">\(\boldsymbol z_c\)</span>.</li>
<li>In ZCA corresponding components in the whitened and original variables are always positively correlated. This facilitates the interpretation of the whitened variables.</li>
<li>Use ZCA aka Mahalanobis whitening if you want to “just” remove correlations.</li>
</ul>
</section>
<section id="zca-cor-whitening" class="level3">
<h3 class="anchored" data-anchor-id="zca-cor-whitening">ZCA-Cor whitening</h3>
<p><em>Aim</em>: same as above but remove scale in <span class="math inline">\(\boldsymbol x\)</span> first before comparing to <span class="math inline">\(\boldsymbol z\)</span>.</p>
<p><em>ZCA-cor objective function</em>: <strong>minimise</strong> <span class="math inline">\(\text{E}\left( || \boldsymbol V^{-1/2} \boldsymbol x_c -\boldsymbol z_c ||^2_F \right)\)</span> to find an optimal whitening procedure.</p>
<p>This can be simplified as follows: <span class="math display">\[
\begin{split}
\text{E}\left( || \boldsymbol V^{-1/2} \boldsymbol x_c -\boldsymbol z_c||^2_F   \right)&amp;=\text{E}\left( || \boldsymbol V^{-1/2} \boldsymbol x_c ||^2_F   \right)  -2 \text{E}\left(  \text{Tr}\left( \boldsymbol V^{-1/2} \boldsymbol x_c \boldsymbol z_c^T  \right) \right) +  \text{E}\left( || \boldsymbol z_c ||^2_F   \right)\\
&amp; = \text{Tr}(  \text{E}(\boldsymbol V^{-1/2} \boldsymbol x_c \boldsymbol x_c^T \boldsymbol V^{-1/2}) )  
- 2 \text{Tr}( \text{E}( \boldsymbol V^{-1/2} \boldsymbol x_c \boldsymbol z_c^T  ) )
+\text{Tr}( \text{E}( \boldsymbol z_c \boldsymbol z_c^T ) )
  \\
&amp; = \text{Tr}(  \text{Cor}(\boldsymbol x, \boldsymbol x) ) - 2 \text{Tr}( \text{Cor}(\boldsymbol x, \boldsymbol z) ) + \text{Tr}( \text{Var}(\boldsymbol z) )   \\
&amp; = d - 2\text{Tr}(\boldsymbol \Psi)+ d \\
&amp; = 2d - 2\text{Tr}(\boldsymbol \Psi)
\end{split}
\]</span> The same objective function can also be obtained by putting a diagonal constraint on the cross-correlation <span class="math inline">\(\boldsymbol \Psi\)</span>. Specifically, we are looking for the <span class="math inline">\(\boldsymbol \Psi\)</span> that is closest to the diagonal matrix <span class="math inline">\(\boldsymbol I\)</span> by <strong>minimising</strong> <span class="math display">\[
\begin{split}
|| \boldsymbol \Psi- \boldsymbol I||^2_F &amp;= || \boldsymbol \Psi||^2_F  - 2 \text{Tr}(\boldsymbol \Psi^T \boldsymbol I)  + || \boldsymbol I||^2_F \\
&amp;= d - 2 \text{Tr}(\boldsymbol \Psi) + d \\
&amp;= 2 d - 2 \text{Tr}(\boldsymbol \Psi) \\
\end{split}
\]</span> This will force the off-diagonal elements of <span class="math inline">\(\boldsymbol \Psi\)</span> to be close to zero and thus leads to sparsity in the cross-correlation matrix.</p>
<p>The only term in the above that depends on the whitening transformation is <span class="math inline">\(-2 \text{Tr}(\boldsymbol \Psi)\)</span> as <span class="math inline">\(\boldsymbol \Psi\)</span> is a function of <span class="math inline">\(\boldsymbol Q_2\)</span>. Thus we can use the following alternative objective instead:</p>
<p><em>ZCA-cor equivalent objective</em>: <strong>maximise</strong> <span class="math inline">\(\text{Tr}(\boldsymbol \Psi)=\text{Tr}(\boldsymbol P^{1/2} \boldsymbol Q_2^T)\)</span> to find optimal <span class="math inline">\(\boldsymbol Q_2\)</span></p>
<p><em>Solution</em>: same as above for ZCA but using correlation instead of covariance:</p>
<p>From the earlier discussion we know that the optimal matrix is <span class="math display">\[
\boldsymbol Q_2^{\text{ZCA-Cor}}=\boldsymbol I
\]</span> The corresponding whitening matrix for ZCA-cor is therefore <span class="math display">\[
\boldsymbol W^{\text{ZCA-Cor}} = \boldsymbol P^{-1/2}\boldsymbol V^{-1/2}
\]</span> and the cross-covariance matrix is <span class="math display">\[
\boldsymbol \Phi^{\text{ZCA-Cor}} = \boldsymbol V^{1/2} \boldsymbol P^{1/2}
\]</span> and the cross-correlation matrix is <span class="math display">\[
\boldsymbol \Psi^{\text{ZCA-Cor}} = \boldsymbol P^{1/2}
\]</span></p>
<p>For the ZCA-cor transformation we also have <span class="math inline">\(\text{Cov}(x_i, z_i) &gt; 0\)</span> and <span class="math inline">\(\text{Cor}(x_i, z_i) &gt; 0\)</span> so that two corresponding components <span class="math inline">\(x_i\)</span> and <span class="math inline">\(z_i\)</span> are always positively correlated!</p>
<p><em>Proportion of total variation</em>:</p>
<p>For ZCA-cor with <span class="math inline">\(\boldsymbol Q_2=\boldsymbol I\)</span> we find that <span class="math inline">\(\boldsymbol k=\text{Diag}(\boldsymbol P) = d\)</span> with all <span class="math inline">\(k_i =1\)</span>. Thus, in ZCA-cor each whitened component <span class="math inline">\(z_i\)</span> contributes equally to the total variation <span class="math inline">\(\text{Tr}(\boldsymbol P) =d\)</span>, with relative proportion <span class="math inline">\(\frac{1}{d}\)</span>.</p>
<p><em>Summary</em>:</p>
<ul>
<li>ZCA-cor whitening is the unique whitening transformation maximising the total correlation between corresponding elements in <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol z\)</span>.</li>
<li>ZCA-cor leads to interpretable <span class="math inline">\(\boldsymbol z\)</span> because each individual element in <span class="math inline">\(\boldsymbol z\)</span> is (typically strongly) positively correlated with the corresponding element in the original <span class="math inline">\(\boldsymbol x\)</span>.</li>
<li>As ZCA-cor is explicitly constructed to maximise the total pairwise correlations it achieves higher total correlation than ZCA.</li>
<li>If <span class="math inline">\(\boldsymbol x\)</span> is standardised to <span class="math inline">\(\text{Var}(x_i)=1\)</span> then ZCA and ZCA-cor are identical.</li>
</ul>
</section>
<section id="pca-whitening" class="level3">
<h3 class="anchored" data-anchor-id="pca-whitening">PCA whitening</h3>
<p><em>Aim</em>: remove correlations and at the same time compress information into a few latent variables. Specifically, we would like that the first latent component <span class="math inline">\(z_1\)</span> is maximally linked with all variables in <span class="math inline">\(\boldsymbol x\)</span>, followed by the second component <span class="math inline">\(z_2\)</span> and so on:</p>
<p><span class="math display">\[
\begin{array}{c}
z_1 \rightarrow   x_1, x_2, \ldots, x_d \\
z_2 \rightarrow   x_1, x_2, \ldots, x_d \\
\ldots\\
z_d \rightarrow   x_1, x_2, \ldots, x_d \\
\end{array}
\]</span> One way to measure the total association of the latent component <span class="math inline">\(z_j\)</span> with all the original <span class="math inline">\(x_1, \ldots, x_d\)</span> is the sum of the corresponding squared covariances <span class="math display">\[
h_j = \sum^d_{i=1}\text{Cov}(x_i,z_j)^2 = \sum^d_{i=1} \phi_{ij}^2
\]</span> or equivalently the <strong>column sum of squares</strong> of <span class="math inline">\(\boldsymbol \Phi\)</span> <span class="math display">\[
\boldsymbol h= (h_1,\ldots,h_d)^T = \text{Diag}(\boldsymbol \Phi^T\boldsymbol \Phi) = \text{Diag}\left(\boldsymbol Q_1\boldsymbol \Sigma\boldsymbol Q_1^T\right)
\]</span> Each <span class="math inline">\(h_j\)</span> is the contribution of <span class="math inline">\(z_j\)</span> to <span class="math inline">\(\text{Tr}\left( \boldsymbol Q_1 \boldsymbol \Sigma\boldsymbol Q_1^T  \right)= \text{Tr}(\boldsymbol \Sigma)\)</span> i.e.&nbsp;to the total variation based on <span class="math inline">\(\boldsymbol \Sigma\)</span>. As <span class="math inline">\(\text{Tr}(\boldsymbol \Sigma)\)</span> is constant this implies that there are only <span class="math inline">\(d-1\)</span> independent <span class="math inline">\(h_j\)</span>.</p>
<p>In PCA-whitening we wish to concentrate most of the contributions to the total variation based on <span class="math inline">\(\boldsymbol \Sigma\)</span> in a small number of latent components.</p>
<p><em>PCA whitening objective function</em>: find an optimal optimal <span class="math inline">\(\boldsymbol Q_1\)</span> so that the resulting set <span class="math inline">\(h_1 \geq h_2 \ldots \geq h_d\)</span> in <span class="math inline">\(\boldsymbol h= \text{Diag}\left(\boldsymbol Q_1\boldsymbol \Sigma\boldsymbol Q_1^T\right)\)</span> majorizes any other set of relative contributions.</p>
<p><em>Solution:</em></p>
<p>Following the earlier discussion we apply Schur’s theorem and find the optimal solution by diagonalising <span class="math inline">\(\boldsymbol \Phi^T\boldsymbol \Phi\)</span> through eigendecomposition of <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span>. Hence, the optimal value for the <span class="math inline">\(\boldsymbol Q_1\)</span> matrix is <span class="math display">\[
\boldsymbol Q_1^{\text{PCA}}=\boldsymbol U^T
\]</span> However, recall that <span class="math inline">\(\boldsymbol U\)</span> is not uniquely defined — you are free to change the columns signs. The corresponding whitening matrix is <span class="math display">\[
\boldsymbol W^{\text{PCA}} = \boldsymbol U^T\boldsymbol \Sigma^{-1/2}=\boldsymbol \Lambda^{-1/2}\boldsymbol U^T
\]</span><br>
and the cross-covariance matrix is <span class="math display">\[
\boldsymbol \Phi^{\text{PCA}} = \boldsymbol U\boldsymbol \Lambda^{1/2}
\]</span> and the cross-correlation matrix is <span class="math display">\[
\boldsymbol \Psi^{\text{PCA}} = \boldsymbol V^{-1/2} \boldsymbol U\boldsymbol \Lambda^{1/2}
\]</span></p>
<p><em>Identifiability:</em></p>
<p>Note that all of the above (i.e.&nbsp;<span class="math inline">\(\boldsymbol Q_1^{\text{PCA}}, \boldsymbol W^{\text{PCA}}, \boldsymbol \Phi^{\text{PCA}}, \boldsymbol \Psi^{\text{PCA}}\)</span>) is not unique due to the sign ambiguity in the columns of <span class="math inline">\(\boldsymbol U\)</span>.</p>
<p>Therefore, for identifiability reasons we may wish to impose a further constraint on <span class="math inline">\(\boldsymbol Q_1^{\text{PCA}}\)</span> or equivalently <span class="math inline">\(\boldsymbol \Phi^{\text{PCA}}\)</span>. A useful condition is to require (for the given ordering of the original variables!) that <span class="math inline">\(\boldsymbol Q_1^{\text{PCA}}\)</span> has a positive diagonal or equivalently that <span class="math inline">\(\boldsymbol \Phi^{\text{PCA}}\)</span> has a positive diagonal. This implies that <span class="math inline">\(\text{Diag}(\boldsymbol U) &gt; 0\)</span> and <span class="math inline">\(\text{Diag}(\boldsymbol \Psi^{\text{PCA}}) &gt; 0\)</span>, hence all pairs <span class="math inline">\(x_i\)</span> and <span class="math inline">\(z_i\)</span> are positively correlated.</p>
<p>It is particularly important to pay attention to the sign ambiguity when comparing different computer implementations of PCA whitening (and the related PCA approach).</p>
<p>Note that the actual objective of PCA whitening <span class="math inline">\(\text{Diag}(\boldsymbol \Phi^T\boldsymbol \Phi)\)</span> is not affected by the sign ambiguity since the column signs of <span class="math inline">\(\boldsymbol \Phi\)</span> do not matter.</p>
<p><em>Proportion of total variation:</em></p>
<p>In PCA whitening the contribution <span class="math inline">\(h_i^{\text{PCA}}\)</span> of each latent component <span class="math inline">\(z_i\)</span> to the total variation based on the covariance <span class="math inline">\(\text{Tr}(\boldsymbol \Sigma) = \sum_{j=1}^d \lambda_j\)</span> is <span class="math inline">\(h_i^{\text{PCA}} = \lambda_i\)</span>. The fraction <span class="math inline">\(\frac{\lambda_i}{\sum^d_{j=1}\lambda_j}\)</span> is the relative contribution of each element in <span class="math inline">\(\boldsymbol z\)</span> to explain the total variation.</p>
<p>Thus, low ranking components <span class="math inline">\(z_i\)</span> with small <span class="math inline">\(h_i^{\text{PCA}}=\lambda_i\)</span> may be discarded. In this way PCA whitening achieves both compression and dimension reduction.</p>
<p><em>Summary:</em></p>
<ul>
<li>PCA whitening is a whitening transformation that maximises compression with the sum of squared cross-covariances as underlying optimality criterion.</li>
<li>There are sign ambiguities in the PCA whitened variables which are inherited from the sign ambiguities in eigenvectors.</li>
<li>If a positive-diagonal condition on the orthogonal matrices is imposed then these sign ambiguities are fully resolved and corresponding components <span class="math inline">\(z_i\)</span> and <span class="math inline">\(x_i\)</span> are always positively correlated.</li>
</ul>
</section>
<section id="pca-cor-whitening" class="level3">
<h3 class="anchored" data-anchor-id="pca-cor-whitening">PCA-cor whitening</h3>
<p><em>Aim</em>: same as for PCA whitening but remove scale in <span class="math inline">\(\boldsymbol x\)</span> first. This means we use squared correlations rather than squared covariances to measure compression, i.e.<br>
<span class="math display">\[
k_j = \sum^d_{i=1}\text{Cor}(x_i, z_j)^2 = \sum^d_{i=1} \psi_{ij}^2
\]</span> or in vector notation the <strong>column sum of squares</strong> of <span class="math inline">\(\boldsymbol \Psi\)</span> <span class="math display">\[
\boldsymbol k= (k_1,\ldots,k_d)^T = \text{Diag}\left(\boldsymbol \Psi^T\boldsymbol \Psi\right)=\text{Diag}\left(\boldsymbol Q_2\boldsymbol P\boldsymbol Q_2^T\right)
\]</span> Each <span class="math inline">\(k_j\)</span> is the contribution of <span class="math inline">\(z_j\)</span> to <span class="math inline">\(\text{Tr}\left( \boldsymbol Q_2 \boldsymbol P\boldsymbol Q_2^T  \right)= \text{Tr}(\boldsymbol P) = d\)</span> i.e.&nbsp;the total variation based on <span class="math inline">\(\boldsymbol P\)</span>. As <span class="math inline">\(\text{Tr}(\boldsymbol P)=d\)</span> is constant this implies that there are only <span class="math inline">\(d-1\)</span> independent <span class="math inline">\(k_j\)</span>.</p>
<p>In PCA-cor-whitening we wish to concentrate most of the contributions to the total variation based on <span class="math inline">\(\boldsymbol P\)</span> in a small number of latent components.</p>
<p><em>PCA-cor whitening objective function</em>: find an optimal optimal <span class="math inline">\(\boldsymbol Q_2\)</span> so that the resulting set <span class="math inline">\(k_1 \geq k_2 \ldots \geq k_d\)</span> in <span class="math inline">\(\boldsymbol k= \text{Diag}\left(\boldsymbol Q_2\boldsymbol P\boldsymbol Q_2^T\right)\)</span> majorizes any other set of relative contributions.</p>
<p><em>Solution:</em></p>
<p>Following the earlier discussion we apply Schur’s theorem and find the optimal solution by diagonalising <span class="math inline">\(\boldsymbol \Psi^T\boldsymbol \Psi\)</span> through eigendecomposition of <span class="math inline">\(\boldsymbol P= \boldsymbol G\boldsymbol \Theta\boldsymbol G^T\)</span>. Hence, the optimal value for the <span class="math inline">\(\boldsymbol Q_2\)</span> matrix is <span class="math display">\[
\boldsymbol Q_2^{\text{PCA-Cor}}=\boldsymbol G^T
\]</span> Again <span class="math inline">\(\boldsymbol G\)</span> is not uniquely defined — you are free to change signs of the columns. The corresponding whitening matrix is <span class="math display">\[
\boldsymbol Q_2^{\text{PCA-Cor}}=\boldsymbol G^T
\]</span> The corresponding whitening matrix is<br>
<span class="math display">\[
\boldsymbol W^{\text{PCA-Cor}} = \boldsymbol \Theta^{-1/2} \boldsymbol G^T \boldsymbol V^{-1/2}
\]</span><br>
and the cross-covariance matrix is <span class="math display">\[
\boldsymbol \Phi^{\text{PCA-Cor}} = \boldsymbol V^{1/2} \boldsymbol G\boldsymbol \Theta^{1/2}
\]</span> and the cross-correlation matrix is <span class="math display">\[
\boldsymbol \Psi^{\text{PCA-Cor}} = \boldsymbol G\boldsymbol \Theta^{1/2}
\]</span></p>
<p><em>Identifiability:</em></p>
<p>As with PCA whitening, there are sign ambiguities in the above because the column signs of <span class="math inline">\(\boldsymbol G\)</span> can be freely chosen. For identifiability we may wish to impose further constraints on <span class="math inline">\(\boldsymbol Q_2^{\text{PCA-Cor}}\)</span> or equivalently on <span class="math inline">\(\boldsymbol \Psi^{\text{PCA-Cor}}\)</span>. A useful condition is to require (for the given ordering of the original variables!) that the diagonal elements of <span class="math inline">\(\boldsymbol Q_2^{\text{PCA-Cor}}\)</span> are all positive or equivalently that <span class="math inline">\(\boldsymbol \Psi^{\text{PCA-Cor}}\)</span> has a positive diagonal. This implies that <span class="math inline">\(\text{Diag}(\boldsymbol G) &gt; 0\)</span> and <span class="math inline">\(\text{Diag}(\boldsymbol \Phi^{\text{PCA-Cor}}) &gt; 0\)</span>.</p>
<p>Note that the actual objective of PCA-cor whitening <span class="math inline">\(\text{Diag}(\boldsymbol \Psi^T\boldsymbol \Psi)\)</span> is not affected by the sign ambiguity since the column signs of <span class="math inline">\(\boldsymbol \Psi\)</span> do not matter.</p>
<p><em>Proportion of total variation:</em></p>
<p>In PCA-cor whitening the contribution <span class="math inline">\(k_i^{\text{PCA-Cor}}\)</span> of each latent component <span class="math inline">\(z_i\)</span> to the total variation based on the correlation <span class="math inline">\(\text{Tr}(\boldsymbol P) = d\)</span> is <span class="math inline">\(k_i^{\text{PCA-Cor}} = \theta_i\)</span>. The fraction <span class="math inline">\(\frac{\theta_i}{d}\)</span> is the relative contribution of each element in <span class="math inline">\(\boldsymbol z\)</span> to explain the total variation.</p>
<p><em>Summary:</em></p>
<ul>
<li>PCA-cor whitening is a whitening transformation that maximises compression with the sum of squared cross-correlations as underlying optimality criterion.</li>
<li>There are sign ambiguities in the PCA-cor whitened variables which are inherited from the sign ambiguities in the eigenvectors.</li>
<li>If a positive-diagonal condition on the orthogonal matrices is imposed then these sign ambiguities are fully resolved and corresponding components <span class="math inline">\(z_i\)</span> and <span class="math inline">\(x_i\)</span> are always positively correlated.</li>
<li>If <span class="math inline">\(\boldsymbol x\)</span> is standardised to <span class="math inline">\(\text{Var}(x_i)=1\)</span>, then PCA and PCA-cor whitening are identical.</li>
</ul>
</section>
<section id="cholesky-whitening" class="level3">
<h3 class="anchored" data-anchor-id="cholesky-whitening">Cholesky whitening</h3>
<p><strong>Cholesky matrix decomposition:</strong></p>
<p>The Cholesky decomposition of a square matrix <span class="math inline">\(\boldsymbol A= \boldsymbol L\boldsymbol L^T\)</span> requires a positive definite <span class="math inline">\(\boldsymbol A\)</span> and is unique. <span class="math inline">\(\boldsymbol L\)</span> is a lower triangular matrix with positive diagonal elements. Its inverse <span class="math inline">\(\boldsymbol L^{-1}\)</span> is also lower triangular with positive diagonal elements. If <span class="math inline">\(\boldsymbol D\)</span> is a diagonal matrix with positive elements then <span class="math inline">\(\boldsymbol D\boldsymbol L\)</span> is also a lower triangular matrix with a positive diagonal and the Cholesky factor for the matrix <span class="math inline">\(\boldsymbol D\boldsymbol A\boldsymbol D\)</span>.</p>
<p><strong>Aim in Cholesky whitening:</strong></p>
<p>Find a whitening transformation such that the cross-covariance <span class="math inline">\(\boldsymbol \Phi\)</span> and cross-correlation <span class="math inline">\(\boldsymbol \Psi\)</span> have lower triangular structure. Specifically, we wish that the the first whitened variable <span class="math inline">\(z_1\)</span> is linked to all original variables <span class="math inline">\(x_1, \ldots, x_d\)</span>, the second latent variable <span class="math inline">\(z_2\)</span> is linked to <span class="math inline">\(x_2, \ldots, x_d\)</span>, and so on, and the last variable <span class="math inline">\(z_d\)</span> is linked only to <span class="math inline">\(x_d\)</span>. <span class="math display">\[
\begin{array}{cc}
z_1 \rightarrow &amp;  x_1, x_2, x_3, \ldots, x_d \\
z_2 \rightarrow &amp;  x_2, x_3, \ldots, x_d \\
z_3 \rightarrow &amp;  x_3, \ldots, x_d \\
\ldots\\
z_d \rightarrow &amp; x_d \\
\end{array}
\]</span> We also assume that <span class="math inline">\(\text{Cor}(x_i, z_i)&gt; 0\)</span>, i.e.&nbsp;that the cross-correlations between corresponding pairs of original and whitened variables are positive. This requirement of a positive diagonal <span class="math inline">\(\boldsymbol \Psi\)</span> ensures the uniqueness of the whitening transformation (similar as in PCA whitening above).</p>
<p>The Cholesky whitening procedure can be viewed as a middle ground between ZCA whitening and PCA whitening.</p>
<p><em>Solution</em>: In order to find such a whitening transformation we use the Cholesky decomposition and apply it to the covariance matrix <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol L\boldsymbol L^T\)</span></p>
<p>The resulting whitening matrix is <span class="math display">\[
\boldsymbol W^{\text{Chol}}=\boldsymbol L^{-1}
\]</span><br>
By construction, <span class="math inline">\(\boldsymbol W^{\text{Chol}}\)</span> is a lower triangular matrix with positive diagonal. The whitening constraint is satisfied as <span class="math inline">\((\boldsymbol W^{\text{Chol}})^T\boldsymbol W^{\text{Chol}} = (\boldsymbol L^{-1})^T \boldsymbol L^{-1} = (\boldsymbol L^T)^{-1}  \boldsymbol L^{-1} = (\boldsymbol L\boldsymbol L^T)^{-1} = \boldsymbol \Sigma^{-1}\)</span>.</p>
<p>The cross-covariance matrix is the inverse of the whitening matrix <span class="math display">\[
\boldsymbol \Phi^{\text{Chol}} = \boldsymbol L
\]</span> and the cross-correlation matrix is <span class="math display">\[
\boldsymbol \Psi^{\text{Chol}} = \boldsymbol V^{-1/2} \boldsymbol L
\]</span> Both <span class="math inline">\(\boldsymbol \Phi^{\text{Chol}}\)</span> and <span class="math inline">\(\boldsymbol \Psi^{\text{Chol}}\)</span> are lower triangular matrices with positive diagonal elements. Hence two corresponding components <span class="math inline">\(x_i\)</span> and <span class="math inline">\(z_i\)</span> are always positively correlated!</p>
<p>Finally, the corresponding orthogonal matrices are <span class="math display">\[
\boldsymbol Q_1^{\text{Chol}}  =  \boldsymbol \Phi^T \boldsymbol \Sigma^{-1/2} =   \boldsymbol L^T \boldsymbol \Sigma^{-1/2}
\]</span> and <span class="math display">\[
\boldsymbol Q_2^{\text{Chol}} =  \boldsymbol \Psi^T \boldsymbol P^{-1/2} =  \boldsymbol L^T \boldsymbol V^{-1/2} \boldsymbol P^{-1/2}
\]</span></p>
<p><em>Application to correlation instead of covariance</em>:</p>
<p>We may also apply the Cholesky decomposition to the correlation rather than the covariance matrix. However, unlike for ZCA and PCA this does <em>not</em> lead to a different whitening transform:</p>
<p>Let’s denote the Cholesky composition of the correlation matrix by <span class="math inline">\(\boldsymbol P= \boldsymbol L_P \boldsymbol L_P^T\)</span>. Then the corresponding whitening matrix is <span class="math inline">\(\boldsymbol W^{\text{Chol}}_P= \boldsymbol L_P^{-1} \boldsymbol V^{-1/2}\)</span>. As <span class="math inline">\(\boldsymbol P= \boldsymbol V^{-1/2} \boldsymbol \Sigma\boldsymbol V^{-1/2} = \boldsymbol V^{-1/2} \boldsymbol L\boldsymbol L^T \boldsymbol V^{-1/2}\)</span> we see that <span class="math inline">\(\boldsymbol L_P = \boldsymbol V^{-1/2} \boldsymbol L\)</span> and hence <span class="math inline">\(\boldsymbol W^{\text{Chol}}_P = (\boldsymbol V^{-1/2} \boldsymbol L)^{-1} \boldsymbol V^{-1/2} =\boldsymbol L^{-1} =  \boldsymbol W^{\text{Chol}}\)</span>.</p>
<p><em>Dependence on the input order</em>:</p>
<p>Cholesky whitening depends on the ordering of input variables. Each ordering of the original variables will yield a different triangular constraint and thus a different Cholesky whitening transform. For example, by inverting the ordering to <span class="math inline">\(x_d, x_{d-1}, \ldots, x_1\)</span> we effectively enforce an upper triangular shape.</p>
<p>An alternative formulation of Cholesky whitening decomposes the precision matrix rather than the covariance matrix. This yields the upper triangular structure directly and is otherwise fully equivalent to Cholesky whitening based on decomposing the covariance.</p>
</section>
<section id="comparison-of-whitening-procedures---simulated-data" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-whitening-procedures---simulated-data">Comparison of whitening procedures - simulated data</h3>
<p>For comparison, <a href="#fig-compwhite" class="quarto-xref">Figure&nbsp;<span>3.1</span></a> shows the results of ZCA, PCA and Cholesky whitening applied to a simulated bivariate normal data set with correlation <span class="math inline">\(\rho=0.8\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-compwhite" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="p">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-compwhite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-transformations_files/figure-html/fig-compwhite-1.png" class="img-fluid figure-img" data-fig-pos="p" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compwhite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Comparison of ZCA, PCA and Cholesky whitening procedures.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In column 1 you can see the simulated data as scatter plot.</p>
<p>Column 2 shows the scatter plots of the whitened data — as expect all three methods remove correlation and produce an isotropic covariance.</p>
<p>However, the three approaches differ in the cross-correlations. Columns 3 and 4 show the cross-correlations between the first two corresponding components (<span class="math inline">\(x_1\)</span> and <span class="math inline">\(z_1\)</span>, and <span class="math inline">\(x_2\)</span> and <span class="math inline">\(z_2\)</span>) for ZCA, PCA and Cholesky whitening. As expected, in ZCA both pairs show strong correlation, but this is not the case for PCA and Cholesky whitening.</p>
<p>Note that for Cholesky whitening the first component <span class="math inline">\(z_1\)</span> is perfectly positively correlated with the original component <span class="math inline">\(x_1\)</span> because the whitening matrix is lower triangular with a positive diagonal and hence <span class="math inline">\(z_1\)</span> is just <span class="math inline">\(x_1\)</span> multiplied with a positive constant.</p>
</section>
<section id="comparison-of-whitening-procedures---iris-flowers" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-whitening-procedures---iris-flowers">Comparison of whitening procedures - iris flowers</h3>
<p>As an example we consider the well known <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">iris flower data set</a>. It consists of botanical measures (sepal length, sepal width, petal length and petal width) for 150 iris flowers comprising three species (<a href="https://en.wikipedia.org/wiki/Iris_setosa">Iris setosa</a>, <a href="https://en.wikipedia.org/wiki/Iris_versicolor">Iris versicolor</a>, <a href="https://en.wikipedia.org/wiki/Iris_virginica">Iris virginica</a>). Hence this data set has dimension <span class="math inline">\(d=4\)</span> and sample size <span class="math inline">\(n=150\)</span>.</p>
<p>We apply all discussed whitening transforms to this data, and then sort the whitened components by their relative contribution to the total variation. For Cholesky whitening we used the input order for the shape constraint.</p>
<p><a href="#fig-irisexpvarcov" class="quarto-xref">Figure&nbsp;<span>3.2</span></a> shows the results for explained variation based on covariance loadings:</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-irisexpvarcov" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="p">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-irisexpvarcov-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-transformations_files/figure-html/fig-irisexpvarcov-1.png" class="img-fluid figure-img" data-fig-pos="p" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-irisexpvarcov-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Cumulative explained variation (covariance loadings) for various whitening procedures for the Iris flower data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>As expected, the two PCA whitening approaches compress the data most. On the other end of the spectrum, the ZCA whitening methods are the two least compressing approaches. Cholesky whitening is a compromise between ZCA and PCA in terms of compression.</p>
<p>Similar results are obtained based on correlation loadings (<a href="#fig-irisexpvarcor" class="quarto-xref">Figure&nbsp;<span>3.3</span></a>). Note how ZCA-cor provides equal weight for each latent variable.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-irisexpvarcor" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="p">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-irisexpvarcor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-transformations_files/figure-html/fig-irisexpvarcor-1.png" class="img-fluid figure-img" data-fig-pos="p" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-irisexpvarcor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: Cumulative explained variation (correlation-loadings) for various whitening procedures for the Iris flower data.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="recap" class="level3">
<h3 class="anchored" data-anchor-id="recap">Recap</h3>
<p>See <a href="#tbl-appwhitening" class="quarto-xref">Table&nbsp;<span>3.1</span></a> for a summery of the usuage types for the various whitening procedures.</p>
<p>If data are standardised then <span class="math inline">\(\boldsymbol \Phi\)</span> and <span class="math inline">\(\boldsymbol \Psi\)</span> will be the same and hence ZCA will become ZCA-cor and PCA becomes PCA-cor. The triangular shape constraint of Cholesky whitening depends on the ordering of the original variables.</p>
<div id="tbl-appwhitening" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-appwhitening-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Applications of natural whitening procedures.
</figcaption>
<div aria-describedby="tbl-appwhitening-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 68%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Type of usage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ZCA, ZCA-cor:</td>
<td>pure decorrelate, maintain similarity to original data set, interpretability</td>
</tr>
<tr class="even">
<td>PCA, PCA-cor:</td>
<td>compression, find effective dimension, reduce dimensionality, feature identification</td>
</tr>
<tr class="odd">
<td>Cholesky</td>
<td>triangular shaped <span class="math inline">\(\boldsymbol W\)</span>, <span class="math inline">\(\boldsymbol \Phi\)</span> and <span class="math inline">\(\boldsymbol \Psi\)</span>, sparsity</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="principal-component-analysis-pca" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="principal-component-analysis-pca"><span class="header-section-number">3.5</span> Principal Component Analysis (PCA)</h2>
<section id="pca-transformation" class="level3">
<h3 class="anchored" data-anchor-id="pca-transformation">PCA transformation</h3>
<p>Principal component analysis was proposed in 1933 by Harald Hotelling <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and is very closely related to <strong>PCA whitening</strong>. The underlying mathematics was developed earlier in 1901 by Karl Pearson <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> for the problem of orthogonal regression.</p>
<p>Assume random vector <span class="math inline">\(\boldsymbol x\)</span> with <span class="math inline">\(\text{Var}(\boldsymbol x) = \boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span>. PCA is a particular orthogonal transformation (<a href="#exm-orthotrans" class="quarto-xref">Example&nbsp;<span>3.5</span></a>) of the original <span class="math inline">\(\boldsymbol x\)</span> such that the resulting components are orthogonal: <span class="math display">\[
\underbrace{\boldsymbol t^{\text{PCA}}}_{\text{Principal components}} = \underbrace{\boldsymbol U^T}_{\text{Orthogonal matrix}}   \boldsymbol x
\]</span> <span class="math display">\[\text{Var}(\boldsymbol t^{\text{PCA}}) = \boldsymbol \Lambda= \begin{pmatrix} \lambda_1 &amp; \dots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \dots &amp; \lambda_d\end{pmatrix}\]</span> Note that while principal components are <em>orthogonal</em> they do <em>not</em> have unit variance. Instead, the variance of the principal components <span class="math inline">\(t_i\)</span> is equal to the eigenvalues <span class="math inline">\(\lambda_i\)</span>.</p>
<p>Thus PCA itself is <em>not</em> a whitening procedure but it is very closely linked to PCA whitening which is obtained by standardising the principal components to unit variance: <span class="math inline">\(\boldsymbol z^{\text{PCA}} = \boldsymbol \Lambda^{-1/2} \boldsymbol t^{\text{PCA}} = \boldsymbol \Lambda^{-1/2} \boldsymbol U^T \boldsymbol x
= \boldsymbol U^T \boldsymbol \Sigma^{-1/2} \boldsymbol x= \boldsymbol Q_1^{\text{PCA}} \boldsymbol \Sigma^{-1/2} \boldsymbol x= \boldsymbol W^{\text{PCA}} \boldsymbol x\)</span></p>
<p><strong>Compression properties:</strong></p>
<p>The total variation is <span class="math inline">\(\text{Tr}(\text{Var}(\boldsymbol t^{\text{PCA}})) = \text{Tr}( \boldsymbol \Lambda) = \sum^d_{j=1}\lambda_j\)</span>. With principle components the fraction <span class="math inline">\(\frac{\lambda_i}{\sum^d_{j=1}\lambda_j}\)</span> can be interpreted as the proportion of variation contributed by each component in <span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span> to the total variation. Thus, low ranking components in <span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span> with low variation may be discarded, thus leading to a reduction in dimension.</p>
</section>
<section id="application-to-data" class="level3">
<h3 class="anchored" data-anchor-id="application-to-data">Application to data</h3>
<p>Written in terms of a data matrix <span class="math inline">\(\boldsymbol X\)</span> instead of a random vector <span class="math inline">\(\boldsymbol x\)</span> PCA becomes: <span class="math display">\[\underbrace{\boldsymbol T}_{\text{Sample version of principal components}}=\underbrace{\boldsymbol X}_{\text{Data matrix}}\boldsymbol U\]</span> There are now two ways to obtain <span class="math inline">\(\boldsymbol U\)</span>:</p>
<ol type="1">
<li><p>Estimate the covariance matrix, e.g.&nbsp;by <span class="math inline">\(\hat{\boldsymbol \Sigma} = \frac{1}{n}\boldsymbol X_c^T\boldsymbol X_c\)</span> where <span class="math inline">\(\boldsymbol X_c\)</span> is the column-centred data matrix; then apply the eigenvalue decomposition on <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> to get <span class="math inline">\(\boldsymbol U\)</span>.</p></li>
<li><p>Compute the singular value decomposition of <span class="math inline">\(\boldsymbol X_c = \boldsymbol V\boldsymbol D\boldsymbol U^T\)</span>. As <span class="math inline">\(\hat{\boldsymbol \Sigma} = \frac{1}{n}\boldsymbol X_c^T\boldsymbol X_c = \boldsymbol U(\frac{1}{n}\boldsymbol D^2)\boldsymbol U^T\)</span> you can just use <span class="math inline">\(\boldsymbol U\)</span> from the SVD of <span class="math inline">\(\boldsymbol X_c\)</span> and there is no need to compute the covariance.</p></li>
</ol>
</section>
<section id="iris-flower-data-example" class="level3">
<h3 class="anchored" data-anchor-id="iris-flower-data-example">Iris flower data example</h3>
<p>We first standardise the data, then compute PCA components and plot the proportion of total variation contributed by each component. <a href="#fig-irispcatotvar" class="quarto-xref">Figure&nbsp;<span>3.4</span></a> shows that only two PCA components are needed to achieve 95% of the total variation.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-irispcatotvar" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-irispcatotvar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-transformations_files/figure-html/fig-irispcatotvar-1.png" class="img-fluid figure-img" data-fig-pos="t" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-irispcatotvar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: PCA proportion of total variation for the iris flower data.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-pcascatteriris" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pcascatteriris-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-transformations_files/figure-html/fig-pcascatteriris-1.png" class="img-fluid figure-img" data-fig-pos="t" width="432">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pcascatteriris-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: Scatter plot of first two principal components.
</figcaption>
</figure>
</div>
</div>
</div>
<p>A scatter plot plot of the the first two principal components is also informative (<a href="#fig-pcascatteriris" class="quarto-xref">Figure&nbsp;<span>3.5</span></a>). Specifically, it shows that there are groupings among the 150 iris flowers, corresponding to the three known species, and that these three groups can be characterised by looking at just the first two principal components (rather than at all four components).</p>
</section>
<section id="pca-correlation-loadings" class="level3">
<h3 class="anchored" data-anchor-id="pca-correlation-loadings">PCA correlation loadings</h3>
<p>In an earlier section we have learned that for a general whitening transformation the cross-correlations <span class="math inline">\(\boldsymbol \Psi=\text{Cor}(\boldsymbol x, \boldsymbol z)\)</span> play the role of correlation loadings in the inverse transformation: <span class="math display">\[
\boldsymbol V^{-1/2} \boldsymbol x= \boldsymbol \Psi\boldsymbol z\, ,
\]</span> i.e.&nbsp;they are the coefficients linking the whitened variable <span class="math inline">\(\boldsymbol z\)</span> with the standardised original variable <span class="math inline">\(\boldsymbol x\)</span>. This relationship holds therefore also for PCA-whitening with <span class="math inline">\(\boldsymbol z^{\text{PCA}}= \boldsymbol \Lambda^{-1/2} \boldsymbol U^T \boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol \Psi^{\text{PCA}} = \boldsymbol V^{-1/2} \boldsymbol U\boldsymbol \Lambda^{1/2}\)</span>.</p>
<p>The classical PCA is not a whitening approach because <span class="math inline">\(\text{Var}(\boldsymbol t^{\text{PCA}}) \neq \boldsymbol I\)</span>. However, we can still compute cross-correlations between <span class="math inline">\(\boldsymbol x\)</span> and the principal components <span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span>, resulting in <span class="math display">\[
\text{Cor}(\boldsymbol x, \boldsymbol t^{\text{PCA}}) = \boldsymbol V^{-1/2} \boldsymbol U\boldsymbol \Lambda^{1/2}  = \boldsymbol \Psi^{\text{PCA}}
\]</span> Note these are the same as the cross-correlations for PCA-whitening since <span class="math inline">\(\boldsymbol t^{\text{PCA}}\)</span> and <span class="math inline">\(\boldsymbol z^{\text{PCA}}\)</span> only differ in scale.</p>
<p>The inverse PCA transformation is <span class="math display">\[
\boldsymbol x= \boldsymbol U\boldsymbol t^{\text{PCA}}
\]</span> In terms of standardised PCA components <span class="math inline">\(\boldsymbol z^{\text{PCA}} = \boldsymbol \Lambda^{-1/2} \boldsymbol t^{\text{PCA}}\)</span> and standardised original components it becomes <span class="math display">\[
\boldsymbol V^{-1/2} \boldsymbol x= \boldsymbol \Psi\boldsymbol \Lambda^{-1/2} \boldsymbol t^{\text{PCA}}
\]</span> Thus the cross-correlation matrix <span class="math inline">\(\boldsymbol \Psi\)</span> plays the role of <em>correlation loadings</em> also in classical PCA, i.e.&nbsp;they are the coefficients linking the standardised PCA components with the standardised original components.</p>
</section>
<section id="pca-correlation-loadings-plot" class="level3">
<h3 class="anchored" data-anchor-id="pca-correlation-loadings-plot">PCA correlation loadings plot</h3>
<p>In PCA and PCA-cor whitening as well as in classical PCA the aim is compression, i.e. to find latent variables such that most of the total variation is contributed by a small number of components.</p>
<p>In order to be able to better interpret the top ranking PCA component we can use a visual device called <em>correlation loadings plot</em>. For this we compute the correlation between the PCA components 1 and 2 (<span class="math inline">\(t_1^{\text{PCA}}\)</span> and <span class="math inline">\(t_2^{\text{PCA}})\)</span> with all original variables <span class="math inline">\(x_1, \ldots, x_d\)</span>.</p>
<p>For each original variable <span class="math inline">\(x_i\)</span> we therefore have two numbers between -1 and 1, the correlation <span class="math inline">\(\text{Cor}(x_i, t_1^{\text{PCA}}) = \psi_{i1}\)</span> and <span class="math inline">\(\text{Cor}(x_i, t_2^{\text{PCA}}) = \psi_{i2}\)</span> that we use as coordinates to draw a point in a plane. Recall that the row sums of squares of the correlation loadings <span class="math inline">\(\boldsymbol \Psi\)</span> are all identical to 1. Hence, the sum of the squared loadings from just the first two components is also at most 1. Thus, by construction, all points have to lie within a unit circle around the origin.<br>
The original variables most strongly influenced by the two latent variables will have strong correlation and thus lie near the outer circle, whereas variables that are not influenced by the two latent variables will lie near the origin.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-pcacorrloadings" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pcacorrloadings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-transformations_files/figure-html/fig-pcacorrloadings-1.png" class="img-fluid figure-img" data-fig-pos="t" width="432">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pcacorrloadings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: Correlation loadings plot between the first two principal correlations and the original variables.
</figcaption>
</figure>
</div>
</div>
</div>
<p>For illustration <a href="#fig-pcacorrloadings" class="quarto-xref">Figure&nbsp;<span>3.6</span></a> shows the correlation loadings plot for the correlation between the first two PCA components and all four variables of the iris flower data set discussed earlier.</p>
<p>The interpretation of this plot is discussed in Worksheet 5.</p>
</section>
<section id="outlook" class="level3">
<h3 class="anchored" data-anchor-id="outlook">Outlook</h3>
<p>Related methods not discussed in this course:</p>
<ul>
<li><p>Factor models: essentially this is a probabilistic version of whitening / PCA with dimension reduction and an additional error term. Factors have rotational freedom exactly as whitened variables.</p></li>
<li><p>Partial Least Squares (PLS): similar to Principal Components Analysis (PCA) but in a regression setting, with the choice of latent variables depending both on predictors and on the response variable. One can also use PCA with regression (yielding principal components regression, PCR) but in this case the PCA components only depend on the predictor variables.</p></li>
<li><p>Nonlinear dimension reduction methods such as SNE, tSNE and UMAP.</p></li>
</ul>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Gorroochurn, P. 2020. Who Invented the Delta Method, Really? The Mathematical Intelligencer <strong>42</strong>:46–49. <a href="https://doi.org/10.1007/s00283-020-09982-0" class="uri">https://doi.org/10.1007/s00283-020-09982-0</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Kobyzev et al.&nbsp;2021. <em>Normalizing Flows: Introduction and Ideas</em>. <a href="https://doi.org/10.1109/TPAMI.2020.2992934">IEEE Trans. Pattern Anal. Mach. Intell. <strong>43</strong>:3964-3979</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Papamakarios et al.&nbsp;2021. <em>Normalizing Flows for Probabilistic Modeling and Inference</em>.<br>
<a href="https://jmlr.csail.mit.edu/papers/v22/19-1028.html">JMLR <strong>22</strong>:1-64</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Hotelling, H. 1933. Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology <strong>24</strong>:417–441 (Part 1) and <strong>24</strong>:498–520 (Part 2). <a href="https://doi.org/10.1037/h0071325" class="uri">https://doi.org/10.1037/h0071325</a> and <a href="https://doi.org/10.1037/h0070888" class="uri">https://doi.org/10.1037/h0070888</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Pearson, K. 1901. On lines and planes of closest fit to systems of points in space. Philosophical Magazine <strong>2</strong>:559–572. <a href="https://doi.org/10.1080/14786440109462720" class="uri">https://doi.org/10.1080/14786440109462720</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/strimmerlab\.github\.io\/publications\/lecture-notes\/MATH38161");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02-estimation.html" class="pagination-link" aria-label="Multivariate estimation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multivariate estimation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04-clustering.html" class="pagination-link" aria-label="Unsupervised learning and clustering">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised learning and clustering</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>