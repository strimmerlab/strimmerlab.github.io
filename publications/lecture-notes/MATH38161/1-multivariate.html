<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Multivariate random variables | Multivariate Statistics and Machine Learning MATH38161</title>
  <meta name="description" content="1 Multivariate random variables | Multivariate Statistics and Machine Learning MATH38161" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Multivariate random variables | Multivariate Statistics and Machine Learning MATH38161" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Multivariate random variables | Multivariate Statistics and Machine Learning MATH38161" />
  
  
  

<meta name="author" content="Korbinian Strimmer" />


<meta name="date" content="2020-09-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="2-transformations.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH38161/index.html">MATH38161 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-home-page"><i class="fa fa-check"></i>Course home page</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#video-lectures"><i class="fa fa-check"></i>Video lectures</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#further-study"><i class="fa fa-check"></i>Further study</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recommended-reading"><i class="fa fa-check"></i>Recommended reading</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#advanced-reading"><i class="fa fa-check"></i>Advanced reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-multivariate.html"><a href="1-multivariate.html"><i class="fa fa-check"></i><b>1</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="1.1" data-path="1-multivariate.html"><a href="1-multivariate.html#why-multivariate-statistics"><i class="fa fa-check"></i><b>1.1</b> Why multivariate statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="1-multivariate.html"><a href="1-multivariate.html#basics"><i class="fa fa-check"></i><b>1.2</b> Basics</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-vs.multivariate-random-variables"><i class="fa fa-check"></i><b>1.2.1</b> Univariate vs. multivariate random variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-multivariate.html"><a href="1-multivariate.html#mean-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.2</b> Mean of a random vector</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-multivariate.html"><a href="1-multivariate.html#variance-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.3</b> Variance of a random vector</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-multivariate.html"><a href="1-multivariate.html#properties-of-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.4</b> Properties of the covariance matrix</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-multivariate.html"><a href="1-multivariate.html#eigenvalue-decomposition-of-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.2.5</b> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.2.6" data-path="1-multivariate.html"><a href="1-multivariate.html#quantities-related-to-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.6</b> Quantities related to the covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>1.3</b> Multivariate normal distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-normal-distribution"><i class="fa fa-check"></i><b>1.3.1</b> Univariate normal distribution:</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-model"><i class="fa fa-check"></i><b>1.3.2</b> Multivariate normal model</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-multivariate.html"><a href="1-multivariate.html#shape-of-the-contours-depend-on-the-eigenvalues-of-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.3.3</b> Shape of the contours depend on the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span>:</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-multivariate.html"><a href="1-multivariate.html#discrete-multivariate-distributions"><i class="fa fa-check"></i><b>1.4</b> Discrete multivariate distributions</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-multivariate.html"><a href="1-multivariate.html#categorical-distribution"><i class="fa fa-check"></i><b>1.4.1</b> Categorical distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multinomial-distribution"><i class="fa fa-check"></i><b>1.4.2</b> Multinomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-multivariate.html"><a href="1-multivariate.html#continuous-multivariate-distributions"><i class="fa fa-check"></i><b>1.5</b> Continuous multivariate distributions</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-multivariate.html"><a href="1-multivariate.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.5.1</b> Dirichlet distribution</a></li>
<li class="chapter" data-level="1.5.2" data-path="1-multivariate.html"><a href="1-multivariate.html#wishart-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Wishart distribution</a></li>
<li class="chapter" data-level="1.5.3" data-path="1-multivariate.html"><a href="1-multivariate.html#inverse-wishart-distribution"><i class="fa fa-check"></i><b>1.5.3</b> Inverse Wishart distribution</a></li>
<li class="chapter" data-level="1.5.4" data-path="1-multivariate.html"><a href="1-multivariate.html#further-distributions"><i class="fa fa-check"></i><b>1.5.4</b> Further distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-in-large-sample-and-small-sample-settings"><i class="fa fa-check"></i><b>1.6</b> Estimation in large sample and small sample settings</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-multivariate.html"><a href="1-multivariate.html#data-matrix"><i class="fa fa-check"></i><b>1.6.1</b> Data matrix</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-multivariate.html"><a href="1-multivariate.html#strategies-for-large-sample-estimation"><i class="fa fa-check"></i><b>1.6.2</b> Strategies for large sample estimation</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-multivariate.html"><a href="1-multivariate.html#large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.6.3</b> Large sample estimates of mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.6.4" data-path="1-multivariate.html"><a href="1-multivariate.html#problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions"><i class="fa fa-check"></i><b>1.6.4</b> Problems with maximum likelihood in small sample settings and high dimensions</a></li>
<li class="chapter" data-level="1.6.5" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-of-covariance-matrix-in-small-sample-settings"><i class="fa fa-check"></i><b>1.6.5</b> Estimation of covariance matrix in small sample settings</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-transformations.html"><a href="2-transformations.html"><i class="fa fa-check"></i><b>2</b> Transformations and dimension reduction</a><ul>
<li class="chapter" data-level="2.1" data-path="2-transformations.html"><a href="2-transformations.html#linear-transformations"><i class="fa fa-check"></i><b>2.1</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-transformations.html"><a href="2-transformations.html#location-scale-transformation"><i class="fa fa-check"></i><b>2.1.1</b> Location-scale transformation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-transformations.html"><a href="2-transformations.html#invertible-location-scale-transformation"><i class="fa fa-check"></i><b>2.1.2</b> Invertible location-scale transformation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-transformations.html"><a href="2-transformations.html#nonlinear-transformations"><i class="fa fa-check"></i><b>2.2</b> Nonlinear transformations</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-transformations.html"><a href="2-transformations.html#general-transformation"><i class="fa fa-check"></i><b>2.2.1</b> General transformation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-transformations.html"><a href="2-transformations.html#linearisation-of-boldsymbol-hboldsymbol-x"><i class="fa fa-check"></i><b>2.2.2</b> Linearisation of <span class="math inline">\(\boldsymbol h(\boldsymbol x)\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="2-transformations.html"><a href="2-transformations.html#delta-method"><i class="fa fa-check"></i><b>2.2.3</b> Delta method</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-transformations.html"><a href="2-transformations.html#transformation-of-densities-under-general-invertible-transformation"><i class="fa fa-check"></i><b>2.2.4</b> Transformation of densities under general invertible transformation</a></li>
<li class="chapter" data-level="2.2.5" data-path="2-transformations.html"><a href="2-transformations.html#normalising-flows"><i class="fa fa-check"></i><b>2.2.5</b> Normalising flows</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-transformations.html"><a href="2-transformations.html#whitening-transformations"><i class="fa fa-check"></i><b>2.3</b> Whitening transformations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-transformations.html"><a href="2-transformations.html#overview"><i class="fa fa-check"></i><b>2.3.1</b> Overview</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-transformations.html"><a href="2-transformations.html#general-whitening-transformation"><i class="fa fa-check"></i><b>2.3.2</b> General whitening transformation</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-transformations.html"><a href="2-transformations.html#general-solution-of-whitening-constraint-covariance-based"><i class="fa fa-check"></i><b>2.3.3</b> General solution of whitening constraint (covariance-based)</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-transformations.html"><a href="2-transformations.html#another-solution-correlation-based"><i class="fa fa-check"></i><b>2.3.4</b> Another solution (correlation-based)</a></li>
<li class="chapter" data-level="2.3.5" data-path="2-transformations.html"><a href="2-transformations.html#objective-criteria-for-choosing-among-boldsymbol-w-using-boldsymbol-q_1-or-boldsymbol-q_2"><i class="fa fa-check"></i><b>2.3.5</b> Objective criteria for choosing among <span class="math inline">\(\boldsymbol W\)</span> using <span class="math inline">\(\boldsymbol Q_1\)</span> or <span class="math inline">\(\boldsymbol Q_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-transformations.html"><a href="2-transformations.html#natural-whitening-procedures"><i class="fa fa-check"></i><b>2.4</b> Natural whitening procedures</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-transformations.html"><a href="2-transformations.html#zca-whitening"><i class="fa fa-check"></i><b>2.4.1</b> ZCA Whitening</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-transformations.html"><a href="2-transformations.html#zca-cor-whitening"><i class="fa fa-check"></i><b>2.4.2</b> ZCA-Cor Whitening</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-transformations.html"><a href="2-transformations.html#cholesky-whitening"><i class="fa fa-check"></i><b>2.4.3</b> Cholesky Whitening</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-transformations.html"><a href="2-transformations.html#pca-whitening"><i class="fa fa-check"></i><b>2.4.4</b> PCA Whitening</a></li>
<li class="chapter" data-level="2.4.5" data-path="2-transformations.html"><a href="2-transformations.html#pca-cor-whitening"><i class="fa fa-check"></i><b>2.4.5</b> PCA-cor Whitening</a></li>
<li class="chapter" data-level="2.4.6" data-path="2-transformations.html"><a href="2-transformations.html#comparison-of-zca-pca-and-chol-whitening"><i class="fa fa-check"></i><b>2.4.6</b> Comparison of ZCA, PCA and Chol whitening</a></li>
<li class="chapter" data-level="2.4.7" data-path="2-transformations.html"><a href="2-transformations.html#cca-whitening-canonical-correlation-analysis"><i class="fa fa-check"></i><b>2.4.7</b> CCA whitening (Canonical Correlation Analysis)</a></li>
<li class="chapter" data-level="2.4.8" data-path="2-transformations.html"><a href="2-transformations.html#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal"><i class="fa fa-check"></i><b>2.4.8</b> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</a></li>
<li class="chapter" data-level="2.4.9" data-path="2-transformations.html"><a href="2-transformations.html#recap"><i class="fa fa-check"></i><b>2.4.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-transformations.html"><a href="2-transformations.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>2.5</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-transformations.html"><a href="2-transformations.html#application-to-data"><i class="fa fa-check"></i><b>2.5.1</b> Application to data</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-transformations.html"><a href="2-transformations.html#pca-correlation-loadings-and-plots"><i class="fa fa-check"></i><b>2.5.2</b> PCA correlation loadings and plots</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-transformations.html"><a href="2-transformations.html#iris-data-example"><i class="fa fa-check"></i><b>2.5.3</b> Iris data example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-clustering.html"><a href="3-clustering.html"><i class="fa fa-check"></i><b>3</b> Clustering / unsupervised Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="3-clustering.html"><a href="3-clustering.html#overview-of-clustering"><i class="fa fa-check"></i><b>3.1</b> Overview of clustering</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-clustering.html"><a href="3-clustering.html#general-aim"><i class="fa fa-check"></i><b>3.1.1</b> General aim</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-clustering.html"><a href="3-clustering.html#why-is-clustering-difficult"><i class="fa fa-check"></i><b>3.1.2</b> Why is clustering difficult?</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-clustering.html"><a href="3-clustering.html#common-types-of-clustering-methods"><i class="fa fa-check"></i><b>3.1.3</b> Common types of clustering methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-clustering.html"><a href="3-clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>3.2</b> <span class="math inline">\(K\)</span>-means clustering</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-clustering.html"><a href="3-clustering.html#general-aims"><i class="fa fa-check"></i><b>3.2.1</b> General aims</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-clustering.html"><a href="3-clustering.html#algorithm"><i class="fa fa-check"></i><b>3.2.2</b> Algorithm</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-clustering.html"><a href="3-clustering.html#properties"><i class="fa fa-check"></i><b>3.2.3</b> Properties</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>3.2.4</b> Choosing the number of clusters</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-clustering.html"><a href="3-clustering.html#k-medoids-aka-pam"><i class="fa fa-check"></i><b>3.2.5</b> <span class="math inline">\(K\)</span>-medoids aka PAM</a></li>
<li class="chapter" data-level="3.2.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-k-means-to-iris-data"><i class="fa fa-check"></i><b>3.2.6</b> Application of <span class="math inline">\(K\)</span>-means to Iris data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-clustering.html"><a href="3-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>3.3</b> Hierarchical clustering</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-clustering.html"><a href="3-clustering.html#tree-like-structures"><i class="fa fa-check"></i><b>3.3.1</b> Tree-like structures</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-clustering.html"><a href="3-clustering.html#agglomerative-hierarchical-clustering-algorithms"><i class="fa fa-check"></i><b>3.3.2</b> Agglomerative hierarchical clustering algorithms</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-clustering.html"><a href="3-clustering.html#application-to-swiss-banknote-data-set"><i class="fa fa-check"></i><b>3.3.3</b> Application to Swiss banknote data set</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-clustering.html"><a href="3-clustering.html#mixture-models"><i class="fa fa-check"></i><b>3.4</b> Mixture models</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-clustering.html"><a href="3-clustering.html#finite-mixture-model"><i class="fa fa-check"></i><b>3.4.1</b> Finite mixture model</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-clustering.html"><a href="3-clustering.html#decomposition-of-covariance-and-total-variation"><i class="fa fa-check"></i><b>3.4.2</b> Decomposition of covariance and total variation</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-clustering.html"><a href="3-clustering.html#example-of-mixture-of-three-univariate-normal-densities"><i class="fa fa-check"></i><b>3.4.3</b> Example of mixture of three univariate normal densities:</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-clustering.html"><a href="3-clustering.html#example-of-a-mixture-of-two-bivariate-normal-densities"><i class="fa fa-check"></i><b>3.4.4</b> Example of a mixture of two bivariate normal densities</a></li>
<li class="chapter" data-level="3.4.5" data-path="3-clustering.html"><a href="3-clustering.html#sampling-from-a-mixture-model-and-latent-allocation-variable-formulation"><i class="fa fa-check"></i><b>3.4.5</b> Sampling from a mixture model and latent allocation variable formulation</a></li>
<li class="chapter" data-level="3.4.6" data-path="3-clustering.html"><a href="3-clustering.html#predicting-the-group-allocation-of-a-given-sample"><i class="fa fa-check"></i><b>3.4.6</b> Predicting the group allocation of a given sample</a></li>
<li class="chapter" data-level="3.4.7" data-path="3-clustering.html"><a href="3-clustering.html#direct-estimation-of-mixture-model"><i class="fa fa-check"></i><b>3.4.7</b> Direct estimation of mixture model</a></li>
<li class="chapter" data-level="3.4.8" data-path="3-clustering.html"><a href="3-clustering.html#estimate-mixture-model-using-the-em-algorithm"><i class="fa fa-check"></i><b>3.4.8</b> Estimate mixture model using the EM algorithm</a></li>
<li class="chapter" data-level="3.4.9" data-path="3-clustering.html"><a href="3-clustering.html#em-algorithm-for-multivariate-normal-mixture-model"><i class="fa fa-check"></i><b>3.4.9</b> EM algorithm for multivariate normal mixture model</a></li>
<li class="chapter" data-level="3.4.10" data-path="3-clustering.html"><a href="3-clustering.html#connection-with-k-means-clustering-method"><i class="fa fa-check"></i><b>3.4.10</b> Connection with <span class="math inline">\(K\)</span>-means clustering method</a></li>
<li class="chapter" data-level="3.4.11" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-classes"><i class="fa fa-check"></i><b>3.4.11</b> Choosing the number of classes</a></li>
<li class="chapter" data-level="3.4.12" data-path="3-clustering.html"><a href="3-clustering.html#application-of-gmms-to-iris-flower-data"><i class="fa fa-check"></i><b>3.4.12</b> Application of GMMs to Iris flower data</a></li>
<li class="chapter" data-level="3.4.13" data-path="3-clustering.html"><a href="3-clustering.html#variation-1-infinite-mixture-model"><i class="fa fa-check"></i><b>3.4.13</b> Variation 1: Infinite mixture model</a></li>
<li class="chapter" data-level="3.4.14" data-path="3-clustering.html"><a href="3-clustering.html#variation-2-semiparametric-mixture-model-with-two-classes"><i class="fa fa-check"></i><b>3.4.14</b> Variation 2: Semiparametric mixture model with two classes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-classification.html"><a href="4-classification.html"><i class="fa fa-check"></i><b>4</b> Classification / supervised learning</a><ul>
<li class="chapter" data-level="4.1" data-path="4-classification.html"><a href="4-classification.html#supervised-learning-vs.unsupervised-learning"><i class="fa fa-check"></i><b>4.1</b> Supervised learning vs. unsupervised learning</a></li>
<li class="chapter" data-level="4.2" data-path="4-classification.html"><a href="4-classification.html#terminology"><i class="fa fa-check"></i><b>4.2</b> Terminology</a></li>
<li class="chapter" data-level="4.3" data-path="4-classification.html"><a href="4-classification.html#bayesian-discriminant-rule-or-bayes-classifier"><i class="fa fa-check"></i><b>4.3</b> Bayesian discriminant rule or Bayes classifier</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-classification.html"><a href="4-classification.html#general-model"><i class="fa fa-check"></i><b>4.3.1</b> General model</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-classification.html"><a href="4-classification.html#quadratic-discriminant-analysis-qda-and-gaussian-assumption"><i class="fa fa-check"></i><b>4.3.2</b> Quadratic discriminant analysis (QDA) and Gaussian assumption</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-classification.html"><a href="4-classification.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>4.3.3</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="4.3.4" data-path="4-classification.html"><a href="4-classification.html#diagonal-discriminant-analysis-dda-and-naive-bayes-classifier"><i class="fa fa-check"></i><b>4.3.4</b> Diagonal discriminant analysis (DDA) and naive Bayes classifier</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-classification.html"><a href="4-classification.html#the-training-step-learning-qda-lda-and-dda-classifiers-from-data"><i class="fa fa-check"></i><b>4.4</b> The training step — learning QDA, LDA and DDA classifiers from data</a></li>
<li class="chapter" data-level="4.5" data-path="4-classification.html"><a href="4-classification.html#comparison-of-decision-boundaries-lda-vs.qda"><i class="fa fa-check"></i><b>4.5</b> Comparison of decision boundaries: LDA vs. QDA</a></li>
<li class="chapter" data-level="4.6" data-path="4-classification.html"><a href="4-classification.html#goodness-of-fit-and-variable-selection"><i class="fa fa-check"></i><b>4.6</b> Goodness of fit and variable selection</a><ul>
<li class="chapter" data-level="4.6.1" data-path="4-classification.html"><a href="4-classification.html#lda-with-k2-classes"><i class="fa fa-check"></i><b>4.6.1</b> LDA with <span class="math inline">\(K=2\)</span> classes</a></li>
<li class="chapter" data-level="4.6.2" data-path="4-classification.html"><a href="4-classification.html#multiple-classes"><i class="fa fa-check"></i><b>4.6.2</b> Multiple classes</a></li>
<li class="chapter" data-level="4.6.3" data-path="4-classification.html"><a href="4-classification.html#choosing-a-threshold"><i class="fa fa-check"></i><b>4.6.3</b> Choosing a threshold</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4-classification.html"><a href="4-classification.html#estimating-prediction-error"><i class="fa fa-check"></i><b>4.7</b> Estimating prediction error</a><ul>
<li class="chapter" data-level="4.7.1" data-path="4-classification.html"><a href="4-classification.html#quantifying-prediction-error"><i class="fa fa-check"></i><b>4.7.1</b> Quantifying prediction error</a></li>
<li class="chapter" data-level="4.7.2" data-path="4-classification.html"><a href="4-classification.html#estimation-of-prediction-error-without-test-data"><i class="fa fa-check"></i><b>4.7.2</b> Estimation of prediction error without test data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-dependence.html"><a href="5-dependence.html"><i class="fa fa-check"></i><b>5</b> Multivariate dependencies</a><ul>
<li class="chapter" data-level="5.1" data-path="5-dependence.html"><a href="5-dependence.html#measuring-the-association-between-two-sets-of-random-variables"><i class="fa fa-check"></i><b>5.1</b> Measuring the association between two sets of random variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-dependence.html"><a href="5-dependence.html#rozeboom-vector-correlation"><i class="fa fa-check"></i><b>5.1.1</b> Rozeboom vector correlation</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-dependence.html"><a href="5-dependence.html#other-common-approaches"><i class="fa fa-check"></i><b>5.1.2</b> Other common approaches</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-dependence.html"><a href="5-dependence.html#graphical-models"><i class="fa fa-check"></i><b>5.2</b> Graphical models</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-dependence.html"><a href="5-dependence.html#purpose"><i class="fa fa-check"></i><b>5.2.1</b> Purpose</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-dependence.html"><a href="5-dependence.html#basic-notions-from-graph-theory"><i class="fa fa-check"></i><b>5.2.2</b> Basic notions from graph theory</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-dependence.html"><a href="5-dependence.html#probabilistic-graphical-models"><i class="fa fa-check"></i><b>5.2.3</b> Probabilistic graphical models</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-dependence.html"><a href="5-dependence.html#directed-graphical-models"><i class="fa fa-check"></i><b>5.2.4</b> Directed graphical models</a></li>
<li class="chapter" data-level="5.2.5" data-path="5-dependence.html"><a href="5-dependence.html#undirected-graphical-models"><i class="fa fa-check"></i><b>5.2.5</b> Undirected graphical models</a></li>
<li class="chapter" data-level="5.2.6" data-path="5-dependence.html"><a href="5-dependence.html#algorithm-for-learning-ggms"><i class="fa fa-check"></i><b>5.2.6</b> Algorithm for learning GGMs</a></li>
<li class="chapter" data-level="5.2.7" data-path="5-dependence.html"><a href="5-dependence.html#example-exam-score-data-mardia-et-al-1979"><i class="fa fa-check"></i><b>5.2.7</b> Example: exam score data (Mardia et al 1979:)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-nonlinear.html"><a href="6-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Nonlinear and nonparametric models</a><ul>
<li class="chapter" data-level="6.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#relevant-textbooks"><i class="fa fa-check"></i><b>6.1</b> Relevant textbooks</a></li>
<li class="chapter" data-level="6.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#limits-of-linear-models"><i class="fa fa-check"></i><b>6.2</b> Limits of linear models</a></li>
<li class="chapter" data-level="6.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#mutual-information-as-generalised-correlation"><i class="fa fa-check"></i><b>6.3</b> Mutual information as generalised correlation</a></li>
<li class="chapter" data-level="6.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#nonlinear-spline-regression-models"><i class="fa fa-check"></i><b>6.4</b> Nonlinear spline regression models</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#relevant-reading"><i class="fa fa-check"></i><b>6.4.1</b> Relevant reading</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#scatterplot-smoothing"><i class="fa fa-check"></i><b>6.4.2</b> Scatterplot smoothing</a></li>
<li class="chapter" data-level="6.4.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#polynomial-regression-model"><i class="fa fa-check"></i><b>6.4.3</b> Polynomial regression model</a></li>
<li class="chapter" data-level="6.4.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#piecewise-polyomial-regression"><i class="fa fa-check"></i><b>6.4.4</b> Piecewise polyomial regression</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests"><i class="fa fa-check"></i><b>6.5</b> Random forests</a><ul>
<li class="chapter" data-level="6.5.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#relevant-reading-1"><i class="fa fa-check"></i><b>6.5.1</b> Relevant reading</a></li>
<li class="chapter" data-level="6.5.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#stochastic-vs.algorithmic-models"><i class="fa fa-check"></i><b>6.5.2</b> Stochastic vs. algorithmic models</a></li>
<li class="chapter" data-level="6.5.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests-1"><i class="fa fa-check"></i><b>6.5.3</b> Random forests</a></li>
<li class="chapter" data-level="6.5.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#comparison-of-decision-boundaries-decision-tree-vs.random-forest"><i class="fa fa-check"></i><b>6.5.4</b> Comparison of decision boundaries: decision tree vs. random forest</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gaussian-processes"><i class="fa fa-check"></i><b>6.6</b> Gaussian processes</a><ul>
<li class="chapter" data-level="6.6.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#relevant-reading-2"><i class="fa fa-check"></i><b>6.6.1</b> Relevant reading</a></li>
<li class="chapter" data-level="6.6.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#main-concepts"><i class="fa fa-check"></i><b>6.6.2</b> Main concepts</a></li>
<li class="chapter" data-level="6.6.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#technical-background"><i class="fa fa-check"></i><b>6.6.3</b> Technical background:</a></li>
<li class="chapter" data-level="6.6.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#covariance-functions-and-kernel"><i class="fa fa-check"></i><b>6.6.4</b> Covariance functions and kernel</a></li>
<li class="chapter" data-level="6.6.5" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gp-model"><i class="fa fa-check"></i><b>6.6.5</b> GP model</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks"><i class="fa fa-check"></i><b>6.7</b> Neural networks</a><ul>
<li class="chapter" data-level="6.7.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#relevant-reading-3"><i class="fa fa-check"></i><b>6.7.1</b> Relevant reading</a></li>
<li class="chapter" data-level="6.7.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#history"><i class="fa fa-check"></i><b>6.7.2</b> History</a></li>
<li class="chapter" data-level="6.7.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks-1"><i class="fa fa-check"></i><b>6.7.3</b> Neural networks</a></li>
<li class="chapter" data-level="6.7.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#learning-more-about-deep-learning"><i class="fa fa-check"></i><b>6.7.4</b> Learning more about deep learning</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="7-matrices.html"><a href="7-matrices.html"><i class="fa fa-check"></i><b>A</b> Brief refresher on matrices</a><ul>
<li class="chapter" data-level="A.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-notation"><i class="fa fa-check"></i><b>A.1</b> Matrix notation</a></li>
<li class="chapter" data-level="A.2" data-path="7-matrices.html"><a href="7-matrices.html#simple-special-matrices"><i class="fa fa-check"></i><b>A.2</b> Simple special matrices</a></li>
<li class="chapter" data-level="A.3" data-path="7-matrices.html"><a href="7-matrices.html#simple-matrix-operations"><i class="fa fa-check"></i><b>A.3</b> Simple matrix operations</a></li>
<li class="chapter" data-level="A.4" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.4</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="A.5" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.5</b> Eigenvalues and eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.6" data-path="7-matrices.html"><a href="7-matrices.html#singular-value-decomposition"><i class="fa fa-check"></i><b>A.6</b> Singular value decomposition</a></li>
<li class="chapter" data-level="A.7" data-path="7-matrices.html"><a href="7-matrices.html#positive-semi-definiteness-rank-condition"><i class="fa fa-check"></i><b>A.7</b> Positive (semi-)definiteness, rank, condition</a></li>
<li class="chapter" data-level="A.8" data-path="7-matrices.html"><a href="7-matrices.html#trace-and-determinant-of-a-matrix-and-eigenvalues"><i class="fa fa-check"></i><b>A.8</b> Trace and determinant of a matrix and eigenvalues</a></li>
<li class="chapter" data-level="A.9" data-path="7-matrices.html"><a href="7-matrices.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.9</b> Functions of matrices</a></li>
<li class="chapter" data-level="A.10" data-path="7-matrices.html"><a href="7-matrices.html#matrix-calculus"><i class="fa fa-check"></i><b>A.10</b> Matrix calculus</a><ul>
<li class="chapter" data-level="A.10.1" data-path="7-matrices.html"><a href="7-matrices.html#first-order-vector-derivatives"><i class="fa fa-check"></i><b>A.10.1</b> First order vector derivatives</a></li>
<li class="chapter" data-level="A.10.2" data-path="7-matrices.html"><a href="7-matrices.html#second-order-vector-derivatives"><i class="fa fa-check"></i><b>A.10.2</b> Second order vector derivatives</a></li>
<li class="chapter" data-level="A.10.3" data-path="7-matrices.html"><a href="7-matrices.html#first-order-matrix-derivatives"><i class="fa fa-check"></i><b>A.10.3</b> First order matrix derivatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="8-references.html"><a href="8-references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics and Machine Learning MATH38161</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariate-random-variables" class="section level1">
<h1><span class="header-section-number">1</span> Multivariate random variables</h1>
<div id="why-multivariate-statistics" class="section level2">
<h2><span class="header-section-number">1.1</span> Why multivariate statistics?</h2>
<p>Science uses experiments to verify hypotheses about the world.
Statistics provides tools to quantify this procedure and offers methods to
link data (experiments) with probabilistic models (hyptheses).
Since the world is complex we need complex models and complex data, hence
the need for multivariate statistics and machine learning.</p>
<p>Specifically, multivariate statistics (as opposed to univariate statistics) is concerned with methods and models for <strong>random vectors</strong> and <strong>random matrices</strong>, rather than just random univariate (scalar) variables. Therefore, in multivariate statistics we will frequently make use of matrix notation.</p>
<p>Closely related to multivariate statistics (traditionally a subfield of statistics) is machine learning (ML) which is traditionally a subfield of computer science. ML used to focus more on
on algorithms rather on probabilistic
modeling but nowadays most machine learning methods are fully based on statistical
multivariate approaches, so the two fields are converging.</p>
<p>Learning multivariate models allows us to learn dependencies and interactions among the
components of the random variables which in turns allows to draw conclusion about the world.</p>
<p>Two main tasks:</p>
<ul>
<li>unsupervised learning (finding structure, clustering)</li>
<li>supervised learning (training from labeled data, followed by prediction)</li>
</ul>
<p>Challenges:</p>
<ul>
<li>complexity of model needs to be appropriate for problem and available data,</li>
<li>high dimensions make estimation and inference difficult</li>
<li>computational issues.</li>
</ul>
</div>
<div id="basics" class="section level2">
<h2><span class="header-section-number">1.2</span> Basics</h2>
<div id="univariate-vs.multivariate-random-variables" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Univariate vs. multivariate random variables</h3>
<p>Univariate random variable (dimension <span class="math inline">\(d=1\)</span>):
<span class="math display">\[x \sim F\]</span>
where <span class="math inline">\(x\)</span> is a <strong>scalar</strong> and <span class="math inline">\(F\)</span> is the distribution.
<span class="math inline">\(\text{E}(x) = \mu\)</span> denotes the mean and <span class="math inline">\(\text{Var}(x) = \sigma^2\)</span> the variance of <span class="math inline">\(x\)</span>.</p>
<p>Multivariate random <strong>vector</strong> of dimension <span class="math inline">\(d\)</span>:
<span class="math display">\[\boldsymbol x= (x_1, x_2,...,x_d)^T  \sim F\]</span></p>
<p><span class="math inline">\(\boldsymbol x\)</span> is <strong>vector</strong> valued random variable.</p>
<p>The vector <span class="math inline">\(\boldsymbol x\)</span> is column vector (=matrix of size <span class="math inline">\(d \times 1\)</span>).
Its components <span class="math inline">\(x_1, x_2,...,x_d\)</span> are univariate random variables.
The dimension <span class="math inline">\(d\)</span> is also often denoted by <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span>.</p>
</div>
<div id="mean-of-a-random-vector" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Mean of a random vector</h3>
<p>The mean / expectation of a random vector with dimensions <span class="math inline">\(d\)</span> is also a vector with dimensions <span class="math inline">\(d\)</span>:
<span class="math display">\[\text{E}(\boldsymbol x) = \boldsymbol \mu= \begin{pmatrix}
    \text{E}(x_1)       \\
    \text{E}(x_2)       \\
    \vdots \\
    \text{E}(x_d)
\end{pmatrix} = \left( \begin{array}{l}
    \mu_1       \\
    \mu_2       \\
    \vdots \\
    \mu_d
\end{array}\right)\]</span></p>
</div>
<div id="variance-of-a-random-vector" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Variance of a random vector</h3>
<p>Recall the efinition of variance for a univariate random variable:</p>
<p><span class="math display">\[\text{Var}(x) = \text{E}\left( (x-\text{E}(x))^2 \right) = \text{E}\left((x-\mu)^2 \right)=\text{E}\left( (x-\mu)(x-\mu) \right) = \text{E}(x^2)-\mu^2\]</span></p>
<p>Definition of <strong>variance of a random vector:</strong></p>
<p><span class="math display">\[\text{Var}(\boldsymbol x) = \text{E}\left(\underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d\times 1} \underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1\times d}\right) = \underbrace{\boldsymbol \Sigma}_{d\times d} = \text{E}(\boldsymbol x\boldsymbol x^T)-\boldsymbol \mu\boldsymbol \mu^T\]</span></p>
<p>The variance of a random vector is, therefore, <strong>not</strong> a vector but a <strong>matrix</strong>!</p>
<p><span class="math display">\[\boldsymbol \Sigma= (\sigma_{ij}) = \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}\]</span></p>
<p>This matrix is called the <strong>Covariance Matrix</strong>, with off-diagonal elements <span class="math inline">\(\sigma_{ij}= \text{Cov}(x_i,x_j)\)</span> and the diagonal <span class="math inline">\(\sigma_{ii}= \text{Var}(X_i) = \sigma_i^2\)</span>.</p>
</div>
<div id="properties-of-the-covariance-matrix" class="section level3">
<h3><span class="header-section-number">1.2.4</span> Properties of the covariance matrix</h3>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\boldsymbol \Sigma\)</span> is real valued: <span class="math inline">\(\sigma_{ij} \in \mathbb{R}\)</span></li>
<li><span class="math inline">\(\boldsymbol \Sigma\)</span> is symmetric: <span class="math inline">\(\sigma_{ij} = \sigma_{ji}\)</span></li>
<li>The diagonal of <span class="math inline">\(\boldsymbol \Sigma\)</span> contains <span class="math inline">\(\sigma_{ii} = \text{Var}(x_i) = \sigma_i^2\)</span>, i.e. the
variances of the components of <span class="math inline">\(\boldsymbol x\)</span>.</li>
<li>Off-diagonal elements <span class="math inline">\(\sigma_{ij} = \text{Cov}(x_i,x_j)\)</span> represent linear dependencies among the <span class="math inline">\(x_i\)</span>. <span class="math inline">\(\Longrightarrow\)</span> linear regression, correlation</li>
</ol>
<p>How many entries does the <span class="math inline">\(\boldsymbol \Sigma\)</span> matrix have?</p>
<p><span class="math display">\[\boldsymbol \Sigma= (\sigma_{ij}) = \underbrace{\begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}}_{d\times d}\]</span></p>
<p>Number of entries: <span class="math inline">\(\frac{d(d+1)}{2}\)</span>, which grows with square of dimension <span class="math inline">\(d\)</span>, i.e. is of order O(<span class="math inline">\(d^2\)</span>).</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(d\)</span></th>
<th># entries</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>10</td>
<td>55</td>
</tr>
<tr class="odd">
<td>100</td>
<td>5050</td>
</tr>
<tr class="even">
<td>1000</td>
<td>500500</td>
</tr>
<tr class="odd">
<td>10000</td>
<td>50005000</td>
</tr>
</tbody>
</table>
<p>For large dimension <span class="math inline">\(d\)</span> the covariance matrix has many components!</p>
<p>–&gt; computationally expensive (both for storage and in handling)
–&gt; very challenging to estimate in high dimensions <span class="math inline">\(d\)</span>.</p>
<p>Note: matrix inversion requires O(<span class="math inline">\(d^3\)</span>) operations! So, computing <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> is difficult for large <span class="math inline">\(d\)</span>!</p>
</div>
<div id="eigenvalue-decomposition-of-boldsymbol-sigma" class="section level3">
<h3><span class="header-section-number">1.2.5</span> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></h3>
<p>Theorem from matrix theory / linear algebra: A symmetric matrix with real entries has real eigenvalues.</p>
<p>Thus, the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span> must be real. However, for covariance
matrices this can be clarified further:</p>
<p>Assume that <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span> is the eigenvalue decomposition of
the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> with <span class="math inline">\(\boldsymbol U\)</span> an orthogonal matrix containing the eigenvectors (eigensystem) and <span class="math inline">\(\boldsymbol \Lambda\)</span> is the diagonal matrix containing eigenvalues:
<span class="math display">\[\boldsymbol \Lambda= \begin{pmatrix}
    \lambda_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}
\end{pmatrix}\]</span>
Assume that <span class="math inline">\(\boldsymbol x\)</span> is a random vector with covariance <span class="math inline">\(\text{Var}(\boldsymbol x) = \boldsymbol \Sigma\)</span>.
Then <span class="math inline">\(\text{Var}(\boldsymbol U^T \boldsymbol x) = \boldsymbol U^T \boldsymbol \Sigma\boldsymbol U= \boldsymbol \Lambda\)</span>. Since the variance is always positive
or zero the eigenvalues <span class="math inline">\(\lambda_i\)</span> of a the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> cannot be negative. Hence, <strong><span class="math inline">\(\boldsymbol \Sigma\)</span> is positive semidefinite</strong>.</p>
<p>In fact, <strong>unless there is collinearity</strong> ( i.e. a variable is a linear function the other variables) all eigenvalues will be positive and <strong><span class="math inline">\(\boldsymbol \Sigma\)</span> is positive definite</strong>.</p>
</div>
<div id="quantities-related-to-the-covariance-matrix" class="section level3">
<h3><span class="header-section-number">1.2.6</span> Quantities related to the covariance matrix</h3>
<div id="correlation-matrix-boldsymbol-p" class="section level4">
<h4><span class="header-section-number">1.2.6.1</span> Correlation matrix <span class="math inline">\(\boldsymbol P\)</span></h4>
<p>The correlation matrix <span class="math inline">\(\boldsymbol P\)</span> (= upper case greek “rho”) is the standardised covariance matrix</p>
<p><span class="math display">\[\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}=\text{Cor}(x_i,x_j)\]</span></p>
<p><span class="math display">\[\rho_{ii} = 1 = \text{Cor}(x_i,x_i)\]</span></p>
<p><span class="math display">\[ \boldsymbol P= (\rho_{ij}) = \begin{pmatrix}
    1 &amp; \dots &amp; \rho_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \rho_{d1} &amp; \dots &amp; 1
\end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol P\)</span> (“capital rho”) is a symmetric matrix (<span class="math inline">\(\rho_{ij}=\rho_{ji}\)</span>).</p>
<p>Note the <strong>variance-correlation decomposition</strong></p>
<p><span class="math display">\[\boldsymbol \Sigma= \boldsymbol V^{\frac{1}{2}} \boldsymbol P\boldsymbol V^{\frac{1}{2}}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol V\)</span> is a diagonal matrix containing the variances:</p>
<p><span class="math display">\[ \boldsymbol V= \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}\]</span></p>
<p><span class="math display">\[\boldsymbol P= \boldsymbol V^{-\frac{1}{2}}\boldsymbol \Sigma\boldsymbol V^{-\frac{1}{2}}\]</span></p>
<p>This is the definition of correlation written in matrix notation.</p>
</div>
<div id="precision-matrix-or-concentration-matrix" class="section level4">
<h4><span class="header-section-number">1.2.6.2</span> Precision matrix or concentration matrix</h4>
<p><span class="math display">\[\boldsymbol \Omega= (\omega_{ij}) = \boldsymbol \Sigma^{-1}\]</span></p>
<p><span class="math inline">\(\boldsymbol \Omega\)</span> (“Omega”) is the inverse of the covariance matrix.</p>
<p>The inverse of the covariance matrix can be obtained via
the spectral decomposition, followed by inverting the eigenvalues <span class="math inline">\(\lambda_i\)</span>:
<span class="math display">\[\boldsymbol \Sigma^{-1} = \boldsymbol U\boldsymbol \Lambda^{-1} \boldsymbol U^T = 
 \boldsymbol U\begin{pmatrix}
    \lambda_{1}^{-1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}^{-1}
\end{pmatrix} \boldsymbol U^T \]</span></p>
<p>Note that <strong>all eigenvalues <span class="math inline">\(\lambda_i\)</span> need to be positive so that <span class="math inline">\(\boldsymbol \Sigma\)</span> can be inverted.</strong> (i.e., <span class="math inline">\(\boldsymbol \Sigma\)</span> needs to be positive definite).<br />
If any <span class="math inline">\(\lambda_i = 0\)</span> then <span class="math inline">\(\boldsymbol \Sigma\)</span> is singular and not invertible.</p>
<p>Importance of <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span>:
- Natural parameter in exponential family.
- Many expressions in multivariate statistics contain <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> and not <span class="math inline">\(\boldsymbol \Sigma\)</span>
- <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> has close connection with graphical models
(e.g. conditional independence graph, partial correlations, see later chapter)</p>
</div>
<div id="partial-correlation-matrix" class="section level4">
<h4><span class="header-section-number">1.2.6.3</span> Partial correlation matrix</h4>
<p>This is a standardised version of the precision matrix, see later chapter on graphical models.</p>
</div>
<div id="total-variation-and-generalised-variance" class="section level4">
<h4><span class="header-section-number">1.2.6.4</span> Total variation and generalised variance</h4>
<p>To summarise the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> in a single scalar value there are two commonly used
measures:</p>
<ul>
<li><strong>total variation</strong>: <span class="math inline">\(\text{Tr}(\boldsymbol \Sigma) = \sum_{i=1}^d \lambda_i\)</span></li>
<li><strong>generalised variance</strong>: <span class="math inline">\(\det(\boldsymbol \Sigma) = \prod_{i=1}^d \lambda_i\)</span></li>
</ul>
<p>If all eigenvalues are positive then <span class="math inline">\(\log \det(\boldsymbol \Sigma) = \sum_{i=1}^d \log \lambda_i = \text{Tr}(\log \boldsymbol \Sigma)\)</span>.</p>
<p><span class="math inline">\(\log \boldsymbol \Sigma\)</span> is the matrix logarithm of <span class="math inline">\(\boldsymbol \Sigma\)</span> and is given by <span class="math inline">\(\log \boldsymbol \Sigma= \boldsymbol U\begin{pmatrix}  \log \lambda_{1} &amp; \dots &amp; 0\\  \vdots &amp; \ddots &amp; \vdots \\  0 &amp; \dots &amp; \log \lambda_{d} \end{pmatrix} \boldsymbol U^T\)</span></p>
</div>
</div>
</div>
<div id="multivariate-normal-distribution" class="section level2">
<h2><span class="header-section-number">1.3</span> Multivariate normal distribution</h2>
<p>The multivariate normal model is a generalisation of the univariate normal distribution
from dimension 1 to dimension <span class="math inline">\(d\)</span>.</p>
<div id="univariate-normal-distribution" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Univariate normal distribution:</h3>
<p><span class="math display">\[\text{Dimension } d = 1\]</span>
<span class="math display">\[x \sim N(\mu, \sigma^2)\]</span>
<span class="math display">\[\text{E}(x) = \mu \space , \space  \text{Var}(x) = \sigma^2\]</span></p>
<p><strong>Density</strong>:</p>
<p><span class="math display">\[f(x |\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right) \]</span></p>
<p><strong>Plot of univariate normal density </strong>:<br />
- Unimodal with peak at <span class="math inline">\(\mu\)</span>, the width is determined by <span class="math inline">\(\sigma\)</span> (in this plot: <span class="math inline">\(\mu=2, \sigma=1\)</span> )</p>
<p><img src="1-multivariate_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Special case: <strong>standard normal</strong> with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=1\)</span>:</p>
<p><span class="math display">\[f(x |\mu=0,\sigma^2=1)=\frac{1}{\sqrt{2\pi}} \exp\left( {-\frac{x^2}{2}} \right) \]</span></p>
<p><strong>Maximum entropy characterisation:</strong> the normal distribution is the unique distribution
that has the
highest (differential) entropy over all continuous distributions with support from minus infinity to plus infinity with a given mean and variance.</p>
<p>This is in fact one of the reasons why the normal distribution is so important (und useful) –
if we only know that a random variable has a mean and variance, and not much else, then using the
normal distribution will be a reasonable and well justified working assumption!</p>
</div>
<div id="multivariate-normal-model" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Multivariate normal model</h3>
<p><span class="math display">\[\text{Dimension } d\]</span>
<span class="math display">\[\boldsymbol x\sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\]</span>
<span class="math display">\[\boldsymbol x\sim \text{MVN}(\boldsymbol \mu,\boldsymbol \Sigma) \]</span>
<span class="math display">\[\text{E}(\boldsymbol x) = \boldsymbol \mu\space , \space  \text{Var}(\boldsymbol x) = \boldsymbol \Sigma\]</span></p>
<p><strong>Density</strong>:</p>
<p><span class="math display">\[f(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma) = (2\pi)^{-\frac{d}{2}} \det(\boldsymbol \Sigma)^{-\frac{1}{2}} \exp\left({{-\frac{1}{2}} \underbrace{\underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1 \times d} \underbrace{\boldsymbol \Sigma^{-1}}_{d \times d} \underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d \times 1} }_{1 \times 1 = \text{scalar!}}}\right)\]</span></p>
<ul>
<li>note that density contains precision matrix <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span></li>
<li>inverting <span class="math inline">\(\boldsymbol \Sigma\)</span> implies inverting the eigenvalues <span class="math inline">\(\lambda_i\)</span> of <span class="math inline">\(\boldsymbol \Sigma\)</span>
(thus we need <span class="math inline">\(\lambda_i &gt; 0\)</span>)</li>
<li>density also contains <span class="math inline">\(\det(\boldsymbol \Sigma) = \prod\limits_{i=1}^d \lambda_i\)</span> <span class="math inline">\(\equiv\)</span> product of eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span></li>
</ul>
<p>Special case: <strong>standard multivariate normal</strong> with <span class="math display">\[\boldsymbol \mu=\boldsymbol 0, \boldsymbol \Sigma=\boldsymbol I=\begin{pmatrix}
    1 &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; 1
\end{pmatrix}\]</span></p>
<p><span class="math display">\[f(\boldsymbol x| \boldsymbol \mu=\boldsymbol 0,\boldsymbol \Sigma=\boldsymbol I)=(2\pi)^{-d/2}\exp\left( -\frac{1}{2} \boldsymbol x^T \boldsymbol x\right) = \prod\limits_{i=1}^d \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x_i^2}{2}\right)\]</span>
which is equivalent to the product of <span class="math inline">\(d\)</span> univariate standard normals!</p>
<p><strong>Misc:</strong></p>
<ul>
<li>for <span class="math inline">\(d=1\)</span>, multivariate normal reduces to normal.</li>
<li>for <span class="math inline">\(\boldsymbol \Sigma\)</span> diagonal (i.e. <span class="math inline">\(\boldsymbol P= \boldsymbol I\)</span>, no correlation), MVN is the product of univariate normals (see Example Sheet 1).</li>
</ul>
<p><strong>Plot of MVN density</strong>:</p>
<p><img src="1-multivariate_files/figure-html/fig1-1.png" width="672" /></p>
<ul>
<li>Location: <span class="math inline">\(\boldsymbol \mu\)</span><br />
</li>
<li>Shape: <span class="math inline">\(\boldsymbol \Sigma\)</span><br />
</li>
<li>Unimodal: <strong>one</strong> peak<br />
</li>
<li>Support from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span> in each dimension</li>
</ul>
</div>
<div id="shape-of-the-contours-depend-on-the-eigenvalues-of-boldsymbol-sigma" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Shape of the contours depend on the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span>:</h3>
<p><strong>Case 1:</strong> No Correlation / Diagonal / Isotropic / Spherical Covariance
<span class="math inline">\(\Sigma = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}\)</span>
with <span class="math inline">\(\sqrt{\lambda_1/ \lambda_2} = 1\)</span>:</p>
<p><img src="1-multivariate_files/figure-html/fig2-1.png" width="672" /></p>
<p><strong>Case 2</strong>: with correlation
<span class="math inline">\(\Sigma = \begin{pmatrix} 1 &amp; 0.8 \\ 0.8 &amp; 1 \end{pmatrix}\)</span>
with <span class="math inline">\(\sqrt{\lambda_1 / \lambda_2} =3\)</span>:</p>
<img src="1-multivariate_files/figure-html/fig3-1.png" width="672" />

<p>For an explanation why the contour lines of the multivariate normal
always assume the shape of an ellipse see Example sheet 2!</p>
</div>
</div>
<div id="discrete-multivariate-distributions" class="section level2">
<h2><span class="header-section-number">1.4</span> Discrete multivariate distributions</h2>
<p>Most univariate distributions have multivariate counterparts. Here are some of the most important ones. First we discuss discrete distributions, then later furhter continuous distributions</p>
<div id="categorical-distribution" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Categorical distribution</h3>
<div id="univariate-case" class="section level4">
<h4><span class="header-section-number">1.4.1.1</span> Univariate case</h4>
<p>Bernoulli distribution (=coin tossing model)</p>
<p><span class="math display">\[x \sim \text{Ber}(\pi)\]</span>
<span class="math display">\[ x \in \{0,1\}\]</span>
<span class="math display">\[\pi \in [0,1]\]</span>
<span class="math display">\[\text{Pr}(x=1) = \pi\]</span>
<span class="math display">\[\text{Pr}(x=0)= 1-\pi\]</span>
<span class="math display">\[\text{E}(x) = \pi\]</span>
<span class="math display">\[\text{Var}(x)=\pi(1-\pi)\]</span></p>
</div>
<div id="multivariate-case" class="section level4">
<h4><span class="header-section-number">1.4.1.2</span> Multivariate case</h4>
<p><span class="math display">\[\boldsymbol x\sim \text{Categ}(\boldsymbol \pi)  \]</span>
<span class="math display">\[\boldsymbol x= (x_1,...,x_d)^T; \, x_i \in \{0,1\}; \, \sum^{d}_{i=1}x_i = 1\]</span>
<span class="math display">\[\boldsymbol \pi= (\pi_1,...,\pi_d)^T; \, \sum^{d}_{i=1}\pi_i = 1\]</span>
<span class="math display">\[\text{Pr}(x_i=1) = \pi_i\]</span>
<span class="math display">\[\text{Pr}(x_i=0)= 1-\pi_i\]</span></p>
<p><span class="math display">\[\text{E}(\boldsymbol x) = \boldsymbol \pi\]</span>
<span class="math display">\[\text{Var}(x_i)=\pi_i(1-\pi_i)\]</span>
<span class="math display">\[\text{Cov}(x_i,x_j)=-\pi_i\pi_j\]</span></p>
</div>
</div>
<div id="multinomial-distribution" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Multinomial distribution</h3>
<div id="univariate-case-1" class="section level4">
<h4><span class="header-section-number">1.4.2.1</span> Univariate case</h4>
<p>Binomial Distribution</p>
<p>Repeat Bernoulli experiment <span class="math inline">\(r\)</span> times</p>
<p><span class="math display">\[x \sim \text{Binom}(\pi,r)\]</span>
<span class="math display">\[ x \in \{0,...,r\}\]</span>
<span class="math display">\[\text{E}(x) = r \, \pi\]</span>
<span class="math display">\[\text{Var}(x)=r \, \pi(1-\pi)\]</span></p>
<p>Standardised to unit interval:
<span class="math display">\[\frac{x}{r} \in \left\{0,\frac{1}{r},...,1\right\}\]</span>
<span class="math display">\[\text{E}\left(\frac{x}{r}\right) = \pi\]</span>
<span class="math display">\[\text{Var}\left(\frac{x}{r}\right)=\frac{\pi(1-\pi)}{r}\]</span></p>
<p><span class="math display">\[\textbf{Urn model:}\]</span></p>
<p>distribute <span class="math inline">\(r\)</span> balls into two bins</p>
<p><img src="1-binom.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="multivariate-case-1" class="section level4">
<h4><span class="header-section-number">1.4.2.2</span> Multivariate case</h4>
<p>Multinomial distribution</p>
<p>Draw <span class="math inline">\(r\)</span> times from categorical distribution</p>
<p><span class="math display">\[\boldsymbol x\sim Multinom(\boldsymbol \pi,r)  \]</span>
<span class="math display">\[ x_i \in \{0,1,...,r\}; \, \sum^{d}_{i=1}x_i = r\]</span>
<span class="math display">\[\text{E}(\boldsymbol x) = r \,\boldsymbol \pi\]</span>
<span class="math display">\[\text{Var}(x_i)=r\, \pi_i(1-\pi_i)\]</span>
<span class="math display">\[\text{Cov}(x_i,x_j)=-r\, \pi_i\pi_j\]</span></p>
<p>Standardised to unit interval:
<span class="math display">\[\frac{x_i}{r} \in \left\{0,\frac{1}{r},\frac{2}{r},...,1\right\}\]</span>
<span class="math display">\[\text{E}\left(\frac{\boldsymbol x}{r}\right) = \boldsymbol \pi\]</span>
<span class="math display">\[\text{Var}\left(\frac{x_i}{r}\right)=\frac{\pi_i(1-\pi_i)}{r}\]</span>
<span class="math display">\[\text{Cov}\left(\frac{x_i}{r},\frac{x_j}{r}\right)=-\frac{\pi_i\pi_j}{r} \]</span>
<span class="math display">\[\textbf{Urn model:}\]</span></p>
<p>distribute <span class="math inline">\(r\)</span> balls into <span class="math inline">\(d\)</span> bins</p>
<p><img src="1-multinom.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="continuous-multivariate-distributions" class="section level2">
<h2><span class="header-section-number">1.5</span> Continuous multivariate distributions</h2>
<div id="dirichlet-distribution" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Dirichlet distribution</h3>
<div id="univariate-case-2" class="section level4">
<h4><span class="header-section-number">1.5.1.1</span> Univariate case</h4>
<p>Beta distribution</p>
<p><span class="math display">\[x \sim \text{Beta}(\alpha,\beta)\]</span>
<span class="math display">\[x \in [0,1]\]</span>
<span class="math display">\[\alpha &gt; 0; \beta &gt; 0\]</span>
<span class="math display">\[m = \alpha + \beta \]</span>
<span class="math display">\[\mu = \frac{\alpha}{m} \in \left[0,1\right]\]</span>
<span class="math display">\[\text{E}(x) = \mu\]</span>
<span class="math display">\[\text{Var}(x)=\frac{\mu(1-\mu)}{m+1}\]</span>
<span class="math inline">\(\text{compare with unit standardised binomial!}\)</span></p>
<p><span class="math inline">\(\textbf{Different shapes}\)</span></p>
<p><img src="1-betashapes.png" width="60%" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[\text{Useful as distribution for a proportion } \pi\]</span></p>
<p><span class="math display">\[ \text{ Bayesian Model:}\]</span><br />
<span class="math display">\[\text{Beta prior:} \; \pi \sim  Beta(\alpha,\beta)\]</span>
<span class="math display">\[\text{Binomial likelihood:} \; x|\pi \sim Binom\]</span></p>
</div>
<div id="multivariate-case-2" class="section level4">
<h4><span class="header-section-number">1.5.1.2</span> Multivariate case</h4>
<p>Dirichlet distribution</p>
<p><span class="math display">\[\boldsymbol x\sim \text{Dirichlet}(\boldsymbol \alpha)\]</span>
<span class="math display">\[x_i \in [0,1]; \, \sum^{d}_{i=1} x_i = 1\]</span>
<span class="math display">\[\boldsymbol \alpha= (\alpha_1,...,\alpha_d)^T &gt;0\]</span>
<span class="math display">\[m = \sum^{d}_{i=1}\alpha_i\]</span>
<span class="math display">\[\mu_i = \frac{\alpha_i}{m} \in \left[0,1\right]\]</span>
<span class="math display">\[\text{E}(x_i) = \mu_i\]</span>
<span class="math display">\[\text{Var}(x_i)=\frac{\mu_i(1-\mu_i)}{m+1}\]</span>
<span class="math display">\[\text{Cov}(x_i,x_j)=-\frac{\mu_i \mu_j}{m+1}\]</span>
<span class="math inline">\(\text{compare with unit standardised multinomial!}\)</span></p>
<p>Stick breaking&quot; model</p>
<p><img src="1-stickbreaking.png" width="60%" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[\text{Useful as distribution for a proportion } \boldsymbol \pi\]</span></p>
<p><span class="math display">\[\text{ Bayesian Model:}\]</span><br />
<span class="math display">\[\text{Dirichlet prior:} \,  \boldsymbol \pi\sim Dirichlet(\boldsymbol \alpha)\]</span>
<span class="math display">\[\text{Multinomial likelihood:} \, \boldsymbol x|\boldsymbol \pi\sim Multinom\]</span></p>
</div>
</div>
<div id="wishart-distribution" class="section level3">
<h3><span class="header-section-number">1.5.2</span> Wishart distribution</h3>
<p>This is a distribution for the sum of squared normally distributed
random variables.</p>
<div id="univariate-case-3" class="section level4">
<h4><span class="header-section-number">1.5.2.1</span> Univariate case</h4>
<p>Scaled <span class="math inline">\(\chi^2\)</span> distribution</p>
<p><span class="math display">\[z_1,z_2,\ldots,z_m \stackrel{\text{iid}}\sim N(0,\sigma^2)\]</span>
<span class="math display">\[x = \sum^{m}_{i=1}z_i^2\]</span></p>
<p><span class="math display">\[x \sim \sigma^2 \chi^2_m = \text{W}_1(\sigma^2, m)\]</span>
<span class="math display">\[\text{E}(x) = m \, \sigma^2\]</span>
<span class="math display">\[\text{Var}(x)= m \, 2 \sigma^4\]</span></p>
<p>Useful as the distribution of sample variance:
<span class="math display">\[y_1, \ldots, y_n \sim N(\mu, \sigma^2)\]</span>
Known mean <span class="math inline">\(\mu\)</span>
<span class="math display">\[\frac{1}{k}\sum_{i=1}^n(y_i -\mu)^2 \sim \text{W}_1\left(\frac{\sigma^2}{k}, n\right)\]</span>
Unknown mean <span class="math inline">\(\mu\)</span>
<span class="math display">\[\frac{1}{k}\sum_{i=1}^n(y_i -\bar{y})^2 \sim \text{W}_1\left(\frac{\sigma^2}{k}, n-1\right)\]</span></p>
</div>
<div id="multivariate-case-3" class="section level4">
<h4><span class="header-section-number">1.5.2.2</span> Multivariate case</h4>
<p>Wishart distribution</p>
<p><span class="math display">\[\boldsymbol z_1,\boldsymbol z_2,\ldots,\boldsymbol z_m \stackrel{\text{iid}}\sim N_d(0,\boldsymbol \Sigma)\]</span>
<span class="math display">\[\underbrace{\boldsymbol X}_{d\times d}=\sum^{m}_{i=1}\underbrace{\boldsymbol z_i\boldsymbol z_i^T}_{d\times d}\]</span><br />
Note that <span class="math inline">\(\boldsymbol X\)</span> is a !</p>
<p><span class="math display">\[\boldsymbol X\sim \text{W}_d\left(\boldsymbol \Sigma, m\right)\]</span>
<span class="math display">\[\text{E}(\boldsymbol X) = m \boldsymbol \Sigma\]</span>
<span class="math display">\[\text{Var}(x_{ij})=m \, \left(\sigma^2_{ij}+\sigma_{ii}\sigma_{jj}\right)\]</span></p>
<p>Useful as distribution of sample covariance:
<span class="math display">\[\boldsymbol y_1, \ldots, \boldsymbol y_n \sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\]</span>
<span class="math display">\[\frac{1}{k}\sum_{i=1}^n (\boldsymbol y_i -\boldsymbol \mu)(\boldsymbol y_i -\boldsymbol \mu)^T \sim \text{W}_d\left(\boldsymbol \Sigma/k, n\right)\]</span>
<span class="math display">\[\frac{1}{k}\sum_{i=1}^n (\boldsymbol y_i -\bar{\boldsymbol y})(\boldsymbol y_i -\bar{\boldsymbol y})^T \sim \text{W}_d\left(\boldsymbol \Sigma/k, n-1\right)\]</span></p>
</div>
<div id="relationship-to-gamma-distribution" class="section level4">
<h4><span class="header-section-number">1.5.2.3</span> Relationship to Gamma distribution</h4>
<p>The scaled <span class="math inline">\(\chi^2\)</span> distribution (=one-dimensional Wishart distribution) with parameters <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(m\)</span> is in fact a reparameterised <strong>Gamma distribution</strong> with shape parameter <span class="math inline">\(\alpha\)</span> and scale parameter <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[\text{Gamma}\left(  \underbrace{\frac{m}{2}}_{\text{shape} } \, , \underbrace{ 2 \sigma^{2}}_{\text{scale}} \right)=  \sigma^2\chi^2_m = \text{W}_1(\sigma^2, m)\]</span></p>
<p>or, equivalently (<span class="math inline">\(m = 2 \alpha\)</span>, <span class="math inline">\(\sigma^2 = \beta/2\)</span>)
<span class="math display">\[\text{Gamma}\left(  \underbrace{\alpha}_{\text{shape} } \, , \underbrace{\beta}_{\text{scale}} \right) = \frac{\beta}{2} \chi^2_{2 \alpha} =  \text{W}_1(\frac{\beta}{2}, 2 \alpha)\]</span></p>
<p>The mean of the Gamma distribution is <span class="math inline">\(\text{E}(x) = \alpha \beta = \mu\)</span> and the variance is <span class="math inline">\(\text{Var}(x) = \alpha \beta^2 = \mu^2/\alpha\)</span>.</p>
<p>The <strong>exponential distribution</strong> with rate parameter <span class="math inline">\(\lambda\)</span> is a special
case of the Gamma distribution with <span class="math inline">\(\alpha=1\)</span>:
<span class="math display">\[
\text{Exp}(\lambda) = \text{Gamma}(1, \frac{1}{\lambda}) = \frac{1}{2\lambda} \chi^2_{2} = \text{W}_1(\frac{1}{2 \lambda}, 2 )
\]</span>
The corresponding mean is <span class="math inline">\(1/\lambda=\mu\)</span> and variance <span class="math inline">\(1/\lambda^2=\mu^2\)</span>.</p>
</div>
</div>
<div id="inverse-wishart-distribution" class="section level3">
<h3><span class="header-section-number">1.5.3</span> Inverse Wishart distribution</h3>
<div id="univariate-case-4" class="section level4">
<h4><span class="header-section-number">1.5.3.1</span> Univariate case</h4>
<p>Inverse <span class="math inline">\(\chi^2\)</span> Distribution</p>
<p><span class="math display">\[x \sim \text{W}^{-1}_1(\psi, k+2) = \psi\,\text{ Inv-}\chi^2_{k+2} \]</span>
<span class="math display">\[\text{E}(x) = \frac{\psi}{k}\]</span>
<span class="math display">\[\text{Var}(x)= \frac{2\psi^2}{k^2 (k-2)}\]</span></p>
<p>Relationship to scaled <span class="math inline">\(\chi^2\)</span> :
<span class="math display">\[
\frac{1}{x} \sim W_1(\psi^{-1}, k+2) =  \psi^{-1} \, \chi^2_{k+2}
\]</span></p>
</div>
<div id="multivariate-case-4" class="section level4">
<h4><span class="header-section-number">1.5.3.2</span> Multivariate case</h4>
<p>Inverse Wishart distribution</p>
<p><span class="math display">\[\underbrace{\boldsymbol X}_{d\times d} \sim \text{W}^{-1}_d\left( \underbrace{\boldsymbol \Psi}_{d\times d} \, , \, k+d+1\right)\]</span>
<span class="math display">\[\text{E}(\boldsymbol X) =\boldsymbol \Psi/ k\]</span>
<span class="math display">\[\text{Var}(x_{ij})= \frac{2 }{k^2 (k-2)} \frac{(k+2) \psi_{ij} + k \, \psi_{ii} \psi_{jj} }{2 k + 2}\]</span></p>
<p>Relationship to Wishart:
<span class="math display">\[\boldsymbol X^{-1} \sim \text{W}_d\left( \boldsymbol \Psi^{-1} \, , k+d+1\right)\]</span></p>
</div>
<div id="relationship-to-inverse-gamma-distribution" class="section level4">
<h4><span class="header-section-number">1.5.3.3</span> Relationship to inverse Gamma distribution</h4>
<p>Another way to express the univariate inverse Wishart distribution is via the <strong>inverse Gamma distribution</strong>:
<span class="math display">\[IG(\underbrace{1+\frac{k}{2}}_{\text{shape } \alpha}, \underbrace{\frac{\psi}{2}}_{\text{scale }\beta}) = \psi\,\text{ Inv-}\chi^2_{k+2} =  \text{W}^{-1}_1(\psi, k+2) \]</span>
or equivalently (<span class="math inline">\(k=2(\alpha-1)\)</span> and <span class="math inline">\(\psi=2\beta\)</span>)
<span class="math display">\[IG( \alpha, \beta) = 2\beta\,\text{ Inv-}\chi^2_{2\alpha} = \text{W}^{-1}_1(2 \beta, 2 \alpha) \]</span>
The mean of the inverse Gamma distribution is
<span class="math inline">\(\text{E}(x) = \frac{\beta}{\alpha-1} = \mu\)</span> the variance
<span class="math inline">\(\text{Var}(x)= \frac{\beta^2}{(\alpha-1)^2(\alpha-2)} = \frac{2 \mu^2}{k-2}\)</span>.</p>
<p>The inverse of <span class="math inline">\(x\)</span> is Gamma distributed:
<span class="math display">\[
\frac{1}{x} \sim \text{Gamma}(1+\frac{k}{2}, 2\psi^{-1})=\text{Gamma}(\alpha, \beta^{-1})
\]</span></p>
<hr />
<p>The inverse Wishart distribution is useful as conjugate distribution for Bayesian modelling
of the variance, with <span class="math inline">\(k\)</span> the sample size parameter
and <span class="math inline">\(\Psi = k \Sigma\)</span> (or <span class="math inline">\(\psi = k \sigma^2\)</span>).</p>
<hr />
</div>
</div>
<div id="further-distributions" class="section level3">
<h3><span class="header-section-number">1.5.4</span> Further distributions</h3>
<p><a href="https://en.wikipedia.org/wiki/List_of_probability_distributions" class="uri">https://en.wikipedia.org/wiki/List_of_probability_distributions</a></p>
<p>Wikipedia is a quite good source for information on distributions!</p>
</div>
</div>
<div id="estimation-in-large-sample-and-small-sample-settings" class="section level2">
<h2><span class="header-section-number">1.6</span> Estimation in large sample and small sample settings</h2>
<p>We focus on the multivariate normal model in this chapter.</p>
<div id="data-matrix" class="section level3">
<h3><span class="header-section-number">1.6.1</span> Data matrix</h3>
<p>Observations from a multivariate normal are vectors:
<span class="math display">\[\boldsymbol x_1,\boldsymbol x_2,...,\boldsymbol x_n \stackrel{\text{iid}}\sim N_d\left(\boldsymbol \mu,\boldsymbol \Sigma\right)\]</span>
<strong>Data matrix (statistics convention):</strong></p>
<p>Each <em>line</em> of the matrix is a transposed vector <span class="math inline">\(\boldsymbol x_k^T\)</span>.</p>
<p>Thus:
<span class="math display">\[\boldsymbol X= (\boldsymbol x_1,\boldsymbol x_2,...,\boldsymbol x_n)^T = \begin{pmatrix}
    x_{11}  &amp; x_{12} &amp; \dots &amp; x_{1d}   \\
    x_{21}  &amp; x_{22} &amp; \dots &amp; x_{2d}   \\
    \vdots \\
    x_{n1}  &amp; x_{n2} &amp; \dots &amp; x_{nd}
\end{pmatrix}\]</span></p>
<p>with
<span class="math display">\[\boldsymbol x_1=\begin{pmatrix}
    x_{11}       \\
    \vdots \\
    x_{1d}
\end{pmatrix} , \space \boldsymbol x_2=\begin{pmatrix}
    x_{21}       \\
    \vdots \\
    x_{2d}
\end{pmatrix} , \ldots , \boldsymbol x_n=\begin{pmatrix}
    x_{n1}       \\
    \vdots \\
    x_{nd}
\end{pmatrix}\]</span></p>
<p>Thus, in statistics the first index runs over <span class="math inline">\((1,...,n)\)</span> and denotes the samples while the second index runs over <span class="math inline">\((1,...,d)\)</span> and refers to the variables.</p>
<p>The convention on data matrices that variables are in columns while samples are in rows. is <em>not</em> universal! In fact it’s the <strong>other way around in machine learning</strong> (where samples are stored in columns and variables in rows). However, some machine learning books also follow the statistics convention.</p>
</div>
<div id="strategies-for-large-sample-estimation" class="section level3">
<h3><span class="header-section-number">1.6.2</span> Strategies for large sample estimation</h3>
<div id="empirical-estimators-outline" class="section level4">
<h4><span class="header-section-number">1.6.2.1</span> Empirical estimators (outline)</h4>
<p>For large <span class="math inline">\(n\)</span>:
<span class="math display">\[\underbrace{F}_{\text{true}} \approx \underbrace{\widehat{F}}_{\text{empirical}}\]</span>
Replacing <span class="math inline">\(F\)</span> by <span class="math inline">\(\hat{F}\)</span> leads to <em>empirical estimators</em>.</p>
<p>For example, the expectation can be approximated/estimated as follows:</p>
<p><span class="math display">\[\text{E}_F(\boldsymbol x) \approx \text{E}_{\widehat{F}}(\boldsymbol x) = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k\]</span></p>
<p><span class="math display">\[\text{E}_F(g(\boldsymbol x)) \approx  \text{E}_{\widehat{F}}(g(\boldsymbol x)) = \frac{1}{n}\sum^{n}_{k=1} g(\boldsymbol x_k)\]</span></p>
<p><strong>Recipe:</strong> to obtain an empirical plug-in estimator simply replace the expectation by the sample average in the population expression of the quantity of interest.</p>
<p><strong>What does this work:</strong> the empirical distribution <span class="math inline">\(\widehat{F}\)</span> is actually the nonparametric maximum likelihood estimate of <span class="math inline">\(F\)</span> (see below for likelihood estimation).</p>
<p>Note: the approximation of <span class="math inline">\(F\)</span> by <span class="math inline">\(\widehat{F}\)</span> also the basis other approaches such as Efron’s bootstrap method.</p>
</div>
<div id="maximum-likelihood-estimation-outline" class="section level4">
<h4><span class="header-section-number">1.6.2.2</span> Maximum likelihood estimation (outline)</h4>
<p>R.A. Fisher (1922): model-based estimators using the density or probability mass function</p>
<p><strong>log-likelihood function</strong>:
<span class="math display">\[\log L(\boldsymbol \theta) = \sum^{n}_{k=1}  \underbrace{f}_{\text{density}}(\underbrace{x_i}_{\text{data}} |\underbrace{\boldsymbol \theta}_{\text{parameters}})\]</span>
= conditional probability of the observed data given the model parameters</p>
<p><strong>Maximum likelihood estimate:</strong>
<span class="math display">\[\hat{\boldsymbol \theta}^{\text{ML}}=\arg\max_{\boldsymbol \theta} \log L(\boldsymbol \theta)\]</span></p>
<p>ML finds the parameters that make the observed data most likely (it does <em>not</em> find the most probable model!)</p>
<p>The great appeal of <strong>MLEs</strong> is that they <strong>are optimal for large</strong> <span class="math inline">\(\mathbf{n}\)</span>, i.e. they use all the information available in the data optimally to estimate parameters, and <strong>for large sample size no estimator can be constructed that outperforms the MLE!</strong></p>
<p>A further advantage of the method of maximum likelihood is that it does not only provide a point estimate but also the asymptotic error (via the Fisher information which is related to the curvature of the log-likelihood function).</p>
</div>
</div>
<div id="large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma" class="section level3">
<h3><span class="header-section-number">1.6.3</span> Large sample estimates of mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance <span class="math inline">\(\boldsymbol \Sigma\)</span></h3>
<div id="empirical-estimates" class="section level4">
<h4><span class="header-section-number">1.6.3.1</span> Empirical estimates:</h4>
<p>These can be written in three different ways:</p>
<p><strong>Vector notation</strong></p>
<p><span class="math display">\[\hat{\boldsymbol \mu} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k\]</span></p>
<p><span class="math display">\[\underbrace{\widehat{\boldsymbol \Sigma}}_{d \times d} = \frac{1}{n}\sum^{n}_{k=1} \underbrace{\left(\boldsymbol x_k-\hat{\boldsymbol \mu}\right)}_{d \times 1} \; \underbrace{\left(\boldsymbol x_k-\hat{\boldsymbol \mu}\right)^T}_{1 \times d}\]</span></p>
<p><strong>Component notation</strong></p>
<p><span class="math display">\[\hat{\mu}_i = \frac{1}{n}\sum^{n}_{k=1} x_{ki}\]</span></p>
<p><span class="math display">\[\hat{\sigma}_{ij} = \frac{1}{n}\sum^{n}_{k=1} \left(x_{ki}-\hat{\mu}_i\right)\left(\
x_{kj}-\hat{\mu}_j\right)\]</span></p>
<p><span class="math display">\[\hat{\boldsymbol \mu}=\begin{pmatrix}
    \hat{\mu}_{1}       \\
    \vdots \\
    \hat{\mu}_{d}
\end{pmatrix}, \widehat{\boldsymbol \Sigma} = (\hat{\sigma}_{ij})\]</span></p>
<p>Variance estimate:<br />
<span class="math display">\[\hat{\sigma}_{ii} = \frac{1}{n}\sum^{n}_{k=1} \left(x_{ki}-\hat{\mu}_i\right)^2\]</span>
Note the factor <span class="math inline">\(\frac{1}{n}\)</span> (not <span class="math inline">\(\frac{1}{n-1}\)</span>)</p>
<p><strong>Data matrix notation</strong></p>
<p>The empirical mean and covariance can also be written in terms of the data matrix <span class="math inline">\(\boldsymbol X\)</span>.
See Example sheet 1 for details.</p>
</div>
<div id="maximum-likelihood-estimates" class="section level4">
<h4><span class="header-section-number">1.6.3.2</span> Maximum likelihood estimates</h4>
<p>It turns out that the empirical estimates are identical to the MLE assuming multivariate normal model:</p>
<p><span class="math inline">\(\Longrightarrow\)</span> empirical estimates <span class="math inline">\(\hat{\boldsymbol \mu}\)</span> and <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span>
<span class="math display">\[\hat{\boldsymbol \mu}^{\text{ML}} = \hat{\boldsymbol \mu}^{\text{emp}}\]</span>
<span class="math display">\[\hat{\boldsymbol \Sigma}^{\text{ML}} = \hat{\boldsymbol \Sigma}^{\text{emp}}\]</span></p>
<p>For a direct derivation of the multivariate normal MLEs by optimising the multivariate normal log-likelihood function see the example sheet 1 (easy for the mean, more difficult for the covariance matrix).</p>
<p>Note the factor <span class="math inline">\(\frac{1}{n}\)</span> in the MLE of the covariance matrix.</p>
</div>
<div id="distribution-of-the-empirical-maximum-likelihood-estimates" class="section level4">
<h4><span class="header-section-number">1.6.3.3</span> Distribution of the empirical / maximum likelihood estimates</h4>
<p>With <span class="math inline">\(\boldsymbol x_1,...,\boldsymbol x_n \sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> one can find the exact distributions
of the estimators.</p>
<p><strong>1. Distribution of the estimate of the mean:</strong></p>
<p><span class="math display">\[\hat{\boldsymbol \mu} \sim N_d\left(\boldsymbol \mu, \frac{\boldsymbol \Sigma}{n}\right)\]</span>
Since
<span class="math inline">\(\text{E}(\hat{\boldsymbol \mu}) = \boldsymbol \mu\Longrightarrow \hat{\boldsymbol \mu} \text{ is unbiased}\)</span></p>
<p><strong>2. Distribution of the covariance estimate:</strong></p>
<p><span class="math display">\[n\widehat{\boldsymbol \Sigma} \sim \text{Wishart}(\boldsymbol \Sigma, n-1)\]</span>
Since
<span class="math inline">\(\text{E}(n\hat{\boldsymbol \Sigma}) = (n-1)\boldsymbol \Sigma\Longrightarrow \widehat{\boldsymbol \Sigma}\text{ is biased!}\)</span></p>
<p>Easy to make unbiased:
<span class="math inline">\(\frac{n}{n-1}\hat{\boldsymbol \Sigma}=\frac{1}{n-1}\sum^n_{k=1}\left(\boldsymbol x_k-\hat{\boldsymbol \mu}\right)\left(\boldsymbol x_k-\hat{\boldsymbol \mu}\right)^T \text{is unbiased}\)</span></p>
<p><strong>But</strong> unbiasedness is <strong>not</strong> a very relevant criteria in multivariate statistics!</p>
</div>
</div>
<div id="problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions" class="section level3">
<h3><span class="header-section-number">1.6.4</span> Problems with maximum likelihood in small sample settings and high dimensions</h3>
<p><strong>Modern data is high dimensional!</strong></p>
<p>Data sets with <span class="math inline">\(n&lt;d\)</span>, i.e. high dimension <span class="math inline">\(d\)</span> and small sample size <span class="math inline">\(n\)</span> are now common in
many fields, e.g., medicine, biology but also finance and business analytics.</p>
<p><span class="math display">\[n = 100 \, \text{(e.g, patients/samples)}\]</span>
<span class="math display">\[d = 20000 \, \text{(e.g., genes/SNPs/proteins/variables)}\]</span>
Reasons:</p>
<ul>
<li>the number of measured variables is increasing quickly with technological advances (e.g. genomics)</li>
<li>but the number of samples cannot be similary increased (for cost and ethical reasons)</li>
</ul>
<p><strong>General problems of MLEs:</strong></p>
<ol style="list-style-type: decimal">
<li>ML estimators are optimal only if <strong>sample size is large</strong> compared to the number of parameters. However, this optimality is not any more valid if sample size is moderate or smaller than the number of parameters.</li>
<li>If there is not enough data the <strong>ML estimate overfits</strong>. This means ML fits the current data perfectly but the resulting model does not generalise well (i.e. model will perform poorly in prediction)</li>
<li>If there is a choice between different models with different complexity <strong>ML will always select the model with the largest number of parameters</strong>.</li>
</ol>
<p><strong>-&gt; for high-dimensional data with small sample size maximum likelihood estimation does not work!!!</strong></p>
<p><strong>History of Statistics:</strong></p>
<p>Much of modern statistics (from 1960 onwards) is devoted to the development of inference and estimation techniques that work with complex, high-dimensional data.</p>
<p><img src="1-history.png" width="90%" style="display: block; margin: auto;" /></p>
<ul>
<li>Maximum likelihood is a method from classical statistics (time up to about 1960).</li>
<li>From 1960 modern (computational) statistics emerges, starting with
<strong>“Stein Paradox” (1956):</strong> Charles Stein showed that in a <strong>multivariate setting</strong> ML estimators are <strong>dominated by</strong> (= are always worse than) shrinkage estimators!</li>
<li>For example, there is a shrinkage estimator for the mean that is better (in terms of MSE) than the average (which is the MLE)!</li>
</ul>
<p>Modern statistics has developed many different (but related) methods for use in high-dimensional, small sample settings:</p>
<ul>
<li>regularised estimators</li>
<li>shrinkage estimators</li>
<li>penalised maximum likelihood estimators</li>
<li>Bayesian estimators</li>
<li>Empirical Bayes estimators</li>
<li>KL / entropy-based estimators</li>
</ul>
<p>Most of this is out of scope for our class, but will be covered in advanced statistical courses.</p>
<p>Next, we describe a <strong>simple regularised estimator for the estimation of the covariance</strong>
that we will use later (i.e. in classification).</p>
</div>
<div id="estimation-of-covariance-matrix-in-small-sample-settings" class="section level3">
<h3><span class="header-section-number">1.6.5</span> Estimation of covariance matrix in small sample settings</h3>
<p><strong>Problems with ML estimate of <span class="math inline">\(\boldsymbol \Sigma\)</span></strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\Sigma\)</span> has O(<span class="math inline">\(d^2\)</span>) number of parameters!
<span class="math inline">\(\Longrightarrow \hat{\boldsymbol \Sigma}^{\text{MLE}}\)</span> requires <em>a lot</em> of data! <span class="math inline">\(n\gg d \text{ or } d^2\)</span></p></li>
<li><p>if <span class="math inline">\(n &lt; d\)</span> then <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> is positive <strong>semi</strong>-definite (even if <span class="math inline">\(\Sigma\)</span> is p.d.f.!)<br />
<span class="math inline">\(\Longrightarrow \hat{\boldsymbol \Sigma}\)</span> will have <strong>vanishing eigenvalues</strong> (some <span class="math inline">\(\lambda_i=0\)</span>) and thus <strong>cannot be inverted</strong> and is singular!</p></li>
</ol>
<p><strong>Simple regularised estimate of <span class="math inline">\(\boldsymbol \Sigma\)</span></strong></p>
<p>Regularised estimator <span class="math inline">\(\boldsymbol S^\ast\)</span> = convex combination of <span class="math inline">\(\boldsymbol S= \hat{\boldsymbol \Sigma}^\text{MLE}\)</span> and <span class="math inline">\(\boldsymbol I_d\)</span> (identity matrix) to get</p>
<p>Regularisation:
<span class="math display">\[\underbrace{\boldsymbol S^\ast}_{\text{regularised estimate}} = \underbrace{\lambda}_{\text{shrinkage intensity}} \, \underbrace{\boldsymbol I_d}_{\text{target}} + (1-\lambda)\underbrace{\boldsymbol S}_{\text{ML estimate}}\]</span>
Next, choose <span class="math inline">\(\lambda \in [0,1]\)</span> such that <span class="math inline">\(\boldsymbol S^\ast\)</span> is better (in terms of MSE) than both <span class="math inline">\(\boldsymbol S\)</span> and <span class="math inline">\(\boldsymbol I_d\)</span>.</p>
<p><strong>Bias-variance trade-off</strong></p>
<p><span class="math inline">\(\text{MSE}\)</span> is the Mean Squared Error, composed of squared bias and variance.</p>
<p><span class="math display">\[\text{MSE}(\theta) = \text{E}((\hat{\theta}-\theta)^2) = \text{Bias}(\hat{\theta})^2 + \text{Var}(\hat{\theta})\]</span>
with <span class="math inline">\(\text{Bias}(\hat{\theta}) = \text{E}(\hat{\theta})-\theta\)</span></p>
<p><span class="math inline">\(\boldsymbol S\)</span>: ML estimate, many parameters, low bias, high variance<br />
<span class="math inline">\(\boldsymbol I_d\)</span>: “target”, no parameters, high bias, low variance<br />
<span class="math inline">\(\Longrightarrow\)</span> <strong>reduce high variance of <span class="math inline">\(\boldsymbol S\)</span> by <em>introducing</em> a bit of bias through <span class="math inline">\(\boldsymbol I_d\)</span></strong>!<br />
<span class="math inline">\(\Longrightarrow\)</span> overall, <span class="math inline">\(\text{MSE}\)</span> is decreased</p>
<p><img src="1-biasvariance.png" width="90%" style="display: block; margin: auto;" /></p>
<p>How to find optimal shrinkage / regularisation parameter <span class="math inline">\(\lambda\)</span>? Minimise <span class="math inline">\(\text{MSE}\)</span>!</p>
<p>Challenge: since we don’t know the true <span class="math inline">\(\boldsymbol \Sigma\)</span> we cannot actually compute the <span class="math inline">\(\text{MSE}\)</span> directly but have to estimate it! How is this done in practise?</p>
<ul>
<li>by cross-validation (=resampling procedure)</li>
<li>by using some analytic approximation (e.g. Stein’s formula)</li>
</ul>
<p><strong>Why does regularisation of <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> work?</strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\boldsymbol S^\ast\)</span> is <strong>positive definite</strong>:<br />
Matrix Theory:<br />
<span class="math display">\[\underbrace{\boldsymbol M_1}_{ \text{positive definite, } \lambda \boldsymbol I} + \underbrace{\boldsymbol M_2}_{\text{positive semi-definite, } (1-\lambda) \boldsymbol S} = \underbrace{\boldsymbol M_3}_{\text{positive definite, } \boldsymbol S^\ast} \]</span><br />
<span class="math inline">\(\Longrightarrow \boldsymbol S^\ast\)</span> can be inverted even if <span class="math inline">\(n&lt;d\)</span></p></li>
<li>It’s <strong>Bayesian</strong> in disguise!
<span class="math display">\[\underbrace{\boldsymbol S^\ast}_{\text{posterior mean}} = \underbrace{\lambda \boldsymbol I_d}_{\text{prior information}}  + (1-\lambda)\underbrace{\boldsymbol S}_{\text{data summarised by maximum likelihood}}\]</span>
<ul>
<li>Prior information helps to infer <span class="math inline">\(\boldsymbol \Sigma\)</span> even in small samples</li>
<li>Since <span class="math inline">\(\lambda\)</span> is chosen from data, it is actually an empirical Bayes.</li>
<li>also called shrinkage estimator since the off-diagonal entries are shrunk towards zero</li>
<li>this type of linear shrinkage/regularisation is natural for models in the exponential family (Diaconis and Ylvisaker, 1979)</li>
</ul></li>
</ol>
<p>In the computer labs we will use the covariance estimator implemented in the R package
“corpcor”. This uses a regularisation similar as above (but for the correlation rather than
covariance matrix) and it
employs an analytic data-adaptive estimate of the shrinkage intensity <span class="math inline">\(\lambda\)</span>.
This estimator is a variant of an empirical Bayes / James-Stein estimator (see Statistical Methods module).</p>
<p><strong>Summary</strong></p>
<ul>
<li>In multivariate statistics, it is useful (and often necessary) to utilise prior information!</li>
<li>Regularisation introduces bias and reduces variance, minimising overall MSE</li>
<li>Unbiased estimation (a highly valued property in classical statistics!) is not a good idea in multivariate settings and often leads to poor estimators.</li>
</ul>


</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2-transformations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math38161-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
