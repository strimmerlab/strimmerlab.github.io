<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Multivariate random variables | HTML</title>
  <meta name="description" content="Multivariate Statistics and Machine Learning" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Multivariate random variables | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Multivariate random variables | HTML" />
  
  
  



<meta name="date" content="2021-03-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="2-transformations.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH38161/index.html">MATH38161 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-multivariate.html"><a href="1-multivariate.html"><i class="fa fa-check"></i><b>1</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="1.1" data-path="1-multivariate.html"><a href="1-multivariate.html#why-multivariate-statistics"><i class="fa fa-check"></i><b>1.1</b> Why multivariate statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="1-multivariate.html"><a href="1-multivariate.html#essentials-in-multivariate-statistics"><i class="fa fa-check"></i><b>1.2</b> Essentials in multivariate statistics</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-vs.multivariate-random-variables"><i class="fa fa-check"></i><b>1.2.1</b> Univariate vs. multivariate random variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-multivariate.html"><a href="1-multivariate.html#mean-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.2</b> Mean of a random vector</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-multivariate.html"><a href="1-multivariate.html#variance-of-a-random-vector"><i class="fa fa-check"></i><b>1.2.3</b> Variance of a random vector</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-multivariate.html"><a href="1-multivariate.html#properties-of-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.4</b> Properties of the covariance matrix</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-multivariate.html"><a href="1-multivariate.html#eigenvalue-decomposition-of-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.2.5</b> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.2.6" data-path="1-multivariate.html"><a href="1-multivariate.html#quantities-related-to-the-covariance-matrix"><i class="fa fa-check"></i><b>1.2.6</b> Quantities related to the covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>1.3</b> Multivariate normal distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-multivariate.html"><a href="1-multivariate.html#univariate-normal-distribution"><i class="fa fa-check"></i><b>1.3.1</b> Univariate normal distribution:</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-normal-model"><i class="fa fa-check"></i><b>1.3.2</b> Multivariate normal model</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-multivariate.html"><a href="1-multivariate.html#shape-of-the-multivariate-normal-density"><i class="fa fa-check"></i><b>1.3.3</b> Shape of the multivariate normal density</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-multivariate.html"><a href="1-multivariate.html#three-types-of-covariances"><i class="fa fa-check"></i><b>1.3.4</b> Three types of covariances</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-in-large-sample-and-small-sample-settings"><i class="fa fa-check"></i><b>1.4</b> Estimation in large sample and small sample settings</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-multivariate.html"><a href="1-multivariate.html#multivariate-data"><i class="fa fa-check"></i><b>1.4.1</b> Multivariate data</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-multivariate.html"><a href="1-multivariate.html#strategies-for-large-sample-estimation"><i class="fa fa-check"></i><b>1.4.2</b> Strategies for large sample estimation</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-multivariate.html"><a href="1-multivariate.html#large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma"><i class="fa fa-check"></i><b>1.4.3</b> Large sample estimates of mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance <span class="math inline">\(\boldsymbol \Sigma\)</span></a></li>
<li class="chapter" data-level="1.4.4" data-path="1-multivariate.html"><a href="1-multivariate.html#problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions"><i class="fa fa-check"></i><b>1.4.4</b> Problems with maximum likelihood in small sample settings and high dimensions</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-multivariate.html"><a href="1-multivariate.html#estimation-of-covariance-matrix-in-small-sample-settings"><i class="fa fa-check"></i><b>1.4.5</b> Estimation of covariance matrix in small sample settings</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-multivariate.html"><a href="1-multivariate.html#categorical-and-multinomial-distribution"><i class="fa fa-check"></i><b>1.5</b> Categorical and multinomial distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-multivariate.html"><a href="1-multivariate.html#categorical-distribution"><i class="fa fa-check"></i><b>1.5.1</b> Categorical distribution</a></li>
<li class="chapter" data-level="1.5.2" data-path="1-multivariate.html"><a href="1-multivariate.html#multinomial-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Multinomial distribution</a></li>
<li class="chapter" data-level="1.5.3" data-path="1-multivariate.html"><a href="1-multivariate.html#entropy-and-maximum-likelihood-analysis-for-the-categorical-distribution"><i class="fa fa-check"></i><b>1.5.3</b> Entropy and maximum likelihood analysis for the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="1-multivariate.html"><a href="1-multivariate.html#further-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Further multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-multivariate.html"><a href="1-multivariate.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.6.1</b> Dirichlet distribution</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-multivariate.html"><a href="1-multivariate.html#wishart-distribution"><i class="fa fa-check"></i><b>1.6.2</b> Wishart distribution</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-multivariate.html"><a href="1-multivariate.html#inverse-wishart-distribution"><i class="fa fa-check"></i><b>1.6.3</b> Inverse Wishart distribution</a></li>
<li class="chapter" data-level="1.6.4" data-path="1-multivariate.html"><a href="1-multivariate.html#further-distributions"><i class="fa fa-check"></i><b>1.6.4</b> Further distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-transformations.html"><a href="2-transformations.html"><i class="fa fa-check"></i><b>2</b> Transformations and dimension reduction</a><ul>
<li class="chapter" data-level="2.1" data-path="2-transformations.html"><a href="2-transformations.html#linear-transformations"><i class="fa fa-check"></i><b>2.1</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-transformations.html"><a href="2-transformations.html#location-scale-transformation"><i class="fa fa-check"></i><b>2.1.1</b> Location-scale transformation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-transformations.html"><a href="2-transformations.html#invertible-location-scale-transformation"><i class="fa fa-check"></i><b>2.1.2</b> Invertible location-scale transformation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-transformations.html"><a href="2-transformations.html#nonlinear-transformations"><i class="fa fa-check"></i><b>2.2</b> Nonlinear transformations</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-transformations.html"><a href="2-transformations.html#general-transformation"><i class="fa fa-check"></i><b>2.2.1</b> General transformation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-transformations.html"><a href="2-transformations.html#delta-method"><i class="fa fa-check"></i><b>2.2.2</b> Delta method</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-transformations.html"><a href="2-transformations.html#transformation-of-a-probability-density-function-under-a-general-invertible-transformation"><i class="fa fa-check"></i><b>2.2.3</b> Transformation of a probability density function under a general invertible transformation</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-transformations.html"><a href="2-transformations.html#normalising-flows"><i class="fa fa-check"></i><b>2.2.4</b> Normalising flows</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-transformations.html"><a href="2-transformations.html#whitening-transformations"><i class="fa fa-check"></i><b>2.3</b> Whitening transformations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-transformations.html"><a href="2-transformations.html#overview"><i class="fa fa-check"></i><b>2.3.1</b> Overview</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-transformations.html"><a href="2-transformations.html#general-whitening-transformation"><i class="fa fa-check"></i><b>2.3.2</b> General whitening transformation</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-transformations.html"><a href="2-transformations.html#solution-of-whitening-constraint-covariance-based"><i class="fa fa-check"></i><b>2.3.3</b> Solution of whitening constraint (covariance-based)</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-transformations.html"><a href="2-transformations.html#another-solution-correlation-based"><i class="fa fa-check"></i><b>2.3.4</b> Another solution (correlation-based)</a></li>
<li class="chapter" data-level="2.3.5" data-path="2-transformations.html"><a href="2-transformations.html#cross-covariance-and-cross-correlation"><i class="fa fa-check"></i><b>2.3.5</b> Cross-covariance and cross-correlation</a></li>
<li class="chapter" data-level="2.3.6" data-path="2-transformations.html"><a href="2-transformations.html#inverse-whitening-transformation-loadings-and-multiple-correlation"><i class="fa fa-check"></i><b>2.3.6</b> Inverse whitening transformation, loadings, and multiple correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-transformations.html"><a href="2-transformations.html#natural-whitening-procedures"><i class="fa fa-check"></i><b>2.4</b> Natural whitening procedures</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-transformations.html"><a href="2-transformations.html#zca-whitening"><i class="fa fa-check"></i><b>2.4.1</b> ZCA whitening</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-transformations.html"><a href="2-transformations.html#zca-cor-whitening"><i class="fa fa-check"></i><b>2.4.2</b> ZCA-Cor whitening</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-transformations.html"><a href="2-transformations.html#pca-whitening"><i class="fa fa-check"></i><b>2.4.3</b> PCA whitening</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-transformations.html"><a href="2-transformations.html#pca-cor-whitening"><i class="fa fa-check"></i><b>2.4.4</b> PCA-cor whitening</a></li>
<li class="chapter" data-level="2.4.5" data-path="2-transformations.html"><a href="2-transformations.html#cholesky-whitening"><i class="fa fa-check"></i><b>2.4.5</b> Cholesky whitening</a></li>
<li class="chapter" data-level="2.4.6" data-path="2-transformations.html"><a href="2-transformations.html#comparison-of-zca-pca-and-cholesky-whitening"><i class="fa fa-check"></i><b>2.4.6</b> Comparison of ZCA, PCA and Cholesky whitening</a></li>
<li class="chapter" data-level="2.4.7" data-path="2-transformations.html"><a href="2-transformations.html#recap"><i class="fa fa-check"></i><b>2.4.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-transformations.html"><a href="2-transformations.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>2.5</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-transformations.html"><a href="2-transformations.html#pca-transformation"><i class="fa fa-check"></i><b>2.5.1</b> PCA transformation</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-transformations.html"><a href="2-transformations.html#application-to-data"><i class="fa fa-check"></i><b>2.5.2</b> Application to data</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-transformations.html"><a href="2-transformations.html#iris-data-example"><i class="fa fa-check"></i><b>2.5.3</b> Iris data example</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-transformations.html"><a href="2-transformations.html#correlation-loadings-plot-to-interpret-pca-components"><i class="fa fa-check"></i><b>2.6</b> Correlation loadings plot to interpret PCA components</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-transformations.html"><a href="2-transformations.html#pca-correlation-loadings"><i class="fa fa-check"></i><b>2.6.1</b> PCA correlation loadings</a></li>
<li class="chapter" data-level="2.6.2" data-path="2-transformations.html"><a href="2-transformations.html#pca-correlation-loadings-plot"><i class="fa fa-check"></i><b>2.6.2</b> PCA correlation loadings plot</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-transformations.html"><a href="2-transformations.html#cca-whitening-canonical-correlation-analysis"><i class="fa fa-check"></i><b>2.7</b> CCA whitening (Canonical Correlation Analysis)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-transformations.html"><a href="2-transformations.html#how-to-make-cross-correlation-matrix-textcorboldsymbol-z_boldsymbol-xboldsymbol-z_boldsymbol-y-diagonal"><i class="fa fa-check"></i><b>2.7.1</b> How to make cross-correlation matrix <span class="math inline">\(\text{Cor}(\boldsymbol z_{\boldsymbol x},\boldsymbol z_{\boldsymbol y})\)</span> diagonal?</a></li>
<li class="chapter" data-level="2.7.2" data-path="2-transformations.html"><a href="2-transformations.html#related-methods"><i class="fa fa-check"></i><b>2.7.2</b> Related methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-clustering.html"><a href="3-clustering.html"><i class="fa fa-check"></i><b>3</b> Unsupervised learning and clustering</a><ul>
<li class="chapter" data-level="3.1" data-path="3-clustering.html"><a href="3-clustering.html#challenges-in-supervised-learning"><i class="fa fa-check"></i><b>3.1</b> Challenges in supervised learning</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-clustering.html"><a href="3-clustering.html#objective"><i class="fa fa-check"></i><b>3.1.1</b> Objective</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-clustering.html"><a href="3-clustering.html#questions-and-problems"><i class="fa fa-check"></i><b>3.1.2</b> Questions and problems</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-clustering.html"><a href="3-clustering.html#why-is-clustering-difficult"><i class="fa fa-check"></i><b>3.1.3</b> Why is clustering difficult?</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-clustering.html"><a href="3-clustering.html#common-types-of-clustering-methods"><i class="fa fa-check"></i><b>3.1.4</b> Common types of clustering methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-clustering.html"><a href="3-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>3.2</b> Hierarchical clustering</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-clustering.html"><a href="3-clustering.html#tree-like-structures"><i class="fa fa-check"></i><b>3.2.1</b> Tree-like structures</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-clustering.html"><a href="3-clustering.html#agglomerative-hierarchical-clustering-algorithms"><i class="fa fa-check"></i><b>3.2.2</b> Agglomerative hierarchical clustering algorithms</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-clustering.html"><a href="3-clustering.html#wards-clustering-method"><i class="fa fa-check"></i><b>3.2.3</b> Ward’s clustering method</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-clustering.html"><a href="3-clustering.html#application-to-swiss-banknote-data-set"><i class="fa fa-check"></i><b>3.2.4</b> Application to Swiss banknote data set</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-clustering.html"><a href="3-clustering.html#assessment-of-the-uncertainty-of-hierarchical-clusterings"><i class="fa fa-check"></i><b>3.2.5</b> Assessment of the uncertainty of hierarchical clusterings</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-clustering.html"><a href="3-clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>3.3</b> <span class="math inline">\(K\)</span>-means clustering</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-clustering.html"><a href="3-clustering.html#general-aims"><i class="fa fa-check"></i><b>3.3.1</b> General aims</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-clustering.html"><a href="3-clustering.html#algorithm"><i class="fa fa-check"></i><b>3.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-clustering.html"><a href="3-clustering.html#properties"><i class="fa fa-check"></i><b>3.3.3</b> Properties</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>3.3.4</b> Choosing the number of clusters</a></li>
<li class="chapter" data-level="3.3.5" data-path="3-clustering.html"><a href="3-clustering.html#k-medoids-aka-pam"><i class="fa fa-check"></i><b>3.3.5</b> <span class="math inline">\(K\)</span>-medoids aka PAM</a></li>
<li class="chapter" data-level="3.3.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-k-means-to-iris-data"><i class="fa fa-check"></i><b>3.3.6</b> Application of <span class="math inline">\(K\)</span>-means to Iris data</a></li>
<li class="chapter" data-level="3.3.7" data-path="3-clustering.html"><a href="3-clustering.html#arbitrariness-of-cluster-labels-and-label-switching"><i class="fa fa-check"></i><b>3.3.7</b> Arbitrariness of cluster labels and label switching</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-clustering.html"><a href="3-clustering.html#mixture-models"><i class="fa fa-check"></i><b>3.4</b> Mixture models</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-clustering.html"><a href="3-clustering.html#finite-mixture-model"><i class="fa fa-check"></i><b>3.4.1</b> Finite mixture model</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-clustering.html"><a href="3-clustering.html#total-variance-and-variation-of-mixture-model"><i class="fa fa-check"></i><b>3.4.2</b> Total variance and variation of mixture model</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-clustering.html"><a href="3-clustering.html#example-of-mixtures"><i class="fa fa-check"></i><b>3.4.3</b> Example of mixtures</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-clustering.html"><a href="3-clustering.html#generative-view-sampling-from-a-mixture-model"><i class="fa fa-check"></i><b>3.4.4</b> Generative view: sampling from a mixture model</a></li>
<li class="chapter" data-level="3.4.5" data-path="3-clustering.html"><a href="3-clustering.html#predicting-the-group-allocation-of-a-given-sample"><i class="fa fa-check"></i><b>3.4.5</b> Predicting the group allocation of a given sample</a></li>
<li class="chapter" data-level="3.4.6" data-path="3-clustering.html"><a href="3-clustering.html#variation-1-infinite-mixture-model"><i class="fa fa-check"></i><b>3.4.6</b> Variation 1: Infinite mixture model</a></li>
<li class="chapter" data-level="3.4.7" data-path="3-clustering.html"><a href="3-clustering.html#variation-2-semiparametric-mixture-model-with-two-classes"><i class="fa fa-check"></i><b>3.4.7</b> Variation 2: Semiparametric mixture model with two classes</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-clustering.html"><a href="3-clustering.html#fitting-mixture-models-to-data"><i class="fa fa-check"></i><b>3.5</b> Fitting mixture models to data</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-clustering.html"><a href="3-clustering.html#direct-estimation-of-mixture-model-parameters"><i class="fa fa-check"></i><b>3.5.1</b> Direct estimation of mixture model parameters</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-clustering.html"><a href="3-clustering.html#estimating-mixture-model-parameters-using-the-em-algorithm"><i class="fa fa-check"></i><b>3.5.2</b> Estimating mixture model parameters using the EM algorithm</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-clustering.html"><a href="3-clustering.html#em-algorithm-for-multivariate-normal-mixture-model"><i class="fa fa-check"></i><b>3.5.3</b> EM algorithm for multivariate normal mixture model</a></li>
<li class="chapter" data-level="3.5.4" data-path="3-clustering.html"><a href="3-clustering.html#connection-with-k-means-clustering-method"><i class="fa fa-check"></i><b>3.5.4</b> Connection with <span class="math inline">\(K\)</span>-means clustering method</a></li>
<li class="chapter" data-level="3.5.5" data-path="3-clustering.html"><a href="3-clustering.html#choosing-the-number-of-classes"><i class="fa fa-check"></i><b>3.5.5</b> Choosing the number of classes</a></li>
<li class="chapter" data-level="3.5.6" data-path="3-clustering.html"><a href="3-clustering.html#application-of-gmms-to-iris-flower-data"><i class="fa fa-check"></i><b>3.5.6</b> Application of GMMs to Iris flower data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-classification.html"><a href="4-classification.html"><i class="fa fa-check"></i><b>4</b> Supervised learning and classification</a><ul>
<li class="chapter" data-level="4.1" data-path="4-classification.html"><a href="4-classification.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-classification.html"><a href="4-classification.html#supervised-learning-vs.unsupervised-learning"><i class="fa fa-check"></i><b>4.1.1</b> Supervised learning vs. unsupervised learning</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-classification.html"><a href="4-classification.html#terminology"><i class="fa fa-check"></i><b>4.1.2</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-classification.html"><a href="4-classification.html#bayesian-discriminant-rule-or-bayes-classifier"><i class="fa fa-check"></i><b>4.2</b> Bayesian discriminant rule or Bayes classifier</a></li>
<li class="chapter" data-level="4.3" data-path="4-classification.html"><a href="4-classification.html#normal-bayes-classifier"><i class="fa fa-check"></i><b>4.3</b> Normal Bayes classifier</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-classification.html"><a href="4-classification.html#quadratic-discriminant-analysis-qda-and-gaussian-assumption"><i class="fa fa-check"></i><b>4.3.1</b> Quadratic discriminant analysis (QDA) and Gaussian assumption</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-classification.html"><a href="4-classification.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>4.3.2</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-classification.html"><a href="4-classification.html#diagonal-discriminant-analysis-dda"><i class="fa fa-check"></i><b>4.3.3</b> Diagonal discriminant analysis (DDA)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-classification.html"><a href="4-classification.html#the-training-step-learning-qda-lda-and-dda-classifiers-from-data"><i class="fa fa-check"></i><b>4.4</b> The training step — learning QDA, LDA and DDA classifiers from data</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-classification.html"><a href="4-classification.html#number-of-model-parameters"><i class="fa fa-check"></i><b>4.4.1</b> Number of model parameters</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-classification.html"><a href="4-classification.html#estimating-the-discriminant-predictor-function"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the discriminant / predictor function</a></li>
<li class="chapter" data-level="4.4.3" data-path="4-classification.html"><a href="4-classification.html#comparison-of-estimated-decision-boundaries-lda-vs.qda"><i class="fa fa-check"></i><b>4.4.3</b> Comparison of estimated decision boundaries: LDA vs. QDA</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-classification.html"><a href="4-classification.html#goodness-of-fit-and-variable-ranking"><i class="fa fa-check"></i><b>4.5</b> Goodness of fit and variable ranking</a><ul>
<li class="chapter" data-level="4.5.1" data-path="4-classification.html"><a href="4-classification.html#lda-with-k2-classes"><i class="fa fa-check"></i><b>4.5.1</b> LDA with <span class="math inline">\(K=2\)</span> classes</a></li>
<li class="chapter" data-level="4.5.2" data-path="4-classification.html"><a href="4-classification.html#multiple-classes"><i class="fa fa-check"></i><b>4.5.2</b> Multiple classes</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4-classification.html"><a href="4-classification.html#variable-selection-and-cross-validation"><i class="fa fa-check"></i><b>4.6</b> Variable selection and cross-validation</a><ul>
<li class="chapter" data-level="4.6.1" data-path="4-classification.html"><a href="4-classification.html#choosing-a-threshold-by-multiple-testing-using-false-discovery-rates"><i class="fa fa-check"></i><b>4.6.1</b> Choosing a threshold by multiple testing using false discovery rates</a></li>
<li class="chapter" data-level="4.6.2" data-path="4-classification.html"><a href="4-classification.html#quantifying-prediction-error"><i class="fa fa-check"></i><b>4.6.2</b> Quantifying prediction error</a></li>
<li class="chapter" data-level="4.6.3" data-path="4-classification.html"><a href="4-classification.html#estimation-of-prediction-error-without-validation-data-using-cross-validation"><i class="fa fa-check"></i><b>4.6.3</b> Estimation of prediction error without validation data using cross-validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-dependence.html"><a href="5-dependence.html"><i class="fa fa-check"></i><b>5</b> Multivariate dependencies</a><ul>
<li class="chapter" data-level="5.1" data-path="5-dependence.html"><a href="5-dependence.html#measuring-the-linear-association-between-two-sets-of-random-variables"><i class="fa fa-check"></i><b>5.1</b> Measuring the linear association between two sets of random variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-dependence.html"><a href="5-dependence.html#outline"><i class="fa fa-check"></i><b>5.1.1</b> Outline</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-dependence.html"><a href="5-dependence.html#special-cases"><i class="fa fa-check"></i><b>5.1.2</b> Special cases</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-dependence.html"><a href="5-dependence.html#rozeboom-vector-correlation"><i class="fa fa-check"></i><b>5.1.3</b> Rozeboom vector correlation</a></li>
<li class="chapter" data-level="5.1.4" data-path="5-dependence.html"><a href="5-dependence.html#rv-coefficient"><i class="fa fa-check"></i><b>5.1.4</b> RV coefficient</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-as-generalisation-of-correlation"><i class="fa fa-check"></i><b>5.2</b> Mutual information as generalisation of correlation</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-dependence.html"><a href="5-dependence.html#definition-of-mutual-information"><i class="fa fa-check"></i><b>5.2.1</b> Definition of mutual information</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normal-variables"><i class="fa fa-check"></i><b>5.2.2</b> Mutual information between two normal variables</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-dependence.html"><a href="5-dependence.html#mutual-information-between-two-normally-distributed-random-vectors"><i class="fa fa-check"></i><b>5.2.3</b> Mutual information between two normally distributed random vectors</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-dependence.html"><a href="5-dependence.html#using-mi-for-variable-selection"><i class="fa fa-check"></i><b>5.2.4</b> Using MI for variable selection</a></li>
<li class="chapter" data-level="5.2.5" data-path="5-dependence.html"><a href="5-dependence.html#other-measures-of-general-dependence"><i class="fa fa-check"></i><b>5.2.5</b> Other measures of general dependence</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-dependence.html"><a href="5-dependence.html#graphical-models"><i class="fa fa-check"></i><b>5.3</b> Graphical models</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-dependence.html"><a href="5-dependence.html#purpose"><i class="fa fa-check"></i><b>5.3.1</b> Purpose</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-dependence.html"><a href="5-dependence.html#basic-notions-from-graph-theory"><i class="fa fa-check"></i><b>5.3.2</b> Basic notions from graph theory</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-dependence.html"><a href="5-dependence.html#probabilistic-graphical-models"><i class="fa fa-check"></i><b>5.3.3</b> Probabilistic graphical models</a></li>
<li class="chapter" data-level="5.3.4" data-path="5-dependence.html"><a href="5-dependence.html#directed-graphical-models"><i class="fa fa-check"></i><b>5.3.4</b> Directed graphical models</a></li>
<li class="chapter" data-level="5.3.5" data-path="5-dependence.html"><a href="5-dependence.html#undirected-graphical-models"><i class="fa fa-check"></i><b>5.3.5</b> Undirected graphical models</a></li>
<li class="chapter" data-level="5.3.6" data-path="5-dependence.html"><a href="5-dependence.html#algorithm-for-learning-ggms"><i class="fa fa-check"></i><b>5.3.6</b> Algorithm for learning GGMs</a></li>
<li class="chapter" data-level="5.3.7" data-path="5-dependence.html"><a href="5-dependence.html#example-exam-score-data-mardia-et-al-1979"><i class="fa fa-check"></i><b>5.3.7</b> Example: exam score data (Mardia et al 1979:)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-nonlinear.html"><a href="6-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Nonlinear and nonparametric models</a><ul>
<li class="chapter" data-level="6.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#limits-of-linear-models-and-correlation"><i class="fa fa-check"></i><b>6.1</b> Limits of linear models and correlation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#correlation-only-measures-linear-dependence"><i class="fa fa-check"></i><b>6.1.1</b> Correlation only measures linear dependence</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#anscombe-data-sets"><i class="fa fa-check"></i><b>6.1.2</b> Anscombe data sets</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#alternatives"><i class="fa fa-check"></i><b>6.1.3</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests"><i class="fa fa-check"></i><b>6.2</b> Random forests</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#stochastic-vs.algorithmic-models"><i class="fa fa-check"></i><b>6.2.1</b> Stochastic vs. algorithmic models</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#random-forests-1"><i class="fa fa-check"></i><b>6.2.2</b> Random forests</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#comparison-of-decision-boundaries-decision-tree-vs.random-forest"><i class="fa fa-check"></i><b>6.2.3</b> Comparison of decision boundaries: decision tree vs. random forest</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gaussian-processes"><i class="fa fa-check"></i><b>6.3</b> Gaussian processes</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#main-concepts"><i class="fa fa-check"></i><b>6.3.1</b> Main concepts</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#conditional-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.3.2</b> Conditional multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#covariance-functions-and-kernels"><i class="fa fa-check"></i><b>6.3.3</b> Covariance functions and kernels</a></li>
<li class="chapter" data-level="6.3.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gp-model"><i class="fa fa-check"></i><b>6.3.4</b> GP model</a></li>
<li class="chapter" data-level="6.3.5" data-path="6-nonlinear.html"><a href="6-nonlinear.html#gaussian-process-example"><i class="fa fa-check"></i><b>6.3.5</b> Gaussian process example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks"><i class="fa fa-check"></i><b>6.4</b> Neural networks</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-nonlinear.html"><a href="6-nonlinear.html#history"><i class="fa fa-check"></i><b>6.4.1</b> History</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-nonlinear.html"><a href="6-nonlinear.html#neural-networks-1"><i class="fa fa-check"></i><b>6.4.2</b> Neural networks</a></li>
<li class="chapter" data-level="6.4.3" data-path="6-nonlinear.html"><a href="6-nonlinear.html#learning-more-about-deep-learning"><i class="fa fa-check"></i><b>6.4.3</b> Learning more about deep learning</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="7-matrices.html"><a href="7-matrices.html"><i class="fa fa-check"></i><b>A</b> Brief refresher on matrices</a><ul>
<li class="chapter" data-level="A.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-basics"><i class="fa fa-check"></i><b>A.1</b> Matrix basics</a><ul>
<li class="chapter" data-level="A.1.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-notation"><i class="fa fa-check"></i><b>A.1.1</b> Matrix notation</a></li>
<li class="chapter" data-level="A.1.2" data-path="7-matrices.html"><a href="7-matrices.html#random-matrix"><i class="fa fa-check"></i><b>A.1.2</b> Random matrix</a></li>
<li class="chapter" data-level="A.1.3" data-path="7-matrices.html"><a href="7-matrices.html#special-matrices"><i class="fa fa-check"></i><b>A.1.3</b> Special matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="7-matrices.html"><a href="7-matrices.html#simple-matrix-operations"><i class="fa fa-check"></i><b>A.2</b> Simple matrix operations</a><ul>
<li class="chapter" data-level="A.2.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-addition-and-multiplication"><i class="fa fa-check"></i><b>A.2.1</b> Matrix addition and multiplication</a></li>
<li class="chapter" data-level="A.2.2" data-path="7-matrices.html"><a href="7-matrices.html#matrix-transpose"><i class="fa fa-check"></i><b>A.2.2</b> Matrix transpose</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="7-matrices.html"><a href="7-matrices.html#matrix-summaries"><i class="fa fa-check"></i><b>A.3</b> Matrix summaries</a><ul>
<li class="chapter" data-level="A.3.1" data-path="7-matrices.html"><a href="7-matrices.html#matrix-trace"><i class="fa fa-check"></i><b>A.3.1</b> Matrix trace</a></li>
<li class="chapter" data-level="A.3.2" data-path="7-matrices.html"><a href="7-matrices.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>A.3.2</b> Determinant of a matrix</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="7-matrices.html"><a href="7-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.4</b> Matrix inverse</a><ul>
<li class="chapter" data-level="A.4.1" data-path="7-matrices.html"><a href="7-matrices.html#inversion-of-square-matrix"><i class="fa fa-check"></i><b>A.4.1</b> Inversion of square matrix</a></li>
<li class="chapter" data-level="A.4.2" data-path="7-matrices.html"><a href="7-matrices.html#inversion-of-structured-matrices"><i class="fa fa-check"></i><b>A.4.2</b> Inversion of structured matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.5</b> Orthogonal matrices</a><ul>
<li class="chapter" data-level="A.5.1" data-path="7-matrices.html"><a href="7-matrices.html#properties-1"><i class="fa fa-check"></i><b>A.5.1</b> Properties</a></li>
<li class="chapter" data-level="A.5.2" data-path="7-matrices.html"><a href="7-matrices.html#generating-orthogonal-matrices"><i class="fa fa-check"></i><b>A.5.2</b> Generating orthogonal matrices</a></li>
<li class="chapter" data-level="A.5.3" data-path="7-matrices.html"><a href="7-matrices.html#permutation-matrix"><i class="fa fa-check"></i><b>A.5.3</b> Permutation matrix</a></li>
</ul></li>
<li class="chapter" data-level="A.6" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>A.6</b> Eigenvalues and eigenvectors</a><ul>
<li class="chapter" data-level="A.6.1" data-path="7-matrices.html"><a href="7-matrices.html#definition"><i class="fa fa-check"></i><b>A.6.1</b> Definition</a></li>
<li class="chapter" data-level="A.6.2" data-path="7-matrices.html"><a href="7-matrices.html#finding-eigenvalues-and-vectors"><i class="fa fa-check"></i><b>A.6.2</b> Finding eigenvalues and vectors</a></li>
<li class="chapter" data-level="A.6.3" data-path="7-matrices.html"><a href="7-matrices.html#eigenequation-in-matrix-notation"><i class="fa fa-check"></i><b>A.6.3</b> Eigenequation in matrix notation</a></li>
<li class="chapter" data-level="A.6.4" data-path="7-matrices.html"><a href="7-matrices.html#defective-matrix"><i class="fa fa-check"></i><b>A.6.4</b> Defective matrix</a></li>
<li class="chapter" data-level="A.6.5" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-of-a-diagonal-or-triangular-matrix"><i class="fa fa-check"></i><b>A.6.5</b> Eigenvalues of a diagonal or triangular matrix</a></li>
<li class="chapter" data-level="A.6.6" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-and-vectors-of-a-symmetric-matrix"><i class="fa fa-check"></i><b>A.6.6</b> Eigenvalues and vectors of a symmetric matrix</a></li>
<li class="chapter" data-level="A.6.7" data-path="7-matrices.html"><a href="7-matrices.html#eigenvalues-of-orthogonal-matrices"><i class="fa fa-check"></i><b>A.6.7</b> Eigenvalues of orthogonal matrices</a></li>
<li class="chapter" data-level="A.6.8" data-path="7-matrices.html"><a href="7-matrices.html#positive-definite-matrices"><i class="fa fa-check"></i><b>A.6.8</b> Positive definite matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.7" data-path="7-matrices.html"><a href="7-matrices.html#matrix-decompositions"><i class="fa fa-check"></i><b>A.7</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="A.7.1" data-path="7-matrices.html"><a href="7-matrices.html#diagonalisation-and-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.7.1</b> Diagonalisation and eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.7.2" data-path="7-matrices.html"><a href="7-matrices.html#orthogonal-eigenvalue-decomposition"><i class="fa fa-check"></i><b>A.7.2</b> Orthogonal eigenvalue decomposition</a></li>
<li class="chapter" data-level="A.7.3" data-path="7-matrices.html"><a href="7-matrices.html#singular-value-decomposition"><i class="fa fa-check"></i><b>A.7.3</b> Singular value decomposition</a></li>
<li class="chapter" data-level="A.7.4" data-path="7-matrices.html"><a href="7-matrices.html#polar-decomposition"><i class="fa fa-check"></i><b>A.7.4</b> Polar decomposition</a></li>
<li class="chapter" data-level="A.7.5" data-path="7-matrices.html"><a href="7-matrices.html#cholesky-decomposition"><i class="fa fa-check"></i><b>A.7.5</b> Cholesky decomposition</a></li>
</ul></li>
<li class="chapter" data-level="A.8" data-path="7-matrices.html"><a href="7-matrices.html#matrix-summaries-based-on-eigenvalues-and-singular-values"><i class="fa fa-check"></i><b>A.8</b> Matrix summaries based on eigenvalues and singular values</a><ul>
<li class="chapter" data-level="A.8.1" data-path="7-matrices.html"><a href="7-matrices.html#trace-and-determinant-computed-from-eigenvalues"><i class="fa fa-check"></i><b>A.8.1</b> Trace and determinant computed from eigenvalues</a></li>
<li class="chapter" data-level="A.8.2" data-path="7-matrices.html"><a href="7-matrices.html#rank-and-condition-number"><i class="fa fa-check"></i><b>A.8.2</b> Rank and condition number</a></li>
</ul></li>
<li class="chapter" data-level="A.9" data-path="7-matrices.html"><a href="7-matrices.html#functions-of-symmetric-matrices"><i class="fa fa-check"></i><b>A.9</b> Functions of symmetric matrices</a><ul>
<li class="chapter" data-level="A.9.1" data-path="7-matrices.html"><a href="7-matrices.html#definition-of-a-matrix-function"><i class="fa fa-check"></i><b>A.9.1</b> Definition of a matrix function</a></li>
<li class="chapter" data-level="A.9.2" data-path="7-matrices.html"><a href="7-matrices.html#identities-for-the-matrix-exponential-and-logarithm"><i class="fa fa-check"></i><b>A.9.2</b> Identities for the matrix exponential and logarithm</a></li>
</ul></li>
<li class="chapter" data-level="A.10" data-path="7-matrices.html"><a href="7-matrices.html#matrix-calculus"><i class="fa fa-check"></i><b>A.10</b> Matrix calculus</a><ul>
<li class="chapter" data-level="A.10.1" data-path="7-matrices.html"><a href="7-matrices.html#first-order-vector-derivatives"><i class="fa fa-check"></i><b>A.10.1</b> First order vector derivatives</a></li>
<li class="chapter" data-level="A.10.2" data-path="7-matrices.html"><a href="7-matrices.html#second-order-vector-derivatives"><i class="fa fa-check"></i><b>A.10.2</b> Second order vector derivatives</a></li>
<li class="chapter" data-level="A.10.3" data-path="7-matrices.html"><a href="7-matrices.html#first-order-matrix-derivatives"><i class="fa fa-check"></i><b>A.10.3</b> First order matrix derivatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="8-further-study.html"><a href="8-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="8-further-study.html"><a href="8-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="8-further-study.html"><a href="8-further-study.html#advanced-reading"><i class="fa fa-check"></i><b>B.2</b> Advanced reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="9-references.html"><a href="9-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Multivariate Statistics and Machine Learning</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariate-random-variables" class="section level1">
<h1><span class="header-section-number">1</span> Multivariate random variables</h1>
<div id="why-multivariate-statistics" class="section level2">
<h2><span class="header-section-number">1.1</span> Why multivariate statistics?</h2>
<p>Science uses experiments to verify hypotheses about the world.
Statistics provides tools to quantify this procedure and offers methods to
link data (experiments) with probabilistic models (hyptheses).
Since the world is complex we need complex models and complex data, hence
the need for multivariate statistics and machine learning.</p>
<p>Specifically, multivariate statistics (as opposed to univariate statistics) is concerned with methods and models for <strong>random vectors</strong> and <strong>random matrices</strong>, rather than just random univariate (scalar) variables. Therefore, in multivariate statistics we will frequently make use of matrix notation.</p>
<p>Closely related to multivariate statistics (traditionally a subfield of statistics) is machine learning (ML) which is traditionally a subfield of computer science. ML used to focus more on on algorithms rather on probabilistic modelling but nowadays most machine learning methods are fully based on statistical multivariate approaches, so the two fields are converging.</p>
<p>Learning multivariate models allows us to learn dependencies and interactions among the
components of the random variables which in turns allows to draw conclusion about the world.</p>
<p>Two main tasks:</p>
<ul>
<li>unsupervised learning (finding structure, clustering)</li>
<li>supervised learning (training from labeled data, followed by prediction)</li>
</ul>
<p>Challenges:</p>
<ul>
<li>complexity of model needs to be appropriate for problem and available data,</li>
<li>high dimensions make estimation and inference difficult</li>
<li>computational issues.</li>
</ul>
</div>
<div id="essentials-in-multivariate-statistics" class="section level2">
<h2><span class="header-section-number">1.2</span> Essentials in multivariate statistics</h2>
<div id="univariate-vs.multivariate-random-variables" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Univariate vs. multivariate random variables</h3>
<p>Univariate random variable (dimension <span class="math inline">\(d=1\)</span>):
<span class="math display">\[x \sim F\]</span>
where <span class="math inline">\(x\)</span> is a <strong>scalar</strong> and <span class="math inline">\(F\)</span> is the distribution.
<span class="math inline">\(\text{E}(x) = \mu\)</span> denotes the mean and <span class="math inline">\(\text{Var}(x) = \sigma^2\)</span> the variance of <span class="math inline">\(x\)</span>.</p>
<p>Multivariate random <strong>vector</strong> of dimension <span class="math inline">\(d\)</span>:
<span class="math display">\[\boldsymbol x= (x_1, x_2,...,x_d)^T  \sim F\]</span></p>
<p><span class="math inline">\(\boldsymbol x\)</span> is <strong>vector</strong> valued random variable.</p>
<p>The vector <span class="math inline">\(\boldsymbol x\)</span> is column vector (=matrix of size <span class="math inline">\(d \times 1\)</span>).
Its components <span class="math inline">\(x_1, x_2,...,x_d\)</span> are univariate random variables.
The dimension <span class="math inline">\(d\)</span> is also often denoted by <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span>.</p>
</div>
<div id="mean-of-a-random-vector" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Mean of a random vector</h3>
<p>The mean / expectation of a random vector with dimensions <span class="math inline">\(d\)</span> is also a vector with dimensions <span class="math inline">\(d\)</span>:
<span class="math display">\[\text{E}(\boldsymbol x) = \boldsymbol \mu= \begin{pmatrix}
    \text{E}(x_1)       \\
    \text{E}(x_2)       \\
    \vdots \\
    \text{E}(x_d)
\end{pmatrix} = \left( \begin{array}{l}
    \mu_1       \\
    \mu_2       \\
    \vdots \\
    \mu_d
\end{array}\right)\]</span></p>
</div>
<div id="variance-of-a-random-vector" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Variance of a random vector</h3>
<p>Recall the definition of mean and variance for a univariate random variable:</p>
<p><span class="math display">\[\text{E}(x) = \mu\]</span></p>
<p><span class="math display">\[\text{Var}(x) = \sigma^2 = \text{E}( (x-\mu)^2 )=\text{E}( (x-\mu)(x-\mu) ) = \text{E}(x^2)-\mu^2\]</span></p>
<p>Definition of <strong>variance of a random vector:</strong></p>
<p><span class="math display">\[\text{Var}(\boldsymbol x) = \underbrace{\boldsymbol \Sigma}_{d\times d} = 
\text{E}\left(\underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d\times 1} \underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1\times d} \right) 
 = \text{E}(\boldsymbol x\boldsymbol x^T)-\boldsymbol \mu\boldsymbol \mu^T\]</span></p>
<p>The variance of a random vector is, therefore, <strong>not</strong> a vector but a <strong>matrix</strong>!</p>
<p><span class="math display">\[\boldsymbol \Sigma= (\sigma_{ij}) = \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}\]</span></p>
<p>This matrix is called the <strong>Covariance Matrix</strong>, with off-diagonal elements <span class="math inline">\(\sigma_{ij}= \text{Cov}(x_i,x_j)\)</span> and the diagonal <span class="math inline">\(\sigma_{ii}= \text{Var}(X_i) = \sigma_i^2\)</span>.</p>
</div>
<div id="properties-of-the-covariance-matrix" class="section level3">
<h3><span class="header-section-number">1.2.4</span> Properties of the covariance matrix</h3>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\boldsymbol \Sigma\)</span> is real valued: <span class="math inline">\(\sigma_{ij} \in \mathbb{R}\)</span></li>
<li><span class="math inline">\(\boldsymbol \Sigma\)</span> is symmetric: <span class="math inline">\(\sigma_{ij} = \sigma_{ji}\)</span></li>
<li>The diagonal of <span class="math inline">\(\boldsymbol \Sigma\)</span> contains <span class="math inline">\(\sigma_{ii} = \text{Var}(x_i) = \sigma_i^2\)</span>, i.e. the
variances of the components of <span class="math inline">\(\boldsymbol x\)</span>.</li>
<li>Off-diagonal elements <span class="math inline">\(\sigma_{ij} = \text{Cov}(x_i,x_j)\)</span> represent linear dependencies among the <span class="math inline">\(x_i\)</span>. <span class="math inline">\(\Longrightarrow\)</span> linear regression, correlation</li>
</ol>
<p>How many separate entries does <span class="math inline">\(\boldsymbol \Sigma\)</span> have?</p>
<p><span class="math display">\[\boldsymbol \Sigma= (\sigma_{ij}) = \underbrace{\begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}}_{d\times d}\]</span>
with <span class="math inline">\(\sigma_{ij} = \sigma_{ji}\)</span>.</p>
<p>Number of separate entries: <span class="math inline">\(\frac{d(d+1)}{2}\)</span>.</p>
<p>This numbers grows with the square of the dimension <span class="math inline">\(d\)</span>, i.e. is of order O(<span class="math inline">\(d^2\)</span>):</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(d\)</span></th>
<th># entries</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>10</td>
<td>55</td>
</tr>
<tr class="odd">
<td>100</td>
<td>5050</td>
</tr>
<tr class="even">
<td>1000</td>
<td>500500</td>
</tr>
<tr class="odd">
<td>10000</td>
<td>50005000</td>
</tr>
</tbody>
</table>
<p>For large dimension <span class="math inline">\(d\)</span> the covariance matrix has many components!</p>
<p>–&gt; computationally expensive (both for storage and in handling)
–&gt; very challenging to estimate in high dimensions <span class="math inline">\(d\)</span>.</p>
<p>Note: matrix inversion requires O(<span class="math inline">\(d^3\)</span>) operations! So, computing <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> is difficult for large <span class="math inline">\(d\)</span>!</p>
</div>
<div id="eigenvalue-decomposition-of-boldsymbol-sigma" class="section level3">
<h3><span class="header-section-number">1.2.5</span> Eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma\)</span></h3>
<p>Recall the orthogonal eigendecomposition from matrix analysis and linear algebra: A symmetric matrix with real entries has real eigenvalues and a complete set of orthogonal eigenvectors.</p>
<p>Applying eigenvalue decomposition to the covariance matrix yields
<span class="math display">\[
\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T
\]</span>
where <span class="math inline">\(\boldsymbol U\)</span> is an orthogonal matrix containing the eigenvectors
and
<span class="math display">\[\boldsymbol \Lambda= \begin{pmatrix}
    \lambda_{1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}
\end{pmatrix}\]</span>
contains the eigenvalues <span class="math inline">\(\lambda_i\)</span>.</p>
<p>Importantly, the eigenvalues of the covariance matrix are not only real valued
but they are further constrained to be non-negative.
This can be seen by computing the quadratic form <span class="math inline">\(\boldsymbol z^T \boldsymbol \Sigma\boldsymbol z\)</span>
for a non-zero non-random vector <span class="math inline">\(\boldsymbol z\)</span> which yields
<span class="math display">\[
\begin{split}
\boldsymbol z^T  \boldsymbol \Sigma\boldsymbol z&amp; = \boldsymbol z^T \text{E}\left(  (\boldsymbol x-\boldsymbol \mu) (\boldsymbol x-\boldsymbol \mu)^T  \right) \boldsymbol z\\
 &amp; =  \text{E}\left( \boldsymbol z^T (\boldsymbol x-\boldsymbol \mu) (\boldsymbol x-\boldsymbol \mu)^T \boldsymbol z\right) \\
 &amp; =  \text{E}\left( \left( \boldsymbol z^T (\boldsymbol x-\boldsymbol \mu) \right)^2 \right) \geq 0 \, .\\
\end{split}
\]</span>
Hence the covariance matrix <strong><span class="math inline">\(\boldsymbol \Sigma\)</span> is always
positive semi-definite</strong>.</p>
<p>In fact, <strong>unless there is collinearity</strong> ( i.e. a variable is a linear function the other variables) all eigenvalues will be positive and <strong><span class="math inline">\(\boldsymbol \Sigma\)</span> is positive definite</strong>.</p>
</div>
<div id="quantities-related-to-the-covariance-matrix" class="section level3">
<h3><span class="header-section-number">1.2.6</span> Quantities related to the covariance matrix</h3>
<div id="correlation-matrix-boldsymbol-p" class="section level4">
<h4><span class="header-section-number">1.2.6.1</span> Correlation matrix <span class="math inline">\(\boldsymbol P\)</span></h4>
<p>The correlation matrix <span class="math inline">\(\boldsymbol P\)</span> (= upper case greek “rho”) is the standardised covariance matrix</p>
<p><span class="math display">\[\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}=\text{Cor}(x_i,x_j)\]</span></p>
<p><span class="math display">\[\rho_{ii} = 1 = \text{Cor}(x_i,x_i)\]</span></p>
<p><span class="math display">\[ \boldsymbol P= (\rho_{ij}) = \begin{pmatrix}
    1 &amp; \dots &amp; \rho_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \rho_{d1} &amp; \dots &amp; 1
\end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol P\)</span> (“capital rho”) is a symmetric matrix (<span class="math inline">\(\rho_{ij}=\rho_{ji}\)</span>).</p>
<p>Note the <strong>variance-correlation decomposition</strong></p>
<p><span class="math display">\[\boldsymbol \Sigma= \boldsymbol V^{\frac{1}{2}} \boldsymbol P\boldsymbol V^{\frac{1}{2}}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol V\)</span> is a diagonal matrix containing the variances:</p>
<p><span class="math display">\[ \boldsymbol V= \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}\]</span></p>
<p><span class="math display">\[\boldsymbol P= \boldsymbol V^{-\frac{1}{2}}\boldsymbol \Sigma\boldsymbol V^{-\frac{1}{2}}\]</span></p>
<p>This is the definition of correlation written in matrix notation.</p>
</div>
<div id="precision-matrix-or-concentration-matrix" class="section level4">
<h4><span class="header-section-number">1.2.6.2</span> Precision matrix or concentration matrix</h4>
<p><span class="math display">\[\boldsymbol \Omega= (\omega_{ij}) = \boldsymbol \Sigma^{-1}\]</span></p>
<p><span class="math inline">\(\boldsymbol \Omega\)</span> (“Omega”) is the inverse of the covariance matrix.</p>
<p>The inverse of the covariance matrix can be obtained via
the spectral decomposition, followed by inverting the eigenvalues <span class="math inline">\(\lambda_i\)</span>:
<span class="math display">\[\boldsymbol \Sigma^{-1} = \boldsymbol U\boldsymbol \Lambda^{-1} \boldsymbol U^T = 
 \boldsymbol U\begin{pmatrix}
    \lambda_{1}^{-1} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_{d}^{-1}
\end{pmatrix} \boldsymbol U^T \]</span></p>
<p>Note that <strong>all eigenvalues <span class="math inline">\(\lambda_i\)</span> need to be positive so that <span class="math inline">\(\boldsymbol \Sigma\)</span> can be inverted.</strong> (i.e., <span class="math inline">\(\boldsymbol \Sigma\)</span> needs to be positive definite).<br />
If any <span class="math inline">\(\lambda_i = 0\)</span> then <span class="math inline">\(\boldsymbol \Sigma\)</span> is singular and not invertible.</p>
<p>Importance of <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span>:</p>
<ul>
<li>Many expressions in multivariate statistics contain <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> and not <span class="math inline">\(\boldsymbol \Sigma\)</span></li>
<li><span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> has close connection with graphical models
(e.g. conditional independence graph, partial correlations, see later chapter)</li>
<li>More generally, <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> is a natural parameter in the exponential family.</li>
</ul>
</div>
<div id="partial-correlation-matrix" class="section level4">
<h4><span class="header-section-number">1.2.6.3</span> Partial correlation matrix</h4>
<p>This is a standardised version of the precision matrix, see later chapter on graphical models.</p>
</div>
<div id="total-variation-and-generalised-variance" class="section level4">
<h4><span class="header-section-number">1.2.6.4</span> Total variation and generalised variance</h4>
<p>To summarise the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> in a single scalar value there are two commonly used
measures:</p>
<ul>
<li><strong>total variation</strong>: <span class="math inline">\(\text{Tr}(\boldsymbol \Sigma) = \sum_{i=1}^d \lambda_i\)</span></li>
<li><strong>generalised variance</strong>: <span class="math inline">\(\det(\boldsymbol \Sigma) = \prod_{i=1}^d \lambda_i\)</span></li>
</ul>
<p>The generalised variance <span class="math inline">\(\det(\boldsymbol \Sigma)\)</span> is also known as the volume of <span class="math inline">\(\boldsymbol \Sigma\)</span>.</p>
</div>
</div>
</div>
<div id="multivariate-normal-distribution" class="section level2">
<h2><span class="header-section-number">1.3</span> Multivariate normal distribution</h2>
<p>The multivariate normal model is a generalisation of the univariate normal distribution
from dimension 1 to dimension <span class="math inline">\(d\)</span>.</p>
<div id="univariate-normal-distribution" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Univariate normal distribution:</h3>
<p><span class="math display">\[\text{Dimension } d = 1\]</span>
<span class="math display">\[x \sim N(\mu, \sigma^2)\]</span>
<span class="math display">\[\text{E}(x) = \mu \space , \space  \text{Var}(x) = \sigma^2\]</span></p>
<p><strong>Density</strong>:</p>
<p><span class="math display">\[f(x |\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right) \]</span></p>
<p><strong>Plot of univariate normal density </strong>:<br />
- Unimodal with peak at <span class="math inline">\(\mu\)</span>, the width is determined by <span class="math inline">\(\sigma\)</span> (in this plot: <span class="math inline">\(\mu=2, \sigma=1\)</span> )</p>
<p><img src="1-multivariate_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Special case: <strong>standard normal</strong> with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=1\)</span>:</p>
<p><span class="math display">\[f(x |\mu=0,\sigma^2=1)=\frac{1}{\sqrt{2\pi}} \exp\left( {-\frac{x^2}{2}} \right) \]</span></p>
<p><strong>Differential entropy</strong>:<br />
<span class="math display">\[
H(F) = \frac{1}{2} (\log(2 \pi \sigma^2) + 1) 
\]</span></p>
<p><strong>Cross-entropy</strong>:<br />
<span class="math display">\[
H(F_{\text{ref}}, F) = \frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 } 
 +\frac{\sigma^2_{\text{ref}}}{\sigma^2}  +\log(2 \pi \sigma^2) \right)
\]</span>
<strong>KL divergence</strong>:<br />
<span class="math display">\[
D_{\text{KL}}(F_{\text{ref}}, F) = H(F_{\text{ref}}, F) - H(F_{\text{ref}}) = 
\frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 } 
 +\frac{\sigma^2_{\text{ref}}}{\sigma^2}  -\log\left(\frac{\sigma^2_{\text{ref}}}{ \sigma^2}\right) -1
\right)
\]</span></p>
<p><strong>Maximum entropy characterisation:</strong> the normal distribution is the unique distribution
that has the
highest (differential) entropy over all continuous distributions with support from minus infinity to plus infinity with a given mean and variance.</p>
<p>This is in fact one of the reasons why the normal distribution is so important (und useful) –
if we only know that a random variable has a mean and variance, and not much else, then using the
normal distribution will be a reasonable and well justified working assumption!</p>
</div>
<div id="multivariate-normal-model" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Multivariate normal model</h3>
<p><span class="math display">\[\text{Dimension } d\]</span>
<span class="math display">\[\boldsymbol x\sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\]</span>
<span class="math display">\[\boldsymbol x\sim \text{MVN}(\boldsymbol \mu,\boldsymbol \Sigma) \]</span>
<span class="math display">\[\text{E}(\boldsymbol x) = \boldsymbol \mu\space , \space  \text{Var}(\boldsymbol x) = \boldsymbol \Sigma\]</span></p>
<p><strong>Density</strong>:</p>
<p><span class="math display">\[f(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma) = \det(2 \pi \boldsymbol \Sigma)^{-\frac{1}{2}} \exp\left({{-\frac{1}{2}} \underbrace{\underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1 \times d} \underbrace{\boldsymbol \Sigma^{-1}}_{d \times d} \underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d \times 1} }_{1 \times 1 = \text{scalar!}}}\right)\]</span></p>
<ul>
<li>note that density contains precision matrix <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span></li>
<li>inverting <span class="math inline">\(\boldsymbol \Sigma\)</span> implies inverting the eigenvalues <span class="math inline">\(\lambda_i\)</span> of <span class="math inline">\(\boldsymbol \Sigma\)</span>
(thus we need <span class="math inline">\(\lambda_i &gt; 0\)</span>)</li>
<li>density also contains <span class="math inline">\(\det(\boldsymbol \Sigma) = \prod\limits_{i=1}^d \lambda_i\)</span> <span class="math inline">\(\equiv\)</span> product of eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span></li>
</ul>
<p>Special case: <strong>standard multivariate normal</strong> with <span class="math display">\[\boldsymbol \mu=\boldsymbol 0, \boldsymbol \Sigma=\boldsymbol I=\begin{pmatrix}
    1 &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; 1
\end{pmatrix}\]</span></p>
<p><span class="math display">\[f(\boldsymbol x| \boldsymbol \mu=\boldsymbol 0,\boldsymbol \Sigma=\boldsymbol I)=(2\pi)^{-d/2}\exp\left( -\frac{1}{2} \boldsymbol x^T \boldsymbol x\right) = \prod\limits_{i=1}^d \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x_i^2}{2}\right)\]</span>
which is equivalent to the product of <span class="math inline">\(d\)</span> univariate standard normals!</p>
<p><strong>Misc:</strong></p>
<ul>
<li>for <span class="math inline">\(d=1\)</span>, multivariate normal reduces to normal.</li>
<li>for <span class="math inline">\(\boldsymbol \Sigma\)</span> diagonal (i.e. <span class="math inline">\(\boldsymbol P= \boldsymbol I\)</span>, no correlation), MVN is the product of univariate normals (see Worksheet 2).</li>
</ul>
<p><strong>Plot of MVN density</strong>:</p>
<p><img src="1-multivariate_files/figure-html/fig1-1.png" width="672" /></p>
<ul>
<li>Location: <span class="math inline">\(\boldsymbol \mu\)</span><br />
</li>
<li>Shape: <span class="math inline">\(\boldsymbol \Sigma\)</span><br />
</li>
<li>Unimodal: <strong>one</strong> peak<br />
</li>
<li>Support from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span> in each dimension</li>
</ul>
<p>An interactive <a href="https://shiny.rstudio.com/">R Shiny web app</a> of the bivariate normal density plot
is available online at <a href="https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/" class="uri">https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/</a> .</p>
<p><strong>Differential entropy</strong>:<br />
<span class="math display">\[
H = \frac{1}{2} (\log \det(2 \pi \boldsymbol \Sigma) + d) 
\]</span></p>
<p><strong>Cross-entropy</strong>:<br />
<span class="math display">\[
H(F_{\text{ref}}, F) = \frac{1}{2}   \biggl\{
        (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})
      + \text{Tr}\biggl(\boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
    + \log \det \biggl( 2 \pi \boldsymbol \Sigma\biggr)    \biggr\} 
\]</span>
<strong>KL divergence</strong>:<br />
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\text{ref}}, F) &amp;= H(F_{\text{ref}}, F) - H(F_{\text{ref}}) \\
&amp;= \frac{1}{2}   \biggl\{
        (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})^T \boldsymbol \Sigma^{-1} (\boldsymbol \mu-\boldsymbol \mu_{\text{ref}})
      + \text{Tr}\biggl(\boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr)
    - \log \det \biggl( \boldsymbol \Sigma^{-1} \boldsymbol \Sigma_{\text{ref}} \biggr) 
     - d   \biggr\} \\
\end{split}
\]</span></p>
</div>
<div id="shape-of-the-multivariate-normal-density" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Shape of the multivariate normal density</h3>
<p>Now we show that the contour lines of the multivariate normal density always take on the form of an ellipse, and that the radii of the ellipse is determined by the eigenvalues of
<span class="math inline">\(\boldsymbol \Sigma\)</span>.</p>
<p>We start by observing that a circle with radius <span class="math inline">\(r\)</span> around the origin can be described as the set of points <span class="math inline">\((x_1,x_2)\)</span> satisfying
<span class="math inline">\(x_1^2+x_2^2 = r^2\)</span>, or equivalently, <span class="math inline">\(\frac{x_1^2}{r^2} + \frac{x_2^2}{r^2} = 1\)</span>.
This is generalised to the shape of an ellipse by allowing (in two dimensions) for two radii
<span class="math inline">\(r_1\)</span> and <span class="math inline">\(r_2\)</span> with
<span class="math inline">\(\frac{x_1^2}{r_1^2} + \frac{x_2^2}{r_2^2} = 1\)</span>, or in vector notation
<span class="math inline">\(\boldsymbol x^T \text{Diag}(r_1^2, r_2^2)^{-1} \boldsymbol x= 1\)</span>. In <span class="math inline">\(d\)</span> dimensions and allowing for rotation of
the axes and a shift of the origin from 0 to <span class="math inline">\(\boldsymbol \mu\)</span> the condition for an ellipse is
<span class="math display">\[(\boldsymbol x-\boldsymbol \mu)^T \boldsymbol Q\, \text{Diag}(r_1^2, \ldots , r_d^2)^{-1} \boldsymbol Q^T (\boldsymbol x-\boldsymbol \mu) = 1\]</span>
where <span class="math inline">\(\boldsymbol Q\)</span> is an orthogonal rotation matrix.</p>
<p>A contour line of a probability density function is a set of connected points where the density assumes the same constant value. In the case of the multivariate normal distribution keeping the density at some fixed value implies that <span class="math inline">\((\boldsymbol x-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu) = c\)</span> where <span class="math inline">\(c\)</span> is a constant. Using the eigenvalue decomposition of <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol U\boldsymbol \Lambda\boldsymbol U^T\)</span> we can rewrite this condition as
<span class="math display">\[
(\boldsymbol x-\boldsymbol \mu)^T \boldsymbol U\boldsymbol \Lambda^{-1} \boldsymbol U^T (\boldsymbol x-\boldsymbol \mu) = c \,.
\]</span>
This implies that the contour lines of the multivariate normal density are indeed ellipses and that the squared radii are proportional to the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span>. Equivalently, the positive square roots of the eigenvalues are proportional to the radii of the ellipse. Hence, for singular covariance matrix with one or more <span class="math inline">\(\lambda_i=0\)</span> the corresponding radii are zero.</p>
<p>An interactive <a href="https://shiny.rstudio.com/">R Shiny web app</a> to play with the contour lines of the
bivariate normal distribution is online at <a href="https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/" class="uri">https://minerva.it.manchester.ac.uk/shiny/strimmer/bvn/</a> .</p>
</div>
<div id="three-types-of-covariances" class="section level3">
<h3><span class="header-section-number">1.3.4</span> Three types of covariances</h3>
<p>Following the above we can parameterise a covariance matrix in terms of its
i) volume, ii) shape, and iii) orientation by writing
<span class="math display">\[
\boldsymbol \Sigma= \lambda \, \boldsymbol U\boldsymbol A\boldsymbol U^T
\]</span>
with <span class="math inline">\(\boldsymbol A=\text{Diag}(a_1, \ldots, a_d)\)</span> and <span class="math inline">\(\prod_{i=1}^d a_i = 1\)</span> (and <span class="math inline">\(\boldsymbol \Lambda=\lambda \boldsymbol A\)</span>).</p>
<ol style="list-style-type: lower-roman">
<li>The <strong>volume</strong> is <span class="math inline">\(\det(\boldsymbol \Sigma) = \lambda^d\)</span>, determined by a single parameter <span class="math inline">\(\lambda\)</span>.</li>
<li>The <strong>shape</strong> is determined by <span class="math inline">\(\boldsymbol A\)</span>, with <span class="math inline">\(d-1\)</span> free parameters.</li>
<li>The <strong>orientation</strong> is given by the orthogonal matrix <span class="math inline">\(\boldsymbol U\)</span>, with <span class="math inline">\(d (d-1)/2\)</span> free parameters.</li>
</ol>
<p>This leads to classification of covariances into three varieties:</p>
<p><strong>Type 1:</strong> <strong>spherical covariance</strong> <span class="math inline">\(\boldsymbol \Sigma=\lambda \boldsymbol I\)</span>,
with spherical contour lines, 1 free parameter (<span class="math inline">\(\boldsymbol A=\boldsymbol I\)</span>, <span class="math inline">\(\boldsymbol U=\boldsymbol I\)</span>).</p>
<p>Example:
<span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\)</span>
with <span class="math inline">\(\sqrt{\lambda_1/ \lambda_2} = 1\)</span>:</p>
<p><img src="1-multivariate_files/figure-html/fig2-1.png" width="672" /></p>
<p><strong>Type 2</strong>: <strong>diagonal covariance</strong> <span class="math inline">\(\boldsymbol \Sigma= \lambda \boldsymbol A\)</span>, with elliptical contour lines oriented along the coordinate axes, <span class="math inline">\(d\)</span> free parameters (<span class="math inline">\(\boldsymbol U=\boldsymbol I\)</span>).</p>
<p>Example:
<span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\)</span>
with <span class="math inline">\(\sqrt{\lambda_1 / \lambda_2} \approx 1.41\)</span>:</p>
<p><img src="1-multivariate_files/figure-html/fig3-1.png" width="672" /></p>
<p><strong>Type 3</strong>: <strong>general unrestricted covariance</strong> <span class="math inline">\(\boldsymbol \Sigma\)</span>,
with elliptical contour lines oriented in any direction,
<span class="math inline">\(d (d+1)/2\)</span> free parameters.</p>
<p>Example:
<span class="math inline">\(\boldsymbol \Sigma= \begin{pmatrix} 2 &amp; 0.6 \\ 0.6 &amp; 1 \end{pmatrix}\)</span>
with <span class="math inline">\(\sqrt{\lambda_1 / \lambda_2} \approx 2.20\)</span>:</p>
<p><img src="1-multivariate_files/figure-html/fig4-1.png" width="672" /></p>
</div>
</div>
<div id="estimation-in-large-sample-and-small-sample-settings" class="section level2">
<h2><span class="header-section-number">1.4</span> Estimation in large sample and small sample settings</h2>
<p>In practical application of multivariate normal model we need to
learn its parameters from data. We first consider the case when
there are many measurements available, and then second the case when
the number of data points is small compared to the number of parameters.</p>
<p>In a previous course in year 2
(see <a href="http://www.strimmerlab.org/publications/lecture-notes/MATH20802/">MATH20802 Statistical Methods</a>)
the method of maximum likelihood as well as essential of Bayesian statistics
were introduced. Here we apply these approaches in the setting of the
multivariate normal distribution.</p>
<div id="multivariate-data" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Multivariate data</h3>
<p><strong>Vector notation:</strong></p>
<p>Samples from a multivariate normal distribution are <em>vectors</em> (not scalars as for univariate normal):
<span class="math display">\[\boldsymbol x_1,\boldsymbol x_2,...,\boldsymbol x_n \stackrel{\text{iid}}\sim N_d\left(\boldsymbol \mu,\boldsymbol \Sigma\right)\]</span></p>
<p><strong>Matrix and component notation:</strong></p>
<p>All the data points are commonly collected into a matrix <span class="math inline">\(\boldsymbol X\)</span>.</p>
<p>In statistics the convention is to store each data vector in the rows of <span class="math inline">\(\boldsymbol X\)</span>:</p>
<p><span class="math display">\[\boldsymbol X= (\boldsymbol x_1,\boldsymbol x_2,...,\boldsymbol x_n)^T = \begin{pmatrix}
    x_{11}  &amp; x_{12} &amp; \dots &amp; x_{1d}   \\
    x_{21}  &amp; x_{22} &amp; \dots &amp; x_{2d}   \\
    \vdots \\
    x_{n1}  &amp; x_{n2} &amp; \dots &amp; x_{nd}
\end{pmatrix}\]</span></p>
<p>Therefore,
<span class="math display">\[\boldsymbol x_1=\begin{pmatrix}
    x_{11}       \\
    \vdots \\
    x_{1d}
\end{pmatrix} , \space \boldsymbol x_2=\begin{pmatrix}
    x_{21}       \\
    \vdots \\
    x_{2d}
\end{pmatrix} , \ldots , \boldsymbol x_n=\begin{pmatrix}
    x_{n1}       \\
    \vdots \\
    x_{nd}
\end{pmatrix}\]</span></p>
<p>Thus, in statistics the first index runs over <span class="math inline">\((1,...,n)\)</span> and denotes the samples while the second index runs over <span class="math inline">\((1,...,d)\)</span> and refers to the variables.</p>
<p>The statistics convention on data matrices is <em>not</em> universal! In fact, in most of the machine learning literature in engineering and computer science the data samples are stored in the columns so that the variables appear in the rows (thus in the engineering convention the data matrix is transposed compared to the statistics convention).</p>
<p>In order to avoid confusion it is recommended to use vector notation for data instead of
matrix notation because this is not ambiguous.</p>
</div>
<div id="strategies-for-large-sample-estimation" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Strategies for large sample estimation</h3>
<div id="empirical-estimators-outline" class="section level4">
<h4><span class="header-section-number">1.4.2.1</span> Empirical estimators (outline)</h4>
<p>For large <span class="math inline">\(n\)</span> we have thanks to the law of large numbers:
<span class="math display">\[\underbrace{F}_{\text{true}} \approx \underbrace{\widehat{F}}_{\text{empirical}}\]</span></p>
<p>We now would like to estimate <span class="math inline">\(A\)</span> which is a functional of <span class="math inline">\(F\)</span>, i.e. <span class="math inline">\(A=m(F)\)</span>.
For example the mean, the median or some other quantity.</p>
<p>The <em>empirical estimate</em> is obtained by replacing the unknown true distribution
<span class="math inline">\(F\)</span> with the observed empirical distribution: <span class="math inline">\(\hat{A} = m(\widehat{F})\)</span>.</p>
<p>For example, the expectation of a random variable is approximated/estimated
as the average over the observation:
<span class="math display">\[\text{E}_F(\boldsymbol x) \approx \text{E}_{\widehat{F}}(\boldsymbol x) = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k\]</span>
<span class="math display">\[\text{E}_F(g(\boldsymbol x)) \approx  \text{E}_{\widehat{F}}(g(\boldsymbol x)) = \frac{1}{n}\sum^{n}_{k=1} g(\boldsymbol x_k)\]</span></p>
<p><strong>Simple recipe to obtain an empirical estimator</strong>: simply replace the expectation operator
by the sample average in the quantity of interest.</p>
<p><strong>What does this work:</strong> the empirical distribution <span class="math inline">\(\widehat{F}\)</span> is actually the nonparametric maximum likelihood estimate of <span class="math inline">\(F\)</span> (see below for likelihood estimation).</p>
<p>Note: the approximation of <span class="math inline">\(F\)</span> by <span class="math inline">\(\widehat{F}\)</span> also the basis other approaches such as Efron’s bootstrap method.</p>
</div>
<div id="maximum-likelihood-estimation-outline" class="section level4">
<h4><span class="header-section-number">1.4.2.2</span> Maximum likelihood estimation (outline)</h4>
<p>R.A. Fisher (1922): model-based estimators using the density or probability mass function</p>
<p><strong>log-likelihood function</strong>:
<span class="math display">\[\log L(\boldsymbol \theta) = \sum^{n}_{k=1}  \underbrace{\log f}_{\text{log-density}}(\underbrace{x_i}_{\text{data}} |\underbrace{\boldsymbol \theta}_{\text{parameters}})\]</span>
likelihood = probability to observe data given the model parameters</p>
<p><strong>Maximum likelihood estimate:</strong>
<span class="math display">\[\hat{\boldsymbol \theta}^{\text{ML}}=\underset{\boldsymbol \theta}{\arg\,\max} \log L(\boldsymbol \theta)\]</span></p>
<p>Maximum likelihood (ML) finds the parameters that make the observed data most likely (it does <em>not</em> find the most probable model!)</p>
<p>Recall from <a href="http://www.strimmerlab.org/publications/lecture-notes/MATH20802/">MATH20802 Statistical Methods</a>
that maximum likelihood is closely linked to minimising the relative entropy (KL divergence)
<span class="math inline">\(D_{\text{KL}}(F, F_{\boldsymbol \theta})\)</span> between the unknown true model <span class="math inline">\(F\)</span> to the specified model <span class="math inline">\(F_{\boldsymbol \theta}\)</span>. Specifically, for large
sample size <span class="math inline">\(n\)</span> the model <span class="math inline">\(F_{\hat{\boldsymbol \theta}}\)</span> fit by maximum likelihood is indeed the model that is closest to <span class="math inline">\(F\)</span>.</p>
<p>Correspondingly, the great appeal of <strong>maximum likelihood estimates</strong> (MLEs) is that they <strong>are optimal for large</strong> <span class="math inline">\(\mathbf{n}\)</span>, i.e. so that <strong>for large sample size no estimator can be constructed that outperforms the MLE</strong> (note the emphasis on “for large <span class="math inline">\(n\)</span>”!).
A further advantage of the method of maximum likelihood is that it does not only provide a point estimate but also the asymptotic error (via the Fisher information which is related to the curvature of the log-likelihood function).</p>
</div>
</div>
<div id="large-sample-estimates-of-mean-boldsymbol-mu-and-covariance-boldsymbol-sigma" class="section level3">
<h3><span class="header-section-number">1.4.3</span> Large sample estimates of mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance <span class="math inline">\(\boldsymbol \Sigma\)</span></h3>
<div id="empirical-estimates" class="section level4">
<h4><span class="header-section-number">1.4.3.1</span> Empirical estimates:</h4>
<p>Recall the definitions:
<span class="math display">\[
\boldsymbol \mu= \text{E}(\boldsymbol x)
\]</span>
and
<span class="math display">\[
\boldsymbol \Sigma= \text{E}\left(   (\boldsymbol x-\boldsymbol \mu) (\boldsymbol x-\boldsymbol \mu)^T \right)
\]</span></p>
<p>For the empirical estimate we replace the expectations by the
corresponding sample averages.</p>
<p>These resulting estimators can be written in three different ways:</p>
<p><strong>Vector notation:</strong></p>
<p><span class="math display">\[\hat{\boldsymbol \mu} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k\]</span></p>
<p><span class="math display">\[
\widehat{\boldsymbol \Sigma} = \frac{1}{n}\sum^{n}_{k=1} (\boldsymbol x_k-\hat{\boldsymbol \mu})  (\boldsymbol x_k-\hat{\boldsymbol \mu})^T
= \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k  \boldsymbol x_k^T - \hat{\boldsymbol \mu} \hat{\boldsymbol \mu}^T
\]</span></p>
<p><strong>Data matrix notation:</strong></p>
<p>The empirical mean and covariance can also be written in terms of the data matrix <span class="math inline">\(\boldsymbol X\)</span> (using statistics convention):</p>
<p><span class="math display">\[\hat{\boldsymbol \mu} = \frac{1}{n} \boldsymbol X^T \boldsymbol 1_n\]</span></p>
<p><span class="math display">\[\widehat{\boldsymbol \Sigma} = \frac{1}{n} \boldsymbol X^T \boldsymbol X- \hat{\boldsymbol \mu} \hat{\boldsymbol \mu}^T\]</span></p>
<p>See Worksheet 2 for details.</p>
<p><strong>Component notation:</strong></p>
<p>The corresponding component notation with <span class="math inline">\(\boldsymbol X= (x_{ki})\)</span> is:</p>
<p><span class="math display">\[\hat{\mu}_i = \frac{1}{n}\sum^{n}_{k=1} x_{ki}\]</span></p>
<p><span class="math display">\[\hat{\sigma}_{ij} = \frac{1}{n}\sum^{n}_{k=1} (x_{ki}-\hat{\mu}_i) ( 
x_{kj}-\hat{\mu}_j )\]</span></p>
<p><span class="math display">\[\hat{\boldsymbol \mu}=\begin{pmatrix}
    \hat{\mu}_{1}       \\
    \vdots \\
    \hat{\mu}_{d}
\end{pmatrix}, \widehat{\boldsymbol \Sigma} = (\hat{\sigma}_{ij})\]</span></p>
<p>Variance estimate:<br />
<span class="math display">\[\hat{\sigma}_{ii} = \frac{1}{n}\sum^{n}_{k=1} \left(x_{ki}-\hat{\mu}_i\right)^2\]</span>
Note the factor <span class="math inline">\(\frac{1}{n}\)</span> (not <span class="math inline">\(\frac{1}{n-1}\)</span>)</p>
<p><strong>Engineering and machine learning convention:</strong></p>
<p>Using the engineering and machine learning convention for the data matrix <span class="math inline">\(\boldsymbol X\)</span> the estimators are written as</p>
<p><span class="math display">\[\hat{\boldsymbol \mu} = \frac{1}{n} \boldsymbol X\boldsymbol 1_n\]</span></p>
<p><span class="math display">\[\widehat{\boldsymbol \Sigma} = \frac{1}{n} \boldsymbol X\boldsymbol X^T - \hat{\boldsymbol \mu} \hat{\boldsymbol \mu}^T\]</span></p>
<p>In the corresponding component notation the two indices for the columns and rowns are interchanged.</p>
<p>To avoid confusion when using matrix or component notation you need to always state which
convention is used! In these notes we strictly follow the statistics convention.</p>
</div>
<div id="maximum-likelihood-estimates" class="section level4">
<h4><span class="header-section-number">1.4.3.2</span> Maximum likelihood estimates</h4>
<p>We now derive the MLE of the parameters <span class="math inline">\(\boldsymbol \mu\)</span> and <span class="math inline">\(\boldsymbol \Sigma\)</span> of the multivariate normal distribution.
The corresponding log-likelihood function is
<span class="math display">\[
\begin{split}
\log L(\boldsymbol \mu, \boldsymbol \Sigma) &amp; = \sum_{k=1}^n \log f( \boldsymbol x_k | \boldsymbol \mu, \boldsymbol \Sigma) \\
  &amp; = -\frac{n d}{2} \log(2\pi) -\frac{n}{2} \log \det(\boldsymbol \Sigma)  
   - \frac{1}{2}  \sum_{k=1}^n  (\boldsymbol x_k-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x_k-\boldsymbol \mu) \,.\\
\end{split}
\]</span>
Written in terms of the precision matrix <span class="math inline">\(\boldsymbol \Omega= \boldsymbol \Sigma^{-1}\)</span> this becomes
<span class="math display">\[
\log L(\boldsymbol \mu, \boldsymbol \Omega) = -\frac{n d}{2} \log(2\pi) +\frac{n}{2} \log \det(\boldsymbol \Omega)  - \frac{1}{2}  \sum_{k=1}^n  (\boldsymbol x_k-\boldsymbol \mu)^T \boldsymbol \Omega(\boldsymbol x_k-\boldsymbol \mu) \,.
\]</span>
Exploiting identities for the trace and log det (see Appendix) we can rewrite
<span class="math inline">\((\boldsymbol x_k-\boldsymbol \mu)^T \boldsymbol \Omega(\boldsymbol x_k-\boldsymbol \mu) = \text{Tr}( (\boldsymbol x_k-\boldsymbol \mu) (\boldsymbol x_k-\boldsymbol \mu)^T \boldsymbol \Omega)\)</span>
and <span class="math inline">\(\log \det(\boldsymbol \Omega) = \text{Tr}( \log \boldsymbol \Omega)\)</span>
we rewrite the log-likelihood as
<span class="math display">\[\log L(\boldsymbol \mu, \boldsymbol \Omega) =  -\frac{n d}{2} \log(2\pi) +\frac{n}{2}  \text{Tr}( \log \boldsymbol \Omega)  - \frac{1}{2}  \sum_{k=1}^n  \text{Tr}( (\boldsymbol x_k-\boldsymbol \mu) (\boldsymbol x_k-\boldsymbol \mu)^T \boldsymbol \Omega) \,.\]</span></p>
<p>First, to find the MLE for <span class="math inline">\(\boldsymbol \mu\)</span> we compute (see Appendix for some rules in vector and matrix calculus)
<span class="math display">\[\frac{\partial \log L(\boldsymbol \mu, \boldsymbol \Omega) }{\partial \boldsymbol \mu}= \sum_{k=1}^n (\boldsymbol x_k-\boldsymbol \mu)^T  \boldsymbol \Omega\,.\]</span>
Setting this equal to zero we get <span class="math inline">\(\sum_{k=1}^n \boldsymbol x_k = n \hat{\boldsymbol \mu}_{ML}\)</span> and thus
<span class="math display">\[\hat{\boldsymbol \mu}_{ML} = \frac{1}{n} \sum_{k=1}^n \boldsymbol x_k\,.\]</span></p>
<p>Next, to obtain the MLE for <span class="math inline">\(\boldsymbol \Omega\)</span> we compute
<span class="math display">\[\frac{\partial \log L(\boldsymbol \mu, \boldsymbol \Omega) }{\partial \boldsymbol \Omega}=\frac{n}{2}\boldsymbol \Omega^{-1} - \frac{1}{2}  \sum_{k=1}^n (\boldsymbol x_k-\boldsymbol \mu) (\boldsymbol x_k-\boldsymbol \mu)^T\,.\]</span>
Setting this equal to zero and substituting the MLE for <span class="math inline">\(\boldsymbol \mu\)</span> we get
<span class="math display">\[\widehat{\boldsymbol \Omega}^{-1}_{ML}=  \frac{1}{n} \sum_{k=1}^n  (\boldsymbol x_k-\hat{\boldsymbol \mu}) (\boldsymbol x_k-\hat{\boldsymbol \mu})^T=\widehat{\boldsymbol \Sigma}_{ML}\,.\]</span></p>
<p>Therefore, the MLEs are identical to the empirical estimates.</p>
<p>Note the factor <span class="math inline">\(\frac{1}{n}\)</span> in the MLE of the covariance matrix.</p>
</div>
<div id="distribution-of-the-empirical-maximum-likelihood-estimates" class="section level4">
<h4><span class="header-section-number">1.4.3.3</span> Distribution of the empirical / maximum likelihood estimates</h4>
<p>With <span class="math inline">\(\boldsymbol x_1,...,\boldsymbol x_n \sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> one can find the exact distributions
of the estimators.</p>
<p><strong>1. Distribution of the estimate of the mean:</strong></p>
<p><span class="math display">\[\hat{\boldsymbol \mu}_{ML} \sim N_d\left(\boldsymbol \mu, \frac{\boldsymbol \Sigma}{n}\right)\]</span>
Since
<span class="math inline">\(\text{E}(\hat{\boldsymbol \mu}_{ML}) = \boldsymbol \mu\Longrightarrow \hat{\boldsymbol \mu}_{ML}\)</span> is unbiased.</p>
<p><strong>2. Distribution of the covariance estimate:</strong></p>
<p><span class="math display">\[\widehat{\boldsymbol \Sigma}_{ML} \sim \text{Wishart}(\frac{\boldsymbol \Sigma}{n}, n-1)\]</span>
Since
<span class="math inline">\(\text{E}(\widehat{\boldsymbol \Sigma}_{ML}) = \frac{n-1}{n}\boldsymbol \Sigma\)</span> <span class="math inline">\(\Longrightarrow \widehat{\boldsymbol \Sigma}_{ML}\)</span> is biased, with <span class="math inline">\(\text{Bias}(\widehat{\boldsymbol \Sigma}_{ML} ) = \boldsymbol \Sigma- \text{E}(\widehat{\boldsymbol \Sigma}_{ML}) = -\frac{\boldsymbol \Sigma}{n}\)</span>.</p>
<p>Easy to make unbiased:
<span class="math inline">\(\widehat{\boldsymbol \Sigma}_{UB} = \frac{n}{n-1}\widehat{\boldsymbol \Sigma}=\frac{1}{n-1}\sum^n_{k=1}\left(\boldsymbol x_k-\hat{\boldsymbol \mu}\right)\left(\boldsymbol x_k-\hat{\boldsymbol \mu}\right)^T\)</span> is unbiased.</p>
<p>But unbiasedness of an estimator is <strong>not</strong> a very relevant criterion in multivariate statistics as we will see in the next section.</p>
</div>
</div>
<div id="problems-with-maximum-likelihood-in-small-sample-settings-and-high-dimensions" class="section level3">
<h3><span class="header-section-number">1.4.4</span> Problems with maximum likelihood in small sample settings and high dimensions</h3>
<p><strong>Modern data is high dimensional!</strong></p>
<p>Data sets with <span class="math inline">\(n&lt;d\)</span>, i.e. high dimension <span class="math inline">\(d\)</span> and small sample size <span class="math inline">\(n\)</span> are now common in
many fields, e.g., medicine, biology but also finance and business analytics.</p>
<p><span class="math display">\[n = 100 \, \text{(e.g, patients/samples)}\]</span>
<span class="math display">\[d = 20000 \, \text{(e.g., genes/SNPs/proteins/variables)}\]</span>
Reasons:</p>
<ul>
<li>the number of measured variables is increasing quickly with technological advances (e.g. genomics)</li>
<li>but the number of samples cannot be similary increased (for cost and ethical reasons)</li>
</ul>
<p><strong>General problems of MLEs:</strong></p>
<ol style="list-style-type: decimal">
<li>ML estimators are optimal only if <strong>sample size is large</strong> compared to the number of parameters. However, this optimality is not any more valid if sample size is moderate or smaller than the number of parameters.</li>
<li>If there is not enough data the <strong>ML estimate overfits</strong>. This means ML fits the current data perfectly but the resulting model does not generalise well (i.e. model will perform poorly in prediction)</li>
<li>If there is a choice between different models with different complexity <strong>ML will always select the model with the largest number of parameters</strong>.</li>
</ol>
<p><strong>-&gt; for high-dimensional data with small sample size maximum likelihood estimation does not work!!!</strong></p>
<p><strong>History of Statistics:</strong></p>
<p>Much of modern statistics (from 1960 onwards) is devoted to the development of inference and estimation techniques that work with complex, high-dimensional data.</p>
<p><img src="fig/fig1-history.png" width="90%" style="display: block; margin: auto;" /></p>
<ul>
<li>Maximum likelihood is a method from classical statistics (time up to about 1960).</li>
<li>From 1960 modern (computational) statistics emerges, starting with
<strong>“Stein Paradox” (1956):</strong> Charles Stein showed that in a <strong>multivariate setting</strong> ML estimators are <strong>dominated by</strong> (= are always worse than) shrinkage estimators!</li>
<li>For example, there is a shrinkage estimator for the mean that is better (in terms of MSE) than the average (which is the MLE)!</li>
</ul>
<p>Modern statistics has developed many different (but related) methods for use in high-dimensional, small sample settings:</p>
<ul>
<li>regularised estimators</li>
<li>shrinkage estimators</li>
<li>penalised maximum likelihood estimators</li>
<li>Bayesian estimators</li>
<li>Empirical Bayes estimators</li>
<li>KL / entropy-based estimators</li>
</ul>
<p>Most of this is out of scope for our class, but will be covered in advanced statistical courses.</p>
<p>Next, we describe a <strong>simple regularised estimator for the estimation of the covariance</strong>
that we will use later (i.e. in classification).</p>
<div style="page-break-after: always;"></div>
</div>
<div id="estimation-of-covariance-matrix-in-small-sample-settings" class="section level3">
<h3><span class="header-section-number">1.4.5</span> Estimation of covariance matrix in small sample settings</h3>
<p><strong>Problems with ML estimate of <span class="math inline">\(\boldsymbol \Sigma\)</span></strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\Sigma\)</span> has O(<span class="math inline">\(d^2\)</span>) number of parameters!
<span class="math inline">\(\Longrightarrow \hat{\boldsymbol \Sigma}^{\text{MLE}}\)</span> requires <em>a lot</em> of data! <span class="math inline">\(n\gg d \text{ or } d^2\)</span></p></li>
<li><p>if <span class="math inline">\(n &lt; d\)</span> then <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> is positive <strong>semi</strong>-definite (even if <span class="math inline">\(\Sigma\)</span> is p.d.f.!)<br />
<span class="math inline">\(\Longrightarrow \hat{\boldsymbol \Sigma}\)</span> will have <strong>vanishing eigenvalues</strong> (some <span class="math inline">\(\lambda_i=0\)</span>) and thus <strong>cannot be inverted</strong> and is singular!</p></li>
</ol>
<p><strong>Simple regularised estimate of <span class="math inline">\(\boldsymbol \Sigma\)</span></strong></p>
<p>Regularised estimator <span class="math inline">\(\boldsymbol S^\ast\)</span> = convex combination of <span class="math inline">\(\boldsymbol S= \hat{\boldsymbol \Sigma}^\text{MLE}\)</span> and <span class="math inline">\(\boldsymbol I_d\)</span> (identity matrix) to get</p>
<p>Regularisation:
<span class="math display">\[\underbrace{\boldsymbol S^\ast}_{\text{regularised estimate}} = \underbrace{\lambda}_{\text{shrinkage intensity}} \, \underbrace{\boldsymbol I_d}_{\text{target}} + (1-\lambda)\underbrace{\boldsymbol S}_{\text{ML estimate}}\]</span>
Next, choose <span class="math inline">\(\lambda \in [0,1]\)</span> such that <span class="math inline">\(\boldsymbol S^\ast\)</span> is better (in terms of MSE) than both <span class="math inline">\(\boldsymbol S\)</span> and <span class="math inline">\(\boldsymbol I_d\)</span>.</p>
<p><strong>Bias-variance trade-off</strong></p>
<p><span class="math inline">\(\text{MSE}\)</span> is the Mean Squared Error, composed of squared bias and variance.</p>
<p><span class="math display">\[\text{MSE}(\theta) = \text{E}((\hat{\theta}-\theta)^2) = \text{Bias}(\hat{\theta})^2 + \text{Var}(\hat{\theta})\]</span>
with <span class="math inline">\(\text{Bias}(\hat{\theta}) = \text{E}(\hat{\theta})-\theta\)</span></p>
<p><span class="math inline">\(\boldsymbol S\)</span>: ML estimate, many parameters, low bias, high variance<br />
<span class="math inline">\(\boldsymbol I_d\)</span>: “target”, no parameters, high bias, low variance<br />
<span class="math inline">\(\Longrightarrow\)</span> <strong>reduce high variance of <span class="math inline">\(\boldsymbol S\)</span> by <em>introducing</em> a bit of bias through <span class="math inline">\(\boldsymbol I_d\)</span></strong>!<br />
<span class="math inline">\(\Longrightarrow\)</span> overall, <span class="math inline">\(\text{MSE}\)</span> is decreased</p>
<p><img src="fig/fig1-biasvariance.png" width="90%" style="display: block; margin: auto;" /></p>
<p>How to find optimal shrinkage / regularisation parameter <span class="math inline">\(\lambda\)</span>? Minimise <span class="math inline">\(\text{MSE}\)</span>!</p>
<p>Challenge: since we don’t know the true <span class="math inline">\(\boldsymbol \Sigma\)</span> we cannot actually compute the <span class="math inline">\(\text{MSE}\)</span> directly but have to estimate it! How is this done in practise?</p>
<ul>
<li>by cross-validation (=resampling procedure)</li>
<li>by using some analytic approximation (e.g. Stein’s formula)</li>
</ul>
<p><strong>Why does regularisation of <span class="math inline">\(\hat{\boldsymbol \Sigma}\)</span> work?</strong></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\boldsymbol S^\ast\)</span> is <strong>positive definite</strong>:<br />
Matrix Theory:<br />
<span class="math display">\[\underbrace{\boldsymbol M_1}_{ \text{symmetric positive definite, } \lambda \boldsymbol I} + \underbrace{\boldsymbol M_2}_{\text{symmetric positive semi-definite, } (1-\lambda) \boldsymbol S} = \underbrace{\boldsymbol M_3}_{\text{symmetric positive definite, } \boldsymbol S^\ast} \]</span><br />
<span class="math inline">\(\Longrightarrow \boldsymbol S^\ast\)</span> can be inverted even if <span class="math inline">\(n&lt;d\)</span></li>
</ol>
<p>(see Appendix A for details).</p>
<ol start="2" style="list-style-type: decimal">
<li>It’s <strong>Bayesian</strong> in disguise!
<span class="math display">\[\underbrace{\boldsymbol S^\ast}_{\text{posterior mean}} = \underbrace{\lambda \boldsymbol I_d}_{\text{prior information}}  + (1-\lambda)\underbrace{\boldsymbol S}_{\text{data summarised by maximum likelihood}}\]</span>
<ul>
<li>Prior information helps to infer <span class="math inline">\(\boldsymbol \Sigma\)</span> even in small samples</li>
<li>Since <span class="math inline">\(\lambda\)</span> is chosen from data, it is actually an empirical Bayes.</li>
<li>also called shrinkage estimator since the off-diagonal entries are shrunk towards zero</li>
<li>this type of linear shrinkage/regularisation is natural for models in the exponential family (Diaconis and Ylvisaker, 1979)</li>
</ul></li>
</ol>
<p>In Worksheet 2 the empirical estimator of covariance is compared with the covariance estimator implemented in the R package
“corpcor”. This uses a regularisation similar as above (but for the correlation rather than
covariance matrix) and it
employs an analytic data-adaptive estimate of the shrinkage intensity <span class="math inline">\(\lambda\)</span>.
This estimator is a variant of an empirical Bayes / James-Stein estimator (see <a href="http://www.strimmerlab.org/publications/lecture-notes/MATH20802/">MATH20802 Statistical Methods</a>).
.</p>
<p><strong>Summary</strong></p>
<ul>
<li>In multivariate statistics, it is useful (and often necessary) to utilise prior information!</li>
<li>Regularisation introduces bias and reduces variance, minimising overall MSE</li>
<li>Unbiased estimation (a highly valued property in classical statistics!) is not a good idea in multivariate settings and often leads to poor estimators.</li>
</ul>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="categorical-and-multinomial-distribution" class="section level2">
<h2><span class="header-section-number">1.5</span> Categorical and multinomial distribution</h2>
<div id="categorical-distribution" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Categorical distribution</h3>
<p>The <strong>categorical distribution</strong> is a generalisation of the Bernoulli distribution
and is correspondingly also known as <strong>Multinoulli</strong> distribution.</p>
<p>Assume we have <span class="math inline">\(K\)</span> classes labeled “class 1”, “class 2”, …, “class K”.
A <em>discrete</em> random variable with a state space consisting of these <span class="math inline">\(K\)</span> classes
has a categorical distribution <span class="math inline">\(\text{Cat}(\boldsymbol \pi)\)</span>.
The parameter vector
<span class="math inline">\(\boldsymbol \pi= (\pi_1, \ldots, \pi_K)^T\)</span> specifies
the probabilities of each of the <span class="math inline">\(K\)</span> classes with <span class="math inline">\(\text{Pr}(\text{&quot;class k&quot;}) = \pi_k\)</span>.
The parameters satisfy <span class="math inline">\(\pi_k \in [0,1]\)</span> and
<span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span>, hence there are <span class="math inline">\(K-1\)</span> independent parameters in a categorical distribution (and not <span class="math inline">\(K\)</span>).</p>
<p>Sampling from a categorical distributions <span class="math inline">\(\text{Cat}(\boldsymbol \pi)\)</span> yields one of <span class="math inline">\(K\)</span> classes.
There are several ways to numerically
represent “class k”, for example simply by the corresponding number <span class="math inline">\(k\)</span>. However, instead
of this “integer encoding” it is often
more convenient to use the so-called “one hot encoding” where the class
is represented by an indicator vector
<span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_K)^T = (0, 0, \ldots, 1, \ldots, 0)^T\)</span> containing zeros everywhere except for
the element <span class="math inline">\(x_k=1\)</span> at position <span class="math inline">\(k\)</span>. Thus all <span class="math inline">\(x_k \in \{ 0, 1\}\)</span> and <span class="math inline">\(\sum_{k=1}^K x_k = 1\)</span>.</p>
<p>The expectation of <span class="math inline">\(\boldsymbol x\sim \text{Cat}(\boldsymbol \pi)\)</span> is <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \pi\)</span>, with
<span class="math inline">\(\text{E}(x_k) = \pi_k\)</span>.
The covariance matrix is <span class="math inline">\(\text{Var}(\boldsymbol x) = \text{Diag}(\boldsymbol \pi) - \boldsymbol \pi\boldsymbol \pi^T\)</span>.
In component notation <span class="math inline">\(\text{Var}(x_i) = \pi_i (1-\pi_i)\)</span> and <span class="math inline">\(\text{Cov}(x_i, x_j) = -\pi_i \pi_j\)</span>.
This follows directly from the definition of the variance <span class="math inline">\(\text{Var}(\boldsymbol x) = \text{E}( \boldsymbol x\boldsymbol x^T) - \text{E}( \boldsymbol x) \text{E}( \boldsymbol x)^T\)</span>
and noting that <span class="math inline">\(x_i^2 = x_i\)</span> and <span class="math inline">\(x_i x_j = 0\)</span> if <span class="math inline">\(i \neq j\)</span>.
Note that the variance matrix <span class="math inline">\(\text{Var}(\boldsymbol x)\)</span> is singular by construction, as the <span class="math inline">\(K\)</span> random variables
<span class="math inline">\(x_1, \ldots, x_K\)</span> are dependent through the constraint <span class="math inline">\(\sum_{k=1}^K x_k = 1\)</span>.</p>
<p>The corresponding probability mass function (pmf)
can be written conveniently in terms of <span class="math inline">\(x_k\)</span> as
<span class="math display">\[
f(\boldsymbol x) = \prod_{k=1}^K \pi_k^{x_k} = 
\begin{cases} 
   \pi_k  &amp; \text{if } x_k = 1 \\
\end{cases}
\]</span>
and the log pmf as
<span class="math display">\[
\log f(\boldsymbol x) = \sum_{k=1}^K x_k \log \pi_k   =
\begin{cases} 
   \log \pi_k  &amp; \text{if } x_k = 1 \\
\end{cases}
\]</span></p>
<p>In order to be more explicit that the categorical distribution has <span class="math inline">\(K-1\)</span> and not <span class="math inline">\(K\)</span> parameters
we rewrite the log-density with
<span class="math inline">\(\pi_K = 1 - \sum_{k=1}^{K-1} \pi_k\)</span> and <span class="math inline">\(x_K = 1 - \sum_{k=1}^{K-1} x_k\)</span> as
<span class="math display">\[
\begin{split}
\log f(\boldsymbol x) &amp; =\sum_{k=1}^{K-1}  x_k \log \pi_k    + x_K \log \pi_K \\
 &amp; =\sum_{k=1}^{K-1}  x_k \log \pi_k    + \left( 1 - \sum_{k=1}^{K-1} x_k  \right) \log \left( 1 - \sum_{k=1}^{K-1} \pi_k \right) \\
\end{split}
\]</span>
Note that there is no particular reason to choose <span class="math inline">\(\pi_K\)</span> as derived, in its place any other of the
<span class="math inline">\(\pi_k\)</span> may be selected.</p>
<p>For <span class="math inline">\(K=2\)</span> the categorical distribution reduces to the Bernoulli <span class="math inline">\(\text{Ber}(p)\)</span> distribution,
with <span class="math inline">\(\pi_1=p\)</span> and <span class="math inline">\(\pi_2=1-p\)</span>.</p>
</div>
<div id="multinomial-distribution" class="section level3">
<h3><span class="header-section-number">1.5.2</span> Multinomial distribution</h3>
<p>The multinomial distribution arises from repeated categorical sampling,
just like the Binomial distribution arises from repeated Bernoulli sampling.</p>
<div id="univariate-case" class="section level4">
<h4><span class="header-section-number">1.5.2.1</span> Univariate case</h4>
<p>Binomial Distribution:</p>
<p>Repeat Bernoulli <span class="math inline">\(\text{Ber}(\pi)\)</span> experiment <span class="math inline">\(n\)</span> times:</p>
<p><span class="math display">\[x \sim \text{Bin}(n, \pi)\]</span>
<span class="math display">\[ x \in \{0,...,n\}\]</span>
<span class="math display">\[\text{E}(x) = n \, \pi\]</span>
<span class="math display">\[\text{Var}(x)=n \, \pi(1-\pi)\]</span></p>
<p>Standardised to unit interval:
<span class="math display">\[\frac{x}{n} \in \left\{0,\frac{1}{n},...,1\right\}\]</span>
<span class="math display">\[\text{E}\left(\frac{x}{n}\right) = \pi\]</span>
<span class="math display">\[\text{Var}\left(\frac{x}{n}\right)=\frac{\pi(1-\pi)}{n}\]</span></p>
<p><span class="math display">\[\textbf{Urn model:}\]</span></p>
<p>distribute <span class="math inline">\(n\)</span> balls into two bins</p>
<p><img src="fig/fig1-binom.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="multivariate-case" class="section level4">
<h4><span class="header-section-number">1.5.2.2</span> Multivariate case</h4>
<p>Multinomial distribution:</p>
<p>Draw <span class="math inline">\(n\)</span> times from categorical distribution <span class="math inline">\(\text{Cat}(\boldsymbol \pi)\)</span>:</p>
<p><span class="math display">\[\boldsymbol x\sim \text{Mult}(n, \boldsymbol \pi)  \]</span>
<span class="math display">\[ x_i \in \{0,1,...,n\}; \, \sum^{K}_{i=1}x_i = n\]</span>
<span class="math display">\[\text{E}(\boldsymbol x) = n \,\boldsymbol \pi\]</span>
<span class="math display">\[\text{Var}(x_i)=n\, \pi_i(1-\pi_i)\]</span>
<span class="math display">\[\text{Cov}(x_i,x_j)=-n\, \pi_i\pi_j\]</span></p>
<p>Standardised to unit interval:
<span class="math display">\[\frac{x_i}{n} \in \left\{0,\frac{1}{n},\frac{2}{n},...,1\right\}\]</span>
<span class="math display">\[\text{E}\left(\frac{\boldsymbol x}{n}\right) = \boldsymbol \pi\]</span>
<span class="math display">\[\text{Var}\left(\frac{x_i}{n}\right)=\frac{\pi_i(1-\pi_i)}{n}\]</span>
<span class="math display">\[\text{Cov}\left(\frac{x_i}{n},\frac{x_j}{n}\right)=-\frac{\pi_i\pi_j}{n} \]</span>
<span class="math display">\[\textbf{Urn model:}\]</span></p>
<p>distribute <span class="math inline">\(n\)</span> balls into <span class="math inline">\(K\)</span> bins:</p>
<p><img src="fig/fig1-multinom.png" width="60%" style="display: block; margin: auto;" /></p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="entropy-and-maximum-likelihood-analysis-for-the-categorical-distribution" class="section level3">
<h3><span class="header-section-number">1.5.3</span> Entropy and maximum likelihood analysis for the categorical distribution</h3>
<p>In the folling examples we compute the KL divergence and MLE as well related quantities for the categorical distribution.</p>
<p>These generalise the same calculations for the Bernoulli distribution discussed in the earlier module <a href="http://www.strimmerlab.org/publications/lecture-notes/MATH20802/">MATH20802 Statistical Methods</a>.</p>
<div class="example">
<p><span id="exm:catkl" class="example"><strong>Example 1.1  </strong></span>KL divergence between two categorical distributions with <span class="math inline">\(K\)</span> classes:</p>
<p>With <span class="math inline">\(P=\text{Cat}(\boldsymbol p)\)</span> and <span class="math inline">\(Q=\text{Cat}(\boldsymbol q)\)</span> and corresponding
probabilities <span class="math inline">\(p_1,\dots,p_K\)</span> and <span class="math inline">\(q_1,\dots,q_K\)</span> satisfying <span class="math inline">\(\sum_{i=1}^K p_i =1\)</span> and <span class="math inline">\(\sum_{i=1}^K q_i = 1\)</span> we get:</p>
<p><span class="math display">\[\begin{equation*}
D_{\text{KL}}(P, Q)=\sum_{i=1}^K p_i\log\left(\frac{p_i}{q_i}\right) 
\end{equation*}\]</span></p>
<p>To be explicit that there are only <span class="math inline">\(K-1\)</span> parameters in a categorical distribution we can also write
<span class="math display">\[\begin{equation*}
D_{\text{KL}}(P, Q)=\sum_{i=1}^{K-1} p_i\log\left(\frac{p_i}{q_i}\right)  + p_K\log\left(\frac{p_K}{q_K}\right)
\end{equation*}\]</span>
with <span class="math inline">\(p_K=\left(1- \sum_{i=1}^{K-1} p_i\right)\)</span> and
<span class="math inline">\(q_K=\left(1- \sum_{i=1}^{K-1} q_i\right)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:catexpectfisher" class="example"><strong>Example 1.2  </strong></span>Expected Fisher information of the categorical distribution:</p>
<p>We first compute the Hessian matrix
<span class="math inline">\(\nabla^T \nabla \log f(\boldsymbol x)\)</span> of the log-probability mass function, where the
differentiation is with regard to <span class="math inline">\(\pi_1, \ldots, \pi_{K-1}\)</span>.</p>
<p>The diagonal entries of the Hessian matrix (with <span class="math inline">\(i=1, \ldots, K-1\)</span>) are
<span class="math display">\[
\frac{\partial^2}{\partial \pi_i^2} \log f(\boldsymbol x) =
  -\frac{x_i}{\pi_i^2}-\frac{x_K}{\pi_K^2}
\]</span>
and its off-diagonal entries are (with <span class="math inline">\(j=1, \ldots, K-1\)</span>)
<span class="math display">\[
\frac{\partial^2}{\partial \pi_i \partial \pi_j} \log f(\boldsymbol x) =
 -\frac{ x_K}{\pi_K^2}
\]</span>
Recalling that <span class="math inline">\(\text{E}(x_i) = \pi_i\)</span> we can compute the expected Fisher information matrix for a categorical distribution as
<span class="math display">\[
\begin{split}
\boldsymbol I^{\text{Fisher}}\left( \pi_1, \ldots, \pi_{K-1}  \right) &amp;= -\text{E}\left( \nabla^T \nabla \log f(\boldsymbol x) \right) \\
&amp; =
\begin{pmatrix}
 \frac{1}{\pi_1} + \frac{1}{\pi_K} &amp; \cdots &amp; \frac{1}{\pi_K} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{1}{\pi_K} &amp; \cdots &amp; \frac{1}{\pi_{K-1}} + \frac{1}{\pi_K} \\
\end{pmatrix}\\
&amp; = \text{Diag}\left( \frac{1}{\pi_1} , \ldots,  \frac{1}{\pi_{K-1}}   \right) + \frac{1}{\pi_K} \boldsymbol 1\\
\end{split}
\]</span></p>
<p>For <span class="math inline">\(K=2\)</span> and <span class="math inline">\(\pi_1=p\)</span> this reduces to the expected Fisher information of a Bernoulli variable
<span class="math display">\[
\begin{split}
I^{\text{Fisher}}(p) &amp; =  \left(\frac{1}{p} + \frac{1}{1-p} \right) \\
  &amp;= \frac{1}{p (1-p)} \\
\end{split}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:catquadapproxkl" class="example"><strong>Example 1.3  </strong></span>Quadratic approximation of KL divergence of the categorical distribution:</p>
<p>The expected Fisher information arises from a local quadratic approximation of the KL divergence:
<span class="math display">\[
D_{\text{KL}}(F_{\boldsymbol \theta}, F_{\boldsymbol \theta+\boldsymbol \varepsilon})  \approx  \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)  \boldsymbol \varepsilon
\]</span>
and
<span class="math display">\[
D_{\text{KL}}(F_{\boldsymbol \theta+\boldsymbol \varepsilon}, F_{\boldsymbol \theta}) \approx  \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)  \boldsymbol \varepsilon
\]</span></p>
<p>We now consider the KL divergence <span class="math inline">\(D_{\text{KL}}(P, Q)\)</span> between the categorical distribution <span class="math inline">\(P=\text{Cat}(\boldsymbol p)\)</span> with probabilities <span class="math inline">\(\boldsymbol p=(p_1, \ldots, p_K)^T\)</span> with the categorical distribution <span class="math inline">\(Q=\text{Cat}(\boldsymbol q)\)</span> with probabilities <span class="math inline">\(\boldsymbol q= (q_1, \ldots, q_K)^T\)</span>.</p>
<p>First, we keep <span class="math inline">\(P\)</span> fixed and assume that <span class="math inline">\(Q\)</span> is a perturbed version of <span class="math inline">\(P\)</span> with <span class="math inline">\(\boldsymbol q= \boldsymbol p+\boldsymbol \varepsilon\)</span>.
Note that the perturbations <span class="math inline">\(\boldsymbol \varepsilon=(\varepsilon_1, \ldots, \varepsilon_K)^T\)</span> satisfy
<span class="math inline">\(\sum_{k=1}^K \varepsilon_k = 0\)</span> because <span class="math inline">\(\sum_{k=1}^K p_i=1\)</span> and <span class="math inline">\(\sum_{k=1}^K q_i=1\)</span>.
Thus <span class="math inline">\(\varepsilon_K = -\sum_{k=1}^{K-1} \varepsilon_k\)</span>. Then
<span class="math display">\[
\begin{split}
D_{\text{KL}}(P, Q=P+\varepsilon) &amp;  = D_{\text{KL}}(\text{Cat}(\boldsymbol p), \text{Cat}(\boldsymbol p+\boldsymbol \varepsilon)) \\
&amp;  \approx \frac{1}{2} (\varepsilon_1, \ldots,  \varepsilon_{K-1}) \,
\boldsymbol I^{\text{Fisher}}\left( p_1, \ldots, p_{K-1}  \right) 
\begin{pmatrix} \varepsilon_1 \\ \vdots \\  \varepsilon_{K-1}\\
\end{pmatrix} \\
&amp;= \frac{1}{2} \left( \sum_{k=1}^{K-1} \frac{\varepsilon_k^2}{p_k}   + \frac{ \left(\sum_{k=1}^{K-1} \varepsilon_k\right)^2}{p_K} \right)  \\
&amp;= \frac{1}{2}  \sum_{k=1}^{K} \frac{\varepsilon_k^2}{p_k}\\
&amp;= \frac{1}{2}  \sum_{k=1}^{K} \frac{(p_k-q_k)^2}{p_k}\\
&amp; = \frac{1}{2} D_{\text{Neyman}}(P, Q)\\
\end{split} 
\]</span>
Similarly, if we keep <span class="math inline">\(Q\)</span> fixed and consider <span class="math inline">\(P\)</span> as a disturbed version of <span class="math inline">\(Q\)</span> we get
<span class="math display">\[
\begin{split}
D_{\text{KL}}(P=Q+\varepsilon, Q) &amp;  =D_{\text{KL}}(\text{Cat}(\boldsymbol q+\boldsymbol \varepsilon), \text{Cat}(\boldsymbol q)) \\
&amp;\approx \frac{1}{2}  \sum_{k=1}^{K} \frac{(p_k-q_k)^2}{q_k}\\
&amp;= \frac{1}{2} D_{\text{Pearson}}(P, Q)
\end{split}
\]</span>
Note that in both approximations we divide by the probabilities of the distribution that
is kept fixed.</p>
<p>Note the appearance of the <em>Pearson <span class="math inline">\(\chi^2\)</span> divergence</em> and the <em>Neyman <span class="math inline">\(\chi^2\)</span> divergence</em> in the above. Both are, like the KL divergence, part of the family of <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergences</a>. The Neyman <span class="math inline">\(\chi^2\)</span>
divergence is also known as the reverse Pearson divergence as <span class="math inline">\(D_{\text{Neyman}}(P, Q) = D_{\text{Pearson}}(Q, P)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:catmle" class="example"><strong>Example 1.4  </strong></span>Maximum likelihood estimation of the parameters of the categorical distribution:</p>
<p>Maximum likelihood estimation seems trivial at first sight but it is in fact a bit more complicated since there are only <span class="math inline">\(K-1\)</span> free parameters, and not <span class="math inline">\(K\)</span>. So we either need to optimise with regard to a specific set of <span class="math inline">\(K-1\)</span> parameters (which is what we do below) or use a constrained optimisation procedure to enforce that <span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span> (for example by using a Lagrange multiplier).</p>
<ul>
<li><p>The data: We observe <span class="math inline">\(n\)</span> samples <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n\)</span>.
The data matrix of dimension <span class="math inline">\(n \times K\)</span> is
<span class="math inline">\(\boldsymbol X= (\boldsymbol x_1, \ldots, \boldsymbol x_n)^T = (x_{ik})\)</span>.
It contains each <span class="math inline">\(\boldsymbol x_i = (x_{i1}, \ldots, x_{iK})^T\)</span>. The corresponding summary
(minimal sufficient) statistics are
<span class="math inline">\(\bar{\boldsymbol x} = \frac{1}{n} \sum_{i=1}^n \boldsymbol x_i = (\bar{x}_1, \ldots, \bar{x}_K)^T\)</span>
with
<span class="math inline">\(\bar{x}_k = \frac{1}{n} \sum_{i=1}^n x_{ik}\)</span>. We can also write
<span class="math inline">\(\bar{x}_{K} = 1 - \sum_{k=1}^{K-1} \bar{x}_{k}\)</span>.
The number of samples for class <span class="math inline">\(k\)</span> is <span class="math inline">\(n_k = n \bar{x}_k\)</span> with
<span class="math inline">\(\sum_{k=1}^K n_k = n\)</span>.</p></li>
<li><p>The log-likelihood is
<span class="math display">\[
\begin{split}
l_n(\pi_1, \ldots, \pi_{K-1}) &amp; = \sum_{i=1}^n \log f(\boldsymbol x_i) \\
&amp; =\sum_{i=1}^n \left( \sum_{k=1}^{K-1}  x_{ik} \log \pi_k    + \left( 1 - \sum_{k=1}^{K-1} x_{ik}  \right) \log \left( 1 - \sum_{k=1}^{K-1} \pi_k \right) \right)\\
&amp; = n \left(  \sum_{k=1}^{K-1}  \bar{x}_k \log \pi_k    + \left(
1 - \sum_{k=1}^{K-1} \bar{x}_{k} \right) \log\left(1 - \sum_{k=1}^{K-1} \pi_k\right) \right) \\ 
&amp; = n \left(  \sum_{k=1}^{K-1}  \bar{x}_k \log \pi_k    + \bar{x}_K \log \pi_K \right) \\  
\end{split}
\]</span></p></li>
<li><p>Score function (gradient, row vector)
<span class="math display">\[
\begin{split}
\boldsymbol S_n(\pi_1, \ldots, \pi_{K-1}) &amp;=  \nabla l_n(\pi_1, \ldots, \pi_{K-1} ) \\
&amp; = 
\begin{pmatrix}
 \frac{\partial}{\partial \pi_1} l_n(\pi_1, \ldots, \pi_{K-1} )  \\
\vdots\\
\frac{\partial}{\partial \pi_{K-1}} l_n(\pi_1, \ldots, \pi_{K-1} )  \\
\end{pmatrix}^T\\
&amp; = n
\begin{pmatrix}
\frac{\bar{x}_1}{\pi_1}-\frac{\bar{x}_K}{\pi_K}  \\
\vdots\\
\frac{\bar{x}_{K-1}}{\pi_{K-1}}-\frac{\bar{x}_K}{\pi_K}  \\
\end{pmatrix}^T\\
\end{split}
\]</span>
Note in particular the need for the second term that arises because <span class="math inline">\(\pi_K\)</span> depends on all
the <span class="math inline">\(\pi_1, \ldots, \pi_{K-1}\)</span>.</p></li>
<li>Maximum likelihood estimate: Setting <span class="math inline">\(\boldsymbol S_n(\hat{\pi}_1^{ML}, \ldots, \hat{\pi}_{K-1}^{ML})=0\)</span> yields
<span class="math inline">\(K-1\)</span> equations
<span class="math display">\[
\bar{x}_i \left(1 - \sum_{k=1}^{K-1} \hat{\pi}_k^{ML}\right)  = \hat{\pi}_i^{ML} \left(
1 - \sum_{k=1}^{K-1} \bar{x}_{k} \right)
\]</span>
for <span class="math inline">\(i=1, \ldots, K-1\)</span> and with solution
<span class="math display">\[
\hat{\pi}_i^{ML} = \bar{x}_i
\]</span>
It also follows that
<span class="math display">\[ 
\hat{\pi}_K^{ML} = 1 - \sum_{k=1}^{K-1} \hat{\pi}_k^{ML} = 1 - \sum_{k=1}^{K-1} \bar{x}_{k} = \bar{x}_K
\]</span>
The maximum likelihood estimator is therefore the frequency
of of the occurance of a class among the <span class="math inline">\(n\)</span> samples.</li>
</ul>
</div>
<div class="example">
<p><span id="exm:catobsfisher" class="example"><strong>Example 1.5  </strong></span>Observed Fisher information of the categorical distribution:</p>
<p>We first need to compute the negative Hessian matrix of the log likelihood function
<span class="math inline">\(- \nabla^T \nabla l_n(\pi_1, \ldots, \pi_{K-1} )\)</span> and then evaluate it at the
MLEs <span class="math inline">\(\hat{\pi}_1^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}\)</span>.</p>
<p>The diagonal entries of the Hessian matrix (with <span class="math inline">\(i=1, \ldots, K-1\)</span>) are
<span class="math display">\[
\frac{\partial^2}{\partial \pi_i^2} l_n(\pi_1, \ldots, \pi_{K-1} ) =
 -n \left( \frac{\bar{x}_i}{\pi_i^2} +\frac{\bar{x}_K}{\pi_K^2}\right)
\]</span>
and its off-diagonal entries are (with <span class="math inline">\(j=1, \ldots, K-1\)</span>)
<span class="math display">\[
\frac{\partial^2}{\partial \pi_i \partial \pi_j} l_n(\pi_1, \ldots, \pi_{K-1} ) =
 -\frac{n \bar{x}_K}{\pi_K^2}
\]</span>
Thus, the observed Fisher information matrix at the MLE for a categorical distribution is
<span class="math display">\[
\boldsymbol J_n(\hat{\pi}_{1}^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  ) =
n 
\begin{pmatrix}
 \frac{1}{\hat{\pi}_1^{ML}} + \frac{1}{\hat{\pi}_K^{ML}} &amp; \cdots &amp; \frac{1}{\hat{\pi}_K^{ML}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{1}{\hat{\pi}_K^{ML}} &amp; \cdots &amp; \frac{1}{\hat{\pi}_{K-1}^{ML}} + \frac{1}{\hat{\pi}_K^{ML}} \\
\end{pmatrix} 
\]</span></p>
<p>For <span class="math inline">\(K=2\)</span> this reduces to the observed Fisher information of a Bernoulli variable
<span class="math display">\[
\begin{split}
J_n(\hat{p}_{ML}) &amp; = n \left(\frac{1}{\hat{p}_{ML}} + \frac{1}{1-\hat{p}_{ML}} \right) \\
  &amp;= \frac{n}{\hat{p}_{ML} (1-\hat{p}_{ML})} \\
\end{split}
\]</span></p>
<p>The inverse of the observed Fisher information is:
<span class="math display">\[
\boldsymbol J_n(\hat{\pi}_{1}^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  )^{-1} =
\frac{1}{n} 
\begin{pmatrix}
\hat{\pi}_1^{ML} (1- \hat{\pi}_1^{ML} )  &amp; \cdots &amp; -  \hat{\pi}_{1}^{ML} \hat{\pi}_{K-1}^{ML}   \\
\vdots &amp; \ddots &amp; \vdots \\
-  \hat{\pi}_{K-1}^{ML} \hat{\pi}_{1}^{ML} &amp; \cdots &amp; \hat{\pi}_{K-1}^{ML} (1- \hat{\pi}_{K-1}^{ML} )  \\
\end{pmatrix}
\]</span></p>
<p>To show that this is indeed the inverse we use the Woodbury matrix identity (see Appendix), with B=1, <span class="math inline">\(\boldsymbol u= (\pi_1, \ldots, \pi_{K-1})^T\)</span>, <span class="math inline">\(\boldsymbol v=-\boldsymbol u^T\)</span>,
<span class="math inline">\(\boldsymbol A= \text{Diag}(\boldsymbol u)\)</span> and its inverse <span class="math inline">\(\boldsymbol A^{-1} = \text{Diag}(\pi_1^{-1}, \ldots, \pi_{K-1}^{-1})\)</span>. Then <span class="math inline">\(\boldsymbol A^{-1} \boldsymbol u= \boldsymbol 1_{K-1}\)</span> and <span class="math inline">\(1-\boldsymbol u^T \boldsymbol A^{-1} \boldsymbol u= \pi_K\)</span>.
With this
<span class="math inline">\(\boldsymbol J_n(\hat{\pi}_{1}^{ML}, \ldots, \hat{\pi}_{K-1}^{ML} )^{-1} = \frac{1}{n} \left( \boldsymbol A- \boldsymbol u\boldsymbol u^T \right)\)</span>
and
<span class="math inline">\(\boldsymbol J_n(\hat{\pi}_{1}^{ML}, \ldots, \hat{\pi}_{K-1}^{ML} ) = n \left( \boldsymbol A^{-1} + \frac{1}{\pi_K} \boldsymbol 1_{K-1 \times K-1} \right)\)</span>.</p>
<p>For <span class="math inline">\(K=2\)</span> the inverse observed Fisher information of the categorical distribution reduceds to that of the Bernoulli distribution
<span class="math display">\[
J_n(\hat{p}_{ML})^{-1}=\frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n}
\]</span></p>
<p>The inverse observed Fisher information is useful, e.g., as
the asymptotic variance of the maximum likelihood estimates.</p>
</div>
<div class="example">
<p><span id="exm:catwald" class="example"><strong>Example 1.6  </strong></span>Wald statistic for the categorical distribution:</p>
<p>The squared Wald statistic is
<span class="math display">\[
\begin{split}
t(\boldsymbol p_0)^2 &amp;= 
(\hat{\pi}_{1}^{ML}-p_1^0, \ldots,  \hat{\pi}_{K-1}^{ML}-p_{K-1}^0)   \boldsymbol J_n(\hat{\pi}_{1}^{ML}, \ldots, \hat{\pi}_{K-1}^{ML} ) \begin{pmatrix} \hat{\pi}_{1}^{ML}-p_1^0 \\
\vdots \\
\hat{\pi}_{K-1}^{ML}-p_{K-1}^0\\
\end{pmatrix}\\
&amp;= n  \left( \sum_{k=1}^{K-1} \frac{(\hat{\pi}_{k}^{ML}-p_{k}^0)^2}{\hat{\pi}_{k}^{ML}}   + \frac{ \left(\sum_{k=1}^{K-1} (\hat{\pi}_{k}^{ML}-p_{k}^0)\right)^2}{\hat{\pi}_{K}^{ML}} \right)  \\
&amp;= n  \left( \sum_{k=1}^{K} \frac{(\hat{\pi}_{k}^{ML}-p_{k}^0)^2}{\hat{\pi}_{k}^{ML}}    \right)  \\
&amp; = n D_{\text{Neyman}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) )
\end{split}
\]</span></p>
<p>With <span class="math inline">\(n_1, \ldots, n_K\)</span> the observed counts with <span class="math inline">\(n = \sum_{k=1}^K n_k\)</span>
and <span class="math inline">\(\hat{\pi}_k^{ML} = \frac{n_k}{n} = \bar{x}_k\)</span>,
and <span class="math inline">\(n_1^{\text{expect}}, \ldots, n_K^{\text{expect}}\)</span> the
expected counts <span class="math inline">\(n_k^{\text{expect}} = n p_k^{0}\)</span> under <span class="math inline">\(\boldsymbol p_0\)</span>
we can write the squared Wald statistic
as follows:
<span class="math display">\[
t(\boldsymbol p_0)^2 = \sum_{k=1}^K \frac{(n_k-n_k^{\text{expect}} )^2}{n_k} =  \chi^2_{\text{Neyman}}
\]</span>
This is known as the Neyman chi-squared statistic (note the <em>observed</em> counts in its denominator) and it is asymptotically distributed as <span class="math inline">\(\chi^2_{K-1}\)</span> because there
are <span class="math inline">\(K-1\)</span> free parameters in <span class="math inline">\(\boldsymbol p_0\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:catwilks" class="example"><strong>Example 1.7  </strong></span>Wilks log-likelihood ratio statistic for the categorical distribution:</p>
<p>The Wilks log-likelihood ratio is
<span class="math display">\[
W(\boldsymbol p_0) = 2 (l_n(\hat{\pi}_1^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  ) - l_n(p_1^{0}, \ldots, p_{K-1}^{0}    ))
\]</span>
with <span class="math inline">\(\boldsymbol p_0 = c(p_1^{0}, \ldots, p_{K}^{0} )^T\)</span>.
As the probabilities sum up to 1 there are only <span class="math inline">\(K-1\)</span> free parameters.</p>
<p>The log-likelihood at the MLE is
<span class="math display">\[
l_n(\hat{\pi}_1^{ML}, \ldots, \hat{\pi}_{K-1}^{ML}  ) =  n   \sum_{k=1}^{K}  \bar{x}_k \log \hat{\pi}_k^{ML}  =  n   \sum_{k=1}^{K}  \bar{x}_k \log \bar{x}_k 
\]</span>
with <span class="math inline">\(\hat{\pi}_k^{ML} = \frac{n_k}{n} = \bar{x}_k\)</span>.
Note that here and in the following the sums run from <span class="math inline">\(1\)</span> to <span class="math inline">\(K\)</span> where the <span class="math inline">\(K\)</span>-th component is always computed from the components <span class="math inline">\(1\)</span> to <span class="math inline">\(K-1\)</span>, as in the previous section.
The log-likelihood at <span class="math inline">\(\boldsymbol p_0\)</span> is
<span class="math display">\[l_n( p_1^{0}, \ldots, p_{K-1}^{0}    ) =  n   \sum_{k=1}^{K}  \bar{x}_k \log p_k^{0} 
\]</span>
so that the Wilks statistic becomes
<span class="math display">\[
W(\boldsymbol p_0) = 2 n   \sum_{k=1}^{K}  \bar{x}_k \log\left(\frac{\bar{x}_k}{ p_k^{0}} \right) 
\]</span>
It is asymptotically chi-squared distributed with <span class="math inline">\(K-1\)</span> degrees of freedom.</p>
<p>Note that for this model the Wilks statistic is equal to the KL Divergence
<span class="math display">\[
W(\boldsymbol p_0) = 2 n D_{\text{KL}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) )
\]</span></p>
<p>The Wilks log-likelihood ratio statistic for the categorical distribution is also known as the <a href="https://en.wikipedia.org/wiki/G-test"><span class="math inline">\(G\)</span> test statistic</a> where <span class="math inline">\(\hat{\boldsymbol \pi}_{ML}\)</span> corresponds to the observed frequencies (as observed in data) and <span class="math inline">\(\boldsymbol p_0\)</span> are the expected frequencies (i.e. hypothesised to be the true frequencies).</p>
<p>Using observed counts <span class="math inline">\(n_k\)</span> and expected counts <span class="math inline">\(n_k^{\text{expect}} = n p_k^{0}\)</span>
we can write the Wilks statistic respectively the <span class="math inline">\(G\)</span>-statistic
as follows:
<span class="math display">\[
W(\boldsymbol p_0) = 2   \sum_{k=1}^{K}  n_k \log\left(\frac{  n_k }{  n_k^{\text{expect}}   } \right) 
\]</span></p>
</div>
<div class="example">
<p><span id="exm:catwilksquadapprox" class="example"><strong>Example 1.8  </strong></span>Quadratic approximation of the Wilks log-likelihood ratio statistic for the categorical distribution:</p>
<p>Developing the Wilks statistic <span class="math inline">\(W(\boldsymbol p_0)\)</span> around the MLE <span class="math inline">\(\hat{\boldsymbol \pi}_{ML}\)</span> yields the squared Wald statistic which for the categorical distribution is the Neyman chi-squared statistic:
<span class="math display">\[
\begin{split}
W(\boldsymbol p_0)&amp; = 2 n D_{\text{KL}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) ) \\
&amp; \approx n D_{\text{Neyman}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) ) \\
&amp; = \sum_{k=1}^K \frac{(n_k-n_k^{\text{expect}} )^2}{n_k} \\
&amp; =  \chi^2_{\text{Neyman}}\\
\end{split}
\]</span></p>
<p>If instead we approximate the KL divergence assuming <span class="math inline">\(\boldsymbol p_0\)</span> as fixed we arrive at
<span class="math display">\[ 
\begin{split}
2 n D_{\text{KL}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}), \text{Cat}(\boldsymbol p_0 ) ) &amp;\approx n D_{\text{Pearson}}( \text{Cat}(\hat{\boldsymbol \pi}_{ML}),  \text{Cat}(\boldsymbol p_0 ) )\\
&amp; = \sum_{k=1}^K \frac{(n_k-n_k^{\text{expect}})^2}{n_k^{\text{expect}}} \\
&amp; = \chi^2_{\text{Pearson}}
\end{split}
\]</span>
which is the well-known Pearson chi-squared statistic (note the <em>expected</em> counts in its denominator).</p>
</div>
</div>
</div>
<div id="further-multivariate-distributions" class="section level2">
<h2><span class="header-section-number">1.6</span> Further multivariate distributions</h2>
<p>For most univariate distributions there are multivariate versions.</p>
<p>In the following we describe the multivariate versions of the Beta distribution,
the Gamma distribution (also known as scaled <span class="math inline">\(\chi^2\)</span> distribution)
and of the inverse Gamma distribution.</p>
<div id="dirichlet-distribution" class="section level3">
<h3><span class="header-section-number">1.6.1</span> Dirichlet distribution</h3>
<div id="univariate-case-1" class="section level4">
<h4><span class="header-section-number">1.6.1.1</span> Univariate case</h4>
<p>Beta distribution</p>
<p><span class="math display">\[x \sim \text{Beta}(\alpha,\beta)\]</span>
<span class="math display">\[x \in [0,1]\]</span>
<span class="math display">\[\alpha &gt; 0; \beta &gt; 0\]</span>
<span class="math display">\[m = \alpha + \beta \]</span>
<span class="math display">\[\mu = \frac{\alpha}{m} \in \left[0,1\right]\]</span>
<span class="math display">\[\text{E}(x) = \mu\]</span>
<span class="math display">\[\text{Var}(x)=\frac{\mu(1-\mu)}{m+1}\]</span>
<span class="math inline">\(\text{compare with unit standardised binomial!}\)</span></p>
<p><span class="math inline">\(\textbf{Different shapes}\)</span></p>
<p><img src="fig/fig1-betashapes.png" width="60%" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[\text{Useful as distribution for a proportion } \pi\]</span></p>
<p><span class="math display">\[ \text{ Bayesian Model:}\]</span><br />
<span class="math display">\[\text{Beta prior:} \; \pi \sim  \text{Beta}(\alpha,\beta)\]</span>
<span class="math display">\[\text{Binomial likelihood:} \; x|\pi \sim \text{Bin}(n, \pi)\]</span></p>
</div>
<div id="multivariate-case-1" class="section level4">
<h4><span class="header-section-number">1.6.1.2</span> Multivariate case</h4>
<p>Dirichlet distribution</p>
<p><span class="math display">\[\boldsymbol x\sim \text{Dir}(\boldsymbol \alpha)\]</span>
<span class="math display">\[x_i \in [0,1]; \, \sum^{d}_{i=1} x_i = 1\]</span>
<span class="math display">\[\boldsymbol \alpha= (\alpha_1,...,\alpha_d)^T &gt;0\]</span>
<span class="math display">\[m = \sum^{d}_{i=1}\alpha_i\]</span>
<span class="math display">\[\mu_i = \frac{\alpha_i}{m} \in \left[0,1\right]\]</span>
<span class="math display">\[\text{E}(x_i) = \mu_i\]</span>
<span class="math display">\[\text{Var}(x_i)=\frac{\mu_i(1-\mu_i)}{m+1}\]</span>
<span class="math display">\[\text{Cov}(x_i,x_j)=-\frac{\mu_i \mu_j}{m+1}\]</span>
<span class="math inline">\(\text{compare with unit standardised multinomial!}\)</span></p>
<p>Stick breaking&quot; model</p>
<p><img src="fig/fig1-stickbreaking.png" width="60%" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[\text{Useful as distribution for a proportion } \boldsymbol \pi\]</span></p>
<p><span class="math display">\[\text{ Bayesian Model:}\]</span><br />
<span class="math display">\[\text{Dirichlet prior:} \,  \boldsymbol \pi\sim \text{Dir}(\boldsymbol \alpha)\]</span>
<span class="math display">\[\text{Multinomial likelihood:} \, \boldsymbol x|\boldsymbol \pi\sim \text{Mult}(n, \boldsymbol \pi)\]</span></p>
</div>
</div>
<div id="wishart-distribution" class="section level3">
<h3><span class="header-section-number">1.6.2</span> Wishart distribution</h3>
<p>This multivariate distribution generalises the univariate
scaled <span class="math inline">\(\chi^2\)</span> distribution (also known as Gamma distribution).</p>
<div id="univariate-case-2" class="section level4">
<h4><span class="header-section-number">1.6.2.1</span> Univariate case</h4>
<p>Scaled <span class="math inline">\(\chi^2\)</span> distribution (=Gamma distribution, see below)</p>
<p><span class="math display">\[z_1,z_2,\ldots,z_m \stackrel{\text{iid}}\sim N(0,\sigma^2)\]</span>
<span class="math display">\[x = \sum^{m}_{i=1}z_i^2\]</span></p>
<p><span class="math display">\[x \sim \sigma^2 \chi^2_m = \text{W}_1(\sigma^2, m)\]</span>
<span class="math display">\[\text{E}(x) = m \, \sigma^2\]</span>
<span class="math display">\[\text{Var}(x)= m \, 2 \sigma^4\]</span></p>
<p>Useful as the distribution of sample variance:
<span class="math display">\[y_1, \ldots, y_n \sim N(\mu, \sigma^2)\]</span>
Known mean <span class="math inline">\(\mu\)</span>:
<span class="math display">\[\frac{1}{n}\sum_{i=1}^n(y_i -\mu)^2 \sim \text{W}_1\left(\frac{\sigma^2}{n}, n\right)\]</span>
Unknown mean <span class="math inline">\(\mu\)</span> (estimated by <span class="math inline">\(\bar{y}\)</span>):
<span class="math display">\[\widehat{\sigma^2}_{ML} = \frac{1}{n}\sum_{i=1}^n(y_i -\bar{y})^2 \sim \text{W}_1\left(\frac{\sigma^2}{n}, n-1\right)\]</span>
<span class="math display">\[\widehat{\sigma^2}_{UB} = \frac{1}{n-1}\sum_{i=1}^n(y_i -\bar{y})^2 \sim \text{W}_1\left(\frac{\sigma^2}{n-1}, n-1\right)\]</span></p>
</div>
<div id="multivariate-case-2" class="section level4">
<h4><span class="header-section-number">1.6.2.2</span> Multivariate case</h4>
<p>Wishart distribution</p>
<p><span class="math display">\[\boldsymbol z_1,\boldsymbol z_2,\ldots,\boldsymbol z_m \stackrel{\text{iid}}\sim N_d(0,\boldsymbol \Sigma)\]</span>
<span class="math display">\[\underbrace{\boldsymbol X}_{d\times d}=\sum^{m}_{i=1}\underbrace{\boldsymbol z_i\boldsymbol z_i^T}_{d\times d}\]</span><br />
Note that <span class="math inline">\(\boldsymbol X\)</span> is a !</p>
<p><span class="math display">\[\boldsymbol X\sim \text{W}_d\left(\boldsymbol \Sigma, m\right)\]</span>
<span class="math display">\[\text{E}(\boldsymbol X) = m \boldsymbol \Sigma\]</span>
<span class="math display">\[\text{Var}(x_{ij})=m \, \left(\sigma^2_{ij}+\sigma_{ii}\sigma_{jj}\right)\]</span></p>
<p>Useful as distribution of sample covariance:
<span class="math display">\[\boldsymbol y_1, \ldots, \boldsymbol y_n \sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\]</span>
<span class="math display">\[\frac{1}{n}\sum_{i=1}^n (\boldsymbol y_i -\boldsymbol \mu)(\boldsymbol y_i -\boldsymbol \mu)^T \sim \text{W}_d\left(\boldsymbol \Sigma/n, n\right)\]</span>
<span class="math display">\[\widehat{\boldsymbol \Sigma}_{ML} = \frac{1}{n}\sum_{i=1}^n (\boldsymbol y_i -\bar{\boldsymbol y})(\boldsymbol y_i -\bar{\boldsymbol y})^T \sim \text{W}_d\left(\boldsymbol \Sigma/n, n-1\right)\]</span>
<span class="math display">\[\widehat{\boldsymbol \Sigma}_{UB} = \frac{1}{n-1}\sum_{i=1}^n (\boldsymbol y_i -\bar{\boldsymbol y})(\boldsymbol y_i -\bar{\boldsymbol y})^T \sim \text{W}_d\left(\boldsymbol \Sigma/(n-1), n-1\right)\]</span></p>
</div>
<div id="relationship-to-gamma-distribution" class="section level4">
<h4><span class="header-section-number">1.6.2.3</span> Relationship to Gamma distribution</h4>
<p>The scaled <span class="math inline">\(\chi^2\)</span> distribution (=one-dimensional Wishart distribution) with parameters <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(m\)</span> is a reparameterised <strong>Gamma distribution</strong> with shape parameter <span class="math inline">\(\alpha\)</span> and scale parameter <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[\text{Gam}\left(  \underbrace{\frac{m}{2}}_{\text{shape} } \, , \underbrace{ 2 \sigma^{2}}_{\text{scale}} \right)=  \sigma^2\chi^2_m = \text{W}_1(\sigma^2, m)\]</span></p>
<p>or, equivalently (<span class="math inline">\(m = 2 \alpha\)</span>, <span class="math inline">\(\sigma^2 = \beta/2\)</span>)
<span class="math display">\[\text{Gam}\left(  \underbrace{\alpha}_{\text{shape} } \, , \underbrace{\beta}_{\text{scale}} \right) = \frac{\beta}{2} \chi^2_{2 \alpha} =  \text{W}_1(\frac{\beta}{2}, 2 \alpha)\]</span></p>
<p>The mean of the Gamma distribution is <span class="math inline">\(\text{E}(x) = \alpha \beta\)</span> and the variance is <span class="math inline">\(\text{Var}(x) = \alpha \beta^2\)</span>.</p>
<p>The <strong>exponential distribution</strong> with scale parameter <span class="math inline">\(\beta\)</span> is a special
case of the Gamma distribution with <span class="math inline">\(\alpha=1\)</span>:
<span class="math display">\[
\text{Exp}(\beta) = \text{Gam}(1, \beta) = \frac{\beta}{2} \chi^2_{2} = \text{W}_1\left(\frac{\beta}{2}, 2 \right)
\]</span>
The corresponding mean is <span class="math inline">\(\beta\)</span> and the variance is <span class="math inline">\(\beta^2\)</span>.</p>
<p>The density expressed in terms of scale parameter <span class="math inline">\(\beta\)</span> is
<span class="math display">\[
f(x| \beta) = \frac{1}{\beta} e^{-x/\beta}
\]</span></p>
<p>Instead of the scale parameter <span class="math inline">\(\beta\)</span> the exponential distribution is often
also parameterised in terms of a rate parameter <span class="math inline">\(\lambda = \frac{1}{\beta}\)</span>.</p>
</div>
</div>
<div id="inverse-wishart-distribution" class="section level3">
<h3><span class="header-section-number">1.6.3</span> Inverse Wishart distribution</h3>
<div id="univariate-case-3" class="section level4">
<h4><span class="header-section-number">1.6.3.1</span> Univariate case</h4>
<p>Inverse <span class="math inline">\(\chi^2\)</span> Distribution:</p>
<p><span class="math display">\[x \sim \text{W}^{-1}_1(\psi, k+2) = \psi\,\text{Inv-$\chi^2_{k+2}$}\]</span>
<span class="math display">\[\text{E}(x) = \frac{\psi}{k}\]</span>
<span class="math display">\[\text{Var}(x)= \frac{2\psi^2}{k^2 (k-2)}\]</span></p>
<p>Relationship to scaled <span class="math inline">\(\chi^2\)</span> :
<span class="math display">\[
\frac{1}{x} \sim W_1(\psi^{-1}, k+2) =  \psi^{-1} \, \chi^2_{k+2}
\]</span></p>
</div>
<div id="multivariate-case-3" class="section level4">
<h4><span class="header-section-number">1.6.3.2</span> Multivariate case</h4>
<p>Inverse Wishart distribution:</p>
<p><span class="math display">\[\underbrace{\boldsymbol X}_{d\times d} \sim \text{W}^{-1}_d\left( \underbrace{\boldsymbol \Psi}_{d\times d} \, , \, k+d+1\right)\]</span>
<span class="math display">\[\text{E}(\boldsymbol X) =\boldsymbol \Psi/ k\]</span>
<span class="math display">\[\text{Var}(x_{ij})= \frac{2 }{k^2 (k-2)} \frac{(k+2) \psi_{ij} + k \, \psi_{ii} \psi_{jj} }{2 k + 2}\]</span></p>
<p>Relationship to Wishart:
<span class="math display">\[\boldsymbol X^{-1} \sim \text{W}_d\left( \boldsymbol \Psi^{-1} \, , k+d+1\right)\]</span></p>
</div>
<div id="relationship-to-inverse-gamma-distribution" class="section level4">
<h4><span class="header-section-number">1.6.3.3</span> Relationship to inverse Gamma distribution</h4>
<p>Another way to express the univariate inverse Wishart distribution is via the <strong>inverse Gamma distribution</strong>:
<span class="math display">\[\text{Inv-Gam}(\underbrace{1+\frac{k}{2}}_{\text{shape } \alpha}, \underbrace{\frac{\psi}{2}}_{\text{scale }\beta}) = \psi\,\text{Inv-$\chi^2_{k+2}$} =  \text{W}^{-1}_1(\psi, k+2) \]</span>
or equivalently (<span class="math inline">\(k=2(\alpha-1)\)</span> and <span class="math inline">\(\psi=2\beta\)</span>)
<span class="math display">\[\text{Inv-Gam}( \alpha, \beta) = 2\beta\,\text{Inv-$\chi^2_{2\alpha}$} = \text{W}^{-1}_1(2 \beta, 2 \alpha) \]</span>
The mean of the inverse Gamma distribution is
<span class="math inline">\(\text{E}(x) = \frac{\beta}{\alpha-1} = \mu\)</span> the variance
<span class="math inline">\(\text{Var}(x)= \frac{\beta^2}{(\alpha-1)^2(\alpha-2)} = \frac{2 \mu^2}{k-2}\)</span>.</p>
<p>The inverse of <span class="math inline">\(x\)</span> is Gamma distributed:
<span class="math display">\[
\frac{1}{x} \sim \text{Gam}(1+\frac{k}{2}, 2\psi^{-1})=\text{Gam}(\alpha, \beta^{-1})
\]</span></p>
<hr />
<p>The inverse Wishart distribution is useful as conjugate distribution for Bayesian modelling
of the variance, with <span class="math inline">\(k\)</span> the sample size parameter
and <span class="math inline">\(\Psi = k \Sigma\)</span> (or <span class="math inline">\(\psi = k \sigma^2\)</span>).</p>
<hr />
</div>
</div>
<div id="further-distributions" class="section level3">
<h3><span class="header-section-number">1.6.4</span> Further distributions</h3>
<p><a href="https://en.wikipedia.org/wiki/List_of_probability_distributions" class="uri">https://en.wikipedia.org/wiki/List_of_probability_distributions</a></p>
<p>Wikipedia is a quite good source for information on distributions!</p>

<p></p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2-transformations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math38161-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
