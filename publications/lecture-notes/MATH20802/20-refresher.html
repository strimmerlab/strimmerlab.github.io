<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>A Refresher | HTML</title>
  <meta name="description" content="Statistical Methods:<br />
Likelihood, Bayes and Regression</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="A Refresher | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Refresher | HTML" />
  
  
  



<meta name="date" content="2021-02-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="19-regression5.html"/>
<link rel="next" href="21-further-study.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH20802/index.html">MATH20802 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Likelihood estimation and inference</b></span></li>
<li class="chapter" data-level="1" data-path="01-likelihood1.html"><a href="01-likelihood1.html"><i class="fa fa-check"></i><b>1</b> Overview of statistical learning</a><ul>
<li class="chapter" data-level="1.1" data-path="01-likelihood1.html"><a href="01-likelihood1.html#how-to-learn-from-data"><i class="fa fa-check"></i><b>1.1</b> How to learn from data?</a></li>
<li class="chapter" data-level="1.2" data-path="01-likelihood1.html"><a href="01-likelihood1.html#probability-theory-versus-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Probability theory versus statistical learning</a></li>
<li class="chapter" data-level="1.3" data-path="01-likelihood1.html"><a href="01-likelihood1.html#cartoon-of-statistical-learning"><i class="fa fa-check"></i><b>1.3</b> Cartoon of statistical learning</a></li>
<li class="chapter" data-level="1.4" data-path="01-likelihood1.html"><a href="01-likelihood1.html#likelihood"><i class="fa fa-check"></i><b>1.4</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="02-likelihood2.html"><a href="02-likelihood2.html"><i class="fa fa-check"></i><b>2</b> From information theory to likelihood</a><ul>
<li class="chapter" data-level="2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy"><i class="fa fa-check"></i><b>2.1</b> Entropy</a><ul>
<li class="chapter" data-level="2.1.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#surprise"><i class="fa fa-check"></i><b>2.1.2</b> Surprise</a></li>
<li class="chapter" data-level="2.1.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#shannon-entropy"><i class="fa fa-check"></i><b>2.1.3</b> Shannon entropy</a></li>
<li class="chapter" data-level="2.1.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#differential-entropy"><i class="fa fa-check"></i><b>2.1.4</b> Differential entropy</a></li>
<li class="chapter" data-level="2.1.5" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-entropy-principle-to-characterise-distributions"><i class="fa fa-check"></i><b>2.1.5</b> Maximum entropy principle to characterise distributions</a></li>
<li class="chapter" data-level="2.1.6" data-path="02-likelihood2.html"><a href="02-likelihood2.html#cross-entropy"><i class="fa fa-check"></i><b>2.1.6</b> Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>2.2</b> Kullback-Leibler divergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#definition"><i class="fa fa-check"></i><b>2.2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#properties-of-kl-divergence"><i class="fa fa-check"></i><b>2.2.2</b> Properties of KL divergence</a></li>
<li class="chapter" data-level="2.2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#examples"><i class="fa fa-check"></i><b>2.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#local-quadratic-approximation-and-expected-fisher-information"><i class="fa fa-check"></i><b>2.3</b> Local quadratic approximation and expected Fisher information</a></li>
<li class="chapter" data-level="2.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy-learning-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4</b> Entropy learning and maximum likelihood</a><ul>
<li class="chapter" data-level="2.4.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#the-relative-entropy-between-true-model-and-approximating-model"><i class="fa fa-check"></i><b>2.4.1</b> The relative entropy between true model and approximating model</a></li>
<li class="chapter" data-level="2.4.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#minimum-kl-divergence-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4.2</b> Minimum KL divergence and maximum likelihood</a></li>
<li class="chapter" data-level="2.4.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#further-connections"><i class="fa fa-check"></i><b>2.4.3</b> Further connections</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="03-likelihood3.html"><a href="03-likelihood3.html"><i class="fa fa-check"></i><b>3</b> Maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#principle-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.1</b> Principle of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#outline"><i class="fa fa-check"></i><b>3.1.1</b> Outline</a></li>
<li class="chapter" data-level="3.1.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#recipe-for-obtaining-mles"><i class="fa fa-check"></i><b>3.1.2</b> Recipe for obtaining MLEs</a></li>
<li class="chapter" data-level="3.1.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#invariance-property-of-the-mle"><i class="fa fa-check"></i><b>3.1.3</b> Invariance property of the MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#examples-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.2</b> Examples of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.2.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-estimation-of-a-proportion"><i class="fa fa-check"></i><b>3.2.1</b> Example 1: Estimation of a proportion</a></li>
<li class="chapter" data-level="3.2.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-exponential-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Example 2: Exponential Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-3-normal-distribution-with-unknown-mean-and-known-variance"><i class="fa fa-check"></i><b>3.2.3</b> Example 3: Normal distribution with unknown mean and known variance</a></li>
<li class="chapter" data-level="3.2.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-4-normal-distribution-with-both-mean-and-variance-unknown"><i class="fa fa-check"></i><b>3.2.4</b> Example 4: Normal Distribution with both mean and variance unknown</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information"><i class="fa fa-check"></i><b>3.3</b> Observed Fisher information</a><ul>
<li class="chapter" data-level="3.3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#motivation"><i class="fa fa-check"></i><b>3.3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#curvature-of-log-likelihood-function"><i class="fa fa-check"></i><b>3.3.2</b> Curvature of log-likelihood function</a></li>
<li class="chapter" data-level="3.3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information-1"><i class="fa fa-check"></i><b>3.3.3</b> Observed Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information---examples"><i class="fa fa-check"></i><b>3.4</b> Observed Fisher information - Examples</a><ul>
<li class="chapter" data-level="3.4.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-bernoulli-binomial-model"><i class="fa fa-check"></i><b>3.4.1</b> Example 1: Bernoulli / Binomial model</a></li>
<li class="chapter" data-level="3.4.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-normal-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Example 2: Normal distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#differences-of-observed-to-expected-fisher-information"><i class="fa fa-check"></i><b>3.4.3</b> Differences of observed to expected Fisher information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="04-likelihood4.html"><a href="04-likelihood4.html"><i class="fa fa-check"></i><b>4</b> Quadratic approximation and normal asymptotics</a><ul>
<li class="chapter" data-level="4.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#covariance-correlation-and-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.1</b> Covariance, correlation and multivariate normal distribution</a></li>
<li class="chapter" data-level="4.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#maximum-likelihood-estimates-of-the-parameters-of-the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.2</b> Maximum likelihood estimates of the parameters of the multivariate normal distribution</a></li>
<li class="chapter" data-level="4.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-approximation-of-log-likelihood-function-around-mle"><i class="fa fa-check"></i><b>4.3</b> Quadratic approximation of log-likelihood function around MLE</a></li>
<li class="chapter" data-level="4.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-normality-of-mle"><i class="fa fa-check"></i><b>4.4</b> Asymptotic normality of MLE</a></li>
<li class="chapter" data-level="4.5" data-path="04-likelihood4.html"><a href="04-likelihood4.html#observed-or-expected-fisher-information-to-estimate-variance-of-the-mle"><i class="fa fa-check"></i><b>4.5</b> Observed or expected Fisher information to estimate variance of the MLE?</a></li>
<li class="chapter" data-level="4.6" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-confidence-intervals-for-mles"><i class="fa fa-check"></i><b>4.6</b> Normal confidence intervals for MLEs</a></li>
<li class="chapter" data-level="4.7" data-path="04-likelihood4.html"><a href="04-likelihood4.html#wald-statistic"><i class="fa fa-check"></i><b>4.7</b> Wald statistic</a></li>
<li class="chapter" data-level="4.8" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-ci-expressed-using-the-squared-wald-statistics"><i class="fa fa-check"></i><b>4.8</b> Normal CI expressed using the squared Wald statistics</a></li>
<li class="chapter" data-level="4.9" data-path="04-likelihood4.html"><a href="04-likelihood4.html#testing-and-confidence-intervals"><i class="fa fa-check"></i><b>4.9</b> Testing and confidence intervals</a></li>
<li class="chapter" data-level="4.10" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-normal-distribution"><i class="fa fa-check"></i><b>4.10</b> Example: normal distribution</a></li>
<li class="chapter" data-level="4.11" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-of-non-regular-model"><i class="fa fa-check"></i><b>4.11</b> Example of non-regular model</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="05-likelihood5.html"><a href="05-likelihood5.html"><i class="fa fa-check"></i><b>5</b> Likelihood-based confidence interval and likelihood ratio</a><ul>
<li class="chapter" data-level="5.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-based-confidence-intervals"><i class="fa fa-check"></i><b>5.1</b> Likelihood-based confidence intervals</a></li>
<li class="chapter" data-level="5.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#wilks-log-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.2</b> Wilks log likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.3" data-path="05-likelihood5.html"><a href="05-likelihood5.html#quadratic-approximation-of-wilks-statistic"><i class="fa fa-check"></i><b>5.3</b> Quadratic approximation of Wilks statistic</a></li>
<li class="chapter" data-level="5.4" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistics"><i class="fa fa-check"></i><b>5.4</b> Distribution of Wilks statistics</a><ul>
<li class="chapter" data-level="5.4.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#cutoff-values-delta"><i class="fa fa-check"></i><b>5.4.1</b> Cutoff values <span class="math inline">\(\Delta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="05-likelihood5.html"><a href="05-likelihood5.html#example-likelihood-ci-for-exponential-model"><i class="fa fa-check"></i><b>5.5</b> Example: likelihood CI for exponential model</a></li>
<li class="chapter" data-level="5.6" data-path="05-likelihood5.html"><a href="05-likelihood5.html#origin-of-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.6</b> Origin of likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.7" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistic-and-likelihood-ci"><i class="fa fa-check"></i><b>5.7</b> Distribution of Wilks statistic and Likelihood CI</a></li>
<li class="chapter" data-level="5.8" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>5.8</b> Likelihood ratio test (LRT)</a></li>
<li class="chapter" data-level="5.9" data-path="05-likelihood5.html"><a href="05-likelihood5.html#optimality-of-lrts"><i class="fa fa-check"></i><b>5.9</b> Optimality of LRTs</a></li>
<li class="chapter" data-level="5.10" data-path="05-likelihood5.html"><a href="05-likelihood5.html#generalised-likelihood-ratio-test-glrt"><i class="fa fa-check"></i><b>5.10</b> Generalised likelihood ratio test (GLRT)</a></li>
<li class="chapter" data-level="5.11" data-path="05-likelihood5.html"><a href="05-likelihood5.html#glrt-example"><i class="fa fa-check"></i><b>5.11</b> GLRT example</a></li>
<li class="chapter" data-level="5.12" data-path="05-likelihood5.html"><a href="05-likelihood5.html#thoughts-on-model-selection"><i class="fa fa-check"></i><b>5.12</b> Thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="06-likelihood6.html"><a href="06-likelihood6.html"><i class="fa fa-check"></i><b>6</b> Optimality properties, minimal sufficiency and summary</a><ul>
<li class="chapter" data-level="6.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#properties-of-mles-encountered-so-far"><i class="fa fa-check"></i><b>6.1</b> Properties of MLEs encountered so far</a></li>
<li class="chapter" data-level="6.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#further-optimality-properties-of-mles"><i class="fa fa-check"></i><b>6.2</b> Further optimality properties of MLEs</a></li>
<li class="chapter" data-level="6.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency"><i class="fa fa-check"></i><b>6.3</b> Summarising data and the concept of minimal sufficiency</a></li>
<li class="chapter" data-level="6.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summary-and-concluding-remarks-on-maximum-likelihood"><i class="fa fa-check"></i><b>6.4</b> Summary and concluding remarks on maximum likelihood</a><ul>
<li class="chapter" data-level="6.4.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#starting-point-kl-divergence"><i class="fa fa-check"></i><b>6.4.1</b> Starting point: KL divergence</a></li>
<li class="chapter" data-level="6.4.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#connections-between-kl-divergence-likelihood-and-expected-and-observed-fisher-information"><i class="fa fa-check"></i><b>6.4.2</b> Connections between KL divergence, likelihood and expected and observed Fisher information</a></li>
<li class="chapter" data-level="6.4.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#likelihood-estimation"><i class="fa fa-check"></i><b>6.4.3</b> Likelihood estimation</a></li>
<li class="chapter" data-level="6.4.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#what-happens-if-n-is-small"><i class="fa fa-check"></i><b>6.4.4</b> What happens if <span class="math inline">\(n\)</span> is small?</a></li>
<li class="chapter" data-level="6.4.5" data-path="06-likelihood6.html"><a href="06-likelihood6.html#inference-with-likelihood"><i class="fa fa-check"></i><b>6.4.5</b> Inference with likelihood:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Bayesian Statistics</b></span></li>
<li class="chapter" data-level="7" data-path="07-bayes1.html"><a href="07-bayes1.html"><i class="fa fa-check"></i><b>7</b> Essentials of Bayesian statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#bayes-theorem"><i class="fa fa-check"></i><b>7.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#principle-of-bayesian-learning"><i class="fa fa-check"></i><b>7.2</b> Principle of Bayesian learning</a></li>
<li class="chapter" data-level="7.3" data-path="07-bayes1.html"><a href="07-bayes1.html#what-is-exactly-is-the-bayesian-estimate"><i class="fa fa-check"></i><b>7.3</b> What is exactly is the “Bayesian estimate”?</a></li>
<li class="chapter" data-level="7.4" data-path="07-bayes1.html"><a href="07-bayes1.html#computer-implementation-of-bayesian-learning"><i class="fa fa-check"></i><b>7.4</b> Computer implementation of Bayesian learning</a></li>
<li class="chapter" data-level="7.5" data-path="07-bayes1.html"><a href="07-bayes1.html#bayesian-interpretation-of-probability"><i class="fa fa-check"></i><b>7.5</b> Bayesian interpretation of probability</a><ul>
<li class="chapter" data-level="7.5.1" data-path="07-bayes1.html"><a href="07-bayes1.html#what-makes-you-bayesian"><i class="fa fa-check"></i><b>7.5.1</b> What makes you “Bayesian”?</a></li>
<li class="chapter" data-level="7.5.2" data-path="07-bayes1.html"><a href="07-bayes1.html#mathematics-of-probability"><i class="fa fa-check"></i><b>7.5.2</b> Mathematics of probability</a></li>
<li class="chapter" data-level="7.5.3" data-path="07-bayes1.html"><a href="07-bayes1.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.5.3</b> Interpretations of probability</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="07-bayes1.html"><a href="07-bayes1.html#historical-developments"><i class="fa fa-check"></i><b>7.6</b> Historical developments</a></li>
<li class="chapter" data-level="7.7" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning"><i class="fa fa-check"></i><b>7.7</b> Connection with entropy learning</a><ul>
<li class="chapter" data-level="7.7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#zero-forcing-property"><i class="fa fa-check"></i><b>7.7.1</b> Zero forcing property</a></li>
<li class="chapter" data-level="7.7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning-1"><i class="fa fa-check"></i><b>7.7.2</b> Connection with entropy learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="08-bayes2.html"><a href="08-bayes2.html"><i class="fa fa-check"></i><b>8</b> Beta-Binomial model for estimating a proportion</a><ul>
<li class="chapter" data-level="8.1" data-path="08-bayes2.html"><a href="08-bayes2.html#binomial-likelihood"><i class="fa fa-check"></i><b>8.1</b> Binomial likelihood</a></li>
<li class="chapter" data-level="8.2" data-path="08-bayes2.html"><a href="08-bayes2.html#excursion-properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>8.2</b> Excursion: Properties of the Beta distribution</a></li>
<li class="chapter" data-level="8.3" data-path="08-bayes2.html"><a href="08-bayes2.html#beta-prior-distribution"><i class="fa fa-check"></i><b>8.3</b> Beta prior distribution</a></li>
<li class="chapter" data-level="8.4" data-path="08-bayes2.html"><a href="08-bayes2.html#computing-the-posterior-distribution"><i class="fa fa-check"></i><b>8.4</b> Computing the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="09-bayes3.html"><a href="09-bayes3.html"><i class="fa fa-check"></i><b>9</b> Properties of Bayesian learning</a><ul>
<li class="chapter" data-level="9.1" data-path="09-bayes3.html"><a href="09-bayes3.html#prior-acting-as-pseudo-data"><i class="fa fa-check"></i><b>9.1</b> Prior acting as pseudo-data</a></li>
<li class="chapter" data-level="9.2" data-path="09-bayes3.html"><a href="09-bayes3.html#linear-shrinkage-of-mean"><i class="fa fa-check"></i><b>9.2</b> Linear shrinkage of mean</a></li>
<li class="chapter" data-level="9.3" data-path="09-bayes3.html"><a href="09-bayes3.html#conjugacy-of-prior-and-posterior-distribution"><i class="fa fa-check"></i><b>9.3</b> Conjugacy of prior and posterior distribution</a></li>
<li class="chapter" data-level="9.4" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-asymptotics"><i class="fa fa-check"></i><b>9.4</b> Large sample asymptotics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-limits-of-mean-and-variance"><i class="fa fa-check"></i><b>9.4.1</b> Large sample limits of mean and variance</a></li>
<li class="chapter" data-level="9.4.2" data-path="09-bayes3.html"><a href="09-bayes3.html#asymptotic-normality-of-the-posterior-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Asymptotic Normality of the Posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="09-bayes3.html"><a href="09-bayes3.html#posterior-variance-for-finite-n"><i class="fa fa-check"></i><b>9.5</b> Posterior variance for finite <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-bayes4.html"><a href="10-bayes4.html"><i class="fa fa-check"></i><b>10</b> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a><ul>
<li class="chapter" data-level="10.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-normal-model-to-estimate-mean"><i class="fa fa-check"></i><b>10.1</b> Normal-Normal model to estimate mean</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihood"><i class="fa fa-check"></i><b>10.1.1</b> Normal likelihood</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-prior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Normal prior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-posterior-distribution"><i class="fa fa-check"></i><b>10.1.3</b> Normal posterior distribution</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-and-stein-paradox"><i class="fa fa-check"></i><b>10.1.4</b> Large sample asymptotics and Stein paradox</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-normal-model-to-estimate-variance"><i class="fa fa-check"></i><b>10.2</b> Inverse-Gamma-Normal model to estimate variance</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Inverse Gamma distribution</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihoood"><i class="fa fa-check"></i><b>10.2.2</b> Normal likelihoood</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-prior-distribution"><i class="fa fa-check"></i><b>10.2.3</b> Inverse Gamma prior distribution</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-posterior-distribution"><i class="fa fa-check"></i><b>10.2.4</b> Inverse Gamma posterior distribution</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-1"><i class="fa fa-check"></i><b>10.2.5</b> Large sample asymptotics</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-bayes4.html"><a href="10-bayes4.html#estimating-precision"><i class="fa fa-check"></i><b>10.2.6</b> Estimating precision</a></li>
<li class="chapter" data-level="10.2.7" data-path="10-bayes4.html"><a href="10-bayes4.html#joint-estimation-of-mean-and-variance"><i class="fa fa-check"></i><b>10.2.7</b> Joint estimation of mean and variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-bayes5.html"><a href="11-bayes5.html"><i class="fa fa-check"></i><b>11</b> Shrinkage estimation using empirical risk minimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="11-bayes5.html"><a href="11-bayes5.html#linear-shrinkage"><i class="fa fa-check"></i><b>11.1</b> Linear shrinkage</a></li>
<li class="chapter" data-level="11.2" data-path="11-bayes5.html"><a href="11-bayes5.html#james-stein-estimator"><i class="fa fa-check"></i><b>11.2</b> James-Stein estimator</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-bayes6.html"><a href="12-bayes6.html"><i class="fa fa-check"></i><b>12</b> Bayesian model comparison using Bayes factors and the BIC</a><ul>
<li class="chapter" data-level="12.1" data-path="12-bayes6.html"><a href="12-bayes6.html#the-bayes-factor"><i class="fa fa-check"></i><b>12.1</b> The Bayes factor</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-bayes6.html"><a href="12-bayes6.html#connection-with-relative-entropy"><i class="fa fa-check"></i><b>12.1.1</b> Connection with relative entropy</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-bayes6.html"><a href="12-bayes6.html#interpretation-of-and-scale-for-bayes-factor"><i class="fa fa-check"></i><b>12.1.2</b> Interpretation of and scale for Bayes factor</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-bayes6.html"><a href="12-bayes6.html#computing-textprd-m-for-simple-and-composite-models"><i class="fa fa-check"></i><b>12.1.3</b> Computing <span class="math inline">\(\text{Pr}(D | M)\)</span> for simple and composite models</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-bayes6.html"><a href="12-bayes6.html#bayes-factor-versus-likelihood-ratio"><i class="fa fa-check"></i><b>12.1.4</b> Bayes factor versus likelihood ratio</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-bayes6.html"><a href="12-bayes6.html#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><i class="fa fa-check"></i><b>12.2</b> Approximate computation of the marginal likelihood and of the log-Bayes factor</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-bayes6.html"><a href="12-bayes6.html#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fa fa-check"></i><b>12.2.1</b> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-bayes6.html"><a href="12-bayes6.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>12.2.2</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-bayes6.html"><a href="12-bayes6.html#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fa fa-check"></i><b>12.2.3</b> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-bayes6.html"><a href="12-bayes6.html#model-complexity-and-occams-razor"><i class="fa fa-check"></i><b>12.2.4</b> Model complexity and Occams razor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-bayes7.html"><a href="13-bayes7.html"><i class="fa fa-check"></i><b>13</b> False discovery rates</a><ul>
<li class="chapter" data-level="13.1" data-path="13-bayes7.html"><a href="13-bayes7.html#general-setup"><i class="fa fa-check"></i><b>13.1</b> General setup</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-bayes7.html"><a href="13-bayes7.html#overview-1"><i class="fa fa-check"></i><b>13.1.1</b> Overview</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-bayes7.html"><a href="13-bayes7.html#choosing-between-h_0-and-h_a"><i class="fa fa-check"></i><b>13.1.2</b> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span></a></li>
<li class="chapter" data-level="13.1.3" data-path="13-bayes7.html"><a href="13-bayes7.html#true-and-false-positives-and-negatives"><i class="fa fa-check"></i><b>13.1.3</b> True and false positives and negatives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-bayes7.html"><a href="13-bayes7.html#specificity-and-sensitivity"><i class="fa fa-check"></i><b>13.2</b> Specificity and Sensitivity</a></li>
<li class="chapter" data-level="13.3" data-path="13-bayes7.html"><a href="13-bayes7.html#fdr-and-fndr"><i class="fa fa-check"></i><b>13.3</b> FDR and FNDR</a></li>
<li class="chapter" data-level="13.4" data-path="13-bayes7.html"><a href="13-bayes7.html#bayesian-perspective"><i class="fa fa-check"></i><b>13.4</b> Bayesian perspective</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-bayes7.html"><a href="13-bayes7.html#two-component-mixture-model"><i class="fa fa-check"></i><b>13.4.1</b> Two component mixture model</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-bayes7.html"><a href="13-bayes7.html#local-fdr"><i class="fa fa-check"></i><b>13.4.2</b> Local FDR</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-bayes7.html"><a href="13-bayes7.html#q-values"><i class="fa fa-check"></i><b>13.4.3</b> q-values</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="13-bayes7.html"><a href="13-bayes7.html#software"><i class="fa fa-check"></i><b>13.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-bayes8.html"><a href="14-bayes8.html"><i class="fa fa-check"></i><b>14</b> Optimality properties and summary</a><ul>
<li class="chapter" data-level="14.1" data-path="14-bayes8.html"><a href="14-bayes8.html#bayesian-statistics-in-a-nutshell"><i class="fa fa-check"></i><b>14.1</b> Bayesian statistics in a nutshell</a><ul>
<li class="chapter" data-level="14.1.1" data-path="14-bayes8.html"><a href="14-bayes8.html#remarks"><i class="fa fa-check"></i><b>14.1.1</b> Remarks</a></li>
<li class="chapter" data-level="14.1.2" data-path="14-bayes8.html"><a href="14-bayes8.html#advantages"><i class="fa fa-check"></i><b>14.1.2</b> Advantages</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="14-bayes8.html"><a href="14-bayes8.html#frequentist-properties-of-bayesian-estimators"><i class="fa fa-check"></i><b>14.2</b> Frequentist properties of Bayesian estimators</a></li>
<li class="chapter" data-level="14.3" data-path="14-bayes8.html"><a href="14-bayes8.html#specifying-the-prior-problem-or-advantage"><i class="fa fa-check"></i><b>14.3</b> Specifying the prior — problem or advantage?</a></li>
<li class="chapter" data-level="14.4" data-path="14-bayes8.html"><a href="14-bayes8.html#choosing-a-prior"><i class="fa fa-check"></i><b>14.4</b> Choosing a prior</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-bayes8.html"><a href="14-bayes8.html#some-guidelines"><i class="fa fa-check"></i><b>14.4.1</b> Some guidelines</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-bayes8.html"><a href="14-bayes8.html#jeffreys-prior"><i class="fa fa-check"></i><b>14.4.2</b> Jeffreys prior</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-bayes8.html"><a href="14-bayes8.html#optimality-of-bayes-inference"><i class="fa fa-check"></i><b>14.5</b> Optimality of Bayes inference</a></li>
<li class="chapter" data-level="14.6" data-path="14-bayes8.html"><a href="14-bayes8.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a><ul>
<li class="chapter" data-level="14.6.1" data-path="14-bayes8.html"><a href="14-bayes8.html#current-research"><i class="fa fa-check"></i><b>14.6.1</b> Current research</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Regression</b></span></li>
<li class="chapter" data-level="15" data-path="15-regression1.html"><a href="15-regression1.html"><i class="fa fa-check"></i><b>15</b> Overview over regression modelling</a><ul>
<li class="chapter" data-level="15.1" data-path="15-regression1.html"><a href="15-regression1.html#general-setup-1"><i class="fa fa-check"></i><b>15.1</b> General setup</a></li>
<li class="chapter" data-level="15.2" data-path="15-regression1.html"><a href="15-regression1.html#objectives"><i class="fa fa-check"></i><b>15.2</b> Objectives</a></li>
<li class="chapter" data-level="15.3" data-path="15-regression1.html"><a href="15-regression1.html#regression-as-a-form-of-supervised-learning"><i class="fa fa-check"></i><b>15.3</b> Regression as a form of supervised learning</a></li>
<li class="chapter" data-level="15.4" data-path="15-regression1.html"><a href="15-regression1.html#various-regression-models-used-in-statistics"><i class="fa fa-check"></i><b>15.4</b> Various regression models used in statistics</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="16-regression2.html"><a href="16-regression2.html"><i class="fa fa-check"></i><b>16</b> Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="16-regression2.html"><a href="16-regression2.html#the-linear-regression-model"><i class="fa fa-check"></i><b>16.1</b> The linear regression model</a></li>
<li class="chapter" data-level="16.2" data-path="16-regression2.html"><a href="16-regression2.html#interpretation-of-regression-coefficients-and-intercept"><i class="fa fa-check"></i><b>16.2</b> Interpretation of regression coefficients and intercept</a></li>
<li class="chapter" data-level="16.3" data-path="16-regression2.html"><a href="16-regression2.html#different-types-of-linear-regression"><i class="fa fa-check"></i><b>16.3</b> Different types of linear regression:</a></li>
<li class="chapter" data-level="16.4" data-path="16-regression2.html"><a href="16-regression2.html#distributional-assumptions-and-properties"><i class="fa fa-check"></i><b>16.4</b> Distributional assumptions and properties</a></li>
<li class="chapter" data-level="16.5" data-path="16-regression2.html"><a href="16-regression2.html#regression-in-data-matrix-notation"><i class="fa fa-check"></i><b>16.5</b> Regression in data matrix notation</a></li>
<li class="chapter" data-level="16.6" data-path="16-regression2.html"><a href="16-regression2.html#centering-and-vanishing-of-the-intercept-beta_0"><i class="fa fa-check"></i><b>16.6</b> Centering and vanishing of the intercept <span class="math inline">\(\beta_0\)</span></a></li>
<li class="chapter" data-level="16.7" data-path="16-regression2.html"><a href="16-regression2.html#regression-objectives-for-linear-model"><i class="fa fa-check"></i><b>16.7</b> Regression objectives for linear model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="17-regression3.html"><a href="17-regression3.html"><i class="fa fa-check"></i><b>17</b> Estimating regression coefficients</a><ul>
<li class="chapter" data-level="17.1" data-path="17-regression3.html"><a href="17-regression3.html#ordinary-least-squares-ols-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.1</b> Ordinary Least Squares (OLS) estimator of regression coefficients</a></li>
<li class="chapter" data-level="17.2" data-path="17-regression3.html"><a href="17-regression3.html#maximum-likelihood-estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>17.2</b> Maximum likelihood estimation of regression coefficients</a><ul>
<li class="chapter" data-level="17.2.1" data-path="17-regression3.html"><a href="17-regression3.html#detailed-derivation-of-the-mles"><i class="fa fa-check"></i><b>17.2.1</b> Detailed derivation of the MLEs</a></li>
<li class="chapter" data-level="17.2.2" data-path="17-regression3.html"><a href="17-regression3.html#asymptotics"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotics</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="17-regression3.html"><a href="17-regression3.html#covariance-plug-in-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.3</b> Covariance plug-in estimator of regression coefficients</a><ul>
<li class="chapter" data-level="17.3.1" data-path="17-regression3.html"><a href="17-regression3.html#importance-of-positive-definiteness-of-estimated-covariance-matrix"><i class="fa fa-check"></i><b>17.3.1</b> Importance of positive definiteness of estimated covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="17-regression3.html"><a href="17-regression3.html#best-linear-predictor"><i class="fa fa-check"></i><b>17.4</b> Best linear predictor</a><ul>
<li class="chapter" data-level="17.4.1" data-path="17-regression3.html"><a href="17-regression3.html#result"><i class="fa fa-check"></i><b>17.4.1</b> Result:</a></li>
<li class="chapter" data-level="17.4.2" data-path="17-regression3.html"><a href="17-regression3.html#irreducible-error"><i class="fa fa-check"></i><b>17.4.2</b> Irreducible Error</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="17-regression3.html"><a href="17-regression3.html#regression-by-conditioning"><i class="fa fa-check"></i><b>17.5</b> Regression by conditioning</a><ul>
<li class="chapter" data-level="17.5.1" data-path="17-regression3.html"><a href="17-regression3.html#general-idea"><i class="fa fa-check"></i><b>17.5.1</b> General idea:</a></li>
<li class="chapter" data-level="17.5.2" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-normal-assumption"><i class="fa fa-check"></i><b>17.5.2</b> Multivariate normal assumption</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="17-regression3.html"><a href="17-regression3.html#standardised-regression-coefficients-and-relationship-to-correlation"><i class="fa fa-check"></i><b>17.6</b> Standardised regression coefficients and relationship to correlation</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="18-regression4.html"><a href="18-regression4.html"><i class="fa fa-check"></i><b>18</b> Squared multiple correlation and variance decomposition in linear regression</a><ul>
<li class="chapter" data-level="18.1" data-path="18-regression4.html"><a href="18-regression4.html#squared-multiple-correlation-omega2-and-the-r2-coefficient"><i class="fa fa-check"></i><b>18.1</b> Squared multiple correlation <span class="math inline">\(\Omega^2\)</span> and the <span class="math inline">\(R^2\)</span> coefficient</a><ul>
<li class="chapter" data-level="18.1.1" data-path="18-regression4.html"><a href="18-regression4.html#estimation-of-omega2-and-the-multiple-r2-coefficient"><i class="fa fa-check"></i><b>18.1.1</b> Estimation of <span class="math inline">\(\Omega^2\)</span> and the multiple <span class="math inline">\(R^2\)</span> coefficient</a></li>
<li class="chapter" data-level="18.1.2" data-path="18-regression4.html"><a href="18-regression4.html#r-output"><i class="fa fa-check"></i><b>18.1.2</b> R output</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="18-regression4.html"><a href="18-regression4.html#variance-decomposition-in-regression"><i class="fa fa-check"></i><b>18.2</b> Variance decomposition in regression</a><ul>
<li class="chapter" data-level="18.2.1" data-path="18-regression4.html"><a href="18-regression4.html#law-of-total-variance-and-variance-decomposition"><i class="fa fa-check"></i><b>18.2.1</b> Law of total variance and variance decomposition</a></li>
<li class="chapter" data-level="18.2.2" data-path="18-regression4.html"><a href="18-regression4.html#related-quantities"><i class="fa fa-check"></i><b>18.2.2</b> Related quantities</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="18-regression4.html"><a href="18-regression4.html#sample-version-of-variance-decomposition"><i class="fa fa-check"></i><b>18.3</b> Sample version of variance decomposition</a><ul>
<li class="chapter" data-level="18.3.1" data-path="18-regression4.html"><a href="18-regression4.html#geometric-interpretation-of-regression-as-orthogonal-projection"><i class="fa fa-check"></i><b>18.3.1</b> Geometric interpretation of regression as orthogonal projection:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="19-regression5.html"><a href="19-regression5.html"><i class="fa fa-check"></i><b>19</b> Prediction and variable selection</a><ul>
<li class="chapter" data-level="19.1" data-path="19-regression5.html"><a href="19-regression5.html#prediction-and-prediction-intervals"><i class="fa fa-check"></i><b>19.1</b> Prediction and prediction intervals</a></li>
<li class="chapter" data-level="19.2" data-path="19-regression5.html"><a href="19-regression5.html#variable-importance-and-prediction"><i class="fa fa-check"></i><b>19.2</b> Variable importance and prediction</a><ul>
<li class="chapter" data-level="19.2.1" data-path="19-regression5.html"><a href="19-regression5.html#how-to-quantify-variable-importance"><i class="fa fa-check"></i><b>19.2.1</b> How to quantify variable importance?</a></li>
<li class="chapter" data-level="19.2.2" data-path="19-regression5.html"><a href="19-regression5.html#some-candidates-for-vims"><i class="fa fa-check"></i><b>19.2.2</b> Some candidates for VIMs</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="19-regression5.html"><a href="19-regression5.html#regression-t-scores."><i class="fa fa-check"></i><b>19.3</b> Regression <span class="math inline">\(t\)</span>-scores.</a><ul>
<li class="chapter" data-level="19.3.1" data-path="19-regression5.html"><a href="19-regression5.html#computation"><i class="fa fa-check"></i><b>19.3.1</b> Computation</a></li>
<li class="chapter" data-level="19.3.2" data-path="19-regression5.html"><a href="19-regression5.html#connection-with-partial-correlation"><i class="fa fa-check"></i><b>19.3.2</b> Connection with partial correlation</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="19-regression5.html"><a href="19-regression5.html#further-approaches-for-variable-selection"><i class="fa fa-check"></i><b>19.4</b> Further approaches for variable selection</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="20-refresher.html"><a href="20-refresher.html"><i class="fa fa-check"></i><b>A</b> Refresher</a><ul>
<li class="chapter" data-level="A.1" data-path="20-refresher.html"><a href="20-refresher.html#vectors-and-matrices"><i class="fa fa-check"></i><b>A.1</b> Vectors and matrices</a></li>
<li class="chapter" data-level="A.2" data-path="20-refresher.html"><a href="20-refresher.html#functions"><i class="fa fa-check"></i><b>A.2</b> Functions</a><ul>
<li class="chapter" data-level="A.2.1" data-path="20-refresher.html"><a href="20-refresher.html#gradient"><i class="fa fa-check"></i><b>A.2.1</b> Gradient</a></li>
<li class="chapter" data-level="A.2.2" data-path="20-refresher.html"><a href="20-refresher.html#hessian-matrix"><i class="fa fa-check"></i><b>A.2.2</b> Hessian matrix</a></li>
<li class="chapter" data-level="A.2.3" data-path="20-refresher.html"><a href="20-refresher.html#conditions-for-local-maximum-of-a-function"><i class="fa fa-check"></i><b>A.2.3</b> Conditions for local maximum of a function</a></li>
<li class="chapter" data-level="A.2.4" data-path="20-refresher.html"><a href="20-refresher.html#linear-and-quadratic-approximation"><i class="fa fa-check"></i><b>A.2.4</b> Linear and quadratic approximation</a></li>
<li class="chapter" data-level="A.2.5" data-path="20-refresher.html"><a href="20-refresher.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.2.5</b> Functions of matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="20-refresher.html"><a href="20-refresher.html#probability"><i class="fa fa-check"></i><b>A.3</b> Probability</a><ul>
<li class="chapter" data-level="A.3.1" data-path="20-refresher.html"><a href="20-refresher.html#law-of-large-numbers"><i class="fa fa-check"></i><b>A.3.1</b> Law of large numbers:</a></li>
<li class="chapter" data-level="A.3.2" data-path="20-refresher.html"><a href="20-refresher.html#jensens-inequality"><i class="fa fa-check"></i><b>A.3.2</b> Jensen’s inequality</a></li>
<li class="chapter" data-level="A.3.3" data-path="20-refresher.html"><a href="20-refresher.html#transformation-of-univariate-densities"><i class="fa fa-check"></i><b>A.3.3</b> Transformation of univariate densities</a></li>
<li class="chapter" data-level="A.3.4" data-path="20-refresher.html"><a href="20-refresher.html#normal-distribution"><i class="fa fa-check"></i><b>A.3.4</b> Normal distribution</a></li>
<li class="chapter" data-level="A.3.5" data-path="20-refresher.html"><a href="20-refresher.html#chi-squared-distribution"><i class="fa fa-check"></i><b>A.3.5</b> Chi-squared distribution</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="20-refresher.html"><a href="20-refresher.html#statistics"><i class="fa fa-check"></i><b>A.4</b> Statistics</a><ul>
<li class="chapter" data-level="A.4.1" data-path="20-refresher.html"><a href="20-refresher.html#statistical-learning"><i class="fa fa-check"></i><b>A.4.1</b> Statistical learning</a></li>
<li class="chapter" data-level="A.4.2" data-path="20-refresher.html"><a href="20-refresher.html#point-and-interval-estimation"><i class="fa fa-check"></i><b>A.4.2</b> Point and interval estimation</a></li>
<li class="chapter" data-level="A.4.3" data-path="20-refresher.html"><a href="20-refresher.html#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fa fa-check"></i><b>A.4.3</b> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span></a></li>
<li class="chapter" data-level="A.4.4" data-path="20-refresher.html"><a href="20-refresher.html#asymptotics-1"><i class="fa fa-check"></i><b>A.4.4</b> Asymptotics</a></li>
<li class="chapter" data-level="A.4.5" data-path="20-refresher.html"><a href="20-refresher.html#confidence-intervals"><i class="fa fa-check"></i><b>A.4.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="A.4.6" data-path="20-refresher.html"><a href="20-refresher.html#symmetric-normal-confidence-interval"><i class="fa fa-check"></i><b>A.4.6</b> Symmetric normal confidence interval</a></li>
<li class="chapter" data-level="A.4.7" data-path="20-refresher.html"><a href="20-refresher.html#confidence-interval-for-chi-squared-distribution"><i class="fa fa-check"></i><b>A.4.7</b> Confidence interval for chi-squared distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="21-further-study.html"><a href="21-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="21-further-study.html"><a href="21-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="21-further-study.html"><a href="21-further-study.html#additional-references"><i class="fa fa-check"></i><b>B.2</b> Additional references</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="22-references.html"><a href="22-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Statistical Methods:<br />
Likelihood, Bayes and Regression</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="refresher" class="section level1">
<h1><span class="header-section-number">A</span> Refresher</h1>
<p>Statistics is a mathematical science that requires practical use of tools from probability,
vector and matrices, analysis etc.</p>
<p>Here we briefly list some essentials that are needed for “Statistical Methods”.
Please familiarise yourself (again) with these topics.</p>
<div id="vectors-and-matrices" class="section level2">
<h2><span class="header-section-number">A.1</span> Vectors and matrices</h2>
<p>Vector and matrix notation.</p>
<p>Vector algebra.</p>
<p>Eigenvectors and eigenvalues for a real symmetric matrix.</p>
<p>Eigenvalue (spectral) decomposition of a real symmetric matrix.</p>
<p>Positive and negative definiteness of a real symmetric matrix (containing only positive or only negative eigenvalues).</p>
<p>Singularity of a real symmetric matrix (containing one or more eigenvalues identical to zero).</p>
<p>Singular value decomposition of a real matrix.</p>
</div>
<div id="functions" class="section level2">
<h2><span class="header-section-number">A.2</span> Functions</h2>
<div id="gradient" class="section level3">
<h3><span class="header-section-number">A.2.1</span> Gradient</h3>
<p>The <strong>nabla operator</strong> (also known as <strong>del operator</strong>) is the <em>row</em> vector
<span class="math display">\[
\nabla =  (\frac{\partial}{\partial x_1}, \ldots, 
\frac{\partial}{\partial x_d}) = \frac{\partial}{\partial \boldsymbol x}
\]</span>
containing
the first order partial derivative operators.</p>
<p>The <strong>gradient</strong> of a scalar-valued function
<span class="math inline">\(f(\boldsymbol x)\)</span> with vector argument <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_d)^T\)</span>
is also a <em>row</em> vector (with <span class="math inline">\(d\)</span> columns) and
can be expressed using the nabla operator
<span class="math display">\[
\nabla f(\boldsymbol x) = \left( \frac{\partial f(\boldsymbol x)}{\partial x_1}, \ldots, 
\frac{\partial f(\boldsymbol x)}{\partial x_d} \right) = 
 \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = \text{grad} f(\boldsymbol x) \, .
\]</span>
Note the various notations for the gradient.</p>
<p>Examples:</p>
<ul>
<li><span class="math inline">\(f(\boldsymbol x)=\boldsymbol a^T \boldsymbol x+ b\)</span>. Then <span class="math inline">\(\nabla f(\boldsymbol x) = \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol a^T\)</span>.</li>
<li><span class="math inline">\(f(\boldsymbol x)=\boldsymbol x^T \boldsymbol x\)</span>. Then <span class="math inline">\(\nabla f(\boldsymbol x) = \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = 2 \boldsymbol x^T\)</span>.</li>
<li><span class="math inline">\(f(\boldsymbol x)=\boldsymbol x^T \boldsymbol A\boldsymbol x\)</span>. Then <span class="math inline">\(\nabla f(\boldsymbol x) = \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol x^T (\boldsymbol A+ \boldsymbol A^T)\)</span>.</li>
</ul>
</div>
<div id="hessian-matrix" class="section level3">
<h3><span class="header-section-number">A.2.2</span> Hessian matrix</h3>
<p>The matrix of all second order partial derivates of scalar-valued
function with vector-valued argument is called the <strong>Hessian matrix</strong>
and is computed by double application of the nabla operator:
<span class="math display">\[
\nabla^T \nabla f(\boldsymbol x) =
\begin{pmatrix}
  \frac{\partial^2 f(\boldsymbol x)}{\partial x_1^2}
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_1 \partial x_2} 
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_1 \partial x_d} \\
  \frac{\partial^2 f(\boldsymbol x)}{\partial x_2 \partial x_1} 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_2^2}
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_2 \partial x_d} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  \frac{\partial^2 f(\boldsymbol x)}{\partial x_d \partial x_1} 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_d \partial x_2}  
     &amp; \cdots 
     &amp; \frac{\partial^2 f(\boldsymbol x)}{\partial x_d^2}
 \end{pmatrix} = \left(\frac{\partial f(\boldsymbol x)}{\partial x_i \partial x_j}\right)  
= {\left(\frac{\partial}{\partial \boldsymbol x}\right)}^T \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x}
\,.
\]</span>
By construction it is square and symmetric.</p>
</div>
<div id="conditions-for-local-maximum-of-a-function" class="section level3">
<h3><span class="header-section-number">A.2.3</span> Conditions for local maximum of a function</h3>
<p>Function has one variable:</p>
<ol style="list-style-type: lower-roman">
<li>First derivative is zero at maximum.</li>
<li>Second derivative is negative at maximum (negative curvature).</li>
</ol>
<p>Function has several variables:</p>
<ol style="list-style-type: lower-roman">
<li>Gradient vanishes at maximum.</li>
<li>Negative definite Hessian matrix at maximum (all eigenvalues of Hessian matrix are negative).</li>
</ol>
</div>
<div id="linear-and-quadratic-approximation" class="section level3">
<h3><span class="header-section-number">A.2.4</span> Linear and quadratic approximation</h3>
<p>Taylor series of first / second order.</p>
<p>Applied to scalar-valued function of a scalar:
<span class="math display">\[
f(x) \approx f(x_0) + f&#39;(x_0) (x-x_0) + \frac{1}{2} f&#39;&#39;(x_0) (x-x_0)^2
\]</span>
With <span class="math inline">\(x = x_0+ \varepsilon\)</span> this can be written as
<span class="math display">\[
f(x_0+ \varepsilon) \approx f(x_0) + f&#39;(x_0) \, \varepsilon + \frac{1}{2} f&#39;&#39;(x_0)\, \varepsilon^2
\]</span></p>
<p>Applied to scalar-valued function of a vector:
<span class="math display">\[
f(\boldsymbol x) \approx f(\boldsymbol x_0) + \nabla f(\boldsymbol x_0) (\boldsymbol x-\boldsymbol x_0) + \frac{1}{2} 
(\boldsymbol x-\boldsymbol x_0)^T \nabla^T \nabla f(\boldsymbol x_0) (\boldsymbol x-\boldsymbol x_0)
\]</span>
With <span class="math inline">\(\boldsymbol x= \boldsymbol x_0+ \boldsymbol \varepsilon\)</span> this can be written as
<span class="math display">\[
f(\boldsymbol x_0+ \boldsymbol \varepsilon) \approx f(\boldsymbol x_0) + \nabla f(\boldsymbol x_0)\boldsymbol \varepsilon+ \frac{1}{2} \boldsymbol \varepsilon^T \nabla^T \nabla f(\boldsymbol x_0) \boldsymbol \varepsilon
\]</span></p>
</div>
<div id="functions-of-matrices" class="section level3">
<h3><span class="header-section-number">A.2.5</span> Functions of matrices</h3>
<p>Matrix inverse,
matrix square root etc. of symmetric real matrices.</p>
<p>Computation via eigenvalue decomposition
i.e. apply function such as inverse, sqrt etc. on the eigenvalues.</p>
<p>In this course we do not actually compute matrix functions, but we will use matrix
notation for matrix square roots, so you do need to know that it is exists and that it
is not the same as taking the square root of the matrix entries.</p>
<p>Trace and determinant of a square matrix.</p>
<p>Connection with eigenvalues (trace = sum of eigenvalues, determinant = product of eigenvalues).</p>
</div>
</div>
<div id="probability" class="section level2">
<h2><span class="header-section-number">A.3</span> Probability</h2>
<div id="law-of-large-numbers" class="section level3">
<h3><span class="header-section-number">A.3.1</span> Law of large numbers:</h3>
<ul>
<li><p>By the strong law of large numbers the empirical distribution
<span class="math inline">\(\hat{F}_n\)</span> converges to the true underlying distribution <span class="math inline">\(F\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> almost surely:
<span class="math display">\[
\hat{F}_n\overset{a. s.}{\to} F
\]</span>
The Glivenko–Cantelli theorem asserts that the convergence is uniform. Since the strong law implies the weak law we also have convergence in probability:
<span class="math display">\[
\hat{F}_n\overset{P}{\to} F
\]</span></p></li>
<li><p>Correspondingly, for <span class="math inline">\(n \rightarrow \infty\)</span> the average <span class="math inline">\(\text{E}_{\hat{F}_n}(h(X)) = \frac{1}{n} \sum_{i=1}^n h(x_i)\)</span> converges to the expectation <span class="math inline">\(\text{E}_{F}(h(X))\)</span>.</p></li>
</ul>
</div>
<div id="jensens-inequality" class="section level3">
<h3><span class="header-section-number">A.3.2</span> Jensen’s inequality</h3>
<p><span class="math display">\[\text{E}(h(X)) \geq h(\text{E}(X))\]</span>
for a <em>convex</em> function <span class="math inline">\(h(x)\)</span>.</p>
<p>A function is convex if <span class="math inline">\(h&#39;&#39;(x) \geq 0\)</span>.
Note: if <span class="math inline">\(h(x)\)</span> is convex, then <span class="math inline">\(-h(x)\)</span> is <em>concave</em>.</p>
</div>
<div id="transformation-of-univariate-densities" class="section level3">
<h3><span class="header-section-number">A.3.3</span> Transformation of univariate densities</h3>
<p>For a general coordinate transformation <span class="math inline">\(y = h(x)\)</span> the backtransformation is <span class="math inline">\(x = h^{-1}(y)\)</span>.</p>
<p>The transformation of
the infinitesimal volume element is <span class="math inline">\(dy = |\frac{dy}{dx}| dx\)</span>.</p>
<p>The transformation of the density is <span class="math inline">\(f_y(y) =\left|\frac{dy}{dx}\right|^{-1} f_x(h^{-1} (y))\)</span>.</p>
</div>
<div id="normal-distribution" class="section level3">
<h3><span class="header-section-number">A.3.4</span> Normal distribution</h3>
<p>Univariate normal distribution:</p>
<p><span class="math inline">\(x \sim N(\mu,\sigma^2)\)</span> with <span class="math inline">\(\text{E}(x)=\mu\)</span> and <span class="math inline">\(\text{Var}(x) = \sigma^2\)</span>.</p>
<p>Probability density function (PDF): <span class="math display">\[f(x| \mu, \sigma^2)=(2\pi\sigma^2)^{-\frac{1}{2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</span></p>
<p>In R the density function is called <code>dnorm()</code>.</p>
<p>The standard normal distribution is <span class="math inline">\(N(0, 1)\)</span> with mean 1 and variance 1.</p>
<p>Plot of the PDF of the standard normal:</p>
<p><img src="20-refresher_files/figure-html/unnamed-chunk-1-1.png" width="384" /></p>
<p>The cumulative distribution function (CDF) of the standard normal <span class="math inline">\(N(0,1)\)</span>
is
<span class="math display">\[
\Phi (x ) = \int_{-\infty}^{x} f(x&#39;| \mu=0, \sigma^2=1) dx&#39; 
\]</span>
There is no analytic expression for <span class="math inline">\(\Phi(x)\)</span>. In R the function is called <code>pnorm()</code>.</p>
<p><img src="20-refresher_files/figure-html/unnamed-chunk-2-1.png" width="384" /></p>
<p>The inverse <span class="math inline">\(\Phi^{-1}(p)\)</span> is called the quantile function of the standard normal.
In R the function is called <code>qnorm()</code>.</p>
<p><img src="20-refresher_files/figure-html/unnamed-chunk-3-1.png" width="384" /></p>
</div>
<div id="chi-squared-distribution" class="section level3">
<h3><span class="header-section-number">A.3.5</span> Chi-squared distribution</h3>
<p>Asume <span class="math inline">\(m\)</span> independent standard normal random variables
<span class="math display">\[z_1,z_2,\dots,z_m\sim N(0,1)\]</span>
Then the sum of the squares
<span class="math display">\[x = \sum_{i=1}^{m} z_i^2\]</span>
is a chi-squared random variable <span class="math inline">\(x\sim \chi^2_m\)</span> with degree of freedom <span class="math inline">\(m\)</span> and <span class="math inline">\(x\geq 0\)</span>.</p>
<p>The mean of a <span class="math inline">\(\chi^2_m\)</span> distributed random variable <span class="math inline">\(x\)</span> is <span class="math inline">\(\text{E}(x)=m\)</span> and the variance <span class="math inline">\(\text{Var}(x)=2m\)</span>.</p>
<p>The chi-squared distribution is a special case (assuming <span class="math inline">\(\sigma^2=1\)</span>) of the scaled chi-squared distribution <span class="math inline">\(\sigma^2 \chi^2_m\)</span> that arises if the <span class="math inline">\(z_i \sim N(0,\sigma^2)\)</span> have variance <span class="math inline">\(\sigma^2\)</span>. The mean and variance of a scaled chi-squared distributed variable is <span class="math inline">\(\text{E}(x)=m \sigma^2\)</span> and <span class="math inline">\(\text{Var}(x)=2m\sigma^4\)</span>.</p>
<p>The Gamma distribution <span class="math inline">\(Gamma(\alpha, \beta)\)</span> is another name of the scaled chi-squared distribution but by convention is uses a different parameterisation with shape parameter <span class="math inline">\(\alpha\)</span> and scale parameter <span class="math inline">\(\beta\)</span>. The scaled chi-squared distribution <span class="math inline">\(\sigma^2 \chi^2_m\)</span> equals <span class="math inline">\(Gamma(\frac{m}{2}, 2 \sigma^2)\)</span> and the chi-squared distribution <span class="math inline">\(\chi^2_m\)</span> equals <span class="math inline">\(Gamma(\frac{m}{2}, 2)\)</span>.</p>
<p>Density of the chi-squared distribution
for degrees of freedom <span class="math inline">\(m=1\)</span> and <span class="math inline">\(m=3\)</span>:</p>
<p><img src="20-refresher_files/figure-html/unnamed-chunk-4-1.png" width="768" /></p>
<p>In R the density of the chi-squared distribution is given by <code>dchisq()</code>. The cumulative density function is <code>pchisq()</code> and
the quantile function is <code>qchisq()</code>.</p>
<p>The density of the Gamma distribution (aka scaled chi-squared distribution) is available in by <code>dgamma()</code>. The cumulative density function is <code>pgamma()</code> and
the quantile function is <code>qgamma()</code>.</p>
</div>
</div>
<div id="statistics" class="section level2">
<h2><span class="header-section-number">A.4</span> Statistics</h2>
<div id="statistical-learning" class="section level3">
<h3><span class="header-section-number">A.4.1</span> Statistical learning</h3>
<p>The aim in statistics - data science - statistics - machine learning is to learn from data
(from experiments, observations, measurements) to learn about and understand the world.</p>
<p>Specifically, to identify the best model(s) for the data in order to</p>
<ul>
<li>to explain the current data, and</li>
<li>to enable good prediction of future data</li>
</ul>
<p>Note that it is easy to get models that only explain the data but do not predict well!</p>
<p>This is called <em>overfitting</em> the data and happens in particular if the model is overparameterized for the amount of data available.</p>
<p>Specifically, we have data <span class="math inline">\(x_1, \ldots, x_n\)</span> and models <span class="math inline">\(f(x| \theta)\)</span> that are indexed the parameter <span class="math inline">\(\theta\)</span>.</p>
<p>Often (but not always) <span class="math inline">\(\theta\)</span> can be interpreted and/or is associated with some property of the model.</p>
<p>If there is only a single parameter we write <span class="math inline">\(\theta\)</span> (scalar parameter). For a parameter vector we write <span class="math inline">\(\boldsymbol \theta\)</span> (in bold type).</p>
</div>
<div id="point-and-interval-estimation" class="section level3">
<h3><span class="header-section-number">A.4.2</span> Point and interval estimation</h3>
<ul>
<li>There is a parameter <span class="math inline">\(\theta\)</span> of interest in a model</li>
<li>we are uncertain about this parameter (i.e. we don’t know the exact value)</li>
<li>we would like to learn about this parameter by observing data <span class="math inline">\(x_1, \ldots, x_n\)</span> from the model</li>
</ul>
<p>Estimation:</p>
<ul>
<li>An <strong>estimator for <span class="math inline">\(\theta\)</span></strong> is a function <span class="math inline">\(\hat{\theta}(x_1, \ldots, x_n)\)</span> that maps the data (input) to a “guess” (output) about <span class="math inline">\(\theta\)</span>.<br />
</li>
<li>A <strong>point estimator</strong> provides a single number for each parameter</li>
<li>An <strong>interval estimator</strong> provides a set of possible values for each parameter.</li>
</ul>
</div>
<div id="sampling-properties-of-a-point-estimator-hatboldsymbol-theta" class="section level3">
<h3><span class="header-section-number">A.4.3</span> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span></h3>
<p>A point estimator <span class="math inline">\(\hat\theta\)</span> depends on the data, hence it has sampling variation (i.e. estimate will be different for a new set of observations)</p>
<p>Thus <span class="math inline">\(\hat\theta\)</span> can be seen as a random variable, and its distribution is called sampling distribution (accross different experiments).</p>
<p>Properties of this distribution can be used to evaluate how far the estimator
deviates (on average across different experiments) from the true value:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{rr}
\text{Bias:}\\
\text{Variance:}\\
\text{Mean squared error:}\\
\\
\end{array}
\begin{array}{rr}
$$\text{Bias}(\hat{\theta})$$\\
$$\text{Var}(\hat{\theta})$$\\
$$\text{MSE}(\hat{\theta})$$\\
\\
\end{array}
\begin{array}{ll}
$$=\text{E}(\hat{\theta})-\theta$$\\
$$=\text{E}\left((\hat{\theta}-\text{E}(\hat{\theta}))^2\right)$$\\
$$=\text{E}((\hat{\theta}-\theta)^2)$$\\
$$=\text{Var}(\hat{\theta})+\text{Bias}(\hat{\theta})^2$$\\
\end{array}
\end{align*}\]</span></p>
<p>The last identity about MSE follows from <span class="math inline">\(\text{E}(X^2)=\text{Var}(X)+\text{E}(X)^2\)</span>.</p>
<p>At first sight it seems desirable to focus on unbiased (for finite <span class="math inline">\(n\)</span>) estimators.
However, requiring strict unbiasedness is not always a good idea!</p>
<p>In many situations it is better to allow for some small bias and in order to achieve a smaller variance and an overall total smaller MSE. This is called <em>bias-variance tradeoff</em> — as more bias
is traded for smaller variance (or, conversely, less bias is traded for higher variance)</p>
</div>
<div id="asymptotics-1" class="section level3">
<h3><span class="header-section-number">A.4.4</span> Asymptotics</h3>
<p>Typically, <span class="math inline">\(\text{Bias}\)</span>, <span class="math inline">\(\text{Var}\)</span> and <span class="math inline">\(\text{MSE}\)</span> all decrease with increasing sample size
so that with more data <span class="math inline">\(n \to \infty\)</span> the errors become smaller and smaller.</p>
<p>The typical rate of decrease of variance of a good estimator is <span class="math inline">\(\frac{1}{n}\)</span>.
Thus, when sample size is doubled the variance is divided by 2
(and the standard deviation is divided by <span class="math inline">\(\sqrt{2}\)</span>).</p>
<p>Consistency: <span class="math inline">\(\hat{\theta}\)</span> is called consistent if</p>
<p><span class="math display">\[\text{MSE}(\hat{\theta}) \longrightarrow 0 \text{ with $n\rightarrow \infty$ }\]</span>.</p>
<p>Consistency implies we recover the true model in the limit of infinite data and if the model class contains the true model.</p>
<p>Consistency is a <em>minimum</em> essential requirement for any reasonable estimator! Of all consistent
estimators we typically prefer the estimator that is most efficient (i.e. with fasted decrease in
MSE) and that thus has smallest variance and/or MSE for given finite <span class="math inline">\(n\)</span>.</p>
<p>Note that if the model class does not contain the true model then strict consistency
cannot be achived but we still wish to get at least as close as possible
to the true model.</p>
</div>
<div id="confidence-intervals" class="section level3">
<h3><span class="header-section-number">A.4.5</span> Confidence intervals</h3>
<ul>
<li>A <strong>confidence</strong> interval (CI) is an <strong>interval estimate</strong> with a <strong>frequentist</strong> interpretation.</li>
<li>Definition of <strong>coverage</strong> <span class="math inline">\(\kappa\)</span> of a CI: how often (in repeated identical experiment) does the estimated CI overlap the true parameter value <span class="math inline">\(\theta\)</span>
<ul>
<li>Eg.: Coverage <span class="math inline">\(\kappa=0.95\)</span> (95%) means that in 95 out of 100 case the estimated CI will contain the (unknown) true value (i.e. it will “cover” <span class="math inline">\(\theta\)</span>).</li>
</ul></li>
</ul>
<p>Illustration of the repeated construction of a CI for <span class="math inline">\(\theta\)</span>:</p>
<p><img src="fig/refresher_p1.PNG" width="40%" style="display: block; margin: auto;" /></p>
<ul>
<li>Note that a CI is actually an <strong>estimate</strong>: <span class="math inline">\(\widehat{\text{CI}}(x_1, \ldots, x_n)\)</span>, i.e. it depends on data and has a random (sampling) variation.<br />
</li>
<li>A good CI has high coverage and is compact.</li>
</ul>
<p><strong>Note:</strong> the coverage probability is <strong>not</strong> the probability that the true value is contained in a given estimated interval (that would be the Bayesian <em>Credible</em> Interval).</p>
</div>
<div id="symmetric-normal-confidence-interval" class="section level3">
<h3><span class="header-section-number">A.4.6</span> Symmetric normal confidence interval</h3>
<p>For a normally distributed univariate random variable
it is straightforward to construct a symmetric two-sided CI with a given desired coverage <span class="math inline">\(\kappa\)</span>.</p>
<p><img src="fig/refresher_p2_1.PNG" width="80%" style="display: block; margin: auto;" /></p>
<p>For a normal random variable <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> and density function <span class="math inline">\(f(x)\)</span> we can compute the probability</p>
<p><span class="math display">\[\text{Pr}(X \leq \mu + c \sigma) =  \int_{-\infty}^{\mu+c\sigma} f(x) dx  = \Phi (c) = \frac{1+\kappa}{2}\]</span>
Note <span class="math inline">\(\Phi(c)\)</span> is the cumulative distribution function (CDF) of the standard normal <span class="math inline">\(N(0,1)\)</span>:</p>
<p>From the above we obtain the critical point <span class="math inline">\(c\)</span> from the quantile function, i.e. by inversion of <span class="math inline">\(\Phi\)</span>:</p>
<p><span class="math display">\[c=\Phi^{-1}\left(\frac{1+\kappa}{2}\right)\]</span></p>
<p>The following table lists <span class="math inline">\(c\)</span> for the three most commonly used values of <span class="math inline">\(\kappa\)</span> - it is useful to memorise these values!</p>
<table>
<thead>
<tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span></th>
<th>Critical value <span class="math inline">\(c\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>1.64</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>1.96</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>2.58</td>
</tr>
</tbody>
</table>
<p>A <strong>symmetric standard normal CI</strong> with nominal coverage <span class="math inline">\(\kappa\)</span> for</p>
<ul>
<li>a scalar parameter <span class="math inline">\(\theta\)</span></li>
<li>with normally distributed estimate <span class="math inline">\(\hat{\theta}\)</span> and</li>
<li>with estimated standard deviation <span class="math inline">\(\hat{\text{SD}}(\hat{\theta}) = \hat{\sigma}\)</span></li>
</ul>
<p>is then given by
<span class="math display">\[\widehat{\text{CI}}=[\hat{\theta} \pm c \hat{\sigma}]\]</span></p>
<p>where <span class="math inline">\(c\)</span> is chosen for desired coverage level <span class="math inline">\(\kappa\)</span>.</p>
</div>
<div id="confidence-interval-for-chi-squared-distribution" class="section level3">
<h3><span class="header-section-number">A.4.7</span> Confidence interval for chi-squared distribution</h3>
<p><img src="fig/refresher_p4_2.PNG" width="80%" style="display: block; margin: auto;" /></p>
<p>As for the normal CI we can compute critical values but for the
chi-squared distribution we use a one-sided interval:
<span class="math display">\[
\text{Pr}(X \leq c) = \kappa
\]</span>
As before we get <span class="math inline">\(c\)</span> by the quantile function, i.e. by inverting the CDF of the chi-squared distribution.</p>
<p>The following list the critical values for the three most common choice of <span class="math inline">\(\kappa\)</span>
for <span class="math inline">\(m=1\)</span> (one degree of freedom):</p>
<table>
<thead>
<tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span></th>
<th>Critical value <span class="math inline">\(c\)</span> (<span class="math inline">\(m=1\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>2.71</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>3.84</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>6.63</td>
</tr>
</tbody>
</table>
<p>A one-sided CI with nominal coverage <span class="math inline">\(\kappa\)</span> is then given by <span class="math inline">\([0, c ]\)</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="19-regression5.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="21-further-study.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math20802-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
