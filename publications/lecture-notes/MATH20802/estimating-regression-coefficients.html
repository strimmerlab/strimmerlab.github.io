<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>17 Estimating regression coefficients | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="17 Estimating regression coefficients | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="17 Estimating regression coefficients | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.13/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><meta name="description" content="In this chapter we discuss various ways to estimate the regression coefficients. First, we discuss estimation by Ordinary Least Squares (OLS) by minimising the residual sum of squares. This yields...">
<meta property="og:description" content="In this chapter we discuss various ways to estimate the regression coefficients. First, we discuss estimation by Ordinary Least Squares (OLS) by minimising the residual sum of squares. This yields...">
<meta name="twitter:description" content="In this chapter we discuss various ways to estimate the regression coefficients. First, we discuss estimation by Ordinary Least Squares (OLS) by minimising the residual sum of squares. This yields...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="active" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="estimating-regression-coefficients" class="section level1" number="17">
<h1>
<span class="header-section-number">17</span> Estimating regression coefficients<a class="anchor" aria-label="anchor" href="#estimating-regression-coefficients"><i class="fas fa-link"></i></a>
</h1>
<p>In this chapter we discuss various ways to estimate the regression coefficients. First, we discuss estimation by Ordinary Least Squares (OLS)
by minimising the residual sum of squares. This yields the famous
Gauss estimator. Second, we derive estimates of the regression coefficients
using the methods of maximum likelihood assuming normal errors. This also leads to the Gauss estimator. Third, we show that the coefficients in
linear regression can written and interpreted in terms of two
covariance matrices and that the Gauss estimator of the regression coefficients
is a plug-in estimator using the MLEs of these covariance matrices.
Furthermore, we show that the (population version) of the Gauss estimator
can also be derived by finding the best linear predictor and by conditioning.
Finally, we discuss special cases of regression coefficients and their relationship
to marginal correlation.</p>
<div id="ordinary-least-squares-ols-estimator-of-regression-coefficients" class="section level2" number="17.1">
<h2>
<span class="header-section-number">17.1</span> Ordinary Least Squares (OLS) estimator of regression coefficients<a class="anchor" aria-label="anchor" href="#ordinary-least-squares-ols-estimator-of-regression-coefficients"><i class="fas fa-link"></i></a>
</h2>
<p>Now we show the classic way (Gauss 1809; Legendre 1805) to <strong>estimate regression coefficients</strong> by the method of
<strong>ordinary least squares (OLS)</strong>.</p>
<p><em>Idea:</em> choose regression coefficients such as to <em>minimise</em> the <em>squared error</em> between observations and the prediction.</p>
<div class="inline-figure"><img src="fig/regression3-p1.png" width="80%" style="display: block; margin: auto;"></div>
<p>In data matrix notation (note we assume <span class="math inline">\(\beta_0=0\)</span> and thus <em>centered data</em> <span class="math inline">\(\boldsymbol X\)</span> and <span class="math inline">\(\boldsymbol y\)</span>):</p>
<p><span class="math display">\[\text{RSS}(\boldsymbol \beta)=(\boldsymbol y-\boldsymbol X\boldsymbol \beta)^T(\boldsymbol y-\boldsymbol X\boldsymbol \beta)\]</span></p>
<p>RSS is an abbreviation for “Residual Sum of Squares” which is is a function of <span class="math inline">\(\boldsymbol \beta\)</span>.
Minimising RSS yields the OLS estimate:</p>
<p><span class="math display">\[\widehat{\boldsymbol \beta}_{\text{OLS}}=\underset{\boldsymbol \beta}{\arg \min}\, \text{RSS}(\boldsymbol \beta)\]</span></p>
<p><span class="math display">\[\text{RSS}(\boldsymbol \beta) = \boldsymbol y^T \boldsymbol y- 2 \boldsymbol \beta^T \boldsymbol X^T \boldsymbol y+ \boldsymbol \beta^T \boldsymbol X^T \boldsymbol X\boldsymbol \beta\]</span></p>
<p>Gradient:
<span class="math display">\[\nabla \text{RSS}(\boldsymbol \beta) = -2\boldsymbol X^T \boldsymbol y+ 2\boldsymbol X^T \boldsymbol X\boldsymbol \beta\]</span></p>
<p><span class="math display">\[\nabla \text{RSS}(\widehat{\boldsymbol \beta}) = 0 \longrightarrow \boldsymbol X^T \boldsymbol y= \boldsymbol X^T\boldsymbol X\widehat{\boldsymbol \beta}\]</span></p>
<p><span class="math display">\[\Longrightarrow \widehat{\boldsymbol \beta}_{\text{OLS}} = \left(\boldsymbol X^T\boldsymbol X\right)^{-1} \boldsymbol X^T \boldsymbol y\]</span></p>
<p>Note the similarities in the procedure to maximum likelihood (ML) estimation (with minimisation instead of maximisation)! In fact, as we
see next this is not by chance as OLS <em>is</em> indeed a special case of ML!
This also implies that OLS is generally a good method — but only if sample size <span class="math inline">\(n\)</span> is large!</p>
<p>The above Gauss’ estimator is fundamental in statistics so it is worthwile to memorise it!</p>
</div>
<div id="maximum-likelihood-estimation-of-regression-coefficients" class="section level2" number="17.2">
<h2>
<span class="header-section-number">17.2</span> Maximum likelihood estimation of regression coefficients<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimation-of-regression-coefficients"><i class="fas fa-link"></i></a>
</h2>
<p>We now show how to estimate regression coefficients using the method
of maximum likelihood. This is a second method to derive <span class="math inline">\(\hat{\boldsymbol \beta}\)</span>.</p>
<p>We recall the basic regression equation
<span class="math display">\[
y = \beta_0 + \boldsymbol \beta^T \boldsymbol x+ \varepsilon
\]</span>
with <span class="math inline">\(\text{E}(\varepsilon)=0\)</span> and observed data <span class="math inline">\(y_1, \ldots, y_n\)</span> and <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n\)</span>.
The intercept is identified as
<span class="math display">\[
\beta_0 = \mu_{y}- \boldsymbol \beta^T \boldsymbol \mu_{\boldsymbol x}
\]</span>
so that we can solve for the noise variable
<span class="math display">\[
\varepsilon = (y- \mu_{y}) - \boldsymbol \beta^T (\boldsymbol x-\boldsymbol \mu_{\boldsymbol x})
\]</span></p>
<p>Assuming joint (multivariate) normality for the response <span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> we get as the MLEs for
the respective means and (co)variances:</p>
<ul>
<li><span class="math inline">\(\hat{\mu}_y=\hat{\text{E}}(y)= \frac{1}{n}\sum^n_{i=1} y_i\)</span></li>
<li><span class="math inline">\(\hat{\sigma}^2_y=\widehat{\text{Var}}(y)= \frac{1}{n}\sum^n_{i=1} (y_i - \hat{\mu}_y)^2\)</span></li>
<li><span class="math inline">\(\hat{\boldsymbol \mu}_{\boldsymbol x}=\hat{\text{E}}(\boldsymbol x)= \frac{1}{n}\sum^n_{i=1} \boldsymbol x_i\)</span></li>
<li><span class="math inline">\(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}=\widehat{\text{Var}}(\boldsymbol x)= \frac{1}{n}\sum^n_{i=1} (\boldsymbol x_i-\hat{\boldsymbol \mu}_{\boldsymbol x}) (\boldsymbol x_i-\hat{\boldsymbol \mu}_{\boldsymbol x})^T\)</span></li>
<li><span class="math inline">\(\hat{\boldsymbol \Sigma}_{\boldsymbol xy}=\widehat{\text{Cov}}(\boldsymbol x, y)= \frac{1}{n}\sum^n_{i=1} (\boldsymbol x_i-\hat{\boldsymbol \mu}_{\boldsymbol x}) (y_i - \hat{\mu}_y)\)</span></li>
</ul>
<p>The noise <span class="math inline">\(\varepsilon \sim N(0, \sigma^2_{\varepsilon})\)</span>
is normally distributed with mean 0 and variance <span class="math inline">\(\text{Var}(\varepsilon) = \sigma^2_{\varepsilon}\)</span>.
Having obtained MLEs <span class="math inline">\(\hat{\mu}_y\)</span> and <span class="math inline">\(\hat{\boldsymbol \mu}_{\boldsymbol x}\)</span> corresponding (indirect) observations are given by
<span class="math display">\[
(y_i- \hat{\mu}_{y}) - \boldsymbol \beta^T (\boldsymbol x_i -\hat{\boldsymbol \mu}_{\boldsymbol x})
\]</span>
which leads to the normal log-likelihood function
<span class="math display">\[
\begin{split}
\log L(\boldsymbol \beta,\sigma^2_{\varepsilon} ) 
&amp;= -\frac{n}{2} \log \sigma^2_{\varepsilon} - \frac{1}{2\sigma^2_{\varepsilon}} \sum^n_{i=1} \left((y_i- \hat{\mu}_{y}) - \boldsymbol \beta^T (\boldsymbol x_i -\hat{\boldsymbol \mu}_{\boldsymbol x})\right)^2\\
\end{split}
\]</span></p>
<p>We now only need to maximise the log-likelihood to obtain MLEs of <span class="math inline">\(\sigma^2_{\varepsilon}\)</span> and <span class="math inline">\(\boldsymbol \beta\)</span>!</p>
<p>Note that the residual sum of squares appears in the log-likelihood function
(with a minus sign), which implies that ML assuming normal distribution will recover
the OLS estimator for the regression coefficients! So OLS is a special case of ML !</p>
<div id="detailed-derivation-of-the-mles" class="section level3" number="17.2.1">
<h3>
<span class="header-section-number">17.2.1</span> Detailed derivation of the MLEs<a class="anchor" aria-label="anchor" href="#detailed-derivation-of-the-mles"><i class="fas fa-link"></i></a>
</h3>
<p>The gradient with regard to <span class="math inline">\(\boldsymbol \beta\)</span> is
<span class="math display">\[
\begin{split}
\nabla_{\boldsymbol \beta} \log L(\boldsymbol \beta,\sigma^2_{\varepsilon} ) &amp;= \frac{1}{\sigma^2_{\varepsilon}} \sum^n_{i=1} \left((\boldsymbol x_i -\hat{\boldsymbol \mu}_{\boldsymbol x} ) (y_i - \hat{\mu}_{y}) - (\boldsymbol x_i -\hat{\boldsymbol \mu}_{\boldsymbol x} )(\boldsymbol x_i -\hat{\boldsymbol \mu}_{\boldsymbol x})^T \boldsymbol \beta\right)  \\
 &amp;= \frac{n}{\sigma^2_{\varepsilon}} \left( \hat{\boldsymbol \Sigma}_{\boldsymbol xy} -  \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}\boldsymbol \beta\right)\\
\end{split}
\]</span>
Setting this equal to zero yields the Gauss estimator
<span class="math display">\[
\hat{\boldsymbol \beta} = \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1} \hat{\boldsymbol \Sigma}_{\boldsymbol xy}  
\]</span>
By plugin we the get the MLE of <span class="math inline">\(\beta_0\)</span>
as
<span class="math display">\[
\hat{\beta}_0 = \hat{\mu}_{y}- \hat{\boldsymbol \beta}^T \hat{\boldsymbol \mu}_{\boldsymbol x}
\]</span>
Taking the derivative of <span class="math inline">\(\log L(\hat{\boldsymbol \beta},\sigma^2_{\varepsilon} )\)</span> with regard to <span class="math inline">\(\sigma^2_{\varepsilon}\)</span> yields
<span class="math display">\[
\frac{\partial}{\partial \sigma^2_{\varepsilon}} \log L(\hat{\boldsymbol \beta},\sigma^2_{\varepsilon} ) = -\frac{n}{2\sigma^2_{\varepsilon}} +\frac{1}{2\sigma^4_{\varepsilon}} \sum^n_{i=1}  (y_i-\hat{y}_i)^2
\]</span>
with <span class="math inline">\(\hat{y}_i = \hat{\beta}_0 + \hat{\boldsymbol \beta}^T \boldsymbol x_i\)</span> and the residuals <span class="math inline">\(y_i-\hat{y}_i\)</span> resulting from the fitted linear model.
This leads to the MLE of the noise variance
<span class="math display">\[
\widehat{\sigma^2_{\varepsilon}}  = \frac{1}{n}\sum^n_{i=1}(y_i-\hat{y}_i)^2
\]</span></p>
<p>Note that the MLE <span class="math inline">\(\widehat{\sigma^2_{\varepsilon}}\)</span> is a biased estimate of <span class="math inline">\(\sigma^2_{\varepsilon}\)</span>. The unbiased estimate is <span class="math inline">\(\frac{1}{n-d-1}\sum^n_{i=1}(y_i-\hat{y}_i)^2\)</span>, where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(\boldsymbol \beta\)</span>
(i.e. the number of predictors).</p>
</div>
<div id="asymptotics" class="section level3" number="17.2.2">
<h3>
<span class="header-section-number">17.2.2</span> Asymptotics<a class="anchor" aria-label="anchor" href="#asymptotics"><i class="fas fa-link"></i></a>
</h3>
<p>The advantage of using maximum likelihood is that we also get the (asympotic) variance associated with each estimator and typically can also assume asymptotic normality.</p>
<p>Specifically, for <span class="math inline">\(\hat{\boldsymbol \beta}\)</span> we get via the observed Fisher information at the MLE
an asymptotic estimator of its variance
<span class="math display">\[\widehat{\text{Var}}(\widehat{\boldsymbol \beta})=\frac{1}{n} \widehat{\sigma^2_{\varepsilon}} \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1}\]</span>
Similarly, for <span class="math inline">\(\hat{\beta}_0\)</span> we have
<span class="math display">\[
\widehat{\text{Var}}(\widehat{\beta}_0)=\frac{1}{n} \widehat{\sigma^2_{\varepsilon}} (1 + \hat{\boldsymbol \mu}^T \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1} \hat{\boldsymbol \mu})
\]</span></p>
<p>For finite sample size <span class="math inline">\(n\)</span> with known <span class="math inline">\(\text{Var}(\varepsilon)\)</span> one can show that the variances are
<span class="math display">\[\text{Var}(\widehat{\boldsymbol \beta})=\frac{1}{n} \sigma^2_{\varepsilon}\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1}\]</span>
and
<span class="math display">\[
\text{Var}(\widehat{\beta}_0)=\frac{1}{n} \sigma^2_{\varepsilon} (1 + \hat{\boldsymbol \mu}^T_{\boldsymbol x} \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1} \hat{\boldsymbol \mu}_{\boldsymbol x})
\]</span>
and that the regression coefficients and the intercept are normally distributed according to
<span class="math display">\[
\widehat{\boldsymbol \beta} \sim N_d(\boldsymbol \beta, \text{Var}(\widehat{\boldsymbol \beta}))
\]</span>
and
<span class="math display">\[
\widehat{\beta}_0 \sim N(\beta_0, \text{Var}(\widehat{\beta}_0))
\]</span></p>
<p>We may use this to test whether whether <span class="math inline">\(\beta_j = 0\)</span> and <span class="math inline">\(\beta_0 = 0\)</span>.</p>
</div>
</div>
<div id="covariance-plug-in-estimator-of-regression-coefficients" class="section level2" number="17.3">
<h2>
<span class="header-section-number">17.3</span> Covariance plug-in estimator of regression coefficients<a class="anchor" aria-label="anchor" href="#covariance-plug-in-estimator-of-regression-coefficients"><i class="fas fa-link"></i></a>
</h2>
<p>We now try to understand regression coefficients in terms of covariances (thus obtaining a third way to compute and estimate them).</p>
<p>We recall that the Gauss regression coefficients are given by</p>
<p><span class="math display">\[\widehat{\boldsymbol \beta} = \left(\boldsymbol X^T\boldsymbol X\right)^{-1}\boldsymbol X^T \boldsymbol y\]</span>
where <span class="math inline">\(\boldsymbol X\)</span> is the <span class="math inline">\(n \times d\)</span> data matrix (in statistics convention)</p>
<p><span class="math display">\[\boldsymbol X= \begin{pmatrix} x_{11} &amp; \dots &amp; x_{1d} \\ \vdots &amp; \ddots &amp; \vdots \\ x_{n1} &amp; \dots &amp; x_{nd} \end{pmatrix}\]</span>
Note that we assume that the data matrix <span class="math inline">\(\boldsymbol X\)</span> is centered (i.e. column sums
<span class="math inline">\(\boldsymbol X^T \boldsymbol 1_n = \boldsymbol 0\)</span> are zero).</p>
<p>Likewise <span class="math inline">\(\boldsymbol y= (y_1, \ldots, y_n)^T\)</span> is the response data vector (also centered with <span class="math inline">\(\boldsymbol y^T \boldsymbol 1_n = 0\)</span>).</p>
<p>Noting that
<span class="math display">\[\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}=\frac{1}{n}(\boldsymbol X^T\boldsymbol X)\]</span>
is the MLE of covariance matrix among <span class="math inline">\(\boldsymbol x\)</span>
and
<span class="math display">\[\hat{\boldsymbol \Sigma}_{\boldsymbol xy}=\frac{1}{n}(\boldsymbol X^T \boldsymbol y)\]</span>
is the MLE of the covariance between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(y\)</span>
we see that the OLS estimate of the regression coefficients can be expressed as
<span class="math display">\[\widehat{\boldsymbol \beta} = \left(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}\right)^{-1}\hat{\boldsymbol \Sigma}_{\boldsymbol xy}\]</span>
We can write down a population version (with no hats!): <span class="math display">\[\boldsymbol \beta= \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}\]</span></p>
<p>Thus, OLS regression coefficients can be interpreted as plugin estimator using MLEs of covariances!
In fact, we may also use the unbiased estimates since the scale factor (<span class="math inline">\(1/n\)</span> or <span class="math inline">\(1/(n-1)\)</span>) cancels out so it does not matter
which one you use!</p>
<div id="importance-of-positive-definiteness-of-estimated-covariance-matrix" class="section level3" number="17.3.1">
<h3>
<span class="header-section-number">17.3.1</span> Importance of positive definiteness of estimated covariance matrix<a class="anchor" aria-label="anchor" href="#importance-of-positive-definiteness-of-estimated-covariance-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>Note that <span class="math inline">\(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}\)</span> is inverted in <span class="math inline">\(\widehat{\boldsymbol \beta} = \left(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}\right)^{-1}\hat{\boldsymbol \Sigma}_{\boldsymbol xy}\)</span>.</p>
<ul>
<li>Hence, the estimate <span class="math inline">\(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}\)</span> needs to be positive definite!</li>
<li>But <span class="math inline">\(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{\text{MLE}}\)</span> is only positive definite if <span class="math inline">\(n&gt;d\)</span>!</li>
</ul>
<p>Therefore we can use the ML estimate (empirical estimator) only for large <span class="math inline">\(n\)</span> &gt; <span class="math inline">\(d\)</span>,
otherwise we need to employ a different (regularised) estimation approach (e.g. Bayes or a penalised ML)!</p>
<p>Remark: writing <span class="math inline">\(\hat{\boldsymbol \beta}\)</span> explicitly based on covariance estimates has the advantage that
we can construct plug-in estimators of regression coefficients based on regularised
covariance estimators that improve over ML for small sample size.
This leads to the so-called SCOUT method (=covariance-regularized regression by Witten and Tibshirani, 2008).</p>
</div>
</div>
<div id="best-linear-predictor" class="section level2" number="17.4">
<h2>
<span class="header-section-number">17.4</span> Best linear predictor<a class="anchor" aria-label="anchor" href="#best-linear-predictor"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>best linear predictor</strong> is a fourth way to arrive at the linear model. This is closely related to OLS and minimising squared residual error.</p>
<p>Without assuming normality the above multiple regression model
can be shown to be optimal linear predictor under the minimum mean squared prediction error:</p>
<p>Assumptions:</p>
<ul>
<li>
<span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> are random variables</li>
<li>we construct a new variable (the linear predictor)
<span class="math inline">\(y^{\star\star} = b_0 + \boldsymbol b^T \boldsymbol x\)</span> to optimally approximate <span class="math inline">\(y\)</span>
</li>
</ul>
<p>Aim:</p>
<ul>
<li>choose <span class="math inline">\(b_0\)</span> and <span class="math inline">\(\boldsymbol b\)</span> such to minimize the mean squared prediction error
<span class="math inline">\(\text{E}( (y - y^{\star\star})^2 )\)</span>
</li>
</ul>
<div id="result" class="section level3" number="17.4.1">
<h3>
<span class="header-section-number">17.4.1</span> Result:<a class="anchor" aria-label="anchor" href="#result"><i class="fas fa-link"></i></a>
</h3>
<p>The mean squared prediction error <span class="math inline">\(MSPE\)</span> in dependence of <span class="math inline">\((b_0, \boldsymbol b)\)</span> is
<span class="math display">\[
\begin{split}
\text{E}( (y - y^{\star\star} )^2) &amp; = \text{Var}(y - y^{\star\star}) + \text{E}(y - y^{\star\star})^2 \\
  &amp; = \text{Var}(y - b_0 -\boldsymbol b^T \boldsymbol x) + ( \text{E}(y) -b_0 - \boldsymbol b^T \text{E}(\boldsymbol x) )^2 \\
 &amp; = \sigma^2_y + \text{Var}(\boldsymbol b^T \boldsymbol x) + 2 \, \text{Cov}(y, -\boldsymbol b^T \boldsymbol x)  + ( \mu_y -b_0 - \boldsymbol b^T \boldsymbol \mu_{\boldsymbol x} )^2 \\
  &amp; = \sigma^2_y + \boldsymbol b^T \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol b- 2 \, \boldsymbol b^T \boldsymbol \Sigma_{\boldsymbol xy} + ( \mu_y -b_0 - \boldsymbol b^T \boldsymbol \mu_{\boldsymbol x} )^2 \\
  &amp; = MSPE(b_0, \boldsymbol b) \\
\end{split}
\]</span></p>
<p>We look for
<span class="math display">\[
(\beta_0, \boldsymbol \beta) = \underset{b_0,\boldsymbol b}{\arg\min} \,\, MSPE(b_0, \boldsymbol b)
\]</span></p>
<p>In order to find the minimum we compute the gradient with regard to <span class="math inline">\((b_0, \boldsymbol b)\)</span>
<span class="math display">\[
\nabla MSPE = 
\begin{pmatrix} 
-2( \mu_y -b_0 - \boldsymbol b^T \boldsymbol \mu_{\boldsymbol x} ) \\
2 \, \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol b- 2 \, \boldsymbol \Sigma_{\boldsymbol xy} -2 \boldsymbol \mu_{\boldsymbol x} (\mu_y -b_0 - \boldsymbol b^T \boldsymbol \mu_{\boldsymbol x}) \\
\end{pmatrix}
\]</span>
and setting this equal to zero yields
<span class="math display">\[
\begin{pmatrix}
\beta_0\\
\boldsymbol \beta\\
\end{pmatrix}
= 
\begin{pmatrix}
\mu_y- \boldsymbol \beta^T \boldsymbol \mu_{\boldsymbol x} \\
\boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}\\
\end{pmatrix}
\]</span>
Thus, the optimal values for <span class="math inline">\(b_0\)</span> and <span class="math inline">\(\boldsymbol b\)</span> in the best linear predictor
correspond to the previously derived coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\boldsymbol \beta\)</span>!</p>
</div>
<div id="irreducible-error" class="section level3" number="17.4.2">
<h3>
<span class="header-section-number">17.4.2</span> Irreducible Error<a class="anchor" aria-label="anchor" href="#irreducible-error"><i class="fas fa-link"></i></a>
</h3>
<p>The minimum achieved MSPE (=<strong>irreducible error</strong>) is
<span class="math display">\[
MSPE(\beta_0,\boldsymbol \beta)
= \sigma^2_{y} - \boldsymbol \beta^T \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol \beta= \sigma^2_{y} -  \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy} 
\]</span>
With the abbreviation
<span class="math inline">\(\Omega^2 = \boldsymbol P_{y \boldsymbol x} \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy} = \sigma_y^{-2} \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}\)</span>
we can simplify this to
<span class="math display">\[
MSPE(\beta_0,\boldsymbol \beta)
=  \sigma^2_y (1-\Omega^2) = \text{Var}(\varepsilon)
\]</span></p>
<p>Writing <span class="math inline">\(b_0=\beta_0 + \Delta_0\)</span> and <span class="math inline">\(\boldsymbol b= \boldsymbol \beta+ \boldsymbol \Delta\)</span> it is easy to see that the
mean squared predictive error is a quadratic function around the minimum:
<span class="math display">\[
MSPE(\beta_0 + \Delta_0, \boldsymbol \beta+ \boldsymbol \Delta) = \text{Var}(\varepsilon) + \Delta_0^2 + \boldsymbol \Delta^T \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol \Delta
\]</span></p>
<p>Note that usually <span class="math inline">\(y^{\star} = \beta_0 + \boldsymbol \beta^T \boldsymbol x\)</span> does not perfectly approximate <span class="math inline">\(y\)</span> so there <em>will</em> be an irreducible error (= noise variance)
<span class="math display">\[\text{Var}(\varepsilon) =\sigma^2_y (1-\Omega^2) &gt; 0\]</span>
which implies <span class="math inline">\(\Omega^2 &lt; 1\)</span>.</p>
<p>The quantity <span class="math inline">\(\Omega^2\)</span> has a further interpretation of the population version of
as the squared multiple correlation coefficient between the response and the predictors
and plays a vital role in decomposition of variance, as discussed later.</p>
</div>
</div>
<div id="regression-by-conditioning" class="section level2" number="17.5">
<h2>
<span class="header-section-number">17.5</span> Regression by conditioning<a class="anchor" aria-label="anchor" href="#regression-by-conditioning"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Conditioning</strong> is a fifth way to arrive at the linear model. This is also the most general way and can be used to derive many other regression models
(not just the simple linear model).</p>
<div id="general-idea" class="section level3" number="17.5.1">
<h3>
<span class="header-section-number">17.5.1</span> General idea:<a class="anchor" aria-label="anchor" href="#general-idea"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>two random variables <span class="math inline">\(y\)</span> (response, scalar) and <span class="math inline">\(\boldsymbol x\)</span> (predictor variables, vector)</li>
<li>we assume that <span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> have a joint distribution <span class="math inline">\(F_{y,\boldsymbol x}\)</span>
</li>
<li>compute <em>conditional</em> random variable <span class="math inline">\(y | \boldsymbol x\)</span> and
the corresponding distribution <span class="math inline">\(F_{y | \boldsymbol x}\)</span>
</li>
</ul>
</div>
<div id="multivariate-normal-assumption" class="section level3" number="17.5.2">
<h3>
<span class="header-section-number">17.5.2</span> Multivariate normal assumption<a class="anchor" aria-label="anchor" href="#multivariate-normal-assumption"><i class="fas fa-link"></i></a>
</h3>
<p>Now we assume that <span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> are (jointly) multivariate normal. Then the conditional distribution <span class="math inline">\(F_{y | \boldsymbol x}\)</span> is a univariate normal with the following moments (you can verify this by looking up the general conditional multivariate normal distribution):</p>
<p><strong>a) Conditional expectation:</strong></p>
<p><span class="math display">\[ \text{E}( y | \boldsymbol x) = y^{\star} = \beta_0 + \boldsymbol \beta^T \boldsymbol x\]</span></p>
<p>with coefficients <span class="math inline">\(\boldsymbol \beta= \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1}\boldsymbol \Sigma_{\boldsymbol xy}\)</span> and
intercept <span class="math inline">\(\beta_0 = \mu_{y} - \boldsymbol \beta^T \boldsymbol \mu_{\boldsymbol x}\)</span> .</p>
<p>Note that as <span class="math inline">\(y^{\star}\)</span> depends on <span class="math inline">\(\boldsymbol x\)</span> it is a random variable itself with
mean
<span class="math display">\[
\text{E}(y^{\star}) = \beta_0 + \boldsymbol \beta^T  \boldsymbol \mu_{\boldsymbol x} = \mu_{y}
\]</span>
and variance
<span class="math display">\[
\begin{split}
\text{Var}(y^{\star}) &amp; = \text{Var}(\text{E}( y | \boldsymbol x)) \\
&amp; = \boldsymbol \beta^T \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol \beta= \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy} \\
&amp; = \sigma^2_y \boldsymbol P_{y \boldsymbol x} \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy} \\
&amp; = \sigma_y^2 \Omega^2\\
\end{split}
\]</span></p>
<p><strong>b) Conditional variance:</strong></p>
<p><span class="math display">\[
\begin{split}
\text{Var}( y | \boldsymbol x) &amp;=\sigma^2_y - \boldsymbol \beta^T \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol \beta\\
&amp; = \sigma^2_y - \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}  \\
&amp; = \sigma^2_y (1-\Omega^2)\\
\end{split}
\]</span>
Note this is a constant so <span class="math inline">\(\text{E}(\text{Var}( y | \boldsymbol x)) = \sigma^2_y (1-\Omega^2)\)</span> as well.</p>
</div>
</div>
<div id="standardised-regression-coefficients-and-relationship-to-correlation" class="section level2" number="17.6">
<h2>
<span class="header-section-number">17.6</span> Standardised regression coefficients and relationship to correlation<a class="anchor" aria-label="anchor" href="#standardised-regression-coefficients-and-relationship-to-correlation"><i class="fas fa-link"></i></a>
</h2>
<p>First we note that we can decompose regression coefficients into the product
of marginal correlations and correlations among predictors.</p>
<p>Using the variance-correlation decompositions <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}= \boldsymbol V_{\boldsymbol x}^{1/2} \boldsymbol P_{\boldsymbol x\boldsymbol x} \boldsymbol V_{\boldsymbol x}^{1/2}\)</span> and <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol xy}= \boldsymbol V_{\boldsymbol x}^{1/2} \boldsymbol P_{\boldsymbol xy} \sigma_y\)</span> we rewrite the regression coefficients as
<span class="math display">\[
\boldsymbol \beta= {\underbrace{\boldsymbol V_{\boldsymbol x}^{-1/2}}_{\text{(inverse) scale of } x_i}} \,\, {\underbrace{\boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1}}_{\text{ (inverse) correlation among predictors }}} \underbrace{\boldsymbol P_{\boldsymbol xy}}_{\text{ marginal correlations}}\,\,
\underbrace{\sigma_y}_{\text{ scale of }y}
\]</span>
Thus the regression coefficients <span class="math inline">\(\boldsymbol \beta\)</span> contain the scale of the variables, and
take into account the correlations among the predictors (<span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol x}\)</span>)
in addition to the marginal correlations between the response <span class="math inline">\(y\)</span> and the predictors <span class="math inline">\(x_i\)</span> (<span class="math inline">\(\boldsymbol P_{\boldsymbol xy}\)</span>).</p>
<p>This decomposition allows to understand a number special cases when the regression coefficient simplify further:</p>
<ol style="list-style-type: lower-alpha">
<li><p>If the response and the predictors are standardised to have variance one, i.e. <span class="math inline">\(\text{Var}(y)=1\)</span> and <span class="math inline">\(\text{Var}(x_i)\)</span>, then <span class="math inline">\(\boldsymbol \beta\)</span> becomes equal to the
<strong>standardised regression coefficients</strong>
<span class="math display">\[\boldsymbol \beta_{\text{std}} = \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy}\]</span>
Note that standardised regression coefficients do not make use of variances and and thus are scale-independent.</p></li>
<li><p>If there is no correlation among the predictors , i.e. <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol x} = \boldsymbol I\)</span>
the the regression coefficients reduce to
<span class="math display">\[\boldsymbol \beta= \boldsymbol V_{\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}\]</span>
where <span class="math inline">\(\boldsymbol V_{\boldsymbol x}\)</span> is a diagonal matrix containing the variances oft the predictors.
This is also called <strong>marginal regression</strong>. Note that the inversion of <span class="math inline">\(\boldsymbol V_{\boldsymbol x}\)</span>
is trival since you only need to invert each diagonal element individually.</p></li>
<li><p>If both a) and b) apply simultaneously (i.e. there is no correlation among predictors and response and predictors and predictors
are standardised) then
the regression coefficients simplify even further to
<span class="math display">\[
\boldsymbol \beta= \boldsymbol P_{\boldsymbol xy}
\]</span>
Thus, in this very special case the regression coefficients are identical to the correlations between the response and the predictors!</p></li>
</ol>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></div>
<div class="next"><a href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#estimating-regression-coefficients"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="nav-link" href="#ordinary-least-squares-ols-estimator-of-regression-coefficients"><span class="header-section-number">17.1</span> Ordinary Least Squares (OLS) estimator of regression coefficients</a></li>
<li>
<a class="nav-link" href="#maximum-likelihood-estimation-of-regression-coefficients"><span class="header-section-number">17.2</span> Maximum likelihood estimation of regression coefficients</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#detailed-derivation-of-the-mles"><span class="header-section-number">17.2.1</span> Detailed derivation of the MLEs</a></li>
<li><a class="nav-link" href="#asymptotics"><span class="header-section-number">17.2.2</span> Asymptotics</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#covariance-plug-in-estimator-of-regression-coefficients"><span class="header-section-number">17.3</span> Covariance plug-in estimator of regression coefficients</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#importance-of-positive-definiteness-of-estimated-covariance-matrix"><span class="header-section-number">17.3.1</span> Importance of positive definiteness of estimated covariance matrix</a></li></ul>
</li>
<li>
<a class="nav-link" href="#best-linear-predictor"><span class="header-section-number">17.4</span> Best linear predictor</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#result"><span class="header-section-number">17.4.1</span> Result:</a></li>
<li><a class="nav-link" href="#irreducible-error"><span class="header-section-number">17.4.2</span> Irreducible Error</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#regression-by-conditioning"><span class="header-section-number">17.5</span> Regression by conditioning</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#general-idea"><span class="header-section-number">17.5.1</span> General idea:</a></li>
<li><a class="nav-link" href="#multivariate-normal-assumption"><span class="header-section-number">17.5.2</span> Multivariate normal assumption</a></li>
</ul>
</li>
<li><a class="nav-link" href="#standardised-regression-coefficients-and-relationship-to-correlation"><span class="header-section-number">17.6</span> Standardised regression coefficients and relationship to correlation</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 16 March 2022.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
