<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>16 Linear Regression | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="16 Linear Regression | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="16 Linear Regression | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><meta name="description" content="16.1 The linear regression model In this module we assume that \(f\) is a linear function: \[f(x_1, \ldots, x_d) = \beta_0 + \sum^{d}_{j=1} \beta_j x_j = y^{\star}\] In vector notation: \[...">
<meta property="og:description" content="16.1 The linear regression model In this module we assume that \(f\) is a linear function: \[f(x_1, \ldots, x_d) = \beta_0 + \sum^{d}_{j=1} \beta_j x_j = y^{\star}\] In vector notation: \[...">
<meta name="twitter:description" content="16.1 The linear regression model In this module we assume that \(f\) is a linear function: \[f(x_1, \ldots, x_d) = \beta_0 + \sum^{d}_{j=1} \beta_j x_j = y^{\star}\] In vector notation: \[...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="active" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="linear-regression" class="section level1" number="16">
<h1>
<span class="header-section-number">16</span> Linear Regression<a class="anchor" aria-label="anchor" href="#linear-regression"><i class="fas fa-link"></i></a>
</h1>
<div id="the-linear-regression-model" class="section level2" number="16.1">
<h2>
<span class="header-section-number">16.1</span> The linear regression model<a class="anchor" aria-label="anchor" href="#the-linear-regression-model"><i class="fas fa-link"></i></a>
</h2>
<p>In this module we assume that <span class="math inline">\(f\)</span> is a linear function:
<span class="math display">\[f(x_1, \ldots, x_d) = \beta_0 + \sum^{d}_{j=1} \beta_j x_j = y^{\star}\]</span></p>
<p>In vector notation:
<span class="math display">\[
f(\boldsymbol x) = \beta_0 + \boldsymbol \beta^T \boldsymbol x= y^{\star} 
\]</span>
with <span class="math inline">\(\boldsymbol \beta=\begin{pmatrix} \beta_1 \\ \vdots \\ \beta_d \end{pmatrix}\)</span> and <span class="math inline">\(\boldsymbol x=\begin{pmatrix} x_1 \\ \vdots \\ x_d \end{pmatrix}\)</span></p>
<p>Therefore, the linear regression model is
<span class="math display">\[
\begin{split}
y &amp;= \beta_0 + \sum^{d}_{j=1} \beta_j x_j + \varepsilon\\
  &amp;= \beta_0 + \boldsymbol \beta^T \boldsymbol x+\varepsilon \\
  &amp;= y^{\star} +\varepsilon
\end{split}
\]</span>
where:</p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span> is the <strong>intercept</strong>
</li>
<li>
<span class="math inline">\(\boldsymbol \beta= (\beta_1,\ldots,\beta_d)^T\)</span> are the <strong>regression coefficients</strong>
</li>
<li>
<span class="math inline">\(\boldsymbol x= (x_1,\ldots,x_d)^T\)</span> is the predictor vector containing the <strong>predictor variables</strong>
</li>
</ul>
</div>
<div id="interpretation-of-regression-coefficients-and-intercept" class="section level2" number="16.2">
<h2>
<span class="header-section-number">16.2</span> Interpretation of regression coefficients and intercept<a class="anchor" aria-label="anchor" href="#interpretation-of-regression-coefficients-and-intercept"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>The regression coefficient <span class="math inline">\(\beta_i\)</span> corresponds to the slope (first partial derivative) of the regression function in the direction of <span class="math inline">\(x_i\)</span>. In other words,
the gradient of <span class="math inline">\(f(\boldsymbol x)\)</span> are the regression coefficients: <span class="math inline">\(\nabla f(\boldsymbol x) = \boldsymbol \beta\)</span>
</li>
<li>The intercept <span class="math inline">\(\beta_0\)</span> is the offset at the origin (<span class="math inline">\(x_1=x_2=\ldots=x_d=0\)</span>):</li>
</ul>
<div class="inline-figure"><img src="fig/regression1-p2.png" width="80%" style="display: block; margin: auto;"></div>
</div>
<div id="different-types-of-linear-regression" class="section level2" number="16.3">
<h2>
<span class="header-section-number">16.3</span> Different types of linear regression:<a class="anchor" aria-label="anchor" href="#different-types-of-linear-regression"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>
<strong>Simple linear regression</strong>: <span class="math inline">\(y=\beta_0 + \beta x + \varepsilon\)</span> (=single predictor)</li>
<li>
<strong>Multiple linear regression</strong>: <span class="math inline">\(y =\beta_0 + \sum^{d}_{j=1} \beta_j x_j + \varepsilon\)</span> (= multiple predictor variables)</li>
<li>
<strong>Multivariate regression</strong>: multivariate response <span class="math inline">\(\boldsymbol y\)</span>
</li>
</ul>
</div>
<div id="distributional-assumptions-and-properties" class="section level2" number="16.4">
<h2>
<span class="header-section-number">16.4</span> Distributional assumptions and properties<a class="anchor" aria-label="anchor" href="#distributional-assumptions-and-properties"><i class="fas fa-link"></i></a>
</h2>
<p><em>General assumptions:</em></p>
<ul>
<li><p>We treat <span class="math inline">\(y\)</span> and <span class="math inline">\(x_1, \ldots, x_d\)</span> as the primary observables that can be described by random variables.</p></li>
<li><p><span class="math inline">\(\beta_0, \boldsymbol \beta\)</span> are parameters to be inferred from the observations
on <span class="math inline">\(y\)</span> and <span class="math inline">\(x_1, \ldots,x_d\)</span>.</p></li>
<li>
<p>Specifically, will we assume that response and predictors have a mean and a (cov)variance:</p>
<ol style="list-style-type: lower-roman">
<li><p>Response:<br><span class="math inline">\(\text{E}(y) = \mu_y\)</span><br><span class="math inline">\(\text{Var}(y) = \sigma_y^2\)</span><br>
The <strong>variance of the response</strong> <span class="math inline">\(\text{Var}(y)\)</span> is also called the <strong>total variation</strong> .</p></li>
<li><p>Predictors:<br><span class="math inline">\(\text{E}(x_i) = \mu_{x_i}\)</span> (or <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \mu_{\boldsymbol x}\)</span>)<br><span class="math inline">\(\text{Var}(x_i) = \sigma^2_{x_i}\)</span> and <span class="math inline">\(\text{Cor}(x_i, x_j) = \rho_{ij}\)</span> (or <span class="math inline">\(\text{Var}(\boldsymbol x) = \boldsymbol \Sigma_{\boldsymbol x}\)</span>)<br>
The <strong>signal variance</strong> <span class="math inline">\(\text{Var}(y^{\star})=\text{Var}(\beta_0 + \boldsymbol \beta^T \boldsymbol x) = \boldsymbol \beta^T \boldsymbol \Sigma_{\boldsymbol x} \boldsymbol \beta\)</span> is also called the <strong>explained variation</strong>.</p></li>
</ol>
</li>
<li><p>We assume that <span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> are jointly distributed with correlation <span class="math inline">\(\text{Cor}(y, x_j) = \rho_{y,x_{j}}\)</span>
between each predictor variable <span class="math inline">\(x_j\)</span> and the response <span class="math inline">\(y\)</span>.</p></li>
<li><p>In contrast to <span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> the noise variable <span class="math inline">\(\varepsilon\)</span> is only indirectly observed
via the difference <span class="math inline">\(\varepsilon = y - y^{\star}\)</span>. We denote the mean and variance of the noise by
<span class="math inline">\(\text{E}(\varepsilon)\)</span> and <span class="math inline">\(\text{Var}(\varepsilon)\)</span>.<br>
The <strong>noise variance</strong> <span class="math inline">\(\text{Var}(\varepsilon)\)</span> is also called the <strong>unexplained variation</strong>.</p></li>
</ul>
<p><em>Identifiability assumptions:</em></p>
<p>In a statistical analysis we would like to be able to separate signal (<span class="math inline">\(y^{\star}\)</span>)
from noise (<span class="math inline">\(\varepsilon\)</span>). To achieve this we require some <strong>distributional assumptions
to ensure identifiability</strong> and avoid confounding:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Assumption 1:</strong> <span class="math inline">\(\varepsilon\)</span> and <span class="math inline">\(y^{\star}\)</span> are
are independent. This implies <span class="math inline">\(\text{Var}(y) = \text{Var}(y^{\star}) + \text{Var}(\varepsilon)\)</span>, or equivalently
<span class="math inline">\(\text{Var}(\varepsilon) = \text{Var}(y) - \text{Var}(y^{\star})\)</span>.</p>
<p>Thus, this assumption implies the <strong>decomposition of variance</strong>, i.e. that
the <strong>total variation</strong> <span class="math inline">\(\text{Var}(y)\)</span> equals the sum of the <strong>explained variation</strong><span class="math inline">\(\text{Var}(y^{\star})\)</span>
and the <strong>unexplained variation</strong><span class="math inline">\(\text{Var}(\varepsilon)\)</span>.</p>
</li>
<li><p><strong>Assumption 2:</strong> <span class="math inline">\(\text{E}(\varepsilon)=0\)</span>. This allows to identify the intercept <span class="math inline">\(\beta_0\)</span> and implies <span class="math inline">\(\text{E}(y) = \text{E}(y^{\star})\)</span>.</p></li>
</ol>
<p><em>Optional assumptions (often but not always):</em></p>
<ul>
<li>The noise <span class="math inline">\(\varepsilon\)</span> is normally distributed</li>
<li>The response <span class="math inline">\(y\)</span> and and the predictor
variables <span class="math inline">\(x_i\)</span> are continuous variables</li>
<li>The response and predictor variables are jointly normally distributed</li>
</ul>
<p><em>Further properties:</em></p>
<ul>
<li>As a result of the independence assumption 1) we can only choose two out of the three
variances freely:
<ol style="list-style-type: lower-roman">
<li>in a generative perspective we will choose signal variance <span class="math inline">\(\text{Var}(y^{\star})\)</span> (or equivalently the variances <span class="math inline">\(\text{Var}(x_j)\)</span>) and the noise variance <span class="math inline">\(\text{Var}(\varepsilon)\)</span>, then
the variance of the response <span class="math inline">\(\text{Var}(y)\)</span> follows.</li>
<li>in an observational perspective we will observe the variance of the reponse
<span class="math inline">\(\text{Var}(y)\)</span> and the variances <span class="math inline">\(\text{Var}(x_j)\)</span>, and then the error variance <span class="math inline">\(\text{Var}(\varepsilon)\)</span> follows.</li>
</ol>
</li>
<li>As we will see later the regression coefficients <span class="math inline">\(\beta_j\)</span> depend on the correlations between the response <span class="math inline">\(y\)</span> and and the predictor variables <span class="math inline">\(x_j\)</span>.
Thus, the choice of regression coefficients implies a specific correlation pattern,
and vice vera (in fact, we will use this correlation pattern to infer the regression coefficient from data!).</li>
</ul>
</div>
<div id="regression-in-data-matrix-notation" class="section level2" number="16.5">
<h2>
<span class="header-section-number">16.5</span> Regression in data matrix notation<a class="anchor" aria-label="anchor" href="#regression-in-data-matrix-notation"><i class="fas fa-link"></i></a>
</h2>
<p>We can also write the regression in terms of actual observed data (rather than random variables):</p>
<p>Data matrix for the predictors:
<span class="math display">\[\boldsymbol X= \begin{pmatrix} x_{11} &amp; \dots &amp; x_{1d} \\ \vdots &amp; \ddots &amp; \vdots \\ x_{n1} &amp; \dots &amp; x_{nd} \end{pmatrix}\]</span></p>
<p>Note the statistics convention: the <span class="math inline">\(n\)</span> rows of <span class="math inline">\(\boldsymbol X\)</span> contain the samples, and the <span class="math inline">\(d\)</span> columns contain variables.</p>
<p>Response data vector: <span class="math inline">\((y_1,\dots,y_n)^T = \boldsymbol y\)</span></p>
<p>Then the regression equation is written in data matrix notation:</p>
<p><span class="math display">\[\underbrace{\boldsymbol y}_{n\times 1} = \underbrace{\boldsymbol 1_n \beta_0}_{n\times 1} + \underbrace{\boldsymbol X}_{n \times d} \underbrace{\boldsymbol \beta}_{d\times 1}+\underbrace{\boldsymbol \varepsilon}_{\underbrace{n\times 1}_{\text{residuals}}}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol 1_n = \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}\)</span> is a column vector of length <span class="math inline">\(n\)</span> (size <span class="math inline">\(n \times 1\)</span>).</p>
<p>Note that here the regression coefficients are now multiplied <em>after</em> the
data matrix (compare with the original vector notation where the <em>transpose</em> of regression coefficients come <em>before</em>
the vector of the predictors).</p>
<p>The <strong>observed noise</strong> values (i.e. realisations of <span class="math inline">\(\varepsilon\)</span>) are called the <strong>residuals</strong>.</p>
</div>
<div id="centering-and-vanishing-of-the-intercept-beta_0" class="section level2" number="16.6">
<h2>
<span class="header-section-number">16.6</span> Centering and vanishing of the intercept <span class="math inline">\(\beta_0\)</span><a class="anchor" aria-label="anchor" href="#centering-and-vanishing-of-the-intercept-beta_0"><i class="fas fa-link"></i></a>
</h2>
<p>If <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(y\)</span> are centered, i.e. <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \mu_{\boldsymbol x}= 0\)</span> and <span class="math inline">\(\text{E}(y) = \mu_{y} = 0\)</span>
then the intercept <span class="math inline">\(\beta_0\)</span> disappears:</p>
<p>The regression equation is
<span class="math display">\[y=\beta_0 + \boldsymbol \beta^T \boldsymbol x+\varepsilon\]</span>
with <span class="math inline">\(E(\varepsilon)\)</span>. Taking the expectation on both sides we get
<span class="math inline">\(\mu_{y} = \beta_0 + \boldsymbol \beta^T \boldsymbol \mu_{\boldsymbol x}\)</span>
and therefore
<span class="math display">\[
\beta_0 = \mu_{y}- \boldsymbol \beta^T \boldsymbol \mu_{\boldsymbol x}
\]</span>
This is equal to zero if the means of the response and predictors vanish.
Conversely, if we assume that the intercept vanishes (<span class="math inline">\(\beta_0=0\)</span>) this is only possible for general <span class="math inline">\(\boldsymbol \beta\)</span> if both <span class="math inline">\(\boldsymbol \mu_{\boldsymbol x}=0\)</span> and <span class="math inline">\(\mu_{y}=0\)</span>.</p>
<p>Thus, in the linear model is always possible to transform <span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> (or data <span class="math inline">\(\boldsymbol y\)</span> and <span class="math inline">\(\boldsymbol X\)</span>) so that the intercept vanishes!<br><span class="math inline">\(\Rightarrow\)</span> we will therefore often set <span class="math inline">\(\beta_0=0\)</span>.</p>
</div>
<div id="regression-objectives-for-linear-model" class="section level2" number="16.7">
<h2>
<span class="header-section-number">16.7</span> Regression objectives for linear model<a class="anchor" aria-label="anchor" href="#regression-objectives-for-linear-model"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li><p>Understand functional relationship: find estimates of intercept (<span class="math inline">\(\hat{\beta}_0\)</span>) and regression coefficients (<span class="math inline">\(\hat{\beta}_j\)</span>)</p></li>
<li>
<p>Prediction:</p>
<ul>
<li>Known coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\boldsymbol \beta\)</span>:
<span class="math inline">\(y^{\star} = \beta_0 + \boldsymbol \beta^T \boldsymbol x\)</span>
</li>
<li>Estimated coefficients <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}\)</span> (note the “hat”!):
<span class="math inline">\(\hat{y} =\hat{\beta}_0 + \sum^{d}_{j=1} \hat{\beta}_j x_j = \hat{\beta}_0 + \hat{\boldsymbol \beta}^T \boldsymbol x\)</span>
</li>
</ul>
<p>Also find the <strong>corresponding prediction errors!</strong></p>
</li>
<li><p>Variable importance: Which predictors <span class="math inline">\(x_j\)</span> are most relevant?<br><span class="math inline">\(\rightarrow\)</span> test whether <span class="math inline">\(\beta_j=0\)</span><br><span class="math inline">\(\rightarrow\)</span> find measures of variable importance<br>
Remark: as we will see <span class="math inline">\(\beta_j\)</span> or <span class="math inline">\(\hat{\beta}_j\)</span> itself is <strong>not</strong> a measure of importance!</p></li>
</ol>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></div>
<div class="next"><a href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#linear-regression"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="nav-link" href="#the-linear-regression-model"><span class="header-section-number">16.1</span> The linear regression model</a></li>
<li><a class="nav-link" href="#interpretation-of-regression-coefficients-and-intercept"><span class="header-section-number">16.2</span> Interpretation of regression coefficients and intercept</a></li>
<li><a class="nav-link" href="#different-types-of-linear-regression"><span class="header-section-number">16.3</span> Different types of linear regression:</a></li>
<li><a class="nav-link" href="#distributional-assumptions-and-properties"><span class="header-section-number">16.4</span> Distributional assumptions and properties</a></li>
<li><a class="nav-link" href="#regression-in-data-matrix-notation"><span class="header-section-number">16.5</span> Regression in data matrix notation</a></li>
<li><a class="nav-link" href="#centering-and-vanishing-of-the-intercept-beta_0"><span class="header-section-number">16.6</span> Centering and vanishing of the intercept \(\beta_0\)</a></li>
<li><a class="nav-link" href="#regression-objectives-for-linear-model"><span class="header-section-number">16.7</span> Regression objectives for linear model</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 22 January 2022.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
