<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>12 Bayesian model comparison using Bayes factors and the BIC | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="12 Bayesian model comparison using Bayes factors and the BIC | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="12 Bayesian model comparison using Bayes factors and the BIC | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><meta name="description" content="12.1 The Bayes factor We would like to compare two models \(M_1\) and \(M_2\). Before seeing data \(D\) we can check their Prior odds (= ratio of prior probabilities of the models \(M_1\) and...">
<meta property="og:description" content="12.1 The Bayes factor We would like to compare two models \(M_1\) and \(M_2\). Before seeing data \(D\) we can check their Prior odds (= ratio of prior probabilities of the models \(M_1\) and...">
<meta name="twitter:description" content="12.1 The Bayes factor We would like to compare two models \(M_1\) and \(M_2\). Before seeing data \(D\) we can check their Prior odds (= ratio of prior probabilities of the models \(M_1\) and...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="active" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bayesian-model-comparison-using-bayes-factors-and-the-bic" class="section level1" number="12">
<h1>
<span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC<a class="anchor" aria-label="anchor" href="#bayesian-model-comparison-using-bayes-factors-and-the-bic"><i class="fas fa-link"></i></a>
</h1>
<div id="the-bayes-factor" class="section level2" number="12.1">
<h2>
<span class="header-section-number">12.1</span> The Bayes factor<a class="anchor" aria-label="anchor" href="#the-bayes-factor"><i class="fas fa-link"></i></a>
</h2>
<p>We would like to compare two models <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span>. Before seeing data <span class="math inline">\(D\)</span> we can check their <strong>Prior odds</strong> (= ratio of prior probabilities of the models <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span>):<br><span class="math display">\[\frac{\text{Pr}(M_1)}{\text{Pr}(M_2)}\]</span></p>
<p>After seeing data <span class="math inline">\(D\)</span> we arrive at the <strong>Posterior odds</strong> (= ratio of posterior probabilities):
<span class="math display">\[\frac{\text{Pr}(M_1 | D)}{\text{Pr}(M_2  | D)}\]</span></p>
<p>Using Bayes Theorem <span class="math inline">\(\text{Pr}(M_i | D) = \text{Pr}(M_i) \frac{p(D | M_i) }{p(D)}\)</span> we can rewrite the
posterior odds as
<span class="math display">\[
\underbrace{\frac{\text{Pr}(M_1 | D)}{\text{Pr}(M_2 | D)}}_{\text{posterior odds}} = \underbrace{\frac{p(D | M_1)}{p(D | M_2)}}_{\text{Bayes factor $B_{12}$}} \, 
\underbrace{\frac{\text{Pr}(M_1)}{\text{Pr}(M_2)}}_{\text{prior odds}}
\]</span></p>
<p>The <strong>Bayes factor</strong> is the multiplicative factor that updates the prior odds to the posterior odds, and is the ratio of the (marginal) likelihoods
of the two models:
<span class="math display">\[
B_{12} = \frac{p(D | M_1)}{p(D | M_2)}
\]</span></p>
<p>The <strong>log-Bayes factor</strong>
<span class="math inline">\(\log B_{12}\)</span>
is also called the <strong>weight of evidence</strong> for <span class="math inline">\(M_1\)</span> over <span class="math inline">\(M_2\)</span>. Therefore, we see that</p>
<p><span class="math display">\[
\text{log-posterior odds = weight of evidence + log-prior odds}
\]</span></p>
<div id="connection-with-relative-entropy" class="section level3" number="12.1.1">
<h3>
<span class="header-section-number">12.1.1</span> Connection with relative entropy<a class="anchor" aria-label="anchor" href="#connection-with-relative-entropy"><i class="fas fa-link"></i></a>
</h3>
<p>The <em>expected</em> weight of evidence, with expectation taken with regard to one of the two models,
is in fact the <strong>KL divergence</strong> between the two models (plus a minus sign depending on direction):</p>
<p><span class="math display">\[\text{E}_{M_1}( \log B_{12} ) = KL(M_1 || M_2)\]</span></p>
<p><span class="math display">\[\text{E}_{M_2}( \log B_{12} ) = -\text{E}_{M_2}( \log B_{21} ) = -KL(M_2 || M_1)\]</span></p>
</div>
<div id="interpretation-of-and-scale-for-bayes-factor" class="section level3" number="12.1.2">
<h3>
<span class="header-section-number">12.1.2</span> Interpretation of and scale for Bayes factor<a class="anchor" aria-label="anchor" href="#interpretation-of-and-scale-for-bayes-factor"><i class="fas fa-link"></i></a>
</h3>
<p>Following Harold Jeffreys (1961) one may interpret the strength of the Bayes factor as follows:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th><span class="math inline">\(B_{12}\)</span></th>
<th><span class="math inline">\(\log B_{12}\)</span></th>
<th>evidence in favour of <span class="math inline">\(M_1\)</span> versus <span class="math inline">\(M_2\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>&gt; 100</td>
<td>&gt; 4.6</td>
<td>decisive</td>
</tr>
<tr class="even">
<td>10 to 100</td>
<td>2.3 to 4.6</td>
<td>strong</td>
</tr>
<tr class="odd">
<td>3.2 to 10</td>
<td>1.16 to 2.3</td>
<td>substantial</td>
</tr>
<tr class="even">
<td>1 to 3.2</td>
<td>0 to 1.16</td>
<td>not worth more than a bare mention</td>
</tr>
</tbody>
</table></div>
<p>More recently, Kass and Raftery (1995) proposed to use the following slightly modified scale:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th><span class="math inline">\(B_{12}\)</span></th>
<th><span class="math inline">\(\log B_{12}\)</span></th>
<th>evidence in favour of <span class="math inline">\(M_1\)</span> versus <span class="math inline">\(M_2\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>&gt; 150</td>
<td>&gt; 5</td>
<td>very strong</td>
</tr>
<tr class="even">
<td>20 to 150</td>
<td>3 to 5</td>
<td>strong</td>
</tr>
<tr class="odd">
<td>3 to 20</td>
<td>1 to 3</td>
<td>positive</td>
</tr>
<tr class="even">
<td>1 to 3</td>
<td>0 to 1</td>
<td>not worth more than a bare mention</td>
</tr>
</tbody>
</table></div>
</div>
<div id="computing-pd-m-for-simple-and-composite-models" class="section level3" number="12.1.3">
<h3>
<span class="header-section-number">12.1.3</span> Computing <span class="math inline">\(p(D | M)\)</span> for simple and composite models<a class="anchor" aria-label="anchor" href="#computing-pd-m-for-simple-and-composite-models"><i class="fas fa-link"></i></a>
</h3>
<p>In the Bayes factor we need to compute <span class="math inline">\(p(D | M)\)</span>, and it turns
out that this is different for simple and composite models.</p>
<p>A model is called “simple” if it directly corresponds to a specific distribution,
say, a Normal with fixed mean and variance, or a Binomial distribution with a set probability for the two classes. Thus, a simple model is a point in the model space described by the parameters of a distribution family (e.g.
<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> for the normal family <span class="math inline">\(N(\mu, \sigma^2\)</span>). For a simple model <span class="math inline">\(M\)</span> the density
<span class="math inline">\(p(D | M)\)</span> corresponds to standard likelihood of <span class="math inline">\(M\)</span>.</p>
<p>On the other hand, a model is “composite” if it is composed of simple models. This can be a finite set, or it can be comprised of infinite number of models.
For example, a Normal with a given mean but unspecified variance, or a Binomial model with unspecified parameter <span class="math inline">\(p\)</span>, is a composite model.</p>
<p>If <span class="math inline">\(M\)</span> is a composite model, with the underlying simple models indexed by
a parameter <span class="math inline">\(\theta\)</span>, the probability of the data given the model is
obtained by marginalisation over <span class="math inline">\(\theta\)</span>:
<span class="math display">\[
\begin{split}
p(D | M) &amp;= \int_{\theta} p(D | \theta, M) p(\theta| M) d\theta\\
             &amp;= \int_{\theta} p(D , \theta | M) d\theta\\
\end{split}
\]</span>
i.e. we <em>integrate</em> over all parameter values <span class="math inline">\(\theta\)</span>. The resulting probability is called the <em>marginal likelihood</em> of the model <span class="math inline">\(M\)</span>. Note the marginal likelihood appears also in the denominator of Bayes formula!
The marginal distribution for <span class="math inline">\(D\)</span> is also called the prior predictive distribution given <span class="math inline">\(M\)</span>.</p>
<p>If the distribution over <span class="math inline">\(\theta\)</span> is strongly concentrated around a specific value then the composite model degenerates to a simple point model.</p>
<p>A worked example (in the form of the Beta-Binomial distribution) is discussed in more detail in the Worksheet 6, Question 3.</p>
</div>
<div id="bayes-factor-versus-likelihood-ratio" class="section level3" number="12.1.4">
<h3>
<span class="header-section-number">12.1.4</span> Bayes factor versus likelihood ratio<a class="anchor" aria-label="anchor" href="#bayes-factor-versus-likelihood-ratio"><i class="fas fa-link"></i></a>
</h3>
<p><strong>If both <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span> are simple models</strong> then the <strong>Bayes factor is identical to the likelihood ratio</strong> of the two models.</p>
<p>However, if one of the two models is composite then the Bayes factor and the
generalised likelihood ratio differ:
In the Bayes factor the representative of a composite model is
the <strong>model average</strong> of the simple models indexed by <span class="math inline">\(\theta\)</span>, with weights
taken from the prior distribution over the simple models contained in <span class="math inline">\(M\)</span>. In contrast, in contrast in the generalised likelihood ratio statistic the representative of a composite model is chosen by <em>maximisation</em>!</p>
<p>Thus, <strong>for composite models</strong>, the <strong>Bayes factor does <em>not</em> equal the corresponding generalised likelihood ratio statistic.</strong> As we will see next when studying the BIC approximation, the key difference is that the Bayes factor takes into account the dimension of the composite models.</p>
</div>
</div>
<div id="approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor" class="section level2" number="12.2">
<h2>
<span class="header-section-number">12.2</span> Approximate computation of the marginal likelihood and of the log-Bayes factor<a class="anchor" aria-label="anchor" href="#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><i class="fas fa-link"></i></a>
</h2>
<p>The marginal likelihood and the Bayes factor can be difficult to compute
in practise. Therefore, a number of approximations for Bayesian modeling and model selection have been developed
The most important is the so-called BIC approximation.</p>
<div id="schwarz-1978-approximation-of-log-marginal-likelihood" class="section level3" number="12.2.1">
<h3>
<span class="header-section-number">12.2.1</span> Schwarz (1978) approximation of log-marginal likelihood<a class="anchor" aria-label="anchor" href="#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>The logarithm of the marginal likelihood of a model can be approximated
using the so-called BIC approximation (Schwarz 1978) as follow:
<span class="math display">\[
\log p(D | M) \approx l_n^M(\hat{\boldsymbol \theta}_{ML}^{M}) - \frac{1}{2} d_M \log n  
\]</span>
where <span class="math inline">\(d_M\)</span> is the dimension of the model <span class="math inline">\(M\)</span> (number of parameters in <span class="math inline">\(\boldsymbol \theta\)</span> belonging to <span class="math inline">\(M\)</span>) and <span class="math inline">\(n\)</span> is the sample size
and <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}^{M}\)</span> is the MLE.
For a simple model <span class="math inline">\(d_M=0\)</span> so then
there is no approximation as in this case the marginal likelihood equals the likelihood.</p>
<p>The above formula can be obtained by quadratic approximation of the likelihood <strong>assuming large <span class="math inline">\(n\)</span></strong> and that the prior is uniform around the MLE.</p>
<p>Note that the approximation is the maximum log-likelihood minus a penalty that depends on the model complexity (as measured by dimension <span class="math inline">\(d\)</span>), thus this is an example of penalised ML! Also note that the distribution over the parameter <span class="math inline">\(\theta\)</span> is not required in the approximation.</p>
</div>
<div id="bayesian-information-criterion-bic" class="section level3" number="12.2.2">
<h3>
<span class="header-section-number">12.2.2</span> Bayesian information criterion (BIC)<a class="anchor" aria-label="anchor" href="#bayesian-information-criterion-bic"><i class="fas fa-link"></i></a>
</h3>
<p>The BIC (Bayesian information criterion) of the model <span class="math inline">\(M\)</span> is
the approximated log-marginal likelihood times the factor -2:</p>
<p><span class="math display">\[
BIC(M) = -2 l_n^M(\hat{\boldsymbol \theta}_{ML}^{M}) + d_M \log n
\]</span></p>
<p>Thus, when comparing models one aimes to maximise the marginal likelihood or, as approximation, minimise the BIC.</p>
<p>The reason for the factor “-2” is simply to have a quantity that is
on the same scale as the Wilks log likelihood ratio. Some people / software packages also use the factor “2”.</p>
</div>
<div id="approximating-the-weight-of-evidence-log-bayes-factor-with-bic" class="section level3" number="12.2.3">
<h3>
<span class="header-section-number">12.2.3</span> Approximating the weight of evidence (log-Bayes factor) with BIC<a class="anchor" aria-label="anchor" href="#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fas fa-link"></i></a>
</h3>
<p>Using BIC (twice) the log-Bayes factor can be approximated as
<span class="math display">\[ 
\begin{split}
2 \log B_{12} &amp;\approx -BIC(M_1) + BIC(M_2) \\
&amp;=2 \left( l_n^{M_{1}}(\hat{\boldsymbol \theta}_{ML}^{M_{1}}) - l_n^{M_{2}}(\hat{\boldsymbol \theta}_{ML}^{M_{2}}) \right) - \log(n) (d_{M_{1}}-d_{M_{2}}) \\
\end{split}
\]</span>
i.e. it is the penalised log-likelihood ratio of model <span class="math inline">\(M_1\)</span> vs. <span class="math inline">\(M_2\)</span>.</p>
</div>
<div id="model-complexity-and-occams-razor" class="section level3" number="12.2.4">
<h3>
<span class="header-section-number">12.2.4</span> Model complexity and Occams razor<a class="anchor" aria-label="anchor" href="#model-complexity-and-occams-razor"><i class="fas fa-link"></i></a>
</h3>
<p>As demonstrated above the averaging over <span class="math inline">\(\theta\)</span> in the marginal likelihood has the effect of automatically penalising complex models.</p>
<p>Therefore, when comparing models using marginal likelihood, as in the Bayes factor, a complex model may be ranked below simpler models.
In contrast, when selecting a model by maximum likelihood directly, without averaging, the model with the highest number of parameters always wins over simpler models.</p>
<p>Thus, the penalisation implicit in the marginal likelihood is very much desired as it prevents the overfitting of maximum likelihood. <strong>The principle of preferring a less complex model is called “Occam’s razor”</strong>, and it is a natural propery of the Bayes factor.</p>
<p>Note that when comparing models a simpler model is often preferably over a more complex model, because the simpler model is typically better suited to both explaining the currently observed data as well as future data, whereas a complex model will only excel in fitting the current data but will then perform poorly in prediction.</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></div>
<div class="next"><a href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#bayesian-model-comparison-using-bayes-factors-and-the-bic"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li>
<a class="nav-link" href="#the-bayes-factor"><span class="header-section-number">12.1</span> The Bayes factor</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#connection-with-relative-entropy"><span class="header-section-number">12.1.1</span> Connection with relative entropy</a></li>
<li><a class="nav-link" href="#interpretation-of-and-scale-for-bayes-factor"><span class="header-section-number">12.1.2</span> Interpretation of and scale for Bayes factor</a></li>
<li><a class="nav-link" href="#computing-pd-m-for-simple-and-composite-models"><span class="header-section-number">12.1.3</span> Computing \(p(D | M)\) for simple and composite models</a></li>
<li><a class="nav-link" href="#bayes-factor-versus-likelihood-ratio"><span class="header-section-number">12.1.4</span> Bayes factor versus likelihood ratio</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><span class="header-section-number">12.2</span> Approximate computation of the marginal likelihood and of the log-Bayes factor</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#schwarz-1978-approximation-of-log-marginal-likelihood"><span class="header-section-number">12.2.1</span> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li><a class="nav-link" href="#bayesian-information-criterion-bic"><span class="header-section-number">12.2.2</span> Bayesian information criterion (BIC)</a></li>
<li><a class="nav-link" href="#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><span class="header-section-number">12.2.3</span> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
<li><a class="nav-link" href="#model-complexity-and-occams-razor"><span class="header-section-number">12.2.4</span> Model complexity and Occams razor</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 18 May 2021.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
