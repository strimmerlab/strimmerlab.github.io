<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>17 Estimating regression coefficients | HTML</title>
  <meta name="description" content="Statistical Methods:<br />
Likelihood, Bayes and Regression</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="17 Estimating regression coefficients | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="17 Estimating regression coefficients | HTML" />
  
  
  



<meta name="date" content="2021-02-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="16-regression2.html"/>
<link rel="next" href="18-regression4.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH20802/index.html">MATH20802 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Likelihood estimation and inference</b></span></li>
<li class="chapter" data-level="1" data-path="01-likelihood1.html"><a href="01-likelihood1.html"><i class="fa fa-check"></i><b>1</b> Overview of statistical learning</a><ul>
<li class="chapter" data-level="1.1" data-path="01-likelihood1.html"><a href="01-likelihood1.html#how-to-learn-from-data"><i class="fa fa-check"></i><b>1.1</b> How to learn from data?</a></li>
<li class="chapter" data-level="1.2" data-path="01-likelihood1.html"><a href="01-likelihood1.html#probability-theory-versus-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Probability theory versus statistical learning</a></li>
<li class="chapter" data-level="1.3" data-path="01-likelihood1.html"><a href="01-likelihood1.html#cartoon-of-statistical-learning"><i class="fa fa-check"></i><b>1.3</b> Cartoon of statistical learning</a></li>
<li class="chapter" data-level="1.4" data-path="01-likelihood1.html"><a href="01-likelihood1.html#likelihood"><i class="fa fa-check"></i><b>1.4</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="02-likelihood2.html"><a href="02-likelihood2.html"><i class="fa fa-check"></i><b>2</b> From information theory to likelihood</a><ul>
<li class="chapter" data-level="2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy"><i class="fa fa-check"></i><b>2.1</b> Entropy</a><ul>
<li class="chapter" data-level="2.1.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#surprise"><i class="fa fa-check"></i><b>2.1.2</b> Surprise</a></li>
<li class="chapter" data-level="2.1.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#shannon-entropy"><i class="fa fa-check"></i><b>2.1.3</b> Shannon entropy</a></li>
<li class="chapter" data-level="2.1.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#differential-entropy"><i class="fa fa-check"></i><b>2.1.4</b> Differential entropy</a></li>
<li class="chapter" data-level="2.1.5" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-entropy-principle-to-characterise-distributions"><i class="fa fa-check"></i><b>2.1.5</b> Maximum entropy principle to characterise distributions</a></li>
<li class="chapter" data-level="2.1.6" data-path="02-likelihood2.html"><a href="02-likelihood2.html#cross-entropy"><i class="fa fa-check"></i><b>2.1.6</b> Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>2.2</b> Kullback-Leibler divergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#definition"><i class="fa fa-check"></i><b>2.2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#properties-of-kl-divergence"><i class="fa fa-check"></i><b>2.2.2</b> Properties of KL divergence</a></li>
<li class="chapter" data-level="2.2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#examples"><i class="fa fa-check"></i><b>2.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#local-quadratic-approximation-and-expected-fisher-information"><i class="fa fa-check"></i><b>2.3</b> Local quadratic approximation and expected Fisher information</a></li>
<li class="chapter" data-level="2.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy-learning-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4</b> Entropy learning and maximum likelihood</a><ul>
<li class="chapter" data-level="2.4.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#the-relative-entropy-between-true-model-and-approximating-model"><i class="fa fa-check"></i><b>2.4.1</b> The relative entropy between true model and approximating model</a></li>
<li class="chapter" data-level="2.4.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#minimum-kl-divergence-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4.2</b> Minimum KL divergence and maximum likelihood</a></li>
<li class="chapter" data-level="2.4.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#further-connections"><i class="fa fa-check"></i><b>2.4.3</b> Further connections</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="03-likelihood3.html"><a href="03-likelihood3.html"><i class="fa fa-check"></i><b>3</b> Maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#principle-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.1</b> Principle of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#outline"><i class="fa fa-check"></i><b>3.1.1</b> Outline</a></li>
<li class="chapter" data-level="3.1.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#recipe-for-obtaining-mles"><i class="fa fa-check"></i><b>3.1.2</b> Recipe for obtaining MLEs</a></li>
<li class="chapter" data-level="3.1.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#invariance-property-of-the-mle"><i class="fa fa-check"></i><b>3.1.3</b> Invariance property of the MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#examples-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.2</b> Examples of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.2.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-estimation-of-a-proportion"><i class="fa fa-check"></i><b>3.2.1</b> Example 1: Estimation of a proportion</a></li>
<li class="chapter" data-level="3.2.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-exponential-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Example 2: Exponential Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-3-normal-distribution-with-unknown-mean-and-known-variance"><i class="fa fa-check"></i><b>3.2.3</b> Example 3: Normal distribution with unknown mean and known variance</a></li>
<li class="chapter" data-level="3.2.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-4-normal-distribution-with-both-mean-and-variance-unknown"><i class="fa fa-check"></i><b>3.2.4</b> Example 4: Normal Distribution with both mean and variance unknown</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information"><i class="fa fa-check"></i><b>3.3</b> Observed Fisher information</a><ul>
<li class="chapter" data-level="3.3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#motivation"><i class="fa fa-check"></i><b>3.3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#curvature-of-log-likelihood-function"><i class="fa fa-check"></i><b>3.3.2</b> Curvature of log-likelihood function</a></li>
<li class="chapter" data-level="3.3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information-1"><i class="fa fa-check"></i><b>3.3.3</b> Observed Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information---examples"><i class="fa fa-check"></i><b>3.4</b> Observed Fisher information - Examples</a><ul>
<li class="chapter" data-level="3.4.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-bernoulli-binomial-model"><i class="fa fa-check"></i><b>3.4.1</b> Example 1: Bernoulli / Binomial model</a></li>
<li class="chapter" data-level="3.4.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-normal-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Example 2: Normal distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#differences-of-observed-to-expected-fisher-information"><i class="fa fa-check"></i><b>3.4.3</b> Differences of observed to expected Fisher information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="04-likelihood4.html"><a href="04-likelihood4.html"><i class="fa fa-check"></i><b>4</b> Quadratic approximation and normal asymptotics</a><ul>
<li class="chapter" data-level="4.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#covariance-correlation-and-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.1</b> Covariance, correlation and multivariate normal distribution</a></li>
<li class="chapter" data-level="4.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#maximum-likelihood-estimates-of-the-parameters-of-the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.2</b> Maximum likelihood estimates of the parameters of the multivariate normal distribution</a></li>
<li class="chapter" data-level="4.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-approximation-of-log-likelihood-function-around-mle"><i class="fa fa-check"></i><b>4.3</b> Quadratic approximation of log-likelihood function around MLE</a></li>
<li class="chapter" data-level="4.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-normality-of-mle"><i class="fa fa-check"></i><b>4.4</b> Asymptotic normality of MLE</a></li>
<li class="chapter" data-level="4.5" data-path="04-likelihood4.html"><a href="04-likelihood4.html#observed-or-expected-fisher-information-to-estimate-variance-of-the-mle"><i class="fa fa-check"></i><b>4.5</b> Observed or expected Fisher information to estimate variance of the MLE?</a></li>
<li class="chapter" data-level="4.6" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-confidence-intervals-for-mles"><i class="fa fa-check"></i><b>4.6</b> Normal confidence intervals for MLEs</a></li>
<li class="chapter" data-level="4.7" data-path="04-likelihood4.html"><a href="04-likelihood4.html#wald-statistic"><i class="fa fa-check"></i><b>4.7</b> Wald statistic</a></li>
<li class="chapter" data-level="4.8" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-ci-expressed-using-the-squared-wald-statistics"><i class="fa fa-check"></i><b>4.8</b> Normal CI expressed using the squared Wald statistics</a></li>
<li class="chapter" data-level="4.9" data-path="04-likelihood4.html"><a href="04-likelihood4.html#testing-and-confidence-intervals"><i class="fa fa-check"></i><b>4.9</b> Testing and confidence intervals</a></li>
<li class="chapter" data-level="4.10" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-normal-distribution"><i class="fa fa-check"></i><b>4.10</b> Example: normal distribution</a></li>
<li class="chapter" data-level="4.11" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-of-non-regular-model"><i class="fa fa-check"></i><b>4.11</b> Example of non-regular model</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="05-likelihood5.html"><a href="05-likelihood5.html"><i class="fa fa-check"></i><b>5</b> Likelihood-based confidence interval and likelihood ratio</a><ul>
<li class="chapter" data-level="5.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-based-confidence-intervals"><i class="fa fa-check"></i><b>5.1</b> Likelihood-based confidence intervals</a></li>
<li class="chapter" data-level="5.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#wilks-log-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.2</b> Wilks log likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.3" data-path="05-likelihood5.html"><a href="05-likelihood5.html#quadratic-approximation-of-wilks-statistic"><i class="fa fa-check"></i><b>5.3</b> Quadratic approximation of Wilks statistic</a></li>
<li class="chapter" data-level="5.4" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistics"><i class="fa fa-check"></i><b>5.4</b> Distribution of Wilks statistics</a><ul>
<li class="chapter" data-level="5.4.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#cutoff-values-delta"><i class="fa fa-check"></i><b>5.4.1</b> Cutoff values <span class="math inline">\(\Delta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="05-likelihood5.html"><a href="05-likelihood5.html#example-likelihood-ci-for-exponential-model"><i class="fa fa-check"></i><b>5.5</b> Example: likelihood CI for exponential model</a></li>
<li class="chapter" data-level="5.6" data-path="05-likelihood5.html"><a href="05-likelihood5.html#origin-of-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.6</b> Origin of likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.7" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistic-and-likelihood-ci"><i class="fa fa-check"></i><b>5.7</b> Distribution of Wilks statistic and Likelihood CI</a></li>
<li class="chapter" data-level="5.8" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>5.8</b> Likelihood ratio test (LRT)</a></li>
<li class="chapter" data-level="5.9" data-path="05-likelihood5.html"><a href="05-likelihood5.html#optimality-of-lrts"><i class="fa fa-check"></i><b>5.9</b> Optimality of LRTs</a></li>
<li class="chapter" data-level="5.10" data-path="05-likelihood5.html"><a href="05-likelihood5.html#generalised-likelihood-ratio-test-glrt"><i class="fa fa-check"></i><b>5.10</b> Generalised likelihood ratio test (GLRT)</a></li>
<li class="chapter" data-level="5.11" data-path="05-likelihood5.html"><a href="05-likelihood5.html#glrt-example"><i class="fa fa-check"></i><b>5.11</b> GLRT example</a></li>
<li class="chapter" data-level="5.12" data-path="05-likelihood5.html"><a href="05-likelihood5.html#thoughts-on-model-selection"><i class="fa fa-check"></i><b>5.12</b> Thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="06-likelihood6.html"><a href="06-likelihood6.html"><i class="fa fa-check"></i><b>6</b> Optimality properties, minimal sufficiency and summary</a><ul>
<li class="chapter" data-level="6.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#properties-of-mles-encountered-so-far"><i class="fa fa-check"></i><b>6.1</b> Properties of MLEs encountered so far</a></li>
<li class="chapter" data-level="6.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#further-optimality-properties-of-mles"><i class="fa fa-check"></i><b>6.2</b> Further optimality properties of MLEs</a></li>
<li class="chapter" data-level="6.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency"><i class="fa fa-check"></i><b>6.3</b> Summarising data and the concept of minimal sufficiency</a></li>
<li class="chapter" data-level="6.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summary-and-concluding-remarks-on-maximum-likelihood"><i class="fa fa-check"></i><b>6.4</b> Summary and concluding remarks on maximum likelihood</a><ul>
<li class="chapter" data-level="6.4.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#starting-point-kl-divergence"><i class="fa fa-check"></i><b>6.4.1</b> Starting point: KL divergence</a></li>
<li class="chapter" data-level="6.4.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#connections-between-kl-divergence-likelihood-and-expected-and-observed-fisher-information"><i class="fa fa-check"></i><b>6.4.2</b> Connections between KL divergence, likelihood and expected and observed Fisher information</a></li>
<li class="chapter" data-level="6.4.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#likelihood-estimation"><i class="fa fa-check"></i><b>6.4.3</b> Likelihood estimation</a></li>
<li class="chapter" data-level="6.4.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#what-happens-if-n-is-small"><i class="fa fa-check"></i><b>6.4.4</b> What happens if <span class="math inline">\(n\)</span> is small?</a></li>
<li class="chapter" data-level="6.4.5" data-path="06-likelihood6.html"><a href="06-likelihood6.html#inference-with-likelihood"><i class="fa fa-check"></i><b>6.4.5</b> Inference with likelihood:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Bayesian Statistics</b></span></li>
<li class="chapter" data-level="7" data-path="07-bayes1.html"><a href="07-bayes1.html"><i class="fa fa-check"></i><b>7</b> Essentials of Bayesian statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#bayes-theorem"><i class="fa fa-check"></i><b>7.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#principle-of-bayesian-learning"><i class="fa fa-check"></i><b>7.2</b> Principle of Bayesian learning</a></li>
<li class="chapter" data-level="7.3" data-path="07-bayes1.html"><a href="07-bayes1.html#what-is-exactly-is-the-bayesian-estimate"><i class="fa fa-check"></i><b>7.3</b> What is exactly is the “Bayesian estimate”?</a></li>
<li class="chapter" data-level="7.4" data-path="07-bayes1.html"><a href="07-bayes1.html#computer-implementation-of-bayesian-learning"><i class="fa fa-check"></i><b>7.4</b> Computer implementation of Bayesian learning</a></li>
<li class="chapter" data-level="7.5" data-path="07-bayes1.html"><a href="07-bayes1.html#bayesian-interpretation-of-probability"><i class="fa fa-check"></i><b>7.5</b> Bayesian interpretation of probability</a><ul>
<li class="chapter" data-level="7.5.1" data-path="07-bayes1.html"><a href="07-bayes1.html#what-makes-you-bayesian"><i class="fa fa-check"></i><b>7.5.1</b> What makes you “Bayesian”?</a></li>
<li class="chapter" data-level="7.5.2" data-path="07-bayes1.html"><a href="07-bayes1.html#mathematics-of-probability"><i class="fa fa-check"></i><b>7.5.2</b> Mathematics of probability</a></li>
<li class="chapter" data-level="7.5.3" data-path="07-bayes1.html"><a href="07-bayes1.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.5.3</b> Interpretations of probability</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="07-bayes1.html"><a href="07-bayes1.html#historical-developments"><i class="fa fa-check"></i><b>7.6</b> Historical developments</a></li>
<li class="chapter" data-level="7.7" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning"><i class="fa fa-check"></i><b>7.7</b> Connection with entropy learning</a><ul>
<li class="chapter" data-level="7.7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#zero-forcing-property"><i class="fa fa-check"></i><b>7.7.1</b> Zero forcing property</a></li>
<li class="chapter" data-level="7.7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning-1"><i class="fa fa-check"></i><b>7.7.2</b> Connection with entropy learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="08-bayes2.html"><a href="08-bayes2.html"><i class="fa fa-check"></i><b>8</b> Beta-Binomial model for estimating a proportion</a><ul>
<li class="chapter" data-level="8.1" data-path="08-bayes2.html"><a href="08-bayes2.html#binomial-likelihood"><i class="fa fa-check"></i><b>8.1</b> Binomial likelihood</a></li>
<li class="chapter" data-level="8.2" data-path="08-bayes2.html"><a href="08-bayes2.html#excursion-properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>8.2</b> Excursion: Properties of the Beta distribution</a></li>
<li class="chapter" data-level="8.3" data-path="08-bayes2.html"><a href="08-bayes2.html#beta-prior-distribution"><i class="fa fa-check"></i><b>8.3</b> Beta prior distribution</a></li>
<li class="chapter" data-level="8.4" data-path="08-bayes2.html"><a href="08-bayes2.html#computing-the-posterior-distribution"><i class="fa fa-check"></i><b>8.4</b> Computing the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="09-bayes3.html"><a href="09-bayes3.html"><i class="fa fa-check"></i><b>9</b> Properties of Bayesian learning</a><ul>
<li class="chapter" data-level="9.1" data-path="09-bayes3.html"><a href="09-bayes3.html#prior-acting-as-pseudo-data"><i class="fa fa-check"></i><b>9.1</b> Prior acting as pseudo-data</a></li>
<li class="chapter" data-level="9.2" data-path="09-bayes3.html"><a href="09-bayes3.html#linear-shrinkage-of-mean"><i class="fa fa-check"></i><b>9.2</b> Linear shrinkage of mean</a></li>
<li class="chapter" data-level="9.3" data-path="09-bayes3.html"><a href="09-bayes3.html#conjugacy-of-prior-and-posterior-distribution"><i class="fa fa-check"></i><b>9.3</b> Conjugacy of prior and posterior distribution</a></li>
<li class="chapter" data-level="9.4" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-asymptotics"><i class="fa fa-check"></i><b>9.4</b> Large sample asymptotics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-limits-of-mean-and-variance"><i class="fa fa-check"></i><b>9.4.1</b> Large sample limits of mean and variance</a></li>
<li class="chapter" data-level="9.4.2" data-path="09-bayes3.html"><a href="09-bayes3.html#asymptotic-normality-of-the-posterior-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Asymptotic Normality of the Posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="09-bayes3.html"><a href="09-bayes3.html#posterior-variance-for-finite-n"><i class="fa fa-check"></i><b>9.5</b> Posterior variance for finite <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-bayes4.html"><a href="10-bayes4.html"><i class="fa fa-check"></i><b>10</b> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a><ul>
<li class="chapter" data-level="10.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-normal-model-to-estimate-mean"><i class="fa fa-check"></i><b>10.1</b> Normal-Normal model to estimate mean</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihood"><i class="fa fa-check"></i><b>10.1.1</b> Normal likelihood</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-prior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Normal prior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-posterior-distribution"><i class="fa fa-check"></i><b>10.1.3</b> Normal posterior distribution</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-and-stein-paradox"><i class="fa fa-check"></i><b>10.1.4</b> Large sample asymptotics and Stein paradox</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-normal-model-to-estimate-variance"><i class="fa fa-check"></i><b>10.2</b> Inverse-Gamma-Normal model to estimate variance</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Inverse Gamma distribution</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihoood"><i class="fa fa-check"></i><b>10.2.2</b> Normal likelihoood</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-prior-distribution"><i class="fa fa-check"></i><b>10.2.3</b> Inverse Gamma prior distribution</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-posterior-distribution"><i class="fa fa-check"></i><b>10.2.4</b> Inverse Gamma posterior distribution</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-1"><i class="fa fa-check"></i><b>10.2.5</b> Large sample asymptotics</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-bayes4.html"><a href="10-bayes4.html#estimating-precision"><i class="fa fa-check"></i><b>10.2.6</b> Estimating precision</a></li>
<li class="chapter" data-level="10.2.7" data-path="10-bayes4.html"><a href="10-bayes4.html#joint-estimation-of-mean-and-variance"><i class="fa fa-check"></i><b>10.2.7</b> Joint estimation of mean and variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-bayes5.html"><a href="11-bayes5.html"><i class="fa fa-check"></i><b>11</b> Shrinkage estimation using empirical risk minimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="11-bayes5.html"><a href="11-bayes5.html#linear-shrinkage"><i class="fa fa-check"></i><b>11.1</b> Linear shrinkage</a></li>
<li class="chapter" data-level="11.2" data-path="11-bayes5.html"><a href="11-bayes5.html#james-stein-estimator"><i class="fa fa-check"></i><b>11.2</b> James-Stein estimator</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-bayes6.html"><a href="12-bayes6.html"><i class="fa fa-check"></i><b>12</b> Bayesian model comparison using Bayes factors and the BIC</a><ul>
<li class="chapter" data-level="12.1" data-path="12-bayes6.html"><a href="12-bayes6.html#the-bayes-factor"><i class="fa fa-check"></i><b>12.1</b> The Bayes factor</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-bayes6.html"><a href="12-bayes6.html#connection-with-relative-entropy"><i class="fa fa-check"></i><b>12.1.1</b> Connection with relative entropy</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-bayes6.html"><a href="12-bayes6.html#interpretation-of-and-scale-for-bayes-factor"><i class="fa fa-check"></i><b>12.1.2</b> Interpretation of and scale for Bayes factor</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-bayes6.html"><a href="12-bayes6.html#computing-textprd-m-for-simple-and-composite-models"><i class="fa fa-check"></i><b>12.1.3</b> Computing <span class="math inline">\(\text{Pr}(D | M)\)</span> for simple and composite models</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-bayes6.html"><a href="12-bayes6.html#bayes-factor-versus-likelihood-ratio"><i class="fa fa-check"></i><b>12.1.4</b> Bayes factor versus likelihood ratio</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-bayes6.html"><a href="12-bayes6.html#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><i class="fa fa-check"></i><b>12.2</b> Approximate computation of the marginal likelihood and of the log-Bayes factor</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-bayes6.html"><a href="12-bayes6.html#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fa fa-check"></i><b>12.2.1</b> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-bayes6.html"><a href="12-bayes6.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>12.2.2</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-bayes6.html"><a href="12-bayes6.html#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fa fa-check"></i><b>12.2.3</b> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-bayes6.html"><a href="12-bayes6.html#model-complexity-and-occams-razor"><i class="fa fa-check"></i><b>12.2.4</b> Model complexity and Occams razor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-bayes7.html"><a href="13-bayes7.html"><i class="fa fa-check"></i><b>13</b> False discovery rates</a><ul>
<li class="chapter" data-level="13.1" data-path="13-bayes7.html"><a href="13-bayes7.html#general-setup"><i class="fa fa-check"></i><b>13.1</b> General setup</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-bayes7.html"><a href="13-bayes7.html#overview-1"><i class="fa fa-check"></i><b>13.1.1</b> Overview</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-bayes7.html"><a href="13-bayes7.html#choosing-between-h_0-and-h_a"><i class="fa fa-check"></i><b>13.1.2</b> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span></a></li>
<li class="chapter" data-level="13.1.3" data-path="13-bayes7.html"><a href="13-bayes7.html#true-and-false-positives-and-negatives"><i class="fa fa-check"></i><b>13.1.3</b> True and false positives and negatives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-bayes7.html"><a href="13-bayes7.html#specificity-and-sensitivity"><i class="fa fa-check"></i><b>13.2</b> Specificity and Sensitivity</a></li>
<li class="chapter" data-level="13.3" data-path="13-bayes7.html"><a href="13-bayes7.html#fdr-and-fndr"><i class="fa fa-check"></i><b>13.3</b> FDR and FNDR</a></li>
<li class="chapter" data-level="13.4" data-path="13-bayes7.html"><a href="13-bayes7.html#bayesian-perspective"><i class="fa fa-check"></i><b>13.4</b> Bayesian perspective</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-bayes7.html"><a href="13-bayes7.html#two-component-mixture-model"><i class="fa fa-check"></i><b>13.4.1</b> Two component mixture model</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-bayes7.html"><a href="13-bayes7.html#local-fdr"><i class="fa fa-check"></i><b>13.4.2</b> Local FDR</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-bayes7.html"><a href="13-bayes7.html#q-values"><i class="fa fa-check"></i><b>13.4.3</b> q-values</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="13-bayes7.html"><a href="13-bayes7.html#software"><i class="fa fa-check"></i><b>13.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-bayes8.html"><a href="14-bayes8.html"><i class="fa fa-check"></i><b>14</b> Optimality properties and summary</a><ul>
<li class="chapter" data-level="14.1" data-path="14-bayes8.html"><a href="14-bayes8.html#bayesian-statistics-in-a-nutshell"><i class="fa fa-check"></i><b>14.1</b> Bayesian statistics in a nutshell</a><ul>
<li class="chapter" data-level="14.1.1" data-path="14-bayes8.html"><a href="14-bayes8.html#remarks"><i class="fa fa-check"></i><b>14.1.1</b> Remarks</a></li>
<li class="chapter" data-level="14.1.2" data-path="14-bayes8.html"><a href="14-bayes8.html#advantages"><i class="fa fa-check"></i><b>14.1.2</b> Advantages</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="14-bayes8.html"><a href="14-bayes8.html#frequentist-properties-of-bayesian-estimators"><i class="fa fa-check"></i><b>14.2</b> Frequentist properties of Bayesian estimators</a></li>
<li class="chapter" data-level="14.3" data-path="14-bayes8.html"><a href="14-bayes8.html#specifying-the-prior-problem-or-advantage"><i class="fa fa-check"></i><b>14.3</b> Specifying the prior — problem or advantage?</a></li>
<li class="chapter" data-level="14.4" data-path="14-bayes8.html"><a href="14-bayes8.html#choosing-a-prior"><i class="fa fa-check"></i><b>14.4</b> Choosing a prior</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-bayes8.html"><a href="14-bayes8.html#some-guidelines"><i class="fa fa-check"></i><b>14.4.1</b> Some guidelines</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-bayes8.html"><a href="14-bayes8.html#jeffreys-prior"><i class="fa fa-check"></i><b>14.4.2</b> Jeffreys prior</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-bayes8.html"><a href="14-bayes8.html#optimality-of-bayes-inference"><i class="fa fa-check"></i><b>14.5</b> Optimality of Bayes inference</a></li>
<li class="chapter" data-level="14.6" data-path="14-bayes8.html"><a href="14-bayes8.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a><ul>
<li class="chapter" data-level="14.6.1" data-path="14-bayes8.html"><a href="14-bayes8.html#current-research"><i class="fa fa-check"></i><b>14.6.1</b> Current research</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Regression</b></span></li>
<li class="chapter" data-level="15" data-path="15-regression1.html"><a href="15-regression1.html"><i class="fa fa-check"></i><b>15</b> Overview over regression modelling</a><ul>
<li class="chapter" data-level="15.1" data-path="15-regression1.html"><a href="15-regression1.html#general-setup-1"><i class="fa fa-check"></i><b>15.1</b> General setup</a></li>
<li class="chapter" data-level="15.2" data-path="15-regression1.html"><a href="15-regression1.html#objectives"><i class="fa fa-check"></i><b>15.2</b> Objectives</a></li>
<li class="chapter" data-level="15.3" data-path="15-regression1.html"><a href="15-regression1.html#regression-as-a-form-of-supervised-learning"><i class="fa fa-check"></i><b>15.3</b> Regression as a form of supervised learning</a></li>
<li class="chapter" data-level="15.4" data-path="15-regression1.html"><a href="15-regression1.html#various-regression-models-used-in-statistics"><i class="fa fa-check"></i><b>15.4</b> Various regression models used in statistics</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="16-regression2.html"><a href="16-regression2.html"><i class="fa fa-check"></i><b>16</b> Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="16-regression2.html"><a href="16-regression2.html#the-linear-regression-model"><i class="fa fa-check"></i><b>16.1</b> The linear regression model</a></li>
<li class="chapter" data-level="16.2" data-path="16-regression2.html"><a href="16-regression2.html#interpretation-of-regression-coefficients-and-intercept"><i class="fa fa-check"></i><b>16.2</b> Interpretation of regression coefficients and intercept</a></li>
<li class="chapter" data-level="16.3" data-path="16-regression2.html"><a href="16-regression2.html#different-types-of-linear-regression"><i class="fa fa-check"></i><b>16.3</b> Different types of linear regression:</a></li>
<li class="chapter" data-level="16.4" data-path="16-regression2.html"><a href="16-regression2.html#distributional-assumptions-and-properties"><i class="fa fa-check"></i><b>16.4</b> Distributional assumptions and properties</a></li>
<li class="chapter" data-level="16.5" data-path="16-regression2.html"><a href="16-regression2.html#regression-in-data-matrix-notation"><i class="fa fa-check"></i><b>16.5</b> Regression in data matrix notation</a></li>
<li class="chapter" data-level="16.6" data-path="16-regression2.html"><a href="16-regression2.html#centering-and-vanishing-of-the-intercept-beta_0"><i class="fa fa-check"></i><b>16.6</b> Centering and vanishing of the intercept <span class="math inline">\(\beta_0\)</span></a></li>
<li class="chapter" data-level="16.7" data-path="16-regression2.html"><a href="16-regression2.html#regression-objectives-for-linear-model"><i class="fa fa-check"></i><b>16.7</b> Regression objectives for linear model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="17-regression3.html"><a href="17-regression3.html"><i class="fa fa-check"></i><b>17</b> Estimating regression coefficients</a><ul>
<li class="chapter" data-level="17.1" data-path="17-regression3.html"><a href="17-regression3.html#ordinary-least-squares-ols-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.1</b> Ordinary Least Squares (OLS) estimator of regression coefficients</a></li>
<li class="chapter" data-level="17.2" data-path="17-regression3.html"><a href="17-regression3.html#maximum-likelihood-estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>17.2</b> Maximum likelihood estimation of regression coefficients</a><ul>
<li class="chapter" data-level="17.2.1" data-path="17-regression3.html"><a href="17-regression3.html#detailed-derivation-of-the-mles"><i class="fa fa-check"></i><b>17.2.1</b> Detailed derivation of the MLEs</a></li>
<li class="chapter" data-level="17.2.2" data-path="17-regression3.html"><a href="17-regression3.html#asymptotics"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotics</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="17-regression3.html"><a href="17-regression3.html#covariance-plug-in-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.3</b> Covariance plug-in estimator of regression coefficients</a><ul>
<li class="chapter" data-level="17.3.1" data-path="17-regression3.html"><a href="17-regression3.html#importance-of-positive-definiteness-of-estimated-covariance-matrix"><i class="fa fa-check"></i><b>17.3.1</b> Importance of positive definiteness of estimated covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="17-regression3.html"><a href="17-regression3.html#best-linear-predictor"><i class="fa fa-check"></i><b>17.4</b> Best linear predictor</a><ul>
<li class="chapter" data-level="17.4.1" data-path="17-regression3.html"><a href="17-regression3.html#result"><i class="fa fa-check"></i><b>17.4.1</b> Result:</a></li>
<li class="chapter" data-level="17.4.2" data-path="17-regression3.html"><a href="17-regression3.html#irreducible-error"><i class="fa fa-check"></i><b>17.4.2</b> Irreducible Error</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="17-regression3.html"><a href="17-regression3.html#regression-by-conditioning"><i class="fa fa-check"></i><b>17.5</b> Regression by conditioning</a><ul>
<li class="chapter" data-level="17.5.1" data-path="17-regression3.html"><a href="17-regression3.html#general-idea"><i class="fa fa-check"></i><b>17.5.1</b> General idea:</a></li>
<li class="chapter" data-level="17.5.2" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-normal-assumption"><i class="fa fa-check"></i><b>17.5.2</b> Multivariate normal assumption</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="17-regression3.html"><a href="17-regression3.html#standardised-regression-coefficients-and-relationship-to-correlation"><i class="fa fa-check"></i><b>17.6</b> Standardised regression coefficients and relationship to correlation</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="18-regression4.html"><a href="18-regression4.html"><i class="fa fa-check"></i><b>18</b> Squared multiple correlation and variance decomposition in linear regression</a><ul>
<li class="chapter" data-level="18.1" data-path="18-regression4.html"><a href="18-regression4.html#squared-multiple-correlation-omega2-and-the-r2-coefficient"><i class="fa fa-check"></i><b>18.1</b> Squared multiple correlation <span class="math inline">\(\Omega^2\)</span> and the <span class="math inline">\(R^2\)</span> coefficient</a><ul>
<li class="chapter" data-level="18.1.1" data-path="18-regression4.html"><a href="18-regression4.html#estimation-of-omega2-and-the-multiple-r2-coefficient"><i class="fa fa-check"></i><b>18.1.1</b> Estimation of <span class="math inline">\(\Omega^2\)</span> and the multiple <span class="math inline">\(R^2\)</span> coefficient</a></li>
<li class="chapter" data-level="18.1.2" data-path="18-regression4.html"><a href="18-regression4.html#r-output"><i class="fa fa-check"></i><b>18.1.2</b> R output</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="18-regression4.html"><a href="18-regression4.html#variance-decomposition-in-regression"><i class="fa fa-check"></i><b>18.2</b> Variance decomposition in regression</a><ul>
<li class="chapter" data-level="18.2.1" data-path="18-regression4.html"><a href="18-regression4.html#law-of-total-variance-and-variance-decomposition"><i class="fa fa-check"></i><b>18.2.1</b> Law of total variance and variance decomposition</a></li>
<li class="chapter" data-level="18.2.2" data-path="18-regression4.html"><a href="18-regression4.html#related-quantities"><i class="fa fa-check"></i><b>18.2.2</b> Related quantities</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="18-regression4.html"><a href="18-regression4.html#sample-version-of-variance-decomposition"><i class="fa fa-check"></i><b>18.3</b> Sample version of variance decomposition</a><ul>
<li class="chapter" data-level="18.3.1" data-path="18-regression4.html"><a href="18-regression4.html#geometric-interpretation-of-regression-as-orthogonal-projection"><i class="fa fa-check"></i><b>18.3.1</b> Geometric interpretation of regression as orthogonal projection:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="19-regression5.html"><a href="19-regression5.html"><i class="fa fa-check"></i><b>19</b> Prediction and variable selection</a><ul>
<li class="chapter" data-level="19.1" data-path="19-regression5.html"><a href="19-regression5.html#prediction-and-prediction-intervals"><i class="fa fa-check"></i><b>19.1</b> Prediction and prediction intervals</a></li>
<li class="chapter" data-level="19.2" data-path="19-regression5.html"><a href="19-regression5.html#variable-importance-and-prediction"><i class="fa fa-check"></i><b>19.2</b> Variable importance and prediction</a><ul>
<li class="chapter" data-level="19.2.1" data-path="19-regression5.html"><a href="19-regression5.html#how-to-quantify-variable-importance"><i class="fa fa-check"></i><b>19.2.1</b> How to quantify variable importance?</a></li>
<li class="chapter" data-level="19.2.2" data-path="19-regression5.html"><a href="19-regression5.html#some-candidates-for-vims"><i class="fa fa-check"></i><b>19.2.2</b> Some candidates for VIMs</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="19-regression5.html"><a href="19-regression5.html#regression-t-scores."><i class="fa fa-check"></i><b>19.3</b> Regression <span class="math inline">\(t\)</span>-scores.</a><ul>
<li class="chapter" data-level="19.3.1" data-path="19-regression5.html"><a href="19-regression5.html#computation"><i class="fa fa-check"></i><b>19.3.1</b> Computation</a></li>
<li class="chapter" data-level="19.3.2" data-path="19-regression5.html"><a href="19-regression5.html#connection-with-partial-correlation"><i class="fa fa-check"></i><b>19.3.2</b> Connection with partial correlation</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="19-regression5.html"><a href="19-regression5.html#further-approaches-for-variable-selection"><i class="fa fa-check"></i><b>19.4</b> Further approaches for variable selection</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="20-refresher.html"><a href="20-refresher.html"><i class="fa fa-check"></i><b>A</b> Refresher</a><ul>
<li class="chapter" data-level="A.1" data-path="20-refresher.html"><a href="20-refresher.html#vectors-and-matrices"><i class="fa fa-check"></i><b>A.1</b> Vectors and matrices</a></li>
<li class="chapter" data-level="A.2" data-path="20-refresher.html"><a href="20-refresher.html#functions"><i class="fa fa-check"></i><b>A.2</b> Functions</a><ul>
<li class="chapter" data-level="A.2.1" data-path="20-refresher.html"><a href="20-refresher.html#gradient"><i class="fa fa-check"></i><b>A.2.1</b> Gradient</a></li>
<li class="chapter" data-level="A.2.2" data-path="20-refresher.html"><a href="20-refresher.html#hessian-matrix"><i class="fa fa-check"></i><b>A.2.2</b> Hessian matrix</a></li>
<li class="chapter" data-level="A.2.3" data-path="20-refresher.html"><a href="20-refresher.html#conditions-for-local-maximum-of-a-function"><i class="fa fa-check"></i><b>A.2.3</b> Conditions for local maximum of a function</a></li>
<li class="chapter" data-level="A.2.4" data-path="20-refresher.html"><a href="20-refresher.html#linear-and-quadratic-approximation"><i class="fa fa-check"></i><b>A.2.4</b> Linear and quadratic approximation</a></li>
<li class="chapter" data-level="A.2.5" data-path="20-refresher.html"><a href="20-refresher.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.2.5</b> Functions of matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="20-refresher.html"><a href="20-refresher.html#probability"><i class="fa fa-check"></i><b>A.3</b> Probability</a><ul>
<li class="chapter" data-level="A.3.1" data-path="20-refresher.html"><a href="20-refresher.html#law-of-large-numbers"><i class="fa fa-check"></i><b>A.3.1</b> Law of large numbers:</a></li>
<li class="chapter" data-level="A.3.2" data-path="20-refresher.html"><a href="20-refresher.html#jensens-inequality"><i class="fa fa-check"></i><b>A.3.2</b> Jensen’s inequality</a></li>
<li class="chapter" data-level="A.3.3" data-path="20-refresher.html"><a href="20-refresher.html#transformation-of-univariate-densities"><i class="fa fa-check"></i><b>A.3.3</b> Transformation of univariate densities</a></li>
<li class="chapter" data-level="A.3.4" data-path="20-refresher.html"><a href="20-refresher.html#normal-distribution"><i class="fa fa-check"></i><b>A.3.4</b> Normal distribution</a></li>
<li class="chapter" data-level="A.3.5" data-path="20-refresher.html"><a href="20-refresher.html#chi-squared-distribution"><i class="fa fa-check"></i><b>A.3.5</b> Chi-squared distribution</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="20-refresher.html"><a href="20-refresher.html#statistics"><i class="fa fa-check"></i><b>A.4</b> Statistics</a><ul>
<li class="chapter" data-level="A.4.1" data-path="20-refresher.html"><a href="20-refresher.html#statistical-learning"><i class="fa fa-check"></i><b>A.4.1</b> Statistical learning</a></li>
<li class="chapter" data-level="A.4.2" data-path="20-refresher.html"><a href="20-refresher.html#point-and-interval-estimation"><i class="fa fa-check"></i><b>A.4.2</b> Point and interval estimation</a></li>
<li class="chapter" data-level="A.4.3" data-path="20-refresher.html"><a href="20-refresher.html#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fa fa-check"></i><b>A.4.3</b> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span></a></li>
<li class="chapter" data-level="A.4.4" data-path="20-refresher.html"><a href="20-refresher.html#asymptotics-1"><i class="fa fa-check"></i><b>A.4.4</b> Asymptotics</a></li>
<li class="chapter" data-level="A.4.5" data-path="20-refresher.html"><a href="20-refresher.html#confidence-intervals"><i class="fa fa-check"></i><b>A.4.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="A.4.6" data-path="20-refresher.html"><a href="20-refresher.html#symmetric-normal-confidence-interval"><i class="fa fa-check"></i><b>A.4.6</b> Symmetric normal confidence interval</a></li>
<li class="chapter" data-level="A.4.7" data-path="20-refresher.html"><a href="20-refresher.html#confidence-interval-for-chi-squared-distribution"><i class="fa fa-check"></i><b>A.4.7</b> Confidence interval for chi-squared distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="21-further-study.html"><a href="21-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="21-further-study.html"><a href="21-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="21-further-study.html"><a href="21-further-study.html#additional-references"><i class="fa fa-check"></i><b>B.2</b> Additional references</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="22-references.html"><a href="22-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Statistical Methods:<br />
Likelihood, Bayes and Regression</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimating-regression-coefficients" class="section level1">
<h1><span class="header-section-number">17</span> Estimating regression coefficients</h1>
<p>In this chapter we discuss various ways to estimate the regression coefficients. First, we discuss estimation by Ordinary Least Squares (OLS)
by minimising the residual sum of squares. This yields the famous
Gauss estimator. Second, we derive estimates of the regression coefficients
using the methods of maximum likelihood assuming normal errors. This also leads to the Gauss estimator. Third, we show that the coefficients in
linear regression can written and interpreted in terms of two
covariance matrices and that the Gauss estimator of the regression coefficients
is a plug-in estimator using the MLEs of these covariance matrices.
Furthermore, we show that the (population version) of the Gauss estimator
can also be derived by finding the best linear predictor and by conditioning.
Finally, we discuss special cases of regression coefficients and their relationship
to marginal correlation.</p>
<div id="ordinary-least-squares-ols-estimator-of-regression-coefficients" class="section level2">
<h2><span class="header-section-number">17.1</span> Ordinary Least Squares (OLS) estimator of regression coefficients</h2>
<p>Now we show the classic way (Gauss 1809; Legendre 1805) to <strong>estimate regression coefficients</strong> by the method of
<strong>ordinary least squares (OLS)</strong>.</p>
<p><em>Idea:</em> choose regression coefficients such as to <em>minimise</em> the <em>squared error</em> between observations and the prediction (=RSS, the Residual Sum of Squares, a function of <span class="math inline">\(\boldsymbol \beta\)</span>)</p>
<p><img src="fig/regression3-p1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>In data matrix notation (note we assume <span class="math inline">\(\beta_0=0\)</span> and thus <em>centered data</em> <span class="math inline">\(\boldsymbol X\)</span> and <span class="math inline">\(\boldsymbol y\)</span>):</p>
<p><span class="math display">\[\text{RSS}(\boldsymbol \beta)=(\boldsymbol y-\boldsymbol X\boldsymbol \beta)^T(\boldsymbol y-\boldsymbol X\boldsymbol \beta)=\boldsymbol \varepsilon^T\boldsymbol \varepsilon= \sum^{n}_{i=1}\epsilon^2_i\]</span></p>
<p><span class="math display">\[\widehat{\boldsymbol \beta}_{\text{OLS}}=\arg\min \text{RSS}(\boldsymbol \beta)\]</span></p>
<p><span class="math display">\[\text{RSS}(\boldsymbol \beta) = \boldsymbol y^T \boldsymbol y- 2 \boldsymbol \beta^T \boldsymbol X^T \boldsymbol y+ \boldsymbol \beta^T \boldsymbol X^T \boldsymbol X\boldsymbol \beta\]</span></p>
<p>Gradient:
<span class="math display">\[\nabla \text{RSS}(\boldsymbol \beta) = -2\boldsymbol X^T \boldsymbol y+ 2\boldsymbol X^T \boldsymbol X\boldsymbol \beta\]</span></p>
<p><span class="math display">\[\nabla \text{RSS}(\widehat{\boldsymbol \beta}) = 0 \longrightarrow \boldsymbol X^T \boldsymbol y= \boldsymbol X^T\boldsymbol X\widehat{\boldsymbol \beta}\]</span></p>
<p><span class="math display">\[\Longrightarrow \widehat{\boldsymbol \beta}_{\text{OLS}} = \left(\boldsymbol X^T\boldsymbol X\right)^{-1} \boldsymbol X^T \boldsymbol y\]</span></p>
<p>Note the similarities in the procedure to maximum likelihood (ML) estimation (with minimisation instead of maximisation)! In fact, as we
see nextm this is not by chance as OLS <em>is</em> indeed a special case of ML!
This also implies that OLS is generally a good method — but only if sample size <span class="math inline">\(n\)</span> is large!</p>
<p>The above Gauss’ estimator is fundamental in statistics so it is worthwile to memorise it!</p>
</div>
<div id="maximum-likelihood-estimation-of-regression-coefficients" class="section level2">
<h2><span class="header-section-number">17.2</span> Maximum likelihood estimation of regression coefficients</h2>
<p>We now show how to estimate regression coefficients using the method
of maximum likelihood. This is a second method to derive <span class="math inline">\(\hat{\boldsymbol \beta}\)</span>.</p>
<p>We recall the basic regression equation
<span class="math display">\[
y = \beta_0 + \boldsymbol \beta^T \boldsymbol x+ \epsilon
\]</span>
with <span class="math inline">\(\text{E}(\varepsilon)=0\)</span> and observed data <span class="math inline">\(y_1, \ldots, y_n\)</span> and <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n\)</span>.
The intercept is identified as
<span class="math display">\[
\beta_0 = \mu_{y}- \boldsymbol \beta^T \boldsymbol \mu_{\boldsymbol x}
\]</span>
so that we can solve for the noise variable
<span class="math display">\[
\varepsilon = (y- \mu_{y}) - \boldsymbol \beta^T (\boldsymbol x-\boldsymbol \mu_{\boldsymbol x})
\]</span></p>
<p>Assuming joint (multivariate) normality for the response <span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> we get as the MLEs for
the respective means and (co)variances:</p>
<ul>
<li><span class="math inline">\(\hat{\mu}_y=\hat{\text{E}(y)}= \frac{1}{n}\sum^n_{i=1} y_i\)</span></li>
<li><span class="math inline">\(\hat{\sigma}^2_y=\widehat{\text{Var}}(y)= \frac{1}{n}\sum^n_{i=1} (y_i - \hat{\mu}_y)^2\)</span></li>
<li><span class="math inline">\(\hat{\boldsymbol \mu}_{\boldsymbol x}=\hat{\text{E}}(\boldsymbol x)= \frac{1}{n}\sum^n_{i=1} \boldsymbol x_i\)</span></li>
<li><span class="math inline">\(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}=\widehat{\text{Var}}(\boldsymbol x)= \frac{1}{n}\sum^n_{i=1} (\boldsymbol x_i-\hat{\boldsymbol \mu}_{\boldsymbol x}) (\boldsymbol x_i-\hat{\boldsymbol \mu}_{\boldsymbol x})^T\)</span></li>
<li><span class="math inline">\(\hat{\boldsymbol \Sigma}_{\boldsymbol xy}=\widehat{\text{Cov}}(\boldsymbol x, y)= \frac{1}{n}\sum^n_{i=1} (\boldsymbol x_i-\hat{\boldsymbol \mu}_{\boldsymbol x}) (y_i - \hat{\mu}_y)\)</span></li>
</ul>
<p>The noise <span class="math inline">\(\varepsilon_i \sim N(0, \sigma^2_{\varepsilon})\)</span>
is normally distributed with mean 0 and variance <span class="math inline">\(\text{Var}(\varepsilon) = \sigma^2_{\varepsilon}\)</span>
which leads to the normal log-likelihood function:
<span class="math display">\[
\begin{split}
\log L  &amp;= -\frac{n}{2} \log \sigma^2_{\varepsilon} - \frac{1}{2\sigma^2_{\varepsilon}}{\sum^n_{i=1} \epsilon_i^2}\\
&amp;= -\frac{n}{2} \log \sigma^2_{\varepsilon} - \frac{1}{2\sigma^2_{\varepsilon}}\underbrace{\sum^n_{i=1} \left((y_i- \hat{\mu}_{y}) - \boldsymbol \beta^T (\boldsymbol x_i -\hat{\boldsymbol \mu}_{\boldsymbol x})\right)^2}_{\text{ = RSS($\boldsymbol \beta$)}}\\
\end{split}
\]</span></p>
<p>We now only need to maximise the log-likelihood to obtain MLEs of <span class="math inline">\(\sigma^2_{\varepsilon}\)</span> and <span class="math inline">\(\boldsymbol \beta\)</span>!</p>
<p>Note that the residual sum of squares (RSS) appears in the log-likelihood function
(with a minus sign), which implies that ML assuming normal distribution will recover
the OLS estimator for the regression coefficients! So OLS is a special case of ML !</p>
<div id="detailed-derivation-of-the-mles" class="section level3">
<h3><span class="header-section-number">17.2.1</span> Detailed derivation of the MLEs</h3>
<p>The gradient with regard to <span class="math inline">\(\boldsymbol \beta\)</span> is
<span class="math display">\[
\begin{split}
\nabla_{\boldsymbol \beta} \log L &amp;= \frac{1}{\sigma^2_{\varepsilon}} \sum^n_{i=1} \left((\boldsymbol x_i -\hat{\boldsymbol \mu}_{\boldsymbol x} ) (y_i - \hat{\mu}_{y}) - (\boldsymbol x_i -\hat{\boldsymbol \mu}_{\boldsymbol x} )(\boldsymbol x_i -\hat{\boldsymbol \mu}_{\boldsymbol x})^T \boldsymbol \beta\right)  \\
 &amp;= \frac{n}{\sigma^2_{\varepsilon}} \left( \hat{\boldsymbol \Sigma}_{\boldsymbol xy} -  \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}\boldsymbol \beta\right)\\
\end{split}
\]</span>
Setting this equal to zero yields the Gauss estimator
<span class="math display">\[
\hat{\boldsymbol \beta} = \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1} \hat{\boldsymbol \Sigma}_{\boldsymbol xy}  
\]</span>
By plugin we the get the MLE of <span class="math inline">\(\beta_0\)</span>
as
<span class="math display">\[
\hat{\beta}_0 = \hat{\mu}_{y}- \hat{\boldsymbol \beta}^T \hat{\boldsymbol \mu}_{\boldsymbol x}
\]</span>
Taking the derivate of <span class="math inline">\(\log L\)</span> with regard to <span class="math inline">\(\sigma^2_{\varepsilon}\)</span> results in
<span class="math display">\[
\frac{\partial}{\partial \sigma^2_{\varepsilon}} \log L = -\frac{n}{2\sigma^2_{\varepsilon}} +\frac{1}{2\sigma^4_{\varepsilon}} RSS(\beta)
\]</span>
which leads to the MLE
<span class="math display">\[
\hat{\sigma^2}_{\varepsilon} =\frac{RSS(\hat{\boldsymbol \beta})}{n}  = \frac{1}{n}\sum^n_{i=1}(y_i-\hat{y}_i)^2
\]</span>
with <span class="math inline">\(\hat{y}_i = \hat{\beta}_0 + \hat{\boldsymbol \beta}^T \boldsymbol x_i\)</span>.</p>
<p>Note that the MLE <span class="math inline">\(\widehat{\sigma^2}_{\varepsilon}\)</span> is a biased estimate of <span class="math inline">\(\sigma^2_{\varepsilon}\)</span>. The unbiased estimate is <span class="math inline">\(\frac{1}{n-d-1}\sum^n_{i=1}(y_i-\hat{y}_i)^2\)</span>, where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(\boldsymbol \beta\)</span>
(i.e. the number of predictors).</p>
</div>
<div id="asymptotics" class="section level3">
<h3><span class="header-section-number">17.2.2</span> Asymptotics</h3>
<p>The advantage of using maximum likelihood is that we also get the (asympotic) variance associated with each estimator and typically can also assume asymptotic normality.</p>
<p>Specifically, for <span class="math inline">\(\hat{\boldsymbol \beta}\)</span> we get via the observed Fisher information at the MLE
an asymptotic estimator of its variance
<span class="math display">\[\widehat{\text{Var}}(\widehat{\boldsymbol \beta})=\frac{1}{n} \hat{\sigma^2}_{\varepsilon}\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1}\]</span>
Similarly, for <span class="math inline">\(\hat{\beta}_0\)</span> we have
<span class="math display">\[
\widehat{\text{Var}}(\widehat{\beta}_0)=\frac{1}{n} \hat{\sigma^2}_{\varepsilon} (1 + \hat{\boldsymbol \mu}^T \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1} \hat{\boldsymbol \mu})
\]</span></p>
<p>For finite sample size <span class="math inline">\(n\)</span> with known <span class="math inline">\(\text{Var}(\varepsilon)\)</span> one can show that the variances are
<span class="math display">\[\text{Var}(\widehat{\boldsymbol \beta})=\frac{1}{n} \sigma^2_{\varepsilon}\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1}\]</span>
and
<span class="math display">\[
\text{Var}(\widehat{\beta}_0)=\frac{1}{n} \sigma^2_{\varepsilon} (1 + \hat{\boldsymbol \mu}^T_{\boldsymbol x} \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1} \hat{\boldsymbol \mu}_{\boldsymbol x})
\]</span>
and that the regression coefficients and the intercept are normally distributed according to
<span class="math display">\[
\widehat{\boldsymbol \beta} \sim N_d(\boldsymbol \beta, \text{Var}(\widehat{\boldsymbol \beta}))
\]</span>
and
<span class="math display">\[
\widehat{\beta}_0 \sim N(\beta_0, \text{Var}(\widehat{\beta}_0))
\]</span></p>
<p>We may use this to test whether whether <span class="math inline">\(\beta_j = 0\)</span> and <span class="math inline">\(\beta_0 = 0\)</span>.</p>
</div>
</div>
<div id="covariance-plug-in-estimator-of-regression-coefficients" class="section level2">
<h2><span class="header-section-number">17.3</span> Covariance plug-in estimator of regression coefficients</h2>
<p>We now try to understand regression coefficients in terms of covariances (thus obtaining a third way to compute and estimate them).</p>
<p>We recall that the Gauss regression coefficients are given by</p>
<p><span class="math display">\[\widehat{\boldsymbol \beta} = \left(\boldsymbol X^T\boldsymbol X\right)^{-1}\boldsymbol X^T \boldsymbol y\]</span>
where <span class="math inline">\(\boldsymbol X\)</span> is the <span class="math inline">\(n \times d\)</span> data matrix (in statistics convention)</p>
<p><span class="math display">\[\boldsymbol X= \begin{pmatrix} x_{11} &amp; \dots &amp; x_{1d} \\ \vdots &amp; \ddots &amp; \vdots \\ x_{n1} &amp; \dots &amp; x_{nd} \end{pmatrix}\]</span>
Note that we assume that the data matrix <span class="math inline">\(\boldsymbol X\)</span> is centered (i.e. column sums
<span class="math inline">\(\boldsymbol X^T \boldsymbol 1_n = \boldsymbol 0\)</span> are zero).</p>
<p>Likewise <span class="math inline">\(\boldsymbol y= (y_1, \ldots, y_n)^T\)</span> is the response data vector (also centered with <span class="math inline">\(\boldsymbol y^T \boldsymbol 1_n = 0\)</span>).</p>
<p>Noting that
<span class="math display">\[\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}=\frac{1}{n}(\boldsymbol X^T\boldsymbol X)\]</span>
is the MLE of covariance matrix among <span class="math inline">\(\boldsymbol x\)</span>
and
<span class="math display">\[\hat{\boldsymbol \Sigma}_{\boldsymbol xy}=\frac{1}{n}(\boldsymbol X^T \boldsymbol y)\]</span>
is the MLE of the covariance between <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(y\)</span>
we see that the OLS estimate of the regression coefficients can be expressed as
<span class="math display">\[\widehat{\boldsymbol \beta} = \left(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}\right)^{-1}\hat{\boldsymbol \Sigma}_{\boldsymbol xy}\]</span>
We can write down a population version (with no hats!): <span class="math display">\[\boldsymbol \beta= \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}\]</span></p>
<p>Thus, OLS regression coefficients can be interpreted as plugin estimator using MLEs of covariances!
In fact, we may also use the unbiased estimates since the scale factor (<span class="math inline">\(1/n\)</span> or <span class="math inline">\(1/(n-1)\)</span>) cancels out so it does not matter
which one you use!</p>
<div id="importance-of-positive-definiteness-of-estimated-covariance-matrix" class="section level3">
<h3><span class="header-section-number">17.3.1</span> Importance of positive definiteness of estimated covariance matrix</h3>
<p>Note that <span class="math inline">\(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}\)</span> is inverted in <span class="math inline">\(\widehat{\boldsymbol \beta} = \left(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}\right)^{-1}\hat{\boldsymbol \Sigma}_{\boldsymbol xy}\)</span>.</p>
<ul>
<li>Hence, the estimate <span class="math inline">\(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}\)</span> needs to be positive definite!</li>
<li>But <span class="math inline">\(\hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{\text{MLE}}\)</span> is only positive definite if <span class="math inline">\(n&gt;d\)</span>!</li>
</ul>
<p>Therefore we can use the ML estimate (empirical estimator) only for large <span class="math inline">\(n\)</span> &gt; <span class="math inline">\(d\)</span>,
otherwise we need to employ a different (regularised) estimation approach (e.g. Bayes or a penalised ML)!</p>
<p>Remark: writing <span class="math inline">\(\hat{\boldsymbol \beta}\)</span> explicitly based on covariance estimates has the advantage that
we can construct plug-in estimators of regression coefficient based on regularised
covariance estimators that improve over ML for small sample size.
This leads to the so-called SCOUT method (=covariance-regularized regression by Witten and Tibshirani, 2008).</p>
</div>
</div>
<div id="best-linear-predictor" class="section level2">
<h2><span class="header-section-number">17.4</span> Best linear predictor</h2>
<p>The <strong>best linear predictor</strong> is a fourth way to arrive at the linear model. This is closely related to OLS and minimising squared residual error.</p>
<p>Without assuming normality the above multiple regression model
can be shown to be optimal linear predictor under the minimum mean squared prediction error:</p>
<p>Assumptions:</p>
<ul>
<li><span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> are random variables</li>
<li>we construct a new variable (the linear predictor)
<span class="math inline">\(y^{\star\star} = b_0 + \boldsymbol b^T \boldsymbol x\)</span> to optimally approximate <span class="math inline">\(y\)</span></li>
</ul>
<p>Aim:</p>
<ul>
<li>choose <span class="math inline">\(b_0\)</span> and <span class="math inline">\(\boldsymbol b\)</span> such to minimize the mean squared prediction error
<span class="math inline">\(\text{E}( (y - y^{\star\star})^2 )\)</span></li>
</ul>
<div id="result" class="section level3">
<h3><span class="header-section-number">17.4.1</span> Result:</h3>
<p>The mean squared prediction error <span class="math inline">\(MSPE\)</span> in dependence of <span class="math inline">\((b_0, \boldsymbol b)\)</span> is
<span class="math display">\[
\begin{split}
\text{E}( (y - y^{\star\star} )^2) &amp; = \text{Var}(y - y^{\star\star}) + \text{E}(y - y^{\star\star})^2 \\
  &amp; = \text{Var}(y - b_0 -\boldsymbol b^T \boldsymbol x) + ( \text{E}(y) -b_0 - \boldsymbol b^T \text{E}(\boldsymbol x) )^2 \\
 &amp; = \sigma^2_y + \text{Var}(\boldsymbol b^T \boldsymbol x) + 2 \, \text{Cov}(y, -\boldsymbol b^T \boldsymbol x)  + ( \mu_y -b_0 - \boldsymbol b^T \boldsymbol \mu_{\boldsymbol x} )^2 \\
  &amp; = \sigma^2_y + \boldsymbol b^T \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol b- 2 \, \boldsymbol b^T \boldsymbol \Sigma_{\boldsymbol xy} + ( \mu_y -b_0 - \boldsymbol b^T \boldsymbol \mu_{\boldsymbol x} )^2 \\
  &amp; = MSPE(b_0, \boldsymbol b) \\
\end{split}
\]</span></p>
<p>We look for
<span class="math display">\[
(\beta_0, \boldsymbol \beta) = \underset{b_0,\boldsymbol b}{\arg\min} \,\, MSPE(b_0, \boldsymbol b)
\]</span></p>
<p>In order to find the minimum we compute the gradient with regard to <span class="math inline">\((b_0, \boldsymbol b)\)</span>
<span class="math display">\[
\nabla MSPE = 
\begin{pmatrix} 
-2( \mu_y -b_0 - \boldsymbol b^T \boldsymbol \mu_{\boldsymbol x} ) \\
2 \, \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol b- 2 \, \boldsymbol \Sigma_{\boldsymbol xy} -2 \boldsymbol \mu_{\boldsymbol x} (\mu_y -b_0 - \boldsymbol b^T \boldsymbol \mu_{\boldsymbol x}) \\
\end{pmatrix}
\]</span>
and setting this equal to zero yields
<span class="math display">\[
\begin{pmatrix}
\beta_0\\
\boldsymbol \beta\\
\end{pmatrix}
= 
\begin{pmatrix}
\mu_y- \boldsymbol \beta^T \boldsymbol \mu_{\boldsymbol x} \\
\boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}\\
\end{pmatrix}
\]</span>
Thus, the optimal values for <span class="math inline">\(b_0\)</span> and <span class="math inline">\(\boldsymbol b\)</span> in the best linear predictor
correspond to the previously derived coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\boldsymbol \beta\)</span>!</p>
</div>
<div id="irreducible-error" class="section level3">
<h3><span class="header-section-number">17.4.2</span> Irreducible Error</h3>
<p>The minimum achieved MSPE (=<strong>irreducible error</strong>) is
<span class="math display">\[
MSPE(\beta_0,\boldsymbol \beta)
= \sigma^2_{y} - \boldsymbol \beta^T \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol \beta= \sigma^2_{y} -  \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy} 
\]</span>
With the abbreviation
<span class="math inline">\(\Omega^2 = \boldsymbol P_{y \boldsymbol x} \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy} = \sigma_y^{-2} \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}\)</span>
we can simplify this to
<span class="math display">\[
MSPE(\beta_0,\boldsymbol \beta)
=  \sigma^2_y (1-\Omega^2) = \text{Var}(\varepsilon)
\]</span></p>
<p>Writing <span class="math inline">\(b_0=\beta_0 + \Delta_0\)</span> and <span class="math inline">\(\boldsymbol b= \boldsymbol \beta+ \boldsymbol \Delta\)</span> it is easy to see that the
mean squared predictive error is a quadratic function around the minimum:
<span class="math display">\[
MSPE(\beta_0 + \Delta_0, \boldsymbol \beta+ \boldsymbol \Delta) = \text{Var}(\varepsilon) + \Delta_0^2 + \boldsymbol \Delta^T \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol \Delta
\]</span></p>
<p>Note that usually <span class="math inline">\(y^{\star} = \beta_0 + \boldsymbol \beta^T \boldsymbol x\)</span> does not perfectly approximate <span class="math inline">\(y\)</span> so there <em>will</em> be an irreducible error (= noise variance)
<span class="math display">\[\text{Var}(\varepsilon) =\sigma^2_y (1-\Omega^2) &gt; 0\]</span>
which implies <span class="math inline">\(\Omega^2 &lt; 1\)</span>.</p>
<p>The quantity <span class="math inline">\(\Omega^2\)</span> has a further interpretation of the population version of
as the squared multiple correlation coefficient between the response and the predictors
and plays a vital role in decomposition of variance, as discussed later.</p>
</div>
</div>
<div id="regression-by-conditioning" class="section level2">
<h2><span class="header-section-number">17.5</span> Regression by conditioning</h2>
<p><strong>Conditioning</strong> is a fifth way to arrive at the linear model. This is also the most general way and can be used to derive many other regression models
(not just the simple linear model).</p>
<div id="general-idea" class="section level3">
<h3><span class="header-section-number">17.5.1</span> General idea:</h3>
<ul>
<li>two random variables <span class="math inline">\(y\)</span> (response, scalar) and <span class="math inline">\(\boldsymbol x\)</span> (predictor variables, vector)</li>
<li>we assume that <span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> have a joint distribution <span class="math inline">\(F_{y,\boldsymbol x}\)</span></li>
<li>compute <em>conditional</em> random variable <span class="math inline">\(y | \boldsymbol x\)</span> and
the corresponding distribution <span class="math inline">\(F_{y | \boldsymbol x}\)</span></li>
</ul>
</div>
<div id="multivariate-normal-assumption" class="section level3">
<h3><span class="header-section-number">17.5.2</span> Multivariate normal assumption</h3>
<p>Now we assume that <span class="math inline">\(y\)</span> and <span class="math inline">\(\boldsymbol x\)</span> are (jointly) multivariate normal. Then the conditional distribution <span class="math inline">\(F_{y | \boldsymbol x}\)</span> is a univariate normal with the following moments (you can verify this by looking up the general conditional multivariate normal distribution):</p>
<p><strong>a) Conditional expectation:</strong></p>
<p><span class="math display">\[ \text{E}( y | \boldsymbol x) = y^{\star} = \beta_0 + \boldsymbol \beta^T \boldsymbol x\]</span></p>
<p>with coefficients <span class="math inline">\(\boldsymbol \beta= \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1}\boldsymbol \Sigma_{\boldsymbol xy}\)</span> and
intercept <span class="math inline">\(\beta_0 = \mu_{y} - \boldsymbol \beta^T \boldsymbol \mu_{\boldsymbol x}\)</span> .</p>
<p>Note that as <span class="math inline">\(y^{\star}\)</span> depends on <span class="math inline">\(\boldsymbol x\)</span> it is a random variable itself with
mean
<span class="math display">\[\text{E}(y^{\star}) = \beta_0 + \boldsymbol \beta^T  \boldsymbol \mu_{\boldsymbol x} = \mu_{y}\]</span> and variance <span class="math display">\[\text{Var}(y^{\star})= \text{Var}(\text{E}( y | \boldsymbol x)) = \boldsymbol \beta^T \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol \beta= \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy} = \sigma^2_y \boldsymbol P_{y \boldsymbol x} \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy} = \sigma_y^2 \Omega^2\]</span>.</p>
<p><strong>b) Conditional variance:</strong></p>
<p><span class="math display">\[\text{Var}( y | \boldsymbol x) =\sigma^2_y - \boldsymbol \beta^T \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol \beta= \sigma^2_y - \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}  = \sigma^2_y (1-\Omega^2)\]</span>
Note this is a constant so <span class="math inline">\(\text{E}(\text{Var}( y | \boldsymbol x)) = \sigma^2_y (1-\Omega^2)\)</span> as well.</p>
</div>
</div>
<div id="standardised-regression-coefficients-and-relationship-to-correlation" class="section level2">
<h2><span class="header-section-number">17.6</span> Standardised regression coefficients and relationship to correlation</h2>
<p>First we note that we can decompose regression coefficients into the product
of marginal correlations and correlations among predictors.</p>
<p>Using the variance-correlation decompositions <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}= \boldsymbol V_{\boldsymbol x}^{1/2} \boldsymbol P_{\boldsymbol x\boldsymbol x} \boldsymbol V_{\boldsymbol x}^{1/2}\)</span> and <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol xy}= \boldsymbol V_{\boldsymbol x}^{1/2} \boldsymbol P_{\boldsymbol xy} \sigma_y\)</span> we rewrite the regression coefficients as
<span class="math display">\[
\boldsymbol \beta= {\underbrace{\boldsymbol V_{\boldsymbol x}^{-1/2}}_{\text{(inverse) scale of } x_i}} \,\, {\underbrace{\boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1}}_{\text{ (inverse) correlation among predictors }}} \underbrace{\boldsymbol P_{\boldsymbol xy}}_{\text{ marginal correlations}}\,\,
\underbrace{\sigma_y}_{\text{ scale of }y}
\]</span>
Thus the regression coefficients <span class="math inline">\(\boldsymbol \beta\)</span> contain the scale of the variables, and
take into account the correlations among the predictors (<span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol x}\)</span>)
in addition to the marginal correlations between the response <span class="math inline">\(y\)</span> and the predictors <span class="math inline">\(x_i\)</span> (<span class="math inline">\(\boldsymbol P_{\boldsymbol xy}\)</span>).</p>
<p>This decomposition allows to understand a number special cases when the regression coefficient simplify further:</p>
<ol style="list-style-type: lower-alpha">
<li><p>If the response and the predictors are standardised to have variance one, i.e. <span class="math inline">\(\text{Var}(y)=1\)</span> and <span class="math inline">\(\text{Var}(x_i)\)</span>, then <span class="math inline">\(\boldsymbol \beta\)</span> becomes equal to the
<strong>standardised regression coefficients</strong>
<span class="math display">\[\boldsymbol \beta_{\text{std}} = \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy}\]</span>
Note that standardised regression coefficients do not make use of variances and and thus are scale-independent.</p></li>
<li><p>If there is no correlation among the predictors , i.e. <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol x} = \boldsymbol I\)</span>
the the regression coefficients reduce to
<span class="math display">\[\boldsymbol \beta= \boldsymbol V_{\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}\]</span>
where <span class="math inline">\(\boldsymbol V_{\boldsymbol x}\)</span> is a diagonal matrix containing the variances oft the predictors.
This is also called <strong>marginal regression</strong>. Note that the inversion of <span class="math inline">\(\boldsymbol V_{\boldsymbol x}\)</span>
is trival since you only need to invert each diagonal element individually.</p></li>
<li><p>If both a) and b) apply simultaneously (i.e. there is no correlation among predictors and response and predictors and predictors
are standardised) then
the regression coefficients simplify even further to
<span class="math display">\[
\boldsymbol \beta= \boldsymbol P_{\boldsymbol xy}
\]</span>
Thus, in this very special case the regression coefficients are identical to the correlations between the response and the predictors!</p></li>
</ol>

<p></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="16-regression2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="18-regression4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math20802-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
