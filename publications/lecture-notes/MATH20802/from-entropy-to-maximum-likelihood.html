<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>2 From entropy to maximum likelihood | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="2 From entropy to maximum likelihood | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="2 From entropy to maximum likelihood | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="2.1 Entropy  2.1.1 Overview In this chapter we discuss various information criteria and their connection to maximum likelihood. The modern definition of (relative) entropy, or “disorder”, was...">
<meta property="og:description" content="2.1 Entropy  2.1.1 Overview In this chapter we discuss various information criteria and their connection to maximum likelihood. The modern definition of (relative) entropy, or “disorder”, was...">
<meta name="twitter:description" content="2.1 Entropy  2.1.1 Overview In this chapter we discuss various information criteria and their connection to maximum likelihood. The modern definition of (relative) entropy, or “disorder”, was...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="active" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="conditioning-and-bayes-rule.html"><span class="header-section-number">7</span> Conditioning and Bayes rule</a></li>
<li><a class="" href="models-with-latent-variables-and-missing-data.html"><span class="header-section-number">8</span> Models with latent variables and missing data</a></li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li><a class="" href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li><a class="" href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">14</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">15</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">16</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">17</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">18</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="from-entropy-to-maximum-likelihood" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> From entropy to maximum likelihood<a class="anchor" aria-label="anchor" href="#from-entropy-to-maximum-likelihood"><i class="fas fa-link"></i></a>
</h1>
<div id="entropy" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Entropy<a class="anchor" aria-label="anchor" href="#entropy"><i class="fas fa-link"></i></a>
</h2>
<div id="overview" class="section level3" number="2.1.1">
<h3>
<span class="header-section-number">2.1.1</span> Overview<a class="anchor" aria-label="anchor" href="#overview"><i class="fas fa-link"></i></a>
</h3>
<p>In this chapter we discuss various information criteria and their connection to maximum likelihood.</p>
<p>The modern definition of (relative) entropy, or “disorder”, was first discovered in the 1870s by physicist <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann">L. Boltzmann (1844–1906)</a>
in the context of thermodynamics. The probabilistic interpretation of statistical mechanics and
entropy was further developed by <a href="https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs">J. W. Gibbs (1839–1903)</a>.</p>
<p>In the 1940–1950’s the notion of entropy turned out to be central in information theory, a field pioneered by mathematicians such as
<a href="https://en.wikipedia.org/wiki/Ralph_Hartley">R. Hartley (1988–1970)</a>,
<a href="https://en.wikipedia.org/wiki/Solomon_Kullback">S. Kullback (1907–1994)</a>,
<a href="https://en.wikipedia.org/wiki/Richard_Leibler">R. Leibler (1914–2003)</a>,
<a href="https://en.wikipedia.org/wiki/Alan_Turing">A. Turing (1912–1954)</a>,
<a href="https://en.wikipedia.org/wiki/I._J._Good">I. J. Good (1916–2009)</a>,
<a href="https://en.wikipedia.org/wiki/Claude_Shannon">C. Shannon (1916–2001)</a>, and
<a href="https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes">E. T. Jaynes (1922–1998)</a>,
and later further explored by
<a href="https://de.wikipedia.org/wiki/Shun%E2%80%99ichi_Amari">S. Amari (1936–)</a>,
<a href="https://en.wikipedia.org/wiki/Imre_Csisz%C3%A1r">I. Ciszár (1938–)</a>,
<a href="https://de.wikipedia.org/wiki/Bradley_Efron">B. Efron (1938–)</a>,
<a href="https://en.wikipedia.org/wiki/Philip_Dawid">A. P. Dawid (1946–)</a> and
many others.</p>
<p><span class="math display">\[\begin{align*}
\left.
\begin{array}{cc}
\\
\textbf{Entropy} \\
\\
\end{array}
\right.
\left.
\begin{array}{cc}
\\
\nearrow  \\
\searrow  \\
\\
\end{array}
\right.
\begin{array}{ll}
\text{Shannon Entropy} \\
\\
\text{Relative Entropy}  \\
\end{array}
\begin{array}{ll}
\text{(Shannon 1948)} \\
\\
\text{(Kullback-Leibler 1951)}  \\
\end{array}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\left.
\begin{array}{ll}
\text{Fisher information} \\
\\
\text{Mutual Information} \\
\end{array}
\right.
\begin{array}{ll}
\rightarrow\text{ Likelihood theory} \\
\\
\rightarrow\text{ Information theory} \\
\end{array}
\begin{array}{ll}
\text{(Fisher 1922)} \\
\\
\text{(Shannon 1948, Lindley 1953)}  \\
\end{array}
\end{align*}\]</span></p>
</div>
<div id="surprise-surprisal-or-shannon-information" class="section level3" number="2.1.2">
<h3>
<span class="header-section-number">2.1.2</span> Surprise, surprisal or Shannon information<a class="anchor" aria-label="anchor" href="#surprise-surprisal-or-shannon-information"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>surprise</strong> to observe an event of probability <span class="math inline">\(p\)</span> is <strong>defined</strong> as <span class="math inline">\(-\log(p)\)</span>.
This is also called <strong>surprisal</strong> or <strong>Shannon information</strong>.</p>
<p>Thus, the surprise to observe a certain event (with <span class="math inline">\(p=1\)</span>) is zero,
and conversely the surprise to observe an event that is certain not to happen
(with <span class="math inline">\(p=0\)</span>) is infinite.</p>
<p>The <strong>log-odds ratio</strong> can be viewed as the difference of the surprise of an event and the surprise of the complementary event:
<span class="math display">\[
\log\left( \frac{p}{1-p} \right) =  -\log(1-p) - ( -\log(p))
\]</span></p>
<p>In this module we always use the <em>natural logarithm</em> by default, and will explicitly write <span class="math inline">\(\log_2\)</span> and <span class="math inline">\(\log_{10}\)</span> for logarithms with respect to base 2 and 10, respectively.</p>
<p>Surprise and entropy computed with the natural logarithm (<span class="math inline">\(\log\)</span>) is given in “nats” (=<a href="https://en.wikipedia.org/wiki/Nat_(unit)">natural information units</a> ). Using <span class="math inline">\(\log_2\)</span> leads to “bits” and using <span class="math inline">\(\log_{10}\)</span> to “ban” or “<a href="https://en.wikipedia.org/wiki/Hartley_(unit)">Hartley</a>”.</p>
</div>
<div id="shannon-entropy" class="section level3" number="2.1.3">
<h3>
<span class="header-section-number">2.1.3</span> Shannon entropy<a class="anchor" aria-label="anchor" href="#shannon-entropy"><i class="fas fa-link"></i></a>
</h3>
<p>Assume we have a categorical distribution <span class="math inline">\(P\)</span> with <span class="math inline">\(K\)</span> classes/categories. The corresponding
class probabilities are <span class="math inline">\(p_1, \ldots, p_K\)</span> with <span class="math inline">\(\text{Pr}(\text{"class k"}) = p_k\)</span>
and <span class="math inline">\(\sum_{k=1}^K p_k= 1\)</span>. The probability mass function (PMF)
is <span class="math inline">\(p(x = \text{"class k"})= p_k\)</span>.</p>
<p>As the random variable <span class="math inline">\(x\)</span> is discrete the categorical distribution
<span class="math inline">\(P\)</span> is a discrete distribution but <span class="math inline">\(P\)</span> is generally also known as <em>the</em> discrete distribution.</p>
<p>The <strong>Shannon entropy</strong> (1948)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Shannon, C. E. 1948. A mathematical theory of communication. Bell System Technical Journal &lt;strong&gt;27&lt;/strong&gt;:379–423. &lt;a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x" class="uri"&gt;https://doi.org/10.1002/j.1538-7305.1948.tb01338.x&lt;/a&gt;&lt;/p&gt;'><sup>1</sup></a> of the distribution <span class="math inline">\(P\)</span> is defined as the
<strong>expected surprise</strong>, i.e. the negative expected log-probability
<span class="math display">\[
\begin{split}
H(P) &amp;=-\text{E}_P\left(\log p(x)\right) \\
     &amp;= - \sum_{k=1}^{K}p_k \log(p_k) \\
\end{split}
\]</span>
As all <span class="math inline">\(p_k \in [0,1]\)</span> by construction Shannon entropy must be larger or equal to 0.</p>
<p>Furthermore, it is bounded above by <span class="math inline">\(\log K\)</span>. This can be seen by maximising Shannon entropy
as a function with regard to the <span class="math inline">\(p_k\)</span> under the constraint <span class="math inline">\(\sum_{k=1}^K p_k= 1\)</span>, e.g., by constrained optimisation using Langrange
multipliers. The maximum is achieved for <span class="math inline">\(P\)</span> being the discrete uniform - see Example <a href="from-entropy-to-maximum-likelihood.html#exm:entropyuniform">2.1</a>.</p>
<p>Hence for any categorical distribution <span class="math inline">\(P\)</span> with <span class="math inline">\(K\)</span> categories we have
<span class="math display">\[\log K \geq  H(P) \geq 0\]</span></p>
<p>In statistical physics, the Shannon entropy is known as <a href="https://en.wikipedia.org/wiki/Entropy_(statistical_thermodynamics)#Gibbs_entropy_formula">Gibbs entropy (1878)</a>.</p>
<div class="example">
<p><span id="exm:entropyuniform" class="example"><strong>Example 2.1  </strong></span><strong>Discrete uniform distribution</strong> <span class="math inline">\(U_K\)</span>: let <span class="math inline">\(p_1=p_2= \ldots = p_K = \frac{1}{K}\)</span>.
Then <span class="math display">\[H(U_K) = - \sum_{k=1}^{K}\frac{1}{K} \log\left(\frac{1}{K}\right) = \log K\]</span></p>
<p>Note this is the largest value the Shannon entropy can assume
with <span class="math inline">\(K\)</span> classes.</p>
</div>
<div class="example">
<p><span id="exm:entropyconcentrated" class="example"><strong>Example 2.2  </strong></span><strong>Concentrated probability mass</strong>: let
<span class="math inline">\(p_1=1\)</span> and <span class="math inline">\(p_2=p_3=\ldots=p_K=0\)</span>.
Using <span class="math inline">\(0\times\log(0)=0\)</span> we obtain for the Shannon entropy
<span class="math display">\[H(P) = 1\times\log(1) + 0\times\log(0) + \dots = 0\]</span></p>
<p>Note that 0 is the smallest value that Shannon entropy can assume, and corresponds to maximum concentration.</p>
</div>
<p>Thus, <strong>large entropy</strong> implies that the <strong>distribution is spread out</strong> whereas <strong>small entropy</strong>
means the <strong>distribution is concentrated</strong>.</p>
<p>Correspondingly, maximum entropy distributions can be considered minimally informative about a random variable.</p>
<p>This interpretation is also supported by the close link of Shannon entropy with multinomial coefficients counting the permutations of <span class="math inline">\(n\)</span> items (samples) of <span class="math inline">\(K\)</span> distinct types (classes).</p>
<div class="example">
<p><span id="exm:entropymultinomial" class="example"><strong>Example 2.3  </strong></span>Large sample asymptotics of the log-multinomial coefficient and link to Shannon entropy:</p>
<p>The number of possible permutation of <span class="math inline">\(n\)</span> items of <span class="math inline">\(K\)</span> distinct types, with <span class="math inline">\(n_1\)</span> of type 1, <span class="math inline">\(n_2\)</span> of type 2 and so on, is given by the multinomial coefficient
<span class="math display">\[
W = \binom{n}{n_1, \ldots, n_K} = \frac {n!}{n_1! \times n_2! \times\ldots \times n_K! } 
\]</span>
with <span class="math inline">\(\sum_{k=1}^K n_k = n\)</span> and <span class="math inline">\(K \leq n\)</span>.</p>
<p>Now recall the Moivre-Sterling formula which for large <span class="math inline">\(n\)</span> allow to approximate
the factorial by
<span class="math display">\[
\log n! \approx  n \log n  -n 
\]</span></p>
<p>With this
<span class="math display">\[
\begin{split}
\log W &amp;= \log \binom{n}{n_1, \ldots, n_K} \\
       &amp;= \log n! - \sum_{k=1}^K \log n_k!\\
&amp; \approx    n \log n  -n - \sum_{k=1}^K (n_k \log n_k  -n_k) \\
&amp; = n \log n - \sum_{k=1}^K n_k \log n_k\\
&amp; = \sum_{k=1}^K n_k \log n - \sum_{k=1}^K n_k \log n_k\\
&amp; = - n \sum_{k=1}^K \frac{n_k}{n} \log\left( \frac{n_k}{n} \right)\\
\end{split}
\]</span>
and thus
<span class="math display">\[
\begin{split}
\frac{1}{n}\log \binom{n}{n_1, \ldots, n_K} &amp;\approx - \sum_{k=1}^K \hat{p}_k \log \hat{p}_k \\
&amp;=H(\hat{P})
\end{split}
\]</span>
where <span class="math inline">\(\hat{P}\)</span> is the empirical categorical distribution with <span class="math inline">\(\hat{p}_k = \frac{n_k}{n}\)</span>.</p>
<p>The combinatorical derivation of Shannon entropy is now credited to Wallis (1962)
but has already been used a century earlier by Boltzmann (1877) who discovered it in his work
in statistical mechanics (recall <span class="math inline">\(S = k_b \log W\)</span> is the
<a href="https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula">Boltzmann entropy</a> ).</p>
</div>
</div>
<div id="differential-entropy" class="section level3" number="2.1.4">
<h3>
<span class="header-section-number">2.1.4</span> Differential entropy<a class="anchor" aria-label="anchor" href="#differential-entropy"><i class="fas fa-link"></i></a>
</h3>
<p>Shannon entropy is only defined for discrete random variables.</p>
<p><em>Differential Entropy</em> results from applying the
definition of Shannon entropy to a <em>continuous</em> random variable <span class="math inline">\(x\)</span> with density <span class="math inline">\(p(x)\)</span>:
<span class="math display">\[
H(P) = -\text{E}_P(\log p(x)) = - \int_x p(x) \log p(x) \, dx
\]</span>
Despite having essentially the same formula the different name is justified because differential entropy exhibits different properties compared to Shannon entropy, because the logarithm is taken of a density
which in contrast to a probability can assumes values larger than one.
As a consequence, differential entropy is <em>not</em> bounded below by zero and can be negative.</p>
<div class="example">
<p><span id="exm:entropyunif" class="example"><strong>Example 2.4  </strong></span>Consider the uniform distribution <span class="math inline">\(U(0, a)\)</span> with <span class="math inline">\(a&gt;0\)</span>, support from <span class="math inline">\(0\)</span> to <span class="math inline">\(a\)</span> and density <span class="math inline">\(p(x) = 1/a\)</span>. As <span class="math inline">\(-\int_0^a p(x) \log p(x) dx =- \int_0^a \frac{1}{a} \log(\frac{1}{a}) dx = \log a\)</span>
the differential entropy is
<span class="math display">\[H( U(0, a) ) =  \log a \,.\]</span>
Note that for <span class="math inline">\(a &lt; 1\)</span> the differential entropy is negative.</p>
</div>
<div class="example">
<p><span id="exm:entropynormal" class="example"><strong>Example 2.5  </strong></span>The log density of the univariate normal <span class="math inline">\(N(\mu, \sigma^2)\)</span> distribution
is <span class="math inline">\(\log p(x |\mu, \sigma^2) = -\frac{1}{2} \left( \log(2\pi\sigma^2) + \frac{(x-\mu)^2}{\sigma^2} \right)\)</span> with <span class="math inline">\(\sigma^2 &gt; 0\)</span>.
The corresponding differential entropy is with <span class="math inline">\(\text{E}((x-\mu)^2) = \sigma^2\)</span>
<span class="math display">\[
\begin{split}
H(P) &amp; = -\text{E}\left( \log p(x |\mu, \sigma^2) \right)\\
&amp; = \frac{1}{2} \left( \log(2 \pi \sigma^2)+1\right) \,. \\
\end{split}
\]</span>
Interestingly, <span class="math inline">\(H(P)\)</span> only depends on the variance and not on the mean, and the entropy grows with the variance. Note that for <span class="math inline">\(\sigma^2 &lt; 1/(2 \pi e) \approx 0.0585\)</span> the differential entropy is negative.</p>
</div>
</div>
<div id="maximum-entropy-principle-to-characterise-distributions" class="section level3" number="2.1.5">
<h3>
<span class="header-section-number">2.1.5</span> Maximum entropy principle to characterise distributions<a class="anchor" aria-label="anchor" href="#maximum-entropy-principle-to-characterise-distributions"><i class="fas fa-link"></i></a>
</h3>
<p>Both maximum Shannon entropy and differential entropy are useful to characterise distributions:</p>
<ol style="list-style-type: decimal">
<li><p>The <strong>discrete
uniform distribution</strong> is the <strong>maximum entropy distribution</strong> among all discrete distributions.</p></li>
<li><p>the maximum entropy distribution of a continuous random variable with support <span class="math inline">\([-\infty, \infty]\)</span> with a specific mean and variance is the normal distribution</p></li>
<li><p>the maximum entropy distribution among all continuous distributions supported in <span class="math inline">\([0, \infty]\)</span> with a specified mean is the exponential distribution.</p></li>
</ol>
<p>The higher the entropy the more spread out (and more uninformative) is a distribution.</p>
<p>Using maximum entropy to characterise maximally uniformative distributions
was advocated by E.T. Jaynes (who also proposed to use maximum entropy in the context of finding Bayesian priors). The maximum entropy principle in statistical physics goes back to Boltzmann.</p>
<p>A list of maximum entropy distribution is given here:
<a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution" class="uri">https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution</a> .</p>
<p>Many distributions commonly used in statistical modelling are exponential families.
Intriguingly, these distribution are all maximum entropy distributions, so there is a very close link
between the principle of maximum entropy and common model choices in statistics and machine learning.</p>
</div>
<div id="cross-entropy" class="section level3" number="2.1.6">
<h3>
<span class="header-section-number">2.1.6</span> Cross-entropy<a class="anchor" aria-label="anchor" href="#cross-entropy"><i class="fas fa-link"></i></a>
</h3>
<p>If in the definition of Shannon entropy (and differential entropy) the expectation over
the log-density (say <span class="math inline">\(g(x)\)</span> of distribution <span class="math inline">\(G\)</span>) is with regard to a different distribution <span class="math inline">\(F\)</span> over the same state space
we arrive at the <strong>cross-entropy</strong>
<span class="math display">\[
H(F, G) =-\text{E}_F\left( \log g(x)  \right)
\]</span>
For discrete distributions <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> with class probabilities
<span class="math inline">\(f_1, \ldots, f_K\)</span> and <span class="math inline">\(g_1, \ldots, g_K\)</span> the cross-entropy is computed as the weighted sum <span class="math inline">\(H(F, G) = - \sum_{k=1}^{K} f_k \log g_k\)</span>.
For continuous distributions <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> with densities <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g(x)\)</span> we compute the integral
<span class="math inline">\(H(F, G) =- \int_x f(x) \log g(x) \, dx\)</span>.</p>
<p>Therefore, cross-entropy is a measure linking two distributions <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span>.</p>
<p>Note that</p>
<ul>
<li>cross-entropy is not symmetric with regard to <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span>, because
the expectation is taken with reference to <span class="math inline">\(F\)</span>.</li>
<li>By construction <span class="math inline">\(H(F, F) = H(F)\)</span>. Thus if both distributions are identical cross-entropy reduces to Shannon and differential entropy, respectively.</li>
</ul>
<p>A crucial property of the cross-entropy <span class="math inline">\(H(F, G)\)</span> is that it is bounded below by the entropy of <span class="math inline">\(F\)</span>, therefore
<span class="math display">\[
H(F, G) \geq H(F)
\]</span>
with equality for <span class="math inline">\(F=G\)</span>. This is known as <a href="https://en.wikipedia.org/wiki/Gibbs%27_inequality"><strong>Gibbs’ inequality</strong></a>.</p>
<p>Equivalently we can write
<span class="math display">\[
 \underbrace{H(F, G)-H(F)}_{\text{relative entropy}} \geq 0
\]</span><br>
In fact, this recalibrated cross-entropy (known as KL divergence or relative entropy turns out to be more fundamental than both cross-entropy and entropy. It will be studied in detail in the next section.</p>
<div class="example">
<p><span id="exm:crossentropynormals" class="example"><strong>Example 2.6  </strong></span>Cross-entropy between two normals:</p>
<p>Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2_{\text{ref}})\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>.
The cross-entropy <span class="math inline">\(H(F_{\text{ref}}, F)\)</span> is
<span class="math display">\[
\begin{split}
H(F_{\text{ref}}, F) &amp;=  -\text{E}_{F_{\text{ref}}} \left( \log p(x |\mu, \sigma^2) \right)\\
&amp;=  \frac{1}{2}  \text{E}_{F_{\text{ref}}} \left(  \log(2\pi\sigma^2)  + \frac{(x-\mu)^2}{\sigma^2} \right) \\
&amp;= \frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 } 
 +\frac{\sigma^2_{\text{ref}}}{\sigma^2}  +\log(2 \pi \sigma^2) \right)  \\
\end{split}
\]</span>
using <span class="math inline">\(\text{E}_{F_{\text{ref}}} ((x-\mu)^2) = (\mu_{\text{ref}}-\mu)^2 + \sigma^2_{\text{ref}}\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:crossentropylowerbound" class="example"><strong>Example 2.7  </strong></span>If <span class="math inline">\(\mu_{\text{ref}} = \mu\)</span> and <span class="math inline">\(\sigma^2_{\text{ref}} = \sigma^2\)</span>
then the cross-entropy <span class="math inline">\(H(F_{\text{ref}},F)\)</span> in Example <a href="from-entropy-to-maximum-likelihood.html#exm:crossentropynormals">2.6</a>
degenerates to the differential entropy
<span class="math inline">\(H(F_{\text{ref}}) = \frac{1}{2} \left(\log( 2 \pi \sigma^2_{\text{ref}}) +1 \right)\)</span>.</p>
</div>
</div>
</div>
<div id="kullback-leibler-divergence" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Kullback-Leibler divergence<a class="anchor" aria-label="anchor" href="#kullback-leibler-divergence"><i class="fas fa-link"></i></a>
</h2>
<div id="definition" class="section level3" number="2.2.1">
<h3>
<span class="header-section-number">2.2.1</span> Definition<a class="anchor" aria-label="anchor" href="#definition"><i class="fas fa-link"></i></a>
</h3>
<p>Also known as <strong>relative entropy</strong> and <strong>discrimination information</strong>.</p>
<p>The <strong>relative entropy</strong> measures the <strong>divergence</strong>
of a distribution <span class="math inline">\(G\)</span> from the distribution <span class="math inline">\(F\)</span> and is defined as
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F,G) &amp;= \text{E}_F\log\left(\frac{dF}{dG}\right) \\
&amp; = \text{E}_F\log\left(\frac{f(x)}{g(x)}\right) \\
&amp; =
\underbrace{-\text{E}_F(\log g(x))}_{\text{cross-entropy}} - 
 (\underbrace{-\text{E}_F (\log f(x))  }_\text{(differential) entropy})  \\
&amp; = H(F, G)-H(F) \\
\end{split}
\]</span></p>
<ul>
<li>
<span class="math inline">\(D_{\text{KL}}(F, G)\)</span> measures the amount of information lost if <span class="math inline">\(G\)</span> is used to approximate <span class="math inline">\(F\)</span>.</li>
<li>If <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are identical (and no information is lost) then <span class="math inline">\(D_{\text{KL}}(F,G)=0\)</span>.</li>
</ul>
<p>(Note: here “divergence” measures the dissimilarity between probability distributions. This type of divergence is not related and should not be confused with divergence (div) as used in vector analysis.)</p>
<p>The term divergence (rather than distance) implies also that the distributions <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are not interchangeable in <span class="math inline">\(D_{\text{KL}}(F, G)\)</span>.</p>
<p>In applications in statistics the typical roles of <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are:</p>
<ul>
<li>
<span class="math inline">\(F\)</span> as the (unknown) underlying true model for the data generating process</li>
<li>
<span class="math inline">\(G\)</span> as the approximating model (e.g. some parametric family)</li>
</ul>
<p>In Bayesian statistics we use</p>
<ul>
<li>
<span class="math inline">\(F\)</span> as posterior distribution</li>
<li>
<span class="math inline">\(G\)</span> as prior distribution</li>
</ul>
<p>There exist various notations for KL divergence in the literature.
Here we use <span class="math inline">\(D_{\text{KL}}(F, G)\)</span> but you often find as well <span class="math inline">\(\text{KL}(F || G)\)</span>
and <span class="math inline">\(\boldsymbol I^{KL}(F; G)\)</span>.</p>
<p>Some authors (e.g. Efron) call twice the KL divergence <span class="math inline">\(2 D_{\text{KL}}(F, G) = D(F, G)\)</span> the <strong>deviance</strong> of <span class="math inline">\(G\)</span> from <span class="math inline">\(F\)</span>.</p>
<p>Historically, in physics (negative) relative entropy was discovered by Boltzmann (1878).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Boltzmann, L. 1878. Weitere Bemerkungen über einige Probleme der mechanischen
Wärmetheorie. Wien Ber. &lt;strong&gt;78&lt;/strong&gt;:7–46. &lt;a href="https://doi.org/10.1017/CBO9781139381437.013" class="uri"&gt;https://doi.org/10.1017/CBO9781139381437.013&lt;/a&gt;&lt;/p&gt;'><sup>2</sup></a>
In statistics and information theory it was introduced by Kullback and Leibler (1951).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Kullback, S., and R. A. Leibler. 1951. On information and sufficiency. Ann. Math. Statist. &lt;strong&gt;22&lt;/strong&gt; 79–86.
&lt;a href="https://doi.org/10.1214/aoms/1177729694" class="uri"&gt;https://doi.org/10.1214/aoms/1177729694&lt;/a&gt;&lt;/p&gt;'><sup>3</sup></a></p>
</div>
<div id="properties-of-kl-divergence" class="section level3" number="2.2.2">
<h3>
<span class="header-section-number">2.2.2</span> Properties of KL divergence<a class="anchor" aria-label="anchor" href="#properties-of-kl-divergence"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(D_{\text{KL}}(F, G) \neq D_{\text{KL}}(G, F)\)</span>, i.e. the KL divergence is not symmetric, <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> cannot be interchanged.</li>
<li>
<span class="math inline">\(D_{\text{KL}}(F, G) = 0\)</span> if and only if <span class="math inline">\(F=G\)</span>, i.e., the KL divergence is zero if and only if <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are identical.</li>
<li>
<span class="math inline">\(D_{\text{KL}}(F, G)\geq 0\)</span>, proof via <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality"><strong>Jensen’s inequality</strong></a>.</li>
<li>
<span class="math inline">\(D_{\text{KL}}(F, G)\)</span> remains invariant under coordinate transformations,
i.e. it is an invariant geometric quantity.</li>
</ol>
<p>Note that in the KL divergence the expectation is taken over a ratio of densities (or ratio of probabilities for discrete random variables). This is what creates the transformation invariance.</p>
<p>For more details and proofs of properties 3 and 4 see Worksheet 1.</p>
</div>
<div id="kl-divergence-examples" class="section level3" number="2.2.3">
<h3>
<span class="header-section-number">2.2.3</span> KL divergence examples<a class="anchor" aria-label="anchor" href="#kl-divergence-examples"><i class="fas fa-link"></i></a>
</h3>
<div class="example">
<p><span id="exm:klbernoulli" class="example"><strong>Example 2.8  </strong></span>KL divergence between two Bernoulli distributions <span class="math inline">\(\text{Ber}(p)\)</span> and <span class="math inline">\(\text{Ber}(q)\)</span>:</p>
<p>The “success” probabilities for the two distributions are <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>,
respectively, and the complementary “failure” probabilities are <span class="math inline">\(1-p\)</span> and <span class="math inline">\(1-q\)</span>.
With this we get for the KL divergence
<span class="math display">\[
D_{\text{KL}}(\text{Ber}(p), \text{Ber}(q))=p \log\left( \frac{p}{q}\right) + (1-p) \log\left(\frac{1-p}{1-q}\right)
\]</span></p>
</div>
<div class="example">
<p><span id="exm:klnormal" class="example"><strong>Example 2.9  </strong></span>KL divergence between two univariate normals with different means and variances:</p>
<p>Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2_{\text{ref}})\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>.
Then
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\text{ref}},F) &amp;= H(F_{\text{ref}}, F) - H(F_{\text{ref}}) \\
&amp;= \frac{1}{2} \left(   \frac{(\mu-\mu_{\text{ref}})^2}{\sigma^2}  + \frac{\sigma_{\text{ref}}^2}{\sigma^2}
-\log\left(\frac{\sigma_{\text{ref}}^2}{\sigma^2}\right)-1  
   \right) \\
\end{split}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:klnormalequalvar" class="example"><strong>Example 2.10  </strong></span>KL divergence between two univariate normals with different means and common variance:</p>
<p>An important special case of the previous Example <a href="from-entropy-to-maximum-likelihood.html#exm:klnormal">2.9</a> occurs if
the variances are equal. Then we get
<span class="math display">\[D_{\text{KL}}(N(\mu_{\text{ref}}, \sigma^2),   N(\mu, \sigma^2)  )=\frac{1}{2} \left(\frac{(\mu-\mu_{\text{ref}})^2}{\sigma^2}\right)\]</span></p>
</div>
</div>
</div>
<div id="local-quadratic-approximation-and-expected-fisher-information" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Local quadratic approximation and expected Fisher information<a class="anchor" aria-label="anchor" href="#local-quadratic-approximation-and-expected-fisher-information"><i class="fas fa-link"></i></a>
</h2>
<div id="definition-of-expected-fisher-information" class="section level3" number="2.3.1">
<h3>
<span class="header-section-number">2.3.1</span> Definition of expected Fisher information<a class="anchor" aria-label="anchor" href="#definition-of-expected-fisher-information"><i class="fas fa-link"></i></a>
</h3>
<p>KL information measures the divergence of two distributions.
We may thus use relative entropy to measure the divergence between two distributions in the same family, separated in parameter space only by some small <span class="math inline">\(\boldsymbol \varepsilon\)</span>.</p>
<p>Let <span class="math inline">\(h(\boldsymbol \theta) = D_{\text{KL}}(F_{\boldsymbol \theta_0}, F_{\boldsymbol \theta}) = \text{E}_{F_{\boldsymbol \theta_0}}\left( \log f(\boldsymbol x| \boldsymbol \theta_0) - \log f(\boldsymbol x| \boldsymbol \theta) \right)\)</span>.
Note that the first distribution in the KL divergence is fixed
at <span class="math inline">\(F_{\boldsymbol \theta_0}\)</span> and the second distribution is varied. Then <span class="math inline">\(h(\boldsymbol \theta_0+\boldsymbol \varepsilon) = D_{\text{KL}}(F_{\boldsymbol \theta_0}, F_{\boldsymbol \theta_0+\boldsymbol \varepsilon})\)</span>.
Since the KL divergence vanishes only when the two arguments are identical <span class="math inline">\(h(\boldsymbol \theta)\)</span> reaches a minimum at <span class="math inline">\(\boldsymbol \theta_0\)</span> with <span class="math inline">\(h(\boldsymbol \theta_0)=0\)</span> and flat gradient <span class="math inline">\(\nabla h(\boldsymbol \theta_0) =0\)</span>.</p>
<p>We can therefore approximate <span class="math inline">\(h(\boldsymbol \theta_0+\boldsymbol \varepsilon)\)</span> by
a quadratic function around <span class="math inline">\(\boldsymbol \theta_0\)</span>
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\boldsymbol \theta_0}, F_{\boldsymbol \theta_0+\boldsymbol \varepsilon}) = h(\boldsymbol \theta_0+\boldsymbol \varepsilon) &amp; \approx \frac{1}{2} \boldsymbol \varepsilon^T  \nabla \nabla^T h(\boldsymbol \theta_0) \boldsymbol \varepsilon\\
&amp; = \frac{1}{2} \boldsymbol \varepsilon^T \left( -\text{E}_{F_{\boldsymbol \theta_0}} \nabla \nabla^T  \log f(\boldsymbol x| \boldsymbol \theta_0)   \right)   \boldsymbol \varepsilon\\
&amp; = \frac{1}{2} \boldsymbol \varepsilon^T  \underbrace{\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta_0)}_{\text{expected Fisher information} }\boldsymbol \varepsilon\\
\end{split}
\]</span>
This yields the <strong>expected Fisher information</strong> at <span class="math inline">\(\boldsymbol \theta_0\)</span> as the negative
expected Hessian matrix of the log-density at <span class="math inline">\(\boldsymbol \theta_0\)</span>.
Since <span class="math inline">\(\boldsymbol \theta_0\)</span> is a minimum the expected Fisher information matrix must be positive definite!</p>
<p>We can use the above approximation also to compute the divergence <span class="math inline">\(D_{\text{KL}}(F_{\boldsymbol \theta_0+\boldsymbol \varepsilon}, F_{\boldsymbol \theta_0})\)</span> where the first argument varies and the second is kept fixed:
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\boldsymbol \theta_0+\boldsymbol \varepsilon}, F_{\boldsymbol \theta_0}) &amp;\approx \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta_0+\boldsymbol \varepsilon)\, \boldsymbol \varepsilon\\
\end{split} 
\]</span>
In a linear approximation <span class="math inline">\(\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta_0+\boldsymbol \varepsilon) \approx \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta_0) + \boldsymbol \Delta_{\boldsymbol \varepsilon}\)</span>
each element of the matrix <span class="math inline">\(\boldsymbol \Delta_{\boldsymbol \varepsilon}\)</span> is the scalar product of <span class="math inline">\(\boldsymbol \varepsilon\)</span>
and the gradient of the corresponding element in <span class="math inline">\(\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta_0)\)</span>
evaluated at <span class="math inline">\(\boldsymbol \theta_0\)</span>. Therefore <span class="math inline">\(\boldsymbol \varepsilon^T \boldsymbol \Delta_{\boldsymbol \varepsilon} \boldsymbol \varepsilon\)</span>
is of <em>cubic order</em> in <span class="math inline">\(\boldsymbol \varepsilon\)</span> and hence
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\boldsymbol \theta_0+\boldsymbol \varepsilon}, F_{\boldsymbol \theta_0}) &amp;\approx \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta_0+\boldsymbol \varepsilon) \boldsymbol \varepsilon\\
&amp;\approx \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta_0) \boldsymbol \varepsilon+ \boldsymbol \varepsilon^T \boldsymbol \Delta_{\boldsymbol \varepsilon} \boldsymbol \varepsilon\\
&amp;\approx \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta_0) \boldsymbol \varepsilon
\end{split} 
\]</span>
keeping only terms quadratic in <span class="math inline">\(\boldsymbol \varepsilon\)</span>.</p>
<p>Note that there is no data involved in computing the expected Fisher information, hence it is purely a property of the model, or more precisely of the space of the models indexed by <span class="math inline">\(\boldsymbol \theta\)</span>.
In the next Chapter we will study a related quantity, the <em>observed Fisher information</em> that in contrast <em>is</em> a function of the observed data.</p>
</div>
<div id="examples" class="section level3" number="2.3.2">
<h3>
<span class="header-section-number">2.3.2</span> Examples<a class="anchor" aria-label="anchor" href="#examples"><i class="fas fa-link"></i></a>
</h3>
<div class="example">
<p><span id="exm:expectedfisherbernoulli" class="example"><strong>Example 2.11  </strong></span>Expected Fisher information for the Bernoulli distribution:</p>
<p>The log-probability mass function of the Bernoulli <span class="math inline">\(\text{Ber}(p)\)</span> distribution is
<span class="math display">\[
\log f(x | p) = x \log(p) + (1-x) \log(1-p)
\]</span>
where <span class="math inline">\(p\)</span> is the proportion of “success”.
The second derivative with regard to the parameter <span class="math inline">\(p\)</span> is
<span class="math display">\[
\frac{d^2}{dp^2} \log f(x | p)  =  -\frac{x}{p^2}-  \frac{1-x}{(1-p)^2}
\]</span>
Since <span class="math inline">\(\text{E}(x) = p\)</span> we get as Fisher information
<span class="math display">\[
\begin{split}
I^{\text{Fisher}}(p) &amp; = -\text{E}\left(\frac{d^2}{dp^2} \log f(x | p)  \right)\\
                           &amp;= \frac{p}{p^2}+  \frac{1-p}{(1-p)^2} \\
                            &amp;= \frac{1}{p(1-p)}\\
\end{split}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:quadapproxklbernoulli" class="example"><strong>Example 2.12  </strong></span>Quadratic approximations of the KL divergence between two Bernoulli distributions:</p>
<p>From Example <a href="from-entropy-to-maximum-likelihood.html#exm:klbernoulli">2.8</a> we have as KL divergence
<span class="math display">\[
D_{\text{KL}}\left (\text{Ber}(p_1), \text{Ber}(p_2) \right)=p_1 \log\left( \frac{p_1}{p_2}\right) + (1-p_1) \log\left(\frac{1-p_1}{1-p_2}\right)
\]</span>
and from
Example <a href="from-entropy-to-maximum-likelihood.html#exm:expectedfisherbernoulli">2.11</a> the corresponding expected Fisher information.</p>
<p>The quadratic approximation implies that
<span class="math display">\[
D_{\text{KL}}\left( \text{Ber}(p), \text{Ber}(p + \varepsilon) \right) \approx \frac{\varepsilon^2}{2}  I^{\text{Fisher}}(p) =  \frac{\varepsilon^2}{2 p (1-p)}
\]</span>
and also that
<span class="math display">\[
D_{\text{KL}}\left( \text{Ber}(p+\varepsilon), \text{Ber}(p) \right) \approx \frac{\varepsilon^2}{2} I^{\text{Fisher}}(p) =  \frac{\varepsilon^2}{2 p (1-p)}
\]</span></p>
<p>In Worksheet 1 this is verified by using a second order Taylor series applied to the KL divergence.</p>
</div>
<div class="example">
<p><span id="exm:expectedfishernormal" class="example"><strong>Example 2.13  </strong></span>Expected Fisher information for the normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<p>The log-density is
<span class="math display">\[
\log f(x | \mu, \sigma^2) = -\frac{1}{2} \log(\sigma^2) 
-\frac{1}{2 \sigma^2} (x-\mu)^2 - \frac{1}{2}\log(2 \pi)
\]</span>
The gradient with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> (!) is the vector
<span class="math display">\[
\nabla \log f(x | \mu, \sigma^2) =
\begin{pmatrix}
\frac{1}{\sigma^2} (x-\mu) \\
- \frac{1}{2 \sigma^2} + \frac{1}{2 \sigma^4} (x- \mu)^2 \\
\end{pmatrix}
\]</span>
Hint for calculating the gradient: replace <span class="math inline">\(\sigma^2\)</span> by <span class="math inline">\(v\)</span> and then take the partial derivative with regard to <span class="math inline">\(v\)</span>, then substitute back.</p>
<p>The Hessian matrix is
<span class="math display">\[
\nabla \nabla^T \log f(x | \mu, \sigma^2) =
\begin{pmatrix}
-\frac{1}{\sigma^2} &amp; -\frac{1}{\sigma^4} (x-\mu)\\
-\frac{1}{\sigma^4} (x-\mu) &amp;  \frac{1}{2\sigma^4} - \frac{1}{\sigma^6}(x- \mu)^2 \\
\end{pmatrix}
\]</span>
As <span class="math inline">\(\text{E}(x) = \mu\)</span> we have <span class="math inline">\(\text{E}(x-\mu) =0\)</span>.
Furthermore, with <span class="math inline">\(\text{E}( (x-\mu)^2 ) =\sigma^2\)</span> we see that
<span class="math inline">\(\text{E}\left(\frac{1}{\sigma^6}(x- \mu)^2\right) = \frac{1}{\sigma^4}\)</span>. Therefore
the expected Fisher information matrix as the negative expected Hessian matrix is
<span class="math display">\[
\boldsymbol I^{\text{Fisher}}\left(\mu,\sigma^2\right) = \begin{pmatrix} \frac{1}{\sigma^2} &amp; 0 \\ 0 &amp; \frac{1}{2\sigma^4} \end{pmatrix}
\]</span></p>
</div>
</div>
</div>
<div id="entropy-learning-and-maximum-likelihood" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> Entropy learning and maximum likelihood<a class="anchor" aria-label="anchor" href="#entropy-learning-and-maximum-likelihood"><i class="fas fa-link"></i></a>
</h2>
<div id="the-relative-entropy-between-true-model-and-approximating-model" class="section level3" number="2.4.1">
<h3>
<span class="header-section-number">2.4.1</span> The relative entropy between true model and approximating model<a class="anchor" aria-label="anchor" href="#the-relative-entropy-between-true-model-and-approximating-model"><i class="fas fa-link"></i></a>
</h3>
<p>Assume we have observations <span class="math inline">\(x_1, \ldots, x_n\)</span>. The data is sampled from <span class="math inline">\(F\)</span>, the true but unknown data generating distribution. We also specify a family of distributions <span class="math inline">\(G_{\boldsymbol \theta}\)</span>
indexed by <span class="math inline">\(\boldsymbol \theta\)</span> to approximate <span class="math inline">\(F\)</span>.</p>
<p>The relative entropy <span class="math inline">\(D_{\text{KL}}(F,G_{\boldsymbol \theta})\)</span> then measures the divergence of the approximation <span class="math inline">\(G_{\boldsymbol \theta}\)</span>
from the unknown true model <span class="math inline">\(F\)</span>. It can be written as:
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F,G_{\boldsymbol \theta}) &amp;= H(F,G_{\boldsymbol \theta}) - H(F) \\
&amp;= \underbrace{- \text{E}_{F}\log g_{\boldsymbol \theta}(x)}_{\text{cross-entropy}}
-(\underbrace{-\text{E}_{F}\log f(x)}_{\text{entropy of $F$, does not depend on $\boldsymbol \theta$}})\\
\end{split}
\]</span></p>
<p>However, since we do not know <span class="math inline">\(F\)</span> we cannot actually compute this divergence. Nonetheless, we may use
the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> — a function of the observed data — as approximation for <span class="math inline">\(F\)</span>, and in this way we arrive at an approximation for <span class="math inline">\(D_{\text{KL}}(F,G_{\boldsymbol \theta})\)</span> that becomes more and more accurate with growing sample size.</p>
<hr>
<p>Recall the “Law of Large Numbers” :</p>
<ul>
<li><p>By the strong law of large numbers the empirical distribution
<span class="math inline">\(\hat{F}_n\)</span> based on observed data <span class="math inline">\(D=\{x_1, \ldots, x_n\}\)</span> converges to the true underlying distribution <span class="math inline">\(F\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> almost surely:
<span class="math display">\[
\hat{F}_n\overset{a. s.}{\to} F
\]</span></p></li>
<li><p>For <span class="math inline">\(n \rightarrow \infty\)</span> the average <span class="math inline">\(\text{E}_{\hat{F}_n}(h(x)) = \frac{1}{n} \sum_{i=1}^n h(x_i)\)</span> converges to the expectation <span class="math inline">\(\text{E}_{F}(h(x))\)</span>.</p></li>
</ul>
<hr>
<p>Hence, for large sample size <span class="math inline">\(n\)</span> we can approximate cross-entropy and as a result the KL divergence. The cross-entropy <span class="math inline">\(H(F, G_{\boldsymbol \theta})\)</span> is approximated by the empirical cross-entropy where the expectation is taken with regard to <span class="math inline">\(\hat{F}_n\)</span> rather than <span class="math inline">\(F\)</span>:
<span class="math display">\[
\begin{split}
H(F, G_{\boldsymbol \theta}) &amp; \approx H(\hat{F}_n, G_{\boldsymbol \theta}) \\
                  &amp; = - \text{E}_{\hat{F}_n} (\log g(x|\boldsymbol \theta))  \\
                  &amp; = -\frac{1}{n} \sum_{i=1}^n \log g(x_i | \boldsymbol \theta) \\
                  &amp; = -\frac{1}{n} l_n ({\boldsymbol \theta}| D)
\end{split}
\]</span>
This turns out to be equal to the negative log-likelihood standardised by the sample size <span class="math inline">\(n\)</span>! Or in other words, the <strong>log-likelihood</strong> is the <strong>negative empirical cross-entropy multiplied by sample size <span class="math inline">\(n\)</span></strong>.</p>
<p>From the link of the multinomial coefficient with Shannon entropy (Example <a href="from-entropy-to-maximum-likelihood.html#exm:entropymultinomial">2.3</a>) we already know that for large sample size
<span class="math display">\[
H(\hat{F}) \approx \frac{1}{n} \log \binom{n}{n_1, \ldots, n_K}
\]</span></p>
<p>The KL divergence <span class="math inline">\(D_{\text{KL}}(F,G_{\boldsymbol \theta})\)</span> can therefore be approximated by
<span class="math display">\[
D_{\text{KL}}(F,G_{\boldsymbol \theta}) \approx -\frac{1}{n} \left( \log \binom{n}{n_1, \ldots, n_K} + l_n ({\boldsymbol \theta}| D)  \right)
\]</span>
Thus, with the KL divergence we obtain not just the log-likelihood (the cross-entropy part) but also the multiplicity factor taking account of the possible orderings of the data (the entropy part).</p>
</div>
<div id="minimum-kl-divergence-and-maximum-likelihood" class="section level3" number="2.4.2">
<h3>
<span class="header-section-number">2.4.2</span> Minimum KL divergence and maximum likelihood<a class="anchor" aria-label="anchor" href="#minimum-kl-divergence-and-maximum-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>If we knew <span class="math inline">\(F\)</span> we would simply minimise <span class="math inline">\(D_{\text{KL}}(F, G_{\boldsymbol \theta})\)</span> to find the particular model <span class="math inline">\(G_{\boldsymbol \theta}\)</span> that is closest to the true model. Equivalently, we would minimise the cross-entropy <span class="math inline">\(H(F, G_{\boldsymbol \theta})\)</span>.
However, since we actually don’t know <span class="math inline">\(F\)</span> this is not possible.</p>
<p>However, for large sample size <span class="math inline">\(n\)</span> when the empirical distribution <span class="math inline">\(\hat{F}_n\)</span>
is a good approximation for <span class="math inline">\(F\)</span>, we can use the results from the previous section.
Thus, instead of minimising the KL divergence
<span class="math inline">\(D_{\text{KL}}(F, G_{\boldsymbol \theta})\)</span> we simply minimise <span class="math inline">\(H(\hat{F}_n, G_{\boldsymbol \theta})\)</span> which
is the same as maximising the log-likelihood <span class="math inline">\(l_n ({\boldsymbol \theta}| D)\)</span>.
Note that the entropy of the true distribution <span class="math inline">\(F\)</span> (and the corresponding empirical
distribution <span class="math inline">\(\hat{F}\)</span>) that does not depend on the parameters <span class="math inline">\(\boldsymbol \theta\)</span>
and hence it does not matter when minimising the divergence.</p>
<p>Conversely, this implies that maximising the likelihood with regard to the <span class="math inline">\(\boldsymbol \theta\)</span> is equivalent ( asymptotically for large <span class="math inline">\(n\)</span>) to minimising the KL divergence of the approximating model and the unknown true model!</p>
<p><span class="math display">\[
\begin{split}
\hat{\boldsymbol \theta}^{ML} &amp;= \underset{\boldsymbol \theta}{\arg \max}\,\, l_n(\boldsymbol \theta| D) \\
 &amp;= \underset{\boldsymbol \theta}{\arg \min}\,\, H(\hat{F}_n, G_{\boldsymbol \theta}) \\
 &amp;\approx \underset{\boldsymbol \theta}{\arg \min}\,\, D_{\text{KL}}(F, G_{\boldsymbol \theta}) \\
\end{split}
\]</span></p>
<p>Therefore, the reasoning behind the method of <strong>maximum likelihood</strong> is that it minimises a <strong>large sample approximation of the KL divergence</strong> of the candidate model <span class="math inline">\(G_{\boldsymbol \theta}\)</span> from the unkown true model <span class="math inline">\(F\)</span>.</p>
<p>As a consequence of the close link of maximum likelhood and relative entropy
maximum likelihood inherits for large <span class="math inline">\(n\)</span> (and only then!) all the optimality properties from KL divergence. These will be discussed in more detail later in the course.</p>
</div>
<div id="further-connections" class="section level3" number="2.4.3">
<h3>
<span class="header-section-number">2.4.3</span> Further connections<a class="anchor" aria-label="anchor" href="#further-connections"><i class="fas fa-link"></i></a>
</h3>
<p>Since minimising KL divergence contains ML estimation as special case you may wonder whether there is a broader justification of relative entropy in the context of statistical data analysis?</p>
<p>Indeed, KL divergence has strong geometrical interpretation that forms the basis of <em>information geometry</em>.
In this field the manifold of distributions
is studied using tools from differential geometry. The expected Fisher information
plays an important role as <a href="https://en.wikipedia.org/wiki/Fisher_information_metric">metric tensor in the space of distributions</a>.</p>
<p>Furthermore, it is also linked to probabilistic forecasting.
In the framework of so-called <a href="https://en.wikipedia.org/wiki/Scoring_rule"><strong>scoring rules</strong></a>.
the only local proper scoring rule is the negative log-probability (“surprise”).
The expected “surprise” is the cross-entropy
and relative entropy is the corresponding natural divergence connected with the log scoring rule.</p>
<p>Furthermore, another intriguing property of KL divergence is that the relative entropy <span class="math inline">\(D_{\text{KL}}(F, G)\)</span> is the <em>only divergence measure</em> that is both a Bregman and an <span class="math inline">\(f\)</span>-divergence.
Note that <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergences</a> and <a href="https://en.wikipedia.org/wiki/Bregman_divergence">Bregman-divergences</a> (in turn related to proper scoring rules) are two large classes of measures of similarity and divergence between two probability distributions.</p>
<p>Finally, not only the likelihood estimation but also the Bayesian update rule (as discussed later in this module) is another special case of entropy learning.</p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></div>
<div class="next"><a href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#from-entropy-to-maximum-likelihood"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li>
<a class="nav-link" href="#entropy"><span class="header-section-number">2.1</span> Entropy</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#overview"><span class="header-section-number">2.1.1</span> Overview</a></li>
<li><a class="nav-link" href="#surprise-surprisal-or-shannon-information"><span class="header-section-number">2.1.2</span> Surprise, surprisal or Shannon information</a></li>
<li><a class="nav-link" href="#shannon-entropy"><span class="header-section-number">2.1.3</span> Shannon entropy</a></li>
<li><a class="nav-link" href="#differential-entropy"><span class="header-section-number">2.1.4</span> Differential entropy</a></li>
<li><a class="nav-link" href="#maximum-entropy-principle-to-characterise-distributions"><span class="header-section-number">2.1.5</span> Maximum entropy principle to characterise distributions</a></li>
<li><a class="nav-link" href="#cross-entropy"><span class="header-section-number">2.1.6</span> Cross-entropy</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#kullback-leibler-divergence"><span class="header-section-number">2.2</span> Kullback-Leibler divergence</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#definition"><span class="header-section-number">2.2.1</span> Definition</a></li>
<li><a class="nav-link" href="#properties-of-kl-divergence"><span class="header-section-number">2.2.2</span> Properties of KL divergence</a></li>
<li><a class="nav-link" href="#kl-divergence-examples"><span class="header-section-number">2.2.3</span> KL divergence examples</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#local-quadratic-approximation-and-expected-fisher-information"><span class="header-section-number">2.3</span> Local quadratic approximation and expected Fisher information</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#definition-of-expected-fisher-information"><span class="header-section-number">2.3.1</span> Definition of expected Fisher information</a></li>
<li><a class="nav-link" href="#examples"><span class="header-section-number">2.3.2</span> Examples</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#entropy-learning-and-maximum-likelihood"><span class="header-section-number">2.4</span> Entropy learning and maximum likelihood</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-relative-entropy-between-true-model-and-approximating-model"><span class="header-section-number">2.4.1</span> The relative entropy between true model and approximating model</a></li>
<li><a class="nav-link" href="#minimum-kl-divergence-and-maximum-likelihood"><span class="header-section-number">2.4.2</span> Minimum KL divergence and maximum likelihood</a></li>
<li><a class="nav-link" href="#further-connections"><span class="header-section-number">2.4.3</span> Further connections</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 1 May 2022.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
