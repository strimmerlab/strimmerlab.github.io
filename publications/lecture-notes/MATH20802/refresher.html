<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>A Refresher | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.4/tabs.js"></script><script src="libs/bs3compat-0.2.4/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="active" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="refresher" class="section level1">
<h1>
<span class="header-section-number">A</span> Refresher<a class="anchor" aria-label="anchor" href="#refresher"><i class="fas fa-link"></i></a>
</h1>
<p>Statistics is a mathematical science that requires practical use of tools from probability,
vector and matrices, analysis etc.</p>
<p>Here we briefly list some essentials that are needed for “Statistical Methods”.
Please familiarise yourself (again) with these topics.</p>
<div id="basic-mathematical-notation" class="section level2">
<h2>
<span class="header-section-number">A.1</span> Basic mathematical notation<a class="anchor" aria-label="anchor" href="#basic-mathematical-notation"><i class="fas fa-link"></i></a>
</h2>
<p>Summation:
<span class="math display">\[
\sum_{i=1}^n x_i = x_1 + x_2 + \ldots + x_n
\]</span></p>
<p>Multiplication:
<span class="math display">\[
\prod_{i=1}^n x_i = x_1 \times x_2 \times \ldots \times x_n
\]</span></p>
</div>
<div id="vectors-and-matrices" class="section level2">
<h2>
<span class="header-section-number">A.2</span> Vectors and matrices<a class="anchor" aria-label="anchor" href="#vectors-and-matrices"><i class="fas fa-link"></i></a>
</h2>
<p>Vector and matrix notation.</p>
<p>Vector algebra.</p>
<p>Eigenvectors and eigenvalues for a real symmetric matrix.</p>
<p>Eigenvalue (spectral) decomposition of a real symmetric matrix.</p>
<p>Positive and negative definiteness of a real symmetric matrix (containing only positive or only negative eigenvalues).</p>
<p>Singularity of a real symmetric matrix (containing one or more eigenvalues identical to zero).</p>
<p>Singular value decomposition of a real matrix.</p>
</div>
<div id="functions" class="section level2">
<h2>
<span class="header-section-number">A.3</span> Functions<a class="anchor" aria-label="anchor" href="#functions"><i class="fas fa-link"></i></a>
</h2>
<div id="gradient" class="section level3">
<h3>
<span class="header-section-number">A.3.1</span> Gradient<a class="anchor" aria-label="anchor" href="#gradient"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>nabla operator</strong> (also known as <strong>del operator</strong>) is the <em>row</em> vector
<span class="math display">\[
\nabla =  \left(\frac{\partial}{\partial x_1}, \ldots, 
\frac{\partial}{\partial x_d}\right) = \frac{\partial}{\partial \boldsymbol x}
\]</span>
containing
the first order partial derivative operators.</p>
<p>The <strong>gradient</strong> of a scalar-valued function
<span class="math inline">\(h(\boldsymbol x)\)</span> with vector argument <span class="math inline">\(\boldsymbol x= (x_1, \ldots, x_d)^T\)</span>
is also a <em>row</em> vector (with <span class="math inline">\(d\)</span> columns) and
can be expressed using the nabla operator
<span class="math display">\[
\nabla h(\boldsymbol x) = \left( \frac{\partial h(\boldsymbol x)}{\partial x_1}, \ldots, 
\frac{\partial h(\boldsymbol x)}{\partial x_d} \right) = 
 \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = \text{grad } h(\boldsymbol x) \, .
\]</span>
Note the various notations for the gradient.</p>
<div class="example">
<p><span id="exm:gradientexamples" class="example"><strong>Example A.1  </strong></span>Examples for the gradient:</p>
<ul>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol a^T \boldsymbol x+ b\)</span>. Then <span class="math inline">\(\nabla h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol a^T\)</span>.</li>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol x^T \boldsymbol x\)</span>. Then <span class="math inline">\(\nabla h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = 2 \boldsymbol x^T\)</span>.</li>
<li>
<span class="math inline">\(h(\boldsymbol x)=\boldsymbol x^T \boldsymbol A\boldsymbol x\)</span>. Then <span class="math inline">\(\nabla h(\boldsymbol x) = \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x} = \boldsymbol x^T (\boldsymbol A+ \boldsymbol A^T)\)</span>.</li>
</ul>
</div>
</div>
<div id="hessian-matrix" class="section level3">
<h3>
<span class="header-section-number">A.3.2</span> Hessian matrix<a class="anchor" aria-label="anchor" href="#hessian-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>The matrix of all second order partial derivates of scalar-valued
function with vector-valued argument is called the <strong>Hessian matrix</strong>
and is computed by double application of the nabla operator:
<span class="math display">\[
\nabla^T \nabla h(\boldsymbol x) =
\begin{pmatrix}
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_1^2}
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_1 \partial x_2} 
     &amp; \cdots 
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_1 \partial x_d} \\
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_2 \partial x_1} 
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_2^2}
     &amp; \cdots 
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_2 \partial x_d} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  \frac{\partial^2 h(\boldsymbol x)}{\partial x_d \partial x_1} 
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_d \partial x_2}  
     &amp; \cdots 
     &amp; \frac{\partial^2 h(\boldsymbol x)}{\partial x_d^2}
 \end{pmatrix} = \left(\frac{\partial h(\boldsymbol x)}{\partial x_i \partial x_j}\right)  
= {\left(\frac{\partial}{\partial \boldsymbol x}\right)}^T \frac{\partial h(\boldsymbol x)}{\partial \boldsymbol x}
\,.
\]</span>
By construction it is square and symmetric.</p>
</div>
<div id="convex-and-concave-functions" class="section level3">
<h3>
<span class="header-section-number">A.3.3</span> Convex and concave functions<a class="anchor" aria-label="anchor" href="#convex-and-concave-functions"><i class="fas fa-link"></i></a>
</h3>
<p>A function <span class="math inline">\(h(x)\)</span> is convex if the second derivative <span class="math inline">\(h''(x) \geq 0\)</span> for all <span class="math inline">\(x\)</span>.
More generally, a function <span class="math inline">\(h(\boldsymbol x)\)</span> is convex if the Hessian matrix <span class="math inline">\(\nabla^T \nabla h(\boldsymbol x)\)</span>
is positive definite, i.e. if it contains only positive eigenvalues.</p>
<p>If <span class="math inline">\(h(\boldsymbol x)\)</span> is convex, then <span class="math inline">\(-h(\boldsymbol x)\)</span> is <em>concave</em>. A function is concave if the Hessian matrix is negative definite.</p>
<div class="example">
<p><span id="exm:convexconcave" class="example"><strong>Example A.2  </strong></span>The logarithm <span class="math inline">\(\log(x)\)</span> is an example of a concave function whereas <span class="math inline">\(x^2\)</span> is a convex function.</p>
<p>To memorise, a <em>v</em>alley is con<em>v</em>ex.</p>
</div>
</div>
<div id="linear-and-quadratic-approximation" class="section level3">
<h3>
<span class="header-section-number">A.3.4</span> Linear and quadratic approximation<a class="anchor" aria-label="anchor" href="#linear-and-quadratic-approximation"><i class="fas fa-link"></i></a>
</h3>
<p>Taylor series of first / second order.</p>
<p>Applied to scalar-valued function of a scalar:
<span class="math display">\[
h(x) \approx h(x_0) + h'(x_0) (x-x_0) + \frac{1}{2} h''(x_0) (x-x_0)^2
\]</span>
With <span class="math inline">\(x = x_0+ \varepsilon\)</span> this can be written as
<span class="math display">\[
h(x_0+ \varepsilon) \approx h(x_0) + h'(x_0) \, \varepsilon + \frac{1}{2} h''(x_0)\, \varepsilon^2
\]</span></p>
<p>Applied to scalar-valued function of a vector:
<span class="math display">\[
h(\boldsymbol x) \approx h(\boldsymbol x_0) + \nabla h(\boldsymbol x_0) (\boldsymbol x-\boldsymbol x_0) + \frac{1}{2} 
(\boldsymbol x-\boldsymbol x_0)^T \nabla^T \nabla h(\boldsymbol x_0) (\boldsymbol x-\boldsymbol x_0)
\]</span>
With <span class="math inline">\(\boldsymbol x= \boldsymbol x_0+ \boldsymbol \varepsilon\)</span> this can be written as
<span class="math display">\[
h(\boldsymbol x_0+ \boldsymbol \varepsilon) \approx h(\boldsymbol x_0) + \nabla h(\boldsymbol x_0)\boldsymbol \varepsilon+ \frac{1}{2} \boldsymbol \varepsilon^T \nabla^T \nabla h(\boldsymbol x_0) \boldsymbol \varepsilon
\]</span></p>
<div class="example">
<p><span id="exm:taylorexamples" class="example"><strong>Example A.3  </strong></span>Commonly occurring Taylor series approximations of second order are for example
<span class="math display">\[
\log(x_0+\varepsilon) \approx \log(x_0) + \frac{\varepsilon}{x_0} - \frac{\varepsilon^2}{2 x_0^2}
\]</span>
and
<span class="math display">\[
\frac{x_0}{x_0+\varepsilon} \approx 1 - \frac{\varepsilon}{x_0} + \frac{\varepsilon^2}{ x_0^2}
\]</span></p>
</div>
</div>
<div id="conditions-for-local-optimum-of-a-function" class="section level3">
<h3>
<span class="header-section-number">A.3.5</span> Conditions for local optimum of a function<a class="anchor" aria-label="anchor" href="#conditions-for-local-optimum-of-a-function"><i class="fas fa-link"></i></a>
</h3>
<p>To check if <span class="math inline">\(x_0\)</span> or <span class="math inline">\(\boldsymbol x_0\)</span> is a local maximum or minimum we can use the following conditions:</p>
<p>For a function of a single variable:</p>
<ol style="list-style-type: lower-roman">
<li>First derivative is zero at optimum <span class="math inline">\(h'(x_0) = 0\)</span>.</li>
<li>If the second derivative <span class="math inline">\(h''(x_0) &lt; 0\)</span> at the optimum is negative the function is locally concave and the optimum is a maximum.</li>
<li>If the second derivative <span class="math inline">\(h''(x_0) &gt; 0\)</span> is positive at the optimum the function is locally convex and the optimum is a minimum.</li>
</ol>
<p>For a function of several variables:</p>
<ol style="list-style-type: lower-roman">
<li>Gradient vanishes at maximum, <span class="math inline">\(\nabla h(\boldsymbol x_0)=0\)</span>.</li>
<li>If the Hessian <span class="math inline">\(\nabla^T \nabla h(\boldsymbol x_0)\)</span> is negative definite (= all eigenvalues of Hessian matrix are negative) then the function is locally concave and the optimum is a maximum.</li>
<li>If the Hessian is positive definite (= all eigenvalues of Hessian matrix are positive) then the function is locally convec and the optimum is a minimum.</li>
</ol>
<p>Around the local optimum <span class="math inline">\(\boldsymbol x_0\)</span> we can approximate the function quadratically using
<span class="math display">\[
h(\boldsymbol x_0+ \boldsymbol \varepsilon) \approx h(\boldsymbol x_0) +  \frac{1}{2} \boldsymbol \varepsilon^T \nabla^T \nabla h(\boldsymbol x_0) \boldsymbol \varepsilon
\]</span>
Note the linear term is missing due to the gradient being zero at <span class="math inline">\(\boldsymbol x_0\)</span>.</p>
</div>
<div id="functions-of-matrices" class="section level3">
<h3>
<span class="header-section-number">A.3.6</span> Functions of matrices<a class="anchor" aria-label="anchor" href="#functions-of-matrices"><i class="fas fa-link"></i></a>
</h3>
<p>Matrix inverse,
matrix square root etc. of symmetric real matrices.</p>
<p>Computation via eigenvalue decomposition
i.e. apply function such as inverse, sqrt etc. on the eigenvalues.</p>
<p>In this course we do not actually compute matrix functions, but we will use matrix
notation for matrix square roots, so you do need to know that it is exists and that it
is not the same as taking the square root of the matrix entries.</p>
<p>Trace and determinant of a square matrix.</p>
<p>Connection with eigenvalues (trace = sum of eigenvalues, determinant = product of eigenvalues).</p>
</div>
</div>
<div id="combinatorics" class="section level2">
<h2>
<span class="header-section-number">A.4</span> Combinatorics<a class="anchor" aria-label="anchor" href="#combinatorics"><i class="fas fa-link"></i></a>
</h2>
<div id="number-of-permutations" class="section level3">
<h3>
<span class="header-section-number">A.4.1</span> Number of permutations<a class="anchor" aria-label="anchor" href="#number-of-permutations"><i class="fas fa-link"></i></a>
</h3>
<p>The number of possible orderings, or permutations, of <span class="math inline">\(n\)</span> distinct items is
the number of ways to put <span class="math inline">\(n\)</span> items in <span class="math inline">\(n\)</span> bins with exactly one item in each bin. It is given by
the factorial
<span class="math display">\[
n! = \prod_{i=1}^n i = 1 \times 2 \times \ldots \times n
\]</span>
where <span class="math inline">\(n\)</span> is a positive integer.
For <span class="math inline">\(n=0\)</span> the factorial is defined as
<span class="math display">\[
0! = 1 
\]</span>
as there is exactly one permutation of zero objects.</p>
<p>The factorial can also be obtained using the <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma function</a>
<span class="math display">\[
n! = \Gamma(n+1)
\]</span>
which can be viewed as continuous version of the factorial.</p>
</div>
<div id="multinomial-and-binomial-coefficient" class="section level3">
<h3>
<span class="header-section-number">A.4.2</span> Multinomial and binomial coefficient<a class="anchor" aria-label="anchor" href="#multinomial-and-binomial-coefficient"><i class="fas fa-link"></i></a>
</h3>
<p>The number of possible permutation of <span class="math inline">\(n\)</span> items of <span class="math inline">\(K\)</span> distinct types, with <span class="math inline">\(n_1\)</span> of type 1, <span class="math inline">\(n_2\)</span> of type 2 and so on, equals the number of ways
to put <span class="math inline">\(n\)</span> items into <span class="math inline">\(K\)</span> bins with <span class="math inline">\(n_1\)</span> items in the first bin, <span class="math inline">\(n_2\)</span> in the second and so on.
It is given by the <strong>multinomial</strong> coefficient
<span class="math display">\[
\binom{n}{n_1, \ldots, n_K} = \frac {n!}{n_1! \times n_2! \times\ldots \times n_K! } 
\]</span>
with <span class="math inline">\(\sum_{k=1}^K n_k = n\)</span> and <span class="math inline">\(K \leq n\)</span>.
Note that it equals the number of permutation of all items divided by the number of permutations of the items in each bin (or of each type).</p>
<p>If all <span class="math inline">\(n_k=1\)</span> and hence <span class="math inline">\(K=n\)</span> the multinomial coefficient reduces to the factorial.</p>
<p>If there are only two bins / types (<span class="math inline">\(K=2\)</span>) the multinomial coefficients becomes the
<strong>binomial coefficient</strong>
<span class="math display">\[
 \binom{n}{n_1} = \binom{n}{n_1, n-n_1}    =  \frac {n!}{n_1! (n - n_1)!} 
\]</span>
which counts the number of ways to choose <span class="math inline">\(n_1\)</span> elements from a set of <span class="math inline">\(n\)</span> elements.</p>
</div>
<div id="de-moivre-sterling-approximation-of-the-factorial" class="section level3">
<h3>
<span class="header-section-number">A.4.3</span> De Moivre-Sterling approximation of the factorial<a class="anchor" aria-label="anchor" href="#de-moivre-sterling-approximation-of-the-factorial"><i class="fas fa-link"></i></a>
</h3>
<p>The factorial is frequently approximated by the following formula derived by <a href="https://en.wikipedia.org/wiki/Abraham_de_Moivre">Abraham de Moivre (1667–1754)</a> and <a href="https://en.wikipedia.org/wiki/James_Stirling_(mathematician)">James Stirling (1692-1770)</a>
<span class="math display">\[
n! \approx \sqrt{2 \pi} n^{n+\frac{1}{2}} e^{-n}
\]</span>
or equivalently on logarithmic scale
<span class="math display">\[
\log n!  \approx \left(n+\frac{1}{2}\right) \log n  -n + \frac{1}{2}\log \left( 2 \pi\right)
\]</span>
The approximation is good for small <span class="math inline">\(n\)</span> (but fails for <span class="math inline">\(n=0\)</span>) and becomes
more and more accurate with increasing <span class="math inline">\(n\)</span>. For large <span class="math inline">\(n\)</span> the approximation can be simplified to
<span class="math display">\[
\log n! \approx  n \log n  -n 
\]</span></p>
</div>
</div>
<div id="probability" class="section level2">
<h2>
<span class="header-section-number">A.5</span> Probability<a class="anchor" aria-label="anchor" href="#probability"><i class="fas fa-link"></i></a>
</h2>
<div id="random-variables" class="section level3">
<h3>
<span class="header-section-number">A.5.1</span> Random variables<a class="anchor" aria-label="anchor" href="#random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>A <strong>random variable</strong> describes a random experiment. The set of possible outcomes
is the <strong>sample space</strong> or <strong>state space</strong> and is denoted by
<span class="math inline">\(\Omega = \{\omega_1, \omega_2, \ldots\}\)</span>. The outcomes <span class="math inline">\(\omega_i\)</span> are the <strong>elementary events</strong>.
The sample space <span class="math inline">\(\Omega\)</span> can be finite or infinite. Depending on type of outcomes
the random variable is <strong>discrete</strong> or <strong>continous</strong>.</p>
<p>An event <span class="math inline">\(A \subseteq \Omega\)</span> is subset of <span class="math inline">\(\Omega\)</span> and thus itself a set of elementary events <span class="math inline">\(A = \{a_1, a_2, \ldots\}\)</span>.
This includes as special cases the full set <span class="math inline">\(A = \Omega\)</span>, the empty set <span class="math inline">\(A = \emptyset\)</span>, and the elementary
events <span class="math inline">\(A=\omega_i\)</span>. The complementary event <span class="math inline">\(A^C\)</span> is the complement of the set <span class="math inline">\(A\)</span> in the set <span class="math inline">\(\Omega\)</span>
so that <span class="math inline">\(A^C = \Omega \setminus A = \{\omega_i \in \Omega: \omega_i \notin A\}\)</span>.</p>
<p>The probability of an event is denoted by <span class="math inline">\(\text{Pr}(A)\)</span>.
We assume that</p>
<ul>
<li>
<span class="math inline">\(\text{Pr}(A) \geq 0\)</span>, probabilities are positive,</li>
<li>
<span class="math inline">\(\text{Pr}(\Omega) = 1\)</span>,
the certain event has probability 1, and</li>
<li>
<span class="math inline">\(\text{Pr}(A) = \sum_{a_i \in A} \text{Pr}(a_i)\)</span>, the probability of
an event equals the sum of its constituting elementary events <span class="math inline">\(a_i\)</span>.</li>
</ul>
<p>This implies</p>
<ul>
<li>
<span class="math inline">\(\text{Pr}(A) \leq 1\)</span>, i.e. probabilities all lie in the interval <span class="math inline">\([0,1]\)</span>
</li>
<li>
<span class="math inline">\(\text{Pr}(A^C) = 1 - \text{Pr}(A)\)</span>, and</li>
<li><span class="math inline">\(\text{Pr}(\emptyset) = 0\)</span></li>
</ul>
<p>Assume now we have two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.
The probability of the event “<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>” is then given by the probability of the set intersection
<span class="math inline">\(\text{Pr}(A \cap B)\)</span>.
Likewise the probability of the event “<span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>” is given by the probability of the set union
<span class="math inline">\(\text{Pr}(A \cup B)\)</span>.</p>
<p>From the above it is clear that probability theory is closely linked to set theory,
and in particular to measure theory. This allows for an unified treatment of discrete
and continuous random variables (an elegant framework but not needed for this module).</p>
</div>
<div id="probability-mass-and-density-function-and-distribution-and-quantile-function" class="section level3">
<h3>
<span class="header-section-number">A.5.2</span> Probability mass and density function and distribution and quantile function<a class="anchor" aria-label="anchor" href="#probability-mass-and-density-function-and-distribution-and-quantile-function"><i class="fas fa-link"></i></a>
</h3>
<p>To describe a random variable <span class="math inline">\(x\)</span> we need to assign probabilities to the corresponding elementary outcomes
<span class="math inline">\(x \in \Omega\)</span>. For convenience we use the same name to denote the random variable and the elementary outcomes.</p>
<p>For a discrete random variable we employ a probability mass function (PMF).
We denote the it by a lower case <span class="math inline">\(f\)</span> but occasionally we also use
<span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span>. In the discrete case we can define the
event <span class="math inline">\(A = \{x: x=a\} = \{a\}\)</span> and obtain the probability directly from the PMF:
<span class="math display">\[\text{Pr}(A) = \text{Pr}(x=a) =f(a) \,.\]</span>
The PMF has the property that <span class="math inline">\(\sum_{x \in \Omega} f(x) = 1\)</span> and that
<span class="math inline">\(f(x) \in [0,1]\)</span>.</p>
<p>For continuous random variables we need to use a probability density function (PDF)
instead. We define the event
<span class="math inline">\(A = \{x: a &lt; x \leq a + da\}\)</span> as an infinitesimal interval
and then assign the probability
<span class="math display">\[
\text{Pr}(A) = \text{Pr}( a &lt; x \leq a + da) = f(a) da \,.
\]</span>
The PDF has the property that <span class="math inline">\(\int_{x \in \Omega} f(x) dx = 1\)</span>
but in contrast to a PMF the density <span class="math inline">\(f(x)\geq 0\)</span> may take on values larger
than 1.</p>
<p>Assuming an ordering
we can define the event <span class="math inline">\(A = \{x: x \leq a \}\)</span> and compute its
probability
<span class="math display">\[
F(a) = \text{Pr}(A) = \text{Pr}( x \leq a ) =
\begin{cases}
 \sum_{x \in A} f(x) &amp; \text{discrete case} \\
\int_{x \in A} f(x) dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
This is known as the <strong>distribution function</strong>, or <strong>cumulative distribution function</strong> (CDF)
and is denoted by upper case <span class="math inline">\(F\)</span> if the corresponding PDF/PMF is <span class="math inline">\(f\)</span>
(or <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> if the corresponding PDF/PMF are <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>).
By construction the distribution function is monotonically increasing and its value ranges from 0 to 1.
With its help we can compute the probability of general interval sets such as
<span class="math display">\[
\text{Pr}( a &lt; x \leq b ) = F(b)-F(a) \,.
\]</span></p>
<p>The inverse of the distribution function <span class="math inline">\(y=F(x)\)</span> is the <strong>quantile function</strong> <span class="math inline">\(x=F^{-1}(y)\)</span>.
The 50% quantile <span class="math inline">\(F^{-1}\left(\frac{1}{2}\right)\)</span> is the <strong>median</strong>.</p>
<p>If the random variable <span class="math inline">\(x\)</span> has distribution function <span class="math inline">\(F\)</span> we write <span class="math inline">\(x \sim F\)</span>.</p>
<div class="inline-figure"><img src="fig/refresher_dens-dist.png" width="100%" style="display: block; margin: auto;"></div>
</div>
<div id="expection-and-variance-of-a-random-variable" class="section level3">
<h3>
<span class="header-section-number">A.5.3</span> Expection and variance of a random variable<a class="anchor" aria-label="anchor" href="#expection-and-variance-of-a-random-variable"><i class="fas fa-link"></i></a>
</h3>
<p>The expected value <span class="math inline">\(\text{E}(x)\)</span> of a random variable is defined as
weighted average over all possible outcomes:
<span class="math display">\[
\text{E}(x) = 
\begin{cases}
 \sum_{x \in \Omega} x f(x) &amp; \text{discrete case} \\
\int_{x \in \Omega} x f(x) dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
The expectation is not necessarily always defined for a continuous
random variable as the integral can diverge.</p>
<p>The expected value of a function of a random variable <span class="math inline">\(h(x)\)</span> is
obtained similarly:
<span class="math display">\[
\text{E}(h(x)) = 
\begin{cases}
 \sum_{x \in \Omega} h(x) f(x) &amp; \text{discrete case} \\
\int_{x \in \Omega} h(x) f(x) dx &amp; \text{continuous case} \\
\end{cases}
\]</span>
This is called the <a href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician">“law of the unconscious statistician”</a>, or short LOTUS.</p>
<p>For an event <span class="math inline">\(A\)</span> we can define a corresponding <strong>indicator function</strong>
<span class="math display">\[
1_A(x) =
\begin{cases}
1 &amp; x \in A\\
0 &amp; x \notin A\\
\end{cases}
\]</span>
Intriguingly,
<span class="math display">\[
\text{E}(1_A(x) ) = \text{Pr}(A)
\]</span>
i.e. the expectation of the indicator variable for <span class="math inline">\(A\)</span> is the probability
of <span class="math inline">\(A\)</span>.</p>
<p>The moments of random variables are also defined by expectation:</p>
<ul>
<li>Zeroth moment: <span class="math inline">\(\text{E}(x^0) = 1\)</span> by definition of PDF and PMF,</li>
<li>First moment: <span class="math inline">\(\text{E}(x^1) = \text{E}(x) = \mu\)</span> , the mean,</li>
<li>Second moment: <span class="math inline">\(\text{E}(x^2)\)</span>
</li>
<li>The variance is the second momented centered about the mean <span class="math inline">\(\mu\)</span>:
<span class="math display">\[\text{Var}(x) = \text{E}( (x - \mu)^2 ) = \sigma^2\]</span>
</li>
<li>The variance can also be computed by <span class="math inline">\(\text{Var}(x) = \text{E}(x^2)-\text{E}(x)^2\)</span>.</li>
</ul>
<p>A distribution does not necessarily need to have any finite first or higher moments.
An example is the the <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a> that does not have a mean or variance (or any other higher moment).</p>
</div>
<div id="transformation-of-random-variables" class="section level3">
<h3>
<span class="header-section-number">A.5.4</span> Transformation of random variables<a class="anchor" aria-label="anchor" href="#transformation-of-random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>Linear transformation of random variables: if <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants and <span class="math inline">\(x\)</span> is a random variable, then the random variable <span class="math inline">\(y= a + b x\)</span> has mean <span class="math inline">\(\text{E}(y) = a + b \text{E}(x)\)</span> and variance <span class="math inline">\(\text{Var}(y) = b^2 \text{Var}(x)\)</span>.</p>
<p>For a general invertible coordinate transformation <span class="math inline">\(y = h(x) = y(x)\)</span> the backtransformation is <span class="math inline">\(x = h^{-1}(y) = x(y)\)</span>.</p>
<p>The transformation of the infinitesimal volume element is <span class="math inline">\(dy = |\frac{dy}{dx}| dx\)</span>.</p>
<p>The transformation of the density is <span class="math inline">\(f_y(y) =\left|\frac{dx}{dy}\right| f_x(x(y))\)</span>.</p>
<p>Note that <span class="math inline">\(\left|\frac{dx}{dy}\right| = \left|\frac{dy}{dx}\right|^{-1}\)</span>.</p>
</div>
<div id="law-of-large-numbers" class="section level3">
<h3>
<span class="header-section-number">A.5.5</span> Law of large numbers:<a class="anchor" aria-label="anchor" href="#law-of-large-numbers"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>By the strong law of large numbers the empirical distribution
<span class="math inline">\(\hat{F}_n\)</span> converges to the true underlying distribution <span class="math inline">\(F\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> almost surely:
<span class="math display">\[
\hat{F}_n\overset{a. s.}{\to} F
\]</span>
The Glivenko–Cantelli theorem asserts that the convergence is uniform. Since the strong law implies the weak law we also have convergence in probability:
<span class="math display">\[
\hat{F}_n\overset{P}{\to} F
\]</span></p></li>
<li><p>Correspondingly, for <span class="math inline">\(n \rightarrow \infty\)</span> the average <span class="math inline">\(\text{E}_{\hat{F}_n}(h(X)) = \frac{1}{n} \sum_{i=1}^n h(x_i)\)</span> converges to the expectation <span class="math inline">\(\text{E}_{F}(h(X))\)</span>.</p></li>
</ul>
</div>
<div id="jensens-inequality" class="section level3">
<h3>
<span class="header-section-number">A.5.6</span> Jensen’s inequality<a class="anchor" aria-label="anchor" href="#jensens-inequality"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[\text{E}(h(\boldsymbol x)) \geq h(\text{E}(\boldsymbol x))\]</span>
for a <em>convex</em> function <span class="math inline">\(h(\boldsymbol x)\)</span>.</p>
<p>Recall: a convex function (such as <span class="math inline">\(x^2\)</span>) has the shape of a “valley”.</p>
</div>
</div>
<div id="distributions" class="section level2">
<h2>
<span class="header-section-number">A.6</span> Distributions<a class="anchor" aria-label="anchor" href="#distributions"><i class="fas fa-link"></i></a>
</h2>
<div id="bernoulli-and-binomial-distribution" class="section level3">
<h3>
<span class="header-section-number">A.6.1</span> Bernoulli and Binomial distribution<a class="anchor" aria-label="anchor" href="#bernoulli-and-binomial-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The Bernoulli distribution <span class="math inline">\(\text{Ber}(p)\)</span> is simplest distribution possible.
It is named after <a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Jacob Bernoulli (1655-1705)</a>
who also invented the law of large numbers.</p>
<p>It describes a discrete binary random variable
with two states <span class="math inline">\(x=0\)</span> (“failure”) and <span class="math inline">\(x=1\)</span> (“success”),
where the parameter <span class="math inline">\(p \in [0,1]\)</span> is the probability of “success”.
Often the Bernoulli distribution is also referred to as “coin tossing” model with
the two outcomes “heads” and “tails”.</p>
<p>Correspondingly, the probability mass function of <span class="math inline">\(\text{Ber}(p)\)</span> is
<span class="math display">\[
f(x=0) = \text{Pr}(\text{"failure"}) = 1-p  
\]</span>
and
<span class="math display">\[
f(x=1) = \text{Pr}(\text{"success"}) = p 
\]</span>
A compact way to write the PMF of the Bernoulli distribution is
<span class="math display">\[
f(x | p ) = p^{x} (1-p)^{1-x}
\]</span></p>
<p>If a random variable <span class="math inline">\(x\)</span> follows the Bernoulli distribution we
write
<span class="math display">\[
x \sim \text{Ber}(p) \,.
\]</span>
The expected value is <span class="math inline">\(\text{E}(x) = p\)</span> and the variance is <span class="math inline">\(\text{Var}(x) = p (1 - p)\)</span>.</p>
<p>Closely related to the Bernoulli distribution is the Binomial distribution
<span class="math inline">\(\text{Bin}(m, p)\)</span> which results from repeating a
Bernoulli experiment <span class="math inline">\(m\)</span> times and counting the number of successes among
the <span class="math inline">\(m\)</span> trials (without keeping track of the ordering of the experiments).</p>
<p>Its probability mass function is:
<span class="math display">\[
f(x | p) = \binom{m}{x} p^x (1 - p)^{m - x}
\]</span>
for <span class="math inline">\(x = 0, 1, 2, \ldots, m\)</span>.
The Binomial coefficient <span class="math inline">\(\binom{m}{x}\)</span> is needed to account for the multiplicity
of ways (orderings of samples) in which we can observe <span class="math inline">\(x\)</span> sucesses.</p>
<p>The expected value is <span class="math inline">\(\text{E}(x) = mp\)</span> and the variance is <span class="math inline">\(\text{Var}(x) = mp (1 - p)\)</span>.</p>
<p>If a random variable <span class="math inline">\(x\)</span> follows the Binomial distribution we
write
<span class="math display">\[
x \sim \text{Bin}(m, p)\,
\]</span>
For <span class="math inline">\(m=1\)</span> it reduces to the Bernoulli distribution <span class="math inline">\(\text{Ber}(p)\)</span>.</p>
<p>In R the PMF of the Binomial distribution is called <code><a href="https://rdrr.io/r/stats/Binomial.html">dbinom()</a></code>. The Binomial coefficient itself is computed by <code><a href="https://rdrr.io/r/base/Special.html">choose()</a></code>.</p>
</div>
<div id="normal-distribution" class="section level3">
<h3>
<span class="header-section-number">A.6.2</span> Normal distribution<a class="anchor" aria-label="anchor" href="#normal-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Univariate normal distribution:</p>
<p><span class="math inline">\(x \sim N(\mu,\sigma^2)\)</span> with <span class="math inline">\(\text{E}(x)=\mu\)</span> and <span class="math inline">\(\text{Var}(x) = \sigma^2\)</span>.</p>
<p>Probability density function (PDF): <span class="math display">\[f(x| \mu, \sigma^2)=(2\pi\sigma^2)^{-\frac{1}{2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</span></p>
<p>In R the density function is called <code><a href="https://rdrr.io/r/stats/Normal.html">dnorm()</a></code>.</p>
<p>The standard normal distribution is <span class="math inline">\(N(0, 1)\)</span> with mean 1 and variance 1.</p>
<p>Plot of the PDF of the standard normal:</p>
<div class="inline-figure"><img src="20-refresher_files/figure-html/unnamed-chunk-2-1.png" width="384"></div>
<p>The cumulative distribution function (CDF) of the standard normal <span class="math inline">\(N(0,1)\)</span>
is
<span class="math display">\[
\Phi (x ) = \int_{-\infty}^{x} f(x'| \mu=0, \sigma^2=1) dx' 
\]</span>
There is no analytic expression for <span class="math inline">\(\Phi(x)\)</span>. In R the function is called <code><a href="https://rdrr.io/r/stats/Normal.html">pnorm()</a></code>.</p>
<div class="inline-figure"><img src="20-refresher_files/figure-html/unnamed-chunk-3-1.png" width="384"></div>
<p>The inverse <span class="math inline">\(\Phi^{-1}(p)\)</span> is called the quantile function of the standard normal.
In R the function is called <code><a href="https://rdrr.io/r/stats/Normal.html">qnorm()</a></code>.</p>
<div class="inline-figure"><img src="20-refresher_files/figure-html/unnamed-chunk-4-1.png" width="384"></div>
<p>The sum of two normal random variables is also normal (with the appropriate mean and variance).</p>
</div>
<div id="scaled-chi-squared-wishart-gamma-distribution-and-exponential-distribution" class="section level3">
<h3>
<span class="header-section-number">A.6.3</span> Scaled chi-squared / Wishart / gamma distribution and exponential distribution<a class="anchor" aria-label="anchor" href="#scaled-chi-squared-wishart-gamma-distribution-and-exponential-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Assume <span class="math inline">\(m\)</span> independent normal random variables
<span class="math display">\[z_1,z_2,\dots,z_m\sim N(0,\sigma^2)\]</span>
Then the sum of the squares
<span class="math display">\[
x = \sum_{i=1}^{m} z_i^2
\]</span>
follows a <strong>scaled chi-squared distribution</strong>
<span class="math display">\[
x\sim \sigma^2 \text{$\chi^2_{m}$}
\]</span>
with degree of freedom <span class="math inline">\(m\)</span> and <span class="math inline">\(x \geq 0\)</span>.
The mean and variance of a scaled chi-squared distributed variable is <span class="math inline">\(\text{E}(x)=m \sigma^2\)</span> and <span class="math inline">\(\text{Var}(x)=2m\sigma^4\)</span>.</p>
<p>Another name for the scaled chi-squared distribution is <strong>univariate Wishart distribution</strong> <span class="math inline">\(W_1(\sigma^2, m)\)</span> which uses the same parameters.</p>
<p>The <strong>gamma distribution</strong> <span class="math inline">\(\text{Gam}(\alpha, \beta)\)</span> is a further variant of the scaled chi-squared distribution which uses a different parameterisation in terms of a shape parameter <span class="math inline">\(\alpha\)</span> and a scale parameter <span class="math inline">\(\beta\)</span>. The scaled chi-squared distribution <span class="math inline">\(\sigma^2 \text{$\chi^2_{m}$}\)</span> equals <span class="math inline">\(\text{Gam}(\frac{m}{2}, 2 \sigma^2)\)</span>. The mean of <span class="math inline">\(\text{Gam}(\alpha, \beta)\)</span> is <span class="math inline">\(\alpha \beta\)</span> and its variance is <span class="math inline">\(\alpha \beta^2\)</span>.</p>
<p>The <strong>chi-squared distribution</strong> is a special case with <span class="math inline">\(\sigma^2=1\)</span> with mean <span class="math inline">\(\text{E}(x)=m\)</span> and variance <span class="math inline">\(\text{Var}(x)=2m\)</span>. The chi-squared distribution <span class="math inline">\(\text{$\chi^2_{m}$}\)</span> equals <span class="math inline">\(\text{Gam}(\frac{m}{2}, 2)\)</span>.</p>
<p>The <strong>exponential distribution</strong> <span class="math inline">\(\text{Exp}(\beta)\)</span> with scale parameter <span class="math inline">\(\beta\)</span> (and mean <span class="math inline">\(\beta\)</span> and variance <span class="math inline">\(\beta^2\)</span>) is another special case of the gamma distribution with shape parameter <span class="math inline">\(\alpha=1\)</span>. Instead of the scale parameter the exponential distribution is also often specified using a rate parameter <span class="math inline">\(\lambda= \frac{1}{\beta}\)</span>.</p>
<p>Here is a plot of the density of the chi-squared distribution
for degrees of freedom <span class="math inline">\(m=1\)</span> and <span class="math inline">\(m=3\)</span>:</p>
<div class="inline-figure"><img src="20-refresher_files/figure-html/unnamed-chunk-5-1.png" width="768"></div>
<p>In R the density of the chi-squared distribution is given by <code><a href="https://rdrr.io/r/stats/Chisquare.html">dchisq()</a></code>. The cumulative density function is <code><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq()</a></code> and
the quantile function is <code><a href="https://rdrr.io/r/stats/Chisquare.html">qchisq()</a></code>.</p>
<p>The density of the gamma distribution (aka scaled chi-squared distribution) is available in the R function <code><a href="https://rdrr.io/r/stats/GammaDist.html">dgamma()</a></code>. The cumulative density function is <code><a href="https://rdrr.io/r/stats/GammaDist.html">pgamma()</a></code> and the quantile function is <code><a href="https://rdrr.io/r/stats/GammaDist.html">qgamma()</a></code>.</p>
</div>
</div>
<div id="statistics" class="section level2">
<h2>
<span class="header-section-number">A.7</span> Statistics<a class="anchor" aria-label="anchor" href="#statistics"><i class="fas fa-link"></i></a>
</h2>
<div id="statistical-learning" class="section level3">
<h3>
<span class="header-section-number">A.7.1</span> Statistical learning<a class="anchor" aria-label="anchor" href="#statistical-learning"><i class="fas fa-link"></i></a>
</h3>
<p>The aim in statistics - data science - machine learning is to learn from data
(from experiments, observations, measurements) to learn about and understand the world.</p>
<p>Specifically, to identify the best model(s) for the data in order to</p>
<ul>
<li>to explain the current data, and</li>
<li>to enable good prediction of future data</li>
</ul>
<p>Note that it is easy to get models that only explain the data but do not predict well!</p>
<p>This is called <em>overfitting</em> the data and happens in particular if the model is overparameterized for the amount of data available.</p>
<p>Specifically, we have data <span class="math inline">\(x_1, \ldots, x_n\)</span> and models <span class="math inline">\(f(x| \theta)\)</span> that are indexed the parameter <span class="math inline">\(\theta\)</span>.</p>
<p>Often (but not always) <span class="math inline">\(\theta\)</span> can be interpreted and/or is associated with some property of the model.</p>
<p>If there is only a single parameter we write <span class="math inline">\(\theta\)</span> (scalar parameter). For a parameter vector we write <span class="math inline">\(\boldsymbol \theta\)</span> (in bold type).</p>
</div>
<div id="point-and-interval-estimation" class="section level3">
<h3>
<span class="header-section-number">A.7.2</span> Point and interval estimation<a class="anchor" aria-label="anchor" href="#point-and-interval-estimation"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>There is a parameter <span class="math inline">\(\theta\)</span> of interest in a model</li>
<li>we are uncertain about this parameter (i.e. we don’t know the exact value)</li>
<li>we would like to learn about this parameter by observing data <span class="math inline">\(x_1, \ldots, x_n\)</span> from the model</li>
</ul>
<p>Estimation:</p>
<ul>
<li>An <strong>estimator for <span class="math inline">\(\theta\)</span></strong> is a function <span class="math inline">\(\hat{\theta}(x_1, \ldots, x_n)\)</span> that maps the data (input) to a “guess” (output) about <span class="math inline">\(\theta\)</span>.<br>
</li>
<li>A <strong>point estimator</strong> provides a single number for each parameter</li>
<li>An <strong>interval estimator</strong> provides a set of possible values for each parameter.</li>
</ul>
</div>
<div id="sampling-properties-of-a-point-estimator-hatboldsymbol-theta" class="section level3">
<h3>
<span class="header-section-number">A.7.3</span> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span><a class="anchor" aria-label="anchor" href="#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fas fa-link"></i></a>
</h3>
<p>A point estimator <span class="math inline">\(\hat\theta\)</span> depends on the data, hence it has sampling variation (i.e. estimate will be different for a new set of observations)</p>
<p>Thus <span class="math inline">\(\hat\theta\)</span> can be seen as a random variable, and its distribution is called sampling distribution (accross different experiments).</p>
<p>Properties of this distribution can be used to evaluate how far the estimator
deviates (on average across different experiments) from the true value:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{rr}
\text{Bias:}\\
\text{Variance:}\\
\text{Mean squared error:}\\
\\
\end{array}
\begin{array}{rr}
\text{Bias}(\hat{\theta})\\
\text{Var}(\hat{\theta})\\
\text{MSE}(\hat{\theta})\\
\\
\end{array}
\begin{array}{ll}
=\text{E}(\hat{\theta})-\theta\\
=\text{E}\left((\hat{\theta}-\text{E}(\hat{\theta}))^2\right)\\
=\text{E}((\hat{\theta}-\theta)^2)\\
=\text{Var}(\hat{\theta})+\text{Bias}(\hat{\theta})^2\\
\end{array}
\end{align*}\]</span></p>
<p>The last identity about MSE follows from <span class="math inline">\(\text{E}(X^2)=\text{Var}(X)+\text{E}(X)^2\)</span>.</p>
<p>At first sight it seems desirable to focus on unbiased (for finite <span class="math inline">\(n\)</span>) estimators.
However, requiring strict unbiasedness is not always a good idea!</p>
<p>In many situations it is better to allow for some small bias and in order to achieve a smaller variance and an overall total smaller MSE. This is called <em>bias-variance tradeoff</em> — as more bias
is traded for smaller variance (or, conversely, less bias is traded for higher variance)</p>
</div>
<div id="asymptotics-1" class="section level3">
<h3>
<span class="header-section-number">A.7.4</span> Asymptotics<a class="anchor" aria-label="anchor" href="#asymptotics-1"><i class="fas fa-link"></i></a>
</h3>
<p>Typically, <span class="math inline">\(\text{Bias}\)</span>, <span class="math inline">\(\text{Var}\)</span> and <span class="math inline">\(\text{MSE}\)</span> all decrease with increasing sample size
so that with more data <span class="math inline">\(n \to \infty\)</span> the errors become smaller and smaller.</p>
<p>The typical rate of decrease of variance of a good estimator is <span class="math inline">\(\frac{1}{n}\)</span>.
Thus, when sample size is doubled the variance is divided by 2
(and the standard deviation is divided by <span class="math inline">\(\sqrt{2}\)</span>).</p>
<p>Consistency: <span class="math inline">\(\hat{\theta}\)</span> is called consistent if</p>
<p><span class="math display">\[\text{MSE}(\hat{\theta}) \longrightarrow 0 \text{ with $n\rightarrow \infty$ }\]</span>.</p>
<p>Consistency implies we recover the true model in the limit of infinite data and if the model class contains the true model.</p>
<p>Consistency is a <em>minimum</em> essential requirement for any reasonable estimator! Of all consistent
estimators we typically prefer the estimator that is most efficient (i.e. with fasted decrease in
MSE) and that thus has smallest variance and/or MSE for given finite <span class="math inline">\(n\)</span>.</p>
<p>Note that if the model class does not contain the true model then strict consistency
cannot be achived but we still wish to get at least as close as possible
to the true model.</p>
</div>
<div id="confidence-intervals" class="section level3">
<h3>
<span class="header-section-number">A.7.5</span> Confidence intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>A <strong>confidence</strong> interval (CI) is an <strong>interval estimate</strong> with a <strong>frequentist</strong> interpretation.</li>
<li>Definition of <strong>coverage</strong> <span class="math inline">\(\kappa\)</span> of a CI: how often (in repeated identical experiment) does the estimated CI overlap the true parameter value <span class="math inline">\(\theta\)</span>
<ul>
<li>Eg.: Coverage <span class="math inline">\(\kappa=0.95\)</span> (95%) means that in 95 out of 100 case the estimated CI will contain the (unknown) true value (i.e. it will “cover” <span class="math inline">\(\theta\)</span>).</li>
</ul>
</li>
</ul>
<p>Illustration of the repeated construction of a CI for <span class="math inline">\(\theta\)</span>:</p>
<div class="inline-figure"><img src="fig/refresher_p1.PNG" width="40%" style="display: block; margin: auto;"></div>
<ul>
<li>Note that a CI is actually an <strong>estimate</strong>: <span class="math inline">\(\widehat{\text{CI}}(x_1, \ldots, x_n)\)</span>, i.e. it depends on data and has a random (sampling) variation.<br>
</li>
<li>A good CI has high coverage and is compact.</li>
</ul>
<p><strong>Note:</strong> the coverage probability is <strong>not</strong> the probability that the true value is contained in a given estimated interval (that would be the Bayesian <em>Credible</em> Interval).</p>
</div>
<div id="symmetric-normal-confidence-interval" class="section level3">
<h3>
<span class="header-section-number">A.7.6</span> Symmetric normal confidence interval<a class="anchor" aria-label="anchor" href="#symmetric-normal-confidence-interval"><i class="fas fa-link"></i></a>
</h3>
<p>For a normally distributed univariate random variable
it is straightforward to construct a symmetric two-sided CI with a given desired coverage <span class="math inline">\(\kappa\)</span>.</p>
<div class="inline-figure"><img src="fig/refresher_p2_1.PNG" width="80%" style="display: block; margin: auto;"></div>
<p>For a normal random variable <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> and density function <span class="math inline">\(f(x)\)</span> we can compute the probability</p>
<p><span class="math display">\[\text{Pr}(X \leq \mu + c \sigma) =  \int_{-\infty}^{\mu+c\sigma} f(x) dx  = \Phi (c) = \frac{1+\kappa}{2}\]</span>
Note <span class="math inline">\(\Phi(c)\)</span> is the cumulative distribution function (CDF) of the standard normal <span class="math inline">\(N(0,1)\)</span>:</p>
<p>From the above we obtain the critical point <span class="math inline">\(c\)</span> from the quantile function, i.e. by inversion of <span class="math inline">\(\Phi\)</span>:</p>
<p><span class="math display">\[c=\Phi^{-1}\left(\frac{1+\kappa}{2}\right)\]</span></p>
<p>The following table lists <span class="math inline">\(c\)</span> for the three most commonly used values of <span class="math inline">\(\kappa\)</span> - it is useful to memorise these values!</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span>
</th>
<th>Critical value <span class="math inline">\(c\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>1.64</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>1.96</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>2.58</td>
</tr>
</tbody>
</table></div>
<p>A <strong>symmetric standard normal CI</strong> with nominal coverage <span class="math inline">\(\kappa\)</span> for</p>
<ul>
<li>a scalar parameter <span class="math inline">\(\theta\)</span>
</li>
<li>with normally distributed estimate <span class="math inline">\(\hat{\theta}\)</span> and</li>
<li>with estimated standard deviation <span class="math inline">\(\hat{\text{SD}}(\hat{\theta}) = \hat{\sigma}\)</span>
</li>
</ul>
<p>is then given by
<span class="math display">\[\widehat{\text{CI}}=[\hat{\theta} \pm c \hat{\sigma}]\]</span></p>
<p>where <span class="math inline">\(c\)</span> is chosen for desired coverage level <span class="math inline">\(\kappa\)</span>.</p>
</div>
<div id="confidence-interval-for-chi-squared-distribution" class="section level3">
<h3>
<span class="header-section-number">A.7.7</span> Confidence interval for chi-squared distribution<a class="anchor" aria-label="anchor" href="#confidence-interval-for-chi-squared-distribution"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-figure"><img src="fig/refresher_p4_2.PNG" width="80%" style="display: block; margin: auto;"></div>
<p>As for the normal CI we can compute critical values but for the
chi-squared distribution we use a one-sided interval:
<span class="math display">\[
\text{Pr}(X \leq c) = \kappa
\]</span>
As before we get <span class="math inline">\(c\)</span> by the quantile function, i.e. by inverting the CDF of the chi-squared distribution.</p>
<p>The following list the critical values for the three most common choice of <span class="math inline">\(\kappa\)</span>
for <span class="math inline">\(m=1\)</span> (one degree of freedom):</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Coverage <span class="math inline">\(\kappa\)</span>
</th>
<th>Critical value <span class="math inline">\(c\)</span> (<span class="math inline">\(m=1\)</span>)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>2.71</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>3.84</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>6.63</td>
</tr>
</tbody>
</table></div>
<p>A one-sided CI with nominal coverage <span class="math inline">\(\kappa\)</span> is then given by <span class="math inline">\([0, c ]\)</span>.</p>

<p></p>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></div>
<div class="next"><a href="further-study.html"><span class="header-section-number">B</span> Further study</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#refresher"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="nav-link" href="#basic-mathematical-notation"><span class="header-section-number">A.1</span> Basic mathematical notation</a></li>
<li><a class="nav-link" href="#vectors-and-matrices"><span class="header-section-number">A.2</span> Vectors and matrices</a></li>
<li>
<a class="nav-link" href="#functions"><span class="header-section-number">A.3</span> Functions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#gradient"><span class="header-section-number">A.3.1</span> Gradient</a></li>
<li><a class="nav-link" href="#hessian-matrix"><span class="header-section-number">A.3.2</span> Hessian matrix</a></li>
<li><a class="nav-link" href="#convex-and-concave-functions"><span class="header-section-number">A.3.3</span> Convex and concave functions</a></li>
<li><a class="nav-link" href="#linear-and-quadratic-approximation"><span class="header-section-number">A.3.4</span> Linear and quadratic approximation</a></li>
<li><a class="nav-link" href="#conditions-for-local-optimum-of-a-function"><span class="header-section-number">A.3.5</span> Conditions for local optimum of a function</a></li>
<li><a class="nav-link" href="#functions-of-matrices"><span class="header-section-number">A.3.6</span> Functions of matrices</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#combinatorics"><span class="header-section-number">A.4</span> Combinatorics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#number-of-permutations"><span class="header-section-number">A.4.1</span> Number of permutations</a></li>
<li><a class="nav-link" href="#multinomial-and-binomial-coefficient"><span class="header-section-number">A.4.2</span> Multinomial and binomial coefficient</a></li>
<li><a class="nav-link" href="#de-moivre-sterling-approximation-of-the-factorial"><span class="header-section-number">A.4.3</span> De Moivre-Sterling approximation of the factorial</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#probability"><span class="header-section-number">A.5</span> Probability</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#random-variables"><span class="header-section-number">A.5.1</span> Random variables</a></li>
<li><a class="nav-link" href="#probability-mass-and-density-function-and-distribution-and-quantile-function"><span class="header-section-number">A.5.2</span> Probability mass and density function and distribution and quantile function</a></li>
<li><a class="nav-link" href="#expection-and-variance-of-a-random-variable"><span class="header-section-number">A.5.3</span> Expection and variance of a random variable</a></li>
<li><a class="nav-link" href="#transformation-of-random-variables"><span class="header-section-number">A.5.4</span> Transformation of random variables</a></li>
<li><a class="nav-link" href="#law-of-large-numbers"><span class="header-section-number">A.5.5</span> Law of large numbers:</a></li>
<li><a class="nav-link" href="#jensens-inequality"><span class="header-section-number">A.5.6</span> Jensen’s inequality</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#distributions"><span class="header-section-number">A.6</span> Distributions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#bernoulli-and-binomial-distribution"><span class="header-section-number">A.6.1</span> Bernoulli and Binomial distribution</a></li>
<li><a class="nav-link" href="#normal-distribution"><span class="header-section-number">A.6.2</span> Normal distribution</a></li>
<li><a class="nav-link" href="#scaled-chi-squared-wishart-gamma-distribution-and-exponential-distribution"><span class="header-section-number">A.6.3</span> Scaled chi-squared / Wishart / gamma distribution and exponential distribution</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#statistics"><span class="header-section-number">A.7</span> Statistics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#statistical-learning"><span class="header-section-number">A.7.1</span> Statistical learning</a></li>
<li><a class="nav-link" href="#point-and-interval-estimation"><span class="header-section-number">A.7.2</span> Point and interval estimation</a></li>
<li><a class="nav-link" href="#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><span class="header-section-number">A.7.3</span> Sampling properties of a point estimator \(\hat{\boldsymbol \theta}\)</a></li>
<li><a class="nav-link" href="#asymptotics-1"><span class="header-section-number">A.7.4</span> Asymptotics</a></li>
<li><a class="nav-link" href="#confidence-intervals"><span class="header-section-number">A.7.5</span> Confidence intervals</a></li>
<li><a class="nav-link" href="#symmetric-normal-confidence-interval"><span class="header-section-number">A.7.6</span> Symmetric normal confidence interval</a></li>
<li><a class="nav-link" href="#confidence-interval-for-chi-squared-distribution"><span class="header-section-number">A.7.7</span> Confidence interval for chi-squared distribution</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 10 May 2021.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
