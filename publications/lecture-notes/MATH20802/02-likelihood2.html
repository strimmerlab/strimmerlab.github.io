<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 From information theory to likelihood | HTML</title>
  <meta name="description" content="Statistical Methods:<br />
Likelihood, Bayes and Regression</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2 From information theory to likelihood | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 From information theory to likelihood | HTML" />
  
  
  



<meta name="date" content="2021-02-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="01-likelihood1.html"/>
<link rel="next" href="03-likelihood3.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH20802/index.html">MATH20802 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Likelihood estimation and inference</b></span></li>
<li class="chapter" data-level="1" data-path="01-likelihood1.html"><a href="01-likelihood1.html"><i class="fa fa-check"></i><b>1</b> Overview of statistical learning</a><ul>
<li class="chapter" data-level="1.1" data-path="01-likelihood1.html"><a href="01-likelihood1.html#how-to-learn-from-data"><i class="fa fa-check"></i><b>1.1</b> How to learn from data?</a></li>
<li class="chapter" data-level="1.2" data-path="01-likelihood1.html"><a href="01-likelihood1.html#probability-theory-versus-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Probability theory versus statistical learning</a></li>
<li class="chapter" data-level="1.3" data-path="01-likelihood1.html"><a href="01-likelihood1.html#cartoon-of-statistical-learning"><i class="fa fa-check"></i><b>1.3</b> Cartoon of statistical learning</a></li>
<li class="chapter" data-level="1.4" data-path="01-likelihood1.html"><a href="01-likelihood1.html#likelihood"><i class="fa fa-check"></i><b>1.4</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="02-likelihood2.html"><a href="02-likelihood2.html"><i class="fa fa-check"></i><b>2</b> From information theory to likelihood</a><ul>
<li class="chapter" data-level="2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy"><i class="fa fa-check"></i><b>2.1</b> Entropy</a><ul>
<li class="chapter" data-level="2.1.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#surprise"><i class="fa fa-check"></i><b>2.1.2</b> Surprise</a></li>
<li class="chapter" data-level="2.1.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#shannon-entropy"><i class="fa fa-check"></i><b>2.1.3</b> Shannon entropy</a></li>
<li class="chapter" data-level="2.1.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#differential-entropy"><i class="fa fa-check"></i><b>2.1.4</b> Differential entropy</a></li>
<li class="chapter" data-level="2.1.5" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-entropy-principle-to-characterise-distributions"><i class="fa fa-check"></i><b>2.1.5</b> Maximum entropy principle to characterise distributions</a></li>
<li class="chapter" data-level="2.1.6" data-path="02-likelihood2.html"><a href="02-likelihood2.html#cross-entropy"><i class="fa fa-check"></i><b>2.1.6</b> Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>2.2</b> Kullback-Leibler divergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#definition"><i class="fa fa-check"></i><b>2.2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#properties-of-kl-divergence"><i class="fa fa-check"></i><b>2.2.2</b> Properties of KL divergence</a></li>
<li class="chapter" data-level="2.2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#examples"><i class="fa fa-check"></i><b>2.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#local-quadratic-approximation-and-expected-fisher-information"><i class="fa fa-check"></i><b>2.3</b> Local quadratic approximation and expected Fisher information</a></li>
<li class="chapter" data-level="2.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy-learning-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4</b> Entropy learning and maximum likelihood</a><ul>
<li class="chapter" data-level="2.4.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#the-relative-entropy-between-true-model-and-approximating-model"><i class="fa fa-check"></i><b>2.4.1</b> The relative entropy between true model and approximating model</a></li>
<li class="chapter" data-level="2.4.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#minimum-kl-divergence-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4.2</b> Minimum KL divergence and maximum likelihood</a></li>
<li class="chapter" data-level="2.4.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#further-connections"><i class="fa fa-check"></i><b>2.4.3</b> Further connections</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="03-likelihood3.html"><a href="03-likelihood3.html"><i class="fa fa-check"></i><b>3</b> Maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#principle-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.1</b> Principle of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#outline"><i class="fa fa-check"></i><b>3.1.1</b> Outline</a></li>
<li class="chapter" data-level="3.1.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#recipe-for-obtaining-mles"><i class="fa fa-check"></i><b>3.1.2</b> Recipe for obtaining MLEs</a></li>
<li class="chapter" data-level="3.1.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#invariance-property-of-the-mle"><i class="fa fa-check"></i><b>3.1.3</b> Invariance property of the MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#examples-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.2</b> Examples of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.2.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-estimation-of-a-proportion"><i class="fa fa-check"></i><b>3.2.1</b> Example 1: Estimation of a proportion</a></li>
<li class="chapter" data-level="3.2.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-exponential-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Example 2: Exponential Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-3-normal-distribution-with-unknown-mean-and-known-variance"><i class="fa fa-check"></i><b>3.2.3</b> Example 3: Normal distribution with unknown mean and known variance</a></li>
<li class="chapter" data-level="3.2.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-4-normal-distribution-with-both-mean-and-variance-unknown"><i class="fa fa-check"></i><b>3.2.4</b> Example 4: Normal Distribution with both mean and variance unknown</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information"><i class="fa fa-check"></i><b>3.3</b> Observed Fisher information</a><ul>
<li class="chapter" data-level="3.3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#motivation"><i class="fa fa-check"></i><b>3.3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#curvature-of-log-likelihood-function"><i class="fa fa-check"></i><b>3.3.2</b> Curvature of log-likelihood function</a></li>
<li class="chapter" data-level="3.3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information-1"><i class="fa fa-check"></i><b>3.3.3</b> Observed Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information---examples"><i class="fa fa-check"></i><b>3.4</b> Observed Fisher information - Examples</a><ul>
<li class="chapter" data-level="3.4.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-bernoulli-binomial-model"><i class="fa fa-check"></i><b>3.4.1</b> Example 1: Bernoulli / Binomial model</a></li>
<li class="chapter" data-level="3.4.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-normal-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Example 2: Normal distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#differences-of-observed-to-expected-fisher-information"><i class="fa fa-check"></i><b>3.4.3</b> Differences of observed to expected Fisher information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="04-likelihood4.html"><a href="04-likelihood4.html"><i class="fa fa-check"></i><b>4</b> Quadratic approximation and normal asymptotics</a><ul>
<li class="chapter" data-level="4.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#covariance-correlation-and-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.1</b> Covariance, correlation and multivariate normal distribution</a></li>
<li class="chapter" data-level="4.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#maximum-likelihood-estimates-of-the-parameters-of-the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.2</b> Maximum likelihood estimates of the parameters of the multivariate normal distribution</a></li>
<li class="chapter" data-level="4.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-approximation-of-log-likelihood-function-around-mle"><i class="fa fa-check"></i><b>4.3</b> Quadratic approximation of log-likelihood function around MLE</a></li>
<li class="chapter" data-level="4.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-normality-of-mle"><i class="fa fa-check"></i><b>4.4</b> Asymptotic normality of MLE</a></li>
<li class="chapter" data-level="4.5" data-path="04-likelihood4.html"><a href="04-likelihood4.html#observed-or-expected-fisher-information-to-estimate-variance-of-the-mle"><i class="fa fa-check"></i><b>4.5</b> Observed or expected Fisher information to estimate variance of the MLE?</a></li>
<li class="chapter" data-level="4.6" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-confidence-intervals-for-mles"><i class="fa fa-check"></i><b>4.6</b> Normal confidence intervals for MLEs</a></li>
<li class="chapter" data-level="4.7" data-path="04-likelihood4.html"><a href="04-likelihood4.html#wald-statistic"><i class="fa fa-check"></i><b>4.7</b> Wald statistic</a></li>
<li class="chapter" data-level="4.8" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-ci-expressed-using-the-squared-wald-statistics"><i class="fa fa-check"></i><b>4.8</b> Normal CI expressed using the squared Wald statistics</a></li>
<li class="chapter" data-level="4.9" data-path="04-likelihood4.html"><a href="04-likelihood4.html#testing-and-confidence-intervals"><i class="fa fa-check"></i><b>4.9</b> Testing and confidence intervals</a></li>
<li class="chapter" data-level="4.10" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-normal-distribution"><i class="fa fa-check"></i><b>4.10</b> Example: normal distribution</a></li>
<li class="chapter" data-level="4.11" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-of-non-regular-model"><i class="fa fa-check"></i><b>4.11</b> Example of non-regular model</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="05-likelihood5.html"><a href="05-likelihood5.html"><i class="fa fa-check"></i><b>5</b> Likelihood-based confidence interval and likelihood ratio</a><ul>
<li class="chapter" data-level="5.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-based-confidence-intervals"><i class="fa fa-check"></i><b>5.1</b> Likelihood-based confidence intervals</a></li>
<li class="chapter" data-level="5.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#wilks-log-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.2</b> Wilks log likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.3" data-path="05-likelihood5.html"><a href="05-likelihood5.html#quadratic-approximation-of-wilks-statistic"><i class="fa fa-check"></i><b>5.3</b> Quadratic approximation of Wilks statistic</a></li>
<li class="chapter" data-level="5.4" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistics"><i class="fa fa-check"></i><b>5.4</b> Distribution of Wilks statistics</a><ul>
<li class="chapter" data-level="5.4.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#cutoff-values-delta"><i class="fa fa-check"></i><b>5.4.1</b> Cutoff values <span class="math inline">\(\Delta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="05-likelihood5.html"><a href="05-likelihood5.html#example-likelihood-ci-for-exponential-model"><i class="fa fa-check"></i><b>5.5</b> Example: likelihood CI for exponential model</a></li>
<li class="chapter" data-level="5.6" data-path="05-likelihood5.html"><a href="05-likelihood5.html#origin-of-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.6</b> Origin of likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.7" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistic-and-likelihood-ci"><i class="fa fa-check"></i><b>5.7</b> Distribution of Wilks statistic and Likelihood CI</a></li>
<li class="chapter" data-level="5.8" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>5.8</b> Likelihood ratio test (LRT)</a></li>
<li class="chapter" data-level="5.9" data-path="05-likelihood5.html"><a href="05-likelihood5.html#optimality-of-lrts"><i class="fa fa-check"></i><b>5.9</b> Optimality of LRTs</a></li>
<li class="chapter" data-level="5.10" data-path="05-likelihood5.html"><a href="05-likelihood5.html#generalised-likelihood-ratio-test-glrt"><i class="fa fa-check"></i><b>5.10</b> Generalised likelihood ratio test (GLRT)</a></li>
<li class="chapter" data-level="5.11" data-path="05-likelihood5.html"><a href="05-likelihood5.html#glrt-example"><i class="fa fa-check"></i><b>5.11</b> GLRT example</a></li>
<li class="chapter" data-level="5.12" data-path="05-likelihood5.html"><a href="05-likelihood5.html#thoughts-on-model-selection"><i class="fa fa-check"></i><b>5.12</b> Thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="06-likelihood6.html"><a href="06-likelihood6.html"><i class="fa fa-check"></i><b>6</b> Optimality properties, minimal sufficiency and summary</a><ul>
<li class="chapter" data-level="6.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#properties-of-mles-encountered-so-far"><i class="fa fa-check"></i><b>6.1</b> Properties of MLEs encountered so far</a></li>
<li class="chapter" data-level="6.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#further-optimality-properties-of-mles"><i class="fa fa-check"></i><b>6.2</b> Further optimality properties of MLEs</a></li>
<li class="chapter" data-level="6.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency"><i class="fa fa-check"></i><b>6.3</b> Summarising data and the concept of minimal sufficiency</a></li>
<li class="chapter" data-level="6.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summary-and-concluding-remarks-on-maximum-likelihood"><i class="fa fa-check"></i><b>6.4</b> Summary and concluding remarks on maximum likelihood</a><ul>
<li class="chapter" data-level="6.4.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#starting-point-kl-divergence"><i class="fa fa-check"></i><b>6.4.1</b> Starting point: KL divergence</a></li>
<li class="chapter" data-level="6.4.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#connections-between-kl-divergence-likelihood-and-expected-and-observed-fisher-information"><i class="fa fa-check"></i><b>6.4.2</b> Connections between KL divergence, likelihood and expected and observed Fisher information</a></li>
<li class="chapter" data-level="6.4.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#likelihood-estimation"><i class="fa fa-check"></i><b>6.4.3</b> Likelihood estimation</a></li>
<li class="chapter" data-level="6.4.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#what-happens-if-n-is-small"><i class="fa fa-check"></i><b>6.4.4</b> What happens if <span class="math inline">\(n\)</span> is small?</a></li>
<li class="chapter" data-level="6.4.5" data-path="06-likelihood6.html"><a href="06-likelihood6.html#inference-with-likelihood"><i class="fa fa-check"></i><b>6.4.5</b> Inference with likelihood:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Bayesian Statistics</b></span></li>
<li class="chapter" data-level="7" data-path="07-bayes1.html"><a href="07-bayes1.html"><i class="fa fa-check"></i><b>7</b> Essentials of Bayesian statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#bayes-theorem"><i class="fa fa-check"></i><b>7.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#principle-of-bayesian-learning"><i class="fa fa-check"></i><b>7.2</b> Principle of Bayesian learning</a></li>
<li class="chapter" data-level="7.3" data-path="07-bayes1.html"><a href="07-bayes1.html#what-is-exactly-is-the-bayesian-estimate"><i class="fa fa-check"></i><b>7.3</b> What is exactly is the “Bayesian estimate”?</a></li>
<li class="chapter" data-level="7.4" data-path="07-bayes1.html"><a href="07-bayes1.html#computer-implementation-of-bayesian-learning"><i class="fa fa-check"></i><b>7.4</b> Computer implementation of Bayesian learning</a></li>
<li class="chapter" data-level="7.5" data-path="07-bayes1.html"><a href="07-bayes1.html#bayesian-interpretation-of-probability"><i class="fa fa-check"></i><b>7.5</b> Bayesian interpretation of probability</a><ul>
<li class="chapter" data-level="7.5.1" data-path="07-bayes1.html"><a href="07-bayes1.html#what-makes-you-bayesian"><i class="fa fa-check"></i><b>7.5.1</b> What makes you “Bayesian”?</a></li>
<li class="chapter" data-level="7.5.2" data-path="07-bayes1.html"><a href="07-bayes1.html#mathematics-of-probability"><i class="fa fa-check"></i><b>7.5.2</b> Mathematics of probability</a></li>
<li class="chapter" data-level="7.5.3" data-path="07-bayes1.html"><a href="07-bayes1.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.5.3</b> Interpretations of probability</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="07-bayes1.html"><a href="07-bayes1.html#historical-developments"><i class="fa fa-check"></i><b>7.6</b> Historical developments</a></li>
<li class="chapter" data-level="7.7" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning"><i class="fa fa-check"></i><b>7.7</b> Connection with entropy learning</a><ul>
<li class="chapter" data-level="7.7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#zero-forcing-property"><i class="fa fa-check"></i><b>7.7.1</b> Zero forcing property</a></li>
<li class="chapter" data-level="7.7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning-1"><i class="fa fa-check"></i><b>7.7.2</b> Connection with entropy learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="08-bayes2.html"><a href="08-bayes2.html"><i class="fa fa-check"></i><b>8</b> Beta-Binomial model for estimating a proportion</a><ul>
<li class="chapter" data-level="8.1" data-path="08-bayes2.html"><a href="08-bayes2.html#binomial-likelihood"><i class="fa fa-check"></i><b>8.1</b> Binomial likelihood</a></li>
<li class="chapter" data-level="8.2" data-path="08-bayes2.html"><a href="08-bayes2.html#excursion-properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>8.2</b> Excursion: Properties of the Beta distribution</a></li>
<li class="chapter" data-level="8.3" data-path="08-bayes2.html"><a href="08-bayes2.html#beta-prior-distribution"><i class="fa fa-check"></i><b>8.3</b> Beta prior distribution</a></li>
<li class="chapter" data-level="8.4" data-path="08-bayes2.html"><a href="08-bayes2.html#computing-the-posterior-distribution"><i class="fa fa-check"></i><b>8.4</b> Computing the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="09-bayes3.html"><a href="09-bayes3.html"><i class="fa fa-check"></i><b>9</b> Properties of Bayesian learning</a><ul>
<li class="chapter" data-level="9.1" data-path="09-bayes3.html"><a href="09-bayes3.html#prior-acting-as-pseudo-data"><i class="fa fa-check"></i><b>9.1</b> Prior acting as pseudo-data</a></li>
<li class="chapter" data-level="9.2" data-path="09-bayes3.html"><a href="09-bayes3.html#linear-shrinkage-of-mean"><i class="fa fa-check"></i><b>9.2</b> Linear shrinkage of mean</a></li>
<li class="chapter" data-level="9.3" data-path="09-bayes3.html"><a href="09-bayes3.html#conjugacy-of-prior-and-posterior-distribution"><i class="fa fa-check"></i><b>9.3</b> Conjugacy of prior and posterior distribution</a></li>
<li class="chapter" data-level="9.4" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-asymptotics"><i class="fa fa-check"></i><b>9.4</b> Large sample asymptotics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-limits-of-mean-and-variance"><i class="fa fa-check"></i><b>9.4.1</b> Large sample limits of mean and variance</a></li>
<li class="chapter" data-level="9.4.2" data-path="09-bayes3.html"><a href="09-bayes3.html#asymptotic-normality-of-the-posterior-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Asymptotic Normality of the Posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="09-bayes3.html"><a href="09-bayes3.html#posterior-variance-for-finite-n"><i class="fa fa-check"></i><b>9.5</b> Posterior variance for finite <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-bayes4.html"><a href="10-bayes4.html"><i class="fa fa-check"></i><b>10</b> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a><ul>
<li class="chapter" data-level="10.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-normal-model-to-estimate-mean"><i class="fa fa-check"></i><b>10.1</b> Normal-Normal model to estimate mean</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihood"><i class="fa fa-check"></i><b>10.1.1</b> Normal likelihood</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-prior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Normal prior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-posterior-distribution"><i class="fa fa-check"></i><b>10.1.3</b> Normal posterior distribution</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-and-stein-paradox"><i class="fa fa-check"></i><b>10.1.4</b> Large sample asymptotics and Stein paradox</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-normal-model-to-estimate-variance"><i class="fa fa-check"></i><b>10.2</b> Inverse-Gamma-Normal model to estimate variance</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Inverse Gamma distribution</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihoood"><i class="fa fa-check"></i><b>10.2.2</b> Normal likelihoood</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-prior-distribution"><i class="fa fa-check"></i><b>10.2.3</b> Inverse Gamma prior distribution</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-posterior-distribution"><i class="fa fa-check"></i><b>10.2.4</b> Inverse Gamma posterior distribution</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-1"><i class="fa fa-check"></i><b>10.2.5</b> Large sample asymptotics</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-bayes4.html"><a href="10-bayes4.html#estimating-precision"><i class="fa fa-check"></i><b>10.2.6</b> Estimating precision</a></li>
<li class="chapter" data-level="10.2.7" data-path="10-bayes4.html"><a href="10-bayes4.html#joint-estimation-of-mean-and-variance"><i class="fa fa-check"></i><b>10.2.7</b> Joint estimation of mean and variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-bayes5.html"><a href="11-bayes5.html"><i class="fa fa-check"></i><b>11</b> Shrinkage estimation using empirical risk minimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="11-bayes5.html"><a href="11-bayes5.html#linear-shrinkage"><i class="fa fa-check"></i><b>11.1</b> Linear shrinkage</a></li>
<li class="chapter" data-level="11.2" data-path="11-bayes5.html"><a href="11-bayes5.html#james-stein-estimator"><i class="fa fa-check"></i><b>11.2</b> James-Stein estimator</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-bayes6.html"><a href="12-bayes6.html"><i class="fa fa-check"></i><b>12</b> Bayesian model comparison using Bayes factors and the BIC</a><ul>
<li class="chapter" data-level="12.1" data-path="12-bayes6.html"><a href="12-bayes6.html#the-bayes-factor"><i class="fa fa-check"></i><b>12.1</b> The Bayes factor</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-bayes6.html"><a href="12-bayes6.html#connection-with-relative-entropy"><i class="fa fa-check"></i><b>12.1.1</b> Connection with relative entropy</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-bayes6.html"><a href="12-bayes6.html#interpretation-of-and-scale-for-bayes-factor"><i class="fa fa-check"></i><b>12.1.2</b> Interpretation of and scale for Bayes factor</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-bayes6.html"><a href="12-bayes6.html#computing-textprd-m-for-simple-and-composite-models"><i class="fa fa-check"></i><b>12.1.3</b> Computing <span class="math inline">\(\text{Pr}(D | M)\)</span> for simple and composite models</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-bayes6.html"><a href="12-bayes6.html#bayes-factor-versus-likelihood-ratio"><i class="fa fa-check"></i><b>12.1.4</b> Bayes factor versus likelihood ratio</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-bayes6.html"><a href="12-bayes6.html#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><i class="fa fa-check"></i><b>12.2</b> Approximate computation of the marginal likelihood and of the log-Bayes factor</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-bayes6.html"><a href="12-bayes6.html#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fa fa-check"></i><b>12.2.1</b> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-bayes6.html"><a href="12-bayes6.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>12.2.2</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-bayes6.html"><a href="12-bayes6.html#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fa fa-check"></i><b>12.2.3</b> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-bayes6.html"><a href="12-bayes6.html#model-complexity-and-occams-razor"><i class="fa fa-check"></i><b>12.2.4</b> Model complexity and Occams razor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-bayes7.html"><a href="13-bayes7.html"><i class="fa fa-check"></i><b>13</b> False discovery rates</a><ul>
<li class="chapter" data-level="13.1" data-path="13-bayes7.html"><a href="13-bayes7.html#general-setup"><i class="fa fa-check"></i><b>13.1</b> General setup</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-bayes7.html"><a href="13-bayes7.html#overview-1"><i class="fa fa-check"></i><b>13.1.1</b> Overview</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-bayes7.html"><a href="13-bayes7.html#choosing-between-h_0-and-h_a"><i class="fa fa-check"></i><b>13.1.2</b> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span></a></li>
<li class="chapter" data-level="13.1.3" data-path="13-bayes7.html"><a href="13-bayes7.html#true-and-false-positives-and-negatives"><i class="fa fa-check"></i><b>13.1.3</b> True and false positives and negatives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-bayes7.html"><a href="13-bayes7.html#specificity-and-sensitivity"><i class="fa fa-check"></i><b>13.2</b> Specificity and Sensitivity</a></li>
<li class="chapter" data-level="13.3" data-path="13-bayes7.html"><a href="13-bayes7.html#fdr-and-fndr"><i class="fa fa-check"></i><b>13.3</b> FDR and FNDR</a></li>
<li class="chapter" data-level="13.4" data-path="13-bayes7.html"><a href="13-bayes7.html#bayesian-perspective"><i class="fa fa-check"></i><b>13.4</b> Bayesian perspective</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-bayes7.html"><a href="13-bayes7.html#two-component-mixture-model"><i class="fa fa-check"></i><b>13.4.1</b> Two component mixture model</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-bayes7.html"><a href="13-bayes7.html#local-fdr"><i class="fa fa-check"></i><b>13.4.2</b> Local FDR</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-bayes7.html"><a href="13-bayes7.html#q-values"><i class="fa fa-check"></i><b>13.4.3</b> q-values</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="13-bayes7.html"><a href="13-bayes7.html#software"><i class="fa fa-check"></i><b>13.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-bayes8.html"><a href="14-bayes8.html"><i class="fa fa-check"></i><b>14</b> Optimality properties and summary</a><ul>
<li class="chapter" data-level="14.1" data-path="14-bayes8.html"><a href="14-bayes8.html#bayesian-statistics-in-a-nutshell"><i class="fa fa-check"></i><b>14.1</b> Bayesian statistics in a nutshell</a><ul>
<li class="chapter" data-level="14.1.1" data-path="14-bayes8.html"><a href="14-bayes8.html#remarks"><i class="fa fa-check"></i><b>14.1.1</b> Remarks</a></li>
<li class="chapter" data-level="14.1.2" data-path="14-bayes8.html"><a href="14-bayes8.html#advantages"><i class="fa fa-check"></i><b>14.1.2</b> Advantages</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="14-bayes8.html"><a href="14-bayes8.html#frequentist-properties-of-bayesian-estimators"><i class="fa fa-check"></i><b>14.2</b> Frequentist properties of Bayesian estimators</a></li>
<li class="chapter" data-level="14.3" data-path="14-bayes8.html"><a href="14-bayes8.html#specifying-the-prior-problem-or-advantage"><i class="fa fa-check"></i><b>14.3</b> Specifying the prior — problem or advantage?</a></li>
<li class="chapter" data-level="14.4" data-path="14-bayes8.html"><a href="14-bayes8.html#choosing-a-prior"><i class="fa fa-check"></i><b>14.4</b> Choosing a prior</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-bayes8.html"><a href="14-bayes8.html#some-guidelines"><i class="fa fa-check"></i><b>14.4.1</b> Some guidelines</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-bayes8.html"><a href="14-bayes8.html#jeffreys-prior"><i class="fa fa-check"></i><b>14.4.2</b> Jeffreys prior</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-bayes8.html"><a href="14-bayes8.html#optimality-of-bayes-inference"><i class="fa fa-check"></i><b>14.5</b> Optimality of Bayes inference</a></li>
<li class="chapter" data-level="14.6" data-path="14-bayes8.html"><a href="14-bayes8.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a><ul>
<li class="chapter" data-level="14.6.1" data-path="14-bayes8.html"><a href="14-bayes8.html#current-research"><i class="fa fa-check"></i><b>14.6.1</b> Current research</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Regression</b></span></li>
<li class="chapter" data-level="15" data-path="15-regression1.html"><a href="15-regression1.html"><i class="fa fa-check"></i><b>15</b> Overview over regression modelling</a><ul>
<li class="chapter" data-level="15.1" data-path="15-regression1.html"><a href="15-regression1.html#general-setup-1"><i class="fa fa-check"></i><b>15.1</b> General setup</a></li>
<li class="chapter" data-level="15.2" data-path="15-regression1.html"><a href="15-regression1.html#objectives"><i class="fa fa-check"></i><b>15.2</b> Objectives</a></li>
<li class="chapter" data-level="15.3" data-path="15-regression1.html"><a href="15-regression1.html#regression-as-a-form-of-supervised-learning"><i class="fa fa-check"></i><b>15.3</b> Regression as a form of supervised learning</a></li>
<li class="chapter" data-level="15.4" data-path="15-regression1.html"><a href="15-regression1.html#various-regression-models-used-in-statistics"><i class="fa fa-check"></i><b>15.4</b> Various regression models used in statistics</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="16-regression2.html"><a href="16-regression2.html"><i class="fa fa-check"></i><b>16</b> Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="16-regression2.html"><a href="16-regression2.html#the-linear-regression-model"><i class="fa fa-check"></i><b>16.1</b> The linear regression model</a></li>
<li class="chapter" data-level="16.2" data-path="16-regression2.html"><a href="16-regression2.html#interpretation-of-regression-coefficients-and-intercept"><i class="fa fa-check"></i><b>16.2</b> Interpretation of regression coefficients and intercept</a></li>
<li class="chapter" data-level="16.3" data-path="16-regression2.html"><a href="16-regression2.html#different-types-of-linear-regression"><i class="fa fa-check"></i><b>16.3</b> Different types of linear regression:</a></li>
<li class="chapter" data-level="16.4" data-path="16-regression2.html"><a href="16-regression2.html#distributional-assumptions-and-properties"><i class="fa fa-check"></i><b>16.4</b> Distributional assumptions and properties</a></li>
<li class="chapter" data-level="16.5" data-path="16-regression2.html"><a href="16-regression2.html#regression-in-data-matrix-notation"><i class="fa fa-check"></i><b>16.5</b> Regression in data matrix notation</a></li>
<li class="chapter" data-level="16.6" data-path="16-regression2.html"><a href="16-regression2.html#centering-and-vanishing-of-the-intercept-beta_0"><i class="fa fa-check"></i><b>16.6</b> Centering and vanishing of the intercept <span class="math inline">\(\beta_0\)</span></a></li>
<li class="chapter" data-level="16.7" data-path="16-regression2.html"><a href="16-regression2.html#regression-objectives-for-linear-model"><i class="fa fa-check"></i><b>16.7</b> Regression objectives for linear model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="17-regression3.html"><a href="17-regression3.html"><i class="fa fa-check"></i><b>17</b> Estimating regression coefficients</a><ul>
<li class="chapter" data-level="17.1" data-path="17-regression3.html"><a href="17-regression3.html#ordinary-least-squares-ols-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.1</b> Ordinary Least Squares (OLS) estimator of regression coefficients</a></li>
<li class="chapter" data-level="17.2" data-path="17-regression3.html"><a href="17-regression3.html#maximum-likelihood-estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>17.2</b> Maximum likelihood estimation of regression coefficients</a><ul>
<li class="chapter" data-level="17.2.1" data-path="17-regression3.html"><a href="17-regression3.html#detailed-derivation-of-the-mles"><i class="fa fa-check"></i><b>17.2.1</b> Detailed derivation of the MLEs</a></li>
<li class="chapter" data-level="17.2.2" data-path="17-regression3.html"><a href="17-regression3.html#asymptotics"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotics</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="17-regression3.html"><a href="17-regression3.html#covariance-plug-in-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.3</b> Covariance plug-in estimator of regression coefficients</a><ul>
<li class="chapter" data-level="17.3.1" data-path="17-regression3.html"><a href="17-regression3.html#importance-of-positive-definiteness-of-estimated-covariance-matrix"><i class="fa fa-check"></i><b>17.3.1</b> Importance of positive definiteness of estimated covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="17-regression3.html"><a href="17-regression3.html#best-linear-predictor"><i class="fa fa-check"></i><b>17.4</b> Best linear predictor</a><ul>
<li class="chapter" data-level="17.4.1" data-path="17-regression3.html"><a href="17-regression3.html#result"><i class="fa fa-check"></i><b>17.4.1</b> Result:</a></li>
<li class="chapter" data-level="17.4.2" data-path="17-regression3.html"><a href="17-regression3.html#irreducible-error"><i class="fa fa-check"></i><b>17.4.2</b> Irreducible Error</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="17-regression3.html"><a href="17-regression3.html#regression-by-conditioning"><i class="fa fa-check"></i><b>17.5</b> Regression by conditioning</a><ul>
<li class="chapter" data-level="17.5.1" data-path="17-regression3.html"><a href="17-regression3.html#general-idea"><i class="fa fa-check"></i><b>17.5.1</b> General idea:</a></li>
<li class="chapter" data-level="17.5.2" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-normal-assumption"><i class="fa fa-check"></i><b>17.5.2</b> Multivariate normal assumption</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="17-regression3.html"><a href="17-regression3.html#standardised-regression-coefficients-and-relationship-to-correlation"><i class="fa fa-check"></i><b>17.6</b> Standardised regression coefficients and relationship to correlation</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="18-regression4.html"><a href="18-regression4.html"><i class="fa fa-check"></i><b>18</b> Squared multiple correlation and variance decomposition in linear regression</a><ul>
<li class="chapter" data-level="18.1" data-path="18-regression4.html"><a href="18-regression4.html#squared-multiple-correlation-omega2-and-the-r2-coefficient"><i class="fa fa-check"></i><b>18.1</b> Squared multiple correlation <span class="math inline">\(\Omega^2\)</span> and the <span class="math inline">\(R^2\)</span> coefficient</a><ul>
<li class="chapter" data-level="18.1.1" data-path="18-regression4.html"><a href="18-regression4.html#estimation-of-omega2-and-the-multiple-r2-coefficient"><i class="fa fa-check"></i><b>18.1.1</b> Estimation of <span class="math inline">\(\Omega^2\)</span> and the multiple <span class="math inline">\(R^2\)</span> coefficient</a></li>
<li class="chapter" data-level="18.1.2" data-path="18-regression4.html"><a href="18-regression4.html#r-output"><i class="fa fa-check"></i><b>18.1.2</b> R output</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="18-regression4.html"><a href="18-regression4.html#variance-decomposition-in-regression"><i class="fa fa-check"></i><b>18.2</b> Variance decomposition in regression</a><ul>
<li class="chapter" data-level="18.2.1" data-path="18-regression4.html"><a href="18-regression4.html#law-of-total-variance-and-variance-decomposition"><i class="fa fa-check"></i><b>18.2.1</b> Law of total variance and variance decomposition</a></li>
<li class="chapter" data-level="18.2.2" data-path="18-regression4.html"><a href="18-regression4.html#related-quantities"><i class="fa fa-check"></i><b>18.2.2</b> Related quantities</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="18-regression4.html"><a href="18-regression4.html#sample-version-of-variance-decomposition"><i class="fa fa-check"></i><b>18.3</b> Sample version of variance decomposition</a><ul>
<li class="chapter" data-level="18.3.1" data-path="18-regression4.html"><a href="18-regression4.html#geometric-interpretation-of-regression-as-orthogonal-projection"><i class="fa fa-check"></i><b>18.3.1</b> Geometric interpretation of regression as orthogonal projection:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="19-regression5.html"><a href="19-regression5.html"><i class="fa fa-check"></i><b>19</b> Prediction and variable selection</a><ul>
<li class="chapter" data-level="19.1" data-path="19-regression5.html"><a href="19-regression5.html#prediction-and-prediction-intervals"><i class="fa fa-check"></i><b>19.1</b> Prediction and prediction intervals</a></li>
<li class="chapter" data-level="19.2" data-path="19-regression5.html"><a href="19-regression5.html#variable-importance-and-prediction"><i class="fa fa-check"></i><b>19.2</b> Variable importance and prediction</a><ul>
<li class="chapter" data-level="19.2.1" data-path="19-regression5.html"><a href="19-regression5.html#how-to-quantify-variable-importance"><i class="fa fa-check"></i><b>19.2.1</b> How to quantify variable importance?</a></li>
<li class="chapter" data-level="19.2.2" data-path="19-regression5.html"><a href="19-regression5.html#some-candidates-for-vims"><i class="fa fa-check"></i><b>19.2.2</b> Some candidates for VIMs</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="19-regression5.html"><a href="19-regression5.html#regression-t-scores."><i class="fa fa-check"></i><b>19.3</b> Regression <span class="math inline">\(t\)</span>-scores.</a><ul>
<li class="chapter" data-level="19.3.1" data-path="19-regression5.html"><a href="19-regression5.html#computation"><i class="fa fa-check"></i><b>19.3.1</b> Computation</a></li>
<li class="chapter" data-level="19.3.2" data-path="19-regression5.html"><a href="19-regression5.html#connection-with-partial-correlation"><i class="fa fa-check"></i><b>19.3.2</b> Connection with partial correlation</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="19-regression5.html"><a href="19-regression5.html#further-approaches-for-variable-selection"><i class="fa fa-check"></i><b>19.4</b> Further approaches for variable selection</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="20-refresher.html"><a href="20-refresher.html"><i class="fa fa-check"></i><b>A</b> Refresher</a><ul>
<li class="chapter" data-level="A.1" data-path="20-refresher.html"><a href="20-refresher.html#vectors-and-matrices"><i class="fa fa-check"></i><b>A.1</b> Vectors and matrices</a></li>
<li class="chapter" data-level="A.2" data-path="20-refresher.html"><a href="20-refresher.html#functions"><i class="fa fa-check"></i><b>A.2</b> Functions</a><ul>
<li class="chapter" data-level="A.2.1" data-path="20-refresher.html"><a href="20-refresher.html#gradient"><i class="fa fa-check"></i><b>A.2.1</b> Gradient</a></li>
<li class="chapter" data-level="A.2.2" data-path="20-refresher.html"><a href="20-refresher.html#hessian-matrix"><i class="fa fa-check"></i><b>A.2.2</b> Hessian matrix</a></li>
<li class="chapter" data-level="A.2.3" data-path="20-refresher.html"><a href="20-refresher.html#conditions-for-local-maximum-of-a-function"><i class="fa fa-check"></i><b>A.2.3</b> Conditions for local maximum of a function</a></li>
<li class="chapter" data-level="A.2.4" data-path="20-refresher.html"><a href="20-refresher.html#linear-and-quadratic-approximation"><i class="fa fa-check"></i><b>A.2.4</b> Linear and quadratic approximation</a></li>
<li class="chapter" data-level="A.2.5" data-path="20-refresher.html"><a href="20-refresher.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.2.5</b> Functions of matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="20-refresher.html"><a href="20-refresher.html#probability"><i class="fa fa-check"></i><b>A.3</b> Probability</a><ul>
<li class="chapter" data-level="A.3.1" data-path="20-refresher.html"><a href="20-refresher.html#law-of-large-numbers"><i class="fa fa-check"></i><b>A.3.1</b> Law of large numbers:</a></li>
<li class="chapter" data-level="A.3.2" data-path="20-refresher.html"><a href="20-refresher.html#jensens-inequality"><i class="fa fa-check"></i><b>A.3.2</b> Jensen’s inequality</a></li>
<li class="chapter" data-level="A.3.3" data-path="20-refresher.html"><a href="20-refresher.html#transformation-of-univariate-densities"><i class="fa fa-check"></i><b>A.3.3</b> Transformation of univariate densities</a></li>
<li class="chapter" data-level="A.3.4" data-path="20-refresher.html"><a href="20-refresher.html#normal-distribution"><i class="fa fa-check"></i><b>A.3.4</b> Normal distribution</a></li>
<li class="chapter" data-level="A.3.5" data-path="20-refresher.html"><a href="20-refresher.html#chi-squared-distribution"><i class="fa fa-check"></i><b>A.3.5</b> Chi-squared distribution</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="20-refresher.html"><a href="20-refresher.html#statistics"><i class="fa fa-check"></i><b>A.4</b> Statistics</a><ul>
<li class="chapter" data-level="A.4.1" data-path="20-refresher.html"><a href="20-refresher.html#statistical-learning"><i class="fa fa-check"></i><b>A.4.1</b> Statistical learning</a></li>
<li class="chapter" data-level="A.4.2" data-path="20-refresher.html"><a href="20-refresher.html#point-and-interval-estimation"><i class="fa fa-check"></i><b>A.4.2</b> Point and interval estimation</a></li>
<li class="chapter" data-level="A.4.3" data-path="20-refresher.html"><a href="20-refresher.html#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fa fa-check"></i><b>A.4.3</b> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span></a></li>
<li class="chapter" data-level="A.4.4" data-path="20-refresher.html"><a href="20-refresher.html#asymptotics-1"><i class="fa fa-check"></i><b>A.4.4</b> Asymptotics</a></li>
<li class="chapter" data-level="A.4.5" data-path="20-refresher.html"><a href="20-refresher.html#confidence-intervals"><i class="fa fa-check"></i><b>A.4.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="A.4.6" data-path="20-refresher.html"><a href="20-refresher.html#symmetric-normal-confidence-interval"><i class="fa fa-check"></i><b>A.4.6</b> Symmetric normal confidence interval</a></li>
<li class="chapter" data-level="A.4.7" data-path="20-refresher.html"><a href="20-refresher.html#confidence-interval-for-chi-squared-distribution"><i class="fa fa-check"></i><b>A.4.7</b> Confidence interval for chi-squared distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="21-further-study.html"><a href="21-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="21-further-study.html"><a href="21-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="21-further-study.html"><a href="21-further-study.html#additional-references"><i class="fa fa-check"></i><b>B.2</b> Additional references</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="22-references.html"><a href="22-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Statistical Methods:<br />
Likelihood, Bayes and Regression</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="from-information-theory-to-likelihood" class="section level1">
<h1><span class="header-section-number">2</span> From information theory to likelihood</h1>
<div id="entropy" class="section level2">
<h2><span class="header-section-number">2.1</span> Entropy</h2>
<div id="overview" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Overview</h3>
<p>In this chapter we discuss various information criteria and their connection to maximum likelihood.</p>
<p>The modern definition of (relative) entropy, or “disorder”, was first discovered by physicist Boltzmann in 1875 in the context of thermodynamics. In the 1940-1950’s the notion of entropy turned out to be central in information theory, a field pioneered by mathematicians such as Hartley, Good, Jaynes, Kullback, Leibler, Shannon, and Turing, and later further explored by Amari, Ciszar, Dawid, Efron and many others.</p>
<p><span class="math display">\[\begin{align*}
\left.
\begin{array}{cc}
\\
\textbf{Entropy} \\
\\
\end{array}
\right.
\left.
\begin{array}{cc}
\\
\nearrow  \\
\searrow  \\
\\
\end{array}
\right.
\begin{array}{ll}
\text{Shannon Entropy} \\
\\
\text{Relative Entropy}  \\
\end{array}
\begin{array}{ll}
\text{(Shannon 1948)} \\
\\
\text{(Kullback-Leibler 1951)}  \\
\end{array}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\left.
\begin{array}{ll}
\text{Fisher information} \\
\\
\text{Mutual Information} \\
\end{array}
\right.
\begin{array}{ll}
\rightarrow\text{ Likelihood theory} \\
\\
\rightarrow\text{ Information theory} \\
\end{array}
\begin{array}{ll}
\text{(Fisher 1922)} \\
\\
\text{(Shannon 1948, Lindley 1953)}  \\
\end{array}
\end{align*}\]</span></p>
</div>
<div id="surprise" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Surprise</h3>
<p>We assume a <em>discrete</em> random variable <span class="math inline">\(x\)</span> with state space
<span class="math inline">\(\Omega = \left\{x_1,x_2,\dots,x_K\right\}\)</span> with finite <span class="math inline">\(K\)</span> and an associated
distribution <span class="math inline">\(F\)</span>. The corresponding probability mass function (pmf)
is given by <span class="math inline">\(f(x_i) = \text{Pr}(x=x_i) = p_i\)</span>, with each probability <span class="math inline">\(p_i \in [0,1]\)</span>
and <span class="math inline">\(\sum_{i=1}^K p_i = 1\)</span>.
<span class="math inline">\(F\)</span> is known as <strong>categorical distribution</strong>
with <span class="math inline">\(K\)</span> classes. For <span class="math inline">\(K=2\)</span> it reduces to the <strong>Bernoulli distribution</strong>.</p>
<p>The <strong>surprise</strong> to observe <span class="math inline">\(x_i\)</span> is then defined as <span class="math inline">\(-\log(p_i)\)</span>.
Thus, the surprise to observe a certain event (with <span class="math inline">\(p_i=1\)</span>) is zero,
and conversely the surprise to observe an event that is certain not to happen
(with <span class="math inline">\(p_i=0\)</span>) is infinite.</p>
<p>Note that in this module we always use the <em>natural logarithm</em> by default, and will explicitly write <span class="math inline">\(\log_2\)</span> and <span class="math inline">\(\log_{10}\)</span> for logarithms with respect to base 2 and 10, respectively.</p>
<p>Surprise and entropy computed with the natural logarithm (<span class="math inline">\(\log\)</span>) is given in “nats” (=<a href="https://en.wikipedia.org/wiki/Nat_(unit)">natural information units</a>. If it is computed using <span class="math inline">\(\log_2\)</span> then the units are “bits”, and in terms
of <span class="math inline">\(\log_{10}\)</span> the units is “ban” or “Hartley”, cf. <a href="https://en.wikipedia.org/wiki/Hartley_(unit)" class="uri">https://en.wikipedia.org/wiki/Hartley_(unit)</a> .</p>
</div>
<div id="shannon-entropy" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Shannon entropy</h3>
<p>The <strong>Shannon</strong> entropy of the distribution <span class="math inline">\(F\)</span> is defined as the
<strong>expected surprise</strong>, i.e. the negative expected log-probability
<span class="math display">\[H(F) =-\text{E}_F\left(\log f(x)\right) = - \sum_{i=1}^{K}f(x_i) \log f(x_i) = - \sum_{i=1}^{K}p_i \log(p_i)\]</span></p>
<p>Often the entropy <span class="math inline">\(H(F)\)</span> is written as <span class="math inline">\(H(x)\)</span>, but the first notation is less ambiguous especially if more than one distribution is considered.</p>
<p>As <span class="math inline">\(p \in [0,1]\)</span> the term <span class="math inline">\(-p \log(p)\)</span> equals zero only for <span class="math inline">\(p=0\)</span> or <span class="math inline">\(p=1\)</span> and is otherwise positive. As a consequence, Shannon entropy is bounded below by zero. If <span class="math inline">\(K\)</span> is finite Shannon entropy is also bounded above by <span class="math inline">\(\log K\)</span> and therefore
<span class="math display">\[\log K \geq  H(F) \geq 0\]</span>
for any discrete distribution <span class="math inline">\(F\)</span> with <span class="math inline">\(K\)</span> categories.</p>

<div class="example">
<p><span id="exm:entropydunif" class="example"><strong>Example 2.1  </strong></span><strong>Discrete uniform distribution</strong> <span class="math inline">\(U_K\)</span>: let <span class="math inline">\(p_1=p_2= \ldots = p_K = \frac{1}{K}\)</span>.
Then <span class="math display">\[H(U_k) = - \sum_{i=1}^{K}\frac{1}{K} \log\left(\frac{1}{K}\right) = \log K\]</span></p>
Note this is the largest value the Shannon entropy can assume
with <span class="math inline">\(K\)</span> classes.
</div>


<div class="example">
<p><span id="exm:entropyconcentrated" class="example"><strong>Example 2.2  </strong></span><strong>Concentrated probability mass</strong>: let
<span class="math inline">\(p_1=1\)</span> and <span class="math inline">\(p_2=p_3=\ldots=p_K=0\)</span>.
Using <span class="math inline">\(0\times\log(0)=0\)</span> we obtain for the Shannon probability
<span class="math display">\[ H(F) = 1\times\log(1) + 0\times\log(0) + \dots = 0\]</span></p>
Note that 0 is the smallest value that Shannon entropy can assume, and corresponds to maximum concentration.
</div>

<p>Thus, <strong>large entropy</strong> implies that the <strong>distribution is spread out</strong> whereas <strong>small entropy</strong>
means the <strong>distribution is concentrated</strong>.</p>
<p>Correspondingly, maximum entropy distributions can be considered minimally informative about a random variable.</p>
<p><strong>Multivariate case:</strong></p>
<p>If <span class="math inline">\(\boldsymbol x=(x_1, x_2, \ldots, x_d)\)</span> is a multivariate random variable of dimension <span class="math inline">\(d\)</span>
with distribution <span class="math inline">\(F\)</span> then <span class="math inline">\(H(F)\)</span> is called
the <em>joint entropy</em> of the random variables <span class="math inline">\(x_1, x_2, \ldots, x_d\)</span>. It is also often written as <span class="math inline">\(H(x_1, x_2, \ldots, x_d)\)</span>.</p>
</div>
<div id="differential-entropy" class="section level3">
<h3><span class="header-section-number">2.1.4</span> Differential entropy</h3>
<p>Shannon entropy is only defined for discrete random variables.</p>
<p><em>Differential Entropy</em> results from applying the
definition of Shannon entropy to a <em>continuous</em> random variable <span class="math inline">\(x\)</span> with density <span class="math inline">\(f(x)\)</span>:
<span class="math display">\[
H(F) = -\text{E}_F(\log f(x)) = - \int f(x) \log f(x) \, dx
\]</span>
Despite having essentially the same formula the different name is justified because differential entropy exhibits different properties compared to Shannon entropy, because the logarithm is taken of a density
which in contrast to a probability can assumes values larger than one.
As a consequence, differential entropy is <em>not</em> bounded below by zero and can be negative.</p>

<div class="example">
<span id="exm:entropyunif" class="example"><strong>Example 2.3  </strong></span>Consider the uniform distribution <span class="math inline">\(U(0, a)\)</span> with <span class="math inline">\(a&gt;0\)</span>, support from <span class="math inline">\(0\)</span> to <span class="math inline">\(a\)</span> and density <span class="math inline">\(f(x) = 1/a\)</span>. As <span class="math inline">\(-\int_0^a f(x) \log f(x) dx =- \int_0^a \frac{1}{a} \log(\frac{1}{a}) dx = \log a\)</span>
the differential entropy is
<span class="math display">\[H( U(0, a) ) =  \log a \,.\]</span>
Note that for <span class="math inline">\(a &lt; 1\)</span> the differential entropy is negative.
</div>


<div class="example">
<span id="exm:entropynormal" class="example"><strong>Example 2.4  </strong></span>The density of the univariate normal <span class="math inline">\(N(\mu, \sigma^2)\)</span> distribution
is <span class="math inline">\(f(x |\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right)\)</span> with <span class="math inline">\(\sigma^2 &gt; 0\)</span>.
The corresponding differential entropy is
is
<span class="math display">\[
H(F) = \frac{1}{2} ( \log(2 \pi \sigma^2)+1) \,.
\]</span>
Note that it only depends on the variance and not on the mean, and
that for <span class="math inline">\(\sigma^2 &lt; 1/(2 \pi e) \approx 0.0585\)</span> the differential entropy is negative.
</div>

</div>
<div id="maximum-entropy-principle-to-characterise-distributions" class="section level3">
<h3><span class="header-section-number">2.1.5</span> Maximum entropy principle to characterise distributions</h3>
<p>Both maximum Shannon entropy and differential entropy are useful to characterise distributions:</p>
<ol style="list-style-type: decimal">
<li><p>The <strong>discrete
uniform distribution</strong> is the <strong>maximum entropy distribution</strong> among all categorical distributions.</p></li>
<li><p>the maximum entropy distribution of a continuous random variable with support <span class="math inline">\([-\infty, \infty]\)</span> with a specific mean and variance is the normal distribution</p></li>
<li><p>the maximum entropy distribution among all continuous distributions supported in <span class="math inline">\([0, \infty]\)</span> with a specified mean is the exponential distribution.</p></li>
</ol>
<p>The higher the entropy the more spread out (and more uninformative) is a distribution.</p>
<p>Using maximum entropy to characterise maximally uniformative distributions
was advocated by E.T. Jaynes (who also proposed to use maximum entropy in the context of finding Bayesian priors).</p>
<p>A list of maximum entropy distribution is given here:
<a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution" class="uri">https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution</a> .</p>
</div>
<div id="cross-entropy" class="section level3">
<h3><span class="header-section-number">2.1.6</span> Cross-entropy</h3>
<p>If in the definition of Shannon entropy (and differential entropy) the expectation over
the log-density (say <span class="math inline">\(g(x)\)</span> of distribution <span class="math inline">\(G\)</span>) is with regard to a different distribution <span class="math inline">\(F\)</span> over the same state space
we arrive at the <strong>cross-entropy</strong>
<span class="math display">\[H(F, G) =-\text{E}_F\left( \log g(x)  \right)\]</span></p>
<p>Therefore, cross-entropy is a measure linking two distributions <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span>.</p>
<p>Note that</p>
<ul>
<li>cross-entropy is not symmetric with regard to <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span>, because
the expectation is taken with reference to <span class="math inline">\(F\)</span>.</li>
<li>By construction <span class="math inline">\(H(F, F) = H(F)\)</span>.</li>
<li>if <span class="math inline">\(G\)</span> is uniform then <span class="math inline">\(H(F, G) = C\)</span> (constant, not depending on <span class="math inline">\(F\)</span>).</li>
</ul>
<p>A crucial property of the cross-entropy <span class="math inline">\(H(F, G)\)</span> is that it is bounded below by the entropy of <span class="math inline">\(F\)</span>, therefore
<span class="math display">\[
H(F, G) \geq H(F)
\]</span>
with equality for <span class="math inline">\(F=G\)</span>.</p>
<p>Equivalently we can write
<span class="math display">\[
H(F, G)-H(F) \geq 0
\]</span><br />
In fact, this recalibrated cross-entropy turns out to be more fundamental than both cross-entropy and Shannon resp. differential entropy. It will be studied in detail in the next section.</p>

<div class="example">
<p><span id="exm:crossentropynormals" class="example"><strong>Example 2.5  </strong></span>Cross-entropy between two normals:</p>
Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2_{\text{ref}})\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>.
Then the cross-entropy is
<span class="math display">\[
H(F_{\text{ref}}, F) = \frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 } 
 +\frac{\sigma^2_{\text{ref}}}{\sigma^2}  +\log(2 \pi \sigma^2) \right)
\]</span>
</div>


<div class="example">
<span id="exm:crossentropylowerbount" class="example"><strong>Example 2.6  </strong></span>If <span class="math inline">\(\mu_{\text{ref}} = \mu\)</span> and <span class="math inline">\(\sigma^2_{\text{ref}} = \sigma^2\)</span>
then the above cross-entropy <span class="math inline">\(H(F,G)\)</span> degenerates to the differential entropy
<span class="math inline">\(H(F_{\text{ref}}) = \frac{1}{2} \left(\log( 2 \pi \sigma^2_{\text{ref}}) +1 \right)\)</span>.
</div>

</div>
</div>
<div id="kullback-leibler-divergence" class="section level2">
<h2><span class="header-section-number">2.2</span> Kullback-Leibler divergence</h2>
<div id="definition" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Definition</h3>
<p>Also known as <strong>relative entropy</strong> and <strong>discrimination information</strong>.</p>
<p>The <strong>relative entropy</strong> measures the <strong>divergence</strong>
of a distribution <span class="math inline">\(G\)</span> from the distribution <span class="math inline">\(F\)</span> and is defined as
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F,G) &amp;= \text{E}_F\log\left(\frac{f(x)}{g(x)}\right) \\
&amp; =
\underbrace{-\text{E}_F(\log g(x))}_{\text{cross-entropy}} - 
 (\underbrace{-\text{E}_F (\log f(x))  }_\text{(differential) entropy})  \\
&amp; = H(F, G)-H(F) \\
\end{split}
\]</span></p>
<ul>
<li><span class="math inline">\(D_{\text{KL}}(F, G)\)</span> measures the amount of information lost if <span class="math inline">\(G\)</span> is used to approximate <span class="math inline">\(F\)</span>.</li>
<li>If <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are identical (and no information is lost) then <span class="math inline">\(D_{\text{KL}}(F,G)=0\)</span>.</li>
<li>If <span class="math inline">\(G\)</span> is uniform then <span class="math inline">\(D_{\text{KL}}(F, G) = -H(F) + C\)</span>, i.e. the negative Shannon/differential entropy of <span class="math inline">\(F\)</span> measures the loss when <span class="math inline">\(F\)</span> is approximated by a uniform.</li>
</ul>
<p>(Note: here “divergence” measures the dissimilarity between probability distributions. This type of divergence is not related and should not be confused with divergence (div) as used in vector analysis.)</p>
<p>The term divergence (rather than distance) implies also that the distributions <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are not interchangeable in <span class="math inline">\(D_{\text{KL}}(F, G)\)</span>.</p>
<p>In applications in statistics the typical roles of <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are:</p>
<ul>
<li><span class="math inline">\(F\)</span> as the (unknown) underlying true model for the data generating process</li>
<li><span class="math inline">\(G\)</span> as the approximating model (e.g. some parametric family)</li>
</ul>
<p>In Bayesian statistics we use</p>
<ul>
<li><span class="math inline">\(F\)</span> as posterior distribution</li>
<li><span class="math inline">\(G\)</span> as prior distribution</li>
</ul>
<p>There exist various notations for KL divergence in the literature.
Here we use <span class="math inline">\(D_{KL}(F, G)\)</span> but often you can find <span class="math inline">\(KL(F || G)\)</span>
or <span class="math inline">\(\boldsymbol I^{KL}(F; G)\)</span> in other references.</p>
</div>
<div id="properties-of-kl-divergence" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Properties of KL divergence</h3>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(D_{\text{KL}}(F, G) \neq D_{\text{KL}}(G, F)\)</span>, i.e., KL divergence is not symmetric, <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> cannot be interchanged.</li>
<li><span class="math inline">\(D_{\text{KL}}(F, G) = 0\)</span> if and only if <span class="math inline">\(F=G\)</span>, i.e., the KL divergence is zero if and only if <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are identical.</li>
<li><span class="math inline">\(D_{\text{KL}}(F, G)\geq 0\)</span>, proof via the <strong>Jensen Inequality</strong>.</li>
<li><span class="math inline">\(D_{\text{KL}}(F, G)\)</span> remains invariant under coordinate transformations,
i.e. it is an invariant geometric quantity.</li>
</ol>
<p>Note that in the KL divergence the expectation is taken over a ratio of densities (or ratio of probabilities for discrete random variables). This is what creates the transformation invariance.</p>
<p>For more details and proofs of properties 3 and 4 see Worksheet 1.</p>
</div>
<div id="examples" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Examples</h3>

<div class="example">
<p><span id="exm:klnormal" class="example"><strong>Example 2.7  </strong></span>KL divergence between two univariate normals:</p>
Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2_{\text{ref}})\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>.
Then
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\text{ref}},F) &amp;= H(F_{\text{ref}}, F) - H(F_{\text{ref}}) \\
&amp;= \frac{1}{2} \left(   \frac{(\mu-\mu_{\text{ref}})^2}{\sigma^2}  + \frac{\sigma_{\text{ref}}^2}{\sigma^2}
-\log\left(\frac{\sigma_{\text{ref}}^2}{\sigma^2}\right)-1  
   \right) \\
\end{split}
\]</span>
As special case if variances are equal <span class="math inline">\(\sigma^2=\sigma^2_{\text{ref}}\)</span> we get
<span class="math display">\[D_{\text{KL}}(F_{\text{ref}},F)=\frac{1}{2} \left(\frac{(\mu-\mu_{\text{ref}})^2}{\sigma^2_{\text{ref}}}\right)\]</span>
so the KL divergence reduces to the squared standardised difference of means.
</div>


<div class="example">
<p><span id="exm:klcateg" class="example"><strong>Example 2.8  </strong></span>KL divergence between two categorical distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>:</p>
<p>With probabilities <span class="math inline">\(p_1,\dots,p_k\)</span> and <span class="math inline">\(q_1,\dots,q_k\)</span> we get:</p>
<p><span class="math display">\[\begin{equation*}
D_{\text{KL}}(P, Q)=\sum_{i}p_i\log\left(\frac{p_i}{q_i}\right) \approx \begin{cases}
 \frac{1}{2}\sum_{i}\frac{(p_i-q_i)^2}{q_i} =\frac{1}{2} D_{\text{Pearson}}(P, Q)\, \text{   (=$p_i$ around $q_i$) }\\
\frac{1}{2}\sum_{i}\frac{(p_i-q_i)^2}{p_i} =\frac{1}{2} D_{\text{Pearson}}(Q, P)\, \text{   (=$q_i$ around $p_i$) } \\
\end{cases}
\end{equation*}\]</span>
<span class="math inline">\(D_{\text{Pearson}}(P; Q) = \sum_{i}\frac{(p_i-q_i)^2}{q_i}\)</span> is called the Pearson <span class="math inline">\(\chi^2\)</span> divergence, a member of
the family of <span class="math inline">\(f\)</span>-divergences.</p>
<p>If <span class="math inline">\(O\)</span> and <span class="math inline">\(E\)</span> are given as observed and expected distribution, with corresponding observed frequencies <span class="math inline">\(o_i\)</span> and expected frequencies <span class="math inline">\(e_i\)</span> and <span class="math inline">\(n\)</span> the number of total counts, then
<span class="math inline">\(n D_{\text{Pearson}}(O; E) = n \sum_{i=1}^d\frac{(o_i-e_i)^2}{e_i} = X^2_{\text{Pearson}}\)</span> yields the Pearson <span class="math inline">\(\chi^2\)</span> test statistic.</p>
<p>Thus, the chi-squared statistic is in effect an second order approximation to the KL divergence
between two discrete distributions!</p>
See Worksheet 1 for further details and derivations.
</div>

</div>
</div>
<div id="local-quadratic-approximation-and-expected-fisher-information" class="section level2">
<h2><span class="header-section-number">2.3</span> Local quadratic approximation and expected Fisher information</h2>
<p>KL information measures the divergence of two distributions.
We may thus use relative entropy to measure the divergence between two distributions in the same family, separated in parameter space only by some small <span class="math inline">\(\boldsymbol \varepsilon\)</span>:</p>
<p><span class="math display">\[D_{\text{KL}}(F_{\boldsymbol \theta}, F_{\boldsymbol \theta+\boldsymbol \varepsilon}) = ?\]</span></p>
<p>To evaluate this we first consider the quadratic approximation of
the log density <span class="math inline">\(\log f(x | \boldsymbol \theta) = h(\boldsymbol \theta)\)</span> as a function of the parameter vector <span class="math inline">\(\boldsymbol \theta\)</span>. A Taylor series expansion <span class="math inline">\(h(\boldsymbol \theta+\boldsymbol \varepsilon) = h(\boldsymbol \theta) + \nabla h(\boldsymbol \theta)\boldsymbol \varepsilon+ \frac{1}{2} \boldsymbol \varepsilon^T \nabla^T \nabla h(\boldsymbol \theta) \boldsymbol \varepsilon+ \ldots\)</span>
yields in second order
<span class="math display">\[
\log f(x|\boldsymbol \theta+\boldsymbol \varepsilon) \approx \log f(x|\boldsymbol \theta) + 
   \nabla\log f(x|\boldsymbol \theta) \, \boldsymbol \varepsilon+ \frac{1}{2}\boldsymbol \varepsilon^T \nabla^T\nabla\log f(x|\boldsymbol \theta)\boldsymbol \varepsilon\,.
\]</span>
Note that the gradient and the Hessian matrix is computed with regard to <span class="math inline">\(\boldsymbol \theta\)</span> and <span class="math inline">\(x\)</span> is assumed fixed.</p>
<p>With this the KL divergence between <span class="math inline">\(F_{\boldsymbol \theta}\)</span> and <span class="math inline">\(F_{\boldsymbol \theta+\boldsymbol \varepsilon}\)</span> becomes
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\boldsymbol \theta}, F_{\boldsymbol \theta+\boldsymbol \varepsilon}) 
   &amp;= \text{E}_{F_{\boldsymbol \theta}} \log f(x|\boldsymbol \theta) -  \text{E}_{F_{\boldsymbol \theta}} \log f(x|\boldsymbol \theta+ \boldsymbol \varepsilon)  \\
 &amp; \approx -\text{E}_{F_{\boldsymbol \theta}} \left(\nabla\log f(x|\boldsymbol \theta) \, \boldsymbol \varepsilon\right) -\text{E}_{F_{\boldsymbol \theta}} \left(\frac{1}{2}\boldsymbol \varepsilon^T\nabla^T\nabla\log f(x|\boldsymbol \theta)\boldsymbol \varepsilon\right)\\
&amp;= -\underbrace{\text{E}_{F_{\boldsymbol \theta}} \left(\nabla\log f(x|\boldsymbol \theta) \right)}_{\text{vanishes, see below}}  \, \boldsymbol \varepsilon+ \frac{1}{2}\boldsymbol \varepsilon^T \, \text{E}_{F_{\boldsymbol \theta}} \left( -\nabla^T\nabla\log f(x|\boldsymbol \theta)\right) \boldsymbol \varepsilon\\
&amp; =\frac{1}{2}\boldsymbol \varepsilon^T \underbrace{\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)}_{\text{expected Fisher information}}\boldsymbol \varepsilon\\
\end{split} 
\]</span></p>
<p>The term <span class="math inline">\(\text{E}_{F_{\boldsymbol \theta}} \left( \nabla\log f(x|\boldsymbol \theta) \right) =0\)</span> because
<span class="math inline">\(\nabla\log f(x|\boldsymbol \theta) = f(x|\boldsymbol \theta)^{-1} \nabla f(x|\boldsymbol \theta)\)</span>
and thus <span class="math inline">\(\text{E}_{F_{\boldsymbol \theta}} \left( \nabla \log f(x|\boldsymbol \theta) \right) = \int \nabla f(x|\boldsymbol \theta) d\boldsymbol \theta= \nabla \int f(x|\boldsymbol \theta) d\boldsymbol \theta= \nabla 1 = 0\)</span> assuming
exchange of integration and differentiation is possible.</p>
<p>Therefore, the expected Fisher information matrix
<span class="math display">\[
\boldsymbol I^{\text{Fisher}} = -\text{E}_{F_{\boldsymbol \theta}} \left( \nabla^T\nabla\log f(x|\boldsymbol \theta)\right)
\]</span>
arises from a local quadratic approximation of KL divergence.</p>
<p>Note that as there is no data involved as the expected Fisher information is purely a property of the model, or more precisely of the space of the models indexed by <span class="math inline">\(\boldsymbol \theta\)</span>.
In fact, in <em>information geometry</em> the expected Fisher information
plays an important role as the metric tensor of this space ( <a href="https://en.wikipedia.org/wiki/Fisher_information_metric" class="uri">https://en.wikipedia.org/wiki/Fisher_information_metric</a> ).</p>

<div class="example">
<p><span id="exm:expectedfishernormal" class="example"><strong>Example 2.9  </strong></span>Expected Fisher information for the normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<p>The log-density is
<span class="math display">\[
\log f(x | \mu, \sigma^2) = -\frac{1}{2} \log(\sigma^2) 
-\frac{1}{2 \sigma^2} (x-\mu)^2 - \frac{1}{2}\log(2 \pi)
\]</span>
The gradient with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> (!) is
<span class="math display">\[
\nabla \log f(x | \mu, \sigma^2) =
\begin{pmatrix}
\frac{1}{\sigma^2} (x-\mu) \\
- \frac{1}{2 \sigma^2} + \frac{1}{2 \sigma^4} (x- \mu)^2 \\
\end{pmatrix}
\]</span>
Hint for calculating the gradient: replace <span class="math inline">\(\sigma^2\)</span> by <span class="math inline">\(v\)</span> and then take the partial derivative with regard to <span class="math inline">\(v\)</span>, then substitute back.</p>
The Hessian matrix is
<span class="math display">\[
\nabla^T\nabla \log f(x | \mu, \sigma^2) =
\begin{pmatrix}
-\frac{1}{\sigma^2} &amp; -\frac{1}{\sigma^4} (x-\mu)\\
-\frac{1}{\sigma^4} (x-\mu) &amp;  \frac{1}{2\sigma^4} - \frac{1}{\sigma^6}(x- \mu)^2 \\
\\
\end{pmatrix}
\]</span>
As <span class="math inline">\(\text{E}(x) = \mu\)</span> we have <span class="math inline">\(\text{E}(x-\mu) =0\)</span>.
Furthermore, with <span class="math inline">\(\text{E}( (x-\mu)^2 ) =\sigma^2\)</span> we see that
<span class="math inline">\(\text{E}\left(\frac{1}{\sigma^6}(x- \mu)^2\right) = \frac{1}{\sigma^4}\)</span>. Therefore
the expected Fisher information matrix as the negative expected Hessian matrix is
<span class="math display">\[
\boldsymbol I^{\text{Fisher}}\left(\mu,\sigma^2\right) = \begin{pmatrix} \frac{1}{\sigma^2} &amp; 0 \\ 0 &amp; \frac{1}{2\sigma^4} \end{pmatrix}
\]</span>
</div>

</div>
<div id="entropy-learning-and-maximum-likelihood" class="section level2">
<h2><span class="header-section-number">2.4</span> Entropy learning and maximum likelihood</h2>
<div id="the-relative-entropy-between-true-model-and-approximating-model" class="section level3">
<h3><span class="header-section-number">2.4.1</span> The relative entropy between true model and approximating model</h3>
<p>Assume we have observations <span class="math inline">\(x_1, \ldots, x_n\)</span>. The data is sampled from <span class="math inline">\(F\)</span>, the true but unknown data generating distribution. We also specify models <span class="math inline">\(G_{\boldsymbol \theta}\)</span>
indexed by <span class="math inline">\(\boldsymbol \theta\)</span> to approximate <span class="math inline">\(F\)</span>.</p>
<p>The relative entropy <span class="math inline">\(D_{\text{KL}}(F,G_{\boldsymbol \theta})\)</span> then measures the divergence of the approximation <span class="math inline">\(G_{\boldsymbol \theta}\)</span>
from the unknow true model <span class="math inline">\(F\)</span>. It can be written as:
<span class="math display">\[
D_{\text{KL}}(F,G_{\boldsymbol \theta})=
\underbrace{- \text{E}_{F}\log g_{\boldsymbol \theta}(x)}_{\text{cross-entropy}}
-(\underbrace{-\text{E}_{F}\log f(x)}_{\text{entropy of $F$, does not depend on $\boldsymbol \theta$}})
\]</span></p>
<p>However, since we do not know <span class="math inline">\(F\)</span> we cannot actually compute this divergence. Nonetheless, we may use
the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> — a function of the observed data — as approximation for <span class="math inline">\(F\)</span>, and in this way arrive at an approximation for <span class="math inline">\(D_{\text{KL}}(F,G_{\boldsymbol \theta})\)</span> that becomes more and more accurate with growing sample size.</p>
<hr />
<p>Recall “Law of Large Numbers” :</p>
<ul>
<li><p>By the strong law of large numbers the empirical distribution
<span class="math inline">\(\hat{F}_n\)</span> converges to the true underlying distribution <span class="math inline">\(F\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> almost surely:
<span class="math display">\[
\hat{F}_n\overset{a. s.}{\to} F
\]</span></p></li>
<li><p>For <span class="math inline">\(n \rightarrow \infty\)</span> the average <span class="math inline">\(\text{E}_{\hat{F}_n}(h(X)) = \frac{1}{n} \sum_{i=1}^n h(x_i)\)</span> converges to the expectation <span class="math inline">\(\text{E}_{F}(h(X))\)</span>.</p></li>
</ul>
<hr />
<p>Hence, for large sample size <span class="math inline">\(n\)</span> we can approximate cross-entropy and as a result the KL divergence. The cross-entropy <span class="math inline">\(H(F, G_{\boldsymbol \theta})\)</span> is approximated by the empirical cross-entropy where the expectation is taken with regard to <span class="math inline">\(\hat{F}_n\)</span> rather than <span class="math inline">\(F\)</span>:
<span class="math display">\[
\begin{split}
H(F, G_{\boldsymbol \theta}) &amp; \approx H(\hat{F}_n, G_{\boldsymbol \theta}) \\
                  &amp; = - \text{E}_{\hat{F}_n} (\log(x))  \\
                  &amp; = -\frac{1}{n} \sum_{i=1}^n \log g(x_i | \boldsymbol \theta) \\
                  &amp; -\frac{1}{n} l_n ({\boldsymbol \theta})
\end{split}
\]</span>
This turns out to be equal to the negative log-likelihood standardised by the sample size <span class="math inline">\(n\)</span>! Or in other words, the <strong>likelihood</strong> is the <strong>negative empirical cross-entropy (times sample size <span class="math inline">\(n\)</span>)</strong>.</p>
<p>The KL divergence <span class="math inline">\(D_{\text{KL}}(F,G_{\boldsymbol \theta})\)</span> can therefore be approximated by
<span class="math display">\[
D_{\text{KL}}(F,G_{\boldsymbol \theta}) \approx  -\frac{1}{n} l_n ({\boldsymbol \theta}) + C
\]</span>
where <span class="math inline">\(C\)</span> is a constant (the negative entropy of the true distribution <span class="math inline">\(F\)</span>) that
does not depend on the parameters <span class="math inline">\(\boldsymbol \theta\)</span> (and hence does not matter when optimising the divergence).</p>
</div>
<div id="minimum-kl-divergence-and-maximum-likelihood" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Minimum KL divergence and maximum likelihood</h3>
<p>If we were to know <span class="math inline">\(F\)</span> we would simply minimise <span class="math inline">\(D_{\text{KL}}(F, G_{\boldsymbol \theta})\)</span> to find the particular model <span class="math inline">\(G_{\boldsymbol \theta}\)</span> that is closest to the true model. Equivalently, we would minimise the cross-entropy <span class="math inline">\(H(F, G_{\boldsymbol \theta})\)</span>.
However, since we actually don’t know <span class="math inline">\(F\)</span> this is not possible.</p>
<p>However, for large sample size <span class="math inline">\(n\)</span> when the empirical distribution <span class="math inline">\(\hat{F}_n\)</span>
is a good approximation for <span class="math inline">\(F\)</span>, we can use the above approximation.
Thus, instead of minimising the KL divergence
<span class="math inline">\(D_{\text{KL}}(F, G_{\boldsymbol \theta})\)</span> we simply minimise <span class="math inline">\(H(\hat{F}_n, G_{\boldsymbol \theta})\)</span> which
is the same as maximising the likelihood <span class="math inline">\(l_n ({\boldsymbol \theta})\)</span>.</p>
<p>Conversely, it turns out that maximising the likelihood with regard to the <span class="math inline">\(\boldsymbol \theta\)</span> is equivalent (at least asymptotically for large <span class="math inline">\(n\)</span>!) to minimising the KL divergence of the approximating model and the unknown true model!</p>
<p><span class="math display">\[
\begin{split}
\hat{\boldsymbol \theta}^{ML} &amp;= \underset{\boldsymbol \theta}{\arg \max}\,\, l_n(\boldsymbol \theta) \\
 &amp;= \underset{\boldsymbol \theta}{\arg \min}\,\, H(\hat{F}_n, G_{\boldsymbol \theta}) \\
 &amp;\approx \underset{\boldsymbol \theta}{\arg \min}\,\, D_{\text{KL}}(F, G_{\boldsymbol \theta}) \\
\end{split}
\]</span></p>
<p>Therefore, the reasoning behind the method of <strong>maximum likelihood</strong> is that it minimises a <strong>large sample approximation of the KL divergence</strong> of the candidate model <span class="math inline">\(G_{\boldsymbol \theta}\)</span> from the unkown true model <span class="math inline">\(F\)</span>.</p>
<p>As a consequence of the close link of maximum likelhood and relative entropy
maximum likelihood inherits for large <span class="math inline">\(n\)</span> (and only then!) all the optimality properties from KL divergence. These will be discussed in more detail later in the course.</p>
</div>
<div id="further-connections" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Further connections</h3>
<p>Since minimising KL divergence contains ML estimation as special case you may wonder whether there is a broader justification of
relative entropy in the context of statistical data analysis? Indeed, KL divergence has strong geometrical interpretation that forms the basis of <em>information geometry</em> and it is also linked stronlgy to to probabilistic forecasting.</p>
<p>In the framework of so-called <em>scoring rules</em> (cf. <a href="https://en.wikipedia.org/wiki/Scoring_rule" class="uri">https://en.wikipedia.org/wiki/Scoring_rule</a> )
the only local proper scoring rule is the negative log-probability (“surprise”).
The expected “surprise” is the cross-entropy
and relative entropy is the corresponding natural divergence connected with the log scoring rule.</p>
<p>Furthermore, another intriguing property of KL divergence is that the relative entropy <span class="math inline">\(D_{\text{KL}}(F, G)\)</span> is the <em>only divergence measure</em> that is both a Bregman and <span class="math inline">\(f\)</span>-divergence.
Note that <span class="math inline">\(f\)</span>-divergences and Bregman-divergences (in turn related to proper scoring rules) are two large classes of measures of similarity and divergence between two probability distributions
(see <a href="https://en.wikipedia.org/wiki/Bregman_divergence" class="uri">https://en.wikipedia.org/wiki/Bregman_divergence</a> and <a href="https://en.wikipedia.org/wiki/F-divergence" class="uri">https://en.wikipedia.org/wiki/F-divergence</a>
)
Finally, not only the likelihood estimation but also the Bayesian update rule (as discussed later in this module) is another special case of entropy learning.</p>

<p></p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="01-likelihood1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="03-likelihood3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math20802-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
