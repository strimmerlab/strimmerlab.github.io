<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 From entropy to maximum likelihood | HTML</title>
  <meta name="description" content="Statistical Methods:<br />
Likelihood, Bayes and Regression</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2 From entropy to maximum likelihood | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 From entropy to maximum likelihood | HTML" />
  
  
  



<meta name="date" content="2021-03-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="01-likelihood1.html"/>
<link rel="next" href="03-likelihood3.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH20802/index.html">MATH20802 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Likelihood estimation and inference</b></span></li>
<li class="chapter" data-level="1" data-path="01-likelihood1.html"><a href="01-likelihood1.html"><i class="fa fa-check"></i><b>1</b> Overview of statistical learning</a><ul>
<li class="chapter" data-level="1.1" data-path="01-likelihood1.html"><a href="01-likelihood1.html#how-to-learn-from-data"><i class="fa fa-check"></i><b>1.1</b> How to learn from data?</a></li>
<li class="chapter" data-level="1.2" data-path="01-likelihood1.html"><a href="01-likelihood1.html#probability-theory-versus-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Probability theory versus statistical learning</a></li>
<li class="chapter" data-level="1.3" data-path="01-likelihood1.html"><a href="01-likelihood1.html#cartoon-of-statistical-learning"><i class="fa fa-check"></i><b>1.3</b> Cartoon of statistical learning</a></li>
<li class="chapter" data-level="1.4" data-path="01-likelihood1.html"><a href="01-likelihood1.html#likelihood"><i class="fa fa-check"></i><b>1.4</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="02-likelihood2.html"><a href="02-likelihood2.html"><i class="fa fa-check"></i><b>2</b> From entropy to maximum likelihood</a><ul>
<li class="chapter" data-level="2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy"><i class="fa fa-check"></i><b>2.1</b> Entropy</a><ul>
<li class="chapter" data-level="2.1.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#surprise-surprisal-or-shannon-information"><i class="fa fa-check"></i><b>2.1.2</b> Surprise, surprisal or Shannon information</a></li>
<li class="chapter" data-level="2.1.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#shannon-entropy"><i class="fa fa-check"></i><b>2.1.3</b> Shannon entropy</a></li>
<li class="chapter" data-level="2.1.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#differential-entropy"><i class="fa fa-check"></i><b>2.1.4</b> Differential entropy</a></li>
<li class="chapter" data-level="2.1.5" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-entropy-principle-to-characterise-distributions"><i class="fa fa-check"></i><b>2.1.5</b> Maximum entropy principle to characterise distributions</a></li>
<li class="chapter" data-level="2.1.6" data-path="02-likelihood2.html"><a href="02-likelihood2.html#cross-entropy"><i class="fa fa-check"></i><b>2.1.6</b> Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>2.2</b> Kullback-Leibler divergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#definition"><i class="fa fa-check"></i><b>2.2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#properties-of-kl-divergence"><i class="fa fa-check"></i><b>2.2.2</b> Properties of KL divergence</a></li>
<li class="chapter" data-level="2.2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#examples"><i class="fa fa-check"></i><b>2.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#local-quadratic-approximation-and-expected-fisher-information"><i class="fa fa-check"></i><b>2.3</b> Local quadratic approximation and expected Fisher information</a><ul>
<li class="chapter" data-level="2.3.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#linear-approximation"><i class="fa fa-check"></i><b>2.3.1</b> Linear approximation</a></li>
<li class="chapter" data-level="2.3.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#quadratic-approximation"><i class="fa fa-check"></i><b>2.3.2</b> Quadratic approximation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy-learning-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4</b> Entropy learning and maximum likelihood</a><ul>
<li class="chapter" data-level="2.4.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#the-relative-entropy-between-true-model-and-approximating-model"><i class="fa fa-check"></i><b>2.4.1</b> The relative entropy between true model and approximating model</a></li>
<li class="chapter" data-level="2.4.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#minimum-kl-divergence-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4.2</b> Minimum KL divergence and maximum likelihood</a></li>
<li class="chapter" data-level="2.4.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#further-connections"><i class="fa fa-check"></i><b>2.4.3</b> Further connections</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="03-likelihood3.html"><a href="03-likelihood3.html"><i class="fa fa-check"></i><b>3</b> Maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#principle-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.1</b> Principle of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#outline"><i class="fa fa-check"></i><b>3.1.1</b> Outline</a></li>
<li class="chapter" data-level="3.1.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#obtaining-mles-for-a-regular-model"><i class="fa fa-check"></i><b>3.1.2</b> Obtaining MLEs for a regular model</a></li>
<li class="chapter" data-level="3.1.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#invariance-property-of-the-maximum-likelihood"><i class="fa fa-check"></i><b>3.1.3</b> Invariance property of the maximum likelihood</a></li>
<li class="chapter" data-level="3.1.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#consistency-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>3.1.4</b> Consistency of maximum likelihood estimates</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#maximum-likelihood-estimation-in-practise"><i class="fa fa-check"></i><b>3.2</b> Maximum likelihood estimation in practise</a><ul>
<li class="chapter" data-level="3.2.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#worked-examples"><i class="fa fa-check"></i><b>3.2.1</b> Worked examples</a></li>
<li class="chapter" data-level="3.2.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#relationship-with-least-squares-estimation"><i class="fa fa-check"></i><b>3.2.2</b> Relationship with least squares estimation</a></li>
<li class="chapter" data-level="3.2.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#bias-and-maximum-likelihood"><i class="fa fa-check"></i><b>3.2.3</b> Bias and maximum likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information"><i class="fa fa-check"></i><b>3.3</b> Observed Fisher information</a><ul>
<li class="chapter" data-level="3.3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#motivation-and-definition"><i class="fa fa-check"></i><b>3.3.1</b> Motivation and definition</a></li>
<li class="chapter" data-level="3.3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#examples-of-observed-fisher-information"><i class="fa fa-check"></i><b>3.3.2</b> Examples of observed Fisher information</a></li>
<li class="chapter" data-level="3.3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#relationship-between-observed-and-expected-fisher-information"><i class="fa fa-check"></i><b>3.3.3</b> Relationship between observed and expected Fisher information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="04-likelihood4.html"><a href="04-likelihood4.html"><i class="fa fa-check"></i><b>4</b> Quadratic approximation and normal asymptotics</a><ul>
<li class="chapter" data-level="4.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#multivariate-statistics-for-random-vectors"><i class="fa fa-check"></i><b>4.1</b> Multivariate statistics for random vectors</a><ul>
<li class="chapter" data-level="4.1.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#covariance-and-correlation"><i class="fa fa-check"></i><b>4.1.1</b> Covariance and correlation</a></li>
<li class="chapter" data-level="4.1.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.1.2</b> Multivariate normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#approximate-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.2</b> Approximate distribution of maximum likelihood estimates</a><ul>
<li class="chapter" data-level="4.2.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-log-likelihood-resulting-from-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Quadratic log-likelihood resulting from normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-approximation-of-a-log-likelihood-function"><i class="fa fa-check"></i><b>4.2.2</b> Quadratic approximation of a log-likelihood function</a></li>
<li class="chapter" data-level="4.2.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.2.3</b> Asymptotic normality of maximum likelihood estimates</a></li>
<li class="chapter" data-level="4.2.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-optimal-efficiency"><i class="fa fa-check"></i><b>4.2.4</b> Asymptotic optimal efficiency</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quantifying-the-uncertainty-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.3</b> Quantifying the uncertainty of maximum likelihood estimates</a><ul>
<li class="chapter" data-level="4.3.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#estimating-the-variance-of-mles"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the variance of MLEs</a></li>
<li class="chapter" data-level="4.3.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#wald-statistic"><i class="fa fa-check"></i><b>4.3.2</b> Wald statistic</a></li>
<li class="chapter" data-level="4.3.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-confidence-intervals-using-the-wald-statistic"><i class="fa fa-check"></i><b>4.3.3</b> Normal confidence intervals using the Wald statistic</a></li>
<li class="chapter" data-level="4.3.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-tests-using-the-wald-statistic"><i class="fa fa-check"></i><b>4.3.4</b> Normal tests using the Wald statistic</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-of-a-non-regular-model"><i class="fa fa-check"></i><b>4.4</b> Example of a non-regular model</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="05-likelihood5.html"><a href="05-likelihood5.html"><i class="fa fa-check"></i><b>5</b> Likelihood-based confidence interval and likelihood ratio</a><ul>
<li class="chapter" data-level="5.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-based-confidence-intervals-and-wilks-statistic"><i class="fa fa-check"></i><b>5.1</b> Likelihood-based confidence intervals and Wilks statistic</a><ul>
<li class="chapter" data-level="5.1.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#general-idea-and-definition-of-wilks-statistic"><i class="fa fa-check"></i><b>5.1.1</b> General idea and definition of Wilks statistic</a></li>
<li class="chapter" data-level="5.1.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#quadratic-approximation-of-wilks-statistic-and-squared-wald-statistic"><i class="fa fa-check"></i><b>5.1.2</b> Quadratic approximation of Wilks statistic and squared Wald statistic</a></li>
<li class="chapter" data-level="5.1.3" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-the-wilks-statistic"><i class="fa fa-check"></i><b>5.1.3</b> Distribution of the Wilks statistic</a></li>
<li class="chapter" data-level="5.1.4" data-path="05-likelihood5.html"><a href="05-likelihood5.html#cutoff-values-for-the-likelihood-ci"><i class="fa fa-check"></i><b>5.1.4</b> Cutoff values for the likelihood CI</a></li>
<li class="chapter" data-level="5.1.5" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-ratio-test-lrt-using-wilks-statistic"><i class="fa fa-check"></i><b>5.1.5</b> Likelihood ratio test (LRT) using Wilks statistic</a></li>
<li class="chapter" data-level="5.1.6" data-path="05-likelihood5.html"><a href="05-likelihood5.html#origin-of-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.1.6</b> Origin of likelihood ratio statistic</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#generalised-likelihood-ratio-test-glrt"><i class="fa fa-check"></i><b>5.2</b> Generalised likelihood ratio test (GLRT)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="06-likelihood6.html"><a href="06-likelihood6.html"><i class="fa fa-check"></i><b>6</b> Optimality properties and conclusion</a><ul>
<li class="chapter" data-level="6.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#properties-of-maximum-likelihood-encountered-so-far"><i class="fa fa-check"></i><b>6.1</b> Properties of maximum likelihood encountered so far</a></li>
<li class="chapter" data-level="6.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency"><i class="fa fa-check"></i><b>6.2</b> Summarising data and the concept of minimal sufficiency</a></li>
<li class="chapter" data-level="6.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#concluding-remarks-on-maximum-likelihood"><i class="fa fa-check"></i><b>6.3</b> Concluding remarks on maximum likelihood</a><ul>
<li class="chapter" data-level="6.3.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#remark-on-kl-divergence"><i class="fa fa-check"></i><b>6.3.1</b> Remark on KL divergence</a></li>
<li class="chapter" data-level="6.3.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#what-happens-if-n-is-small"><i class="fa fa-check"></i><b>6.3.2</b> What happens if <span class="math inline">\(n\)</span> is small?</a></li>
<li class="chapter" data-level="6.3.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#model-selection"><i class="fa fa-check"></i><b>6.3.3</b> Model selection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Bayesian Statistics</b></span></li>
<li class="chapter" data-level="7" data-path="07-bayes1.html"><a href="07-bayes1.html"><i class="fa fa-check"></i><b>7</b> Essentials of Bayesian statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#bayes-theorem"><i class="fa fa-check"></i><b>7.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#principle-of-bayesian-learning"><i class="fa fa-check"></i><b>7.2</b> Principle of Bayesian learning</a></li>
<li class="chapter" data-level="7.3" data-path="07-bayes1.html"><a href="07-bayes1.html#what-is-exactly-is-the-bayesian-estimate"><i class="fa fa-check"></i><b>7.3</b> What is exactly is the “Bayesian estimate”?</a></li>
<li class="chapter" data-level="7.4" data-path="07-bayes1.html"><a href="07-bayes1.html#computer-implementation-of-bayesian-learning"><i class="fa fa-check"></i><b>7.4</b> Computer implementation of Bayesian learning</a></li>
<li class="chapter" data-level="7.5" data-path="07-bayes1.html"><a href="07-bayes1.html#bayesian-interpretation-of-probability"><i class="fa fa-check"></i><b>7.5</b> Bayesian interpretation of probability</a><ul>
<li class="chapter" data-level="7.5.1" data-path="07-bayes1.html"><a href="07-bayes1.html#what-makes-you-bayesian"><i class="fa fa-check"></i><b>7.5.1</b> What makes you “Bayesian”?</a></li>
<li class="chapter" data-level="7.5.2" data-path="07-bayes1.html"><a href="07-bayes1.html#mathematics-of-probability"><i class="fa fa-check"></i><b>7.5.2</b> Mathematics of probability</a></li>
<li class="chapter" data-level="7.5.3" data-path="07-bayes1.html"><a href="07-bayes1.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.5.3</b> Interpretations of probability</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="07-bayes1.html"><a href="07-bayes1.html#historical-developments"><i class="fa fa-check"></i><b>7.6</b> Historical developments</a></li>
<li class="chapter" data-level="7.7" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning"><i class="fa fa-check"></i><b>7.7</b> Connection with entropy learning</a><ul>
<li class="chapter" data-level="7.7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#zero-forcing-property"><i class="fa fa-check"></i><b>7.7.1</b> Zero forcing property</a></li>
<li class="chapter" data-level="7.7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning-1"><i class="fa fa-check"></i><b>7.7.2</b> Connection with entropy learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="08-bayes2.html"><a href="08-bayes2.html"><i class="fa fa-check"></i><b>8</b> Beta-Binomial model for estimating a proportion</a><ul>
<li class="chapter" data-level="8.1" data-path="08-bayes2.html"><a href="08-bayes2.html#binomial-likelihood"><i class="fa fa-check"></i><b>8.1</b> Binomial likelihood</a></li>
<li class="chapter" data-level="8.2" data-path="08-bayes2.html"><a href="08-bayes2.html#excursion-properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>8.2</b> Excursion: Properties of the Beta distribution</a></li>
<li class="chapter" data-level="8.3" data-path="08-bayes2.html"><a href="08-bayes2.html#beta-prior-distribution"><i class="fa fa-check"></i><b>8.3</b> Beta prior distribution</a></li>
<li class="chapter" data-level="8.4" data-path="08-bayes2.html"><a href="08-bayes2.html#computing-the-posterior-distribution"><i class="fa fa-check"></i><b>8.4</b> Computing the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="09-bayes3.html"><a href="09-bayes3.html"><i class="fa fa-check"></i><b>9</b> Properties of Bayesian learning</a><ul>
<li class="chapter" data-level="9.1" data-path="09-bayes3.html"><a href="09-bayes3.html#prior-acting-as-pseudo-data"><i class="fa fa-check"></i><b>9.1</b> Prior acting as pseudo-data</a></li>
<li class="chapter" data-level="9.2" data-path="09-bayes3.html"><a href="09-bayes3.html#linear-shrinkage-of-mean"><i class="fa fa-check"></i><b>9.2</b> Linear shrinkage of mean</a></li>
<li class="chapter" data-level="9.3" data-path="09-bayes3.html"><a href="09-bayes3.html#conjugacy-of-prior-and-posterior-distribution"><i class="fa fa-check"></i><b>9.3</b> Conjugacy of prior and posterior distribution</a></li>
<li class="chapter" data-level="9.4" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-asymptotics"><i class="fa fa-check"></i><b>9.4</b> Large sample asymptotics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-limits-of-mean-and-variance"><i class="fa fa-check"></i><b>9.4.1</b> Large sample limits of mean and variance</a></li>
<li class="chapter" data-level="9.4.2" data-path="09-bayes3.html"><a href="09-bayes3.html#asymptotic-normality-of-the-posterior-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Asymptotic Normality of the Posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="09-bayes3.html"><a href="09-bayes3.html#posterior-variance-for-finite-n"><i class="fa fa-check"></i><b>9.5</b> Posterior variance for finite <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-bayes4.html"><a href="10-bayes4.html"><i class="fa fa-check"></i><b>10</b> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a><ul>
<li class="chapter" data-level="10.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-normal-model-to-estimate-mean"><i class="fa fa-check"></i><b>10.1</b> Normal-Normal model to estimate mean</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihood"><i class="fa fa-check"></i><b>10.1.1</b> Normal likelihood</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-prior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Normal prior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-posterior-distribution"><i class="fa fa-check"></i><b>10.1.3</b> Normal posterior distribution</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-and-stein-paradox"><i class="fa fa-check"></i><b>10.1.4</b> Large sample asymptotics and Stein paradox</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-normal-model-to-estimate-variance"><i class="fa fa-check"></i><b>10.2</b> Inverse-Gamma-Normal model to estimate variance</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Inverse Gamma distribution</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihoood"><i class="fa fa-check"></i><b>10.2.2</b> Normal likelihoood</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-prior-distribution"><i class="fa fa-check"></i><b>10.2.3</b> Inverse Gamma prior distribution</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-posterior-distribution"><i class="fa fa-check"></i><b>10.2.4</b> Inverse Gamma posterior distribution</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-1"><i class="fa fa-check"></i><b>10.2.5</b> Large sample asymptotics</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-bayes4.html"><a href="10-bayes4.html#estimating-precision"><i class="fa fa-check"></i><b>10.2.6</b> Estimating precision</a></li>
<li class="chapter" data-level="10.2.7" data-path="10-bayes4.html"><a href="10-bayes4.html#joint-estimation-of-mean-and-variance"><i class="fa fa-check"></i><b>10.2.7</b> Joint estimation of mean and variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-bayes5.html"><a href="11-bayes5.html"><i class="fa fa-check"></i><b>11</b> Shrinkage estimation using empirical risk minimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="11-bayes5.html"><a href="11-bayes5.html#linear-shrinkage"><i class="fa fa-check"></i><b>11.1</b> Linear shrinkage</a></li>
<li class="chapter" data-level="11.2" data-path="11-bayes5.html"><a href="11-bayes5.html#james-stein-estimator"><i class="fa fa-check"></i><b>11.2</b> James-Stein estimator</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-bayes6.html"><a href="12-bayes6.html"><i class="fa fa-check"></i><b>12</b> Bayesian model comparison using Bayes factors and the BIC</a><ul>
<li class="chapter" data-level="12.1" data-path="12-bayes6.html"><a href="12-bayes6.html#the-bayes-factor"><i class="fa fa-check"></i><b>12.1</b> The Bayes factor</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-bayes6.html"><a href="12-bayes6.html#connection-with-relative-entropy"><i class="fa fa-check"></i><b>12.1.1</b> Connection with relative entropy</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-bayes6.html"><a href="12-bayes6.html#interpretation-of-and-scale-for-bayes-factor"><i class="fa fa-check"></i><b>12.1.2</b> Interpretation of and scale for Bayes factor</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-bayes6.html"><a href="12-bayes6.html#computing-textprd-m-for-simple-and-composite-models"><i class="fa fa-check"></i><b>12.1.3</b> Computing <span class="math inline">\(\text{Pr}(D | M)\)</span> for simple and composite models</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-bayes6.html"><a href="12-bayes6.html#bayes-factor-versus-likelihood-ratio"><i class="fa fa-check"></i><b>12.1.4</b> Bayes factor versus likelihood ratio</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-bayes6.html"><a href="12-bayes6.html#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><i class="fa fa-check"></i><b>12.2</b> Approximate computation of the marginal likelihood and of the log-Bayes factor</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-bayes6.html"><a href="12-bayes6.html#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fa fa-check"></i><b>12.2.1</b> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-bayes6.html"><a href="12-bayes6.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>12.2.2</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-bayes6.html"><a href="12-bayes6.html#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fa fa-check"></i><b>12.2.3</b> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-bayes6.html"><a href="12-bayes6.html#model-complexity-and-occams-razor"><i class="fa fa-check"></i><b>12.2.4</b> Model complexity and Occams razor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-bayes7.html"><a href="13-bayes7.html"><i class="fa fa-check"></i><b>13</b> False discovery rates</a><ul>
<li class="chapter" data-level="13.1" data-path="13-bayes7.html"><a href="13-bayes7.html#general-setup"><i class="fa fa-check"></i><b>13.1</b> General setup</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-bayes7.html"><a href="13-bayes7.html#overview-1"><i class="fa fa-check"></i><b>13.1.1</b> Overview</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-bayes7.html"><a href="13-bayes7.html#choosing-between-h_0-and-h_a"><i class="fa fa-check"></i><b>13.1.2</b> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span></a></li>
<li class="chapter" data-level="13.1.3" data-path="13-bayes7.html"><a href="13-bayes7.html#true-and-false-positives-and-negatives"><i class="fa fa-check"></i><b>13.1.3</b> True and false positives and negatives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-bayes7.html"><a href="13-bayes7.html#specificity-and-sensitivity"><i class="fa fa-check"></i><b>13.2</b> Specificity and Sensitivity</a></li>
<li class="chapter" data-level="13.3" data-path="13-bayes7.html"><a href="13-bayes7.html#fdr-and-fndr"><i class="fa fa-check"></i><b>13.3</b> FDR and FNDR</a></li>
<li class="chapter" data-level="13.4" data-path="13-bayes7.html"><a href="13-bayes7.html#bayesian-perspective"><i class="fa fa-check"></i><b>13.4</b> Bayesian perspective</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-bayes7.html"><a href="13-bayes7.html#two-component-mixture-model"><i class="fa fa-check"></i><b>13.4.1</b> Two component mixture model</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-bayes7.html"><a href="13-bayes7.html#local-fdr"><i class="fa fa-check"></i><b>13.4.2</b> Local FDR</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-bayes7.html"><a href="13-bayes7.html#q-values"><i class="fa fa-check"></i><b>13.4.3</b> q-values</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="13-bayes7.html"><a href="13-bayes7.html#software"><i class="fa fa-check"></i><b>13.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-bayes8.html"><a href="14-bayes8.html"><i class="fa fa-check"></i><b>14</b> Optimality properties and summary</a><ul>
<li class="chapter" data-level="14.1" data-path="14-bayes8.html"><a href="14-bayes8.html#bayesian-statistics-in-a-nutshell"><i class="fa fa-check"></i><b>14.1</b> Bayesian statistics in a nutshell</a><ul>
<li class="chapter" data-level="14.1.1" data-path="14-bayes8.html"><a href="14-bayes8.html#remarks"><i class="fa fa-check"></i><b>14.1.1</b> Remarks</a></li>
<li class="chapter" data-level="14.1.2" data-path="14-bayes8.html"><a href="14-bayes8.html#advantages"><i class="fa fa-check"></i><b>14.1.2</b> Advantages</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="14-bayes8.html"><a href="14-bayes8.html#frequentist-properties-of-bayesian-estimators"><i class="fa fa-check"></i><b>14.2</b> Frequentist properties of Bayesian estimators</a></li>
<li class="chapter" data-level="14.3" data-path="14-bayes8.html"><a href="14-bayes8.html#specifying-the-prior-problem-or-advantage"><i class="fa fa-check"></i><b>14.3</b> Specifying the prior — problem or advantage?</a></li>
<li class="chapter" data-level="14.4" data-path="14-bayes8.html"><a href="14-bayes8.html#choosing-a-prior"><i class="fa fa-check"></i><b>14.4</b> Choosing a prior</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-bayes8.html"><a href="14-bayes8.html#some-guidelines"><i class="fa fa-check"></i><b>14.4.1</b> Some guidelines</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-bayes8.html"><a href="14-bayes8.html#jeffreys-prior"><i class="fa fa-check"></i><b>14.4.2</b> Jeffreys prior</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-bayes8.html"><a href="14-bayes8.html#optimality-of-bayes-inference"><i class="fa fa-check"></i><b>14.5</b> Optimality of Bayes inference</a></li>
<li class="chapter" data-level="14.6" data-path="14-bayes8.html"><a href="14-bayes8.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a><ul>
<li class="chapter" data-level="14.6.1" data-path="14-bayes8.html"><a href="14-bayes8.html#current-research"><i class="fa fa-check"></i><b>14.6.1</b> Current research</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Regression</b></span></li>
<li class="chapter" data-level="15" data-path="15-regression1.html"><a href="15-regression1.html"><i class="fa fa-check"></i><b>15</b> Overview over regression modelling</a><ul>
<li class="chapter" data-level="15.1" data-path="15-regression1.html"><a href="15-regression1.html#general-setup-1"><i class="fa fa-check"></i><b>15.1</b> General setup</a></li>
<li class="chapter" data-level="15.2" data-path="15-regression1.html"><a href="15-regression1.html#objectives"><i class="fa fa-check"></i><b>15.2</b> Objectives</a></li>
<li class="chapter" data-level="15.3" data-path="15-regression1.html"><a href="15-regression1.html#regression-as-a-form-of-supervised-learning"><i class="fa fa-check"></i><b>15.3</b> Regression as a form of supervised learning</a></li>
<li class="chapter" data-level="15.4" data-path="15-regression1.html"><a href="15-regression1.html#various-regression-models-used-in-statistics"><i class="fa fa-check"></i><b>15.4</b> Various regression models used in statistics</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="16-regression2.html"><a href="16-regression2.html"><i class="fa fa-check"></i><b>16</b> Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="16-regression2.html"><a href="16-regression2.html#the-linear-regression-model"><i class="fa fa-check"></i><b>16.1</b> The linear regression model</a></li>
<li class="chapter" data-level="16.2" data-path="16-regression2.html"><a href="16-regression2.html#interpretation-of-regression-coefficients-and-intercept"><i class="fa fa-check"></i><b>16.2</b> Interpretation of regression coefficients and intercept</a></li>
<li class="chapter" data-level="16.3" data-path="16-regression2.html"><a href="16-regression2.html#different-types-of-linear-regression"><i class="fa fa-check"></i><b>16.3</b> Different types of linear regression:</a></li>
<li class="chapter" data-level="16.4" data-path="16-regression2.html"><a href="16-regression2.html#distributional-assumptions-and-properties"><i class="fa fa-check"></i><b>16.4</b> Distributional assumptions and properties</a></li>
<li class="chapter" data-level="16.5" data-path="16-regression2.html"><a href="16-regression2.html#regression-in-data-matrix-notation"><i class="fa fa-check"></i><b>16.5</b> Regression in data matrix notation</a></li>
<li class="chapter" data-level="16.6" data-path="16-regression2.html"><a href="16-regression2.html#centering-and-vanishing-of-the-intercept-beta_0"><i class="fa fa-check"></i><b>16.6</b> Centering and vanishing of the intercept <span class="math inline">\(\beta_0\)</span></a></li>
<li class="chapter" data-level="16.7" data-path="16-regression2.html"><a href="16-regression2.html#regression-objectives-for-linear-model"><i class="fa fa-check"></i><b>16.7</b> Regression objectives for linear model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="17-regression3.html"><a href="17-regression3.html"><i class="fa fa-check"></i><b>17</b> Estimating regression coefficients</a><ul>
<li class="chapter" data-level="17.1" data-path="17-regression3.html"><a href="17-regression3.html#ordinary-least-squares-ols-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.1</b> Ordinary Least Squares (OLS) estimator of regression coefficients</a></li>
<li class="chapter" data-level="17.2" data-path="17-regression3.html"><a href="17-regression3.html#maximum-likelihood-estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>17.2</b> Maximum likelihood estimation of regression coefficients</a><ul>
<li class="chapter" data-level="17.2.1" data-path="17-regression3.html"><a href="17-regression3.html#detailed-derivation-of-the-mles"><i class="fa fa-check"></i><b>17.2.1</b> Detailed derivation of the MLEs</a></li>
<li class="chapter" data-level="17.2.2" data-path="17-regression3.html"><a href="17-regression3.html#asymptotics"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotics</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="17-regression3.html"><a href="17-regression3.html#covariance-plug-in-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.3</b> Covariance plug-in estimator of regression coefficients</a><ul>
<li class="chapter" data-level="17.3.1" data-path="17-regression3.html"><a href="17-regression3.html#importance-of-positive-definiteness-of-estimated-covariance-matrix"><i class="fa fa-check"></i><b>17.3.1</b> Importance of positive definiteness of estimated covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="17-regression3.html"><a href="17-regression3.html#best-linear-predictor"><i class="fa fa-check"></i><b>17.4</b> Best linear predictor</a><ul>
<li class="chapter" data-level="17.4.1" data-path="17-regression3.html"><a href="17-regression3.html#result"><i class="fa fa-check"></i><b>17.4.1</b> Result:</a></li>
<li class="chapter" data-level="17.4.2" data-path="17-regression3.html"><a href="17-regression3.html#irreducible-error"><i class="fa fa-check"></i><b>17.4.2</b> Irreducible Error</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="17-regression3.html"><a href="17-regression3.html#regression-by-conditioning"><i class="fa fa-check"></i><b>17.5</b> Regression by conditioning</a><ul>
<li class="chapter" data-level="17.5.1" data-path="17-regression3.html"><a href="17-regression3.html#general-idea"><i class="fa fa-check"></i><b>17.5.1</b> General idea:</a></li>
<li class="chapter" data-level="17.5.2" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-normal-assumption"><i class="fa fa-check"></i><b>17.5.2</b> Multivariate normal assumption</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="17-regression3.html"><a href="17-regression3.html#standardised-regression-coefficients-and-relationship-to-correlation"><i class="fa fa-check"></i><b>17.6</b> Standardised regression coefficients and relationship to correlation</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="18-regression4.html"><a href="18-regression4.html"><i class="fa fa-check"></i><b>18</b> Squared multiple correlation and variance decomposition in linear regression</a><ul>
<li class="chapter" data-level="18.1" data-path="18-regression4.html"><a href="18-regression4.html#squared-multiple-correlation-omega2-and-the-r2-coefficient"><i class="fa fa-check"></i><b>18.1</b> Squared multiple correlation <span class="math inline">\(\Omega^2\)</span> and the <span class="math inline">\(R^2\)</span> coefficient</a><ul>
<li class="chapter" data-level="18.1.1" data-path="18-regression4.html"><a href="18-regression4.html#estimation-of-omega2-and-the-multiple-r2-coefficient"><i class="fa fa-check"></i><b>18.1.1</b> Estimation of <span class="math inline">\(\Omega^2\)</span> and the multiple <span class="math inline">\(R^2\)</span> coefficient</a></li>
<li class="chapter" data-level="18.1.2" data-path="18-regression4.html"><a href="18-regression4.html#r-output"><i class="fa fa-check"></i><b>18.1.2</b> R output</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="18-regression4.html"><a href="18-regression4.html#variance-decomposition-in-regression"><i class="fa fa-check"></i><b>18.2</b> Variance decomposition in regression</a><ul>
<li class="chapter" data-level="18.2.1" data-path="18-regression4.html"><a href="18-regression4.html#law-of-total-variance-and-variance-decomposition"><i class="fa fa-check"></i><b>18.2.1</b> Law of total variance and variance decomposition</a></li>
<li class="chapter" data-level="18.2.2" data-path="18-regression4.html"><a href="18-regression4.html#related-quantities"><i class="fa fa-check"></i><b>18.2.2</b> Related quantities</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="18-regression4.html"><a href="18-regression4.html#sample-version-of-variance-decomposition"><i class="fa fa-check"></i><b>18.3</b> Sample version of variance decomposition</a><ul>
<li class="chapter" data-level="18.3.1" data-path="18-regression4.html"><a href="18-regression4.html#geometric-interpretation-of-regression-as-orthogonal-projection"><i class="fa fa-check"></i><b>18.3.1</b> Geometric interpretation of regression as orthogonal projection:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="19-regression5.html"><a href="19-regression5.html"><i class="fa fa-check"></i><b>19</b> Prediction and variable selection</a><ul>
<li class="chapter" data-level="19.1" data-path="19-regression5.html"><a href="19-regression5.html#prediction-and-prediction-intervals"><i class="fa fa-check"></i><b>19.1</b> Prediction and prediction intervals</a></li>
<li class="chapter" data-level="19.2" data-path="19-regression5.html"><a href="19-regression5.html#variable-importance-and-prediction"><i class="fa fa-check"></i><b>19.2</b> Variable importance and prediction</a><ul>
<li class="chapter" data-level="19.2.1" data-path="19-regression5.html"><a href="19-regression5.html#how-to-quantify-variable-importance"><i class="fa fa-check"></i><b>19.2.1</b> How to quantify variable importance?</a></li>
<li class="chapter" data-level="19.2.2" data-path="19-regression5.html"><a href="19-regression5.html#some-candidates-for-vims"><i class="fa fa-check"></i><b>19.2.2</b> Some candidates for VIMs</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="19-regression5.html"><a href="19-regression5.html#regression-t-scores."><i class="fa fa-check"></i><b>19.3</b> Regression <span class="math inline">\(t\)</span>-scores.</a><ul>
<li class="chapter" data-level="19.3.1" data-path="19-regression5.html"><a href="19-regression5.html#computation"><i class="fa fa-check"></i><b>19.3.1</b> Computation</a></li>
<li class="chapter" data-level="19.3.2" data-path="19-regression5.html"><a href="19-regression5.html#connection-with-partial-correlation"><i class="fa fa-check"></i><b>19.3.2</b> Connection with partial correlation</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="19-regression5.html"><a href="19-regression5.html#further-approaches-for-variable-selection"><i class="fa fa-check"></i><b>19.4</b> Further approaches for variable selection</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="20-refresher.html"><a href="20-refresher.html"><i class="fa fa-check"></i><b>A</b> Refresher</a><ul>
<li class="chapter" data-level="A.1" data-path="20-refresher.html"><a href="20-refresher.html#basic-mathematical-notation"><i class="fa fa-check"></i><b>A.1</b> Basic mathematical notation</a></li>
<li class="chapter" data-level="A.2" data-path="20-refresher.html"><a href="20-refresher.html#vectors-and-matrices"><i class="fa fa-check"></i><b>A.2</b> Vectors and matrices</a></li>
<li class="chapter" data-level="A.3" data-path="20-refresher.html"><a href="20-refresher.html#functions"><i class="fa fa-check"></i><b>A.3</b> Functions</a><ul>
<li class="chapter" data-level="A.3.1" data-path="20-refresher.html"><a href="20-refresher.html#convex-and-concave-functions"><i class="fa fa-check"></i><b>A.3.1</b> Convex and concave functions</a></li>
<li class="chapter" data-level="A.3.2" data-path="20-refresher.html"><a href="20-refresher.html#gradient"><i class="fa fa-check"></i><b>A.3.2</b> Gradient</a></li>
<li class="chapter" data-level="A.3.3" data-path="20-refresher.html"><a href="20-refresher.html#hessian-matrix"><i class="fa fa-check"></i><b>A.3.3</b> Hessian matrix</a></li>
<li class="chapter" data-level="A.3.4" data-path="20-refresher.html"><a href="20-refresher.html#conditions-for-local-maximum-of-a-function"><i class="fa fa-check"></i><b>A.3.4</b> Conditions for local maximum of a function</a></li>
<li class="chapter" data-level="A.3.5" data-path="20-refresher.html"><a href="20-refresher.html#linear-and-quadratic-approximation"><i class="fa fa-check"></i><b>A.3.5</b> Linear and quadratic approximation</a></li>
<li class="chapter" data-level="A.3.6" data-path="20-refresher.html"><a href="20-refresher.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.3.6</b> Functions of matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="20-refresher.html"><a href="20-refresher.html#probability"><i class="fa fa-check"></i><b>A.4</b> Probability</a><ul>
<li class="chapter" data-level="A.4.1" data-path="20-refresher.html"><a href="20-refresher.html#random-variables"><i class="fa fa-check"></i><b>A.4.1</b> Random variables</a></li>
<li class="chapter" data-level="A.4.2" data-path="20-refresher.html"><a href="20-refresher.html#transformation-of-random-variables"><i class="fa fa-check"></i><b>A.4.2</b> Transformation of random variables</a></li>
<li class="chapter" data-level="A.4.3" data-path="20-refresher.html"><a href="20-refresher.html#bernoulli-and-binomial-distribution"><i class="fa fa-check"></i><b>A.4.3</b> Bernoulli and Binomial distribution</a></li>
<li class="chapter" data-level="A.4.4" data-path="20-refresher.html"><a href="20-refresher.html#normal-distribution"><i class="fa fa-check"></i><b>A.4.4</b> Normal distribution</a></li>
<li class="chapter" data-level="A.4.5" data-path="20-refresher.html"><a href="20-refresher.html#scaled-chi-squared-distribution-and-gamma-and-exponential-distribution"><i class="fa fa-check"></i><b>A.4.5</b> Scaled Chi-squared distribution (and Gamma and Exponential distribution)</a></li>
<li class="chapter" data-level="A.4.6" data-path="20-refresher.html"><a href="20-refresher.html#law-of-large-numbers"><i class="fa fa-check"></i><b>A.4.6</b> Law of large numbers:</a></li>
<li class="chapter" data-level="A.4.7" data-path="20-refresher.html"><a href="20-refresher.html#jensens-inequality"><i class="fa fa-check"></i><b>A.4.7</b> Jensen’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="20-refresher.html"><a href="20-refresher.html#statistics"><i class="fa fa-check"></i><b>A.5</b> Statistics</a><ul>
<li class="chapter" data-level="A.5.1" data-path="20-refresher.html"><a href="20-refresher.html#statistical-learning"><i class="fa fa-check"></i><b>A.5.1</b> Statistical learning</a></li>
<li class="chapter" data-level="A.5.2" data-path="20-refresher.html"><a href="20-refresher.html#point-and-interval-estimation"><i class="fa fa-check"></i><b>A.5.2</b> Point and interval estimation</a></li>
<li class="chapter" data-level="A.5.3" data-path="20-refresher.html"><a href="20-refresher.html#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fa fa-check"></i><b>A.5.3</b> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span></a></li>
<li class="chapter" data-level="A.5.4" data-path="20-refresher.html"><a href="20-refresher.html#asymptotics-1"><i class="fa fa-check"></i><b>A.5.4</b> Asymptotics</a></li>
<li class="chapter" data-level="A.5.5" data-path="20-refresher.html"><a href="20-refresher.html#confidence-intervals"><i class="fa fa-check"></i><b>A.5.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="A.5.6" data-path="20-refresher.html"><a href="20-refresher.html#symmetric-normal-confidence-interval"><i class="fa fa-check"></i><b>A.5.6</b> Symmetric normal confidence interval</a></li>
<li class="chapter" data-level="A.5.7" data-path="20-refresher.html"><a href="20-refresher.html#confidence-interval-for-chi-squared-distribution"><i class="fa fa-check"></i><b>A.5.7</b> Confidence interval for chi-squared distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="21-further-study.html"><a href="21-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="21-further-study.html"><a href="21-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="21-further-study.html"><a href="21-further-study.html#additional-references"><i class="fa fa-check"></i><b>B.2</b> Additional references</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="22-references.html"><a href="22-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Statistical Methods:<br />
Likelihood, Bayes and Regression</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="from-entropy-to-maximum-likelihood" class="section level1">
<h1><span class="header-section-number">2</span> From entropy to maximum likelihood</h1>
<div id="entropy" class="section level2">
<h2><span class="header-section-number">2.1</span> Entropy</h2>
<div id="overview" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Overview</h3>
<p>In this chapter we discuss various information criteria and their connection to maximum likelihood.</p>
<p>The modern definition of (relative) entropy, or “disorder”, was first discovered in 1875 by physicist <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann">L. Boltzmann (1844–1906)</a>
in the context of thermodynamics. In the 1940–1950’s the notion of entropy turned out to be central in information theory, a field pioneered by mathematicians such as
<a href="https://en.wikipedia.org/wiki/Ralph_Hartley">R. Hartley (1988–1970)</a>,
<a href="https://en.wikipedia.org/wiki/Solomon_Kullback">S. Kullback (1907–1994)</a>,
<a href="https://en.wikipedia.org/wiki/Richard_Leibler">R. Leibler (1914–2003)</a>,
<a href="https://en.wikipedia.org/wiki/Alan_Turing">A. Turing (1912–1954)</a>,
<a href="https://en.wikipedia.org/wiki/I._J._Good">I. J. Good (1916–2009)</a>,
<a href="https://en.wikipedia.org/wiki/Claude_Shannon">C. Shannon (1916–2001)</a>, and
<a href="https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes">E. T. Jaynes (1922–1998)</a>,
and later further explored by
<a href="https://de.wikipedia.org/wiki/Shun%E2%80%99ichi_Amari">S. Amari (1936–)</a>,
<a href="https://en.wikipedia.org/wiki/Imre_Csisz%C3%A1r">I. Ciszár (1938–)</a>,
<a href="https://de.wikipedia.org/wiki/Bradley_Efron">B. Efron (1938–)</a>,
<a href="https://en.wikipedia.org/wiki/Philip_Dawid">A. P. Dawid (1946–)</a> and
many others.</p>
<p><span class="math display">\[\begin{align*}
\left.
\begin{array}{cc}
\\
\textbf{Entropy} \\
\\
\end{array}
\right.
\left.
\begin{array}{cc}
\\
\nearrow  \\
\searrow  \\
\\
\end{array}
\right.
\begin{array}{ll}
\text{Shannon Entropy} \\
\\
\text{Relative Entropy}  \\
\end{array}
\begin{array}{ll}
\text{(Shannon 1948)} \\
\\
\text{(Kullback-Leibler 1951)}  \\
\end{array}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\left.
\begin{array}{ll}
\text{Fisher information} \\
\\
\text{Mutual Information} \\
\end{array}
\right.
\begin{array}{ll}
\rightarrow\text{ Likelihood theory} \\
\\
\rightarrow\text{ Information theory} \\
\end{array}
\begin{array}{ll}
\text{(Fisher 1922)} \\
\\
\text{(Shannon 1948, Lindley 1953)}  \\
\end{array}
\end{align*}\]</span></p>
</div>
<div id="surprise-surprisal-or-shannon-information" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Surprise, surprisal or Shannon information</h3>
<p>The <strong>surprise</strong> to observe an event of probability <span class="math inline">\(p\)</span> is <strong>defined</strong> as <span class="math inline">\(-\log(p)\)</span>.
This is also called <strong>surprisal</strong> or <strong>Shannon information</strong>.</p>
<p>Thus, the surprise to observe a certain event (with <span class="math inline">\(p=1\)</span>) is zero,
and conversely the surprise to observe an event that is certain not to happen
(with <span class="math inline">\(p=0\)</span>) is infinite.</p>
<p>The <strong>log-odds ratio</strong> can be viewed as the difference of the surprise of an event and the surprise of the complementary event:
<span class="math display">\[
\log\left( \frac{p}{1-p} \right) =  -\log(1-p) - ( -\log(p))
\]</span></p>
<p>In this module we always use the <em>natural logarithm</em> by default, and will explicitly write <span class="math inline">\(\log_2\)</span> and <span class="math inline">\(\log_{10}\)</span> for logarithms with respect to base 2 and 10, respectively.</p>
<p>Surprise and entropy computed with the natural logarithm (<span class="math inline">\(\log\)</span>) is given in “nats” (=<a href="https://en.wikipedia.org/wiki/Nat_(unit)">natural information units</a> ). Using <span class="math inline">\(\log_2\)</span> leads to “bits” and using <span class="math inline">\(\log_{10}\)</span> to “ban” or “Hartley”, cf. <a href="https://en.wikipedia.org/wiki/Hartley_(unit)" class="uri">https://en.wikipedia.org/wiki/Hartley_(unit)</a>.</p>
</div>
<div id="shannon-entropy" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Shannon entropy</h3>
<p>Assume we have a discrete distribution <span class="math inline">\(F\)</span> with <span class="math inline">\(K\)</span> classes and
class probabilities <span class="math inline">\(p_1, \ldots, p_K\)</span> with <span class="math inline">\(\text{Pr}(\text{&quot;class k&quot;}) = p_k\)</span>
and <span class="math inline">\(\sum_{k=1}^K = 1\)</span>.</p>
<p>The <strong>Shannon entropy</strong> of the discrete distribution <span class="math inline">\(F\)</span> is defined as the
<strong>expected surprise</strong>, i.e. the negative expected log-probability
<span class="math display">\[
\begin{split}
H(F) &amp;=-\text{E}_F\left(\log f(\boldsymbol x)\right) \\
     &amp;= - \sum_{i=1}^{K}p_i \log(p_i) \\
\end{split}
\]</span></p>
<p>Shannon entropy is bounded below by zero and bounded above by <span class="math inline">\(\log K\)</span>. Therefore
<span class="math display">\[\log K \geq  H(F) \geq 0\]</span>
for any discrete distribution <span class="math inline">\(F\)</span> with <span class="math inline">\(K\)</span> categories.</p>
<div class="example">
<p><span id="exm:entropydunif" class="example"><strong>Example 2.1  </strong></span><strong>Discrete uniform distribution</strong> <span class="math inline">\(U_K\)</span>: let <span class="math inline">\(p_1=p_2= \ldots = p_K = \frac{1}{K}\)</span>.
Then <span class="math display">\[H(U_K) = - \sum_{i=1}^{K}\frac{1}{K} \log\left(\frac{1}{K}\right) = \log K\]</span></p>
<p>Note this is the largest value the Shannon entropy can assume
with <span class="math inline">\(K\)</span> classes.</p>
</div>
<div class="example">
<p><span id="exm:entropyconcentrated" class="example"><strong>Example 2.2  </strong></span><strong>Concentrated probability mass</strong>: let
<span class="math inline">\(p_1=1\)</span> and <span class="math inline">\(p_2=p_3=\ldots=p_K=0\)</span>.
Using <span class="math inline">\(0\times\log(0)=0\)</span> we obtain for the Shannon probability
<span class="math display">\[ H(F) = 1\times\log(1) + 0\times\log(0) + \dots = 0\]</span></p>
<p>Note that 0 is the smallest value that Shannon entropy can assume, and corresponds to maximum concentration.</p>
</div>
<p>Thus, <strong>large entropy</strong> implies that the <strong>distribution is spread out</strong> whereas <strong>small entropy</strong>
means the <strong>distribution is concentrated</strong>.</p>
<p>Correspondingly, maximum entropy distributions can be considered minimally informative about a random variable.</p>
<p><strong>Multivariate case:</strong></p>
<p>If <span class="math inline">\(\boldsymbol x=(x_1, x_2, \ldots, x_d)^T\)</span> is a random vector of dimension <span class="math inline">\(d\)</span>
with distribution <span class="math inline">\(F\)</span> then <span class="math inline">\(H(F)\)</span> is called
the <em>joint entropy</em> of the random variables <span class="math inline">\(x_1, x_2, \ldots, x_d\)</span>.</p>
</div>
<div id="differential-entropy" class="section level3">
<h3><span class="header-section-number">2.1.4</span> Differential entropy</h3>
<p>Shannon entropy is only defined for discrete random variables.</p>
<p><em>Differential Entropy</em> results from applying the
definition of Shannon entropy to a <em>continuous</em> random variable <span class="math inline">\(x\)</span> with density <span class="math inline">\(f(x)\)</span>:
<span class="math display">\[
H(F) = -\text{E}_F(\log f(x)) = - \int f(x) \log f(x) \, dx
\]</span>
Despite having essentially the same formula the different name is justified because differential entropy exhibits different properties compared to Shannon entropy, because the logarithm is taken of a density
which in contrast to a probability can assumes values larger than one.
As a consequence, differential entropy is <em>not</em> bounded below by zero and can be negative.</p>
<div class="example">
<p><span id="exm:entropyunif" class="example"><strong>Example 2.3  </strong></span>Consider the uniform distribution <span class="math inline">\(U(0, a)\)</span> with <span class="math inline">\(a&gt;0\)</span>, support from <span class="math inline">\(0\)</span> to <span class="math inline">\(a\)</span> and density <span class="math inline">\(f(x) = 1/a\)</span>. As <span class="math inline">\(-\int_0^a f(x) \log f(x) dx =- \int_0^a \frac{1}{a} \log(\frac{1}{a}) dx = \log a\)</span>
the differential entropy is
<span class="math display">\[H( U(0, a) ) =  \log a \,.\]</span>
Note that for <span class="math inline">\(a &lt; 1\)</span> the differential entropy is negative.</p>
</div>
<div class="example">
<p><span id="exm:entropynormal" class="example"><strong>Example 2.4  </strong></span>The density of the univariate normal <span class="math inline">\(N(\mu, \sigma^2)\)</span> distribution
is <span class="math inline">\(f(x |\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right)\)</span> with <span class="math inline">\(\sigma^2 &gt; 0\)</span>.
The corresponding differential entropy is
is
<span class="math display">\[
H(F) = \frac{1}{2} ( \log(2 \pi \sigma^2)+1) \,.
\]</span>
Note that it only depends on the variance and not on the mean, and
that for <span class="math inline">\(\sigma^2 &lt; 1/(2 \pi e) \approx 0.0585\)</span> the differential entropy is negative.</p>
</div>
</div>
<div id="maximum-entropy-principle-to-characterise-distributions" class="section level3">
<h3><span class="header-section-number">2.1.5</span> Maximum entropy principle to characterise distributions</h3>
<p>Both maximum Shannon entropy and differential entropy are useful to characterise distributions:</p>
<ol style="list-style-type: decimal">
<li><p>The <strong>discrete
uniform distribution</strong> is the <strong>maximum entropy distribution</strong> among all discrete distributions.</p></li>
<li><p>the maximum entropy distribution of a continuous random variable with support <span class="math inline">\([-\infty, \infty]\)</span> with a specific mean and variance is the normal distribution</p></li>
<li><p>the maximum entropy distribution among all continuous distributions supported in <span class="math inline">\([0, \infty]\)</span> with a specified mean is the exponential distribution.</p></li>
</ol>
<p>The higher the entropy the more spread out (and more uninformative) is a distribution.</p>
<p>Using maximum entropy to characterise maximally uniformative distributions
was advocated by E.T. Jaynes (who also proposed to use maximum entropy in the context of finding Bayesian priors).</p>
<p>A list of maximum entropy distribution is given here:
<a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution" class="uri">https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution</a> .</p>
</div>
<div id="cross-entropy" class="section level3">
<h3><span class="header-section-number">2.1.6</span> Cross-entropy</h3>
<p>If in the definition of Shannon entropy (and differential entropy) the expectation over
the log-density (say <span class="math inline">\(g(x)\)</span> of distribution <span class="math inline">\(G\)</span>) is with regard to a different distribution <span class="math inline">\(F\)</span> over the same state space
we arrive at the <strong>cross-entropy</strong>
<span class="math display">\[H(F, G) =-\text{E}_F\left( \log g(x)  \right)\]</span></p>
<p>Therefore, cross-entropy is a measure linking two distributions <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span>.</p>
<p>Note that</p>
<ul>
<li>cross-entropy is not symmetric with regard to <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span>, because
the expectation is taken with reference to <span class="math inline">\(F\)</span>.</li>
<li>By construction <span class="math inline">\(H(F, F) = H(F)\)</span>.</li>
<li>if <span class="math inline">\(G\)</span> is uniform then <span class="math inline">\(H(F, G) = C\)</span> (constant, not depending on <span class="math inline">\(F\)</span>).</li>
</ul>
<p>A crucial property of the cross-entropy <span class="math inline">\(H(F, G)\)</span> is that it is bounded below by the entropy of <span class="math inline">\(F\)</span>, therefore
<span class="math display">\[
H(F, G) \geq H(F)
\]</span>
with equality for <span class="math inline">\(F=G\)</span>.</p>
<p>Equivalently we can write
<span class="math display">\[
H(F, G)-H(F) \geq 0
\]</span><br />
In fact, this recalibrated cross-entropy turns out to be more fundamental than both cross-entropy and Shannon resp. differential entropy. It will be studied in detail in the next section.</p>
<div class="example">
<p><span id="exm:crossentropynormals" class="example"><strong>Example 2.5  </strong></span>Cross-entropy between two normals:</p>
<p>Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2_{\text{ref}})\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>.
Then the cross-entropy is
<span class="math display">\[
H(F_{\text{ref}}, F) = \frac{1}{2} \left( \frac{(\mu - \mu_{\text{ref}})^2}{ \sigma^2 } 
 +\frac{\sigma^2_{\text{ref}}}{\sigma^2}  +\log(2 \pi \sigma^2) \right)
\]</span></p>
</div>
<div class="example">
<p><span id="exm:crossentropylowerbound" class="example"><strong>Example 2.6  </strong></span>If <span class="math inline">\(\mu_{\text{ref}} = \mu\)</span> and <span class="math inline">\(\sigma^2_{\text{ref}} = \sigma^2\)</span>
then the above cross-entropy <span class="math inline">\(H(F,G)\)</span> degenerates to the differential entropy
<span class="math inline">\(H(F_{\text{ref}}) = \frac{1}{2} \left(\log( 2 \pi \sigma^2_{\text{ref}}) +1 \right)\)</span>.</p>
</div>
</div>
</div>
<div id="kullback-leibler-divergence" class="section level2">
<h2><span class="header-section-number">2.2</span> Kullback-Leibler divergence</h2>
<div id="definition" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Definition</h3>
<p>Also known as <strong>relative entropy</strong> and <strong>discrimination information</strong>.</p>
<p>The <strong>relative entropy</strong> measures the <strong>divergence</strong>
of a distribution <span class="math inline">\(G\)</span> from the distribution <span class="math inline">\(F\)</span> and is defined as
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F,G) &amp;= \text{E}_F\log\left(\frac{dF}{dG}\right) \\
&amp; = \text{E}_F\log\left(\frac{f(x)}{g(x)}\right) \\
&amp; =
\underbrace{-\text{E}_F(\log g(x))}_{\text{cross-entropy}} - 
 (\underbrace{-\text{E}_F (\log f(x))  }_\text{(differential) entropy})  \\
&amp; = H(F, G)-H(F) \\
\end{split}
\]</span></p>
<ul>
<li><span class="math inline">\(D_{\text{KL}}(F, G)\)</span> measures the amount of information lost if <span class="math inline">\(G\)</span> is used to approximate <span class="math inline">\(F\)</span>.</li>
<li>If <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are identical (and no information is lost) then <span class="math inline">\(D_{\text{KL}}(F,G)=0\)</span>.</li>
<li>If <span class="math inline">\(G\)</span> is uniform then <span class="math inline">\(D_{\text{KL}}(F, G) = -H(F) + C\)</span>, i.e. the negative Shannon/differential entropy of <span class="math inline">\(F\)</span> measures the loss when <span class="math inline">\(F\)</span> is approximated by a uniform.</li>
</ul>
<p>(Note: here “divergence” measures the dissimilarity between probability distributions. This type of divergence is not related and should not be confused with divergence (div) as used in vector analysis.)</p>
<p>The term divergence (rather than distance) implies also that the distributions <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are not interchangeable in <span class="math inline">\(D_{\text{KL}}(F, G)\)</span>.</p>
<p>In applications in statistics the typical roles of <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are:</p>
<ul>
<li><span class="math inline">\(F\)</span> as the (unknown) underlying true model for the data generating process</li>
<li><span class="math inline">\(G\)</span> as the approximating model (e.g. some parametric family)</li>
</ul>
<p>In Bayesian statistics we use</p>
<ul>
<li><span class="math inline">\(F\)</span> as posterior distribution</li>
<li><span class="math inline">\(G\)</span> as prior distribution</li>
</ul>
<p>There exist various notations for KL divergence in the literature.
Here we use <span class="math inline">\(D_{KL}(F, G)\)</span> but often you can find <span class="math inline">\(KL(F || G)\)</span>
or <span class="math inline">\(\boldsymbol I^{KL}(F; G)\)</span> in other references.</p>
<p>Some authors (e.g. Efron) call twice the KL divergence <span class="math inline">\(2 D_{KL}(F, G) = D(F, G)\)</span> the <strong>deviance</strong> of <span class="math inline">\(G\)</span> from <span class="math inline">\(F\)</span>.</p>
</div>
<div id="properties-of-kl-divergence" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Properties of KL divergence</h3>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(D_{\text{KL}}(F, G) \neq D_{\text{KL}}(G, F)\)</span>, i.e., KL divergence is not symmetric, <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> cannot be interchanged.</li>
<li><span class="math inline">\(D_{\text{KL}}(F, G) = 0\)</span> if and only if <span class="math inline">\(F=G\)</span>, i.e., the KL divergence is zero if and only if <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are identical.</li>
<li><span class="math inline">\(D_{\text{KL}}(F, G)\geq 0\)</span>, proof via the <strong>Jensen Inequality</strong>.</li>
<li><span class="math inline">\(D_{\text{KL}}(F, G)\)</span> remains invariant under coordinate transformations,
i.e. it is an invariant geometric quantity.</li>
</ol>
<p>Note that in the KL divergence the expectation is taken over a ratio of densities (or ratio of probabilities for discrete random variables). This is what creates the transformation invariance.</p>
<p>For more details and proofs of properties 3 and 4 see Worksheet 1.</p>
</div>
<div id="examples" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Examples</h3>
<div class="example">
<p><span id="exm:klbernoulli" class="example"><strong>Example 2.7  </strong></span>KL divergence between two Bernoulli distributions <span class="math inline">\(\text{Ber}(p)\)</span> and <span class="math inline">\(\text{Ber}(q)\)</span>:</p>
<p>The “success” probabilities for the two distributions are <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>,
respectively, and the complementary “failure” probabilities are <span class="math inline">\(1-p\)</span> and <span class="math inline">\(1-q\)</span>.
With this we get for the KL divergence
<span class="math display">\[
D_{\text{KL}}(\text{Ber}(p), \text{Ber}(q))=p \log\left( \frac{p}{q}\right) + (1-p) \log\left(\frac{1-p}{1-q}\right)
\]</span></p>
</div>
<div class="example">
<p><span id="exm:klnormal" class="example"><strong>Example 2.8  </strong></span>KL divergence between two univariate normals with different means and variances:</p>
<p>Assume <span class="math inline">\(F_{\text{ref}}=N(\mu_{\text{ref}},\sigma^2_{\text{ref}})\)</span> and <span class="math inline">\(F=N(\mu,\sigma^2)\)</span>.
Then
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\text{ref}},F) &amp;= H(F_{\text{ref}}, F) - H(F_{\text{ref}}) \\
&amp;= \frac{1}{2} \left(   \frac{(\mu-\mu_{\text{ref}})^2}{\sigma^2}  + \frac{\sigma_{\text{ref}}^2}{\sigma^2}
-\log\left(\frac{\sigma_{\text{ref}}^2}{\sigma^2}\right)-1  
   \right) \\
\end{split}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:klnormalequalvar" class="example"><strong>Example 2.9  </strong></span>KL divergence between two univariate normals with different means and common variance:</p>
<p>An important special case of the previous Example <a href="02-likelihood2.html#exm:klnormal">2.8</a> occurs if
the variances are equal. Then we get
<span class="math display">\[D_{\text{KL}}(N(\mu_{\text{ref}}, \sigma^2),   N(\mu, \sigma^2)  )=\frac{1}{2} \left(\frac{(\mu-\mu_{\text{ref}})^2}{\sigma^2}\right)\]</span></p>
</div>
</div>
</div>
<div id="local-quadratic-approximation-and-expected-fisher-information" class="section level2">
<h2><span class="header-section-number">2.3</span> Local quadratic approximation and expected Fisher information</h2>
<div id="linear-approximation" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Linear approximation</h3>
<p>KL information measures the divergence of two distributions.
We may thus use relative entropy to measure the divergence between two distributions in the same family, separated in parameter space only by some small <span class="math inline">\(\boldsymbol \varepsilon\)</span>:</p>
<p><span class="math display">\[
D_{\text{KL}}(F_{\boldsymbol \theta}, F_{\boldsymbol \theta+\boldsymbol \varepsilon}) = ?
\]</span>
Note that we keep the first argument fixed, and vary the second.</p>
<p>To evaluate this expression we first consider a linear approximation
of the log density as a function of the parameter vector <span class="math inline">\(\boldsymbol \theta\)</span>:<br />
<span class="math display">\[
\log f(x|\boldsymbol \theta+\boldsymbol \varepsilon) \approx \log f(x|\boldsymbol \theta) + 
   \nabla\log f(x|\boldsymbol \theta) \, \boldsymbol \varepsilon
\]</span>
where the gradient is computed with regard to <span class="math inline">\(\boldsymbol \theta\)</span>. With this
<span class="math display">\[
\log f(x|\boldsymbol \theta) -   \log f(x|\boldsymbol \theta+ \boldsymbol \varepsilon) \approx  -\nabla\log f(x|\boldsymbol \theta) \, \boldsymbol \varepsilon
\]</span>
and therefore the KL divergence between <span class="math inline">\(F_{\boldsymbol \theta}\)</span> and <span class="math inline">\(F_{\boldsymbol \theta+\boldsymbol \varepsilon}\)</span> becomes
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\boldsymbol \theta}, F_{\boldsymbol \theta+\boldsymbol \varepsilon}) 
   &amp;= \text{E}_{F_{\boldsymbol \theta}}  \left( \log f(x|\boldsymbol \theta) -   \log f(x|\boldsymbol \theta+ \boldsymbol \varepsilon) \right) \\
 &amp; \approx -\text{E}_{F_{\boldsymbol \theta}} \left(\nabla\log f(x|\boldsymbol \theta)   \right) \, \boldsymbol \varepsilon\\
&amp;= 0\\
\end{split}
\]</span>
Why the expected gradient of the log-density is zero is derived below in Example <a href="02-likelihood2.html#exm:expectationgradient">2.10</a>.
However, the result is also intuitively clear as
<span class="math inline">\(D_{\text{KL}}(F_{\boldsymbol \theta}, F_{\boldsymbol \theta+\boldsymbol \varepsilon})\)</span> as a function of <span class="math inline">\(\boldsymbol \varepsilon\)</span>
has a minimum at <span class="math inline">\(\boldsymbol \varepsilon=0\)</span> (when the relative entropy vanishes)
so it is locally flat for small <span class="math inline">\(\boldsymbol \varepsilon\)</span>.</p>
<div class="example">
<p><span id="exm:expectationgradient" class="example"><strong>Example 2.10  </strong></span>Expectation of the gradient of the log-density:</p>
<p>The gradient of the log-density
<span class="math inline">\(\text{E}_{F_{\boldsymbol \theta}} \left( \nabla\log f(x|\boldsymbol \theta) \right)\)</span> vanishes because
<span class="math display">\[
\nabla\log f(x|\boldsymbol \theta) = f(x|\boldsymbol \theta)^{-1} \nabla f(x|\boldsymbol \theta)
\]</span>
and since <span class="math inline">\(f(x|\boldsymbol \theta)\)</span> is a density integrating to 1 therefore
<span class="math display">\[
\begin{split}
\text{E}_{F_{\boldsymbol \theta}} \left( \nabla \log f(x|\boldsymbol \theta) \right) &amp; = \int \nabla f(x|\boldsymbol \theta) dx\\
&amp; = \nabla \int f(x|\boldsymbol \theta) dx \\
&amp; = \nabla 1 = 0 \\
\end{split}
\]</span>
This assumes that exchange of integration and differentiation is possible.</p>
</div>
</div>
<div id="quadratic-approximation" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Quadratic approximation</h3>
<p>We therefore need a second order (quadratic) approximation in <span class="math inline">\(\boldsymbol \varepsilon\)</span> by including the Hessian term:
<span class="math display">\[
\log f(x|\boldsymbol \theta+\boldsymbol \varepsilon) \approx \log f(x|\boldsymbol \theta) + 
   \nabla\log f(x|\boldsymbol \theta) \, \boldsymbol \varepsilon+ \frac{1}{2}\boldsymbol \varepsilon^T \nabla^T\nabla\log f(x|\boldsymbol \theta)\boldsymbol \varepsilon\,.
\]</span>
The KL divergence between <span class="math inline">\(F_{\boldsymbol \theta}\)</span> and <span class="math inline">\(F_{\boldsymbol \theta+\boldsymbol \varepsilon}\)</span> now becomes
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\boldsymbol \theta}, F_{\boldsymbol \theta+\boldsymbol \varepsilon}) 
   &amp;= \text{E}_{F_{\boldsymbol \theta}}  \left( \log f(x|\boldsymbol \theta) -   \log f(x|\boldsymbol \theta+ \boldsymbol \varepsilon) \right)\\
 &amp; \approx  -\text{E}_{F_{\boldsymbol \theta}} \left(\frac{1}{2}\boldsymbol \varepsilon^T\nabla^T\nabla\log f(x|\boldsymbol \theta)\boldsymbol \varepsilon\right)\\
&amp;=   \frac{1}{2}\boldsymbol \varepsilon^T \, \text{E}_{F_{\boldsymbol \theta}} \left( -\nabla^T\nabla\log f(x|\boldsymbol \theta)\right) \boldsymbol \varepsilon\\
&amp; =\frac{1}{2}\boldsymbol \varepsilon^T \underbrace{\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)}_{\text{expected Fisher information}}\boldsymbol \varepsilon\\
\end{split} 
\]</span></p>
<p>The expected negative Hessian of the log-density arising from the
local quadratic approximation of KL divergence is
called the
the <strong>expected Fisher information matrix</strong>
<span class="math display">\[
\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) = -\text{E}_{F_{\boldsymbol \theta}} \left( \nabla^T\nabla\log f(x|\boldsymbol \theta)\right)
\]</span></p>
<p>Since as there is no data involved as the expected Fisher information is purely a property of the model, or more precisely of the space of the models indexed by <span class="math inline">\(\boldsymbol \theta\)</span>.
In the next Chapter we will study a related quantity, the <em>observed Fisher information</em>, that in contrast is a function of the observed data.</p>
<p>We can use the above approximation also to compute the divergence <span class="math inline">\(D_{\text{KL}}(F_{\boldsymbol \theta+\boldsymbol \varepsilon}, F_{\boldsymbol \theta})\)</span> where
the the first argument varies and the second is kept fixed:
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\boldsymbol \theta+\boldsymbol \varepsilon}, F_{\boldsymbol \theta}) &amp;\approx \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta+\boldsymbol \varepsilon)\, \boldsymbol \varepsilon\\
\end{split} 
\]</span>
In a linear approximation <span class="math inline">\(\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta+\boldsymbol \varepsilon) \approx \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) + \boldsymbol \Delta_{\boldsymbol \varepsilon}\)</span>
each element of the matrix <span class="math inline">\(\boldsymbol \Delta_{\boldsymbol \varepsilon}\)</span> is the scalar product of <span class="math inline">\(\boldsymbol \varepsilon\)</span>
and the gradient of the corresponding element in <span class="math inline">\(\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\)</span>
evaluated at <span class="math inline">\(\boldsymbol \theta\)</span>. Therefore <span class="math inline">\(\boldsymbol \varepsilon^T \boldsymbol \Delta_{\boldsymbol \varepsilon} \boldsymbol \varepsilon\)</span>
is of <em>cubic order</em> in <span class="math inline">\(\boldsymbol \varepsilon\)</span> and hence
<span class="math display">\[
\begin{split}
D_{\text{KL}}(F_{\boldsymbol \theta+\boldsymbol \varepsilon}, F_{\boldsymbol \theta}) &amp;\approx \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta+\boldsymbol \varepsilon) \boldsymbol \varepsilon\\
&amp;\approx \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) \boldsymbol \varepsilon+ \boldsymbol \varepsilon^T \boldsymbol \Delta_{\boldsymbol \varepsilon} \boldsymbol \varepsilon\\
&amp;\approx \frac{1}{2}\boldsymbol \varepsilon^T \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) \boldsymbol \varepsilon
\end{split} 
\]</span>
keeping only terms quadratic in <span class="math inline">\(\boldsymbol \varepsilon\)</span>.</p>
<div class="example">
<p><span id="exm:expectedfisherbernoulli" class="example"><strong>Example 2.11  </strong></span>Expected Fisher information for the Bernoulli distribution:</p>
<p>The log-probability mass function of the Bernoulli <span class="math inline">\(\text{Ber}(p)\)</span> distribution is
<span class="math display">\[
\log f(x | p) = x \log(p) + (1-x) \log(1-p)
\]</span>
where <span class="math inline">\(p\)</span> is the proportion of “success”.
The second derivative with regard to the parameter <span class="math inline">\(p\)</span> is
<span class="math display">\[
\frac{d^2}{dp^2} \log f(x | p)  =  -\frac{x}{p^2}-  \frac{1-x}{(1-p)^2}
\]</span>
Since <span class="math inline">\(\text{E}(x) = p\)</span> we get as Fisher information
<span class="math display">\[
\begin{split}
I^{\text{Fisher}}(p) &amp; = -\text{E}\left(\frac{d^2}{dp^2} \log f(x | p)  \right)\\
                           &amp;= \frac{p}{p^2}+  \frac{1-p}{(1-p)^2} \\
                            &amp;= \frac{1}{p(1-p)}\\
\end{split}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:quadapproxklbernoulli" class="example"><strong>Example 2.12  </strong></span>Quadratic approximations of the KL divergence between two Bernoulli distributions:</p>
<p>Example <a href="03-likelihood3.html#exm:mlenormalmeanvar">3.3</a> derives the KL divergence
<span class="math display">\[
D_{\text{KL}}\left (\text{Ber}(p_1), \text{Ber}(p_2) \right)=p_1 \log\left( \frac{p_1}{p_2}\right) + (1-p_1) \log\left(\frac{1-p_1}{1-p_2}\right)
\]</span>
where <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> are the parameters of the two Bernoulli distributions.</p>
<p>Example <a href="02-likelihood2.html#exm:expectedfisherbernoulli">2.11</a> computes the corresponding expected Fisher information.</p>
<p>The quadratic approximation implies that
<span class="math display">\[
D_{\text{KL}}\left( \text{Ber}(p), \text{Ber}(p + \varepsilon) \right) \approx \frac{\varepsilon^2}{2}  I^{\text{Fisher}}(p) =  \frac{\varepsilon^2}{2 p (1-p)}
\]</span>
and also that
<span class="math display">\[
D_{\text{KL}}\left( \text{Ber}(p+\varepsilon), \text{Ber}(p) \right) \approx \frac{\varepsilon^2}{2} I^{\text{Fisher}}(p) =  \frac{\varepsilon^2}{2 p (1-p)}
\]</span></p>
<p>In Worksheet 1 this is verified by using a second order Taylor series applied to the KL divergence.</p>
</div>
<div class="example">
<p><span id="exm:expectedfishernormal" class="example"><strong>Example 2.13  </strong></span>Expected Fisher information for the normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<p>The log-density is
<span class="math display">\[
\log f(x | \mu, \sigma^2) = -\frac{1}{2} \log(\sigma^2) 
-\frac{1}{2 \sigma^2} (x-\mu)^2 - \frac{1}{2}\log(2 \pi)
\]</span>
The gradient with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> (!) is the row vector
<span class="math display">\[
\nabla \log f(x | \mu, \sigma^2) =
\begin{pmatrix}
\frac{1}{\sigma^2} (x-\mu) \\
- \frac{1}{2 \sigma^2} + \frac{1}{2 \sigma^4} (x- \mu)^2 \\
\end{pmatrix}^T
\]</span>
Hint for calculating the gradient: replace <span class="math inline">\(\sigma^2\)</span> by <span class="math inline">\(v\)</span> and then take the partial derivative with regard to <span class="math inline">\(v\)</span>, then substitute back.</p>
<p>The Hessian matrix is
<span class="math display">\[
\nabla^T\nabla \log f(x | \mu, \sigma^2) =
\begin{pmatrix}
-\frac{1}{\sigma^2} &amp; -\frac{1}{\sigma^4} (x-\mu)\\
-\frac{1}{\sigma^4} (x-\mu) &amp;  \frac{1}{2\sigma^4} - \frac{1}{\sigma^6}(x- \mu)^2 \\
\\
\end{pmatrix}
\]</span>
As <span class="math inline">\(\text{E}(x) = \mu\)</span> we have <span class="math inline">\(\text{E}(x-\mu) =0\)</span>.
Furthermore, with <span class="math inline">\(\text{E}( (x-\mu)^2 ) =\sigma^2\)</span> we see that
<span class="math inline">\(\text{E}\left(\frac{1}{\sigma^6}(x- \mu)^2\right) = \frac{1}{\sigma^4}\)</span>. Therefore
the expected Fisher information matrix as the negative expected Hessian matrix is
<span class="math display">\[
\boldsymbol I^{\text{Fisher}}\left(\mu,\sigma^2\right) = \begin{pmatrix} \frac{1}{\sigma^2} &amp; 0 \\ 0 &amp; \frac{1}{2\sigma^4} \end{pmatrix}
\]</span></p>
</div>
</div>
</div>
<div id="entropy-learning-and-maximum-likelihood" class="section level2">
<h2><span class="header-section-number">2.4</span> Entropy learning and maximum likelihood</h2>
<div id="the-relative-entropy-between-true-model-and-approximating-model" class="section level3">
<h3><span class="header-section-number">2.4.1</span> The relative entropy between true model and approximating model</h3>
<p>Assume we have observations <span class="math inline">\(x_1, \ldots, x_n\)</span>. The data is sampled from <span class="math inline">\(F\)</span>, the true but unknown data generating distribution. We also specify models <span class="math inline">\(G_{\boldsymbol \theta}\)</span>
indexed by <span class="math inline">\(\boldsymbol \theta\)</span> to approximate <span class="math inline">\(F\)</span>.</p>
<p>The relative entropy <span class="math inline">\(D_{\text{KL}}(F,G_{\boldsymbol \theta})\)</span> then measures the divergence of the approximation <span class="math inline">\(G_{\boldsymbol \theta}\)</span>
from the unknow true model <span class="math inline">\(F\)</span>. It can be written as:
<span class="math display">\[
D_{\text{KL}}(F,G_{\boldsymbol \theta})=
\underbrace{- \text{E}_{F}\log g_{\boldsymbol \theta}(x)}_{\text{cross-entropy}}
-(\underbrace{-\text{E}_{F}\log f(x)}_{\text{entropy of $F$, does not depend on $\boldsymbol \theta$}})
\]</span></p>
<p>However, since we do not know <span class="math inline">\(F\)</span> we cannot actually compute this divergence. Nonetheless, we may use
the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> — a function of the observed data — as approximation for <span class="math inline">\(F\)</span>, and in this way arrive at an approximation for <span class="math inline">\(D_{\text{KL}}(F,G_{\boldsymbol \theta})\)</span> that becomes more and more accurate with growing sample size.</p>
<hr />
<p>Recall “Law of Large Numbers” :</p>
<ul>
<li><p>By the strong law of large numbers the empirical distribution
<span class="math inline">\(\hat{F}_n\)</span> converges to the true underlying distribution <span class="math inline">\(F\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> almost surely:
<span class="math display">\[
\hat{F}_n\overset{a. s.}{\to} F
\]</span></p></li>
<li><p>For <span class="math inline">\(n \rightarrow \infty\)</span> the average <span class="math inline">\(\text{E}_{\hat{F}_n}(h(X)) = \frac{1}{n} \sum_{i=1}^n h(x_i)\)</span> converges to the expectation <span class="math inline">\(\text{E}_{F}(h(X))\)</span>.</p></li>
</ul>
<hr />
<p>Hence, for large sample size <span class="math inline">\(n\)</span> we can approximate cross-entropy and as a result the KL divergence. The cross-entropy <span class="math inline">\(H(F, G_{\boldsymbol \theta})\)</span> is approximated by the empirical cross-entropy where the expectation is taken with regard to <span class="math inline">\(\hat{F}_n\)</span> rather than <span class="math inline">\(F\)</span>:
<span class="math display">\[
\begin{split}
H(F, G_{\boldsymbol \theta}) &amp; \approx H(\hat{F}_n, G_{\boldsymbol \theta}) \\
                  &amp; = - \text{E}_{\hat{F}_n} (\log(x))  \\
                  &amp; = -\frac{1}{n} \sum_{i=1}^n \log g(x_i | \boldsymbol \theta) \\
                  &amp; -\frac{1}{n} l_n ({\boldsymbol \theta})
\end{split}
\]</span>
This turns out to be equal to the negative log-likelihood standardised by the sample size <span class="math inline">\(n\)</span>! Or in other words, the <strong>likelihood</strong> is the <strong>negative empirical cross-entropy (times sample size <span class="math inline">\(n\)</span>)</strong>.</p>
<p>The KL divergence <span class="math inline">\(D_{\text{KL}}(F,G_{\boldsymbol \theta})\)</span> can therefore be approximated by
<span class="math display">\[
D_{\text{KL}}(F,G_{\boldsymbol \theta}) \approx  -\frac{1}{n} l_n ({\boldsymbol \theta}) + C
\]</span>
where <span class="math inline">\(C\)</span> is a constant (the negative entropy of the true distribution <span class="math inline">\(F\)</span>) that
does not depend on the parameters <span class="math inline">\(\boldsymbol \theta\)</span> (and hence does not matter when optimising the divergence).</p>
</div>
<div id="minimum-kl-divergence-and-maximum-likelihood" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Minimum KL divergence and maximum likelihood</h3>
<p>If we were to know <span class="math inline">\(F\)</span> we would simply minimise <span class="math inline">\(D_{\text{KL}}(F, G_{\boldsymbol \theta})\)</span> to find the particular model <span class="math inline">\(G_{\boldsymbol \theta}\)</span> that is closest to the true model. Equivalently, we would minimise the cross-entropy <span class="math inline">\(H(F, G_{\boldsymbol \theta})\)</span>.
However, since we actually don’t know <span class="math inline">\(F\)</span> this is not possible.</p>
<p>However, for large sample size <span class="math inline">\(n\)</span> when the empirical distribution <span class="math inline">\(\hat{F}_n\)</span>
is a good approximation for <span class="math inline">\(F\)</span>, we can use the above approximation.
Thus, instead of minimising the KL divergence
<span class="math inline">\(D_{\text{KL}}(F, G_{\boldsymbol \theta})\)</span> we simply minimise <span class="math inline">\(H(\hat{F}_n, G_{\boldsymbol \theta})\)</span> which
is the same as maximising the likelihood <span class="math inline">\(l_n ({\boldsymbol \theta})\)</span>.</p>
<p>Conversely, it turns out that maximising the likelihood with regard to the <span class="math inline">\(\boldsymbol \theta\)</span> is equivalent (at least asymptotically for large <span class="math inline">\(n\)</span>!) to minimising the KL divergence of the approximating model and the unknown true model!</p>
<p><span class="math display">\[
\begin{split}
\hat{\boldsymbol \theta}^{ML} &amp;= \underset{\boldsymbol \theta}{\arg \max}\,\, l_n(\boldsymbol \theta) \\
 &amp;= \underset{\boldsymbol \theta}{\arg \min}\,\, H(\hat{F}_n, G_{\boldsymbol \theta}) \\
 &amp;\approx \underset{\boldsymbol \theta}{\arg \min}\,\, D_{\text{KL}}(F, G_{\boldsymbol \theta}) \\
\end{split}
\]</span></p>
<p>Therefore, the reasoning behind the method of <strong>maximum likelihood</strong> is that it minimises a <strong>large sample approximation of the KL divergence</strong> of the candidate model <span class="math inline">\(G_{\boldsymbol \theta}\)</span> from the unkown true model <span class="math inline">\(F\)</span>.</p>
<p>As a consequence of the close link of maximum likelhood and relative entropy
maximum likelihood inherits for large <span class="math inline">\(n\)</span> (and only then!) all the optimality properties from KL divergence. These will be discussed in more detail later in the course.</p>
</div>
<div id="further-connections" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Further connections</h3>
<p>Since minimising KL divergence contains ML estimation as special case you may wonder whether there is a broader justification of relative entropy in the context of statistical data analysis?</p>
<p>Indeed, KL divergence has strong geometrical interpretation that forms the basis of <em>information geometry</em>.
In this field the manifold of distributions
is studied using tools from differential geometry. The expected Fisher information
plays an important role as <a href="https://en.wikipedia.org/wiki/Fisher_information_metric">metric tensor in the space of distributions</a>.</p>
<p>Furthermore, it is also linked to probabilistic forecasting.
In the framework of so-called <a href="https://en.wikipedia.org/wiki/Scoring_rule"><strong>scoring rules</strong></a>.
the only local proper scoring rule is the negative log-probability (“surprise”).
The expected “surprise” is the cross-entropy
and relative entropy is the corresponding natural divergence connected with the log scoring rule.</p>
<p>Furthermore, another intriguing property of KL divergence is that the relative entropy <span class="math inline">\(D_{\text{KL}}(F, G)\)</span> is the <em>only divergence measure</em> that is both a Bregman and an <span class="math inline">\(f\)</span>-divergence.
Note that <a href="https://en.wikipedia.org/wiki/F-divergence"><span class="math inline">\(f\)</span>-divergences</a> and <a href="https://en.wikipedia.org/wiki/Bregman_divergence">Bregman-divergences</a> (in turn related to proper scoring rules) are two large classes of measures of similarity and divergence between two probability distributions.</p>
<p>Finally, not only the likelihood estimation but also the Bayesian update rule (as discussed later in this module) is another special case of entropy learning.</p>

<p></p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="01-likelihood1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="03-likelihood3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math20802-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
