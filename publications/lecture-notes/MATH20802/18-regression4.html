<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>18 Squared multiple correlation and variance decomposition in linear regression | HTML</title>
  <meta name="description" content="Statistical Methods:<br />
Likelihood, Bayes and Regression</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="18 Squared multiple correlation and variance decomposition in linear regression | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="18 Squared multiple correlation and variance decomposition in linear regression | HTML" />
  
  
  



<meta name="date" content="2021-02-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="17-regression3.html"/>
<link rel="next" href="19-regression5.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH20802/index.html">MATH20802 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Likelihood estimation and inference</b></span></li>
<li class="chapter" data-level="1" data-path="01-likelihood1.html"><a href="01-likelihood1.html"><i class="fa fa-check"></i><b>1</b> Overview of statistical learning</a><ul>
<li class="chapter" data-level="1.1" data-path="01-likelihood1.html"><a href="01-likelihood1.html#how-to-learn-from-data"><i class="fa fa-check"></i><b>1.1</b> How to learn from data?</a></li>
<li class="chapter" data-level="1.2" data-path="01-likelihood1.html"><a href="01-likelihood1.html#probability-theory-versus-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Probability theory versus statistical learning</a></li>
<li class="chapter" data-level="1.3" data-path="01-likelihood1.html"><a href="01-likelihood1.html#cartoon-of-statistical-learning"><i class="fa fa-check"></i><b>1.3</b> Cartoon of statistical learning</a></li>
<li class="chapter" data-level="1.4" data-path="01-likelihood1.html"><a href="01-likelihood1.html#likelihood"><i class="fa fa-check"></i><b>1.4</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="02-likelihood2.html"><a href="02-likelihood2.html"><i class="fa fa-check"></i><b>2</b> From information theory to likelihood</a><ul>
<li class="chapter" data-level="2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy"><i class="fa fa-check"></i><b>2.1</b> Entropy</a><ul>
<li class="chapter" data-level="2.1.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#surprise"><i class="fa fa-check"></i><b>2.1.2</b> Surprise</a></li>
<li class="chapter" data-level="2.1.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#shannon-entropy"><i class="fa fa-check"></i><b>2.1.3</b> Shannon entropy</a></li>
<li class="chapter" data-level="2.1.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#differential-entropy"><i class="fa fa-check"></i><b>2.1.4</b> Differential entropy</a></li>
<li class="chapter" data-level="2.1.5" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-entropy-principle-to-characterise-distributions"><i class="fa fa-check"></i><b>2.1.5</b> Maximum entropy principle to characterise distributions</a></li>
<li class="chapter" data-level="2.1.6" data-path="02-likelihood2.html"><a href="02-likelihood2.html#cross-entropy"><i class="fa fa-check"></i><b>2.1.6</b> Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>2.2</b> Kullback-Leibler divergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#definition"><i class="fa fa-check"></i><b>2.2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#properties-of-kl-divergence"><i class="fa fa-check"></i><b>2.2.2</b> Properties of KL divergence</a></li>
<li class="chapter" data-level="2.2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#examples"><i class="fa fa-check"></i><b>2.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#local-quadratic-approximation-and-expected-fisher-information"><i class="fa fa-check"></i><b>2.3</b> Local quadratic approximation and expected Fisher information</a></li>
<li class="chapter" data-level="2.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy-learning-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4</b> Entropy learning and maximum likelihood</a><ul>
<li class="chapter" data-level="2.4.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#the-relative-entropy-between-true-model-and-approximating-model"><i class="fa fa-check"></i><b>2.4.1</b> The relative entropy between true model and approximating model</a></li>
<li class="chapter" data-level="2.4.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#minimum-kl-divergence-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4.2</b> Minimum KL divergence and maximum likelihood</a></li>
<li class="chapter" data-level="2.4.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#further-connections"><i class="fa fa-check"></i><b>2.4.3</b> Further connections</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="03-likelihood3.html"><a href="03-likelihood3.html"><i class="fa fa-check"></i><b>3</b> Maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#principle-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.1</b> Principle of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#outline"><i class="fa fa-check"></i><b>3.1.1</b> Outline</a></li>
<li class="chapter" data-level="3.1.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#recipe-for-obtaining-mles"><i class="fa fa-check"></i><b>3.1.2</b> Recipe for obtaining MLEs</a></li>
<li class="chapter" data-level="3.1.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#invariance-property-of-the-mle"><i class="fa fa-check"></i><b>3.1.3</b> Invariance property of the MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#examples-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.2</b> Examples of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.2.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-estimation-of-a-proportion"><i class="fa fa-check"></i><b>3.2.1</b> Example 1: Estimation of a proportion</a></li>
<li class="chapter" data-level="3.2.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-exponential-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Example 2: Exponential Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-3-normal-distribution-with-unknown-mean-and-known-variance"><i class="fa fa-check"></i><b>3.2.3</b> Example 3: Normal distribution with unknown mean and known variance</a></li>
<li class="chapter" data-level="3.2.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-4-normal-distribution-with-both-mean-and-variance-unknown"><i class="fa fa-check"></i><b>3.2.4</b> Example 4: Normal Distribution with both mean and variance unknown</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information"><i class="fa fa-check"></i><b>3.3</b> Observed Fisher information</a><ul>
<li class="chapter" data-level="3.3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#motivation"><i class="fa fa-check"></i><b>3.3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#curvature-of-log-likelihood-function"><i class="fa fa-check"></i><b>3.3.2</b> Curvature of log-likelihood function</a></li>
<li class="chapter" data-level="3.3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information-1"><i class="fa fa-check"></i><b>3.3.3</b> Observed Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information---examples"><i class="fa fa-check"></i><b>3.4</b> Observed Fisher information - Examples</a><ul>
<li class="chapter" data-level="3.4.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-bernoulli-binomial-model"><i class="fa fa-check"></i><b>3.4.1</b> Example 1: Bernoulli / Binomial model</a></li>
<li class="chapter" data-level="3.4.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-normal-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Example 2: Normal distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#differences-of-observed-to-expected-fisher-information"><i class="fa fa-check"></i><b>3.4.3</b> Differences of observed to expected Fisher information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="04-likelihood4.html"><a href="04-likelihood4.html"><i class="fa fa-check"></i><b>4</b> Quadratic approximation and normal asymptotics</a><ul>
<li class="chapter" data-level="4.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#covariance-correlation-and-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.1</b> Covariance, correlation and multivariate normal distribution</a></li>
<li class="chapter" data-level="4.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#maximum-likelihood-estimates-of-the-parameters-of-the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.2</b> Maximum likelihood estimates of the parameters of the multivariate normal distribution</a></li>
<li class="chapter" data-level="4.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-approximation-of-log-likelihood-function-around-mle"><i class="fa fa-check"></i><b>4.3</b> Quadratic approximation of log-likelihood function around MLE</a></li>
<li class="chapter" data-level="4.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-normality-of-mle"><i class="fa fa-check"></i><b>4.4</b> Asymptotic normality of MLE</a></li>
<li class="chapter" data-level="4.5" data-path="04-likelihood4.html"><a href="04-likelihood4.html#observed-or-expected-fisher-information-to-estimate-variance-of-the-mle"><i class="fa fa-check"></i><b>4.5</b> Observed or expected Fisher information to estimate variance of the MLE?</a></li>
<li class="chapter" data-level="4.6" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-confidence-intervals-for-mles"><i class="fa fa-check"></i><b>4.6</b> Normal confidence intervals for MLEs</a></li>
<li class="chapter" data-level="4.7" data-path="04-likelihood4.html"><a href="04-likelihood4.html#wald-statistic"><i class="fa fa-check"></i><b>4.7</b> Wald statistic</a></li>
<li class="chapter" data-level="4.8" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-ci-expressed-using-the-squared-wald-statistics"><i class="fa fa-check"></i><b>4.8</b> Normal CI expressed using the squared Wald statistics</a></li>
<li class="chapter" data-level="4.9" data-path="04-likelihood4.html"><a href="04-likelihood4.html#testing-and-confidence-intervals"><i class="fa fa-check"></i><b>4.9</b> Testing and confidence intervals</a></li>
<li class="chapter" data-level="4.10" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-normal-distribution"><i class="fa fa-check"></i><b>4.10</b> Example: normal distribution</a></li>
<li class="chapter" data-level="4.11" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-of-non-regular-model"><i class="fa fa-check"></i><b>4.11</b> Example of non-regular model</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="05-likelihood5.html"><a href="05-likelihood5.html"><i class="fa fa-check"></i><b>5</b> Likelihood-based confidence interval and likelihood ratio</a><ul>
<li class="chapter" data-level="5.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-based-confidence-intervals"><i class="fa fa-check"></i><b>5.1</b> Likelihood-based confidence intervals</a></li>
<li class="chapter" data-level="5.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#wilks-log-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.2</b> Wilks log likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.3" data-path="05-likelihood5.html"><a href="05-likelihood5.html#quadratic-approximation-of-wilks-statistic"><i class="fa fa-check"></i><b>5.3</b> Quadratic approximation of Wilks statistic</a></li>
<li class="chapter" data-level="5.4" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistics"><i class="fa fa-check"></i><b>5.4</b> Distribution of Wilks statistics</a><ul>
<li class="chapter" data-level="5.4.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#cutoff-values-delta"><i class="fa fa-check"></i><b>5.4.1</b> Cutoff values <span class="math inline">\(\Delta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="05-likelihood5.html"><a href="05-likelihood5.html#example-likelihood-ci-for-exponential-model"><i class="fa fa-check"></i><b>5.5</b> Example: likelihood CI for exponential model</a></li>
<li class="chapter" data-level="5.6" data-path="05-likelihood5.html"><a href="05-likelihood5.html#origin-of-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.6</b> Origin of likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.7" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistic-and-likelihood-ci"><i class="fa fa-check"></i><b>5.7</b> Distribution of Wilks statistic and Likelihood CI</a></li>
<li class="chapter" data-level="5.8" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>5.8</b> Likelihood ratio test (LRT)</a></li>
<li class="chapter" data-level="5.9" data-path="05-likelihood5.html"><a href="05-likelihood5.html#optimality-of-lrts"><i class="fa fa-check"></i><b>5.9</b> Optimality of LRTs</a></li>
<li class="chapter" data-level="5.10" data-path="05-likelihood5.html"><a href="05-likelihood5.html#generalised-likelihood-ratio-test-glrt"><i class="fa fa-check"></i><b>5.10</b> Generalised likelihood ratio test (GLRT)</a></li>
<li class="chapter" data-level="5.11" data-path="05-likelihood5.html"><a href="05-likelihood5.html#glrt-example"><i class="fa fa-check"></i><b>5.11</b> GLRT example</a></li>
<li class="chapter" data-level="5.12" data-path="05-likelihood5.html"><a href="05-likelihood5.html#thoughts-on-model-selection"><i class="fa fa-check"></i><b>5.12</b> Thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="06-likelihood6.html"><a href="06-likelihood6.html"><i class="fa fa-check"></i><b>6</b> Optimality properties, minimal sufficiency and summary</a><ul>
<li class="chapter" data-level="6.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#properties-of-mles-encountered-so-far"><i class="fa fa-check"></i><b>6.1</b> Properties of MLEs encountered so far</a></li>
<li class="chapter" data-level="6.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#further-optimality-properties-of-mles"><i class="fa fa-check"></i><b>6.2</b> Further optimality properties of MLEs</a></li>
<li class="chapter" data-level="6.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency"><i class="fa fa-check"></i><b>6.3</b> Summarising data and the concept of minimal sufficiency</a></li>
<li class="chapter" data-level="6.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summary-and-concluding-remarks-on-maximum-likelihood"><i class="fa fa-check"></i><b>6.4</b> Summary and concluding remarks on maximum likelihood</a><ul>
<li class="chapter" data-level="6.4.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#starting-point-kl-divergence"><i class="fa fa-check"></i><b>6.4.1</b> Starting point: KL divergence</a></li>
<li class="chapter" data-level="6.4.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#connections-between-kl-divergence-likelihood-and-expected-and-observed-fisher-information"><i class="fa fa-check"></i><b>6.4.2</b> Connections between KL divergence, likelihood and expected and observed Fisher information</a></li>
<li class="chapter" data-level="6.4.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#likelihood-estimation"><i class="fa fa-check"></i><b>6.4.3</b> Likelihood estimation</a></li>
<li class="chapter" data-level="6.4.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#what-happens-if-n-is-small"><i class="fa fa-check"></i><b>6.4.4</b> What happens if <span class="math inline">\(n\)</span> is small?</a></li>
<li class="chapter" data-level="6.4.5" data-path="06-likelihood6.html"><a href="06-likelihood6.html#inference-with-likelihood"><i class="fa fa-check"></i><b>6.4.5</b> Inference with likelihood:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Bayesian Statistics</b></span></li>
<li class="chapter" data-level="7" data-path="07-bayes1.html"><a href="07-bayes1.html"><i class="fa fa-check"></i><b>7</b> Essentials of Bayesian statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#bayes-theorem"><i class="fa fa-check"></i><b>7.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#principle-of-bayesian-learning"><i class="fa fa-check"></i><b>7.2</b> Principle of Bayesian learning</a></li>
<li class="chapter" data-level="7.3" data-path="07-bayes1.html"><a href="07-bayes1.html#what-is-exactly-is-the-bayesian-estimate"><i class="fa fa-check"></i><b>7.3</b> What is exactly is the “Bayesian estimate”?</a></li>
<li class="chapter" data-level="7.4" data-path="07-bayes1.html"><a href="07-bayes1.html#computer-implementation-of-bayesian-learning"><i class="fa fa-check"></i><b>7.4</b> Computer implementation of Bayesian learning</a></li>
<li class="chapter" data-level="7.5" data-path="07-bayes1.html"><a href="07-bayes1.html#bayesian-interpretation-of-probability"><i class="fa fa-check"></i><b>7.5</b> Bayesian interpretation of probability</a><ul>
<li class="chapter" data-level="7.5.1" data-path="07-bayes1.html"><a href="07-bayes1.html#what-makes-you-bayesian"><i class="fa fa-check"></i><b>7.5.1</b> What makes you “Bayesian”?</a></li>
<li class="chapter" data-level="7.5.2" data-path="07-bayes1.html"><a href="07-bayes1.html#mathematics-of-probability"><i class="fa fa-check"></i><b>7.5.2</b> Mathematics of probability</a></li>
<li class="chapter" data-level="7.5.3" data-path="07-bayes1.html"><a href="07-bayes1.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.5.3</b> Interpretations of probability</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="07-bayes1.html"><a href="07-bayes1.html#historical-developments"><i class="fa fa-check"></i><b>7.6</b> Historical developments</a></li>
<li class="chapter" data-level="7.7" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning"><i class="fa fa-check"></i><b>7.7</b> Connection with entropy learning</a><ul>
<li class="chapter" data-level="7.7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#zero-forcing-property"><i class="fa fa-check"></i><b>7.7.1</b> Zero forcing property</a></li>
<li class="chapter" data-level="7.7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning-1"><i class="fa fa-check"></i><b>7.7.2</b> Connection with entropy learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="08-bayes2.html"><a href="08-bayes2.html"><i class="fa fa-check"></i><b>8</b> Beta-Binomial model for estimating a proportion</a><ul>
<li class="chapter" data-level="8.1" data-path="08-bayes2.html"><a href="08-bayes2.html#binomial-likelihood"><i class="fa fa-check"></i><b>8.1</b> Binomial likelihood</a></li>
<li class="chapter" data-level="8.2" data-path="08-bayes2.html"><a href="08-bayes2.html#excursion-properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>8.2</b> Excursion: Properties of the Beta distribution</a></li>
<li class="chapter" data-level="8.3" data-path="08-bayes2.html"><a href="08-bayes2.html#beta-prior-distribution"><i class="fa fa-check"></i><b>8.3</b> Beta prior distribution</a></li>
<li class="chapter" data-level="8.4" data-path="08-bayes2.html"><a href="08-bayes2.html#computing-the-posterior-distribution"><i class="fa fa-check"></i><b>8.4</b> Computing the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="09-bayes3.html"><a href="09-bayes3.html"><i class="fa fa-check"></i><b>9</b> Properties of Bayesian learning</a><ul>
<li class="chapter" data-level="9.1" data-path="09-bayes3.html"><a href="09-bayes3.html#prior-acting-as-pseudo-data"><i class="fa fa-check"></i><b>9.1</b> Prior acting as pseudo-data</a></li>
<li class="chapter" data-level="9.2" data-path="09-bayes3.html"><a href="09-bayes3.html#linear-shrinkage-of-mean"><i class="fa fa-check"></i><b>9.2</b> Linear shrinkage of mean</a></li>
<li class="chapter" data-level="9.3" data-path="09-bayes3.html"><a href="09-bayes3.html#conjugacy-of-prior-and-posterior-distribution"><i class="fa fa-check"></i><b>9.3</b> Conjugacy of prior and posterior distribution</a></li>
<li class="chapter" data-level="9.4" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-asymptotics"><i class="fa fa-check"></i><b>9.4</b> Large sample asymptotics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-limits-of-mean-and-variance"><i class="fa fa-check"></i><b>9.4.1</b> Large sample limits of mean and variance</a></li>
<li class="chapter" data-level="9.4.2" data-path="09-bayes3.html"><a href="09-bayes3.html#asymptotic-normality-of-the-posterior-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Asymptotic Normality of the Posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="09-bayes3.html"><a href="09-bayes3.html#posterior-variance-for-finite-n"><i class="fa fa-check"></i><b>9.5</b> Posterior variance for finite <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-bayes4.html"><a href="10-bayes4.html"><i class="fa fa-check"></i><b>10</b> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a><ul>
<li class="chapter" data-level="10.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-normal-model-to-estimate-mean"><i class="fa fa-check"></i><b>10.1</b> Normal-Normal model to estimate mean</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihood"><i class="fa fa-check"></i><b>10.1.1</b> Normal likelihood</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-prior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Normal prior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-posterior-distribution"><i class="fa fa-check"></i><b>10.1.3</b> Normal posterior distribution</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-and-stein-paradox"><i class="fa fa-check"></i><b>10.1.4</b> Large sample asymptotics and Stein paradox</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-normal-model-to-estimate-variance"><i class="fa fa-check"></i><b>10.2</b> Inverse-Gamma-Normal model to estimate variance</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Inverse Gamma distribution</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihoood"><i class="fa fa-check"></i><b>10.2.2</b> Normal likelihoood</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-prior-distribution"><i class="fa fa-check"></i><b>10.2.3</b> Inverse Gamma prior distribution</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-posterior-distribution"><i class="fa fa-check"></i><b>10.2.4</b> Inverse Gamma posterior distribution</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-1"><i class="fa fa-check"></i><b>10.2.5</b> Large sample asymptotics</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-bayes4.html"><a href="10-bayes4.html#estimating-precision"><i class="fa fa-check"></i><b>10.2.6</b> Estimating precision</a></li>
<li class="chapter" data-level="10.2.7" data-path="10-bayes4.html"><a href="10-bayes4.html#joint-estimation-of-mean-and-variance"><i class="fa fa-check"></i><b>10.2.7</b> Joint estimation of mean and variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-bayes5.html"><a href="11-bayes5.html"><i class="fa fa-check"></i><b>11</b> Shrinkage estimation using empirical risk minimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="11-bayes5.html"><a href="11-bayes5.html#linear-shrinkage"><i class="fa fa-check"></i><b>11.1</b> Linear shrinkage</a></li>
<li class="chapter" data-level="11.2" data-path="11-bayes5.html"><a href="11-bayes5.html#james-stein-estimator"><i class="fa fa-check"></i><b>11.2</b> James-Stein estimator</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-bayes6.html"><a href="12-bayes6.html"><i class="fa fa-check"></i><b>12</b> Bayesian model comparison using Bayes factors and the BIC</a><ul>
<li class="chapter" data-level="12.1" data-path="12-bayes6.html"><a href="12-bayes6.html#the-bayes-factor"><i class="fa fa-check"></i><b>12.1</b> The Bayes factor</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-bayes6.html"><a href="12-bayes6.html#connection-with-relative-entropy"><i class="fa fa-check"></i><b>12.1.1</b> Connection with relative entropy</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-bayes6.html"><a href="12-bayes6.html#interpretation-of-and-scale-for-bayes-factor"><i class="fa fa-check"></i><b>12.1.2</b> Interpretation of and scale for Bayes factor</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-bayes6.html"><a href="12-bayes6.html#computing-textprd-m-for-simple-and-composite-models"><i class="fa fa-check"></i><b>12.1.3</b> Computing <span class="math inline">\(\text{Pr}(D | M)\)</span> for simple and composite models</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-bayes6.html"><a href="12-bayes6.html#bayes-factor-versus-likelihood-ratio"><i class="fa fa-check"></i><b>12.1.4</b> Bayes factor versus likelihood ratio</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-bayes6.html"><a href="12-bayes6.html#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><i class="fa fa-check"></i><b>12.2</b> Approximate computation of the marginal likelihood and of the log-Bayes factor</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-bayes6.html"><a href="12-bayes6.html#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fa fa-check"></i><b>12.2.1</b> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-bayes6.html"><a href="12-bayes6.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>12.2.2</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-bayes6.html"><a href="12-bayes6.html#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fa fa-check"></i><b>12.2.3</b> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-bayes6.html"><a href="12-bayes6.html#model-complexity-and-occams-razor"><i class="fa fa-check"></i><b>12.2.4</b> Model complexity and Occams razor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-bayes7.html"><a href="13-bayes7.html"><i class="fa fa-check"></i><b>13</b> False discovery rates</a><ul>
<li class="chapter" data-level="13.1" data-path="13-bayes7.html"><a href="13-bayes7.html#general-setup"><i class="fa fa-check"></i><b>13.1</b> General setup</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-bayes7.html"><a href="13-bayes7.html#overview-1"><i class="fa fa-check"></i><b>13.1.1</b> Overview</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-bayes7.html"><a href="13-bayes7.html#choosing-between-h_0-and-h_a"><i class="fa fa-check"></i><b>13.1.2</b> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span></a></li>
<li class="chapter" data-level="13.1.3" data-path="13-bayes7.html"><a href="13-bayes7.html#true-and-false-positives-and-negatives"><i class="fa fa-check"></i><b>13.1.3</b> True and false positives and negatives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-bayes7.html"><a href="13-bayes7.html#specificity-and-sensitivity"><i class="fa fa-check"></i><b>13.2</b> Specificity and Sensitivity</a></li>
<li class="chapter" data-level="13.3" data-path="13-bayes7.html"><a href="13-bayes7.html#fdr-and-fndr"><i class="fa fa-check"></i><b>13.3</b> FDR and FNDR</a></li>
<li class="chapter" data-level="13.4" data-path="13-bayes7.html"><a href="13-bayes7.html#bayesian-perspective"><i class="fa fa-check"></i><b>13.4</b> Bayesian perspective</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-bayes7.html"><a href="13-bayes7.html#two-component-mixture-model"><i class="fa fa-check"></i><b>13.4.1</b> Two component mixture model</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-bayes7.html"><a href="13-bayes7.html#local-fdr"><i class="fa fa-check"></i><b>13.4.2</b> Local FDR</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-bayes7.html"><a href="13-bayes7.html#q-values"><i class="fa fa-check"></i><b>13.4.3</b> q-values</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="13-bayes7.html"><a href="13-bayes7.html#software"><i class="fa fa-check"></i><b>13.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-bayes8.html"><a href="14-bayes8.html"><i class="fa fa-check"></i><b>14</b> Optimality properties and summary</a><ul>
<li class="chapter" data-level="14.1" data-path="14-bayes8.html"><a href="14-bayes8.html#bayesian-statistics-in-a-nutshell"><i class="fa fa-check"></i><b>14.1</b> Bayesian statistics in a nutshell</a><ul>
<li class="chapter" data-level="14.1.1" data-path="14-bayes8.html"><a href="14-bayes8.html#remarks"><i class="fa fa-check"></i><b>14.1.1</b> Remarks</a></li>
<li class="chapter" data-level="14.1.2" data-path="14-bayes8.html"><a href="14-bayes8.html#advantages"><i class="fa fa-check"></i><b>14.1.2</b> Advantages</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="14-bayes8.html"><a href="14-bayes8.html#frequentist-properties-of-bayesian-estimators"><i class="fa fa-check"></i><b>14.2</b> Frequentist properties of Bayesian estimators</a></li>
<li class="chapter" data-level="14.3" data-path="14-bayes8.html"><a href="14-bayes8.html#specifying-the-prior-problem-or-advantage"><i class="fa fa-check"></i><b>14.3</b> Specifying the prior — problem or advantage?</a></li>
<li class="chapter" data-level="14.4" data-path="14-bayes8.html"><a href="14-bayes8.html#choosing-a-prior"><i class="fa fa-check"></i><b>14.4</b> Choosing a prior</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-bayes8.html"><a href="14-bayes8.html#some-guidelines"><i class="fa fa-check"></i><b>14.4.1</b> Some guidelines</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-bayes8.html"><a href="14-bayes8.html#jeffreys-prior"><i class="fa fa-check"></i><b>14.4.2</b> Jeffreys prior</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-bayes8.html"><a href="14-bayes8.html#optimality-of-bayes-inference"><i class="fa fa-check"></i><b>14.5</b> Optimality of Bayes inference</a></li>
<li class="chapter" data-level="14.6" data-path="14-bayes8.html"><a href="14-bayes8.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a><ul>
<li class="chapter" data-level="14.6.1" data-path="14-bayes8.html"><a href="14-bayes8.html#current-research"><i class="fa fa-check"></i><b>14.6.1</b> Current research</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Regression</b></span></li>
<li class="chapter" data-level="15" data-path="15-regression1.html"><a href="15-regression1.html"><i class="fa fa-check"></i><b>15</b> Overview over regression modelling</a><ul>
<li class="chapter" data-level="15.1" data-path="15-regression1.html"><a href="15-regression1.html#general-setup-1"><i class="fa fa-check"></i><b>15.1</b> General setup</a></li>
<li class="chapter" data-level="15.2" data-path="15-regression1.html"><a href="15-regression1.html#objectives"><i class="fa fa-check"></i><b>15.2</b> Objectives</a></li>
<li class="chapter" data-level="15.3" data-path="15-regression1.html"><a href="15-regression1.html#regression-as-a-form-of-supervised-learning"><i class="fa fa-check"></i><b>15.3</b> Regression as a form of supervised learning</a></li>
<li class="chapter" data-level="15.4" data-path="15-regression1.html"><a href="15-regression1.html#various-regression-models-used-in-statistics"><i class="fa fa-check"></i><b>15.4</b> Various regression models used in statistics</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="16-regression2.html"><a href="16-regression2.html"><i class="fa fa-check"></i><b>16</b> Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="16-regression2.html"><a href="16-regression2.html#the-linear-regression-model"><i class="fa fa-check"></i><b>16.1</b> The linear regression model</a></li>
<li class="chapter" data-level="16.2" data-path="16-regression2.html"><a href="16-regression2.html#interpretation-of-regression-coefficients-and-intercept"><i class="fa fa-check"></i><b>16.2</b> Interpretation of regression coefficients and intercept</a></li>
<li class="chapter" data-level="16.3" data-path="16-regression2.html"><a href="16-regression2.html#different-types-of-linear-regression"><i class="fa fa-check"></i><b>16.3</b> Different types of linear regression:</a></li>
<li class="chapter" data-level="16.4" data-path="16-regression2.html"><a href="16-regression2.html#distributional-assumptions-and-properties"><i class="fa fa-check"></i><b>16.4</b> Distributional assumptions and properties</a></li>
<li class="chapter" data-level="16.5" data-path="16-regression2.html"><a href="16-regression2.html#regression-in-data-matrix-notation"><i class="fa fa-check"></i><b>16.5</b> Regression in data matrix notation</a></li>
<li class="chapter" data-level="16.6" data-path="16-regression2.html"><a href="16-regression2.html#centering-and-vanishing-of-the-intercept-beta_0"><i class="fa fa-check"></i><b>16.6</b> Centering and vanishing of the intercept <span class="math inline">\(\beta_0\)</span></a></li>
<li class="chapter" data-level="16.7" data-path="16-regression2.html"><a href="16-regression2.html#regression-objectives-for-linear-model"><i class="fa fa-check"></i><b>16.7</b> Regression objectives for linear model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="17-regression3.html"><a href="17-regression3.html"><i class="fa fa-check"></i><b>17</b> Estimating regression coefficients</a><ul>
<li class="chapter" data-level="17.1" data-path="17-regression3.html"><a href="17-regression3.html#ordinary-least-squares-ols-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.1</b> Ordinary Least Squares (OLS) estimator of regression coefficients</a></li>
<li class="chapter" data-level="17.2" data-path="17-regression3.html"><a href="17-regression3.html#maximum-likelihood-estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>17.2</b> Maximum likelihood estimation of regression coefficients</a><ul>
<li class="chapter" data-level="17.2.1" data-path="17-regression3.html"><a href="17-regression3.html#detailed-derivation-of-the-mles"><i class="fa fa-check"></i><b>17.2.1</b> Detailed derivation of the MLEs</a></li>
<li class="chapter" data-level="17.2.2" data-path="17-regression3.html"><a href="17-regression3.html#asymptotics"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotics</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="17-regression3.html"><a href="17-regression3.html#covariance-plug-in-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.3</b> Covariance plug-in estimator of regression coefficients</a><ul>
<li class="chapter" data-level="17.3.1" data-path="17-regression3.html"><a href="17-regression3.html#importance-of-positive-definiteness-of-estimated-covariance-matrix"><i class="fa fa-check"></i><b>17.3.1</b> Importance of positive definiteness of estimated covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="17-regression3.html"><a href="17-regression3.html#best-linear-predictor"><i class="fa fa-check"></i><b>17.4</b> Best linear predictor</a><ul>
<li class="chapter" data-level="17.4.1" data-path="17-regression3.html"><a href="17-regression3.html#result"><i class="fa fa-check"></i><b>17.4.1</b> Result:</a></li>
<li class="chapter" data-level="17.4.2" data-path="17-regression3.html"><a href="17-regression3.html#irreducible-error"><i class="fa fa-check"></i><b>17.4.2</b> Irreducible Error</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="17-regression3.html"><a href="17-regression3.html#regression-by-conditioning"><i class="fa fa-check"></i><b>17.5</b> Regression by conditioning</a><ul>
<li class="chapter" data-level="17.5.1" data-path="17-regression3.html"><a href="17-regression3.html#general-idea"><i class="fa fa-check"></i><b>17.5.1</b> General idea:</a></li>
<li class="chapter" data-level="17.5.2" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-normal-assumption"><i class="fa fa-check"></i><b>17.5.2</b> Multivariate normal assumption</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="17-regression3.html"><a href="17-regression3.html#standardised-regression-coefficients-and-relationship-to-correlation"><i class="fa fa-check"></i><b>17.6</b> Standardised regression coefficients and relationship to correlation</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="18-regression4.html"><a href="18-regression4.html"><i class="fa fa-check"></i><b>18</b> Squared multiple correlation and variance decomposition in linear regression</a><ul>
<li class="chapter" data-level="18.1" data-path="18-regression4.html"><a href="18-regression4.html#squared-multiple-correlation-omega2-and-the-r2-coefficient"><i class="fa fa-check"></i><b>18.1</b> Squared multiple correlation <span class="math inline">\(\Omega^2\)</span> and the <span class="math inline">\(R^2\)</span> coefficient</a><ul>
<li class="chapter" data-level="18.1.1" data-path="18-regression4.html"><a href="18-regression4.html#estimation-of-omega2-and-the-multiple-r2-coefficient"><i class="fa fa-check"></i><b>18.1.1</b> Estimation of <span class="math inline">\(\Omega^2\)</span> and the multiple <span class="math inline">\(R^2\)</span> coefficient</a></li>
<li class="chapter" data-level="18.1.2" data-path="18-regression4.html"><a href="18-regression4.html#r-output"><i class="fa fa-check"></i><b>18.1.2</b> R output</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="18-regression4.html"><a href="18-regression4.html#variance-decomposition-in-regression"><i class="fa fa-check"></i><b>18.2</b> Variance decomposition in regression</a><ul>
<li class="chapter" data-level="18.2.1" data-path="18-regression4.html"><a href="18-regression4.html#law-of-total-variance-and-variance-decomposition"><i class="fa fa-check"></i><b>18.2.1</b> Law of total variance and variance decomposition</a></li>
<li class="chapter" data-level="18.2.2" data-path="18-regression4.html"><a href="18-regression4.html#related-quantities"><i class="fa fa-check"></i><b>18.2.2</b> Related quantities</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="18-regression4.html"><a href="18-regression4.html#sample-version-of-variance-decomposition"><i class="fa fa-check"></i><b>18.3</b> Sample version of variance decomposition</a><ul>
<li class="chapter" data-level="18.3.1" data-path="18-regression4.html"><a href="18-regression4.html#geometric-interpretation-of-regression-as-orthogonal-projection"><i class="fa fa-check"></i><b>18.3.1</b> Geometric interpretation of regression as orthogonal projection:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="19-regression5.html"><a href="19-regression5.html"><i class="fa fa-check"></i><b>19</b> Prediction and variable selection</a><ul>
<li class="chapter" data-level="19.1" data-path="19-regression5.html"><a href="19-regression5.html#prediction-and-prediction-intervals"><i class="fa fa-check"></i><b>19.1</b> Prediction and prediction intervals</a></li>
<li class="chapter" data-level="19.2" data-path="19-regression5.html"><a href="19-regression5.html#variable-importance-and-prediction"><i class="fa fa-check"></i><b>19.2</b> Variable importance and prediction</a><ul>
<li class="chapter" data-level="19.2.1" data-path="19-regression5.html"><a href="19-regression5.html#how-to-quantify-variable-importance"><i class="fa fa-check"></i><b>19.2.1</b> How to quantify variable importance?</a></li>
<li class="chapter" data-level="19.2.2" data-path="19-regression5.html"><a href="19-regression5.html#some-candidates-for-vims"><i class="fa fa-check"></i><b>19.2.2</b> Some candidates for VIMs</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="19-regression5.html"><a href="19-regression5.html#regression-t-scores."><i class="fa fa-check"></i><b>19.3</b> Regression <span class="math inline">\(t\)</span>-scores.</a><ul>
<li class="chapter" data-level="19.3.1" data-path="19-regression5.html"><a href="19-regression5.html#computation"><i class="fa fa-check"></i><b>19.3.1</b> Computation</a></li>
<li class="chapter" data-level="19.3.2" data-path="19-regression5.html"><a href="19-regression5.html#connection-with-partial-correlation"><i class="fa fa-check"></i><b>19.3.2</b> Connection with partial correlation</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="19-regression5.html"><a href="19-regression5.html#further-approaches-for-variable-selection"><i class="fa fa-check"></i><b>19.4</b> Further approaches for variable selection</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="20-refresher.html"><a href="20-refresher.html"><i class="fa fa-check"></i><b>A</b> Refresher</a><ul>
<li class="chapter" data-level="A.1" data-path="20-refresher.html"><a href="20-refresher.html#vectors-and-matrices"><i class="fa fa-check"></i><b>A.1</b> Vectors and matrices</a></li>
<li class="chapter" data-level="A.2" data-path="20-refresher.html"><a href="20-refresher.html#functions"><i class="fa fa-check"></i><b>A.2</b> Functions</a><ul>
<li class="chapter" data-level="A.2.1" data-path="20-refresher.html"><a href="20-refresher.html#gradient"><i class="fa fa-check"></i><b>A.2.1</b> Gradient</a></li>
<li class="chapter" data-level="A.2.2" data-path="20-refresher.html"><a href="20-refresher.html#hessian-matrix"><i class="fa fa-check"></i><b>A.2.2</b> Hessian matrix</a></li>
<li class="chapter" data-level="A.2.3" data-path="20-refresher.html"><a href="20-refresher.html#conditions-for-local-maximum-of-a-function"><i class="fa fa-check"></i><b>A.2.3</b> Conditions for local maximum of a function</a></li>
<li class="chapter" data-level="A.2.4" data-path="20-refresher.html"><a href="20-refresher.html#linear-and-quadratic-approximation"><i class="fa fa-check"></i><b>A.2.4</b> Linear and quadratic approximation</a></li>
<li class="chapter" data-level="A.2.5" data-path="20-refresher.html"><a href="20-refresher.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.2.5</b> Functions of matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="20-refresher.html"><a href="20-refresher.html#probability"><i class="fa fa-check"></i><b>A.3</b> Probability</a><ul>
<li class="chapter" data-level="A.3.1" data-path="20-refresher.html"><a href="20-refresher.html#law-of-large-numbers"><i class="fa fa-check"></i><b>A.3.1</b> Law of large numbers:</a></li>
<li class="chapter" data-level="A.3.2" data-path="20-refresher.html"><a href="20-refresher.html#jensens-inequality"><i class="fa fa-check"></i><b>A.3.2</b> Jensen’s inequality</a></li>
<li class="chapter" data-level="A.3.3" data-path="20-refresher.html"><a href="20-refresher.html#transformation-of-univariate-densities"><i class="fa fa-check"></i><b>A.3.3</b> Transformation of univariate densities</a></li>
<li class="chapter" data-level="A.3.4" data-path="20-refresher.html"><a href="20-refresher.html#normal-distribution"><i class="fa fa-check"></i><b>A.3.4</b> Normal distribution</a></li>
<li class="chapter" data-level="A.3.5" data-path="20-refresher.html"><a href="20-refresher.html#chi-squared-distribution"><i class="fa fa-check"></i><b>A.3.5</b> Chi-squared distribution</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="20-refresher.html"><a href="20-refresher.html#statistics"><i class="fa fa-check"></i><b>A.4</b> Statistics</a><ul>
<li class="chapter" data-level="A.4.1" data-path="20-refresher.html"><a href="20-refresher.html#statistical-learning"><i class="fa fa-check"></i><b>A.4.1</b> Statistical learning</a></li>
<li class="chapter" data-level="A.4.2" data-path="20-refresher.html"><a href="20-refresher.html#point-and-interval-estimation"><i class="fa fa-check"></i><b>A.4.2</b> Point and interval estimation</a></li>
<li class="chapter" data-level="A.4.3" data-path="20-refresher.html"><a href="20-refresher.html#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fa fa-check"></i><b>A.4.3</b> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span></a></li>
<li class="chapter" data-level="A.4.4" data-path="20-refresher.html"><a href="20-refresher.html#asymptotics-1"><i class="fa fa-check"></i><b>A.4.4</b> Asymptotics</a></li>
<li class="chapter" data-level="A.4.5" data-path="20-refresher.html"><a href="20-refresher.html#confidence-intervals"><i class="fa fa-check"></i><b>A.4.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="A.4.6" data-path="20-refresher.html"><a href="20-refresher.html#symmetric-normal-confidence-interval"><i class="fa fa-check"></i><b>A.4.6</b> Symmetric normal confidence interval</a></li>
<li class="chapter" data-level="A.4.7" data-path="20-refresher.html"><a href="20-refresher.html#confidence-interval-for-chi-squared-distribution"><i class="fa fa-check"></i><b>A.4.7</b> Confidence interval for chi-squared distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="21-further-study.html"><a href="21-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="21-further-study.html"><a href="21-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="21-further-study.html"><a href="21-further-study.html#additional-references"><i class="fa fa-check"></i><b>B.2</b> Additional references</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="22-references.html"><a href="22-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Statistical Methods:<br />
Likelihood, Bayes and Regression</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="squared-multiple-correlation-and-variance-decomposition-in-linear-regression" class="section level1">
<h1><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</h1>
<p>In this chapter we first introduce the (squared) multiple correlation and the multiple and adjusted <span class="math inline">\(R^2\)</span> coefficients as estimators. Subsequently
we discuss variance decomposition.</p>
<div id="squared-multiple-correlation-omega2-and-the-r2-coefficient" class="section level2">
<h2><span class="header-section-number">18.1</span> Squared multiple correlation <span class="math inline">\(\Omega^2\)</span> and the <span class="math inline">\(R^2\)</span> coefficient</h2>
<p>In the previous chapter we encountered the following quantity:
<span class="math display">\[
\Omega^2 =  \boldsymbol P_{y \boldsymbol x} \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy} = \sigma_y^{-2} \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}
\]</span></p>
<p>With <span class="math inline">\(\boldsymbol \beta=\boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy}\)</span>
and
<span class="math inline">\(\beta_0=\mu_y- \boldsymbol \beta^T \boldsymbol \mu_{\boldsymbol x}\)</span> it is straightforward to verify the following:</p>
<ul>
<li>the cross-covariance between <span class="math inline">\(y\)</span> and <span class="math inline">\(y^{\star}\)</span> is
<span class="math inline">\(\text{Cov}(y, y^{\star}) = \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \beta= \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy} = \sigma^2_y \boldsymbol P_{y \boldsymbol x} \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy} = \sigma_y^2 \Omega^2\)</span></li>
<li>the (signal) variance of <span class="math inline">\(y^{\star}\)</span> is
<span class="math inline">\(\text{Var}(y^{\star})= \boldsymbol \beta^T \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x} \boldsymbol \beta= \boldsymbol \Sigma_{y \boldsymbol x} \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol \Sigma_{\boldsymbol xy} = \sigma^2_y \boldsymbol P_{y \boldsymbol x} \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy} = \sigma_y^2 \Omega^2\)</span>.</li>
</ul>
<p>hence the correlation <span class="math inline">\(\text{Cor}(y, y^{\star}) = \frac{\text{Cov}(y, y^{\star})}{\text{SD}(y) \text{SD}(y^{\star})} = \Omega\)</span> with <span class="math inline">\(\Omega \geq 0\)</span>.</p>
<p>This helps to understand the <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(\Omega^2\)</span> coefficients:</p>
<ul>
<li><p><span class="math inline">\(\Omega\)</span> is the linear correlation between the response (<span class="math inline">\(y\)</span>) and prediction <span class="math inline">\(y^{\star}\)</span>.</p></li>
<li><p><span class="math inline">\(\Omega^2\)</span> is called the <strong>squared multiple correlation</strong> between the scalar <span class="math inline">\(y\)</span> and the vector <span class="math inline">\(\boldsymbol x\)</span>.</p></li>
<li><p>Note that if we only have one predictor (if <span class="math inline">\(x\)</span> is a scalar) then
<span class="math inline">\(\boldsymbol P_{x x} = 1\)</span> and <span class="math inline">\(\boldsymbol P_{y x} = \rho_{yx}\)</span> so the multiple squared correlation coefficient reduces to squared correlation
<span class="math inline">\(\Omega^2 = \rho_{yx}^2\)</span> between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>.</p></li>
</ul>
<div id="estimation-of-omega2-and-the-multiple-r2-coefficient" class="section level3">
<h3><span class="header-section-number">18.1.1</span> Estimation of <span class="math inline">\(\Omega^2\)</span> and the multiple <span class="math inline">\(R^2\)</span> coefficient</h3>
<p>The multiple squared correlation coefficient <span class="math inline">\(\Omega^2\)</span> can be estimated by plug-in of empirical estimates for the corresponding correlation matrices:
<span class="math display">\[R^2 =  \hat{\boldsymbol P}_{y \boldsymbol x} \hat{\boldsymbol P}_{\boldsymbol x\boldsymbol x}^{-1} \hat{\boldsymbol P}_{\boldsymbol xy} = \hat{\sigma}_y^{-2} \hat {\boldsymbol \Sigma}_{y \boldsymbol x} \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1} \hat{\boldsymbol \Sigma}_{\boldsymbol xy}\]</span>
This estimator of <span class="math inline">\(\Omega^2\)</span> is called the <strong>multiple <span class="math inline">\(R^2\)</span> coefficient</strong>.</p>
<p>Note that it does not matter whether the scale factor <span class="math inline">\(1/n\)</span> or <span class="math inline">\(1/(n-1)\)</span> (or something else) is used in estimating the
(co)variances since that factor will cancel out
when standardising the covariance matrix! So for estimating the correlations it does not matter whether the ML or the unbiased estimator is used.</p>
<p>Above we have seen that <span class="math inline">\(\Omega^2\)</span> is directly linked with the noise variance via
<span class="math display">\[
\text{Var}(\varepsilon) =\sigma^2_y (1-\Omega^2) \,.
\]</span>
An <strong>unbiased estimate</strong> of the noise variance <span class="math inline">\(\text{Var}(\varepsilon)\)</span> (also called <strong>residual variance</strong>) can be computed from the residual sum of squares <span class="math inline">\(RSS = \sum_{i=1}^n (y_i -\hat{y}_i)^2\)</span> and the <strong>degree of freedom</strong> <span class="math inline">\(df=n-d-1\)</span> by
<span class="math display">\[
\widehat{\text{Var}}(\varepsilon)_{UB} = \frac{RSS}{df}
\]</span>
We can also estimate this from estimates of <span class="math inline">\(\sigma^2_y\)</span> and <span class="math inline">\(\Omega^2\)</span>.
However, with <span class="math inline">\(s^2_y\)</span> an unbiased estimate of <span class="math inline">\(\sigma^2_y\)</span>
(with standardisation factor <span class="math inline">\(\frac{1}{n-1}\)</span>) a correction factor <span class="math inline">\(\kappa=\frac{n-1}{df}\)</span> is needed to recover the unbiased residual variance:
<span class="math display">\[
\widehat{\text{Var}}(\varepsilon)_{UB} = s^2_y \, (1-R^2)\, \kappa 
\]</span>
Setting <span class="math inline">\((1-R^2)\, \kappa = (1 - R^2_{\text{adj}})\)</span>
yields the <strong>adjusted multiple <span class="math inline">\(R^2\)</span> coefficient</strong>
<span class="math display">\[
R^2_{\text{adj}} = 1- (1-R^2)\, \kappa  
\]</span>
so that the unbiased residual variance can be written as
<span class="math display">\[
\widehat{\text{Var}}(\varepsilon)_{UB} = s^2_y\, (1-R^2_{\text{adj}}) 
\]</span></p>
</div>
<div id="r-output" class="section level3">
<h3><span class="header-section-number">18.1.2</span> R output</h3>
<p>In R the command lm() fits the linear regression model.</p>
<p>In addition to the regression cofficients (and derived quantities) the R function lm() also lists</p>
<ul>
<li>the multiple R-squared <span class="math inline">\(R^2\)</span>,</li>
<li>the adjusted R-squared <span class="math inline">\(R^2_{\text{adj}}\)</span>,</li>
<li>the degrees of freedom <span class="math inline">\(df\)</span> and</li>
<li>the residual standard error <span class="math inline">\(\sqrt{\widehat{\text{Var}}(\varepsilon)_{UB}}\)</span> (computed from the unbiased variance estimate).</li>
</ul>
<p>See also Worksheet 8.</p>
</div>
</div>
<div id="variance-decomposition-in-regression" class="section level2">
<h2><span class="header-section-number">18.2</span> Variance decomposition in regression</h2>
<p>The squared multiple correlation coefficient is useful also because it plays an important role in the decomposition of the total variance:</p>
<ul>
<li>total variance: <span class="math inline">\(\text{Var}(y) = \sigma^2_y\)</span></li>
<li>unexplained variance (irreducible error): <span class="math inline">\(\sigma^2_y (1-\Omega^2) = \text{Var}(\varepsilon)\)</span></li>
<li>the explained variance is the complement: <span class="math inline">\(\sigma^2_y \Omega^2 = \text{Var}(y^{\star})\)</span></li>
</ul>
<p>In summary:</p>
<p><span class="math display">\[\text{Var}(y)  =  \text{Var}(y^{\star}) + \text{Var}(\varepsilon)\]</span>
becomes
<span class="math display">\[\underbrace{\sigma^2_y}_{\text{total variance}}  = \underbrace{\sigma_y^2 \Omega^2}_{\text{explained variance}}
 + \underbrace{ \sigma^2_y (1-\Omega^2)}_{\text{unexplained variance}}\]</span></p>
<p>The unexplained variance measures the fit after introducing predictors into the model (smaller means better fit).
The total variance measures the fit of the model without any predictors. The explained variance
is the difference between total and unexplained variance, it indicates the increase in model fit
due to the predictors.</p>
<div id="law-of-total-variance-and-variance-decomposition" class="section level3">
<h3><span class="header-section-number">18.2.1</span> Law of total variance and variance decomposition</h3>
<p>The <strong>law of total variance</strong> is</p>
<p><span class="math display">\[\underbrace{\text{Var}(y)}_{\text{total variance}}  = \underbrace{\text{Var}( \text{E}(y | \boldsymbol x) ) }_{\text{explained variance}}
 + \underbrace{ \text{E}( \text{Var}( y | \boldsymbol x) )}_{\text{unexplained variance}}\]</span></p>
<p>provides a very general decomposition in explained and unexplained parts of the variance that is valid regardless of the form of the distributions <span class="math inline">\(F_{y, \boldsymbol x}\)</span> and <span class="math inline">\(F_{y | \boldsymbol x}\)</span>.</p>
<p>In regression it conncects
variance decomposition and conditioning. If you plug-in the conditional expections for the multivariate
normal model (cf. previous chapter) we recover</p>
<p><span class="math display">\[\underbrace{\sigma^2_y}_{\text{total variance}}  = \underbrace{\sigma_y^2 \Omega^2 }_{\text{explained variance}}
 + \underbrace{ \sigma^2_y (1-\Omega^2)}_{\text{unexplained variance}}\]</span></p>
</div>
<div id="related-quantities" class="section level3">
<h3><span class="header-section-number">18.2.2</span> Related quantities</h3>
<p>Using the above three quantities (total variance, explained variance, and unexplained variance)
we can construct a number of scores:</p>
<ol style="list-style-type: decimal">
<li><p><strong>coefficient of determination</strong>, <strong>squared multiple correlation</strong>: <span class="math inline">\(\frac{\text{explained var}}{\text{total var}} = \frac{\sigma_y^2 \Omega^2}{\sigma_y^2} = \Omega^2\)</span><br />
(range 0 to 1, with 1 indicating perfect fit)</p></li>
<li><p><strong>coefficient of non-determination</strong>, <strong>coefficient of alienation</strong>: <span class="math inline">\(\frac{\text{unexplained var}}{\text{total var}} = \frac{\sigma_y^2 (1-\Omega^2)}{\sigma_y^2} = 1-\Omega^2 = \alpha\)</span><br />
(range 0 to 1, with 0 indicating perfect fit)</p></li>
<li><p><strong>inverse alienation coefficient</strong>: <span class="math inline">\(\alpha^{-1} = \frac{\text{total var}}{\text{unexplained var}} = \frac{1}{1-\Omega^2} = 1+ \frac{\text{explained var}}{\text{unexplained var}}\)</span><br />
(range 1 to <span class="math inline">\(\infty\)</span>, with <span class="math inline">\(\infty\)</span> indicating perfect fit)</p></li>
<li><p><strong><span class="math inline">\(F\)</span> score</strong>, <strong><span class="math inline">\(t^2\)</span> score</strong>: <span class="math inline">\(n \frac{\text{explained var}}{\text{unexplained var}} = n \frac{\sigma_y^2 \Omega^2}{\sigma_y^2 (1-\Omega^2)} = n \frac{\Omega^2}{1-\Omega^2} = \mathcal{F} = \tau^2\)</span><br />
(range 0 to <span class="math inline">\(\infty\)</span>, with <span class="math inline">\(\infty\)</span> indicating perfect fit)</p></li>
</ol>
<p>Note that <span class="math inline">\(\mathcal{F}\)</span> and <span class="math inline">\(\tau^2\)</span> scores (=population versions of <span class="math inline">\(F\)</span> and <span class="math inline">\(t^2\)</span> statistics) by definition scale with sample size <span class="math inline">\(n\)</span>, and that <span class="math inline">\(\Omega^2 = \frac{\tau^2}{\tau^2 + n} = \frac{\mathcal{F}}{\mathcal{F} + n}\)</span> links squared correlation with squared <span class="math inline">\(t\)</span>-scores and <span class="math inline">\(F\)</span>-scores.</p>
</div>
</div>
<div id="sample-version-of-variance-decomposition" class="section level2">
<h2><span class="header-section-number">18.3</span> Sample version of variance decomposition</h2>
<p>If <span class="math inline">\(\Omega^2\)</span> and <span class="math inline">\(\sigma^2_y\)</span> are replaced by their MLEs this can be written in a sample version as follows using data points <span class="math inline">\(y_i\)</span>, predictions <span class="math inline">\(\hat{y}_i\)</span> and <span class="math inline">\(\bar{y} = \frac{1}{n}\sum_{i=1}^n {y_i}\)</span></p>
<p><span class="math display">\[\underbrace{\sum_{i=1}^n (y_i-\bar{y})^2}_{\text{total sum of squares (TSS)}}  = \underbrace{\sum_{i=1}^n (\hat{y}_i-\bar{y})^2 }_{\text{explained sum of squares (ESS)}}
 + \underbrace{\sum_{i=1}^n (y_i-\hat{y}_i)^2 }_{\text{residual sum of squares (RSS)}}\]</span></p>
<p>Note that TSS, ESS and RSS all scale with <span class="math inline">\(n\)</span>.
Using data vector notation the sample-based variance decomposition can be written in form of the Pythagorean theorem:
<span class="math display">\[\underbrace{|| \boldsymbol y-\bar{y} \boldsymbol 1\ ||^2}_{\text{total sum of squares (TSS)}}  = 
\underbrace{||\hat{\boldsymbol y}-\bar{y} \boldsymbol 1||^2 }_{\text{explained sum of squares (ESS)}}
 + \underbrace{|| \boldsymbol y-\hat{\boldsymbol y} ||^2 }_{\text{residual sum of squares (RSS)}}\]</span></p>
<div id="geometric-interpretation-of-regression-as-orthogonal-projection" class="section level3">
<h3><span class="header-section-number">18.3.1</span> Geometric interpretation of regression as orthogonal projection:</h3>
<p>The above equation can be further simplified to</p>
<p><span class="math display">\[|| \boldsymbol y||^2  = 
||\hat{\boldsymbol y}||^2 
 + \underbrace{|| \boldsymbol y-\hat{\boldsymbol y} ||^2 }_{\text{RSS}}
\]</span></p>
<p>Geometrically speaking, this implies <span class="math inline">\(\hat{\boldsymbol y}\)</span> is an orthogonal projection of <span class="math inline">\(\boldsymbol y\)</span>, since the
residuals <span class="math inline">\(\boldsymbol y-\hat{\boldsymbol y}\)</span> and the predictions <span class="math inline">\(\hat{\boldsymbol y}\)</span> are orthogonal (by construction!).</p>
<p>This also valid for the centered versions of the vectors, i.e.
<span class="math inline">\(\hat{\boldsymbol y}-\bar{y} \boldsymbol 1_n\)</span> is an orthogonal projection of <span class="math inline">\(\boldsymbol y-\bar{y} \boldsymbol 1_n\)</span> (see Figure).</p>
<p>Also note that the angle <span class="math inline">\(\theta\)</span> between the two centered vectors is directly related to the (estimated) multiple correlation, with <span class="math inline">\(R = \cos(\theta) = \frac{||\hat{\boldsymbol y}-\bar{y} \boldsymbol 1_n ||}{|| \boldsymbol y-\bar{y} \boldsymbol 1_n||}\)</span>, or <span class="math inline">\(R^2 = \cos(\theta)^2 = \frac{||\hat{\boldsymbol y}-\bar{y} \boldsymbol 1_n ||^2}{|| \boldsymbol y-\bar{y} \boldsymbol 1_n||^2} = \frac{\text{ESS}}{\text{TSS}}\)</span>.</p>
<p><img src="fig/regression5-p1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Source of Figure: <a href="http://stats.stackexchange.com/questions/123651/geometric-interpretation-of-multiple-correlation-coefficient-r-and-coefficient">Stack Exchange</a></p>

<p></p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="17-regression3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="19-regression5.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math20802-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
