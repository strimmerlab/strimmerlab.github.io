<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>19 Prediction and variable selection | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="19 Prediction and variable selection | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="19 Prediction and variable selection | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.13/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><meta name="description" content="In this chapter we discuss how to compute (lower bounds) of the prediction error and how to select variables relevant for prediction  19.1 Prediction and prediction intervals Learning the...">
<meta property="og:description" content="In this chapter we discuss how to compute (lower bounds) of the prediction error and how to select variables relevant for prediction  19.1 Prediction and prediction intervals Learning the...">
<meta name="twitter:description" content="In this chapter we discuss how to compute (lower bounds) of the prediction error and how to select variables relevant for prediction  19.1 Prediction and prediction intervals Learning the...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="active" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="prediction-and-variable-selection" class="section level1" number="19">
<h1>
<span class="header-section-number">19</span> Prediction and variable selection<a class="anchor" aria-label="anchor" href="#prediction-and-variable-selection"><i class="fas fa-link"></i></a>
</h1>
<p>In this chapter we discuss how to compute (lower bounds) of
the prediction error and how to select variables relevant for prediction</p>
<div id="prediction-and-prediction-intervals" class="section level2" number="19.1">
<h2>
<span class="header-section-number">19.1</span> Prediction and prediction intervals<a class="anchor" aria-label="anchor" href="#prediction-and-prediction-intervals"><i class="fas fa-link"></i></a>
</h2>
<p>Learning the regression function from (training) data is only the first step in application of regression models.</p>
<p>The next step is to actually make <strong>prediction</strong> of future outcomes <span class="math inline">\(y^{\text{test}}\)</span> given test data
<span class="math inline">\(\boldsymbol x^{\text{test}}\)</span>:
<span class="math display">\[
y^{\text{test}} = \hat{y}(\boldsymbol x^{\text{test}}) = \hat{f}_{\hat{\beta}_0, \hat{\boldsymbol \beta}}(\boldsymbol x^{\text{test}})
\]</span></p>
<p>Note that
<span class="math inline">\(y^{\text{test}}\)</span> is a point estimator. Is it possible also to construct a corresponding interval estimate?</p>
<div class="inline-figure"><img src="fig/regression5-p2.png" width="80%" style="display: block; margin: auto;"></div>
<p>The answer is yes, and leads back to the conditioning approach:</p>
<p><span class="math display">\[y^\star = \text{E}(y| \boldsymbol x) = \beta_0 + \boldsymbol \beta^T \boldsymbol x\]</span></p>
<p><span class="math display">\[\text{Var}(\varepsilon) = \text{Var}(y|\boldsymbol x) = \sigma^2_y (1-\Omega^2)\]</span></p>
<p>We know that the mean squared prediction error for <span class="math inline">\(y^{\star}\)</span> is <span class="math inline">\(\text{E}((y -y^{\star})^2)=\text{Var}(\varepsilon)\)</span> and that this is the minimal irreducible error. Hence, we may use <span class="math inline">\(\text{Var}(\varepsilon)\)</span> as the <em>minimum</em> variability for the prediction.</p>
<p>The corresponding prediction interval is
<span class="math display">\[\left[ y^{\star}(\boldsymbol x^{\text{test}}) \pm c \times \text{SD}(\varepsilon) \right]\]</span>
where <span class="math inline">\(c\)</span> is some suitable constant (e.g. 1.96 for symmetric 95% normal intervals).</p>
<p>However, please note that the prediction interval constructed in this fashion will be an <em>underestimate</em>.
The reason is that this assumes that we employ <span class="math inline">\(y^{\star} = \beta_0 + \boldsymbol \beta^T \boldsymbol x\)</span> but in reality we actually use <span class="math inline">\(\hat{y} = \hat{\beta}_0 + \hat{\boldsymbol \beta}^T \boldsymbol x\)</span> for prediction — note the estimated coefficients! We recall from an earlier chapter (best linear predictor) that this leads to increase of MSPE compared with using the optimal <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\boldsymbol \beta\)</span>.</p>
<p>Thus, for better prediction intervals we would need to consider the mean squared prediction error of <span class="math inline">\(\hat{y}\)</span> that can be written as <span class="math inline">\(\text{E}((y -\hat{y})^2) = \text{Var}(\varepsilon) + \delta\)</span> where <span class="math inline">\(\delta\)</span> is an <strong>additional error term due to using an estimated rather than the true regression function</strong>. <span class="math inline">\(\delta\)</span> typically declines with <span class="math inline">\(1/n\)</span> but can be substantial for small <span class="math inline">\(n\)</span> (in particular as it usually depends on the number of predictors <span class="math inline">\(d\)</span>).</p>
<p>For more details on this we refer to later modules on regression.</p>
</div>
<div id="variable-importance-and-prediction" class="section level2" number="19.2">
<h2>
<span class="header-section-number">19.2</span> Variable importance and prediction<a class="anchor" aria-label="anchor" href="#variable-importance-and-prediction"><i class="fas fa-link"></i></a>
</h2>
<p>Another key question in regression modelling is to find out
predictor variables <span class="math inline">\(x_1, x_2, \dots, x_d\)</span> are actually important for predicting the outcome <span class="math inline">\(y\)</span>.</p>
<p><span class="math inline">\(\rightarrow\)</span> We need to study variable importance measures (VIM).</p>
<div id="how-to-quantify-variable-importance" class="section level3" number="19.2.1">
<h3>
<span class="header-section-number">19.2.1</span> How to quantify variable importance?<a class="anchor" aria-label="anchor" href="#how-to-quantify-variable-importance"><i class="fas fa-link"></i></a>
</h3>
<p>A variable <span class="math inline">\(x_i\)</span> is <strong>important</strong> if it <strong>improves prediction</strong> of the response <span class="math inline">\(y\)</span>.</p>
<p>Recall variance decomposition:</p>
<p><span class="math display">\[\text{Var}(y) = \sigma_y^2 = \underbrace{\sigma^2_y\Omega^2}_{\text{explained variance}} + \underbrace{\sigma^2_y(1-\Omega^2)}_{\text{unexplained/residual variance =} \text{Var}(\varepsilon)}\]</span></p>
<ul>
<li>
<span class="math inline">\(\Omega^2\)</span> squared multiple correlation <span class="math inline">\(\in [0,1]\)</span>
</li>
<li>
<span class="math inline">\(\Omega^2\)</span> large <span class="math inline">\(\rightarrow 1\)</span> predictor variables explain most of <span class="math inline">\(\sigma_y^2\)</span>
</li>
<li>
<span class="math inline">\(\Omega^2\)</span> small <span class="math inline">\(\rightarrow 0\)</span> linear model fails and predictors do not explain variability</li>
<li>
<span class="math inline">\(\Rightarrow\)</span> If a predictor helps to <span class="math inline">\(\begin{array}{ll} \text{increase explained variance} \\ \text{decrease unexplained variance} \end{array}\)</span> then it is important!</li>
<li>
<span class="math inline">\(\Omega^2 = \boldsymbol P_{y\boldsymbol x} \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy} \hat{=}\)</span> a function of the <span class="math inline">\(X\)</span>!</li>
</ul>
<p>VIM: which predictors contribute most to <span class="math inline">\(\Omega^2\)</span></p>
</div>
<div id="some-candidates-for-vims" class="section level3" number="19.2.2">
<h3>
<span class="header-section-number">19.2.2</span> Some candidates for VIMs<a class="anchor" aria-label="anchor" href="#some-candidates-for-vims"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<p>The regression coefficients <span class="math inline">\(\boldsymbol \beta\)</span></p>
<ul>
<li><span class="math inline">\(\boldsymbol \beta= \boldsymbol \Sigma^{-1}_{\boldsymbol x\boldsymbol x} \boldsymbol \Sigma_{\boldsymbol xy}= \boldsymbol V_{\boldsymbol x}^{-1/2} \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy} \sigma_y\)</span></li>
<li>Not a good VIM since <span class="math inline">\(\boldsymbol \beta\)</span> contains the scale!</li>
<li>Large <span class="math inline">\(\hat{\beta}_i\)</span> does not indicate that <span class="math inline">\(x_i\)</span> is important.</li>
<li>Small <span class="math inline">\(\hat{\beta}_i\)</span> does not indicate that <span class="math inline">\(x_i\)</span> is not important.</li>
</ul>
</li>
<li>
<p>Standardised regression coefficients <span class="math inline">\(\boldsymbol \beta_{\text{std}}\)</span></p>
<ul>
<li><span class="math inline">\(\boldsymbol \beta_{\text{std}} = \boldsymbol P_{\boldsymbol x\boldsymbol x}^{-1} \boldsymbol P_{\boldsymbol xy}\)</span></li>
<li>implies <span class="math inline">\(\text{Var}(y)=1\)</span>, <span class="math inline">\(\text{Var}(x_i)=1\)</span>
</li>
<li>These do not contain the scale (so better than <span class="math inline">\(\hat{\beta}\)</span>)</li>
<li>But still unclear how this relates to decomposition of variance</li>
</ul>
</li>
<li>
<p>Squared marginal correlations <span class="math inline">\(\rho_{y, x_i}^2\)</span></p>
<p>Consider case of uncorrelated predictors: <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol x}=\boldsymbol I\)</span> (no correlation among <span class="math inline">\(x_i\)</span>)</p>
<p><span class="math display">\[\Rightarrow \Omega^2 = \boldsymbol P_{y\boldsymbol x} \boldsymbol P_{\boldsymbol xy} = \sum^d_{i=1} \rho_{y, x_i}^2\]</span></p>
<p><span class="math inline">\(\rho_{y, x_i}^2 = \text{Cor}(y, x_i)\)</span> is the marginal correlation between <span class="math inline">\(y\)</span> and <span class="math inline">\(x_i\)</span>, and <span class="math inline">\(\Omega^2\)</span> is (for uncorrelated predictors) the sum of squared marginal correlations.</p>
<ul>
<li>If <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol x}=\boldsymbol I\)</span>, then <em>ranking</em> predictors by <span class="math inline">\(\rho_{y, x_i}^2\)</span> is optimal!</li>
<li>The predictor with largest marginal correlation reduces the unexplained variance most!</li>
<li>good news: even if there is weak correlation among predictors the marginal correlations are still good as VIM (but then they will not perfectly add up to <span class="math inline">\(\Omega^2\)</span>)</li>
<li>Advantage: very simple but often also very effective.</li>
<li>Caution! If there is strong correlation in <span class="math inline">\(\boldsymbol P_{\boldsymbol x\boldsymbol x}\)</span>, then there is <strong>colinearity</strong> (in this case it is oftern best to remove one of the strongly correlated variables, or to merge the correlated variables).</li>
</ul>
</li>
</ol>
<p>Often, ranking predictors by their squared marginal correlations is done as a prefiltering step
(independence screening).</p>
</div>
</div>
<div id="regression-t-scores." class="section level2" number="19.3">
<h2>
<span class="header-section-number">19.3</span> Regression <span class="math inline">\(t\)</span>-scores.<a class="anchor" aria-label="anchor" href="#regression-t-scores."><i class="fas fa-link"></i></a>
</h2>
<div id="wald-statistic-for-regression-coefficients" class="section level3" number="19.3.1">
<h3>
<span class="header-section-number">19.3.1</span> Wald statistic for regression coefficients<a class="anchor" aria-label="anchor" href="#wald-statistic-for-regression-coefficients"><i class="fas fa-link"></i></a>
</h3>
<p>So far, we discussed three obvious candidates for for variable importance measures
(regression coefficients, standardised regression coefficients, marginal correlations).</p>
<p>In this section we consider a further quantity, the
<strong>regression <span class="math inline">\(t-\)</span>score</strong>:</p>
<p>Recall that ML estimation of the regression coefficients yields</p>
<ul>
<li>a point estimate <span class="math inline">\(\hat{\boldsymbol \beta}\)</span>
</li>
<li>the (asymptotic) variance <span class="math inline">\(\widehat{\text{Var}}(\hat{\boldsymbol \beta})\)</span>
</li>
<li>the asymptotic normal distribution <span class="math inline">\(\hat{\boldsymbol \beta} \overset{a}{\sim} N_d(\boldsymbol \beta, \widehat{\text{Var}}(\hat{\boldsymbol \beta}))\)</span>
</li>
</ul>
<p>Corresponding to each predictor <span class="math inline">\(x_i\)</span> we can construct from the above a <span class="math inline">\(t\)</span>-score
<span class="math display">\[
t_i = \frac{\hat{\beta}_i}{\widehat{\text{SD}}(\hat{\beta}_i)}
\]</span>
where the standard deviations are computed by <span class="math inline">\(\widehat{\text{SD}}(\hat{\beta}_i) = \text{Diag}(\widehat{\text{Var}}(\hat{\boldsymbol \beta}))_{i}\)</span>. This corresponds to the <strong>Wald statistic</strong> to test that the underlying true
regression coefficient is zero (<span class="math inline">\(\beta_i =0\)</span>).</p>
<p>Correspondingly, under the null hypthesis that <span class="math inline">\(\beta_i=0\)</span> asymptotically for large <span class="math inline">\(n\)</span>
the regression <span class="math inline">\(t\)</span>-score is standard normal distributed:
<span class="math display">\[
t_i \overset{a}{\sim} N(0,1)
\]</span>
This allows to compute (symmetric) <span class="math inline">\(p\)</span>-values <span class="math inline">\(p = 2 \Phi(-|t_i|)\)</span> where <span class="math inline">\(\Phi\)</span> is the standard normal distribution function.</p>
<p>For finite <span class="math inline">\(n\)</span>, assuming normality of the observation and using the unbiased estimate for variance
when computing <span class="math inline">\(t_i\)</span>, the exact distribution of <span class="math inline">\(i_i\)</span> is given by the Student-<span class="math inline">\(t\)</span> distribution:
<span class="math display">\[
t_i \sim t_{n-d-1}
\]</span></p>
<p>Regression <span class="math inline">\(t\)</span>-scores can thus be used to test whether a regression coefficient is zero.
A large magnitude of the <span class="math inline">\(t_i\)</span> score indicates that the hypothesis <span class="math inline">\(\beta_i=0\)</span> can be rejected.
Thus, a small <span class="math inline">\(p\)</span>-value (say smaller than 0.05) signals that the regression coefficient is non-zero and hence that the corresponding predictor variable should be included in the model.</p>
<p>This allows rank predictor variables by <span class="math inline">\(|t_i|\)</span> or the corresponding <span class="math inline">\(p\)</span>-values with regard to their relevance in the linear model. Typically, in order to simplify a model, predictors with the largest <span class="math inline">\(p\)</span>-values (and thus smallest absolute <span class="math inline">\(t\)</span>-scores) may be removed from a model. However, note that having a <span class="math inline">\(p\)</span>-value say larger than 0.05 by itself is not sufficient to declare a regression coefficient to be zero (because in classical statistical testing you can only reject the null hypothesis, but not accept it!).</p>
<p>Note that by construction the regression <span class="math inline">\(t\)</span>-scores do not depend on the scale, so when the original data are rescaled this will not affect the corresponding regression <span class="math inline">\(t\)</span>-scores.
Furthermore, if <span class="math inline">\(\widehat{\text{SD}}(\hat{\beta}_i)\)</span> is small, then the regression <span class="math inline">\(t\)</span>-score
<span class="math inline">\(t_i\)</span> can still be large even if <span class="math inline">\(\hat{\beta}_i\)</span> is small!</p>
</div>
<div id="computing" class="section level3" number="19.3.2">
<h3>
<span class="header-section-number">19.3.2</span> Computing<a class="anchor" aria-label="anchor" href="#computing"><i class="fas fa-link"></i></a>
</h3>
<p>When you perform regression analysis in R (or another statistical software package) the computer will return the following:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cc}
\hat{\beta}_i\\
\text{Estimated}\\
\text{repression}\\
\text{coefficient}\\
\\
\end{array}
\begin{array}{cc}
\widehat{\text{SD}}(\hat{\beta}_i)\\
\text{Error of}\\
\hat{\beta}_i\\
\\
\\
\end{array}
\begin{array}{cc}
t_i = \frac{\hat{\beta}_i}{\widehat{\text{SD}}(\hat\beta_i)}\\
\text{t-score}\\
\text{computed from }\\
\text{first two columns}\\
\\
\end{array}
\begin{array}{cc}
\text{p-values}\\
\text{for } t_i\\
\text{based on t-distribution}\\
\\
\\
\end{array}
\begin{array}{ll}
\text{Indicator of}\\
\text{Significance}\\
\text{*     } 0.9\\
\text{**    } 0.95\\
\text{***   } 0.99\\
\end{array}
\end{align*}\]</span></p>
<p>In the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function in R the standard deviation is the square root of the unbiased estimate
of the variance (but note that it itself is not unbiased!).</p>
</div>
<div id="connection-with-partial-correlation" class="section level3" number="19.3.3">
<h3>
<span class="header-section-number">19.3.3</span> Connection with partial correlation<a class="anchor" aria-label="anchor" href="#connection-with-partial-correlation"><i class="fas fa-link"></i></a>
</h3>
<p>The deeper reason why ranking predictors by regression <span class="math inline">\(t\)</span>-scores and associated <span class="math inline">\(p\)</span>-values is useful is their link with
<strong>partial correlation</strong>.</p>
<p>In particular, the (squared) regression <span class="math inline">\(t\)</span>-score can be 1:1 transformed into the
(estimated) (squared) partial correlation
<span class="math display">\[
\hat{\rho}_{y, x_i | x_{j \neq i}}^2 = \frac{t_i^2}{t_i^2 + df}
\]</span>
with <span class="math inline">\(df=n-d-1\)</span>, and it can be shown that the <span class="math inline">\(p\)</span>-values for testing that <span class="math inline">\(\beta_i=0\)</span> are exactly the
same as the <span class="math inline">\(p\)</span>-values for testing that the partial correlation <span class="math inline">\(\rho_{y, x_i | x_{j \neq i}}\)</span> vanishes!</p>
<p>Therefore, ranking the predictors <span class="math inline">\(x_i\)</span> by regression <span class="math inline">\(t\)</span>-scores leads to exactly the same ranking
and <span class="math inline">\(p\)</span>-values as partial correlation!</p>
</div>
<div id="squared-wald-statistic-and-the-f-statistic" class="section level3" number="19.3.4">
<h3>
<span class="header-section-number">19.3.4</span> Squared Wald statistic and the <span class="math inline">\(F\)</span> statistic<a class="anchor" aria-label="anchor" href="#squared-wald-statistic-and-the-f-statistic"><i class="fas fa-link"></i></a>
</h3>
<p>In the above we looked at individual regression coefficients. However, we can also
construct a Wald test using the complete vector <span class="math inline">\(\hat{\boldsymbol \beta}\)</span>. The squared Wald statistic
to test that <span class="math inline">\(\boldsymbol \beta= 0\)</span> is given by
<span class="math display">\[
\begin{split}
t^2 &amp; = \hat{\boldsymbol \beta}^T \widehat{\text{Var}}(\hat{\boldsymbol \beta}^{-1}) \hat{\boldsymbol \beta}\\
     &amp; = \left( \hat{\boldsymbol \Sigma}_{y \boldsymbol x} \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1} \right)  
       \left(  \frac{n}{ \widehat{\sigma^2_{\varepsilon}} } \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}\right) 
       \left(  \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1} \hat{\boldsymbol \Sigma}_{\boldsymbol xy} \right)  \\
    &amp; = \frac{n}{ \widehat{\sigma^2_{\varepsilon}} } 
         \hat{\boldsymbol \Sigma}_{y \boldsymbol x} \hat{\boldsymbol \Sigma}_{\boldsymbol x\boldsymbol x}^{-1}   
         \hat{\boldsymbol \Sigma}_{\boldsymbol xy}  \\
    &amp; = \frac{n}{ \widehat{\sigma^2_{\varepsilon}} } \hat{\sigma}_y^{2} R^2\\
\end{split}
\]</span>
With <span class="math inline">\(\widehat{\sigma^2_{\varepsilon}} / \hat{\sigma}_y^{2} = 1- R^2\)</span>
we finally get the related <span class="math inline">\(F\)</span> statistic
<span class="math display">\[
\frac{t^2}{n} = \frac{R^2}{1-R^2} = F
\]</span>
which is a function of <span class="math inline">\(R^2\)</span>. If <span class="math inline">\(R^2=0\)</span> then <span class="math inline">\(F=0\)</span>. If <span class="math inline">\(R^2\)</span> is large (<span class="math inline">\(&lt; 1\)</span>) then <span class="math inline">\(F\)</span> is large as well (<span class="math inline">\(&lt; \infty\)</span>) and the null hypothesis <span class="math inline">\(\boldsymbol \beta= 0\)</span> can be rejected, which implies that at least one regression coefficient is non-zero.
Note that the squared Wald statistic <span class="math inline">\(t^2\)</span> is asymptotically <span class="math inline">\(\chi^2_d\)</span> distributed which is useful
to find critical values and to compute <span class="math inline">\(p\)</span>-values.</p>
</div>
</div>
<div id="further-approaches-for-variable-selection" class="section level2" number="19.4">
<h2>
<span class="header-section-number">19.4</span> Further approaches for variable selection<a class="anchor" aria-label="anchor" href="#further-approaches-for-variable-selection"><i class="fas fa-link"></i></a>
</h2>
<p>In addition to ranking by marginal and partial correlation, there are many other approaches for variable selection in regression!</p>
<ol style="list-style-type: lower-alpha">
<li>
<p>Search-based methods:</p>
<ul>
<li>search through subsets of linear models for <span class="math inline">\(d\)</span> variables, ranging from full model (including
all predictors) to the empty model (includes no predictor) and everything inbetween.</li>
<li>Problem: exhaustive search not possible even for relatively small <span class="math inline">\(d\)</span> as space of models is very large!</li>
<li>Therefore heuristic approaches such as forward selection (adding predictors), backward selection (removing predictors), or monte-carlo random search are employed.</li>
<li>Problem: maximum likelihood cannot be used for choosing among the models - since ML will always pick the best model. Therefore, penalised ML criteria such as AIC or Bayesian criteria are often employed instead.</li>
</ul>
</li>
<li>
<p>Integrative estimation and variable selection:</p>
<ul>
<li>there are methods that fit the regression model and perform variable selection <em>simultaneously</em>.</li>
<li>the most well-known approach of this type is “lasso” regression (Tibshirani 1996)</li>
<li>This applies a (generalised) linear model with ML plus L1 penalty.</li>
<li>Alternative: Bayesian variable selection and estimation procedures</li>
</ul>
</li>
<li>
<p>Entropy-based variable selection:</p>
<p>As seen above, two of the most popular approaches in linear models are based on correlation, either marginal correlation or partial correlation (via regression <span class="math inline">\(t\)</span>-scores).</p>
<p>Correlation measures can be generalised to non-linear settings. One very popular measure is the <strong>mutual information</strong> which is computed using the KL divergence. In case of two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> with joint normal distribution and correlation <span class="math inline">\(\rho\)</span> the mutual information is a function of the correlation:
<span class="math display">\[\text{MI}(x,y) = \frac{1}{2} \log (1-\rho^2)\]</span></p>
<p>In regression he mutual information between the response <span class="math inline">\(y\)</span> and predictor <span class="math inline">\(x_i\)</span> is <span class="math inline">\(\text{MI}(y, x_i)\)</span>, and this widely used for feature selection, in particular in machine learning.</p>
</li>
</ol>
</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></div>
<div class="next"><a href="refresher.html"><span class="header-section-number">A</span> Refresher</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#prediction-and-variable-selection"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li><a class="nav-link" href="#prediction-and-prediction-intervals"><span class="header-section-number">19.1</span> Prediction and prediction intervals</a></li>
<li>
<a class="nav-link" href="#variable-importance-and-prediction"><span class="header-section-number">19.2</span> Variable importance and prediction</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#how-to-quantify-variable-importance"><span class="header-section-number">19.2.1</span> How to quantify variable importance?</a></li>
<li><a class="nav-link" href="#some-candidates-for-vims"><span class="header-section-number">19.2.2</span> Some candidates for VIMs</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#regression-t-scores."><span class="header-section-number">19.3</span> Regression \(t\)-scores.</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#wald-statistic-for-regression-coefficients"><span class="header-section-number">19.3.1</span> Wald statistic for regression coefficients</a></li>
<li><a class="nav-link" href="#computing"><span class="header-section-number">19.3.2</span> Computing</a></li>
<li><a class="nav-link" href="#connection-with-partial-correlation"><span class="header-section-number">19.3.3</span> Connection with partial correlation</a></li>
<li><a class="nav-link" href="#squared-wald-statistic-and-the-f-statistic"><span class="header-section-number">19.3.4</span> Squared Wald statistic and the \(F\) statistic</a></li>
</ul>
</li>
<li><a class="nav-link" href="#further-approaches-for-variable-selection"><span class="header-section-number">19.4</span> Further approaches for variable selection</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 16 March 2022.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
