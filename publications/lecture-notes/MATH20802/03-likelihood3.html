<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Maximum likelihood estimation | HTML</title>
  <meta name="description" content="Statistical Methods:<br />
Likelihood, Bayes and Regression</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Maximum likelihood estimation | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Maximum likelihood estimation | HTML" />
  
  
  



<meta name="date" content="2021-03-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="02-likelihood2.html"/>
<link rel="next" href="04-likelihood4.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH20802/index.html">MATH20802 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Likelihood estimation and inference</b></span></li>
<li class="chapter" data-level="1" data-path="01-likelihood1.html"><a href="01-likelihood1.html"><i class="fa fa-check"></i><b>1</b> Overview of statistical learning</a><ul>
<li class="chapter" data-level="1.1" data-path="01-likelihood1.html"><a href="01-likelihood1.html#how-to-learn-from-data"><i class="fa fa-check"></i><b>1.1</b> How to learn from data?</a></li>
<li class="chapter" data-level="1.2" data-path="01-likelihood1.html"><a href="01-likelihood1.html#probability-theory-versus-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Probability theory versus statistical learning</a></li>
<li class="chapter" data-level="1.3" data-path="01-likelihood1.html"><a href="01-likelihood1.html#cartoon-of-statistical-learning"><i class="fa fa-check"></i><b>1.3</b> Cartoon of statistical learning</a></li>
<li class="chapter" data-level="1.4" data-path="01-likelihood1.html"><a href="01-likelihood1.html#likelihood"><i class="fa fa-check"></i><b>1.4</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="02-likelihood2.html"><a href="02-likelihood2.html"><i class="fa fa-check"></i><b>2</b> From entropy to maximum likelihood</a><ul>
<li class="chapter" data-level="2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy"><i class="fa fa-check"></i><b>2.1</b> Entropy</a><ul>
<li class="chapter" data-level="2.1.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#surprise-surprisal-or-shannon-information"><i class="fa fa-check"></i><b>2.1.2</b> Surprise, surprisal or Shannon information</a></li>
<li class="chapter" data-level="2.1.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#shannon-entropy"><i class="fa fa-check"></i><b>2.1.3</b> Shannon entropy</a></li>
<li class="chapter" data-level="2.1.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#differential-entropy"><i class="fa fa-check"></i><b>2.1.4</b> Differential entropy</a></li>
<li class="chapter" data-level="2.1.5" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-entropy-principle-to-characterise-distributions"><i class="fa fa-check"></i><b>2.1.5</b> Maximum entropy principle to characterise distributions</a></li>
<li class="chapter" data-level="2.1.6" data-path="02-likelihood2.html"><a href="02-likelihood2.html#cross-entropy"><i class="fa fa-check"></i><b>2.1.6</b> Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>2.2</b> Kullback-Leibler divergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#definition"><i class="fa fa-check"></i><b>2.2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#properties-of-kl-divergence"><i class="fa fa-check"></i><b>2.2.2</b> Properties of KL divergence</a></li>
<li class="chapter" data-level="2.2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#examples"><i class="fa fa-check"></i><b>2.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#local-quadratic-approximation-and-expected-fisher-information"><i class="fa fa-check"></i><b>2.3</b> Local quadratic approximation and expected Fisher information</a></li>
<li class="chapter" data-level="2.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy-learning-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4</b> Entropy learning and maximum likelihood</a><ul>
<li class="chapter" data-level="2.4.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#the-relative-entropy-between-true-model-and-approximating-model"><i class="fa fa-check"></i><b>2.4.1</b> The relative entropy between true model and approximating model</a></li>
<li class="chapter" data-level="2.4.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#minimum-kl-divergence-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4.2</b> Minimum KL divergence and maximum likelihood</a></li>
<li class="chapter" data-level="2.4.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#further-connections"><i class="fa fa-check"></i><b>2.4.3</b> Further connections</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="03-likelihood3.html"><a href="03-likelihood3.html"><i class="fa fa-check"></i><b>3</b> Maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#principle-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.1</b> Principle of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#outline"><i class="fa fa-check"></i><b>3.1.1</b> Outline</a></li>
<li class="chapter" data-level="3.1.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#obtaining-mles-for-a-regular-model"><i class="fa fa-check"></i><b>3.1.2</b> Obtaining MLEs for a regular model</a></li>
<li class="chapter" data-level="3.1.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#invariance-property-of-the-maximum-likelihood"><i class="fa fa-check"></i><b>3.1.3</b> Invariance property of the maximum likelihood</a></li>
<li class="chapter" data-level="3.1.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#consistency-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>3.1.4</b> Consistency of maximum likelihood estimates</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#maximum-likelihood-estimation-in-practise"><i class="fa fa-check"></i><b>3.2</b> Maximum likelihood estimation in practise</a><ul>
<li class="chapter" data-level="3.2.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#worked-examples"><i class="fa fa-check"></i><b>3.2.1</b> Worked examples</a></li>
<li class="chapter" data-level="3.2.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#relationship-with-least-squares-estimation"><i class="fa fa-check"></i><b>3.2.2</b> Relationship with least squares estimation</a></li>
<li class="chapter" data-level="3.2.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#bias-and-maximum-likelihood"><i class="fa fa-check"></i><b>3.2.3</b> Bias and maximum likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information"><i class="fa fa-check"></i><b>3.3</b> Observed Fisher information</a><ul>
<li class="chapter" data-level="3.3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#motivation-and-definition"><i class="fa fa-check"></i><b>3.3.1</b> Motivation and definition</a></li>
<li class="chapter" data-level="3.3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#examples-of-observed-fisher-information"><i class="fa fa-check"></i><b>3.3.2</b> Examples of observed Fisher information</a></li>
<li class="chapter" data-level="3.3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#relationship-between-observed-and-expected-fisher-information"><i class="fa fa-check"></i><b>3.3.3</b> Relationship between observed and expected Fisher information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="04-likelihood4.html"><a href="04-likelihood4.html"><i class="fa fa-check"></i><b>4</b> Quadratic approximation and normal asymptotics</a><ul>
<li class="chapter" data-level="4.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#multivariate-statistics-for-random-vectors"><i class="fa fa-check"></i><b>4.1</b> Multivariate statistics for random vectors</a><ul>
<li class="chapter" data-level="4.1.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#covariance-and-correlation"><i class="fa fa-check"></i><b>4.1.1</b> Covariance and correlation</a></li>
<li class="chapter" data-level="4.1.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.1.2</b> Multivariate normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#approximate-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.2</b> Approximate distribution of maximum likelihood estimates</a><ul>
<li class="chapter" data-level="4.2.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-log-likelihood-resulting-from-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Quadratic log-likelihood resulting from normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-approximation-of-a-log-likelihood-function"><i class="fa fa-check"></i><b>4.2.2</b> Quadratic approximation of a log-likelihood function</a></li>
<li class="chapter" data-level="4.2.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.2.3</b> Asymptotic normality of maximum likelihood estimates</a></li>
<li class="chapter" data-level="4.2.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-optimal-efficiency"><i class="fa fa-check"></i><b>4.2.4</b> Asymptotic optimal efficiency</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quantifying-the-uncertainty-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.3</b> Quantifying the uncertainty of maximum likelihood estimates</a><ul>
<li class="chapter" data-level="4.3.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#estimating-the-variance-of-mles"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the variance of MLEs</a></li>
<li class="chapter" data-level="4.3.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#wald-statistic"><i class="fa fa-check"></i><b>4.3.2</b> Wald statistic</a></li>
<li class="chapter" data-level="4.3.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-confidence-intervals-using-the-wald-statistic"><i class="fa fa-check"></i><b>4.3.3</b> Normal confidence intervals using the Wald statistic</a></li>
<li class="chapter" data-level="4.3.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-tests-using-the-wald-statistic"><i class="fa fa-check"></i><b>4.3.4</b> Normal tests using the Wald statistic</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-of-a-non-regular-model"><i class="fa fa-check"></i><b>4.4</b> Example of a non-regular model</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="05-likelihood5.html"><a href="05-likelihood5.html"><i class="fa fa-check"></i><b>5</b> Likelihood-based confidence interval and likelihood ratio</a><ul>
<li class="chapter" data-level="5.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-based-confidence-intervals-and-wilks-statistic"><i class="fa fa-check"></i><b>5.1</b> Likelihood-based confidence intervals and Wilks statistic</a><ul>
<li class="chapter" data-level="5.1.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#general-idea-and-definition-of-wilks-statistic"><i class="fa fa-check"></i><b>5.1.1</b> General idea and definition of Wilks statistic</a></li>
<li class="chapter" data-level="5.1.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#quadratic-approximation-of-wilks-statistic-and-squared-wald-statistic"><i class="fa fa-check"></i><b>5.1.2</b> Quadratic approximation of Wilks statistic and squared Wald statistic</a></li>
<li class="chapter" data-level="5.1.3" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-the-wilks-statistic"><i class="fa fa-check"></i><b>5.1.3</b> Distribution of the Wilks statistic</a></li>
<li class="chapter" data-level="5.1.4" data-path="05-likelihood5.html"><a href="05-likelihood5.html#cutoff-values-for-the-likelihood-ci"><i class="fa fa-check"></i><b>5.1.4</b> Cutoff values for the likelihood CI</a></li>
<li class="chapter" data-level="5.1.5" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-ratio-test-lrt-using-wilks-statistic"><i class="fa fa-check"></i><b>5.1.5</b> Likelihood ratio test (LRT) using Wilks statistic</a></li>
<li class="chapter" data-level="5.1.6" data-path="05-likelihood5.html"><a href="05-likelihood5.html#origin-of-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.1.6</b> Origin of likelihood ratio statistic</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#generalised-likelihood-ratio-test-glrt"><i class="fa fa-check"></i><b>5.2</b> Generalised likelihood ratio test (GLRT)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="06-likelihood6.html"><a href="06-likelihood6.html"><i class="fa fa-check"></i><b>6</b> Optimality properties and conclusion</a><ul>
<li class="chapter" data-level="6.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#properties-of-maximum-likelihood-encountered-so-far"><i class="fa fa-check"></i><b>6.1</b> Properties of maximum likelihood encountered so far</a></li>
<li class="chapter" data-level="6.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency"><i class="fa fa-check"></i><b>6.2</b> Summarising data and the concept of minimal sufficiency</a></li>
<li class="chapter" data-level="6.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#concluding-remarks-on-maximum-likelihood"><i class="fa fa-check"></i><b>6.3</b> Concluding remarks on maximum likelihood</a><ul>
<li class="chapter" data-level="6.3.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#remark-on-kl-divergence"><i class="fa fa-check"></i><b>6.3.1</b> Remark on KL divergence</a></li>
<li class="chapter" data-level="6.3.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#what-happens-if-n-is-small"><i class="fa fa-check"></i><b>6.3.2</b> What happens if <span class="math inline">\(n\)</span> is small?</a></li>
<li class="chapter" data-level="6.3.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#model-selection"><i class="fa fa-check"></i><b>6.3.3</b> Model selection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Bayesian Statistics</b></span></li>
<li class="chapter" data-level="7" data-path="07-bayes1.html"><a href="07-bayes1.html"><i class="fa fa-check"></i><b>7</b> Essentials of Bayesian statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#bayes-theorem"><i class="fa fa-check"></i><b>7.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#principle-of-bayesian-learning"><i class="fa fa-check"></i><b>7.2</b> Principle of Bayesian learning</a></li>
<li class="chapter" data-level="7.3" data-path="07-bayes1.html"><a href="07-bayes1.html#what-is-exactly-is-the-bayesian-estimate"><i class="fa fa-check"></i><b>7.3</b> What is exactly is the “Bayesian estimate”?</a></li>
<li class="chapter" data-level="7.4" data-path="07-bayes1.html"><a href="07-bayes1.html#computer-implementation-of-bayesian-learning"><i class="fa fa-check"></i><b>7.4</b> Computer implementation of Bayesian learning</a></li>
<li class="chapter" data-level="7.5" data-path="07-bayes1.html"><a href="07-bayes1.html#bayesian-interpretation-of-probability"><i class="fa fa-check"></i><b>7.5</b> Bayesian interpretation of probability</a><ul>
<li class="chapter" data-level="7.5.1" data-path="07-bayes1.html"><a href="07-bayes1.html#what-makes-you-bayesian"><i class="fa fa-check"></i><b>7.5.1</b> What makes you “Bayesian”?</a></li>
<li class="chapter" data-level="7.5.2" data-path="07-bayes1.html"><a href="07-bayes1.html#mathematics-of-probability"><i class="fa fa-check"></i><b>7.5.2</b> Mathematics of probability</a></li>
<li class="chapter" data-level="7.5.3" data-path="07-bayes1.html"><a href="07-bayes1.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.5.3</b> Interpretations of probability</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="07-bayes1.html"><a href="07-bayes1.html#historical-developments"><i class="fa fa-check"></i><b>7.6</b> Historical developments</a></li>
<li class="chapter" data-level="7.7" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning"><i class="fa fa-check"></i><b>7.7</b> Connection with entropy learning</a><ul>
<li class="chapter" data-level="7.7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#zero-forcing-property"><i class="fa fa-check"></i><b>7.7.1</b> Zero forcing property</a></li>
<li class="chapter" data-level="7.7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning-1"><i class="fa fa-check"></i><b>7.7.2</b> Connection with entropy learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="08-bayes2.html"><a href="08-bayes2.html"><i class="fa fa-check"></i><b>8</b> Beta-Binomial model for estimating a proportion</a><ul>
<li class="chapter" data-level="8.1" data-path="08-bayes2.html"><a href="08-bayes2.html#binomial-likelihood"><i class="fa fa-check"></i><b>8.1</b> Binomial likelihood</a></li>
<li class="chapter" data-level="8.2" data-path="08-bayes2.html"><a href="08-bayes2.html#excursion-properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>8.2</b> Excursion: Properties of the Beta distribution</a></li>
<li class="chapter" data-level="8.3" data-path="08-bayes2.html"><a href="08-bayes2.html#beta-prior-distribution"><i class="fa fa-check"></i><b>8.3</b> Beta prior distribution</a></li>
<li class="chapter" data-level="8.4" data-path="08-bayes2.html"><a href="08-bayes2.html#computing-the-posterior-distribution"><i class="fa fa-check"></i><b>8.4</b> Computing the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="09-bayes3.html"><a href="09-bayes3.html"><i class="fa fa-check"></i><b>9</b> Properties of Bayesian learning</a><ul>
<li class="chapter" data-level="9.1" data-path="09-bayes3.html"><a href="09-bayes3.html#prior-acting-as-pseudo-data"><i class="fa fa-check"></i><b>9.1</b> Prior acting as pseudo-data</a></li>
<li class="chapter" data-level="9.2" data-path="09-bayes3.html"><a href="09-bayes3.html#linear-shrinkage-of-mean"><i class="fa fa-check"></i><b>9.2</b> Linear shrinkage of mean</a></li>
<li class="chapter" data-level="9.3" data-path="09-bayes3.html"><a href="09-bayes3.html#conjugacy-of-prior-and-posterior-distribution"><i class="fa fa-check"></i><b>9.3</b> Conjugacy of prior and posterior distribution</a></li>
<li class="chapter" data-level="9.4" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-asymptotics"><i class="fa fa-check"></i><b>9.4</b> Large sample asymptotics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-limits-of-mean-and-variance"><i class="fa fa-check"></i><b>9.4.1</b> Large sample limits of mean and variance</a></li>
<li class="chapter" data-level="9.4.2" data-path="09-bayes3.html"><a href="09-bayes3.html#asymptotic-normality-of-the-posterior-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Asymptotic Normality of the Posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="09-bayes3.html"><a href="09-bayes3.html#posterior-variance-for-finite-n"><i class="fa fa-check"></i><b>9.5</b> Posterior variance for finite <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-bayes4.html"><a href="10-bayes4.html"><i class="fa fa-check"></i><b>10</b> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a><ul>
<li class="chapter" data-level="10.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-normal-model-to-estimate-mean"><i class="fa fa-check"></i><b>10.1</b> Normal-Normal model to estimate mean</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihood"><i class="fa fa-check"></i><b>10.1.1</b> Normal likelihood</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-prior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Normal prior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-posterior-distribution"><i class="fa fa-check"></i><b>10.1.3</b> Normal posterior distribution</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-and-stein-paradox"><i class="fa fa-check"></i><b>10.1.4</b> Large sample asymptotics and Stein paradox</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-normal-model-to-estimate-variance"><i class="fa fa-check"></i><b>10.2</b> Inverse-Gamma-Normal model to estimate variance</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Inverse Gamma distribution</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihoood"><i class="fa fa-check"></i><b>10.2.2</b> Normal likelihoood</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-prior-distribution"><i class="fa fa-check"></i><b>10.2.3</b> Inverse Gamma prior distribution</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-posterior-distribution"><i class="fa fa-check"></i><b>10.2.4</b> Inverse Gamma posterior distribution</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-1"><i class="fa fa-check"></i><b>10.2.5</b> Large sample asymptotics</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-bayes4.html"><a href="10-bayes4.html#estimating-precision"><i class="fa fa-check"></i><b>10.2.6</b> Estimating precision</a></li>
<li class="chapter" data-level="10.2.7" data-path="10-bayes4.html"><a href="10-bayes4.html#joint-estimation-of-mean-and-variance"><i class="fa fa-check"></i><b>10.2.7</b> Joint estimation of mean and variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-bayes5.html"><a href="11-bayes5.html"><i class="fa fa-check"></i><b>11</b> Shrinkage estimation using empirical risk minimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="11-bayes5.html"><a href="11-bayes5.html#linear-shrinkage"><i class="fa fa-check"></i><b>11.1</b> Linear shrinkage</a></li>
<li class="chapter" data-level="11.2" data-path="11-bayes5.html"><a href="11-bayes5.html#james-stein-estimator"><i class="fa fa-check"></i><b>11.2</b> James-Stein estimator</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-bayes6.html"><a href="12-bayes6.html"><i class="fa fa-check"></i><b>12</b> Bayesian model comparison using Bayes factors and the BIC</a><ul>
<li class="chapter" data-level="12.1" data-path="12-bayes6.html"><a href="12-bayes6.html#the-bayes-factor"><i class="fa fa-check"></i><b>12.1</b> The Bayes factor</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-bayes6.html"><a href="12-bayes6.html#connection-with-relative-entropy"><i class="fa fa-check"></i><b>12.1.1</b> Connection with relative entropy</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-bayes6.html"><a href="12-bayes6.html#interpretation-of-and-scale-for-bayes-factor"><i class="fa fa-check"></i><b>12.1.2</b> Interpretation of and scale for Bayes factor</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-bayes6.html"><a href="12-bayes6.html#computing-textprd-m-for-simple-and-composite-models"><i class="fa fa-check"></i><b>12.1.3</b> Computing <span class="math inline">\(\text{Pr}(D | M)\)</span> for simple and composite models</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-bayes6.html"><a href="12-bayes6.html#bayes-factor-versus-likelihood-ratio"><i class="fa fa-check"></i><b>12.1.4</b> Bayes factor versus likelihood ratio</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-bayes6.html"><a href="12-bayes6.html#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><i class="fa fa-check"></i><b>12.2</b> Approximate computation of the marginal likelihood and of the log-Bayes factor</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-bayes6.html"><a href="12-bayes6.html#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fa fa-check"></i><b>12.2.1</b> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-bayes6.html"><a href="12-bayes6.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>12.2.2</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-bayes6.html"><a href="12-bayes6.html#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fa fa-check"></i><b>12.2.3</b> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-bayes6.html"><a href="12-bayes6.html#model-complexity-and-occams-razor"><i class="fa fa-check"></i><b>12.2.4</b> Model complexity and Occams razor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-bayes7.html"><a href="13-bayes7.html"><i class="fa fa-check"></i><b>13</b> False discovery rates</a><ul>
<li class="chapter" data-level="13.1" data-path="13-bayes7.html"><a href="13-bayes7.html#general-setup"><i class="fa fa-check"></i><b>13.1</b> General setup</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-bayes7.html"><a href="13-bayes7.html#overview-1"><i class="fa fa-check"></i><b>13.1.1</b> Overview</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-bayes7.html"><a href="13-bayes7.html#choosing-between-h_0-and-h_a"><i class="fa fa-check"></i><b>13.1.2</b> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span></a></li>
<li class="chapter" data-level="13.1.3" data-path="13-bayes7.html"><a href="13-bayes7.html#true-and-false-positives-and-negatives"><i class="fa fa-check"></i><b>13.1.3</b> True and false positives and negatives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-bayes7.html"><a href="13-bayes7.html#specificity-and-sensitivity"><i class="fa fa-check"></i><b>13.2</b> Specificity and Sensitivity</a></li>
<li class="chapter" data-level="13.3" data-path="13-bayes7.html"><a href="13-bayes7.html#fdr-and-fndr"><i class="fa fa-check"></i><b>13.3</b> FDR and FNDR</a></li>
<li class="chapter" data-level="13.4" data-path="13-bayes7.html"><a href="13-bayes7.html#bayesian-perspective"><i class="fa fa-check"></i><b>13.4</b> Bayesian perspective</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-bayes7.html"><a href="13-bayes7.html#two-component-mixture-model"><i class="fa fa-check"></i><b>13.4.1</b> Two component mixture model</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-bayes7.html"><a href="13-bayes7.html#local-fdr"><i class="fa fa-check"></i><b>13.4.2</b> Local FDR</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-bayes7.html"><a href="13-bayes7.html#q-values"><i class="fa fa-check"></i><b>13.4.3</b> q-values</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="13-bayes7.html"><a href="13-bayes7.html#software"><i class="fa fa-check"></i><b>13.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-bayes8.html"><a href="14-bayes8.html"><i class="fa fa-check"></i><b>14</b> Optimality properties and summary</a><ul>
<li class="chapter" data-level="14.1" data-path="14-bayes8.html"><a href="14-bayes8.html#bayesian-statistics-in-a-nutshell"><i class="fa fa-check"></i><b>14.1</b> Bayesian statistics in a nutshell</a><ul>
<li class="chapter" data-level="14.1.1" data-path="14-bayes8.html"><a href="14-bayes8.html#remarks"><i class="fa fa-check"></i><b>14.1.1</b> Remarks</a></li>
<li class="chapter" data-level="14.1.2" data-path="14-bayes8.html"><a href="14-bayes8.html#advantages"><i class="fa fa-check"></i><b>14.1.2</b> Advantages</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="14-bayes8.html"><a href="14-bayes8.html#frequentist-properties-of-bayesian-estimators"><i class="fa fa-check"></i><b>14.2</b> Frequentist properties of Bayesian estimators</a></li>
<li class="chapter" data-level="14.3" data-path="14-bayes8.html"><a href="14-bayes8.html#specifying-the-prior-problem-or-advantage"><i class="fa fa-check"></i><b>14.3</b> Specifying the prior — problem or advantage?</a></li>
<li class="chapter" data-level="14.4" data-path="14-bayes8.html"><a href="14-bayes8.html#choosing-a-prior"><i class="fa fa-check"></i><b>14.4</b> Choosing a prior</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-bayes8.html"><a href="14-bayes8.html#some-guidelines"><i class="fa fa-check"></i><b>14.4.1</b> Some guidelines</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-bayes8.html"><a href="14-bayes8.html#jeffreys-prior"><i class="fa fa-check"></i><b>14.4.2</b> Jeffreys prior</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-bayes8.html"><a href="14-bayes8.html#optimality-of-bayes-inference"><i class="fa fa-check"></i><b>14.5</b> Optimality of Bayes inference</a></li>
<li class="chapter" data-level="14.6" data-path="14-bayes8.html"><a href="14-bayes8.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a><ul>
<li class="chapter" data-level="14.6.1" data-path="14-bayes8.html"><a href="14-bayes8.html#current-research"><i class="fa fa-check"></i><b>14.6.1</b> Current research</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Regression</b></span></li>
<li class="chapter" data-level="15" data-path="15-regression1.html"><a href="15-regression1.html"><i class="fa fa-check"></i><b>15</b> Overview over regression modelling</a><ul>
<li class="chapter" data-level="15.1" data-path="15-regression1.html"><a href="15-regression1.html#general-setup-1"><i class="fa fa-check"></i><b>15.1</b> General setup</a></li>
<li class="chapter" data-level="15.2" data-path="15-regression1.html"><a href="15-regression1.html#objectives"><i class="fa fa-check"></i><b>15.2</b> Objectives</a></li>
<li class="chapter" data-level="15.3" data-path="15-regression1.html"><a href="15-regression1.html#regression-as-a-form-of-supervised-learning"><i class="fa fa-check"></i><b>15.3</b> Regression as a form of supervised learning</a></li>
<li class="chapter" data-level="15.4" data-path="15-regression1.html"><a href="15-regression1.html#various-regression-models-used-in-statistics"><i class="fa fa-check"></i><b>15.4</b> Various regression models used in statistics</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="16-regression2.html"><a href="16-regression2.html"><i class="fa fa-check"></i><b>16</b> Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="16-regression2.html"><a href="16-regression2.html#the-linear-regression-model"><i class="fa fa-check"></i><b>16.1</b> The linear regression model</a></li>
<li class="chapter" data-level="16.2" data-path="16-regression2.html"><a href="16-regression2.html#interpretation-of-regression-coefficients-and-intercept"><i class="fa fa-check"></i><b>16.2</b> Interpretation of regression coefficients and intercept</a></li>
<li class="chapter" data-level="16.3" data-path="16-regression2.html"><a href="16-regression2.html#different-types-of-linear-regression"><i class="fa fa-check"></i><b>16.3</b> Different types of linear regression:</a></li>
<li class="chapter" data-level="16.4" data-path="16-regression2.html"><a href="16-regression2.html#distributional-assumptions-and-properties"><i class="fa fa-check"></i><b>16.4</b> Distributional assumptions and properties</a></li>
<li class="chapter" data-level="16.5" data-path="16-regression2.html"><a href="16-regression2.html#regression-in-data-matrix-notation"><i class="fa fa-check"></i><b>16.5</b> Regression in data matrix notation</a></li>
<li class="chapter" data-level="16.6" data-path="16-regression2.html"><a href="16-regression2.html#centering-and-vanishing-of-the-intercept-beta_0"><i class="fa fa-check"></i><b>16.6</b> Centering and vanishing of the intercept <span class="math inline">\(\beta_0\)</span></a></li>
<li class="chapter" data-level="16.7" data-path="16-regression2.html"><a href="16-regression2.html#regression-objectives-for-linear-model"><i class="fa fa-check"></i><b>16.7</b> Regression objectives for linear model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="17-regression3.html"><a href="17-regression3.html"><i class="fa fa-check"></i><b>17</b> Estimating regression coefficients</a><ul>
<li class="chapter" data-level="17.1" data-path="17-regression3.html"><a href="17-regression3.html#ordinary-least-squares-ols-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.1</b> Ordinary Least Squares (OLS) estimator of regression coefficients</a></li>
<li class="chapter" data-level="17.2" data-path="17-regression3.html"><a href="17-regression3.html#maximum-likelihood-estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>17.2</b> Maximum likelihood estimation of regression coefficients</a><ul>
<li class="chapter" data-level="17.2.1" data-path="17-regression3.html"><a href="17-regression3.html#detailed-derivation-of-the-mles"><i class="fa fa-check"></i><b>17.2.1</b> Detailed derivation of the MLEs</a></li>
<li class="chapter" data-level="17.2.2" data-path="17-regression3.html"><a href="17-regression3.html#asymptotics"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotics</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="17-regression3.html"><a href="17-regression3.html#covariance-plug-in-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.3</b> Covariance plug-in estimator of regression coefficients</a><ul>
<li class="chapter" data-level="17.3.1" data-path="17-regression3.html"><a href="17-regression3.html#importance-of-positive-definiteness-of-estimated-covariance-matrix"><i class="fa fa-check"></i><b>17.3.1</b> Importance of positive definiteness of estimated covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="17-regression3.html"><a href="17-regression3.html#best-linear-predictor"><i class="fa fa-check"></i><b>17.4</b> Best linear predictor</a><ul>
<li class="chapter" data-level="17.4.1" data-path="17-regression3.html"><a href="17-regression3.html#result"><i class="fa fa-check"></i><b>17.4.1</b> Result:</a></li>
<li class="chapter" data-level="17.4.2" data-path="17-regression3.html"><a href="17-regression3.html#irreducible-error"><i class="fa fa-check"></i><b>17.4.2</b> Irreducible Error</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="17-regression3.html"><a href="17-regression3.html#regression-by-conditioning"><i class="fa fa-check"></i><b>17.5</b> Regression by conditioning</a><ul>
<li class="chapter" data-level="17.5.1" data-path="17-regression3.html"><a href="17-regression3.html#general-idea"><i class="fa fa-check"></i><b>17.5.1</b> General idea:</a></li>
<li class="chapter" data-level="17.5.2" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-normal-assumption"><i class="fa fa-check"></i><b>17.5.2</b> Multivariate normal assumption</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="17-regression3.html"><a href="17-regression3.html#standardised-regression-coefficients-and-relationship-to-correlation"><i class="fa fa-check"></i><b>17.6</b> Standardised regression coefficients and relationship to correlation</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="18-regression4.html"><a href="18-regression4.html"><i class="fa fa-check"></i><b>18</b> Squared multiple correlation and variance decomposition in linear regression</a><ul>
<li class="chapter" data-level="18.1" data-path="18-regression4.html"><a href="18-regression4.html#squared-multiple-correlation-omega2-and-the-r2-coefficient"><i class="fa fa-check"></i><b>18.1</b> Squared multiple correlation <span class="math inline">\(\Omega^2\)</span> and the <span class="math inline">\(R^2\)</span> coefficient</a><ul>
<li class="chapter" data-level="18.1.1" data-path="18-regression4.html"><a href="18-regression4.html#estimation-of-omega2-and-the-multiple-r2-coefficient"><i class="fa fa-check"></i><b>18.1.1</b> Estimation of <span class="math inline">\(\Omega^2\)</span> and the multiple <span class="math inline">\(R^2\)</span> coefficient</a></li>
<li class="chapter" data-level="18.1.2" data-path="18-regression4.html"><a href="18-regression4.html#r-output"><i class="fa fa-check"></i><b>18.1.2</b> R output</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="18-regression4.html"><a href="18-regression4.html#variance-decomposition-in-regression"><i class="fa fa-check"></i><b>18.2</b> Variance decomposition in regression</a><ul>
<li class="chapter" data-level="18.2.1" data-path="18-regression4.html"><a href="18-regression4.html#law-of-total-variance-and-variance-decomposition"><i class="fa fa-check"></i><b>18.2.1</b> Law of total variance and variance decomposition</a></li>
<li class="chapter" data-level="18.2.2" data-path="18-regression4.html"><a href="18-regression4.html#related-quantities"><i class="fa fa-check"></i><b>18.2.2</b> Related quantities</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="18-regression4.html"><a href="18-regression4.html#sample-version-of-variance-decomposition"><i class="fa fa-check"></i><b>18.3</b> Sample version of variance decomposition</a><ul>
<li class="chapter" data-level="18.3.1" data-path="18-regression4.html"><a href="18-regression4.html#geometric-interpretation-of-regression-as-orthogonal-projection"><i class="fa fa-check"></i><b>18.3.1</b> Geometric interpretation of regression as orthogonal projection:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="19-regression5.html"><a href="19-regression5.html"><i class="fa fa-check"></i><b>19</b> Prediction and variable selection</a><ul>
<li class="chapter" data-level="19.1" data-path="19-regression5.html"><a href="19-regression5.html#prediction-and-prediction-intervals"><i class="fa fa-check"></i><b>19.1</b> Prediction and prediction intervals</a></li>
<li class="chapter" data-level="19.2" data-path="19-regression5.html"><a href="19-regression5.html#variable-importance-and-prediction"><i class="fa fa-check"></i><b>19.2</b> Variable importance and prediction</a><ul>
<li class="chapter" data-level="19.2.1" data-path="19-regression5.html"><a href="19-regression5.html#how-to-quantify-variable-importance"><i class="fa fa-check"></i><b>19.2.1</b> How to quantify variable importance?</a></li>
<li class="chapter" data-level="19.2.2" data-path="19-regression5.html"><a href="19-regression5.html#some-candidates-for-vims"><i class="fa fa-check"></i><b>19.2.2</b> Some candidates for VIMs</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="19-regression5.html"><a href="19-regression5.html#regression-t-scores."><i class="fa fa-check"></i><b>19.3</b> Regression <span class="math inline">\(t\)</span>-scores.</a><ul>
<li class="chapter" data-level="19.3.1" data-path="19-regression5.html"><a href="19-regression5.html#computation"><i class="fa fa-check"></i><b>19.3.1</b> Computation</a></li>
<li class="chapter" data-level="19.3.2" data-path="19-regression5.html"><a href="19-regression5.html#connection-with-partial-correlation"><i class="fa fa-check"></i><b>19.3.2</b> Connection with partial correlation</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="19-regression5.html"><a href="19-regression5.html#further-approaches-for-variable-selection"><i class="fa fa-check"></i><b>19.4</b> Further approaches for variable selection</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="20-refresher.html"><a href="20-refresher.html"><i class="fa fa-check"></i><b>A</b> Refresher</a><ul>
<li class="chapter" data-level="A.1" data-path="20-refresher.html"><a href="20-refresher.html#basic-mathematical-notation"><i class="fa fa-check"></i><b>A.1</b> Basic mathematical notation</a></li>
<li class="chapter" data-level="A.2" data-path="20-refresher.html"><a href="20-refresher.html#vectors-and-matrices"><i class="fa fa-check"></i><b>A.2</b> Vectors and matrices</a></li>
<li class="chapter" data-level="A.3" data-path="20-refresher.html"><a href="20-refresher.html#functions"><i class="fa fa-check"></i><b>A.3</b> Functions</a><ul>
<li class="chapter" data-level="A.3.1" data-path="20-refresher.html"><a href="20-refresher.html#convex-and-concave-functions"><i class="fa fa-check"></i><b>A.3.1</b> Convex and concave functions</a></li>
<li class="chapter" data-level="A.3.2" data-path="20-refresher.html"><a href="20-refresher.html#gradient"><i class="fa fa-check"></i><b>A.3.2</b> Gradient</a></li>
<li class="chapter" data-level="A.3.3" data-path="20-refresher.html"><a href="20-refresher.html#hessian-matrix"><i class="fa fa-check"></i><b>A.3.3</b> Hessian matrix</a></li>
<li class="chapter" data-level="A.3.4" data-path="20-refresher.html"><a href="20-refresher.html#conditions-for-local-maximum-of-a-function"><i class="fa fa-check"></i><b>A.3.4</b> Conditions for local maximum of a function</a></li>
<li class="chapter" data-level="A.3.5" data-path="20-refresher.html"><a href="20-refresher.html#linear-and-quadratic-approximation"><i class="fa fa-check"></i><b>A.3.5</b> Linear and quadratic approximation</a></li>
<li class="chapter" data-level="A.3.6" data-path="20-refresher.html"><a href="20-refresher.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.3.6</b> Functions of matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="20-refresher.html"><a href="20-refresher.html#probability"><i class="fa fa-check"></i><b>A.4</b> Probability</a><ul>
<li class="chapter" data-level="A.4.1" data-path="20-refresher.html"><a href="20-refresher.html#random-variables"><i class="fa fa-check"></i><b>A.4.1</b> Random variables</a></li>
<li class="chapter" data-level="A.4.2" data-path="20-refresher.html"><a href="20-refresher.html#transformation-of-random-variables"><i class="fa fa-check"></i><b>A.4.2</b> Transformation of random variables</a></li>
<li class="chapter" data-level="A.4.3" data-path="20-refresher.html"><a href="20-refresher.html#bernoulli-and-binomial-distribution"><i class="fa fa-check"></i><b>A.4.3</b> Bernoulli and Binomial distribution</a></li>
<li class="chapter" data-level="A.4.4" data-path="20-refresher.html"><a href="20-refresher.html#normal-distribution"><i class="fa fa-check"></i><b>A.4.4</b> Normal distribution</a></li>
<li class="chapter" data-level="A.4.5" data-path="20-refresher.html"><a href="20-refresher.html#scaled-chi-squared-distribution-and-gamma-and-exponential-distribution"><i class="fa fa-check"></i><b>A.4.5</b> Scaled Chi-squared distribution (and Gamma and Exponential distribution)</a></li>
<li class="chapter" data-level="A.4.6" data-path="20-refresher.html"><a href="20-refresher.html#law-of-large-numbers"><i class="fa fa-check"></i><b>A.4.6</b> Law of large numbers:</a></li>
<li class="chapter" data-level="A.4.7" data-path="20-refresher.html"><a href="20-refresher.html#jensens-inequality"><i class="fa fa-check"></i><b>A.4.7</b> Jensen’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="20-refresher.html"><a href="20-refresher.html#statistics"><i class="fa fa-check"></i><b>A.5</b> Statistics</a><ul>
<li class="chapter" data-level="A.5.1" data-path="20-refresher.html"><a href="20-refresher.html#statistical-learning"><i class="fa fa-check"></i><b>A.5.1</b> Statistical learning</a></li>
<li class="chapter" data-level="A.5.2" data-path="20-refresher.html"><a href="20-refresher.html#point-and-interval-estimation"><i class="fa fa-check"></i><b>A.5.2</b> Point and interval estimation</a></li>
<li class="chapter" data-level="A.5.3" data-path="20-refresher.html"><a href="20-refresher.html#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fa fa-check"></i><b>A.5.3</b> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span></a></li>
<li class="chapter" data-level="A.5.4" data-path="20-refresher.html"><a href="20-refresher.html#asymptotics-1"><i class="fa fa-check"></i><b>A.5.4</b> Asymptotics</a></li>
<li class="chapter" data-level="A.5.5" data-path="20-refresher.html"><a href="20-refresher.html#confidence-intervals"><i class="fa fa-check"></i><b>A.5.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="A.5.6" data-path="20-refresher.html"><a href="20-refresher.html#symmetric-normal-confidence-interval"><i class="fa fa-check"></i><b>A.5.6</b> Symmetric normal confidence interval</a></li>
<li class="chapter" data-level="A.5.7" data-path="20-refresher.html"><a href="20-refresher.html#confidence-interval-for-chi-squared-distribution"><i class="fa fa-check"></i><b>A.5.7</b> Confidence interval for chi-squared distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="21-further-study.html"><a href="21-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="21-further-study.html"><a href="21-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="21-further-study.html"><a href="21-further-study.html#additional-references"><i class="fa fa-check"></i><b>B.2</b> Additional references</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="22-references.html"><a href="22-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Statistical Methods:<br />
Likelihood, Bayes and Regression</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="maximum-likelihood-estimation" class="section level1">
<h1><span class="header-section-number">3</span> Maximum likelihood estimation</h1>
<div id="principle-of-maximum-likelihood-estimation" class="section level2">
<h2><span class="header-section-number">3.1</span> Principle of maximum likelihood estimation</h2>
<div id="outline" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Outline</h3>
<p>The starting points in an ML analysis are</p>
<ul>
<li>the observed <span class="math inline">\(n\)</span> data samples <span class="math inline">\(x_1,\ldots,x_n\)</span>, iid (=independent and identically distributed), with the ordering irrelevant, and a</li>
<li>model <span class="math inline">\(F_{\boldsymbol \theta}\)</span> with corresponding probability density or probability mass function <span class="math inline">\(f(x|\boldsymbol \theta)\)</span> with parameters <span class="math inline">\(\boldsymbol \theta\)</span></li>
</ul>
<p>From this we construct the likelihood function:</p>
<ul>
<li><span class="math inline">\(L_n(\boldsymbol \theta|x_1,\dots,x_n)=\prod_{i=1}^{n} f(x_i|\boldsymbol \theta)\)</span></li>
</ul>
<p>Historically, the likelihood is also often interpreted as the probability of the data given the model. However, this is not strictly correct. First this interpretation only applies to discrete random variables. Second, since the samples are iid even in this case one would still need to add a factor accounting for the multiplicity of possible orderings of the samples to obtain the correct probability of the data. Third, the interpretation of likelihood as probability of the data completely breaks down for continuous random variables because then <span class="math inline">\(f(x)\)</span> is a density, not a probability.</p>
<p>As we have seen in the previous chapter the origin of the likelihood function
lies in its connection to relative entropy. Specifically, the
log-likelihood function</p>
<ul>
<li><span class="math inline">\(l_n(\boldsymbol \theta|x_1,\dots,x_n)=\sum_{i=1}^n \log f(x_i|\boldsymbol \theta)\)</span></li>
</ul>
<p>divided by sample size <span class="math inline">\(n\)</span> is a large sample approximation of the cross-entropy between the unknown true data generating model and the approximating model <span class="math inline">\(F_{\boldsymbol \theta}\)</span>.
Note that log-likelihood is additive over the samples <span class="math inline">\(x_i\)</span>.</p>
<p>The maximum likelihood point estimate <span class="math inline">\(\hat{\boldsymbol \theta}^{ML}\)</span> is then
given by maximising the (log)-likelihood</p>
<p><span class="math display">\[\hat{\boldsymbol \theta}^{ML} = \text{arg max} l_n(\boldsymbol \theta|x_1,\dots,x_n)\]</span></p>
<p><img src="fig/lecture3_p3.PNG" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="obtaining-mles-for-a-regular-model" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Obtaining MLEs for a regular model</h3>
<p>In regular situations, i.e. when</p>
<ul>
<li>the log-likelihood function is smooth and twice differentiable,</li>
<li>the second derivative is negative and not zero, and for more than
one parameter the Hessian matrix is negative definite and not singular,</li>
<li>the parameters of the model are all identifiable (in particular the model is not overparameterised), and</li>
<li>the true parameter values lie inside the support and not on the border,</li>
</ul>
<p>then in order to maximise <span class="math inline">\(l_n\)</span> one may use the <strong>score function</strong> <span class="math inline">\(\boldsymbol S(\boldsymbol \theta)\)</span>
which is the first order derivative of the log-likelihood function:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cc}
S_n(\theta) = \frac{d l_n(\theta|x_1,\dots,x_n)}{d \theta}\\
\\
\\
\boldsymbol S_n(\boldsymbol \theta)=\nabla l_n(\boldsymbol \theta|x_1,\dots,x_n)\\
\\
\end{array}
\begin{array}{ll}
\text{scalar parameter: first derivative}\\
\text{of log-likelihood function}\\
\\
\text{gradient if } \boldsymbol \theta\text{ is a vector}\\
\text{(i.e. if there&#39;s more than one parameter)}\\
\end{array}
\end{align*}\]</span></p>
<p>A necessary (but not sufficient) condition for the MLE is that
<span class="math display">\[
\boldsymbol S_n(\hat{\boldsymbol \theta}_{ML}) = 0
\]</span></p>
<p>To demonstrate that the log-likelihood function actually achieves a
maximum at <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> the curvature
at the MLE must negative, i.e. that the log-likelihood must be locally concave at the MLE.</p>
<p>In the case of a single parameter (scalar <span class="math inline">\(\theta\)</span>) this requires to check
that the second derivative of the log-likelihood function is negative:
<span class="math display">\[
\frac{d^2 l_n(\hat{\theta}_{ML})}{d \theta^2} &lt;0
\]</span>
In the case of a parameter vector (multivariate <span class="math inline">\(\boldsymbol \theta\)</span>) you need to compute
the Hessian matrix (matrix of second order derivatives)
at the MLE:
<span class="math display">\[
\nabla^T\nabla l_n(\hat{\boldsymbol \theta}_{ML})
\]</span>
and then verify that this matrix is negative definite (i.e. all its eigenvalues must be negative).</p>
<p>As we will see later the second order derivatives of the log-likelihood function also play an important role for assessing the uncertainty of the MLE.</p>
</div>
<div id="invariance-property-of-the-maximum-likelihood" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Invariance property of the maximum likelihood</h3>
<p>Maximisation is a procedure that is invariant against coordinate transformations of the argument. Suppose <span class="math inline">\(x_{\max} = \text{arg max } h(x)\)</span> and <span class="math inline">\(y = g(x)\)</span>
where <span class="math inline">\(g\)</span> is an invertible function.
Then <span class="math inline">\(y_{\max} = \text{arg max } h( g^{-1}(y) ) = g(x_{\max})\)</span>. The achieved maximum itself remains
invariant: <span class="math inline">\(h( x_{\max} ) = h(g^{-1}(y_{\max} ) )\)</span>.</p>
<p>With regard to maximum likelihood estimation this implies the following <strong>invariance property</strong> of the maximum likelihood:</p>
<ul>
<li>Suppose that <span class="math inline">\(\hat{\theta}_{ML}\)</span> is the MLE of <span class="math inline">\(\theta\)</span>.</li>
<li>We transform the parameter to <span class="math inline">\(\theta^{\star} = g(\theta)\)</span>
where <span class="math inline">\(g\)</span> is an invertible function.</li>
<li>Then <span class="math inline">\(g(\hat{\theta}_{ML})=\hat{\theta}^{\star}\)</span> is the MLE of <span class="math inline">\(\theta^{\star}\)</span>.</li>
<li>The value of the achieved maximum likelihood is the same
in both cases, i.e. it is invariant against transformation of the parameters.</li>
</ul>
<p>The invariance property can be very useful in practise because it may be easier to perform the maximisation required for finding the MLE in a particular coordinate system.</p>
<p>See Worksheet 2 for an example application of the invariance principle.</p>
</div>
<div id="consistency-of-maximum-likelihood-estimates" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Consistency of maximum likelihood estimates</h3>
<p>One important property of maximum likelihood is that it produces <strong>consistent estimates</strong>.</p>
<p>Specifically, if the true underlying model <span class="math inline">\(F_{\text{true}}\)</span> with parameter <span class="math inline">\(\boldsymbol \theta_{\text{true}}\)</span> is contained in the set of specified candidates models <span class="math inline">\(F_{\boldsymbol \theta}\)</span>
<span class="math display">\[\underbrace{F_{\text{true}}}_{\text{true model}} \subset \underbrace{F_{\boldsymbol \theta}}_{\text{specified models}}\]</span> then <span class="math display">\[\hat{\boldsymbol \theta}_{ML} \overset{\text{large }n}{\longrightarrow} \boldsymbol \theta_{\text{true}}\]</span></p>
<p>This is a consequence of <span class="math inline">\(D_{\text{KL}}(F_{\text{true}},F_{\boldsymbol \theta})\rightarrow 0\)</span> for <span class="math inline">\(F_{\boldsymbol \theta} \rightarrow F_{\text{true}}\)</span>, and that maximisation of the likelihood function is for large <span class="math inline">\(n\)</span> equivalent to minimising the relative entropy.</p>
<p>Thus given sufficient data the MLE will converge to the true value. As a consequence, <strong>MLEs are asympotically unbiased</strong>. As we will see in the examples they can still be biased in finite samples.</p>
<p>Note that even if the candidate model <span class="math inline">\(F_{\boldsymbol \theta}\)</span> is misspecified (i.e. it does not contain the actual true model) the MLE is still optimal in the sense in that it will find the closest possible model.</p>
<p>It is possible to find inconsistent MLEs, but this occurs only in situations where the dimension of the model / number of parameters increases with sample size, or when the MLE is at a boundary or when there are singularities in the likelihood function.</p>
</div>
</div>
<div id="maximum-likelihood-estimation-in-practise" class="section level2">
<h2><span class="header-section-number">3.2</span> Maximum likelihood estimation in practise</h2>
<div id="worked-examples" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Worked examples</h3>
<p>In this section we now provide a number of worked example how ML estimation
works in practise.</p>
<div class="example">
<p><span id="exm:mleproportion" class="example"><strong>Example 3.1  </strong></span>Estimation of a proportion:</p>
<p>We aim to estimate the true proportion <span class="math inline">\(p\)</span> in a Bernoulli experiment with binary
outcomes, say the proportion of “successes” vs. “failures” or of “heads” vs. “tails” in a coin tossing experiment.</p>
<ul>
<li>Bernoulli model <span class="math inline">\(Ber(p)\)</span>: <span class="math inline">\(\text{Pr}(\text{&quot;success&quot;}) = p\)</span> and <span class="math inline">\(\text{Pr}(\text{&quot;failure&quot;}) = 1-p\)</span>.</li>
<li>The “success” is indicated by outcome <span class="math inline">\(x=1\)</span> and the “failure” by <span class="math inline">\(x=0\)</span>.</li>
<li>We conduct <span class="math inline">\(n\)</span> trials and record <span class="math inline">\(n_1\)</span> successes and <span class="math inline">\(n-n_1\)</span> failures.</li>
<li>Parameter: <span class="math inline">\(p\)</span>: probability of “success”.</li>
</ul>
<p>What is the MLE of <span class="math inline">\(p\)</span>?</p>
<ul>
<li><p>the data <span class="math inline">\(x_1, \ldots, x_n\)</span> take on values 0 or 1.</p></li>
<li><p>the average of the data points is <span class="math inline">\(\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i = \frac{n_1}{n}\)</span>.</p></li>
<li><p>the probability mass function (PMF) of the Bernoulli distribution <span class="math inline">\(Ber(p)\)</span> is:
<span class="math display">\[
f(x) = p^x (1-p)^{1-x} = 
\begin{cases}
p &amp;  \text{if $x=1$ }\\
1-p &amp; \text{if $x=0$} \\ 
\end{cases}
\]</span></p></li>
<li><p>log-PMF:
<span class="math display">\[
\log f(x) =  x \log(p) + (1-x) \log(1 - p)
\]</span></p></li>
<li><p>log-likelihood function:
<span class="math display">\[
\begin{split}
l_n(p) &amp; = \sum_{i=1}^n \log f(x_i) \\
    &amp; = n_1 \log p + (n-n_1) \log(1-p) \\
    &amp; = n \left( \bar{x} \log p + (1-\bar{x}) \log(1-p) \right) \\
\end{split}
\]</span>
Note how the log-likelihood depends on the data only through <span class="math inline">\(\bar{x}\)</span>! This is an
example of a <em>sufficient statistic</em> for the parameter <span class="math inline">\(p\)</span> (in fact it is also a <em>minimally</em> sufficient statistic). This will be discussed in more detail later.</p></li>
<li><p>Score function:
<span class="math display">\[
S_n(p)=  \frac{dl_n(p)}{dp}= n \left( \frac{\bar{x}}{p}-\frac{1-\bar{x}}{1-p} \right)
\]</span></p></li>
<li><p>Maximum likelihood estimate: Setting <span class="math inline">\(S_n(\hat{p}_{ML})=0\)</span> yields as solution
<span class="math display">\[
\hat{p}_{ML} = \bar{x} = \frac{n_1}{n}
\]</span></p>
<p>With <span class="math inline">\(\frac{dS_n(p)}{dp} = -n \left( \frac{\bar{x}}{p^2} + \frac{1-\bar{x}}{(1-p)^2} \right) &lt;0\)</span> the optimum corresponds indeed to the maximum of the (log-)likelihood function as this is negative for <span class="math inline">\(\hat{p}_{ML}\)</span> (and indeed for any <span class="math inline">\(p\)</span>).</p>
<p>The maximum likelihood estimator of <span class="math inline">\(p\)</span> is therefore identical to the frequency
of the successes among all observations.</p></li>
</ul>
</div>
<p>Note that to analyse the coin tossing experiment and to estimate <span class="math inline">\(p\)</span> we may equally well use the Binomial distribution <span class="math inline">\(\sim Binomial(n, p)\)</span> as model for the number
of successes. In this case we then have only a single observation, namely the observed <span class="math inline">\(k\)</span> . This results in the same MLE
for <span class="math inline">\(p\)</span> but the likelihood function based on the Binomial PMF includes the Binomial cofficient <span class="math inline">\(\binom{n}{k}\)</span> . However, as this factor does not depend on <span class="math inline">\(p\)</span> it disappears in the score function and has no influence in the derivation of the MLE.</p>
<div class="example">
<p><span id="exm:mlenormalmean" class="example"><strong>Example 3.2  </strong></span>Normal distribution with unknown mean and known variance:</p>
<ul>
<li><span class="math inline">\(x \sim N(\mu,\sigma^2)\)</span> with <span class="math inline">\(\text{E}(x)=\mu\)</span> and <span class="math inline">\(\text{Var}(x) = \sigma^2\)</span></li>
<li>the parameter to be estimated is <span class="math inline">\(\mu\)</span> whereas <span class="math inline">\(\sigma^2\)</span> is known.</li>
</ul>
<p>What’s the MLE of parameter <span class="math inline">\(\mu\)</span>?</p>
<ul>
<li>the data <span class="math inline">\(x_1, \ldots, x_n \in [-\infty, \infty]\)</span> are real values.</li>
<li>the average <span class="math inline">\(\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i\)</span> is real as well.</li>
<li>Density: <span class="math display">\[ f(x)=
\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</span></li>
<li>Log-Density:
<span class="math display">\[\log f(x) =-\frac{1}{2} \log(2\pi\sigma^2) - \frac{(x-\mu)^2}{2\sigma^2}\]</span></li>
<li><p>Log-likelihood function:
<span class="math display">\[
\begin{split}
l_n(\mu) &amp;= \sum_{i=1}^n \log f(x_i)\\
&amp;=-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2  \underbrace{-\frac{n}{2}\log(2 \pi \sigma^2) }_{\text{constant term, does not depend on } \mu \text{, can be removed}}\\
&amp;=-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i^2 - 2 x_i \mu+\mu^2)  + C\\
&amp;=\frac{n}{\sigma^2}  ( \bar{x} \mu  - \frac{1}{2}\mu^2)  \underbrace{ - \frac{1}{2\sigma^2}\sum_{i=1}^n   x_i^2 }_{\text{another constant term}}   + C\\
\end{split}
\]</span>
Note how the non-constant terms of the log-likelihood depend on the data only through <span class="math inline">\(\bar{x}\)</span>!</p></li>
<li>Score function:
<span class="math display">\[
S_n(\mu) = 
\frac{n}{\sigma^2} ( \bar{x}- \mu)
\]</span></li>
<li>Maximum likelihood estimate:
<span class="math display">\[S_n(\hat{\mu}_{ML})=0 \Rightarrow \hat{\mu}_{ML} = \bar{x}\]</span></li>
<li><p>With <span class="math inline">\(\frac{dS_n(\mu)}{d\mu} = -\frac{n}{\sigma^2}&lt;0\)</span> the optimum is indeed the maximum</p></li>
</ul>
</div>
<p>The constant term <span class="math inline">\(C\)</span> in the log-likelihood function collects all terms that do not depend on the parameter. After taking the first derivative with regard to the parameter this term disappears thus <strong><span class="math inline">\(C\)</span> is not relevant for finding the MLE</strong> of the parameter.
<strong>In the future we will often omit such constant terms from the log-likelihood function without further mention.</strong></p>
<div class="example">
<p><span id="exm:mlenormalmeanvar" class="example"><strong>Example 3.3  </strong></span>Normal distribution with mean and variance both unknown:</p>
<ul>
<li><span class="math inline">\(x \sim N(\mu,\sigma^2)\)</span> with <span class="math inline">\(\text{E}(x)=\mu\)</span> and <span class="math inline">\(\text{Var}(x) = \sigma^2\)</span></li>
<li>both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> need to be estimated.</li>
</ul>
<p>What’s the MLE of the parameter vector <span class="math inline">\(\boldsymbol \theta= (\mu,\sigma^2)^T\)</span>?</p>
<ul>
<li>the data <span class="math inline">\(x_1, \ldots, x_n \in [-\infty, \infty]\)</span> are real values.</li>
<li>the average <span class="math inline">\(\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i\)</span> is real as well.</li>
<li><p>the average of the squared data <span class="math inline">\(\overline{x^2} = \frac{1}{n} \sum_{i=1}^n x_i^2 \geq 0\)</span> is non-negative.</p></li>
<li>Density: <span class="math display">\[ f(x)=(2\pi\sigma^2)^{-\frac{1}{2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</span></li>
<li>Log-Density:
<span class="math display">\[\log f(x) =-\frac{1}{2} \log(2\pi\sigma^2) - \frac{(x-\mu)^2}{2\sigma^2}\]</span></li>
<li><p>Log-likelihood function:
<span class="math display">\[
\begin{split}
l_n(\boldsymbol \theta) &amp; = \sum_{i=1}^n \log f(x_i)\\
 &amp;= -\frac{n}{2}\log(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2  \underbrace{-\frac{n}{2} \log(2 \pi) }_{\text{constant not depending on }\mu \text{ or } \sigma^2}\\
 &amp;= -\frac{n}{2}\log(\sigma^2)-\frac{n}{2\sigma^2}  ( \overline{x^2} -2 \bar{x} \mu + \mu^2)  + C\\
\end{split}
\]</span>
Note how the log-likelihood function depends on the data only through <span class="math inline">\(\bar{x}\)</span>
and <span class="math inline">\(\overline{x^2}\)</span>!</p></li>
<li><p>Score function <span class="math inline">\(\boldsymbol S\)</span> (row vector!), gradient of <span class="math inline">\(l_n(\boldsymbol \theta)\)</span>:
<span class="math display">\[
\begin{split}
\boldsymbol S(\boldsymbol \theta) &amp;= \nabla l_n(\boldsymbol \theta) \\
&amp;=
\begin{pmatrix}
\frac{n}{\sigma^2} (\bar{x}-\mu) \\
-\frac{n}{2\sigma^2}+\frac{n}{2\sigma^4}   \left( \overline{x^2} - 2\bar{x} \mu +\mu^2 \right)  \\
\end{pmatrix}^T\\
\end{split}
\]</span></p>
<p>Note that to obtain the second component of the score function the partial derivative needs to be taken with regard to the variance parameter <span class="math inline">\(\sigma^2\)</span> — not with regard to <span class="math inline">\(\sigma\)</span>! Hint: replace <span class="math inline">\(\sigma^2 = v\)</span> in the log-likelihood function, then take the partial derivative with regard to <span class="math inline">\(v\)</span>, then backsubstitute <span class="math inline">\(v=\sigma^2\)</span> in the result.</p></li>
<li><p>Maximum likelihood estimate:
<span class="math display">\[
\boldsymbol S(\hat{\boldsymbol \theta}_{ML})=0 \Rightarrow 
\]</span>
<span class="math display">\[
\hat{\boldsymbol \theta}_{ML}=
\begin{pmatrix}
 \hat{\mu}_{ML}  \\
 \widehat{\sigma^2}_{ML} \\
\end{pmatrix}
 =
\begin{pmatrix}
\bar{x} \\
\overline{x^2} -\bar{x}^2\\
\end{pmatrix}
\]</span>
The ML estimate of the variance we can also write
<span class="math inline">\(\widehat{\sigma^2}_{ML} = \overline{x^2} -\bar{x}^2 =  \frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})^2\)</span>.</p></li>
<li><p>To confirm that we actually have maximum we need to verify that the eigenvalues
of the Hessian matrix are all negative. This is indeed the case, for
details see Example <a href="03-likelihood3.html#exm:obsfishernormalmeanvar">3.6</a>.</p></li>
</ul>
</div>
</div>
<div id="relationship-with-least-squares-estimation" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Relationship with least squares estimation</h3>
<p>In Example <a href="03-likelihood3.html#exm:mlenormalmean">3.2</a>
the form of the log-likelihood function
is a function of the sum of squared differences. Maximising <span class="math inline">\(l_n(\mu) =-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\)</span> is equivalent to <em>minimising</em> <span class="math inline">\(\sum_{i=1}^n(x_i-\mu)^2\)</span>. Hence, finding the mean by <strong>maximum likelihood assuming a normal model</strong> is <strong>equivalent to least-squares estimation</strong>!</p>
<p>Note that least-squares estimation has been in use at least since the early 1800s and thus predates maximum likelihood (1924). Due to its simplicity it is still very popular in particular in regression and the link with maximum likelihood and normality allows to understand why it usually works well!</p>
</div>
<div id="bias-and-maximum-likelihood" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Bias and maximum likelihood</h3>
<p>Example <a href="03-likelihood3.html#exm:mlenormalmeanvar">3.3</a> is interesting because it shows that maximum likelihood can result in both biased and as well unbiased estimators.</p>
<p>Recall that <span class="math inline">\(x \sim N(\mu, \sigma^2)\)</span>. As a result
<span class="math display">\[\hat{\mu}_{ML}=\bar{x} \sim N\left(\mu, \frac{\sigma^2}{n}\right)\]</span>
with <span class="math inline">\(\text{E}( \hat{\mu}_{ML} ) = \mu\)</span>
and
<span class="math display">\[\widehat{\sigma^2}_{ML} \sim \frac{\sigma^2}{n} \chi^2_{n-1}\]</span>
with <span class="math inline">\(\text{E}(\widehat{\sigma^2}_{ML}) = \frac{n-1}{n} \, \sigma^2\)</span>.</p>
<p>Therefore, the MLE of <span class="math inline">\(\mu\)</span> is unbiased as<br />
<span class="math display">\[
\text{Bias}(\hat{\mu}_{ML}) = \text{E}( \hat{\mu}_{ML} ) - \mu = 0
\]</span>
In contrast, however, the MLE of <span class="math inline">\(\sigma^2\)</span> is negatively biased because
<span class="math display">\[
\text{Bias}(\widehat{\sigma^2}_{ML}) = \text{E}( \widehat{\sigma^2}_{ML} ) - \sigma^2 = -\frac{1}{n} \, \sigma^2
\]</span></p>
<p>Thus, in the case of the variance parameter of the normal distribution the MLE is <em>not</em> recovering the well-known unbiased estimator of the variance<br />
<span class="math display">\[
\widehat{\sigma^2}_{UB} = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2 = \frac{n}{n-1} \widehat{\sigma^2}_{ML}
\]</span>
Conversely, the unbiased estimator is not a maximum likelihood estimate!</p>
<p>Therefore it is worth keeping in mind that maximum likelihood can result in biased estimates for finite <span class="math inline">\(n\)</span>.
For large <span class="math inline">\(n\)</span>, however, the bias disappears as MLEs are consistent.</p>
</div>
</div>
<div id="observed-fisher-information" class="section level2">
<h2><span class="header-section-number">3.3</span> Observed Fisher information</h2>
<div id="motivation-and-definition" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Motivation and definition</h3>
<p><img src="03-likelihood3_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>By inspection of some log-likelihood curves it is apparent that the log-likelihood function contains more information about the parameter <span class="math inline">\(\boldsymbol \theta\)</span> than just the maximum point <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span>.</p>
<p>In particular the <strong>curvature</strong> of the log-likelihood function at the MLE must be somehow related the accuracy of <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span>: if the likelihood surface is flat near the maximum
(low curvature) then if is more difficult to find the optimal parameter (also numerically!). Conversely, if the likelihood surface is peaked (strong curvature) then the maximum point is clearly defined.</p>
<p>The curvature is described by the second-order derivatives (Hessian matrix) of the log-likelihood function.</p>
<p>For univariate <span class="math inline">\(\theta\)</span> the Hessian is a scalar:
<span class="math display">\[\frac{d^2 l_n(\theta)}{d\theta^2}\]</span></p>
<p>For multivariate parameter vector <span class="math inline">\(\boldsymbol \theta\)</span> of dimension <span class="math inline">\(d\)</span> the Hessian is a matrix of size <span class="math inline">\(d \times d\)</span>:
<span class="math display">\[\nabla^T\nabla l_n(\boldsymbol \theta)\]</span></p>
<p>By construction the Hessian is negative definite at the MLE (i.e. its eigenvalues are all negative) to ensure the the function is concave at the MLE (i.e. peak shaped).</p>
<p>The <strong>observed Fisher information</strong> (matrix) is defined as the
negative curvature at the MLE <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span>:
<span class="math display">\[{\boldsymbol J_n}(\hat{\boldsymbol \theta}_{ML}) = -\nabla^T\nabla l_n(\hat{\boldsymbol \theta}_{ML})\]</span></p>
<p>Sometimes this is simply called the “observed information”.
To avoid confusion with the expected Fisher information introduced earlier<br />
<span class="math display">\[
\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta) = -\text{E}_{F_{\boldsymbol \theta}} \left( \nabla^T\nabla\log f(x|\boldsymbol \theta)\right)
\]</span>
it is necessary to always use the qualifier “observed” when referring to <span class="math inline">\({\boldsymbol J_n}(\hat{\boldsymbol \theta}_{ML})\)</span>.</p>
</div>
<div id="examples-of-observed-fisher-information" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Examples of observed Fisher information</h3>
<div class="example">
<p><span id="exm:obsfisherproportion" class="example"><strong>Example 3.4  </strong></span>Bernoulli model <span class="math inline">\(Ber(p)\)</span>:</p>
<p>We continue Example <a href="03-likelihood3.html#exm:mleproportion">3.1</a>. Recall that
<span class="math inline">\(\hat{p}_{ML} = \bar{x}=\frac{n_1}{n}\)</span> and the score function
<span class="math inline">\(S_n(p)=n \left( \frac{\bar{x} }{p} - \frac{1-\bar{x}}{1-p} \right)\)</span>. The negative second derivative of the log-likelihood function is
<span class="math display">\[-\frac{d S_n(p)}{dp}=n \left( \frac{ \bar{x} }{p^2} + \frac{1 - \bar{x} }{(1-p)^2} \right) \]</span>
The observed Fisher information is therefore
<span class="math display">\[
\begin{split}
J_n(\hat{p}_{ML}) &amp; = n \left(\frac{ \bar{x} }{\hat{p}_{ML}^2} + \frac{ 1 - \bar{x} }{  (1-\hat{p}_{ML})^2  } \right) \\
  &amp; = n \left(\frac{1}{\hat{p}_{ML}} + \frac{1}{1-\hat{p}_{ML}} \right) \\
  &amp;= \frac{n}{\hat{p}_{ML} (1-\hat{p}_{ML})} \\
\end{split}
\]</span></p>
<p>The inverse of the observed Fisher information is:
<span class="math display">\[J_n(\hat{p}_{ML})^{-1}=\frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n}\]</span></p>
<p>Compare this with <span class="math inline">\(\text{Var}\left(\frac{x}{n}\right) = \frac{p(1-p)}{n}\)</span> for
<span class="math inline">\(x \sim Binomial(n, p)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:obsfishernormalmean" class="example"><strong>Example 3.5  </strong></span>Normal distribution with unknown mean and known variance:</p>
<p>This is the continuation of Example <a href="03-likelihood3.html#exm:mlenormalmean">3.2</a>.
Recall the MLE for the mean
<span class="math inline">\(\hat{\mu}_{ML}=\frac{1}{n}\sum_{i=1}^n x_i=\bar{x}\)</span>
and the score function
<span class="math inline">\(\boldsymbol S_n(\mu) = \frac{n}{\sigma^2} (\bar{x} -\mu)\)</span>.
The negative second derivative of the score function is
<span class="math display">\[
-\frac{d S_n(\mu)}{d\mu}= \frac{n}{\sigma^2} 
\]</span>
The observed Fisher information at the MLE is therefore
<span class="math display">\[
J_n(\hat{\mu}_{ML}) = \frac{n}{\sigma^2} 
\]</span>
and the inverse of the observed Fisher information is
<span class="math display">\[
J_n(\hat{\nu}_{ML})^{-1} = \frac{\sigma^2}{n}
\]</span></p>
<p>For <span class="math inline">\(x_i \sim N(\mu, \sigma^2)\)</span> we have <span class="math inline">\(\text{Var}(x_i) = \sigma^2\)</span>
and hence <span class="math inline">\(\text{Var}(\bar{x}) = \frac{\sigma^2}{n}\)</span>,
which is equal to the inverse observed Fisher information.</p>
</div>
<div class="example">
<p><span id="exm:obsfishernormalmeanvar" class="example"><strong>Example 3.6  </strong></span>Normal distribution with mean and variance parameter:</p>
<p>This is the continuation of Example <a href="03-likelihood3.html#exm:mlenormalmeanvar">3.3</a>.
Recall the MLE for the mean and variance:
<span class="math display">\[\hat{\mu}_{ML}=\frac{1}{n}\sum_{i=1}^n x_i=\bar{x}\]</span>
<span class="math display">\[\widehat{\sigma^2}_{ML} = \frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2 =  \overline{x^2} - \bar{x}^2\]</span>
with score function
<span class="math display">\[\boldsymbol S_n(\mu,\sigma^2)=\nabla l_n(\mu, \sigma^2) = 
\begin{pmatrix}
\frac{n}{\sigma^2}   (\bar{x}-\mu) \\
-\frac{n}{2\sigma^2}+\frac{n}{2\sigma^4} \left(\overline{x^2} - 2 \mu \bar{x} + \mu^2\right) \\
\end{pmatrix}^T
\]</span>
The Hessian matrix of the log-likelihood function is
<span class="math display">\[\nabla^T\nabla l_n(\mu,\sigma^2) =
 \begin{pmatrix}
    - \frac{n}{\sigma^2}&amp;  -\frac{n}{\sigma^4} (\bar{x} -\mu)\\
    - \frac{n}{\sigma^4} (\bar{x} -\mu) &amp; \frac{n}{2\sigma^4}-\frac{n}{\sigma^6} \left(\overline{x^2} - 2 \mu \bar{x} + \mu^2\right) \\
    \end{pmatrix}
\]</span>
The negative Hessian at the MLE, i.e. at <span class="math inline">\(\hat{\mu}_{ML} = \bar{x}\)</span>
and <span class="math inline">\(\widehat{\sigma^2}_{ML} = \overline{x^2} -\bar{x}^2\)</span>
yields the <strong>observed Fisher information matrix</strong>:
<span class="math display">\[
\boldsymbol J_n(\hat{\mu}_{ML},\widehat{\sigma^2}_{ML}) = \begin{pmatrix}
    \frac{n}{\widehat{\sigma^2}_{ML}}&amp;0 \\
    0 &amp; \frac{n}{2(\widehat{\sigma^2}_{ML})^2}
    \end{pmatrix}
\]</span>
Note that the observed Fisher information matrix is diagonal
with positive entries. Therefore its
eigenvalues are all positive as required for a maximum, because for a diagonal matrix the eigenvalues are simply the
the entries on the diagonal.</p>
<p>The inverse of the observed Fisher information matrix is
<span class="math display">\[
\boldsymbol J_n(\hat{\mu}_{ML},\widehat{\sigma^2}_{ML})^{-1} = \begin{pmatrix}
    \frac{\widehat{\sigma^2}_{ML}}{n}&amp; 0\\
    0 &amp; \frac{2(\widehat{\sigma^2}_{ML})^2}{n}
    \end{pmatrix}
\]</span></p>
</div>
<p>Recall that <span class="math inline">\(x \sim N(\mu, \sigma^2)\)</span> and therefore
<span class="math display">\[\hat{\mu}_{ML}=\bar{x} \sim N\left(\mu, \frac{\sigma^2}{n}\right)\]</span>
Hence <span class="math inline">\(\text{Var}(\hat{\mu}_{ML}) = \frac{\sigma^2}{n}\)</span>. If you compare this
with the
first diagonal entry of the inverse observed Fisher information matrix you see that this is essentially the same expression (apart from the “hat”).</p>
<p>The empirical variance <span class="math inline">\(\widehat{\sigma^2}_{ML}\)</span> follows a scaled
chi-squared distribution
<span class="math display">\[\widehat{\sigma^2}_{ML} \sim \frac{\sigma^2}{n} \chi^2_{n-1}\]</span> with
variance
<span class="math inline">\(\text{Var}(\widehat{\sigma^2}_{ML}) = \frac{n-1}{n} \, \frac{2 \sigma ^4}{n}\)</span>. For large <span class="math inline">\(n\)</span> this becomes <span class="math inline">\(\text{Var}(\widehat{\sigma^2}_{ML})\overset{a}{=} \frac{2 \sigma ^4}{n}\)</span> which is essentially (apart from the “hat”) the second diagonal entry of the inverse observed Fisher information matrix.</p>
</div>
<div id="relationship-between-observed-and-expected-fisher-information" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Relationship between observed and expected Fisher information</h3>
<p>The observed Fisher information <span class="math inline">\(\boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})\)</span> and the expected Fisher information
<span class="math inline">\(\boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\)</span> are related but also two clearly different entities:</p>
<ul>
<li><p>Both types of Fisher information are based on computing the second order derivative
(Hessian matrix), thus are based on the curvature of a function.</p></li>
<li><p>The observed Fisher information is computed from the log-likelihood function.
Therefore it takes the observed data into account. It explicitly depends on the sample size <span class="math inline">\(n\)</span>. It contains estimates of the parameters but not the parameters themselves. While the curvature of the log-likelihood function may be computed for any point the the observed Fisher information specifically refers to the MLE <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span>. It is linked to the (asymptotic) variance of the MLE as we have seen in the examples and will discuss in more detail later.</p></li>
<li><p>In contrast, the expected Fisher information is derived directly from the log-density. It does not depend on the observed data, and thus does not have dependency on sample size. It can be computed for any value of the parameters. It describes the geometry of the space of the models, and is the local approximation of relative entropy.</p></li>
<li><p>Asympotically, for large sample size <span class="math inline">\(n\)</span> the MLE converges to <span class="math inline">\(\hat{\boldsymbol \theta}_{ML} \rightarrow \boldsymbol \theta_0\)</span>.
It follows from the construction of
the observed Fisher information and the law of large numbers that correspondingly <span class="math inline">\(\boldsymbol J_n(\hat{\boldsymbol \theta}_{ML}) \rightarrow n \boldsymbol I^{\text{Fisher}}( \boldsymbol \theta_0 )\)</span>.</p></li>
<li><p>In a very important class of models, namely <strong>in the exponential family</strong>, we find that
<span class="math inline">\(\boldsymbol J_n(\hat{\boldsymbol \theta}_{ML}) = n \boldsymbol I^{\text{Fisher}}( \hat{\boldsymbol \theta}_{ML} )\)</span> also for finite sample size <span class="math inline">\(n\)</span>. This is in fact the case in all the examples discussed above (e.g. see
Examples <a href="02-likelihood2.html#exm:expectedfisherbernoulli">2.10</a> and <a href="03-likelihood3.html#exm:obsfisherproportion">3.4</a>
for the Bernoulli and
Examples <a href="02-likelihood2.html#exm:expectedfishernormal">2.12</a> and <a href="03-likelihood3.html#exm:obsfishernormalmeanvar">3.6</a>
for the normal distribution).</p></li>
<li><p>However, this is an exception. In a general model <span class="math inline">\(\boldsymbol J_n(\hat{\boldsymbol \theta}_{ML}) \neq n \boldsymbol I^{\text{Fisher}}( \hat{\boldsymbol \theta}_{ML} )\)</span>
for finite sample size <span class="math inline">\(n\)</span>. An example is provided by the Cauchy distribution with median parameter <span class="math inline">\(\theta\)</span>. It is not part of the exponential family and has expected Fisher information <span class="math inline">\(I^{\text{Fisher}}(\theta )=\frac{1}{2}\)</span> regardless of the choice
the median parameter whereas the observed Fisher information <span class="math inline">\(J_n(\hat{\theta}_{ML})\)</span> depends on the
MLE <span class="math inline">\(\hat{\theta}_{ML}\)</span> of the median parameter and is not simply <span class="math inline">\(\frac{n}{2}\)</span>.</p></li>
</ul>

<p></p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="02-likelihood2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="04-likelihood4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math20802-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
