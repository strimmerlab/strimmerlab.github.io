<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>13 Optimality properties and summary | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.31 with bs4_book()">
<meta property="og:title" content="13 Optimality properties and summary | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="13 Optimality properties and summary | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="13.1 Bayesian statistics in a nutshell Bayesian statistics explicitly models the uncertainty about the parameters of interests by probability In the light of new evidence (observed data) the...">
<meta property="og:description" content="13.1 Bayesian statistics in a nutshell Bayesian statistics explicitly models the uncertainty about the parameters of interests by probability In the light of new evidence (observed data) the...">
<meta name="twitter:description" content="13.1 Bayesian statistics in a nutshell Bayesian statistics explicitly models the uncertainty about the parameters of interests by probability In the light of new evidence (observed data) the...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="conditioning-and-bayes-rule.html"><span class="header-section-number">7</span> Conditioning and Bayes rule</a></li>
<li><a class="" href="models-with-latent-variables-and-missing-data.html"><span class="header-section-number">8</span> Models with latent variables and missing data</a></li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li><a class="" href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li><a class="" href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li><a class="active" href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">14</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">15</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">16</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">17</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">18</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="optimality-properties-and-summary" class="section level1" number="13">
<h1>
<span class="header-section-number">13</span> Optimality properties and summary<a class="anchor" aria-label="anchor" href="#optimality-properties-and-summary"><i class="fas fa-link"></i></a>
</h1>
<div id="bayesian-statistics-in-a-nutshell" class="section level2" number="13.1">
<h2>
<span class="header-section-number">13.1</span> Bayesian statistics in a nutshell<a class="anchor" aria-label="anchor" href="#bayesian-statistics-in-a-nutshell"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Bayesian statistics explicitly models the uncertainty about the parameters
of interests by probability</li>
<li>In the light of new evidence (observed data) the uncertainty is updated, i.e. the prior distribution is combined with the likelihood to form the posterior distribution</li>
</ul>
<p>Example: Beta-Binomial model</p>
<ul>
<li>Binomial likelihood</li>
<li>
<span class="math inline">\(n\)</span> observations: <span class="math inline">\(x\)</span> “heads”, <span class="math inline">\(n-x\)</span> “tails”</li>
<li>Frequency <span class="math inline">\(\hat\theta_{ML} = \frac{x}{n}\)</span>
</li>
<li>Beta prior <span class="math inline">\(\theta \sim \text{Beta}(\alpha_0, \beta_0)\)</span> with mean <span class="math inline">\(\theta_0=\frac{\alpha_0}{m}\)</span> and <span class="math inline">\(m=\alpha_0+\beta_0\)</span>
</li>
<li>Beta posterior <span class="math inline">\(\theta | x,n \sim \text{Beta}(\alpha_1, \beta_1)\)</span> with mean <span class="math inline">\(\theta_1=\frac{\alpha_1}{\alpha_1+\beta_1}\)</span>
and <span class="math inline">\(\alpha_1 = \alpha_0+x\)</span> and <span class="math inline">\(\beta_1=\beta_0+n-x\)</span>
</li>
<li>Update of prior mean to posterior mean by shrinkage of MLE:
<span class="math display">\[\theta_1 = \lambda \theta_0 + (1-\lambda) \hat\theta_{ML}\]</span> with shrinkage intensity <span class="math inline">\(\lambda=\frac{m}{n+m}\)</span>
</li>
<li>
<span class="math inline">\(m\)</span> can be interpreted as prior sample size</li>
</ul>
<div id="remarks" class="section level3" number="13.1.1">
<h3>
<span class="header-section-number">13.1.1</span> Remarks<a class="anchor" aria-label="anchor" href="#remarks"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>If posterior in same family as prior <span class="math inline">\(\rightarrow\)</span> conjugate prior.</p></li>
<li><p>In an exponential family the Bayesian update of the mean is always expressible
as linear shrinkage of the MLE.</p></li>
<li><p>For sample size <span class="math inline">\(n \rightarrow \infty\)</span> then <span class="math inline">\(\lambda \rightarrow 0\)</span> and <span class="math inline">\(\theta_1 \rightarrow \hat\theta_{ML}\)</span> (for large samples posterior mean = maximum likelihood estimator).</p></li>
<li><p>For <span class="math inline">\(n \rightarrow 0\)</span> then <span class="math inline">\(\lambda \rightarrow 1\)</span> and <span class="math inline">\(\theta_1 \rightarrow \hat\theta_0\)</span> (if no data is available fall back to prior).</p></li>
<li><p>Note that the Bayesian estimator is biased for finite <span class="math inline">\(n\)</span> by construction (but asymptotically unbiased like the MLE).</p></li>
</ul>
</div>
<div id="advantages" class="section level3" number="13.1.2">
<h3>
<span class="header-section-number">13.1.2</span> Advantages<a class="anchor" aria-label="anchor" href="#advantages"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>Adding prior information has regularisation properties. This is very important in more complex models with many parameters, e.g., in estimation of a covariance matrix (to avoid singularity).</p></li>
<li><p>Improves small-sample accuracy (e.g. MSE)</p></li>
<li><p>Bayesian estimators tend to perform better than MLEs is not surprising - they use
the obseved data plus extra information!</p></li>
<li><p>Bayesian credible intervals are conceptually much more simple than frequentist
confidence intervals.</p></li>
</ul>
</div>
<div id="frequentist-properties-of-bayesian-estimators" class="section level3" number="13.1.3">
<h3>
<span class="header-section-number">13.1.3</span> Frequentist properties of Bayesian estimators<a class="anchor" aria-label="anchor" href="#frequentist-properties-of-bayesian-estimators"><i class="fas fa-link"></i></a>
</h3>
<p>A Bayesian point estimator (e.g. the posterior mean) can also be assessed by its frequentist properties.</p>
<ul>
<li>First, by construction due to introducing a prior the Bayesian estimator will be biased for
finite <span class="math inline">\(n\)</span> even if the MLE is unbiased.</li>
<li>Second, intriguingly it turns out that the sampling variance of the Bayes point estimator (not to be confused with the posterior variance!) can be smaller than the variance of the MLE. This depends on the choice of the shrinkage parameter <span class="math inline">\(\lambda\)</span> that also determines the posterior variance.</li>
</ul>
<p>As a result, Bayesian estimators may have smaller MSE (=squared bias + variance) than the ML estimator for finite <span class="math inline">\(n\)</span>.</p>
<p>In statistical decision theory this is called the theorem of <strong>admissibility of Bayes rules</strong>.
It states that under mild conditions every admissible estimation rule (i.e. one that dominates all
other estimators with regard to some expected loss, such as the MSE) is in fact a Bayes estimator with some prior.</p>
<p>Unfortunately, this theorem does not tell which prior is needed to achive optimality, however an optimal estimator can often be found by tuning the hyper-parameter <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div id="specifying-the-prior-problem-or-advantage" class="section level3" number="13.1.4">
<h3>
<span class="header-section-number">13.1.4</span> Specifying the prior — problem or advantage?<a class="anchor" aria-label="anchor" href="#specifying-the-prior-problem-or-advantage"><i class="fas fa-link"></i></a>
</h3>
<p>In Bayesian statistics the analysist needs to be very explicit about the modelling assumptions:</p>
<p><strong>Model = data generating process (likelihood) + prior uncertainty (prior distribution)</strong></p>
<p>Note that alternative statistical methods can often be interpreted as Bayesian methods assuming a specific <em>implicit</em> prior!</p>
<p>For example, likelihood estimation for the binomial model is equivalent to Bayes estimation using the Beta-Binomial model with a <span class="math inline">\(\text{Beta}(0,0)\)</span> prior (=Haldane prior).<br>
However, when choosing a prior explicitly for this model, interestingly most analysts would rather use a
flat prior <span class="math inline">\(\text{Beta}(1,1)\)</span> (=Laplace prior) with implicit sample size <span class="math inline">\(m=2\)</span> or a transformation-invariant prior <span class="math inline">\(\text{Beta}(1/2, 1/2)\)</span> (=Jeffreys prior) with implicit sample size <span class="math inline">\(m=1\)</span> than the Haldane prior!</p>
<p><span class="math inline">\(\rightarrow\)</span> be aware about the implicit priors!!</p>
<p>Better to acknowledge that a prior is being used (even if implicit!)<br>
Being specific about all your assumptions is enforced by the Bayesian approach.</p>
<p>Specifying a prior is thus best understood as an intrinsic part of model specification.
It helps to improve inference and it may only be ignored if there is lots of data.</p>
</div>
</div>
<div id="optimality-of-bayesian-inference" class="section level2" number="13.2">
<h2>
<span class="header-section-number">13.2</span> Optimality of Bayesian inference<a class="anchor" aria-label="anchor" href="#optimality-of-bayesian-inference"><i class="fas fa-link"></i></a>
</h2>
<p>The optimality of Bayesian model making use of full model specification (likelihood plus prior) can be shown from a number of different perspectives. Correspondingly,
there are many theorems that prove (or at least indicate) this optimality:</p>
<ol style="list-style-type: decimal">
<li><p><a href="https://en.wikipedia.org/wiki/Cox%27s_theorem">Richard Cox’s theorem</a>: generalising classical logic invariably leads to Bayesian inference.</p></li>
<li><p><a href="https://en.wikipedia.org/wiki/De_Finetti%27s_theorem">de Finetti’s representation theorem</a>: joint distribution of exchangeable observations can always be expressed as weighted mixture over a prior distribution for the parameter of the model. This implies the existence of the prior distribution and the requirement of a Bayesian approach.</p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Admissible_decision_rule">Frequentist decision theory</a>: all admissible decision rules are Bayes rules!</p></li>
<li><p>Entropy perspective: The posterior density (a function!) is obtained as a result of optimising an entropy criterion. Bayesian updating may thus be viewed as a <em>variational optimisation problem</em>. Specifically, Bayes theorem is the minimal update when new information arrives in form of observations
(see below).</p></li>
</ol>
<p>Remark: there exist a number of further (often somewhat esoteric) suggestions for propagating uncertainty such as “fuzzy logic”, imprecise probabilities, etc. These contradict Bayesian learning and are thus in direct violation of the above theorems.</p>
</div>
<div id="connection-with-entropy-learning" class="section level2" number="13.3">
<h2>
<span class="header-section-number">13.3</span> Connection with entropy learning<a class="anchor" aria-label="anchor" href="#connection-with-entropy-learning"><i class="fas fa-link"></i></a>
</h2>
<p>The <em>Bayesian update rule</em> is a very general form of learning when the <em>new information arrives in the form of data</em>. But actually there is an even more general principle of which the Bayesian update rule is just a special case: the <strong>principle of minimal information update</strong> (e.g. Jaynes 1959, 2003) or <strong>principle of minimum information discrimination (MDI) (Kullback 1959)</strong>.</p>
<p>It can be summarised as follows: <strong>Change your beliefs only as much as necessary to be coherent with new evidence!</strong></p>
<p>Under this principle of “inertia of beliefs” when new information arrives the uncertainty about a parameter is only minimally adjusted, only as much as needed to account for the new information.
To implement this principle KL divergence is a natural measure to quantify the
change of the underlying beliefs. This is known as <strong>entropy learning</strong>.</p>
<p>The Bayes rule emerges a special case of entropy learning:</p>
<ul>
<li>The KL divergence between the joint posterior <span class="math inline">\(Q_{x,\boldsymbol \theta}\)</span> and joint prior distribution <span class="math inline">\(P_{x,\boldsymbol \theta}\)</span> is computed, with the posterior
distribution <span class="math inline">\(Q_{\boldsymbol \theta|x}\)</span> as free parameter.</li>
<li>The conditional distribution <span class="math inline">\(Q_{\boldsymbol \theta|x}\)</span> is found by minimising the KL divergence <span class="math inline">\(D_{\text{KL}}(Q_{x,\boldsymbol \theta}, P_{x,\boldsymbol \theta})\)</span>.</li>
<li>The optimal solution to this <strong>variational optimisation problem</strong> is given by Bayes’ rule!</li>
</ul>
<p>This application of the KL divergence is an example of <strong>reverse KL optimisation</strong> (aka <span class="math inline">\(I\)</span>-projection, see Part I of the notes). Intringuingly, this explains the zero forcing property of Bayes’ rule (because that this is a general property of an <span class="math inline">\(I\)</span>-projection).</p>
<p>Applying entropy learning therefore includes Bayesian learning as special case:</p>
<ol style="list-style-type: decimal">
<li>If information arrives in form of data <span class="math inline">\(\rightarrow\)</span> update prior by Bayes’ theorem (Bayesian learning).</li>
</ol>
<p>Interestingly, entropy learning will lead to other update rules for other types of information:</p>
<ol start="2" style="list-style-type: decimal">
<li><p>If information arrives in the form of another distribution <span class="math inline">\(\rightarrow\)</span> update using R. Jeffrey’s rule of conditioning (1965).</p></li>
<li><p>If the information is presented in the form of constraints <span class="math inline">\(\rightarrow\)</span> Kullback’s principle of minimum MDI (1959), E. T. Jaynes maximum entropy (MaxEnt) principle (1957).</p></li>
</ol>
<p>This shows (again) how fundamentally important KL divergence is in statistics. It not only leads to likelihood inference (via forward KL) but also to Bayesian learning, as well as to other forms of information updating (via reverse KL).</p>
<p>Furthermore, in Bayesian statistics relative entropy is useful to choose priors (e.g. reference priors) and it also helps in (Bayesian) experimental design to quantify the information provided by an experiment.</p>
</div>
<div id="conclusion" class="section level2" number="13.4">
<h2>
<span class="header-section-number">13.4</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"><i class="fas fa-link"></i></a>
</h2>
<p>Bayesian statistics offers a coherent framework for statistical learning from data, with methods for</p>
<ul>
<li>estimation</li>
<li>testing</li>
<li>model building</li>
</ul>
<p>There are a number of theorems that show that “optimal” estimators (defined in various ways) are all Bayesian.</p>
<p>It is conceptually very simple — but can be computationally very involved!</p>
<p>It provides a coherent generalisation of classical TRUE/FALSE logic (and therefore does not suffer from some of the inconsistencies prevalent in frequentist statistics).</p>
<p>Bayesian statistics is a non-asymptotic theory, it works for any sample size.
Asympotically (large <span class="math inline">\(n\)</span>) it is consistent and converges to the true model (like ML!).
But Bayesian reasoning can also be applied to events that take place only once — no assumption of hypothetical infinitely many repetitions as in frequentist statistics is needed.</p>
<p>Moreover, many classical (frequentist) procedures may be viewed as <em>approximations</em> to Bayesian methods and estimators, so using classical approaches in the correct application domain is perfectly in line with the Bayesian framework.</p>
<p>Bayesian estimation and inference also automatically regularises (via the prior) which is important for complex models and when there is the problem of overfitting.</p>

</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></div>
<div class="next"><a href="overview-over-regression-modelling.html"><span class="header-section-number">14</span> Overview over regression modelling</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#optimality-properties-and-summary"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li>
<a class="nav-link" href="#bayesian-statistics-in-a-nutshell"><span class="header-section-number">13.1</span> Bayesian statistics in a nutshell</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#remarks"><span class="header-section-number">13.1.1</span> Remarks</a></li>
<li><a class="nav-link" href="#advantages"><span class="header-section-number">13.1.2</span> Advantages</a></li>
<li><a class="nav-link" href="#frequentist-properties-of-bayesian-estimators"><span class="header-section-number">13.1.3</span> Frequentist properties of Bayesian estimators</a></li>
<li><a class="nav-link" href="#specifying-the-prior-problem-or-advantage"><span class="header-section-number">13.1.4</span> Specifying the prior — problem or advantage?</a></li>
</ul>
</li>
<li><a class="nav-link" href="#optimality-of-bayesian-inference"><span class="header-section-number">13.2</span> Optimality of Bayesian inference</a></li>
<li><a class="nav-link" href="#connection-with-entropy-learning"><span class="header-section-number">13.3</span> Connection with entropy learning</a></li>
<li><a class="nav-link" href="#conclusion"><span class="header-section-number">13.4</span> Conclusion</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 20 January 2023.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
