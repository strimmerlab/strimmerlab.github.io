<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>14 Optimality properties and summary | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="14 Optimality properties and summary | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="14 Optimality properties and summary | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.13/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><meta name="description" content="14.1 Bayesian statistics in a nutshell Bayesian statistics explicitly models the uncertainty about the parameters of interests by probability In the light of new evidence (observed data) the...">
<meta property="og:description" content="14.1 Bayesian statistics in a nutshell Bayesian statistics explicitly models the uncertainty about the parameters of interests by probability In the light of new evidence (observed data) the...">
<meta name="twitter:description" content="14.1 Bayesian statistics in a nutshell Bayesian statistics explicitly models the uncertainty about the parameters of interests by probability In the light of new evidence (observed data) the...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="active" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="optimality-properties-and-summary" class="section level1" number="14">
<h1>
<span class="header-section-number">14</span> Optimality properties and summary<a class="anchor" aria-label="anchor" href="#optimality-properties-and-summary"><i class="fas fa-link"></i></a>
</h1>
<div id="bayesian-statistics-in-a-nutshell" class="section level2" number="14.1">
<h2>
<span class="header-section-number">14.1</span> Bayesian statistics in a nutshell<a class="anchor" aria-label="anchor" href="#bayesian-statistics-in-a-nutshell"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Bayesian statistics explicitly models the uncertainty about the parameters
of interests by probability</li>
<li>In the light of new evidence (observed data) the uncertainty is updated, i.e. the prior distribution is combined with the likelihood to form the posterior distribution</li>
</ul>
<p>Example: Beta-Binomial model</p>
<ul>
<li>Binomial likelihood</li>
<li>
<span class="math inline">\(n\)</span> observations: <span class="math inline">\(x\)</span> “heads”, <span class="math inline">\(n-x\)</span> “tails”</li>
<li>Frequency <span class="math inline">\(\hat\theta_{ML} = \frac{x}{n}\)</span>
</li>
<li>Beta prior <span class="math inline">\(\theta \sim \text{Beta}(\alpha_0, \beta_0)\)</span> with mean <span class="math inline">\(\theta_0=\frac{\alpha_0}{m}\)</span> and <span class="math inline">\(m=\alpha_0+\beta_0\)</span>
</li>
<li>Beta posterior <span class="math inline">\(\theta | x,n \sim \text{Beta}(\alpha_1, \beta_1)\)</span> with mean <span class="math inline">\(\theta_1=\frac{\alpha_1}{\alpha_1+\beta_1}\)</span>
and <span class="math inline">\(\alpha_1 = \alpha_0+x\)</span> and <span class="math inline">\(\beta_1=\beta_0+n-x\)</span>
</li>
<li>Update of prior mean to posterior mean by shrinkage of MLE:
<span class="math display">\[\theta_1 = \lambda \theta_0 + (1-\lambda) \hat\theta_{ML}\]</span> with shrinkage intensity <span class="math inline">\(\lambda=\frac{m}{n+m}\)</span>
</li>
<li>
<span class="math inline">\(m\)</span> can be interpreted as prior sample size</li>
</ul>
<div id="remarks" class="section level3" number="14.1.1">
<h3>
<span class="header-section-number">14.1.1</span> Remarks<a class="anchor" aria-label="anchor" href="#remarks"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>If posterior in same family as prior <span class="math inline">\(\rightarrow\)</span> conjugate prior</p></li>
<li><p>In an exponential family the Bayesian update of the mean is always expressible
as linear shrinkage of the MLE</p></li>
<li><p>For sample size <span class="math inline">\(n \rightarrow \infty\)</span> then <span class="math inline">\(\lambda \rightarrow 0\)</span> and <span class="math inline">\(\theta_1 \rightarrow \hat\theta_{ML}\)</span> (for large samples posterior mean = maximum likelihood estimator)</p></li>
<li><p>For <span class="math inline">\(n \rightarrow 0\)</span> then <span class="math inline">\(\lambda \rightarrow 1\)</span> and <span class="math inline">\(\theta_1 \rightarrow \hat\theta_0\)</span> (if no data is available fall back to prior)</p></li>
<li><p>Note that the Bayesian estimator is biased for finite <span class="math inline">\(n\)</span> by construction (but asymptotically unbiased like the MLE).</p></li>
</ul>
</div>
<div id="advantages" class="section level3" number="14.1.2">
<h3>
<span class="header-section-number">14.1.2</span> Advantages<a class="anchor" aria-label="anchor" href="#advantages"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>adding prior information has regularisation properties. This is very important in more complex models with many parameters, e.g., in estimation of a covariance matrix (to avoid singularity).</p></li>
<li><p>improves small-sample accuracy (e.g. MSE)</p></li>
<li><p>that Bayesian estimators tend to be better than MLE is not surprising - they use
the data plus extra information!</p></li>
<li><p>Bayesian credible intervals are conceptually much more simple than frequentist
confidence intervals</p></li>
</ul>
</div>
</div>
<div id="frequentist-properties-of-bayesian-estimators" class="section level2" number="14.2">
<h2>
<span class="header-section-number">14.2</span> Frequentist properties of Bayesian estimators<a class="anchor" aria-label="anchor" href="#frequentist-properties-of-bayesian-estimators"><i class="fas fa-link"></i></a>
</h2>
<p>A Bayesian point estimator (e.g. the posterior mean) can also be assessed by its frequentist properties.</p>
<ul>
<li>First, we know that, by construction, the Bayesian estimator <span class="math inline">\(\hat{p}_{\text{Bayes}}\)</span> will be biased for
finite <span class="math inline">\(n\)</span> even if the MLE is unbiased (with the bias being the posterior mean in this case).</li>
<li>Second, intriguingly it turns out that the sampling variance of the Bayes point estimator (not to be confused with the posterior variance!) can be smaller than the variance of the MLE. This depends on the choice of the shrinkage parameter <span class="math inline">\(\lambda\)</span> that also determines the posterior variance.</li>
</ul>
<p>As a result, Bayesian estimators may have smaller MSE (=squared bias + variance) than the ML estimator for finite <span class="math inline">\(n\)</span>.</p>
<p>In statistical decision theory this is called the theorem of <strong>admissibility of Bayes rules</strong>.
It states that under mild conditions every admissible estimation rule (i.e. one that dominates all
other estimators with regard to some expected loss, such as the MSE) is in fact a Bayes estimator with some prior.</p>
<p>Unfortunately, this theorem does not tell which prior is needed to achive optimality, however an optimal estimator with minimum MSE can often be found by tuning <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div id="specifying-the-prior-problem-or-advantage" class="section level2" number="14.3">
<h2>
<span class="header-section-number">14.3</span> Specifying the prior — problem or advantage?<a class="anchor" aria-label="anchor" href="#specifying-the-prior-problem-or-advantage"><i class="fas fa-link"></i></a>
</h2>
<p>In Bayesian statistics the analysist needs to be very explicit about the modelling assumptions:</p>
<p>Model = data generating process (likelihood) + prior uncertainty (prior distribution)</p>
<p>Note that alternative statistical methods can often be interpreted as Bayesian methods assuming a specific <em>implicit</em> prior!</p>
<p>For example, likelihood estimation for the Binomial model is equivalent to Bayes estimation using the Beta-Binomial model with a <span class="math inline">\(\text{Beta}(0,0)\)</span> prior (=Haldane prior).<br>
However, when choosing a prior explicitly for this model, interestingly most analysts would rather use a
flat prior <span class="math inline">\(\text{Beta}(1,1)\)</span> (=Laplace prior) with implicit sample size <span class="math inline">\(m=2\)</span> or a transformation-invariant prior <span class="math inline">\(\text{Beta}(1/2, 1/2)\)</span> (=Jeffreys prior) with implicit sample size <span class="math inline">\(m=1\)</span> than the Haldane prior!</p>
<p><span class="math inline">\(\rightarrow\)</span> be aware about the implicit priors!!</p>
<p>Better to acknowledge that a prior is being used (even if implicit!)</p>
<p>Writing down all your assumptions is enforced by the Bayesian approach.</p>
<p>Specifying a prior is thus best understood as an intrinsic part of model specification.
It helps to improve inference and it may only be ignored if there is lots of data.</p>
</div>
<div id="choosing-a-prior" class="section level2" number="14.4">
<h2>
<span class="header-section-number">14.4</span> Choosing a prior<a class="anchor" aria-label="anchor" href="#choosing-a-prior"><i class="fas fa-link"></i></a>
</h2>
<p>It is <strong>essential in a Bayesian analysis to specify your prior
uncertainty about the model parameters</strong>. Note that this is simply <strong>part of the modelling process</strong>!</p>
<p>Typically, the location of the prior determines the amount of bias, and the precision (inverse variance)
of the prior is proportional to the implied sample size of the prior.</p>
<p>As we have seen before for large <span class="math inline">\(n\)</span> the Bayesian estimate converges to the ML estimate, so for large <span class="math inline">\(n\)</span> you may ignore specifying a prior.</p>
<p>However, for small <span class="math inline">\(n\)</span> it is essential that a prior is specified. In non-Bayesian approaches (if interpreted from Bayesian perspective) this prior is still there but it is implicit (e.g. uniform prior for likelihood estimation).</p>
<div id="some-guidelines" class="section level3" number="14.4.1">
<h3>
<span class="header-section-number">14.4.1</span> Some guidelines<a class="anchor" aria-label="anchor" href="#some-guidelines"><i class="fas fa-link"></i></a>
</h3>
<p>So the questions remains what are good ways to choose a prior? Two useful ways (among many others) are:</p>
<ol style="list-style-type: decimal">
<li><p>Use a weakly informative prior (cf. Gelman). This means that you have a vague idea about the suitable values of the
parameter of interest, and you use a corresponding prior (with moderate variance) to model the uncertainty.
This acknowledges that there are no uninformative priors and aims to ensure that the prior will not dominate
the likelihood.</p></li>
<li><p>Empirical Bayes methods can often be used to determine one or all of the hyperparameters (i.e. the parameters in the prior). There are several ways to do this, one of them is to tune the shrinkage parameter <span class="math inline">\(\lambda\)</span> to achieve minimum MSE. We discuss this further below.</p></li>
</ol>
<p>In contrast, there also exists many proposals advocating to select so-called “uninformative priors”.
However, as it is easly shown, there are no true unformative priors, since a prior that looks uninformative (i.e. “flat”) in one coordinate system can be informative in another — this is a simple consequence of the rule for transformation of
probability densities. Furthermore, often these priors are improper, i.e. are not actually probability distributions.
For this (and many other reasons) the search for “uniformative” priors is not just futile but in fact also undesirable (e.g. the prior typically also needs to act as regulariser)!</p>
<p>Instead, <strong>specifying the prior needs to be viewed as part of the modelling process, with specification of the prior
as integral as the specification of the likelihood.</strong></p>
</div>
<div id="jeffreys-prior" class="section level3" number="14.4.2">
<h3>
<span class="header-section-number">14.4.2</span> Jeffreys prior<a class="anchor" aria-label="anchor" href="#jeffreys-prior"><i class="fas fa-link"></i></a>
</h3>
<p>In order to complement the discussion on non-informative priors we now look (briefly) at the proposal by Jeffreys (1946).</p>
<p>Specifically, this prior is constructed from the expected Fisher observation using the log-likelihood function and thus promises automatic construction of objective uninformative priors:
<span class="math display">\[
p(\boldsymbol \theta) \propto \sqrt{\det \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)}
\]</span></p>
<p>The reasoning underlying this prior is <strong>invariance against transformation of the coordinate system of the parameters</strong>.</p>
<p>For the Beta-Binomial model the Jeffreys prior corresponds to <span class="math inline">\(\text{Beta}(\frac{1}{2}, \frac{1}{2})\)</span>.</p>
<p>For the Normal-Normal model it corresponds to the flat improper prior <span class="math inline">\(p(\mu) =1\)</span>.</p>
<p>For the Inverse-Gamma-Norma model the Jeffreys prior is the improper prior <span class="math inline">\(p(\sigma^2) = \frac{1}{\sigma^2}\)</span>.</p>
<p>This already illustrates the main problem with this type of prior – namely that it often is an improper prior.</p>
<p>Another issue is that Jeffreys priors are usually not conjugate which complicates the update from the prior to the posterior.</p>
<p>Furthermore, Jeffreys priors do not usually lead to sensible priors if there are multiple parameters.</p>
</div>
<div id="reference-priors" class="section level3" number="14.4.3">
<h3>
<span class="header-section-number">14.4.3</span> Reference priors<a class="anchor" aria-label="anchor" href="#reference-priors"><i class="fas fa-link"></i></a>
</h3>
<p>An alternative to Jeffreys priors are the <strong>reference priors</strong> developed by Bernardo (1979).
This aims to choose the prior such there is maximal “correlation” between the data and the parameter
(more precisely, one aims to maximise the mutual information between <span class="math inline">\(\theta\)</span> and <span class="math inline">\(x\)</span> which is the expected KL divergence between the posterior and prior distribution). For univariate settings the reference priors are identical to Jeffreys priors. However, they also provide good priors in multivariate settings.</p>
</div>
</div>
<div id="optimality-of-bayesian-inference" class="section level2" number="14.5">
<h2>
<span class="header-section-number">14.5</span> Optimality of Bayesian inference<a class="anchor" aria-label="anchor" href="#optimality-of-bayesian-inference"><i class="fas fa-link"></i></a>
</h2>
<p>The optimality of Bayesian model making use of full model specification (likelihood plus prior) can be shown from a number of different perspectives. Correspondingly,
there are many theorems that prove (or at least indicate) this optimality:</p>
<ol style="list-style-type: decimal">
<li><p><a href="https://en.wikipedia.org/wiki/Cox%27s_theorem">Richard Cox’s theorem</a>: the aim to generalise classic logic inevitably leads to Bayesian inference.</p></li>
<li><p><a href="https://en.wikipedia.org/wiki/De_Finetti%27s_theorem">de Finetti’s representation theorem</a>: joint distribution of exchangeable sequences can be viewed as posterior distributions computed by Bayes theorem)</p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Admissible_decision_rule">Frequentist decision theory</a>: all admissible decision rules are Bayes rules!
(admissible = always better in a specific sense than all other methods!)</p></li>
<li><p>Entropy perspective: Bayesian inference is a consequence of minimal information update where new information arrives in form of observations</p></li>
</ol>
<p>Remark: the above directly excludes a few other (somewhat esoteric) suggestions for
propagating uncertainty (such as Fuzzy Logic, imprecise probabilities, etc).</p>
</div>
<div id="conclusion" class="section level2" number="14.6">
<h2>
<span class="header-section-number">14.6</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"><i class="fas fa-link"></i></a>
</h2>
<p>Bayesian statistics offers a coherent framework for statistical learning from data, with methods for</p>
<ul>
<li>estimation</li>
<li>testing</li>
<li>model building</li>
</ul>
<p>There are a number of theorems that show that “optimal” estimators (defined in various ways) are all Bayesian.</p>
<p>It is conceptually very simple — but can be computationally very involved!</p>
<p>It provides a coherent generalisation of classical TRUE/FALSE logic (and therefore does not suffer from some of the inconsistencies prevalent in frequentist statistics).</p>
<p>Bayesian statistics is a non-asymptotic theory, it works for any sample size.
Asympotically (large <span class="math inline">\(n\)</span>) it is consistent and converges to the true model (like ML!).
But Bayesian reasoning can also be applied to events that take place only once — no assumption of hypothetical infinitely many repetitions as in frequentist statistics is needed.</p>
<p>Moreover, many classical (frequentist) procedures may be viewed as <em>approximations</em> to Bayesian methods and estimators, so using classical approaches in the correct application domain is perfectly in line with the Bayesian framework.</p>
<p>Bayesian estimation and inference also automatically regularises (via the prior) which is important for complex models and when there is the problem of overfitting.</p>
<div id="current-directions-of-research" class="section level3" number="14.6.1">
<h3>
<span class="header-section-number">14.6.1</span> Current directions of research<a class="anchor" aria-label="anchor" href="#current-directions-of-research"><i class="fas fa-link"></i></a>
</h3>
<p>For example: connection between Bayesian models and algorithmic models widely used in machine learning (such as neural networks, deep learning, convolutional networks, ensemble methods, XGBoost etc).</p>
<p>Are these models optimal (as in the Bayesian sense)? Can we learn something about highly complex, non-parametric statistical models?</p>
<p>How do we do effective Bayesian learning for these parameter-rich models? Both in terms of computational and statistical efficiency.</p>

</div>
</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></div>
<div class="next"><a href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#optimality-properties-and-summary"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li>
<a class="nav-link" href="#bayesian-statistics-in-a-nutshell"><span class="header-section-number">14.1</span> Bayesian statistics in a nutshell</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#remarks"><span class="header-section-number">14.1.1</span> Remarks</a></li>
<li><a class="nav-link" href="#advantages"><span class="header-section-number">14.1.2</span> Advantages</a></li>
</ul>
</li>
<li><a class="nav-link" href="#frequentist-properties-of-bayesian-estimators"><span class="header-section-number">14.2</span> Frequentist properties of Bayesian estimators</a></li>
<li><a class="nav-link" href="#specifying-the-prior-problem-or-advantage"><span class="header-section-number">14.3</span> Specifying the prior — problem or advantage?</a></li>
<li>
<a class="nav-link" href="#choosing-a-prior"><span class="header-section-number">14.4</span> Choosing a prior</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#some-guidelines"><span class="header-section-number">14.4.1</span> Some guidelines</a></li>
<li><a class="nav-link" href="#jeffreys-prior"><span class="header-section-number">14.4.2</span> Jeffreys prior</a></li>
<li><a class="nav-link" href="#reference-priors"><span class="header-section-number">14.4.3</span> Reference priors</a></li>
</ul>
</li>
<li><a class="nav-link" href="#optimality-of-bayesian-inference"><span class="header-section-number">14.5</span> Optimality of Bayesian inference</a></li>
<li>
<a class="nav-link" href="#conclusion"><span class="header-section-number">14.6</span> Conclusion</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#current-directions-of-research"><span class="header-section-number">14.6.1</span> Current directions of research</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 15 March 2022.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
