<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>1 Overview of statistical learning | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="1 Overview of statistical learning | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Overview of statistical learning | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<meta name="description" content="1.1 How to learn from data? A fundamental question is how to extract information from data in an optimal way, and to make predictions based on this information. For this purpose, a number of...">
<meta property="og:description" content="1.1 How to learn from data? A fundamental question is how to extract information from data in an optimal way, and to make predictions based on this information. For this purpose, a number of...">
<meta name="twitter:description" content="1.1 How to learn from data? A fundamental question is how to extract information from data in an optimal way, and to make predictions based on this information. For this purpose, a number of...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="active" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="conditioning-and-bayes-rule.html"><span class="header-section-number">7</span> Conditioning and Bayes rule</a></li>
<li><a class="" href="models-with-latent-variables-and-missing-data.html"><span class="header-section-number">8</span> Models with latent variables and missing data</a></li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li><a class="" href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li><a class="" href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">14</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">15</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">16</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">17</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">18</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="distributions-used-in-bayesian-analysis.html"><span class="header-section-number">B</span> Distributions used in Bayesian analysis</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">C</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="overview-of-statistical-learning" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Overview of statistical learning<a class="anchor" aria-label="anchor" href="#overview-of-statistical-learning"><i class="fas fa-link"></i></a>
</h1>
<div id="how-to-learn-from-data" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> How to learn from data?<a class="anchor" aria-label="anchor" href="#how-to-learn-from-data"><i class="fas fa-link"></i></a>
</h2>
<p>A fundamental question is how to extract information from data in an optimal way, and to make predictions based on this information.</p>
<p>For this purpose, a number of competing <strong>theories of information</strong> have been developed. <strong>Statistics</strong> is the oldest science of information and is concerned with offering principled ways to learn from data and to extract and process information using probabilistic models.
However, there are other theories of information (e.g. Vapnik-Chernov theory of learning, computational learning) that are more algorithmic than analytic and sometimes not even based on probability theory.</p>
<p>Furthermore, there are other disciplines, such computer science and machine learning that are closely linked with and also have substantial overlap with statistics. The field of “data science” today comprises of both statistics and machine learning and brings together mathematics, statistics and computer science. Also the growing field of so-called “artificial intelligence” makes substantial use of statistical and machine learning techniques.</p>
<p>The recent popular science book “The Master Algorithm” by <span class="citation">Domingos (<a href="bibliography.html#ref-Domingos2015" role="doc-biblioref">2015</a>)</span> provides an accessible informal overview over
the various schools of science of information. It discusses the main algorithms used in machine learning and statistics:</p>
<ul>
<li><p>Starting as early as 1763, the <strong>Bayesian school</strong> of learning was started which later turned out to be closely linked with <em>likelihood inference</em> established in 1922 by <a href="https://de.wikipedia.org/wiki/Ronald_Aylmer_Fisher">R.A. Fisher (1890–1962)</a> and generalised in 1951 to <strong>entropy learning</strong> by Kullback and Leibler.</p></li>
<li><p>It was also in the 1950s that the concept of artificial <strong>neural network</strong> arises, essentially a nonlinear input-output map that works in a non-probabilistic way. This field saw another leap in the 1980s and further progressed from 2010 onwards with the development of <em>deep dearning</em>. It is now one of the most popular (and most effective) methods for analysing imaging data. Even your mobile phone most likely has a dedicated computer chip with special neural network hardware, for example.</p></li>
<li><p>Further advanced theories of information were developed in the 1960 under the term of
<strong>computational learning</strong>, most notably the Vapnik-Chernov theory, with the most prominent example of the “support vector machine” (another non-probabilistic model).</p></li>
<li><p>With the advent of large-scale genomic and other high-dimensional data there has been a surge of new and exciting developments in the field of high-dimensional (large dimension) and also big data (large dimension and large sample size), both in statistics and in machine learning.</p></li>
</ul>
<p><strong>The connections between various fields of information is still not perfectly understood, but it is clear that an overarching theory will need to be based on probabilistic learning.</strong></p>
</div>
<div id="probability-theory-versus-statistical-learning" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Probability theory versus statistical learning<a class="anchor" aria-label="anchor" href="#probability-theory-versus-statistical-learning"><i class="fas fa-link"></i></a>
</h2>
<p>When you study statistics (or any other information theory) you need to be aware that there is a fundamental difference between probability theory
and statistics, and that relates to the <strong>distinction between
“randomness” and “uncertainty”</strong>.</p>
<p>Probability theory studies <strong>randomness</strong>, by developing mathematical models for randomness (such as probability distributions), and studying corresponding mathematical properties (including asymptotics etc). Probability theory may in fact be viewed as a branch of measure theory, and thus it belongs
to the domain of pure mathematics.</p>
<p>Probability theory provides probabilistic generative models for data, for simulation
of data or for use in learning from data, i.e. inference about the model from observations.
Methods and theory how to best learn from data is the domain of applied mathematics, specifically statistics and the related areas of machine learning and data science.</p>
<p>Note that statistics, in contrast to probability, is in fact not at all concerned with randomness. Instead, the focus is about measuring and elucidating the <strong>uncertainty</strong> of events, predictions, outcomes, parameters and this uncertainty measures the <strong>state of knowledge</strong>. Note that if new data or information becomes available, the state of knowledge and thus the uncertainty changes! Thus, <strong>uncertainty is an epistemological property</strong>.</p>
<p>The uncertainty most often is due to our ignorance of the true underlying processes (on purpose or not), but not because the underlying process is actually random. The success of statistics is based on the fact that we can mathematically model the uncertainty without knowing any specifics of the underlying processes, and we still have procedures for optimal inference despite the uncertainty.</p>
<p>In short, statistics is about describing the state of knowledge of the world, which
may be uncertain and incomplete, and to make decisions and predictions in the face of uncertainty, and this uncertaintly sometimes derives from randomness but most often from our ignorance (and sometimes this ignorance even helps to create a simple yet effective model)!</p>
</div>
<div id="cartoon-of-statistical-learning" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Cartoon of statistical learning<a class="anchor" aria-label="anchor" href="#cartoon-of-statistical-learning"><i class="fas fa-link"></i></a>
</h2>
<p>We observe data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span>
assumed to have been generated by an underlying true model <span class="math inline">\(M_{\text{true}}\)</span> with
true parameters <span class="math inline">\(\boldsymbol \theta_{\text{true}}\)</span></p>
<p>To explain the data, and make predictions, we make hypotheses
in the form of candidate models <span class="math inline">\(M_{1}, M_{2}, \ldots\)</span> and corresponding parameters <span class="math inline">\(\boldsymbol \theta_1, \boldsymbol \theta_2, \ldots\)</span>.
The true model itself is unknown and cannot be observed. However, what we can observe is data <span class="math inline">\(D\)</span> from the true model by measuring properties of objects interest (our observations from experiments). Sometimes we can also perturb the model and see what the effect is (interventional study).</p>
<p>The various candidate models <span class="math inline">\(M_1, M_2, \ldots\)</span> in the <strong>model world</strong> will never be perfect or correct
as the true model <span class="math inline">\(M_{\text{true}}\)</span> will only be among the candidate models in an idealised situation. However, even an imperfect candidate model will often provide a useful mathematical approximation and capture some important characteristics of the true model and thus will help
to interpret observed data.</p>
<p><span class="math display">\[
\begin{array}{cc}
\textbf{Hypothesis} \\
\text{How the world works} \\
\end{array}
\longrightarrow
\begin{array}{cc}
\textbf{Model world} \\
M_1,  \boldsymbol \theta_1  \\
M_2, \boldsymbol \theta_2  \\
\vdots\\
\end{array}
\]</span>
<span class="math display">\[
\longrightarrow
\begin{array}{cc}
\textbf{Real world,} \\
\textbf{unknown true model} \\
M_{\text{true}}, \boldsymbol \theta_{\text{true}} \\
\end{array}
\longrightarrow \textbf{Data } x_1, \ldots, x_n
\]</span></p>
<p><strong>The aim of statistical learning is to identify the model(s) that explain the current data and also predict future data (i.e. predict outcome of experiments that have not been conducted yet).</strong></p>
<p>Thus a good model provides a good fit to the current data (i.e. it explains current observations well) and also to the future data (i.e. it generalises well).</p>
<p>A large proportion of statistical theory is devoted to finding these “good” models
that avoid both <em>overfitting</em> (models being too complex and don’t generalise well) or
<em>underfitting</em> (models being too simplistic and hence also don’t predict well).</p>
<p>Typically the aim is to find a model whose <strong>model complexity</strong> matches the
complexity of the unknown true model and also the complexity of the data observed from the unknown true model.</p>
</div>
<div id="likelihood" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> Likelihood<a class="anchor" aria-label="anchor" href="#likelihood"><i class="fas fa-link"></i></a>
</h2>
<p>In statistics and machine learning most models that are being used are probabilistic to take
account of both randomness and uncertainty.
A core task in statistical learning is to identify those models that explain the existing data well and that also generalise well to unseen data.</p>
<p>For this we need, among other things, a measure of how well a candidate model approximates the (typically unknown) true data generating model and an approach to choose the best model(s).
One such approach is provided by the method of maximum likelihood that enables us to estimate parameters of models and to find the particular model that is the best fit to the data.</p>
<p>Given a probability distribution <span class="math inline">\(P_{\boldsymbol \theta}\)</span> with density or mass function <span class="math inline">\(p(x|\boldsymbol \theta)\)</span> where <span class="math inline">\(\boldsymbol \theta\)</span> is a parameter vector, and <span class="math inline">\(D = \{x_1,\dots,x_n\}\)</span> are the observed iid data (i.e. independent and identically distributed), the <strong>likelihood function</strong> is defined as
<span class="math display">\[
L_n(\boldsymbol \theta| D ) =\prod_{i=1}^{n} p(x_i|\boldsymbol \theta)
\]</span>
Typically, instead of the likelihood one uses the log-likelihood function:
<span class="math display">\[
l_n(\boldsymbol \theta| D) = \log L_n(\boldsymbol \theta| D) =  \sum_{i=1}^n \log p(x_i|\boldsymbol \theta)
\]</span>
Reasons for preferring the log-likelihood (rather than likelihood) include that</p>
<ul>
<li>the log-density is in fact the more “natural” and relevant quantity (this will become clear in the upcoming chapters) and that</li>
<li>addition is numerically more stable than multiplication on a computer.</li>
</ul>
<p>For discrete random variables for which <span class="math inline">\(p(x |\boldsymbol \theta)\)</span> is a probability mass function the likelihood is often interpreted as the probability to observe the data given the model with specified parameters <span class="math inline">\(\boldsymbol \theta\)</span>. In fact, this was indeed the way how the likelihood was historically introduced. However, this view is not strictly correct.
First, given that the samples are iid and thus the ordering of the <span class="math inline">\(x_i\)</span> is not important, an additional factor accounting for the possible permutations is needed in the likelihood to obtain the actual probability of the data. Moreover, for continuous random variables this interpretation breaks down due to the use of densities rather than probability mass functions in the likelihood. Thus, the view that the likelihood is the probability of the data is in fact too simplistic.</p>
<p>In the next chapter we will see that the justification for using likelihood rather stems from its close link to the Kullback-Leibler information and cross-entropy. This also helps to understand why using likelihood for estimation is only optimal in the limit of large sample size.</p>
<p>In the first part of the MATH28082 “Statistical Methods” module we will study likelihood estimation and inference in much detail. We will provide links to related methods of inference and discuss its information-theoretic foundations. We will also discuss the optimality properties as well as the limitations of likelihood inference. Extensions of likelihood analysis, in particular Bayesian learning, which will be discussed in the second part of the module. In the third part of the module we will apply statistical learning to linear regression.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="preface.html">Preface</a></div>
<div class="next"><a href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#overview-of-statistical-learning"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="nav-link" href="#how-to-learn-from-data"><span class="header-section-number">1.1</span> How to learn from data?</a></li>
<li><a class="nav-link" href="#probability-theory-versus-statistical-learning"><span class="header-section-number">1.2</span> Probability theory versus statistical learning</a></li>
<li><a class="nav-link" href="#cartoon-of-statistical-learning"><span class="header-section-number">1.3</span> Cartoon of statistical learning</a></li>
<li><a class="nav-link" href="#likelihood"><span class="header-section-number">1.4</span> Likelihood</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 6 June 2023.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
