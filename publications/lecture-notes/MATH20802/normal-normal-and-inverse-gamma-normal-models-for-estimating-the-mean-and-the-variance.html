<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.13/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><meta name="description" content="10.1 Normal-Normal model to estimate mean  10.1.1 Normal likelihood For the likelihood we assume as data-generating model the normal distribution with known fixed variance \(\sigma^2\) \[ x| \mu...">
<meta property="og:description" content="10.1 Normal-Normal model to estimate mean  10.1.1 Normal likelihood For the likelihood we assume as data-generating model the normal distribution with known fixed variance \(\sigma^2\) \[ x| \mu...">
<meta name="twitter:description" content="10.1 Normal-Normal model to estimate mean  10.1.1 Normal likelihood For the likelihood we assume as data-generating model the normal distribution with known fixed variance \(\sigma^2\) \[ x| \mu...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="active" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance" class="section level1" number="10">
<h1>
<span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance<a class="anchor" aria-label="anchor" href="#normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance"><i class="fas fa-link"></i></a>
</h1>
<div id="normal-normal-model-to-estimate-mean" class="section level2" number="10.1">
<h2>
<span class="header-section-number">10.1</span> Normal-Normal model to estimate mean<a class="anchor" aria-label="anchor" href="#normal-normal-model-to-estimate-mean"><i class="fas fa-link"></i></a>
</h2>
<div id="normal-likelihood" class="section level3" number="10.1.1">
<h3>
<span class="header-section-number">10.1.1</span> Normal likelihood<a class="anchor" aria-label="anchor" href="#normal-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>For the <strong>likelihood</strong> we assume as data-generating model the normal distribution with known fixed variance <span class="math inline">\(\sigma^2\)</span>
<span class="math display">\[
x| \mu \sim N(\mu, \sigma^2)
\]</span>
This yields as the MLE <span class="math inline">\(\hat\mu_{ML} = \bar{x}\)</span>.</p>
</div>
<div id="normal-prior-distribution" class="section level3" number="10.1.2">
<h3>
<span class="header-section-number">10.1.2</span> Normal prior distribution<a class="anchor" aria-label="anchor" href="#normal-prior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>To model the uncertainty about <span class="math inline">\(\mu\)</span> we use the normal distribution <span class="math inline">\(N(\mu, \sigma^2/k)\)</span>
parameterised by the two parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(k\)</span> (remember <span class="math inline">\(\sigma^2\)</span> is fixed).</p>
<p>With <span class="math inline">\(\mu=\mu_0\)</span> and <span class="math inline">\(k=m\)</span> we get the <strong>normal prior</strong>
<span class="math display">\[
\mu \sim N(\mu_0, \sigma^2/m)
\]</span>
with prior mean
<span class="math inline">\(\text{E}(\mu) = \mu_0\)</span>
and prior variance
<span class="math inline">\(\text{Var}(\mu) = \frac{\sigma^2}{m}\)</span>
where <span class="math inline">\(m\)</span> is the implied sample size from the prior. Note that <span class="math inline">\(m\)</span> does not need to be an integer value!</p>
</div>
<div id="normal-posterior-distribution" class="section level3" number="10.1.3">
<h3>
<span class="header-section-number">10.1.3</span> Normal posterior distribution<a class="anchor" aria-label="anchor" href="#normal-posterior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>posterior distribution</strong> after observing <span class="math inline">\(n\)</span> samples <span class="math inline">\(x_1, \ldots x_n\)</span>
is normal with <span class="math inline">\(\mu=\mu_1\)</span> and <span class="math inline">\(k=m+n\)</span>
<span class="math display">\[
\mu | x_1, \ldots x_n \sim N(\mu_1, \sigma^2/(m+n))
\]</span>
with posterior mean
<span class="math display">\[
\text{E}(\mu |  x_1, \ldots x_n) = \mu_1 =\frac{m \mu_0 + n \bar{x}}{n+m}  = \lambda \mu_0 + (1-\lambda) \hat\mu_{ML}
\]</span>
with <span class="math inline">\(\lambda = \frac{m}{n+m}\)</span>. Note the linear shrinkage of
<span class="math inline">\(\hat\mu_{ML}\)</span> towards <span class="math inline">\(\mu_0\)</span>!</p>
<p>The corresponding posterior variance is
<span class="math display">\[
\text{Var}(\mu |  x_1, \ldots x_n) = \frac{\sigma^2}{n+m}
\]</span>
Thus, the <strong>normal distribution is the conjugate distribution to the mean parameter in the normal likelihood</strong>.</p>
</div>
<div id="large-sample-asymptotics-and-stein-paradox" class="section level3" number="10.1.4">
<h3>
<span class="header-section-number">10.1.4</span> Large sample asymptotics and Stein paradox<a class="anchor" aria-label="anchor" href="#large-sample-asymptotics-and-stein-paradox"><i class="fas fa-link"></i></a>
</h3>
<p>For <span class="math inline">\(n\)</span> large and <span class="math inline">\(n &gt;&gt; m\)</span> we get
<span class="math display">\[
\text{E}(\mu |  x_1, \ldots x_n) \overset{a}{=}  \hat\mu_{ML}
\]</span>
<span class="math display">\[
\text{Var}(\mu |  x_1, \ldots x_n) \overset{a}{=} \frac{\sigma^2}{n}
\]</span>
i.e. the MLE and its asymptotic variance!</p>
<p>Note that the posterior variance <span class="math inline">\(\frac{\sigma^2}{n+m}\)</span> is smaller than the asymptotic variance <span class="math inline">\(\frac{\sigma^2}{n}\)</span> and the prior variance <span class="math inline">\(\frac{\sigma^2}{m}\)</span>.</p>
<p>When studying the frequentist properties of the posterior mean <span class="math inline">\(\mu_1\)</span> it turns out that
by an appropriate choice of <span class="math inline">\(m\)</span> (or <span class="math inline">\(\lambda\)</span>) it is possible to construct an estimator that
will outperform the MLE for finite <span class="math inline">\(n\)</span> in terms of MSE (with the reduced variance compensating for the increase in bias)!
Charles Stein was one of the first to present such an estimator (see next chapter), and by many of his contemporaries
it was considered very puzzling to have any estimator outperform the MLE, hence this effect is called
<strong>Stein paradox</strong>.</p>
</div>
</div>
<div id="inverse-gamma-normal-model-to-estimate-variance" class="section level2" number="10.2">
<h2>
<span class="header-section-number">10.2</span> Inverse-Gamma-Normal model to estimate variance<a class="anchor" aria-label="anchor" href="#inverse-gamma-normal-model-to-estimate-variance"><i class="fas fa-link"></i></a>
</h2>
<div id="inverse-gamma-distribution" class="section level3" number="10.2.1">
<h3>
<span class="header-section-number">10.2.1</span> Inverse Gamma distribution<a class="anchor" aria-label="anchor" href="#inverse-gamma-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Next, we study a common Bayesian model for estimating the variance parameter of the normal distribution. For this we use the inverse Gamma distribution:
<span class="math display">\[
x \sim \text{Inv-Gam}(\alpha, \beta)
\]</span>
This distribution is closely linked with the Gamma distribution — the inverse of <span class="math inline">\(x\)</span> is Gamma-distributed with inverted scale parameter:
<span class="math display">\[\frac{1}{x} \sim \text{Gam}(\alpha, \beta^{-1})\]</span></p>
<p>For use as prior and posterior we employ a different parameterisation with <span class="math inline">\(k=2(\alpha-1)\)</span> and <span class="math inline">\(v=\beta/(\alpha-1)\)</span>:
<span class="math display">\[
x \sim \text{Inv-Gam}(1+\frac{k}{2}, \frac{k}{2} v)
\]</span></p>
<p>The first two moments of the inverse Gamma distribution are
<span class="math display">\[\text{E}(x) = \frac{\beta}{\alpha-1} = v\]</span>
and
<span class="math display">\[\text{Var}(x) = \frac{\beta^2}{(\alpha-1)^2 (\alpha-2)} =\frac{2 v^2}{k-2}\]</span></p>
</div>
<div id="normal-likelihoood" class="section level3" number="10.2.2">
<h3>
<span class="header-section-number">10.2.2</span> Normal likelihoood<a class="anchor" aria-label="anchor" href="#normal-likelihoood"><i class="fas fa-link"></i></a>
</h3>
<p>As data likelihood / generating model we use
normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span> with given fixed mean <span class="math inline">\(\mu\)</span>.</p>
<p>This yields as MLE <span class="math inline">\(\widehat\sigma^2_{ML}= \frac{1}{n}\sum_{i=1}^n (x_i-\mu)^2\)</span></p>
</div>
<div id="inverse-gamma-prior-distribution" class="section level3" number="10.2.3">
<h3>
<span class="header-section-number">10.2.3</span> Inverse Gamma prior distribution<a class="anchor" aria-label="anchor" href="#inverse-gamma-prior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>For the prior distribution we use the inverse Gamma
distribution with with <span class="math inline">\(k=m\)</span> and <span class="math inline">\(v=\sigma^2_0\)</span>
<span class="math display">\[
\sigma^2 \sim \text{Inv-Gam}(k=m, v=\sigma^2_0) 
\]</span>
The corresponding prior mean is
<span class="math display">\[
\text{E}(\sigma^2) = \sigma^2_0
\]</span>
and the prior variance is
<span class="math display">\[
\text{Var}(\sigma^2) = \frac{2 \sigma_0^4}{m-2}
\]</span>
(note that <span class="math inline">\(m &gt; 2\)</span>)</p>
</div>
<div id="inverse-gamma-posterior-distribution" class="section level3" number="10.2.4">
<h3>
<span class="header-section-number">10.2.4</span> Inverse Gamma posterior distribution<a class="anchor" aria-label="anchor" href="#inverse-gamma-posterior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>As the inverse Gammma distribution is conjugate to the
normal likelihood the posterior distribution is inverse Gamma as well:
<span class="math display">\[
\sigma^2| x_1 \ldots, x_n \sim \text{Inv-Gam}(k=m+n, v=\sigma^2_1)
\]</span>
with <span class="math inline">\(\sigma^2_1 = \frac{\sigma^2_0 m + n \widehat\sigma^2_{ML}}{m+n}\)</span>.</p>
<p>The posterior mean is
<span class="math display">\[
\text{E}(\sigma^2 | x_1 \ldots, x_n) = \sigma^2_1
\]</span>
and the posterior variance
<span class="math display">\[
\text{Var}(\sigma^2 | x_1 \ldots, x_n) = \frac{ 2 \sigma^4_1}{m+n-2}
\]</span>
The update formula for the posterior mean of the variance follows the usual linear shrinkage rule:
<span class="math display">\[
\sigma^2_1 =  \lambda \sigma^2_0 + (1-\lambda) \widehat\sigma^2_{ML}
\]</span>
with <span class="math inline">\(\lambda=\frac{m}{m+n}\)</span>.</p>
</div>
<div id="large-sample-asymptotics-1" class="section level3" number="10.2.5">
<h3>
<span class="header-section-number">10.2.5</span> Large sample asymptotics<a class="anchor" aria-label="anchor" href="#large-sample-asymptotics-1"><i class="fas fa-link"></i></a>
</h3>
<p>For <span class="math inline">\(n\)</span> large and <span class="math inline">\(n &gt;&gt; m\)</span> we get
<span class="math display">\[
\text{E}(\sigma^2 |  x_1, \ldots x_n) \overset{a}{=}  \widehat\sigma^2_{ML}
\]</span>
<span class="math display">\[
\text{Var}(\sigma^2 |  x_1, \ldots x_n) \overset{a}{=} \frac{2 \sigma^4}{n}
\]</span>
which is indeed the MLE of <span class="math inline">\(\sigma^2\)</span> and its asymptotic variance!</p>
</div>
<div id="estimating-precision" class="section level3" number="10.2.6">
<h3>
<span class="header-section-number">10.2.6</span> Estimating precision<a class="anchor" aria-label="anchor" href="#estimating-precision"><i class="fas fa-link"></i></a>
</h3>
<p>Instead of estimating the variance it is actually a bit simpler to estimate the precision (i.e. the inverse variance). For this one would then use
a Gamma prior and a normal likelihood, resulting in a Gamma posterior.</p>
</div>
<div id="joint-estimation-of-mean-and-variance" class="section level3" number="10.2.7">
<h3>
<span class="header-section-number">10.2.7</span> Joint estimation of mean and variance<a class="anchor" aria-label="anchor" href="#joint-estimation-of-mean-and-variance"><i class="fas fa-link"></i></a>
</h3>
<p>It is possible to combine the Normal-Normal for the mean
and the Inverse-Gamma-Normal model into a joint model for the mean and variance.</p>
<p>This implies having a joint prior and a joint posterior for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Details are not shown here but the resulting joint point estimators are identical to the above individual estimators.</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></div>
<div class="next"><a href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li>
<a class="nav-link" href="#normal-normal-model-to-estimate-mean"><span class="header-section-number">10.1</span> Normal-Normal model to estimate mean</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#normal-likelihood"><span class="header-section-number">10.1.1</span> Normal likelihood</a></li>
<li><a class="nav-link" href="#normal-prior-distribution"><span class="header-section-number">10.1.2</span> Normal prior distribution</a></li>
<li><a class="nav-link" href="#normal-posterior-distribution"><span class="header-section-number">10.1.3</span> Normal posterior distribution</a></li>
<li><a class="nav-link" href="#large-sample-asymptotics-and-stein-paradox"><span class="header-section-number">10.1.4</span> Large sample asymptotics and Stein paradox</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#inverse-gamma-normal-model-to-estimate-variance"><span class="header-section-number">10.2</span> Inverse-Gamma-Normal model to estimate variance</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#inverse-gamma-distribution"><span class="header-section-number">10.2.1</span> Inverse Gamma distribution</a></li>
<li><a class="nav-link" href="#normal-likelihoood"><span class="header-section-number">10.2.2</span> Normal likelihoood</a></li>
<li><a class="nav-link" href="#inverse-gamma-prior-distribution"><span class="header-section-number">10.2.3</span> Inverse Gamma prior distribution</a></li>
<li><a class="nav-link" href="#inverse-gamma-posterior-distribution"><span class="header-section-number">10.2.4</span> Inverse Gamma posterior distribution</a></li>
<li><a class="nav-link" href="#large-sample-asymptotics-1"><span class="header-section-number">10.2.5</span> Large sample asymptotics</a></li>
<li><a class="nav-link" href="#estimating-precision"><span class="header-section-number">10.2.6</span> Estimating precision</a></li>
<li><a class="nav-link" href="#joint-estimation-of-mean-and-variance"><span class="header-section-number">10.2.7</span> Joint estimation of mean and variance</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 15 March 2022.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
