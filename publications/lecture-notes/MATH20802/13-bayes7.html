<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>13 False discovery rates | HTML</title>
  <meta name="description" content="Statistical Methods:<br />
Likelihood, Bayes and Regression</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="13 False discovery rates | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13 False discovery rates | HTML" />
  
  
  



<meta name="date" content="2021-01-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="12-bayes6.html"/>
<link rel="next" href="14-bayes8.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH20802/index.html">MATH20802 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Likelihood estimation and inference</b></span></li>
<li class="chapter" data-level="1" data-path="01-likelihood1.html"><a href="01-likelihood1.html"><i class="fa fa-check"></i><b>1</b> Overview of statistical learning</a><ul>
<li class="chapter" data-level="1.1" data-path="01-likelihood1.html"><a href="01-likelihood1.html#how-to-learn-from-data"><i class="fa fa-check"></i><b>1.1</b> How to learn from data?</a></li>
<li class="chapter" data-level="1.2" data-path="01-likelihood1.html"><a href="01-likelihood1.html#probability-theory-versus-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Probability theory versus statistical learning</a></li>
<li class="chapter" data-level="1.3" data-path="01-likelihood1.html"><a href="01-likelihood1.html#cartoon-of-statistical-learning"><i class="fa fa-check"></i><b>1.3</b> Cartoon of statistical learning</a></li>
<li class="chapter" data-level="1.4" data-path="01-likelihood1.html"><a href="01-likelihood1.html#likelihood"><i class="fa fa-check"></i><b>1.4</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="02-likelihood2.html"><a href="02-likelihood2.html"><i class="fa fa-check"></i><b>2</b> From information theory to likelihood</a><ul>
<li class="chapter" data-level="2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy"><i class="fa fa-check"></i><b>2.1</b> Entropy</a><ul>
<li class="chapter" data-level="2.1.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#surprise"><i class="fa fa-check"></i><b>2.1.2</b> Surprise</a></li>
<li class="chapter" data-level="2.1.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#shannon-entropy"><i class="fa fa-check"></i><b>2.1.3</b> Shannon entropy</a></li>
<li class="chapter" data-level="2.1.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#differential-entropy"><i class="fa fa-check"></i><b>2.1.4</b> Differential entropy</a></li>
<li class="chapter" data-level="2.1.5" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-entropy-principle-to-characterise-distributions"><i class="fa fa-check"></i><b>2.1.5</b> Maximum entropy principle to characterise distributions</a></li>
<li class="chapter" data-level="2.1.6" data-path="02-likelihood2.html"><a href="02-likelihood2.html#cross-entropy"><i class="fa fa-check"></i><b>2.1.6</b> Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>2.2</b> Kullback-Leibler divergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#definition"><i class="fa fa-check"></i><b>2.2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#properties"><i class="fa fa-check"></i><b>2.2.2</b> Properties</a></li>
<li class="chapter" data-level="2.2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#examples"><i class="fa fa-check"></i><b>2.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#local-quadratic-approximation-and-expected-fisher-information"><i class="fa fa-check"></i><b>2.3</b> Local quadratic approximation and expected Fisher information</a></li>
<li class="chapter" data-level="2.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy-learning-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4</b> Entropy learning and maximum likelihood</a><ul>
<li class="chapter" data-level="2.4.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#the-relative-entropy-between-true-model-and-approximating-model"><i class="fa fa-check"></i><b>2.4.1</b> The relative entropy between true model and approximating model</a></li>
<li class="chapter" data-level="2.4.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-likelihood-and-minimum-kl-divergence"><i class="fa fa-check"></i><b>2.4.2</b> Maximum likelihood and minimum KL divergence</a></li>
<li class="chapter" data-level="2.4.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#further-connections"><i class="fa fa-check"></i><b>2.4.3</b> Further connections</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="03-likelihood3.html"><a href="03-likelihood3.html"><i class="fa fa-check"></i><b>3</b> Maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#principle-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.1</b> Principle of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#outline"><i class="fa fa-check"></i><b>3.1.1</b> Outline</a></li>
<li class="chapter" data-level="3.1.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#recipe-for-obtaining-mles"><i class="fa fa-check"></i><b>3.1.2</b> Recipe for obtaining MLEs</a></li>
<li class="chapter" data-level="3.1.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#invariance-property-of-the-mle"><i class="fa fa-check"></i><b>3.1.3</b> Invariance property of the MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#examples-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.2</b> Examples of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.2.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-estimation-of-a-proportion"><i class="fa fa-check"></i><b>3.2.1</b> Example 1: Estimation of a proportion</a></li>
<li class="chapter" data-level="3.2.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-exponential-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Example 2: Exponential Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-3-normal-distribution-with-unknown-mean-and-known-variance"><i class="fa fa-check"></i><b>3.2.3</b> Example 3: Normal distribution with unknown mean and known variance</a></li>
<li class="chapter" data-level="3.2.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-4-normal-distribution-with-both-mean-and-variance-unknown"><i class="fa fa-check"></i><b>3.2.4</b> Example 4: Normal Distribution with both mean and variance unknown</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information"><i class="fa fa-check"></i><b>3.3</b> Observed Fisher information</a><ul>
<li class="chapter" data-level="3.3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#motivation"><i class="fa fa-check"></i><b>3.3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#curvature-of-log-likelihood-function"><i class="fa fa-check"></i><b>3.3.2</b> Curvature of log-likelihood function</a></li>
<li class="chapter" data-level="3.3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information-1"><i class="fa fa-check"></i><b>3.3.3</b> Observed Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information---examples"><i class="fa fa-check"></i><b>3.4</b> Observed Fisher information - Examples</a><ul>
<li class="chapter" data-level="3.4.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-bernoulli-binomial-model"><i class="fa fa-check"></i><b>3.4.1</b> Example 1: Bernoulli / Binomial model</a></li>
<li class="chapter" data-level="3.4.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-normal-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Example 2: Normal distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#from-observed-to-expected-fisher-information"><i class="fa fa-check"></i><b>3.4.3</b> From observed to expected Fisher information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="04-likelihood4.html"><a href="04-likelihood4.html"><i class="fa fa-check"></i><b>4</b> Quadratic approximation and normal asymptotics</a><ul>
<li class="chapter" data-level="4.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-approximation-of-log-likelihood-function-around-mle"><i class="fa fa-check"></i><b>4.1</b> Quadratic approximation of log-likelihood function around MLE</a></li>
<li class="chapter" data-level="4.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-normality-of-mle"><i class="fa fa-check"></i><b>4.2</b> Asymptotic normality of MLE</a></li>
<li class="chapter" data-level="4.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-confidence-intervals-for-mles"><i class="fa fa-check"></i><b>4.3</b> Normal confidence intervals for MLEs</a></li>
<li class="chapter" data-level="4.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#observed-or-expected-fisher-information-to-estimate-variance-of-the-mle"><i class="fa fa-check"></i><b>4.4</b> Observed or expected Fisher information to estimate variance of the MLE?</a></li>
<li class="chapter" data-level="4.5" data-path="04-likelihood4.html"><a href="04-likelihood4.html#wald-statistic"><i class="fa fa-check"></i><b>4.5</b> Wald statistic</a></li>
<li class="chapter" data-level="4.6" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-ci-expressed-using-the-squared-wald-statistics"><i class="fa fa-check"></i><b>4.6</b> Normal CI expressed using the squared Wald statistics</a></li>
<li class="chapter" data-level="4.7" data-path="04-likelihood4.html"><a href="04-likelihood4.html#testing-and-confidence-intervals"><i class="fa fa-check"></i><b>4.7</b> Testing and confidence intervals</a></li>
<li class="chapter" data-level="4.8" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-normal-distribution"><i class="fa fa-check"></i><b>4.8</b> Example: normal distribution</a></li>
<li class="chapter" data-level="4.9" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-of-non-regular-model"><i class="fa fa-check"></i><b>4.9</b> Example of non-regular model</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="05-likelihood5.html"><a href="05-likelihood5.html"><i class="fa fa-check"></i><b>5</b> Likelihood-based confidence interval and likelihood ratio</a><ul>
<li class="chapter" data-level="5.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-based-confidence-intervals"><i class="fa fa-check"></i><b>5.1</b> Likelihood-based confidence intervals</a></li>
<li class="chapter" data-level="5.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#wilks-log-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.2</b> Wilks log likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.3" data-path="05-likelihood5.html"><a href="05-likelihood5.html#quadratic-approximation-of-wilks-statistic"><i class="fa fa-check"></i><b>5.3</b> Quadratic approximation of Wilks statistic</a></li>
<li class="chapter" data-level="5.4" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistics"><i class="fa fa-check"></i><b>5.4</b> Distribution of Wilks statistics</a><ul>
<li class="chapter" data-level="5.4.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#cutoff-values-delta"><i class="fa fa-check"></i><b>5.4.1</b> Cutoff values <span class="math inline">\(\Delta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="05-likelihood5.html"><a href="05-likelihood5.html#example-likelihood-ci-for-exponential-model"><i class="fa fa-check"></i><b>5.5</b> Example: likelihood CI for exponential model</a></li>
<li class="chapter" data-level="5.6" data-path="05-likelihood5.html"><a href="05-likelihood5.html#origin-of-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.6</b> Origin of likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.7" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistic-and-likelihood-ci"><i class="fa fa-check"></i><b>5.7</b> Distribution of Wilks statistic and Likelihood CI</a></li>
<li class="chapter" data-level="5.8" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>5.8</b> Likelihood ratio test (LRT)</a></li>
<li class="chapter" data-level="5.9" data-path="05-likelihood5.html"><a href="05-likelihood5.html#optimality-of-lrts"><i class="fa fa-check"></i><b>5.9</b> Optimality of LRTs</a></li>
<li class="chapter" data-level="5.10" data-path="05-likelihood5.html"><a href="05-likelihood5.html#generalised-likelihood-ratio-test-glrt"><i class="fa fa-check"></i><b>5.10</b> Generalised likelihood ratio test (GLRT)</a></li>
<li class="chapter" data-level="5.11" data-path="05-likelihood5.html"><a href="05-likelihood5.html#glrt-example"><i class="fa fa-check"></i><b>5.11</b> GLRT example</a></li>
<li class="chapter" data-level="5.12" data-path="05-likelihood5.html"><a href="05-likelihood5.html#thoughts-on-model-selection"><i class="fa fa-check"></i><b>5.12</b> Thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="06-likelihood6.html"><a href="06-likelihood6.html"><i class="fa fa-check"></i><b>6</b> Optimality properties, minimal sufficiency and summary</a><ul>
<li class="chapter" data-level="6.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#properties-of-mles-encountered-so-far"><i class="fa fa-check"></i><b>6.1</b> Properties of MLEs encountered so far</a></li>
<li class="chapter" data-level="6.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#further-optimality-properties-of-mles"><i class="fa fa-check"></i><b>6.2</b> Further optimality properties of MLEs</a></li>
<li class="chapter" data-level="6.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency"><i class="fa fa-check"></i><b>6.3</b> Summarising data and the concept of minimal sufficiency</a></li>
<li class="chapter" data-level="6.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summary-and-concluding-remarks-on-maximum-likelihood"><i class="fa fa-check"></i><b>6.4</b> Summary and concluding remarks on maximum likelihood</a><ul>
<li class="chapter" data-level="6.4.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#starting-point-kl-divergence"><i class="fa fa-check"></i><b>6.4.1</b> Starting point: KL divergence</a></li>
<li class="chapter" data-level="6.4.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#connections-between-kl-divergence-likelihood-and-expected-and-observed-fisher-information"><i class="fa fa-check"></i><b>6.4.2</b> Connections between KL divergence, likelihood and expected and observed Fisher information</a></li>
<li class="chapter" data-level="6.4.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#likelihood-estimation"><i class="fa fa-check"></i><b>6.4.3</b> Likelihood estimation</a></li>
<li class="chapter" data-level="6.4.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#what-happens-if-n-is-small"><i class="fa fa-check"></i><b>6.4.4</b> What happens if <span class="math inline">\(n\)</span> is small?</a></li>
<li class="chapter" data-level="6.4.5" data-path="06-likelihood6.html"><a href="06-likelihood6.html#inference-with-likelihood"><i class="fa fa-check"></i><b>6.4.5</b> Inference with likelihood:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Bayesian Statistics</b></span></li>
<li class="chapter" data-level="7" data-path="07-bayes1.html"><a href="07-bayes1.html"><i class="fa fa-check"></i><b>7</b> Essentials of Bayesian statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#bayes-theorem"><i class="fa fa-check"></i><b>7.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#principle-of-bayesian-learning"><i class="fa fa-check"></i><b>7.2</b> Principle of Bayesian learning</a></li>
<li class="chapter" data-level="7.3" data-path="07-bayes1.html"><a href="07-bayes1.html#what-is-exactly-is-the-bayesian-estimate"><i class="fa fa-check"></i><b>7.3</b> What is exactly is the “Bayesian estimate”?</a></li>
<li class="chapter" data-level="7.4" data-path="07-bayes1.html"><a href="07-bayes1.html#computer-implementation-of-bayesian-learning"><i class="fa fa-check"></i><b>7.4</b> Computer implementation of Bayesian learning</a></li>
<li class="chapter" data-level="7.5" data-path="07-bayes1.html"><a href="07-bayes1.html#bayesian-interpretation-of-probability"><i class="fa fa-check"></i><b>7.5</b> Bayesian interpretation of probability</a><ul>
<li class="chapter" data-level="7.5.1" data-path="07-bayes1.html"><a href="07-bayes1.html#what-makes-you-bayesian"><i class="fa fa-check"></i><b>7.5.1</b> What makes you “Bayesian”?</a></li>
<li class="chapter" data-level="7.5.2" data-path="07-bayes1.html"><a href="07-bayes1.html#mathematics-of-probability"><i class="fa fa-check"></i><b>7.5.2</b> Mathematics of probability</a></li>
<li class="chapter" data-level="7.5.3" data-path="07-bayes1.html"><a href="07-bayes1.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.5.3</b> Interpretations of probability</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="07-bayes1.html"><a href="07-bayes1.html#historical-developments"><i class="fa fa-check"></i><b>7.6</b> Historical developments</a></li>
<li class="chapter" data-level="7.7" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning"><i class="fa fa-check"></i><b>7.7</b> Connection with entropy learning</a><ul>
<li class="chapter" data-level="7.7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#zero-forcing-property"><i class="fa fa-check"></i><b>7.7.1</b> Zero forcing property</a></li>
<li class="chapter" data-level="7.7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning-1"><i class="fa fa-check"></i><b>7.7.2</b> Connection with entropy learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="08-bayes2.html"><a href="08-bayes2.html"><i class="fa fa-check"></i><b>8</b> Beta-Binomial model for estimating a proportion</a><ul>
<li class="chapter" data-level="8.1" data-path="08-bayes2.html"><a href="08-bayes2.html#binomial-likelihood"><i class="fa fa-check"></i><b>8.1</b> Binomial likelihood</a></li>
<li class="chapter" data-level="8.2" data-path="08-bayes2.html"><a href="08-bayes2.html#excursion-properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>8.2</b> Excursion: Properties of the Beta distribution</a></li>
<li class="chapter" data-level="8.3" data-path="08-bayes2.html"><a href="08-bayes2.html#beta-prior-distribution"><i class="fa fa-check"></i><b>8.3</b> Beta prior distribution</a></li>
<li class="chapter" data-level="8.4" data-path="08-bayes2.html"><a href="08-bayes2.html#computing-the-posterior-distribution"><i class="fa fa-check"></i><b>8.4</b> Computing the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="09-bayes3.html"><a href="09-bayes3.html"><i class="fa fa-check"></i><b>9</b> Properties of Bayesian learning</a><ul>
<li class="chapter" data-level="9.1" data-path="09-bayes3.html"><a href="09-bayes3.html#prior-acting-as-pseudo-data"><i class="fa fa-check"></i><b>9.1</b> Prior acting as pseudo-data</a></li>
<li class="chapter" data-level="9.2" data-path="09-bayes3.html"><a href="09-bayes3.html#linear-shrinkage-of-mean"><i class="fa fa-check"></i><b>9.2</b> Linear shrinkage of mean</a></li>
<li class="chapter" data-level="9.3" data-path="09-bayes3.html"><a href="09-bayes3.html#conjugacy-of-prior-and-posterior-distribution"><i class="fa fa-check"></i><b>9.3</b> Conjugacy of prior and posterior distribution</a></li>
<li class="chapter" data-level="9.4" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-asymptotics"><i class="fa fa-check"></i><b>9.4</b> Large sample asymptotics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-limits-of-mean-and-variance"><i class="fa fa-check"></i><b>9.4.1</b> Large sample limits of mean and variance</a></li>
<li class="chapter" data-level="9.4.2" data-path="09-bayes3.html"><a href="09-bayes3.html#asymptotic-normality-of-the-posterior-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Asymptotic Normality of the Posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="09-bayes3.html"><a href="09-bayes3.html#posterior-variance-for-finite-n"><i class="fa fa-check"></i><b>9.5</b> Posterior variance for finite <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-bayes4.html"><a href="10-bayes4.html"><i class="fa fa-check"></i><b>10</b> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a><ul>
<li class="chapter" data-level="10.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-normal-model-to-estimate-mean"><i class="fa fa-check"></i><b>10.1</b> Normal-Normal model to estimate mean</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihood"><i class="fa fa-check"></i><b>10.1.1</b> Normal likelihood</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-prior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Normal prior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-posterior-distribution"><i class="fa fa-check"></i><b>10.1.3</b> Normal posterior distribution</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-and-stein-paradox"><i class="fa fa-check"></i><b>10.1.4</b> Large sample asymptotics and Stein paradox</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-normal-model-to-estimate-variance"><i class="fa fa-check"></i><b>10.2</b> Inverse-Gamma-Normal model to estimate variance</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Inverse Gamma distribution</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihoood"><i class="fa fa-check"></i><b>10.2.2</b> Normal likelihoood</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-prior-distribution"><i class="fa fa-check"></i><b>10.2.3</b> Inverse Gamma prior distribution</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-posterior-distribution"><i class="fa fa-check"></i><b>10.2.4</b> Inverse Gamma posterior distribution</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-1"><i class="fa fa-check"></i><b>10.2.5</b> Large sample asymptotics</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-bayes4.html"><a href="10-bayes4.html#estimating-precision"><i class="fa fa-check"></i><b>10.2.6</b> Estimating precision</a></li>
<li class="chapter" data-level="10.2.7" data-path="10-bayes4.html"><a href="10-bayes4.html#joint-estimation-of-mean-and-variance"><i class="fa fa-check"></i><b>10.2.7</b> Joint estimation of mean and variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-bayes5.html"><a href="11-bayes5.html"><i class="fa fa-check"></i><b>11</b> Shrinkage estimation using empirical risk minimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="11-bayes5.html"><a href="11-bayes5.html#linear-shrinkage"><i class="fa fa-check"></i><b>11.1</b> Linear shrinkage</a></li>
<li class="chapter" data-level="11.2" data-path="11-bayes5.html"><a href="11-bayes5.html#james-stein-estimator"><i class="fa fa-check"></i><b>11.2</b> James-Stein estimator</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-bayes6.html"><a href="12-bayes6.html"><i class="fa fa-check"></i><b>12</b> Bayesian model comparison using Bayes factors and the BIC</a><ul>
<li class="chapter" data-level="12.1" data-path="12-bayes6.html"><a href="12-bayes6.html#the-bayes-factor"><i class="fa fa-check"></i><b>12.1</b> The Bayes factor</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-bayes6.html"><a href="12-bayes6.html#connection-with-relative-entropy"><i class="fa fa-check"></i><b>12.1.1</b> Connection with relative entropy</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-bayes6.html"><a href="12-bayes6.html#interpretation-of-and-scale-for-bayes-factor"><i class="fa fa-check"></i><b>12.1.2</b> Interpretation of and scale for Bayes factor</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-bayes6.html"><a href="12-bayes6.html#computing-textprd-m-for-simple-and-composite-models"><i class="fa fa-check"></i><b>12.1.3</b> Computing <span class="math inline">\(\text{Pr}(D | M)\)</span> for simple and composite models</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-bayes6.html"><a href="12-bayes6.html#bayes-factor-versus-likelihood-ratio"><i class="fa fa-check"></i><b>12.1.4</b> Bayes factor versus likelihood ratio</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-bayes6.html"><a href="12-bayes6.html#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><i class="fa fa-check"></i><b>12.2</b> Approximate computation of the marginal likelihood and of the log-Bayes factor</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-bayes6.html"><a href="12-bayes6.html#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fa fa-check"></i><b>12.2.1</b> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-bayes6.html"><a href="12-bayes6.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>12.2.2</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-bayes6.html"><a href="12-bayes6.html#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fa fa-check"></i><b>12.2.3</b> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-bayes6.html"><a href="12-bayes6.html#model-complexity-and-occams-razor"><i class="fa fa-check"></i><b>12.2.4</b> Model complexity and Occams razor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-bayes7.html"><a href="13-bayes7.html"><i class="fa fa-check"></i><b>13</b> False discovery rates</a><ul>
<li class="chapter" data-level="13.1" data-path="13-bayes7.html"><a href="13-bayes7.html#general-setup"><i class="fa fa-check"></i><b>13.1</b> General setup</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-bayes7.html"><a href="13-bayes7.html#overview-1"><i class="fa fa-check"></i><b>13.1.1</b> Overview</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-bayes7.html"><a href="13-bayes7.html#choosing-between-h_0-and-h_a"><i class="fa fa-check"></i><b>13.1.2</b> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span></a></li>
<li class="chapter" data-level="13.1.3" data-path="13-bayes7.html"><a href="13-bayes7.html#true-and-false-positives-and-negatives"><i class="fa fa-check"></i><b>13.1.3</b> True and false positives and negatives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-bayes7.html"><a href="13-bayes7.html#specificity-and-sensitivity"><i class="fa fa-check"></i><b>13.2</b> Specificity and Sensitivity</a></li>
<li class="chapter" data-level="13.3" data-path="13-bayes7.html"><a href="13-bayes7.html#fdr-and-fndr"><i class="fa fa-check"></i><b>13.3</b> FDR and FNDR</a></li>
<li class="chapter" data-level="13.4" data-path="13-bayes7.html"><a href="13-bayes7.html#bayesian-perspective"><i class="fa fa-check"></i><b>13.4</b> Bayesian perspective</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-bayes7.html"><a href="13-bayes7.html#two-component-mixture-model"><i class="fa fa-check"></i><b>13.4.1</b> Two component mixture model</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-bayes7.html"><a href="13-bayes7.html#local-fdr"><i class="fa fa-check"></i><b>13.4.2</b> Local FDR</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-bayes7.html"><a href="13-bayes7.html#q-values"><i class="fa fa-check"></i><b>13.4.3</b> q-values</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="13-bayes7.html"><a href="13-bayes7.html#software"><i class="fa fa-check"></i><b>13.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-bayes8.html"><a href="14-bayes8.html"><i class="fa fa-check"></i><b>14</b> Optimality properties and summary</a><ul>
<li class="chapter" data-level="14.1" data-path="14-bayes8.html"><a href="14-bayes8.html#bayesian-statistics-in-a-nutshell"><i class="fa fa-check"></i><b>14.1</b> Bayesian statistics in a nutshell</a><ul>
<li class="chapter" data-level="14.1.1" data-path="14-bayes8.html"><a href="14-bayes8.html#remarks"><i class="fa fa-check"></i><b>14.1.1</b> Remarks</a></li>
<li class="chapter" data-level="14.1.2" data-path="14-bayes8.html"><a href="14-bayes8.html#advantages"><i class="fa fa-check"></i><b>14.1.2</b> Advantages</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="14-bayes8.html"><a href="14-bayes8.html#frequentist-properties-of-bayesian-estimators"><i class="fa fa-check"></i><b>14.2</b> Frequentist properties of Bayesian estimators</a></li>
<li class="chapter" data-level="14.3" data-path="14-bayes8.html"><a href="14-bayes8.html#specifying-the-prior-problem-or-advantage"><i class="fa fa-check"></i><b>14.3</b> Specifying the prior — problem or advantage?</a></li>
<li class="chapter" data-level="14.4" data-path="14-bayes8.html"><a href="14-bayes8.html#choosing-a-prior"><i class="fa fa-check"></i><b>14.4</b> Choosing a prior</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-bayes8.html"><a href="14-bayes8.html#some-guidelines"><i class="fa fa-check"></i><b>14.4.1</b> Some guidelines</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-bayes8.html"><a href="14-bayes8.html#jeffreys-prior"><i class="fa fa-check"></i><b>14.4.2</b> Jeffreys prior</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-bayes8.html"><a href="14-bayes8.html#optimality-of-bayes-inference"><i class="fa fa-check"></i><b>14.5</b> Optimality of Bayes inference</a></li>
<li class="chapter" data-level="14.6" data-path="14-bayes8.html"><a href="14-bayes8.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a><ul>
<li class="chapter" data-level="14.6.1" data-path="14-bayes8.html"><a href="14-bayes8.html#current-research"><i class="fa fa-check"></i><b>14.6.1</b> Current research</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Regression</b></span></li>
<li class="chapter" data-level="15" data-path="15-regression1.html"><a href="15-regression1.html"><i class="fa fa-check"></i><b>15</b> Overview over regression modelling</a><ul>
<li class="chapter" data-level="15.1" data-path="15-regression1.html"><a href="15-regression1.html#general-setup-1"><i class="fa fa-check"></i><b>15.1</b> General setup</a></li>
<li class="chapter" data-level="15.2" data-path="15-regression1.html"><a href="15-regression1.html#objectives"><i class="fa fa-check"></i><b>15.2</b> Objectives</a></li>
<li class="chapter" data-level="15.3" data-path="15-regression1.html"><a href="15-regression1.html#regression-as-a-form-of-supervised-learning"><i class="fa fa-check"></i><b>15.3</b> Regression as a form of supervised learning</a></li>
<li class="chapter" data-level="15.4" data-path="15-regression1.html"><a href="15-regression1.html#various-regression-models-used-in-statistics"><i class="fa fa-check"></i><b>15.4</b> Various regression models used in statistics</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="16-regression2.html"><a href="16-regression2.html"><i class="fa fa-check"></i><b>16</b> Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="16-regression2.html"><a href="16-regression2.html#the-linear-regression-model"><i class="fa fa-check"></i><b>16.1</b> The linear regression model</a></li>
<li class="chapter" data-level="16.2" data-path="16-regression2.html"><a href="16-regression2.html#interpretation-of-regression-coefficients-and-intercept"><i class="fa fa-check"></i><b>16.2</b> Interpretation of regression coefficients and intercept</a></li>
<li class="chapter" data-level="16.3" data-path="16-regression2.html"><a href="16-regression2.html#different-types-of-linear-regression"><i class="fa fa-check"></i><b>16.3</b> Different types of linear regression:</a></li>
<li class="chapter" data-level="16.4" data-path="16-regression2.html"><a href="16-regression2.html#distributional-assumptions-and-properties"><i class="fa fa-check"></i><b>16.4</b> Distributional assumptions and properties</a></li>
<li class="chapter" data-level="16.5" data-path="16-regression2.html"><a href="16-regression2.html#regression-in-data-matrix-notation"><i class="fa fa-check"></i><b>16.5</b> Regression in data matrix notation</a></li>
<li class="chapter" data-level="16.6" data-path="16-regression2.html"><a href="16-regression2.html#centering-and-vanishing-of-the-intercept-beta_0"><i class="fa fa-check"></i><b>16.6</b> Centering and vanishing of the intercept <span class="math inline">\(\beta_0\)</span></a></li>
<li class="chapter" data-level="16.7" data-path="16-regression2.html"><a href="16-regression2.html#regression-objectives-for-linear-model"><i class="fa fa-check"></i><b>16.7</b> Regression objectives for linear model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="17-regression3.html"><a href="17-regression3.html"><i class="fa fa-check"></i><b>17</b> Essential multivariate statistics</a><ul>
<li class="chapter" data-level="17.1" data-path="17-regression3.html"><a href="17-regression3.html#notation"><i class="fa fa-check"></i><b>17.1</b> Notation</a></li>
<li class="chapter" data-level="17.2" data-path="17-regression3.html"><a href="17-regression3.html#univariate-random-variables"><i class="fa fa-check"></i><b>17.2</b> Univariate random variables</a></li>
<li class="chapter" data-level="17.3" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-random-vector-of-dimension-d"><i class="fa fa-check"></i><b>17.3</b> Multivariate random vector of dimension <span class="math inline">\(d\)</span></a></li>
<li class="chapter" data-level="17.4" data-path="17-regression3.html"><a href="17-regression3.html#moments-of-a-random-vector"><i class="fa fa-check"></i><b>17.4</b> Moments of a random vector</a></li>
<li class="chapter" data-level="17.5" data-path="17-regression3.html"><a href="17-regression3.html#definition-of-variance-for-univariate-random-variable"><i class="fa fa-check"></i><b>17.5</b> Definition of variance for univariate random variable:</a></li>
<li class="chapter" data-level="17.6" data-path="17-regression3.html"><a href="17-regression3.html#definition-of-variance-of-a-random-vector"><i class="fa fa-check"></i><b>17.6</b> Definition of variance of a random vector:</a></li>
<li class="chapter" data-level="17.7" data-path="17-regression3.html"><a href="17-regression3.html#correlation-matrix-boldsymbol-p-or-standardised-covariance-matrix"><i class="fa fa-check"></i><b>17.7</b> Correlation Matrix <span class="math inline">\(\boldsymbol P\)</span> or Standardised Covariance Matrix</a></li>
<li class="chapter" data-level="17.8" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>17.8</b> Multivariate Normal Distribution</a><ul>
<li class="chapter" data-level="17.8.1" data-path="17-regression3.html"><a href="17-regression3.html#univariate-normal-model"><i class="fa fa-check"></i><b>17.8.1</b> Univariate Normal Model</a></li>
<li class="chapter" data-level="17.8.2" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-normal-model"><i class="fa fa-check"></i><b>17.8.2</b> Multivariate Normal Model</a></li>
</ul></li>
<li class="chapter" data-level="17.9" data-path="17-regression3.html"><a href="17-regression3.html#maximum-likelihood-estimates"><i class="fa fa-check"></i><b>17.9</b> Maximum likelihood estimates</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="18-regression4.html"><a href="18-regression4.html"><i class="fa fa-check"></i><b>18</b> Estimating regression coefficients</a><ul>
<li class="chapter" data-level="18.1" data-path="18-regression4.html"><a href="18-regression4.html#ordinary-least-squares-ols-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>18.1</b> Ordinary Least Squares (OLS) estimator of regression coefficients</a></li>
<li class="chapter" data-level="18.2" data-path="18-regression4.html"><a href="18-regression4.html#maximum-likelihood-estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>18.2</b> Maximum likelihood estimation of regression coefficients</a><ul>
<li class="chapter" data-level="18.2.1" data-path="18-regression4.html"><a href="18-regression4.html#detailed-derivation-of-the-mles"><i class="fa fa-check"></i><b>18.2.1</b> Detailed derivation of the MLEs</a></li>
<li class="chapter" data-level="18.2.2" data-path="18-regression4.html"><a href="18-regression4.html#asymptotics"><i class="fa fa-check"></i><b>18.2.2</b> Asymptotics</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="18-regression4.html"><a href="18-regression4.html#covariance-plug-in-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>18.3</b> Covariance plug-in estimator of regression coefficients</a><ul>
<li class="chapter" data-level="18.3.1" data-path="18-regression4.html"><a href="18-regression4.html#importance-of-positive-definiteness-of-estimated-covariance-matrix"><i class="fa fa-check"></i><b>18.3.1</b> Importance of positive definiteness of estimated covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="18-regression4.html"><a href="18-regression4.html#best-linear-predictor"><i class="fa fa-check"></i><b>18.4</b> Best linear predictor</a><ul>
<li class="chapter" data-level="18.4.1" data-path="18-regression4.html"><a href="18-regression4.html#result"><i class="fa fa-check"></i><b>18.4.1</b> Result:</a></li>
<li class="chapter" data-level="18.4.2" data-path="18-regression4.html"><a href="18-regression4.html#irreducible-error"><i class="fa fa-check"></i><b>18.4.2</b> Irreducible Error</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="18-regression4.html"><a href="18-regression4.html#regression-by-conditioning"><i class="fa fa-check"></i><b>18.5</b> Regression by conditioning</a><ul>
<li class="chapter" data-level="18.5.1" data-path="18-regression4.html"><a href="18-regression4.html#general-idea"><i class="fa fa-check"></i><b>18.5.1</b> General idea:</a></li>
<li class="chapter" data-level="18.5.2" data-path="18-regression4.html"><a href="18-regression4.html#multivariate-normal-assumption"><i class="fa fa-check"></i><b>18.5.2</b> Multivariate normal assumption</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="18-regression4.html"><a href="18-regression4.html#standardised-regression-coefficients-and-relationship-to-correlation"><i class="fa fa-check"></i><b>18.6</b> Standardised regression coefficients and relationship to correlation</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="19-regression5.html"><a href="19-regression5.html"><i class="fa fa-check"></i><b>19</b> Squared multiple correlation and variance decomposition in linear regression</a><ul>
<li class="chapter" data-level="19.1" data-path="19-regression5.html"><a href="19-regression5.html#squared-multiple-correlation-omega2-and-the-r2-coefficient"><i class="fa fa-check"></i><b>19.1</b> Squared multiple correlation <span class="math inline">\(\Omega^2\)</span> and the <span class="math inline">\(R^2\)</span> coefficient</a><ul>
<li class="chapter" data-level="19.1.1" data-path="19-regression5.html"><a href="19-regression5.html#estimation-of-omega2-and-the-multiple-r2-coefficient"><i class="fa fa-check"></i><b>19.1.1</b> Estimation of <span class="math inline">\(\Omega^2\)</span> and the multiple <span class="math inline">\(R^2\)</span> coefficient</a></li>
<li class="chapter" data-level="19.1.2" data-path="19-regression5.html"><a href="19-regression5.html#r-output"><i class="fa fa-check"></i><b>19.1.2</b> R output</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="19-regression5.html"><a href="19-regression5.html#variance-decomposition-in-regression"><i class="fa fa-check"></i><b>19.2</b> Variance decomposition in regression</a><ul>
<li class="chapter" data-level="19.2.1" data-path="19-regression5.html"><a href="19-regression5.html#law-of-total-variance-and-variance-decomposition"><i class="fa fa-check"></i><b>19.2.1</b> Law of total variance and variance decomposition</a></li>
<li class="chapter" data-level="19.2.2" data-path="19-regression5.html"><a href="19-regression5.html#related-quantities"><i class="fa fa-check"></i><b>19.2.2</b> Related quantities</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="19-regression5.html"><a href="19-regression5.html#sample-version-of-variance-decomposition"><i class="fa fa-check"></i><b>19.3</b> Sample version of variance decomposition</a><ul>
<li class="chapter" data-level="19.3.1" data-path="19-regression5.html"><a href="19-regression5.html#geometric-interpretation-of-regression-as-orthogonal-projection"><i class="fa fa-check"></i><b>19.3.1</b> Geometric interpretation of regression as orthogonal projection:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="20-regression6.html"><a href="20-regression6.html"><i class="fa fa-check"></i><b>20</b> Prediction and variable selection</a><ul>
<li class="chapter" data-level="20.1" data-path="20-regression6.html"><a href="20-regression6.html#prediction-and-prediction-intervals"><i class="fa fa-check"></i><b>20.1</b> Prediction and prediction intervals</a></li>
<li class="chapter" data-level="20.2" data-path="20-regression6.html"><a href="20-regression6.html#variable-importance-and-prediction"><i class="fa fa-check"></i><b>20.2</b> Variable importance and prediction</a><ul>
<li class="chapter" data-level="20.2.1" data-path="20-regression6.html"><a href="20-regression6.html#how-to-quantify-variable-importance"><i class="fa fa-check"></i><b>20.2.1</b> How to quantify variable importance?</a></li>
<li class="chapter" data-level="20.2.2" data-path="20-regression6.html"><a href="20-regression6.html#some-candidates-for-vims"><i class="fa fa-check"></i><b>20.2.2</b> Some candidates for VIMs</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="20-regression6.html"><a href="20-regression6.html#regression-t-scores."><i class="fa fa-check"></i><b>20.3</b> Regression <span class="math inline">\(t\)</span>-scores.</a><ul>
<li class="chapter" data-level="20.3.1" data-path="20-regression6.html"><a href="20-regression6.html#computation"><i class="fa fa-check"></i><b>20.3.1</b> Computation</a></li>
<li class="chapter" data-level="20.3.2" data-path="20-regression6.html"><a href="20-regression6.html#connection-with-partial-correlation"><i class="fa fa-check"></i><b>20.3.2</b> Connection with partial correlation</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="20-regression6.html"><a href="20-regression6.html#further-approaches-for-variable-selection"><i class="fa fa-check"></i><b>20.4</b> Further approaches for variable selection</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="21-refresher.html"><a href="21-refresher.html"><i class="fa fa-check"></i><b>A</b> Refresher</a><ul>
<li class="chapter" data-level="A.1" data-path="21-refresher.html"><a href="21-refresher.html#vectors-and-matrices"><i class="fa fa-check"></i><b>A.1</b> Vectors and matrices</a></li>
<li class="chapter" data-level="A.2" data-path="21-refresher.html"><a href="21-refresher.html#functions"><i class="fa fa-check"></i><b>A.2</b> Functions</a><ul>
<li class="chapter" data-level="A.2.1" data-path="21-refresher.html"><a href="21-refresher.html#gradient"><i class="fa fa-check"></i><b>A.2.1</b> Gradient</a></li>
<li class="chapter" data-level="A.2.2" data-path="21-refresher.html"><a href="21-refresher.html#hessian-matrix"><i class="fa fa-check"></i><b>A.2.2</b> Hessian matrix</a></li>
<li class="chapter" data-level="A.2.3" data-path="21-refresher.html"><a href="21-refresher.html#conditions-for-local-maximum-of-a-function"><i class="fa fa-check"></i><b>A.2.3</b> Conditions for local maximum of a function</a></li>
<li class="chapter" data-level="A.2.4" data-path="21-refresher.html"><a href="21-refresher.html#linear-and-quadratic-approximation-taylor-series"><i class="fa fa-check"></i><b>A.2.4</b> Linear and quadratic approximation (Taylor series)</a></li>
<li class="chapter" data-level="A.2.5" data-path="21-refresher.html"><a href="21-refresher.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.2.5</b> Functions of matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="21-refresher.html"><a href="21-refresher.html#probability"><i class="fa fa-check"></i><b>A.3</b> Probability</a><ul>
<li class="chapter" data-level="A.3.1" data-path="21-refresher.html"><a href="21-refresher.html#law-of-large-numbers"><i class="fa fa-check"></i><b>A.3.1</b> Law of large numbers:</a></li>
<li class="chapter" data-level="A.3.2" data-path="21-refresher.html"><a href="21-refresher.html#jensens-inequality"><i class="fa fa-check"></i><b>A.3.2</b> Jensen’s inequality</a></li>
<li class="chapter" data-level="A.3.3" data-path="21-refresher.html"><a href="21-refresher.html#transformation-of-univariate-densities"><i class="fa fa-check"></i><b>A.3.3</b> Transformation of univariate densities</a></li>
<li class="chapter" data-level="A.3.4" data-path="21-refresher.html"><a href="21-refresher.html#normal-distribution"><i class="fa fa-check"></i><b>A.3.4</b> Normal distribution</a></li>
<li class="chapter" data-level="A.3.5" data-path="21-refresher.html"><a href="21-refresher.html#chi-square-distribution"><i class="fa fa-check"></i><b>A.3.5</b> Chi-square distribution</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="21-refresher.html"><a href="21-refresher.html#statistics"><i class="fa fa-check"></i><b>A.4</b> Statistics</a><ul>
<li class="chapter" data-level="A.4.1" data-path="21-refresher.html"><a href="21-refresher.html#statistical-learning"><i class="fa fa-check"></i><b>A.4.1</b> Statistical learning</a></li>
<li class="chapter" data-level="A.4.2" data-path="21-refresher.html"><a href="21-refresher.html#point-and-interval-estimation"><i class="fa fa-check"></i><b>A.4.2</b> Point and interval estimation</a></li>
<li class="chapter" data-level="A.4.3" data-path="21-refresher.html"><a href="21-refresher.html#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fa fa-check"></i><b>A.4.3</b> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span></a></li>
<li class="chapter" data-level="A.4.4" data-path="21-refresher.html"><a href="21-refresher.html#asymptotics-1"><i class="fa fa-check"></i><b>A.4.4</b> Asymptotics</a></li>
<li class="chapter" data-level="A.4.5" data-path="21-refresher.html"><a href="21-refresher.html#confidence-intervals"><i class="fa fa-check"></i><b>A.4.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="A.4.6" data-path="21-refresher.html"><a href="21-refresher.html#symmetric-normal-confidence-interval"><i class="fa fa-check"></i><b>A.4.6</b> Symmetric normal confidence interval</a></li>
<li class="chapter" data-level="A.4.7" data-path="21-refresher.html"><a href="21-refresher.html#chi-square-confidence-interval"><i class="fa fa-check"></i><b>A.4.7</b> Chi-square confidence interval</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="22-further-study.html"><a href="22-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="22-further-study.html"><a href="22-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="22-further-study.html"><a href="22-further-study.html#additional-references"><i class="fa fa-check"></i><b>B.2</b> Additional references</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="23-references.html"><a href="23-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Statistical Methods:<br />
Likelihood, Bayes and Regression</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="false-discovery-rates" class="section level1">
<h1><span class="header-section-number">13</span> False discovery rates</h1>
<div id="general-setup" class="section level2">
<h2><span class="header-section-number">13.1</span> General setup</h2>
<div id="overview-1" class="section level3">
<h3><span class="header-section-number">13.1.1</span> Overview</h3>
<p>In this chapter we introduce False Discovery Rates (FDR) as a Bayesian method to
distinguish a null model from an alternative model. This is closely linked with classical
frequentist multiple testing procedures.</p>
</div>
<div id="choosing-between-h_0-and-h_a" class="section level3">
<h3><span class="header-section-number">13.1.2</span> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span></h3>
<p>We consider two models:</p>
<p><span class="math inline">\(H_0:\)</span> null model, with density <span class="math inline">\(f_0(x)\)</span> and distribution <span class="math inline">\(F_0(x)\)</span></p>
<p><span class="math inline">\(H_A:\)</span> alternative model, with density <span class="math inline">\(f_A(x)\)</span> and distribution <span class="math inline">\(F_A(x)\)</span></p>
<p>Aim: given observations <span class="math inline">\(x_1, \ldots, x_n\)</span> we would like to decide for each <span class="math inline">\(x_i\)</span> whether
it belongs to <span class="math inline">\(H_0\)</span> or <span class="math inline">\(H_A\)</span>.</p>
<p>This is done by a critical decision threshold <span class="math inline">\(x_c\)</span>: if <span class="math inline">\(x_i &gt; x_c\)</span> then <span class="math inline">\(x_i\)</span> is called “significant” and otherwise called “not significant”.</p>
<p>In classical statistics one of the the most widely used approach to find the decision threshold is by computing <span class="math inline">\(p\)</span>-values from the <span class="math inline">\(x_i\)</span>
(this uses only the null model but not the alternative model), and then thresholding the <span class="math inline">\(p\)</span>-values a a certain level (say 5%). If <span class="math inline">\(n\)</span> is large then often the test is modified by adjusting the <span class="math inline">\(p\)</span>-values or the threshold (e.g. if Bonferroni correction).</p>
<p>Note that this procedure ignores any information we may have about the alternative model!</p>
</div>
<div id="true-and-false-positives-and-negatives" class="section level3">
<h3><span class="header-section-number">13.1.3</span> True and false positives and negatives</h3>
<p>For any decision threshold <span class="math inline">\(x_c\)</span> we can distinguish the following errors:</p>
<ul>
<li>False positives (FP), “false alarm”, type I error: <span class="math inline">\(x_i\)</span> belongs to null but is called “significant”</li>
<li>False negative (FN), “miss”, type II error: <span class="math inline">\(x_i\)</span> belongs to alternative, but is called “not significant”</li>
</ul>
<p>In addition we have:</p>
<ul>
<li>True positives (TP), “hits”: belongs to alternative and is called “significant”</li>
<li>True negatives (TN), “correct rejections”: belongs to null and is called “not significant”</li>
</ul>
</div>
</div>
<div id="specificity-and-sensitivity" class="section level2">
<h2><span class="header-section-number">13.2</span> Specificity and Sensitivity</h2>
<p>From counts of TP, TN, FN, FP we can derive further quantities:</p>
<ul>
<li>True Negative Rate TNR, <strong>specificity</strong>: <span class="math inline">\(TNR= \frac{TN}{TN+FP} = 1- FPR\)</span> with FPR=False Positive Rate = <span class="math inline">\(1-\alpha_I\)</span></li>
<li><p>True Positive Rate TPR, <strong>sensitivity</strong>, <strong>power</strong>, recall: <span class="math inline">\(TNR= \frac{TP}{TP+FN} = 1- FNR\)</span> with FNR=False negative rate = <span class="math inline">\(1-\alpha_{II}\)</span></p></li>
<li><p>Accuracy: <span class="math inline">\(ACC = \frac{TP+TN}{TP+TN+FP+FN}\)</span></p></li>
</ul>
<p>Another common way to choose the decision threshold <span class="math inline">\(x_d\)</span> in classical statistics is to balance sensitivity/power vs. specificity (maximising both power and specificity, or equivalently, minimising both false positive and false negative rates). ROC curves plot TPR/sensitivity vs. FPR = 1-specificity.</p>
</div>
<div id="fdr-and-fndr" class="section level2">
<h2><span class="header-section-number">13.3</span> FDR and FNDR</h2>
<p>It is possible to link the above with the observed counts of TP, FP, TN, FN:</p>
<ul>
<li>False Discovery Rate (FDR): <span class="math inline">\(FDR = \frac{FP}{FP+TP}\)</span></li>
<li>False Nondiscovery Rate (FNDR): <span class="math inline">\(FNDR = \frac{FN}{TN+FN}\)</span></li>
<li>Positive predictive value (PPV), True Discovery Rate (TDR), precision: <span class="math inline">\(PPV = \frac{TP}{FP+TP} = 1-FDR\)</span></li>
<li>Negative predictive value (NPV): <span class="math inline">\(NPV = \frac{TN}{TN+FN} = 1-FNDR\)</span></li>
</ul>
<p>In order to choose the decision threshold it is natural to balance FDR and FDNR (or PPV and NPV), by minimising both FDR and FNDR or maximising both PPV and NPV.</p>
<p>In machine learning it is common to use “precision-recall plots” that plot precision (=PPV, TDR)
vs. recall (=power, sensitivity).</p>
</div>
<div id="bayesian-perspective" class="section level2">
<h2><span class="header-section-number">13.4</span> Bayesian perspective</h2>
<div id="two-component-mixture-model" class="section level3">
<h3><span class="header-section-number">13.4.1</span> Two component mixture model</h3>
<p>In the Bayesian perspective the problem of choosing the decision threshold is related to computing the posterior probability
<span class="math display">\[\text{Pr}(H_0 | x_i) , \]</span>
i.e. probability of the null model given the observation <span class="math inline">\(x_i\)</span>, or equivalently
computing
<span class="math display">\[\text{Pr}(H_A | x_i) = 1- \text{Pr}(H_0 | x_i)\]</span>
the probability of the alternative model given the observation <span class="math inline">\(x_i\)</span>.</p>
<p>This is done by assuming a mixture model
<span class="math display">\[
f(x) = \pi_0 f_0(x) + (1-\pi_0) f_A(x)
\]</span>
where <span class="math inline">\(\pi_0 = \text{Pr}(H_0)\)</span> is the prior probability of <span class="math inline">\(H_0\)</span> and.
<span class="math inline">\(\pi_A = 1- \pi_0 = \text{Pr}(H_A)\)</span> the prior probabiltiy of <span class="math inline">\(H_A\)</span>.</p>
<p>Note that the weights <span class="math inline">\(\pi_0\)</span> can in fact be estimated from the observations by fitting the mixture distribution
to the observations <span class="math inline">\(x_1, \ldots, x_n\)</span> (which implies that this yields a form of empirical Bayes method).</p>
</div>
<div id="local-fdr" class="section level3">
<h3><span class="header-section-number">13.4.2</span> Local FDR</h3>
<p>The posterior probability of the null model given a data point is then given by
<span class="math display">\[\text{Pr}(H_0 | x_i) = \frac{\pi_0 f_0(x_i)}{f(x_i)} = LFDR(x_i)\]</span>
This quantity is also known as the <strong>local FDR</strong> or <strong>local False Discovery Rate</strong>.</p>
<p>In the given one-sided setup the local FDR is large (close to 1) for small <span class="math inline">\(x\)</span>, and
will become close to 0 for large <span class="math inline">\(x\)</span>. A common decision rule is given by thresholding
local false discovery rates: if <span class="math inline">\(LFDR(x_i) &lt; 0.1\)</span> the <span class="math inline">\(x_i\)</span> is called significant.</p>
</div>
<div id="q-values" class="section level3">
<h3><span class="header-section-number">13.4.3</span> q-values</h3>
<p>In correspondence to <span class="math inline">\(p\)</span>-values one can also define tail-area based false discovery rates:
<span class="math display">\[
Fdr(x_i) = \text{Pr}(H_0 | X &gt; x_i) = \frac{\pi_0 F_0(x_i)}{F(x_i)} 
\]</span></p>
<p>These are called <strong>q-values</strong>, or simply <strong>False Discovery Rates (FDR)</strong>. Intriguingly, these also have a frequentist
interpretation as adjusted p-values (using a Benjamini-Hochberg adjustment procedure).</p>
</div>
</div>
<div id="software" class="section level2">
<h2><span class="header-section-number">13.5</span> Software</h2>
<p>There are a number of R packages to compute (local) FDR values:</p>
<p>For example:</p>
<ul>
<li>locfdr</li>
<li>qvalue</li>
<li>fdrtool</li>
</ul>
<p>and many more.</p>
<p>Using FDR values for screening is especially useful in high-dimensional settings
(e.g. when analysing genomic and other high-throughput data).</p>
<p>FDR values have both a Bayesian as well as frequentist interpretation, providing further evidence that
good classical statistical methods do have a Bayesian interpretation.</p>

<p></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="12-bayes6.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="14-bayes8.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math20802-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
