<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Quadratic approximation and normal asymptotics | HTML</title>
  <meta name="description" content="Statistical Methods:<br />
Likelihood, Bayes and Regression</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Quadratic approximation and normal asymptotics | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Quadratic approximation and normal asymptotics | HTML" />
  
  
  



<meta name="date" content="2021-03-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="03-likelihood3.html"/>
<link rel="next" href="05-likelihood5.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH20802/index.html">MATH20802 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Likelihood estimation and inference</b></span></li>
<li class="chapter" data-level="1" data-path="01-likelihood1.html"><a href="01-likelihood1.html"><i class="fa fa-check"></i><b>1</b> Overview of statistical learning</a><ul>
<li class="chapter" data-level="1.1" data-path="01-likelihood1.html"><a href="01-likelihood1.html#how-to-learn-from-data"><i class="fa fa-check"></i><b>1.1</b> How to learn from data?</a></li>
<li class="chapter" data-level="1.2" data-path="01-likelihood1.html"><a href="01-likelihood1.html#probability-theory-versus-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Probability theory versus statistical learning</a></li>
<li class="chapter" data-level="1.3" data-path="01-likelihood1.html"><a href="01-likelihood1.html#cartoon-of-statistical-learning"><i class="fa fa-check"></i><b>1.3</b> Cartoon of statistical learning</a></li>
<li class="chapter" data-level="1.4" data-path="01-likelihood1.html"><a href="01-likelihood1.html#likelihood"><i class="fa fa-check"></i><b>1.4</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="02-likelihood2.html"><a href="02-likelihood2.html"><i class="fa fa-check"></i><b>2</b> From entropy to maximum likelihood</a><ul>
<li class="chapter" data-level="2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy"><i class="fa fa-check"></i><b>2.1</b> Entropy</a><ul>
<li class="chapter" data-level="2.1.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#categorical-distribution"><i class="fa fa-check"></i><b>2.1.2</b> Categorical distribution</a></li>
<li class="chapter" data-level="2.1.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#surprise-surprisal-or-shannon-information"><i class="fa fa-check"></i><b>2.1.3</b> Surprise, surprisal or Shannon information</a></li>
<li class="chapter" data-level="2.1.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#shannon-entropy"><i class="fa fa-check"></i><b>2.1.4</b> Shannon entropy</a></li>
<li class="chapter" data-level="2.1.5" data-path="02-likelihood2.html"><a href="02-likelihood2.html#differential-entropy"><i class="fa fa-check"></i><b>2.1.5</b> Differential entropy</a></li>
<li class="chapter" data-level="2.1.6" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-entropy-principle-to-characterise-distributions"><i class="fa fa-check"></i><b>2.1.6</b> Maximum entropy principle to characterise distributions</a></li>
<li class="chapter" data-level="2.1.7" data-path="02-likelihood2.html"><a href="02-likelihood2.html#cross-entropy"><i class="fa fa-check"></i><b>2.1.7</b> Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>2.2</b> Kullback-Leibler divergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#definition"><i class="fa fa-check"></i><b>2.2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#properties-of-kl-divergence"><i class="fa fa-check"></i><b>2.2.2</b> Properties of KL divergence</a></li>
<li class="chapter" data-level="2.2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#examples"><i class="fa fa-check"></i><b>2.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#local-quadratic-approximation-and-expected-fisher-information"><i class="fa fa-check"></i><b>2.3</b> Local quadratic approximation and expected Fisher information</a></li>
<li class="chapter" data-level="2.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy-learning-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4</b> Entropy learning and maximum likelihood</a><ul>
<li class="chapter" data-level="2.4.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#the-relative-entropy-between-true-model-and-approximating-model"><i class="fa fa-check"></i><b>2.4.1</b> The relative entropy between true model and approximating model</a></li>
<li class="chapter" data-level="2.4.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#minimum-kl-divergence-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4.2</b> Minimum KL divergence and maximum likelihood</a></li>
<li class="chapter" data-level="2.4.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#further-connections"><i class="fa fa-check"></i><b>2.4.3</b> Further connections</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="03-likelihood3.html"><a href="03-likelihood3.html"><i class="fa fa-check"></i><b>3</b> Maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#principle-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.1</b> Principle of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#outline"><i class="fa fa-check"></i><b>3.1.1</b> Outline</a></li>
<li class="chapter" data-level="3.1.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#obtaining-mles-for-a-regular-model"><i class="fa fa-check"></i><b>3.1.2</b> Obtaining MLEs for a regular model</a></li>
<li class="chapter" data-level="3.1.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#invariance-property-of-the-maximum-likelihood"><i class="fa fa-check"></i><b>3.1.3</b> Invariance property of the maximum likelihood</a></li>
<li class="chapter" data-level="3.1.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#consistency-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>3.1.4</b> Consistency of maximum likelihood estimates</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#maximum-likelihood-estimation-in-practise"><i class="fa fa-check"></i><b>3.2</b> Maximum likelihood estimation in practise</a><ul>
<li class="chapter" data-level="3.2.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#worked-examples"><i class="fa fa-check"></i><b>3.2.1</b> Worked examples</a></li>
<li class="chapter" data-level="3.2.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#relationship-with-least-squares-estimation"><i class="fa fa-check"></i><b>3.2.2</b> Relationship with least squares estimation</a></li>
<li class="chapter" data-level="3.2.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#bias-and-maximum-likelihood"><i class="fa fa-check"></i><b>3.2.3</b> Bias and maximum likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information"><i class="fa fa-check"></i><b>3.3</b> Observed Fisher information</a><ul>
<li class="chapter" data-level="3.3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#motivation-and-definition"><i class="fa fa-check"></i><b>3.3.1</b> Motivation and definition</a></li>
<li class="chapter" data-level="3.3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#examples-of-observed-fisher-information"><i class="fa fa-check"></i><b>3.3.2</b> Examples of observed Fisher information</a></li>
<li class="chapter" data-level="3.3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#relationship-between-observed-and-expected-fisher-information"><i class="fa fa-check"></i><b>3.3.3</b> Relationship between observed and expected Fisher information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="04-likelihood4.html"><a href="04-likelihood4.html"><i class="fa fa-check"></i><b>4</b> Quadratic approximation and normal asymptotics</a><ul>
<li class="chapter" data-level="4.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#multivariate-statistics-for-random-vectors"><i class="fa fa-check"></i><b>4.1</b> Multivariate statistics for random vectors</a><ul>
<li class="chapter" data-level="4.1.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#covariance-and-correlation"><i class="fa fa-check"></i><b>4.1.1</b> Covariance and correlation</a></li>
<li class="chapter" data-level="4.1.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.1.2</b> Multivariate normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#approximate-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.2</b> Approximate distribution of maximum likelihood estimates</a><ul>
<li class="chapter" data-level="4.2.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-log-likelihood-resulting-from-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Quadratic log-likelihood resulting from normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-approximation-of-a-log-likelihood-function"><i class="fa fa-check"></i><b>4.2.2</b> Quadratic approximation of a log-likelihood function</a></li>
<li class="chapter" data-level="4.2.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.2.3</b> Asymptotic normality of maximum likelihood estimates</a></li>
<li class="chapter" data-level="4.2.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-optimal-efficiency"><i class="fa fa-check"></i><b>4.2.4</b> Asymptotic optimal efficiency</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quantifying-the-uncertainty-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.3</b> Quantifying the uncertainty of maximum likelihood estimates</a><ul>
<li class="chapter" data-level="4.3.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#estimating-the-variance-of-mles"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the variance of MLEs</a></li>
<li class="chapter" data-level="4.3.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#wald-statistic"><i class="fa fa-check"></i><b>4.3.2</b> Wald statistic</a></li>
<li class="chapter" data-level="4.3.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-confidence-intervals-using-the-wald-statistic"><i class="fa fa-check"></i><b>4.3.3</b> Normal confidence intervals using the Wald statistic</a></li>
<li class="chapter" data-level="4.3.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-tests-using-the-wald-statistic"><i class="fa fa-check"></i><b>4.3.4</b> Normal tests using the Wald statistic</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-of-a-non-regular-model"><i class="fa fa-check"></i><b>4.4</b> Example of a non-regular model</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="05-likelihood5.html"><a href="05-likelihood5.html"><i class="fa fa-check"></i><b>5</b> Likelihood-based confidence interval and likelihood ratio</a><ul>
<li class="chapter" data-level="5.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-based-confidence-intervals-and-wilks-statistic"><i class="fa fa-check"></i><b>5.1</b> Likelihood-based confidence intervals and Wilks statistic</a><ul>
<li class="chapter" data-level="5.1.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#general-idea-and-definition-of-wilks-statistics"><i class="fa fa-check"></i><b>5.1.1</b> General idea and definition of Wilks statistics</a></li>
<li class="chapter" data-level="5.1.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#quadratic-approximation-of-wilks-statistic-and-squared-wald-statistic"><i class="fa fa-check"></i><b>5.1.2</b> Quadratic approximation of Wilks statistic and squared Wald statistic</a></li>
<li class="chapter" data-level="5.1.3" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-the-wilks-statistic"><i class="fa fa-check"></i><b>5.1.3</b> Distribution of the Wilks statistic</a></li>
<li class="chapter" data-level="5.1.4" data-path="05-likelihood5.html"><a href="05-likelihood5.html#cutoff-values-for-the-likelihood-ci"><i class="fa fa-check"></i><b>5.1.4</b> Cutoff values for the likelihood CI</a></li>
<li class="chapter" data-level="5.1.5" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-ratio-test-lrt-using-wilks-statistics"><i class="fa fa-check"></i><b>5.1.5</b> Likelihood ratio test (LRT) using Wilks statistics</a></li>
<li class="chapter" data-level="5.1.6" data-path="05-likelihood5.html"><a href="05-likelihood5.html#origin-of-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.1.6</b> Origin of likelihood ratio statistic</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#generalised-likelihood-ratio-test-glrt"><i class="fa fa-check"></i><b>5.2</b> Generalised likelihood ratio test (GLRT)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="06-likelihood6.html"><a href="06-likelihood6.html"><i class="fa fa-check"></i><b>6</b> Optimality properties and conclusion</a><ul>
<li class="chapter" data-level="6.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#properties-of-maximum-likelihood-encountered-so-far"><i class="fa fa-check"></i><b>6.1</b> Properties of maximum likelihood encountered so far</a></li>
<li class="chapter" data-level="6.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency"><i class="fa fa-check"></i><b>6.2</b> Summarising data and the concept of minimal sufficiency</a></li>
<li class="chapter" data-level="6.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#concluding-remarks-on-maximum-likelihood"><i class="fa fa-check"></i><b>6.3</b> Concluding remarks on maximum likelihood</a><ul>
<li class="chapter" data-level="6.3.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#remark-on-kl-divergence"><i class="fa fa-check"></i><b>6.3.1</b> Remark on KL divergence</a></li>
<li class="chapter" data-level="6.3.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#what-happens-if-n-is-small"><i class="fa fa-check"></i><b>6.3.2</b> What happens if <span class="math inline">\(n\)</span> is small?</a></li>
<li class="chapter" data-level="6.3.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#model-selection"><i class="fa fa-check"></i><b>6.3.3</b> Model selection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Bayesian Statistics</b></span></li>
<li class="chapter" data-level="7" data-path="07-bayes1.html"><a href="07-bayes1.html"><i class="fa fa-check"></i><b>7</b> Essentials of Bayesian statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#bayes-theorem"><i class="fa fa-check"></i><b>7.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#principle-of-bayesian-learning"><i class="fa fa-check"></i><b>7.2</b> Principle of Bayesian learning</a></li>
<li class="chapter" data-level="7.3" data-path="07-bayes1.html"><a href="07-bayes1.html#what-is-exactly-is-the-bayesian-estimate"><i class="fa fa-check"></i><b>7.3</b> What is exactly is the “Bayesian estimate”?</a></li>
<li class="chapter" data-level="7.4" data-path="07-bayes1.html"><a href="07-bayes1.html#computer-implementation-of-bayesian-learning"><i class="fa fa-check"></i><b>7.4</b> Computer implementation of Bayesian learning</a></li>
<li class="chapter" data-level="7.5" data-path="07-bayes1.html"><a href="07-bayes1.html#bayesian-interpretation-of-probability"><i class="fa fa-check"></i><b>7.5</b> Bayesian interpretation of probability</a><ul>
<li class="chapter" data-level="7.5.1" data-path="07-bayes1.html"><a href="07-bayes1.html#what-makes-you-bayesian"><i class="fa fa-check"></i><b>7.5.1</b> What makes you “Bayesian”?</a></li>
<li class="chapter" data-level="7.5.2" data-path="07-bayes1.html"><a href="07-bayes1.html#mathematics-of-probability"><i class="fa fa-check"></i><b>7.5.2</b> Mathematics of probability</a></li>
<li class="chapter" data-level="7.5.3" data-path="07-bayes1.html"><a href="07-bayes1.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.5.3</b> Interpretations of probability</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="07-bayes1.html"><a href="07-bayes1.html#historical-developments"><i class="fa fa-check"></i><b>7.6</b> Historical developments</a></li>
<li class="chapter" data-level="7.7" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning"><i class="fa fa-check"></i><b>7.7</b> Connection with entropy learning</a><ul>
<li class="chapter" data-level="7.7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#zero-forcing-property"><i class="fa fa-check"></i><b>7.7.1</b> Zero forcing property</a></li>
<li class="chapter" data-level="7.7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning-1"><i class="fa fa-check"></i><b>7.7.2</b> Connection with entropy learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="08-bayes2.html"><a href="08-bayes2.html"><i class="fa fa-check"></i><b>8</b> Beta-Binomial model for estimating a proportion</a><ul>
<li class="chapter" data-level="8.1" data-path="08-bayes2.html"><a href="08-bayes2.html#binomial-likelihood"><i class="fa fa-check"></i><b>8.1</b> Binomial likelihood</a></li>
<li class="chapter" data-level="8.2" data-path="08-bayes2.html"><a href="08-bayes2.html#excursion-properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>8.2</b> Excursion: Properties of the Beta distribution</a></li>
<li class="chapter" data-level="8.3" data-path="08-bayes2.html"><a href="08-bayes2.html#beta-prior-distribution"><i class="fa fa-check"></i><b>8.3</b> Beta prior distribution</a></li>
<li class="chapter" data-level="8.4" data-path="08-bayes2.html"><a href="08-bayes2.html#computing-the-posterior-distribution"><i class="fa fa-check"></i><b>8.4</b> Computing the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="09-bayes3.html"><a href="09-bayes3.html"><i class="fa fa-check"></i><b>9</b> Properties of Bayesian learning</a><ul>
<li class="chapter" data-level="9.1" data-path="09-bayes3.html"><a href="09-bayes3.html#prior-acting-as-pseudo-data"><i class="fa fa-check"></i><b>9.1</b> Prior acting as pseudo-data</a></li>
<li class="chapter" data-level="9.2" data-path="09-bayes3.html"><a href="09-bayes3.html#linear-shrinkage-of-mean"><i class="fa fa-check"></i><b>9.2</b> Linear shrinkage of mean</a></li>
<li class="chapter" data-level="9.3" data-path="09-bayes3.html"><a href="09-bayes3.html#conjugacy-of-prior-and-posterior-distribution"><i class="fa fa-check"></i><b>9.3</b> Conjugacy of prior and posterior distribution</a></li>
<li class="chapter" data-level="9.4" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-asymptotics"><i class="fa fa-check"></i><b>9.4</b> Large sample asymptotics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-limits-of-mean-and-variance"><i class="fa fa-check"></i><b>9.4.1</b> Large sample limits of mean and variance</a></li>
<li class="chapter" data-level="9.4.2" data-path="09-bayes3.html"><a href="09-bayes3.html#asymptotic-normality-of-the-posterior-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Asymptotic Normality of the Posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="09-bayes3.html"><a href="09-bayes3.html#posterior-variance-for-finite-n"><i class="fa fa-check"></i><b>9.5</b> Posterior variance for finite <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-bayes4.html"><a href="10-bayes4.html"><i class="fa fa-check"></i><b>10</b> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a><ul>
<li class="chapter" data-level="10.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-normal-model-to-estimate-mean"><i class="fa fa-check"></i><b>10.1</b> Normal-Normal model to estimate mean</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihood"><i class="fa fa-check"></i><b>10.1.1</b> Normal likelihood</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-prior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Normal prior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-posterior-distribution"><i class="fa fa-check"></i><b>10.1.3</b> Normal posterior distribution</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-and-stein-paradox"><i class="fa fa-check"></i><b>10.1.4</b> Large sample asymptotics and Stein paradox</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-normal-model-to-estimate-variance"><i class="fa fa-check"></i><b>10.2</b> Inverse-Gamma-Normal model to estimate variance</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Inverse Gamma distribution</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihoood"><i class="fa fa-check"></i><b>10.2.2</b> Normal likelihoood</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-prior-distribution"><i class="fa fa-check"></i><b>10.2.3</b> Inverse Gamma prior distribution</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-posterior-distribution"><i class="fa fa-check"></i><b>10.2.4</b> Inverse Gamma posterior distribution</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-1"><i class="fa fa-check"></i><b>10.2.5</b> Large sample asymptotics</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-bayes4.html"><a href="10-bayes4.html#estimating-precision"><i class="fa fa-check"></i><b>10.2.6</b> Estimating precision</a></li>
<li class="chapter" data-level="10.2.7" data-path="10-bayes4.html"><a href="10-bayes4.html#joint-estimation-of-mean-and-variance"><i class="fa fa-check"></i><b>10.2.7</b> Joint estimation of mean and variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-bayes5.html"><a href="11-bayes5.html"><i class="fa fa-check"></i><b>11</b> Shrinkage estimation using empirical risk minimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="11-bayes5.html"><a href="11-bayes5.html#linear-shrinkage"><i class="fa fa-check"></i><b>11.1</b> Linear shrinkage</a></li>
<li class="chapter" data-level="11.2" data-path="11-bayes5.html"><a href="11-bayes5.html#james-stein-estimator"><i class="fa fa-check"></i><b>11.2</b> James-Stein estimator</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-bayes6.html"><a href="12-bayes6.html"><i class="fa fa-check"></i><b>12</b> Bayesian model comparison using Bayes factors and the BIC</a><ul>
<li class="chapter" data-level="12.1" data-path="12-bayes6.html"><a href="12-bayes6.html#the-bayes-factor"><i class="fa fa-check"></i><b>12.1</b> The Bayes factor</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-bayes6.html"><a href="12-bayes6.html#connection-with-relative-entropy"><i class="fa fa-check"></i><b>12.1.1</b> Connection with relative entropy</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-bayes6.html"><a href="12-bayes6.html#interpretation-of-and-scale-for-bayes-factor"><i class="fa fa-check"></i><b>12.1.2</b> Interpretation of and scale for Bayes factor</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-bayes6.html"><a href="12-bayes6.html#computing-textprd-m-for-simple-and-composite-models"><i class="fa fa-check"></i><b>12.1.3</b> Computing <span class="math inline">\(\text{Pr}(D | M)\)</span> for simple and composite models</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-bayes6.html"><a href="12-bayes6.html#bayes-factor-versus-likelihood-ratio"><i class="fa fa-check"></i><b>12.1.4</b> Bayes factor versus likelihood ratio</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-bayes6.html"><a href="12-bayes6.html#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><i class="fa fa-check"></i><b>12.2</b> Approximate computation of the marginal likelihood and of the log-Bayes factor</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-bayes6.html"><a href="12-bayes6.html#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fa fa-check"></i><b>12.2.1</b> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-bayes6.html"><a href="12-bayes6.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>12.2.2</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-bayes6.html"><a href="12-bayes6.html#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fa fa-check"></i><b>12.2.3</b> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-bayes6.html"><a href="12-bayes6.html#model-complexity-and-occams-razor"><i class="fa fa-check"></i><b>12.2.4</b> Model complexity and Occams razor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-bayes7.html"><a href="13-bayes7.html"><i class="fa fa-check"></i><b>13</b> False discovery rates</a><ul>
<li class="chapter" data-level="13.1" data-path="13-bayes7.html"><a href="13-bayes7.html#general-setup"><i class="fa fa-check"></i><b>13.1</b> General setup</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-bayes7.html"><a href="13-bayes7.html#overview-1"><i class="fa fa-check"></i><b>13.1.1</b> Overview</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-bayes7.html"><a href="13-bayes7.html#choosing-between-h_0-and-h_a"><i class="fa fa-check"></i><b>13.1.2</b> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span></a></li>
<li class="chapter" data-level="13.1.3" data-path="13-bayes7.html"><a href="13-bayes7.html#true-and-false-positives-and-negatives"><i class="fa fa-check"></i><b>13.1.3</b> True and false positives and negatives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-bayes7.html"><a href="13-bayes7.html#specificity-and-sensitivity"><i class="fa fa-check"></i><b>13.2</b> Specificity and Sensitivity</a></li>
<li class="chapter" data-level="13.3" data-path="13-bayes7.html"><a href="13-bayes7.html#fdr-and-fndr"><i class="fa fa-check"></i><b>13.3</b> FDR and FNDR</a></li>
<li class="chapter" data-level="13.4" data-path="13-bayes7.html"><a href="13-bayes7.html#bayesian-perspective"><i class="fa fa-check"></i><b>13.4</b> Bayesian perspective</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-bayes7.html"><a href="13-bayes7.html#two-component-mixture-model"><i class="fa fa-check"></i><b>13.4.1</b> Two component mixture model</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-bayes7.html"><a href="13-bayes7.html#local-fdr"><i class="fa fa-check"></i><b>13.4.2</b> Local FDR</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-bayes7.html"><a href="13-bayes7.html#q-values"><i class="fa fa-check"></i><b>13.4.3</b> q-values</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="13-bayes7.html"><a href="13-bayes7.html#software"><i class="fa fa-check"></i><b>13.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-bayes8.html"><a href="14-bayes8.html"><i class="fa fa-check"></i><b>14</b> Optimality properties and summary</a><ul>
<li class="chapter" data-level="14.1" data-path="14-bayes8.html"><a href="14-bayes8.html#bayesian-statistics-in-a-nutshell"><i class="fa fa-check"></i><b>14.1</b> Bayesian statistics in a nutshell</a><ul>
<li class="chapter" data-level="14.1.1" data-path="14-bayes8.html"><a href="14-bayes8.html#remarks"><i class="fa fa-check"></i><b>14.1.1</b> Remarks</a></li>
<li class="chapter" data-level="14.1.2" data-path="14-bayes8.html"><a href="14-bayes8.html#advantages"><i class="fa fa-check"></i><b>14.1.2</b> Advantages</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="14-bayes8.html"><a href="14-bayes8.html#frequentist-properties-of-bayesian-estimators"><i class="fa fa-check"></i><b>14.2</b> Frequentist properties of Bayesian estimators</a></li>
<li class="chapter" data-level="14.3" data-path="14-bayes8.html"><a href="14-bayes8.html#specifying-the-prior-problem-or-advantage"><i class="fa fa-check"></i><b>14.3</b> Specifying the prior — problem or advantage?</a></li>
<li class="chapter" data-level="14.4" data-path="14-bayes8.html"><a href="14-bayes8.html#choosing-a-prior"><i class="fa fa-check"></i><b>14.4</b> Choosing a prior</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-bayes8.html"><a href="14-bayes8.html#some-guidelines"><i class="fa fa-check"></i><b>14.4.1</b> Some guidelines</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-bayes8.html"><a href="14-bayes8.html#jeffreys-prior"><i class="fa fa-check"></i><b>14.4.2</b> Jeffreys prior</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-bayes8.html"><a href="14-bayes8.html#optimality-of-bayes-inference"><i class="fa fa-check"></i><b>14.5</b> Optimality of Bayes inference</a></li>
<li class="chapter" data-level="14.6" data-path="14-bayes8.html"><a href="14-bayes8.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a><ul>
<li class="chapter" data-level="14.6.1" data-path="14-bayes8.html"><a href="14-bayes8.html#current-research"><i class="fa fa-check"></i><b>14.6.1</b> Current research</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Regression</b></span></li>
<li class="chapter" data-level="15" data-path="15-regression1.html"><a href="15-regression1.html"><i class="fa fa-check"></i><b>15</b> Overview over regression modelling</a><ul>
<li class="chapter" data-level="15.1" data-path="15-regression1.html"><a href="15-regression1.html#general-setup-1"><i class="fa fa-check"></i><b>15.1</b> General setup</a></li>
<li class="chapter" data-level="15.2" data-path="15-regression1.html"><a href="15-regression1.html#objectives"><i class="fa fa-check"></i><b>15.2</b> Objectives</a></li>
<li class="chapter" data-level="15.3" data-path="15-regression1.html"><a href="15-regression1.html#regression-as-a-form-of-supervised-learning"><i class="fa fa-check"></i><b>15.3</b> Regression as a form of supervised learning</a></li>
<li class="chapter" data-level="15.4" data-path="15-regression1.html"><a href="15-regression1.html#various-regression-models-used-in-statistics"><i class="fa fa-check"></i><b>15.4</b> Various regression models used in statistics</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="16-regression2.html"><a href="16-regression2.html"><i class="fa fa-check"></i><b>16</b> Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="16-regression2.html"><a href="16-regression2.html#the-linear-regression-model"><i class="fa fa-check"></i><b>16.1</b> The linear regression model</a></li>
<li class="chapter" data-level="16.2" data-path="16-regression2.html"><a href="16-regression2.html#interpretation-of-regression-coefficients-and-intercept"><i class="fa fa-check"></i><b>16.2</b> Interpretation of regression coefficients and intercept</a></li>
<li class="chapter" data-level="16.3" data-path="16-regression2.html"><a href="16-regression2.html#different-types-of-linear-regression"><i class="fa fa-check"></i><b>16.3</b> Different types of linear regression:</a></li>
<li class="chapter" data-level="16.4" data-path="16-regression2.html"><a href="16-regression2.html#distributional-assumptions-and-properties"><i class="fa fa-check"></i><b>16.4</b> Distributional assumptions and properties</a></li>
<li class="chapter" data-level="16.5" data-path="16-regression2.html"><a href="16-regression2.html#regression-in-data-matrix-notation"><i class="fa fa-check"></i><b>16.5</b> Regression in data matrix notation</a></li>
<li class="chapter" data-level="16.6" data-path="16-regression2.html"><a href="16-regression2.html#centering-and-vanishing-of-the-intercept-beta_0"><i class="fa fa-check"></i><b>16.6</b> Centering and vanishing of the intercept <span class="math inline">\(\beta_0\)</span></a></li>
<li class="chapter" data-level="16.7" data-path="16-regression2.html"><a href="16-regression2.html#regression-objectives-for-linear-model"><i class="fa fa-check"></i><b>16.7</b> Regression objectives for linear model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="17-regression3.html"><a href="17-regression3.html"><i class="fa fa-check"></i><b>17</b> Estimating regression coefficients</a><ul>
<li class="chapter" data-level="17.1" data-path="17-regression3.html"><a href="17-regression3.html#ordinary-least-squares-ols-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.1</b> Ordinary Least Squares (OLS) estimator of regression coefficients</a></li>
<li class="chapter" data-level="17.2" data-path="17-regression3.html"><a href="17-regression3.html#maximum-likelihood-estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>17.2</b> Maximum likelihood estimation of regression coefficients</a><ul>
<li class="chapter" data-level="17.2.1" data-path="17-regression3.html"><a href="17-regression3.html#detailed-derivation-of-the-mles"><i class="fa fa-check"></i><b>17.2.1</b> Detailed derivation of the MLEs</a></li>
<li class="chapter" data-level="17.2.2" data-path="17-regression3.html"><a href="17-regression3.html#asymptotics"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotics</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="17-regression3.html"><a href="17-regression3.html#covariance-plug-in-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.3</b> Covariance plug-in estimator of regression coefficients</a><ul>
<li class="chapter" data-level="17.3.1" data-path="17-regression3.html"><a href="17-regression3.html#importance-of-positive-definiteness-of-estimated-covariance-matrix"><i class="fa fa-check"></i><b>17.3.1</b> Importance of positive definiteness of estimated covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="17-regression3.html"><a href="17-regression3.html#best-linear-predictor"><i class="fa fa-check"></i><b>17.4</b> Best linear predictor</a><ul>
<li class="chapter" data-level="17.4.1" data-path="17-regression3.html"><a href="17-regression3.html#result"><i class="fa fa-check"></i><b>17.4.1</b> Result:</a></li>
<li class="chapter" data-level="17.4.2" data-path="17-regression3.html"><a href="17-regression3.html#irreducible-error"><i class="fa fa-check"></i><b>17.4.2</b> Irreducible Error</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="17-regression3.html"><a href="17-regression3.html#regression-by-conditioning"><i class="fa fa-check"></i><b>17.5</b> Regression by conditioning</a><ul>
<li class="chapter" data-level="17.5.1" data-path="17-regression3.html"><a href="17-regression3.html#general-idea"><i class="fa fa-check"></i><b>17.5.1</b> General idea:</a></li>
<li class="chapter" data-level="17.5.2" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-normal-assumption"><i class="fa fa-check"></i><b>17.5.2</b> Multivariate normal assumption</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="17-regression3.html"><a href="17-regression3.html#standardised-regression-coefficients-and-relationship-to-correlation"><i class="fa fa-check"></i><b>17.6</b> Standardised regression coefficients and relationship to correlation</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="18-regression4.html"><a href="18-regression4.html"><i class="fa fa-check"></i><b>18</b> Squared multiple correlation and variance decomposition in linear regression</a><ul>
<li class="chapter" data-level="18.1" data-path="18-regression4.html"><a href="18-regression4.html#squared-multiple-correlation-omega2-and-the-r2-coefficient"><i class="fa fa-check"></i><b>18.1</b> Squared multiple correlation <span class="math inline">\(\Omega^2\)</span> and the <span class="math inline">\(R^2\)</span> coefficient</a><ul>
<li class="chapter" data-level="18.1.1" data-path="18-regression4.html"><a href="18-regression4.html#estimation-of-omega2-and-the-multiple-r2-coefficient"><i class="fa fa-check"></i><b>18.1.1</b> Estimation of <span class="math inline">\(\Omega^2\)</span> and the multiple <span class="math inline">\(R^2\)</span> coefficient</a></li>
<li class="chapter" data-level="18.1.2" data-path="18-regression4.html"><a href="18-regression4.html#r-output"><i class="fa fa-check"></i><b>18.1.2</b> R output</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="18-regression4.html"><a href="18-regression4.html#variance-decomposition-in-regression"><i class="fa fa-check"></i><b>18.2</b> Variance decomposition in regression</a><ul>
<li class="chapter" data-level="18.2.1" data-path="18-regression4.html"><a href="18-regression4.html#law-of-total-variance-and-variance-decomposition"><i class="fa fa-check"></i><b>18.2.1</b> Law of total variance and variance decomposition</a></li>
<li class="chapter" data-level="18.2.2" data-path="18-regression4.html"><a href="18-regression4.html#related-quantities"><i class="fa fa-check"></i><b>18.2.2</b> Related quantities</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="18-regression4.html"><a href="18-regression4.html#sample-version-of-variance-decomposition"><i class="fa fa-check"></i><b>18.3</b> Sample version of variance decomposition</a><ul>
<li class="chapter" data-level="18.3.1" data-path="18-regression4.html"><a href="18-regression4.html#geometric-interpretation-of-regression-as-orthogonal-projection"><i class="fa fa-check"></i><b>18.3.1</b> Geometric interpretation of regression as orthogonal projection:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="19-regression5.html"><a href="19-regression5.html"><i class="fa fa-check"></i><b>19</b> Prediction and variable selection</a><ul>
<li class="chapter" data-level="19.1" data-path="19-regression5.html"><a href="19-regression5.html#prediction-and-prediction-intervals"><i class="fa fa-check"></i><b>19.1</b> Prediction and prediction intervals</a></li>
<li class="chapter" data-level="19.2" data-path="19-regression5.html"><a href="19-regression5.html#variable-importance-and-prediction"><i class="fa fa-check"></i><b>19.2</b> Variable importance and prediction</a><ul>
<li class="chapter" data-level="19.2.1" data-path="19-regression5.html"><a href="19-regression5.html#how-to-quantify-variable-importance"><i class="fa fa-check"></i><b>19.2.1</b> How to quantify variable importance?</a></li>
<li class="chapter" data-level="19.2.2" data-path="19-regression5.html"><a href="19-regression5.html#some-candidates-for-vims"><i class="fa fa-check"></i><b>19.2.2</b> Some candidates for VIMs</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="19-regression5.html"><a href="19-regression5.html#regression-t-scores."><i class="fa fa-check"></i><b>19.3</b> Regression <span class="math inline">\(t\)</span>-scores.</a><ul>
<li class="chapter" data-level="19.3.1" data-path="19-regression5.html"><a href="19-regression5.html#computation"><i class="fa fa-check"></i><b>19.3.1</b> Computation</a></li>
<li class="chapter" data-level="19.3.2" data-path="19-regression5.html"><a href="19-regression5.html#connection-with-partial-correlation"><i class="fa fa-check"></i><b>19.3.2</b> Connection with partial correlation</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="19-regression5.html"><a href="19-regression5.html#further-approaches-for-variable-selection"><i class="fa fa-check"></i><b>19.4</b> Further approaches for variable selection</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="20-refresher.html"><a href="20-refresher.html"><i class="fa fa-check"></i><b>A</b> Refresher</a><ul>
<li class="chapter" data-level="A.1" data-path="20-refresher.html"><a href="20-refresher.html#basic-mathematical-notation"><i class="fa fa-check"></i><b>A.1</b> Basic mathematical notation</a></li>
<li class="chapter" data-level="A.2" data-path="20-refresher.html"><a href="20-refresher.html#vectors-and-matrices"><i class="fa fa-check"></i><b>A.2</b> Vectors and matrices</a></li>
<li class="chapter" data-level="A.3" data-path="20-refresher.html"><a href="20-refresher.html#functions"><i class="fa fa-check"></i><b>A.3</b> Functions</a><ul>
<li class="chapter" data-level="A.3.1" data-path="20-refresher.html"><a href="20-refresher.html#convex-and-concave-functions"><i class="fa fa-check"></i><b>A.3.1</b> Convex and concave functions</a></li>
<li class="chapter" data-level="A.3.2" data-path="20-refresher.html"><a href="20-refresher.html#gradient"><i class="fa fa-check"></i><b>A.3.2</b> Gradient</a></li>
<li class="chapter" data-level="A.3.3" data-path="20-refresher.html"><a href="20-refresher.html#hessian-matrix"><i class="fa fa-check"></i><b>A.3.3</b> Hessian matrix</a></li>
<li class="chapter" data-level="A.3.4" data-path="20-refresher.html"><a href="20-refresher.html#conditions-for-local-maximum-of-a-function"><i class="fa fa-check"></i><b>A.3.4</b> Conditions for local maximum of a function</a></li>
<li class="chapter" data-level="A.3.5" data-path="20-refresher.html"><a href="20-refresher.html#linear-and-quadratic-approximation"><i class="fa fa-check"></i><b>A.3.5</b> Linear and quadratic approximation</a></li>
<li class="chapter" data-level="A.3.6" data-path="20-refresher.html"><a href="20-refresher.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.3.6</b> Functions of matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="20-refresher.html"><a href="20-refresher.html#probability"><i class="fa fa-check"></i><b>A.4</b> Probability</a><ul>
<li class="chapter" data-level="A.4.1" data-path="20-refresher.html"><a href="20-refresher.html#random-variables"><i class="fa fa-check"></i><b>A.4.1</b> Random variables</a></li>
<li class="chapter" data-level="A.4.2" data-path="20-refresher.html"><a href="20-refresher.html#transformation-of-random-variables"><i class="fa fa-check"></i><b>A.4.2</b> Transformation of random variables</a></li>
<li class="chapter" data-level="A.4.3" data-path="20-refresher.html"><a href="20-refresher.html#bernoulli-and-binomial-distribution"><i class="fa fa-check"></i><b>A.4.3</b> Bernoulli and Binomial distribution</a></li>
<li class="chapter" data-level="A.4.4" data-path="20-refresher.html"><a href="20-refresher.html#normal-distribution"><i class="fa fa-check"></i><b>A.4.4</b> Normal distribution</a></li>
<li class="chapter" data-level="A.4.5" data-path="20-refresher.html"><a href="20-refresher.html#chi-squared-distribution"><i class="fa fa-check"></i><b>A.4.5</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="A.4.6" data-path="20-refresher.html"><a href="20-refresher.html#law-of-large-numbers"><i class="fa fa-check"></i><b>A.4.6</b> Law of large numbers:</a></li>
<li class="chapter" data-level="A.4.7" data-path="20-refresher.html"><a href="20-refresher.html#jensens-inequality"><i class="fa fa-check"></i><b>A.4.7</b> Jensen’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="20-refresher.html"><a href="20-refresher.html#statistics"><i class="fa fa-check"></i><b>A.5</b> Statistics</a><ul>
<li class="chapter" data-level="A.5.1" data-path="20-refresher.html"><a href="20-refresher.html#statistical-learning"><i class="fa fa-check"></i><b>A.5.1</b> Statistical learning</a></li>
<li class="chapter" data-level="A.5.2" data-path="20-refresher.html"><a href="20-refresher.html#point-and-interval-estimation"><i class="fa fa-check"></i><b>A.5.2</b> Point and interval estimation</a></li>
<li class="chapter" data-level="A.5.3" data-path="20-refresher.html"><a href="20-refresher.html#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fa fa-check"></i><b>A.5.3</b> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span></a></li>
<li class="chapter" data-level="A.5.4" data-path="20-refresher.html"><a href="20-refresher.html#asymptotics-1"><i class="fa fa-check"></i><b>A.5.4</b> Asymptotics</a></li>
<li class="chapter" data-level="A.5.5" data-path="20-refresher.html"><a href="20-refresher.html#confidence-intervals"><i class="fa fa-check"></i><b>A.5.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="A.5.6" data-path="20-refresher.html"><a href="20-refresher.html#symmetric-normal-confidence-interval"><i class="fa fa-check"></i><b>A.5.6</b> Symmetric normal confidence interval</a></li>
<li class="chapter" data-level="A.5.7" data-path="20-refresher.html"><a href="20-refresher.html#confidence-interval-for-chi-squared-distribution"><i class="fa fa-check"></i><b>A.5.7</b> Confidence interval for chi-squared distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="21-further-study.html"><a href="21-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="21-further-study.html"><a href="21-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="21-further-study.html"><a href="21-further-study.html#additional-references"><i class="fa fa-check"></i><b>B.2</b> Additional references</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="22-references.html"><a href="22-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Statistical Methods:<br />
Likelihood, Bayes and Regression</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="quadratic-approximation-and-normal-asymptotics" class="section level1">
<h1><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</h1>
<div id="multivariate-statistics-for-random-vectors" class="section level2">
<h2><span class="header-section-number">4.1</span> Multivariate statistics for random vectors</h2>
<div id="covariance-and-correlation" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Covariance and correlation</h3>
<p>Assume a random variable <span class="math inline">\(x\)</span> with mean <span class="math inline">\(\text{E}(x) = \mu\)</span>. The corresponding
variance is given by
<span class="math display">\[
\begin{split}
\text{Var}(x) &amp; = \text{E}\left((x-\mu)^2 \right) \\
        &amp; =\text{E}\left( (x-\mu)(x-\mu) \right) \\
        &amp; = \text{E}(x^2)-\mu^2 \\
\end{split}
\]</span></p>
<p>For a random vector <span class="math inline">\(\boldsymbol x= (x_1, x_2,...,x_d)^T\)</span> the mean <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \mu\)</span>
is simply comprised of the means of its components, i.e. <span class="math inline">\(\boldsymbol \mu= (\mu_1, \ldots, \mu_d)^T\)</span>.
Thus, the mean of a random vector of dimension is a vector of of the same length.</p>
<p>The variance of a random vector of length <span class="math inline">\(d\)</span>, however, is not a vector but a matrix of size <span class="math inline">\(d\times d\)</span>.
This matrix is called the <strong>covariance matrix</strong>:
<span class="math display">\[
\begin{split}
\text{Var}(\boldsymbol x) &amp;= \underbrace{\boldsymbol \Sigma}_{d\times d} = (\sigma_{ij}) = \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; \sigma_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \dots &amp; \sigma_{dd}
\end{pmatrix} \\
  &amp;=\text{E}\left(\underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d\times 1} \underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1\times d}\right) \\
  &amp; = \text{E}(\boldsymbol x\boldsymbol x^T)-\boldsymbol \mu\boldsymbol \mu^T \\
\end{split}
\]</span>
The entries of the covariance matrix <span class="math inline">\(\sigma_{ij} =\text{Cov}(x_i, x_j)\)</span> describe the covariance between
the random variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>.
The covariance matrix is symmetric, hence <span class="math inline">\(\sigma_{ij}=\sigma_{ji}\)</span>.
The diagonal entries <span class="math inline">\(\sigma_{ii} = \text{Cov}(x_i, x_i) = \text{Var}(x_i) = \sigma_i^2\)</span> correspond to the variances of the components
of <span class="math inline">\(\boldsymbol x\)</span>.
The covariance matrix is <strong>positive semi-definite</strong>, i.e. the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span> are all positive or equal to zero. However, in practise one aims to use non-singular covariance matrices, with all eigenvalues positive, so that they are invertible.</p>
<p>A covariance matrix can be factorised into the product
<span class="math display">\[\boldsymbol \Sigma= \boldsymbol V^{\frac{1}{2}} \boldsymbol P\boldsymbol V^{\frac{1}{2}}\]</span>
where <span class="math inline">\(\boldsymbol V\)</span> is a diagonal matrix containing the variances
<span class="math display">\[ \boldsymbol V= \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}\]</span>
and the matrix <span class="math inline">\(\boldsymbol P\)</span> (“capital rho”) is the symmetric <strong>correlation matrix</strong>
<span class="math display">\[ \boldsymbol P= (\rho_{ij}) = \begin{pmatrix}
    1 &amp; \dots &amp; \rho_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \rho_{d1} &amp; \dots &amp; 1
\end{pmatrix}   = \boldsymbol V^{-\frac{1}{2}} \boldsymbol \Sigma\boldsymbol V^{-\frac{1}{2}}\]</span>
Thus, the correlation between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> is defined as
<span class="math display">\[\rho_{ij} = \text{Cor}(x_i,x_j) = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}\]</span></p>
</div>
<div id="multivariate-normal-distribution" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Multivariate normal distribution</h3>
<p>The density of a normally distributed scalar variable <span class="math inline">\(x \sim N(\mu, \sigma^2)\)</span>
with mean <span class="math inline">\(\text{E}(x) = \mu\)</span> and variance <span class="math inline">\(\text{Var}(x) = \sigma^2\)</span> is
<span class="math display">\[
f(x |\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right)
\]</span></p>
<p>The univariate normal distribution for a scalar <span class="math inline">\(x\)</span> generalises to the* *multivariate normal distribution**
for a vector <span class="math inline">\(\boldsymbol x= (x_1, x_2,...,x_d)^T \sim N_d(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> with with mean <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \mu\)</span> and covariance matrix <span class="math inline">\(\text{Var}(\boldsymbol x) = \boldsymbol \Sigma\)</span>. The corresponding
density is
<span class="math display">\[f(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma) = (2\pi)^{-\frac{d}{2}} \det(\boldsymbol \Sigma)^{-\frac{1}{2}} \exp\left({{-\frac{1}{2}} \underbrace{\underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1 \times d} \underbrace{\boldsymbol \Sigma^{-1}}_{d \times d} \underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d \times 1} }_{1 \times 1 = \text{scalar!}}}\right)\]</span></p>
<p>For <span class="math inline">\(d=1\)</span> we have <span class="math inline">\(bx=x\)</span>, <span class="math inline">\(\boldsymbol \mu= \mu\)</span> and <span class="math inline">\(\boldsymbol \Sigma= \sigma^2\)</span> so that the multivariate normal density reduces
to the univariate normal density.</p>
<div class="example">
<p><span id="exm:mlemultinorm" class="example"><strong>Example 4.1  </strong></span>Maximum likelihood estimates of the parameters of the multivariate normal distribution:</p>
<p>Maximising the log-likelihood based on the multivariate normal density yields the
MLEs for <span class="math inline">\(\boldsymbol \mu\)</span> and <span class="math inline">\(\boldsymbol \Sigma\)</span>. These are generalisations of the MLEs for the mean <span class="math inline">\(\mu\)</span>
and variance <span class="math inline">\(\sigma^2\)</span> of the univariate normal as encountered in Example <a href="03-likelihood3.html#exm:mlenormalmeanvar">3.4</a>.</p>
<p>The estimates can be written in three different ways:</p>
<p><strong>a) data vector notation</strong></p>
<p>with <span class="math inline">\(\boldsymbol x_1,\ldots, \boldsymbol x_n\)</span> the <span class="math inline">\(n\)</span> vector-valued observations from the multivariate normal:</p>
<p>MLE for the mean:
<span class="math display">\[
\hat{\boldsymbol \mu}_{ML} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k = \bar{\boldsymbol x}
\]</span></p>
<p>MLE for the covariance:
<span class="math display">\[\underbrace{\widehat{\boldsymbol \Sigma}_{ML}}_{d \times d} = \frac{1}{n}\sum^{n}_{k=1} \underbrace{\left(\boldsymbol x_k-\bar{\boldsymbol x}\right)}_{d \times 1} \; \underbrace{\left(\boldsymbol x_k-\bar{\boldsymbol x}\right)^T}_{1 \times d}\]</span>
Note the factor <span class="math inline">\(\frac{1}{n}\)</span> in the estimator of the covariance matrix.</p>
<p>With <span class="math inline">\(\overline{\boldsymbol x\boldsymbol x^T} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k \boldsymbol x_k^T\)</span>
we can also write
<span class="math display">\[
\widehat{\boldsymbol \Sigma}_{ML} = \overline{\boldsymbol x\boldsymbol x^T} - \bar{\boldsymbol x} \bar{\boldsymbol x}^T
\]</span></p>
<p><strong>b) data component notation</strong></p>
<p>with <span class="math inline">\(x_{ki}\)</span> the <span class="math inline">\(i\)</span>-th component of the <span class="math inline">\(k\)</span>-th sample <span class="math inline">\(\boldsymbol x_k\)</span>:</p>
<p><span class="math display">\[\hat{\mu}_i = \frac{1}{n}\sum^{n}_{k=1} x_{ki} \text{ with } 
\hat{\boldsymbol \mu}=\begin{pmatrix}
    \hat{\mu}_{1}       \\
    \vdots \\
    \hat{\mu}_{d}
\end{pmatrix}\]</span></p>
<p><span class="math display">\[\hat{\sigma}_{ij} = \frac{1}{n}\sum^{n}_{k=1} \left(x_{ki}-\hat{\mu}_i\right) (\
x_{kj}-\hat{\mu}_j) \text{ with } \widehat{\boldsymbol \Sigma} = (\hat{\sigma}_{ij})
\]</span></p>
<p><strong>c) data matrix notation</strong></p>
<p>with <span class="math inline">\(\boldsymbol X= \begin{pmatrix} \boldsymbol x_1^T \\ ... \\ \boldsymbol x_n^T \\\end{pmatrix}\)</span> as a data matrix containing the samples in its rows. Note that this is the <em>statistics convention</em> — in much of the engineering and computer science literature the data matrix is often transposed and samples are stored in the columns. Thus, the formulas below are only correct assuming the statistics convention.</p>
<p><span class="math display">\[
\hat{\boldsymbol \mu} = \frac{1}{n} \boldsymbol X^T \boldsymbol 1_n
\]</span>
Here <span class="math inline">\(\boldsymbol 1_n\)</span> is a vector of length <span class="math inline">\(n\)</span> containing 1 at each component.</p>
<p><span class="math display">\[
\hat{\boldsymbol \Sigma} = \frac{1}{n} \boldsymbol X^T \boldsymbol X- \hat{\boldsymbol \mu} \hat{\boldsymbol \mu}^T
\]</span>
To simplify the expression for the estimate of the covariance matrix
one often assumes that the data matrix is centered, i.e. that <span class="math inline">\(\hat{\boldsymbol \mu} = 0\)</span>.</p>
<p>Because of the ambiguity in convention (machine learning vs statistics convention) and the often implicit use
of centered data matrices the matrix notation is often confusing. Hence, using the other two
notations is generally preferable.</p>
</div>
</div>
</div>
<div id="approximate-distribution-of-maximum-likelihood-estimates" class="section level2">
<h2><span class="header-section-number">4.2</span> Approximate distribution of maximum likelihood estimates</h2>
<div id="quadratic-log-likelihood-resulting-from-normal-model" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Quadratic log-likelihood resulting from normal model</h3>
<p>Assume we observe a single sample <span class="math inline">\(\boldsymbol x\sim N(\boldsymbol \mu, \boldsymbol \Sigma^2)\)</span> with known covariance. The corresponding log-likelihood for <span class="math inline">\(\boldsymbol \mu\)</span> is
<span class="math display">\[
l_1(\boldsymbol \mu) = C - \frac{1}{2}(\boldsymbol x-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu)
\]</span>
where <span class="math inline">\(C\)</span> is a constant that does not depend on <span class="math inline">\(\boldsymbol \mu\)</span>.
Note that the log-likelihood is exactly quadratic and the maximum lies
at <span class="math inline">\((\boldsymbol x, C)\)</span>.</p>
</div>
<div id="quadratic-approximation-of-a-log-likelihood-function" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Quadratic approximation of a log-likelihood function</h3>
<p>Now consider the quadratic approximation of the log-likelihood function <span class="math inline">\(l_n(\boldsymbol \theta)\)</span> for
a general model around the MLE <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span>.</p>
<p><img src="fig/lecture4_p1.PNG" width="80%" style="display: block; margin: auto;" /></p>
<p>We assume the model is regular with
<span class="math inline">\(\nabla l_n(\hat{\boldsymbol \theta}_{ML} ) = 0\)</span>. The Taylor series approximation of scalar-valued function <span class="math inline">\(f(\boldsymbol x)\)</span> around <span class="math inline">\(\boldsymbol x_0\)</span> is
<span class="math display">\[
f(\boldsymbol x) = f(\boldsymbol x_0) + \nabla f(\boldsymbol x_0) (\boldsymbol x-\boldsymbol x_0) + \frac{1}{2} 
(\boldsymbol x-\boldsymbol x_0)^T \nabla^T \nabla f(\boldsymbol x_0) (\boldsymbol x-\boldsymbol x_0) + \ldots
\]</span>
Applied to the log-likelihood function this yields</p>
<p><span class="math display">\[l_n(\boldsymbol \theta) \approx l_n(\hat{\boldsymbol \theta}_{ML})- \frac{1}{2}(\hat{\boldsymbol \theta}_{ML}- \boldsymbol \theta)^T J_n(\hat{\boldsymbol \theta}_{ML})(\hat{\boldsymbol \theta}_{ML}-\boldsymbol \theta)\]</span></p>
<p>This is a quadratic function with maximum at <span class="math inline">\(( \hat{\boldsymbol \theta}_{ML} , l_n(\hat{\boldsymbol \theta}_{ML})\)</span>.
Note the natural appearance
of the observed Fisher information <span class="math inline">\(J_n(\hat{\boldsymbol \theta}_{ML})\)</span> in the quadratic term.
There is no linear term because of the vanishing gradient at the MLE.</p>
<p>Crucially, we realise that the approximation has the same form as if <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> was a sample
from a multivariate normal distribution with mean <span class="math inline">\(\boldsymbol \theta\)</span> and with covariance given by the <em>inverse</em>
observed Fisher information! Note that this requires a positive definite observed
Fisher information matrix so that <span class="math inline">\(J_n(\hat{\boldsymbol \theta}_{ML})\)</span> is actually invertible!</p>
<div class="example">
<p><span id="exm:quadapproxproportion" class="example"><strong>Example 4.2  </strong></span>Quadratic approximation of the log-likelihood for a proportion:</p>
<p>From Example <a href="03-likelihood3.html#exm:mleproportion">3.1</a> we have the log-likelihood
<span class="math display">\[
l_n(p) = n \left( \bar{x} \log p + (1-\bar{x}) \log(1-p) \right)
\]</span>
and the MLE
<span class="math display">\[
\hat{p}_{ML} = \bar{x} 
\]</span>
and from Example <a href="03-likelihood3.html#exm:obsfisherproportion">3.5</a> the observed Fisher information
<span class="math display">\[
\begin{split}
J_n(\hat{p}_{ML}) = \frac{n}{\bar{x} (1-\bar{x})}
\end{split}
\]</span>
The log-likelihood at the MLE is
<span class="math display">\[
l_n(\hat{p}_{ML}) = n \left( \bar{x} \log \bar{x} + (1-\bar{x}) \log(1-\bar{x}) \right)
\]</span>
This allows us to construct the quadratic approximation of the log-likelihood
around the MLE as
<span class="math display">\[
\begin{split}
l_n(p) &amp; \approx  l_n(\hat{p}_{ML}) - \frac{1}{2} J_n(\hat{p}_{ML}) (p-\hat{p}_{ML})^2 \\
   &amp;= n \left( \bar{x} \log \bar{x} + (1-\bar{x}) \log(1-\bar{x}) - \frac{(p-\bar{x})^2}{2 \bar{x} (1-\bar{x})}  \right) \\
&amp;=  C + \frac{ \bar{x} p -\frac{1}{2} p^2}{ \bar{x} (1-\bar{x})/n} \\
\end{split}
\]</span>
The constant <span class="math inline">\(C\)</span> does not depend on <span class="math inline">\(p\)</span>, its only purpose is to match the approximate log-likelihood at the MLE with that of the corresponding original log-likelihood. The
approximate log-likelihood takes on the form of a normal log-likelihood
(Example <a href="03-likelihood3.html#exm:mlenormalmean">3.3</a>) for one observation
of <span class="math inline">\(\hat{p}_{ML}=\bar{x}\)</span> from <span class="math inline">\(N\left(p, \frac{\bar{x} (1-\bar{x})}{n} \right)\)</span>.</p>
<p>The following figure shows the above log-likelihood function and its quadratic approximation
for example data with <span class="math inline">\(n = 30\)</span> and <span class="math inline">\(\bar{x} = 0.7\)</span>:</p>
<p><img src="04-likelihood4_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
</div>
<div id="asymptotic-normality-of-maximum-likelihood-estimates" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Asymptotic normality of maximum likelihood estimates</h3>
<p>Intuitively, it makes sense to associate large amount of curvature of the log-likelihood at the MLE with low variance of the MLE (and conversely, low amount of curvature with high variance).</p>
<p>From the above we see that</p>
<ul>
<li>normality implies a quadratic log-likelihood,</li>
<li>conversely, taking an quadratic approximation of the log-likelihood implies
approximate normality, and</li>
<li>in the quadratic approximation <strong>the inverse observed Fisher information plays the role of the covariance</strong> of the MLE.</li>
</ul>
<p>This suggests the following theorem: <strong>Asymptotically, the MLE is normally distributed around the true parameter and with covariance equal to the inverse of the observed Fisher information</strong>:</p>
<p><span class="math display">\[\hat{\boldsymbol \theta}_{ML} \overset{a}{\sim}\underbrace{N_d}_{\text{multivariate normal}}\left(\underbrace{\boldsymbol \theta}_{\text{mean vector}},\underbrace{\boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})^{-1}}_{\text{ covariance matrix}}\right)\]</span></p>
<p>This theorem about the distributional properties of MLEs greatly enhances the usefulness of the method of maximum likelihood. It implies that in regular settings maximum likelihood is not just a method for obtaining point estimates but also also provides estimates of their uncertainty.</p>
<p>However, we need to clarify what “asymptotic” actually means in the context of the above theorem:</p>
<ol style="list-style-type: decimal">
<li><p>Primarily, it means to have suffient sample size so that the log-likelihood <span class="math inline">\(l_n(\boldsymbol \theta)\)</span>
is sufficiently well approximated by a quadratic function around <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span>.
The better the local quadratic approximation the better the normal approximation!</p></li>
<li><p>In a regular model with positive definite observed Fisher information matrix this is guaranteed for large sample size <span class="math inline">\(n \rightarrow \infty\)</span> thanks to the central limit theorem).</p></li>
<li><p>However, <span class="math inline">\(n\)</span> going to infinity is in fact not always required for the normal approximation to hold!
Depending on the particular model a good local fit to a quadratic log-likelihood
may be available also for finite <span class="math inline">\(n\)</span>. As a trivial example, for the normal log-likelihood it is valid for any <span class="math inline">\(n\)</span>.</p></li>
<li><p>In the other hand, in non-regular models (with nondifferentiable log-likelihood at the MLE and/or a singular Fisher information matrix) no amount of data, not even <span class="math inline">\(n\rightarrow \infty\)</span>, will make the quadratic approximation work.</p></li>
</ol>
<p>Remarks:</p>
<ul>
<li><p>The technical details of the above considerations are worked out in the theory of <a href="https://en.wikipedia.org/wiki/Local_asymptotic_normality">locally asymptotically normal (LAN) models</a> pioneered in 1960 by <a href="https://en.wikipedia.org/wiki/Lucien_Le_Cam">Lucien LeCam (1924–2000)</a>.</p></li>
<li><p>There are also methods to obtain higher-order (higher than quadratic and thus non-normal) asymptotic approximations. These relate to so-called <a href="https://en.wikipedia.org/wiki/Saddlepoint_approximation_method">saddle point approximations</a>.</p></li>
</ul>
</div>
<div id="asymptotic-optimal-efficiency" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Asymptotic optimal efficiency</h3>
<p>Assume now that <span class="math inline">\(\hat{\boldsymbol \theta}\)</span> is an arbitrary and unbiased estimator for <span class="math inline">\(\boldsymbol \theta\)</span> and
the underlying data generating model is regular with density <span class="math inline">\(f(\boldsymbol x| \boldsymbol \theta)\)</span>.</p>
<p><a href="https://en.wikipedia.org/wiki/Harald_Cram%C3%A9r">H. Cramér (1893–1985)</a>,
<a href="https://en.wikipedia.org/wiki/C._R._Rao">C. R. Rao (1920–)</a>
and others demonstrated in 1945 the so-called <strong>information inequality</strong>,
<span class="math display">\[
\text{Var}(\hat{\boldsymbol \theta}) \geq \frac{1}{n} \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)^{-1}
\]</span>
which puts a lower bound on the variance of an estimator for <span class="math inline">\(\boldsymbol \theta\)</span>.
(Note for <span class="math inline">\(d&gt;1\)</span> this is a matrix inequality, meaning that the difference matrix is positive semidefinite).</p>
<p>For large sample size with <span class="math inline">\(n \rightarrow \infty\)</span> and <span class="math inline">\(\hat{\boldsymbol \theta}_{ML} \rightarrow \boldsymbol \theta\)</span> the observed
Fisher information becomes
<span class="math inline">\(J_n(\hat{\boldsymbol \theta}) \rightarrow n \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)\)</span>
and therefore we can write the asymptotic distribution of <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> as
<span class="math display">\[
\hat{\boldsymbol \theta}_{ML} \overset{a}{\sim} N_d\left(  \boldsymbol \theta,  \frac{1}{n} \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)^{-1}  \right)
\]</span>
This means that for large <span class="math inline">\(n\)</span> in regular models <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> achieves the lowest variance possible according to the Cramér-Rao information inequality. In other words, for large sample size maximum likelihood is optimally efficient and thus the best available estimator will in fact be the MLE!</p>
<p>However, as we will see later this does not hold for small sample size where it is indeed possible (and necessary) to improve over the MLE (e.g. via Bayesian estimation or regularisation).</p>
</div>
</div>
<div id="quantifying-the-uncertainty-of-maximum-likelihood-estimates" class="section level2">
<h2><span class="header-section-number">4.3</span> Quantifying the uncertainty of maximum likelihood estimates</h2>
<div id="estimating-the-variance-of-mles" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Estimating the variance of MLEs</h3>
<p>In the previous section we saw that MLEs are asymptotically normally distributed,
with the inverse Fisher information (both expected and observed) linked to the asymptotic variance.</p>
<p>This leads to the question whether to use the observed Fisher information
<span class="math inline">\(J_n(\hat{\boldsymbol \theta}_{ML})\)</span> or the expected Fisher information at the MLE
<span class="math inline">\(n \boldsymbol I^{\text{Fisher}}( \hat{\boldsymbol \theta}_{ML} )\)</span> to estimate the variance of the MLE?</p>
<ul>
<li>Clearly, for <span class="math inline">\(n\rightarrow \infty\)</span> both can be used interchangeably.</li>
<li>However, they can be very different for finite <span class="math inline">\(n\)</span>
in particular for models outside the exponential family.</li>
<li>Also normality may occur well before <span class="math inline">\(n\)</span> goes to <span class="math inline">\(\infty\)</span>.</li>
</ul>
<p>Therefore one needs to choose between the two, considering also that</p>
<ul>
<li>the expected Fisher information at the MLE is the average curvature at the MLE,
whereas the observed Fisher information is the actual observed curvature, and</li>
<li>the observed Fisher information naturally occurs in the quadratic approximation of the log-likelihood.</li>
</ul>
<p>All in all, the observed Fisher information as estimator of the variance is more appropriate
as it is based on the actual observed data and also works for large <span class="math inline">\(n\)</span> (in which case it yields
the same result as using expected Fisher information):
<span class="math display">\[
\widehat{\text{Var}}(\hat{\boldsymbol \theta}_{ML}) = \boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})^{-1}
\]</span>
and its square-root as the estimate of the standard deviation
<span class="math display">\[
\widehat{\text{SD}}(\hat{\boldsymbol \theta}_{ML}) = \boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})^{-1/2}
\]</span>
Note that in the above we use <em>matrix inversion</em> and the (inverse) <em>matrix square root</em>.</p>
<p>The reasons for preferring observed Fisher information are made mathematically precise in a classic paper by <span class="citation">Efron and Hinkley (<a href="22-references.html#ref-EfronHinkley1978">1978</a>)</span>.</p>
<div class="example">
<p><span id="exm:distproportion" class="example"><strong>Example 4.3  </strong></span>Estimated variance and distribution of the MLE of a proportion:</p>
<p>From Examples <a href="03-likelihood3.html#exm:mleproportion">3.1</a> and <a href="03-likelihood3.html#exm:obsfisherproportion">3.5</a>
we know the MLE
<span class="math display">\[
\hat{p}_{ML} = \bar{x} = \frac{k}{n}
\]</span>
and the corresponding observed Fisher information
<span class="math display">\[
J_n(\hat{p}_{ML})=\frac{n}{\hat{p}_{ML}(1-\hat{p}_{ML})}
\]</span>
The estimated variance of the MLE is therefore
<span class="math display">\[
\widehat{\text{Var}}(   \hat{p}_{ML}  ) = \frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n}
\]</span>
and the corresponding asymptotic normal distribution is
<span class="math display">\[
 \hat{p}_{ML} \overset{a}{\sim} N\left(p,   \frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n}   \right)
\]</span></p>
</div>
<div class="example">
<p><span id="exm:distnormalmean" class="example"><strong>Example 4.4  </strong></span>Estimated variance and distribution of the MLE of the mean parameter for the normal distribution with known variance:</p>
<p>From Examples <a href="03-likelihood3.html#exm:mlenormalmean">3.3</a> and <a href="03-likelihood3.html#exm:obsfishernormalmean">3.7</a> we know that
<span class="math display">\[\hat{\mu}_{ML} =\bar{x}\]</span>
and that the corresponding observed Fisher information at <span class="math inline">\(\hat{\mu}_{ML}\)</span> is
<span class="math display">\[J_n(\hat{\mu}_{ML})=\frac{n}{\sigma^2}\]</span></p>
<p>The estimated variance of the MLE is therefore
<span class="math display">\[
\widehat{\text{Var}}(\hat{\mu}_{ML}) = \frac{\sigma^2}{n}
\]</span>
and the corresponding asymptotic normal distribution is
<span class="math display">\[
\hat{\mu}_{ML} \sim N\left(\mu,\frac{\sigma^2}{n}\right)
\]</span></p>
<p>Note that in this case the distribution is not asymptotic but is <strong>exact</strong>, i.e. valid
also for small <span class="math inline">\(n\)</span> (as long as the data <span class="math inline">\(x_i\)</span> are actually from <span class="math inline">\(N(\mu, sigma^2)\)</span>!).</p>
</div>
</div>
<div id="wald-statistic" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Wald statistic</h3>
<p>Centering the MLE <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> with <span class="math inline">\(\boldsymbol \theta_0\)</span> followed by
standardising with <span class="math inline">\(\widehat{\text{SD}}(\hat{\boldsymbol \theta}_{ML})\)</span> yields the <strong>Wald statistic</strong>
(named after <a href="https://en.wikipedia.org/wiki/Abraham_Wald">Abraham Wald, 1902–1950</a>):
<span class="math display">\[
\begin{split}
\boldsymbol t(\boldsymbol \theta_0) &amp; = \widehat{\text{SD}}(\hat{\boldsymbol \theta}_{ML})^{-1}(\hat{\boldsymbol \theta}_{ML}-\boldsymbol \theta_0)\\
 &amp; = \boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})^{1/2}(\hat{\boldsymbol \theta}_{ML}-\boldsymbol \theta_0)\\
\end{split}
\]</span></p>
<p>For scalar <span class="math inline">\(\theta\)</span> this simplifies to
<span class="math display">\[
\begin{split}
t(\theta_0) &amp;= \frac{\hat{\theta}_{ML}-\theta_0}{\widehat{\text{SD}}(\hat{\theta}_{ML})} \\
&amp;= J_n(\hat{\theta}_{ML})^{1/2} (\hat{\theta}_{ML}-\theta_0)\\
\end{split}
\]</span></p>
<p>We now assume that the true underlying parameter is <span class="math inline">\(\boldsymbol \theta_0\)</span>. Since the MLE is asymptotically normal the Wald statistic
is asymptotically <strong>standard normal</strong> distributed as follows:
<span class="math display">\[\begin{align*}
\begin{array}{cc}
\boldsymbol t(\boldsymbol \theta_0) \overset{a}{\sim}\\
t(\theta_0) \overset{a}{\sim}\\
\end{array}
\begin{array}{ll}
N_d(0,\boldsymbol I_d)\\
N(0,1)\\
\end{array}
\begin{array}{ll}
  \text{for vector } \boldsymbol \theta\\
  \text{for scalar } \theta\\
\end{array}
\end{align*}\]</span></p>
<p>Correspondingly, the <strong>squared</strong> Wald statistic is chi-squared distributed
assuming <span class="math inline">\(\boldsymbol \theta_0\)</span> as true parameter:
<span class="math display">\[\begin{align*}
\begin{array}{cc}
\boldsymbol t(\boldsymbol \theta_0)^T \boldsymbol t(\boldsymbol \theta_0)\\
t(\theta_0)^2\\
\end{array}
\begin{array}{ll}
\overset{a}{\sim}\chi^2_d\\
\overset{a}{\sim}\chi^2_1\\
\end{array}
\begin{array}{ll}
  \text{for vector } \boldsymbol \theta\\
  \text{for scalar } \theta\\
\end{array}
\end{align*}\]</span>
Note that the degree of freedom of the chi-squared distribution is the dimension of <span class="math inline">\(d\)</span>.</p>
<div class="example">
<p><span id="exm:waldproportion" class="example"><strong>Example 4.5  </strong></span>Wald statistic for a proportion:</p>
<p>We continue from Example <a href="04-likelihood4.html#exm:distproportion">4.3</a>.
With <span class="math inline">\(\hat{p}_{ML} = \bar{x}\)</span>
and
<span class="math inline">\(\widehat{\text{Var}}( \hat{p}_{ML} ) = \frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n}\)</span>
and thus <span class="math inline">\(\widehat{\text{SD}}( \hat{p}_{ML} ) =\sqrt{ \frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n} }\)</span>
we get as <strong>Wald statistic</strong>:</p>
<p><span class="math display">\[
t(p_0) = \frac{\bar{x}-p_0}{ \sqrt{\bar{x}(1-\bar{x}) / n }  }\overset{a}{\sim} N(0,1)
\]</span></p>
<p>The <strong>squared Wald statistic</strong> is:
<span class="math display">\[t(p_0)^2 = n \frac{(\bar{x}-p_0)^2}{ \bar{x}(1-\bar{x})   }\overset{a}{\sim} \chi^2_1 \]</span></p>
</div>
<div class="example">
<p><span id="exm:waldnormalmean" class="example"><strong>Example 4.6  </strong></span>Wald statistic for the mean parameter of a normal distribution with known variance:</p>
<p>We continue from Example <a href="04-likelihood4.html#exm:distnormalmean">4.4</a>.
With <span class="math inline">\(\hat{\mu}_{ML} =\bar{x}\)</span> and
<span class="math inline">\(\widehat{\text{Var}}(\hat{\mu}_{ML}) = \frac{\sigma^2}{n}\)</span>
and thus <span class="math inline">\(\widehat{\text{SD}}(\hat{\mu}_{ML}) = \frac{\sigma}{\sqrt{n}}\)</span>
we get as <strong>Wald statistic</strong>:</p>
<p><span class="math display">\[t(\mu_0) = \frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}}\sim N(0,1)\]</span>
Note this is the one sample <span class="math inline">\(t\)</span>-statistic with given <span class="math inline">\(\sigma\)</span>.
The <strong>squared Wald statistic</strong> is:
<span class="math display">\[t(\mu_0)^2 = \frac{(\bar{x}-\mu_0)^2}{\sigma^2 / n}\sim \chi^2_1 \]</span></p>
<p>Again, in this instance this is the exact distribution, not just the asymptotic one.</p>
<p>Using the Wald or the squared Wald statistics we can test whether a particular
<span class="math inline">\(\mu_0\)</span> can be rejected as underlying true parameter, and we can also
construct corresponding confidence intervals.</p>
</div>
</div>
<div id="normal-confidence-intervals-using-the-wald-statistic" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Normal confidence intervals using the Wald statistic</h3>
<p>The asymptotic normality of MLEs derived from regular models enables us to construct a
corresponding normal confidence interval (CI):</p>
<p><img src="fig/lecture4_p2.PNG" width="80%" style="display: block; margin: auto;" /></p>
<p>For example, to construct the asymptotic normal CI for the MLE of
a scalar parameter <span class="math inline">\(\theta\)</span> we use the MLE <span class="math inline">\(\hat{\theta}_{ML}\)</span> as estimate of the mean
and its standard deviation <span class="math inline">\(\widehat{\text{SD}}(\hat{\theta}_{ML})\)</span> computed from the observed Fisher information:</p>
<p><span class="math display">\[\text{CI}=[\hat{\theta}_{ML} \pm c_{normal} \widehat{\text{SD}}(\hat{\theta}_{ML})]\]</span></p>
<p><span class="math inline">\(c_{normal}\)</span> is a critical value for the standard-normal symmetric confidence interval
chosen to achieve the desired nominal coverage-
The critical values are computed using the inverse standard normal distribution function via
<span class="math inline">\(c_{\text{normal}}=\Phi^{-1}\left(\frac{1+\kappa}{2}\right)\)</span>
(cf. refresher section in the Appendix).</p>
<table>
<thead>
<tr class="header">
<th>coverage <span class="math inline">\(\kappa\)</span></th>
<th>Critical value <span class="math inline">\(c_{\text{normal}}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>1.64</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>1.96</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>2.58</td>
</tr>
</tbody>
</table>
<p>For example, for a CI with 95% coverage one uses the factor 1.96 so that
<span class="math display">\[\text{CI}=[\hat{\theta}_{ML} \pm 1.96\, \widehat{\text{SD}}(\hat{\theta}_{ML}) ]\]</span></p>
<p>The normal CI can be expressed using Wald statistics as follows:</p>
<p><span class="math display">\[\text{CI}=\{\theta_0:  | t(\theta_0)| &lt; c_{\text{normal}} \}\]</span></p>
<p>Similary, it can also be expressed using the squared Wald statistics:</p>
<p><span class="math display">\[\text{CI}=\{\theta_0:   \boldsymbol t(\boldsymbol \theta_0)^T \boldsymbol t(\boldsymbol \theta_0) &lt; c_{\text{chisq}} \}\]</span>
Note that this form facilitates the construction of normal confidence intervals
for a parameter vector.</p>
<p>The following lists containst the critical values resulting from the chi-squared distribution
with degree of freedom <span class="math inline">\(m=1\)</span> for the three most common choices of
coverage <span class="math inline">\(\kappa\)</span> for a normal CI for a univariate parameter:</p>
<table>
<thead>
<tr class="header">
<th>coverage <span class="math inline">\(\kappa\)</span></th>
<th>Critical value <span class="math inline">\(c_{\text{chisq}}\)</span> (<span class="math inline">\(m=1\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>2.71</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>3.84</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>6.63</td>
</tr>
</tbody>
</table>
<div class="example">
<p><span id="exm:ciproportion" class="example"><strong>Example 4.7  </strong></span>Asymptotic normal confidence interval for a proportion:</p>
<p>We continue from Examples <a href="04-likelihood4.html#exm:distproportion">4.3</a> and <a href="04-likelihood4.html#exm:waldproportion">4.5</a>.
Assume we observe <span class="math inline">\(n=30\)</span> measurements with average <span class="math inline">\(\bar{x} = 0.7\)</span>.
Then <span class="math inline">\(\hat{p}_{ML} = \bar{x} = 0.7\)</span> and
<span class="math inline">\(\widehat{\text{SD}}(\hat{p}_{ML}) = \sqrt{ \frac{ \bar{x}(1-\bar{x})}{n} } \approx 0.084\)</span>.</p>
<p>The symmetric asymptotic normal CI for <span class="math inline">\(p\)</span> with 95% coverage is given by
<span class="math inline">\(\hat{p}_{ML} \pm 1.96 \, \widehat{\text{SD}}(\hat{p}_{ML})\)</span> which for the present data results in the interval <span class="math inline">\([0.536, 0.864]\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:cinormalmean" class="example"><strong>Example 4.8  </strong></span>Normal confidence interval for the mean:</p>
<p>We continue from Examples <a href="04-likelihood4.html#exm:distnormalmean">4.4</a> and <a href="04-likelihood4.html#exm:waldnormalmean">4.6</a>.
Assume that we observe <span class="math inline">\(n=25\)</span> measurements with average <span class="math inline">\(\bar{x} = 10\)</span>, from a normal
with unknown mean and variance <span class="math inline">\(\sigma^2=4\)</span>.</p>
<p>Then <span class="math inline">\(\hat{\mu}_{ML} = \bar{x} = 10\)</span> and
<span class="math inline">\(\widehat{\text{SD}}(\hat{\mu}_{ML}) = \sqrt{ \frac{ \sigma^2}{n} } = \frac{2}{5}\)</span>.</p>
<p>The symmetric asymptotic normal CI for <span class="math inline">\(p\)</span> with 95% coverage is given by
<span class="math inline">\(\hat{\mu}_{ML} \pm 1.96 \, \widehat{\text{SD}}(\hat{\mu}_{ML})\)</span> which for the present data results in the interval <span class="math inline">\([9.216, 10.784]\)</span>.</p>
</div>
</div>
<div id="normal-tests-using-the-wald-statistic" class="section level3">
<h3><span class="header-section-number">4.3.4</span> Normal tests using the Wald statistic</h3>
<p>Finally, recall the <strong>duality between confidence intervals and statistical tests</strong>. Specifically,
a confidence interval with coverage <span class="math inline">\(\kappa\)</span> can be also used for testing as follows.</p>
<ul>
<li>for every <span class="math inline">\(\theta_0\)</span> inside the CI the data do not allow to reject the hypothesis that <span class="math inline">\(\theta_0\)</span> is the true parameter with significance level <span class="math inline">\(1-\kappa\)</span>.</li>
<li>Conversely, all values <span class="math inline">\(\theta_0\)</span> outside the CI can be rejected to be the true parameter with significance level <span class="math inline">\(1-\kappa\)</span> .</li>
</ul>
<p>Hence, in order to test whether <span class="math inline">\(\boldsymbol \theta_0\)</span> is the true underlying parameter value we can
compute the corresponding (squared) Wald statistic, find the desired critical
value and then decide on rejection.</p>
<div class="example">
<p><span id="exm:normaltestproportion" class="example"><strong>Example 4.9  </strong></span>Asymptotic normal test for a proportion:</p>
<p>We continue from Example <a href="04-likelihood4.html#exm:ciproportion">4.7</a>.</p>
<p>We now consider two possible values (<span class="math inline">\(p_0=0.5\)</span> and <span class="math inline">\(p_0=0.8\)</span>) as potentially true underlying proportion.</p>
<p>The value <span class="math inline">\(p_0=0.8\)</span> lies inside the 95% confidence interval <span class="math inline">\([0.536, 0.864]\)</span>. This implies we cannot reject the hypthesis that this is the true underlying parameter on 5% significance
level. In contrast, <span class="math inline">\(p_0=0.5\)</span> is outside the
confidence interval so we can indeed reject this value. In other words, data plus model
exlude this value as statistically implausible.</p>
<p>This can be verified more directly by computing the corresponding (squared) Wald statistics
(see Example <a href="04-likelihood4.html#exm:waldproportion">4.5</a>) and comparing them with the relevant critical value (3.84 from chi-squared distribution for 5% significance level):</p>
<ul>
<li><span class="math inline">\(t(0.5)^2 = 5.71 &gt; 3.84\)</span> hence <span class="math inline">\(p_0=0.5\)</span> can be rejected.</li>
<li><span class="math inline">\(t(0.8)^2 = 1.43 &lt; 3.84\)</span> hence <span class="math inline">\(p_0=0.8\)</span> cannot be rejected.</li>
</ul>
<p>Note that the squared Wald statistic at the boundaries of the normal confidence interval
is equal to the critical value.</p>
</div>
<div class="example">
<p><span id="exm:normaltestnormalmean" class="example"><strong>Example 4.10  </strong></span>Normal confidence interval and test for the mean:</p>
<p>We continue from Example <a href="04-likelihood4.html#exm:cinormalmean">4.8</a>.</p>
<p>We now consider two possible values (<span class="math inline">\(\mu_0=9.5\)</span> and <span class="math inline">\(\mu_0=11\)</span>) as potentially true underlying mean parameter.</p>
<p>The value <span class="math inline">\(\mu_0=9.5\)</span> lies inside the 95% confidence interval <span class="math inline">\([9.216, 10.784]\)</span>. This implies we cannot reject the hypthesis that this is the true underlying parameter on 5% significance
level. In contrast, <span class="math inline">\(\mu_0=11\)</span> is outside the
confidence interval so we can indeed reject this value. In other words, data plus model
exlude this value as a statistically implausible.</p>
<p>This can be verified more directly by computing the corresponding (squared) Wald statistics
(see Example <a href="04-likelihood4.html#exm:waldnormalmean">4.6</a>) and comparing them with the relevant critical values:</p>
<ul>
<li><span class="math inline">\(t(9.5)^2 = 1.56 &lt; 3.84\)</span> hence <span class="math inline">\(\mu_0=9.5\)</span> cannot be rejected.</li>
<li><span class="math inline">\(t(11)^2 = 6.25 &gt; 3.84\)</span> hence <span class="math inline">\(\mu_0=11\)</span> can be rejected.</li>
</ul>
<p>The squared Wald statistic at the boundaries of the confidence interval
equals the critical value.</p>
<p>Note that this is the standard one-sample test of the mean, and that it is exact,
not an approximation.</p>
</div>
</div>
</div>
<div id="example-of-a-non-regular-model" class="section level2">
<h2><span class="header-section-number">4.4</span> Example of a non-regular model</h2>
<p>Not all models allow a quadratic approximation of the log-likelihood function around the MLE. This is the case when the log-likelihood function is not differentiable at the MLE. These models are called non-regular and for those models the normal approximation is not available.</p>
<div class="example">
<p><span id="exm:nonregular" class="example"><strong>Example 4.11  </strong></span>Uniform distribution with upper bound <span class="math inline">\(\theta\)</span>:
<span class="math display">\[x_1,\dots,x_n \sim U(0,\theta)\]</span>
With <span class="math inline">\(x_{[i]}\)</span> we denote the <em>ordered</em> observations with
<span class="math inline">\(0 \leq x_{[1]} &lt; x_{[2]} &lt; \ldots &lt; x_{[n]} \leq \theta\)</span> and
<span class="math inline">\(x_{[n]} = \max(x_1,\dots,x_n)\)</span>.</p>
<p>We would like to obtain both the maximum likelihood estimator
<span class="math inline">\(\hat{\theta}_{ML}\)</span> and its distribution.</p>
<p>The probability density function of <span class="math inline">\(U(0,\theta)\)</span> is
<span class="math display">\[f(x|\theta) =\begin{cases}
    \frac{1}{\theta} &amp;\text{if } x \in [0,\theta] \\
    0              &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<p><img src="fig/lecture5_p4.PNG" width="80%" style="display: block; margin: auto;" />
and on the log-scale
<span class="math display">\[
\log f(x|\theta) =\begin{cases}
    - \log \theta &amp;\text{if } x \in [0,\theta] \\
    - \infty              &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<p>Since all observed data <span class="math inline">\(x_1, \ldots, x_n\)</span> lie in the interval <span class="math inline">\([0,\theta]\)</span>
we get as log-likelihood function
<span class="math display">\[
l_n(\theta) =\begin{cases}
    -n\log \theta  &amp;\text{for } x_{[n]} \leq \theta \\
    - \infty              &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>Obtaining the MLE of <span class="math inline">\(\theta\)</span> is straightforward: <span class="math inline">\(-n\log \theta\)</span> is monotonically decreasing therefore the log-likelihood function has a maximum at <span class="math inline">\(\hat{\theta}_{ML}=x_{[n]}\)</span>.</p>
<p>However, there is a discontinuity in <span class="math inline">\(l_n(\theta)\)</span> at <span class="math inline">\(x_{[n]}\)</span> and therefore
<span class="math inline">\(l_n(\theta)\)</span> <strong>is not differentiable</strong> at <span class="math inline">\(\hat{\theta}_{ML}\)</span>.
Thus, <strong>there is no quadratic approximation around <span class="math inline">\(\hat{\theta}_{ML}\)</span></strong>
and the <strong>observed Fisher information cannot be computed</strong>.
Hence, the normal approximation for the distribution of <span class="math inline">\(\hat{\theta}_{ML}\)</span> is not valid regardless of sample size, i.e. not even asymptotically for <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>Nonetheless, we can in fact still obtain the sampling distribution of <span class="math inline">\(\hat{\theta}_{ML}=x_{[n]}\)</span>. However, <em>not</em> via asymptotic arguments but instead by understanding that <span class="math inline">\(x_{[n]}\)</span> is an order statistic (see <a href="https://en.wikipedia.org/wiki/Order_statistic" class="uri">https://en.wikipedia.org/wiki/Order_statistic</a> ) with the following properties:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cc}
x_{[n]}\sim \theta \, \text{Beta}(n,1)\\
\\
\text{E}(x_{[n]})=\frac{n}{n+1} \theta\\
\\
\text{Var}(x_{[n]})=\frac{n}{(n+1)^2(n+2)}\theta^2\\
\end{array}
\begin{array}{ll}
\text{&quot;n-th order statistic&quot; }\\
\\
\\
\\
\approx \frac{\theta^2}{n^2}\\
\end{array}
\end{align*}\]</span></p>
<p>Note that the variance decreases with <span class="math inline">\(\frac{1}{n^2}\)</span> which is much faster than the usual <span class="math inline">\(\frac{1}{n}\)</span> of an “efficient” estimator. Correspondingly,
<span class="math inline">\(\hat{\theta}_{ML}\)</span> is a so-called “super efficient” estimator.</p>
</div>

<p></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="03-likelihood3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="05-likelihood5.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math20802-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
