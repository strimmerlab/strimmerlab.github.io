<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Quadratic approximation and normal asymptotics | HTML</title>
  <meta name="description" content="Statistical Methods:<br />
Likelihood, Bayes and Regression</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Quadratic approximation and normal asymptotics | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Quadratic approximation and normal asymptotics | HTML" />
  
  
  



<meta name="date" content="2021-01-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="03-likelihood3.html"/>
<link rel="next" href="05-likelihood5.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH20802/index.html">MATH20802 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Likelihood estimation and inference</b></span></li>
<li class="chapter" data-level="1" data-path="01-likelihood1.html"><a href="01-likelihood1.html"><i class="fa fa-check"></i><b>1</b> Overview of statistical learning</a><ul>
<li class="chapter" data-level="1.1" data-path="01-likelihood1.html"><a href="01-likelihood1.html#how-to-learn-from-data"><i class="fa fa-check"></i><b>1.1</b> How to learn from data?</a></li>
<li class="chapter" data-level="1.2" data-path="01-likelihood1.html"><a href="01-likelihood1.html#probability-theory-versus-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Probability theory versus statistical learning</a></li>
<li class="chapter" data-level="1.3" data-path="01-likelihood1.html"><a href="01-likelihood1.html#cartoon-of-statistical-learning"><i class="fa fa-check"></i><b>1.3</b> Cartoon of statistical learning</a></li>
<li class="chapter" data-level="1.4" data-path="01-likelihood1.html"><a href="01-likelihood1.html#likelihood"><i class="fa fa-check"></i><b>1.4</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="02-likelihood2.html"><a href="02-likelihood2.html"><i class="fa fa-check"></i><b>2</b> From information theory to likelihood</a><ul>
<li class="chapter" data-level="2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy"><i class="fa fa-check"></i><b>2.1</b> Entropy</a><ul>
<li class="chapter" data-level="2.1.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#surprise"><i class="fa fa-check"></i><b>2.1.2</b> Surprise</a></li>
<li class="chapter" data-level="2.1.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#shannon-entropy"><i class="fa fa-check"></i><b>2.1.3</b> Shannon entropy</a></li>
<li class="chapter" data-level="2.1.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#differential-entropy"><i class="fa fa-check"></i><b>2.1.4</b> Differential entropy</a></li>
<li class="chapter" data-level="2.1.5" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-entropy-principle-to-characterise-distributions"><i class="fa fa-check"></i><b>2.1.5</b> Maximum entropy principle to characterise distributions</a></li>
<li class="chapter" data-level="2.1.6" data-path="02-likelihood2.html"><a href="02-likelihood2.html#cross-entropy"><i class="fa fa-check"></i><b>2.1.6</b> Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>2.2</b> Kullback-Leibler divergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#definition"><i class="fa fa-check"></i><b>2.2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#properties"><i class="fa fa-check"></i><b>2.2.2</b> Properties</a></li>
<li class="chapter" data-level="2.2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#examples"><i class="fa fa-check"></i><b>2.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#local-quadratic-approximation-and-expected-fisher-information"><i class="fa fa-check"></i><b>2.3</b> Local quadratic approximation and expected Fisher information</a></li>
<li class="chapter" data-level="2.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy-learning-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4</b> Entropy learning and maximum likelihood</a><ul>
<li class="chapter" data-level="2.4.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#the-relative-entropy-between-true-model-and-approximating-model"><i class="fa fa-check"></i><b>2.4.1</b> The relative entropy between true model and approximating model</a></li>
<li class="chapter" data-level="2.4.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-likelihood-and-minimum-kl-divergence"><i class="fa fa-check"></i><b>2.4.2</b> Maximum likelihood and minimum KL divergence</a></li>
<li class="chapter" data-level="2.4.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#further-connections"><i class="fa fa-check"></i><b>2.4.3</b> Further connections</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="03-likelihood3.html"><a href="03-likelihood3.html"><i class="fa fa-check"></i><b>3</b> Maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#principle-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.1</b> Principle of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#outline"><i class="fa fa-check"></i><b>3.1.1</b> Outline</a></li>
<li class="chapter" data-level="3.1.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#recipe-for-obtaining-mles"><i class="fa fa-check"></i><b>3.1.2</b> Recipe for obtaining MLEs</a></li>
<li class="chapter" data-level="3.1.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#invariance-property-of-the-mle"><i class="fa fa-check"></i><b>3.1.3</b> Invariance property of the MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#examples-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.2</b> Examples of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.2.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-estimation-of-a-proportion"><i class="fa fa-check"></i><b>3.2.1</b> Example 1: Estimation of a proportion</a></li>
<li class="chapter" data-level="3.2.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-exponential-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Example 2: Exponential Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-3-normal-distribution-with-unknown-mean-and-known-variance"><i class="fa fa-check"></i><b>3.2.3</b> Example 3: Normal distribution with unknown mean and known variance</a></li>
<li class="chapter" data-level="3.2.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-4-normal-distribution-with-both-mean-and-variance-unknown"><i class="fa fa-check"></i><b>3.2.4</b> Example 4: Normal Distribution with both mean and variance unknown</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information"><i class="fa fa-check"></i><b>3.3</b> Observed Fisher information</a><ul>
<li class="chapter" data-level="3.3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#motivation"><i class="fa fa-check"></i><b>3.3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#curvature-of-log-likelihood-function"><i class="fa fa-check"></i><b>3.3.2</b> Curvature of log-likelihood function</a></li>
<li class="chapter" data-level="3.3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information-1"><i class="fa fa-check"></i><b>3.3.3</b> Observed Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information---examples"><i class="fa fa-check"></i><b>3.4</b> Observed Fisher information - Examples</a><ul>
<li class="chapter" data-level="3.4.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-1-bernoulli-binomial-model"><i class="fa fa-check"></i><b>3.4.1</b> Example 1: Bernoulli / Binomial model</a></li>
<li class="chapter" data-level="3.4.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#example-2-normal-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Example 2: Normal distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#from-observed-to-expected-fisher-information"><i class="fa fa-check"></i><b>3.4.3</b> From observed to expected Fisher information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="04-likelihood4.html"><a href="04-likelihood4.html"><i class="fa fa-check"></i><b>4</b> Quadratic approximation and normal asymptotics</a><ul>
<li class="chapter" data-level="4.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#covariance-correlation-and-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.1</b> Covariance, correlation and multivariate normal distribution</a></li>
<li class="chapter" data-level="4.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#maximum-likelihood-estimates-of-the-parameters-of-the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.2</b> Maximum likelihood estimates of the parameters of the multivariate normal distribution</a></li>
<li class="chapter" data-level="4.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-approximation-of-log-likelihood-function-around-mle"><i class="fa fa-check"></i><b>4.3</b> Quadratic approximation of log-likelihood function around MLE</a></li>
<li class="chapter" data-level="4.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-normality-of-mle"><i class="fa fa-check"></i><b>4.4</b> Asymptotic normality of MLE</a></li>
<li class="chapter" data-level="4.5" data-path="04-likelihood4.html"><a href="04-likelihood4.html#observed-or-expected-fisher-information-to-estimate-variance-of-the-mle"><i class="fa fa-check"></i><b>4.5</b> Observed or expected Fisher information to estimate variance of the MLE?</a></li>
<li class="chapter" data-level="4.6" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-confidence-intervals-for-mles"><i class="fa fa-check"></i><b>4.6</b> Normal confidence intervals for MLEs</a></li>
<li class="chapter" data-level="4.7" data-path="04-likelihood4.html"><a href="04-likelihood4.html#wald-statistic"><i class="fa fa-check"></i><b>4.7</b> Wald statistic</a></li>
<li class="chapter" data-level="4.8" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-ci-expressed-using-the-squared-wald-statistics"><i class="fa fa-check"></i><b>4.8</b> Normal CI expressed using the squared Wald statistics</a></li>
<li class="chapter" data-level="4.9" data-path="04-likelihood4.html"><a href="04-likelihood4.html#testing-and-confidence-intervals"><i class="fa fa-check"></i><b>4.9</b> Testing and confidence intervals</a></li>
<li class="chapter" data-level="4.10" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-normal-distribution"><i class="fa fa-check"></i><b>4.10</b> Example: normal distribution</a></li>
<li class="chapter" data-level="4.11" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-of-non-regular-model"><i class="fa fa-check"></i><b>4.11</b> Example of non-regular model</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="05-likelihood5.html"><a href="05-likelihood5.html"><i class="fa fa-check"></i><b>5</b> Likelihood-based confidence interval and likelihood ratio</a><ul>
<li class="chapter" data-level="5.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-based-confidence-intervals"><i class="fa fa-check"></i><b>5.1</b> Likelihood-based confidence intervals</a></li>
<li class="chapter" data-level="5.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#wilks-log-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.2</b> Wilks log likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.3" data-path="05-likelihood5.html"><a href="05-likelihood5.html#quadratic-approximation-of-wilks-statistic"><i class="fa fa-check"></i><b>5.3</b> Quadratic approximation of Wilks statistic</a></li>
<li class="chapter" data-level="5.4" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistics"><i class="fa fa-check"></i><b>5.4</b> Distribution of Wilks statistics</a><ul>
<li class="chapter" data-level="5.4.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#cutoff-values-delta"><i class="fa fa-check"></i><b>5.4.1</b> Cutoff values <span class="math inline">\(\Delta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="05-likelihood5.html"><a href="05-likelihood5.html#example-likelihood-ci-for-exponential-model"><i class="fa fa-check"></i><b>5.5</b> Example: likelihood CI for exponential model</a></li>
<li class="chapter" data-level="5.6" data-path="05-likelihood5.html"><a href="05-likelihood5.html#origin-of-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.6</b> Origin of likelihood ratio statistic</a></li>
<li class="chapter" data-level="5.7" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-wilks-statistic-and-likelihood-ci"><i class="fa fa-check"></i><b>5.7</b> Distribution of Wilks statistic and Likelihood CI</a></li>
<li class="chapter" data-level="5.8" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>5.8</b> Likelihood ratio test (LRT)</a></li>
<li class="chapter" data-level="5.9" data-path="05-likelihood5.html"><a href="05-likelihood5.html#optimality-of-lrts"><i class="fa fa-check"></i><b>5.9</b> Optimality of LRTs</a></li>
<li class="chapter" data-level="5.10" data-path="05-likelihood5.html"><a href="05-likelihood5.html#generalised-likelihood-ratio-test-glrt"><i class="fa fa-check"></i><b>5.10</b> Generalised likelihood ratio test (GLRT)</a></li>
<li class="chapter" data-level="5.11" data-path="05-likelihood5.html"><a href="05-likelihood5.html#glrt-example"><i class="fa fa-check"></i><b>5.11</b> GLRT example</a></li>
<li class="chapter" data-level="5.12" data-path="05-likelihood5.html"><a href="05-likelihood5.html#thoughts-on-model-selection"><i class="fa fa-check"></i><b>5.12</b> Thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="06-likelihood6.html"><a href="06-likelihood6.html"><i class="fa fa-check"></i><b>6</b> Optimality properties, minimal sufficiency and summary</a><ul>
<li class="chapter" data-level="6.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#properties-of-mles-encountered-so-far"><i class="fa fa-check"></i><b>6.1</b> Properties of MLEs encountered so far</a></li>
<li class="chapter" data-level="6.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#further-optimality-properties-of-mles"><i class="fa fa-check"></i><b>6.2</b> Further optimality properties of MLEs</a></li>
<li class="chapter" data-level="6.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency"><i class="fa fa-check"></i><b>6.3</b> Summarising data and the concept of minimal sufficiency</a></li>
<li class="chapter" data-level="6.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summary-and-concluding-remarks-on-maximum-likelihood"><i class="fa fa-check"></i><b>6.4</b> Summary and concluding remarks on maximum likelihood</a><ul>
<li class="chapter" data-level="6.4.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#starting-point-kl-divergence"><i class="fa fa-check"></i><b>6.4.1</b> Starting point: KL divergence</a></li>
<li class="chapter" data-level="6.4.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#connections-between-kl-divergence-likelihood-and-expected-and-observed-fisher-information"><i class="fa fa-check"></i><b>6.4.2</b> Connections between KL divergence, likelihood and expected and observed Fisher information</a></li>
<li class="chapter" data-level="6.4.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#likelihood-estimation"><i class="fa fa-check"></i><b>6.4.3</b> Likelihood estimation</a></li>
<li class="chapter" data-level="6.4.4" data-path="06-likelihood6.html"><a href="06-likelihood6.html#what-happens-if-n-is-small"><i class="fa fa-check"></i><b>6.4.4</b> What happens if <span class="math inline">\(n\)</span> is small?</a></li>
<li class="chapter" data-level="6.4.5" data-path="06-likelihood6.html"><a href="06-likelihood6.html#inference-with-likelihood"><i class="fa fa-check"></i><b>6.4.5</b> Inference with likelihood:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Bayesian Statistics</b></span></li>
<li class="chapter" data-level="7" data-path="07-bayes1.html"><a href="07-bayes1.html"><i class="fa fa-check"></i><b>7</b> Essentials of Bayesian statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#bayes-theorem"><i class="fa fa-check"></i><b>7.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#principle-of-bayesian-learning"><i class="fa fa-check"></i><b>7.2</b> Principle of Bayesian learning</a></li>
<li class="chapter" data-level="7.3" data-path="07-bayes1.html"><a href="07-bayes1.html#what-is-exactly-is-the-bayesian-estimate"><i class="fa fa-check"></i><b>7.3</b> What is exactly is the “Bayesian estimate”?</a></li>
<li class="chapter" data-level="7.4" data-path="07-bayes1.html"><a href="07-bayes1.html#computer-implementation-of-bayesian-learning"><i class="fa fa-check"></i><b>7.4</b> Computer implementation of Bayesian learning</a></li>
<li class="chapter" data-level="7.5" data-path="07-bayes1.html"><a href="07-bayes1.html#bayesian-interpretation-of-probability"><i class="fa fa-check"></i><b>7.5</b> Bayesian interpretation of probability</a><ul>
<li class="chapter" data-level="7.5.1" data-path="07-bayes1.html"><a href="07-bayes1.html#what-makes-you-bayesian"><i class="fa fa-check"></i><b>7.5.1</b> What makes you “Bayesian”?</a></li>
<li class="chapter" data-level="7.5.2" data-path="07-bayes1.html"><a href="07-bayes1.html#mathematics-of-probability"><i class="fa fa-check"></i><b>7.5.2</b> Mathematics of probability</a></li>
<li class="chapter" data-level="7.5.3" data-path="07-bayes1.html"><a href="07-bayes1.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.5.3</b> Interpretations of probability</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="07-bayes1.html"><a href="07-bayes1.html#historical-developments"><i class="fa fa-check"></i><b>7.6</b> Historical developments</a></li>
<li class="chapter" data-level="7.7" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning"><i class="fa fa-check"></i><b>7.7</b> Connection with entropy learning</a><ul>
<li class="chapter" data-level="7.7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#zero-forcing-property"><i class="fa fa-check"></i><b>7.7.1</b> Zero forcing property</a></li>
<li class="chapter" data-level="7.7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning-1"><i class="fa fa-check"></i><b>7.7.2</b> Connection with entropy learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="08-bayes2.html"><a href="08-bayes2.html"><i class="fa fa-check"></i><b>8</b> Beta-Binomial model for estimating a proportion</a><ul>
<li class="chapter" data-level="8.1" data-path="08-bayes2.html"><a href="08-bayes2.html#binomial-likelihood"><i class="fa fa-check"></i><b>8.1</b> Binomial likelihood</a></li>
<li class="chapter" data-level="8.2" data-path="08-bayes2.html"><a href="08-bayes2.html#excursion-properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>8.2</b> Excursion: Properties of the Beta distribution</a></li>
<li class="chapter" data-level="8.3" data-path="08-bayes2.html"><a href="08-bayes2.html#beta-prior-distribution"><i class="fa fa-check"></i><b>8.3</b> Beta prior distribution</a></li>
<li class="chapter" data-level="8.4" data-path="08-bayes2.html"><a href="08-bayes2.html#computing-the-posterior-distribution"><i class="fa fa-check"></i><b>8.4</b> Computing the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="09-bayes3.html"><a href="09-bayes3.html"><i class="fa fa-check"></i><b>9</b> Properties of Bayesian learning</a><ul>
<li class="chapter" data-level="9.1" data-path="09-bayes3.html"><a href="09-bayes3.html#prior-acting-as-pseudo-data"><i class="fa fa-check"></i><b>9.1</b> Prior acting as pseudo-data</a></li>
<li class="chapter" data-level="9.2" data-path="09-bayes3.html"><a href="09-bayes3.html#linear-shrinkage-of-mean"><i class="fa fa-check"></i><b>9.2</b> Linear shrinkage of mean</a></li>
<li class="chapter" data-level="9.3" data-path="09-bayes3.html"><a href="09-bayes3.html#conjugacy-of-prior-and-posterior-distribution"><i class="fa fa-check"></i><b>9.3</b> Conjugacy of prior and posterior distribution</a></li>
<li class="chapter" data-level="9.4" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-asymptotics"><i class="fa fa-check"></i><b>9.4</b> Large sample asymptotics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-limits-of-mean-and-variance"><i class="fa fa-check"></i><b>9.4.1</b> Large sample limits of mean and variance</a></li>
<li class="chapter" data-level="9.4.2" data-path="09-bayes3.html"><a href="09-bayes3.html#asymptotic-normality-of-the-posterior-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Asymptotic Normality of the Posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="09-bayes3.html"><a href="09-bayes3.html#posterior-variance-for-finite-n"><i class="fa fa-check"></i><b>9.5</b> Posterior variance for finite <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-bayes4.html"><a href="10-bayes4.html"><i class="fa fa-check"></i><b>10</b> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a><ul>
<li class="chapter" data-level="10.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-normal-model-to-estimate-mean"><i class="fa fa-check"></i><b>10.1</b> Normal-Normal model to estimate mean</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihood"><i class="fa fa-check"></i><b>10.1.1</b> Normal likelihood</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-prior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Normal prior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-posterior-distribution"><i class="fa fa-check"></i><b>10.1.3</b> Normal posterior distribution</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-and-stein-paradox"><i class="fa fa-check"></i><b>10.1.4</b> Large sample asymptotics and Stein paradox</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-normal-model-to-estimate-variance"><i class="fa fa-check"></i><b>10.2</b> Inverse-Gamma-Normal model to estimate variance</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Inverse Gamma distribution</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihoood"><i class="fa fa-check"></i><b>10.2.2</b> Normal likelihoood</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-prior-distribution"><i class="fa fa-check"></i><b>10.2.3</b> Inverse Gamma prior distribution</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-posterior-distribution"><i class="fa fa-check"></i><b>10.2.4</b> Inverse Gamma posterior distribution</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-1"><i class="fa fa-check"></i><b>10.2.5</b> Large sample asymptotics</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-bayes4.html"><a href="10-bayes4.html#estimating-precision"><i class="fa fa-check"></i><b>10.2.6</b> Estimating precision</a></li>
<li class="chapter" data-level="10.2.7" data-path="10-bayes4.html"><a href="10-bayes4.html#joint-estimation-of-mean-and-variance"><i class="fa fa-check"></i><b>10.2.7</b> Joint estimation of mean and variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-bayes5.html"><a href="11-bayes5.html"><i class="fa fa-check"></i><b>11</b> Shrinkage estimation using empirical risk minimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="11-bayes5.html"><a href="11-bayes5.html#linear-shrinkage"><i class="fa fa-check"></i><b>11.1</b> Linear shrinkage</a></li>
<li class="chapter" data-level="11.2" data-path="11-bayes5.html"><a href="11-bayes5.html#james-stein-estimator"><i class="fa fa-check"></i><b>11.2</b> James-Stein estimator</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-bayes6.html"><a href="12-bayes6.html"><i class="fa fa-check"></i><b>12</b> Bayesian model comparison using Bayes factors and the BIC</a><ul>
<li class="chapter" data-level="12.1" data-path="12-bayes6.html"><a href="12-bayes6.html#the-bayes-factor"><i class="fa fa-check"></i><b>12.1</b> The Bayes factor</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-bayes6.html"><a href="12-bayes6.html#connection-with-relative-entropy"><i class="fa fa-check"></i><b>12.1.1</b> Connection with relative entropy</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-bayes6.html"><a href="12-bayes6.html#interpretation-of-and-scale-for-bayes-factor"><i class="fa fa-check"></i><b>12.1.2</b> Interpretation of and scale for Bayes factor</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-bayes6.html"><a href="12-bayes6.html#computing-textprd-m-for-simple-and-composite-models"><i class="fa fa-check"></i><b>12.1.3</b> Computing <span class="math inline">\(\text{Pr}(D | M)\)</span> for simple and composite models</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-bayes6.html"><a href="12-bayes6.html#bayes-factor-versus-likelihood-ratio"><i class="fa fa-check"></i><b>12.1.4</b> Bayes factor versus likelihood ratio</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-bayes6.html"><a href="12-bayes6.html#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><i class="fa fa-check"></i><b>12.2</b> Approximate computation of the marginal likelihood and of the log-Bayes factor</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-bayes6.html"><a href="12-bayes6.html#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fa fa-check"></i><b>12.2.1</b> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-bayes6.html"><a href="12-bayes6.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>12.2.2</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-bayes6.html"><a href="12-bayes6.html#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fa fa-check"></i><b>12.2.3</b> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-bayes6.html"><a href="12-bayes6.html#model-complexity-and-occams-razor"><i class="fa fa-check"></i><b>12.2.4</b> Model complexity and Occams razor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-bayes7.html"><a href="13-bayes7.html"><i class="fa fa-check"></i><b>13</b> False discovery rates</a><ul>
<li class="chapter" data-level="13.1" data-path="13-bayes7.html"><a href="13-bayes7.html#general-setup"><i class="fa fa-check"></i><b>13.1</b> General setup</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-bayes7.html"><a href="13-bayes7.html#overview-1"><i class="fa fa-check"></i><b>13.1.1</b> Overview</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-bayes7.html"><a href="13-bayes7.html#choosing-between-h_0-and-h_a"><i class="fa fa-check"></i><b>13.1.2</b> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span></a></li>
<li class="chapter" data-level="13.1.3" data-path="13-bayes7.html"><a href="13-bayes7.html#true-and-false-positives-and-negatives"><i class="fa fa-check"></i><b>13.1.3</b> True and false positives and negatives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-bayes7.html"><a href="13-bayes7.html#specificity-and-sensitivity"><i class="fa fa-check"></i><b>13.2</b> Specificity and Sensitivity</a></li>
<li class="chapter" data-level="13.3" data-path="13-bayes7.html"><a href="13-bayes7.html#fdr-and-fndr"><i class="fa fa-check"></i><b>13.3</b> FDR and FNDR</a></li>
<li class="chapter" data-level="13.4" data-path="13-bayes7.html"><a href="13-bayes7.html#bayesian-perspective"><i class="fa fa-check"></i><b>13.4</b> Bayesian perspective</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-bayes7.html"><a href="13-bayes7.html#two-component-mixture-model"><i class="fa fa-check"></i><b>13.4.1</b> Two component mixture model</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-bayes7.html"><a href="13-bayes7.html#local-fdr"><i class="fa fa-check"></i><b>13.4.2</b> Local FDR</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-bayes7.html"><a href="13-bayes7.html#q-values"><i class="fa fa-check"></i><b>13.4.3</b> q-values</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="13-bayes7.html"><a href="13-bayes7.html#software"><i class="fa fa-check"></i><b>13.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-bayes8.html"><a href="14-bayes8.html"><i class="fa fa-check"></i><b>14</b> Optimality properties and summary</a><ul>
<li class="chapter" data-level="14.1" data-path="14-bayes8.html"><a href="14-bayes8.html#bayesian-statistics-in-a-nutshell"><i class="fa fa-check"></i><b>14.1</b> Bayesian statistics in a nutshell</a><ul>
<li class="chapter" data-level="14.1.1" data-path="14-bayes8.html"><a href="14-bayes8.html#remarks"><i class="fa fa-check"></i><b>14.1.1</b> Remarks</a></li>
<li class="chapter" data-level="14.1.2" data-path="14-bayes8.html"><a href="14-bayes8.html#advantages"><i class="fa fa-check"></i><b>14.1.2</b> Advantages</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="14-bayes8.html"><a href="14-bayes8.html#frequentist-properties-of-bayesian-estimators"><i class="fa fa-check"></i><b>14.2</b> Frequentist properties of Bayesian estimators</a></li>
<li class="chapter" data-level="14.3" data-path="14-bayes8.html"><a href="14-bayes8.html#specifying-the-prior-problem-or-advantage"><i class="fa fa-check"></i><b>14.3</b> Specifying the prior — problem or advantage?</a></li>
<li class="chapter" data-level="14.4" data-path="14-bayes8.html"><a href="14-bayes8.html#choosing-a-prior"><i class="fa fa-check"></i><b>14.4</b> Choosing a prior</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-bayes8.html"><a href="14-bayes8.html#some-guidelines"><i class="fa fa-check"></i><b>14.4.1</b> Some guidelines</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-bayes8.html"><a href="14-bayes8.html#jeffreys-prior"><i class="fa fa-check"></i><b>14.4.2</b> Jeffreys prior</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-bayes8.html"><a href="14-bayes8.html#optimality-of-bayes-inference"><i class="fa fa-check"></i><b>14.5</b> Optimality of Bayes inference</a></li>
<li class="chapter" data-level="14.6" data-path="14-bayes8.html"><a href="14-bayes8.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a><ul>
<li class="chapter" data-level="14.6.1" data-path="14-bayes8.html"><a href="14-bayes8.html#current-research"><i class="fa fa-check"></i><b>14.6.1</b> Current research</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Regression</b></span></li>
<li class="chapter" data-level="15" data-path="15-regression1.html"><a href="15-regression1.html"><i class="fa fa-check"></i><b>15</b> Overview over regression modelling</a><ul>
<li class="chapter" data-level="15.1" data-path="15-regression1.html"><a href="15-regression1.html#general-setup-1"><i class="fa fa-check"></i><b>15.1</b> General setup</a></li>
<li class="chapter" data-level="15.2" data-path="15-regression1.html"><a href="15-regression1.html#objectives"><i class="fa fa-check"></i><b>15.2</b> Objectives</a></li>
<li class="chapter" data-level="15.3" data-path="15-regression1.html"><a href="15-regression1.html#regression-as-a-form-of-supervised-learning"><i class="fa fa-check"></i><b>15.3</b> Regression as a form of supervised learning</a></li>
<li class="chapter" data-level="15.4" data-path="15-regression1.html"><a href="15-regression1.html#various-regression-models-used-in-statistics"><i class="fa fa-check"></i><b>15.4</b> Various regression models used in statistics</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="16-regression2.html"><a href="16-regression2.html"><i class="fa fa-check"></i><b>16</b> Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="16-regression2.html"><a href="16-regression2.html#the-linear-regression-model"><i class="fa fa-check"></i><b>16.1</b> The linear regression model</a></li>
<li class="chapter" data-level="16.2" data-path="16-regression2.html"><a href="16-regression2.html#interpretation-of-regression-coefficients-and-intercept"><i class="fa fa-check"></i><b>16.2</b> Interpretation of regression coefficients and intercept</a></li>
<li class="chapter" data-level="16.3" data-path="16-regression2.html"><a href="16-regression2.html#different-types-of-linear-regression"><i class="fa fa-check"></i><b>16.3</b> Different types of linear regression:</a></li>
<li class="chapter" data-level="16.4" data-path="16-regression2.html"><a href="16-regression2.html#distributional-assumptions-and-properties"><i class="fa fa-check"></i><b>16.4</b> Distributional assumptions and properties</a></li>
<li class="chapter" data-level="16.5" data-path="16-regression2.html"><a href="16-regression2.html#regression-in-data-matrix-notation"><i class="fa fa-check"></i><b>16.5</b> Regression in data matrix notation</a></li>
<li class="chapter" data-level="16.6" data-path="16-regression2.html"><a href="16-regression2.html#centering-and-vanishing-of-the-intercept-beta_0"><i class="fa fa-check"></i><b>16.6</b> Centering and vanishing of the intercept <span class="math inline">\(\beta_0\)</span></a></li>
<li class="chapter" data-level="16.7" data-path="16-regression2.html"><a href="16-regression2.html#regression-objectives-for-linear-model"><i class="fa fa-check"></i><b>16.7</b> Regression objectives for linear model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="17-regression3.html"><a href="17-regression3.html"><i class="fa fa-check"></i><b>17</b> Estimating regression coefficients</a><ul>
<li class="chapter" data-level="17.1" data-path="17-regression3.html"><a href="17-regression3.html#ordinary-least-squares-ols-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.1</b> Ordinary Least Squares (OLS) estimator of regression coefficients</a></li>
<li class="chapter" data-level="17.2" data-path="17-regression3.html"><a href="17-regression3.html#maximum-likelihood-estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>17.2</b> Maximum likelihood estimation of regression coefficients</a><ul>
<li class="chapter" data-level="17.2.1" data-path="17-regression3.html"><a href="17-regression3.html#detailed-derivation-of-the-mles"><i class="fa fa-check"></i><b>17.2.1</b> Detailed derivation of the MLEs</a></li>
<li class="chapter" data-level="17.2.2" data-path="17-regression3.html"><a href="17-regression3.html#asymptotics"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotics</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="17-regression3.html"><a href="17-regression3.html#covariance-plug-in-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.3</b> Covariance plug-in estimator of regression coefficients</a><ul>
<li class="chapter" data-level="17.3.1" data-path="17-regression3.html"><a href="17-regression3.html#importance-of-positive-definiteness-of-estimated-covariance-matrix"><i class="fa fa-check"></i><b>17.3.1</b> Importance of positive definiteness of estimated covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="17-regression3.html"><a href="17-regression3.html#best-linear-predictor"><i class="fa fa-check"></i><b>17.4</b> Best linear predictor</a><ul>
<li class="chapter" data-level="17.4.1" data-path="17-regression3.html"><a href="17-regression3.html#result"><i class="fa fa-check"></i><b>17.4.1</b> Result:</a></li>
<li class="chapter" data-level="17.4.2" data-path="17-regression3.html"><a href="17-regression3.html#irreducible-error"><i class="fa fa-check"></i><b>17.4.2</b> Irreducible Error</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="17-regression3.html"><a href="17-regression3.html#regression-by-conditioning"><i class="fa fa-check"></i><b>17.5</b> Regression by conditioning</a><ul>
<li class="chapter" data-level="17.5.1" data-path="17-regression3.html"><a href="17-regression3.html#general-idea"><i class="fa fa-check"></i><b>17.5.1</b> General idea:</a></li>
<li class="chapter" data-level="17.5.2" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-normal-assumption"><i class="fa fa-check"></i><b>17.5.2</b> Multivariate normal assumption</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="17-regression3.html"><a href="17-regression3.html#standardised-regression-coefficients-and-relationship-to-correlation"><i class="fa fa-check"></i><b>17.6</b> Standardised regression coefficients and relationship to correlation</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="18-regression4.html"><a href="18-regression4.html"><i class="fa fa-check"></i><b>18</b> Squared multiple correlation and variance decomposition in linear regression</a><ul>
<li class="chapter" data-level="18.1" data-path="18-regression4.html"><a href="18-regression4.html#squared-multiple-correlation-omega2-and-the-r2-coefficient"><i class="fa fa-check"></i><b>18.1</b> Squared multiple correlation <span class="math inline">\(\Omega^2\)</span> and the <span class="math inline">\(R^2\)</span> coefficient</a><ul>
<li class="chapter" data-level="18.1.1" data-path="18-regression4.html"><a href="18-regression4.html#estimation-of-omega2-and-the-multiple-r2-coefficient"><i class="fa fa-check"></i><b>18.1.1</b> Estimation of <span class="math inline">\(\Omega^2\)</span> and the multiple <span class="math inline">\(R^2\)</span> coefficient</a></li>
<li class="chapter" data-level="18.1.2" data-path="18-regression4.html"><a href="18-regression4.html#r-output"><i class="fa fa-check"></i><b>18.1.2</b> R output</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="18-regression4.html"><a href="18-regression4.html#variance-decomposition-in-regression"><i class="fa fa-check"></i><b>18.2</b> Variance decomposition in regression</a><ul>
<li class="chapter" data-level="18.2.1" data-path="18-regression4.html"><a href="18-regression4.html#law-of-total-variance-and-variance-decomposition"><i class="fa fa-check"></i><b>18.2.1</b> Law of total variance and variance decomposition</a></li>
<li class="chapter" data-level="18.2.2" data-path="18-regression4.html"><a href="18-regression4.html#related-quantities"><i class="fa fa-check"></i><b>18.2.2</b> Related quantities</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="18-regression4.html"><a href="18-regression4.html#sample-version-of-variance-decomposition"><i class="fa fa-check"></i><b>18.3</b> Sample version of variance decomposition</a><ul>
<li class="chapter" data-level="18.3.1" data-path="18-regression4.html"><a href="18-regression4.html#geometric-interpretation-of-regression-as-orthogonal-projection"><i class="fa fa-check"></i><b>18.3.1</b> Geometric interpretation of regression as orthogonal projection:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="19-regression5.html"><a href="19-regression5.html"><i class="fa fa-check"></i><b>19</b> Prediction and variable selection</a><ul>
<li class="chapter" data-level="19.1" data-path="19-regression5.html"><a href="19-regression5.html#prediction-and-prediction-intervals"><i class="fa fa-check"></i><b>19.1</b> Prediction and prediction intervals</a></li>
<li class="chapter" data-level="19.2" data-path="19-regression5.html"><a href="19-regression5.html#variable-importance-and-prediction"><i class="fa fa-check"></i><b>19.2</b> Variable importance and prediction</a><ul>
<li class="chapter" data-level="19.2.1" data-path="19-regression5.html"><a href="19-regression5.html#how-to-quantify-variable-importance"><i class="fa fa-check"></i><b>19.2.1</b> How to quantify variable importance?</a></li>
<li class="chapter" data-level="19.2.2" data-path="19-regression5.html"><a href="19-regression5.html#some-candidates-for-vims"><i class="fa fa-check"></i><b>19.2.2</b> Some candidates for VIMs</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="19-regression5.html"><a href="19-regression5.html#regression-t-scores."><i class="fa fa-check"></i><b>19.3</b> Regression <span class="math inline">\(t\)</span>-scores.</a><ul>
<li class="chapter" data-level="19.3.1" data-path="19-regression5.html"><a href="19-regression5.html#computation"><i class="fa fa-check"></i><b>19.3.1</b> Computation</a></li>
<li class="chapter" data-level="19.3.2" data-path="19-regression5.html"><a href="19-regression5.html#connection-with-partial-correlation"><i class="fa fa-check"></i><b>19.3.2</b> Connection with partial correlation</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="19-regression5.html"><a href="19-regression5.html#further-approaches-for-variable-selection"><i class="fa fa-check"></i><b>19.4</b> Further approaches for variable selection</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="20-refresher.html"><a href="20-refresher.html"><i class="fa fa-check"></i><b>A</b> Refresher</a><ul>
<li class="chapter" data-level="A.1" data-path="20-refresher.html"><a href="20-refresher.html#vectors-and-matrices"><i class="fa fa-check"></i><b>A.1</b> Vectors and matrices</a></li>
<li class="chapter" data-level="A.2" data-path="20-refresher.html"><a href="20-refresher.html#functions"><i class="fa fa-check"></i><b>A.2</b> Functions</a><ul>
<li class="chapter" data-level="A.2.1" data-path="20-refresher.html"><a href="20-refresher.html#gradient"><i class="fa fa-check"></i><b>A.2.1</b> Gradient</a></li>
<li class="chapter" data-level="A.2.2" data-path="20-refresher.html"><a href="20-refresher.html#hessian-matrix"><i class="fa fa-check"></i><b>A.2.2</b> Hessian matrix</a></li>
<li class="chapter" data-level="A.2.3" data-path="20-refresher.html"><a href="20-refresher.html#conditions-for-local-maximum-of-a-function"><i class="fa fa-check"></i><b>A.2.3</b> Conditions for local maximum of a function</a></li>
<li class="chapter" data-level="A.2.4" data-path="20-refresher.html"><a href="20-refresher.html#linear-and-quadratic-approximation-taylor-series"><i class="fa fa-check"></i><b>A.2.4</b> Linear and quadratic approximation (Taylor series)</a></li>
<li class="chapter" data-level="A.2.5" data-path="20-refresher.html"><a href="20-refresher.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.2.5</b> Functions of matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="20-refresher.html"><a href="20-refresher.html#probability"><i class="fa fa-check"></i><b>A.3</b> Probability</a><ul>
<li class="chapter" data-level="A.3.1" data-path="20-refresher.html"><a href="20-refresher.html#law-of-large-numbers"><i class="fa fa-check"></i><b>A.3.1</b> Law of large numbers:</a></li>
<li class="chapter" data-level="A.3.2" data-path="20-refresher.html"><a href="20-refresher.html#jensens-inequality"><i class="fa fa-check"></i><b>A.3.2</b> Jensen’s inequality</a></li>
<li class="chapter" data-level="A.3.3" data-path="20-refresher.html"><a href="20-refresher.html#transformation-of-univariate-densities"><i class="fa fa-check"></i><b>A.3.3</b> Transformation of univariate densities</a></li>
<li class="chapter" data-level="A.3.4" data-path="20-refresher.html"><a href="20-refresher.html#normal-distribution"><i class="fa fa-check"></i><b>A.3.4</b> Normal distribution</a></li>
<li class="chapter" data-level="A.3.5" data-path="20-refresher.html"><a href="20-refresher.html#chi-square-distribution"><i class="fa fa-check"></i><b>A.3.5</b> Chi-square distribution</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="20-refresher.html"><a href="20-refresher.html#statistics"><i class="fa fa-check"></i><b>A.4</b> Statistics</a><ul>
<li class="chapter" data-level="A.4.1" data-path="20-refresher.html"><a href="20-refresher.html#statistical-learning"><i class="fa fa-check"></i><b>A.4.1</b> Statistical learning</a></li>
<li class="chapter" data-level="A.4.2" data-path="20-refresher.html"><a href="20-refresher.html#point-and-interval-estimation"><i class="fa fa-check"></i><b>A.4.2</b> Point and interval estimation</a></li>
<li class="chapter" data-level="A.4.3" data-path="20-refresher.html"><a href="20-refresher.html#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fa fa-check"></i><b>A.4.3</b> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span></a></li>
<li class="chapter" data-level="A.4.4" data-path="20-refresher.html"><a href="20-refresher.html#asymptotics-1"><i class="fa fa-check"></i><b>A.4.4</b> Asymptotics</a></li>
<li class="chapter" data-level="A.4.5" data-path="20-refresher.html"><a href="20-refresher.html#confidence-intervals"><i class="fa fa-check"></i><b>A.4.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="A.4.6" data-path="20-refresher.html"><a href="20-refresher.html#symmetric-normal-confidence-interval"><i class="fa fa-check"></i><b>A.4.6</b> Symmetric normal confidence interval</a></li>
<li class="chapter" data-level="A.4.7" data-path="20-refresher.html"><a href="20-refresher.html#chi-square-confidence-interval"><i class="fa fa-check"></i><b>A.4.7</b> Chi-square confidence interval</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="21-further-study.html"><a href="21-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="21-further-study.html"><a href="21-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="21-further-study.html"><a href="21-further-study.html#additional-references"><i class="fa fa-check"></i><b>B.2</b> Additional references</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="22-references.html"><a href="22-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Statistical Methods:<br />
Likelihood, Bayes and Regression</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="quadratic-approximation-and-normal-asymptotics" class="section level1">
<h1><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</h1>
<p>In this chapter we first introduce the multivariate normal distribution and then
study second order approximation of the likelihood function.</p>
<div id="covariance-correlation-and-multivariate-normal-distribution" class="section level2">
<h2><span class="header-section-number">4.1</span> Covariance, correlation and multivariate normal distribution</h2>
<p>The density of a normally distributed scalar variable <span class="math inline">\(x \sim N(\mu, \sigma^2)\)</span>
with mean <span class="math inline">\(\text{E}(x) = \mu\)</span> and variance <span class="math inline">\(\text{Var}(x) = \sigma^2\)</span> is
<span class="math display">\[
f(x |\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right)
\]</span>
This is known as the univariate normal density.
Note that the variance is given by
<span class="math display">\[
\text{Var}(x) = \text{E}\left( (x-\text{E}(x))^2 \right) = \text{E}\left((x-\mu)^2 \right)=\text{E}\left( (x-\mu)(x-\mu) \right) = \text{E}(x^2)-\mu^2
\]</span></p>
<p>For a random vector <span class="math inline">\(\boldsymbol x= (x_1, x_2,...,x_d)^T\)</span> with mean <span class="math inline">\(\text{E}(\boldsymbol x) = \boldsymbol \mu\)</span>
the variance is generalised to the <strong>covariance matrix</strong> (of size <span class="math inline">\(d\times d\)</span>).
<span class="math display">\[
\text{Var}(\boldsymbol x) = \text{E}\left(\underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d\times 1} \underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1\times d}\right) = \underbrace{\boldsymbol \Sigma}_{d\times d} = \text{E}(\boldsymbol x\boldsymbol x^T)-\boldsymbol \mu\boldsymbol \mu^T
\]</span>
The covariance matrix is symmetric by construction and <strong>positive semi-definite</strong>, i.e. the eigenvalues of <span class="math inline">\(\boldsymbol \Sigma\)</span> are all positive or equal to zero. However, we will aim to use non-singular covariance matrices, with all eigenvalues positive, so that it can be inverted.</p>
<p>A covariance matrix can factorised into the product
<span class="math display">\[\boldsymbol \Sigma= \boldsymbol V^{\frac{1}{2}} \boldsymbol P\boldsymbol V^{\frac{1}{2}}\]</span>
where <span class="math inline">\(\boldsymbol V\)</span> is a diagonal matrix containing the variances
<span class="math display">\[ \boldsymbol V= \begin{pmatrix}
    \sigma_{11} &amp; \dots &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \sigma_{dd}
\end{pmatrix}\]</span>
and the matrix <span class="math inline">\(\boldsymbol P\)</span> (“capital rho”) is the symmetric <strong>correlation matrix</strong>
<span class="math display">\[ \boldsymbol P= (\rho_{ij}) = \begin{pmatrix}
    1 &amp; \dots &amp; \rho_{1d}\\
     \vdots &amp; \ddots &amp; \vdots \\
    \rho_{d1} &amp; \dots &amp; 1
\end{pmatrix}\]</span>
Thus, the correlation between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> is defined as
<span class="math display">\[\rho_{ij} = \text{Cor}(x_i,x_j) = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}\]</span></p>
<p>The generalisation of the normal distribution to random vectors of size <span class="math inline">\(d\)</span> is the <strong>multivariate normal distribution</strong> <span class="math inline">\(N_d(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> with mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span>. The corresponding
density is
<span class="math display">\[f(\boldsymbol x| \boldsymbol \mu, \boldsymbol \Sigma) = (2\pi)^{-\frac{d}{2}} \det(\boldsymbol \Sigma)^{-\frac{1}{2}} \exp\left({{-\frac{1}{2}} \underbrace{\underbrace{(\boldsymbol x-\boldsymbol \mu)^T}_{1 \times d} \underbrace{\boldsymbol \Sigma^{-1}}_{d \times d} \underbrace{(\boldsymbol x-\boldsymbol \mu)}_{d \times 1} }_{1 \times 1 = \text{scalar!}}}\right)\]</span></p>
<p>For <span class="math inline">\(d=1\)</span> we get <span class="math inline">\(\boldsymbol \mu= \mu\)</span> and <span class="math inline">\(\boldsymbol \Sigma= \sigma^2\)</span> and the multivariate normal density reduces
to the univariate normal density.</p>
</div>
<div id="maximum-likelihood-estimates-of-the-parameters-of-the-multivariate-normal-distribution" class="section level2">
<h2><span class="header-section-number">4.2</span> Maximum likelihood estimates of the parameters of the multivariate normal distribution</h2>
<p>Maximising the log-likelihood based on the multivariate normal density yields the
MLEs for <span class="math inline">\(\boldsymbol \mu\)</span> and <span class="math inline">\(\boldsymbol \Sigma\)</span>. These are generalisations of the MLEs for the mean <span class="math inline">\(\mu\)</span>
and variance <span class="math inline">\(\sigma^2\)</span> of the univariate normal as encounteed the previous chapter.</p>
<p>The estimates can be written in three different ways:</p>
<p><strong>Data vector notation</strong></p>
<p>with <span class="math inline">\(\boldsymbol x_1,\ldots, \boldsymbol x_n\)</span> the <span class="math inline">\(n\)</span> vector-valued observations from the multivariate normal:</p>
<p>MLE for the mean:
<span class="math display">\[\hat{\boldsymbol \mu} = \frac{1}{n}\sum^{n}_{k=1} \boldsymbol x_k\]</span></p>
<p>MLE for the covariance:
<span class="math display">\[\underbrace{\widehat{\boldsymbol \Sigma}}_{d \times d} = \frac{1}{n}\sum^{n}_{k=1} \underbrace{\left(\boldsymbol x_k-\hat{\boldsymbol \mu}\right)}_{d \times 1} \; \underbrace{\left(\boldsymbol x_k-\hat{\boldsymbol \mu}\right)^T}_{1 \times d}\]</span>
Note the factor <span class="math inline">\(\frac{1}{n}\)</span> in the estimator of the covariance matrix.</p>
<p><strong>Data component notation</strong></p>
<p>with <span class="math inline">\(x_{ki}\)</span> the <span class="math inline">\(i\)</span>-th component of the <span class="math inline">\(k\)</span>-th sample:</p>
<p><span class="math display">\[\hat{\mu}_i = \frac{1}{n}\sum^{n}_{k=1} x_{ki}\]</span></p>
<p><span class="math display">\[\hat{\sigma}_{ij} = \frac{1}{n}\sum^{n}_{k=1} \left(x_{ki}-\hat{\mu}_i\right)\left(\
x_{kj}-\hat{\mu}_j\right)\]</span></p>
<p><span class="math display">\[\hat{\boldsymbol \mu}=\begin{pmatrix}
    \hat{\mu}_{1}       \\
    \vdots \\
    \hat{\mu}_{d}
\end{pmatrix}, \widehat{\boldsymbol \Sigma} = (\hat{\sigma}_{ij})\]</span></p>
<p>Variance estimate:<br />
<span class="math display">\[\hat{\sigma}_{ii} = \frac{1}{n}\sum^{n}_{k=1} \left(x_{ki}-\hat{\mu}_i\right)^2\]</span></p>
<p><strong>Data matrix notation</strong></p>
<p>with <span class="math inline">\(\boldsymbol X= (x_{ij})\)</span> being the data matrix, with samples in rows and variables in columns.</p>
<p>Note that this is the <em>statistics convention</em> for the data matrix <span class="math inline">\(\boldsymbol X\)</span>.
However, <em>in the machine learning literature the convention is often reversed</em> and variables
are assumed to be in the rows and samples in columns!!</p>
<p><span class="math display">\[
\hat{\boldsymbol \mu} = \frac{1}{n} \boldsymbol X^T \boldsymbol 1_n
\]</span>
Here <span class="math inline">\(\boldsymbol 1_n\)</span> is a vector of length <span class="math inline">\(n\)</span> containing 1 at each component.</p>
<p><span class="math display">\[
\hat{\boldsymbol \Sigma} = \frac{1}{n} \boldsymbol X^T \boldsymbol X- \hat{\boldsymbol \mu} \hat{\boldsymbol \mu}^T
\]</span>
To simplify the expression for the estimate of the covariance matrix
one often assumes that the data matrix is centered, i.e. that <span class="math inline">\(\hat{\boldsymbol \mu} = 0\)</span>.</p>
<p>In <em>machine learning notation</em>, the data matrix has to be transposed and
estimates are
<span class="math display">\[
\hat{\boldsymbol \mu} = \frac{1}{n} \boldsymbol X\boldsymbol 1_n
\]</span>
<span class="math display">\[
\hat{\boldsymbol \Sigma} = \frac{1}{n} \boldsymbol X\boldsymbol X^T - \hat{\boldsymbol \mu} \hat{\boldsymbol \mu}^T
\]</span></p>
<p>Because of the ambiguity in convention (machine learning vs statistics convention) and the often implicit use
of centered data matrices the matrix notation is often confusing. Hence, using the other two
notations is generally preferable.</p>
</div>
<div id="quadratic-approximation-of-log-likelihood-function-around-mle" class="section level2">
<h2><span class="header-section-number">4.3</span> Quadratic approximation of log-likelihood function around MLE</h2>
<p>The observed Fisher information (matrix) <span class="math inline">\({\boldsymbol J_n}(\hat{\boldsymbol \theta}_{ML})\)</span> occurs naturally in the quadratic approximation of the log-likelihood function:</p>
<p><img src="fig/lecture4_p1.PNG" width="80%" style="display: block; margin: auto;" /></p>
<p>Recall the Taylor series approximation of scalar-valued function <span class="math inline">\(f(\boldsymbol x)\)</span> around <span class="math inline">\(\boldsymbol x_0\)</span>:
<span class="math display">\[
f(\boldsymbol x) \approx f(\boldsymbol x_0) + \nabla f(\boldsymbol x_0) (\boldsymbol x-\boldsymbol x_0) + \frac{1}{2} 
(\boldsymbol x-\boldsymbol x_0)^T \nabla^T \nabla f(\boldsymbol x_0) (\boldsymbol x-\boldsymbol x_0) + \ldots
\]</span></p>
<p>The second order Taylor series of <span class="math inline">\(l_n(\boldsymbol \theta)\)</span> around the maximum likelihood <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> yields:</p>
<ul>
<li><p>for univariate <span class="math inline">\(\theta\)</span>:
<span class="math display">\[l_n(\theta) = l_n(\hat{\theta}_{ML})- \frac{1}{2}(\hat{\theta}_{ML}- \theta)^2 
J_n(\hat{\theta}_{ML})+\dots\]</span></p></li>
<li><p>for multivariate <span class="math inline">\(\boldsymbol \theta\)</span>:
<span class="math display">\[l_n(\boldsymbol \theta) = l_n(\hat{\boldsymbol \theta}_{ML})- \frac{1}{2}(\hat{\boldsymbol \theta}_{ML}- \boldsymbol \theta)^T J_n(\hat{\boldsymbol \theta}_{ML})(\hat{\boldsymbol \theta}_{ML}-\boldsymbol \theta)+\dots\]</span></p></li>
</ul>
<p>Note that <em>there is no linear term</em> as we assume
<span class="math inline">\(\nabla l_n(\hat{\boldsymbol \theta}_{ML} ) = 0\)</span>, i.e. that the gradient of the log-likelihood function vanishes at the MLE by construction.</p>
<p>Note the similarity of the above quadratic approximation with the log-density of the univariate and multivariate normal
distribution:</p>
<ul>
<li><p>log-density of univariate normal distribution: <span class="math inline">\(C - \frac{1}{2} {(x-\mu)^2} \sigma^{-2}\)</span></p></li>
<li><p>log-density of multivariate normal: <span class="math inline">\(C - \frac{1}{2}(\boldsymbol x-\boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu)\)</span></p></li>
</ul>
<p>In particular note that in this approximation the observed Fisher information (matrix) plays the role of the <strong>inverse</strong> (co)variance (matrix) <span class="math inline">\(\sigma^{-2}\)</span> and <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span>!</p>
<p>Taking an quadratic approximation is thus closely linked to assuming
normality.</p>
</div>
<div id="asymptotic-normality-of-mle" class="section level2">
<h2><span class="header-section-number">4.4</span> Asymptotic normality of MLE</h2>
<p>Theorem: Asymptotic normality of MLE point estimate, with inverse Fisher information as variance.</p>
<p>Intuitively, it would makes sense to associate large amount of curvature at the MLE with low variance of the MSE (and conversely, low amount of curvature with high variance).</p>
<p>This intuition is confirmed by the following theorem:
<strong>For large sample size <span class="math inline">\(n\)</span> the MLE is normally distributed around the true parameter and with (co)variance equal to the inverse of the observed Fisher information</strong></p>
<p><span class="math display">\[\hat{\boldsymbol \theta}_{ML} \overset{a}{\sim}\underbrace{N_d}_{\text{multivariate normal}}\left(\underbrace{\boldsymbol \theta}_{\text{mean vector}},\underbrace{\boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})^{-1}}_{\text{ covariance matrix}}\right)\]</span></p>
<p>For an single scalar parameter <span class="math inline">\(\theta\)</span> this reduces to
<span class="math display">\[
\hat{\theta} \overset{a}{\sim} N(\theta, J_n(\hat{\theta})^{-1})
\]</span></p>
<p>This theorem is <strong>valid under regularity conditions</strong>. The most important requirements are that the likelihood is twice differentiable at <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> so that the observed Fisher information can be computed, and that the MLE lies at a peak within the support and not at the boundary.</p>
<p>Note we only state the result here, the proof itself will
be explained in more advanced later modules (cf. Year 3 course “Statistical Inference”). Essentially, the proof works by showing that the error of the quadratic approximation becomes negligible for large sample size.</p>
<p><strong>This theorem greatly enhances the usefulness of the method of maximum likelihood</strong>:
In regular setting ML not only yields point estimates
for the parameters but also (asymptotic) estimates of their variance and a corresponding normal sampling distribution
for the estimated parameter.</p>
</div>
<div id="observed-or-expected-fisher-information-to-estimate-variance-of-the-mle" class="section level2">
<h2><span class="header-section-number">4.5</span> Observed or expected Fisher information to estimate variance of the MLE?</h2>
<p>There used to be some discussion whether to use the observed or the expected Fisher information
to estimate the variance. There’s an important classic paper that answers this question:</p>
<p>Efron, B. &amp; Hinkley, D.V. (1978). Assessing the accuracy of the maximum
likelihood estimator: Observed versus expected Fisher information.
Biometrika, 65, 457-87. <a href="https://doi.org/10.1093/biomet/65.3.457" class="uri">https://doi.org/10.1093/biomet/65.3.457</a></p>
<p>Conclusion: <strong>use the observed</strong> Fisher information <span class="math inline">\(\boldsymbol J_n\)</span> <strong>not the expected</strong> Fisher information <span class="math inline">\(\boldsymbol I\)</span>!<br />
Because the observed Fisher information is based on the data at hand (like the MLE).</p>
</div>
<div id="normal-confidence-intervals-for-mles" class="section level2">
<h2><span class="header-section-number">4.6</span> Normal confidence intervals for MLEs</h2>
<p>Given a probability model for the data, maximum likelihood proceeds by</p>
<ul>
<li>maximisation of <span class="math inline">\(l_n(\boldsymbol \theta)\)</span></li>
<li>computation of the curvature at <span class="math inline">\(l_n(\hat{\boldsymbol \theta}^{ML})\)</span>.</li>
</ul>
<p>Then you get:</p>
<ul>
<li>a point estimate <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span></li>
<li>the asymptotic variance of <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span></li>
<li>the corresponding asymptotic normal distribution</li>
</ul>
<p>The asymptotic normality enables us to construct a
corresponding normal confidence interval (CI) and also to conduct associated tests
(is a value included in the CI or not?):</p>
<p><img src="fig/lecture4_p2.PNG" width="80%" style="display: block; margin: auto;" /></p>
<p>Thus, to construct the asymptotic normal CI for a maximum likelihood estimator of
a scalar <span class="math inline">\(\theta\)</span> we use the MLE <span class="math inline">\(\hat{\theta}_{ML}\)</span> and its standard deviation <span class="math inline">\(\hat{\sigma}=\widehat{\text{SD}}(\hat{\theta}_{ML})\)</span> computed from the observed Fisher information:</p>
<p><span class="math display">\[\text{CI}=[\hat{\theta}_{ML} \pm c_{normal} \hat{\sigma}]\]</span></p>
<p><span class="math inline">\(c_{normal}\)</span> is a critical value for the standard-normal symmetric confidence interval
chosen to achieve the desired nominal coverage (see also refresher).</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(\kappa\)</span> coverage</th>
<th>Critical value <span class="math inline">\(c_{\text{normal}}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>1.64</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>1.96</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>2.58</td>
</tr>
</tbody>
</table>
<p>These values are computed using the inverse standard normal distribution function via
<span class="math inline">\(c_{\text{normal}}=\Phi^{-1}\left(\frac{1+\kappa}{2}\right)\)</span>.</p>
<p>For example, for a CI with 95% coverage one uses the factor 1.96 so that
<span class="math display">\[\text{CI}=[\hat{\theta}_{ML} \pm 1.96\, \widehat{\text{SD}}(\hat{\theta}_{ML}) ]\]</span>.</p>
</div>
<div id="wald-statistic" class="section level2">
<h2><span class="header-section-number">4.7</span> Wald statistic</h2>
<p>Centering the MLE <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> with <span class="math inline">\(\boldsymbol \theta_0\)</span> followed by
standardising with <span class="math inline">\(\widehat{\text{SD}}(\hat{\theta}_{ML})\)</span> yields the <strong>Wald statistic</strong>:</p>
<p>(for scalar <span class="math inline">\(\theta\)</span>)
<span class="math display">\[t(\theta_0) = \frac{\hat{\theta}_{ML}-\theta_0}{\text{SD}(\hat{\theta}_{ML})} = J_n(\hat{\theta}_{ML})^{1/2} (\hat{\theta}_{ML}-\theta_0)\]</span></p>
<p>(for vector <span class="math inline">\(\boldsymbol \theta\)</span>)
<span class="math display">\[\boldsymbol t(\boldsymbol \theta_0) = \text{SD}(\hat{\boldsymbol \theta}_{ML})^{-1}(\hat{\boldsymbol \theta}_{ML}-\boldsymbol \theta_0) = \boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})^{1/2}(\hat{\boldsymbol \theta}_{ML}-\boldsymbol \theta_0)\]</span></p>
<p>Note in the multivariate case we need to use <em>matrix inversion</em> and the <em>matrix square root</em>.</p>
<p>We now assume that the true underlying parameter is <span class="math inline">\(\boldsymbol \theta_0\)</span>. Since the MLE is asymptotically normal the Wald statistic
is asymptotically <strong>standard normal</strong> distributed as follows:
<span class="math display">\[\begin{align*}
\begin{array}{cc}
\boldsymbol t(\boldsymbol \theta_0) \overset{a}{\sim}\\
t(\theta_0) \overset{a}{\sim}\\
\end{array}
\begin{array}{ll}
N_d(0,\boldsymbol I_d)\\
N(0,1)\\
\end{array}
\begin{array}{ll}
  \text{for vector } \boldsymbol \theta\\
  \text{for scalar } \theta\\
\end{array}
\end{align*}\]</span></p>
<p>Correspondingly, the <strong>squared</strong> Wald statistic is chi-squared distributed
assuming <span class="math inline">\(\boldsymbol \theta_0\)</span> as true parameter:
<span class="math display">\[\begin{align*}
\begin{array}{cc}
\boldsymbol t(\boldsymbol \theta_0)^T \boldsymbol t(\boldsymbol \theta_0)\\
t(\theta_0)^2\\
\end{array}
\begin{array}{ll}
\overset{a}{\sim}\chi^2_d\\
\overset{a}{\sim}\chi^2_1\\
\end{array}
\begin{array}{ll}
  \text{for vector } \boldsymbol \theta\\
  \text{for scalar } \theta\\
\end{array}
\end{align*}\]</span></p>
</div>
<div id="normal-ci-expressed-using-the-squared-wald-statistics" class="section level2">
<h2><span class="header-section-number">4.8</span> Normal CI expressed using the squared Wald statistics</h2>
<p>The normal CI can be expressed using Wald statistics as follows:</p>
<p><span class="math display">\[\text{CI}=\{\theta_0:  | t(\theta_0)| &lt; c_{\text{normal}} \}\]</span></p>
<p>Similary, it can also be expressed using the squared Wald statistics:</p>
<p><span class="math display">\[\text{CI}=\{\theta_0:   t(\theta_0)^2 &lt; c_{\text{chisq}} \}\]</span></p>
<p>The following list the critical values for the three most common choice of
coverage <span class="math inline">\(\kappa\)</span>
for <span class="math inline">\(df=1\)</span> when using the chi-squared distribution:</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(\kappa\)</span> coverage</th>
<th>Critical value <span class="math inline">\(c_{\text{chisq}}\)</span> (<span class="math inline">\(df=1\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>2.71</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>3.84</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>6.63</td>
</tr>
</tbody>
</table>
</div>
<div id="testing-and-confidence-intervals" class="section level2">
<h2><span class="header-section-number">4.9</span> Testing and confidence intervals</h2>
<p>There is a <strong>duality between confidence intervals and statistical tests</strong>: for every <span class="math inline">\(\theta_0\)</span> inside a CI with coverage <span class="math inline">\(\kappa\)</span> the data do not allow to reject the hypothesis that <span class="math inline">\(\theta_0\)</span> is
the true parameter with significance level <span class="math inline">\(1-\kappa\)</span>.
In contrast, all values <span class="math inline">\(\theta_0\)</span> outside the CI can be rejected with significance level <span class="math inline">\(1-\kappa\)</span> to be the true parameter.<br />
Thus, <strong>the test
decision (reject or not) is mirrored in CIs by whether a parameter
lies outside or inside the CI.</strong></p>
<p>Therefore, the Wald statistic can be used both as a statistic to test
whether <span class="math inline">\(\boldsymbol \theta_0\)</span> is the true underlying parameter value
as well as to construct CIs covering the true parameter.</p>
</div>
<div id="example-normal-distribution" class="section level2">
<h2><span class="header-section-number">4.10</span> Example: normal distribution</h2>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
x_1,\dots,x_n \sim N(\mu, \sigma^2)\\
\\
\\
l_n(\mu) = -\frac{1}{2\sigma^2}\sum^n_{i=1}(x_i-\mu)^2\\
\\
S_n(\mu) = \frac{1}{\sigma^2}\sum^n_{i=1}(x_i-\mu)
\end{array}
\begin{array}{ll}
\mu=?\\
\sigma^2 = \text{ constant/known}\\
\\
\text{log-likelihood}\\
\\
\\
\end{array}
\end{align*}\]</span></p>
<p><span class="math display">\[\hat{\mu}_{ML} = \frac{1}{n}\sum^n_{i=1}(x_i)=\bar{x}\]</span></p>
<p>The corresponding observed Fisher information at <span class="math inline">\(\hat{r}_{ML}=\hat{\mu}_{ML}\)</span>:
<span class="math display">\[J_n(\hat{\mu}_{ML})=\frac{n}{\sigma^2}\]</span></p>
<p>Asymptotic distribution of <span class="math inline">\(\hat{\mu}_{ML}\)</span>:
<span class="math display">\[\hat{\mu}_{ML}\overset{a}{\sim}N\left(\mu,\frac{\sigma^2}{n}\right)\]</span>
Note that in this case it is in fact also the <strong>exact</strong> solution (not just valid asymptotically).</p>
<p><strong>Wald statistic:</strong></p>
<p><span class="math display">\[t(\mu_0) = \frac{\hat{\mu}_{ML}-\mu_0}{\sigma / \sqrt{n}}\overset{a}{\sim}N(0,1)\]</span>
This is the one sample <span class="math inline">\(t\)</span>-statistic (with given <span class="math inline">\(\sigma\)</span>).</p>
<p><strong>Squared Wald statistic:</strong></p>
<p><span class="math display">\[t(\mu_0)^2 = \frac{(\hat{\mu}_{ML}-\mu_0)^2}{\sigma^2 / n}\overset{a}{\sim} \chi^2_1 \]</span></p>
<p>Using the Wald or the squared Wald statistics we can test whether
<span class="math inline">\(\mu_0\)</span> can be rejected as underlying true parameter, and we can
construct corresponding confidence intervals.</p>
</div>
<div id="example-of-non-regular-model" class="section level2">
<h2><span class="header-section-number">4.11</span> Example of non-regular model</h2>
<p>Example of a model with non-differentiable likelihood function at the MLE
is the uniform distribution with upper bound <span class="math inline">\(\theta\)</span>:
<span class="math display">\[x_1,\dots,x_n \sim U(0,\theta)\]</span>
<span class="math display">\[\hat{\theta}_{ML} = ?\]</span>
With <span class="math inline">\(x_{[i]}\)</span> we denote the <em>ordered</em> observations with
<span class="math inline">\(0 \leq x_{[1]} &lt; x_{[2]} &lt; \ldots &lt; x_{[n]} \leq \theta\)</span> and
<span class="math inline">\(x_{[n]} = \max(x_1,\dots,x_n)\)</span>.
The probability density function of <span class="math inline">\(U(0,\theta)\)</span> is
<span class="math display">\[f(x|\theta) =\begin{cases}
    \frac{1}{\theta} &amp;\text{if } x \in [0,\theta] \\
    0              &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p><img src="fig/lecture5_p4.PNG" width="80%" style="display: block; margin: auto;" /></p>
<p>and on log-scale
<span class="math display">\[\log f(x|\theta) =\begin{cases}
    - \log \theta &amp;\text{if } x \in [0,\theta] \\
    - \infty              &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>Since all observed data <span class="math inline">\(x_1, \ldots, x_n\)</span> lie in the interval <span class="math inline">\([0,\theta]\)</span>
we get as log-likelihood function
<span class="math display">\[l_n(\theta) = -n\log \theta\]</span>
with the condition <span class="math inline">\(x_{[n]} \leq \theta\)</span>. Therefore the log-likelihood function is maximised at <span class="math inline">\(\hat{\theta}_{ML}=x_{[n]}\)</span>.</p>
<p>Note that <span class="math inline">\(l_n(\theta)\)</span> <strong>is not differentiable</strong> at <span class="math inline">\(\hat{\theta}_{ML}\)</span>
because it sits at the border of the allowed range for <span class="math inline">\(\theta\)</span>.
This means thta the <strong>observed Fisher information cannot be computed</strong> and the asymptotic <strong>normal approximation is not available.</strong></p>
<p>Nonetheless, we can still obtain the sampling distribution of <span class="math inline">\(\hat{\theta}_{ML}=x_{[n]}\)</span>. However, <em>not</em> via asymptotic ML arguments but instead by understanding that <span class="math inline">\(x_{[n]}\)</span> is an order statistic (see <a href="https://en.wikipedia.org/wiki/Order_statistic" class="uri">https://en.wikipedia.org/wiki/Order_statistic</a> ) with the following properties:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{cc}
x_{[n]}\sim \theta \, \text{Beta}(n,1)\\
\\
\text{E}(x_{[n]})=\frac{n}{n+1} \theta\\
\\
\text{Var}(x_{[n]})=\frac{n}{(n+1)^2(n+2)}\theta^2\\
\end{array}
\begin{array}{ll}
\text{&quot;n-th order statistic&quot; }\\
\\
\\
\\
\approx \frac{\theta^2}{n^2}\\
\end{array}
\end{align*}\]</span></p>
<p>Note that the variance decreases with <span class="math inline">\(\frac{1}{n^2}\)</span> which is much faster than the usual <span class="math inline">\(\frac{1}{n}\)</span>, which makes <span class="math inline">\(\hat{\theta}_{ML}\)</span> a super efficient estimator.</p>

<p></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="03-likelihood3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="05-likelihood5.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math20802-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
