<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>12 Choosing priors in Bayesian analysis | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="12 Choosing priors in Bayesian analysis | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="12 Choosing priors in Bayesian analysis | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="12.1 Choosing a prior  12.1.1 Prior as part of the model It is essential in a Bayesian analysis to specify your prior uncertainty about the model parameters. Note that this is simply part of the...">
<meta property="og:description" content="12.1 Choosing a prior  12.1.1 Prior as part of the model It is essential in a Bayesian analysis to specify your prior uncertainty about the model parameters. Note that this is simply part of the...">
<meta name="twitter:description" content="12.1 Choosing a prior  12.1.1 Prior as part of the model It is essential in a Bayesian analysis to specify your prior uncertainty about the model parameters. Note that this is simply part of the...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="conditioning-and-bayes-rule.html"><span class="header-section-number">7</span> Conditioning and Bayes rule</a></li>
<li><a class="" href="models-with-latent-variables-and-missing-data.html"><span class="header-section-number">8</span> Models with latent variables and missing data</a></li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li><a class="" href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li><a class="active" href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">14</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">15</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">16</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">17</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">18</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="choosing-priors-in-bayesian-analysis" class="section level1" number="12">
<h1>
<span class="header-section-number">12</span> Choosing priors in Bayesian analysis<a class="anchor" aria-label="anchor" href="#choosing-priors-in-bayesian-analysis"><i class="fas fa-link"></i></a>
</h1>
<div id="choosing-a-prior" class="section level2" number="12.1">
<h2>
<span class="header-section-number">12.1</span> Choosing a prior<a class="anchor" aria-label="anchor" href="#choosing-a-prior"><i class="fas fa-link"></i></a>
</h2>
<div id="prior-as-part-of-the-model" class="section level3" number="12.1.1">
<h3>
<span class="header-section-number">12.1.1</span> Prior as part of the model<a class="anchor" aria-label="anchor" href="#prior-as-part-of-the-model"><i class="fas fa-link"></i></a>
</h3>
<p>It is <strong>essential in a Bayesian analysis to specify your prior
uncertainty about the model parameters</strong>. Note that this is simply <strong>part of the modelling process</strong>!</p>
<p>Typically, when choosing a suitable prior distribution we consider the overall form (shape and domain) of the distribution as well as its key characteristics such as mean and variance. As we have seen the precision (inverse variance)
of the prior can of be viewed as implied sample size.</p>
<p>For large sample size <span class="math inline">\(n\)</span> the posterior mean converges to the maximum likelihood estimate (and the posterior distribution to normal distribution centered around the MLE), so for large <span class="math inline">\(n\)</span> we may ignore specifying a prior.</p>
<p>However, for small <span class="math inline">\(n\)</span> it is essential that a prior is specified. In non-Bayesian approaches this prior is still there but it is either implicit (maximum likelihood estimation) or specified via a penality (penalised maximum likelihood estimation).<br>
As a result in a Bayesian approach the data analyst needs to be more specfic about
all modelling assumptions.</p>
</div>
<div id="some-guidelines" class="section level3" number="12.1.2">
<h3>
<span class="header-section-number">12.1.2</span> Some guidelines<a class="anchor" aria-label="anchor" href="#some-guidelines"><i class="fas fa-link"></i></a>
</h3>
<p>So the question remains what are good ways to choose a prior? Two useful ways are:</p>
<ol style="list-style-type: decimal">
<li><p>Use a weakly informative prior. This means that you do have an idea (even if only vague) about the suitable values of the parameter of interest, and you use a corresponding prior (for example with moderate variance) to model the uncertainty. This acknowledges that there are no uninformative priors and but also aims that the prior does not dominate the likelihood (i.e. the data). The result is a weakly regularised estimator. Note that it is often desirable that the prior adds information (if only a little) so that it can act as a regulariser.</p></li>
<li><p>Empirical Bayes methods can often be used to determine one or all of the hyperparameters (i.e. the parameters in the prior) from the observed data. There are several ways to do this, one of them is to tune the shrinkage parameter <span class="math inline">\(\lambda\)</span> to achieve minimum MSE. We discuss this further below.</p></li>
</ol>
<p>Furthermore, there also exist many proposals advocating so-called “uninformative priors” or “objective priors”.
However, there are no actually unformative priors, since a prior distribution that looks uninformative (i.e. “flat”) in one coordinate system can be informative in another — this is a simple consequence of the rule for transformation of
probability densities. As a result, often the suggested objective priors are in fact improper, i.e. are not actually probability distributions!</p>
</div>
</div>
<div id="default-priors-or-uninformative-priors" class="section level2" number="12.2">
<h2>
<span class="header-section-number">12.2</span> Default priors or uninformative priors<a class="anchor" aria-label="anchor" href="#default-priors-or-uninformative-priors"><i class="fas fa-link"></i></a>
</h2>
<p>Objective or for default priors are attempts 1) to automatise specification of a prior and 2) to find uniformative priors.</p>
<div id="jeffreys-prior" class="section level3" number="12.2.1">
<h3>
<span class="header-section-number">12.2.1</span> Jeffreys prior<a class="anchor" aria-label="anchor" href="#jeffreys-prior"><i class="fas fa-link"></i></a>
</h3>
<p>The most well-known non-informative prior is given by a proposal by
<a href="https://en.wikipedia.org/wiki/Harold_Jeffreys">Harold Jeffreys (1891–1989)</a> in
1946.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Jeffreys, H. 1946. &lt;em&gt;An invariant form for the prior probability in estimation problems&lt;/em&gt;. Proc. Roy. Soc. A &lt;strong&gt;186&lt;/strong&gt;:453–461.
&lt;a href="https://doi.org/10.1098/rspa.1946.0056" class="uri"&gt;https://doi.org/10.1098/rspa.1946.0056&lt;/a&gt;.&lt;/p&gt;'><sup>13</sup></a></p>
<p>Specifically, this prior is constructed from the expected Fisher information and thus promises automatic construction of objective uninformative priors using the likelihood:
<span class="math display">\[
p(\boldsymbol \theta) \propto \sqrt{\det \boldsymbol I^{\text{Fisher}}(\boldsymbol \theta)}
\]</span></p>
<p>The reasoning underlying this prior is <strong>invariance against transformation of the coordinate system of the parameters</strong>.</p>
<p>For the Beta-Binomial model the Jeffreys prior corresponds to <span class="math inline">\(\text{Beta}(\frac{1}{2}, \frac{1}{2})\)</span>. Note this is not the uniform distribution but a U-shaped prior.</p>
<p>For the Normal-Normal model it corresponds to the flat improper prior <span class="math inline">\(p(\mu) =1\)</span>.</p>
<p>For the Inverse-Gamma-Norma model the Jeffreys prior is the improper prior <span class="math inline">\(p(\sigma^2) = \frac{1}{\sigma^2}\)</span>.</p>
<p>This already illustrates the main problem with this type of prior – namely that it often is improper, i.e. the prior distribution is not actually a probability distribution (i.e. the density does not integrate to 1).</p>
<p>Another issue is that Jeffreys priors are usually not conjugate which complicates the update from the prior to the posterior.</p>
<p>Furthermore, if there are multiple parameters (<span class="math inline">\(\boldsymbol \theta\)</span> is a vector) then Jeffreys priors do not usually lead to sensible priors.</p>
</div>
<div id="reference-priors" class="section level3" number="12.2.2">
<h3>
<span class="header-section-number">12.2.2</span> Reference priors<a class="anchor" aria-label="anchor" href="#reference-priors"><i class="fas fa-link"></i></a>
</h3>
<p>An alternative to Jeffreys priors are the so-called <strong>reference priors</strong> developed by Bernardo (1979).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Bernardo, J. M. 1979. &lt;em&gt;Reference posterior distributions for Bayesian inference (with
discussion).&lt;/em&gt; JRSS B &lt;strong&gt;41&lt;/strong&gt;:113–147. &lt;a href="https://doi.org/10.1111/j.2517-6161.1979.tb01066.x" class="uri"&gt;https://doi.org/10.1111/j.2517-6161.1979.tb01066.x&lt;/a&gt;&lt;/p&gt;'><sup>14</sup></a>
This type of priors aims to choose the prior such that there is maximal “correlation” between the data and the parameter. More precisely, the mutual information between <span class="math inline">\(\theta\)</span> and <span class="math inline">\(x\)</span> is maximised (i.e. the the expected KL divergence between the posterior and prior distribution). The underlying motivation is that the data and parameters should be maximally linked (thereby minimising the influence of the prior).</p>
<p>For univariate settings the reference priors are identical to Jeffreys priors. However, reference prior also provide reasonable priors in multivariate settings.</p>
<p>In both Jeffreys’ and the reference prior approach the choice of prior is by expectation over the data, i.e. not for the specific data set at hand (this can be seen both as a positive and negative!).</p>
</div>
</div>
<div id="empirical-bayes" class="section level2" number="12.3">
<h2>
<span class="header-section-number">12.3</span> Empirical Bayes<a class="anchor" aria-label="anchor" href="#empirical-bayes"><i class="fas fa-link"></i></a>
</h2>
<p>In empirical Bayes the data analysist specifies a family of prior distribution
(say a Beta distribution with free parameters), and then the data at hand are used to find an optimal choise for the hyper-parameters (hence the name “empirical”). Thus the hyper-parameters are not specified but themselves estimated.</p>
<div id="type-ii-maximum-likelihood" class="section level3" number="12.3.1">
<h3>
<span class="header-section-number">12.3.1</span> Type II maximum likelihood<a class="anchor" aria-label="anchor" href="#type-ii-maximum-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>In particular, assuming data <span class="math inline">\(D\)</span>, a likelihood <span class="math inline">\(p(D|\boldsymbol \theta)\)</span> for some model with parameters <span class="math inline">\(\boldsymbol \theta\)</span> as well as a prior
<span class="math inline">\(p(\boldsymbol \theta| \lambda)\)</span> for <span class="math inline">\(\boldsymbol \theta\)</span> with hyper-parameter <span class="math inline">\(\lambda\)</span> the marginal likelihood now depends on <span class="math inline">\(\lambda\)</span>:
<span class="math display">\[
p(D | \lambda) = \int_{\boldsymbol \theta}  p(D|\boldsymbol \theta) p(\boldsymbol \theta| \lambda) d\boldsymbol \theta
\]</span>
We can therefore use maximum (marginal) likelihood find optimal values of <span class="math inline">\(\lambda\)</span> given the data.</p>
<p>Since maximum-likelihood is used in a second level step (the hyper-parameters) this type of empirical Bayes is also often called “type II maximum likelihood”.</p>
</div>
<div id="shrinkage-estimation-using-empirical-risk-minimisation" class="section level3" number="12.3.2">
<h3>
<span class="header-section-number">12.3.2</span> Shrinkage estimation using empirical risk minimisation<a class="anchor" aria-label="anchor" href="#shrinkage-estimation-using-empirical-risk-minimisation"><i class="fas fa-link"></i></a>
</h3>
<p>An alternative (but related) way to estimate hyper-parameters is by minimising the empirical risk.</p>
<p>In the examples for Bayesian estimation that we have considered so far
the posterior mean of the parameter of interest was obtained
by linear shrinkage
<span class="math display">\[
\hat\theta_{\text{shrink}} = \text{E}( \theta | x_1, \ldots, x_n) = \lambda \theta_0 + (1-\lambda) \hat\theta_{\text{ML}}
\]</span>
of the MLE <span class="math inline">\(\hat\theta_{\text{ML}}\)</span> towards the
prior mean <span class="math inline">\(\theta_0\)</span>, with shrinkage intensity <span class="math inline">\(\lambda=\frac{m}{m+n}\)</span>
determined by the pseudo-sample size <span class="math inline">\(m\)</span> and the sample size <span class="math inline">\(n\)</span>.</p>
<p>The resulting point estimate <span class="math inline">\(\hat\theta_{\text{shrink}}\)</span> is called <em>shrinkage estimate</em>
and is a convex combination of <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\hat\theta_{\text{ML}}\)</span>. The prior mean <span class="math inline">\(\theta_0\)</span> is also
called the “target”.</p>
<p>The hyper-parameter in this setting is <span class="math inline">\(m\)</span> (linked to the precision of the prior) and or equivalently the
shrinkage intensity <span class="math inline">\(\lambda\)</span>.</p>
<p>An optimal value for <span class="math inline">\(\lambda\)</span> can be obtained by minimising the mean squared error of the
estimator <span class="math inline">\(\hat\theta_{\text{shrink}}\)</span>.</p>
<p>In particular, by construction, the target <span class="math inline">\(\theta_0\)</span> has low or even zero variance
but non-vanishing and potentially large bias, whereas the MLE <span class="math inline">\(\hat\theta_{\text{ML}}\)</span> will have low or zero bias but a substantial variance. By combinining these two estimators with opposite properties the aim is to achieve
a <em>bias-variance tradeoff</em> so that the resulting estimator <span class="math inline">\(\hat\theta_{\text{shrink}}\)</span> has lower MSE than either
<span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\hat\theta_{\text{ML}}\)</span>.</p>
<p>Specifically, the aim is to find
<span class="math display">\[
\lambda^{\star} = \underset{\lambda}{\arg \min \ }  
\text{E}\left( ( \theta - \hat\theta_{\text{shrink}} )^2\right) 
\]</span></p>
<p>It turns out that this can be minimised without knowing the actual true value of <span class="math inline">\(\theta\)</span>
and the result for an unbiased <span class="math inline">\(\hat\theta_{\text{ML}}\)</span> is
<span class="math display">\[
\lambda^{\star} = \frac{\text{Var}(\hat\theta_{\text{ML}})}{\text{E}( (\hat\theta_{\text{ML}} - \theta_0)^2 )}
\]</span>
Hence, the shrinkage intensity will be small if the variance of the MLE is small and/or if the target
and the MLE differ substantially. On the other hand, if the variance of the MLE is large and/or the target is close to the MLE the shrinkage intensity will be large.</p>
<p>Choosing the shrinkage parameter by optimising expected risk (here mean squared error) is also a form empirical Bayes.</p>
<div class="example">
<p><span id="exm:jamesstein" class="example"><strong>Example 12.1  </strong></span>James-Stein estimator:</p>
<p>Empirical risk minimisation to estimate the shrinkage parameter of the Normal-Normal model
for a single observation yields the James-Stein estimator (1955).</p>
<p>Specifically, James and Stein propose the following estimate for
the multivariate mean <span class="math inline">\(\boldsymbol \mu\)</span> of using a single sample <span class="math inline">\(\boldsymbol x\)</span>
drawn from the multivariate normal <span class="math inline">\(N_d(\boldsymbol \mu, \boldsymbol I)\)</span>:
<span class="math display">\[
\hat{\boldsymbol \mu}_{JS} = \left(1-\frac{d-2}{||\boldsymbol x||^2}\right) \boldsymbol x
\]</span>
Here, we recognise <span class="math inline">\(\hat{\boldsymbol \mu}_{ML} = \boldsymbol x\)</span>, <span class="math inline">\(\boldsymbol \mu_0=0\)</span> and shrinkage intensity <span class="math inline">\(\lambda^{\star}=\frac{d-2}{||\boldsymbol x||^2}\)</span>.</p>
<p>Efron and Morris (1972) and Lindley and Smith (1972)
later generalised the James-Stein estimator to the case
of multiple observations <span class="math inline">\(\boldsymbol x_1, \ldots \boldsymbol x_n\)</span>
and target <span class="math inline">\(\boldsymbol \mu_0\)</span>, yielding an empirical Bayes estimate of <span class="math inline">\(\mu\)</span> based on the Normal-Normal model.</p>
</div>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></div>
<div class="next"><a href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#choosing-priors-in-bayesian-analysis"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li>
<a class="nav-link" href="#choosing-a-prior"><span class="header-section-number">12.1</span> Choosing a prior</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#prior-as-part-of-the-model"><span class="header-section-number">12.1.1</span> Prior as part of the model</a></li>
<li><a class="nav-link" href="#some-guidelines"><span class="header-section-number">12.1.2</span> Some guidelines</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#default-priors-or-uninformative-priors"><span class="header-section-number">12.2</span> Default priors or uninformative priors</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#jeffreys-prior"><span class="header-section-number">12.2.1</span> Jeffreys prior</a></li>
<li><a class="nav-link" href="#reference-priors"><span class="header-section-number">12.2.2</span> Reference priors</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#empirical-bayes"><span class="header-section-number">12.3</span> Empirical Bayes</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#type-ii-maximum-likelihood"><span class="header-section-number">12.3.1</span> Type II maximum likelihood</a></li>
<li><a class="nav-link" href="#shrinkage-estimation-using-empirical-risk-minimisation"><span class="header-section-number">12.3.2</span> Shrinkage estimation using empirical risk minimisation</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 30 April 2022.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
