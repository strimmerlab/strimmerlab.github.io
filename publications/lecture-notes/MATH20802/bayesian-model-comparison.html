<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>11 Bayesian model comparison | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.31 with bs4_book()">
<meta property="og:title" content="11 Bayesian model comparison | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="11 Bayesian model comparison | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="11.1 Marginal likelihood as model likelihood  11.1.1 Simple and composite models In the introduction of the Bayesian learning we already encountered the marginal likelihood \(p(D | M)\) of a model...">
<meta property="og:description" content="11.1 Marginal likelihood as model likelihood  11.1.1 Simple and composite models In the introduction of the Bayesian learning we already encountered the marginal likelihood \(p(D | M)\) of a model...">
<meta name="twitter:description" content="11.1 Marginal likelihood as model likelihood  11.1.1 Simple and composite models In the introduction of the Bayesian learning we already encountered the marginal likelihood \(p(D | M)\) of a model...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="conditioning-and-bayes-rule.html"><span class="header-section-number">7</span> Conditioning and Bayes rule</a></li>
<li><a class="" href="models-with-latent-variables-and-missing-data.html"><span class="header-section-number">8</span> Models with latent variables and missing data</a></li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li><a class="active" href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li><a class="" href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">14</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">15</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">16</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">17</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">18</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bayesian-model-comparison" class="section level1" number="11">
<h1>
<span class="header-section-number">11</span> Bayesian model comparison<a class="anchor" aria-label="anchor" href="#bayesian-model-comparison"><i class="fas fa-link"></i></a>
</h1>
<div id="marginal-likelihood-as-model-likelihood" class="section level2" number="11.1">
<h2>
<span class="header-section-number">11.1</span> Marginal likelihood as model likelihood<a class="anchor" aria-label="anchor" href="#marginal-likelihood-as-model-likelihood"><i class="fas fa-link"></i></a>
</h2>
<div id="simple-and-composite-models" class="section level3" number="11.1.1">
<h3>
<span class="header-section-number">11.1.1</span> Simple and composite models<a class="anchor" aria-label="anchor" href="#simple-and-composite-models"><i class="fas fa-link"></i></a>
</h3>
<p>In the introduction of the Bayesian learning we already encountered the marginal likelihood <span class="math inline">\(p(D | M)\)</span> of a model class <span class="math inline">\(M\)</span> in the denominator of Bayes’ rule:
<span class="math display">\[
p(\boldsymbol \theta| D, M) =  \frac{p(\boldsymbol \theta| M)  p(D | \boldsymbol \theta, M) }{p(D | M)}
\]</span>
Computing this marginal likelihood is different for simple and composite models.</p>
<p>A model is called “simple” if it directly corresponds to a specific distribution,
say, a Normal with fixed mean and variance, or a Binomial distribution with a set probability for the two classes. Thus, a simple model is a point in the model space described by the parameters of a distribution family (e.g.
<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> for the normal family <span class="math inline">\(N(\mu, \sigma^2\)</span>). For a simple model <span class="math inline">\(M\)</span> the density
<span class="math inline">\(p(D | M)\)</span> corresponds to standard likelihood of <span class="math inline">\(M\)</span> and there are no free parameters.</p>
<p>On the other hand, a model is “composite” if it is composed of simple models. This can be a finite set, or it can be comprised of infinite number of simpple models. Thus a composite model
represent a model class.
For example, a Normal with a given mean but unspecified variance, or a Binomial model with unspecified parameter <span class="math inline">\(p\)</span>, is a composite model.</p>
<p>If <span class="math inline">\(M\)</span> is a composite model, with the underlying simple models indexed by
a parameter <span class="math inline">\(\boldsymbol \theta\)</span>, the likelihood of the model is
obtained by marginalisation over <span class="math inline">\(\boldsymbol \theta\)</span>:
<span class="math display">\[
\begin{split}
p(D | M) &amp;= \int_{\boldsymbol \theta} p(D | \boldsymbol \theta, M) p(\boldsymbol \theta| M) d\boldsymbol \theta\\
             &amp;= \int_{\boldsymbol \theta} p(D , \boldsymbol \theta| M) d\boldsymbol \theta\\
\end{split}
\]</span>
i.e. we <em>integrate</em> over all parameter values <span class="math inline">\(\boldsymbol \theta\)</span>.</p>
<p>If the distribution over the parameter <span class="math inline">\(\boldsymbol \theta\)</span> of a model is strongly concentrated around a specific value <span class="math inline">\(\boldsymbol \theta_0\)</span> then the composite model degenerates to a simple point model, and the marginal likelihood becomes
the likelihood of the parameter <span class="math inline">\(\boldsymbol \theta_0\)</span> under that model.</p>
<div class="example">
<p><span id="exm:betabinomial" class="example"><strong>Example 11.1  </strong></span>Beta-Binomial distribution:</p>
<p>Assume that likelihood is binomial with mean parameter <span class="math inline">\(p\)</span>. If <span class="math inline">\(p\)</span> follows
a Beta distribution then the marginal likelihood with <span class="math inline">\(p\)</span> integrated out is the
<a href="https://en.wikipedia.org/wiki/Beta-binomial_distribution">Beta-Binomial distribution</a> (see also Worksheet B2).
This is an example of a <a href="https://en.wikipedia.org/wiki/Compound_probability_distribution">compound probability distribution</a>.</p>
</div>
</div>
<div id="log-marginal-likelihood-as-penalised-maximum-log-likelihood" class="section level3" number="11.1.2">
<h3>
<span class="header-section-number">11.1.2</span> Log-marginal likelihood as penalised maximum log-likelihood<a class="anchor" aria-label="anchor" href="#log-marginal-likelihood-as-penalised-maximum-log-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>By rearranging Bayes’ rule we see that
<span class="math display">\[
 \log p(D | M) =  \log p(D | \boldsymbol \theta, M) - \log  \frac{ p(\boldsymbol \theta| D, M) }{p(\boldsymbol \theta| M)  } 
\]</span>
The above is valid for all <span class="math inline">\(\boldsymbol \theta\)</span>.</p>
<p>Assuming concentration of the posterior around the MLE <span class="math inline">\(\hat{\boldsymbol \theta}_{\text{ML}}\)</span> we will have <span class="math inline">\(p(\hat{\boldsymbol \theta}_{\text{ML}} | D, M)&gt; p(\hat{\boldsymbol \theta}_{\text{ML}}| M)\)</span>
and thus
<span class="math display">\[
 \log p(D | M) =  \underbrace{\log p(D | \hat{\boldsymbol \theta}_{\text{ML}}, M)}_{\text{maximum log-likelihood}} 
- \underbrace{ \log  \frac{ p( \hat{\boldsymbol \theta}_{\text{ML}} | D, M) }{p( \hat{\boldsymbol \theta}_{\text{ML}}| M)  } }_{\text{penalty &gt; 0}}
\]</span>
Therefore, the log-marginal likelihood is essentially a penalised version of the maximum log-likelihood, and the penalty depends on the concentration of the posterior
around the MLE</p>
</div>
<div id="model-complexity-and-occams-razor" class="section level3" number="11.1.3">
<h3>
<span class="header-section-number">11.1.3</span> Model complexity and Occams razor<a class="anchor" aria-label="anchor" href="#model-complexity-and-occams-razor"><i class="fas fa-link"></i></a>
</h3>
<p>Intriguingly, the penality implicit in the log-marginal likelihood is linked to the complexity of the model, in particular to the number of parameters of <span class="math inline">\(M\)</span>.
We will see this directly in the Schwarz approximation of the
log-marginal likelihood discussed below.</p>
<p>Thus, the averaging over <span class="math inline">\(\boldsymbol \theta\)</span> in the marginal likelihood has the effect of automatically penalising complex models.
Therefore, when comparing models using the marginal likelihood a complex model may be ranked below simpler models.
In contrast, when selecting a model by comparing maximum likelihood directly the model with the highest number of parameters always wins over simpler models.
Hence, the penalisation implicit in the marginal likelihood prevents overfitting
that occurs with maximum likelihood.</p>
<p>The principle of preferring a less complex model is called <strong>Occam’s razor</strong>
or the <strong>law of parsimony</strong>.</p>
<p>When choosing models a simpler model is often preferable over a more complex model, because the simpler model is typically better suited to both explaining the currently observed data as well as future data, whereas a complex model will typically only excel in fitting the current data but will perform poorly in prediction.</p>
</div>
</div>
<div id="the-bayes-factor-for-comparing-two-models" class="section level2" number="11.2">
<h2>
<span class="header-section-number">11.2</span> The Bayes factor for comparing two models<a class="anchor" aria-label="anchor" href="#the-bayes-factor-for-comparing-two-models"><i class="fas fa-link"></i></a>
</h2>
<div id="definition-of-the-bayes-factor" class="section level3" number="11.2.1">
<h3>
<span class="header-section-number">11.2.1</span> Definition of the Bayes factor<a class="anchor" aria-label="anchor" href="#definition-of-the-bayes-factor"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Bayes factor</strong> is the ratio of the likelihoods
of the two models:
<span class="math display">\[
B_{12} = \frac{p(D | M_1)}{p(D | M_2)}
\]</span></p>
<p>The <strong>log-Bayes factor</strong>
<span class="math inline">\(\log B_{12}\)</span>
is also called the <strong>weight of evidence</strong> for <span class="math inline">\(M_1\)</span> over <span class="math inline">\(M_2\)</span>.</p>
</div>
<div id="bayes-theorem-in-terms-of-the-bayes-factor" class="section level3" number="11.2.2">
<h3>
<span class="header-section-number">11.2.2</span> Bayes theorem in terms of the Bayes factor<a class="anchor" aria-label="anchor" href="#bayes-theorem-in-terms-of-the-bayes-factor"><i class="fas fa-link"></i></a>
</h3>
<p>We would like to compare two models <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span>. Before seeing data <span class="math inline">\(D\)</span> we can check their <strong>Prior odds</strong> (= ratio of prior probabilities of the models <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span>):<br><span class="math display">\[\frac{\text{Pr}(M_1)}{\text{Pr}(M_2)}\]</span></p>
<p>After seeing data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> we arrive at the <strong>Posterior odds</strong> (= ratio of posterior probabilities):
<span class="math display">\[\frac{\text{Pr}(M_1 | D)}{\text{Pr}(M_2  | D)}\]</span></p>
<p>Using Bayes Theorem <span class="math inline">\(\text{Pr}(M_i | D) = \text{Pr}(M_i) \frac{p(D | M_i) }{p(D)}\)</span> we can rewrite the
posterior odds as
<span class="math display">\[
\underbrace{\frac{\text{Pr}(M_1 | D)}{\text{Pr}(M_2 | D)}}_{\text{posterior odds}} = \underbrace{\frac{p(D | M_1)}{p(D | M_2)}}_{\text{Bayes factor $B_{12}$}} \, 
\underbrace{\frac{\text{Pr}(M_1)}{\text{Pr}(M_2)}}_{\text{prior odds}}
\]</span></p>
<p>The <strong>Bayes factor</strong> is the multiplicative factor that updates the prior odds to the posterior odds.</p>
<p>On the log scale we see that</p>
<p><span class="math display">\[
\text{log-posterior odds = weight of evidence + log-prior odds}
\]</span></p>
</div>
<div id="scale-for-the-bayes-factor" class="section level3" number="11.2.3">
<h3>
<span class="header-section-number">11.2.3</span> Scale for the Bayes factor<a class="anchor" aria-label="anchor" href="#scale-for-the-bayes-factor"><i class="fas fa-link"></i></a>
</h3>
<p>Following Harold Jeffreys (1961)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Jeffreys, H. &lt;em&gt;Theory of Probability&lt;/em&gt;. 3rd ed. Oxford University Press.&lt;/p&gt;"><sup>13</sup></a> one may interpret the strength of the Bayes factor as follows:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th><span class="math inline">\(B_{12}\)</span></th>
<th><span class="math inline">\(\log B_{12}\)</span></th>
<th>evidence in favour of <span class="math inline">\(M_1\)</span> versus <span class="math inline">\(M_2\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>&gt; 100</td>
<td>&gt; 4.6</td>
<td>decisive</td>
</tr>
<tr class="even">
<td>10 to 100</td>
<td>2.3 to 4.6</td>
<td>strong</td>
</tr>
<tr class="odd">
<td>3.2 to 10</td>
<td>1.16 to 2.3</td>
<td>substantial</td>
</tr>
<tr class="even">
<td>1 to 3.2</td>
<td>0 to 1.16</td>
<td>not worth more than a bare mention</td>
</tr>
</tbody>
</table></div>
<p>More recently, Kass and Raftery (1995)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Kass, R.E., and A.E. Raftery. 1995. &lt;em&gt;Bayes factors&lt;/em&gt;. JASA &lt;strong&gt;90&lt;/strong&gt;:773–795. &lt;a href="https://doi.org/10.1080/01621459.1995.10476572" class="uri"&gt;https://doi.org/10.1080/01621459.1995.10476572&lt;/a&gt;&lt;/p&gt;'><sup>14</sup></a> proposed to use the following slightly modified scale:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th><span class="math inline">\(B_{12}\)</span></th>
<th><span class="math inline">\(\log B_{12}\)</span></th>
<th>evidence in favour of <span class="math inline">\(M_1\)</span> versus <span class="math inline">\(M_2\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>&gt; 150</td>
<td>&gt; 5</td>
<td>very strong</td>
</tr>
<tr class="even">
<td>20 to 150</td>
<td>3 to 5</td>
<td>strong</td>
</tr>
<tr class="odd">
<td>3 to 20</td>
<td>1 to 3</td>
<td>positive</td>
</tr>
<tr class="even">
<td>1 to 3</td>
<td>0 to 1</td>
<td>not worth more than a bare mention</td>
</tr>
</tbody>
</table></div>
</div>
<div id="bayes-factor-versus-likelihood-ratio" class="section level3" number="11.2.4">
<h3>
<span class="header-section-number">11.2.4</span> Bayes factor versus likelihood ratio<a class="anchor" aria-label="anchor" href="#bayes-factor-versus-likelihood-ratio"><i class="fas fa-link"></i></a>
</h3>
<p><strong>If both <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span> are simple models</strong> then the <strong>Bayes factor is identical to the likelihood ratio</strong> of the two models.</p>
<p>However, if one of the two models is composite then the Bayes factor and the
generalised likelihood ratio differ:
In the Bayes factor the representative of a composite model is
the <strong>model average</strong> of the simple models indexed by <span class="math inline">\(\boldsymbol \theta\)</span>, with weights
taken from the prior distribution over the simple models contained in <span class="math inline">\(M\)</span>. In contrast, in the generalised likelihood ratio statistic the representative of a composite model is chosen by <em>maximisation</em>.</p>
<p>Thus, <strong>for composite models</strong>, the <strong>Bayes factor does <em>not</em> equal the corresponding generalised likelihood ratio statistic.</strong> In fact, the key difference is that the Bayes factor is a penalised version of the likelihood ratio, with the penality
depending on the difference in complexity (number of parameters) of the two models</p>
</div>
</div>
<div id="approximate-computations" class="section level2" number="11.3">
<h2>
<span class="header-section-number">11.3</span> Approximate computations<a class="anchor" aria-label="anchor" href="#approximate-computations"><i class="fas fa-link"></i></a>
</h2>
<p>The marginal likelihood and the Bayes factor can be difficult to compute
in practise. Therefore, a number of approximations have been developed.
The most important is the so-called Schwarz (1978) approximation of the log-marginal likelihood. It is used to approximate the log-Bayes factor and also yields
the BIC (Bayesian information criterion) which can be interpreted as penalised maximum
likelihood.</p>
<div id="schwarz-1978-approximation-of-log-marginal-likelihood" class="section level3" number="11.3.1">
<h3>
<span class="header-section-number">11.3.1</span> Schwarz (1978) approximation of log-marginal likelihood<a class="anchor" aria-label="anchor" href="#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>The logarithm of the marginal likelihood of a model can be approximated
following Schwarz (1978)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Schwarz, G. 1978. &lt;em&gt;Estimating the dimension of a model&lt;/em&gt;. Ann. Statist. &lt;strong&gt;6&lt;/strong&gt;:461–464. &lt;a href="https://doi.org/10.1214/aos/1176344136" class="uri"&gt;https://doi.org/10.1214/aos/1176344136&lt;/a&gt;&lt;/p&gt;'><sup>15</sup></a> as follow:
<span class="math display">\[
\log p(D | M) \approx l_n^M(\hat{\boldsymbol \theta}_{ML}^{M}) - \frac{1}{2} d_M \log n  
\]</span>
where <span class="math inline">\(d_M\)</span> is the dimension of the model <span class="math inline">\(M\)</span> (number of parameters in <span class="math inline">\(\boldsymbol \theta\)</span> belonging to <span class="math inline">\(M\)</span>) and <span class="math inline">\(n\)</span> is the sample size
and <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}^{M}\)</span> is the MLE.
For a simple model <span class="math inline">\(d_M=0\)</span> so then
there is no approximation as in this case the marginal likelihood equals the likelihood.</p>
<p>The above formula can be obtained by quadratic approximation of the likelihood <strong>assuming large <span class="math inline">\(n\)</span></strong> and assuming that the prior is locally uniform around the MLE. The Schwarz (1978) approximation is therefore a special case of a <a href="https://en.wikipedia.org/wiki/Laplace%27s_method">Laplace approximation</a>.</p>
<p>Note that the approximation is the maximum log-likelihood minus a penalty that depends on the model complexity (as measured by dimension <span class="math inline">\(d\)</span>), hence this is an example of penalised ML! Also note that the distribution over the parameter <span class="math inline">\(\boldsymbol \theta\)</span> is not required in the approximation.</p>
</div>
<div id="bayesian-information-criterion-bic" class="section level3" number="11.3.2">
<h3>
<span class="header-section-number">11.3.2</span> Bayesian information criterion (BIC)<a class="anchor" aria-label="anchor" href="#bayesian-information-criterion-bic"><i class="fas fa-link"></i></a>
</h3>
<p>The BIC (Bayesian information criterion) of the model <span class="math inline">\(M\)</span> is
the approximated log-marginal likelihood times the factor -2:</p>
<p><span class="math display">\[
BIC(M) = -2 l_n^M(\hat{\boldsymbol \theta}_{ML}^{M}) + d_M \log n
\]</span></p>
<p>Thus, when comparing models one aimes to maximise the marginal likelihood or, as approximation, minimise the BIC.</p>
<p>The reason for the factor “-2” is simply to have a quantity that is
on the same scale as the Wilks log likelihood ratio. Some people / software packages also use the factor “2”.</p>
</div>
<div id="approximating-the-weight-of-evidence-log-bayes-factor-with-bic" class="section level3" number="11.3.3">
<h3>
<span class="header-section-number">11.3.3</span> Approximating the weight of evidence (log-Bayes factor) with BIC<a class="anchor" aria-label="anchor" href="#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fas fa-link"></i></a>
</h3>
<p>Using BIC (twice) the log-Bayes factor can be approximated as
<span class="math display">\[ 
\begin{split}
2 \log B_{12} &amp;\approx -BIC(M_1) + BIC(M_2) \\
&amp;=2 \left( l_n^{M_{1}}(\hat{\boldsymbol \theta}_{ML}^{M_{1}}) - l_n^{M_{2}}(\hat{\boldsymbol \theta}_{ML}^{M_{2}}) \right) - \log(n) (d_{M_{1}}-d_{M_{2}}) \\
\end{split}
\]</span>
i.e. it is the penalised log-likelihood ratio of model <span class="math inline">\(M_1\)</span> vs. <span class="math inline">\(M_2\)</span>.</p>
</div>
</div>
<div id="bayesian-testing-using-false-discovery-rates" class="section level2" number="11.4">
<h2>
<span class="header-section-number">11.4</span> Bayesian testing using false discovery rates<a class="anchor" aria-label="anchor" href="#bayesian-testing-using-false-discovery-rates"><i class="fas fa-link"></i></a>
</h2>
<p>We introduce False Discovery Rates (FDR) as a Bayesian method to
distinguish a null model from an alternative model. This is closely linked with classical
frequentist multiple testing procedures.</p>
<div id="setup-for-testing-a-null-model-h_0-versus-an-alternative-model-h_a" class="section level3" number="11.4.1">
<h3>
<span class="header-section-number">11.4.1</span> Setup for testing a null model <span class="math inline">\(H_0\)</span> versus an alternative model <span class="math inline">\(H_A\)</span><a class="anchor" aria-label="anchor" href="#setup-for-testing-a-null-model-h_0-versus-an-alternative-model-h_a"><i class="fas fa-link"></i></a>
</h3>
<p>We consider two models:</p>
<p><span class="math inline">\(H_0:\)</span> null model, with density <span class="math inline">\(f_0(x)\)</span> and distribution <span class="math inline">\(F_0(x)\)</span></p>
<p><span class="math inline">\(H_A:\)</span> alternative model, with density <span class="math inline">\(f_A(x)\)</span> and distribution <span class="math inline">\(F_A(x)\)</span></p>
<p>Aim: given observations <span class="math inline">\(x_1, \ldots, x_n\)</span> we would like to decide for each <span class="math inline">\(x_i\)</span> whether
it belongs to <span class="math inline">\(H_0\)</span> or <span class="math inline">\(H_A\)</span>.</p>
<p>This is done by a critical decision threshold <span class="math inline">\(x_c\)</span>: if <span class="math inline">\(x_i &gt; x_c\)</span> then <span class="math inline">\(x_i\)</span> is called “significant” and otherwise called “not significant”.</p>
<p>In classical statistics one of the the most widely used approach to find the decision threshold is by computing <span class="math inline">\(p\)</span>-values from the <span class="math inline">\(x_i\)</span>
(this uses only the null model but not the alternative model), and then thresholding the <span class="math inline">\(p\)</span>-values a a certain level (say 5%). If <span class="math inline">\(n\)</span> is large then often the test is modified by adjusting the <span class="math inline">\(p\)</span>-values or the threshold (e.g. if Bonferroni correction).</p>
<p>Note that this procedure ignores any information we may have about the alternative model!</p>
</div>
<div id="test-errors" class="section level3" number="11.4.2">
<h3>
<span class="header-section-number">11.4.2</span> Test errors<a class="anchor" aria-label="anchor" href="#test-errors"><i class="fas fa-link"></i></a>
</h3>
<div id="true-and-false-positives-and-negatives" class="section level4" number="11.4.2.1">
<h4>
<span class="header-section-number">11.4.2.1</span> True and false positives and negatives<a class="anchor" aria-label="anchor" href="#true-and-false-positives-and-negatives"><i class="fas fa-link"></i></a>
</h4>
<p>For any decision threshold <span class="math inline">\(x_c\)</span> we can distinguish the following errors:</p>
<ul>
<li>False positives (FP), “false alarm”, type I error: <span class="math inline">\(x_i\)</span> belongs to null but is called “significant”</li>
<li>False negative (FN), “miss”, type II error: <span class="math inline">\(x_i\)</span> belongs to alternative, but is called “not significant”</li>
</ul>
<p>In addition we have:</p>
<ul>
<li>True positives (TP), “hits”: belongs to alternative and is called “significant”</li>
<li>True negatives (TN), “correct rejections”: belongs to null and is called “not significant”</li>
</ul>
</div>
<div id="specificity-and-sensitivity" class="section level4" number="11.4.2.2">
<h4>
<span class="header-section-number">11.4.2.2</span> Specificity and Sensitivity<a class="anchor" aria-label="anchor" href="#specificity-and-sensitivity"><i class="fas fa-link"></i></a>
</h4>
<p>From counts of TP, TN, FN, FP we can derive further quantities:</p>
<ul>
<li><p>True Negative Rate TNR, <strong>specificity</strong>: <span class="math inline">\(TNR= \frac{TN}{TN+FP} = 1- FPR\)</span> with FPR=False Positive Rate = <span class="math inline">\(1-\alpha_I\)</span></p></li>
<li><p>True Positive Rate TPR, <strong>sensitivity</strong>, <strong>power</strong>, recall: <span class="math inline">\(TPR= \frac{TP}{TP+FN} = 1- FNR\)</span> with FNR=False negative rate = <span class="math inline">\(1-\alpha_{II}\)</span></p></li>
<li><p>Accuracy: <span class="math inline">\(ACC = \frac{TP+TN}{TP+TN+FP+FN}\)</span></p></li>
</ul>
<p>Another common way to choose the decision threshold <span class="math inline">\(x_d\)</span> in classical statistics is to balance sensitivity/power vs. specificity (maximising both power and specificity, or equivalently, minimising both false positive and false negative rates). ROC curves plot TPR/sensitivity vs. FPR = 1-specificity.</p>
</div>
<div id="fdr-and-fndr" class="section level4" number="11.4.2.3">
<h4>
<span class="header-section-number">11.4.2.3</span> FDR and FNDR<a class="anchor" aria-label="anchor" href="#fdr-and-fndr"><i class="fas fa-link"></i></a>
</h4>
<p>It is possible to link the above with the observed counts of TP, FP, TN, FN:</p>
<ul>
<li>False Discovery Rate (FDR): <span class="math inline">\(FDR = \frac{FP}{FP+TP}\)</span>
</li>
<li>False Nondiscovery Rate (FNDR): <span class="math inline">\(FNDR = \frac{FN}{TN+FN}\)</span>
</li>
<li>Positive predictive value (PPV), True Discovery Rate (TDR), precision: <span class="math inline">\(PPV = \frac{TP}{FP+TP} = 1-FDR\)</span>
</li>
<li>Negative predictive value (NPV): <span class="math inline">\(NPV = \frac{TN}{TN+FN} = 1-FNDR\)</span>
</li>
</ul>
<p>In order to choose the decision threshold it is natural to balance FDR and FDNR (or PPV and NPV), by minimising both FDR and FNDR or maximising both PPV and NPV.</p>
<p>In machine learning it is common to use “precision-recall plots” that plot precision (=PPV, TDR)
vs. recall (=power, sensitivity).</p>
</div>
</div>
<div id="bayesian-perspective" class="section level3" number="11.4.3">
<h3>
<span class="header-section-number">11.4.3</span> Bayesian perspective<a class="anchor" aria-label="anchor" href="#bayesian-perspective"><i class="fas fa-link"></i></a>
</h3>
<div id="two-component-mixture-model" class="section level4" number="11.4.3.1">
<h4>
<span class="header-section-number">11.4.3.1</span> Two component mixture model<a class="anchor" aria-label="anchor" href="#two-component-mixture-model"><i class="fas fa-link"></i></a>
</h4>
<p>In the Bayesian perspective the problem of choosing the decision threshold is related to computing the posterior probability
<span class="math display">\[\text{Pr}(H_0 | x_i) , \]</span>
i.e. probability of the null model given the observation <span class="math inline">\(x_i\)</span>, or equivalently
computing
<span class="math display">\[\text{Pr}(H_A | x_i) = 1- \text{Pr}(H_0 | x_i)\]</span>
the probability of the alternative model given the observation <span class="math inline">\(x_i\)</span>.</p>
<p>This is done by assuming a mixture model
<span class="math display">\[
f(x) = \pi_0 f_0(x) + (1-\pi_0) f_A(x)
\]</span>
where <span class="math inline">\(\pi_0 = \text{Pr}(H_0)\)</span> is the prior probability of <span class="math inline">\(H_0\)</span> and.
<span class="math inline">\(\pi_A = 1- \pi_0 = \text{Pr}(H_A)\)</span> the prior probabiltiy of <span class="math inline">\(H_A\)</span>.</p>
<p>Note that the weights <span class="math inline">\(\pi_0\)</span> can in fact be estimated from the observations by fitting the mixture distribution
to the observations <span class="math inline">\(x_1, \ldots, x_n\)</span> (so it is effectively an empirical Bayes method where the prior is informed by the data).</p>
</div>
<div id="local-fdr" class="section level4" number="11.4.3.2">
<h4>
<span class="header-section-number">11.4.3.2</span> Local FDR<a class="anchor" aria-label="anchor" href="#local-fdr"><i class="fas fa-link"></i></a>
</h4>
<p>The posterior probability of the null model given a data point is then given by
<span class="math display">\[\text{Pr}(H_0 | x_i) = \frac{\pi_0 f_0(x_i)}{f(x_i)} = LFDR(x_i)\]</span>
This quantity is also known as the <strong>local FDR</strong> or <strong>local False Discovery Rate</strong>.</p>
<p>In the given one-sided setup the local FDR is large (close to 1) for small <span class="math inline">\(x\)</span>, and
will become close to 0 for large <span class="math inline">\(x\)</span>. A common decision rule is given by thresholding
local false discovery rates: if <span class="math inline">\(LFDR(x_i) &lt; 0.1\)</span> the <span class="math inline">\(x_i\)</span> is called significant.</p>
</div>
<div id="q-values" class="section level4" number="11.4.3.3">
<h4>
<span class="header-section-number">11.4.3.3</span> q-values<a class="anchor" aria-label="anchor" href="#q-values"><i class="fas fa-link"></i></a>
</h4>
<p>In correspondence to <span class="math inline">\(p\)</span>-values one can also define tail-area based false discovery rates:
<span class="math display">\[
Fdr(x_i) = \text{Pr}(H_0 | X &gt; x_i) = \frac{\pi_0 F_0(x_i)}{F(x_i)} 
\]</span></p>
<p>These are called <strong>q-values</strong>, or simply <strong>False Discovery Rates (FDR)</strong>. Intriguingly, these also have a frequentist
interpretation as adjusted p-values (using a Benjamini-Hochberg adjustment procedure).</p>
</div>
</div>
<div id="software" class="section level3" number="11.4.4">
<h3>
<span class="header-section-number">11.4.4</span> Software<a class="anchor" aria-label="anchor" href="#software"><i class="fas fa-link"></i></a>
</h3>
<p>There are a number of R packages to compute (local) FDR values:</p>
<p>For example:</p>
<ul>
<li><a href="https://cran.r-project.org/package=locfdr">locfdr</a></li>
<li><a href="http://www.bioconductor.org/packages/release/bioc/html/qvalue.html">qvalue</a></li>
<li><a href="https://cran.r-project.org/package=fdrtool">fdrtool</a></li>
</ul>
<p>and many more.</p>
<p>Using FDR values for screening is especially useful in high-dimensional settings
(e.g. when analysing genomic and other high-throughput data).</p>
<p>FDR values have both a Bayesian as well as frequentist interpretation, providing further evidence that
good classical statistical methods do have a Bayesian interpretation.</p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></div>
<div class="next"><a href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#bayesian-model-comparison"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li>
<a class="nav-link" href="#marginal-likelihood-as-model-likelihood"><span class="header-section-number">11.1</span> Marginal likelihood as model likelihood</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#simple-and-composite-models"><span class="header-section-number">11.1.1</span> Simple and composite models</a></li>
<li><a class="nav-link" href="#log-marginal-likelihood-as-penalised-maximum-log-likelihood"><span class="header-section-number">11.1.2</span> Log-marginal likelihood as penalised maximum log-likelihood</a></li>
<li><a class="nav-link" href="#model-complexity-and-occams-razor"><span class="header-section-number">11.1.3</span> Model complexity and Occams razor</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#the-bayes-factor-for-comparing-two-models"><span class="header-section-number">11.2</span> The Bayes factor for comparing two models</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#definition-of-the-bayes-factor"><span class="header-section-number">11.2.1</span> Definition of the Bayes factor</a></li>
<li><a class="nav-link" href="#bayes-theorem-in-terms-of-the-bayes-factor"><span class="header-section-number">11.2.2</span> Bayes theorem in terms of the Bayes factor</a></li>
<li><a class="nav-link" href="#scale-for-the-bayes-factor"><span class="header-section-number">11.2.3</span> Scale for the Bayes factor</a></li>
<li><a class="nav-link" href="#bayes-factor-versus-likelihood-ratio"><span class="header-section-number">11.2.4</span> Bayes factor versus likelihood ratio</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#approximate-computations"><span class="header-section-number">11.3</span> Approximate computations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#schwarz-1978-approximation-of-log-marginal-likelihood"><span class="header-section-number">11.3.1</span> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li><a class="nav-link" href="#bayesian-information-criterion-bic"><span class="header-section-number">11.3.2</span> Bayesian information criterion (BIC)</a></li>
<li><a class="nav-link" href="#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><span class="header-section-number">11.3.3</span> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#bayesian-testing-using-false-discovery-rates"><span class="header-section-number">11.4</span> Bayesian testing using false discovery rates</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#setup-for-testing-a-null-model-h_0-versus-an-alternative-model-h_a"><span class="header-section-number">11.4.1</span> Setup for testing a null model \(H_0\) versus an alternative model \(H_A\)</a></li>
<li><a class="nav-link" href="#test-errors"><span class="header-section-number">11.4.2</span> Test errors</a></li>
<li><a class="nav-link" href="#bayesian-perspective"><span class="header-section-number">11.4.3</span> Bayesian perspective</a></li>
<li><a class="nav-link" href="#software"><span class="header-section-number">11.4.4</span> Software</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 20 January 2023.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
