<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>9 Properties of Bayesian learning | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="9 Properties of Bayesian learning | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="9 Properties of Bayesian learning | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><meta name="description" content="The Beta-Binomial models allows to observe a number of intriguing features and properties of Bayesian learning. Many of these extend also to other models as we will see later.  9.1 Prior acting as...">
<meta property="og:description" content="The Beta-Binomial models allows to observe a number of intriguing features and properties of Bayesian learning. Many of these extend also to other models as we will see later.  9.1 Prior acting as...">
<meta name="twitter:description" content="The Beta-Binomial models allows to observe a number of intriguing features and properties of Bayesian learning. Many of these extend also to other models as we will see later.  9.1 Prior acting as...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="active" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="properties-of-bayesian-learning" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Properties of Bayesian learning<a class="anchor" aria-label="anchor" href="#properties-of-bayesian-learning"><i class="fas fa-link"></i></a>
</h1>
<p>The Beta-Binomial models allows to observe a number of intriguing features
and properties of Bayesian learning. Many of these extend also to other models as we will see later.</p>
<div id="prior-acting-as-pseudo-data" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Prior acting as pseudo-data<a class="anchor" aria-label="anchor" href="#prior-acting-as-pseudo-data"><i class="fas fa-link"></i></a>
</h2>
<p>In the expression for the posterior mean and variance you can see that
<span class="math inline">\(m=\alpha + \beta\)</span> behaves like an implicit sample size connected with prior information!</p>
<p>Specifically, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> act as <strong>pseudo-counts</strong> that influence
both the posterior mean and the posterior variance, exactly in the same way as coventional data.</p>
<p>For example, larger <span class="math inline">\(m\)</span> (and thus <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>) the smaller is the posterior variance, with variance decreasing proportional to the inverse of <span class="math inline">\(m\)</span>. If the prior is highly concentrated, i.e. if it has low variance and large precision (=inverse variance) then the implicit data size <span class="math inline">\(m\)</span> is large. Conversely, if the prior has a large variance, then the prior is vague and the implicit data size <span class="math inline">\(m\)</span> is small.</p>
<p>Hence, a prior has the same effect as if one would add data – but without actually adding data! This is precisely this why a prior acts as a regulariser and prevents overfitting, because it increases effective sample size.</p>
<p>Another interpretation is that any prior summarises data
that may have been available previously as observations.</p>
</div>
<div id="linear-shrinkage-of-mean" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Linear shrinkage of mean<a class="anchor" aria-label="anchor" href="#linear-shrinkage-of-mean"><i class="fas fa-link"></i></a>
</h2>
<p>The posterior mean <span class="math inline">\(\mu_{\text{posterior}}\)</span> is a linearly adjusted <span class="math inline">\(\hat\mu_{ML}\)</span>. This becomes evident by writing <span class="math inline">\(\mu_{\text{posterior}}\)</span> as</p>
<p><span class="math display">\[
\mu_{\text{posterior}} = \lambda \mu_{\text{prior}} + (1-\lambda) \hat\mu_{ML}
\]</span>
with weight <span class="math inline">\(\lambda \in [0,1]\)</span>
<span class="math display">\[
\lambda = \frac{m}{m+n} \,.
\]</span>
The <strong>posterior mean is a convex combination (i.e. the weighted average) of the ML estimate and the prior mean</strong>. The factor <span class="math inline">\(\lambda\)</span> is called the <strong>shrinkage intensity</strong> — note that it is the ratio of the “prior sample size” (<span class="math inline">\(m\)</span>) and the “effective overall sample size” (<span class="math inline">\(m+n\)</span>).</p>
<ol style="list-style-type: decimal">
<li><p>This is called <em>shrinkage</em>, because the ML estimator is “shrunk” towards the prior mean (which is often called the “target”, and sometimes the target is zero, and then the terminology “shrinking” makes most sense).</p></li>
<li>
<p>If the shrinkage intensity is zero (<span class="math inline">\(\lambda = 0\)</span>) then the ML point estimator is recovered. This implies <span class="math inline">\(\alpha=0\)</span> and <span class="math inline">\(\beta=0\)</span>, or <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>Note that using maximum likelihood to estimate of the proportion <span class="math inline">\(p\)</span> (for moderate or small <span class="math inline">\(n\)</span>) is the same as Bayesian estimation using the Beta-Binomial model with prior <span class="math inline">\(\alpha=0\)</span> and <span class="math inline">\(\beta=0\)</span>. This prior is extremely “u-shaped” and the implicit prior for the ML estimation. (Would you would use such a prior intentionally?)</p>
</li>
<li><p>If the shrinkage intensity is large (<span class="math inline">\(\lambda \rightarrow 1\)</span>) then the posterior mean corresponds to the prior.
This happens if <span class="math inline">\(n=0\)</span> or if <span class="math inline">\(m\)</span> is very large (implying that the prior is sharply concentrated around the prior mean).</p></li>
<li><p>Since the ML estimate (=frequency) is unbiased the Bayesian point estimate is biased (for finite n)! And the bias is in fact the prior mean! So Bayesian statistics produces by default biased estimators (but asymptotically they will be unbiased like in ML).</p></li>
<li><p>That the posterior mean is a linear combination of the ML estimate and the prior mean is not a coincidence. In fact, this is true for all distributions that are exponential families (see e.g. Diaconis and Ylvisaker, 1979). Furthermore, it is possible (and indeed quite useful for computational reasons!) to formulate Bayes theory completely in terms of linear shrinkage (e.g. Hartigan 1969). The resulting theory is called “Bayes linear statistics” (Goldstein and Wooff, 2007).</p></li>
</ol>
</div>
<div id="conjugacy-of-prior-and-posterior-distribution" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Conjugacy of prior and posterior distribution<a class="anchor" aria-label="anchor" href="#conjugacy-of-prior-and-posterior-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>In the Beta-Binomial model for estimating the proportion <span class="math inline">\(p\)</span> the choice of the <strong>Beta distribution as prior distribution</strong> along with the Binomial likelihood resulted in having the <strong>Beta distribution as posterior distribution</strong> as well.</p>
<p>If the prior and posterior belong to the same distributional family the prior is called a <strong>conjugate prior</strong>. This will be the case if the prior has the same functional form as the likelihood.</p>
<p>In the Beta-Binomial the likelihood is based on the Binomial distribution and has the following form
(only terms depending on the parameter <span class="math inline">\(p\)</span> are shown):
<span class="math display">\[
p^x (1-p)^{n-x}
\]</span>
The form of the Beta prior is (again, only showing terms depending on <span class="math inline">\(p\)</span>):
<span class="math display">\[
p^{\alpha-1} (1-p)^{\beta-1}
\]</span>
Since the posterior is proportional to the product of prior
and likelihood the posterior will have exactly the same form as
the prior:
<span class="math display">\[
p^{\alpha+x-1} (1-p)^{\beta+n-x-1}
\]</span>
Choosing the prior distribution from a family conjugate to the likelihood
greatly simplifies Bayesian analysis since the Bayes formula can then be written in form of an update formula for the parameters of the Beta distribution:
<span class="math display">\[
\alpha \rightarrow \alpha +x
\]</span>
<span class="math display">\[
\beta \rightarrow \beta +n-x
\]</span></p>
<p>Thus, conjugate prior distributions are very convenient choices. However, in their application it must be ensured that the prior distribution is flexible enough to encapsulate all prior information that may be available. In cases where this is not the case alternative priors should be used (and most likely this will then require to compute the posterior distribution numerically rather than analytically).</p>
</div>
<div id="large-sample-asymptotics" class="section level2" number="9.4">
<h2>
<span class="header-section-number">9.4</span> Large sample asymptotics<a class="anchor" aria-label="anchor" href="#large-sample-asymptotics"><i class="fas fa-link"></i></a>
</h2>
<div id="large-sample-limits-of-mean-and-variance" class="section level3" number="9.4.1">
<h3>
<span class="header-section-number">9.4.1</span> Large sample limits of mean and variance<a class="anchor" aria-label="anchor" href="#large-sample-limits-of-mean-and-variance"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(n\)</span> is large and <span class="math inline">\(n &gt;&gt; \alpha, \beta\)</span> the posterior mean and variance become asympotically</p>
<p><span class="math display">\[
\mu_{\text{posterior}} \overset{a}{=} \frac{x }{n} = \hat\mu_{ML}
\]</span>
and
<span class="math display">\[
\sigma^2_{\text{posterior}} \overset{a}{=}   \frac{\hat\mu_{ML} (1-\hat\mu_{ML})}{n}
\]</span></p>
<p>Thus, if sample size is large the Bayes’ estimator turns into the ML estimator! Specifically,
the posterior mean becomes the ML point estimate, and the posterior variance is equal to the asymptotic variance computed via the observed Fisher information!</p>
<p>Thus, for large <span class="math inline">\(n\)</span> the data dominate and any details about the prior (such as values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> become irrelevant!</p>
</div>
<div id="asymptotic-normality-of-the-posterior-distribution" class="section level3" number="9.4.2">
<h3>
<span class="header-section-number">9.4.2</span> Asymptotic Normality of the Posterior distribution<a class="anchor" aria-label="anchor" href="#asymptotic-normality-of-the-posterior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Also known as <strong>Bayesian Central Limit Theorem (CLT)</strong>.</p>
<p>Under some regularity conditions (such as regular likelihood and positive prior probability for all
parameter values, finite number of parameters, etc.) for large sample size the Bayesian posterior distribution converges to a Normal distribution
centered around the MLE and with the variance of the MLE:</p>
<p><span class="math display">\[
\text{for large $n$:  }  p(\boldsymbol \theta| \boldsymbol x_1, \boldsymbol x_2, \ldots, \boldsymbol x_n) \to N(\hat{\boldsymbol \theta}_{ML}, \text{Var}(\hat{\boldsymbol \theta}_{ML}) )
\]</span></p>
<p>So not only are the posterior mean and variance converging to the MLE and the variance of the MLE
for large sample size, but also the posterior distribution itself converges to the sampling distribution!</p>
<p>This holds generally in many regular cases, not just in our example of the Beta-Bernoulli model.</p>
<p>The Bayesian CLT is generally known
as the <strong>Bernstein-van Mises theorem</strong> (who discovered it at around 1920-30), but special cases were already known as by Laplace.</p>
<p>In the Worksheet 5 the asymptotic convergence of the posterior distribution to a normal distribution is demonstrated grapically.</p>
</div>
</div>
<div id="posterior-variance-for-finite-n" class="section level2" number="9.5">
<h2>
<span class="header-section-number">9.5</span> Posterior variance for finite <span class="math inline">\(n\)</span><a class="anchor" aria-label="anchor" href="#posterior-variance-for-finite-n"><i class="fas fa-link"></i></a>
</h2>
<p>In the previous chapter we have the derived a Bayesian point estimate
for the proportion <span class="math inline">\(p\)</span> as the posterior mean
<span class="math display">\[
\text{E}(p | x ) = \frac{x+\alpha}{n+m} = \hat{p}_{\text{Bayes}}
\]</span>
with posterior variance
<span class="math display">\[
\text{Var}(p | x) = \frac{\hat{p}_{\text{Bayes}} (1-\hat{p}_{\text{Bayes}})}{n+m+1}
\]</span></p>
<p>Asymptotically, we have seen that for large <span class="math inline">\(n\)</span> the posterior becomes the ML estimator, and the
posterior variance becomes the asymptotic variance of the MLE.
Thus, the Bayesian estimate will be indistinguishable from the MLE for large <span class="math inline">\(n\)</span>
and shares its favourable properties.</p>
<p>In addition, for finite sample size the posterior variance will tyically be <em>smaller</em> than both the asymptotic
posterior variance (for large <span class="math inline">\(n\)</span>) and the prior variance, showing that combining the information
in the prior and in the data leads to a more efficient estimate.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></div>
<div class="next"><a href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#properties-of-bayesian-learning"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="nav-link" href="#prior-acting-as-pseudo-data"><span class="header-section-number">9.1</span> Prior acting as pseudo-data</a></li>
<li><a class="nav-link" href="#linear-shrinkage-of-mean"><span class="header-section-number">9.2</span> Linear shrinkage of mean</a></li>
<li><a class="nav-link" href="#conjugacy-of-prior-and-posterior-distribution"><span class="header-section-number">9.3</span> Conjugacy of prior and posterior distribution</a></li>
<li>
<a class="nav-link" href="#large-sample-asymptotics"><span class="header-section-number">9.4</span> Large sample asymptotics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#large-sample-limits-of-mean-and-variance"><span class="header-section-number">9.4.1</span> Large sample limits of mean and variance</a></li>
<li><a class="nav-link" href="#asymptotic-normality-of-the-posterior-distribution"><span class="header-section-number">9.4.2</span> Asymptotic Normality of the Posterior distribution</a></li>
</ul>
</li>
<li><a class="nav-link" href="#posterior-variance-for-finite-n"><span class="header-section-number">9.5</span> Posterior variance for finite \(n\)</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 18 May 2021.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
