[["index.html", "Preface About these notes About the author About the module Acknowledgements", " Statistical Methods: Likelihood, Bayes and Regression MATH20802 Lecture Notes 2019–2021 Korbinian Strimmer University of Manchester korbinian.strimmer@manchester.ac.uk 8 May 2021 Preface About these notes This is the course text for MATH20802, an introductory course in Statistical Methods for second year mathematics students. These notes will be updated from time to time. To view the current version in your browser visit the online MATH20802 lecture notes. You may also download the MATH20802 lecture notes as PDF. About the author My name is Korbinian Strimmer and I am a Professor in Statistics in the Statistics group of the Department of Mathematics at the University of Manchester. You can find more information about me on my home page. I have first taught this module in the Spring term 2019 at the University of Manchester. I hope you enjoy the course. If you have any questions, comments, or corrections then please email me at korbinian.strimmer@manchester.ac.uk About the module Topics covered The MATH20802 module is designed to run over the course of 11 weeks. It has three parts: Likelihood estimation and likelihood ratio tests (W1–W5) Bayesian learning and inference (W6–W8) Linear regression (W9–W11) This module focuses on conceptual understanding and methods, not on theory, As such, the presentation in this course is non-technical. The aim is to offer insights how diverse statistical approaches are linked and to demonstrate that statistics offers a concise and coherent theory of information rather than being an adhoc collection of “recipes” for data analysis (a common but wrong perception of statistics). Prerequisites For this module it is important that you refresh your knowledge in: Introduction to statistics Probability R data analysis and programming In addition you will need to know matrix algebra and how to compute the gradient and the curvature of a function of several variables. Check the Appendix for a brief refresher of the essential material. Additional support material Accompanying these notes are lecture videos (visualiser style). Furthermore, there is also an MATH20802 online reading list hosted by the University of Manchester library. If you are a University of Manchester student and enrolled in this module you will find on Blackboard: a weekly learning plan for an 11 week study period, weekly worksheets with examples and solutions and R code, and exam papers of previous years. Acknowledgements Many thanks to Beatriz Costa Gomes for her help in creating the 2019 version of the lecture notes and to Kristijonas Raudys for his extensive feedback on the 2020 version. "],["01-likelihood1.html", "1 Overview of statistical learning 1.1 How to learn from data? 1.2 Probability theory versus statistical learning 1.3 Cartoon of statistical learning 1.4 Likelihood", " 1 Overview of statistical learning 1.1 How to learn from data? A fundamental question is how to extract information from data in an optimal way, and to make predictions based on this information. For this purpose, a number of competing theories of information have been developed. Statistics is the oldest science of information and is concerned with offering principled ways to learn from data and to extract and process information using probabilistic models. However, there are other theories of information (e.g. Vapnik-Chernov theory of learning, computational learning) that are more algorithmic than analytic and sometimes not even based on probability theory. Furthermore, there are other disciplines, such computer science and machine learning that are closely linked with and also have substantial overlap with statistics. The field of “data science” today comprises both statistics and a machine learning and brings together mathematics, statistics and computer science. Also the growing field of so-called “artificial intelligence” makes substantial use of statistical and machine learning techniques. The recent popular science book “The Master Algorithm” by Domingos (2015) provides an accessible informal overview over the various schools of science of information. It discusses the main algorithms used in machine learning and statistics: Starting as early as 1763, the Bayesian school of learning was started which later turned out to be closely linked with likelihood inference established in 1922 by R.A. Fisher (1890–1962) and generalised in 1951 to entropy learning by Kullback and Leibler. It was also in the 1950s that the concept of artificial neural network arises, essentially a nonlinear input-output map that works in a non-probabilistic way. This field saw another leap in the 1980 and further progress from 2010 onwards with the development of deep dearning. It is now one of the most popular (and most effective) methods for analysis of imaging data. Even your mobile phone most likely has a dedicated computer chip with special neural network hardware, for example. Further advanced theories of information where developed in the 1960 under the term of computational learning, most notably the Vapnik-Chernov theory, with the most prominent example of the “support vector machine” (another non-probabilistic model). With the advent of large-scale genomic and other high-dimensional data there has been a surge of new and exciting developments in the field of high-dimensional (large dimension) and also big data (large dimension and large sample size), both in statistics and in machine learning. The connections between various fields of information is still not perfectly understood, but it is clear that an overarching theory will need to be based on probabilistic learning. 1.2 Probability theory versus statistical learning When you study statistics (or any other information theory) you need to be aware that there is a fundamental difference between probability theory and statistics, and that relates to the distinction between “randomness” and “uncertainty”. Probability theory studies randomness, by developing mathematical models for randomness (such as probability distributions), and studying corresponding mathematical properties (including asymptotics etc). Probability theoy may in fact be viewed as a branch of measure theory, and thus it belongs to the domain pure mathematics. Probability theory provides probabilistic generative models for data, for simulation of data or for use in learning from data, i.e. inference about the model from observations. Methods and theory how to best learn from data is the domain of applied mathematics, specifically statistics and the related areas of machine learning and data science. Note that statistics, in contrast to probability, is in fact not at all concerned with randomness. Instead, the focus is about measuring and elucidating the uncertainty of events, predictions, outcomes, parameters and this uncertainty measures the state of knowledge. Note that if new data or information becomes available, the state of knowledge and thus the uncertainty changes! Thus, uncertainty is an epistemological property. The uncertainty most often is due to our ignorance of the true underlying processes (on purpose or not), but not because the underlying process is actually random. The success of statistics is based on the fact that we can mathematically model the uncertainty without knowing any specifics of the underlying processes, and we still have procedures for optimal inference under uncertainty. In short, statistics is about describing the state of knowledge of the world, which may be uncertain and incomplete, and to make decisions and prediction in the face of uncertainty, and this uncertaintly sometimes derives from randomness but most often from our ignorance (and sometimes this ignorance even helps to create a simple yet effective model)! 1.3 Cartoon of statistical learning We observe data \\(x_1, \\ldots, x_n\\) assumed to be generated by the underlying true model \\(M_{\\text{true}}\\). To explain the data, and make prediction, we make hypotheses in the form of candidate models \\(M_{1}, M_{2}, \\ldots\\). The true model \\(M_{\\text{true}}\\) itself is unknown and cannot be observed. However, what we can observe is a finite amount of data from the model by measuring properties of objects interest (our observations from experiments). Sometimes we can also perturb the model and see what the effect is (interventional study). The various candidate models \\(M_1, M_2, \\ldots\\) in the model world will never be perfect or correct as the true model \\(M_{\\text{true}}\\) will only be among the candidate models in an idealised situation. However, even an imperfect candidate model will often provide a useful mathematical approximation and capture some important characteristics of the true model and thus will help to interpret observed data.. \\[ \\begin{array}{cc} \\textbf{Hypothesis} \\\\ \\text{How the world works} \\\\ \\end{array} \\longrightarrow \\begin{array}{cc} \\textbf{Model world} \\\\ M_1: f(x | \\theta_1) \\\\ M_2: f(x | \\theta_2) \\\\ \\vdots\\\\ \\end{array} \\] \\[ \\longrightarrow \\begin{array}{cc} \\textbf{Real world,} \\\\ \\textbf{unknown true model} \\\\ M_{\\text{true}}: f(x | \\theta_{\\text{true}}) \\\\ \\end{array} \\longrightarrow \\textbf{Data } x_1, \\ldots, x_n \\] The aim of statistical learning is to identify the model(s) that explain the current data and also predict future data (i.e. predict outcome of experiments that have not been conducted yet). Thus a good model provides a good fit to the current data (i.e. it explains current observations well) and also to future data (i.e. it generalises well). A large proportion of statistical theory is devoted to finding these “good” models that avoid both overfitting (models being too complex and don’t generalise well) or underfitting (models being too simplistic and hence also don’t predict well). Typically the aim is to find a model whose the model complexity matches the complexity of the unknown true model and also the complexity of the data observed from the unknown true model. 1.4 Likelihood A core problem in statistics is how to find probabilistic models for explaining existing data and predicting new data. For this we need a measure of how good a hypothesis/candidate model \\(M_k\\) is as approximation for the (typically unknown) true data generating model \\(M_{\\text{true}}\\). As you already know from the year 1 module MATH10282 “Introduction to Statistics”, one such measure is provided by the likelihood function which helps to choose among the various candidate models and estimate corresponding parameters by finding the model \\(M\\) that maximises the (log)-likelihood. Given a probability distribution \\(F_{\\boldsymbol \\theta}\\) with density or mass function \\(f(x|\\boldsymbol \\theta)\\) where \\(\\boldsymbol \\theta\\) is a parameter vector, and \\(x_1,\\dots,x_n\\) is the observed iid data (i.e. independent and identically distributed), the likelihood function is defined as \\[ L_n(\\boldsymbol \\theta) =\\prod_{i=1}^{n} f(x_i|\\boldsymbol \\theta) \\] Typically, instead of the likelihood one uses the log-likelihood function: \\[\\log L(_n\\boldsymbol \\theta) = l_n(\\boldsymbol \\theta) = \\sum_{i=1}^n \\log f(x_i|\\boldsymbol \\theta)\\] Reasons for using log-likelihood (rather than likelihood) include that the log density is in fact the more “natural” and relevant quantity (this will become clear in the upcoming chapters) and that addition is numerically more stable than multiplication on a computer. For discrete random variables for which \\(f(x |\\boldsymbol \\theta)\\) is a probability mass function the likelihood is often interpreted as the probability to observe the data given the model with specified parameters \\(\\boldsymbol \\theta\\). In fact, this was indeed the way how the likelihood was historically introduced. However, this view is not stricly correct. First, given that the samples are iid and thus the ordering of the \\(x_i\\) is not important, an additional factor accounting for the possible permutations is needed in the above to obtain the actual probability of the data. Moreover, for continuous random variables this interpretation breaks down due to the use of densities rather than probability mass functions in the likelihood. Thus, the view of the likelihood being the probability of the data is in fact too simplistic. In the next chapter we will see that the justification for using likelihood rather stems from its close link to the Kullback-Leibler information and cross-entropy. This also helps to understand why using likelihood for estimation is only optimal in the limit of large sample size. In the first part of the MATH28082 “Statistical Methods” module we will study likelihood estimation and inference in much detail. We will provide links to related methods of inference and discuss its information-theoretic foundations. We will also discuss the optimality properties as well as the limitation of likelihood inference. Extensions of likelihood analysis, in particular Bayesian learning, which will be discussed in the second part module. In the third part of the module we will apply statistical learning to linear regression. "],["02-likelihood2.html", "2 From entropy to maximum likelihood 2.1 Entropy 2.2 Kullback-Leibler divergence 2.3 Local quadratic approximation and expected Fisher information 2.4 Entropy learning and maximum likelihood", " 2 From entropy to maximum likelihood 2.1 Entropy 2.1.1 Overview In this chapter we discuss various information criteria and their connection to maximum likelihood. The modern definition of (relative) entropy, or “disorder”, was first discovered in 1875 by physicist L. Boltzmann (1844–1906) in the context of thermodynamics. In the 1940–1950’s the notion of entropy turned out to be central in information theory, a field pioneered by mathematicians such as R. Hartley (1988–1970), S. Kullback (1907–1994), R. Leibler (1914–2003), A. Turing (1912–1954), I. J. Good (1916–2009), C. Shannon (1916–2001), and E. T. Jaynes (1922–1998), and later further explored by S. Amari (1936–), I. Ciszár (1938–), B. Efron (1938–), A. P. Dawid (1946–) and many others. \\[\\begin{align*} \\left. \\begin{array}{cc} \\\\ \\textbf{Entropy} \\\\ \\\\ \\end{array} \\right. \\left. \\begin{array}{cc} \\\\ \\nearrow \\\\ \\searrow \\\\ \\\\ \\end{array} \\right. \\begin{array}{ll} \\text{Shannon Entropy} \\\\ \\\\ \\text{Relative Entropy} \\\\ \\end{array} \\begin{array}{ll} \\text{(Shannon 1948)} \\\\ \\\\ \\text{(Kullback-Leibler 1951)} \\\\ \\end{array} \\end{align*}\\] \\[\\begin{align*} \\left. \\begin{array}{ll} \\text{Fisher information} \\\\ \\\\ \\text{Mutual Information} \\\\ \\end{array} \\right. \\begin{array}{ll} \\rightarrow\\text{ Likelihood theory} \\\\ \\\\ \\rightarrow\\text{ Information theory} \\\\ \\end{array} \\begin{array}{ll} \\text{(Fisher 1922)} \\\\ \\\\ \\text{(Shannon 1948, Lindley 1953)} \\\\ \\end{array} \\end{align*}\\] 2.1.2 Surprise, surprisal or Shannon information The surprise to observe an event of probability \\(p\\) is defined as \\(-\\log(p)\\). This is also called surprisal or Shannon information. Thus, the surprise to observe a certain event (with \\(p=1\\)) is zero, and conversely the surprise to observe an event that is certain not to happen (with \\(p=0\\)) is infinite. The log-odds ratio can be viewed as the difference of the surprise of an event and the surprise of the complementary event: \\[ \\log\\left( \\frac{p}{1-p} \\right) = -\\log(1-p) - ( -\\log(p)) \\] In this module we always use the natural logarithm by default, and will explicitly write \\(\\log_2\\) and \\(\\log_{10}\\) for logarithms with respect to base 2 and 10, respectively. Surprise and entropy computed with the natural logarithm (\\(\\log\\)) is given in “nats” (=natural information units ). Using \\(\\log_2\\) leads to “bits” and using \\(\\log_{10}\\) to “ban” or “Hartley”, cf. https://en.wikipedia.org/wiki/Hartley_(unit). 2.1.3 Shannon entropy Assume we have a discrete distribution \\(F\\) with \\(K\\) classes and class probabilities \\(p_1, \\ldots, p_K\\) with \\(\\text{Pr}(\\text{&quot;class k&quot;}) = p_k\\) and \\(\\sum_{k=1}^K = 1\\). Let \\(x\\) indicate the selected class then define the PMF as \\(f(x = \\text{&quot;class k&quot;})= p_k\\). The Shannon entropy of the discrete distribution \\(F\\) is defined as the expected surprise, i.e. the negative expected log-probability \\[ \\begin{split} H(F) &amp;=-\\text{E}_F\\left(\\log f(x)\\right) \\\\ &amp;= - \\sum_{i=1}^{K}p_i \\log(p_i) \\\\ \\end{split} \\] As all \\(p_k in [0,1]\\) by construction Shannon entropy must be larger or equal to 0. Furthermore, it is bounded above by \\(\\log K\\). Hence for for any discrete distribution \\(F\\) with \\(K\\) categories we have \\[\\log K \\geq H(F) \\geq 0\\] Example 2.1 Discrete uniform distribution \\(U_K\\): let \\(p_1=p_2= \\ldots = p_K = \\frac{1}{K}\\). Then \\[H(U_K) = - \\sum_{i=1}^{K}\\frac{1}{K} \\log\\left(\\frac{1}{K}\\right) = \\log K\\] Note this is the largest value the Shannon entropy can assume with \\(K\\) classes. Example 2.2 Concentrated probability mass: let \\(p_1=1\\) and \\(p_2=p_3=\\ldots=p_K=0\\). Using \\(0\\times\\log(0)=0\\) we obtain for the Shannon probability \\[ H(F) = 1\\times\\log(1) + 0\\times\\log(0) + \\dots = 0\\] Note that 0 is the smallest value that Shannon entropy can assume, and corresponds to maximum concentration. Thus, large entropy implies that the distribution is spread out whereas small entropy means the distribution is concentrated. Correspondingly, maximum entropy distributions can be considered minimally informative about a random variable. This interpretation is also supported by the close link of Shannon entropy with multinomial coefficients counting the permutations of \\(n\\) items (samples) of \\(K\\) distinct types (classes). Example 2.3 Large sample asymptotics of the multinomial coefficient and Shannon entropy: The number of possible permutation of \\(n\\) items of \\(K\\) distinct types, with \\(n_1\\) of type 1, \\(n_2\\) of type 2 and so on, is given by the multinomial coefficient \\[ \\binom{n}{n_1, \\ldots, n_K} = \\frac {n!}{n_1! \\times n_2! \\times\\ldots \\times n_K! } \\] with \\(\\sum_{k=1}^K n_k = n\\) and \\(K \\leq n\\). Using the the Moivre-Sterling formula for large \\(n\\) the factorial can be approximated as \\[ \\log n! \\approx n \\log n -n \\] As a result \\[ \\begin{split} \\log \\binom{n}{n_1, \\ldots, n_K} &amp;= \\log n! - \\sum_{k=1}^K \\log n_k!\\\\ &amp; \\approx n \\log n -n - \\sum_{k=1}^K (n_k \\log n_k -n_k) \\\\ &amp; = n \\log n - \\sum_{k=1}^K n_k \\log n_k\\\\ &amp; = \\sum_{k=1}^K n_k \\log n - \\sum_{k=1}^K n_k \\log n_k\\\\ &amp; = - \\sum_{k=1}^K n_k \\log\\left( \\frac{n_k}{n} \\right)\\\\ \\end{split} \\] and thus \\[ \\begin{split} \\frac{1}{n}\\log \\binom{n}{n_1, \\ldots, n_K} &amp;\\approx - \\sum_{k=1}^K \\frac{n_k}{n} \\log\\left( \\frac{n_k}{n} \\right)\\\\ &amp;= - \\sum_{k=1}^K \\hat{p}_k \\log\\left( \\hat{p}_k \\right)\\\\ &amp;=H(\\hat{F}) \\end{split} \\] where \\(\\hat{F}\\) is the empirical discrete distribution with \\(\\hat{p}_k = \\frac{n_k}{n}\\). The combinatorical derivation of entropy is due to Boltzmann (1877) who discovered it in his research in statistical mechanics and thermodynamics. 2.1.4 Differential entropy Shannon entropy is only defined for discrete random variables. Differential Entropy results from applying the definition of Shannon entropy to a continuous random variable \\(x\\) with density \\(f(x)\\): \\[ H(F) = -\\text{E}_F(\\log f(x)) = - \\int f(x) \\log f(x) \\, dx \\] Despite having essentially the same formula the different name is justified because differential entropy exhibits different properties compared to Shannon entropy, because the logarithm is taken of a density which in contrast to a probability can assumes values larger than one. As a consequence, differential entropy is not bounded below by zero and can be negative. Example 2.4 Consider the uniform distribution \\(U(0, a)\\) with \\(a&gt;0\\), support from \\(0\\) to \\(a\\) and density \\(f(x) = 1/a\\). As \\(-\\int_0^a f(x) \\log f(x) dx =- \\int_0^a \\frac{1}{a} \\log(\\frac{1}{a}) dx = \\log a\\) the differential entropy is \\[H( U(0, a) ) = \\log a \\,.\\] Note that for \\(a &lt; 1\\) the differential entropy is negative. Example 2.5 The density of the univariate normal \\(N(\\mu, \\sigma^2)\\) distribution is \\(f(x |\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)\\) with \\(\\sigma^2 &gt; 0\\). The corresponding differential entropy is is \\[ H(F) = \\frac{1}{2} ( \\log(2 \\pi \\sigma^2)+1) \\,. \\] Note that it only depends on the variance and not on the mean, and that for \\(\\sigma^2 &lt; 1/(2 \\pi e) \\approx 0.0585\\) the differential entropy is negative. 2.1.5 Maximum entropy principle to characterise distributions Both maximum Shannon entropy and differential entropy are useful to characterise distributions: The discrete uniform distribution is the maximum entropy distribution among all discrete distributions. the maximum entropy distribution of a continuous random variable with support \\([-\\infty, \\infty]\\) with a specific mean and variance is the normal distribution the maximum entropy distribution among all continuous distributions supported in \\([0, \\infty]\\) with a specified mean is the exponential distribution. The higher the entropy the more spread out (and more uninformative) is a distribution. Using maximum entropy to characterise maximally uniformative distributions was advocated by E.T. Jaynes (who also proposed to use maximum entropy in the context of finding Bayesian priors). A list of maximum entropy distribution is given here: https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution . 2.1.6 Cross-entropy If in the definition of Shannon entropy (and differential entropy) the expectation over the log-density (say \\(g(x)\\) of distribution \\(G\\)) is with regard to a different distribution \\(F\\) over the same state space we arrive at the cross-entropy \\[H(F, G) =-\\text{E}_F\\left( \\log g(x) \\right)\\] Therefore, cross-entropy is a measure linking two distributions \\(F\\) and \\(G\\). Note that cross-entropy is not symmetric with regard to \\(F\\) and \\(G\\), because the expectation is taken with reference to \\(F\\). By construction \\(H(F, F) = H(F)\\). A crucial property of the cross-entropy \\(H(F, G)\\) is that it is bounded below by the entropy of \\(F\\), therefore \\[ H(F, G) \\geq H(F) \\] with equality for \\(F=G\\). Equivalently we can write \\[ H(F, G)-H(F) \\geq 0 \\] In fact, this recalibrated cross-entropy turns out to be more fundamental than both cross-entropy and Shannon resp. differential entropy. It will be studied in detail in the next section. Example 2.6 Cross-entropy between two normals: Assume \\(F_{\\text{ref}}=N(\\mu_{\\text{ref}},\\sigma^2_{\\text{ref}})\\) and \\(F=N(\\mu,\\sigma^2)\\). Then the cross-entropy is \\[ H(F_{\\text{ref}}, F) = \\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 } +\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2} +\\log(2 \\pi \\sigma^2) \\right) \\] Example 2.7 If \\(\\mu_{\\text{ref}} = \\mu\\) and \\(\\sigma^2_{\\text{ref}} = \\sigma^2\\) then the above cross-entropy \\(H(F,G)\\) degenerates to the differential entropy \\(H(F_{\\text{ref}}) = \\frac{1}{2} \\left(\\log( 2 \\pi \\sigma^2_{\\text{ref}}) +1 \\right)\\). 2.2 Kullback-Leibler divergence 2.2.1 Definition Also known as relative entropy and discrimination information. The relative entropy measures the divergence of a distribution \\(G\\) from the distribution \\(F\\) and is defined as \\[ \\begin{split} D_{\\text{KL}}(F,G) &amp;= \\text{E}_F\\log\\left(\\frac{dF}{dG}\\right) \\\\ &amp; = \\text{E}_F\\log\\left(\\frac{f(x)}{g(x)}\\right) \\\\ &amp; = \\underbrace{-\\text{E}_F(\\log g(x))}_{\\text{cross-entropy}} - (\\underbrace{-\\text{E}_F (\\log f(x)) }_\\text{(differential) entropy}) \\\\ &amp; = H(F, G)-H(F) \\\\ \\end{split} \\] \\(D_{\\text{KL}}(F, G)\\) measures the amount of information lost if \\(G\\) is used to approximate \\(F\\). If \\(F\\) and \\(G\\) are identical (and no information is lost) then \\(D_{\\text{KL}}(F,G)=0\\). (Note: here “divergence” measures the dissimilarity between probability distributions. This type of divergence is not related and should not be confused with divergence (div) as used in vector analysis.) The term divergence (rather than distance) implies also that the distributions \\(F\\) and \\(G\\) are not interchangeable in \\(D_{\\text{KL}}(F, G)\\). In applications in statistics the typical roles of \\(F\\) and \\(G\\) are: \\(F\\) as the (unknown) underlying true model for the data generating process \\(G\\) as the approximating model (e.g. some parametric family) In Bayesian statistics we use \\(F\\) as posterior distribution \\(G\\) as prior distribution There exist various notations for KL divergence in the literature. Here we use \\(D_{KL}(F, G)\\) but often you can find \\(KL(F || G)\\) or \\(\\boldsymbol I^{KL}(F; G)\\) in other references. Some authors (e.g. Efron) call twice the KL divergence \\(2 D_{KL}(F, G) = D(F, G)\\) the deviance of \\(G\\) from \\(F\\). 2.2.2 Properties of KL divergence \\(D_{\\text{KL}}(F, G) \\neq D_{\\text{KL}}(G, F)\\), i.e., KL divergence is not symmetric, \\(F\\) and \\(G\\) cannot be interchanged. \\(D_{\\text{KL}}(F, G) = 0\\) if and only if \\(F=G\\), i.e., the KL divergence is zero if and only if \\(F\\) and \\(G\\) are identical. \\(D_{\\text{KL}}(F, G)\\geq 0\\), proof via the Jensen Inequality. \\(D_{\\text{KL}}(F, G)\\) remains invariant under coordinate transformations, i.e. it is an invariant geometric quantity. Note that in the KL divergence the expectation is taken over a ratio of densities (or ratio of probabilities for discrete random variables). This is what creates the transformation invariance. For more details and proofs of properties 3 and 4 see Worksheet 1. 2.2.3 Examples Example 2.8 KL divergence between two Bernoulli distributions \\(\\text{Ber}(p)\\) and \\(\\text{Ber}(q)\\): The “success” probabilities for the two distributions are \\(p\\) and \\(q\\), respectively, and the complementary “failure” probabilities are \\(1-p\\) and \\(1-q\\). With this we get for the KL divergence \\[ D_{\\text{KL}}(\\text{Ber}(p), \\text{Ber}(q))=p \\log\\left( \\frac{p}{q}\\right) + (1-p) \\log\\left(\\frac{1-p}{1-q}\\right) \\] Example 2.9 KL divergence between two univariate normals with different means and variances: Assume \\(F_{\\text{ref}}=N(\\mu_{\\text{ref}},\\sigma^2_{\\text{ref}})\\) and \\(F=N(\\mu,\\sigma^2)\\). Then \\[ \\begin{split} D_{\\text{KL}}(F_{\\text{ref}},F) &amp;= H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) \\\\ &amp;= \\frac{1}{2} \\left( \\frac{(\\mu-\\mu_{\\text{ref}})^2}{\\sigma^2} + \\frac{\\sigma_{\\text{ref}}^2}{\\sigma^2} -\\log\\left(\\frac{\\sigma_{\\text{ref}}^2}{\\sigma^2}\\right)-1 \\right) \\\\ \\end{split} \\] Example 2.10 KL divergence between two univariate normals with different means and common variance: An important special case of the previous Example 2.9 occurs if the variances are equal. Then we get \\[D_{\\text{KL}}(N(\\mu_{\\text{ref}}, \\sigma^2), N(\\mu, \\sigma^2) )=\\frac{1}{2} \\left(\\frac{(\\mu-\\mu_{\\text{ref}})^2}{\\sigma^2}\\right)\\] 2.3 Local quadratic approximation and expected Fisher information 2.3.1 Definition of expected Fisher information KL information measures the divergence of two distributions. We may thus use relative entropy to measure the divergence between two distributions in the same family, separated in parameter space only by some small \\(\\boldsymbol \\varepsilon\\): \\[ \\begin{split} h(\\boldsymbol \\theta) &amp;= D_{\\text{KL}}(F_{\\boldsymbol \\theta_0}, F_{\\boldsymbol \\theta})\\\\ &amp;= D_{\\text{KL}}(F_{\\boldsymbol \\theta_0}, F_{\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon})\\\\ \\end{split} \\] Note that in the above the first argument in the KL divergence is fixed at \\(\\boldsymbol \\theta_0\\) and the second is varied. Since KL information vanishes only when the two arguments are identical \\(h(\\boldsymbol \\theta)\\) reaches a minimum at \\(\\boldsymbol \\theta_0\\) with \\(h(\\boldsymbol \\theta_0)=0\\). We can therefore approximate \\(h(\\boldsymbol \\theta) = h(\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon)\\) by the following quadratic function around \\(\\boldsymbol \\theta_0\\) \\[ \\begin{split} h(\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon) &amp; = \\frac{1}{2} \\boldsymbol \\varepsilon^T \\nabla^T\\nabla h(\\boldsymbol \\theta_0) \\boldsymbol \\varepsilon\\\\ &amp; = \\frac{1}{2} \\boldsymbol \\varepsilon^T \\underbrace{\\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta_0)}_{\\text{expected Fisher information} }\\boldsymbol \\varepsilon\\\\ \\end{split} \\] The Hessian is called the expected Fisher information at \\(\\boldsymbol \\theta_0\\). Since \\(\\boldsymbol \\theta_0\\) is a minimum the expected Fisher information matrix must be positive definite! With \\(h(\\boldsymbol \\theta) = \\text{E}_{\\boldsymbol \\theta_0}\\left( \\log f(\\boldsymbol x| \\boldsymbol \\theta_0) - \\log f(\\boldsymbol x| \\boldsymbol \\theta) \\right)\\) we get \\[ \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta_0) = -\\text{E}_{\\boldsymbol \\theta_0}\\left( \\nabla^T\\nabla \\log f(\\boldsymbol x| \\boldsymbol \\theta_0) \\right) \\] Thus the Fisher information at \\(\\boldsymbol \\theta_0\\) is the negative expected Hessian of the log-density. Ss there is no data involved the expected Fisher information is purely a property of the model, or more precisely of the space of the models indexed by \\(\\boldsymbol \\theta\\). In the next Chapter we will study a related quantity, the observed Fisher information that in contrast is a function of the observed data. We can use the above approximation also to compute the divergence \\(D_{\\text{KL}}(F_{\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon}, F_{\\boldsymbol \\theta_0})\\) where the first argument varies and the second is kept fixed: \\[ \\begin{split} D_{\\text{KL}}(F_{\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon}, F_{\\boldsymbol \\theta_0}) &amp;\\approx \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon)\\, \\boldsymbol \\varepsilon\\\\ \\end{split} \\] In a linear approximation \\(\\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon) \\approx \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta_0) + \\boldsymbol \\Delta_{\\boldsymbol \\varepsilon}\\) each element of the matrix \\(\\boldsymbol \\Delta_{\\boldsymbol \\varepsilon}\\) is the scalar product of \\(\\boldsymbol \\varepsilon\\) and the gradient of the corresponding element in \\(\\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta_0)\\) evaluated at \\(\\boldsymbol \\theta_0\\). Therefore \\(\\boldsymbol \\varepsilon^T \\boldsymbol \\Delta_{\\boldsymbol \\varepsilon} \\boldsymbol \\varepsilon\\) is of cubic order in \\(\\boldsymbol \\varepsilon\\) and hence \\[ \\begin{split} D_{\\text{KL}}(F_{\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon}, F_{\\boldsymbol \\theta_0}) &amp;\\approx \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon) \\boldsymbol \\varepsilon\\\\ &amp;\\approx \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta_0) \\boldsymbol \\varepsilon+ \\boldsymbol \\varepsilon^T \\boldsymbol \\Delta_{\\boldsymbol \\varepsilon} \\boldsymbol \\varepsilon\\\\ &amp;\\approx \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta_0) \\boldsymbol \\varepsilon \\end{split} \\] keeping only terms quadratic in \\(\\boldsymbol \\varepsilon\\). 2.3.2 Examples Example 2.11 Expected Fisher information for the Bernoulli distribution: The log-probability mass function of the Bernoulli \\(\\text{Ber}(p)\\) distribution is \\[ \\log f(x | p) = x \\log(p) + (1-x) \\log(1-p) \\] where \\(p\\) is the proportion of “success”. The second derivative with regard to the parameter \\(p\\) is \\[ \\frac{d^2}{dp^2} \\log f(x | p) = -\\frac{x}{p^2}- \\frac{1-x}{(1-p)^2} \\] Since \\(\\text{E}(x) = p\\) we get as Fisher information \\[ \\begin{split} I^{\\text{Fisher}}(p) &amp; = -\\text{E}\\left(\\frac{d^2}{dp^2} \\log f(x | p) \\right)\\\\ &amp;= \\frac{p}{p^2}+ \\frac{1-p}{(1-p)^2} \\\\ &amp;= \\frac{1}{p(1-p)}\\\\ \\end{split} \\] Example 2.12 Quadratic approximations of the KL divergence between two Bernoulli distributions: From Example 3.3 we have as KL divergence \\[ D_{\\text{KL}}\\left (\\text{Ber}(p_1), \\text{Ber}(p_2) \\right)=p_1 \\log\\left( \\frac{p_1}{p_2}\\right) + (1-p_1) \\log\\left(\\frac{1-p_1}{1-p_2}\\right) \\] and from Example 2.11 the corresponding expected Fisher information. The quadratic approximation implies that \\[ D_{\\text{KL}}\\left( \\text{Ber}(p), \\text{Ber}(p + \\varepsilon) \\right) \\approx \\frac{\\varepsilon^2}{2} I^{\\text{Fisher}}(p) = \\frac{\\varepsilon^2}{2 p (1-p)} \\] and also that \\[ D_{\\text{KL}}\\left( \\text{Ber}(p+\\varepsilon), \\text{Ber}(p) \\right) \\approx \\frac{\\varepsilon^2}{2} I^{\\text{Fisher}}(p) = \\frac{\\varepsilon^2}{2 p (1-p)} \\] In Worksheet 1 this is verified by using a second order Taylor series applied to the KL divergence. Example 2.13 Expected Fisher information for the normal distribution \\(N(\\mu, \\sigma^2)\\). The log-density is \\[ \\log f(x | \\mu, \\sigma^2) = -\\frac{1}{2} \\log(\\sigma^2) -\\frac{1}{2 \\sigma^2} (x-\\mu)^2 - \\frac{1}{2}\\log(2 \\pi) \\] The gradient with respect to \\(\\mu\\) and \\(\\sigma^2\\) (!) is the row vector \\[ \\nabla \\log f(x | \\mu, \\sigma^2) = \\begin{pmatrix} \\frac{1}{\\sigma^2} (x-\\mu) \\\\ - \\frac{1}{2 \\sigma^2} + \\frac{1}{2 \\sigma^4} (x- \\mu)^2 \\\\ \\end{pmatrix}^T \\] Hint for calculating the gradient: replace \\(\\sigma^2\\) by \\(v\\) and then take the partial derivative with regard to \\(v\\), then substitute back. The Hessian matrix is \\[ \\nabla^T\\nabla \\log f(x | \\mu, \\sigma^2) = \\begin{pmatrix} -\\frac{1}{\\sigma^2} &amp; -\\frac{1}{\\sigma^4} (x-\\mu)\\\\ -\\frac{1}{\\sigma^4} (x-\\mu) &amp; \\frac{1}{2\\sigma^4} - \\frac{1}{\\sigma^6}(x- \\mu)^2 \\\\ \\\\ \\end{pmatrix} \\] As \\(\\text{E}(x) = \\mu\\) we have \\(\\text{E}(x-\\mu) =0\\). Furthermore, with \\(\\text{E}( (x-\\mu)^2 ) =\\sigma^2\\) we see that \\(\\text{E}\\left(\\frac{1}{\\sigma^6}(x- \\mu)^2\\right) = \\frac{1}{\\sigma^4}\\). Therefore the expected Fisher information matrix as the negative expected Hessian matrix is \\[ \\boldsymbol I^{\\text{Fisher}}\\left(\\mu,\\sigma^2\\right) = \\begin{pmatrix} \\frac{1}{\\sigma^2} &amp; 0 \\\\ 0 &amp; \\frac{1}{2\\sigma^4} \\end{pmatrix} \\] 2.4 Entropy learning and maximum likelihood 2.4.1 The relative entropy between true model and approximating model Assume we have observations \\(x_1, \\ldots, x_n\\). The data is sampled from \\(F\\), the true but unknown data generating distribution. We also specify models \\(G_{\\boldsymbol \\theta}\\) indexed by \\(\\boldsymbol \\theta\\) to approximate \\(F\\). The relative entropy \\(D_{\\text{KL}}(F,G_{\\boldsymbol \\theta})\\) then measures the divergence of the approximation \\(G_{\\boldsymbol \\theta}\\) from the unknow true model \\(F\\). It can be written as: \\[ \\begin{split} D_{\\text{KL}}(F,G_{\\boldsymbol \\theta} &amp;= H(F,G_{\\boldsymbol \\theta}) - H(F) \\\\ &amp;= \\underbrace{- \\text{E}_{F}\\log g_{\\boldsymbol \\theta}(x)}_{\\text{cross-entropy}} -(\\underbrace{-\\text{E}_{F}\\log f(x)}_{\\text{entropy of $F$, does not depend on $\\boldsymbol \\theta$}})\\\\ \\end{split} \\] However, since we do not know \\(F\\) we cannot actually compute this divergence. Nonetheless, we may use the empirical distribution \\(\\hat{F}_n\\) — a function of the observed data — as approximation for \\(F\\), and in this way arrive at an approximation for \\(D_{\\text{KL}}(F,G_{\\boldsymbol \\theta})\\) that becomes more and more accurate with growing sample size. Recall the “Law of Large Numbers” : By the strong law of large numbers the empirical distribution \\(\\hat{F}_n\\) converges to the true underlying distribution \\(F\\) as \\(n \\rightarrow \\infty\\) almost surely: \\[ \\hat{F}_n\\overset{a. s.}{\\to} F \\] For \\(n \\rightarrow \\infty\\) the average \\(\\text{E}_{\\hat{F}_n}(h(X)) = \\frac{1}{n} \\sum_{i=1}^n h(x_i)\\) converges to the expectation \\(\\text{E}_{F}(h(X))\\). Hence, for large sample size \\(n\\) we can approximate cross-entropy and as a result the KL divergence. The cross-entropy \\(H(F, G_{\\boldsymbol \\theta})\\) is approximated by the empirical cross-entropy where the expectation is taken with regard to \\(\\hat{F}_n\\) rather than \\(F\\): \\[ \\begin{split} H(F, G_{\\boldsymbol \\theta}) &amp; \\approx H(\\hat{F}_n, G_{\\boldsymbol \\theta}) \\\\ &amp; = - \\text{E}_{\\hat{F}_n} (\\log(x)) \\\\ &amp; = -\\frac{1}{n} \\sum_{i=1}^n \\log g(x_i | \\boldsymbol \\theta) \\\\ &amp; -\\frac{1}{n} l_n ({\\boldsymbol \\theta}) \\end{split} \\] This turns out to be equal to the negative log-likelihood standardised by the sample size \\(n\\)! Or in other words, the likelihood is the negative empirical cross-entropy (times sample size \\(n\\)). From the link of the multinomial coefficient with Shannon entropy (Example 2.3) we already know that for large sample size \\[ H(\\hat{F)} \\approx \\frac{1}{n} \\log \\binom{n}{n_1, \\ldots, n_K} \\] The KL divergence \\(D_{\\text{KL}}(F,G_{\\boldsymbol \\theta})\\) can therefore be approximated by \\[ -n D_{\\text{KL}}(F,G_{\\boldsymbol \\theta}) \\approx l_n ({\\boldsymbol \\theta}) + \\log \\binom{n}{n_1, \\ldots, n_K} \\] Thus, with the KL divergence we obtain not just the log-likelihood (the cross-entropy part) but also the multiplicity factor taking account of the possible orderings of the data (the entropy part). 2.4.2 Minimum KL divergence and maximum likelihood If we were to know \\(F\\) we would simply minimise \\(D_{\\text{KL}}(F, G_{\\boldsymbol \\theta})\\) to find the particular model \\(G_{\\boldsymbol \\theta}\\) that is closest to the true model. Equivalently, we would minimise the cross-entropy \\(H(F, G_{\\boldsymbol \\theta})\\). However, since we actually don’t know \\(F\\) this is not possible. However, for large sample size \\(n\\) when the empirical distribution \\(\\hat{F}_n\\) is a good approximation for \\(F\\), we can use the results from the previous section. Thus, instead of minimising the KL divergence \\(D_{\\text{KL}}(F, G_{\\boldsymbol \\theta})\\) we simply minimise \\(H(\\hat{F}_n, G_{\\boldsymbol \\theta})\\) which is the same as maximising the likelihood \\(l_n ({\\boldsymbol \\theta})\\). Note that the entropy of the true distribution \\(F\\) (and the corresponding empirical distribution \\(\\hat{F}\\)) that does not depend on the parameters \\(\\boldsymbol \\theta\\) and hence it does not matter when minimising the divergence. Conversely, this implies that maximising the likelihood with regard to the \\(\\boldsymbol \\theta\\) is equivalent ( asymptotically for large \\(n\\)!) to minimising the KL divergence of the approximating model and the unknown true model! \\[ \\begin{split} \\hat{\\boldsymbol \\theta}^{ML} &amp;= \\underset{\\boldsymbol \\theta}{\\arg \\max}\\,\\, l_n(\\boldsymbol \\theta) \\\\ &amp;= \\underset{\\boldsymbol \\theta}{\\arg \\min}\\,\\, H(\\hat{F}_n, G_{\\boldsymbol \\theta}) \\\\ &amp;\\approx \\underset{\\boldsymbol \\theta}{\\arg \\min}\\,\\, D_{\\text{KL}}(F, G_{\\boldsymbol \\theta}) \\\\ \\end{split} \\] Therefore, the reasoning behind the method of maximum likelihood is that it minimises a large sample approximation of the KL divergence of the candidate model \\(G_{\\boldsymbol \\theta}\\) from the unkown true model \\(F\\). As a consequence of the close link of maximum likelhood and relative entropy maximum likelihood inherits for large \\(n\\) (and only then!) all the optimality properties from KL divergence. These will be discussed in more detail later in the course. 2.4.3 Further connections Since minimising KL divergence contains ML estimation as special case you may wonder whether there is a broader justification of relative entropy in the context of statistical data analysis? Indeed, KL divergence has strong geometrical interpretation that forms the basis of information geometry. In this field the manifold of distributions is studied using tools from differential geometry. The expected Fisher information plays an important role as metric tensor in the space of distributions. Furthermore, it is also linked to probabilistic forecasting. In the framework of so-called scoring rules. the only local proper scoring rule is the negative log-probability (“surprise”). The expected “surprise” is the cross-entropy and relative entropy is the corresponding natural divergence connected with the log scoring rule. Furthermore, another intriguing property of KL divergence is that the relative entropy \\(D_{\\text{KL}}(F, G)\\) is the only divergence measure that is both a Bregman and an \\(f\\)-divergence. Note that \\(f\\)-divergences and Bregman-divergences (in turn related to proper scoring rules) are two large classes of measures of similarity and divergence between two probability distributions. Finally, not only the likelihood estimation but also the Bayesian update rule (as discussed later in this module) is another special case of entropy learning. "],["03-likelihood3.html", "3 Maximum likelihood estimation 3.1 Principle of maximum likelihood estimation 3.2 Maximum likelihood estimation in practise 3.3 Observed Fisher information", " 3 Maximum likelihood estimation 3.1 Principle of maximum likelihood estimation 3.1.1 Outline The starting points in an ML analysis are the observed \\(n\\) data samples \\(x_1,\\ldots,x_n\\), iid (=independent and identically distributed), with the ordering irrelevant, and a model \\(F_{\\boldsymbol \\theta}\\) with corresponding probability density or probability mass function \\(f(x|\\boldsymbol \\theta)\\) with parameters \\(\\boldsymbol \\theta\\) From this we construct the likelihood function: \\(L_n(\\boldsymbol \\theta|x_1,\\dots,x_n)=\\prod_{i=1}^{n} f(x_i|\\boldsymbol \\theta)\\) Historically, the likelihood is also often interpreted as the probability of the data given the model. However, this is not strictly correct. First this interpretation only applies to discrete random variables. Second, since the samples are iid even in this case one would still need to add a factor accounting for the multiplicity of possible orderings of the samples to obtain the correct probability of the data. Third, the interpretation of likelihood as probability of the data completely breaks down for continuous random variables because then \\(f(x)\\) is a density, not a probability. As we have seen in the previous chapter the origin of the likelihood function lies in its connection to relative entropy. Specifically, the log-likelihood function \\(l_n(\\boldsymbol \\theta|x_1,\\dots,x_n)=\\sum_{i=1}^n \\log f(x_i|\\boldsymbol \\theta)\\) divided by sample size \\(n\\) is a large sample approximation of the cross-entropy between the unknown true data generating model and the approximating model \\(F_{\\boldsymbol \\theta}\\). Note that the log-likelihood is additive over the samples \\(x_i\\). The maximum likelihood point estimate \\(\\hat{\\boldsymbol \\theta}^{ML}\\) is then given by maximising the (log)-likelihood \\[\\hat{\\boldsymbol \\theta}^{ML} = \\text{arg max} l_n(\\boldsymbol \\theta|x_1,\\dots,x_n)\\] 3.1.2 Obtaining MLEs for a regular model In regular situations, i.e. when the log-likelihood function is smooth and twice differentiable, the second derivative is negative and not zero, and for more than one parameter the Hessian matrix is negative definite and not singular, the parameters of the model are all identifiable (in particular the model is not overparameterised), and the true parameter values lie inside the support and not on the border, then in order to maximise \\(l_n\\) one may use the score function \\(\\boldsymbol S(\\boldsymbol \\theta)\\) which is the first order derivative of the log-likelihood function: \\[\\begin{align*} \\begin{array}{cc} S_n(\\theta) = \\frac{d l_n(\\theta|x_1,\\dots,x_n)}{d \\theta}\\\\ \\\\ \\\\ \\boldsymbol S_n(\\boldsymbol \\theta)=\\nabla l_n(\\boldsymbol \\theta|x_1,\\dots,x_n)\\\\ \\\\ \\end{array} \\begin{array}{ll} \\text{scalar parameter: first derivative}\\\\ \\text{of log-likelihood function}\\\\ \\\\ \\text{gradient if } \\boldsymbol \\theta\\text{ is a vector}\\\\ \\text{(i.e. if there&#39;s more than one parameter)}\\\\ \\end{array} \\end{align*}\\] A necessary (but not sufficient) condition for the MLE is that \\[ \\boldsymbol S_n(\\hat{\\boldsymbol \\theta}_{ML}) = 0 \\] To demonstrate that the log-likelihood function actually achieves a maximum at \\(\\hat{\\boldsymbol \\theta}_{ML}\\) the curvature at the MLE must negative, i.e. that the log-likelihood must be locally concave at the MLE. In the case of a single parameter (scalar \\(\\theta\\)) this requires to check that the second derivative of the log-likelihood function is negative: \\[ \\frac{d^2 l_n(\\hat{\\theta}_{ML})}{d \\theta^2} &lt;0 \\] In the case of a parameter vector (multivariate \\(\\boldsymbol \\theta\\)) you need to compute the Hessian matrix (matrix of second order derivatives) at the MLE: \\[ \\nabla^T\\nabla l_n(\\hat{\\boldsymbol \\theta}_{ML}) \\] and then verify that this matrix is negative definite (i.e. all its eigenvalues must be negative). As we will see later the second order derivatives of the log-likelihood function also play an important role for assessing the uncertainty of the MLE. 3.1.3 Invariance property of the maximum likelihood Maximisation is a procedure that is invariant against coordinate transformations of the argument. Suppose \\(x_{\\max} = \\text{arg max } h(x)\\) and \\(y = g(x)\\) where \\(g\\) is an invertible function. Then \\(y_{\\max} = \\text{arg max } h( g^{-1}(y) ) = g(x_{\\max})\\). The achieved maximum itself remains invariant: \\(h( x_{\\max} ) = h(g^{-1}(y_{\\max} ) )\\). With regard to maximum likelihood estimation this implies the following invariance property of the maximum likelihood: Suppose that \\(\\hat{\\theta}_{ML}\\) is the MLE of \\(\\theta\\). We transform the parameter to \\(\\theta^{\\star} = g(\\theta)\\) where \\(g\\) is an invertible function. Then \\(g(\\hat{\\theta}_{ML})=\\hat{\\theta}^{\\star}\\) is the MLE of \\(\\theta^{\\star}\\). The value of the achieved maximum likelihood is the same in both cases, i.e. it is invariant against transformation of the parameters. The invariance property can be very useful in practise because it may be easier to perform the maximisation required for finding the MLE in a particular coordinate system. See Worksheet 2 for an example application of the invariance principle. 3.1.4 Consistency of maximum likelihood estimates One important property of maximum likelihood is that it produces consistent estimates. Specifically, if the true underlying model \\(F_{\\text{true}}\\) with parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) is contained in the set of specified candidates models \\(F_{\\boldsymbol \\theta}\\) \\[\\underbrace{F_{\\text{true}}}_{\\text{true model}} \\subset \\underbrace{F_{\\boldsymbol \\theta}}_{\\text{specified models}}\\] then \\[\\hat{\\boldsymbol \\theta}_{ML} \\overset{\\text{large }n}{\\longrightarrow} \\boldsymbol \\theta_{\\text{true}}\\] This is a consequence of \\(D_{\\text{KL}}(F_{\\text{true}},F_{\\boldsymbol \\theta})\\rightarrow 0\\) for \\(F_{\\boldsymbol \\theta} \\rightarrow F_{\\text{true}}\\), and that maximisation of the likelihood function is for large \\(n\\) equivalent to minimising the relative entropy. Thus given sufficient data the MLE will converge to the true value. As a consequence, MLEs are asympotically unbiased. As we will see in the examples they can still be biased in finite samples. Note that even if the candidate model \\(F_{\\boldsymbol \\theta}\\) is misspecified (i.e. it does not contain the actual true model) the MLE is still optimal in the sense in that it will find the closest possible model. It is possible to find inconsistent MLEs, but this occurs only in situations where the dimension of the model / number of parameters increases with sample size, or when the MLE is at a boundary or when there are singularities in the likelihood function. 3.2 Maximum likelihood estimation in practise 3.2.1 Worked examples In this section we now provide a number of worked example how ML estimation works in practise. Example 3.1 Estimation of a proportion: We aim to estimate the true proportion \\(p\\) in a Bernoulli experiment with binary outcomes, say the proportion of “successes” vs. “failures” or of “heads” vs. “tails” in a coin tossing experiment. Bernoulli model \\(\\text{Ber}(p)\\): \\(\\text{Pr}(\\text{&quot;success&quot;}) = p\\) and \\(\\text{Pr}(\\text{&quot;failure&quot;}) = 1-p\\). The “success” is indicated by outcome \\(x=1\\) and the “failure” by \\(x=0\\). We conduct \\(n\\) trials and record \\(n_1\\) successes and \\(n-n_1\\) failures. Parameter: \\(p\\): probability of “success”. What is the MLE of \\(p\\)? the data \\(x_1, \\ldots, x_n\\) take on values 0 or 1. the average of the data points is \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\frac{n_1}{n}\\). the probability mass function (PMF) of the Bernoulli distribution \\(\\text{Ber}(p)\\) is: \\[ f(x) = p^x (1-p)^{1-x} = \\begin{cases} p &amp; \\text{if $x=1$ }\\\\ 1-p &amp; \\text{if $x=0$} \\\\ \\end{cases} \\] log-PMF: \\[ \\log f(x) = x \\log(p) + (1-x) \\log(1 - p) \\] log-likelihood function: \\[ \\begin{split} l_n(p) &amp; = \\sum_{i=1}^n \\log f(x_i) \\\\ &amp; = n_1 \\log p + (n-n_1) \\log(1-p) \\\\ &amp; = n \\left( \\bar{x} \\log p + (1-\\bar{x}) \\log(1-p) \\right) \\\\ \\end{split} \\] Note how the log-likelihood depends on the data only through \\(\\bar{x}\\)! This is an example of a sufficient statistic for the parameter \\(p\\) (in fact it is also a minimally sufficient statistic). This will be discussed in more detail later. Score function: \\[ S_n(p)= \\frac{dl_n(p)}{dp}= n \\left( \\frac{\\bar{x}}{p}-\\frac{1-\\bar{x}}{1-p} \\right) \\] Maximum likelihood estimate: Setting \\(S_n(\\hat{p}_{ML})=0\\) yields as solution \\[ \\hat{p}_{ML} = \\bar{x} = \\frac{n_1}{n} \\] With \\(\\frac{dS_n(p)}{dp} = -n \\left( \\frac{\\bar{x}}{p^2} + \\frac{1-\\bar{x}}{(1-p)^2} \\right) &lt;0\\) the optimum corresponds indeed to the maximum of the (log-)likelihood function as this is negative for \\(\\hat{p}_{ML}\\) (and indeed for any \\(p\\)). The maximum likelihood estimator of \\(p\\) is therefore identical to the frequency of the successes among all observations. Note that to analyse the coin tossing experiment and to estimate \\(p\\) we may equally well use the Binomial distribution \\(\\text{Bin}(n, p)\\) as model for the number of successes. In this case we then have only a single observation, namely the observed \\(k\\) . This results in the same MLE for \\(p\\) but the likelihood function based on the Binomial PMF includes the Binomial coefficient \\(\\binom{n}{k}\\) . However, as this factor does not depend on \\(p\\) it disappears in the score function and has no influence in the derivation of the MLE. Example 3.2 Normal distribution with unknown mean and known variance: \\(x \\sim N(\\mu,\\sigma^2)\\) with \\(\\text{E}(x)=\\mu\\) and \\(\\text{Var}(x) = \\sigma^2\\) the parameter to be estimated is \\(\\mu\\) whereas \\(\\sigma^2\\) is known. What’s the MLE of parameter \\(\\mu\\)? the data \\(x_1, \\ldots, x_n \\in [-\\infty, \\infty]\\) are real values. the average \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) is real as well. Density: \\[ f(x)= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\] Log-Density: \\[\\log f(x) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\] Log-likelihood function: \\[ \\begin{split} l_n(\\mu) &amp;= \\sum_{i=1}^n \\log f(x_i)\\\\ &amp;=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2 \\underbrace{-\\frac{n}{2}\\log(2 \\pi \\sigma^2) }_{\\text{constant term, does not depend on } \\mu \\text{, can be removed}}\\\\ &amp;=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i^2 - 2 x_i \\mu+\\mu^2) + C\\\\ &amp;=\\frac{n}{\\sigma^2} ( \\bar{x} \\mu - \\frac{1}{2}\\mu^2) \\underbrace{ - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2 }_{\\text{another constant term}} + C\\\\ \\end{split} \\] Note how the non-constant terms of the log-likelihood depend on the data only through \\(\\bar{x}\\)! Score function: \\[ S_n(\\mu) = \\frac{n}{\\sigma^2} ( \\bar{x}- \\mu) \\] Maximum likelihood estimate: \\[S_n(\\hat{\\mu}_{ML})=0 \\Rightarrow \\hat{\\mu}_{ML} = \\bar{x}\\] With \\(\\frac{dS_n(\\mu)}{d\\mu} = -\\frac{n}{\\sigma^2}&lt;0\\) the optimum is indeed the maximum The constant term \\(C\\) in the log-likelihood function collects all terms that do not depend on the parameter. After taking the first derivative with regard to the parameter this term disappears thus \\(C\\) is not relevant for finding the MLE of the parameter. In the future we will often omit such constant terms from the log-likelihood function without further mention. Example 3.3 Normal distribution with mean and variance both unknown: \\(x \\sim N(\\mu,\\sigma^2)\\) with \\(\\text{E}(x)=\\mu\\) and \\(\\text{Var}(x) = \\sigma^2\\) both \\(\\mu\\) and \\(\\sigma^2\\) need to be estimated. What’s the MLE of the parameter vector \\(\\boldsymbol \\theta= (\\mu,\\sigma^2)^T\\)? the data \\(x_1, \\ldots, x_n \\in [-\\infty, \\infty]\\) are real values. the average \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) is real as well. the average of the squared data \\(\\overline{x^2} = \\frac{1}{n} \\sum_{i=1}^n x_i^2 \\geq 0\\) is non-negative. Density: \\[ f(x)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\] Log-Density: \\[\\log f(x) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\] Log-likelihood function: \\[ \\begin{split} l_n(\\boldsymbol \\theta) &amp; = \\sum_{i=1}^n \\log f(x_i)\\\\ &amp;= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2 \\underbrace{-\\frac{n}{2} \\log(2 \\pi) }_{\\text{constant not depending on }\\mu \\text{ or } \\sigma^2}\\\\ &amp;= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{n}{2\\sigma^2} ( \\overline{x^2} -2 \\bar{x} \\mu + \\mu^2) + C\\\\ \\end{split} \\] Note how the log-likelihood function depends on the data only through \\(\\bar{x}\\) and \\(\\overline{x^2}\\)! Score function \\(\\boldsymbol S\\) (row vector!), gradient of \\(l_n(\\boldsymbol \\theta)\\): \\[ \\begin{split} \\boldsymbol S(\\boldsymbol \\theta) &amp;= \\nabla l_n(\\boldsymbol \\theta) \\\\ &amp;= \\begin{pmatrix} \\frac{n}{\\sigma^2} (\\bar{x}-\\mu) \\\\ -\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4} \\left( \\overline{x^2} - 2\\bar{x} \\mu +\\mu^2 \\right) \\\\ \\end{pmatrix}^T\\\\ \\end{split} \\] Note that to obtain the second component of the score function the partial derivative needs to be taken with regard to the variance parameter \\(\\sigma^2\\) — not with regard to \\(\\sigma\\)! Hint: replace \\(\\sigma^2 = v\\) in the log-likelihood function, then take the partial derivative with regard to \\(v\\), then backsubstitute \\(v=\\sigma^2\\) in the result. Maximum likelihood estimate: \\[ \\boldsymbol S(\\hat{\\boldsymbol \\theta}_{ML})=0 \\Rightarrow \\] \\[ \\hat{\\boldsymbol \\theta}_{ML}= \\begin{pmatrix} \\hat{\\mu}_{ML} \\\\ \\widehat{\\sigma^2}_{ML} \\\\ \\end{pmatrix} = \\begin{pmatrix} \\bar{x} \\\\ \\overline{x^2} -\\bar{x}^2\\\\ \\end{pmatrix} \\] The ML estimate of the variance we can also write \\(\\widehat{\\sigma^2}_{ML} = \\overline{x^2} -\\bar{x}^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})^2\\). To confirm that we actually have maximum we need to verify that the eigenvalues of the Hessian matrix are all negative. This is indeed the case, for details see Example 3.6. 3.2.2 Relationship with least squares estimation In Example 3.2 the form of the log-likelihood function is a function of the sum of squared differences. Maximising \\(l_n(\\mu) =-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\) is equivalent to minimising \\(\\sum_{i=1}^n(x_i-\\mu)^2\\). Hence, finding the mean by maximum likelihood assuming a normal model is equivalent to least-squares estimation! Note that least-squares estimation has been in use at least since the early 1800s and thus predates maximum likelihood (1924). Due to its simplicity it is still very popular in particular in regression and the link with maximum likelihood and normality allows to understand why it usually works well! 3.2.3 Bias and maximum likelihood Example 3.3 is interesting because it shows that maximum likelihood can result in both biased and as well as unbiased estimators. Recall that \\(x \\sim N(\\mu, \\sigma^2)\\). As a result \\[\\hat{\\mu}_{ML}=\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\] with \\(\\text{E}( \\hat{\\mu}_{ML} ) = \\mu\\) and \\[\\widehat{\\sigma^2}_{ML} \\sim \\frac{\\sigma^2}{n} \\chi^2_{n-1}\\] with \\(\\text{E}(\\widehat{\\sigma^2}_{ML}) = \\frac{n-1}{n} \\, \\sigma^2\\). Therefore, the MLE of \\(\\mu\\) is unbiased as \\[ \\text{Bias}(\\hat{\\mu}_{ML}) = \\text{E}( \\hat{\\mu}_{ML} ) - \\mu = 0 \\] In contrast, however, the MLE of \\(\\sigma^2\\) is negatively biased because \\[ \\text{Bias}(\\widehat{\\sigma^2}_{ML}) = \\text{E}( \\widehat{\\sigma^2}_{ML} ) - \\sigma^2 = -\\frac{1}{n} \\, \\sigma^2 \\] Thus, in the case of the variance parameter of the normal distribution the MLE is not recovering the well-known unbiased estimator of the variance \\[ \\widehat{\\sigma^2}_{UB} = \\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2 = \\frac{n}{n-1} \\widehat{\\sigma^2}_{ML} \\] Conversely, the unbiased estimator is not a maximum likelihood estimate! Therefore it is worth keeping in mind that maximum likelihood can result in biased estimates for finite \\(n\\). For large \\(n\\), however, the bias disappears as MLEs are consistent. 3.3 Observed Fisher information 3.3.1 Motivation and definition By inspection of some log-likelihood curves it is apparent that the log-likelihood function contains more information about the parameter \\(\\boldsymbol \\theta\\) than just the maximum point \\(\\hat{\\boldsymbol \\theta}_{ML}\\). In particular the curvature of the log-likelihood function at the MLE must be somehow related the accuracy of \\(\\hat{\\boldsymbol \\theta}_{ML}\\): if the likelihood surface is flat near the maximum (low curvature) then if is more difficult to find the optimal parameter (also numerically!). Conversely, if the likelihood surface is peaked (strong curvature) then the maximum point is clearly defined. The curvature is described by the second-order derivatives (Hessian matrix) of the log-likelihood function. For univariate \\(\\theta\\) the Hessian is a scalar: \\[\\frac{d^2 l_n(\\theta)}{d\\theta^2}\\] For multivariate parameter vector \\(\\boldsymbol \\theta\\) of dimension \\(d\\) the Hessian is a matrix of size \\(d \\times d\\): \\[\\nabla^T\\nabla l_n(\\boldsymbol \\theta)\\] By construction the Hessian is negative definite at the MLE (i.e. its eigenvalues are all negative) to ensure the the function is concave at the MLE (i.e. peak shaped). The observed Fisher information (matrix) is defined as the negative curvature at the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\): \\[{\\boldsymbol J_n}(\\hat{\\boldsymbol \\theta}_{ML}) = -\\nabla^T\\nabla l_n(\\hat{\\boldsymbol \\theta}_{ML})\\] Sometimes this is simply called the “observed information”. To avoid confusion with the expected Fisher information introduced earlier \\[ \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta) = -\\text{E}_{F_{\\boldsymbol \\theta}} \\left( \\nabla^T\\nabla\\log f(x|\\boldsymbol \\theta)\\right) \\] it is necessary to always use the qualifier “observed” when referring to \\({\\boldsymbol J_n}(\\hat{\\boldsymbol \\theta}_{ML})\\). 3.3.2 Examples of observed Fisher information Example 3.4 Bernoulli model \\(\\text{Ber}(p)\\): We continue Example 3.1. Recall that \\(\\hat{p}_{ML} = \\bar{x}=\\frac{n_1}{n}\\) and the score function \\(S_n(p)=n \\left( \\frac{\\bar{x} }{p} - \\frac{1-\\bar{x}}{1-p} \\right)\\). The negative second derivative of the log-likelihood function is \\[-\\frac{d S_n(p)}{dp}=n \\left( \\frac{ \\bar{x} }{p^2} + \\frac{1 - \\bar{x} }{(1-p)^2} \\right) \\] The observed Fisher information is therefore \\[ \\begin{split} J_n(\\hat{p}_{ML}) &amp; = n \\left(\\frac{ \\bar{x} }{\\hat{p}_{ML}^2} + \\frac{ 1 - \\bar{x} }{ (1-\\hat{p}_{ML})^2 } \\right) \\\\ &amp; = n \\left(\\frac{1}{\\hat{p}_{ML}} + \\frac{1}{1-\\hat{p}_{ML}} \\right) \\\\ &amp;= \\frac{n}{\\hat{p}_{ML} (1-\\hat{p}_{ML})} \\\\ \\end{split} \\] The inverse of the observed Fisher information is: \\[J_n(\\hat{p}_{ML})^{-1}=\\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\\] Compare this with \\(\\text{Var}\\left(\\frac{x}{n}\\right) = \\frac{p(1-p)}{n}\\) for \\(x \\sim \\text{Bin}(n, p)\\). Example 3.5 Normal distribution with unknown mean and known variance: This is the continuation of Example 3.2. Recall the MLE for the mean \\(\\hat{\\mu}_{ML}=\\frac{1}{n}\\sum_{i=1}^n x_i=\\bar{x}\\) and the score function \\(\\boldsymbol S_n(\\mu) = \\frac{n}{\\sigma^2} (\\bar{x} -\\mu)\\). The negative second derivative of the score function is \\[ -\\frac{d S_n(\\mu)}{d\\mu}= \\frac{n}{\\sigma^2} \\] The observed Fisher information at the MLE is therefore \\[ J_n(\\hat{\\mu}_{ML}) = \\frac{n}{\\sigma^2} \\] and the inverse of the observed Fisher information is \\[ J_n(\\hat{\\nu}_{ML})^{-1} = \\frac{\\sigma^2}{n} \\] For \\(x_i \\sim N(\\mu, \\sigma^2)\\) we have \\(\\text{Var}(x_i) = \\sigma^2\\) and hence \\(\\text{Var}(\\bar{x}) = \\frac{\\sigma^2}{n}\\), which is equal to the inverse observed Fisher information. Example 3.6 Normal distribution with mean and variance parameter: This is the continuation of Example 3.3. Recall the MLE for the mean and variance: \\[\\hat{\\mu}_{ML}=\\frac{1}{n}\\sum_{i=1}^n x_i=\\bar{x}\\] \\[\\widehat{\\sigma^2}_{ML} = \\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar{x})^2 = \\overline{x^2} - \\bar{x}^2\\] with score function \\[\\boldsymbol S_n(\\mu,\\sigma^2)=\\nabla l_n(\\mu, \\sigma^2) = \\begin{pmatrix} \\frac{n}{\\sigma^2} (\\bar{x}-\\mu) \\\\ -\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4} \\left(\\overline{x^2} - 2 \\mu \\bar{x} + \\mu^2\\right) \\\\ \\end{pmatrix}^T \\] The Hessian matrix of the log-likelihood function is \\[\\nabla^T\\nabla l_n(\\mu,\\sigma^2) = \\begin{pmatrix} - \\frac{n}{\\sigma^2}&amp; -\\frac{n}{\\sigma^4} (\\bar{x} -\\mu)\\\\ - \\frac{n}{\\sigma^4} (\\bar{x} -\\mu) &amp; \\frac{n}{2\\sigma^4}-\\frac{n}{\\sigma^6} \\left(\\overline{x^2} - 2 \\mu \\bar{x} + \\mu^2\\right) \\\\ \\end{pmatrix} \\] The negative Hessian at the MLE, i.e. at \\(\\hat{\\mu}_{ML} = \\bar{x}\\) and \\(\\widehat{\\sigma^2}_{ML} = \\overline{x^2} -\\bar{x}^2\\) yields the observed Fisher information matrix: \\[ \\boldsymbol J_n(\\hat{\\mu}_{ML},\\widehat{\\sigma^2}_{ML}) = \\begin{pmatrix} \\frac{n}{\\widehat{\\sigma^2}_{ML}}&amp;0 \\\\ 0 &amp; \\frac{n}{2(\\widehat{\\sigma^2}_{ML})^2} \\end{pmatrix} \\] Note that the observed Fisher information matrix is diagonal with positive entries. Therefore its eigenvalues are all positive as required for a maximum, because for a diagonal matrix the eigenvalues are simply the the entries on the diagonal. The inverse of the observed Fisher information matrix is \\[ \\boldsymbol J_n(\\hat{\\mu}_{ML},\\widehat{\\sigma^2}_{ML})^{-1} = \\begin{pmatrix} \\frac{\\widehat{\\sigma^2}_{ML}}{n}&amp; 0\\\\ 0 &amp; \\frac{2(\\widehat{\\sigma^2}_{ML})^2}{n} \\end{pmatrix} \\] Recall that \\(x \\sim N(\\mu, \\sigma^2)\\) and therefore \\[\\hat{\\mu}_{ML}=\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\] Hence \\(\\text{Var}(\\hat{\\mu}_{ML}) = \\frac{\\sigma^2}{n}\\). If you compare this with the first diagonal entry of the inverse observed Fisher information matrix you see that this is essentially the same expression (apart from the “hat”). The empirical variance \\(\\widehat{\\sigma^2}_{ML}\\) follows a scaled chi-squared distribution \\[\\widehat{\\sigma^2}_{ML} \\sim \\frac{\\sigma^2}{n} \\chi^2_{n-1}\\] with variance \\(\\text{Var}(\\widehat{\\sigma^2}_{ML}) = \\frac{n-1}{n} \\, \\frac{2 \\sigma ^4}{n}\\). For large \\(n\\) this becomes \\(\\text{Var}(\\widehat{\\sigma^2}_{ML})\\overset{a}{=} \\frac{2 \\sigma ^4}{n}\\) which is essentially (apart from the “hat”) the second diagonal entry of the inverse observed Fisher information matrix. 3.3.3 Relationship between observed and expected Fisher information The observed Fisher information \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) and the expected Fisher information \\(\\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)\\) are related but also two clearly different entities: Both types of Fisher information are based on computing the second order derivative (Hessian matrix), thus are based on the curvature of a function. The observed Fisher information is computed from the log-likelihood function. Therefore it takes the observed data into account. It explicitly depends on the sample size \\(n\\). It contains estimates of the parameters but not the parameters themselves. While the curvature of the log-likelihood function may be computed for any point the the observed Fisher information specifically refers to the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\). It is linked to the (asymptotic) variance of the MLE as we have seen in the examples and will discuss in more detail later. In contrast, the expected Fisher information is derived directly from the log-density. It does not depend on the observed data, and thus does not have dependency on sample size. It can be computed for any value of the parameters. It describes the geometry of the space of the models, and is the local approximation of relative entropy. Asympotically, for large sample size \\(n\\) the MLE converges to \\(\\hat{\\boldsymbol \\theta}_{ML} \\rightarrow \\boldsymbol \\theta_0\\). It follows from the construction of the observed Fisher information and the law of large numbers that correspondingly \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\rightarrow n \\boldsymbol I^{\\text{Fisher}}( \\boldsymbol \\theta_0 )\\). In a very important class of models, namely in the exponential family, we find that \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) = n \\boldsymbol I^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) also for finite sample size \\(n\\). This is in fact the case in all the examples discussed above (e.g. see Examples 2.11 and 3.4 for the Bernoulli and Examples 2.13 and 3.6 for the normal distribution). However, this is an exception. In a general model \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\neq n \\boldsymbol I^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) for finite sample size \\(n\\). An example is provided by the Cauchy distribution with median parameter \\(\\theta\\). It is not part of the exponential family and has expected Fisher information \\(I^{\\text{Fisher}}(\\theta )=\\frac{1}{2}\\) regardless of the choice the median parameter whereas the observed Fisher information \\(J_n(\\hat{\\theta}_{ML})\\) depends on the MLE \\(\\hat{\\theta}_{ML}\\) of the median parameter and is not simply \\(\\frac{n}{2}\\). "],["04-likelihood4.html", "4 Quadratic approximation and normal asymptotics 4.1 Multivariate statistics for random vectors 4.2 Approximate distribution of maximum likelihood estimates 4.3 Quantifying the uncertainty of maximum likelihood estimates 4.4 Example of a non-regular model", " 4 Quadratic approximation and normal asymptotics 4.1 Multivariate statistics for random vectors 4.1.1 Covariance and correlation Assume a random variable \\(x\\) with mean \\(\\text{E}(x) = \\mu\\). The corresponding variance is given by \\[ \\begin{split} \\text{Var}(x) &amp; = \\text{E}\\left((x-\\mu)^2 \\right) \\\\ &amp; =\\text{E}\\left( (x-\\mu)(x-\\mu) \\right) \\\\ &amp; = \\text{E}(x^2)-\\mu^2 \\\\ \\end{split} \\] For a random vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T\\) the mean \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\) is simply comprised of the means of its components, i.e. \\(\\boldsymbol \\mu= (\\mu_1, \\ldots, \\mu_d)^T\\). Thus, the mean of a random vector of dimension is a vector of of the same length. The variance of a random vector of length \\(d\\), however, is not a vector but a matrix of size \\(d\\times d\\). This matrix is called the covariance matrix: \\[ \\begin{split} \\text{Var}(\\boldsymbol x) &amp;= \\underbrace{\\boldsymbol \\Sigma}_{d\\times d} = (\\sigma_{ij}) = \\begin{pmatrix} \\sigma_{11} &amp; \\dots &amp; \\sigma_{1d}\\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{d1} &amp; \\dots &amp; \\sigma_{dd} \\end{pmatrix} \\\\ &amp;=\\text{E}\\left(\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d\\times 1} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1\\times d}\\right) \\\\ &amp; = \\text{E}(\\boldsymbol x\\boldsymbol x^T)-\\boldsymbol \\mu\\boldsymbol \\mu^T \\\\ \\end{split} \\] The entries of the covariance matrix \\(\\sigma_{ij} =\\text{Cov}(x_i, x_j)\\) describe the covariance between the random variables \\(x_i\\) and \\(x_j\\). The covariance matrix is symmetric, hence \\(\\sigma_{ij}=\\sigma_{ji}\\). The diagonal entries \\(\\sigma_{ii} = \\text{Cov}(x_i, x_i) = \\text{Var}(x_i) = \\sigma_i^2\\) correspond to the variances of the components of \\(\\boldsymbol x\\). The covariance matrix is positive semi-definite, i.e. the eigenvalues of \\(\\boldsymbol \\Sigma\\) are all positive or equal to zero. However, in practise one aims to use non-singular covariance matrices, with all eigenvalues positive, so that they are invertible. A covariance matrix can be factorised into the product \\[\\boldsymbol \\Sigma= \\boldsymbol V^{\\frac{1}{2}} \\boldsymbol P\\boldsymbol V^{\\frac{1}{2}}\\] where \\(\\boldsymbol V\\) is a diagonal matrix containing the variances \\[ \\boldsymbol V= \\begin{pmatrix} \\sigma_{11} &amp; \\dots &amp; 0\\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\dots &amp; \\sigma_{dd} \\end{pmatrix}\\] and the matrix \\(\\boldsymbol P\\) (“capital rho”) is the symmetric correlation matrix \\[ \\boldsymbol P= (\\rho_{ij}) = \\begin{pmatrix} 1 &amp; \\dots &amp; \\rho_{1d}\\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\rho_{d1} &amp; \\dots &amp; 1 \\end{pmatrix} = \\boldsymbol V^{-\\frac{1}{2}} \\boldsymbol \\Sigma\\boldsymbol V^{-\\frac{1}{2}}\\] Thus, the correlation between \\(x_i\\) and \\(x_j\\) is defined as \\[\\rho_{ij} = \\text{Cor}(x_i,x_j) = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}\\] For univariate \\(x\\) and scalar constant \\(a\\) the variance of \\(a x\\) equals \\(\\text{Var}(a x) = a^2 \\text{Var}(x)\\). For a random vector \\(\\boldsymbol x\\) of dimension \\(d\\) and matrix \\(\\boldsymbol A\\) of dimension \\(m \\times d\\) this generalises to \\(\\text{Var}(\\boldsymbol Ax) = \\boldsymbol A\\text{Var}(\\boldsymbol x) \\boldsymbol A^T\\). 4.1.2 Multivariate normal distribution The density of a normally distributed scalar variable \\(x \\sim N(\\mu, \\sigma^2)\\) with mean \\(\\text{E}(x) = \\mu\\) and variance \\(\\text{Var}(x) = \\sigma^2\\) is \\[ f(x |\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right) \\] The univariate normal distribution for a scalar \\(x\\) generalises to the* *multivariate normal distribution** for a vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) with with mean \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\) and covariance matrix \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\). The corresponding density is \\[f(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) = (2\\pi)^{-\\frac{d}{2}} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}} \\exp\\left({{-\\frac{1}{2}} \\underbrace{\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1 \\times d} \\underbrace{\\boldsymbol \\Sigma^{-1}}_{d \\times d} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d \\times 1} }_{1 \\times 1 = \\text{scalar!}}}\\right)\\] For \\(d=1\\) we have \\(bx=x\\), \\(\\boldsymbol \\mu= \\mu\\) and \\(\\boldsymbol \\Sigma= \\sigma^2\\) so that the multivariate normal density reduces to the univariate normal density. Example 4.1 Maximum likelihood estimates of the parameters of the multivariate normal distribution: Maximising the log-likelihood based on the multivariate normal density yields the MLEs for \\(\\boldsymbol \\mu\\) and \\(\\boldsymbol \\Sigma\\). These are generalisations of the MLEs for the mean \\(\\mu\\) and variance \\(\\sigma^2\\) of the univariate normal as encountered in Example 3.3. The estimates can be written in three different ways: a) data vector notation with \\(\\boldsymbol x_1,\\ldots, \\boldsymbol x_n\\) the \\(n\\) vector-valued observations from the multivariate normal: MLE for the mean: \\[ \\hat{\\boldsymbol \\mu}_{ML} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k = \\bar{\\boldsymbol x} \\] MLE for the covariance: \\[\\underbrace{\\widehat{\\boldsymbol \\Sigma}_{ML}}_{d \\times d} = \\frac{1}{n}\\sum^{n}_{k=1} \\underbrace{\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)}_{d \\times 1} \\; \\underbrace{\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)^T}_{1 \\times d}\\] Note the factor \\(\\frac{1}{n}\\) in the estimator of the covariance matrix. With \\(\\overline{\\boldsymbol x\\boldsymbol x^T} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k \\boldsymbol x_k^T\\) we can also write \\[ \\widehat{\\boldsymbol \\Sigma}_{ML} = \\overline{\\boldsymbol x\\boldsymbol x^T} - \\bar{\\boldsymbol x} \\bar{\\boldsymbol x}^T \\] b) data component notation with \\(x_{ki}\\) the \\(i\\)-th component of the \\(k\\)-th sample \\(\\boldsymbol x_k\\): \\[\\hat{\\mu}_i = \\frac{1}{n}\\sum^{n}_{k=1} x_{ki} \\text{ with } \\hat{\\boldsymbol \\mu}=\\begin{pmatrix} \\hat{\\mu}_{1} \\\\ \\vdots \\\\ \\hat{\\mu}_{d} \\end{pmatrix}\\] \\[\\hat{\\sigma}_{ij} = \\frac{1}{n}\\sum^{n}_{k=1} \\left(x_{ki}-\\hat{\\mu}_i\\right) (\\ x_{kj}-\\hat{\\mu}_j) \\text{ with } \\widehat{\\boldsymbol \\Sigma} = (\\hat{\\sigma}_{ij}) \\] c) data matrix notation with \\(\\boldsymbol X= \\begin{pmatrix} \\boldsymbol x_1^T \\\\ ... \\\\ \\boldsymbol x_n^T \\\\\\end{pmatrix}\\) as a data matrix containing the samples in its rows. Note that this is the statistics convention — in much of the engineering and computer science literature the data matrix is often transposed and samples are stored in the columns. Thus, the formulas below are only correct assuming the statistics convention. \\[ \\hat{\\boldsymbol \\mu} = \\frac{1}{n} \\boldsymbol X^T \\boldsymbol 1_n \\] Here \\(\\boldsymbol 1_n\\) is a vector of length \\(n\\) containing 1 at each component. \\[ \\hat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\boldsymbol X^T \\boldsymbol X- \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T \\] To simplify the expression for the estimate of the covariance matrix one often assumes that the data matrix is centered, i.e. that \\(\\hat{\\boldsymbol \\mu} = 0\\). Because of the ambiguity in convention (machine learning vs statistics convention) and the often implicit use of centered data matrices the matrix notation is often confusing. Hence, using the other two notations is generally preferable. 4.2 Approximate distribution of maximum likelihood estimates 4.2.1 Quadratic log-likelihood resulting from normal model Assume we observe a single sample \\(\\boldsymbol x\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma^2)\\) with known covariance. The corresponding log-likelihood for \\(\\boldsymbol \\mu\\) is \\[ l_1(\\boldsymbol \\mu) = C - \\frac{1}{2}(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu) \\] where \\(C\\) is a constant that does not depend on \\(\\boldsymbol \\mu\\). Note that the log-likelihood is exactly quadratic and the maximum lies at \\((\\boldsymbol x, C)\\). 4.2.2 Quadratic approximation of a log-likelihood function Now consider the quadratic approximation of the log-likelihood function \\(l_n(\\boldsymbol \\theta)\\) for a general model around the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\). We assume the model is regular with \\(\\nabla l_n(\\hat{\\boldsymbol \\theta}_{ML} ) = 0\\). The Taylor series approximation of scalar-valued function \\(f(\\boldsymbol x)\\) around \\(\\boldsymbol x_0\\) is \\[ f(\\boldsymbol x) = f(\\boldsymbol x_0) + \\nabla f(\\boldsymbol x_0) (\\boldsymbol x-\\boldsymbol x_0) + \\frac{1}{2} (\\boldsymbol x-\\boldsymbol x_0)^T \\nabla^T \\nabla f(\\boldsymbol x_0) (\\boldsymbol x-\\boldsymbol x_0) + \\ldots \\] Applied to the log-likelihood function this yields \\[l_n(\\boldsymbol \\theta) \\approx l_n(\\hat{\\boldsymbol \\theta}_{ML})- \\frac{1}{2}(\\hat{\\boldsymbol \\theta}_{ML}- \\boldsymbol \\theta)^T J_n(\\hat{\\boldsymbol \\theta}_{ML})(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta)\\] This is a quadratic function with maximum at \\(( \\hat{\\boldsymbol \\theta}_{ML} , l_n(\\hat{\\boldsymbol \\theta}_{ML})\\). Note the natural appearance of the observed Fisher information \\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) in the quadratic term. There is no linear term because of the vanishing gradient at the MLE. Crucially, we realise that the approximation has the same form as if \\(\\hat{\\boldsymbol \\theta}_{ML}\\) was a sample from a multivariate normal distribution with mean \\(\\boldsymbol \\theta\\) and with covariance given by the inverse observed Fisher information! Note that this requires a positive definite observed Fisher information matrix so that \\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) is actually invertible! Example 4.2 Quadratic approximation of the log-likelihood for a proportion: From Example 3.1 we have the log-likelihood \\[ l_n(p) = n \\left( \\bar{x} \\log p + (1-\\bar{x}) \\log(1-p) \\right) \\] and the MLE \\[ \\hat{p}_{ML} = \\bar{x} \\] and from Example 3.4 the observed Fisher information \\[ \\begin{split} J_n(\\hat{p}_{ML}) = \\frac{n}{\\bar{x} (1-\\bar{x})} \\end{split} \\] The log-likelihood at the MLE is \\[ l_n(\\hat{p}_{ML}) = n \\left( \\bar{x} \\log \\bar{x} + (1-\\bar{x}) \\log(1-\\bar{x}) \\right) \\] This allows us to construct the quadratic approximation of the log-likelihood around the MLE as \\[ \\begin{split} l_n(p) &amp; \\approx l_n(\\hat{p}_{ML}) - \\frac{1}{2} J_n(\\hat{p}_{ML}) (p-\\hat{p}_{ML})^2 \\\\ &amp;= n \\left( \\bar{x} \\log \\bar{x} + (1-\\bar{x}) \\log(1-\\bar{x}) - \\frac{(p-\\bar{x})^2}{2 \\bar{x} (1-\\bar{x})} \\right) \\\\ &amp;= C + \\frac{ \\bar{x} p -\\frac{1}{2} p^2}{ \\bar{x} (1-\\bar{x})/n} \\\\ \\end{split} \\] The constant \\(C\\) does not depend on \\(p\\), its only purpose is to match the approximate log-likelihood at the MLE with that of the corresponding original log-likelihood. The approximate log-likelihood takes on the form of a normal log-likelihood (Example 3.2) for one observation of \\(\\hat{p}_{ML}=\\bar{x}\\) from \\(N\\left(p, \\frac{\\bar{x} (1-\\bar{x})}{n} \\right)\\). The following figure shows the above log-likelihood function and its quadratic approximation for example data with \\(n = 30\\) and \\(\\bar{x} = 0.7\\): 4.2.3 Asymptotic normality of maximum likelihood estimates Intuitively, it makes sense to associate large amount of curvature of the log-likelihood at the MLE with low variance of the MLE (and conversely, low amount of curvature with high variance). From the above we see that normality implies a quadratic log-likelihood, conversely, taking an quadratic approximation of the log-likelihood implies approximate normality, and in the quadratic approximation the inverse observed Fisher information plays the role of the covariance of the MLE. This suggests the following theorem: Asymptotically, the MLE is normally distributed around the true parameter and with covariance equal to the inverse of the observed Fisher information: \\[\\hat{\\boldsymbol \\theta}_{ML} \\overset{a}{\\sim}\\underbrace{N_d}_{\\text{multivariate normal}}\\left(\\underbrace{\\boldsymbol \\theta}_{\\text{mean vector}},\\underbrace{\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1}}_{\\text{ covariance matrix}}\\right)\\] This theorem about the distributional properties of MLEs greatly enhances the usefulness of the method of maximum likelihood. It implies that in regular settings maximum likelihood is not just a method for obtaining point estimates but also also provides estimates of their uncertainty. However, we need to clarify what “asymptotic” actually means in the context of the above theorem: Primarily, it means to have suffient sample size so that the log-likelihood \\(l_n(\\boldsymbol \\theta)\\) is sufficiently well approximated by a quadratic function around \\(\\hat{\\boldsymbol \\theta}_{ML}\\). The better the local quadratic approximation the better the normal approximation! In a regular model with positive definite observed Fisher information matrix this is guaranteed for large sample size \\(n \\rightarrow \\infty\\) thanks to the central limit theorem). However, \\(n\\) going to infinity is in fact not always required for the normal approximation to hold! Depending on the particular model a good local fit to a quadratic log-likelihood may be available also for finite \\(n\\). As a trivial example, for the normal log-likelihood it is valid for any \\(n\\). In the other hand, in non-regular models (with nondifferentiable log-likelihood at the MLE and/or a singular Fisher information matrix) no amount of data, not even \\(n\\rightarrow \\infty\\), will make the quadratic approximation work. Remarks: The technical details of the above considerations are worked out in the theory of locally asymptotically normal (LAN) models pioneered in 1960 by Lucien LeCam (1924–2000). There are also methods to obtain higher-order (higher than quadratic and thus non-normal) asymptotic approximations. These relate to so-called saddle point approximations. 4.2.4 Asymptotic optimal efficiency Assume now that \\(\\hat{\\boldsymbol \\theta}\\) is an arbitrary and unbiased estimator for \\(\\boldsymbol \\theta\\) and the underlying data generating model is regular with density \\(f(\\boldsymbol x| \\boldsymbol \\theta)\\). H. Cramér (1893–1985), C. R. Rao (1920–) and others demonstrated in 1945 the so-called information inequality, \\[ \\text{Var}(\\hat{\\boldsymbol \\theta}) \\geq \\frac{1}{n} \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)^{-1} \\] which puts a lower bound on the variance of an estimator for \\(\\boldsymbol \\theta\\). (Note for \\(d&gt;1\\) this is a matrix inequality, meaning that the difference matrix is positive semidefinite). For large sample size with \\(n \\rightarrow \\infty\\) and \\(\\hat{\\boldsymbol \\theta}_{ML} \\rightarrow \\boldsymbol \\theta\\) the observed Fisher information becomes \\(J_n(\\hat{\\boldsymbol \\theta}) \\rightarrow n \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)\\) and therefore we can write the asymptotic distribution of \\(\\hat{\\boldsymbol \\theta}_{ML}\\) as \\[ \\hat{\\boldsymbol \\theta}_{ML} \\overset{a}{\\sim} N_d\\left( \\boldsymbol \\theta, \\frac{1}{n} \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)^{-1} \\right) \\] This means that for large \\(n\\) in regular models \\(\\hat{\\boldsymbol \\theta}_{ML}\\) achieves the lowest variance possible according to the Cramér-Rao information inequality. In other words, for large sample size maximum likelihood is optimally efficient and thus the best available estimator will in fact be the MLE! However, as we will see later this does not hold for small sample size where it is indeed possible (and necessary) to improve over the MLE (e.g. via Bayesian estimation or regularisation). 4.3 Quantifying the uncertainty of maximum likelihood estimates 4.3.1 Estimating the variance of MLEs In the previous section we saw that MLEs are asymptotically normally distributed, with the inverse Fisher information (both expected and observed) linked to the asymptotic variance. This leads to the question whether to use the observed Fisher information \\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) or the expected Fisher information at the MLE \\(n \\boldsymbol I^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) to estimate the variance of the MLE? Clearly, for \\(n\\rightarrow \\infty\\) both can be used interchangeably. However, they can be very different for finite \\(n\\) in particular for models outside the exponential family. Also normality may occur well before \\(n\\) goes to \\(\\infty\\). Therefore one needs to choose between the two, considering also that the expected Fisher information at the MLE is the average curvature at the MLE, whereas the observed Fisher information is the actual observed curvature, and the observed Fisher information naturally occurs in the quadratic approximation of the log-likelihood. All in all, the observed Fisher information as estimator of the variance is more appropriate as it is based on the actual observed data and also works for large \\(n\\) (in which case it yields the same result as using expected Fisher information): \\[ \\widehat{\\text{Var}}(\\hat{\\boldsymbol \\theta}_{ML}) = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1} \\] and its square-root as the estimate of the standard deviation \\[ \\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML}) = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1/2} \\] Note that in the above we use matrix inversion and the (inverse) matrix square root. The reasons for preferring observed Fisher information are made mathematically precise in a classic paper by Efron and Hinkley (1978). Example 4.3 Estimated variance and distribution of the MLE of a proportion: From Examples 3.1 and 3.4 we know the MLE \\[ \\hat{p}_{ML} = \\bar{x} = \\frac{k}{n} \\] and the corresponding observed Fisher information \\[ J_n(\\hat{p}_{ML})=\\frac{n}{\\hat{p}_{ML}(1-\\hat{p}_{ML})} \\] The estimated variance of the MLE is therefore \\[ \\widehat{\\text{Var}}( \\hat{p}_{ML} ) = \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n} \\] and the corresponding asymptotic normal distribution is \\[ \\hat{p}_{ML} \\overset{a}{\\sim} N\\left(p, \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n} \\right) \\] Example 4.4 Estimated variance and distribution of the MLE of the mean parameter for the normal distribution with known variance: From Examples 3.2 and 3.5 we know that \\[\\hat{\\mu}_{ML} =\\bar{x}\\] and that the corresponding observed Fisher information at \\(\\hat{\\mu}_{ML}\\) is \\[J_n(\\hat{\\mu}_{ML})=\\frac{n}{\\sigma^2}\\] The estimated variance of the MLE is therefore \\[ \\widehat{\\text{Var}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma^2}{n} \\] and the corresponding asymptotic normal distribution is \\[ \\hat{\\mu}_{ML} \\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right) \\] Note that in this case the distribution is not asymptotic but is exact, i.e. valid also for small \\(n\\) (as long as the data \\(x_i\\) are actually from \\(N(\\mu, \\sigma^2)\\)!). 4.3.2 Wald statistic Centering the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\) with \\(\\boldsymbol \\theta_0\\) followed by standardising with \\(\\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML})\\) yields the Wald statistic (named after Abraham Wald, 1902–1950): \\[ \\begin{split} \\boldsymbol t(\\boldsymbol \\theta_0) &amp; = \\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML})^{-1}(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\ &amp; = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{1/2}(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\ \\end{split} \\] The squared Wald statistic is a scalar defined as \\[ \\begin{split} t(\\boldsymbol \\theta_0)^2 &amp;= \\boldsymbol t(\\boldsymbol \\theta_0)^T \\boldsymbol t(\\boldsymbol \\theta_0) \\\\ &amp;= (\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)^T \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) (\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\ \\end{split} \\] Note that in the literature both \\(\\boldsymbol t(\\boldsymbol \\theta_0)\\) and \\(t(\\boldsymbol \\theta_0)^2\\) are commonly referred to as Wald statistics. In this text we use the qualifier “squared” if we refer to the latter. We now assume that the true underlying parameter is \\(\\boldsymbol \\theta_0\\). Since the MLE is asymptotically normal the Wald statistic is asymptotically standard normal distributed: \\[\\begin{align*} \\begin{array}{cc} \\boldsymbol t(\\boldsymbol \\theta_0) \\overset{a}{\\sim}\\\\ t(\\theta_0) \\overset{a}{\\sim}\\\\ \\end{array} \\begin{array}{ll} N_d(0,\\boldsymbol I_d)\\\\ N(0,1)\\\\ \\end{array} \\begin{array}{ll} \\text{for vector } \\boldsymbol \\theta\\\\ \\text{for scalar } \\theta\\\\ \\end{array} \\end{align*}\\] Correspondingly, the squared Wald statistic is chi-squared distributed: \\[\\begin{align*} \\begin{array}{cc} t(\\boldsymbol \\theta_0)^2 \\\\ t(\\theta_0)^2\\\\ \\end{array} \\begin{array}{ll} \\overset{a}{\\sim}\\chi^2_d\\\\ \\overset{a}{\\sim}\\chi^2_1\\\\ \\end{array} \\begin{array}{ll} \\text{for vector } \\boldsymbol \\theta\\\\ \\text{for scalar } \\theta\\\\ \\end{array} \\end{align*}\\] The degree of freedom of the chi-squared distribution is the dimension \\(d\\) of the parameter vector \\(\\boldsymbol \\theta\\). Example 4.5 Wald statistic for a proportion: We continue from Example 4.3. With \\(\\hat{p}_{ML} = \\bar{x}\\) and \\(\\widehat{\\text{Var}}( \\hat{p}_{ML} ) = \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\\) and thus \\(\\widehat{\\text{SD}}( \\hat{p}_{ML} ) =\\sqrt{ \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n} }\\) we get as Wald statistic: \\[ t(p_0) = \\frac{\\bar{x}-p_0}{ \\sqrt{\\bar{x}(1-\\bar{x}) / n } }\\overset{a}{\\sim} N(0,1) \\] The squared Wald statistic is: \\[t(p_0)^2 = n \\frac{(\\bar{x}-p_0)^2}{ \\bar{x}(1-\\bar{x}) }\\overset{a}{\\sim} \\chi^2_1 \\] Example 4.6 Wald statistic for the mean parameter of a normal distribution with known variance: We continue from Example 4.4. With \\(\\hat{\\mu}_{ML} =\\bar{x}\\) and \\(\\widehat{\\text{Var}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma^2}{n}\\) and thus \\(\\widehat{\\text{SD}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma}{\\sqrt{n}}\\) we get as Wald statistic: \\[t(\\mu_0) = \\frac{\\bar{x}-\\mu_0}{\\sigma / \\sqrt{n}}\\sim N(0,1)\\] Note this is the one sample \\(t\\)-statistic with given \\(\\sigma\\). The squared Wald statistic is: \\[t(\\mu_0)^2 = \\frac{(\\bar{x}-\\mu_0)^2}{\\sigma^2 / n}\\sim \\chi^2_1 \\] Again, in this instance this is the exact distribution, not just the asymptotic one. Using the Wald statistic or the squared Wald statistic we can test whether a particular \\(\\mu_0\\) can be rejected as underlying true parameter, and we can also construct corresponding confidence intervals. 4.3.3 Normal confidence intervals using the Wald statistic The asymptotic normality of MLEs derived from regular models enables us to construct a corresponding normal confidence interval (CI): For example, to construct the asymptotic normal CI for the MLE of a scalar parameter \\(\\theta\\) we use the MLE \\(\\hat{\\theta}_{ML}\\) as estimate of the mean and its standard deviation \\(\\widehat{\\text{SD}}(\\hat{\\theta}_{ML})\\) computed from the observed Fisher information: \\[\\text{CI}=[\\hat{\\theta}_{ML} \\pm c_{normal} \\widehat{\\text{SD}}(\\hat{\\theta}_{ML})]\\] \\(c_{normal}\\) is a critical value for the standard-normal symmetric confidence interval chosen to achieve the desired nominal coverage- The critical values are computed using the inverse standard normal distribution function via \\(c_{\\text{normal}}=\\Phi^{-1}\\left(\\frac{1+\\kappa}{2}\\right)\\) (cf. refresher section in the Appendix). coverage \\(\\kappa\\) Critical value \\(c_{\\text{normal}}\\) 0.9 1.64 0.95 1.96 0.99 2.58 For example, for a CI with 95% coverage one uses the factor 1.96 so that \\[\\text{CI}=[\\hat{\\theta}_{ML} \\pm 1.96\\, \\widehat{\\text{SD}}(\\hat{\\theta}_{ML}) ]\\] The normal CI can be expressed using Wald statistic as follows: \\[\\text{CI}=\\{\\theta_0: | t(\\theta_0)| &lt; c_{\\text{normal}} \\}\\] Similary, it can also be expressed using the squared Wald statistic: \\[\\text{CI}=\\{\\theta_0: t(\\boldsymbol \\theta_0)^2 &lt; c_{\\text{chisq}} \\}\\] Note that this form facilitates the construction of normal confidence intervals for a parameter vector \\(\\boldsymbol \\theta_0\\). The following lists containst the critical values resulting from the chi-squared distribution with degree of freedom \\(m=1\\) for the three most common choices of coverage \\(\\kappa\\) for a normal CI for a univariate parameter: coverage \\(\\kappa\\) Critical value \\(c_{\\text{chisq}}\\) (\\(m=1\\)) 0.9 2.71 0.95 3.84 0.99 6.63 Example 4.7 Asymptotic normal confidence interval for a proportion: We continue from Examples 4.3 and 4.5. Assume we observe \\(n=30\\) measurements with average \\(\\bar{x} = 0.7\\). Then \\(\\hat{p}_{ML} = \\bar{x} = 0.7\\) and \\(\\widehat{\\text{SD}}(\\hat{p}_{ML}) = \\sqrt{ \\frac{ \\bar{x}(1-\\bar{x})}{n} } \\approx 0.084\\). The symmetric asymptotic normal CI for \\(p\\) with 95% coverage is given by \\(\\hat{p}_{ML} \\pm 1.96 \\, \\widehat{\\text{SD}}(\\hat{p}_{ML})\\) which for the present data results in the interval \\([0.536, 0.864]\\). Example 4.8 Normal confidence interval for the mean: We continue from Examples 4.4 and 4.6. Assume that we observe \\(n=25\\) measurements with average \\(\\bar{x} = 10\\), from a normal with unknown mean and variance \\(\\sigma^2=4\\). Then \\(\\hat{\\mu}_{ML} = \\bar{x} = 10\\) and \\(\\widehat{\\text{SD}}(\\hat{\\mu}_{ML}) = \\sqrt{ \\frac{ \\sigma^2}{n} } = \\frac{2}{5}\\). The symmetric asymptotic normal CI for \\(p\\) with 95% coverage is given by \\(\\hat{\\mu}_{ML} \\pm 1.96 \\, \\widehat{\\text{SD}}(\\hat{\\mu}_{ML})\\) which for the present data results in the interval \\([9.216, 10.784]\\). 4.3.4 Normal tests using the Wald statistic Finally, recall the duality between confidence intervals and statistical tests. Specifically, a confidence interval with coverage \\(\\kappa\\) can be also used for testing as follows. for every \\(\\theta_0\\) inside the CI the data do not allow to reject the hypothesis that \\(\\theta_0\\) is the true parameter with significance level \\(1-\\kappa\\). Conversely, all values \\(\\theta_0\\) outside the CI can be rejected to be the true parameter with significance level \\(1-\\kappa\\) . Hence, in order to test whether \\(\\boldsymbol \\theta_0\\) is the true underlying parameter value we can compute the corresponding (squared) Wald statistic, find the desired critical value and then decide on rejection. Example 4.9 Asymptotic normal test for a proportion: We continue from Example 4.7. We now consider two possible values (\\(p_0=0.5\\) and \\(p_0=0.8\\)) as potentially true underlying proportion. The value \\(p_0=0.8\\) lies inside the 95% confidence interval \\([0.536, 0.864]\\). This implies we cannot reject the hypthesis that this is the true underlying parameter on 5% significance level. In contrast, \\(p_0=0.5\\) is outside the confidence interval so we can indeed reject this value. In other words, data plus model exlude this value as statistically implausible. This can be verified more directly by computing the corresponding (squared) Wald statistics (see Example 4.5) and comparing them with the relevant critical value (3.84 from chi-squared distribution for 5% significance level): \\(t(0.5)^2 = 5.71 &gt; 3.84\\) hence \\(p_0=0.5\\) can be rejected. \\(t(0.8)^2 = 1.43 &lt; 3.84\\) hence \\(p_0=0.8\\) cannot be rejected. Note that the squared Wald statistic at the boundaries of the normal confidence interval is equal to the critical value. Example 4.10 Normal confidence interval and test for the mean: We continue from Example 4.8. We now consider two possible values (\\(\\mu_0=9.5\\) and \\(\\mu_0=11\\)) as potentially true underlying mean parameter. The value \\(\\mu_0=9.5\\) lies inside the 95% confidence interval \\([9.216, 10.784]\\). This implies we cannot reject the hypthesis that this is the true underlying parameter on 5% significance level. In contrast, \\(\\mu_0=11\\) is outside the confidence interval so we can indeed reject this value. In other words, data plus model exlude this value as a statistically implausible. This can be verified more directly by computing the corresponding (squared) Wald statistics (see Example 4.6) and comparing them with the relevant critical values: \\(t(9.5)^2 = 1.56 &lt; 3.84\\) hence \\(\\mu_0=9.5\\) cannot be rejected. \\(t(11)^2 = 6.25 &gt; 3.84\\) hence \\(\\mu_0=11\\) can be rejected. The squared Wald statistic at the boundaries of the confidence interval equals the critical value. Note that this is the standard one-sample test of the mean, and that it is exact, not an approximation. 4.4 Example of a non-regular model Not all models allow a quadratic approximation of the log-likelihood function around the MLE. This is the case when the log-likelihood function is not differentiable at the MLE. These models are called non-regular and for those models the normal approximation is not available. Example 4.11 Uniform distribution with upper bound \\(\\theta\\): \\[x_1,\\dots,x_n \\sim U(0,\\theta)\\] With \\(x_{[i]}\\) we denote the ordered observations with \\(0 \\leq x_{[1]} &lt; x_{[2]} &lt; \\ldots &lt; x_{[n]} \\leq \\theta\\) and \\(x_{[n]} = \\max(x_1,\\dots,x_n)\\). We would like to obtain both the maximum likelihood estimator \\(\\hat{\\theta}_{ML}\\) and its distribution. The probability density function of \\(U(0,\\theta)\\) is \\[f(x|\\theta) =\\begin{cases} \\frac{1}{\\theta} &amp;\\text{if } x \\in [0,\\theta] \\\\ 0 &amp; \\text{otherwise.} \\end{cases} \\] and on the log-scale \\[ \\log f(x|\\theta) =\\begin{cases} - \\log \\theta &amp;\\text{if } x \\in [0,\\theta] \\\\ - \\infty &amp; \\text{otherwise.} \\end{cases} \\] Since all observed data \\(x_1, \\ldots, x_n\\) lie in the interval \\([0,\\theta]\\) we get as log-likelihood function \\[ l_n(\\theta) =\\begin{cases} -n\\log \\theta &amp;\\text{for } x_{[n]} \\leq \\theta \\\\ - \\infty &amp; \\text{otherwise} \\end{cases} \\] Obtaining the MLE of \\(\\theta\\) is straightforward: \\(-n\\log \\theta\\) is monotonically decreasing therefore the log-likelihood function has a maximum at \\(\\hat{\\theta}_{ML}=x_{[n]}\\). However, there is a discontinuity in \\(l_n(\\theta)\\) at \\(x_{[n]}\\) and therefore \\(l_n(\\theta)\\) is not differentiable at \\(\\hat{\\theta}_{ML}\\). Thus, there is no quadratic approximation around \\(\\hat{\\theta}_{ML}\\) and the observed Fisher information cannot be computed. Hence, the normal approximation for the distribution of \\(\\hat{\\theta}_{ML}\\) is not valid regardless of sample size, i.e. not even asymptotically for \\(n \\rightarrow \\infty\\). Nonetheless, we can in fact still obtain the sampling distribution of \\(\\hat{\\theta}_{ML}=x_{[n]}\\). However, not via asymptotic arguments but instead by understanding that \\(x_{[n]}\\) is an order statistic (see https://en.wikipedia.org/wiki/Order_statistic ) with the following properties: \\[\\begin{align*} \\begin{array}{cc} x_{[n]}\\sim \\theta \\, \\text{Beta}(n,1)\\\\ \\\\ \\text{E}(x_{[n]})=\\frac{n}{n+1} \\theta\\\\ \\\\ \\text{Var}(x_{[n]})=\\frac{n}{(n+1)^2(n+2)}\\theta^2\\\\ \\end{array} \\begin{array}{ll} \\text{&quot;n-th order statistic&quot; }\\\\ \\\\ \\\\ \\\\ \\approx \\frac{\\theta^2}{n^2}\\\\ \\end{array} \\end{align*}\\] Note that the variance decreases with \\(\\frac{1}{n^2}\\) which is much faster than the usual \\(\\frac{1}{n}\\) of an “efficient” estimator. Correspondingly, \\(\\hat{\\theta}_{ML}\\) is a so-called “super efficient” estimator. "],["05-likelihood5.html", "5 Likelihood-based confidence interval and likelihood ratio 5.1 Likelihood-based confidence intervals and Wilks statistic 5.2 Generalised likelihood ratio test (GLRT)", " 5 Likelihood-based confidence interval and likelihood ratio 5.1 Likelihood-based confidence intervals and Wilks statistic 5.1.1 General idea and definition of Wilks statistic Instead of relying on normal / quadratic approximation, we can also use the log-likelihood directly to find the so called likelihood confidence intervals: Idea: find all \\(\\boldsymbol \\theta_0\\) that have a log-likelihood that is almost as good as \\(l_n(\\hat{\\boldsymbol \\theta}_{ML})\\). \\[\\text{CI}= \\{\\boldsymbol \\theta_0: l_n(\\hat{\\boldsymbol \\theta}_{ML}) - l_n(\\boldsymbol \\theta_0) \\leq \\Delta\\}\\] Here \\(\\Delta\\) is the tolerated deviation from the maximum log-likelihood. We will see below how to determine a suitable \\(\\Delta\\) further below. The above leads naturally to the Wilks log likelihood ratio statistic \\(W(\\boldsymbol \\theta_0)\\) defined as: \\[ \\begin{split} W(\\boldsymbol \\theta_0) &amp; = 2 \\log \\left(\\frac{L(\\hat{\\boldsymbol \\theta}_{ML})}{L(\\boldsymbol \\theta_0)}\\right) \\\\ &amp; =2(l_n(\\hat{\\boldsymbol \\theta}_{ML})-l_n(\\boldsymbol \\theta_0))\\\\ \\end{split} \\] With its help we can write the likelihood CI follows: \\[\\text{CI}= \\{\\boldsymbol \\theta_0: W(\\boldsymbol \\theta_0) \\leq 2 \\Delta\\}\\] The Wilks statistic is named after Samuel S. Wilks (1906–1964). Advantages of using a likelihood-based CI: not restricted to be symmetric enables to construct multivariate CIs for parameter vector easily even in non-normal cases contains normal CI as special case Question: how to choose \\(\\Delta\\), i.e how to calibrate the likelihood interval? Essentially, by comparing with normal CI! Example 5.1 Wilks statistic for the proportion: The log-likelihood for the parameter \\(p\\) is (cf. Example 3.1) \\[ l_n(p) = n ( \\bar{x} \\log p + (1-\\bar{x}) \\log(1-p) ) \\] Hence the Wilks statistic is \\[ \\begin{split} W(p_0) &amp; = 2 ( l_n( \\hat{p}_{ML} ) -l_n( p_0 ) )\\\\ &amp; = 2 n \\left( \\bar{x} \\log \\left( \\frac{ \\bar{x} }{p_0} \\right) + (1-\\bar{x}) \\log \\left( \\frac{1-\\bar{x} }{1-p_0} \\right) \\right) \\\\ \\end{split} \\] Comparing with Example 2.8 we see that in this case the Wilks statistic is essentially (apart from a scale factor \\(2n\\)) the KL divergence between two Bernoulli distributions: \\[ W(p_0) =2 n D_{\\text{KL}}( \\text{Ber}( \\hat{p}_{ML} ), \\text{Ber}(p_0) ) \\] Example 5.2 Wilks statistic for the mean parameter of a normal model: The Wilks statistic is \\[ W(\\mu_0)^2 = \\frac{(\\bar{x}-\\mu_0)^2}{\\sigma^2 / n} \\] See Worksheet 4 for a derivation of the Wilks statistic directly from the log-likelihood function. Note this is the same as the squared Wald statistic discussed in Example 4.6. Comparing with Example 2.10 we see that in this case the Wilks statistic is essentially (apart from a scale factor \\(2n\\)) the KL divergence between two normal distributions with different means and variance equal to \\(\\sigma^2\\): \\[ W(p_0) =2 n D_{\\text{KL}}( N( \\hat{\\mu}_{ML}, \\sigma^2 ), N(\\mu_0, \\sigma^2) ) \\] 5.1.2 Quadratic approximation of Wilks statistic and squared Wald statistic Recall the quadratic approximation (= second order Taylor series around the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\)) applied the log-likelihood function \\(l_n(\\boldsymbol \\theta_0)\\): \\[l_n(\\boldsymbol \\theta_0)\\approx l_n(\\hat{\\boldsymbol \\theta}_{ML})-\\frac{1}{2}(\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})^T \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) (\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})\\] With this we can then approximate the Wilks statistic: \\[ \\begin{split} W(\\boldsymbol \\theta_0) &amp; = 2(l_n(\\hat{\\boldsymbol \\theta}_{ML})-l_n(\\boldsymbol \\theta_0))\\\\ &amp; \\approx (\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})^T \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})(\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})\\\\ &amp; =t(\\boldsymbol \\theta_0)^2 \\\\ \\end{split} \\] Thus the quadratic approximation of the Wilks statistic yields the squared Wald statistic! Conversely, the Wilks statistic can be understood a generalisation of the squared Wald statistic. Example 5.3 Quadratic approximation of the Wilks statistic for a proportion (continued from Example 5.1): A Taylor series of second order (for \\(p_0\\) around \\(\\bar{x}\\)) yields \\[ \\log \\left( \\frac{ \\bar{x} }{p_0} \\right) \\approx -\\frac{p_0-\\bar{x}}{\\bar{x}} + \\frac{ ( p_0-\\bar{x} )^2 }{2 \\bar{x}^2 } \\] and \\[ \\log \\left( \\frac{ 1- \\bar{x} }{1- p_0} \\right) \\approx \\frac{p_0-\\bar{x}}{1-\\bar{x}} + \\frac{ ( p_0-\\bar{x} )^2 }{2 (1-\\bar{x})^2 } \\] With this we can approximate the Wilks statistic of the proportion as \\[ \\begin{split} W(p_0) &amp; \\approx 2 n \\left( - (p_0-\\bar{x}) +\\frac{ ( p_0-\\bar{x} )^2 }{2 \\bar{x} } + (p_0-\\bar{x}) + \\frac{ ( p_0-\\bar{x} )^2 }{2 (1-\\bar{x}) } \\right) \\\\ &amp; = n \\left( \\frac{ ( p_0-\\bar{x} )^2 }{ \\bar{x} } + \\frac{ ( p_0-\\bar{x} )^2 }{ (1-\\bar{x}) } \\right) \\\\ &amp; = n \\left( \\frac{ ( p_0-\\bar{x} )^2 }{ \\bar{x} (1-\\bar{x}) } \\right) \\\\ &amp;= t(p_0)^2 \\,. \\end{split} \\] This verifies that the quadratic approximation of the Wilks statistic leads back to the squared Wald statistic of Example 4.5. Example 5.4 Quadratic approximation of the Wilks statistic for the mean parameter of a normal model (continued from Example 5.2): The normal log-likelihood is already quadratic in the mean parameter (cf. Example 3.2). Correspondingly, the Wilks statistic is quadratic in the mean parameter as well. Hence in this particular case the quadratic “approximation” is in fact exact and the Wilks statistic and the squared Wald statistic are identical! Correspondingly, confidence intervals and tests based on the Wilks statistic are identical to those obtained using the Wald statistic. 5.1.3 Distribution of the Wilks statistic The connection with the squared Wald statistic implies that both have asympotically the same distribution. Hence, under \\(\\boldsymbol \\theta_0\\) the Wilks statistic is distributed asymptotically as \\[W(\\boldsymbol \\theta_0) \\overset{a}{\\sim} \\chi^2_d\\] where \\(d\\) is the number of parameters in \\(\\boldsymbol \\theta\\), i.e. the dimension of the model. For scalar \\(\\theta\\) (i.e. single parameter and \\(d=1\\)) this becomes \\[ W(\\theta_0) \\overset{a}{\\sim} \\chi^2_1 \\] This fact is known as Wilks’ theorem. 5.1.4 Cutoff values for the likelihood CI coverage \\(\\kappa\\) \\(\\Delta = \\frac{c_{chisq}}{2}\\) (\\(m=1\\)) 0.9 1.35 0.95 1.92 0.99 3.32 The asymptotic distribution for \\(W\\) is useful to choose a suitable \\(\\Delta\\) for the likelihood CI — note that \\(2 \\Delta = c_{chisq}\\) where \\(c_{chisq}\\) is the critical value for a specified coverage \\(\\kappa\\). This yields the above table for scalar parameter Example 5.5 Likelihood confidence interval for a proportion: We continue from Example 5.1, and as in Example 4.7 we asssume we have data with \\(n = 30\\) and \\(\\bar{x} = 0.7\\). This yields (via numerical root finding) as the 95% likelihood confidence interval the interval \\([0.524, 0.843]\\). It is similar but not identical to the corresponding asymptotic normal interval \\([0.536, 0.864]\\) obtained in Example 4.7. The following figure illustrate the relationship between the normal CI, the likelihood CI and also shows the role of the quadratic approximation (see also Example 4.2). Note that: the normal CI is symmetric around the MLE whereas the likelihood CI is not symmetric the normal CI is identical to the likelihood CI when using the quadratic approximation! 5.1.5 Likelihood ratio test (LRT) using Wilks statistic As in the normal case (with Wald statistic and normal CIs) one can also construct a test using the Wilks statistic: \\[\\begin{align*} \\begin{array}{ll} H_0: \\boldsymbol \\theta= \\boldsymbol \\theta_0\\\\ H_1: \\boldsymbol \\theta\\neq \\boldsymbol \\theta_0\\\\ \\end{array} \\begin{array}{ll} \\text{ True model is } \\boldsymbol \\theta_0\\\\ \\text{ True model is } \\textbf{not } \\boldsymbol \\theta_0\\\\ \\end{array} \\begin{array}{ll} \\text{ Null hypothesis}\\\\ \\text{ Alternative hypothesis}\\\\ \\end{array} \\end{align*}\\] As test statistic we use the Wilks log likelihood ratio \\(W(\\boldsymbol \\theta_0)\\). Extreme values of this test statistic imply evidence against \\(H_0\\). Note that the null model is “simple” (= a single parameter value) whereas the alternative model is “composite” (= a set of parameter values). Remarks: The composite alternative \\(H_1\\) is represented by a single point (the MLE). Reject \\(H_0\\) for large values of \\(W(\\boldsymbol \\theta_0)\\) under \\(H_0\\) and for large \\(n\\) the statistic \\(W(\\boldsymbol \\theta_0)\\) is chi-squared distributed, i.e. \\(W(\\boldsymbol \\theta_0) \\overset{a}{\\sim} \\chi^2_d\\). This allows to compute critical values (i.e tresholds to declared rejection under a given significance level) and also \\(p\\)-values corresponding to the observed test statistics. Models outside the CI are rejected Models inside the CI cannot be rejected, i.e. they can’t be statistically distinguished from the best alternative model. A statistic equivalent to \\(W(\\boldsymbol \\theta_0)\\) is the likelihood ratio \\[\\Lambda(\\boldsymbol \\theta_0) = \\frac{L(\\boldsymbol \\theta_0)}{L(\\hat{\\boldsymbol \\theta}_{ML})}\\] The two statistics can be transformed into each other by \\(W(\\boldsymbol \\theta_0) = -2\\log \\Lambda(\\boldsymbol \\theta_0)\\) and \\(\\Lambda(\\boldsymbol \\theta_0) = e^{ - W(\\boldsymbol \\theta_0) / 2 }\\). We reject \\(H_0\\) for small values of \\(\\Lambda\\). It can be shown that the likelihood ratio test to compare two simple model is optimal in the sense that for any given specified type I error (=probability of wrongly rejecting \\(H_0\\), i.e. the sigificance level) it will maximise the power (=1- type II error, probability of correctly accepting \\(H_1\\)). This is known as the Neyman-Pearson theorem. Example 5.6 Likelihood test for a proportion: We continue from Example 5.5 with 95% likelihood confidence interval \\([0.524, 0.843]\\). The value \\(p_0=0.5\\) is outside the CI and hence can be rejected whereas \\(p_0=0.8\\) is insided the CI and hence cannot be rejected on 5% significance level. The Wilks statistic for \\(p_0=0.5\\) and \\(p_0=0.8\\) take on the following values: \\(W(0.5)^2 = 4.94 &gt; 3.84\\) hence \\(p_0=0.5\\) can be rejected. \\(W(0.8)^2 = 1.69 &lt; 3.84\\) hence \\(p_0=0.8\\) cannot be rejected. Note that the Wilks statistic at the boundaries of the likelihood confidence interval is equal to the critical value (3.84 corresponding to 5% significance level for a chi-squared distribution with 1 degree of freedom). 5.1.6 Origin of likelihood ratio statistic The likelihood ratio statistic is asymptotically linked to differences in the KL divergences of the two compared models with the underlying true model. Assume that \\(F\\) is the true (and unknown) data generating model \\(G_{\\boldsymbol \\theta}\\) is a family of models and we would like to compare two candidate models \\(G_A\\) and \\(G_B\\) corresponding to parameters \\(\\boldsymbol \\theta_A\\) and \\(\\boldsymbol \\theta_B\\) on the basis of observed data \\(x_1, \\ldots, x_n\\). The KL divergences \\(D_A = D_{\\text{KL}}(F, G_A)\\) and \\(D_B=D_{\\text{KL}}(F, G_B)\\) indicate how close each of the models \\(G_A\\) and \\(G_B\\) fit the true \\(F\\). The difference \\(D_B-D_A\\) is thus a way to measure the relative fit of the two models, and can be computed as \\[ D_B-D_A = D_{\\text{KL}}(F, G_B)-D_{\\text{KL}}(F, G_A) = \\text{E}_{F} \\log \\frac{g_A(x)}{g_B(x)} \\] Replacing \\(F\\) by the empirical distribution \\(\\hat{F}_n\\) leads to the large sample approximation \\[ 2 n (D_B-D_A) \\approx 2 (l_n(\\boldsymbol \\theta_A) - l_n(\\boldsymbol \\theta_B)) \\] Hence, the difference in the log-likelihoods provides an estimate of the difference in the KL divergence of the two models involved. The Wilks log likelihood ratio statistic \\[ W(\\boldsymbol \\theta_0) = 2 ( l_n( \\hat{\\boldsymbol \\theta}_{ML} ) - l_n(\\boldsymbol \\theta_0) ) \\approx 2 n (D_{F_{\\boldsymbol \\theta_0}} - D_{F_{\\hat{\\boldsymbol \\theta}_{ML}}}) \\] thus compares the best-fit distribution with \\(\\hat{\\boldsymbol \\theta}_{ML}\\) as the parameter to the distribution with parameter \\(\\boldsymbol \\theta_0\\). For some specific models the Wilks statistic can also be written in the form of the KL divergence: \\[ W(\\boldsymbol \\theta_0) = 2n D_{\\text{KL}}( F_{\\hat{\\boldsymbol \\theta}_{ML}}, F_{\\boldsymbol \\theta_0}) \\] This is the case for the examples 5.1 and 5.2 and also more generally for exponential family models, but it is not true in general. 5.2 Generalised likelihood ratio test (GLRT) Also known as maximum likelihood ratio test (MLRT). The Generalised Likelihood Ratio Test (GLRT) works like the standard likelihood ratio test with the difference that now the null model \\(H_0\\) is a composite model. This means that in the denominator in the test statistic needs to be optimised as well. \\[\\begin{align*} \\begin{array}{ll} H_0: \\boldsymbol \\theta\\in \\omega_0 \\subset \\Omega \\\\ H_1: \\boldsymbol \\theta\\in \\omega_1 = \\Omega \\setminus \\omega_0\\\\ \\end{array} \\begin{array}{ll} \\text{ True model lies in restricted model space }\\\\ \\text{ True model is not the restricted model space } \\\\ \\end{array} \\end{align*}\\] Both \\(H_0\\) and \\(H_1\\) are now composite hypotheses. \\(\\Omega\\) represents the unrestricted model space with dimension (=number of free parameters) \\(d = |\\Omega|\\). The constrained space \\(\\omega_0\\) has degree of freedom \\(d_0 = |\\omega_0|\\) with \\(d_0 &lt; d\\). Note that in the standard LRT the set \\(\\omega_0\\) is a simple point with \\(d_0=0\\) as the null model is a simple distribution. Thus, LRT is contained in GLRT as special case! The corresponding generalised (log) likelihood ratio statistic is given by \\[ W = 2\\log\\left(\\frac{L(\\hat{\\theta}_{ML})}{L(\\hat{\\theta}_{ML}^0)}\\right) \\text{ and } \\Lambda = \\frac{\\underset{\\theta \\in \\omega_0}{\\max}\\, L(\\theta)}{\\underset{\\theta \\in \\Omega}{\\max}\\, L(\\theta)} \\] where \\(L(\\hat{\\theta}_{ML})\\) is the maximised likelihood assuming the full model (with parameter space \\(\\Omega\\)) and \\(L(\\hat{\\theta}_{ML}^0)\\) is the maximised likelihood for the restricted model (with parameter space \\(\\omega_0\\)). Remarks: MLE in the restricted model space \\(\\omega_0\\) is taken as a representative of \\(H_0\\). The likelihood is maximised in both numerator and denominator. The restriced model is a special case of the full model (i.e. the two models are nested). The asymptotic distribution of \\(W\\) is chi-squared with degree of freedom depending on both \\(d\\) and \\(d_0\\): \\[W \\overset{a}{\\sim} \\chi^2_{d-d_0}\\] This result is due to Wilks (1938). Note that it assumes that the true model is contained among the investigated models. If \\(H_0\\) is a simple hypothesis (i.e. \\(d_0=0\\)) then the standard LRT (and corresponding CI) is recovered as special case of the GLRT. Example 5.7 GLRT example: Case-control study: (e.g. “healthy” vs. “disease”) we observe normal data from two groups with sample size \\(n_1\\) and \\(n_2\\) (and \\(n=n_1+n_2\\)): \\[x_1,\\dots,x_{n_1} \\sim N(\\mu_1, \\sigma^2)\\] and \\[x_{n_1+1},\\dots,x_{n} \\sim N(\\mu_2, \\sigma^2)\\] Question: are the two means \\(\\mu_1\\) and \\(\\mu_2\\) the same in the two groups? \\[\\begin{align*} \\begin{array}{ll} H_0: \\mu_1=\\mu_2 \\text{ (with variance unknown nuisance parameter)} \\\\ H_1: \\mu_1\\neq\\mu_2\\\\ \\end{array} \\end{align*}\\] Restricted and full models: \\(\\omega_0\\): restricted model with two parameters \\(\\mu_0\\) and \\(\\sigma^2_0\\) (so that \\(x_{1},\\dots,x_{n} \\sim N(\\mu_0, \\sigma_0^2)\\) ). \\(\\Omega\\): full model with three parameters \\(\\mu_1, \\mu_2, \\sigma^2\\). Corresponding log-likelihood functions: Restricted model \\(\\omega_0\\): \\[ \\log L(\\mu_0, \\sigma_0^2) = -\\frac{n}{2} \\log(\\sigma_0^2) - \\frac{1}{2\\sigma_0^2} \\sum_{i=1}^n (x_i-\\mu_0)^2 \\] Full model \\(\\Omega\\): \\[ \\begin{split} \\log L(\\mu_1, \\mu_2, \\sigma^2) &amp; = \\left(-\\frac{n_1}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n_1} (x_i-\\mu_1)^2 \\right) + \\\\ &amp; \\phantom{==} \\left(-\\frac{n_2}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=n_1+1}^{n} (x_i-\\mu_1)^2 \\right) \\\\ &amp;= -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n_1} (x_i-\\mu_1)^2 + \\sum_{i=n_1+1}^n (x_i-\\mu_2)^2 \\right) \\\\ \\end{split} \\] Corresponding MLEs: \\[\\begin{align*} \\begin{array}{ll} \\omega_0:\\\\ \\\\ \\Omega:\\\\ \\\\ \\end{array} \\begin{array}{ll} \\hat{\\mu}_0 = \\frac{1}{n}\\sum^n_{i=1}x_i\\\\ \\\\ \\hat{\\mu}_1 = \\frac{1}{n_1}\\sum^{n_1}_{i=1}x_i\\\\ \\hat{\\mu}_2 = \\frac{1}{n_2}\\sum^{n}_{i=n_1+1}x_i\\\\ \\end{array} \\begin{array}{ll} \\widehat{\\sigma^2_0} = \\frac{1}{n}\\sum^n_{i=1}(x_i-\\hat{\\mu}_0)^2\\\\ \\\\ \\widehat{\\sigma^2} = \\frac{1}{n}\\left\\{\\sum^{n_1}_{i=1}(x_i-\\hat{\\mu}_1)^2+\\sum^n_{i=n_1+1}(x_i-\\hat{\\mu}_2)^2\\right\\}\\\\ \\\\ \\end{array} \\end{align*}\\] We note that the two estimated variances are related by \\[ \\begin{split} \\widehat{\\sigma^2_0} &amp; = \\widehat{\\sigma^2} + \\frac{n_1 n_2}{n^2} (\\hat{\\mu}_1 - \\hat{\\mu}_2)^2\\\\ &amp; = \\widehat{\\sigma^2} \\left( 1+ \\frac{1}{n} \\frac{(\\hat{\\mu}_1 - \\hat{\\mu}_2)^2}{ \\frac{n}{n_1 n_2} \\widehat{\\sigma^2}}\\right) \\\\ &amp; = \\widehat{\\sigma^2} \\left( 1 + \\frac{t^2_{ML}}{n}\\right) \\end{split} \\] with \\[ t_{ML} = \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{\\sqrt{\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right) \\widehat{\\sigma^2}}} \\] This is an example of a variance decomposition, with \\(\\widehat{\\sigma^2_0}\\) being the estimated total variance and \\(\\widehat{\\sigma^2}\\) the estimated within-group variance. Corresponding maximised log-likelihood: Restricted model: \\[\\log L(\\hat{\\mu}_0,\\widehat{\\sigma^2_0}) = -\\frac{n}{2} \\log(\\widehat{\\sigma^2_0}) -\\frac{n}{2} \\] Full model: \\[ \\log L(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2}) = -\\frac{n}{2} \\log(\\widehat{\\sigma^2}) -\\frac{n}{2} \\] Likelihood ratio statistic: \\[ \\begin{split} W &amp; = 2\\log\\left(\\frac{L(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2})}{L(\\hat{\\mu}_0,\\widehat{\\sigma^2_0})}\\right)\\\\ &amp; = 2 \\log L(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2}) - 2 \\log L(\\hat{\\mu}_0,\\widehat{\\sigma^2_0}) \\\\ &amp; = n\\log\\left(\\frac{\\widehat{\\sigma^2_0}}{\\widehat{\\sigma^2}} \\right) \\\\ &amp; = n\\log\\left(1+\\frac{t^2_{ML}}{n}\\right) \\\\ \\end{split} \\] The last step uses the decomposition for the total variance \\(\\widehat{\\sigma^2_0}\\). If an unbiased total variance estimate is used the \\[ W=n\\log\\left(1+\\frac{1}{n-2}t^2\\right) \\] with \\[ t = \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{\\sqrt{\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)\\frac{n}{n-2} \\widehat{\\sigma^2}}} \\] \\(\\longrightarrow\\) the GRLT is a monotone function of the (squared) two-sample \\(t\\)-statistic! It can be shown that all standard tests with normal distributions can be interpreted as GLRTs! "],["06-likelihood6.html", "6 Optimality properties and conclusion 6.1 Properties of maximum likelihood encountered so far 6.2 Summarising data and the concept of minimal sufficiency 6.3 Concluding remarks on maximum likelihood", " 6 Optimality properties and conclusion 6.1 Properties of maximum likelihood encountered so far MLE is a special case of relative entropy minimisation valid for large samples. MLE can be seen as generalisation of least squares (and conversely, least squares is a special case of ML). \\[\\begin{align*} \\begin{array}{cc} \\text{Kullback-Leibler 1951}\\\\ \\textbf{Entropy learning: minimise } D_{\\text{KL}}(F_{\\text{true}},F_{\\boldsymbol \\theta})\\\\ \\downarrow\\\\ \\text{large } n\\\\ \\downarrow\\\\ \\text{Fisher 1922}\\\\ \\textbf{Maximise Likelihood } L(\\boldsymbol \\theta|x_1, \\dots, x_n)\\\\ \\downarrow\\\\ \\text{normal model}\\\\ \\downarrow\\\\ \\text{Gauss 1805}\\\\ \\textbf{Minimise squared error } \\sum_i (x_i-\\theta)^2\\\\ \\end{array} \\end{align*}\\] Given a model, derivation of the MLE is basically automatic (only optimisation required)! MLEs are consistent, i.e. if the true underlying model \\(F_{\\text{true}}\\) with parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) is contained in the set of specified candidates models \\(F_{\\boldsymbol \\theta}\\) then the MLE will converge to the true model. Correspondingly, MLEs are asympotically unbiased. However, MLEs are not necessarily unbiased in finite samples (e.g. the MLE of the variance parameter in the normal distribution). The maximum likelihood is invariant against parameter transformations. In regular situations (when local quadratic approximation is possible) MLEs are asympotically normally distributed, with the asymptotic variance determined by the observed Fisher information. In regular situations and for large sample size MLEs are asympotically optimally efficient (Cramer-Rao theorem): For large samples the MLE achieves the lowest possible variance possible in an estimator — this is the so-called Cramer-Rao lower bound. The variance decreases to zero with \\(n \\rightarrow \\infty\\) typically with rate \\(1/n\\). The likelihood ratio can be used to construct optimal tests (in the sense of the Neyman-Pearson theorem). 6.2 Summarising data and the concept of minimal sufficiency Another important concept in statistics and likelihood theory (especially when applied to the exponential family) is that of a minimally sufficient statistic to optimally summarise the information available in the data about a parameter in a model. Generally, a statistic \\(T(x_1, \\ldots, x_n)= T(x_i)\\) is function of the data \\(x_1, \\ldots, x_n\\). In the following we write \\(x_i\\) as a shorthand for the complete data set with \\(n\\) observations. The statistic \\(T(x_i)\\) can be of any type and value (scalar, vector, matrix etc. — even a function). \\(T(x_i)\\) is called a summary statistic if it describes important aspects of the data such as location (e.g. the average \\(\\text{avg}(x_i) =\\bar{x}\\), the median) or scale (e.g. standard deviation, interquartile range). A statistic \\(T(x_i)\\) is said to be sufficient for a parameter \\(\\boldsymbol \\theta\\) in a model if the corresponding likelihood function can be written in terms of \\(T(x_i)\\) so that \\[ L(\\boldsymbol \\theta| x_i) = h( T(x_i) , \\boldsymbol \\theta) \\, k(x_i) \\,, \\] where \\(h(x)\\) and \\(k(x)\\) are positive-valued functions, and or equivalently on log-scale \\[ l_n(\\boldsymbol \\theta) = \\log h( T(x_i) , \\boldsymbol \\theta) + \\log k(x_i) \\,. \\] This is known as the Fisher-Pearson factorisation. By construction, estimation and inference about \\(\\boldsymbol \\theta\\) based on the factorised likelihood \\(L(\\boldsymbol \\theta)\\) is mediated through the sufficient statistic \\(T(x_i)\\) and does not require the original data \\(x_i\\). Instead, the sufficient statistic \\(T(x_i)\\) contains all the information in \\(x_i\\) required to learn about the parameter \\(\\boldsymbol \\theta\\). Therefore, if the MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\) of \\(\\boldsymbol \\theta\\) exists and is unique then the MLE is a unique function of the sufficient statistic \\(T(x_i)\\). If the MLE is not unique then it can be chosen to be function of \\(T(x_i)\\). Note that a sufficient statistic always exists since the data \\(x_i\\) are themselves sufficient statistics, with \\(T(x_i) = x_i\\). Furthermore, sufficient statistics are not unique since applying a one-to-one transformation to \\(T(x_i)\\) yields another sufficient statistic. Every sufficient statistic \\(T(x_i)\\) induces a partitioning of the space of data sets by clustering all hypothetical outcomes for which the statistic \\(T(x_i)\\) assumes the same value \\(t\\): \\[\\mathcal{X}_t = \\{x_i: T(x_i) = t\\}\\] The data sets in \\(\\mathcal{X}_t\\) are equivalent in terms of the sufficient statistic \\(T(x_i)\\). Note that the dimensions of \\(T(x_i)\\) may be much smaller than those of \\(x_i\\). Instead of \\(n\\) data points as few as one or two summaries may be sufficient to fully convey all the information in the data about the model parameters. Thus, transforming data \\(x_i\\) using a sufficient statistic \\(T(x_i)\\) may result in substantial data reduction. Data sets \\(x_i\\) and \\(y_i\\) for which the ratio of the likelihoods \\(L(\\boldsymbol \\theta| x_i )/L(\\boldsymbol \\theta| y_i)\\) does not depend on \\(\\boldsymbol \\theta\\) (so the two likelihoods are proportional to each other by a constant) are called likelihood equivalent because a likelihood-based procedure to learn about \\(\\boldsymbol \\theta\\) will draw identical conclusions from \\(x_i\\) and \\(y_i\\). For data sets \\(x_i, y_i \\in \\mathcal{X}_t\\) equivalent with respect to a sufficient statistic \\(T(x_i)\\) it follows directly from the Fisher-Pearson factorisation that the ratio \\[L(\\boldsymbol \\theta| x_i )/L(\\boldsymbol \\theta| y_i) = k(x_i)/ k(y_i)\\] and thus is constant with regard to \\(\\boldsymbol \\theta\\). Consequently, all data sets in \\(\\mathcal{X}_t\\) are also likelihood equivalent. However, the converse is not true: depending on the sufficient statistics there usually will be many likelihood equivalent data sets that are not part of the same set \\(\\mathcal{X}_t\\). Of particular interest is therefore to find those sufficient statistics that achieve the coarsest partitioning of the sample space and thus may allow the highest data reduction. Specifically, a minimal sufficient statistic is a sufficient statistic \\(T(x_i)\\) for which all likelihood equivalent data sets also are equivalent under \\(T(x_i)\\). Therefore, to check whether a sufficient statistic \\(T(x_i\\) is minimally sufficient we verify whether for any two likelihood equivalent data sets \\(x_i\\) and \\(y_i\\) it also follows that \\(T(x_i) = T(y_i)\\). If this holds true then \\(T(x_i)\\) is a minimally sufficient statistic. An equivalent non-operational definition is that a minimal sufficient statistic \\(T(x_i)\\) is a sufficient statistic that can be computed from any other sufficient statistic \\(S(x_i)\\). This follows from the above directly: assume any sufficient statistic \\(S(x_i)\\), this defines a corresponding set \\(\\mathcal{X}_s\\) of likelihood equivalent data sets. By implication any \\(x_i, y_i \\in \\mathcal{X}_s\\) will ncecessarily also be in \\(\\mathcal{X}_t\\), thus whenever \\(S(x_i)=S(y_i)\\) we also have \\(T(x_i)=T(y_i)\\), and therefore \\(T(x_i)\\) is a function of \\(S(x_i)\\). A trivial but important example of a minimal sufficient statistic is the likelihood function itself since by definition it can be computed from any set of sufficient statistics. Thus the likelihood function \\(L(\\boldsymbol \\theta)\\) captures all information about \\(\\boldsymbol \\theta\\) that is available in the data. In other words, it provides an optimal summary of the observed data with regard to a model. Note that in Bayesian statistics (to be discussed in Part 2 of the module) the likelihood function is used as proxy/summary of the data. Example 6.1 Sufficient statistics for the parameters of the normal distribution: The normal model \\(N(\\mu, \\sigma^2)\\) with parameter vector \\(\\boldsymbol \\theta= (\\mu, \\sigma^2)^T\\) and log-likelihood \\[ l_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (x_i-\\mu)^2 \\] One possible set of minimal sufficient statistics for \\(\\boldsymbol \\theta\\) are \\(\\bar{x}\\) and \\(\\overline{x^2}\\), and with these we can rewrite the log-likelihood function without any reference to the original data \\(x_i\\) as follows \\[ l_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2) -\\frac{n}{2 \\sigma^2} (\\overline{x^2} - 2 \\bar{x} \\mu + \\mu^2) \\] An alternative set of minimal sufficient statistics for \\(\\boldsymbol \\theta\\) consists of \\(s^2 = \\overline{x^2} - \\bar{x}^2 = \\widehat{\\sigma^2}_{ML}\\) as and \\(\\bar{x} = \\hat{\\mu}_{ML}\\). The log-likelihood written in terms of \\(s^2\\) and \\(\\bar{x}\\) is \\[ l_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2) -\\frac{n}{2 \\sigma^2} (s^2 + (\\bar{x} - \\mu)^2 ) \\] Note that in this example the dimension of the parameter vector \\(\\boldsymbol \\theta\\) equals the dimension of the minimal sufficient statistic, and furthermore, that the MLEs of the parameters are in fact minimal sufficient! The conclusion from Examples 6.1 holds true more generally: in the exponential family (which contains the normal distribution as special case) the MLEs of the natural parameters are minimal sufficient statistics. Thus, there will typically be substantial dimension reduction from the raw data to the sufficient statistics. However, outside the exponential family the MLE is not necessarily a minimal sufficient statistic, and may not even be a sufficient statistic. This is because a (minimal) sufficient statistic of the same dimension as the parameters does not always exist. A classic example is the Cauchy distribution for which the minimal sufficient statistics are the ordered observations, thus the MLE of the parameters do not constitute sufficient statistics, let alone minimal sufficient statistics. However, the MLE is of course still a function of the minimal sufficient statistic. In summary, the likelihood function acts as perfect data summariser (i.e. as minimally sufficient statistic), and in exponential families (e.g. Normal distribution) the MLEs of the parameters \\(\\hat{\\boldsymbol \\theta}_{ML}\\) are minimimally sufficient. Finally, while sufficiency is clearly a useful concept for data reduction one needs to keep in mind that this is always in reference to a specific model Therefore, unless one strongly believes in a certain model it is generally a good idea to keep (and not discard!) the original data. 6.3 Concluding remarks on maximum likelihood 6.3.1 Remark on KL divergence Finding the model \\(F_{\\boldsymbol \\theta}\\) that best approximates the underlying true model \\(F_0\\) is done by minimising the relative entropy \\(D_{\\text{KL}}(F_0,F_{\\boldsymbol \\theta})\\). For large sample size \\(n\\) we may approximate \\(F_0\\) by the empirical distribution\\(\\hat{F}_0\\), and minimising \\(D_{\\text{KL}}(\\hat{F}_0,F_{\\boldsymbol \\theta})\\) then yields the method of maximum likelihood. However, since the KL divergence is not symmetric there are in fact two ways to minimise the divergence between a fixed \\(F_0\\) and the optimised \\(F_{\\boldsymbol \\theta}\\), each with different properties: forward KL, approximation KL: \\(\\min_{\\boldsymbol \\theta} D_{\\text{KL}}(F_0,F_{\\boldsymbol \\theta}\\) This is also called an “M (Moment) projection”. It has a zero avoiding property: \\(f_{\\boldsymbol \\theta}(x)&gt;0 \\text{ whenever } f_0(x)&gt;0\\) reverse KL, inference KL: \\(\\min_{\\boldsymbol \\theta} D_{\\text{KL}}(F_{\\boldsymbol \\theta},F_0)\\) This is also called an “I (Information) projection”. It has a zero forcing property: \\(f_{\\boldsymbol \\theta}(x)=0 \\text{ whenever } f_0(x)=0\\) Maximum likelihood is based on “forward KL”, whereas Bayesian updating and Variational Bayes approximations use “reverse KL”. 6.3.2 What happens if \\(n\\) is small? From the long list of optimality properties of ML it is clear that for large sample size \\(n\\) the best estimator will typically be the MLE. However, for small sample size it is indeed possible (and necessary) to improve over the MLE (e.g. via Bayesian estimation or regularisation). Some of these ideas will be discussed in Part II. Likelihood will overfit! Alternative methods need to be used: regularised/penalised likelihood Bayesian methods which are essentially two sides of the same coin. Classic example of a simple non-ML estimator that is better than the MLE: Stein’s example / Stein paradox (C. Stein, 1955): Problem setting: estimation of the mean in multivariate case Maximum likelihood estimation breaks down! \\(\\rightarrow\\) average (=MLE) is worse in terms of MSE than Stein estimator. For small \\(n\\) the asymptotic distributions for the MLE and for the LRT are not accurate, so for inference in these situations the distributions may need to be obtained by simulation (e.g. parametric or nonparametric bootstrap). 6.3.3 Model selection CI are sets of models that are not statistically distinguishable from the best ML model in doubt, choose the simplest model compatible with data better prediction, avoids overfitting Useful for model exploration and model building. Note that, by construction, the model with more parameters always has a higher likelihood, implying likelihood favours complex models Complex model may overfit! For comparison of models penalised likelihood or Bayesian approaches may be necessary Model selection in small samples and high dimension is challenging Recall that the aim in statistics is not about rejecting models (this is easy as for large sample size any model will be rejected!) Instead, the aim is model building, i.e. to find a model that explains the data well and that predicts well! Typically, this will not be the best-fit ML model, but rather a simpler model that is close enough to the best / most complex model. "],["07-bayes1.html", "7 Essentials of Bayesian statistics 7.1 Conditional probability 7.2 Bayes’ theorem 7.3 Principle of Bayesian learning 7.4 What is exactly is the “Bayesian estimate”? 7.5 Computer implementation of Bayesian learning 7.6 Bayesian interpretation of probability 7.7 Historical developments 7.8 Connection with entropy learning", " 7 Essentials of Bayesian statistics 7.1 Conditional probability Assume we have two random variables \\(x\\) and \\(y\\) with a joint density (or joint PMF) \\(p(x,y)\\). By definition \\(\\int \\int_{x,y} p(x,y) dx dy = 1\\). The marginal densities for the individual \\(x\\) and \\(y\\) are given by \\(p(x) = \\int_y p(x,y) dy\\) and \\(p(y) = \\int_x f(x,y) dx\\). Thus, when computing the marginal densities a variable is removed from the joint density by integrating over all possible states of that variable. It follows also that \\(\\int_x p(x) dx = 1\\) and \\(\\int_y p(y) dy = 1\\), i.e. the marginal densities also integrate to 1. As alternative to integrating out a random variable in the joint density \\(p(x,y)\\) we may wish to keep it fixed at some value, say keep \\(y\\) fixed at \\(y_0\\). In this case \\(p(x, y=y_0)\\) is proportional to the conditional density (or PMF) given by the ratio \\[ p(x | y=y_0) = \\frac{p(x, y=y_0)}{p(y=y_0)} \\] The denominator \\(p(y=y_0) = \\int_x p(x, y=y_0) dx\\) is needed to ensure that \\(\\int_x p(x | y=y_0) dx = 1\\), thus it renormalises \\(p(x, y=y_0)\\) so that it is a proper density. To simplify notation, the specific value on which a variable is conditioned is often left out. The mean and variance of the conditional distribution are called conditional mean and conditional variance. Rearranging the above we see that the joint density can be written as the product of marginal and conditional density in two different ways: \\[ p(x,y) = p(x| y) f(y) = p(y | x) p(x) \\] 7.2 Bayes’ theorem Bayesian statistical learning is linked with the name of Thomas Bayes (1701-1761) who was the first to state Bayes’ theorem (1763) on conditional probability. Interestingly, this work published only after Bayes’ death by Richard Price (1723-1791)): \\[ p(A | B) = p(B | A) \\frac{ p(A) }{ p(B)} \\] This theorem relates the two possible conditional densities (or conditional probability mass functions) for two events \\(A\\) and \\(B\\). It follows directly from the product rule linking the joint density with the marginal and conditional densities. 7.3 Principle of Bayesian learning Ingredients: \\(\\theta\\) parameter of interest, unknown and fixed. prior distribution with density \\(p(\\theta)\\) describing the uncertainty (not randomness!) about \\(\\theta\\) data generating process \\(p(x | \\theta)\\) (likelihood!) Question: new information in the form of new observation \\(x\\) arrives - how does the uncertainty about \\(\\theta\\) change? Answer: use Bayes’ theorem to update prior distribution to posterior distribution. \\[ \\underbrace{p(\\theta | x)}_{\\text{posterior} } = \\frac{p(x | \\theta) }{ p(x)} \\underbrace{p(\\theta)}_{\\text{prior}} \\] Note that this update procedure can be repeated again and again: we can use the posterior as our new prior and then update it with further data. For the denominator in Bayes formula we need to compute \\(p(x)\\). This is obtained by \\[ \\begin{split} p(x) &amp;= \\text{E}_{F_{\\theta}}p(x | \\theta) \\\\ &amp;= \\int_{\\theta} p(x | \\theta) p(\\theta) d\\theta \\\\ &amp;= \\int_{\\theta} p(x , \\theta) d\\theta \\\\ \\end{split} \\] i.e. by marginalisation of the parameter \\(\\theta\\) from the joint distribution of \\(\\theta\\) and \\(x\\). (For discrete \\(\\theta\\) replace the integral by a sum). Depending on the context this quantity is either called marginal likelihood (of the underlying model) or prior predictive distribution (for the data). Intringuingly, to conduct a Bayesian statistical analysis typically require integration and/or averaging (e.g. to compute the marginal likelihood), in contrast to maximum likelihood that requires optimisation (to find the maximum likelihood). 7.4 What is exactly is the “Bayesian estimate”? The Bayesian estimate is the full complete posterior distribution! However, it is useful to summarise aspects of the posterior distribution: Posterior mean \\(\\text{E}(\\theta | x)\\) Posterior variance \\(\\text{Var}(\\theta | x)\\) Posterior mode etc. In particular the mean of the posterior distribution is often taken as a Bayesian point estimate. The posterior distribution also allows to define credible regions or credible intervals. These are the Bayesian equivalent to confidence intervals and are constructed by finding the areas of highest probability mass (say 95%) in the posterior distribution. Bayesian credible intervals (unlike their frequentist confidence counterparts) are thus very easy to interpret - they simply correspond to the area in the parameter space in which the we can find the parameter with a given specified probability. In contrast, in frequentist statistics it does not make sense to assign a probability to a parameter value! Note that there are typically many credible intervals with the given specified coverage \\(\\alpha\\) (say 95%). Therefore, we may need further criteria to construct these intervals. In the univariate case a two-sided equal-tail credible interval is obtained by finding the corresponding lower \\(1-\\alpha/2\\) and upper \\(\\alpha/2\\) quantiles. A highest posterior density (HPD) interval of coverage \\(\\alpha\\) is found by identifying the shortest interval (i.e. with smallest support) for the given \\(\\alpha\\) probability mass. Any point within such an interval has higher density resp. probability than outside the credible interval, and the density / probability at the boundaries are all equal. Thus a Bayesian HPD credible interval is constructed similar like a likelihood based confidence interval. When the posterior has multiple modes this means the the HPD interval may be disjoint. 7.5 Computer implementation of Bayesian learning As we have seen Bayesian learning is conceptually straightforward: Specify prior uncertainty \\(p(\\theta\\)) about the parameters of interest \\(\\theta\\). Specify the data generating process for a specified parameter: \\(p(x | \\theta)\\). Apply Bayes’ theorem to update prior uncertainty in the light of the new data. In practise, however, computing the posterior distribution can be computationally very demanding, especially for complex models. For this reason specialised software packages have been developed for computational Bayesian modelling, for example: Bayesian statistics in R: https://cran.r-project.org/web/views/Bayesian.html Stan probabilistic programming language (can be used with R and Python) — https://mc-stan.org/ Bayesian statistics in Python: PyMC3 using Theano, Pyro using PyTorch, NumPyro using JAX, TensorFlow Probability using Tensorflow Bayesian statistics in Julia: Turing.jl Bayesian hierarchical modelling with BUGS and JAGS. In addition to numerical procedures to sample from the posterior distribution there are also many procedures aiming to approximate the Bayesian posterior, employing the Laplace approximation, integrated nested Laplace approximation (INLA), variational Bayes etc. 7.6 Bayesian interpretation of probability 7.6.1 What makes you “Bayesian”? If you use Bayes’ theorem are you therefore automatically a Bayesian? No!! Bayes’ theorem is a mathematical fact from probability theory. Hence, Bayes’ theorem is valid for everyone, whichever form for statistical learning your are subscribing (such as frequentist ideas, likelihood methods, entropy learning, Bayesian learning). As we discuss now the key difference between Bayesian and frequentist statistical learning lies in the differences in interpretation of probability, not in the mathematical formalism for probability (which includes Bayes’ theorem). 7.6.2 Mathematics of probability The mathematics of probability in its modern foundation was developed by Andrey Kolmogorov (1903–1987). In this book Foundations of the Theory of Probability (1933) he establishes probability in terms of set theory/ measure theory. This theory provides a coherent mathematical framework to work with probabilities. However, Kolmogorov’s theory does not provide an interpretation of probability! \\(\\rightarrow\\) The Kolmogorov framework is the basis for both the frequentist and the Bayesian interpretation of probability. 7.6.3 Interpretations of probability Essentially, there are two major commonly used interpretation of probability in statistics - the frequentist interpretation and the Bayesian interpretation. A: Frequentist interpretation probability = frequency (of an event in a long-running series of identically repeated experiments) This is the ontological view of probability (i.e. probability “exists” and is identical to something that can be observed.). It is also a very restrictive view of probability. For example, frequentist probability cannot be used to describe events that occur only a single time. Frequentist probability thus can only be applied asymptotically, for large samples! B: Bayesian probability “Probability does not exist” — famous quote by Bruno de Finetti (1906–1985), a Bayesian statistician. What does this mean? Probability is a description of the state of knowledge and of uncertainty. Probability is thus an epistemological quantity that is assigned and that changes rather than something that is an inherent property of an object. Note that this does not require any repeated experiments. The Bayesian interpretation of probability is valid regardless of sample size or the number or repetitions of an experiment. Hence, the key difference between frequentist and Bayesian approaches is not the use of Bayes’ theorem. Rather it is whether you consider probability as ontological (frequentist) or epistemological entity (Bayesian). 7.7 Historical developments Thomas Bayes (1701-1761) the father of Bayesian statistics Only after his death his paper on Bayes’ theorem was published (1763). Laplace (from 1800) was actually the first to use Bayes’ theorem for statistical calculations. This activivity was then called “inverse probability”. Between 1900 and 1940 classical mathematical statistics was developed and the field was heavily influenced and dominated by R.A. Fisher (who invented likelihood theory and ANOVA, among other things - he also was working in population genetics). Fisher himself was very much opposed to Bayesian theory. 1931 de Finetti publishes his “representation theorem”. This shows that the joint distribution of a sequence of exchangeable events (i.e. where the ordering can be permuted) can be represented by a mixture distribution that can be constructed via Bayes’ theorem. (Note that exchangeability is a weaker condition than i.i.d.) This theorem is often used as a justification Bayesian statistics (along with the socalled Dutch book argument, also by de Finetti). 1933 publication of Kolmogorov’s book on probability theory. 1946 Cox theorem (Richard T. Cox (1898–1991)): the aim to generalise classical logic (from TRUE/FALSE to continuous measures of uncertainty) inevitably leads to probability theory and Bayesian learning! This justification of Bayesian statistics was later popularised by Edwin T. Jaynes (1922–1998) in various books (1959, 2003). 1955 Stein Paradox - Charles M. Stein (1920–2016) publishes paper on the Stein estimator - an estimator of the mean that dominates ML estimator. His estimator is always better in terms of MSE than the ML estimator, and this was very puzzling at that time! From 1970 onwards Bayesian learning has become more pervasive! Computers allow to do the complex computations needed in Bayesian statistics Metropolis-Hastings algorithm published A lot of work on interpreting Stein estimators as empirical Bayes estimators (Efron and Morris 1975) and on development of regularised estimation techniques such as penalised likelihood in regression (e.g. ridge regression) regularisation originally was only meant to make singular systems/matrices invertible - but then it turned out regularisation has a simple Bayesian interpretation! work on reference priors (Bernado 1979) penalised likelihood via KL divergence for model selection (Akaike 1973) Another boost was in the 1990/2000s when in science (e.g. genomics) many complex and high-dimensional data set were becoming widely available. Classical statistical methods cannot be used in this setting (overfitting!) so many new methods were developed for high-dimensional data analysis, many with direct link to Bayesian statistics: 1996 lasso regression (Tibshirani) Machine learning etc (many Bayesians in this field!) 7.8 Connection with entropy learning 7.8.1 Zero forcing property It is easy to see that if in Bayes rule the prior probability for an event is set to 0, then the posterior probability for that event will remain at 0, regardless of the data! This zero-forcing property of the Bayes update rule has been called Cromwell’s rule by D. Lindley. Therefore, assigning prior probability 0 to an event should be avoided. Note that this implies that assigning prior probability 1 to an event should be avoided, too, since this means assigning 0 to all other alternative events. 7.8.2 Connection with entropy learning The Bayesian update rule is a very general form of learning when the new information arrives in the form of data. But actually there is an even a more general principle: the principle of minimal information update (e.g. Jaynes 1959, 2003) or principle of minimum information discrimination (MDI) (Kullback 1959): Change your beliefs only as much as necessary to be coherent with new evidence! This is also called entropy learning since the KL divergence (\\(F_{\\theta | \\text{new information}};F_{\\theta})\\) is employed to measure the divergence from the updated distribution to the distribution prior to the arrival of the information. Note that this update is based on an \\(I\\)-projection (see Part I, Likelihood), which also does have the zero forcing property (hinting that Bayes rule is a special case). Thus, when new information arrives then the uncertainty about the parameter is only minimally adjusted, just as much as needed to account for the new information (“inertia of beliefs”). There are three main special cases that follow from the entropy learning rule: if information arrives in form of data \\(\\rightarrow\\) update by T. Bayes’ theorem (1763) if information is in the form of another distribution \\(\\rightarrow\\) update using R. Jeffrey’s rule (1965) if the information is in form of constraints \\(\\rightarrow\\) Kullback’s principle of minimum MDI (1959), E. T. Jaynes MaxEnt principle (1957) Since 1) is by far the most common situation it is clear why it is important to study Bayesian learning! This shows (again) how fundamentally important KL divergence is in statistics - it not only leads to likelihood inference but also to Bayesian learning, as well as to other forms of information updating! Furthermore, relative entropy is useful to choose priors (e.g. reference priors) and in experimental design. "],["08-bayes2.html", "8 Beta-Binomial model for estimating a proportion 8.1 Binomial likelihood 8.2 Excursion: Properties of the Beta distribution 8.3 Beta prior distribution 8.4 Computing the posterior distribution", " 8 Beta-Binomial model for estimating a proportion In this chapter we discuss how to estimate a portion in the Bayesian framework. 8.1 Binomial likelihood In order to apply Bayes’ theorem we first need to find a suitable likelihood based on modeling the data generating process. Here we follow the Binomial model as used previously in Part I: Repeated Bernoulli experiment (Binomial model): \\(x \\in \\{0, 1\\}\\) (e.g. “tails” vs. “heads”) proability mass function (pmf): \\(\\text{Pr}(x=1) = p\\), \\(\\text{Pr}(x=0) = 1-p\\) Mean: \\(\\text{E}(x) = p\\) Variance \\(\\text{Var}(x) = p (1-p)\\) \\(\\text{Bin}(n,p)\\) (sum of \\(n\\) Bernoulli experiments) \\(x \\in \\{0, 1, \\ldots, n\\}\\) Mean: \\(\\text{E}(x) = n p\\) Variance: \\(\\text{Var}(x) = n p (1-p)\\) Standardised Binomial (average of \\(n\\) Bernoulli experiments): \\(\\frac{x}{n} \\in \\{0, \\frac{1}{n}, \\ldots,1\\}\\) Mean: \\(\\text{E}(\\frac{x}{n}) = p\\) Variance: \\(\\text{Var}(\\frac{x}{n}) = \\frac{p (1-p)}{n}\\) From part I (likelihood theory) we know that the maximum likelihood estimate of the proportion is the frequency \\(\\hat p_{ML} = \\frac{x}{n}\\) given \\(x\\) (number of “heads”) is observed in \\(n\\) repeats. 8.2 Excursion: Properties of the Beta distribution The density of the Beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\) for \\(x \\in [0,1]\\) and \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\) is \\[f(x | \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}\\] The mean is \\(\\text{E}(x) = \\mu = \\frac{\\alpha}{\\alpha+\\beta}\\) and the variance \\(\\text{Var}(x)=\\frac{\\mu (1-\\mu)}{\\alpha+\\beta+1}\\). The density depends on the Beta function \\(B(a, b) = \\frac{ \\Gamma(a) \\Gamma(b)}{\\Gamma(a + b)}\\) which in turn is defined via Euler’s Gamma function \\[ \\Gamma(x) = \\int_0^\\infty t^{x-1} e^{-t} dt \\] Note that \\(\\Gamma(x) = (x-1)!\\) for any positive integer \\(x\\) A useful reparameterisation of the Beta distribution is in terms of the parameters \\(\\mu \\in [0,1]\\) and \\(m &gt; 0\\), yielding the original parameters via \\(\\alpha= \\mu m\\) and \\(\\beta=(1-\\mu)m\\). Conversely, \\(m=\\alpha+\\beta\\) and \\(\\mu = \\frac{\\alpha}{\\alpha+\\beta}\\). The Beta distribution is very flexible and can assume a number of different shapes, depending on the value of \\(\\alpha\\) and \\(\\beta\\): 8.3 Beta prior distribution In Bayesian learning we need to make explicit our uncertainty about \\(p\\). \\(p\\) has support \\([0,1]\\) \\(\\rightarrow\\) we use the Beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\) as prior for \\(p\\) with parameters \\(\\alpha \\geq 0\\) and \\(\\beta \\geq 0\\): \\[ p \\sim \\text{Beta}(\\alpha, \\beta) \\] Note this does not actually mean that \\(p\\) is random! It only means that we model the uncertainty about \\(p\\) using a Beta random variable! The flexibility of the Beta distribution allows to accomodate a large variety of possible scenarious for our prior knowledge. The prior mean is \\[\\text{E}(p) = \\frac{\\alpha}{m} = \\mu_{\\text{prior}}\\] and the prior variance \\[ \\text{Var}(p) = \\frac{\\mu_{\\text{prior}} (1-\\mu_{\\text{prior}})}{m + 1} \\] where \\(m = \\alpha + \\beta\\). Note the similarity to the moments of the standardised Binomial above! 8.4 Computing the posterior distribution Bayes’ theorem for continous random variables to compute posterior density: \\[ f(p | x) = \\frac{f(x | p) f(p) }{\\int_{p^{&#39;}} f(x | p^{&#39;}) f(p^{&#39;}) dp^{&#39;}} \\] We use in our analysis the Beta-Binomial model: Beta prior: \\[ p \\sim \\text{Beta}(\\alpha, \\beta) \\] \\[ f(p) = \\frac{1}{B(\\alpha, \\beta)} p^{\\alpha-1} (1-p)^{\\beta-1} \\] Binomial likelihood: \\[ x | p \\sim \\text{Bin}(n,p) \\] \\[ f(x|p) = \\begin{pmatrix} n \\\\ x \\end{pmatrix} p^x (1-p)^{(n-x)} \\] Applying Bayes’ theorem results in Beta posterior distribution \\[ p| x \\sim \\text{Beta}(\\alpha+x, \\beta+n-x) \\] \\[ f(p| x) = \\frac{1}{B(\\alpha+x, \\beta+n-x)} p^{\\alpha+x-1} (1-p)^{\\beta+n-x-1} \\] (for a proof see Worksheet 5!) The posterior can be summarised by its first two moments (mean and variance): Posterior mean: \\[ \\mu_{\\text{posterior}}= \\text{E}(p | x) = \\frac{x +\\alpha}{n+ m } \\] Posterior variance: \\[ \\sigma^2_{\\text{posterior}}= \\text{Var}(p | x) = \\frac{\\mu_{\\text{posterior}}(1-\\mu_{\\text{posterior}})}{n+m+1 } \\] "],["09-bayes3.html", "9 Properties of Bayesian learning 9.1 Prior acting as pseudo-data 9.2 Linear shrinkage of mean 9.3 Conjugacy of prior and posterior distribution 9.4 Large sample asymptotics 9.5 Posterior variance for finite \\(n\\)", " 9 Properties of Bayesian learning The Beta-Binomial models allows to observe a number of intriguing features and properties of Bayesian learning. Many of these extend also to other models as we will see later. 9.1 Prior acting as pseudo-data In the expression for the posterior mean and variance you can see that \\(m=\\alpha + \\beta\\) behaves like an implicit sample size connected with prior information! Specifically, \\(\\alpha\\) and \\(\\beta\\) act as pseudo-counts that influence both the posterior mean and the posterior variance, exactly in the same way as coventional data. For example, larger \\(m\\) (and thus \\(\\alpha\\) and \\(\\beta\\)) the smaller is the posterior variance, with variance decreasing proportional to the inverse of \\(m\\). If the prior is highly concentrated, i.e. if it has low variance and large precision (=inverse variance) then the implicit data size \\(m\\) is large. Conversely, if the prior has a large variance, then the prior is vague and the implicit data size \\(m\\) is small. Hence, a prior has the same effect as if one would add data – but without actually adding data! This is precisely this why a prior acts as a regulariser and prevents overfitting, because it increases effective sample size. Another interpretation is that any prior summarises data that may have been available previously as observations. 9.2 Linear shrinkage of mean The posterior mean \\(\\mu_{\\text{posterior}}\\) is a linearly adjusted \\(\\hat\\mu_{ML}\\). This becomes evident by writing \\(\\mu_{\\text{posterior}}\\) as \\[ \\mu_{\\text{posterior}} = \\lambda \\mu_{\\text{prior}} + (1-\\lambda) \\hat\\mu_{ML} \\] with weight \\(\\lambda \\in [0,1]\\) \\[ \\lambda = \\frac{m}{m+n} \\,. \\] The posterior mean is a convex combination (i.e. the weighted average) of the ML estimate and the prior mean. The factor \\(\\lambda\\) is called the shrinkage intensity — note that it is the ratio of the “prior sample size” (\\(m\\)) and the “effective overall sample size” (\\(m+n\\)). This is called shrinkage, because the ML estimator is “shrunk” towards the prior mean (which is often called the “target”, and sometimes the target is zero, and then the terminology “shrinking” makes most sense). If the shrinkage intensity is zero (\\(\\lambda = 0\\)) then the ML point estimator is recovered. This implies \\(\\alpha=0\\) and \\(\\beta=0\\), or \\(n \\rightarrow \\infty\\). Note that using maximum likelihood to estimate of the proportion \\(p\\) (for moderate or small \\(n\\)) is the same as Bayesian estimation using the Beta-Binomial model with prior \\(\\alpha=0\\) and \\(\\beta=0\\). This prior is extremely “u-shaped” and the implicit prior for the ML estimation. (Would you would use such a prior intentionally?) If the shrinkage intensity is large (\\(\\lambda \\rightarrow 1\\)) then the posterior mean corresponds to the prior. This happens if \\(n=0\\) or if \\(m\\) is very large (implying that the prior is sharply concentrated around the prior mean). Since the ML estimate (=frequency) is unbiased the Bayesian point estimate is biased (for finite n)! And the bias is in fact the prior mean! So Bayesian statistics produces by default biased estimators (but asymptotically they will be unbiased like in ML). That the posterior mean is a linear combination of the ML estimate and the prior mean is not a coincidence. In fact, this is true for all distributions in the exponential family (see e.g. Diaconis and Ylvisaker, 1979). Furthermore, it is possible (and indeed quite useful for computational reasons!) to formulate Bayes theory completely in terms of linear shrinkage (e.g. Hartigan 1969). The resulting theory is called “Bayes linear statistics” (Goldstein and Wooff, 2007). 9.3 Conjugacy of prior and posterior distribution In the Beta-Binomial model for estimating the proportion \\(p\\) the choice of the Beta distribution as prior distribution along with the Binomial likelihood resulted in having the Beta distribution as posterior distribution as well. If the prior and posterior belong to the same distributional family the prior is called a conjugate prior. This will be the case if the prior has the same functional form as the likelihood. In the Beta-Binomial the likelihood is based on the Binomial distribution and has the following form (only terms depending on the parameter \\(p\\) are shown): \\[ p^x (1-p)^{n-x} \\] The form of the Beta prior is (again, only showing terms depending on \\(p\\)): \\[ p^{\\alpha-1} (1-p)^{\\beta-1} \\] Since the posterior is proportional to the product of prior and likelihood the posterior will have exactly the same form as the prior: \\[ p^{\\alpha+x-1} (1-p)^{\\beta+n-x-1} \\] Choosing the prior distribution from a family conjugate to the likelihood greatly simplifies Bayesian analysis since the Bayes formula can then be written in form of an update formula for the parameters of the Beta distribution: \\[ \\alpha \\rightarrow \\alpha +x \\] \\[ \\beta \\rightarrow \\beta +n-x \\] Thus, conjugate prior distributions are very convenient choices. However, in their application it must be ensured that the prior distribution is flexible enough to encapsulate all prior information that may be available. In cases where this is not the case alternative priors should be used (and most likely this will then require to compute the posterior distribution numerically rather than analytically). 9.4 Large sample asymptotics 9.4.1 Large sample limits of mean and variance If \\(n\\) is large and \\(n &gt;&gt; \\alpha, \\beta\\) the posterior mean and variance become asympotically \\[ \\mu_{\\text{posterior}} \\overset{a}{=} \\frac{x }{n} = \\hat\\mu_{ML} \\] and \\[ \\sigma^2_{\\text{posterior}} \\overset{a}{=} \\frac{\\hat\\mu_{ML} (1-\\hat\\mu_{ML})}{n} \\] Thus, if sample size is large the Bayes’ estimator turns into the ML estimator! Specifically, the posterior mean becomes the ML point estimate, and the posterior variance is equal to the asymptotic variance computed via the observed Fisher information! Thus, for large \\(n\\) the data dominate and any details about the prior (such as values of \\(\\alpha\\) and \\(\\beta\\) become irrelevant! 9.4.2 Asymptotic Normality of the Posterior distribution Also known as Bayesian Central Limit Theorem (CLT). Under some regularity conditions (such as regular likelihood and positive prior probability for all parameter values, finite number of parameters, etc.) for large sample size the Bayesian posterior distribution converges to a Normal distribution centered around the MLE and with the variance of the MLE: \\[ \\text{for large $n$: } p(\\boldsymbol \\theta| \\boldsymbol x_1, \\boldsymbol x_2, \\ldots, \\boldsymbol x_n) \\to N(\\hat{\\boldsymbol \\theta}_{ML}, \\text{Var}(\\hat{\\boldsymbol \\theta}_{ML}) ) \\] So not only are the posterior mean and variance converging to the MLE and the variance of the MLE for large sample size, but also the posterior distribution itself converges to the sampling distribution! This holds generally in many regular cases, not just in our example of the Beta-Bernoulli model. The Bayesian CLT is generally known as the Bernstein-van Mises theorem (who discovered it at around 1920-30), but special cases were already known as by Laplace. In the Worksheet 5 the asymptotic convergence of the posterior distribution to a normal distribution is demonstrated grapically. 9.5 Posterior variance for finite \\(n\\) In the previous chapter we have the derived a Bayesian point estimate for the proportion \\(p\\) as the posterior mean \\[ \\text{E}(p | x ) = \\frac{x+\\alpha}{n+m} = \\hat{p}_{\\text{Bayes}} \\] with posterior variance \\[ \\text{Var}(p | x) = \\frac{\\hat{p}_{\\text{Bayes}} (1-\\hat{p}_{\\text{Bayes}})}{n+m+1} \\] Asymptotically, we have seen that for large \\(n\\) the posterior becomes the ML estimator, and the posterior variance becomes the asymptotic variance of the MLE. Thus, the Bayesian estimate will be indistinguishable from the MLE for large \\(n\\) and shares its favourable properties. In addition, for finite sample size the posterior variance will tyically be smaller than both the asymptotic posterior variance (for large \\(n\\)) and the prior variance, showing that combining the information in the prior and in the data leads to a more efficient estimate. "],["10-bayes4.html", "10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance 10.1 Normal-Normal model to estimate mean 10.2 Inverse-Gamma-Normal model to estimate variance", " 10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance 10.1 Normal-Normal model to estimate mean 10.1.1 Normal likelihood For the likelihood we assume as data-generating model the normal distribution with known fixed variance \\(\\sigma^2\\) \\[ x| \\mu \\sim N(\\mu, \\sigma^2) \\] This yields as the MLE \\(\\hat\\mu_{ML} = \\bar{x}\\). 10.1.2 Normal prior distribution To model the uncertainty about \\(\\mu\\) we use the normal distribution \\(N(\\mu, \\sigma^2/k)\\) parameterised by the two parameters \\(\\mu\\) and \\(k\\) (remember \\(\\sigma^2\\) is fixed). With \\(\\mu=\\mu_0\\) and \\(k=m\\) we get the normal prior \\[ \\mu \\sim N(\\mu_0, \\sigma^2/m) \\] with prior mean \\(\\text{E}(\\mu) = \\mu_0\\) and prior variance \\(\\text{Var}(\\mu) = \\frac{\\sigma^2}{m}\\) where \\(m\\) is the implied sample size from the prior. Note that \\(m\\) does not need to be an integer value! 10.1.3 Normal posterior distribution The posterior distribution after observing \\(n\\) samples \\(x_1, \\ldots x_n\\) is normal with \\(\\mu=\\mu_1\\) and \\(k=m+n\\) \\[ \\mu | x_1, \\ldots x_n \\sim N(\\mu_1, \\sigma^2/(m+n)) \\] with posterior mean \\[ \\text{E}(\\mu | x_1, \\ldots x_n) = \\mu_1 =\\frac{m \\mu_0 + n \\bar{x}}{n+m} = \\lambda \\mu_0 + (1-\\lambda) \\hat\\mu_{ML} \\] with \\(\\lambda = \\frac{m}{n+m}\\). Note the linear shrinkage of \\(\\hat\\mu_{ML}\\) towards \\(\\mu_0\\)! The corresponding posterior variance is \\[ \\text{Var}(\\mu | x_1, \\ldots x_n) = \\frac{\\sigma^2}{n+m} \\] Thus, the normal distribution is the conjugate distribution to the mean parameter in the normal likelihood. 10.1.4 Large sample asymptotics and Stein paradox For \\(n\\) large and \\(n &gt;&gt; m\\) we get \\[ \\text{E}(\\mu | x_1, \\ldots x_n) \\overset{a}{=} \\hat\\mu_{ML} \\] \\[ \\text{Var}(\\mu | x_1, \\ldots x_n) \\overset{a}{=} \\frac{\\sigma^2}{n} \\] i.e. the MLE and its asymptotic variance! Note that the posterior variance \\(\\frac{\\sigma^2}{n+m}\\) is smaller than the asymptotic variance \\(\\frac{\\sigma^2}{n}\\) and the prior variance \\(\\frac{\\sigma^2}{m}\\). When studying the frequentist properties of the posterior mean \\(\\mu_1\\) it turns out that by an appropriate choice of \\(m\\) (or \\(\\lambda\\)) it is possible to construct an estimator that will outperform the MLE for finite \\(n\\) in terms of MSE (with the reduced variance compensating for the increase in bias)! Charles Stein was one of the first to present such an estimator (see next chapter), and by many of his contemporaries it was considered very puzzling to have any estimator outperform the MLE, hence this effect is called Stein paradox. 10.2 Inverse-Gamma-Normal model to estimate variance 10.2.1 Inverse Gamma distribution Next, we study a common Bayesian model for estimating the variance parameter of the normal distribution. For this we use the inverse Gamma distribution: \\[ x \\sim \\text{Inv-Gam}(\\alpha, \\beta) \\] This distribution is closely linked with the Gamma distribution — the inverse of \\(x\\) is Gamma-distributed with inverted scale parameter: \\[\\frac{1}{x} \\sim \\text{Gam}(\\alpha, \\beta^{-1})\\] For use as prior and posterior we employ a different parameterisation with \\(k=2(\\alpha-1)\\) and \\(v=\\beta/(\\alpha-1)\\): \\[ x \\sim \\text{Inv-Gam}(1+\\frac{k}{2}, \\frac{k}{2} v) \\] The first two moments of the inverse Gamma distribution are \\[\\text{E}(x) = \\frac{\\beta}{\\alpha-1} = v\\] and \\[\\text{Var}(x) = \\frac{\\beta^2}{(\\alpha-1)^2 (\\alpha-2)} =\\frac{2 v^2}{k-2}\\] 10.2.2 Normal likelihoood As data likelihood / generating model we use normal distribution \\(N(\\mu, \\sigma^2)\\) with given fixed mean \\(\\mu\\). This yields as MLE \\(\\widehat\\sigma^2_{ML}= \\frac{1}{n}\\sum_{i=1}^n (x_i-\\mu)^2\\) 10.2.3 Inverse Gamma prior distribution For the prior distribution we use the inverse Gamma distribution with with \\(k=m\\) and \\(v=\\sigma^2_0\\) \\[ \\sigma^2 \\sim \\text{Inv-Gam}(k=m, v=\\sigma^2_0) \\] The corresponding prior mean is \\[ \\text{E}(\\sigma^2) = \\sigma^2_0 \\] and the prior variance is \\[ \\text{Var}(\\sigma^2) = \\frac{2 \\sigma_0^4}{m-2} \\] (note that \\(m &gt; 2\\)) 10.2.4 Inverse Gamma posterior distribution As the inverse Gammma distribution is conjugate to the normal likelihood the posterior distribution is inverse Gamma as well: \\[ \\sigma^2| x_1 \\ldots, x_n \\sim \\text{Inv-Gam}(k=m+n, v=\\sigma^2_1) \\] with \\(\\sigma^2_1 = \\frac{\\sigma^2_0 m + n \\widehat\\sigma^2_{ML}}{m+n}\\). The posterior mean is \\[ \\text{E}(\\sigma^2 | x_1 \\ldots, x_n) = \\sigma^2_1 \\] and the posterior variance \\[ \\text{Var}(\\sigma^2 | x_1 \\ldots, x_n) = \\frac{ 2 \\sigma^4_1}{m+n-2} \\] The update formula for the posterior mean of the variance follows the usual linear shrinkage rule: \\[ \\sigma^2_1 = \\lambda \\sigma^2_0 + (1-\\lambda) \\widehat\\sigma^2_{ML} \\] with \\(\\lambda=\\frac{m}{m+n}\\). 10.2.5 Large sample asymptotics For \\(n\\) large and \\(n &gt;&gt; m\\) we get \\[ \\text{E}(\\sigma^2 | x_1, \\ldots x_n) \\overset{a}{=} \\widehat\\sigma^2_{ML} \\] \\[ \\text{Var}(\\sigma^2 | x_1, \\ldots x_n) \\overset{a}{=} \\frac{2 \\sigma^4}{n} \\] which is indeed the MLE of \\(\\sigma^2\\) and its asymptotic variance! 10.2.6 Estimating precision Instead of estimating the variance it is actually a bit simpler to estimate the precision (i.e. the inverse variance). For this one would then use a Gamma prior and a normal likelihood, resulting in a Gamma posterior. 10.2.7 Joint estimation of mean and variance It is possible to combine the Normal-Normal for the mean and the Inverse-Gamma-Normal model into a joint model for the mean and variance. This implies having a joint prior and a joint posterior for \\(\\mu\\) and \\(\\sigma^2\\). Details are not shown here but the resulting joint point estimators are identical to the above individual estimators. "],["11-bayes5.html", "11 Shrinkage estimation using empirical risk minimisation 11.1 Linear shrinkage 11.2 James-Stein estimator", " 11 Shrinkage estimation using empirical risk minimisation 11.1 Linear shrinkage In the examples for Bayesian estimation we have seen so far the posterior mean of the parameter of interest was obtained by linear shrinkage \\[ \\hat\\theta_{\\text{shrink}} = \\text{E}( \\theta | x_1, \\ldots, x_n) = \\lambda \\theta_0 + (1-\\lambda) \\hat\\theta_{\\text{ML}} \\] of the MLE \\(\\hat\\theta_{\\text{ML}}\\) towards the prior mean \\(\\theta_0\\), with shrinkage intensity \\(\\lambda=\\frac{m}{m+n}\\) determined by the pseudo-sample size \\(m\\) (which in turn is linked the precision of the prior) and the sample size \\(n\\). The resulting point estimate \\(\\hat\\theta_{\\text{shrink}}\\) is called shrinkage estimate and is a convex combination of \\(\\theta_0\\) and \\(\\hat\\theta_{\\text{ML}}\\). The prior mean \\(\\theta_0\\) is also called the “target”. In a Bayesian estimation the parameter \\(m\\) and hence \\(\\lambda\\) is given a priori, but it turns out that it is possible and useful to find an optimal value for \\(\\lambda\\) by minimising the mean squared error of the estimator \\(\\hat\\theta_{\\text{shrink}}\\). In particular, by construction, the target \\(\\theta_0\\) has zero variance but substantial bias, whereas the MLE \\(\\hat\\theta_{\\text{ML}}\\) will have low or zero bias but a non-vanishing variance. By combinining these two estimators with opposite properties the aim is to achieve a bias-variance tradeoff so that the resulting estimator \\(\\hat\\theta_{\\text{shrink}}\\) has lower MSE than either \\(\\theta_0\\) and \\(\\hat\\theta_{\\text{ML}}\\). Specifically, the aim is to find \\[ \\lambda^{\\star} = \\underset{\\lambda}{\\arg \\min \\ } \\text{E}\\left( ( \\theta - \\hat\\theta_{\\text{shrink}} )^2\\right) \\] It turns out that this can be minimised without knowing the actual true value of \\(\\theta\\) and the result for an unbiased \\(\\hat\\theta_{\\text{ML}}\\) is \\[ \\lambda^{\\star} = \\frac{\\text{Var}(\\hat\\theta_{\\text{ML}})}{\\text{E}( (\\hat\\theta_{\\text{ML}} - \\theta_0)^2 )} \\] Hence, the shrinkage intensity will be small if the variance of the MLE is small and/or if the target and the MLE differ substantially. On the other hand, if the variance of the MLE is large and/or the target is close to the MLE the shrinkage intensity will be large. 11.2 James-Stein estimator We can now use empirical risk minimisation to estimate the shrinkage parameter the Normal-Normal model. In 1955 James and Stein propose the following estimate for the multivariate mean \\(\\boldsymbol \\mu\\) of using a single sample \\(\\boldsymbol x\\) drawn from the multivariate normal \\(N_d(\\boldsymbol \\mu, \\boldsymbol I)\\): \\[ \\hat{\\boldsymbol \\mu}_{JS} = (1-\\frac{d-2}{||\\boldsymbol x||^2}) \\boldsymbol x \\] Here, we recognise \\(\\hat{\\boldsymbol \\mu}_{ML} = \\boldsymbol x\\), \\(\\boldsymbol \\mu_0=0\\) and shrinkage intensity \\(\\lambda^{\\star}=\\frac{d-2}{||\\boldsymbol x||^2}\\). Efron and Morris (1972) and Lindley and Smith (1972) generalised this shrinkage estimator to the case of multiple observations \\(\\boldsymbol x_1, \\ldots \\boldsymbol x_n\\) and target \\(\\boldsymbol \\mu_0\\), yielding an empirical Bayes estimate of \\(\\mu\\) based on the Normal-Normal model. "],["12-bayes6.html", "12 Bayesian model comparison using Bayes factors and the BIC 12.1 The Bayes factor 12.2 Approximate computation of the marginal likelihood and of the log-Bayes factor", " 12 Bayesian model comparison using Bayes factors and the BIC 12.1 The Bayes factor We would like to compare two models \\(M_1\\) and \\(M_2\\). Before seeing data \\(D\\) we can check their Prior odds (= ratio of prior probabilities of the models \\(M_1\\) and \\(M_2\\)): \\[\\frac{\\text{Pr}(M_1)}{\\text{Pr}(M_2)}\\] After seeing data \\(D\\) we arrive at the Posterior odds (= ratio of posterior probabilities): \\[\\frac{\\text{Pr}(M_1 | D)}{\\text{Pr}(M_2 | D)}\\] Using Bayes Theorem \\(\\text{Pr}(M_i | D) = \\text{Pr}(M_i) \\frac{p(D | M_i) }{p(D)}\\) we can rewrite the posterior odds as \\[ \\underbrace{\\frac{\\text{Pr}(M_1 | D)}{\\text{Pr}(M_2 | D)}}_{\\text{posterior odds}} = \\underbrace{\\frac{p(D | M_1)}{p(D | M_2)}}_{\\text{Bayes factor $B_{12}$}} \\, \\underbrace{\\frac{\\text{Pr}(M_1)}{\\text{Pr}(M_2)}}_{\\text{prior odds}} \\] The Bayes factor is the multiplicative factor that updates the prior odds to the posterior odds, and is the ratio of the (marginal) likelihoods of the two models: \\[ B_{12} = \\frac{p(D | M_1)}{p(D | M_2)} \\] The log-Bayes factor \\(\\log B_{12}\\) is also called the weight of evidence for \\(M_1\\) over \\(M_2\\). Therefore, we see that \\[ \\text{log-posterior odds = weight of evidence + log-prior odds} \\] 12.1.1 Connection with relative entropy The expected weight of evidence, with expectation taken with regard to one of the two models, is in fact the KL divergence between the two models (plus a minus sign depending on direction): \\[\\text{E}_{M_1}( \\log B_{12} ) = KL(M_1 || M_2)\\] \\[\\text{E}_{M_2}( \\log B_{12} ) = -\\text{E}_{M_2}( \\log B_{21} ) = -KL(M_2 || M_1)\\] 12.1.2 Interpretation of and scale for Bayes factor Following Harold Jeffreys (1961) one may interpret the strength of the Bayes factor as follows: \\(B_{12}\\) \\(\\log B_{12}\\) evidence in favour of \\(M_1\\) versus \\(M_2\\) &gt; 100 &gt; 4.6 decisive 10 to 100 2.3 to 4.6 strong 3.2 to 10 1.16 to 2.3 substantial 1 to 3.2 0 to 1.16 not worth more than a bare mention More recently, Kass and Raftery (1995) proposed to use the following slightly modified scale: \\(B_{12}\\) \\(\\log B_{12}\\) evidence in favour of \\(M_1\\) versus \\(M_2\\) &gt; 150 &gt; 5 very strong 20 to 150 3 to 5 strong 3 to 20 1 to 3 positive 1 to 3 0 to 1 not worth more than a bare mention 12.1.3 Computing \\(p(D | M)\\) for simple and composite models In the Bayes factor we need to compute \\(p(D | M)\\), and it turns out that this is different for simple and composite models. A model is called “simple” if it directly corresponds to a specific distribution, say, a Normal with fixed mean and variance, or a Binomial distribution with a set probability for the two classes. Thus, a simple model is a point in the model space described by the parameters of a distribution family (e.g. \\(\\mu\\) and \\(\\sigma^2\\) for the normal family \\(N(\\mu, \\sigma^2\\)). For a simple model \\(M\\) the density \\(p(D | M)\\) corresponds to standard likelihood of \\(M\\). On the other hand, a model is “composite” if it is composed of simple models. This can be a finite set, or it can be comprised of infinite number of models. For example, a Normal with a given mean but unspecified variance, or a Binomial model with unspecified parameter \\(p\\), is a composite model. If \\(M\\) is a composite model, with the underlying simple models indexed by a parameter \\(\\theta\\), the probability of the data given the model is obtained by marginalisation over \\(\\theta\\): \\[ \\begin{split} p(D | M) &amp;= \\int_{\\theta} p(D | \\theta, M) p(\\theta| M) d\\theta\\\\ &amp;= \\int_{\\theta} p(D , \\theta | M) d\\theta\\\\ \\end{split} \\] i.e. we integrate over all parameter values \\(\\theta\\). The resulting probability is called the marginal likelihood of the model \\(M\\). Note the marginal likelihood appears also in the denominator of Bayes formula! The marginal distribution for \\(D\\) is also called the prior predictive distribution given \\(M\\). If the distribution over \\(\\theta\\) is strongly concentrated around a specific value then the composite model degenerates to a simple point model. A worked example (in the form of the Beta-Binomial distribution) is discussed in more detail in the Worksheet 6, Question 3. 12.1.4 Bayes factor versus likelihood ratio If both \\(M_1\\) and \\(M_2\\) are simple models then the Bayes factor is identical to the likelihood ratio of the two models. However, if one of the two models is composite then the Bayes factor and the generalised likelihood ratio differ: In the Bayes factor the representative of a composite model is the model average of the simple models indexed by \\(\\theta\\), with weights taken from the prior distribution over the simple models contained in \\(M\\). In contrast, in contrast in the generalised likelihood ratio statistic the representative of a composite model is chosen by maximisation! Thus, for composite models, the Bayes factor does not equal the corresponding generalised likelihood ratio statistic. As we will see next when studying the BIC approximation, the key difference is that the Bayes factor takes into account the dimension of the composite models. 12.2 Approximate computation of the marginal likelihood and of the log-Bayes factor The marginal likelihood and the Bayes factor can be difficult to compute in practise. Therefore, a number of approximations for Bayesian modeling and model selection have been developed The most important is the so-called BIC approximation. 12.2.1 Schwarz (1978) approximation of log-marginal likelihood The logarithm of the marginal likelihood of a model can be approximated using the so-called BIC approximation (Schwarz 1978) as follow: \\[ \\log p(D | M) \\approx l_n^M(\\hat{\\boldsymbol \\theta}_{ML}^{M}) - \\frac{1}{2} d_M \\log n \\] where \\(d_M\\) is the dimension of the model \\(M\\) (number of parameters in \\(\\boldsymbol \\theta\\) belonging to \\(M\\)) and \\(n\\) is the sample size and \\(\\hat{\\boldsymbol \\theta}_{ML}^{M}\\) is the MLE. For a simple model \\(d_M=0\\) so then there is no approximation as in this case the marginal likelihood equals the likelihood. The above formula can be obtained by quadratic approximation of the likelihood assuming large \\(n\\) and that the prior is uniform around the MLE. Note that the approximation is the maximum log-likelihood minus a penalty that depends on the model complexity (as measured by dimension \\(d\\)), thus this is an example of penalised ML! Also note that the distribution over the parameter \\(\\theta\\) is not required in the approximation. 12.2.2 Bayesian information criterion (BIC) The BIC (Bayesian information criterion) of the model \\(M\\) is the approximated log-marginal likelihood times the factor -2: \\[ BIC(M) = -2 l_n^M(\\hat{\\boldsymbol \\theta}_{ML}^{M}) + d_M \\log n \\] Thus, when comparing models one aimes to maximise the marginal likelihood or, as approximation, minimise the BIC. The reason for the factor “-2” is simply to have a quantity that is on the same scale as the Wilks log likelihood ratio. Some people / software packages also use the factor “2”. 12.2.3 Approximating the weight of evidence (log-Bayes factor) with BIC Using BIC (twice) the log-Bayes factor can be approximated as \\[ \\begin{split} 2 \\log B_{12} &amp;\\approx -BIC(M_1) + BIC(M_2) \\\\ &amp;=2 \\left( l_n^{M_{1}}(\\hat{\\boldsymbol \\theta}_{ML}^{M_{1}}) - l_n^{M_{2}}(\\hat{\\boldsymbol \\theta}_{ML}^{M_{2}}) \\right) - \\log(n) (d_{M_{1}}-d_{M_{2}}) \\\\ \\end{split} \\] i.e. it is the penalised log-likelihood ratio of model \\(M_1\\) vs. \\(M_2\\). 12.2.4 Model complexity and Occams razor As demonstrated above the averaging over \\(\\theta\\) in the marginal likelihood has the effect of automatically penalising complex models. Therefore, when comparing models using marginal likelihood, as in the Bayes factor, a complex model may be ranked below simpler models. In contrast, when selecting a model by maximum likelihood directly, without averaging, the model with the highest number of parameters always wins over simpler models. Thus, the penalisation implicit in the marginal likelihood is very much desired as it prevents the overfitting of maximum likelihood. The principle of preferring a less complex model is called “Occam’s razor”, and it is a natural propery of the Bayes factor. Note that when comparing models a simpler model is often preferably over a more complex model, because the simpler model is typically better suited to both explaining the currently observed data as well as future data, whereas a complex model will only excel in fitting the current data but will then perform poorly in prediction. "],["13-bayes7.html", "13 False discovery rates 13.1 General setup 13.2 Specificity and Sensitivity 13.3 FDR and FNDR 13.4 Bayesian perspective 13.5 Software", " 13 False discovery rates 13.1 General setup 13.1.1 Overview In this chapter we introduce False Discovery Rates (FDR) as a Bayesian method to distinguish a null model from an alternative model. This is closely linked with classical frequentist multiple testing procedures. 13.1.2 Choosing between \\(H_0\\) and \\(H_A\\) We consider two models: \\(H_0:\\) null model, with density \\(f_0(x)\\) and distribution \\(F_0(x)\\) \\(H_A:\\) alternative model, with density \\(f_A(x)\\) and distribution \\(F_A(x)\\) Aim: given observations \\(x_1, \\ldots, x_n\\) we would like to decide for each \\(x_i\\) whether it belongs to \\(H_0\\) or \\(H_A\\). This is done by a critical decision threshold \\(x_c\\): if \\(x_i &gt; x_c\\) then \\(x_i\\) is called “significant” and otherwise called “not significant”. In classical statistics one of the the most widely used approach to find the decision threshold is by computing \\(p\\)-values from the \\(x_i\\) (this uses only the null model but not the alternative model), and then thresholding the \\(p\\)-values a a certain level (say 5%). If \\(n\\) is large then often the test is modified by adjusting the \\(p\\)-values or the threshold (e.g. if Bonferroni correction). Note that this procedure ignores any information we may have about the alternative model! 13.1.3 True and false positives and negatives For any decision threshold \\(x_c\\) we can distinguish the following errors: False positives (FP), “false alarm”, type I error: \\(x_i\\) belongs to null but is called “significant” False negative (FN), “miss”, type II error: \\(x_i\\) belongs to alternative, but is called “not significant” In addition we have: True positives (TP), “hits”: belongs to alternative and is called “significant” True negatives (TN), “correct rejections”: belongs to null and is called “not significant” 13.2 Specificity and Sensitivity From counts of TP, TN, FN, FP we can derive further quantities: True Negative Rate TNR, specificity: \\(TNR= \\frac{TN}{TN+FP} = 1- FPR\\) with FPR=False Positive Rate = \\(1-\\alpha_I\\) True Positive Rate TPR, sensitivity, power, recall: \\(TPR= \\frac{TP}{TP+FN} = 1- FNR\\) with FNR=False negative rate = \\(1-\\alpha_{II}\\) Accuracy: \\(ACC = \\frac{TP+TN}{TP+TN+FP+FN}\\) Another common way to choose the decision threshold \\(x_d\\) in classical statistics is to balance sensitivity/power vs. specificity (maximising both power and specificity, or equivalently, minimising both false positive and false negative rates). ROC curves plot TPR/sensitivity vs. FPR = 1-specificity. 13.3 FDR and FNDR It is possible to link the above with the observed counts of TP, FP, TN, FN: False Discovery Rate (FDR): \\(FDR = \\frac{FP}{FP+TP}\\) False Nondiscovery Rate (FNDR): \\(FNDR = \\frac{FN}{TN+FN}\\) Positive predictive value (PPV), True Discovery Rate (TDR), precision: \\(PPV = \\frac{TP}{FP+TP} = 1-FDR\\) Negative predictive value (NPV): \\(NPV = \\frac{TN}{TN+FN} = 1-FNDR\\) In order to choose the decision threshold it is natural to balance FDR and FDNR (or PPV and NPV), by minimising both FDR and FNDR or maximising both PPV and NPV. In machine learning it is common to use “precision-recall plots” that plot precision (=PPV, TDR) vs. recall (=power, sensitivity). 13.4 Bayesian perspective 13.4.1 Two component mixture model In the Bayesian perspective the problem of choosing the decision threshold is related to computing the posterior probability \\[\\text{Pr}(H_0 | x_i) , \\] i.e. probability of the null model given the observation \\(x_i\\), or equivalently computing \\[\\text{Pr}(H_A | x_i) = 1- \\text{Pr}(H_0 | x_i)\\] the probability of the alternative model given the observation \\(x_i\\). This is done by assuming a mixture model \\[ f(x) = \\pi_0 f_0(x) + (1-\\pi_0) f_A(x) \\] where \\(\\pi_0 = \\text{Pr}(H_0)\\) is the prior probability of \\(H_0\\) and. \\(\\pi_A = 1- \\pi_0 = \\text{Pr}(H_A)\\) the prior probabiltiy of \\(H_A\\). Note that the weights \\(\\pi_0\\) can in fact be estimated from the observations by fitting the mixture distribution to the observations \\(x_1, \\ldots, x_n\\) (which implies that this yields a form of empirical Bayes method). 13.4.2 Local FDR The posterior probability of the null model given a data point is then given by \\[\\text{Pr}(H_0 | x_i) = \\frac{\\pi_0 f_0(x_i)}{f(x_i)} = LFDR(x_i)\\] This quantity is also known as the local FDR or local False Discovery Rate. In the given one-sided setup the local FDR is large (close to 1) for small \\(x\\), and will become close to 0 for large \\(x\\). A common decision rule is given by thresholding local false discovery rates: if \\(LFDR(x_i) &lt; 0.1\\) the \\(x_i\\) is called significant. 13.4.3 q-values In correspondence to \\(p\\)-values one can also define tail-area based false discovery rates: \\[ Fdr(x_i) = \\text{Pr}(H_0 | X &gt; x_i) = \\frac{\\pi_0 F_0(x_i)}{F(x_i)} \\] These are called q-values, or simply False Discovery Rates (FDR). Intriguingly, these also have a frequentist interpretation as adjusted p-values (using a Benjamini-Hochberg adjustment procedure). 13.5 Software There are a number of R packages to compute (local) FDR values: For example: locfdr qvalue fdrtool and many more. Using FDR values for screening is especially useful in high-dimensional settings (e.g. when analysing genomic and other high-throughput data). FDR values have both a Bayesian as well as frequentist interpretation, providing further evidence that good classical statistical methods do have a Bayesian interpretation. "],["14-bayes8.html", "14 Optimality properties and summary 14.1 Bayesian statistics in a nutshell 14.2 Frequentist properties of Bayesian estimators 14.3 Specifying the prior — problem or advantage? 14.4 Choosing a prior 14.5 Optimality of Bayesian inference 14.6 Conclusion", " 14 Optimality properties and summary 14.1 Bayesian statistics in a nutshell Bayesian statistics explicitly models the uncertainty about the parameters of interests by probability In the light of new evidence (observed data) the uncertainty is updated, i.e. the prior distribution is combined with the likelihood to form the posterior distribution Example: Beta-Binomial model Binomial likelihood \\(n\\) observations: \\(x\\) “heads”, \\(n-x\\) “tails” Frequency \\(\\hat\\theta_{ML} = \\frac{x}{n}\\) Beta prior \\(\\theta \\sim \\text{Beta}(\\alpha_0, \\beta_0)\\) with mean \\(\\theta_0=\\frac{\\alpha_0}{m}\\) and \\(m=\\alpha_0+\\beta_0\\) Beta posterior \\(\\theta | x,n \\sim \\text{Beta}(\\alpha_1, \\beta_1)\\) with mean \\(\\theta_1=\\frac{\\alpha_1}{\\alpha_1+\\beta_1}\\) and \\(\\alpha_1 = \\alpha_0+x\\) and \\(\\beta_1=\\beta_0+n-x\\) Update of prior mean to posterior mean by shrinkage of MLE: \\[\\theta_1 = \\lambda \\theta_0 + (1-\\lambda) \\hat\\theta_{ML}\\] with shrinkage intensity \\(\\lambda=\\frac{m}{n+m}\\) \\(m\\) can be interpreted as prior sample size 14.1.1 Remarks If posterior in same family as prior \\(\\rightarrow\\) conjugate prior In the exponential family the Bayesian update of the mean is always expressible as linear shrinkage of the MLE For sample size \\(n \\rightarrow \\infty\\) then \\(\\lambda \\rightarrow 0\\) and \\(\\theta_1 \\rightarrow \\hat\\theta_{ML}\\) (for large samples posterior mean = maximum likelihood estimator) For \\(n \\rightarrow 0\\) then \\(\\lambda \\rightarrow 1\\) and \\(\\theta_1 \\rightarrow \\hat\\theta_0\\) (if no data is available fall back to prior) Note that the Bayesian estimator is biased for finite \\(n\\) by construction (but asymptotically unbiased like the MLE). 14.1.2 Advantages adding prior information has regularisation properties. This is very important in more complex models with many parameters, e.g., in estimation of a covariance matrix (to avoid singularity). improves small-sample accuracy (e.g. MSE) that Bayesian estimators tend to be better than MLE is not surprising - they use the data plus extra information! Bayesian credible intervals are conceptually much more simple than frequentist confidence intervals 14.2 Frequentist properties of Bayesian estimators A Bayesian point estimator (e.g. the posterior mean) can also be assessed by its frequentist properties. First, we know that, by construction, the Bayesian estimator \\(\\hat{p}_{\\text{Bayes}}\\) will be biased for finite \\(n\\) even if the MLE is unbiased (with the bias being the posterior mean in this case). Second, intriguingly it turns out that the sampling variance of the Bayes point estimator (not to be confused with the posterior variance!) can be smaller than the variance of the MLE. This depends on the choice of the shrinkage parameter \\(\\lambda\\) that also determines the posterior variance. As a result, Bayesian estimators may have smaller MSE (=squared bias + variance) than the ML estimator for finite \\(n\\). In statistical decision theory this is called the theorem of admissibility of Bayes rules. It states that under mild conditions every admissible estimation rule (i.e. one that dominates all other estimators with regard to some expected loss, such as the MSE) is in fact a Bayes estimator with some prior. Unfortunately, this theorem does not tell which prior is needed to achive optimality, however an optimal estimator with minimum MSE can often be found by tuning \\(\\lambda\\). 14.3 Specifying the prior — problem or advantage? In Bayesian statistics the analysist needs to be very explicit about the modeling assumptions: Model = data generating process (likelihood) + prior uncertainty (prior distribution) Note that alternative statistical methods can often be interpreted as Bayesian methods assuming a specific implicit prior! For example, likelihood estimation for the Binomial model is equivalent to Bayes estimation using the Beta-Binomial model with a \\(\\text{Beta}(0,0)\\) prior (=Haldane prior). However, when choosing a prior explicitly for this model, interestingly most analysts would rather use a flat prior \\(\\text{Beta}(1,1)\\) (=Laplace prior) with implicit sample size \\(m=2\\) or a transformation-invariant prior \\(\\text{Beta}(1/2, 1/2)\\) (=Jeffreys prior) with implicit sample size \\(m=1\\) than the Haldane prior! \\(\\rightarrow\\) be aware about the implicit priors!! Better to acknowledge that a prior is being used (even if implicit!) Writing down all your assumptions is enforced by the Bayesian approach. Specifying a prior is thus best understood as an intrinsic part of model specification. It helps to improve inference and it may only be ignored if there is lots of data. 14.4 Choosing a prior It is essential in a Bayesian analysis to specify your prior uncertainty about the model parameters. Note that this is simply part of the modeling process! Typically, the location of the prior determines the amount of bias, and the precision (inverse variance) of the prior is proportional to the implied sample size of the prior. As we have seen before for large \\(n\\) the Bayesian estimate converges to the ML estimate, so for large \\(n\\) you may ignore specifying a prior. However, for small \\(n\\) it is essential that a prior is specified. In non-Bayesian approaches (if interpreted from Bayesian perspective) this prior is still there but it is implicit (e.g. uniform prior for likelihood estimation). 14.4.1 Some guidelines So the questions remains what are good ways to choose a prior? Two useful ways (among many others) are: Use a weakly informative prior (cf. Gelman). This means that you have a vague idea about the suitable values of the parameter of interest, and you use a corresponding prior (with moderate variance) to model the uncertainty. This acknowledges that there are no uninformative priors and aims to ensure that the prior will not dominate the likelihood. Empirical Bayes methods can often be used to determine one or all of the hyperparameters (i.e. the parameters in the prior). There are several ways to do this, one of them is to tune the shrinkage parameter \\(\\lambda\\) to achieve minimum MSE. We discuss this further below. In contrast, there also exists many proposals advocating to select so-called “uninformative priors”. However, as it is easly shown, there are no true unformative priors, since a prior that looks uninformative (i.e. “flat”) in one coordinate system can be informative in another — this is a simple consequence of the rule for transformation of probability densities. Furthermore, often these priors are improper, i.e. are not actually probability distributions. For this (and many other reasons) the search for “uniformative” priors is not just futile but in fact also undesirable (e.g. the prior typically also needs to act as regulariser)! Instead, specifying the prior needs to be viewed as part of the modelling process, with specification of the prior as integral as the specification of the likelihood. 14.4.2 Jeffreys prior In order to complement the discussion on non-informative priors we now look (briefly) at the proposal by Jeffreys (1946). Specifically, this prior is constructed from the expected Fisher observation using the log-likelihood function and thus promises automatic construction of objective uninformative priors: \\[ p(\\boldsymbol \\theta) \\propto \\sqrt{\\det \\boldsymbol I^{\\text{Fisher}}(\\boldsymbol \\theta)} \\] The reasoning underlying this prior is invariance against transformation of the coordinate system of the parameters. For the Beta-Binomial model the Jeffreys prior corresponds to \\(\\text{Beta}(\\frac{1}{2}, \\frac{1}{2})\\). For the Normal-Normal model it corresponds to the flat improper prior \\(p(\\mu) =1\\). For the Inverse-Gamma-Norma model the Jeffreys prior is the improper prior \\(p(\\sigma^2) = \\frac{1}{\\sigma^2}\\). This already illustrates the main problem with this type of prior – namely that it often is an improper prior. Another issue is that Jeffreys priors are usually not conjugate which complicates the update from the prior to the posterior. An alternative to Jeffreys prior is the reference prior developed by Bernardo (1979). 14.5 Optimality of Bayesian inference The optimality of Bayesian model making use of full model specification (likelihood plus prior) can be shown from a number of different perspectives. Correspondingly, there are many theorems that prove (or at least indicate) this optimality: Richard Cox’s theorem: the aim to generalise classic logic inevitably leads to Bayesian inference. Entropy perspective: Bayesian inference is a consequence of minimal information update where new information arrives in form of observations de Finetti’s representation theorem: joint distribution of exchangeable sequences can be viewed as posterior distributions computed by Bayes theorem) Frequentist decision theory: all admissible decision rules are Bayes rules! (admissible = always better than all other methods!) Remark: the above also excludes a few other (somewhat esoteric) suggestions for propagating uncertainty (e.g. Fuzzy Logic, imprecise probabilties, etc). 14.6 Conclusion Bayesian statistics offers a coherent framework for statistical learning from data, with methods for estimation testing model building There are a number of theorems that show that “optimal” estimators (defined in various ways) are all Bayesian. It is conceptually very simple — but can be computationally very involved! It provides a coherent generalisation of classical TRUE/FALSE logic (and therefore does not suffer from some of the inconsistencies prevalent in frequentist statistics). Bayesian statistics is a non-asymptotic theory, it works for any sample size. Asympotically (large \\(n\\)) it is consistent and converges to the true model (like ML!). But Bayesian reasoning can also be applied to events that take place only once — no assumption of hypothetical infinitely many repetitions as in frequentist statistics is needed. Moreover, many classical (frequentist) procedures may be viewed as approximations to Bayesian methods and estimators, so using classical approaches in the correct application domain is perfectly in line with the Bayesian framework. Bayesian estimation and inference also automatically regularises (via the prior) which is important for complex models and when there is the problem of overfitting. 14.6.1 Current directions of research For example: connection between Bayesian models and algorithmic models widely used in machine learning (such as neural networks, deep learning, convolutional networks, ensemble methods, XGBoost etc). Are these models optimal (as in the Bayesian sense)? Can we learn something about highly complex, non-parametric statistical models? How do we do effective Bayesian learning for these parameter-rich models? Both in terms of computational and statistical efficiency. "],["15-regression1.html", "15 Overview over regression modelling 15.1 General setup 15.2 Objectives 15.3 Regression as a form of supervised learning 15.4 Various regression models used in statistics", " 15 Overview over regression modelling 15.1 General setup \\(y\\): response variable, also known as outcome or label \\(x_1, x_2, x_3, \\ldots, x_d\\): predictor variables, also known as covariates or covariables The relationship between the outcomes and the predictor variables is assumed to follow \\[ y = f(x_1,x_2,\\dots,x_d) + \\varepsilon \\] where \\(f\\) is the regression function (not a density) and \\(\\varepsilon\\) represents noise. 15.2 Objectives Understand the relationship between the response \\(y\\) and the predictor variables \\(x_i\\) by learning the regression function \\(f\\) from observed data (training data). The estimated regression function is \\(\\hat{f}\\). Prediction of outcomes \\[\\underbrace{\\hat{y}}_{\\substack{\\text{predicted response} \\\\ \\text{using fitted $\\hat{f}$}}} = \\hat{f}(x_1,x_2,\\dots,x_d)\\] If instead of the fitted function \\(\\hat{f}\\) the known regression function \\(f\\) is used we denote this by \\[\\underbrace{y^{\\star}}_{\\substack{\\text{predicted response} \\\\ \\text{using known $f$}}} = f(x_1,x_2,\\dots,x_d) \\] Variable importance which covariates are most relevant in predicting the outcome? allows to better understand the data and model \\(\\rightarrow\\) variable selection (to build simpler model with same predictive capability) 15.3 Regression as a form of supervised learning Regression modeling is a special case of supervised learning. In supervised learning we make use of labeled data, i.e. \\(\\boldsymbol x_i\\) has an associated label \\(y_i\\). Thus, the data is consists of pairs \\((\\boldsymbol x_1, y_1),(\\boldsymbol x_2 ,y_2),\\dots,(\\boldsymbol x_n ,y_n)\\). The supervision part of in supervised learning refers to the fact that the labels are given. In regression typically the label \\(y_i\\) is continuous and called the response. On the other hand, if the label \\(y_i\\) is discrete/categorical then supervised learning is called classification. \\[\\begin{align*} \\begin{array}{ll} \\\\ $$\\text{Supervised Learning}$$\\\\ \\\\ \\end{array} \\begin{array}{ll} $$\\longrightarrow \\text{Discrete } y$$\\\\ \\\\ $$\\longrightarrow \\text{Continuous } y$$\\\\ \\end{array} \\begin{array}{ll} $$\\longrightarrow \\text{Classification Methods}$$\\\\ \\\\ $$\\longrightarrow \\text{Regression Methods}$$\\\\ \\end{array} \\end{align*}\\] Another important type of statistical learning is unsupervised learning where labels \\(y\\) are inferred from the data \\(\\boldsymbol x\\) (this is also known as clustering). Furthermore, there is also semi-supervised learning with labels only partly known. Note that there are regression models (e.g. logistic regression) with discrete response that are performing classification, so one may argue that “supervised learning”=“generalised regression”. 15.4 Various regression models used in statistics In this course we only study linear multiple regression. However, you should be aware that the linear model is in fact just a special cases of some much more general regression approaches. General regression model: \\[y = f(x_1,\\dots,x_d) + \\text{&quot;noise&quot;}\\] The function \\(f\\) is estimated nonparametrically - splines - Gaussian processes Generalised Additive Models (GAM): - the function \\(f\\) is assumed to be the sum of individual functions \\(f_i(x_i)\\) Generalised Linear Models (GLM): - \\(f\\) is a transformed linear predictor \\(h(\\sum b_i x_i)\\), noise is assumed from exponential family Linear Model (LM): - linear predictor \\(\\sum b_i x_i\\), normal noise In R the linear model is implemented in the function lm(), and generalised linear models in the function glm(). Generalised additive models are available in the package “mgcv”. In the following we focus on the linear regression model with continuous response. "],["16-regression2.html", "16 Linear Regression 16.1 The linear regression model 16.2 Interpretation of regression coefficients and intercept 16.3 Different types of linear regression: 16.4 Distributional assumptions and properties 16.5 Regression in data matrix notation 16.6 Centering and vanishing of the intercept \\(\\beta_0\\) 16.7 Regression objectives for linear model", " 16 Linear Regression 16.1 The linear regression model In this module we assume that \\(f\\) is a linear function: \\[f(x_1, \\ldots, x_d) = \\beta_0 + \\sum^{d}_{j=1} \\beta_j x_j = y^{\\star}\\] In vector notation: \\[ f(\\boldsymbol x) = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x= y^{\\star} \\] with \\(\\boldsymbol \\beta=\\begin{pmatrix} \\beta_1 \\\\ \\vdots \\\\ \\beta_d \\end{pmatrix}\\) and \\(\\boldsymbol x=\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_d \\end{pmatrix}\\) Therefore, the linear regression model is \\[ \\begin{split} y &amp;= \\beta_0 + \\sum^{d}_{j=1} \\beta_j x_j + \\varepsilon\\\\ &amp;= \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x+\\varepsilon \\\\ &amp;= y^{\\star} +\\varepsilon \\end{split} \\] where: \\(\\beta_0\\) is the intercept \\(\\boldsymbol \\beta= (\\beta_1,\\ldots,\\beta_d)^T\\) are the regression coefficients \\(\\boldsymbol x\\) is the predictor vector 16.2 Interpretation of regression coefficients and intercept The regression coefficient \\(\\beta_i\\) corresponds to the slope (first partial derivative) of the regression function in the direction of \\(x_i\\). In other words, the gradient of \\(f(\\boldsymbol x)\\) are the regression coefficients: \\(\\nabla f(\\boldsymbol x) = \\boldsymbol \\beta\\) The intercept \\(\\beta_0\\) is the offset at the origin (\\(x_1=x_2=\\ldots=x_d=0\\)): 16.3 Different types of linear regression: Simple linear regression: \\(y=\\beta_0 + \\beta x + \\epsilon\\) (=single predictor) Multiple linear regression: \\(y =\\beta_0 + \\sum^{d}_{j=1} \\beta_j x_j + \\epsilon\\) (= multiple predictor variables) Multivariate regression: multivariate response \\(\\boldsymbol y\\) 16.4 Distributional assumptions and properties General assumptions: In our application we treat \\(y\\) and \\(x_1, \\ldots, x_d\\) as observables that can be described by random variables. \\(\\beta_0, \\boldsymbol \\beta\\) are parameters to be inferred from the observations on \\(y\\) and \\(x_1, \\ldots,x_d\\). Specifically, will we assume that response and predictors have a mean and a (cov)variance: Response: \\(\\text{E}(y) = \\mu_y\\) \\(\\text{Var}(y) = \\sigma_y^2\\) The variance of the response \\(\\text{Var}(y)\\) is also called the total variation . Predictors: \\(\\text{E}(x_i) = \\mu_{x_i}\\) (or \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu_{\\boldsymbol x}\\)) \\(\\text{Var}(x_i) = \\sigma^2_{x_i}\\) and \\(\\text{Cor}(x_i, x_j) = \\rho_{ij}\\) (or \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma_{\\boldsymbol x}\\)) The signal variance \\(\\text{Var}(y^{\\star})=\\text{Var}(\\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x) = \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol \\beta\\) is also called the explained variation. The noise \\(\\varepsilon\\) is also an observable, but typically only indirectly via the difference \\(\\varepsilon = y - y^{\\star}\\). We denote the mean and variance of the noise by \\(\\text{E}(\\varepsilon)\\) and \\(\\text{Var}(\\varepsilon)\\). The noise variance \\(\\text{Var}(\\varepsilon)\\) is also called the unexplained variation. We assume that \\(y\\) and \\(\\boldsymbol x\\) are jointly distributed with some correlation \\(\\text{Cor}(y, x_j) = \\rho_{y,x_{j}}\\) between each predictor variable \\(x_j\\) and the response \\(y\\). Identifiability assumptions: In a statistical analysis we would like to be able to separate signal (\\(y^{\\star}\\)) from noise (\\(\\varepsilon\\)). To achieve this we require some distributional assumptions to ensure identifiability and avoid confounding: Assumption 1: \\(\\varepsilon\\) and \\(y^{\\star}\\) are are independent. This implies \\(\\text{Var}(y) = \\text{Var}(y^{\\star}) + \\text{Var}(\\varepsilon)\\), or equivalently \\(\\text{Var}(\\varepsilon) = \\text{Var}(y) - \\text{Var}(y^{\\star})\\). Thus, this assumption implies the decomposition of variance, i.e. that the total variation \\(\\text{Var}(y)\\) equals the sum of the explained variation\\(\\text{Var}(y^{\\star})\\) and the unexplained variation\\(\\text{Var}(\\varepsilon)\\). Assumption 2: \\(\\text{E}(\\varepsilon)=0\\). This allows to identify the intercept \\(\\beta_0\\) and implies \\(\\text{E}(y) = \\text{E}(y^{\\star})\\). Optional assumptions (often but not always): The noise \\(\\varepsilon\\) is normally distributed The response \\(y\\) and and the predictor variables \\(x_i\\) are continuous variables The response and predictor variables are jointly normally distributed Further properties: As a result of the independence assumption 1) we can only choose two out of the three variances freely: in a generative perspective we will choose signal variance \\(\\text{Var}(y^{\\star})\\) (or equivalently the variances \\(\\text{Var}(x_j)\\)) and the noise variance \\(\\text{Var}(\\varepsilon)\\), then the variance of the response \\(\\text{Var}(y)\\) follows. in an observational perspective we will observe the variance of the reponse \\(\\text{Var}(y)\\) and the variances \\(\\text{Var}(x_j)\\), and then the error variance \\(\\text{Var}(\\varepsilon)\\) follows. As we will see later the regression coefficients \\(\\beta_j\\) depend on the correlations between the response \\(y\\) and and the predictor variables \\(x_j\\). Thus, the choice of regression coefficients implies a specific correlation pattern, and vice vera (in fact, we will use this correlation pattern to infer the regression coefficient from data!). 16.5 Regression in data matrix notation We can also write the regression in terms of actual observed data (rather than random variables): Data matrix for the predictors: \\[\\boldsymbol X= \\begin{pmatrix} x_{11} &amp; \\dots &amp; x_{1d} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; \\dots &amp; x_{nd} \\end{pmatrix}\\] Note the statistics convention: the \\(n\\) rows of \\(\\boldsymbol X\\) contain the samples, and the \\(d\\) columns contain variables. Response data vector: \\((y_1,\\dots,y_n)^T = \\boldsymbol y\\) Then the regression equation is written in data matrix notation: \\[\\underbrace{\\boldsymbol y}_{n\\times 1} = \\underbrace{\\boldsymbol 1_n \\beta_0}_{n\\times 1} + \\underbrace{\\boldsymbol X}_{n \\times d} \\underbrace{\\boldsymbol \\beta}_{d\\times 1}+\\underbrace{\\boldsymbol \\varepsilon}_{\\underbrace{n\\times 1}_{\\text{residuals}}}\\] where \\(\\boldsymbol 1_n = \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\\) is a column vector of length \\(n\\) (size \\(n \\times 1\\)). Note that here the regression coefficients are now multiplied after the data matrix (compare with the original vector notation where the transpose of regression coefficients come before the vector of the predictors). The observed noise values (i.e. realisations of \\(\\varepsilon\\)) are called the residuals. 16.6 Centering and vanishing of the intercept \\(\\beta_0\\) If \\(\\boldsymbol x\\) and \\(y\\) are centered, i.e. \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu_{\\boldsymbol x}= 0\\) and \\(\\text{E}(y) = \\mu_{y} = 0\\) then the intercept \\(\\beta_0\\) disappears: The regression equation is \\[y=\\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x+\\varepsilon\\] with \\(E(\\varepsilon)\\). Taking the expectation on both sides we get \\(\\mu_{y} = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x}\\) and therefore \\[ \\beta_0 = \\mu_{y}- \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x} \\] This is equal to zero if the means of the response and predictors vanish. Conversely, if we assume that the intercept vanishes (\\(\\beta_0=0\\)) this is only possible for general \\(\\boldsymbol \\beta\\) if both \\(\\boldsymbol \\mu_{\\boldsymbol x}=0\\) and \\(\\mu_{y}=0\\). Thus, in the linear model is always possible to transform \\(y\\) and \\(\\boldsymbol x\\) (or data \\(\\boldsymbol y\\) and \\(\\boldsymbol X\\)) so that the intercept vanishes! \\(\\Rightarrow\\) we will therefore often set \\(\\beta_0=0\\). 16.7 Regression objectives for linear model Understand functional relationship: find estimates of intercept (\\(\\hat{\\beta}_0\\)) and regression coefficients (\\(\\hat{\\beta}_j\\)) Prediction: Known coefficients \\(\\beta_0\\) and \\(\\boldsymbol \\beta\\): \\(y^{\\star} = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x\\) Estimated coefficients \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}\\) (note the “hat”!): \\(\\hat{y} =\\hat{\\beta}_0 + \\sum^{d}_{j=1} \\hat{\\beta}_j x_j = \\hat{\\beta}_0 + \\hat{\\boldsymbol \\beta}^T \\boldsymbol x\\) Also find the corresponding prediction errors! Variable importance: Which predictors \\(x_j\\) are most relevant? \\(\\rightarrow\\) test whether \\(\\beta_j=0\\) \\(\\rightarrow\\) find measures of variable importance Remark: as we will see \\(\\beta_j\\) or \\(\\hat{\\beta}_j\\) itself is not a measure of importance! "],["17-regression3.html", "17 Estimating regression coefficients 17.1 Ordinary Least Squares (OLS) estimator of regression coefficients 17.2 Maximum likelihood estimation of regression coefficients 17.3 Covariance plug-in estimator of regression coefficients 17.4 Best linear predictor 17.5 Regression by conditioning 17.6 Standardised regression coefficients and relationship to correlation", " 17 Estimating regression coefficients In this chapter we discuss various ways to estimate the regression coefficients. First, we discuss estimation by Ordinary Least Squares (OLS) by minimising the residual sum of squares. This yields the famous Gauss estimator. Second, we derive estimates of the regression coefficients using the methods of maximum likelihood assuming normal errors. This also leads to the Gauss estimator. Third, we show that the coefficients in linear regression can written and interpreted in terms of two covariance matrices and that the Gauss estimator of the regression coefficients is a plug-in estimator using the MLEs of these covariance matrices. Furthermore, we show that the (population version) of the Gauss estimator can also be derived by finding the best linear predictor and by conditioning. Finally, we discuss special cases of regression coefficients and their relationship to marginal correlation. 17.1 Ordinary Least Squares (OLS) estimator of regression coefficients Now we show the classic way (Gauss 1809; Legendre 1805) to estimate regression coefficients by the method of ordinary least squares (OLS). Idea: choose regression coefficients such as to minimise the squared error between observations and the prediction (=RSS, the Residual Sum of Squares, a function of \\(\\boldsymbol \\beta\\)) In data matrix notation (note we assume \\(\\beta_0=0\\) and thus centered data \\(\\boldsymbol X\\) and \\(\\boldsymbol y\\)): \\[\\text{RSS}(\\boldsymbol \\beta)=(\\boldsymbol y-\\boldsymbol X\\boldsymbol \\beta)^T(\\boldsymbol y-\\boldsymbol X\\boldsymbol \\beta)=\\boldsymbol \\varepsilon^T\\boldsymbol \\varepsilon= \\sum^{n}_{i=1}\\epsilon^2_i\\] \\[\\widehat{\\boldsymbol \\beta}_{\\text{OLS}}=\\underset{\\boldsymbol \\beta}{\\arg \\min} \\text{RSS}(\\boldsymbol \\beta)\\] \\[\\text{RSS}(\\boldsymbol \\beta) = \\boldsymbol y^T \\boldsymbol y- 2 \\boldsymbol \\beta^T \\boldsymbol X^T \\boldsymbol y+ \\boldsymbol \\beta^T \\boldsymbol X^T \\boldsymbol X\\boldsymbol \\beta\\] Gradient: \\[\\nabla \\text{RSS}(\\boldsymbol \\beta) = -2\\boldsymbol X^T \\boldsymbol y+ 2\\boldsymbol X^T \\boldsymbol X\\boldsymbol \\beta\\] \\[\\nabla \\text{RSS}(\\widehat{\\boldsymbol \\beta}) = 0 \\longrightarrow \\boldsymbol X^T \\boldsymbol y= \\boldsymbol X^T\\boldsymbol X\\widehat{\\boldsymbol \\beta}\\] \\[\\Longrightarrow \\widehat{\\boldsymbol \\beta}_{\\text{OLS}} = \\left(\\boldsymbol X^T\\boldsymbol X\\right)^{-1} \\boldsymbol X^T \\boldsymbol y\\] Note the similarities in the procedure to maximum likelihood (ML) estimation (with minimisation instead of maximisation)! In fact, as we see next this is not by chance as OLS is indeed a special case of ML! This also implies that OLS is generally a good method — but only if sample size \\(n\\) is large! The above Gauss’ estimator is fundamental in statistics so it is worthwile to memorise it! 17.2 Maximum likelihood estimation of regression coefficients We now show how to estimate regression coefficients using the method of maximum likelihood. This is a second method to derive \\(\\hat{\\boldsymbol \\beta}\\). We recall the basic regression equation \\[ y = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x+ \\epsilon \\] with \\(\\text{E}(\\varepsilon)=0\\) and observed data \\(y_1, \\ldots, y_n\\) and \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\). The intercept is identified as \\[ \\beta_0 = \\mu_{y}- \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x} \\] so that we can solve for the noise variable \\[ \\varepsilon = (y- \\mu_{y}) - \\boldsymbol \\beta^T (\\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x}) \\] Assuming joint (multivariate) normality for the response \\(y\\) and \\(\\boldsymbol x\\) we get as the MLEs for the respective means and (co)variances: \\(\\hat{\\mu}_y=\\hat{\\text{E}}(y)= \\frac{1}{n}\\sum^n_{i=1} y_i\\) \\(\\hat{\\sigma}^2_y=\\widehat{\\text{Var}}(y)= \\frac{1}{n}\\sum^n_{i=1} (y_i - \\hat{\\mu}_y)^2\\) \\(\\hat{\\boldsymbol \\mu}_{\\boldsymbol x}=\\hat{\\text{E}}(\\boldsymbol x)= \\frac{1}{n}\\sum^n_{i=1} \\boldsymbol x_i\\) \\(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}=\\widehat{\\text{Var}}(\\boldsymbol x)= \\frac{1}{n}\\sum^n_{i=1} (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_{\\boldsymbol x}) (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_{\\boldsymbol x})^T\\) \\(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}=\\widehat{\\text{Cov}}(\\boldsymbol x, y)= \\frac{1}{n}\\sum^n_{i=1} (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_{\\boldsymbol x}) (y_i - \\hat{\\mu}_y)\\) The noise \\(\\varepsilon_i \\sim N(0, \\sigma^2_{\\varepsilon})\\) is normally distributed with mean 0 and variance \\(\\text{Var}(\\varepsilon) = \\sigma^2_{\\varepsilon}\\) which leads to the normal log-likelihood function: \\[ \\begin{split} \\log L &amp;= -\\frac{n}{2} \\log \\sigma^2_{\\varepsilon} - \\frac{1}{2\\sigma^2_{\\varepsilon}}{\\sum^n_{i=1} \\epsilon_i^2}\\\\ &amp;= -\\frac{n}{2} \\log \\sigma^2_{\\varepsilon} - \\frac{1}{2\\sigma^2_{\\varepsilon}}\\underbrace{\\sum^n_{i=1} \\left((y_i- \\hat{\\mu}_{y}) - \\boldsymbol \\beta^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_{\\boldsymbol x})\\right)^2}_{\\text{ = RSS($\\boldsymbol \\beta$)}}\\\\ \\end{split} \\] We now only need to maximise the log-likelihood to obtain MLEs of \\(\\sigma^2_{\\varepsilon}\\) and \\(\\boldsymbol \\beta\\)! Note that the residual sum of squares (RSS) appears in the log-likelihood function (with a minus sign), which implies that ML assuming normal distribution will recover the OLS estimator for the regression coefficients! So OLS is a special case of ML ! 17.2.1 Detailed derivation of the MLEs The gradient with regard to \\(\\boldsymbol \\beta\\) is \\[ \\begin{split} \\nabla_{\\boldsymbol \\beta} \\log L &amp;= \\frac{1}{\\sigma^2_{\\varepsilon}} \\sum^n_{i=1} \\left((\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_{\\boldsymbol x} ) (y_i - \\hat{\\mu}_{y}) - (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_{\\boldsymbol x} )(\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_{\\boldsymbol x})^T \\boldsymbol \\beta\\right) \\\\ &amp;= \\frac{n}{\\sigma^2_{\\varepsilon}} \\left( \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy} - \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}\\boldsymbol \\beta\\right)\\\\ \\end{split} \\] Setting this equal to zero yields the Gauss estimator \\[ \\hat{\\boldsymbol \\beta} = \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy} \\] By plugin we the get the MLE of \\(\\beta_0\\) as \\[ \\hat{\\beta}_0 = \\hat{\\mu}_{y}- \\hat{\\boldsymbol \\beta}^T \\hat{\\boldsymbol \\mu}_{\\boldsymbol x} \\] Taking the derivate of \\(\\log L\\) with regard to \\(\\sigma^2_{\\varepsilon}\\) results in \\[ \\frac{\\partial}{\\partial \\sigma^2_{\\varepsilon}} \\log L = -\\frac{n}{2\\sigma^2_{\\varepsilon}} +\\frac{1}{2\\sigma^4_{\\varepsilon}} RSS(\\beta) \\] which leads to the MLE \\[ \\hat{\\sigma^2}_{\\varepsilon} =\\frac{RSS(\\hat{\\boldsymbol \\beta})}{n} = \\frac{1}{n}\\sum^n_{i=1}(y_i-\\hat{y}_i)^2 \\] with \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\boldsymbol \\beta}^T \\boldsymbol x_i\\). Note that the MLE \\(\\widehat{\\sigma^2}_{\\varepsilon}\\) is a biased estimate of \\(\\sigma^2_{\\varepsilon}\\). The unbiased estimate is \\(\\frac{1}{n-d-1}\\sum^n_{i=1}(y_i-\\hat{y}_i)^2\\), where \\(d\\) is the dimension of \\(\\boldsymbol \\beta\\) (i.e. the number of predictors). 17.2.2 Asymptotics The advantage of using maximum likelihood is that we also get the (asympotic) variance associated with each estimator and typically can also assume asymptotic normality. Specifically, for \\(\\hat{\\boldsymbol \\beta}\\) we get via the observed Fisher information at the MLE an asymptotic estimator of its variance \\[\\widehat{\\text{Var}}(\\widehat{\\boldsymbol \\beta})=\\frac{1}{n} \\hat{\\sigma^2}_{\\varepsilon}\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1}\\] Similarly, for \\(\\hat{\\beta}_0\\) we have \\[ \\widehat{\\text{Var}}(\\widehat{\\beta}_0)=\\frac{1}{n} \\hat{\\sigma^2}_{\\varepsilon} (1 + \\hat{\\boldsymbol \\mu}^T \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1} \\hat{\\boldsymbol \\mu}) \\] For finite sample size \\(n\\) with known \\(\\text{Var}(\\varepsilon)\\) one can show that the variances are \\[\\text{Var}(\\widehat{\\boldsymbol \\beta})=\\frac{1}{n} \\sigma^2_{\\varepsilon}\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1}\\] and \\[ \\text{Var}(\\widehat{\\beta}_0)=\\frac{1}{n} \\sigma^2_{\\varepsilon} (1 + \\hat{\\boldsymbol \\mu}^T_{\\boldsymbol x} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1} \\hat{\\boldsymbol \\mu}_{\\boldsymbol x}) \\] and that the regression coefficients and the intercept are normally distributed according to \\[ \\widehat{\\boldsymbol \\beta} \\sim N_d(\\boldsymbol \\beta, \\text{Var}(\\widehat{\\boldsymbol \\beta})) \\] and \\[ \\widehat{\\beta}_0 \\sim N(\\beta_0, \\text{Var}(\\widehat{\\beta}_0)) \\] We may use this to test whether whether \\(\\beta_j = 0\\) and \\(\\beta_0 = 0\\). 17.3 Covariance plug-in estimator of regression coefficients We now try to understand regression coefficients in terms of covariances (thus obtaining a third way to compute and estimate them). We recall that the Gauss regression coefficients are given by \\[\\widehat{\\boldsymbol \\beta} = \\left(\\boldsymbol X^T\\boldsymbol X\\right)^{-1}\\boldsymbol X^T \\boldsymbol y\\] where \\(\\boldsymbol X\\) is the \\(n \\times d\\) data matrix (in statistics convention) \\[\\boldsymbol X= \\begin{pmatrix} x_{11} &amp; \\dots &amp; x_{1d} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; \\dots &amp; x_{nd} \\end{pmatrix}\\] Note that we assume that the data matrix \\(\\boldsymbol X\\) is centered (i.e. column sums \\(\\boldsymbol X^T \\boldsymbol 1_n = \\boldsymbol 0\\) are zero). Likewise \\(\\boldsymbol y= (y_1, \\ldots, y_n)^T\\) is the response data vector (also centered with \\(\\boldsymbol y^T \\boldsymbol 1_n = 0\\)). Noting that \\[\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}=\\frac{1}{n}(\\boldsymbol X^T\\boldsymbol X)\\] is the MLE of covariance matrix among \\(\\boldsymbol x\\) and \\[\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}=\\frac{1}{n}(\\boldsymbol X^T \\boldsymbol y)\\] is the MLE of the covariance between \\(\\boldsymbol x\\) and \\(y\\) we see that the OLS estimate of the regression coefficients can be expressed as \\[\\widehat{\\boldsymbol \\beta} = \\left(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}\\right)^{-1}\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}\\] We can write down a population version (with no hats!): \\[\\boldsymbol \\beta= \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\] Thus, OLS regression coefficients can be interpreted as plugin estimator using MLEs of covariances! In fact, we may also use the unbiased estimates since the scale factor (\\(1/n\\) or \\(1/(n-1)\\)) cancels out so it does not matter which one you use! 17.3.1 Importance of positive definiteness of estimated covariance matrix Note that \\(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}\\) is inverted in \\(\\widehat{\\boldsymbol \\beta} = \\left(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}\\right)^{-1}\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}\\). Hence, the estimate \\(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}\\) needs to be positive definite! But \\(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{\\text{MLE}}\\) is only positive definite if \\(n&gt;d\\)! Therefore we can use the ML estimate (empirical estimator) only for large \\(n\\) &gt; \\(d\\), otherwise we need to employ a different (regularised) estimation approach (e.g. Bayes or a penalised ML)! Remark: writing \\(\\hat{\\boldsymbol \\beta}\\) explicitly based on covariance estimates has the advantage that we can construct plug-in estimators of regression coefficient based on regularised covariance estimators that improve over ML for small sample size. This leads to the so-called SCOUT method (=covariance-regularized regression by Witten and Tibshirani, 2008). 17.4 Best linear predictor The best linear predictor is a fourth way to arrive at the linear model. This is closely related to OLS and minimising squared residual error. Without assuming normality the above multiple regression model can be shown to be optimal linear predictor under the minimum mean squared prediction error: Assumptions: \\(y\\) and \\(\\boldsymbol x\\) are random variables we construct a new variable (the linear predictor) \\(y^{\\star\\star} = b_0 + \\boldsymbol b^T \\boldsymbol x\\) to optimally approximate \\(y\\) Aim: choose \\(b_0\\) and \\(\\boldsymbol b\\) such to minimize the mean squared prediction error \\(\\text{E}( (y - y^{\\star\\star})^2 )\\) 17.4.1 Result: The mean squared prediction error \\(MSPE\\) in dependence of \\((b_0, \\boldsymbol b)\\) is \\[ \\begin{split} \\text{E}( (y - y^{\\star\\star} )^2) &amp; = \\text{Var}(y - y^{\\star\\star}) + \\text{E}(y - y^{\\star\\star})^2 \\\\ &amp; = \\text{Var}(y - b_0 -\\boldsymbol b^T \\boldsymbol x) + ( \\text{E}(y) -b_0 - \\boldsymbol b^T \\text{E}(\\boldsymbol x) )^2 \\\\ &amp; = \\sigma^2_y + \\text{Var}(\\boldsymbol b^T \\boldsymbol x) + 2 \\, \\text{Cov}(y, -\\boldsymbol b^T \\boldsymbol x) + ( \\mu_y -b_0 - \\boldsymbol b^T \\boldsymbol \\mu_{\\boldsymbol x} )^2 \\\\ &amp; = \\sigma^2_y + \\boldsymbol b^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol b- 2 \\, \\boldsymbol b^T \\boldsymbol \\Sigma_{\\boldsymbol xy} + ( \\mu_y -b_0 - \\boldsymbol b^T \\boldsymbol \\mu_{\\boldsymbol x} )^2 \\\\ &amp; = MSPE(b_0, \\boldsymbol b) \\\\ \\end{split} \\] We look for \\[ (\\beta_0, \\boldsymbol \\beta) = \\underset{b_0,\\boldsymbol b}{\\arg\\min} \\,\\, MSPE(b_0, \\boldsymbol b) \\] In order to find the minimum we compute the gradient with regard to \\((b_0, \\boldsymbol b)\\) \\[ \\nabla MSPE = \\begin{pmatrix} -2( \\mu_y -b_0 - \\boldsymbol b^T \\boldsymbol \\mu_{\\boldsymbol x} ) \\\\ 2 \\, \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol b- 2 \\, \\boldsymbol \\Sigma_{\\boldsymbol xy} -2 \\boldsymbol \\mu_{\\boldsymbol x} (\\mu_y -b_0 - \\boldsymbol b^T \\boldsymbol \\mu_{\\boldsymbol x}) \\\\ \\end{pmatrix} \\] and setting this equal to zero yields \\[ \\begin{pmatrix} \\beta_0\\\\ \\boldsymbol \\beta\\\\ \\end{pmatrix} = \\begin{pmatrix} \\mu_y- \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x} \\\\ \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\\\ \\end{pmatrix} \\] Thus, the optimal values for \\(b_0\\) and \\(\\boldsymbol b\\) in the best linear predictor correspond to the previously derived coefficients \\(\\beta_0\\) and \\(\\boldsymbol \\beta\\)! 17.4.2 Irreducible Error The minimum achieved MSPE (=irreducible error) is \\[ MSPE(\\beta_0,\\boldsymbol \\beta) = \\sigma^2_{y} - \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\beta= \\sigma^2_{y} - \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} \\] With the abbreviation \\(\\Omega^2 = \\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} = \\sigma_y^{-2} \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\) we can simplify this to \\[ MSPE(\\beta_0,\\boldsymbol \\beta) = \\sigma^2_y (1-\\Omega^2) = \\text{Var}(\\varepsilon) \\] Writing \\(b_0=\\beta_0 + \\Delta_0\\) and \\(\\boldsymbol b= \\boldsymbol \\beta+ \\boldsymbol \\Delta\\) it is easy to see that the mean squared predictive error is a quadratic function around the minimum: \\[ MSPE(\\beta_0 + \\Delta_0, \\boldsymbol \\beta+ \\boldsymbol \\Delta) = \\text{Var}(\\varepsilon) + \\Delta_0^2 + \\boldsymbol \\Delta^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\Delta \\] Note that usually \\(y^{\\star} = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x\\) does not perfectly approximate \\(y\\) so there will be an irreducible error (= noise variance) \\[\\text{Var}(\\varepsilon) =\\sigma^2_y (1-\\Omega^2) &gt; 0\\] which implies \\(\\Omega^2 &lt; 1\\). The quantity \\(\\Omega^2\\) has a further interpretation of the population version of as the squared multiple correlation coefficient between the response and the predictors and plays a vital role in decomposition of variance, as discussed later. 17.5 Regression by conditioning Conditioning is a fifth way to arrive at the linear model. This is also the most general way and can be used to derive many other regression models (not just the simple linear model). 17.5.1 General idea: two random variables \\(y\\) (response, scalar) and \\(\\boldsymbol x\\) (predictor variables, vector) we assume that \\(y\\) and \\(\\boldsymbol x\\) have a joint distribution \\(F_{y,\\boldsymbol x}\\) compute conditional random variable \\(y | \\boldsymbol x\\) and the corresponding distribution \\(F_{y | \\boldsymbol x}\\) 17.5.2 Multivariate normal assumption Now we assume that \\(y\\) and \\(\\boldsymbol x\\) are (jointly) multivariate normal. Then the conditional distribution \\(F_{y | \\boldsymbol x}\\) is a univariate normal with the following moments (you can verify this by looking up the general conditional multivariate normal distribution): a) Conditional expectation: \\[ \\text{E}( y | \\boldsymbol x) = y^{\\star} = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x\\] with coefficients \\(\\boldsymbol \\beta= \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1}\\boldsymbol \\Sigma_{\\boldsymbol xy}\\) and intercept \\(\\beta_0 = \\mu_{y} - \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x}\\) . Note that as \\(y^{\\star}\\) depends on \\(\\boldsymbol x\\) it is a random variable itself with mean \\[\\text{E}(y^{\\star}) = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x} = \\mu_{y}\\] and variance \\[\\text{Var}(y^{\\star})= \\text{Var}(\\text{E}( y | \\boldsymbol x)) = \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\beta= \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} = \\sigma^2_y \\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} = \\sigma_y^2 \\Omega^2\\]. b) Conditional variance: \\[\\text{Var}( y | \\boldsymbol x) =\\sigma^2_y - \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\beta= \\sigma^2_y - \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} = \\sigma^2_y (1-\\Omega^2)\\] Note this is a constant so \\(\\text{E}(\\text{Var}( y | \\boldsymbol x)) = \\sigma^2_y (1-\\Omega^2)\\) as well. 17.6 Standardised regression coefficients and relationship to correlation First we note that we can decompose regression coefficients into the product of marginal correlations and correlations among predictors. Using the variance-correlation decompositions \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}= \\boldsymbol V_{\\boldsymbol x}^{1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x} \\boldsymbol V_{\\boldsymbol x}^{1/2}\\) and \\(\\boldsymbol \\Sigma_{\\boldsymbol xy}= \\boldsymbol V_{\\boldsymbol x}^{1/2} \\boldsymbol P_{\\boldsymbol xy} \\sigma_y\\) we rewrite the regression coefficients as \\[ \\boldsymbol \\beta= {\\underbrace{\\boldsymbol V_{\\boldsymbol x}^{-1/2}}_{\\text{(inverse) scale of } x_i}} \\,\\, {\\underbrace{\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1}}_{\\text{ (inverse) correlation among predictors }}} \\underbrace{\\boldsymbol P_{\\boldsymbol xy}}_{\\text{ marginal correlations}}\\,\\, \\underbrace{\\sigma_y}_{\\text{ scale of }y} \\] Thus the regression coefficients \\(\\boldsymbol \\beta\\) contain the scale of the variables, and take into account the correlations among the predictors (\\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}\\)) in addition to the marginal correlations between the response \\(y\\) and the predictors \\(x_i\\) (\\(\\boldsymbol P_{\\boldsymbol xy}\\)). This decomposition allows to understand a number special cases when the regression coefficient simplify further: If the response and the predictors are standardised to have variance one, i.e. \\(\\text{Var}(y)=1\\) and \\(\\text{Var}(x_i)\\), then \\(\\boldsymbol \\beta\\) becomes equal to the standardised regression coefficients \\[\\boldsymbol \\beta_{\\text{std}} = \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\\] Note that standardised regression coefficients do not make use of variances and and thus are scale-independent. If there is no correlation among the predictors , i.e. \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x} = \\boldsymbol I\\) the the regression coefficients reduce to \\[\\boldsymbol \\beta= \\boldsymbol V_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\] where \\(\\boldsymbol V_{\\boldsymbol x}\\) is a diagonal matrix containing the variances oft the predictors. This is also called marginal regression. Note that the inversion of \\(\\boldsymbol V_{\\boldsymbol x}\\) is trival since you only need to invert each diagonal element individually. If both a) and b) apply simultaneously (i.e. there is no correlation among predictors and response and predictors and predictors are standardised) then the regression coefficients simplify even further to \\[ \\boldsymbol \\beta= \\boldsymbol P_{\\boldsymbol xy} \\] Thus, in this very special case the regression coefficients are identical to the correlations between the response and the predictors! "],["18-regression4.html", "18 Squared multiple correlation and variance decomposition in linear regression 18.1 Squared multiple correlation \\(\\Omega^2\\) and the \\(R^2\\) coefficient 18.2 Variance decomposition in regression 18.3 Sample version of variance decomposition", " 18 Squared multiple correlation and variance decomposition in linear regression In this chapter we first introduce the (squared) multiple correlation and the multiple and adjusted \\(R^2\\) coefficients as estimators. Subsequently we discuss variance decomposition. 18.1 Squared multiple correlation \\(\\Omega^2\\) and the \\(R^2\\) coefficient In the previous chapter we encountered the following quantity: \\[ \\Omega^2 = \\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} = \\sigma_y^{-2} \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} \\] With \\(\\boldsymbol \\beta=\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\) and \\(\\beta_0=\\mu_y- \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x}\\) it is straightforward to verify the following: the cross-covariance between \\(y\\) and \\(y^{\\star}\\) is \\(\\text{Cov}(y, y^{\\star}) = \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\beta= \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} = \\sigma^2_y \\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} = \\sigma_y^2 \\Omega^2\\) the (signal) variance of \\(y^{\\star}\\) is \\(\\text{Var}(y^{\\star})= \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\beta= \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} = \\sigma^2_y \\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} = \\sigma_y^2 \\Omega^2\\). hence the correlation \\(\\text{Cor}(y, y^{\\star}) = \\frac{\\text{Cov}(y, y^{\\star})}{\\text{SD}(y) \\text{SD}(y^{\\star})} = \\Omega\\) with \\(\\Omega \\geq 0\\). This helps to understand the \\(\\Omega\\) and \\(\\Omega^2\\) coefficients: \\(\\Omega\\) is the linear correlation between the response (\\(y\\)) and prediction \\(y^{\\star}\\). \\(\\Omega^2\\) is called the squared multiple correlation between the scalar \\(y\\) and the vector \\(\\boldsymbol x\\). Note that if we only have one predictor (if \\(x\\) is a scalar) then \\(\\boldsymbol P_{x x} = 1\\) and \\(\\boldsymbol P_{y x} = \\rho_{yx}\\) so the multiple squared correlation coefficient reduces to squared correlation \\(\\Omega^2 = \\rho_{yx}^2\\) between two scalar random variables \\(y\\) and \\(x\\). 18.1.1 Estimation of \\(\\Omega^2\\) and the multiple \\(R^2\\) coefficient The multiple squared correlation coefficient \\(\\Omega^2\\) can be estimated by plug-in of empirical estimates for the corresponding correlation matrices: \\[R^2 = \\hat{\\boldsymbol P}_{y \\boldsymbol x} \\hat{\\boldsymbol P}_{\\boldsymbol x\\boldsymbol x}^{-1} \\hat{\\boldsymbol P}_{\\boldsymbol xy} = \\hat{\\sigma}_y^{-2} \\hat {\\boldsymbol \\Sigma}_{y \\boldsymbol x} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}\\] This estimator of \\(\\Omega^2\\) is called the multiple \\(R^2\\) coefficient. If the same scale factor \\(1/n\\) or \\(1/(n-1)\\) is used in estimating the variance \\(\\sigma^2_y\\) and the covariances \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}\\) and \\(\\boldsymbol \\Sigma_{y \\boldsymbol x}\\) then this factor will cancel out. Above we have seen that \\(\\Omega^2\\) is directly linked with the noise variance via \\[ \\text{Var}(\\varepsilon) =\\sigma^2_y (1-\\Omega^2) \\,. \\] so we can express the squared multiple correlation as \\[ \\Omega^2 = 1- \\text{Var}(\\varepsilon) / \\sigma^2_y \\] The maximum likelihood estimate of the noise variance \\(\\text{Var}(\\varepsilon)\\) (also called residual variance) can be computed from the residual sum of squares \\(RSS = \\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\) as follows: \\[ \\widehat{\\text{Var}}(\\varepsilon)_{ML} = \\frac{RSS}{n} \\] whereas the unbiased estimate is obtained by \\[ \\widehat{\\text{Var}}(\\varepsilon)_{UB} = \\frac{RSS}{n+d+1} = \\frac{RSS}{df} \\] where the degree of freedom is \\(df=n-d-1\\) and \\(d\\) is the number of predictors. Similarly, we can find the maximimum likelihood estimate \\(v_y^{ML}\\) for \\(\\sigma^2_y\\) (with factor \\(1/n\\)) as well as an unbiased estimate \\(v_y^{UB}\\) (with scale factor \\(1/(n-1)\\)) The multiple \\(R^2\\) coefficient can then be written as \\[ R^2 =1- \\widehat{\\text{Var}}(\\varepsilon)_{ML} / v_y^{ML} \\] Note we use MLEs. In contrast, the so-called adjusted multiple \\(R^2\\) coefficient is given by \\[ R^2_{\\text{adj}}=1- \\widehat{\\text{Var}}(\\varepsilon)_{UB} / v_y^{UB} \\] where the unbiased variances are used. Both \\(R^2\\) and \\(R^2_{\\text{adj}}\\) are estimates of \\(\\Omega^2\\) and are related by \\[ 1-R^2 = (1- R^2_{\\text{adj}}) \\, \\frac{df}{n-1} \\] 18.1.2 R commands In R the command lm() fits the linear regression model. In addition to the regression cofficients (and derived quantities) the R function lm() also lists the multiple R-squared \\(R^2\\), the adjusted R-squared \\(R^2_{\\text{adj}}\\), the degrees of freedom \\(df\\) and the residual standard error \\(\\sqrt{\\widehat{\\text{Var}}(\\varepsilon)_{UB}}\\) (computed from the unbiased variance estimate). See also Worksheet 9 which provides R code to reproduce the exact output of the native lm() R function. 18.2 Variance decomposition in regression The squared multiple correlation coefficient is useful also because it plays an important role in the decomposition of the total variance: total variance: \\(\\text{Var}(y) = \\sigma^2_y\\) unexplained variance (irreducible error): \\(\\sigma^2_y (1-\\Omega^2) = \\text{Var}(\\varepsilon)\\) the explained variance is the complement: \\(\\sigma^2_y \\Omega^2 = \\text{Var}(y^{\\star})\\) In summary: \\[\\text{Var}(y) = \\text{Var}(y^{\\star}) + \\text{Var}(\\varepsilon)\\] becomes \\[\\underbrace{\\sigma^2_y}_{\\text{total variance}} = \\underbrace{\\sigma_y^2 \\Omega^2}_{\\text{explained variance}} + \\underbrace{ \\sigma^2_y (1-\\Omega^2)}_{\\text{unexplained variance}}\\] The unexplained variance measures the fit after introducing predictors into the model (smaller means better fit). The total variance measures the fit of the model without any predictors. The explained variance is the difference between total and unexplained variance, it indicates the increase in model fit due to the predictors. 18.2.1 Law of total variance and variance decomposition The law of total variance is \\[\\underbrace{\\text{Var}(y)}_{\\text{total variance}} = \\underbrace{\\text{Var}( \\text{E}(y | \\boldsymbol x) ) }_{\\text{explained variance}} + \\underbrace{ \\text{E}( \\text{Var}( y | \\boldsymbol x) )}_{\\text{unexplained variance}}\\] provides a very general decomposition in explained and unexplained parts of the variance that is valid regardless of the form of the distributions \\(F_{y, \\boldsymbol x}\\) and \\(F_{y | \\boldsymbol x}\\). In regression it conncects variance decomposition and conditioning. If you plug-in the conditional expections for the multivariate normal model (cf. previous chapter) we recover \\[\\underbrace{\\sigma^2_y}_{\\text{total variance}} = \\underbrace{\\sigma_y^2 \\Omega^2 }_{\\text{explained variance}} + \\underbrace{ \\sigma^2_y (1-\\Omega^2)}_{\\text{unexplained variance}}\\] 18.2.2 Related quantities Using the above three quantities (total variance, explained variance, and unexplained variance) we can construct a number of scores: coefficient of determination, squared multiple correlation: \\[ \\frac{\\text{explained var}}{\\text{total var}} = \\frac{\\sigma_y^2 \\Omega^2}{\\sigma_y^2} = \\Omega^2 \\] (range 0 to 1, with 1 indicating perfect fit) coefficient of non-determination, coefficient of alienation: \\[ \\frac{\\text{unexplained var}}{\\text{total var}} = \\frac{\\sigma_y^2 (1-\\Omega^2)}{\\sigma_y^2} = 1-\\Omega^2 \\] (range 0 to 1, with 0 indicating perfect fit) \\(F\\) score, \\(t^2\\) score: \\[ n \\frac{\\text{explained var}}{\\text{unexplained var}} = n \\frac{\\sigma_y^2 \\Omega^2}{\\sigma_y^2 (1-\\Omega^2)} = n \\frac{\\Omega^2}{1-\\Omega^2} = \\mathcal{F} = \\tau^2 \\] (range 0 to \\(\\infty\\), with \\(\\infty\\) indicating perfect fit) Note that \\(\\mathcal{F}\\) and \\(\\tau^2\\) scores (=population versions of \\(F\\) and \\(t^2\\) statistics) by definition scale with sample size \\(n\\), and that \\(\\Omega^2 = \\frac{\\tau^2}{\\tau^2 + n} = \\frac{\\mathcal{F}}{\\mathcal{F} + n}\\) links squared correlation with squared \\(t\\)-scores and \\(F\\)-scores. 18.3 Sample version of variance decomposition If \\(\\Omega^2\\) and \\(\\sigma^2_y\\) are replaced by their MLEs this can be written in a sample version as follows using data points \\(y_i\\), predictions \\(\\hat{y}_i\\) and \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n {y_i}\\) \\[\\underbrace{\\sum_{i=1}^n (y_i-\\bar{y})^2}_{\\text{total sum of squares (TSS)}} = \\underbrace{\\sum_{i=1}^n (\\hat{y}_i-\\bar{y})^2 }_{\\text{explained sum of squares (ESS)}} + \\underbrace{\\sum_{i=1}^n (y_i-\\hat{y}_i)^2 }_{\\text{residual sum of squares (RSS)}}\\] Note that TSS, ESS and RSS all scale with \\(n\\). Using data vector notation the sample-based variance decomposition can be written in form of the Pythagorean theorem: \\[\\underbrace{|| \\boldsymbol y-\\bar{y} \\boldsymbol 1\\ ||^2}_{\\text{total sum of squares (TSS)}} = \\underbrace{||\\hat{\\boldsymbol y}-\\bar{y} \\boldsymbol 1||^2 }_{\\text{explained sum of squares (ESS)}} + \\underbrace{|| \\boldsymbol y-\\hat{\\boldsymbol y} ||^2 }_{\\text{residual sum of squares (RSS)}}\\] 18.3.1 Geometric interpretation of regression as orthogonal projection: The above equation can be further simplified to \\[|| \\boldsymbol y||^2 = ||\\hat{\\boldsymbol y}||^2 + \\underbrace{|| \\boldsymbol y-\\hat{\\boldsymbol y} ||^2 }_{\\text{RSS}} \\] Geometrically speaking, this implies \\(\\hat{\\boldsymbol y}\\) is an orthogonal projection of \\(\\boldsymbol y\\), since the residuals \\(\\boldsymbol y-\\hat{\\boldsymbol y}\\) and the predictions \\(\\hat{\\boldsymbol y}\\) are orthogonal (by construction!). This also valid for the centered versions of the vectors, i.e. \\(\\hat{\\boldsymbol y}-\\bar{y} \\boldsymbol 1_n\\) is an orthogonal projection of \\(\\boldsymbol y-\\bar{y} \\boldsymbol 1_n\\) (see Figure). Also note that the angle \\(\\theta\\) between the two centered vectors is directly related to the (estimated) multiple correlation, with \\(R = \\cos(\\theta) = \\frac{||\\hat{\\boldsymbol y}-\\bar{y} \\boldsymbol 1_n ||}{|| \\boldsymbol y-\\bar{y} \\boldsymbol 1_n||}\\), or \\(R^2 = \\cos(\\theta)^2 = \\frac{||\\hat{\\boldsymbol y}-\\bar{y} \\boldsymbol 1_n ||^2}{|| \\boldsymbol y-\\bar{y} \\boldsymbol 1_n||^2} = \\frac{\\text{ESS}}{\\text{TSS}}\\). Source of Figure: Stack Exchange "],["19-regression5.html", "19 Prediction and variable selection 19.1 Prediction and prediction intervals 19.2 Variable importance and prediction 19.3 Regression \\(t\\)-scores. 19.4 Further approaches for variable selection", " 19 Prediction and variable selection In this chapter we discuss how to compute (lower bounds) of the prediction error and how to select variables relevant for prediction 19.1 Prediction and prediction intervals Learning the regression function from (training) data is only the first step in application of regression models. The next step is to actually make prediction of future outcomes \\(y^{\\text{test}}\\) given test data \\(\\boldsymbol x^{\\text{test}}\\): \\[ y^{\\text{test}} = \\hat{y}(\\boldsymbol x^{\\text{test}}) = \\hat{f}_{\\hat{\\beta}_0, \\hat{\\boldsymbol \\beta}}(\\boldsymbol x^{\\text{test}}) \\] Note that \\(y^{\\text{test}}\\) is a point estimator. Is it possible also to construct a corresponding interval estimate? The answer is yes, and leads back to the conditioning approach: \\[y^\\star = \\text{E}(y| \\boldsymbol x) = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x\\] \\[\\text{Var}(\\varepsilon) = \\text{Var}(y|\\boldsymbol x) = \\sigma^2_y (1-\\Omega^2)\\] We know that the mean squared prediction error for \\(y^{\\star}\\) is \\(\\text{E}((y -y^{\\star})^2)=\\text{Var}(\\varepsilon)\\) and that this is the minimal irreducible error. Hence, we may use \\(\\text{Var}(\\varepsilon)\\) as the minimum variability for the prediction. The corresponding prediction interval is \\[\\left[ y^{\\star}(\\boldsymbol x^{\\text{test}}) \\pm c \\times \\text{SD}(\\varepsilon) \\right]\\] where \\(c\\) is some suitable constant (e.g. 1.96 for symmetric 95% normal intervals). However, please note that the prediction interval constructed in this fashion will be an underestimate. The reason is that this assumes that we employ \\(y^{\\star} = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x\\) but in reality we actually use \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\boldsymbol \\beta}^T \\boldsymbol x\\) for prediction — note the estimated coefficients! We recall from an earlier chapter (best linear predictor) that this leads to increase of MSPE compared with using the optimal \\(\\beta_0\\) and \\(\\boldsymbol \\beta\\). Thus, for better prediction intervals we would need to consider the mean squared prediction error of \\(\\hat{y}\\) that can be written as \\(\\text{E}((y -\\hat{y})^2) = \\text{Var}(\\varepsilon) + \\delta\\) where \\(\\delta\\) is an additional error term due to using an estimated rather than the true regression function. \\(\\delta\\) typically declines with \\(1/n\\) but can be substantial for small \\(n\\) (in particular as it usually depends on the number of predictors \\(d\\)). For more details on this we refer to later modules on regression. 19.2 Variable importance and prediction Another key question in regression modeling is to find out predictor variables \\(x_1, x_2, \\dots, x_d\\) are actually important for predicting the outcome \\(y\\). \\(\\rightarrow\\) We need to study variable importance measures (VIM). 19.2.1 How to quantify variable importance? A variable \\(x_i\\) is important if it improves prediction of the response \\(y\\). Recall variance decomposition: \\[\\text{Var}(y) = \\sigma_y^2 = \\underbrace{\\sigma^2_y\\Omega^2}_{\\text{explained variance}} + \\underbrace{\\sigma^2_y(1-\\Omega^2)}_{\\text{unexplained/residual variance =} \\text{Var}(\\varepsilon)}\\] \\(\\Omega^2\\) squared multiple correlation \\(\\in [0,1]\\) \\(\\Omega^2\\) large \\(\\rightarrow 1\\) predictor variables explain most of \\(\\sigma_y^2\\) \\(\\Omega^2\\) small \\(\\rightarrow 0\\) linear model fails and predictors do not explain variability \\(\\Rightarrow\\) If a predictor helps to \\(\\begin{array}{ll} \\text{increase explained variance} \\\\ \\text{decrease unexplained variance} \\end{array}\\) then it is important! \\(\\Omega^2 = \\boldsymbol P_{y\\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} \\hat{=}\\) a function of the \\(X\\)! VIM: which predictors contribute most to \\(\\Omega^2\\) 19.2.2 Some candidates for VIMs The regression coefficients \\(\\boldsymbol \\beta\\) \\(\\boldsymbol \\beta= \\boldsymbol \\Sigma^{-1}_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol xy}= \\boldsymbol V_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} \\sigma_y\\) Not a good VIM since \\(\\boldsymbol \\beta\\) contains the scale! Large \\(\\hat{\\beta}_i\\) does not indicate that \\(x_i\\) is important. Small \\(\\hat{\\beta}_i\\) does not indicate that \\(x_i\\) is not important. Standardised regression coefficients \\(\\boldsymbol \\beta_{\\text{std}}\\) \\(\\boldsymbol \\beta_{\\text{std}} = \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\\) implies \\(\\text{Var}(y)=1\\), \\(\\text{Var}(x_i)=1\\) These do not contain the scale (so better than \\(\\hat{\\beta}\\)) But still unclear how this relates to decomposition of variance Squared marginal correlations \\(\\rho_{y, x_i}^2\\) Consider case of uncorrelated predictors: \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}=\\boldsymbol I\\) (no correlation among \\(x_i\\)) \\[\\Rightarrow \\Omega^2 = \\boldsymbol P_{y\\boldsymbol x} \\boldsymbol P_{\\boldsymbol xy} = \\sum^d_{i=1} \\rho_{y, x_i}^2\\] \\(\\rho_{y, x_i}^2 = \\text{Cor}(y, x_i)\\) is the marginal correlation between \\(y\\) and \\(x_i\\), and \\(\\Omega^2\\) is (for uncorrelated predictors) the sum of squared marginal correlations. If \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}=\\boldsymbol I\\), then ranking predictors by \\(\\rho_{y, x_i}^2\\) is optimal! The predictor with largest marginal correlation reduces the unexplained variance most! good news: even if there is weak correlation among predictors the marginal correlations are still good as VIM (but then they will not perfectly add up to \\(\\Omega^2\\)) Advantage: very simple but often also very effective. Caution! If there is strong correlation in \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}\\), then there is colinearity (in this case it is oftern best to remove one of the strongly correlated variables, or to merge the correlated variables). Often, ranking predictors by their squared marginal correlations is done as a prefiltering step (independence screening). 19.3 Regression \\(t\\)-scores. So far, we discussed three obvious candidates for for variable importance measures (regression coefficients, standardised regression coefficients, marginal correlations). However, of these only marginal correlation between the response \\(y\\) and each predictor \\(x_i\\) are useful for variable ranking (due to the direct link of multiple correlation and explained variation). In this section we consider a further quantity, the regression \\(t-\\)score: Recall that ML estimation of the regression coefficients yields a point estimate \\(\\hat{\\boldsymbol \\beta}\\) the (asymptotic) variance \\(\\widehat{\\text{Var}}(\\hat{\\boldsymbol \\beta})\\) the asymptotic normal distribution \\(\\hat{\\boldsymbol \\beta} \\overset{a}{\\sim} N_d(\\boldsymbol \\beta, \\widehat{\\text{Var}}(\\hat{\\boldsymbol \\beta}))\\) Corresponding to each predictor \\(x_i\\) we can construct from the above a \\(t\\)-score \\[ t_i = \\frac{\\hat{\\beta}_i}{\\widehat{\\text{SD}}(\\hat{\\beta}_i)} \\] where the standard deviations are computed by \\(\\widehat{\\text{SD}}(\\hat{\\beta}_i) = \\text{Diag}(\\widehat{\\text{Var}}(\\hat{\\boldsymbol \\beta}))_{i}\\). This corresponds to the Wald statistic to test that the underlying true \\(\\beta_i =0\\). Correspondingly, under the null hypthesis that \\(\\beta_i=0\\) we have asymptotically for large \\(n\\) \\[ t_i \\overset{a}{\\sim} N(0,1) \\] Corresponding (symmetric) \\(p\\)-values are computed by \\(p = 2 \\Phi(-|t_i|)\\). For finite \\(n\\), normal assumption and using the unbiased estimate for variance \\(t_i\\) follows a \\(t\\)-distribution: \\[ t_i \\sim t_{n-d-1} \\] Regression \\(t\\)-scores can thus be used to test whether a regression coefficient is zero. Large magnitude of the \\(t_i\\) score indicates that the hypothesis \\(\\beta_i=0\\) can be rejected. This allows rank predictor variables by \\(|t_i|\\) or the corresponding \\(p\\)-values with regard to relevance for the linear model. Note that by construction the regression \\(t\\)-scores do not depend on the scale, so when the original data are rescaled this will not affect the corresponding regression \\(t\\)-scores. Furthermore, if \\(\\widehat{\\text{SD}}(\\hat{\\beta}_i)\\) is small, then the regression \\(t\\)-score \\(t_i\\) can still be large even if \\(\\hat{\\beta}_i\\) is small! 19.3.1 Computation When you perform regression analysis in R (or any other software), you will get to see the following table: \\[\\begin{align*} \\begin{array}{cc} $$\\hat{\\beta}_i$$\\\\ $$\\text{Estimated}$$\\\\ $$\\text{repression}$$\\\\ $$\\text{coefficient}$$\\\\ \\\\ \\end{array} \\begin{array}{cc} $$\\widehat{\\text{SD}}(\\hat{\\beta}_i)$$\\\\ $$\\text{Error of}$$\\\\ $$\\hat{\\beta}_i$$\\\\ \\\\ \\\\ \\end{array} \\begin{array}{cc} $$t_i = \\frac{\\hat{\\beta}_i}{\\widehat{\\text{SD}}(\\hat\\beta_i)}$$\\\\ $$\\text{t-score}$$\\\\ $$\\text{computed from }$$\\\\ $$\\text{first two columns}$$\\\\ \\\\ \\end{array} \\begin{array}{cc} $$\\text{p-values}$$\\\\ $$\\text{for } t_i$$\\\\ $$\\text{based on t-distribution}$$\\\\ \\\\ \\\\ \\end{array} \\begin{array}{ll} $$\\text{Indicator of}$$\\\\ $$\\text{Significance}$$\\\\ $$\\text{* } 0.9$$\\\\ $$\\text{** } 0.95$$\\\\ $$\\text{*** } 0.99$$\\\\ \\end{array} \\end{align*}\\] In the lm() function in R the standard deviation is the square root of the unbiased estimate of the variance (but note that it itself is not unbiased!). 19.3.2 Connection with partial correlation The deeper reason why ranking predictors by regression \\(t\\)-scores and associated \\(p\\)-values is useful is their link with partial correlation. In particular, the (squared) regression \\(t\\)-score can be 1:1 transformed into the (estimated) (squared) partial correlation \\[ \\hat{\\rho}_{y, x_i | x_{j \\neq i}}^2 = \\frac{t_i^2}{t_i^2 + df} \\] with \\(df=n-d-1\\), and it can be shown that the \\(p\\)-values for testing that \\(\\beta_i=0\\) are exactly the same as the \\(p\\)-values for testing that the partial correlation \\(\\rho_{y, x_i | x_{j \\neq i}}\\) vanishes! Therefore, ranking the predictors \\(x_i\\) by regression \\(t\\)-scores leads to exactly the same ranking and \\(p\\)-values as partial correlation! 19.4 Further approaches for variable selection In addition to ranking by marginal and partial correlation, there are many other approaches for variable selection in regression! Search-based methods: search through subsets of linear models for \\(d\\) variables, ranging from full model (including all predictors) to the empty model (includes no predictor) and everything inbetween. Problem: exhaustive search not possible even for relatively small \\(d\\) as space of models is very large! Therefore heuristic approaches such as forward selection (adding predictors), backward selection (removing predictors), or monte-carlo random search are employed. Problem: maximum likelihood cannot be used for choosing among the models - since ML will always pick the best model. Therefore, penalised ML criteria such as AIC or Bayesian criteria are often employed instead. Integrative estimation and variable selection: there are methods that fit the regression model and perform variable selection simultaneously. the most well-known approach of this type is “lasso” regression (Tibshirani 1996) This applies a (generalised) linear model with ML plus L1 penalty. Alternative: Bayesian variable selection and estimation procedures Entropy-based variable selection: As seen above, two of the most popular approaches in linear models are based on correlation, eithermarginal correlation or partial correlation (via regression \\(t\\)-scores). Correlation measures can be generalised to non-linear settings. One very popular measure is the mutual information which is computed using the KL divergence. In case of two variables \\(x\\) and \\(y\\) with joint normal distribution and correlation \\(\\rho\\) the mutual information is a function of the correlation: \\[\\text{MI}(x,y) = \\frac{1}{2} \\log (1-\\rho^2)\\] In regression he mutual information between the response \\(y\\) and predictor \\(x_i\\) is \\(\\text{MI}(y, x_i)\\), and this widely used for feature selection, in particular in machine learning. "],["20-refresher.html", "A Refresher A.1 Basic mathematical notation A.2 Vectors and matrices A.3 Functions A.4 Combinatorics A.5 Probability A.6 Distributions A.7 Statistics", " A Refresher Statistics is a mathematical science that requires practical use of tools from probability, vector and matrices, analysis etc. Here we briefly list some essentials that are needed for “Statistical Methods”. Please familiarise yourself (again) with these topics. A.1 Basic mathematical notation Summation: \\[ \\sum_{i=1}^n x_i = x_1 + x_2 + \\ldots + x_n \\] Multiplication: \\[ \\prod_{i=1}^n x_i = x_1 \\times x_2 \\times \\ldots \\times x_n \\] A.2 Vectors and matrices Vector and matrix notation. Vector algebra. Eigenvectors and eigenvalues for a real symmetric matrix. Eigenvalue (spectral) decomposition of a real symmetric matrix. Positive and negative definiteness of a real symmetric matrix (containing only positive or only negative eigenvalues). Singularity of a real symmetric matrix (containing one or more eigenvalues identical to zero). Singular value decomposition of a real matrix. A.3 Functions A.3.1 Gradient The nabla operator (also known as del operator) is the row vector \\[ \\nabla = \\left(\\frac{\\partial}{\\partial x_1}, \\ldots, \\frac{\\partial}{\\partial x_d}\\right) = \\frac{\\partial}{\\partial \\boldsymbol x} \\] containing the first order partial derivative operators. The gradient of a scalar-valued function \\(h(\\boldsymbol x)\\) with vector argument \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\) is also a row vector (with \\(d\\) columns) and can be expressed using the nabla operator \\[ \\nabla h(\\boldsymbol x) = \\left( \\frac{\\partial h(\\boldsymbol x)}{\\partial x_1}, \\ldots, \\frac{\\partial h(\\boldsymbol x)}{\\partial x_d} \\right) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\text{grad } h(\\boldsymbol x) \\, . \\] Note the various notations for the gradient. Example A.1 Examples for the gradient: \\(h(\\boldsymbol x)=\\boldsymbol a^T \\boldsymbol x+ b\\). Then \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol a^T\\). \\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol x\\). Then \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = 2 \\boldsymbol x^T\\). \\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol A\\boldsymbol x\\). Then \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol x^T (\\boldsymbol A+ \\boldsymbol A^T)\\). A.3.2 Hessian matrix The matrix of all second order partial derivates of scalar-valued function with vector-valued argument is called the Hessian matrix and is computed by double application of the nabla operator: \\[ \\nabla^T \\nabla h(\\boldsymbol x) = \\begin{pmatrix} \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1^2} &amp; \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1 \\partial x_d} \\\\ \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2 \\partial x_d} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d \\partial x_1} &amp; \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d^2} \\end{pmatrix} = \\left(\\frac{\\partial h(\\boldsymbol x)}{\\partial x_i \\partial x_j}\\right) = {\\left(\\frac{\\partial}{\\partial \\boldsymbol x}\\right)}^T \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} \\,. \\] By construction it is square and symmetric. A.3.3 Convex and concave functions A function \\(h(x)\\) is convex if the second derivative \\(h&#39;&#39;(x) \\geq 0\\) for all \\(x\\). More generally, a function \\(h(\\boldsymbol x)\\) is convex if the Hessian matrix \\(\\nabla^T \\nabla h(\\boldsymbol x)\\) is positive definite, i.e. if it contains only positive eigenvalues. If \\(h(\\boldsymbol x)\\) is convex, then \\(-h(\\boldsymbol x)\\) is concave. A function is concave if the Hessian matrix is negative definite. Example A.2 The logarithm \\(\\log(x)\\) is an example of a concave function whereas \\(x^2\\) is a convex function. To memorise, a valley is convex. A.3.4 Linear and quadratic approximation Taylor series of first / second order. Applied to scalar-valued function of a scalar: \\[ h(x) \\approx h(x_0) + h&#39;(x_0) (x-x_0) + \\frac{1}{2} h&#39;&#39;(x_0) (x-x_0)^2 \\] With \\(x = x_0+ \\varepsilon\\) this can be written as \\[ h(x_0+ \\varepsilon) \\approx h(x_0) + h&#39;(x_0) \\, \\varepsilon + \\frac{1}{2} h&#39;&#39;(x_0)\\, \\varepsilon^2 \\] Applied to scalar-valued function of a vector: \\[ h(\\boldsymbol x) \\approx h(\\boldsymbol x_0) + \\nabla h(\\boldsymbol x_0) (\\boldsymbol x-\\boldsymbol x_0) + \\frac{1}{2} (\\boldsymbol x-\\boldsymbol x_0)^T \\nabla^T \\nabla h(\\boldsymbol x_0) (\\boldsymbol x-\\boldsymbol x_0) \\] With \\(\\boldsymbol x= \\boldsymbol x_0+ \\boldsymbol \\varepsilon\\) this can be written as \\[ h(\\boldsymbol x_0+ \\boldsymbol \\varepsilon) \\approx h(\\boldsymbol x_0) + \\nabla h(\\boldsymbol x_0)\\boldsymbol \\varepsilon+ \\frac{1}{2} \\boldsymbol \\varepsilon^T \\nabla^T \\nabla h(\\boldsymbol x_0) \\boldsymbol \\varepsilon \\] Example A.3 Commonly occurring Taylor series approximations of second order are for example \\[ \\log(x_0+\\varepsilon) \\approx \\log(x_0) + \\frac{\\varepsilon}{x_0} - \\frac{\\varepsilon^2}{2 x_0^2} \\] and \\[ \\frac{x_0}{x_0+\\varepsilon} \\approx 1 - \\frac{\\varepsilon}{x_0} + \\frac{\\varepsilon^2}{ x_0^2} \\] A.3.5 Conditions for local optimum of a function To check if \\(x_0\\) or \\(\\boldsymbol x_0\\) is a local maximum or minimum we can use the following conditions: For a function of a single variable: First derivative is zero at optimum \\(h&#39;(x_0) = 0\\). If the second derivative \\(h&#39;&#39;(x_0) &lt; 0\\) at the optimum is negative the function is locally concave and the optimum is a maximum. If the second derivative \\(h&#39;&#39;(x_0) &gt; 0\\) is positive at the optimum the function is locally convex and the optimum is a minimum. For a function of several variables: Gradient vanishes at maximum, \\(\\nabla h(\\boldsymbol x_0)=0\\). If the Hessian \\(\\nabla^T \\nabla h(\\boldsymbol x_0)\\) is negative definite (= all eigenvalues of Hessian matrix are negative) then the function is locally concave and the optimum is a maximum. If the Hessian is positive definite (= all eigenvalues of Hessian matrix are positive) then the function is locally convec and the optimum is a minimum. Around the local optimum \\(\\boldsymbol x_0\\) we can approximate the function quadratically using \\[ h(\\boldsymbol x_0+ \\boldsymbol \\varepsilon) \\approx h(\\boldsymbol x_0) + \\frac{1}{2} \\boldsymbol \\varepsilon^T \\nabla^T \\nabla h(\\boldsymbol x_0) \\boldsymbol \\varepsilon \\] Note the linear term is missing due to the gradient being zero at \\(\\boldsymbol x_0\\). A.3.6 Functions of matrices Matrix inverse, matrix square root etc. of symmetric real matrices. Computation via eigenvalue decomposition i.e. apply function such as inverse, sqrt etc. on the eigenvalues. In this course we do not actually compute matrix functions, but we will use matrix notation for matrix square roots, so you do need to know that it is exists and that it is not the same as taking the square root of the matrix entries. Trace and determinant of a square matrix. Connection with eigenvalues (trace = sum of eigenvalues, determinant = product of eigenvalues). A.4 Combinatorics A.4.1 Number of permutations The number of possible orderings, or permutations, of \\(n\\) distinct items is the number of ways to put \\(n\\) items in \\(n\\) bins with exactly one item in each bin. It is given by the factorial \\[ n! = \\prod_{i=1}^n i = 1 \\times 2 \\times \\ldots \\times n \\] where \\(n\\) is a positive integer. For \\(n=0\\) the factorial is defined as \\[ 0! = 1 \\] as there is exactly one permutation of zero objects. The factorial can also be obtained using the Gamma function \\[ n! = \\Gamma(n+1) \\] which can be viewed as continuous version of the factorial. A.4.2 Multinomial and binomial coefficient The number of possible permutation of \\(n\\) items of \\(K\\) distinct types, with \\(n_1\\) of type 1, \\(n_2\\) of type 2 and so on, equals the number of ways to put \\(n\\) items into \\(K\\) bins with \\(n_1\\) items in the first bin, \\(n_2\\) in the second and so on. It is given by the multinomial coefficient \\[ \\binom{n}{n_1, \\ldots, n_K} = \\frac {n!}{n_1! \\times n_2! \\times\\ldots \\times n_K! } \\] with \\(\\sum_{k=1}^K n_k = n\\) and \\(K \\leq n\\). Note that it equals the number of permutation of all items divided by the number of permutations of the items in each bin (or of each type). If all \\(n_k=1\\) and hence \\(K=n\\) the multinomial coefficient reduces to the factorial. If there are only two bins / types (\\(K=2\\)) the multinomial coefficients becomes the binomial coefficient \\[ \\binom{n}{n_1} = \\binom{n}{n_1, n-n_1} = \\frac {n!}{n_1! (n - n_1)!} \\] which counts the number of ways to choose \\(n_1\\) elements from a set of \\(n\\) elements. A.4.3 De Moivre-Sterling approximation of the factorial The factorial is frequently approximated by the following formula derived by Abraham de Moivre (1667–1754) and James Stirling (1692-1770) \\[ n! \\approx \\sqrt{2 \\pi} n^{n+\\frac{1}{2}} e^{-n} \\] or equivalently on logarithmic scale \\[ \\log n! \\approx \\left(n+\\frac{1}{2}\\right) \\log n -n + \\frac{1}{2}\\log \\left( 2 \\pi\\right) \\] The approximation is good for small \\(n\\) (but fails for \\(n=0\\)) and becomes more and more accurate with increasing \\(n\\). For large \\(n\\) the approximation can be simplified to \\[ \\log n! \\approx n \\log n -n \\] A.5 Probability A.5.1 Random variables A random variable describes a random experiment. The set of possible outcomes is the sample space or state space and is denoted by \\(\\Omega = \\{\\omega_1, \\omega_2, \\ldots\\}\\). The outcomes \\(\\omega_i\\) are the elementary events. The sample space \\(\\Omega\\) can be finite or infinite. Depending on type of outcomes the random variable is discrete or continous. An event \\(A \\subseteq \\Omega\\) is subset of \\(\\Omega\\) and thus itself a set of elementary events \\(A = \\{a_1, a_2, \\ldots\\}\\). This includes as special cases the full set \\(A = \\Omega\\), the empty set \\(A = \\emptyset\\), and the elementary events \\(A=\\omega_i\\). The complementary event \\(A^C\\) is the complement of the set \\(A\\) in the set \\(\\Omega\\) so that \\(A^C = \\Omega \\setminus A = \\{\\omega_i \\in \\Omega: \\omega_i \\notin A\\}\\). The probability of an event is denoted by \\(\\text{Pr}(A)\\). We assume that \\(\\text{Pr}(A) \\geq 0\\), probabilities are positive, \\(\\text{Pr}(\\Omega) = 1\\), the certain event has probability 1, and \\(\\text{Pr}(A) = \\sum_{a_i \\in A} \\text{Pr}(a_i)\\), the probability of an event equals the sum of its constituting elementary events \\(a_i\\). This implies \\(\\text{Pr}(A) \\leq 1\\), i.e. probabilities all lie in the interval \\([0,1]\\) \\(\\text{Pr}(A^C) = 1 - \\text{Pr}(A)\\), and \\(\\text{Pr}(\\emptyset) = 0\\) Assume now we have two events \\(A\\) and \\(B\\). The probability of the event “\\(A\\) and \\(B\\)” is then given by the probability of the set intersection \\(\\text{Pr}(A \\cap B)\\). Likewise the probability of the event “\\(A\\) or \\(B\\)” is given by the probability of the set union \\(\\text{Pr}(A \\cup B)\\). From the above it is clear that probability theory is closely linked to set theory, and in particular to measure theory. This allows for an unified treatment of discrete and continuous random variables (an elegant framework but not needed for this module). A.5.2 Probability mass and density function and distribution and quantile function To describe a random variable \\(x\\) we need to assign probabilities to the corresponding elementary outcomes \\(x \\in \\Omega\\). For convenience we use the same name to denote the random variable and the elementary outcomes. For a discrete random variable we employ a probability mass function (PMF). We denote the it by a lower case \\(f\\) but occasionally we also use \\(p\\) or \\(q\\). In the discrete case we can define the event \\(A = \\{x: x=a\\} = \\{a\\}\\) and obtain the probability directly from the PMF: \\[\\text{Pr}(A) = \\text{Pr}(x=a) =f(a) \\,.\\] The PMF has the property that \\(\\sum_{x \\in \\Omega} f(x) = 1\\) and that \\(f(x) \\in [0,1]\\). For continuous random variables we need to use a probability density function (PDF) instead. We define the event \\(A = \\{x: a &lt; x \\leq a + da\\}\\) as an infinitesimal interval and then assign the probability \\[ \\text{Pr}(A) = \\text{Pr}( a &lt; x \\leq a + da) = f(a) da \\,. \\] The PDF has the property that \\(\\int_{x \\in \\Omega} f(x) dx = 1\\) but in contrast to a PMF the density \\(f(x)\\geq 0\\) may take on values larger than 1. Assuming an ordering we can define the event \\(A = \\{x: x \\leq a \\}\\) and compute its probability \\[ F(a) = \\text{Pr}(A) = \\text{Pr}( x \\leq a ) = \\begin{cases} \\sum_{x \\in A} f(x) &amp; \\text{discrete case} \\\\ \\int_{x \\in A} f(x) dx &amp; \\text{continuous case} \\\\ \\end{cases} \\] This is known as the distribution function, or cumulative distribution function (CDF) and is denoted by upper case \\(F\\) if the corresponding PDF/PMF is \\(f\\) (or \\(P\\) and \\(Q\\) if the corresponding PDF/PMF are \\(p\\) and \\(q\\)). By construction the distribution function is monotonically increasing and its value ranges from 0 to 1. With its help we can compute the probability of general interval sets such as \\[ \\text{Pr}( a &lt; x \\leq b ) = F(b)-F(a) \\,. \\] The inverse of the distribution function \\(y=F(x)\\) is the quantile function \\(x=F^{-1}(y)\\). The 50% quantile \\(F^{-1}\\left(\\frac{1}{2}\\right)\\) is the median. If the random variable \\(x\\) has distribution function \\(F\\) we write \\(x \\sim F\\). A.5.3 Expection and variance of a random variable The expected value \\(\\text{E}(x)\\) of a random variable is defined as weighted average over all possible outcomes: \\[ \\text{E}(x) = \\begin{cases} \\sum_{x \\in \\Omega} x f(x) &amp; \\text{discrete case} \\\\ \\int_{x \\in \\Omega} x f(x) dx &amp; \\text{continuous case} \\\\ \\end{cases} \\] The expectation is not necessarily always defined for a continuous random variable as the integral can diverge. The expected value of a function of a random variable \\(h(x)\\) is obtained similarly: \\[ \\text{E}(h(x)) = \\begin{cases} \\sum_{x \\in \\Omega} h(x) f(x) &amp; \\text{discrete case} \\\\ \\int_{x \\in \\Omega} h(x) f(x) dx &amp; \\text{continuous case} \\\\ \\end{cases} \\] This is called the “law of the unconscious statistician”, or short LOTUS. For an event \\(A\\) we can define a corresponding indicator function \\[ 1_A(x) = \\begin{cases} 1 &amp; x \\in A\\\\ 0 &amp; x \\notin A\\\\ \\end{cases} \\] Intriguingly, \\[ \\text{E}(1_A(x) ) = \\text{Pr}(A) \\] i.e. the expectation of the indicator variable for \\(A\\) is the probability of \\(A\\). The moments of random variables are also defined by expectation: Zeroth moment: \\(\\text{E}(x^0) = 1\\) by definition of PDF and PMF, First moment: \\(\\text{E}(x^1) = \\text{E}(x) = \\mu\\) , the mean, Second moment: \\(\\text{E}(x^2)\\) The variance is the second momented centered about the mean \\(\\mu\\): \\[\\text{Var}(x) = \\text{E}( (x - \\mu)^2 ) = \\sigma^2\\] The variance can also be computed by \\(\\text{Var}(x) = \\text{E}(x^2)-\\text{E}(x)^2\\). A distribution does not necessarily need to have any finite first or higher moments. An example is the the Cauchy distribution that does not have a mean or variance (or any other higher moment). A.5.4 Transformation of random variables Linear transformation of random variables: if \\(a\\) and \\(b\\) are constants and \\(x\\) is a random variable, then the random variable \\(y= a + b x\\) has mean \\(\\text{E}(y) = a + b \\text{E}(x)\\) and variance \\(\\text{Var}(y) = b^2 \\text{Var}(x)\\). For a general invertible coordinate transformation \\(y = h(x) = y(x)\\) the backtransformation is \\(x = h^{-1}(y) = x(y)\\). The transformation of the infinitesimal volume element is \\(dy = |\\frac{dy}{dx}| dx\\). The transformation of the density is \\(f_y(y) =\\left|\\frac{dx}{dy}\\right| f_x(x(y))\\). Note that \\(\\left|\\frac{dx}{dy}\\right| = \\left|\\frac{dy}{dx}\\right|^{-1}\\). A.5.5 Law of large numbers: By the strong law of large numbers the empirical distribution \\(\\hat{F}_n\\) converges to the true underlying distribution \\(F\\) as \\(n \\rightarrow \\infty\\) almost surely: \\[ \\hat{F}_n\\overset{a. s.}{\\to} F \\] The Glivenko–Cantelli theorem asserts that the convergence is uniform. Since the strong law implies the weak law we also have convergence in probability: \\[ \\hat{F}_n\\overset{P}{\\to} F \\] Correspondingly, for \\(n \\rightarrow \\infty\\) the average \\(\\text{E}_{\\hat{F}_n}(h(X)) = \\frac{1}{n} \\sum_{i=1}^n h(x_i)\\) converges to the expectation \\(\\text{E}_{F}(h(X))\\). A.5.6 Jensen’s inequality \\[\\text{E}(h(\\boldsymbol x)) \\geq h(\\text{E}(\\boldsymbol x))\\] for a convex function \\(h(\\boldsymbol x)\\). Recall: a convex function (such as \\(x^2\\)) has the shape of a “valley”. A.6 Distributions A.6.1 Bernoulli and Binomial distribution The Bernoulli distribution \\(\\text{Ber}(p)\\) is simplest distribution possible. It is named after Jacob Bernoulli (1655-1705) who also invented the law of large numbers. It describes a discrete binary random variable with two states \\(x=0\\) (“failure”) and \\(x=1\\) (“success”), where the parameter \\(p \\in [0,1]\\) is the probability of “success”. Often the Bernoulli distribution is also referred to as “coin tossing” model with the two outcomes “heads” and “tails”. Correspondingly, the probability mass function of \\(\\text{Ber}(p)\\) is \\[ f(x=0) = \\text{Pr}(\\text{&quot;failure&quot;}) = 1-p \\] and \\[ f(x=1) = \\text{Pr}(\\text{&quot;success&quot;}) = p \\] A compact way to write the PMF of the Bernoulli distribution is \\[ f(x | p ) = p^{x} (1-p)^{1-x} \\] If a random variable \\(x\\) follows the Bernoulli distribution we write \\[ x \\sim \\text{Ber}(p) \\,. \\] The expected value is \\(\\text{E}(x) = p\\) and the variance is \\(\\text{Var}(x) = p (1 - p)\\). Closely related to the Bernoulli distribution is the Binomial distribution \\(\\text{Bin}(m, p)\\) which results from repeating a Bernoulli experiment \\(m\\) times and counting the number of successes among the \\(m\\) trials (without keeping track of the ordering of the experiments). Its probability mass function is: \\[ f(x | p) = \\binom{m}{x} p^x (1 - p)^{m - x} \\] for \\(x = 0, 1, 2, \\ldots, m\\). The Binomial coefficient \\(\\binom{m}{x}\\) is needed to account for the multiplicity of ways (orderings of samples) in which we can observe \\(x\\) sucesses. The expected value is \\(\\text{E}(x) = mp\\) and the variance is \\(\\text{Var}(x) = mp (1 - p)\\). If a random variable \\(x\\) follows the Binomial distribution we write \\[ x \\sim \\text{Bin}(m, p)\\, \\] For \\(m=1\\) it reduces to the Bernoulli distribution \\(\\text{Ber}(p)\\). In R the PMF of the Binomial distribution is called dbinom(). The Binomial coefficient itself is computed by choose(). A.6.2 Normal distribution Univariate normal distribution: \\(x \\sim N(\\mu,\\sigma^2)\\) with \\(\\text{E}(x)=\\mu\\) and \\(\\text{Var}(x) = \\sigma^2\\). Probability density function (PDF): \\[f(x| \\mu, \\sigma^2)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\] In R the density function is called dnorm(). The standard normal distribution is \\(N(0, 1)\\) with mean 1 and variance 1. Plot of the PDF of the standard normal: The cumulative distribution function (CDF) of the standard normal \\(N(0,1)\\) is \\[ \\Phi (x ) = \\int_{-\\infty}^{x} f(x&#39;| \\mu=0, \\sigma^2=1) dx&#39; \\] There is no analytic expression for \\(\\Phi(x)\\). In R the function is called pnorm(). The inverse \\(\\Phi^{-1}(p)\\) is called the quantile function of the standard normal. In R the function is called qnorm(). The sum of two normal random variables is also normal (with the appropriate mean and variance). A.6.3 Scaled chi-squared / Wishart / gamma distribution and exponential distribution Assume \\(m\\) independent normal random variables \\[z_1,z_2,\\dots,z_m\\sim N(0,\\sigma^2)\\] Then the sum of the squares \\[ x = \\sum_{i=1}^{m} z_i^2 \\] follows a scaled chi-squared distribution \\[ x\\sim \\sigma^2 \\text{$\\chi^2_{m}$} \\] with degree of freedom \\(m\\) and \\(x \\geq 0\\). The mean and variance of a scaled chi-squared distributed variable is \\(\\text{E}(x)=m \\sigma^2\\) and \\(\\text{Var}(x)=2m\\sigma^4\\). Another name for the scaled chi-squared distribution is univariate Wishart distribution \\(W_1(\\sigma^2, m)\\) which uses the same parameters. The gamma distribution \\(\\text{Gam}(\\alpha, \\beta)\\) is a further variant of the scaled chi-squared distribution which uses a different parameterisation in terms of a shape parameter \\(\\alpha\\) and a scale parameter \\(\\beta\\). The scaled chi-squared distribution \\(\\sigma^2 \\text{$\\chi^2_{m}$}\\) equals \\(\\text{Gam}(\\frac{m}{2}, 2 \\sigma^2)\\). The mean of \\(\\text{Gam}(\\alpha, \\beta)\\) is \\(\\alpha \\beta\\) and its variance is \\(\\alpha \\beta^2\\). The chi-squared distribution is a special case with \\(\\sigma^2=1\\) with mean \\(\\text{E}(x)=m\\) and variance \\(\\text{Var}(x)=2m\\). The chi-squared distribution \\(\\text{$\\chi^2_{m}$}\\) equals \\(\\text{Gam}(\\frac{m}{2}, 2)\\). The exponential distribution \\(\\text{Exp}(\\beta)\\) with scale parameter \\(\\beta\\) (and mean \\(\\beta\\) and variance \\(\\beta^2\\)) is another special case of the gamma distribution with shape parameter \\(\\alpha=1\\). Instead of the scale parameter the exponential distribution is also often specified using a rate parameter \\(\\lambda= \\frac{1}{\\beta}\\). Here is a plot of the density of the chi-squared distribution for degrees of freedom \\(m=1\\) and \\(m=3\\): In R the density of the chi-squared distribution is given by dchisq(). The cumulative density function is pchisq() and the quantile function is qchisq(). The density of the gamma distribution (aka scaled chi-squared distribution) is available in the R function dgamma(). The cumulative density function is pgamma() and the quantile function is qgamma(). A.7 Statistics A.7.1 Statistical learning The aim in statistics - data science - machine learning is to learn from data (from experiments, observations, measurements) to learn about and understand the world. Specifically, to identify the best model(s) for the data in order to to explain the current data, and to enable good prediction of future data Note that it is easy to get models that only explain the data but do not predict well! This is called overfitting the data and happens in particular if the model is overparameterized for the amount of data available. Specifically, we have data \\(x_1, \\ldots, x_n\\) and models \\(f(x| \\theta)\\) that are indexed the parameter \\(\\theta\\). Often (but not always) \\(\\theta\\) can be interpreted and/or is associated with some property of the model. If there is only a single parameter we write \\(\\theta\\) (scalar parameter). For a parameter vector we write \\(\\boldsymbol \\theta\\) (in bold type). A.7.2 Point and interval estimation There is a parameter \\(\\theta\\) of interest in a model we are uncertain about this parameter (i.e. we don’t know the exact value) we would like to learn about this parameter by observing data \\(x_1, \\ldots, x_n\\) from the model Estimation: An estimator for \\(\\theta\\) is a function \\(\\hat{\\theta}(x_1, \\ldots, x_n)\\) that maps the data (input) to a “guess” (output) about \\(\\theta\\). A point estimator provides a single number for each parameter An interval estimator provides a set of possible values for each parameter. A.7.3 Sampling properties of a point estimator \\(\\hat{\\boldsymbol \\theta}\\) A point estimator \\(\\hat\\theta\\) depends on the data, hence it has sampling variation (i.e. estimate will be different for a new set of observations) Thus \\(\\hat\\theta\\) can be seen as a random variable, and its distribution is called sampling distribution (accross different experiments). Properties of this distribution can be used to evaluate how far the estimator deviates (on average across different experiments) from the true value: \\[\\begin{align*} \\begin{array}{rr} \\text{Bias:}\\\\ \\text{Variance:}\\\\ \\text{Mean squared error:}\\\\ \\\\ \\end{array} \\begin{array}{rr} \\text{Bias}(\\hat{\\theta})\\\\ \\text{Var}(\\hat{\\theta})\\\\ \\text{MSE}(\\hat{\\theta})\\\\ \\\\ \\end{array} \\begin{array}{ll} =\\text{E}(\\hat{\\theta})-\\theta\\\\ =\\text{E}\\left((\\hat{\\theta}-\\text{E}(\\hat{\\theta}))^2\\right)\\\\ =\\text{E}((\\hat{\\theta}-\\theta)^2)\\\\ =\\text{Var}(\\hat{\\theta})+\\text{Bias}(\\hat{\\theta})^2\\\\ \\end{array} \\end{align*}\\] The last identity about MSE follows from \\(\\text{E}(X^2)=\\text{Var}(X)+\\text{E}(X)^2\\). At first sight it seems desirable to focus on unbiased (for finite \\(n\\)) estimators. However, requiring strict unbiasedness is not always a good idea! In many situations it is better to allow for some small bias and in order to achieve a smaller variance and an overall total smaller MSE. This is called bias-variance tradeoff — as more bias is traded for smaller variance (or, conversely, less bias is traded for higher variance) A.7.4 Asymptotics Typically, \\(\\text{Bias}\\), \\(\\text{Var}\\) and \\(\\text{MSE}\\) all decrease with increasing sample size so that with more data \\(n \\to \\infty\\) the errors become smaller and smaller. The typical rate of decrease of variance of a good estimator is \\(\\frac{1}{n}\\). Thus, when sample size is doubled the variance is divided by 2 (and the standard deviation is divided by \\(\\sqrt{2}\\)). Consistency: \\(\\hat{\\theta}\\) is called consistent if \\[\\text{MSE}(\\hat{\\theta}) \\longrightarrow 0 \\text{ with $n\\rightarrow \\infty$ }\\]. Consistency implies we recover the true model in the limit of infinite data and if the model class contains the true model. Consistency is a minimum essential requirement for any reasonable estimator! Of all consistent estimators we typically prefer the estimator that is most efficient (i.e. with fasted decrease in MSE) and that thus has smallest variance and/or MSE for given finite \\(n\\). Note that if the model class does not contain the true model then strict consistency cannot be achived but we still wish to get at least as close as possible to the true model. A.7.5 Confidence intervals A confidence interval (CI) is an interval estimate with a frequentist interpretation. Definition of coverage \\(\\kappa\\) of a CI: how often (in repeated identical experiment) does the estimated CI overlap the true parameter value \\(\\theta\\) Eg.: Coverage \\(\\kappa=0.95\\) (95%) means that in 95 out of 100 case the estimated CI will contain the (unknown) true value (i.e. it will “cover” \\(\\theta\\)). Illustration of the repeated construction of a CI for \\(\\theta\\): Note that a CI is actually an estimate: \\(\\widehat{\\text{CI}}(x_1, \\ldots, x_n)\\), i.e. it depends on data and has a random (sampling) variation. A good CI has high coverage and is compact. Note: the coverage probability is not the probability that the true value is contained in a given estimated interval (that would be the Bayesian Credible Interval). A.7.6 Symmetric normal confidence interval For a normally distributed univariate random variable it is straightforward to construct a symmetric two-sided CI with a given desired coverage \\(\\kappa\\). For a normal random variable \\(X \\sim N(\\mu, \\sigma^2)\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\) and density function \\(f(x)\\) we can compute the probability \\[\\text{Pr}(X \\leq \\mu + c \\sigma) = \\int_{-\\infty}^{\\mu+c\\sigma} f(x) dx = \\Phi (c) = \\frac{1+\\kappa}{2}\\] Note \\(\\Phi(c)\\) is the cumulative distribution function (CDF) of the standard normal \\(N(0,1)\\): From the above we obtain the critical point \\(c\\) from the quantile function, i.e. by inversion of \\(\\Phi\\): \\[c=\\Phi^{-1}\\left(\\frac{1+\\kappa}{2}\\right)\\] The following table lists \\(c\\) for the three most commonly used values of \\(\\kappa\\) - it is useful to memorise these values! Coverage \\(\\kappa\\) Critical value \\(c\\) 0.9 1.64 0.95 1.96 0.99 2.58 A symmetric standard normal CI with nominal coverage \\(\\kappa\\) for a scalar parameter \\(\\theta\\) with normally distributed estimate \\(\\hat{\\theta}\\) and with estimated standard deviation \\(\\hat{\\text{SD}}(\\hat{\\theta}) = \\hat{\\sigma}\\) is then given by \\[\\widehat{\\text{CI}}=[\\hat{\\theta} \\pm c \\hat{\\sigma}]\\] where \\(c\\) is chosen for desired coverage level \\(\\kappa\\). A.7.7 Confidence interval for chi-squared distribution As for the normal CI we can compute critical values but for the chi-squared distribution we use a one-sided interval: \\[ \\text{Pr}(X \\leq c) = \\kappa \\] As before we get \\(c\\) by the quantile function, i.e. by inverting the CDF of the chi-squared distribution. The following list the critical values for the three most common choice of \\(\\kappa\\) for \\(m=1\\) (one degree of freedom): Coverage \\(\\kappa\\) Critical value \\(c\\) (\\(m=1\\)) 0.9 2.71 0.95 3.84 0.99 6.63 A one-sided CI with nominal coverage \\(\\kappa\\) is then given by \\([0, c ]\\). "],["21-further-study.html", "B Further study B.1 Recommended reading B.2 Additional references", " B Further study In this module we can only touch the surface of likelihood and Bayes inference. As a starting point for further reading the following text books are recommended. B.1 Recommended reading Held and Bové (2014) Applied Statistical Inference: Likelihood and Bayes. Springer. Faraway (2015) Linear Models with R (second edition). Chapman and Hall/CRC. B.2 Additional references Wood (2015) Core Statistics. Cambridge University Press. Gelman et al. (2014) Bayesian data analysis (3rd edition). CRC Press. "],["22-references.html", "Bibliography", " Bibliography "]]
