<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>6 Optimality properties and conclusion | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="6 Optimality properties and conclusion | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="6 Optimality properties and conclusion | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><meta name="description" content="6.1 Properties of maximum likelihood encountered so far MLE is a special case of relative entropy minimisation valid for large samples. MLE can be seen as generalisation of least squares (and...">
<meta property="og:description" content="6.1 Properties of maximum likelihood encountered so far MLE is a special case of relative entropy minimisation valid for large samples. MLE can be seen as generalisation of least squares (and...">
<meta name="twitter:description" content="6.1 Properties of maximum likelihood encountered so far MLE is a special case of relative entropy minimisation valid for large samples. MLE can be seen as generalisation of least squares (and...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="active" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="optimality-properties-and-conclusion" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Optimality properties and conclusion<a class="anchor" aria-label="anchor" href="#optimality-properties-and-conclusion"><i class="fas fa-link"></i></a>
</h1>
<div id="properties-of-maximum-likelihood-encountered-so-far" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Properties of maximum likelihood encountered so far<a class="anchor" aria-label="anchor" href="#properties-of-maximum-likelihood-encountered-so-far"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>MLE is a special case of relative entropy minimisation <em>valid for large samples</em>.</li>
<li>MLE can be seen as generalisation of least squares (and conversely, least squares is a special case of ML).</li>
</ol>
<p><span class="math display">\[\begin{align*}
\begin{array}{cc}
\text{Kullback-Leibler 1951}\\
\textbf{Entropy learning: minimise  } D_{\text{KL}}(F_{\text{true}},F_{\boldsymbol \theta})\\
\downarrow\\
\text{large } n\\
\downarrow\\
\text{Fisher 1922}\\
\textbf{Maximise Likelihood  } L(\boldsymbol \theta|x_1, \dots, x_n)\\
\downarrow\\
\text{normal model}\\
\downarrow\\
\text{Gauss 1805}\\
\textbf{Minimise squared error  } \sum_i (x_i-\theta)^2\\
\end{array}
\end{align*}\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p>Given a model, derivation of the MLE is basically automatic (only optimisation required)!</p></li>
<li><p>MLEs are <strong>consistent</strong>, i.e. if the true underlying model <span class="math inline">\(F_{\text{true}}\)</span> with parameter <span class="math inline">\(\boldsymbol \theta_{\text{true}}\)</span> is contained in the set of specified candidates models <span class="math inline">\(F_{\boldsymbol \theta}\)</span>
then the MLE will converge to the true model.</p></li>
<li><p>Correspondingly, <strong>MLEs are asympotically unbiased</strong>.</p></li>
<li><p>However, MLEs are <em>not</em> necessarily unbiased in finite samples
(e.g. the MLE of the variance parameter in the normal distribution).</p></li>
<li><p>The maximum likelihood is invariant against parameter transformations.</p></li>
<li><p>In regular situations (when local quadratic approximation is possible)
MLEs are <strong>asympotically normally distributed</strong>, with the asymptotic variance determined by the
observed Fisher information.</p></li>
<li><p>In regular situations and for large sample size MLEs are <strong>asympotically optimally efficient</strong> (Cramer-Rao theorem): For large samples the MLE achieves the lowest possible variance possible in an estimator — this is the so-called Cramer-Rao lower bound. The variance decreases to zero with <span class="math inline">\(n \rightarrow \infty\)</span> typically with rate <span class="math inline">\(1/n\)</span>.</p></li>
<li><p>The likelihood ratio can be used to construct optimal tests (in the sense of the Neyman-Pearson theorem).</p></li>
</ol>
</div>
<div id="summarising-data-and-the-concept-of-minimal-sufficiency" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Summarising data and the concept of minimal sufficiency<a class="anchor" aria-label="anchor" href="#summarising-data-and-the-concept-of-minimal-sufficiency"><i class="fas fa-link"></i></a>
</h2>
<p>Another important concept in statistics and likelihood theory (especially when applied to exponential family models)
is that of a <strong>minimally sufficient statistic</strong> to optimally summarise the information available in the data about a parameter in a model.</p>
<p>Generally, a <strong>statistic</strong> <span class="math inline">\(T(x_1, \ldots, x_n)= T(x_i)\)</span> is function of the data <span class="math inline">\(x_1, \ldots, x_n\)</span>. In the following we write <span class="math inline">\(x_i\)</span> as a shorthand for the complete data set with
<span class="math inline">\(n\)</span> observations.
The statistic <span class="math inline">\(T(x_i)\)</span> can be of any type and value (scalar, vector, matrix etc. — even a function). <span class="math inline">\(T(x_i)\)</span> is called a <em>summary statistic</em> if it describes important
aspects of the data such as location (e.g. the average <span class="math inline">\(\text{avg}(x_i) =\bar{x}\)</span>, the median) or scale (e.g. standard
deviation, interquartile range).</p>
<p>A statistic <span class="math inline">\(T(x_i)\)</span> is said to be <strong>sufficient</strong> for
a parameter <span class="math inline">\(\boldsymbol \theta\)</span> in a model if the corresponding likelihood function can be written in terms of <span class="math inline">\(T(x_i)\)</span> so that
<span class="math display">\[
L(\boldsymbol \theta| x_i) = h( T(x_i) , \boldsymbol \theta) \, k(x_i) \,,
\]</span>
where <span class="math inline">\(h(x)\)</span> and <span class="math inline">\(k(x)\)</span> are positive-valued functions, and or equivalently on log-scale
<span class="math display">\[
l_n(\boldsymbol \theta) = \log h( T(x_i) , \boldsymbol \theta) + \log k(x_i) \,.
\]</span>
This is known as the <strong>Fisher-Pearson factorisation</strong>.
By construction, estimation and inference about <span class="math inline">\(\boldsymbol \theta\)</span> based on the factorised likelihood <span class="math inline">\(L(\boldsymbol \theta)\)</span> is mediated through the sufficient statistic <span class="math inline">\(T(x_i)\)</span> and does not require the original data <span class="math inline">\(x_i\)</span>. Instead, the sufficient statistic <span class="math inline">\(T(x_i)\)</span> contains all the information in <span class="math inline">\(x_i\)</span> required to learn about the parameter <span class="math inline">\(\boldsymbol \theta\)</span>.
Therefore, if the MLE <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> of <span class="math inline">\(\boldsymbol \theta\)</span> exists and is unique then <strong>the MLE is a unique function of the sufficient statistic <span class="math inline">\(T(x_i)\)</span></strong>. If the MLE is not unique then it can be chosen to be function of <span class="math inline">\(T(x_i)\)</span>.
Note that <strong>a sufficient statistic always exists</strong> since the data <span class="math inline">\(x_i\)</span>
are themselves sufficient statistics, with <span class="math inline">\(T(x_i) = x_i\)</span>. Furthermore, sufficient statistics are <strong>not unique</strong> since applying a one-to-one transformation to
<span class="math inline">\(T(x_i)\)</span> yields another sufficient statistic.</p>
<p>Every sufficient statistic <span class="math inline">\(T(x_i)\)</span> induces a partitioning of the space of data sets
by clustering all hypothetical outcomes for which the statistic <span class="math inline">\(T(x_i)\)</span> assumes the same value <span class="math inline">\(t\)</span>:
<span class="math display">\[\mathcal{X}_t = \{x_i: T(x_i) = t\}\]</span>
The <strong>data sets in <span class="math inline">\(\mathcal{X}_t\)</span> are equivalent in terms of the sufficient statistic <span class="math inline">\(T(x_i)\)</span></strong>. Note that the dimensions of <span class="math inline">\(T(x_i)\)</span> may be much smaller than
those of <span class="math inline">\(x_i\)</span>.
Instead of <span class="math inline">\(n\)</span> data points as few as one or two summaries may be sufficient to fully convey all the information in the data about the model parameters.
Thus, transforming data <span class="math inline">\(x_i\)</span> using a sufficient statistic <span class="math inline">\(T(x_i)\)</span> may result in substantial <strong>data reduction</strong>.</p>
<p>Data sets <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> for which the ratio of the
likelihoods
<span class="math inline">\(L(\boldsymbol \theta| x_i )/L(\boldsymbol \theta| y_i)\)</span> does not depend on <span class="math inline">\(\boldsymbol \theta\)</span> (so the two likelihoods
are proportional to each other by a constant)
are called <strong>likelihood equivalent</strong> because a likelihood-based procedure
to learn about <span class="math inline">\(\boldsymbol \theta\)</span> will draw identical conclusions from <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>.
For data sets <span class="math inline">\(x_i, y_i \in \mathcal{X}_t\)</span> equivalent with respect
to a sufficient statistic <span class="math inline">\(T(x_i)\)</span>
it follows directly from the Fisher-Pearson factorisation that
the ratio
<span class="math display">\[L(\boldsymbol \theta| x_i )/L(\boldsymbol \theta| y_i) = k(x_i)/ k(y_i)\]</span>
and thus is constant with regard to <span class="math inline">\(\boldsymbol \theta\)</span>. Consequently, all <strong>data sets in <span class="math inline">\(\mathcal{X}_t\)</span> are also likelihood equivalent</strong>.
However, the converse is not true: depending on the sufficient statistics there usually will be many likelihood equivalent data
sets that are not part of the same set <span class="math inline">\(\mathcal{X}_t\)</span>.</p>
<p>Of particular interest is therefore to find those sufficient statistics that achieve the coarsest partitioning of the sample space and thus may allow the highest data reduction.
Specifically, a <strong>minimal sufficient statistic</strong> is a sufficient statistic <span class="math inline">\(T(x_i)\)</span>
for which all likelihood equivalent data sets also are equivalent under <span class="math inline">\(T(x_i)\)</span>.
Therefore, to check whether a sufficient statistic <span class="math inline">\(T(x_i\)</span> is minimally sufficient we verify whether for any two likelihood equivalent data sets <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>
it also follows that <span class="math inline">\(T(x_i) = T(y_i)\)</span>. If this holds true
then <span class="math inline">\(T(x_i)\)</span> is a minimally sufficient statistic.</p>
<p>An equivalent non-operational definition is that a minimal sufficient statistic <span class="math inline">\(T(x_i)\)</span> is a sufficient statistic that can be computed from any other sufficient statistic <span class="math inline">\(S(x_i)\)</span>. This follows from the above directly: assume any sufficient statistic <span class="math inline">\(S(x_i)\)</span>, this defines
a corresponding set <span class="math inline">\(\mathcal{X}_s\)</span> of likelihood equivalent data sets. By implication
any <span class="math inline">\(x_i, y_i \in \mathcal{X}_s\)</span> will ncecessarily also be in <span class="math inline">\(\mathcal{X}_t\)</span>, thus
whenever <span class="math inline">\(S(x_i)=S(y_i)\)</span> we also have <span class="math inline">\(T(x_i)=T(y_i)\)</span>, and therefore
<span class="math inline">\(T(x_i)\)</span> is a function of <span class="math inline">\(S(x_i)\)</span>.</p>
<p>A trivial but <strong>important example of
a minimal sufficient statistic is the likelihood function itself</strong>
since by definition it can be computed from any set of sufficient statistics. Thus the likelihood function <span class="math inline">\(L(\boldsymbol \theta)\)</span> captures all information about <span class="math inline">\(\boldsymbol \theta\)</span> that is available in the data. In other words, it provides an <em>optimal summary</em> of the observed data with regard to a model. Note that in Bayesian statistics (to be discussed in Part 2 of the module) the likelihood function is used as proxy/summary of the data.</p>
<div class="example">
<p><span id="exm:suffstatnormal" class="example"><strong>Example 6.1  </strong></span>Sufficient statistics for the parameters of the normal distribution:</p>
<p>The normal model <span class="math inline">\(N(\mu, \sigma^2)\)</span>
with parameter vector <span class="math inline">\(\boldsymbol \theta= (\mu, \sigma^2)^T\)</span> and log-likelihood
<span class="math display">\[
l_n(\boldsymbol \theta) = -\frac{n}{2} \log(2 \pi \sigma^2)  - \frac{1}{2 \sigma^2} \sum_{i=1}^n (x_i-\mu)^2
\]</span>
One possible set of minimal sufficient statistics for <span class="math inline">\(\boldsymbol \theta\)</span> are <span class="math inline">\(\bar{x}\)</span>
and <span class="math inline">\(\overline{x^2}\)</span>, and with these we can rewrite the log-likelihood function
without any reference to the original data <span class="math inline">\(x_i\)</span> as follows
<span class="math display">\[
l_n(\boldsymbol \theta) = -\frac{n}{2} \log(2 \pi \sigma^2) 
-\frac{n}{2 \sigma^2} (\overline{x^2} - 2 \bar{x} \mu + \mu^2)
\]</span>
An alternative set of minimal sufficient statistics for <span class="math inline">\(\boldsymbol \theta\)</span>
consists of <span class="math inline">\(s^2 = \overline{x^2} - \bar{x}^2 = \widehat{\sigma^2}_{ML}\)</span> as
and <span class="math inline">\(\bar{x} = \hat{\mu}_{ML}\)</span>. The log-likelihood written in terms of <span class="math inline">\(s^2\)</span> and <span class="math inline">\(\bar{x}\)</span> is
<span class="math display">\[
l_n(\boldsymbol \theta) = -\frac{n}{2} \log(2 \pi \sigma^2) 
-\frac{n}{2 \sigma^2} (s^2 + (\bar{x} - \mu)^2 )
\]</span></p>
<p>Note that in this example the dimension of the parameter
vector <span class="math inline">\(\boldsymbol \theta\)</span> equals the dimension of the minimal sufficient statistic,
and furthermore, that the MLEs of the parameters are in fact minimal sufficient!</p>
</div>
<p>The conclusion from Examples <a href="optimality-properties-and-conclusion.html#exm:suffstatnormal">6.1</a>
holds true more generally: in an exponential family model (such as the normal distribution as particular important case) the MLEs of the parameters are minimal sufficient statistics.
Thus, there will typically be substantial dimension reduction from the raw data to the sufficient statistics.</p>
<p>However, outside exponential families
the MLE is not necessarily a minimal sufficient statistic, and may not even be a sufficient statistic.
This is because <strong>a (minimal) sufficient statistic of the same dimension as the
parameters does not always exist</strong>. A classic example is the Cauchy distribution for
which the minimal sufficient statistics are the ordered observations,
thus the MLE of the parameters do not constitute sufficient statistics, let alone minimal sufficient statistics.
However, the MLE is of course still a function of the minimal sufficient statistic.</p>
<p>In summary, the likelihood function acts as perfect data summariser
(i.e. as minimally sufficient statistic),
and in exponential families (e.g. Normal distribution) the
MLEs of the parameters <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span> are minimimally sufficient.</p>
<p>Finally, while sufficiency is clearly a useful concept for data reduction one needs to keep in mind that this is always in reference to a specific model Therefore, unless one strongly believes in a certain model it is generally a good idea to keep (and not discard!) the original data.</p>
</div>
<div id="concluding-remarks-on-maximum-likelihood" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Concluding remarks on maximum likelihood<a class="anchor" aria-label="anchor" href="#concluding-remarks-on-maximum-likelihood"><i class="fas fa-link"></i></a>
</h2>
<div id="remark-on-kl-divergence" class="section level3" number="6.3.1">
<h3>
<span class="header-section-number">6.3.1</span> Remark on KL divergence<a class="anchor" aria-label="anchor" href="#remark-on-kl-divergence"><i class="fas fa-link"></i></a>
</h3>
<p>Finding the model <span class="math inline">\(F_{\boldsymbol \theta}\)</span> that best approximates the underlying true model <span class="math inline">\(F_0\)</span>
is done by minimising the relative entropy <span class="math inline">\(D_{\text{KL}}(F_0,F_{\boldsymbol \theta})\)</span>. For large sample size <span class="math inline">\(n\)</span>
we may approximate <span class="math inline">\(F_0\)</span> by the empirical distribution<span class="math inline">\(\hat{F}_0\)</span>,
and minimising <span class="math inline">\(D_{\text{KL}}(\hat{F}_0,F_{\boldsymbol \theta})\)</span> then yields the method of maximum likelihood.</p>
<p>However, since the KL divergence is not symmetric there are in fact two ways to minimise the divergence
between a fixed <span class="math inline">\(F_0\)</span> and the optimised <span class="math inline">\(F_{\boldsymbol \theta}\)</span>, each with different properties:</p>
<ol style="list-style-type: lower-alpha">
<li>
<p><strong>forward KL</strong>, <strong>approximation KL</strong>: <span class="math inline">\(\min_{\boldsymbol \theta} D_{\text{KL}}(F_0,F_{\boldsymbol \theta}\)</span></p>
<p>This is also called an “M (Moment) projection”. It has a <strong>zero avoiding</strong> property:
<span class="math inline">\(f_{\boldsymbol \theta}(x)&gt;0 \text{ whenever } f_0(x)&gt;0\)</span></p>
</li>
<li>
<p><strong>reverse KL</strong>, <strong>inference KL</strong>: <span class="math inline">\(\min_{\boldsymbol \theta} D_{\text{KL}}(F_{\boldsymbol \theta},F_0)\)</span></p>
<p>This is also called an “I (Information) projection”. It has a <strong>zero forcing</strong> property:
<span class="math inline">\(f_{\boldsymbol \theta}(x)=0 \text{ whenever } f_0(x)=0\)</span></p>
</li>
</ol>
<p>Maximum likelihood is based on “forward KL”, whereas Bayesian updating and Variational Bayes
approximations use “reverse KL”.</p>
</div>
<div id="what-happens-if-n-is-small" class="section level3" number="6.3.2">
<h3>
<span class="header-section-number">6.3.2</span> What happens if <span class="math inline">\(n\)</span> is small?<a class="anchor" aria-label="anchor" href="#what-happens-if-n-is-small"><i class="fas fa-link"></i></a>
</h3>
<p>From the long list of optimality properties of ML
it is clear that for large sample size <span class="math inline">\(n\)</span> the best estimator will typically be the MLE.</p>
<p>However, for <strong>small sample size it is indeed possible (and necessary) to improve over the MLE</strong> (e.g. via Bayesian estimation or regularisation). Some of these ideas will be discussed in Part II.</p>
<ul>
<li>Likelihood will <em>overfit</em>!</li>
</ul>
<p>Alternative methods need to be used:</p>
<ul>
<li>regularised/penalised likelihood</li>
<li>Bayesian methods</li>
</ul>
<p>which are essentially two sides of the same coin.</p>
<p>Classic example of a simple non-ML estimator that is better than the MLE:
<strong>Stein’s example / Stein paradox</strong> (C. Stein, 1955):</p>
<ul>
<li><p>Problem setting: estimation of the mean in multivariate case</p></li>
<li><p>Maximum likelihood estimation breaks down! <span class="math inline">\(\rightarrow\)</span> average (=MLE) is worse in terms of MSE than Stein estimator.</p></li>
<li><p>For small <span class="math inline">\(n\)</span> the asymptotic distributions for the MLE and for the LRT are not accurate, so for inference in these situations the distributions may need to be obtained by simulation
(e.g. parametric or nonparametric bootstrap).</p></li>
</ul>
</div>
<div id="model-selection" class="section level3" number="6.3.3">
<h3>
<span class="header-section-number">6.3.3</span> Model selection<a class="anchor" aria-label="anchor" href="#model-selection"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>CI are sets of models that are not statistically distinguishable from the best ML model</li>
<li>in doubt, choose the simplest model compatible with data</li>
<li>better prediction, avoids overfitting</li>
<li>Useful for model exploration and model building.</li>
</ul>
<div class="inline-figure"><img src="fig/lecture7_p4.PNG" width="80%" style="display: block; margin: auto;"></div>
<ul>
<li><p>Note that, by construction, the model with more parameters always has a higher likelihood, implying likelihood favours complex models</p></li>
<li><p>Complex model may overfit!<br></p></li>
<li><p>For comparison of models penalised likelihood or Bayesian approaches may be necessary</p></li>
<li><p>Model selection in small samples and high dimension is challenging</p></li>
<li><p>Recall that the aim in statistics is <strong>not</strong> about rejecting models (this is easy as for large sample size any model will be rejected!)<br></p></li>
<li><p>Instead, the aim is model building, i.e. to find a model that <strong>explains the data well</strong> and that <strong>predicts well</strong>!<br></p></li>
<li><p>Typically, this will not be the best-fit ML model, but rather a simpler model that is close enough to the best / most complex model.</p></li>
</ul>
</div>
</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></div>
<div class="next"><a href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#optimality-properties-and-conclusion"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li><a class="nav-link" href="#properties-of-maximum-likelihood-encountered-so-far"><span class="header-section-number">6.1</span> Properties of maximum likelihood encountered so far</a></li>
<li><a class="nav-link" href="#summarising-data-and-the-concept-of-minimal-sufficiency"><span class="header-section-number">6.2</span> Summarising data and the concept of minimal sufficiency</a></li>
<li>
<a class="nav-link" href="#concluding-remarks-on-maximum-likelihood"><span class="header-section-number">6.3</span> Concluding remarks on maximum likelihood</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#remark-on-kl-divergence"><span class="header-section-number">6.3.1</span> Remark on KL divergence</a></li>
<li><a class="nav-link" href="#what-happens-if-n-is-small"><span class="header-section-number">6.3.2</span> What happens if \(n\) is small?</a></li>
<li><a class="nav-link" href="#model-selection"><span class="header-section-number">6.3.3</span> Model selection</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 16 May 2021.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
