<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>13 False discovery rates | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="active" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="false-discovery-rates" class="section level1">
<h1>
<span class="header-section-number">13</span> False discovery rates<a class="anchor" aria-label="anchor" href="#false-discovery-rates"><i class="fas fa-link"></i></a>
</h1>
<div id="general-setup" class="section level2">
<h2>
<span class="header-section-number">13.1</span> General setup<a class="anchor" aria-label="anchor" href="#general-setup"><i class="fas fa-link"></i></a>
</h2>
<div id="overview-1" class="section level3">
<h3>
<span class="header-section-number">13.1.1</span> Overview<a class="anchor" aria-label="anchor" href="#overview-1"><i class="fas fa-link"></i></a>
</h3>
<p>In this chapter we introduce False Discovery Rates (FDR) as a Bayesian method to
distinguish a null model from an alternative model. This is closely linked with classical
frequentist multiple testing procedures.</p>
</div>
<div id="choosing-between-h_0-and-h_a" class="section level3">
<h3>
<span class="header-section-number">13.1.2</span> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span><a class="anchor" aria-label="anchor" href="#choosing-between-h_0-and-h_a"><i class="fas fa-link"></i></a>
</h3>
<p>We consider two models:</p>
<p><span class="math inline">\(H_0:\)</span> null model, with density <span class="math inline">\(f_0(x)\)</span> and distribution <span class="math inline">\(F_0(x)\)</span></p>
<p><span class="math inline">\(H_A:\)</span> alternative model, with density <span class="math inline">\(f_A(x)\)</span> and distribution <span class="math inline">\(F_A(x)\)</span></p>
<p>Aim: given observations <span class="math inline">\(x_1, \ldots, x_n\)</span> we would like to decide for each <span class="math inline">\(x_i\)</span> whether
it belongs to <span class="math inline">\(H_0\)</span> or <span class="math inline">\(H_A\)</span>.</p>
<p>This is done by a critical decision threshold <span class="math inline">\(x_c\)</span>: if <span class="math inline">\(x_i &gt; x_c\)</span> then <span class="math inline">\(x_i\)</span> is called “significant” and otherwise called “not significant”.</p>
<p>In classical statistics one of the the most widely used approach to find the decision threshold is by computing <span class="math inline">\(p\)</span>-values from the <span class="math inline">\(x_i\)</span>
(this uses only the null model but not the alternative model), and then thresholding the <span class="math inline">\(p\)</span>-values a a certain level (say 5%). If <span class="math inline">\(n\)</span> is large then often the test is modified by adjusting the <span class="math inline">\(p\)</span>-values or the threshold (e.g. if Bonferroni correction).</p>
<p>Note that this procedure ignores any information we may have about the alternative model!</p>
</div>
<div id="true-and-false-positives-and-negatives" class="section level3">
<h3>
<span class="header-section-number">13.1.3</span> True and false positives and negatives<a class="anchor" aria-label="anchor" href="#true-and-false-positives-and-negatives"><i class="fas fa-link"></i></a>
</h3>
<p>For any decision threshold <span class="math inline">\(x_c\)</span> we can distinguish the following errors:</p>
<ul>
<li>False positives (FP), “false alarm”, type I error: <span class="math inline">\(x_i\)</span> belongs to null but is called “significant”</li>
<li>False negative (FN), “miss”, type II error: <span class="math inline">\(x_i\)</span> belongs to alternative, but is called “not significant”</li>
</ul>
<p>In addition we have:</p>
<ul>
<li>True positives (TP), “hits”: belongs to alternative and is called “significant”</li>
<li>True negatives (TN), “correct rejections”: belongs to null and is called “not significant”</li>
</ul>
</div>
</div>
<div id="specificity-and-sensitivity" class="section level2">
<h2>
<span class="header-section-number">13.2</span> Specificity and Sensitivity<a class="anchor" aria-label="anchor" href="#specificity-and-sensitivity"><i class="fas fa-link"></i></a>
</h2>
<p>From counts of TP, TN, FN, FP we can derive further quantities:</p>
<ul>
<li>True Negative Rate TNR, <strong>specificity</strong>: <span class="math inline">\(TNR= \frac{TN}{TN+FP} = 1- FPR\)</span> with FPR=False Positive Rate = <span class="math inline">\(1-\alpha_I\)</span>
</li>
<li><p>True Positive Rate TPR, <strong>sensitivity</strong>, <strong>power</strong>, recall: <span class="math inline">\(TPR= \frac{TP}{TP+FN} = 1- FNR\)</span> with FNR=False negative rate = <span class="math inline">\(1-\alpha_{II}\)</span></p></li>
<li><p>Accuracy: <span class="math inline">\(ACC = \frac{TP+TN}{TP+TN+FP+FN}\)</span></p></li>
</ul>
<p>Another common way to choose the decision threshold <span class="math inline">\(x_d\)</span> in classical statistics is to balance sensitivity/power vs. specificity (maximising both power and specificity, or equivalently, minimising both false positive and false negative rates). ROC curves plot TPR/sensitivity vs. FPR = 1-specificity.</p>
</div>
<div id="fdr-and-fndr" class="section level2">
<h2>
<span class="header-section-number">13.3</span> FDR and FNDR<a class="anchor" aria-label="anchor" href="#fdr-and-fndr"><i class="fas fa-link"></i></a>
</h2>
<p>It is possible to link the above with the observed counts of TP, FP, TN, FN:</p>
<ul>
<li>False Discovery Rate (FDR): <span class="math inline">\(FDR = \frac{FP}{FP+TP}\)</span>
</li>
<li>False Nondiscovery Rate (FNDR): <span class="math inline">\(FNDR = \frac{FN}{TN+FN}\)</span>
</li>
<li>Positive predictive value (PPV), True Discovery Rate (TDR), precision: <span class="math inline">\(PPV = \frac{TP}{FP+TP} = 1-FDR\)</span>
</li>
<li>Negative predictive value (NPV): <span class="math inline">\(NPV = \frac{TN}{TN+FN} = 1-FNDR\)</span>
</li>
</ul>
<p>In order to choose the decision threshold it is natural to balance FDR and FDNR (or PPV and NPV), by minimising both FDR and FNDR or maximising both PPV and NPV.</p>
<p>In machine learning it is common to use “precision-recall plots” that plot precision (=PPV, TDR)
vs. recall (=power, sensitivity).</p>
</div>
<div id="bayesian-perspective" class="section level2">
<h2>
<span class="header-section-number">13.4</span> Bayesian perspective<a class="anchor" aria-label="anchor" href="#bayesian-perspective"><i class="fas fa-link"></i></a>
</h2>
<div id="two-component-mixture-model" class="section level3">
<h3>
<span class="header-section-number">13.4.1</span> Two component mixture model<a class="anchor" aria-label="anchor" href="#two-component-mixture-model"><i class="fas fa-link"></i></a>
</h3>
<p>In the Bayesian perspective the problem of choosing the decision threshold is related to computing the posterior probability
<span class="math display">\[\text{Pr}(H_0 | x_i) , \]</span>
i.e. probability of the null model given the observation <span class="math inline">\(x_i\)</span>, or equivalently
computing
<span class="math display">\[\text{Pr}(H_A | x_i) = 1- \text{Pr}(H_0 | x_i)\]</span>
the probability of the alternative model given the observation <span class="math inline">\(x_i\)</span>.</p>
<p>This is done by assuming a mixture model
<span class="math display">\[
f(x) = \pi_0 f_0(x) + (1-\pi_0) f_A(x)
\]</span>
where <span class="math inline">\(\pi_0 = \text{Pr}(H_0)\)</span> is the prior probability of <span class="math inline">\(H_0\)</span> and.
<span class="math inline">\(\pi_A = 1- \pi_0 = \text{Pr}(H_A)\)</span> the prior probabiltiy of <span class="math inline">\(H_A\)</span>.</p>
<p>Note that the weights <span class="math inline">\(\pi_0\)</span> can in fact be estimated from the observations by fitting the mixture distribution
to the observations <span class="math inline">\(x_1, \ldots, x_n\)</span> (which implies that this yields a form of empirical Bayes method).</p>
</div>
<div id="local-fdr" class="section level3">
<h3>
<span class="header-section-number">13.4.2</span> Local FDR<a class="anchor" aria-label="anchor" href="#local-fdr"><i class="fas fa-link"></i></a>
</h3>
<p>The posterior probability of the null model given a data point is then given by
<span class="math display">\[\text{Pr}(H_0 | x_i) = \frac{\pi_0 f_0(x_i)}{f(x_i)} = LFDR(x_i)\]</span>
This quantity is also known as the <strong>local FDR</strong> or <strong>local False Discovery Rate</strong>.</p>
<p>In the given one-sided setup the local FDR is large (close to 1) for small <span class="math inline">\(x\)</span>, and
will become close to 0 for large <span class="math inline">\(x\)</span>. A common decision rule is given by thresholding
local false discovery rates: if <span class="math inline">\(LFDR(x_i) &lt; 0.1\)</span> the <span class="math inline">\(x_i\)</span> is called significant.</p>
</div>
<div id="q-values" class="section level3">
<h3>
<span class="header-section-number">13.4.3</span> q-values<a class="anchor" aria-label="anchor" href="#q-values"><i class="fas fa-link"></i></a>
</h3>
<p>In correspondence to <span class="math inline">\(p\)</span>-values one can also define tail-area based false discovery rates:
<span class="math display">\[
Fdr(x_i) = \text{Pr}(H_0 | X &gt; x_i) = \frac{\pi_0 F_0(x_i)}{F(x_i)} 
\]</span></p>
<p>These are called <strong>q-values</strong>, or simply <strong>False Discovery Rates (FDR)</strong>. Intriguingly, these also have a frequentist
interpretation as adjusted p-values (using a Benjamini-Hochberg adjustment procedure).</p>
</div>
</div>
<div id="software" class="section level2">
<h2>
<span class="header-section-number">13.5</span> Software<a class="anchor" aria-label="anchor" href="#software"><i class="fas fa-link"></i></a>
</h2>
<p>There are a number of R packages to compute (local) FDR values:</p>
<p>For example:</p>
<ul>
<li>locfdr</li>
<li>qvalue</li>
<li>fdrtool</li>
</ul>
<p>and many more.</p>
<p>Using FDR values for screening is especially useful in high-dimensional settings
(e.g. when analysing genomic and other high-throughput data).</p>
<p>FDR values have both a Bayesian as well as frequentist interpretation, providing further evidence that
good classical statistical methods do have a Bayesian interpretation.</p>

<p></p>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></div>
<div class="next"><a href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#false-discovery-rates"><span class="header-section-number">13</span> False discovery rates</a></li>
<li>
<a class="nav-link" href="#general-setup"><span class="header-section-number">13.1</span> General setup</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#overview-1"><span class="header-section-number">13.1.1</span> Overview</a></li>
<li><a class="nav-link" href="#choosing-between-h_0-and-h_a"><span class="header-section-number">13.1.2</span> Choosing between \(H_0\) and \(H_A\)</a></li>
<li><a class="nav-link" href="#true-and-false-positives-and-negatives"><span class="header-section-number">13.1.3</span> True and false positives and negatives</a></li>
</ul>
</li>
<li><a class="nav-link" href="#specificity-and-sensitivity"><span class="header-section-number">13.2</span> Specificity and Sensitivity</a></li>
<li><a class="nav-link" href="#fdr-and-fndr"><span class="header-section-number">13.3</span> FDR and FNDR</a></li>
<li>
<a class="nav-link" href="#bayesian-perspective"><span class="header-section-number">13.4</span> Bayesian perspective</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#two-component-mixture-model"><span class="header-section-number">13.4.1</span> Two component mixture model</a></li>
<li><a class="nav-link" href="#local-fdr"><span class="header-section-number">13.4.2</span> Local FDR</a></li>
<li><a class="nav-link" href="#q-values"><span class="header-section-number">13.4.3</span> q-values</a></li>
</ul>
</li>
<li><a class="nav-link" href="#software"><span class="header-section-number">13.5</span> Software</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 14 May 2021.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
