<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Likelihood-based confidence interval and likelihood ratio | HTML</title>
  <meta name="description" content="Statistical Methods:<br />
Likelihood, Bayes and Regression</div>" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Likelihood-based confidence interval and likelihood ratio | HTML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Likelihood-based confidence interval and likelihood ratio | HTML" />
  
  
  



<meta name="date" content="2021-03-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="04-likelihood4.html"/>
<link rel="next" href="06-likelihood6.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://strimmerlab.org/publications/lecture-notes/MATH20802/index.html">MATH20802 Lecture Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-module"><i class="fa fa-check"></i>About the module</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#topics-covered"><i class="fa fa-check"></i>Topics covered</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-support-material"><i class="fa fa-check"></i>Additional support material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Likelihood estimation and inference</b></span></li>
<li class="chapter" data-level="1" data-path="01-likelihood1.html"><a href="01-likelihood1.html"><i class="fa fa-check"></i><b>1</b> Overview of statistical learning</a><ul>
<li class="chapter" data-level="1.1" data-path="01-likelihood1.html"><a href="01-likelihood1.html#how-to-learn-from-data"><i class="fa fa-check"></i><b>1.1</b> How to learn from data?</a></li>
<li class="chapter" data-level="1.2" data-path="01-likelihood1.html"><a href="01-likelihood1.html#probability-theory-versus-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Probability theory versus statistical learning</a></li>
<li class="chapter" data-level="1.3" data-path="01-likelihood1.html"><a href="01-likelihood1.html#cartoon-of-statistical-learning"><i class="fa fa-check"></i><b>1.3</b> Cartoon of statistical learning</a></li>
<li class="chapter" data-level="1.4" data-path="01-likelihood1.html"><a href="01-likelihood1.html#likelihood"><i class="fa fa-check"></i><b>1.4</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="02-likelihood2.html"><a href="02-likelihood2.html"><i class="fa fa-check"></i><b>2</b> From entropy to maximum likelihood</a><ul>
<li class="chapter" data-level="2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy"><i class="fa fa-check"></i><b>2.1</b> Entropy</a><ul>
<li class="chapter" data-level="2.1.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#surprise-surprisal-or-shannon-information"><i class="fa fa-check"></i><b>2.1.2</b> Surprise, surprisal or Shannon information</a></li>
<li class="chapter" data-level="2.1.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#shannon-entropy"><i class="fa fa-check"></i><b>2.1.3</b> Shannon entropy</a></li>
<li class="chapter" data-level="2.1.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#differential-entropy"><i class="fa fa-check"></i><b>2.1.4</b> Differential entropy</a></li>
<li class="chapter" data-level="2.1.5" data-path="02-likelihood2.html"><a href="02-likelihood2.html#maximum-entropy-principle-to-characterise-distributions"><i class="fa fa-check"></i><b>2.1.5</b> Maximum entropy principle to characterise distributions</a></li>
<li class="chapter" data-level="2.1.6" data-path="02-likelihood2.html"><a href="02-likelihood2.html#cross-entropy"><i class="fa fa-check"></i><b>2.1.6</b> Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>2.2</b> Kullback-Leibler divergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#definition"><i class="fa fa-check"></i><b>2.2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#properties-of-kl-divergence"><i class="fa fa-check"></i><b>2.2.2</b> Properties of KL divergence</a></li>
<li class="chapter" data-level="2.2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#examples"><i class="fa fa-check"></i><b>2.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#local-quadratic-approximation-and-expected-fisher-information"><i class="fa fa-check"></i><b>2.3</b> Local quadratic approximation and expected Fisher information</a><ul>
<li class="chapter" data-level="2.3.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#linear-approximation"><i class="fa fa-check"></i><b>2.3.1</b> Linear approximation</a></li>
<li class="chapter" data-level="2.3.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#quadratic-approximation"><i class="fa fa-check"></i><b>2.3.2</b> Quadratic approximation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="02-likelihood2.html"><a href="02-likelihood2.html#entropy-learning-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4</b> Entropy learning and maximum likelihood</a><ul>
<li class="chapter" data-level="2.4.1" data-path="02-likelihood2.html"><a href="02-likelihood2.html#the-relative-entropy-between-true-model-and-approximating-model"><i class="fa fa-check"></i><b>2.4.1</b> The relative entropy between true model and approximating model</a></li>
<li class="chapter" data-level="2.4.2" data-path="02-likelihood2.html"><a href="02-likelihood2.html#minimum-kl-divergence-and-maximum-likelihood"><i class="fa fa-check"></i><b>2.4.2</b> Minimum KL divergence and maximum likelihood</a></li>
<li class="chapter" data-level="2.4.3" data-path="02-likelihood2.html"><a href="02-likelihood2.html#further-connections"><i class="fa fa-check"></i><b>2.4.3</b> Further connections</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="03-likelihood3.html"><a href="03-likelihood3.html"><i class="fa fa-check"></i><b>3</b> Maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#principle-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.1</b> Principle of maximum likelihood estimation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#outline"><i class="fa fa-check"></i><b>3.1.1</b> Outline</a></li>
<li class="chapter" data-level="3.1.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#obtaining-mles-for-a-regular-model"><i class="fa fa-check"></i><b>3.1.2</b> Obtaining MLEs for a regular model</a></li>
<li class="chapter" data-level="3.1.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#invariance-property-of-the-maximum-likelihood"><i class="fa fa-check"></i><b>3.1.3</b> Invariance property of the maximum likelihood</a></li>
<li class="chapter" data-level="3.1.4" data-path="03-likelihood3.html"><a href="03-likelihood3.html#consistency-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>3.1.4</b> Consistency of maximum likelihood estimates</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#maximum-likelihood-estimation-in-practise"><i class="fa fa-check"></i><b>3.2</b> Maximum likelihood estimation in practise</a><ul>
<li class="chapter" data-level="3.2.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#worked-examples"><i class="fa fa-check"></i><b>3.2.1</b> Worked examples</a></li>
<li class="chapter" data-level="3.2.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#relationship-with-least-squares-estimation"><i class="fa fa-check"></i><b>3.2.2</b> Relationship with least squares estimation</a></li>
<li class="chapter" data-level="3.2.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#bias-and-maximum-likelihood"><i class="fa fa-check"></i><b>3.2.3</b> Bias and maximum likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#observed-fisher-information"><i class="fa fa-check"></i><b>3.3</b> Observed Fisher information</a><ul>
<li class="chapter" data-level="3.3.1" data-path="03-likelihood3.html"><a href="03-likelihood3.html#motivation-and-definition"><i class="fa fa-check"></i><b>3.3.1</b> Motivation and definition</a></li>
<li class="chapter" data-level="3.3.2" data-path="03-likelihood3.html"><a href="03-likelihood3.html#examples-of-observed-fisher-information"><i class="fa fa-check"></i><b>3.3.2</b> Examples of observed Fisher information</a></li>
<li class="chapter" data-level="3.3.3" data-path="03-likelihood3.html"><a href="03-likelihood3.html#relationship-between-observed-and-expected-fisher-information"><i class="fa fa-check"></i><b>3.3.3</b> Relationship between observed and expected Fisher information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="04-likelihood4.html"><a href="04-likelihood4.html"><i class="fa fa-check"></i><b>4</b> Quadratic approximation and normal asymptotics</a><ul>
<li class="chapter" data-level="4.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#multivariate-statistics-for-random-vectors"><i class="fa fa-check"></i><b>4.1</b> Multivariate statistics for random vectors</a><ul>
<li class="chapter" data-level="4.1.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#covariance-and-correlation"><i class="fa fa-check"></i><b>4.1.1</b> Covariance and correlation</a></li>
<li class="chapter" data-level="4.1.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.1.2</b> Multivariate normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#approximate-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.2</b> Approximate distribution of maximum likelihood estimates</a><ul>
<li class="chapter" data-level="4.2.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-log-likelihood-resulting-from-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Quadratic log-likelihood resulting from normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quadratic-approximation-of-a-log-likelihood-function"><i class="fa fa-check"></i><b>4.2.2</b> Quadratic approximation of a log-likelihood function</a></li>
<li class="chapter" data-level="4.2.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.2.3</b> Asymptotic normality of maximum likelihood estimates</a></li>
<li class="chapter" data-level="4.2.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#asymptotic-optimal-efficiency"><i class="fa fa-check"></i><b>4.2.4</b> Asymptotic optimal efficiency</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#quantifying-the-uncertainty-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>4.3</b> Quantifying the uncertainty of maximum likelihood estimates</a><ul>
<li class="chapter" data-level="4.3.1" data-path="04-likelihood4.html"><a href="04-likelihood4.html#estimating-the-variance-of-mles"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the variance of MLEs</a></li>
<li class="chapter" data-level="4.3.2" data-path="04-likelihood4.html"><a href="04-likelihood4.html#wald-statistic"><i class="fa fa-check"></i><b>4.3.2</b> Wald statistic</a></li>
<li class="chapter" data-level="4.3.3" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-confidence-intervals-using-the-wald-statistic"><i class="fa fa-check"></i><b>4.3.3</b> Normal confidence intervals using the Wald statistic</a></li>
<li class="chapter" data-level="4.3.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#normal-tests-using-the-wald-statistic"><i class="fa fa-check"></i><b>4.3.4</b> Normal tests using the Wald statistic</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="04-likelihood4.html"><a href="04-likelihood4.html#example-of-a-non-regular-model"><i class="fa fa-check"></i><b>4.4</b> Example of a non-regular model</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="05-likelihood5.html"><a href="05-likelihood5.html"><i class="fa fa-check"></i><b>5</b> Likelihood-based confidence interval and likelihood ratio</a><ul>
<li class="chapter" data-level="5.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-based-confidence-intervals-and-wilks-statistic"><i class="fa fa-check"></i><b>5.1</b> Likelihood-based confidence intervals and Wilks statistic</a><ul>
<li class="chapter" data-level="5.1.1" data-path="05-likelihood5.html"><a href="05-likelihood5.html#general-idea-and-definition-of-wilks-statistic"><i class="fa fa-check"></i><b>5.1.1</b> General idea and definition of Wilks statistic</a></li>
<li class="chapter" data-level="5.1.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#quadratic-approximation-of-wilks-statistic-and-squared-wald-statistic"><i class="fa fa-check"></i><b>5.1.2</b> Quadratic approximation of Wilks statistic and squared Wald statistic</a></li>
<li class="chapter" data-level="5.1.3" data-path="05-likelihood5.html"><a href="05-likelihood5.html#distribution-of-the-wilks-statistic"><i class="fa fa-check"></i><b>5.1.3</b> Distribution of the Wilks statistic</a></li>
<li class="chapter" data-level="5.1.4" data-path="05-likelihood5.html"><a href="05-likelihood5.html#cutoff-values-for-the-likelihood-ci"><i class="fa fa-check"></i><b>5.1.4</b> Cutoff values for the likelihood CI</a></li>
<li class="chapter" data-level="5.1.5" data-path="05-likelihood5.html"><a href="05-likelihood5.html#likelihood-ratio-test-lrt-using-wilks-statistic"><i class="fa fa-check"></i><b>5.1.5</b> Likelihood ratio test (LRT) using Wilks statistic</a></li>
<li class="chapter" data-level="5.1.6" data-path="05-likelihood5.html"><a href="05-likelihood5.html#origin-of-likelihood-ratio-statistic"><i class="fa fa-check"></i><b>5.1.6</b> Origin of likelihood ratio statistic</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="05-likelihood5.html"><a href="05-likelihood5.html#generalised-likelihood-ratio-test-glrt"><i class="fa fa-check"></i><b>5.2</b> Generalised likelihood ratio test (GLRT)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="06-likelihood6.html"><a href="06-likelihood6.html"><i class="fa fa-check"></i><b>6</b> Optimality properties and conclusion</a><ul>
<li class="chapter" data-level="6.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#properties-of-maximum-likelihood-encountered-so-far"><i class="fa fa-check"></i><b>6.1</b> Properties of maximum likelihood encountered so far</a></li>
<li class="chapter" data-level="6.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#summarising-data-and-the-concept-of-minimal-sufficiency"><i class="fa fa-check"></i><b>6.2</b> Summarising data and the concept of minimal sufficiency</a></li>
<li class="chapter" data-level="6.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#concluding-remarks-on-maximum-likelihood"><i class="fa fa-check"></i><b>6.3</b> Concluding remarks on maximum likelihood</a><ul>
<li class="chapter" data-level="6.3.1" data-path="06-likelihood6.html"><a href="06-likelihood6.html#remark-on-kl-divergence"><i class="fa fa-check"></i><b>6.3.1</b> Remark on KL divergence</a></li>
<li class="chapter" data-level="6.3.2" data-path="06-likelihood6.html"><a href="06-likelihood6.html#what-happens-if-n-is-small"><i class="fa fa-check"></i><b>6.3.2</b> What happens if <span class="math inline">\(n\)</span> is small?</a></li>
<li class="chapter" data-level="6.3.3" data-path="06-likelihood6.html"><a href="06-likelihood6.html#model-selection"><i class="fa fa-check"></i><b>6.3.3</b> Model selection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Bayesian Statistics</b></span></li>
<li class="chapter" data-level="7" data-path="07-bayes1.html"><a href="07-bayes1.html"><i class="fa fa-check"></i><b>7</b> Essentials of Bayesian statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#bayes-theorem"><i class="fa fa-check"></i><b>7.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#principle-of-bayesian-learning"><i class="fa fa-check"></i><b>7.2</b> Principle of Bayesian learning</a></li>
<li class="chapter" data-level="7.3" data-path="07-bayes1.html"><a href="07-bayes1.html#what-is-exactly-is-the-bayesian-estimate"><i class="fa fa-check"></i><b>7.3</b> What is exactly is the “Bayesian estimate”?</a></li>
<li class="chapter" data-level="7.4" data-path="07-bayes1.html"><a href="07-bayes1.html#computer-implementation-of-bayesian-learning"><i class="fa fa-check"></i><b>7.4</b> Computer implementation of Bayesian learning</a></li>
<li class="chapter" data-level="7.5" data-path="07-bayes1.html"><a href="07-bayes1.html#bayesian-interpretation-of-probability"><i class="fa fa-check"></i><b>7.5</b> Bayesian interpretation of probability</a><ul>
<li class="chapter" data-level="7.5.1" data-path="07-bayes1.html"><a href="07-bayes1.html#what-makes-you-bayesian"><i class="fa fa-check"></i><b>7.5.1</b> What makes you “Bayesian”?</a></li>
<li class="chapter" data-level="7.5.2" data-path="07-bayes1.html"><a href="07-bayes1.html#mathematics-of-probability"><i class="fa fa-check"></i><b>7.5.2</b> Mathematics of probability</a></li>
<li class="chapter" data-level="7.5.3" data-path="07-bayes1.html"><a href="07-bayes1.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.5.3</b> Interpretations of probability</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="07-bayes1.html"><a href="07-bayes1.html#historical-developments"><i class="fa fa-check"></i><b>7.6</b> Historical developments</a></li>
<li class="chapter" data-level="7.7" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning"><i class="fa fa-check"></i><b>7.7</b> Connection with entropy learning</a><ul>
<li class="chapter" data-level="7.7.1" data-path="07-bayes1.html"><a href="07-bayes1.html#zero-forcing-property"><i class="fa fa-check"></i><b>7.7.1</b> Zero forcing property</a></li>
<li class="chapter" data-level="7.7.2" data-path="07-bayes1.html"><a href="07-bayes1.html#connection-with-entropy-learning-1"><i class="fa fa-check"></i><b>7.7.2</b> Connection with entropy learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="08-bayes2.html"><a href="08-bayes2.html"><i class="fa fa-check"></i><b>8</b> Beta-Binomial model for estimating a proportion</a><ul>
<li class="chapter" data-level="8.1" data-path="08-bayes2.html"><a href="08-bayes2.html#binomial-likelihood"><i class="fa fa-check"></i><b>8.1</b> Binomial likelihood</a></li>
<li class="chapter" data-level="8.2" data-path="08-bayes2.html"><a href="08-bayes2.html#excursion-properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>8.2</b> Excursion: Properties of the Beta distribution</a></li>
<li class="chapter" data-level="8.3" data-path="08-bayes2.html"><a href="08-bayes2.html#beta-prior-distribution"><i class="fa fa-check"></i><b>8.3</b> Beta prior distribution</a></li>
<li class="chapter" data-level="8.4" data-path="08-bayes2.html"><a href="08-bayes2.html#computing-the-posterior-distribution"><i class="fa fa-check"></i><b>8.4</b> Computing the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="09-bayes3.html"><a href="09-bayes3.html"><i class="fa fa-check"></i><b>9</b> Properties of Bayesian learning</a><ul>
<li class="chapter" data-level="9.1" data-path="09-bayes3.html"><a href="09-bayes3.html#prior-acting-as-pseudo-data"><i class="fa fa-check"></i><b>9.1</b> Prior acting as pseudo-data</a></li>
<li class="chapter" data-level="9.2" data-path="09-bayes3.html"><a href="09-bayes3.html#linear-shrinkage-of-mean"><i class="fa fa-check"></i><b>9.2</b> Linear shrinkage of mean</a></li>
<li class="chapter" data-level="9.3" data-path="09-bayes3.html"><a href="09-bayes3.html#conjugacy-of-prior-and-posterior-distribution"><i class="fa fa-check"></i><b>9.3</b> Conjugacy of prior and posterior distribution</a></li>
<li class="chapter" data-level="9.4" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-asymptotics"><i class="fa fa-check"></i><b>9.4</b> Large sample asymptotics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="09-bayes3.html"><a href="09-bayes3.html#large-sample-limits-of-mean-and-variance"><i class="fa fa-check"></i><b>9.4.1</b> Large sample limits of mean and variance</a></li>
<li class="chapter" data-level="9.4.2" data-path="09-bayes3.html"><a href="09-bayes3.html#asymptotic-normality-of-the-posterior-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Asymptotic Normality of the Posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="09-bayes3.html"><a href="09-bayes3.html#posterior-variance-for-finite-n"><i class="fa fa-check"></i><b>9.5</b> Posterior variance for finite <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-bayes4.html"><a href="10-bayes4.html"><i class="fa fa-check"></i><b>10</b> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a><ul>
<li class="chapter" data-level="10.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-normal-model-to-estimate-mean"><i class="fa fa-check"></i><b>10.1</b> Normal-Normal model to estimate mean</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihood"><i class="fa fa-check"></i><b>10.1.1</b> Normal likelihood</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-prior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Normal prior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-posterior-distribution"><i class="fa fa-check"></i><b>10.1.3</b> Normal posterior distribution</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-and-stein-paradox"><i class="fa fa-check"></i><b>10.1.4</b> Large sample asymptotics and Stein paradox</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-normal-model-to-estimate-variance"><i class="fa fa-check"></i><b>10.2</b> Inverse-Gamma-Normal model to estimate variance</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Inverse Gamma distribution</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-bayes4.html"><a href="10-bayes4.html#normal-likelihoood"><i class="fa fa-check"></i><b>10.2.2</b> Normal likelihoood</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-prior-distribution"><i class="fa fa-check"></i><b>10.2.3</b> Inverse Gamma prior distribution</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-bayes4.html"><a href="10-bayes4.html#inverse-gamma-posterior-distribution"><i class="fa fa-check"></i><b>10.2.4</b> Inverse Gamma posterior distribution</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-bayes4.html"><a href="10-bayes4.html#large-sample-asymptotics-1"><i class="fa fa-check"></i><b>10.2.5</b> Large sample asymptotics</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-bayes4.html"><a href="10-bayes4.html#estimating-precision"><i class="fa fa-check"></i><b>10.2.6</b> Estimating precision</a></li>
<li class="chapter" data-level="10.2.7" data-path="10-bayes4.html"><a href="10-bayes4.html#joint-estimation-of-mean-and-variance"><i class="fa fa-check"></i><b>10.2.7</b> Joint estimation of mean and variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-bayes5.html"><a href="11-bayes5.html"><i class="fa fa-check"></i><b>11</b> Shrinkage estimation using empirical risk minimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="11-bayes5.html"><a href="11-bayes5.html#linear-shrinkage"><i class="fa fa-check"></i><b>11.1</b> Linear shrinkage</a></li>
<li class="chapter" data-level="11.2" data-path="11-bayes5.html"><a href="11-bayes5.html#james-stein-estimator"><i class="fa fa-check"></i><b>11.2</b> James-Stein estimator</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-bayes6.html"><a href="12-bayes6.html"><i class="fa fa-check"></i><b>12</b> Bayesian model comparison using Bayes factors and the BIC</a><ul>
<li class="chapter" data-level="12.1" data-path="12-bayes6.html"><a href="12-bayes6.html#the-bayes-factor"><i class="fa fa-check"></i><b>12.1</b> The Bayes factor</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-bayes6.html"><a href="12-bayes6.html#connection-with-relative-entropy"><i class="fa fa-check"></i><b>12.1.1</b> Connection with relative entropy</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-bayes6.html"><a href="12-bayes6.html#interpretation-of-and-scale-for-bayes-factor"><i class="fa fa-check"></i><b>12.1.2</b> Interpretation of and scale for Bayes factor</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-bayes6.html"><a href="12-bayes6.html#computing-textprd-m-for-simple-and-composite-models"><i class="fa fa-check"></i><b>12.1.3</b> Computing <span class="math inline">\(\text{Pr}(D | M)\)</span> for simple and composite models</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-bayes6.html"><a href="12-bayes6.html#bayes-factor-versus-likelihood-ratio"><i class="fa fa-check"></i><b>12.1.4</b> Bayes factor versus likelihood ratio</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-bayes6.html"><a href="12-bayes6.html#approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor"><i class="fa fa-check"></i><b>12.2</b> Approximate computation of the marginal likelihood and of the log-Bayes factor</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-bayes6.html"><a href="12-bayes6.html#schwarz-1978-approximation-of-log-marginal-likelihood"><i class="fa fa-check"></i><b>12.2.1</b> Schwarz (1978) approximation of log-marginal likelihood</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-bayes6.html"><a href="12-bayes6.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>12.2.2</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-bayes6.html"><a href="12-bayes6.html#approximating-the-weight-of-evidence-log-bayes-factor-with-bic"><i class="fa fa-check"></i><b>12.2.3</b> Approximating the weight of evidence (log-Bayes factor) with BIC</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-bayes6.html"><a href="12-bayes6.html#model-complexity-and-occams-razor"><i class="fa fa-check"></i><b>12.2.4</b> Model complexity and Occams razor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-bayes7.html"><a href="13-bayes7.html"><i class="fa fa-check"></i><b>13</b> False discovery rates</a><ul>
<li class="chapter" data-level="13.1" data-path="13-bayes7.html"><a href="13-bayes7.html#general-setup"><i class="fa fa-check"></i><b>13.1</b> General setup</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-bayes7.html"><a href="13-bayes7.html#overview-1"><i class="fa fa-check"></i><b>13.1.1</b> Overview</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-bayes7.html"><a href="13-bayes7.html#choosing-between-h_0-and-h_a"><i class="fa fa-check"></i><b>13.1.2</b> Choosing between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span></a></li>
<li class="chapter" data-level="13.1.3" data-path="13-bayes7.html"><a href="13-bayes7.html#true-and-false-positives-and-negatives"><i class="fa fa-check"></i><b>13.1.3</b> True and false positives and negatives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-bayes7.html"><a href="13-bayes7.html#specificity-and-sensitivity"><i class="fa fa-check"></i><b>13.2</b> Specificity and Sensitivity</a></li>
<li class="chapter" data-level="13.3" data-path="13-bayes7.html"><a href="13-bayes7.html#fdr-and-fndr"><i class="fa fa-check"></i><b>13.3</b> FDR and FNDR</a></li>
<li class="chapter" data-level="13.4" data-path="13-bayes7.html"><a href="13-bayes7.html#bayesian-perspective"><i class="fa fa-check"></i><b>13.4</b> Bayesian perspective</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-bayes7.html"><a href="13-bayes7.html#two-component-mixture-model"><i class="fa fa-check"></i><b>13.4.1</b> Two component mixture model</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-bayes7.html"><a href="13-bayes7.html#local-fdr"><i class="fa fa-check"></i><b>13.4.2</b> Local FDR</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-bayes7.html"><a href="13-bayes7.html#q-values"><i class="fa fa-check"></i><b>13.4.3</b> q-values</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="13-bayes7.html"><a href="13-bayes7.html#software"><i class="fa fa-check"></i><b>13.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-bayes8.html"><a href="14-bayes8.html"><i class="fa fa-check"></i><b>14</b> Optimality properties and summary</a><ul>
<li class="chapter" data-level="14.1" data-path="14-bayes8.html"><a href="14-bayes8.html#bayesian-statistics-in-a-nutshell"><i class="fa fa-check"></i><b>14.1</b> Bayesian statistics in a nutshell</a><ul>
<li class="chapter" data-level="14.1.1" data-path="14-bayes8.html"><a href="14-bayes8.html#remarks"><i class="fa fa-check"></i><b>14.1.1</b> Remarks</a></li>
<li class="chapter" data-level="14.1.2" data-path="14-bayes8.html"><a href="14-bayes8.html#advantages"><i class="fa fa-check"></i><b>14.1.2</b> Advantages</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="14-bayes8.html"><a href="14-bayes8.html#frequentist-properties-of-bayesian-estimators"><i class="fa fa-check"></i><b>14.2</b> Frequentist properties of Bayesian estimators</a></li>
<li class="chapter" data-level="14.3" data-path="14-bayes8.html"><a href="14-bayes8.html#specifying-the-prior-problem-or-advantage"><i class="fa fa-check"></i><b>14.3</b> Specifying the prior — problem or advantage?</a></li>
<li class="chapter" data-level="14.4" data-path="14-bayes8.html"><a href="14-bayes8.html#choosing-a-prior"><i class="fa fa-check"></i><b>14.4</b> Choosing a prior</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-bayes8.html"><a href="14-bayes8.html#some-guidelines"><i class="fa fa-check"></i><b>14.4.1</b> Some guidelines</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-bayes8.html"><a href="14-bayes8.html#jeffreys-prior"><i class="fa fa-check"></i><b>14.4.2</b> Jeffreys prior</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-bayes8.html"><a href="14-bayes8.html#optimality-of-bayes-inference"><i class="fa fa-check"></i><b>14.5</b> Optimality of Bayes inference</a></li>
<li class="chapter" data-level="14.6" data-path="14-bayes8.html"><a href="14-bayes8.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a><ul>
<li class="chapter" data-level="14.6.1" data-path="14-bayes8.html"><a href="14-bayes8.html#current-research"><i class="fa fa-check"></i><b>14.6.1</b> Current research</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Regression</b></span></li>
<li class="chapter" data-level="15" data-path="15-regression1.html"><a href="15-regression1.html"><i class="fa fa-check"></i><b>15</b> Overview over regression modelling</a><ul>
<li class="chapter" data-level="15.1" data-path="15-regression1.html"><a href="15-regression1.html#general-setup-1"><i class="fa fa-check"></i><b>15.1</b> General setup</a></li>
<li class="chapter" data-level="15.2" data-path="15-regression1.html"><a href="15-regression1.html#objectives"><i class="fa fa-check"></i><b>15.2</b> Objectives</a></li>
<li class="chapter" data-level="15.3" data-path="15-regression1.html"><a href="15-regression1.html#regression-as-a-form-of-supervised-learning"><i class="fa fa-check"></i><b>15.3</b> Regression as a form of supervised learning</a></li>
<li class="chapter" data-level="15.4" data-path="15-regression1.html"><a href="15-regression1.html#various-regression-models-used-in-statistics"><i class="fa fa-check"></i><b>15.4</b> Various regression models used in statistics</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="16-regression2.html"><a href="16-regression2.html"><i class="fa fa-check"></i><b>16</b> Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="16-regression2.html"><a href="16-regression2.html#the-linear-regression-model"><i class="fa fa-check"></i><b>16.1</b> The linear regression model</a></li>
<li class="chapter" data-level="16.2" data-path="16-regression2.html"><a href="16-regression2.html#interpretation-of-regression-coefficients-and-intercept"><i class="fa fa-check"></i><b>16.2</b> Interpretation of regression coefficients and intercept</a></li>
<li class="chapter" data-level="16.3" data-path="16-regression2.html"><a href="16-regression2.html#different-types-of-linear-regression"><i class="fa fa-check"></i><b>16.3</b> Different types of linear regression:</a></li>
<li class="chapter" data-level="16.4" data-path="16-regression2.html"><a href="16-regression2.html#distributional-assumptions-and-properties"><i class="fa fa-check"></i><b>16.4</b> Distributional assumptions and properties</a></li>
<li class="chapter" data-level="16.5" data-path="16-regression2.html"><a href="16-regression2.html#regression-in-data-matrix-notation"><i class="fa fa-check"></i><b>16.5</b> Regression in data matrix notation</a></li>
<li class="chapter" data-level="16.6" data-path="16-regression2.html"><a href="16-regression2.html#centering-and-vanishing-of-the-intercept-beta_0"><i class="fa fa-check"></i><b>16.6</b> Centering and vanishing of the intercept <span class="math inline">\(\beta_0\)</span></a></li>
<li class="chapter" data-level="16.7" data-path="16-regression2.html"><a href="16-regression2.html#regression-objectives-for-linear-model"><i class="fa fa-check"></i><b>16.7</b> Regression objectives for linear model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="17-regression3.html"><a href="17-regression3.html"><i class="fa fa-check"></i><b>17</b> Estimating regression coefficients</a><ul>
<li class="chapter" data-level="17.1" data-path="17-regression3.html"><a href="17-regression3.html#ordinary-least-squares-ols-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.1</b> Ordinary Least Squares (OLS) estimator of regression coefficients</a></li>
<li class="chapter" data-level="17.2" data-path="17-regression3.html"><a href="17-regression3.html#maximum-likelihood-estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>17.2</b> Maximum likelihood estimation of regression coefficients</a><ul>
<li class="chapter" data-level="17.2.1" data-path="17-regression3.html"><a href="17-regression3.html#detailed-derivation-of-the-mles"><i class="fa fa-check"></i><b>17.2.1</b> Detailed derivation of the MLEs</a></li>
<li class="chapter" data-level="17.2.2" data-path="17-regression3.html"><a href="17-regression3.html#asymptotics"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotics</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="17-regression3.html"><a href="17-regression3.html#covariance-plug-in-estimator-of-regression-coefficients"><i class="fa fa-check"></i><b>17.3</b> Covariance plug-in estimator of regression coefficients</a><ul>
<li class="chapter" data-level="17.3.1" data-path="17-regression3.html"><a href="17-regression3.html#importance-of-positive-definiteness-of-estimated-covariance-matrix"><i class="fa fa-check"></i><b>17.3.1</b> Importance of positive definiteness of estimated covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="17-regression3.html"><a href="17-regression3.html#best-linear-predictor"><i class="fa fa-check"></i><b>17.4</b> Best linear predictor</a><ul>
<li class="chapter" data-level="17.4.1" data-path="17-regression3.html"><a href="17-regression3.html#result"><i class="fa fa-check"></i><b>17.4.1</b> Result:</a></li>
<li class="chapter" data-level="17.4.2" data-path="17-regression3.html"><a href="17-regression3.html#irreducible-error"><i class="fa fa-check"></i><b>17.4.2</b> Irreducible Error</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="17-regression3.html"><a href="17-regression3.html#regression-by-conditioning"><i class="fa fa-check"></i><b>17.5</b> Regression by conditioning</a><ul>
<li class="chapter" data-level="17.5.1" data-path="17-regression3.html"><a href="17-regression3.html#general-idea"><i class="fa fa-check"></i><b>17.5.1</b> General idea:</a></li>
<li class="chapter" data-level="17.5.2" data-path="17-regression3.html"><a href="17-regression3.html#multivariate-normal-assumption"><i class="fa fa-check"></i><b>17.5.2</b> Multivariate normal assumption</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="17-regression3.html"><a href="17-regression3.html#standardised-regression-coefficients-and-relationship-to-correlation"><i class="fa fa-check"></i><b>17.6</b> Standardised regression coefficients and relationship to correlation</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="18-regression4.html"><a href="18-regression4.html"><i class="fa fa-check"></i><b>18</b> Squared multiple correlation and variance decomposition in linear regression</a><ul>
<li class="chapter" data-level="18.1" data-path="18-regression4.html"><a href="18-regression4.html#squared-multiple-correlation-omega2-and-the-r2-coefficient"><i class="fa fa-check"></i><b>18.1</b> Squared multiple correlation <span class="math inline">\(\Omega^2\)</span> and the <span class="math inline">\(R^2\)</span> coefficient</a><ul>
<li class="chapter" data-level="18.1.1" data-path="18-regression4.html"><a href="18-regression4.html#estimation-of-omega2-and-the-multiple-r2-coefficient"><i class="fa fa-check"></i><b>18.1.1</b> Estimation of <span class="math inline">\(\Omega^2\)</span> and the multiple <span class="math inline">\(R^2\)</span> coefficient</a></li>
<li class="chapter" data-level="18.1.2" data-path="18-regression4.html"><a href="18-regression4.html#r-output"><i class="fa fa-check"></i><b>18.1.2</b> R output</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="18-regression4.html"><a href="18-regression4.html#variance-decomposition-in-regression"><i class="fa fa-check"></i><b>18.2</b> Variance decomposition in regression</a><ul>
<li class="chapter" data-level="18.2.1" data-path="18-regression4.html"><a href="18-regression4.html#law-of-total-variance-and-variance-decomposition"><i class="fa fa-check"></i><b>18.2.1</b> Law of total variance and variance decomposition</a></li>
<li class="chapter" data-level="18.2.2" data-path="18-regression4.html"><a href="18-regression4.html#related-quantities"><i class="fa fa-check"></i><b>18.2.2</b> Related quantities</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="18-regression4.html"><a href="18-regression4.html#sample-version-of-variance-decomposition"><i class="fa fa-check"></i><b>18.3</b> Sample version of variance decomposition</a><ul>
<li class="chapter" data-level="18.3.1" data-path="18-regression4.html"><a href="18-regression4.html#geometric-interpretation-of-regression-as-orthogonal-projection"><i class="fa fa-check"></i><b>18.3.1</b> Geometric interpretation of regression as orthogonal projection:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="19-regression5.html"><a href="19-regression5.html"><i class="fa fa-check"></i><b>19</b> Prediction and variable selection</a><ul>
<li class="chapter" data-level="19.1" data-path="19-regression5.html"><a href="19-regression5.html#prediction-and-prediction-intervals"><i class="fa fa-check"></i><b>19.1</b> Prediction and prediction intervals</a></li>
<li class="chapter" data-level="19.2" data-path="19-regression5.html"><a href="19-regression5.html#variable-importance-and-prediction"><i class="fa fa-check"></i><b>19.2</b> Variable importance and prediction</a><ul>
<li class="chapter" data-level="19.2.1" data-path="19-regression5.html"><a href="19-regression5.html#how-to-quantify-variable-importance"><i class="fa fa-check"></i><b>19.2.1</b> How to quantify variable importance?</a></li>
<li class="chapter" data-level="19.2.2" data-path="19-regression5.html"><a href="19-regression5.html#some-candidates-for-vims"><i class="fa fa-check"></i><b>19.2.2</b> Some candidates for VIMs</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="19-regression5.html"><a href="19-regression5.html#regression-t-scores."><i class="fa fa-check"></i><b>19.3</b> Regression <span class="math inline">\(t\)</span>-scores.</a><ul>
<li class="chapter" data-level="19.3.1" data-path="19-regression5.html"><a href="19-regression5.html#computation"><i class="fa fa-check"></i><b>19.3.1</b> Computation</a></li>
<li class="chapter" data-level="19.3.2" data-path="19-regression5.html"><a href="19-regression5.html#connection-with-partial-correlation"><i class="fa fa-check"></i><b>19.3.2</b> Connection with partial correlation</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="19-regression5.html"><a href="19-regression5.html#further-approaches-for-variable-selection"><i class="fa fa-check"></i><b>19.4</b> Further approaches for variable selection</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="20-refresher.html"><a href="20-refresher.html"><i class="fa fa-check"></i><b>A</b> Refresher</a><ul>
<li class="chapter" data-level="A.1" data-path="20-refresher.html"><a href="20-refresher.html#basic-mathematical-notation"><i class="fa fa-check"></i><b>A.1</b> Basic mathematical notation</a></li>
<li class="chapter" data-level="A.2" data-path="20-refresher.html"><a href="20-refresher.html#vectors-and-matrices"><i class="fa fa-check"></i><b>A.2</b> Vectors and matrices</a></li>
<li class="chapter" data-level="A.3" data-path="20-refresher.html"><a href="20-refresher.html#functions"><i class="fa fa-check"></i><b>A.3</b> Functions</a><ul>
<li class="chapter" data-level="A.3.1" data-path="20-refresher.html"><a href="20-refresher.html#convex-and-concave-functions"><i class="fa fa-check"></i><b>A.3.1</b> Convex and concave functions</a></li>
<li class="chapter" data-level="A.3.2" data-path="20-refresher.html"><a href="20-refresher.html#gradient"><i class="fa fa-check"></i><b>A.3.2</b> Gradient</a></li>
<li class="chapter" data-level="A.3.3" data-path="20-refresher.html"><a href="20-refresher.html#hessian-matrix"><i class="fa fa-check"></i><b>A.3.3</b> Hessian matrix</a></li>
<li class="chapter" data-level="A.3.4" data-path="20-refresher.html"><a href="20-refresher.html#conditions-for-local-maximum-of-a-function"><i class="fa fa-check"></i><b>A.3.4</b> Conditions for local maximum of a function</a></li>
<li class="chapter" data-level="A.3.5" data-path="20-refresher.html"><a href="20-refresher.html#linear-and-quadratic-approximation"><i class="fa fa-check"></i><b>A.3.5</b> Linear and quadratic approximation</a></li>
<li class="chapter" data-level="A.3.6" data-path="20-refresher.html"><a href="20-refresher.html#functions-of-matrices"><i class="fa fa-check"></i><b>A.3.6</b> Functions of matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="20-refresher.html"><a href="20-refresher.html#probability"><i class="fa fa-check"></i><b>A.4</b> Probability</a><ul>
<li class="chapter" data-level="A.4.1" data-path="20-refresher.html"><a href="20-refresher.html#random-variables"><i class="fa fa-check"></i><b>A.4.1</b> Random variables</a></li>
<li class="chapter" data-level="A.4.2" data-path="20-refresher.html"><a href="20-refresher.html#transformation-of-random-variables"><i class="fa fa-check"></i><b>A.4.2</b> Transformation of random variables</a></li>
<li class="chapter" data-level="A.4.3" data-path="20-refresher.html"><a href="20-refresher.html#bernoulli-and-binomial-distribution"><i class="fa fa-check"></i><b>A.4.3</b> Bernoulli and Binomial distribution</a></li>
<li class="chapter" data-level="A.4.4" data-path="20-refresher.html"><a href="20-refresher.html#normal-distribution"><i class="fa fa-check"></i><b>A.4.4</b> Normal distribution</a></li>
<li class="chapter" data-level="A.4.5" data-path="20-refresher.html"><a href="20-refresher.html#scaled-chi-squared-distribution-and-gamma-and-exponential-distribution"><i class="fa fa-check"></i><b>A.4.5</b> Scaled Chi-squared distribution (and Gamma and Exponential distribution)</a></li>
<li class="chapter" data-level="A.4.6" data-path="20-refresher.html"><a href="20-refresher.html#law-of-large-numbers"><i class="fa fa-check"></i><b>A.4.6</b> Law of large numbers:</a></li>
<li class="chapter" data-level="A.4.7" data-path="20-refresher.html"><a href="20-refresher.html#jensens-inequality"><i class="fa fa-check"></i><b>A.4.7</b> Jensen’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="20-refresher.html"><a href="20-refresher.html#statistics"><i class="fa fa-check"></i><b>A.5</b> Statistics</a><ul>
<li class="chapter" data-level="A.5.1" data-path="20-refresher.html"><a href="20-refresher.html#statistical-learning"><i class="fa fa-check"></i><b>A.5.1</b> Statistical learning</a></li>
<li class="chapter" data-level="A.5.2" data-path="20-refresher.html"><a href="20-refresher.html#point-and-interval-estimation"><i class="fa fa-check"></i><b>A.5.2</b> Point and interval estimation</a></li>
<li class="chapter" data-level="A.5.3" data-path="20-refresher.html"><a href="20-refresher.html#sampling-properties-of-a-point-estimator-hatboldsymbol-theta"><i class="fa fa-check"></i><b>A.5.3</b> Sampling properties of a point estimator <span class="math inline">\(\hat{\boldsymbol \theta}\)</span></a></li>
<li class="chapter" data-level="A.5.4" data-path="20-refresher.html"><a href="20-refresher.html#asymptotics-1"><i class="fa fa-check"></i><b>A.5.4</b> Asymptotics</a></li>
<li class="chapter" data-level="A.5.5" data-path="20-refresher.html"><a href="20-refresher.html#confidence-intervals"><i class="fa fa-check"></i><b>A.5.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="A.5.6" data-path="20-refresher.html"><a href="20-refresher.html#symmetric-normal-confidence-interval"><i class="fa fa-check"></i><b>A.5.6</b> Symmetric normal confidence interval</a></li>
<li class="chapter" data-level="A.5.7" data-path="20-refresher.html"><a href="20-refresher.html#confidence-interval-for-chi-squared-distribution"><i class="fa fa-check"></i><b>A.5.7</b> Confidence interval for chi-squared distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="21-further-study.html"><a href="21-further-study.html"><i class="fa fa-check"></i><b>B</b> Further study</a><ul>
<li class="chapter" data-level="B.1" data-path="21-further-study.html"><a href="21-further-study.html#recommended-reading"><i class="fa fa-check"></i><b>B.1</b> Recommended reading</a></li>
<li class="chapter" data-level="B.2" data-path="21-further-study.html"><a href="21-further-study.html#additional-references"><i class="fa fa-check"></i><b>B.2</b> Additional references</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="22-references.html"><a href="22-references.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">Statistical Methods:<br />
Likelihood, Bayes and Regression</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="likelihood-based-confidence-interval-and-likelihood-ratio" class="section level1">
<h1><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</h1>
<div id="likelihood-based-confidence-intervals-and-wilks-statistic" class="section level2">
<h2><span class="header-section-number">5.1</span> Likelihood-based confidence intervals and Wilks statistic</h2>
<div id="general-idea-and-definition-of-wilks-statistic" class="section level3">
<h3><span class="header-section-number">5.1.1</span> General idea and definition of Wilks statistic</h3>
<p>Instead of relying on normal / quadratic approximation, we can also use the log-likelihood directly to find the so called <strong>likelihood confidence intervals</strong>:</p>
<p><img src="fig/lecture5_p5_2.PNG" width="80%" style="display: block; margin: auto;" /></p>
<p>Idea: find all <span class="math inline">\(\boldsymbol \theta_0\)</span> that have a log-likelihood that is almost as good as <span class="math inline">\(l_n(\hat{\boldsymbol \theta}_{ML})\)</span>.
<span class="math display">\[\text{CI}= \{\boldsymbol \theta_0: l_n(\hat{\boldsymbol \theta}_{ML}) - l_n(\boldsymbol \theta_0) \leq \Delta\}\]</span>
Here <span class="math inline">\(\Delta\)</span> is the tolerated deviation from the maximum log-likelihood.
We will see below how to determine a suitable <span class="math inline">\(\Delta\)</span> further below.</p>
<p>The above leads naturally to the <strong>Wilks log likelihood ratio statistic</strong> <span class="math inline">\(W(\boldsymbol \theta_0)\)</span> defined as:
<span class="math display">\[
\begin{split}
W(\boldsymbol \theta_0) &amp; = 2 \log \left(\frac{L(\hat{\boldsymbol \theta}_{ML})}{L(\boldsymbol \theta_0)}\right) \\
&amp; =2(l_n(\hat{\boldsymbol \theta}_{ML})-l_n(\boldsymbol \theta_0))\\
\end{split}
\]</span>
With its help we can write the likelihood CI follows:
<span class="math display">\[\text{CI}= \{\boldsymbol \theta_0: W(\boldsymbol \theta_0) \leq 2 \Delta\}\]</span></p>
<p>The Wilks statistic is named after <a href="https://en.wikipedia.org/wiki/Samuel_S._Wilks">Samuel S. Wilks (1906–1964)</a>.</p>
<p>Advantages of using a likelihood-based CI:</p>
<ul>
<li>not restricted to be symmetric</li>
<li>enables to construct multivariate CIs for parameter vector easily even in non-normal cases</li>
<li>contains normal CI as special case</li>
</ul>
<p><strong>Question</strong>: how to choose <span class="math inline">\(\Delta\)</span>, i.e how to calibrate the likelihood interval?<br />
Essentially, by comparing with normal CI!</p>
<div class="example">
<p><span id="exm:wilksproportion" class="example"><strong>Example 5.1  </strong></span>Wilks statistic for the proportion:</p>
<p>The log-likelihood for the parameter <span class="math inline">\(p\)</span> is (cf. Example <a href="03-likelihood3.html#exm:mleproportion">3.1</a>)
<span class="math display">\[
l_n(p) = n ( \bar{x} \log p + (1-\bar{x}) \log(1-p) )
\]</span>
Hence the Wilks statistic is
<span class="math display">\[
\begin{split}
W(p_0) &amp; = 2 ( l_n( \hat{p}_{ML} )  -l_n( p_0  ) )\\
&amp; = 2 n \left(  \bar{x} \log \left( \frac{  \bar{x}  }{p_0}  \right)  
                + (1-\bar{x}) \log \left( \frac{1-\bar{x} }{1-p_0}  \right)  
    \right) \\
\end{split}
\]</span></p>
<p>Comparing with Example <a href="02-likelihood2.html#exm:klbernoulli">2.7</a> we see that in this case the Wilks
statistic is essentially (apart from a scale factor <span class="math inline">\(2n\)</span>) the KL divergence between two
Bernoulli distributions:
<span class="math display">\[
W(p_0) =2 n D_{\text{KL}}( \text{Ber}( \hat{p}_{ML} ), \text{Ber}(p_0)  )
\]</span></p>
</div>
<div class="example">
<p><span id="exm:wilksnormalmean" class="example"><strong>Example 5.2  </strong></span>Wilks statistic for the mean parameter of a normal model:</p>
<p>The Wilks statistic is
<span class="math display">\[
W(\mu_0)^2 = \frac{(\bar{x}-\mu_0)^2}{\sigma^2 / n}
\]</span></p>
<p>See Worksheet 4 for a derivation
of the Wilks statistic directly from the log-likelihood function.</p>
<p>Note this is the same as the squared Wald statistic discussed in
Example <a href="04-likelihood4.html#exm:waldnormalmean">4.6</a>.</p>
<p>Comparing with Example <a href="02-likelihood2.html#exm:klnormalequalvar">2.9</a> we see that in this case the Wilks
statistic is essentially (apart from a scale factor <span class="math inline">\(2n\)</span>) the KL divergence between two
normal distributions with different means and variance equal to <span class="math inline">\(\sigma^2\)</span>:
<span class="math display">\[
W(p_0) =2 n D_{\text{KL}}( N( \hat{\mu}_{ML}, \sigma^2 ), N(\mu_0, \sigma^2)  )
\]</span></p>
</div>
</div>
<div id="quadratic-approximation-of-wilks-statistic-and-squared-wald-statistic" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Quadratic approximation of Wilks statistic and squared Wald statistic</h3>
<p>Recall the <em>quadratic approximation</em> (= second order Taylor series around the MLE <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span>) applied the log-likelihood function <span class="math inline">\(l_n(\boldsymbol \theta_0)\)</span>:</p>
<p><span class="math display">\[l_n(\boldsymbol \theta_0)\approx l_n(\hat{\boldsymbol \theta}_{ML})-\frac{1}{2}(\boldsymbol \theta_0-\hat{\boldsymbol \theta}_{ML})^T \boldsymbol J_n(\hat{\boldsymbol \theta}_{ML}) (\boldsymbol \theta_0-\hat{\boldsymbol \theta}_{ML})\]</span></p>
<p>With this we can then approximate the Wilks statistic:
<span class="math display">\[
\begin{split}
W(\boldsymbol \theta_0) &amp; = 2(l_n(\hat{\boldsymbol \theta}_{ML})-l_n(\boldsymbol \theta_0))\\
&amp; \approx (\boldsymbol \theta_0-\hat{\boldsymbol \theta}_{ML})^T \boldsymbol J_n(\hat{\boldsymbol \theta}_{ML})(\boldsymbol \theta_0-\hat{\boldsymbol \theta}_{ML})\\
&amp; =t(\boldsymbol \theta_0)^2 \\
\end{split}
\]</span></p>
<p>Thus the quadratic approximation of the Wilks statistic yields the squared Wald statistic!</p>
<p>Conversely, the Wilks statistic can be understood a generalisation of the squared Wald statistic.</p>
<div class="example">
<p><span id="exm:wilksproportionquadraticapprox" class="example"><strong>Example 5.3  </strong></span>Quadratic approximation of the Wilks statistic for a proportion (continued from Example <a href="05-likelihood5.html#exm:wilksproportion">5.1</a>):</p>
<p>A Taylor series of second order (for <span class="math inline">\(p_0\)</span> around <span class="math inline">\(\bar{x}\)</span>) yields
<span class="math display">\[
\log \left( \frac{  \bar{x}  }{p_0} \right) \approx -\frac{p_0-\bar{x}}{\bar{x}} + \frac{ ( p_0-\bar{x} )^2    }{2  \bar{x}^2   }
\]</span>
and
<span class="math display">\[
\log \left( \frac{ 1- \bar{x}  }{1- p_0} \right) \approx \frac{p_0-\bar{x}}{1-\bar{x}} + \frac{ ( p_0-\bar{x} )^2    }{2  (1-\bar{x})^2   }
\]</span>
With this we can approximate the Wilks statistic of the proportion as
<span class="math display">\[
\begin{split}
W(p_0) &amp; \approx  2 n \left(  - (p_0-\bar{x})  +\frac{ ( p_0-\bar{x} )^2    }{2  \bar{x}  } 
+ (p_0-\bar{x}) + \frac{ ( p_0-\bar{x} )^2    }{2  (1-\bar{x}) } \right)   \\
&amp; = n \left(    \frac{ ( p_0-\bar{x} )^2    }{  \bar{x}  } + \frac{ ( p_0-\bar{x} )^2    }{  (1-\bar{x}) } \right)  \\
&amp; = n \left(    \frac{ ( p_0-\bar{x} )^2    }{  \bar{x} (1-\bar{x})  } \right)   \\
&amp;= t(p_0)^2 \,.
\end{split}
\]</span>
This verifies that the quadratic approximation of the Wilks statistic leads
back to the squared Wald statistic of Example <a href="04-likelihood4.html#exm:waldproportion">4.5</a>.</p>
</div>
<div class="example">
<p><span id="exm:wilksnormalmeanquadraticapprox" class="example"><strong>Example 5.4  </strong></span>Quadratic approximation of the Wilks statistic for the mean parameter of a normal model
(continued from Example <a href="05-likelihood5.html#exm:wilksnormalmean">5.2</a>):</p>
<p>The normal log-likelihood is already quadratic in the mean parameter (cf. Example <a href="03-likelihood3.html#exm:mlenormalmean">3.2</a>).
Correspondingly, the Wilks statistic is quadratic in the mean parameter as well.
Hence in this particular case the quadratic “approximation” is in fact exact
and the Wilks statistic and the squared Wald statistic are identical!</p>
<p>Correspondingly, confidence intervals and tests based on the Wilks statistic
are identical to those obtained using the Wald statistic.</p>
</div>
</div>
<div id="distribution-of-the-wilks-statistic" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Distribution of the Wilks statistic</h3>
<p>The connection with the squared Wald statistic implies that both have asympotically the
same distribution.</p>
<p>Hence, under <span class="math inline">\(\boldsymbol \theta_0\)</span> the Wilks statistic is distributed asymptotically as
<span class="math display">\[W(\boldsymbol \theta_0) \overset{a}{\sim} \chi^2_d\]</span>
where <span class="math inline">\(d\)</span> is the number of parameters in <span class="math inline">\(\boldsymbol \theta\)</span>, i.e. the dimension of the model.</p>
<p>For scalar <span class="math inline">\(\theta\)</span> (i.e. single parameter and <span class="math inline">\(d=1\)</span>) this becomes
<span class="math display">\[
W(\theta_0) \overset{a}{\sim} \chi^2_1
\]</span></p>
<p>This fact is known as <strong>Wilks’ theorem</strong>.</p>
</div>
<div id="cutoff-values-for-the-likelihood-ci" class="section level3">
<h3><span class="header-section-number">5.1.4</span> Cutoff values for the likelihood CI</h3>
<table>
<thead>
<tr class="header">
<th>coverage <span class="math inline">\(\kappa\)</span></th>
<th><span class="math inline">\(\Delta = \frac{c_{chisq}}{2}\)</span> (<span class="math inline">\(m=1\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.9</td>
<td>1.35</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>1.92</td>
</tr>
<tr class="odd">
<td>0.99</td>
<td>3.32</td>
</tr>
</tbody>
</table>
<p>The asymptotic distribution for <span class="math inline">\(W\)</span> is useful to choose a suitable <span class="math inline">\(\Delta\)</span> for the likelihood
CI — note that <span class="math inline">\(2 \Delta = c_{chisq}\)</span> where <span class="math inline">\(c_{chisq}\)</span> is the critical value for a specified coverage <span class="math inline">\(\kappa\)</span>. This yields the above table for scalar parameter</p>
<div class="example">
<p><span id="exm:likciproportion" class="example"><strong>Example 5.5  </strong></span>Likelihood confidence interval for a proportion:</p>
<p>We continue from Example <a href="05-likelihood5.html#exm:wilksproportion">5.1</a>, and as in Example <a href="04-likelihood4.html#exm:ciproportion">4.7</a> we asssume we have data with <span class="math inline">\(n = 30\)</span> and <span class="math inline">\(\bar{x} = 0.7\)</span>.</p>
<p>This yields (via numerical root finding) as the 95% likelihood confidence interval
the interval <span class="math inline">\([0.524, 0.843]\)</span>.
It is similar but not identical to the corresponding
asymptotic normal interval <span class="math inline">\([0.536, 0.864]\)</span> obtained in Example <a href="04-likelihood4.html#exm:ciproportion">4.7</a>.</p>
<p>The following figure illustrate the relationship between the normal CI, the likelihood
CI and also shows the role of the quadratic approximation (see also Example <a href="04-likelihood4.html#exm:quadapproxproportion">4.2</a>). Note that:</p>
<ul>
<li>the normal CI is symmetric around the MLE whereas the likelihood CI is not symmetric</li>
<li>the normal CI is identical to the likelihood CI when using the quadratic approximation!</li>
</ul>
<p><img src="05-likelihood5_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
</div>
<div id="likelihood-ratio-test-lrt-using-wilks-statistic" class="section level3">
<h3><span class="header-section-number">5.1.5</span> Likelihood ratio test (LRT) using Wilks statistic</h3>
<p>As in the normal case (with Wald statistic and normal CIs) one can also construct
a test using the Wilks statistic:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
H_0: \boldsymbol \theta= \boldsymbol \theta_0\\
H_1: \boldsymbol \theta\neq \boldsymbol \theta_0\\
\end{array}
\begin{array}{ll}
  \text{ True model is } \boldsymbol \theta_0\\
  \text{ True model is } \textbf{not } \boldsymbol \theta_0\\
\end{array}
\begin{array}{ll}
 \text{  Null hypothesis / null model / simple}\\
 \text{  Alternative hypothesis / alternative models / composite}\\
\end{array}
\end{align*}\]</span></p>
<p>As test statistic we use the Wilks log likelihood ratio <span class="math inline">\(W(\boldsymbol \theta_0)\)</span>.</p>
<p>Extreme values of this test statistic imply evidence against <span class="math inline">\(H_0\)</span>.</p>
<p><strong>Remarks:</strong></p>
<ul>
<li>The composite alternative <span class="math inline">\(H_1\)</span> is represented by a single point (the MLE).</li>
<li><strong>Reject</strong> <span class="math inline">\(H_0\)</span> for <strong>large values of <span class="math inline">\(W(\boldsymbol \theta_0)\)</span></strong></li>
<li>under <span class="math inline">\(H_0\)</span> and for large <span class="math inline">\(n\)</span> the statistic <span class="math inline">\(W(\boldsymbol \theta_0)\)</span> is chi-squared distributed, i.e. <span class="math inline">\(W(\boldsymbol \theta_0) \overset{a}{\sim} \chi^2_d\)</span>. This allows to compute
critical values (i.e tresholds to declared rejection under a given significance level) and also <span class="math inline">\(p\)</span>-values corresponding to the observed test statistics.</li>
<li>Models <strong>outside</strong> the CI are <strong>rejected</strong></li>
<li>Models <strong>inside</strong> the CI <strong>cannot be rejected</strong>, i.e. they can’t be statistically distinguished from the best alternative model.</li>
</ul>
<p>A statistic equivalent to <span class="math inline">\(W(\boldsymbol \theta_0)\)</span> is the <strong>likelihood ratio</strong>
<span class="math display">\[\Lambda(\boldsymbol \theta_0)  = \frac{L(\boldsymbol \theta_0)}{L(\hat{\boldsymbol \theta}_{ML})}\]</span>
The two statistics can be transformed into each other by <span class="math inline">\(W(\boldsymbol \theta_0) = -2\log \Lambda(\boldsymbol \theta_0)\)</span>
and <span class="math inline">\(\Lambda(\boldsymbol \theta_0) = e^{ - W(\boldsymbol \theta_0) / 2 }\)</span>.
We <strong>reject</strong> <span class="math inline">\(H_0\)</span> for <strong>small values of <span class="math inline">\(\Lambda\)</span></strong>.</p>
<p>It can be shown that the likelihood ratio test to compare two simple model is optimal in the sense that for any given specified type I error (=probability of wrongly rejecting <span class="math inline">\(H_0\)</span>,
i.e. the sigificance level) it will maximise the power (=1- type II error, probability of correctly
accepting <span class="math inline">\(H_1\)</span>). This is known as the <strong>Neyman-Pearson theorem</strong>.</p>
<div class="example">
<p><span id="exm:liktestproportion" class="example"><strong>Example 5.6  </strong></span>Likelihood test for a proportion:</p>
<p>We continue from Example <a href="05-likelihood5.html#exm:likciproportion">5.5</a> with 95% likelihood confidence
interval <span class="math inline">\([0.524, 0.843]\)</span>.</p>
<p>The value <span class="math inline">\(p_0=0.5\)</span> is outside the CI and hence can be rejected whereas <span class="math inline">\(p_0=0.8\)</span>
is insided the CI and hence cannot be rejected on 5% significance level.</p>
<p>The Wilks statistic for <span class="math inline">\(p_0=0.5\)</span> and <span class="math inline">\(p_0=0.8\)</span> take on the following values:</p>
<ul>
<li><span class="math inline">\(W(0.5)^2 = 4.94 &gt; 3.84\)</span> hence <span class="math inline">\(p_0=0.5\)</span> can be rejected.</li>
<li><span class="math inline">\(W(0.8)^2 = 1.69 &lt; 3.84\)</span> hence <span class="math inline">\(p_0=0.8\)</span> cannot be rejected.</li>
</ul>
<p>Note that the Wilks statistic at the boundaries of the likelihood confidence interval
is equal to the critical value (3.84 corresponding to 5% significance level for a chi-squared
distribution with 1 degree of freedom).</p>
</div>
</div>
<div id="origin-of-likelihood-ratio-statistic" class="section level3">
<h3><span class="header-section-number">5.1.6</span> Origin of likelihood ratio statistic</h3>
<p>The likelihood ratio statistic is asymptotically linked to differences in the KL divergences of the two compared models with the underlying true model.</p>
<p>Assume that <span class="math inline">\(F\)</span> is the true (and unknown) data generating model
<span class="math inline">\(G_{\boldsymbol \theta}\)</span> is a family of models
and we would like to compare two candidate models <span class="math inline">\(G_A\)</span> and <span class="math inline">\(G_B\)</span> corresponding
to parameters <span class="math inline">\(\boldsymbol \theta_A\)</span> and <span class="math inline">\(\boldsymbol \theta_B\)</span> on the
basis of observed data <span class="math inline">\(x_1, \ldots, x_n\)</span>.
The KL divergences <span class="math inline">\(D_A = D_{\text{KL}}(F, G_A)\)</span> and <span class="math inline">\(D_B=D_{\text{KL}}(F, G_B)\)</span> indicate how close
each of the models <span class="math inline">\(G_A\)</span> and <span class="math inline">\(G_B\)</span> fit the true <span class="math inline">\(F\)</span>.
The difference <span class="math inline">\(D_B-D_A\)</span> is thus a way to measure the relative fit of the two models,
and can be computed as
<span class="math display">\[
D_B-D_A = D_{\text{KL}}(F, G_B)-D_{\text{KL}}(F, G_A) = \text{E}_{F} \log \frac{g_A(x)}{g_B(x)}
\]</span>
Replacing <span class="math inline">\(F\)</span> by the empirical distribution <span class="math inline">\(\hat{F}_n\)</span> leads to the
large sample approximation
<span class="math display">\[
2 n (D_B-D_A)  \approx 2 (l_n(\boldsymbol \theta_A) - l_n(\boldsymbol \theta_B))
\]</span>
Hence, the difference in the log-likelihoods provides an estimate of the difference
in the KL divergence of the two models involved.</p>
<p>The Wilks log likelihood ratio statistic
<span class="math display">\[
W(\boldsymbol \theta_0) = 2 ( l_n( \hat{\boldsymbol \theta}_{ML} ) - l_n(\boldsymbol \theta_0) ) \approx 2 n (D_{F_{\boldsymbol \theta_0}}  - D_{F_{\hat{\boldsymbol \theta}_{ML}}})
\]</span>
thus compares the best-fit distribution with <span class="math inline">\(\hat{\boldsymbol \theta}_{ML}\)</span>
as the parameter to the distribution with parameter <span class="math inline">\(\boldsymbol \theta_0\)</span>.</p>
<p>For some specific models the Wilks statistic
can also be written in the form of the KL divergence:
<span class="math display">\[
W(\boldsymbol \theta_0) = 2n D_{\text{KL}}( F_{\hat{\boldsymbol \theta}_{ML}}, F_{\boldsymbol \theta_0})
\]</span>
This is the case for the examples <a href="05-likelihood5.html#exm:wilksproportion">5.1</a> and <a href="05-likelihood5.html#exm:wilksnormalmean">5.2</a> and also more generally for exponential
family models, but it is not true in general.</p>
</div>
</div>
<div id="generalised-likelihood-ratio-test-glrt" class="section level2">
<h2><span class="header-section-number">5.2</span> Generalised likelihood ratio test (GLRT)</h2>
<p>Also known as <strong>maximum likelihood ratio test (MLRT)</strong>. The Generalised Likelihood Ratio Test (GLRT) works like the standard likelihood ratio test with the difference that now the null model <span class="math inline">\(H_0\)</span> is a composite model. This means that in the denominator in the test statistic needs to be optimised as well.
<span class="math display">\[\begin{align*}
\begin{array}{ll}
H_0: \boldsymbol \theta\in \omega_0 \subset \Omega \\
H_1: \boldsymbol \theta\in \omega_1  = \Omega \setminus \omega_0\\
\end{array}
\begin{array}{ll}
\text{ True model lies in restricted model space }\\
\text{ True model is not the restricted model space } \\
\end{array}
\end{align*}\]</span></p>
<p>Both <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> are now composite hypotheses.
<span class="math inline">\(\Omega\)</span> represents the unrestricted model space with dimension
(=number of free parameters)
<span class="math inline">\(d = |\Omega|\)</span>. The constrained space <span class="math inline">\(\omega_0\)</span> has degree of freedom
<span class="math inline">\(d_0 = |\omega_0|\)</span> with <span class="math inline">\(d_0 &lt; d\)</span>.
Note that in the standard LRT the set <span class="math inline">\(\omega_0\)</span> is a simple point
with <span class="math inline">\(d_0=0\)</span>
as the null model is a simple distribution. Thus, LRT is contained in GLRT
as special case!</p>
<p>The corresponding generalised (log) likelihood ratio statistic is given by</p>
<p><span class="math display">\[
W = 2\log\left(\frac{L(\hat{\theta}_{ML})}{L(\hat{\theta}_{ML}^0)}\right)
\text{ and }
\Lambda = \frac{\underset{\theta \in \omega_0}{\max}\, L(\theta)}{\underset{\theta \in \Omega}{\max}\, L(\theta)}
\]</span></p>
<p>where <span class="math inline">\(L(\hat{\theta}_{ML})\)</span> is the maximised likelihood assuming the full model
(with parameter space <span class="math inline">\(\Omega\)</span>) and <span class="math inline">\(L(\hat{\theta}_{ML}^0)\)</span> is the maximised likelihood for the restricted model (with parameter space <span class="math inline">\(\omega_0\)</span>).</p>
<p><strong>Remarks:</strong></p>
<ul>
<li>MLE in the restricted model space <span class="math inline">\(\omega_0\)</span> is taken as a representative of <span class="math inline">\(H_0\)</span>.</li>
<li>The likelihood is <strong>maximised</strong> in <strong>both numerator</strong> and <strong>denominator</strong>.</li>
<li>The restriced model is a special case of the full model (i.e. the two models are nested).</li>
<li>The asymptotic distribution of <span class="math inline">\(W\)</span> is chi-squared with degree of freedom depending on both <span class="math inline">\(d\)</span> and <span class="math inline">\(d_0\)</span>:</li>
</ul>
<p><span class="math display">\[W \overset{a}{\sim} \chi^2_{d-d_0}\]</span></p>
<ul>
<li><p>This result is due to <span class="citation">Wilks (<a href="22-references.html#ref-Wilks1938">1938</a>)</span>. Note that it
assumes that the true model is contained among the investigated models.</p></li>
<li><p>If <span class="math inline">\(H_0\)</span> is a simple hypothesis (i.e. <span class="math inline">\(d_0=0\)</span>) then the standard LRT (and corresponding CI) is recovered as special case of the GLRT.</p></li>
</ul>
<div class="example">
<p><span id="exm:glrtnormal" class="example"><strong>Example 5.7  </strong></span>GLRT example:</p>
<p><em>Case-control study:</em> (e.g. “healthy” vs. “disease”)<br />
we observe normal data from two groups with sample size <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>
(and <span class="math inline">\(n=n_1+n_2\)</span>):</p>
<p><span class="math display">\[x_1,\dots,x_{n_1} \sim N(\mu_1, \sigma^2)\]</span>
and
<span class="math display">\[x_{n_1+1},\dots,x_{n} \sim N(\mu_2, \sigma^2)\]</span></p>
<p>Question: are the two means <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> the same in the two groups?</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
H_0: \mu_1=\mu_2  \text{ (with variance unknown nuisance parameter)}
\\
H_1: \mu_1\neq\mu_2\\
\end{array}
\end{align*}\]</span></p>
<p><em>Restricted and full models:</em></p>
<p><span class="math inline">\(\omega_0\)</span>: restricted model with two parameters <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\sigma^2_0\)</span>
(so that <span class="math inline">\(x_{1},\dots,x_{n} \sim N(\mu_0, \sigma_0^2)\)</span> ).</p>
<p><span class="math inline">\(\Omega\)</span>: full model with three parameters <span class="math inline">\(\mu_1, \mu_2, \sigma^2\)</span>.</p>
<p><em>Corresponding log-likelihood functions:</em></p>
<p>Restricted model <span class="math inline">\(\omega_0\)</span>:
<span class="math display">\[
\log L(\mu_0, \sigma_0^2) = -\frac{n}{2} \log(\sigma_0^2) 
- \frac{1}{2\sigma_0^2} \sum_{i=1}^n (x_i-\mu_0)^2
\]</span></p>
<p>Full model <span class="math inline">\(\Omega\)</span>:
<span class="math display">\[
\begin{split}
\log L(\mu_1, \mu_2, \sigma^2) &amp; =
 \left(-\frac{n_1}{2} \log(\sigma^2) - \frac{1}{2\sigma^2}  \sum_{i=1}^{n_1} (x_i-\mu_1)^2   \right) +
\left(-\frac{n_2}{2} \log(\sigma^2) - \frac{1}{2\sigma^2}  \sum_{i=n_1+1}^{n} (x_i-\mu_1)^2   \right)
 \\
&amp;= -\frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \left( \sum_{i=1}^{n_1} (x_i-\mu_1)^2 + \sum_{i=n_1+1}^n (x_i-\mu_2)^2 \right) \\
\end{split}
\]</span></p>
<p><em>Corresponding MLEs:</em></p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\omega_0:\\
\\
\Omega:\\
\\
\end{array}
\begin{array}{ll}
\hat{\mu}_0 = \frac{1}{n}\sum^n_{i=1}x_i\\
\\
\hat{\mu}_1 = \frac{1}{n_1}\sum^{n_1}_{i=1}x_i\\
\hat{\mu}_2 = \frac{1}{n_2}\sum^{n}_{i=n_1+1}x_i\\
\end{array}
\begin{array}{ll}
 \widehat{\sigma^2_0} = \frac{1}{n}\sum^n_{i=1}(x_i-\hat{\mu}_0)^2\\
\\
 \widehat{\sigma^2} = \frac{1}{n}\left\{\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)^2+\sum^n_{i=n_1+1}(x_i-\hat{\mu}_2)^2\right\}\\
\\
\end{array}
\end{align*}\]</span></p>
<p><em>Corresponding maximised log-likelihood:</em></p>
<p>Restricted model:</p>
<p><span class="math display">\[\log L(\hat{\mu}_0,\widehat{\sigma^2_0}) = -\frac{n}{2} \log(\widehat{\sigma^2_0}) -\frac{n}{2} \]</span></p>
<p>Full model:</p>
<p><span class="math display">\[
\log L(\hat{\mu}_1,\hat{\mu}_2,\widehat{\sigma^2}) = -\frac{n}{2} \log(\widehat{\sigma^2}) -\frac{n}{2}
\]</span></p>
<p><em>Likelihood ratio statistic:</em></p>
<p><span class="math display">\[
W = 2\log\left(\frac{L(\hat{\mu}_1,\hat{\mu}_2,\widehat{\sigma^2})}{L(\hat{\mu}_0,\widehat{\sigma^2_0})}\right)
= 2 \log L(\hat{\mu}_1,\hat{\mu}_2,\widehat{\sigma^2}) - 2 \log L(\hat{\mu}_0,\widehat{\sigma^2_0})
= n\log\left(\frac{\widehat{\sigma^2_0}}{\widehat{\sigma^2}} \right)
\]</span></p>
<p>Using <span class="math inline">\(\widehat{\sigma^2_0} - \widehat{\sigma^2} = \frac{n_1 n_2}{n^2} (\hat{\mu}_1 - \hat{\mu}_2)^2\)</span> this can be further simplified:
<span class="math display">\[
W = n\log\left(\frac{\widehat{\sigma^2_0}}{\widehat{\sigma^2}} \right)
=n\log\left(1+\frac{t^2_{ML}}{n}\right)
=n\log\left(1+\frac{1}{n-2}t^2\right)
\]</span>
with (ML variance)
<span class="math display">\[
t_{ML} = \frac{\hat{\mu}_1-\hat{\mu}_2}{\sqrt{\left(\frac{1}{n_1}+\frac{1}{n_2}\right) \widehat{\sigma^2}}}
\]</span>
and (unbiased variance)
<span class="math display">\[
t = \frac{\hat{\mu}_1-\hat{\mu}_2}{\sqrt{\left(\frac{1}{n_1}+\frac{1}{n_2}\right)\frac{n}{n-2} \widehat{\sigma^2}}}
\]</span></p>
<p><span class="math inline">\(\longrightarrow\)</span> the GRLT is a monotone function of the (squared) two-sample <span class="math inline">\(t\)</span>-statistic!</p>
<p><strong>It can be shown that all standard tests with normal distributions can be interpreted as GLRTs!</strong></p>
</div>

<p></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="04-likelihood4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="06-likelihood6.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["math20802-script.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
