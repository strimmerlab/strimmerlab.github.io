<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>11 Shrinkage estimation using empirical risk minimisation | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.4/tabs.js"></script><script src="libs/bs3compat-0.2.4/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="active" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="shrinkage-estimation-using-empirical-risk-minimisation" class="section level1">
<h1>
<span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation<a class="anchor" aria-label="anchor" href="#shrinkage-estimation-using-empirical-risk-minimisation"><i class="fas fa-link"></i></a>
</h1>
<div id="linear-shrinkage" class="section level2">
<h2>
<span class="header-section-number">11.1</span> Linear shrinkage<a class="anchor" aria-label="anchor" href="#linear-shrinkage"><i class="fas fa-link"></i></a>
</h2>
<p>In the examples for Bayesian estimation we have seen so far
the posterior mean of the parameter of interest was obtained
by linear shrinkage
<span class="math display">\[
\hat\theta_{\text{shrink}} = \text{E}( \theta | x_1, \ldots, x_n) = \lambda \theta_0 + (1-\lambda) \hat\theta_{\text{ML}}
\]</span>
of the MLE <span class="math inline">\(\hat\theta_{\text{ML}}\)</span> towards the
prior mean <span class="math inline">\(\theta_0\)</span>, with shrinkage intensity <span class="math inline">\(\lambda=\frac{m}{m+n}\)</span>
determined by the pseudo-sample size <span class="math inline">\(m\)</span>
(which in turn is linked the precision of the prior)
and the sample size <span class="math inline">\(n\)</span>.</p>
<p>The resulting point estimate <span class="math inline">\(\hat\theta_{\text{shrink}}\)</span> is called <em>shrinkage estimate</em>
and is a convex combination of <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\hat\theta_{\text{ML}}\)</span>. The prior mean <span class="math inline">\(\theta_0\)</span> is also
called the “target”.</p>
<p>In a Bayesian estimation the parameter <span class="math inline">\(m\)</span> and hence <span class="math inline">\(\lambda\)</span> is given a priori, but it turns out that it is possible and useful to find an optimal value for <span class="math inline">\(\lambda\)</span> by minimising the mean squared error of the
estimator <span class="math inline">\(\hat\theta_{\text{shrink}}\)</span>.</p>
<p>In particular, by construction, the target <span class="math inline">\(\theta_0\)</span> has zero variance
but substantial bias, whereas the MLE <span class="math inline">\(\hat\theta_{\text{ML}}\)</span> will have low or zero bias but a non-vanishing variance. By combinining these two estimators with opposite properties the aim is to achieve
a <em>bias-variance tradeoff</em> so that the resulting estimator <span class="math inline">\(\hat\theta_{\text{shrink}}\)</span> has lower MSE than either
<span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\hat\theta_{\text{ML}}\)</span>.</p>
<p>Specifically, the aim is to find
<span class="math display">\[
\lambda^{\star} = \underset{\lambda}{\arg \min \ }  
\text{E}\left( ( \theta - \hat\theta_{\text{shrink}} )^2\right) 
\]</span></p>
<p>It turns out that this can be minimised without knowing the actual true value of <span class="math inline">\(\theta\)</span>
and the result for an unbiased <span class="math inline">\(\hat\theta_{\text{ML}}\)</span> is
<span class="math display">\[
\lambda^{\star} = \frac{\text{Var}(\hat\theta_{\text{ML}})}{\text{E}( (\hat\theta_{\text{ML}} - \theta_0)^2 )}
\]</span>
Hence, the shrinkage intensity will be small if the variance of the MLE is small and/or if the target
and the MLE differ substantially. On the other hand, if the variance of the MLE is large and/or the target is close to the MLE the shrinkage intensity will be large.</p>
</div>
<div id="james-stein-estimator" class="section level2">
<h2>
<span class="header-section-number">11.2</span> James-Stein estimator<a class="anchor" aria-label="anchor" href="#james-stein-estimator"><i class="fas fa-link"></i></a>
</h2>
<p>We can now use empirical risk minimisation to estimate the shrinkage parameter the Normal-Normal model.</p>
<p>In 1955 James and Stein propose the following estimate for
the multivariate mean <span class="math inline">\(\boldsymbol \mu\)</span> of using a single sample <span class="math inline">\(\boldsymbol x\)</span>
drawn from the multivariate normal <span class="math inline">\(N_d(\boldsymbol \mu, \boldsymbol I)\)</span>:
<span class="math display">\[
\hat{\boldsymbol \mu}_{JS} = (1-\frac{d-2}{||\boldsymbol x||^2}) \boldsymbol x
\]</span>
Here, we recognise <span class="math inline">\(\hat{\boldsymbol \mu}_{ML} = \boldsymbol x\)</span>, <span class="math inline">\(\boldsymbol \mu_0=0\)</span> and shrinkage intensity <span class="math inline">\(\lambda^{\star}=\frac{d-2}{||\boldsymbol x||^2}\)</span>.</p>
<p>Efron and Morris (1972) and Lindley and Smith (1972)
generalised this shrinkage estimator to the case
of multiple observations <span class="math inline">\(\boldsymbol x_1, \ldots \boldsymbol x_n\)</span>
and target <span class="math inline">\(\boldsymbol \mu_0\)</span>, yielding an empirical Bayes estimate of <span class="math inline">\(\mu\)</span> based on the Normal-Normal model.</p>

<p></p>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></div>
<div class="next"><a href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#shrinkage-estimation-using-empirical-risk-minimisation"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="nav-link" href="#linear-shrinkage"><span class="header-section-number">11.1</span> Linear shrinkage</a></li>
<li><a class="nav-link" href="#james-stein-estimator"><span class="header-section-number">11.2</span> James-Stein estimator</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 10 May 2021.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
