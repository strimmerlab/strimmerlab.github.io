<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>7 Essentials of Bayesian statistics | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="7 Essentials of Bayesian statistics | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="7 Essentials of Bayesian statistics | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.13/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><meta name="description" content="7.1 Probabilistic prerequisites  7.1.1 Conditional probability Assume we have two random variables \(x\) and \(y\) with a joint density (or joint PMF) \(p(x,y)\). By definition \(\int_{x,y} p(x,y)...">
<meta property="og:description" content="7.1 Probabilistic prerequisites  7.1.1 Conditional probability Assume we have two random variables \(x\) and \(y\) with a joint density (or joint PMF) \(p(x,y)\). By definition \(\int_{x,y} p(x,y)...">
<meta name="twitter:description" content="7.1 Probabilistic prerequisites  7.1.1 Conditional probability Assume we have two random variables \(x\) and \(y\) with a joint density (or joint PMF) \(p(x,y)\). By definition \(\int_{x,y} p(x,y)...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="active" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></li>
<li><a class="" href="properties-of-bayesian-learning.html"><span class="header-section-number">9</span> Properties of Bayesian learning</a></li>
<li><a class="" href="normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html"><span class="header-section-number">10</span> Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance</a></li>
<li><a class="" href="shrinkage-estimation-using-empirical-risk-minimisation.html"><span class="header-section-number">11</span> Shrinkage estimation using empirical risk minimisation</a></li>
<li><a class="" href="bayesian-model-comparison-using-bayes-factors-and-the-bic.html"><span class="header-section-number">12</span> Bayesian model comparison using Bayes factors and the BIC</a></li>
<li><a class="" href="false-discovery-rates.html"><span class="header-section-number">13</span> False discovery rates</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">14</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="" href="overview-over-regression-modelling.html"><span class="header-section-number">15</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">16</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">17</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">18</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">19</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="essentials-of-bayesian-statistics" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Essentials of Bayesian statistics<a class="anchor" aria-label="anchor" href="#essentials-of-bayesian-statistics"><i class="fas fa-link"></i></a>
</h1>
<div id="probabilistic-prerequisites" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> Probabilistic prerequisites<a class="anchor" aria-label="anchor" href="#probabilistic-prerequisites"><i class="fas fa-link"></i></a>
</h2>
<div id="conditional-probability" class="section level3" number="7.1.1">
<h3>
<span class="header-section-number">7.1.1</span> Conditional probability<a class="anchor" aria-label="anchor" href="#conditional-probability"><i class="fas fa-link"></i></a>
</h3>
<p>Assume we have two random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> with a <strong>joint density</strong> (or joint PMF) <span class="math inline">\(p(x,y)\)</span>.
By definition <span class="math inline">\(\int_{x,y} p(x,y) dx dy = 1\)</span>.</p>
<p>The <strong>marginal densities</strong> for the individual <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are given by <span class="math inline">\(p(x) = \int_y p(x,y) dy\)</span>
and <span class="math inline">\(p(y) = \int_x f(x,y) dx\)</span>. Thus, when computing the marginal densities a variable is removed
from the joint density by integrating over all possible states of that variable.
It follows also that <span class="math inline">\(\int_x p(x) dx = 1\)</span> and <span class="math inline">\(\int_y p(y) dy = 1\)</span>, i.e.Â the
marginal densities also integrate to 1.</p>
<p>As alternative to integrating out a random variable in the joint density <span class="math inline">\(p(x,y)\)</span>
we may wish to keep it fixed at some value, say keep <span class="math inline">\(y\)</span> fixed at <span class="math inline">\(y_0\)</span>.
In this case <span class="math inline">\(p(x, y=y_0)\)</span> is proportional to the <strong>conditional density</strong> (or PMF)
given by the ratio
<span class="math display">\[
p(x | y=y_0) = \frac{p(x, y=y_0)}{p(y=y_0)}
\]</span>
The denominator <span class="math inline">\(p(y=y_0) = \int_x p(x, y=y_0) dx\)</span> is
needed to ensure that <span class="math inline">\(\int_x p(x | y=y_0) dx = 1\)</span>, thus it renormalises
<span class="math inline">\(p(x, y=y_0)\)</span> so that it is a proper density.</p>
<p>To simplify notation, the specific value on which a variable is conditioned is often left out.</p>
<p>The mean and variance of the conditional distribution are called conditional mean and conditional variance.</p>
<p>Rearranging the above we see that the joint density can be written as the
product of marginal and conditional density in two different ways:
<span class="math display">\[
p(x,y) = p(x| y) f(y) = p(y | x) p(x)
\]</span></p>
</div>
<div id="bayes-theorem" class="section level3" number="7.1.2">
<h3>
<span class="header-section-number">7.1.2</span> Bayesâ theorem<a class="anchor" aria-label="anchor" href="#bayes-theorem"><i class="fas fa-link"></i></a>
</h3>
<p><a href="https://de.wikipedia.org/wiki/Thomas_Bayes">Thomas Bayes (1701-1761)</a> was
the first to state <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayesâ theorem</a>
on conditional probabilities:
<span class="math display">\[
p(x | y) = p(y | x) \frac{ p(x) }{ p(y)}
\]</span>
This rule relates the two possible conditional densities (or conditional probability mass functions) for two random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. It thus allows to reverse the ordering of conditioning. This theorem follows directly from the product rule linking the joint density with the marginal and conditional densities.</p>
<p>Bayesâs theorem was <a href="https://en.wikipedia.org/wiki/An_Essay_towards_solving_a_Problem_in_the_Doctrine_of_Chances">published in 1763</a> only after his death by <a href="https://en.wikipedia.org/wiki/Richard_Price">Richard Price (1723-1791)</a>:</p>
<p><a href="https://de.wikipedia.org/wiki/Pierre-Simon_Laplace">Pierre-Simon Laplace</a> independently published Bayesâ theorem in 1774 and he was in fact the first to routinely apply it to statistical calculations.</p>
</div>
</div>
<div id="principle-of-bayesian-learning" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> Principle of Bayesian learning<a class="anchor" aria-label="anchor" href="#principle-of-bayesian-learning"><i class="fas fa-link"></i></a>
</h2>
<div id="from-prior-to-posterior-distribution" class="section level3" number="7.2.1">
<h3>
<span class="header-section-number">7.2.1</span> From prior to posterior distribution<a class="anchor" aria-label="anchor" href="#from-prior-to-posterior-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Bayesian statistical learning applies Bayesâ theorem to update our state of knowledge in the light of data.</p>
<p>Ingredients:</p>
<ul>
<li>
<span class="math inline">\(\theta\)</span> parameter of interest, unknown and fixed.</li>
<li>prior distribution with density <span class="math inline">\(p(\theta)\)</span> describing the <em>uncertainty</em> (not randomness!) about <span class="math inline">\(\theta\)</span>
</li>
<li>data generating process <span class="math inline">\(p(x | \theta)\)</span>
</li>
</ul>
<p>Note the underlying <strong>model in the Bayesian approach is the joint distribution</strong>
<span class="math display">\[
p(\theta, x) = p(\theta) p(x | \theta)
\]</span>
as both prior and data generating process have to be specified.</p>
<p>Question: new information in the form of new observation <span class="math inline">\(x\)</span> arrives - how does the uncertainty about <span class="math inline">\(\theta\)</span> change?</p>
<p>Answer: use Bayesâ theorem to <strong>update the prior density to the posterior density</strong>.</p>
<p><span class="math display">\[
\underbrace{p(\theta | x)}_{\text{posterior} } = \underbrace{p(\theta)}_{\text{prior}} \frac{p(x | \theta) }{ p(x)} 
\]</span></p>
<div class="inline-figure"><img src="fig/bayes1-learning.png" width="90%" style="display: block; margin: auto;"></div>
<p>For the denominator in Bayes formula we need to compute <span class="math inline">\(p(x)\)</span>.
This is obtained by<br><span class="math display">\[
\begin{split}
p(x) &amp;=  \text{E}_{F_{\theta}}p(x | \theta) \\
 &amp;= \int_{\theta} p(x | \theta) p(\theta) d\theta \\
 &amp;= \int_{\theta} p(x , \theta) d\theta \\
\end{split}
\]</span>
i.e.Â by marginalisation of the parameter <span class="math inline">\(\theta\)</span> from the joint
distribution of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(x\)</span>.
(For discrete <span class="math inline">\(\theta\)</span> replace the integral by a sum).
Depending on the context this quantity is either called <em>marginal density</em> (of the underlying model) or <em>prior predictive distribution</em>
(for the data).</p>
</div>
<div id="zero-forcing-property" class="section level3" number="7.2.2">
<h3>
<span class="header-section-number">7.2.2</span> Zero forcing property<a class="anchor" aria-label="anchor" href="#zero-forcing-property"><i class="fas fa-link"></i></a>
</h3>
<p>It is easy to see that if in Bayes rule the prior density/probability is zero for some parameter value <span class="math inline">\(\theta\)</span> then the posterior density/probability will remain at zero for that <span class="math inline">\(\theta\)</span>, regardless of any data collected. This <strong>zero-forcing property</strong> of the Bayes update rule has been called <strong>Cromwellâs rule</strong> by <a href="https://en.wikipedia.org/wiki/Dennis_Lindley">Dennis Lindley (1923â2013)</a>. Therefore, assigning prior density/probability 0 to an event should be avoided.</p>
<p>Note that this implies that assigning prior probability 1 should be avoided, too.</p>
</div>
<div id="bayesian-update-and-likelihood" class="section level3" number="7.2.3">
<h3>
<span class="header-section-number">7.2.3</span> Bayesian update and likelihood<a class="anchor" aria-label="anchor" href="#bayesian-update-and-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>After independent and identically distributed data <span class="math inline">\(D = \{x_1, \ldots, x_n\}\)</span> have been observed the Bayesian posterior is computed by<br><span class="math display">\[
\underbrace{p(\theta | D) }_{\text{posterior} } = \underbrace{p(\theta)}_{\text{prior}} \frac{ L(\theta| D) }{ L(D)} 
\]</span>
involving the likelihood <span class="math inline">\(L(\theta| D) = \prod_{i=1}^n p(x_i | \theta)\)</span>
and the marginal likelihood <span class="math inline">\(L(D) = \int_{\theta} p(\theta) L(\theta| D) d\theta\)</span> with <span class="math inline">\(\theta\)</span> integrated out.</p>
<p>The marginal likelihood serves as a standardising factor so that the posterior density for <span class="math inline">\(\theta\)</span> integrates to 1:
<span class="math display">\[
\int_{\theta} p(\theta | D) d\theta = \frac{1}{L(D)} \int_{\theta} p(\theta) L(\theta| D) d\theta  = 1
\]</span>
Unfortunately, the integral to compute the marginal likelihood is typically analytically intractable and requires
numerical integration and/or approximation.</p>
<p>Comparing likelihood and Bayes procedures note that</p>
<ul>
<li>conducting a Bayesian statistical analysis requires integration respectively averaging (to compute the marginal likelihood)</li>
<li>in contrast to a likelihood analysis that requires optimisation (to find the maximum likelihood).</li>
</ul>
</div>
<div id="sequential-updates" class="section level3" number="7.2.4">
<h3>
<span class="header-section-number">7.2.4</span> Sequential updates<a class="anchor" aria-label="anchor" href="#sequential-updates"><i class="fas fa-link"></i></a>
</h3>
<p>Note that the Bayesian update procedure can be repeated again and again: we can use the posterior as our new prior and then update it with further data. Thus, we may also update the posterior density sequentially, with the data points <span class="math inline">\(x_1, \ldots, x_n\)</span> arriving one after the other, by computing first <span class="math inline">\(p(\theta| x_1)\)</span>, then <span class="math inline">\(p(\theta| x_1, x_2)\)</span> and so on until we reach <span class="math inline">\(p(\theta | x_1, \ldots, x_n) = p(\theta| D)\)</span>.</p>
<p>For example, for the first update we have
<span class="math display">\[
p(\theta| x_1) =  p(\theta)   \frac{p(x_1 | \theta)  }{p(x_1)}
\]</span>
with <span class="math inline">\(p(x_1) =\int_{\theta} p(x_1 | \theta) p(\theta) d\theta\)</span>.
The second update yields
<span class="math display">\[
\begin{split}
p(\theta| x_1, x_2) &amp;=  p(\theta| x_1)   \frac{p(x_2 | \theta, x_1)  }{p(x_2| x_1)}\\
&amp;= p(\theta| x_1)   \frac{p(x_2 | \theta)  }{p(x_2| x_1)}\\
&amp;=  p(\theta) \frac{  p(x_1 | \theta)    p(x_2 | \theta)  }{p(x_1) p(x_2| x_1)}\\
\end{split}
\]</span>
with <span class="math inline">\(p(x_2| x_1) = \int_{\theta} p(x_2 | \theta) p(\theta| x_1) d\theta\)</span>.
The final step is
<span class="math display">\[
\begin{split}
p(\theta| D)  = p(\theta| x_1, \ldots, x_n) &amp;=   p(\theta) \frac{ \prod_{i=1}^n p(x_i | \theta)  }{ L(D)  }\\
\end{split}
\]</span>
with the marginal likelihood factorising into
<span class="math display">\[
L(D) = \prod_{i=1}^n p(x_i| x_{&lt;i})
\]</span>
with
<span class="math display">\[
p(x_i| x_{&lt;i}) = \int_{\theta} p(x_i | \theta) p(\theta| x_{&lt;i}) d\theta
\]</span>
Therefore, the marginal likelihood <span class="math inline">\(L(D)\)</span> is <em>not</em> the product
of the marginal densities <span class="math inline">\(p(x_i)\)</span> at each <span class="math inline">\(x_i\)</span> but the product of the conditional densities <span class="math inline">\(p(x_i| x_{&lt;i})\)</span>.</p>
</div>
<div id="summaries-of-posterior-distributions-and-credible-intervals" class="section level3" number="7.2.5">
<h3>
<span class="header-section-number">7.2.5</span> Summaries of posterior distributions and credible intervals<a class="anchor" aria-label="anchor" href="#summaries-of-posterior-distributions-and-credible-intervals"><i class="fas fa-link"></i></a>
</h3>
<p><strong>The Bayesian estimate is the full complete posterior distribution!</strong></p>
<p>However, it is useful to summarise aspects of the posterior distribution:</p>
<ul>
<li>Posterior mean <span class="math inline">\(\text{E}(\theta | D)\)</span>
</li>
<li>Posterior variance <span class="math inline">\(\text{Var}(\theta | D)\)</span>
</li>
<li>Posterior mode
etc.</li>
</ul>
<p>In particular the mean of the posterior distribution is often taken as a <em>Bayesian point estimate</em>.</p>
<p>The posterior distribution also allows to define <strong>credible regions</strong> or <strong>credible intervals</strong>.
These are the <strong>Bayesian equivalent to confidence intervals</strong> and are constructed by
finding the areas of highest probability mass (say 95%) in the posterior distribution.</p>
<div class="inline-figure"><img src="fig/bayes2-ci.png" width="80%" style="display: block; margin: auto;"></div>
<p>Bayesian credible intervals (unlike their frequentist confidence counterparts) are thus very easy to interpret - they simply correspond to the area in the parameter space in which the we can find the parameter with a given specified probability.
In contrast, in frequentist statistics it does not make sense to assign a
probability to a parameter value!</p>
<p>Note that there are typically many credible intervals with the given specified coverage <span class="math inline">\(\alpha\)</span> (say 95%). Therefore, we may need further criteria
to construct these intervals.</p>
<p>In the univariate case a <strong>two-sided equal-tail credible interval</strong> is obtained by finding the corresponding lower <span class="math inline">\(1-\alpha/2\)</span>
and upper <span class="math inline">\(\alpha/2\)</span> quantiles. Typically this type of credible interval is easy to compute. However, note that the density values at the left and right boundary points of such an interval are typically different.</p>
<p>A <strong>highest posterior density (HPD)</strong> credible interval of coverage <span class="math inline">\(\alpha\)</span> is found by identifying the shortest interval (i.e.Â with smallest support) for the given <span class="math inline">\(\alpha\)</span>
probability mass. Any point within an HDP credible interval has higher density than a point outside the HDP credible interval. Correspondingly, the density
at the boundary of an HPD credible interval is constant taking on the same value everywhere along the boundary.</p>
<p>A Bayesian HPD credible interval is constructed in a similar fashion as a likelihood-based confidence interval, starting from the mode of the posterior density. When the posterior density has multiple modes the HPD interval may be disjoint.</p>
<p>In the Worksheet 5 examples for both types of credible intervals are given and compared visually.</p>
</div>
<div id="practical-application-of-bayes-statistics-on-the-computer" class="section level3" number="7.2.6">
<h3>
<span class="header-section-number">7.2.6</span> Practical application of Bayes statistics on the computer<a class="anchor" aria-label="anchor" href="#practical-application-of-bayes-statistics-on-the-computer"><i class="fas fa-link"></i></a>
</h3>
<p>As we have seen Bayesian learning is <em>conceptually straightforward</em>:</p>
<ol style="list-style-type: decimal">
<li>Specify prior uncertainty <span class="math inline">\(p(\theta\)</span>) about the parameters of interest <span class="math inline">\(\theta\)</span>.</li>
<li>Specify the data generating process for a specified parameter: <span class="math inline">\(p(x | \theta)\)</span>.</li>
<li>Apply Bayesâ theorem to update prior uncertainty in the light
of the new data.</li>
</ol>
<p>In practise, however, computing the posterior distribution can be <em>computationally very demanding</em>, especially
for complex models.</p>
<p>For this reason specialised software packages have been developed for computational Bayesian modelling, for example:</p>
<ul>
<li><p>Bayesian statistics in R: <a href="https://cran.r-project.org/web/views/Bayesian.html" class="uri">https://cran.r-project.org/web/views/Bayesian.html</a></p></li>
<li><p>Stan probabilistic programming language (interfaces with R, Python, Julia and other languages) â <a href="https://mc-stan.org/" class="uri">https://mc-stan.org/</a></p></li>
<li><p>Bayesian statistics in Python:
<a href="https://github.com/pymc-devs/pymc">PyMC</a> using <a href="https://aesara.readthedocs.io">Aesara</a>/<a href="https://jax.readthedocs.io">JAX</a> as frontend,
<a href="http://num.pyro.ai/">NumPyro</a> using <a href="https://jax.readthedocs.io">JAX</a> as frontend,
<a href="https://www.tensorflow.org/probability/examples/TensorFlow_Probability_on_JAX">TensorFlow Probability on JAX</a> using <a href="https://jax.readthedocs.io">JAX</a> as frontend,
<a href="https://docs.pymc.io/">PyMC3</a> using <a href="https://theano.readthedocs.io">Theano</a> as frontend,
<a href="http://docs.pyro.ai/">Pyro</a> using <a href="https://pytorch.org/">PyTorch</a> as frontend,
<a href="https://www.tensorflow.org/probability/">TensorFlow Probability</a> using <a href="https://www.tensorflow.org/">Tensorflow</a> as frontend.</p></li>
<li><p>Bayesian statistics in Julia: <a href="https://github.com/TuringLang/Turing.jl">Turing.jl</a></p></li>
<li><p>Bayesian hierarchical modelling with <a href="https://www.mrc-bsu.cam.ac.uk/software/bugs/">BUGS</a>, <a href="https://mcmc-jags.sourceforge.io/">JAGS</a> and <a href="https://r-nimble.org/">NIMBLE</a>.</p></li>
</ul>
<p>In addition to numerical procedures to sample from the posterior distribution there are also many procedures aiming to approximate the Bayesian posterior, employing the <a href="https://en.wikipedia.org/wiki/Laplace%27s_method">Laplace approximation</a>, integrated nested Laplace approximation (INLA), <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational Bayes</a> etc.</p>
</div>
</div>
<div id="some-background-on-bayesian-statistics" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> Some background on Bayesian statistics<a class="anchor" aria-label="anchor" href="#some-background-on-bayesian-statistics"><i class="fas fa-link"></i></a>
</h2>
<div id="bayesian-interpretation-of-probability" class="section level3" number="7.3.1">
<h3>
<span class="header-section-number">7.3.1</span> Bayesian interpretation of probability<a class="anchor" aria-label="anchor" href="#bayesian-interpretation-of-probability"><i class="fas fa-link"></i></a>
</h3>
<div id="what-makes-you-bayesian" class="section level4" number="7.3.1.1">
<h4>
<span class="header-section-number">7.3.1.1</span> What makes you âBayesianâ?<a class="anchor" aria-label="anchor" href="#what-makes-you-bayesian"><i class="fas fa-link"></i></a>
</h4>
<p>If you use Bayesâ theorem are you therefore automatically a Bayesian? No!!</p>
<p>Bayesâ theorem is a mathematical fact from probability theory.
Hence, Bayesâ theorem is valid for everyone, whichever form for
statistical learning your are subscribing (such as frequentist ideas,
likelihood methods, entropy learning, Bayesian learning).</p>
<p>As we discuss now the key difference between Bayesian and frequentist
statistical learning lies in the differences in <em>interpretation of probability</em>,
not in the mathematical formalism for probability (which includes Bayesâ theorem).</p>
</div>
<div id="mathematics-of-probability" class="section level4" number="7.3.1.2">
<h4>
<span class="header-section-number">7.3.1.2</span> Mathematics of probability<a class="anchor" aria-label="anchor" href="#mathematics-of-probability"><i class="fas fa-link"></i></a>
</h4>
<p>The mathematics of probability in its modern foundation was developed by <a href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov">Andrey Kolmogorov (1903â1987)</a>. In this book <a href="https://en.wikipedia.org/wiki/Probability_axioms">Foundations of the Theory of Probability (1933)</a> he establishes probability in terms of set theory/ measure theory. This theory provides a coherent mathematical framework to work with probabilities.</p>
<p>However, Kolmogorovâs theory does <em>not</em> provide an interpretation of probability!</p>
<p><span class="math inline">\(\rightarrow\)</span> The Kolmogorov framework is the basis for both the frequentist and the Bayesian interpretation of probability.</p>
</div>
<div id="interpretations-of-probability" class="section level4" number="7.3.1.3">
<h4>
<span class="header-section-number">7.3.1.3</span> Interpretations of probability<a class="anchor" aria-label="anchor" href="#interpretations-of-probability"><i class="fas fa-link"></i></a>
</h4>
<p>Essentially, there are two major commonly used interpretation of probability in statistics - the <strong>frequentist interpretation</strong> and the <strong>Bayesian interpretation</strong>.</p>
<p><strong>A: Frequentist interpretation</strong></p>
<p>probability = frequency (of an event in a long-running series of identically repeated experiments)</p>
<p>This is the <em>ontological view</em> of probability (i.e.Â probability âexistsâ and is identical to something that can be observed.).</p>
<p>It is also a very restrictive view of probability. For example, frequentist probability
cannot be used to describe events that occur only a single time.
Frequentist probability thus can only be applied asymptotically, for large samples!</p>
<p><strong>B: Bayesian probability</strong></p>
<p>âProbability does not existâ â famous quote by <a href="https://en.wikipedia.org/wiki/Bruno_de_Finetti">Bruno de Finetti (1906â1985)</a>, a Bayesian statistician.</p>
<p>What does this mean?</p>
<p>Probability is a <strong>description of the state of knowledge</strong> and of <strong>uncertainty</strong>.</p>
<p>Probability is thus an <em>epistemological quantity</em> that is assigned and that changes rather than something that is an inherent property of an object.</p>
<p>Note that this does not require any repeated experiments.
The Bayesian interpretation of probability is valid regardless of sample size or the number or repetitions of an experiment.</p>
<p><strong>Hence, the key difference between frequentist and Bayesian approaches is not the use of Bayesâ theorem.
Rather it is whether you consider probability as ontological (frequentist) or epistemological entity (Bayesian).</strong></p>
</div>
</div>
<div id="historical-developments" class="section level3" number="7.3.2">
<h3>
<span class="header-section-number">7.3.2</span> Historical developments<a class="anchor" aria-label="anchor" href="#historical-developments"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<a href="https://de.wikipedia.org/wiki/Thomas_Bayes">Thomas Bayes</a> (1701-1761) is the father of Bayesian statistics.<br>
His famous paper<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Bayes, T. 1763. &lt;em&gt;An essay towards solving a problem in the doctrine of chances&lt;/em&gt;.
The Philosophical Transactions &lt;strong&gt;53&lt;/strong&gt;:370â418. &lt;a href="https://doi.org/10.1098/rstl.1763.0053" class="uri"&gt;https://doi.org/10.1098/rstl.1763.0053&lt;/a&gt;&lt;/p&gt;'><sup>4</sup></a> on Bayesâ theorem was published only after his death (1763).</li>
</ul>
<ul>
<li>
<a href="https://de.wikipedia.org/wiki/Pierre-Simon_Laplace">Pierre-Simon Laplace</a> (1749-1827) was the first to practically use Bayesâ theorem for statistical calculations, and he also independently discovered Bayesâ theorem in 1774<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Laplace, P-S. 1774. &lt;em&gt;MÃ©moire sur la probabilitÃ© de causes par les Ã©venements&lt;/em&gt;.
MÃ©moires de mathÃ©matique et de physique, prÃ©sentÃ©s Ã  lâAcadÃ©mie Royale des sciences par divers savants et lus dans ses assemblÃ©es. Paris, Imprimerie Royale, pp.Â 621â657.&lt;/p&gt;"><sup>5</sup></a>
</li>
</ul>
<ul>
<li><p>This activity was then called â<a href="https://en.wikipedia.org/wiki/Inverse_probability">inverse probability</a>â and not âBayesian statisticsâ.</p></li>
<li><p>Between 1900 and 1940 classical mathematical statistics was developed and the field was heavily influenced and dominated by <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">R.A. Fisher</a> (who invented likelihood theory and ANOVA, among other things - he also was working in population genetics). Fisher himself was very much opposed to Bayesian theory.</p></li>
<li><p>1931 <a href="https://en.wikipedia.org/wiki/Bruno_de_Finetti">Bruno de Finetti</a> publishes his â<a href="https://en.wikipedia.org/wiki/De_Finetti%27s_theorem">representation theorem</a>â. This shows that the joint distribution of a sequence of exchangeable events (i.e.Â where the ordering can be permuted) can be represented by a mixture distribution that can be constructed via Bayesâ theorem. (Note that exchangeability is a weaker condition than i.i.d.)
This theorem is often used as a justification Bayesian statistics (along with the so-called Dutch book argument, also by de Finetti).</p></li>
<li><p>1933 publication of <a href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov">Andrey Kolmogorov</a>âs book on probability theory.</p></li>
<li><p>1946 Cox theorem by <a href="https://en.wikipedia.org/wiki/Richard_Threlkeld_Cox">Richard T. Cox (1898â1991)</a>: the aim to generalise classical logic from TRUE/FALSE statements to continuous measures of uncertainty inevitably leads to probability theory and Bayesian learning! This justification of Bayesian statistics was later popularised by <a href="https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes">Edwin T. Jaynes (1922â1998)</a> in various books (1959, 2003).</p></li>
<li><p>1955 Stein Paradox - <a href="https://en.wikipedia.org/wiki/Charles_M._Stein">Charles M. Stein (1920â2016)</a> publishes paper on the Stein estimator - an estimator of the mean that dominates ML estimator. His estimator is always better in terms of MSE than the ML estimator, and this was very puzzling at that time!</p></li>
<li><p>Only from the 1950s the use of the term âBayesian statisticsâ became prevalent â
see S. E. Fienberg. 2006. When Did Bayesian Inference Become âBayesianâ? Bayesian Analysis 1:1â40. <a href="https://doi.org/10.1214/06-BA101" class="uri">https://doi.org/10.1214/06-BA101</a></p></li>
</ul>
<p>Due to advances in personal computing from 1970 onwards Bayesian learning has become more pervasive!</p>
<ul>
<li>Computers allow to do the complex (numerical) calculations needed in Bayesian statistics</li>
<li>Metropolis-Hastings algorithm published in 1970 (which allows to sample from a posterior distribution without explicitly computing the marginal likelihood)</li>
<li>A lot of work on interpreting Stein estimators as empirical Bayes estimators (Efron and Morris 1975)
and on development of regularised estimation techniques such as penalised likelihood in regression (e.g.Â ridge regression)</li>
<li>regularisation originally was only meant to make singular systems/matrices invertible - but then it turned out regularisation has a simple Bayesian interpretation!</li>
<li>work on reference priors (Bernado 1979)</li>
<li>penalised likelihood via KL divergence for model selection (Akaike 1973)</li>
<li>EM algorithm published which uses Bayes theorem for imputing distribution of latent variable</li>
</ul>
<p>Another boost was in the 1990/2000s when in science (e.g.Â genomics) many complex and high-dimensional data set were becoming widely available. Classical statistical methods cannot be used in this setting (overfitting!) so many new methods were developed for high-dimensional data analysis, many with direct link to Bayesian statistics:</p>
<ul>
<li>1996 lasso regression (<a href="https://en.wikipedia.org/wiki/Robert_Tibshirani">Robert Tibshirani</a>)</li>
<li>Machine learning etc (many Bayesians in this field, due to variational Bayes techniques)</li>
</ul>
</div>
<div id="connection-with-entropy-learning" class="section level3" number="7.3.3">
<h3>
<span class="header-section-number">7.3.3</span> Connection with entropy learning<a class="anchor" aria-label="anchor" href="#connection-with-entropy-learning"><i class="fas fa-link"></i></a>
</h3>
<p>The <em>Bayesian update rule</em> is a very general form of learning when the <em>new information arrives in the form of data</em>.</p>
<p>But actually there is an even a more general principle: the <strong>principle of minimal information update</strong> (e.g.Â Jaynes 1959, 2003) or <strong>principle of minimum information discrimination (MDI) (Kullback 1959)</strong>:</p>
<ul>
<li><strong>Change your beliefs only as much as necessary to be coherent with new evidence!</strong></li>
</ul>
<p>Thus, when new information arrives then the uncertainty about the parameter is only minimally adjusted, just as much as needed to account for the new information (âinertia of beliefsâ).</p>
<p>To implement this principle quantitatively we use KL divergence as underlying measure to quantify the
change of the underlying beliefs. This is also known as <strong>entropy learning</strong>.</p>
<p>The Bayes rule emerges a special case of entropy learning:</p>
<ul>
<li>The KL divergence between the joint posterior <span class="math inline">\(Q_{x,\theta}\)</span> and joint prior distribution <span class="math inline">\(P_{x,\theta}\)</span> is computed, with the posterior
distribution <span class="math inline">\(Q_{\theta|x}\)</span> as free parameter.</li>
<li>The conditional distribution <span class="math inline">\(Q_{\theta|x}\)</span> is found by minimising the KL divergence <span class="math inline">\(D_{\text{KL}}(Q_{x,\theta}, P_{x,\theta})\)</span>.</li>
<li>It turns out that the optimal solution to this variational optimisation problem is given by Bayesâ rule!</li>
</ul>
<p>Note that this application of the KL divergence is an example of <strong>reverse KL optimisation</strong> (aka <span class="math inline">\(I\)</span>-projection, see Part I of the notes), which also explains the zero forcing property Bayesâ rule.</p>
<p>Applying the entropy learning rule therefore leads to Bayesian learning:</p>
<ol style="list-style-type: decimal">
<li>if information arrives in form of data <span class="math inline">\(\rightarrow\)</span> update prior by Bayesâ theorem (Bayesian learning)</li>
</ol>
<p>Interestingly, entropy learning will lead to other update rules as special case for other types of information:</p>
<ol start="2" style="list-style-type: decimal">
<li><p>if information is in the form of another distribution <span class="math inline">\(\rightarrow\)</span> update using R. Jeffreyâs rule of conditioning (1965)</p></li>
<li><p>if the information is in form of constraints <span class="math inline">\(\rightarrow\)</span> Kullbackâs principle of minimum MDI (1959),
E. T. Jaynes maximum entropy (MaxEnt) principle (1957)</p></li>
</ol>
<p>Since 1) is by far the most common situation it is clear why it is important to study Bayesian learning!</p>
<p>This shows (again) how fundamentally important KL divergence is in statistics - it not only leads to likelihood inference but also to Bayesian learning, as well as to other forms of information updating!</p>
<p>Furthermore, in Bayesian statistics relative entropy is useful to choose priors (e.g.Â reference priors) and
also helps in experimental design to quantify the information provided by an experiment.</p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></div>
<div class="next"><a href="beta-binomial-model-for-estimating-a-proportion.html"><span class="header-section-number">8</span> Beta-Binomial model for estimating a proportion</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#essentials-of-bayesian-statistics"><span class="header-section-number">7</span> Essentials of Bayesian statistics</a></li>
<li>
<a class="nav-link" href="#probabilistic-prerequisites"><span class="header-section-number">7.1</span> Probabilistic prerequisites</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#conditional-probability"><span class="header-section-number">7.1.1</span> Conditional probability</a></li>
<li><a class="nav-link" href="#bayes-theorem"><span class="header-section-number">7.1.2</span> Bayesâ theorem</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#principle-of-bayesian-learning"><span class="header-section-number">7.2</span> Principle of Bayesian learning</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#from-prior-to-posterior-distribution"><span class="header-section-number">7.2.1</span> From prior to posterior distribution</a></li>
<li><a class="nav-link" href="#zero-forcing-property"><span class="header-section-number">7.2.2</span> Zero forcing property</a></li>
<li><a class="nav-link" href="#bayesian-update-and-likelihood"><span class="header-section-number">7.2.3</span> Bayesian update and likelihood</a></li>
<li><a class="nav-link" href="#sequential-updates"><span class="header-section-number">7.2.4</span> Sequential updates</a></li>
<li><a class="nav-link" href="#summaries-of-posterior-distributions-and-credible-intervals"><span class="header-section-number">7.2.5</span> Summaries of posterior distributions and credible intervals</a></li>
<li><a class="nav-link" href="#practical-application-of-bayes-statistics-on-the-computer"><span class="header-section-number">7.2.6</span> Practical application of Bayes statistics on the computer</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#some-background-on-bayesian-statistics"><span class="header-section-number">7.3</span> Some background on Bayesian statistics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#bayesian-interpretation-of-probability"><span class="header-section-number">7.3.1</span> Bayesian interpretation of probability</a></li>
<li><a class="nav-link" href="#historical-developments"><span class="header-section-number">7.3.2</span> Historical developments</a></li>
<li><a class="nav-link" href="#connection-with-entropy-learning"><span class="header-section-number">7.3.3</span> Connection with entropy learning</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 16 March 2022.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
