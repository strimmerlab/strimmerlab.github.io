[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"lecture notes MATH20802, course Statistical Methods second year mathematics students Department Mathematics University Manchester.course text written Korbinian Strimmer 2019–2021. version 13 May 2021.notes updated time time. view current\nversion visit \nonline MATH20802 lecture notes.\nmay also download MATH20802 lecture notes PDF.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"notes licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"about-the-author","chapter":"Preface","heading":"About the author","text":"Hello! name Korbinian Strimmer Professor Statistics. part Statistics group\nDepartment Mathematics University Manchester. can find information home page.first taught module spring term 2019 University Manchester, subsequently also 2020 2021.hope enjoy course! questions, comments, corrections please email korbinian.strimmer@manchester.ac.uk.","code":""},{"path":"preface.html","id":"about-the-module","chapter":"Preface","heading":"About the module","text":"","code":""},{"path":"preface.html","id":"topics-covered","chapter":"Preface","heading":"Topics covered","text":"MATH20802 module designed run course 11 weeks.\nthree parts:Likelihood estimation likelihood ratio tests (W1–W5)Bayesian learning inference (W6–W8)Linear regression (W9–W11)module focuses conceptual understanding methods, theory,\n, presentation course non-technical.\naim offer insights diverse statistical approaches\nlinked demonstrate statistics offers concise coherent theory information rather adhoc collection “recipes” data analysis (common wrong perception statistics).","code":""},{"path":"preface.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"module important refresh knowledge :Introduction statisticsProbabilityR data analysis programmingIn addition need know matrix algebra compute\ngradient curvature function several variables.Check Appendix brief refresher essential material.","code":""},{"path":"preface.html","id":"additional-support-material","chapter":"Preface","heading":"Additional support material","text":"Accompanying notes arelecture videos (visualiser style).Furthermore, also MATH20802 online reading list hosted University Manchester library.University Manchester student enrolled module\nfind Blackboard:weekly learning plan 11 week study period,weekly worksheets examples solutions R code, andexam papers previous years.","code":""},{"path":"preface.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"Many thanks Beatriz Costa Gomes help creating 2019 version lecture notes Kristijonas Raudys extensive feedback 2020 version.","code":""},{"path":"overview-of-statistical-learning.html","id":"overview-of-statistical-learning","chapter":"1 Overview of statistical learning","heading":"1 Overview of statistical learning","text":"","code":""},{"path":"overview-of-statistical-learning.html","id":"how-to-learn-from-data","chapter":"1 Overview of statistical learning","heading":"1.1 How to learn from data?","text":"fundamental question extract information data optimal way, make predictions based information.purpose, number competing theories information developed. Statistics oldest science information concerned offering principled ways learn data extract process information using probabilistic models.\nHowever, theories information (e.g. Vapnik-Chernov theory learning, computational learning) algorithmic analytic sometimes even based probability theory.Furthermore, disciplines, computer science machine learning closely linked also substantial overlap statistics. field “data science” today comprises statistics machine learning brings together mathematics, statistics computer science. Also growing field -called “artificial intelligence” makes substantial use statistical machine learning techniques.recent popular science book “Master Algorithm” Domingos (2015) provides accessible informal overview \nvarious schools science information. discusses main algorithms used machine learning statistics:Starting early 1763, Bayesian school learning started later turned closely linked likelihood inference established 1922 R.. Fisher (1890–1962) generalised 1951 entropy learning Kullback Leibler.Starting early 1763, Bayesian school learning started later turned closely linked likelihood inference established 1922 R.. Fisher (1890–1962) generalised 1951 entropy learning Kullback Leibler.also 1950s concept artificial neural network arises, essentially nonlinear input-output map works non-probabilistic way. field saw another leap 1980 progress 2010 onwards development deep dearning. now one popular (effective) methods analysis imaging data. Even mobile phone likely dedicated computer chip special neural network hardware, example.also 1950s concept artificial neural network arises, essentially nonlinear input-output map works non-probabilistic way. field saw another leap 1980 progress 2010 onwards development deep dearning. now one popular (effective) methods analysis imaging data. Even mobile phone likely dedicated computer chip special neural network hardware, example.advanced theories information developed 1960 term \ncomputational learning, notably Vapnik-Chernov theory, prominent example “support vector machine” (another non-probabilistic model).advanced theories information developed 1960 term \ncomputational learning, notably Vapnik-Chernov theory, prominent example “support vector machine” (another non-probabilistic model).advent large-scale genomic high-dimensional data surge new exciting developments field high-dimensional (large dimension) also big data (large dimension large sample size), statistics machine learning.advent large-scale genomic high-dimensional data surge new exciting developments field high-dimensional (large dimension) also big data (large dimension large sample size), statistics machine learning.connections various fields information still perfectly understood, clear overarching theory need based probabilistic learning.","code":""},{"path":"overview-of-statistical-learning.html","id":"probability-theory-versus-statistical-learning","chapter":"1 Overview of statistical learning","heading":"1.2 Probability theory versus statistical learning","text":"study statistics (information theory) need aware fundamental difference probability theory\nstatistics, relates distinction \n“randomness” “uncertainty”.Probability theory studies randomness, developing mathematical models randomness (probability distributions), studying corresponding mathematical properties (including asymptotics etc). Probability theoy may fact viewed branch measure theory, thus belongs\ndomain pure mathematics.Probability theory provides probabilistic generative models data, simulation\ndata use learning data, .e. inference model observations.\nMethods theory best learn data domain applied mathematics, specifically statistics related areas machine learning data science.Note statistics, contrast probability, fact concerned randomness. Instead, focus measuring elucidating uncertainty events, predictions, outcomes, parameters uncertainty measures state knowledge. Note new data information becomes available, state knowledge thus uncertainty changes! Thus, uncertainty epistemological property.uncertainty often due ignorance true underlying processes (purpose ), underlying process actually random. success statistics based fact can mathematically model uncertainty without knowing specifics underlying processes, still procedures optimal inference uncertainty.short, statistics describing state knowledge world, \nmay uncertain incomplete, make decisions prediction face uncertainty, uncertaintly sometimes derives randomness often ignorance (sometimes ignorance even helps create simple yet effective model)!","code":""},{"path":"overview-of-statistical-learning.html","id":"cartoon-of-statistical-learning","chapter":"1 Overview of statistical learning","heading":"1.3 Cartoon of statistical learning","text":"observe data \\(x_1, \\ldots, x_n\\)\nassumed generated underlying true model \\(M_{\\text{true}}\\).explain data, make prediction, make hypotheses\nform candidate models \\(M_{1}, M_{2}, \\ldots\\).\ntrue model \\(M_{\\text{true}}\\) unknown observed. However, can observe finite amount data model\nmeasuring properties objects interest (observations experiments). Sometimes can also perturb model see effect (interventional study).various candidate models \\(M_1, M_2, \\ldots\\) model world never perfect correct\ntrue model \\(M_{\\text{true}}\\) among candidate models idealised situation. However, even imperfect candidate model often provide useful mathematical approximation capture important characteristics true model thus help\ninterpret observed data..\\[\n\\begin{array}{cc}\n\\textbf{Hypothesis} \\\\\n\\text{world works} \\\\\n\\end{array}\n\\longrightarrow \n\\begin{array}{cc}\n\\textbf{Model world} \\\\\nM_1: f(x | \\theta_1)  \\\\\nM_2: f(x | \\theta_2)  \\\\\n\\vdots\\\\\n\\end{array}\n\\]\n\\[\n\\longrightarrow \n\\begin{array}{cc}\n\\textbf{Real world,} \\\\\n\\textbf{unknown true model} \\\\\nM_{\\text{true}}: f(x | \\theta_{\\text{true}}) \\\\\n\\end{array}\n\\longrightarrow \\textbf{Data } x_1, \\ldots, x_n\n\\]aim statistical learning identify model(s) explain current data also predict future data (.e. predict outcome experiments conducted yet).Thus good model provides good fit current data (.e. explains current observations well) also future data (.e. generalises well).large proportion statistical theory devoted finding “good” models\navoid overfitting (models complex don’t generalise well) \nunderfitting (models simplistic hence also don’t predict well).Typically aim find model whose model complexity matches \ncomplexity unknown true model also complexity data observed unknown true model.","code":""},{"path":"overview-of-statistical-learning.html","id":"likelihood","chapter":"1 Overview of statistical learning","heading":"1.4 Likelihood","text":"core problem statistics find probabilistic models explaining existing data predicting new data.\nneed measure good hypothesis/candidate model \\(M_k\\) approximation (typically unknown) true\ndata generating model \\(M_{\\text{true}}\\).already know year 1 module MATH10282 “Introduction Statistics”, one measure provided likelihood function helps choose among various candidate models estimate corresponding parameters finding model \\(M\\) maximises (log)-likelihood.Given probability distribution \\(F_{\\boldsymbol \\theta}\\) density mass function \\(f(x|\\boldsymbol \\theta)\\) \\(\\boldsymbol \\theta\\) parameter vector, \\(x_1,\\dots,x_n\\) observed iid data (.e. independent identically distributed), likelihood function defined \n\\[\nL_n(\\boldsymbol \\theta) =\\prod_{=1}^{n} f(x_i|\\boldsymbol \\theta)\n\\]\nTypically, instead likelihood one uses log-likelihood function:\n\\[\\log L(_n\\boldsymbol \\theta) = l_n(\\boldsymbol \\theta) = \\sum_{=1}^n \\log f(x_i|\\boldsymbol \\theta)\\]\nReasons using log-likelihood (rather likelihood) include thatthe log density fact “natural” relevant quantity (become clear upcoming chapters) thataddition numerically stable multiplication computer.discrete random variables \\(f(x |\\boldsymbol \\theta)\\) probability mass function likelihood often interpreted probability observe data given model specified parameters \\(\\boldsymbol \\theta\\). fact, indeed way likelihood historically introduced. However, view stricly correct.\nFirst, given samples iid thus ordering \\(x_i\\) important, additional factor accounting possible permutations needed obtain actual probability data. Moreover, continuous random variables interpretation breaks due use densities rather probability mass functions likelihood. Thus, view likelihood probability data fact simplistic.next chapter see justification using likelihood rather stems close link Kullback-Leibler information cross-entropy. also helps understand using likelihood estimation optimal limit large sample size.first part MATH28082 “Statistical Methods” module study likelihood estimation inference much detail. provide links related methods inference discuss information-theoretic foundations. also discuss optimality properties well limitation likelihood inference. Extensions likelihood analysis, particular Bayesian learning, discussed second part module. third part module apply statistical learning linear regression.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"from-entropy-to-maximum-likelihood","chapter":"2 From entropy to maximum likelihood","heading":"2 From entropy to maximum likelihood","text":"","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"entropy","chapter":"2 From entropy to maximum likelihood","heading":"2.1 Entropy","text":"","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"overview","chapter":"2 From entropy to maximum likelihood","heading":"2.1.1 Overview","text":"chapter discuss various information criteria connection maximum likelihood.modern definition (relative) entropy, “disorder”, first discovered 1875 physicist L. Boltzmann (1844–1906)\ncontext thermodynamics. 1940–1950’s notion entropy turned central information theory, field pioneered mathematicians \nR. Hartley (1988–1970),\nS. Kullback (1907–1994),\nR. Leibler (1914–2003),\n. Turing (1912–1954),\n. J. Good (1916–2009),\nC. Shannon (1916–2001), \nE. T. Jaynes (1922–1998),\nlater explored \nS. Amari (1936–),\n. Ciszár (1938–),\nB. Efron (1938–),\n. P. Dawid (1946–) \nmany others.\\[\\begin{align*}\n\\left.\n\\begin{array}{cc}\n\\\\\n\\textbf{Entropy} \\\\\n\\\\\n\\end{array}\n\\right.\n\\left.\n\\begin{array}{cc}\n\\\\\n\\nearrow  \\\\\n\\searrow  \\\\\n\\\\\n\\end{array}\n\\right.\n\\begin{array}{ll}\n\\text{Shannon Entropy} \\\\\n\\\\\n\\text{Relative Entropy}  \\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{(Shannon 1948)} \\\\\n\\\\\n\\text{(Kullback-Leibler 1951)}  \\\\\n\\end{array}\n\\end{align*}\\]\\[\\begin{align*}\n\\left.\n\\begin{array}{ll}\n\\text{Fisher information} \\\\\n\\\\\n\\text{Mutual Information} \\\\\n\\end{array}\n\\right.\n\\begin{array}{ll}\n\\rightarrow\\text{ Likelihood theory} \\\\\n\\\\\n\\rightarrow\\text{ Information theory} \\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{(Fisher 1922)} \\\\\n\\\\\n\\text{(Shannon 1948, Lindley 1953)}  \\\\\n\\end{array}\n\\end{align*}\\]","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"surprise-surprisal-or-shannon-information","chapter":"2 From entropy to maximum likelihood","heading":"2.1.2 Surprise, surprisal or Shannon information","text":"surprise observe event probability \\(p\\) defined \\(-\\log(p)\\).\nalso called surprisal Shannon information.Thus, surprise observe certain event (\\(p=1\\)) zero,\nconversely surprise observe event certain happen\n(\\(p=0\\)) infinite.log-odds ratio can viewed difference surprise event surprise complementary event:\n\\[\n\\log\\left( \\frac{p}{1-p} \\right) =  -\\log(1-p) - ( -\\log(p))\n\\]module always use natural logarithm default, explicitly write \\(\\log_2\\) \\(\\log_{10}\\) logarithms respect base 2 10, respectively.Surprise entropy computed natural logarithm (\\(\\log\\)) given “nats” (=natural information units ). Using \\(\\log_2\\) leads “bits” using \\(\\log_{10}\\) “ban” “Hartley”.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"shannon-entropy","chapter":"2 From entropy to maximum likelihood","heading":"2.1.3 Shannon entropy","text":"Assume discrete distribution \\(F\\) \\(K\\) classes \nclass probabilities \\(p_1, \\ldots, p_K\\) \\(\\text{Pr}(\\text{\"class k\"}) = p_k\\)\n\\(\\sum_{k=1}^K = 1\\). Let \\(x\\) indicate selected class define PMF\n\\(f(x = \\text{\"class k\"})= p_k\\).Shannon entropy discrete distribution \\(F\\) defined \nexpected surprise, .e. negative expected log-probability\n\\[\n\\begin{split}\nH(F) &=-\\text{E}_F\\left(\\log f(x)\\right) \\\\\n     &= - \\sum_{=1}^{K}p_i \\log(p_i) \\\\\n\\end{split}\n\\]\n\\(p_k [0,1]\\) construction Shannon entropy must larger equal 0.\nFurthermore, bounded \\(\\log K\\). Hence \ndiscrete distribution \\(F\\) \\(K\\) categories \n\\[\\log K \\geq  H(F) \\geq 0\\]Example 2.1  Discrete uniform distribution \\(U_K\\): let \\(p_1=p_2= \\ldots = p_K = \\frac{1}{K}\\).\n\\[H(U_K) = - \\sum_{=1}^{K}\\frac{1}{K} \\log\\left(\\frac{1}{K}\\right) = \\log K\\]Note largest value Shannon entropy can assume\n\\(K\\) classes.Example 2.2  Concentrated probability mass: let\n\\(p_1=1\\) \\(p_2=p_3=\\ldots=p_K=0\\).\nUsing \\(0\\times\\log(0)=0\\) obtain Shannon probability\n\\[ H(F) = 1\\times\\log(1) + 0\\times\\log(0) + \\dots = 0\\]Note 0 smallest value Shannon entropy can assume, corresponds maximum concentration.Thus, large entropy implies distribution spread whereas small entropy\nmeans distribution concentrated.Correspondingly, maximum entropy distributions can considered minimally informative random variable.interpretation also supported close link Shannon entropy multinomial coefficients counting permutations \\(n\\) items (samples) \\(K\\) distinct types (classes).Example 2.3  Large sample asymptotics multinomial coefficient Shannon entropy:number possible permutation \\(n\\) items \\(K\\) distinct types, \\(n_1\\) type 1, \\(n_2\\) type 2 , given multinomial coefficient\n\\[\n\\binom{n}{n_1, \\ldots, n_K} = \\frac {n!}{n_1! \\times n_2! \\times\\ldots \\times n_K! } \n\\]\n\\(\\sum_{k=1}^K n_k = n\\) \\(K \\leq n\\).Using Moivre-Sterling formula large \\(n\\) factorial can approximated \n\\[\n\\log n! \\approx  n \\log n  -n \n\\]result\n\\[\n\\begin{split}\n\\log \\binom{n}{n_1, \\ldots, n_K} &= \\log n! - \\sum_{k=1}^K \\log n_k!\\\\\n& \\approx    n \\log n  -n - \\sum_{k=1}^K (n_k \\log n_k  -n_k) \\\\\n& = n \\log n - \\sum_{k=1}^K n_k \\log n_k\\\\\n& = \\sum_{k=1}^K n_k \\log n - \\sum_{k=1}^K n_k \\log n_k\\\\\n& = - \\sum_{k=1}^K n_k \\log\\left( \\frac{n_k}{n} \\right)\\\\\n\\end{split}\n\\]\nthus\n\\[\n\\begin{split}\n\\frac{1}{n}\\log \\binom{n}{n_1, \\ldots, n_K} &\\approx - \\sum_{k=1}^K \\frac{n_k}{n} \\log\\left( \\frac{n_k}{n} \\right)\\\\\n&=  - \\sum_{k=1}^K \\hat{p}_k \\log\\left( \\hat{p}_k \\right)\\\\\n&=H(\\hat{F})\n\\end{split}\n\\]\n\\(\\hat{F}\\) empirical discrete distribution \\(\\hat{p}_k = \\frac{n_k}{n}\\).combinatorical derivation entropy due Boltzmann (1877) discovered research statistical mechanics thermodynamics.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"differential-entropy","chapter":"2 From entropy to maximum likelihood","heading":"2.1.4 Differential entropy","text":"Shannon entropy defined discrete random variables.Differential Entropy results applying \ndefinition Shannon entropy continuous random variable \\(x\\) density \\(f(x)\\):\n\\[\nH(F) = -\\text{E}_F(\\log f(x)) = - \\int f(x) \\log f(x) \\, dx\n\\]\nDespite essentially formula different name justified differential entropy exhibits different properties compared Shannon entropy, logarithm taken density\ncontrast probability can assumes values larger one.\nconsequence, differential entropy bounded zero can negative.Example 2.4  Consider uniform distribution \\(U(0, )\\) \\(>0\\), support \\(0\\) \\(\\) density \\(f(x) = 1/\\). \\(-\\int_0^f(x) \\log f(x) dx =- \\int_0^\\frac{1}{} \\log(\\frac{1}{}) dx = \\log \\)\ndifferential entropy \n\\[H( U(0, ) ) =  \\log \\,.\\]\nNote \\(< 1\\) differential entropy negative.Example 2.5  density univariate normal \\(N(\\mu, \\sigma^2)\\) distribution\n\\(f(x |\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)\\) \\(\\sigma^2 > 0\\).\ncorresponding differential entropy \n\n\\[\nH(F) = \\frac{1}{2} ( \\log(2 \\pi \\sigma^2)+1) \\,.\n\\]\nNote depends variance mean, \n\\(\\sigma^2 < 1/(2 \\pi e) \\approx 0.0585\\) differential entropy negative.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"maximum-entropy-principle-to-characterise-distributions","chapter":"2 From entropy to maximum likelihood","heading":"2.1.5 Maximum entropy principle to characterise distributions","text":"maximum Shannon entropy differential entropy useful characterise distributions:discrete\nuniform distribution maximum entropy distribution among discrete distributions.discrete\nuniform distribution maximum entropy distribution among discrete distributions.maximum entropy distribution continuous random variable support \\([-\\infty, \\infty]\\) specific mean variance normal distributionthe maximum entropy distribution continuous random variable support \\([-\\infty, \\infty]\\) specific mean variance normal distributionthe maximum entropy distribution among continuous distributions supported \\([0, \\infty]\\) specified mean exponential distribution.maximum entropy distribution among continuous distributions supported \\([0, \\infty]\\) specified mean exponential distribution.higher entropy spread (uninformative) distribution.Using maximum entropy characterise maximally uniformative distributions\nadvocated E.T. Jaynes (also proposed use maximum entropy context finding Bayesian priors).list maximum entropy distribution given :\nhttps://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution .","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"cross-entropy","chapter":"2 From entropy to maximum likelihood","heading":"2.1.6 Cross-entropy","text":"definition Shannon entropy (differential entropy) expectation \nlog-density (say \\(g(x)\\) distribution \\(G\\)) regard different distribution \\(F\\) state space\narrive cross-entropy\n\\[H(F, G) =-\\text{E}_F\\left( \\log g(x)  \\right)\\]Therefore, cross-entropy measure linking two distributions \\(F\\) \\(G\\).Note thatcross-entropy symmetric regard \\(F\\) \\(G\\), \nexpectation taken reference \\(F\\).construction \\(H(F, F) = H(F)\\).crucial property cross-entropy \\(H(F, G)\\) bounded entropy \\(F\\), therefore\n\\[\nH(F, G) \\geq H(F)\n\\]\nequality \\(F=G\\).Equivalently can write\n\\[\nH(F, G)-H(F) \\geq 0\n\\]\nfact, recalibrated cross-entropy turns fundamental cross-entropy Shannon resp. differential entropy. studied detail next section.Example 2.6  Cross-entropy two normals:Assume \\(F_{\\text{ref}}=N(\\mu_{\\text{ref}},\\sigma^2_{\\text{ref}})\\) \\(F=N(\\mu,\\sigma^2)\\).\ncross-entropy \n\\[\nH(F_{\\text{ref}}, F) = \\frac{1}{2} \\left( \\frac{(\\mu - \\mu_{\\text{ref}})^2}{ \\sigma^2 } \n +\\frac{\\sigma^2_{\\text{ref}}}{\\sigma^2}  +\\log(2 \\pi \\sigma^2) \\right)\n\\]Example 2.7  \\(\\mu_{\\text{ref}} = \\mu\\) \\(\\sigma^2_{\\text{ref}} = \\sigma^2\\)\ncross-entropy \\(H(F,G)\\) degenerates differential entropy\n\\(H(F_{\\text{ref}}) = \\frac{1}{2} \\left(\\log( 2 \\pi \\sigma^2_{\\text{ref}}) +1 \\right)\\).","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"kullback-leibler-divergence","chapter":"2 From entropy to maximum likelihood","heading":"2.2 Kullback-Leibler divergence","text":"","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"definition","chapter":"2 From entropy to maximum likelihood","heading":"2.2.1 Definition","text":"Also known relative entropy discrimination information.relative entropy measures divergence\ndistribution \\(G\\) distribution \\(F\\) defined \n\\[\n\\begin{split}\nD_{\\text{KL}}(F,G) &= \\text{E}_F\\log\\left(\\frac{dF}{dG}\\right) \\\\\n& = \\text{E}_F\\log\\left(\\frac{f(x)}{g(x)}\\right) \\\\\n& =\n\\underbrace{-\\text{E}_F(\\log g(x))}_{\\text{cross-entropy}} - \n (\\underbrace{-\\text{E}_F (\\log f(x))  }_\\text{(differential) entropy})  \\\\\n& = H(F, G)-H(F) \\\\\n\\end{split}\n\\]\\(D_{\\text{KL}}(F, G)\\) measures amount information lost \\(G\\) used approximate \\(F\\).\\(F\\) \\(G\\) identical (information lost) \\(D_{\\text{KL}}(F,G)=0\\).(Note: “divergence” measures dissimilarity probability distributions. type divergence related confused divergence (div) used vector analysis.)term divergence (rather distance) implies also distributions \\(F\\) \\(G\\) interchangeable \\(D_{\\text{KL}}(F, G)\\).applications statistics typical roles \\(F\\) \\(G\\) :\\(F\\) (unknown) underlying true model data generating process\\(G\\) approximating model (e.g. parametric family)Bayesian statistics use\\(F\\) posterior distribution\\(G\\) prior distributionThere exist various notations KL divergence literature.\nuse \\(D_{KL}(F, G)\\) often can find \\(KL(F || G)\\)\n\\(\\boldsymbol ^{KL}(F; G)\\) references.authors (e.g. Efron) call twice KL divergence \\(2 D_{KL}(F, G) = D(F, G)\\) deviance \\(G\\) \\(F\\).","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"properties-of-kl-divergence","chapter":"2 From entropy to maximum likelihood","heading":"2.2.2 Properties of KL divergence","text":"\\(D_{\\text{KL}}(F, G) \\neq D_{\\text{KL}}(G, F)\\), .e., KL divergence symmetric, \\(F\\) \\(G\\) interchanged.\\(D_{\\text{KL}}(F, G) = 0\\) \\(F=G\\), .e., KL divergence zero \\(F\\) \\(G\\) identical.\\(D_{\\text{KL}}(F, G)\\geq 0\\), proof via Jensen Inequality.\\(D_{\\text{KL}}(F, G)\\) remains invariant coordinate transformations,\n.e. invariant geometric quantity.Note KL divergence expectation taken ratio densities (ratio probabilities discrete random variables). creates transformation invariance.details proofs properties 3 4 see Worksheet 1.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"examples","chapter":"2 From entropy to maximum likelihood","heading":"2.2.3 Examples","text":"Example 2.8  KL divergence two Bernoulli distributions \\(\\text{Ber}(p)\\) \\(\\text{Ber}(q)\\):“success” probabilities two distributions \\(p\\) \\(q\\),\nrespectively, complementary “failure” probabilities \\(1-p\\) \\(1-q\\).\nget KL divergence\n\\[\nD_{\\text{KL}}(\\text{Ber}(p), \\text{Ber}(q))=p \\log\\left( \\frac{p}{q}\\right) + (1-p) \\log\\left(\\frac{1-p}{1-q}\\right)\n\\]Example 2.9  KL divergence two univariate normals different means variances:Assume \\(F_{\\text{ref}}=N(\\mu_{\\text{ref}},\\sigma^2_{\\text{ref}})\\) \\(F=N(\\mu,\\sigma^2)\\).\n\n\\[\n\\begin{split}\nD_{\\text{KL}}(F_{\\text{ref}},F) &= H(F_{\\text{ref}}, F) - H(F_{\\text{ref}}) \\\\\n&= \\frac{1}{2} \\left(   \\frac{(\\mu-\\mu_{\\text{ref}})^2}{\\sigma^2}  + \\frac{\\sigma_{\\text{ref}}^2}{\\sigma^2}\n-\\log\\left(\\frac{\\sigma_{\\text{ref}}^2}{\\sigma^2}\\right)-1  \n   \\right) \\\\\n\\end{split}\n\\]Example 2.10  KL divergence two univariate normals different means common variance:important special case previous Example 2.9 occurs \nvariances equal. get\n\\[D_{\\text{KL}}(N(\\mu_{\\text{ref}}, \\sigma^2),   N(\\mu, \\sigma^2)  )=\\frac{1}{2} \\left(\\frac{(\\mu-\\mu_{\\text{ref}})^2}{\\sigma^2}\\right)\\]","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"local-quadratic-approximation-and-expected-fisher-information","chapter":"2 From entropy to maximum likelihood","heading":"2.3 Local quadratic approximation and expected Fisher information","text":"","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"definition-of-expected-fisher-information","chapter":"2 From entropy to maximum likelihood","heading":"2.3.1 Definition of expected Fisher information","text":"KL information measures divergence two distributions.\nmay thus use relative entropy measure divergence two distributions family, separated parameter space small \\(\\boldsymbol \\varepsilon\\).Let \\(h(\\boldsymbol \\theta) = D_{\\text{KL}}(F_{\\boldsymbol \\theta_0}, F_{\\boldsymbol \\theta}) = \\text{E}_{\\boldsymbol \\theta_0}\\left( \\log f(\\boldsymbol x| \\boldsymbol \\theta_0) - \\log f(\\boldsymbol x| \\boldsymbol \\theta) \\right)\\).\nNote first distribution KL divergence fixed\n\\(F_{\\boldsymbol \\theta_0}\\) second distribution varied. \\(h(\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon) = D_{\\text{KL}}(F_{\\boldsymbol \\theta_0}, F_{\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon})\\).\nSince KL divergence vanishes two arguments identical \\(h(\\boldsymbol \\theta)\\) reaches minimum \\(\\boldsymbol \\theta_0\\) \\(h(\\boldsymbol \\theta_0)=0\\) flat gradient \\(\\nabla h(\\boldsymbol \\theta_0) =0\\).can therefore approximate \\(h(\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon)\\) \nquadratic function around \\(\\boldsymbol \\theta_0\\)\n\\[\n\\begin{split}\nh(\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon) & \\approx \\frac{1}{2} \\boldsymbol \\varepsilon^T  \\nabla^T\\nabla h(\\boldsymbol \\theta_0) \\boldsymbol \\varepsilon\\\\\n& = \\frac{1}{2} \\boldsymbol \\varepsilon^T \\left( -\\text{E}_{\\boldsymbol \\theta_0} \\nabla^T\\nabla  \\log f(\\boldsymbol x| \\boldsymbol \\theta_0)   \\right)   \\boldsymbol \\varepsilon\\\\\n& = \\frac{1}{2} \\boldsymbol \\varepsilon^T  \\underbrace{\\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta_0)}_{\\text{expected Fisher information} }\\boldsymbol \\varepsilon\\\\\n\\end{split}\n\\]\nyields expected Fisher information \\(\\boldsymbol \\theta_0\\) negative\nexpected Hessian matrix log-density \\(\\boldsymbol \\theta_0\\).\nSince \\(\\boldsymbol \\theta_0\\) minimum expected Fisher information matrix must positive definite!Ss data involved expected Fisher information purely property model, precisely space models indexed \\(\\boldsymbol \\theta\\).\nnext Chapter study related quantity, observed Fisher information contrast function observed data.can use approximation also compute divergence \\(D_{\\text{KL}}(F_{\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon}, F_{\\boldsymbol \\theta_0})\\) first argument varies second kept fixed:\n\\[\n\\begin{split}\nD_{\\text{KL}}(F_{\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon}, F_{\\boldsymbol \\theta_0}) &\\approx \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon)\\, \\boldsymbol \\varepsilon\\\\\n\\end{split} \n\\]\nlinear approximation \\(\\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon) \\approx \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta_0) + \\boldsymbol \\Delta_{\\boldsymbol \\varepsilon}\\)\nelement matrix \\(\\boldsymbol \\Delta_{\\boldsymbol \\varepsilon}\\) scalar product \\(\\boldsymbol \\varepsilon\\)\ngradient corresponding element \\(\\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta_0)\\)\nevaluated \\(\\boldsymbol \\theta_0\\). Therefore \\(\\boldsymbol \\varepsilon^T \\boldsymbol \\Delta_{\\boldsymbol \\varepsilon} \\boldsymbol \\varepsilon\\)\ncubic order \\(\\boldsymbol \\varepsilon\\) hence\n\\[\n\\begin{split}\nD_{\\text{KL}}(F_{\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon}, F_{\\boldsymbol \\theta_0}) &\\approx \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta_0+\\boldsymbol \\varepsilon) \\boldsymbol \\varepsilon\\\\\n&\\approx \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta_0) \\boldsymbol \\varepsilon+ \\boldsymbol \\varepsilon^T \\boldsymbol \\Delta_{\\boldsymbol \\varepsilon} \\boldsymbol \\varepsilon\\\\\n&\\approx \\frac{1}{2}\\boldsymbol \\varepsilon^T \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta_0) \\boldsymbol \\varepsilon\n\\end{split} \n\\]\nkeeping terms quadratic \\(\\boldsymbol \\varepsilon\\).","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"examples-1","chapter":"2 From entropy to maximum likelihood","heading":"2.3.2 Examples","text":"Example 2.11  Expected Fisher information Bernoulli distribution:log-probability mass function Bernoulli \\(\\text{Ber}(p)\\) distribution \n\\[\n\\log f(x | p) = x \\log(p) + (1-x) \\log(1-p)\n\\]\n\\(p\\) proportion “success”.\nsecond derivative regard parameter \\(p\\) \n\\[\n\\frac{d^2}{dp^2} \\log f(x | p)  =  -\\frac{x}{p^2}-  \\frac{1-x}{(1-p)^2}\n\\]\nSince \\(\\text{E}(x) = p\\) get Fisher information\n\\[\n\\begin{split}\n^{\\text{Fisher}}(p) & = -\\text{E}\\left(\\frac{d^2}{dp^2} \\log f(x | p)  \\right)\\\\\n                           &= \\frac{p}{p^2}+  \\frac{1-p}{(1-p)^2} \\\\\n                            &= \\frac{1}{p(1-p)}\\\\\n\\end{split}\n\\]Example 2.12  Quadratic approximations KL divergence two Bernoulli distributions:Example 2.8 KL divergence\n\\[\nD_{\\text{KL}}\\left (\\text{Ber}(p_1), \\text{Ber}(p_2) \\right)=p_1 \\log\\left( \\frac{p_1}{p_2}\\right) + (1-p_1) \\log\\left(\\frac{1-p_1}{1-p_2}\\right)\n\\]\n\nExample 2.11 corresponding expected Fisher information.quadratic approximation implies \n\\[\nD_{\\text{KL}}\\left( \\text{Ber}(p), \\text{Ber}(p + \\varepsilon) \\right) \\approx \\frac{\\varepsilon^2}{2}  ^{\\text{Fisher}}(p) =  \\frac{\\varepsilon^2}{2 p (1-p)}\n\\]\nalso \n\\[\nD_{\\text{KL}}\\left( \\text{Ber}(p+\\varepsilon), \\text{Ber}(p) \\right) \\approx \\frac{\\varepsilon^2}{2} ^{\\text{Fisher}}(p) =  \\frac{\\varepsilon^2}{2 p (1-p)}\n\\]Worksheet 1 verified using second order Taylor series applied KL divergence.Example 2.13  Expected Fisher information normal distribution \\(N(\\mu, \\sigma^2)\\).log-density \n\\[\n\\log f(x | \\mu, \\sigma^2) = -\\frac{1}{2} \\log(\\sigma^2) \n-\\frac{1}{2 \\sigma^2} (x-\\mu)^2 - \\frac{1}{2}\\log(2 \\pi)\n\\]\ngradient respect \\(\\mu\\) \\(\\sigma^2\\) (!) row vector\n\\[\n\\nabla \\log f(x | \\mu, \\sigma^2) =\n\\begin{pmatrix}\n\\frac{1}{\\sigma^2} (x-\\mu) \\\\\n- \\frac{1}{2 \\sigma^2} + \\frac{1}{2 \\sigma^4} (x- \\mu)^2 \\\\\n\\end{pmatrix}^T\n\\]\nHint calculating gradient: replace \\(\\sigma^2\\) \\(v\\) take partial derivative regard \\(v\\), substitute back.Hessian matrix \n\\[\n\\nabla^T\\nabla \\log f(x | \\mu, \\sigma^2) =\n\\begin{pmatrix}\n-\\frac{1}{\\sigma^2} & -\\frac{1}{\\sigma^4} (x-\\mu)\\\\\n-\\frac{1}{\\sigma^4} (x-\\mu) &  \\frac{1}{2\\sigma^4} - \\frac{1}{\\sigma^6}(x- \\mu)^2 \\\\\n\\\\\n\\end{pmatrix}\n\\]\n\\(\\text{E}(x) = \\mu\\) \\(\\text{E}(x-\\mu) =0\\).\nFurthermore, \\(\\text{E}( (x-\\mu)^2 ) =\\sigma^2\\) see \n\\(\\text{E}\\left(\\frac{1}{\\sigma^6}(x- \\mu)^2\\right) = \\frac{1}{\\sigma^4}\\). Therefore\nexpected Fisher information matrix negative expected Hessian matrix \n\\[\n\\boldsymbol ^{\\text{Fisher}}\\left(\\mu,\\sigma^2\\right) = \\begin{pmatrix} \\frac{1}{\\sigma^2} & 0 \\\\ 0 & \\frac{1}{2\\sigma^4} \\end{pmatrix}\n\\]","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"entropy-learning-and-maximum-likelihood","chapter":"2 From entropy to maximum likelihood","heading":"2.4 Entropy learning and maximum likelihood","text":"","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"the-relative-entropy-between-true-model-and-approximating-model","chapter":"2 From entropy to maximum likelihood","heading":"2.4.1 The relative entropy between true model and approximating model","text":"Assume observations \\(x_1, \\ldots, x_n\\). data sampled \\(F\\), true unknown data generating distribution. also specify models \\(G_{\\boldsymbol \\theta}\\)\nindexed \\(\\boldsymbol \\theta\\) approximate \\(F\\).relative entropy \\(D_{\\text{KL}}(F,G_{\\boldsymbol \\theta})\\) measures divergence approximation \\(G_{\\boldsymbol \\theta}\\)\nunknow true model \\(F\\). can written :\n\\[\n\\begin{split}\nD_{\\text{KL}}(F,G_{\\boldsymbol \\theta} &= H(F,G_{\\boldsymbol \\theta}) - H(F) \\\\\n&= \\underbrace{- \\text{E}_{F}\\log g_{\\boldsymbol \\theta}(x)}_{\\text{cross-entropy}}\n-(\\underbrace{-\\text{E}_{F}\\log f(x)}_{\\text{entropy $F$, depend $\\boldsymbol \\theta$}})\\\\\n\\end{split}\n\\]However, since know \\(F\\) actually compute divergence. Nonetheless, may use\nempirical distribution \\(\\hat{F}_n\\) — function observed data — approximation \\(F\\), way arrive approximation \\(D_{\\text{KL}}(F,G_{\\boldsymbol \\theta})\\) becomes accurate growing sample size.Recall “Law Large Numbers” :strong law large numbers empirical distribution\n\\(\\hat{F}_n\\) converges true underlying distribution \\(F\\) \\(n \\rightarrow \\infty\\) almost surely:\n\\[\n\\hat{F}_n\\overset{. s.}{\\} F\n\\]strong law large numbers empirical distribution\n\\(\\hat{F}_n\\) converges true underlying distribution \\(F\\) \\(n \\rightarrow \\infty\\) almost surely:\n\\[\n\\hat{F}_n\\overset{. s.}{\\} F\n\\]\\(n \\rightarrow \\infty\\) average \\(\\text{E}_{\\hat{F}_n}(h(X)) = \\frac{1}{n} \\sum_{=1}^n h(x_i)\\) converges expectation \\(\\text{E}_{F}(h(X))\\).\\(n \\rightarrow \\infty\\) average \\(\\text{E}_{\\hat{F}_n}(h(X)) = \\frac{1}{n} \\sum_{=1}^n h(x_i)\\) converges expectation \\(\\text{E}_{F}(h(X))\\).Hence, large sample size \\(n\\) can approximate cross-entropy result KL divergence. cross-entropy \\(H(F, G_{\\boldsymbol \\theta})\\) approximated empirical cross-entropy expectation taken regard \\(\\hat{F}_n\\) rather \\(F\\):\n\\[\n\\begin{split}\nH(F, G_{\\boldsymbol \\theta}) & \\approx H(\\hat{F}_n, G_{\\boldsymbol \\theta}) \\\\\n                  & = - \\text{E}_{\\hat{F}_n} (\\log(x))  \\\\\n                  & = -\\frac{1}{n} \\sum_{=1}^n \\log g(x_i | \\boldsymbol \\theta) \\\\\n                  & -\\frac{1}{n} l_n ({\\boldsymbol \\theta})\n\\end{split}\n\\]\nturns equal negative log-likelihood standardised sample size \\(n\\)! words, likelihood negative empirical cross-entropy (times sample size \\(n\\)).link multinomial coefficient Shannon entropy (Example 2.3) already know large sample size\n\\[\nH(\\hat{F)} \\approx \\frac{1}{n} \\log \\binom{n}{n_1, \\ldots, n_K}\n\\]KL divergence \\(D_{\\text{KL}}(F,G_{\\boldsymbol \\theta})\\) can therefore approximated \n\\[\nD_{\\text{KL}}(F,G_{\\boldsymbol \\theta}) \\approx -\\frac{1}{n} \\left( \\log \\binom{n}{n_1, \\ldots, n_K} + l_n ({\\boldsymbol \\theta})  \\right)\n\\]\nThus, KL divergence obtain just log-likelihood (cross-entropy part) also multiplicity factor taking account possible orderings data (entropy part).","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"minimum-kl-divergence-and-maximum-likelihood","chapter":"2 From entropy to maximum likelihood","heading":"2.4.2 Minimum KL divergence and maximum likelihood","text":"know \\(F\\) simply minimise \\(D_{\\text{KL}}(F, G_{\\boldsymbol \\theta})\\) find particular model \\(G_{\\boldsymbol \\theta}\\) closest true model. Equivalently, minimise cross-entropy \\(H(F, G_{\\boldsymbol \\theta})\\).\nHowever, since actually don’t know \\(F\\) possible.However, large sample size \\(n\\) empirical distribution \\(\\hat{F}_n\\)\ngood approximation \\(F\\), can use results previous section.\nThus, instead minimising KL divergence\n\\(D_{\\text{KL}}(F, G_{\\boldsymbol \\theta})\\) simply minimise \\(H(\\hat{F}_n, G_{\\boldsymbol \\theta})\\) \nmaximising likelihood \\(l_n ({\\boldsymbol \\theta})\\).\nNote entropy true distribution \\(F\\) (corresponding empirical\ndistribution \\(\\hat{F}\\)) depend parameters \\(\\boldsymbol \\theta\\)\nhence matter minimising divergence.Conversely, implies maximising likelihood regard \\(\\boldsymbol \\theta\\) equivalent ( asymptotically large \\(n\\)!) minimising KL divergence approximating model unknown true model!\\[\n\\begin{split}\n\\hat{\\boldsymbol \\theta}^{ML} &= \\underset{\\boldsymbol \\theta}{\\arg \\max}\\,\\, l_n(\\boldsymbol \\theta) \\\\\n &= \\underset{\\boldsymbol \\theta}{\\arg \\min}\\,\\, H(\\hat{F}_n, G_{\\boldsymbol \\theta}) \\\\\n &\\approx \\underset{\\boldsymbol \\theta}{\\arg \\min}\\,\\, D_{\\text{KL}}(F, G_{\\boldsymbol \\theta}) \\\\\n\\end{split}\n\\]Therefore, reasoning behind method maximum likelihood minimises large sample approximation KL divergence candidate model \\(G_{\\boldsymbol \\theta}\\) unkown true model \\(F\\).consequence close link maximum likelhood relative entropy\nmaximum likelihood inherits large \\(n\\) (!) optimality properties KL divergence. discussed detail later course.","code":""},{"path":"from-entropy-to-maximum-likelihood.html","id":"further-connections","chapter":"2 From entropy to maximum likelihood","heading":"2.4.3 Further connections","text":"Since minimising KL divergence contains ML estimation special case may wonder whether broader justification relative entropy context statistical data analysis?Indeed, KL divergence strong geometrical interpretation forms basis information geometry.\nfield manifold distributions\nstudied using tools differential geometry. expected Fisher information\nplays important role metric tensor space distributions.Furthermore, also linked probabilistic forecasting.\nframework -called scoring rules.\nlocal proper scoring rule negative log-probability (“surprise”).\nexpected “surprise” cross-entropy\nrelative entropy corresponding natural divergence connected log scoring rule.Furthermore, another intriguing property KL divergence relative entropy \\(D_{\\text{KL}}(F, G)\\) divergence measure Bregman \\(f\\)-divergence.\nNote \\(f\\)-divergences Bregman-divergences (turn related proper scoring rules) two large classes measures similarity divergence two probability distributions.Finally, likelihood estimation also Bayesian update rule (discussed later module) another special case entropy learning.","code":""},{"path":"maximum-likelihood-estimation.html","id":"maximum-likelihood-estimation","chapter":"3 Maximum likelihood estimation","heading":"3 Maximum likelihood estimation","text":"","code":""},{"path":"maximum-likelihood-estimation.html","id":"principle-of-maximum-likelihood-estimation","chapter":"3 Maximum likelihood estimation","heading":"3.1 Principle of maximum likelihood estimation","text":"","code":""},{"path":"maximum-likelihood-estimation.html","id":"outline","chapter":"3 Maximum likelihood estimation","heading":"3.1.1 Outline","text":"starting points ML analysis arethe observed \\(n\\) data samples \\(x_1,\\ldots,x_n\\), iid (=independent identically distributed), ordering irrelevant, amodel \\(F_{\\boldsymbol \\theta}\\) corresponding probability density probability mass function \\(f(x|\\boldsymbol \\theta)\\) parameters \\(\\boldsymbol \\theta\\)construct likelihood function:\\(L_n(\\boldsymbol \\theta|x_1,\\dots,x_n)=\\prod_{=1}^{n} f(x_i|\\boldsymbol \\theta)\\)Historically, likelihood also often interpreted probability data given model. However, strictly correct. First interpretation applies discrete random variables. Second, since samples iid even case one still need add factor accounting multiplicity possible orderings samples obtain correct probability data. Third, interpretation likelihood probability data completely breaks continuous random variables \\(f(x)\\) density, probability.seen previous chapter origin likelihood function\nlies connection relative entropy. Specifically, \nlog-likelihood function\\(l_n(\\boldsymbol \\theta|x_1,\\dots,x_n)=\\sum_{=1}^n \\log f(x_i|\\boldsymbol \\theta)\\)divided sample size \\(n\\) large sample approximation cross-entropy unknown true data generating model approximating model \\(F_{\\boldsymbol \\theta}\\).\nNote log-likelihood additive samples \\(x_i\\).maximum likelihood point estimate \\(\\hat{\\boldsymbol \\theta}^{ML}\\) \ngiven maximising (log)-likelihood\\[\\hat{\\boldsymbol \\theta}^{ML} = \\text{arg max} l_n(\\boldsymbol \\theta|x_1,\\dots,x_n)\\]","code":""},{"path":"maximum-likelihood-estimation.html","id":"obtaining-mles-for-a-regular-model","chapter":"3 Maximum likelihood estimation","heading":"3.1.2 Obtaining MLEs for a regular model","text":"regular situations, .e. whenthe log-likelihood function smooth twice differentiable,second derivative negative zero, \none parameter Hessian matrix negative definite singular,parameters model identifiable (particular model overparameterised), andthe true parameter values lie inside support border,order maximise \\(l_n\\) one may use score function \\(\\boldsymbol S(\\boldsymbol \\theta)\\)\nfirst order derivative log-likelihood function:\\[\\begin{align*}\n\\begin{array}{cc}\nS_n(\\theta) = \\frac{d l_n(\\theta|x_1,\\dots,x_n)}{d \\theta}\\\\\n\\\\\n\\\\\n\\boldsymbol S_n(\\boldsymbol \\theta)=\\nabla l_n(\\boldsymbol \\theta|x_1,\\dots,x_n)\\\\\n\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{scalar parameter: first derivative}\\\\\n\\text{log-likelihood function}\\\\\n\\\\\n\\text{gradient } \\boldsymbol \\theta\\text{ vector}\\\\\n\\text{(.e. one parameter)}\\\\\n\\end{array}\n\\end{align*}\\]necessary (sufficient) condition MLE \n\\[\n\\boldsymbol S_n(\\hat{\\boldsymbol \\theta}_{ML}) = 0\n\\]demonstrate log-likelihood function actually achieves \nmaximum \\(\\hat{\\boldsymbol \\theta}_{ML}\\) curvature\nMLE must negative, .e. log-likelihood must locally concave MLE.case single parameter (scalar \\(\\theta\\)) requires check\nsecond derivative log-likelihood function negative:\n\\[\n\\frac{d^2 l_n(\\hat{\\theta}_{ML})}{d \\theta^2} <0\n\\]\ncase parameter vector (multivariate \\(\\boldsymbol \\theta\\)) need compute\nHessian matrix (matrix second order derivatives)\nMLE:\n\\[\n\\nabla^T\\nabla l_n(\\hat{\\boldsymbol \\theta}_{ML})\n\\]\nverify matrix negative definite (.e. eigenvalues must negative).see later second order derivatives log-likelihood function also play important role assessing uncertainty MLE.","code":""},{"path":"maximum-likelihood-estimation.html","id":"invariance-property-of-the-maximum-likelihood","chapter":"3 Maximum likelihood estimation","heading":"3.1.3 Invariance property of the maximum likelihood","text":"Maximisation procedure invariant coordinate transformations argument. Suppose \\(x_{\\max} = \\text{arg max } h(x)\\) \\(y = g(x)\\)\n\\(g\\) invertible function.\n\\(y_{\\max} = \\text{arg max } h( g^{-1}(y) ) = g(x_{\\max})\\). achieved maximum remains\ninvariant: \\(h( x_{\\max} ) = h(g^{-1}(y_{\\max} ) )\\).regard maximum likelihood estimation implies following invariance property maximum likelihood:Suppose \\(\\hat{\\theta}_{ML}\\) MLE \\(\\theta\\).transform parameter \\(\\theta^{\\star} = g(\\theta)\\)\n\\(g\\) invertible function.\\(g(\\hat{\\theta}_{ML})=\\hat{\\theta}^{\\star}\\) MLE \\(\\theta^{\\star}\\).value achieved maximum likelihood \ncases, .e. invariant transformation parameters.invariance property can useful practise may easier perform maximisation required finding MLE particular coordinate system.See Worksheet 2 example application invariance principle.","code":""},{"path":"maximum-likelihood-estimation.html","id":"consistency-of-maximum-likelihood-estimates","chapter":"3 Maximum likelihood estimation","heading":"3.1.4 Consistency of maximum likelihood estimates","text":"One important property maximum likelihood produces consistent estimates.Specifically, true underlying model \\(F_{\\text{true}}\\) parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) contained set specified candidates models \\(F_{\\boldsymbol \\theta}\\)\n\\[\\underbrace{F_{\\text{true}}}_{\\text{true model}} \\subset \\underbrace{F_{\\boldsymbol \\theta}}_{\\text{specified models}}\\] \\[\\hat{\\boldsymbol \\theta}_{ML} \\overset{\\text{large }n}{\\longrightarrow} \\boldsymbol \\theta_{\\text{true}}\\]consequence \\(D_{\\text{KL}}(F_{\\text{true}},F_{\\boldsymbol \\theta})\\rightarrow 0\\) \\(F_{\\boldsymbol \\theta} \\rightarrow F_{\\text{true}}\\), maximisation likelihood function large \\(n\\) equivalent minimising relative entropy.Thus given sufficient data MLE converge true value. consequence, MLEs asympotically unbiased. see examples can still biased finite samples.Note even candidate model \\(F_{\\boldsymbol \\theta}\\) misspecified (.e. contain actual true model) MLE still optimal sense find closest possible model.possible find inconsistent MLEs, occurs situations dimension model / number parameters increases sample size, MLE boundary singularities likelihood function.","code":""},{"path":"maximum-likelihood-estimation.html","id":"maximum-likelihood-estimation-in-practise","chapter":"3 Maximum likelihood estimation","heading":"3.2 Maximum likelihood estimation in practise","text":"","code":""},{"path":"maximum-likelihood-estimation.html","id":"worked-examples","chapter":"3 Maximum likelihood estimation","heading":"3.2.1 Worked examples","text":"section now provide number worked example ML estimation\nworks practise.Example 3.1  Estimation proportion:aim estimate true proportion \\(p\\) Bernoulli experiment binary\noutcomes, say proportion “successes” vs. “failures” “heads” vs. “tails” coin tossing experiment.Bernoulli model \\(\\text{Ber}(p)\\): \\(\\text{Pr}(\\text{\"success\"}) = p\\) \\(\\text{Pr}(\\text{\"failure\"}) = 1-p\\).“success” indicated outcome \\(x=1\\) “failure” \\(x=0\\).conduct \\(n\\) trials record \\(n_1\\) successes \\(n-n_1\\) failures.Parameter: \\(p\\): probability “success”.MLE \\(p\\)?data \\(x_1, \\ldots, x_n\\) take values 0 1.data \\(x_1, \\ldots, x_n\\) take values 0 1.average data points \\(\\bar{x} = \\frac{1}{n} \\sum_{=1}^n x_i = \\frac{n_1}{n}\\).average data points \\(\\bar{x} = \\frac{1}{n} \\sum_{=1}^n x_i = \\frac{n_1}{n}\\).probability mass function (PMF) Bernoulli distribution \\(\\text{Ber}(p)\\) :\n\\[\nf(x) = p^x (1-p)^{1-x} = \n\\begin{cases}\np &  \\text{$x=1$ }\\\\\n1-p & \\text{$x=0$} \\\\ \n\\end{cases}\n\\]probability mass function (PMF) Bernoulli distribution \\(\\text{Ber}(p)\\) :\n\\[\nf(x) = p^x (1-p)^{1-x} = \n\\begin{cases}\np &  \\text{$x=1$ }\\\\\n1-p & \\text{$x=0$} \\\\ \n\\end{cases}\n\\]log-PMF:\n\\[\n\\log f(x) =  x \\log(p) + (1-x) \\log(1 - p)\n\\]log-PMF:\n\\[\n\\log f(x) =  x \\log(p) + (1-x) \\log(1 - p)\n\\]log-likelihood function:\n\\[\n\\begin{split}\nl_n(p) & = \\sum_{=1}^n \\log f(x_i) \\\\\n    & = n_1 \\log p + (n-n_1) \\log(1-p) \\\\\n    & = n \\left( \\bar{x} \\log p + (1-\\bar{x}) \\log(1-p) \\right) \\\\\n\\end{split}\n\\]\nNote log-likelihood depends data \\(\\bar{x}\\)! \nexample sufficient statistic parameter \\(p\\) (fact also minimally sufficient statistic). discussed detail later.log-likelihood function:\n\\[\n\\begin{split}\nl_n(p) & = \\sum_{=1}^n \\log f(x_i) \\\\\n    & = n_1 \\log p + (n-n_1) \\log(1-p) \\\\\n    & = n \\left( \\bar{x} \\log p + (1-\\bar{x}) \\log(1-p) \\right) \\\\\n\\end{split}\n\\]\nNote log-likelihood depends data \\(\\bar{x}\\)! \nexample sufficient statistic parameter \\(p\\) (fact also minimally sufficient statistic). discussed detail later.Score function:\n\\[\nS_n(p)=  \\frac{dl_n(p)}{dp}= n \\left( \\frac{\\bar{x}}{p}-\\frac{1-\\bar{x}}{1-p} \\right)\n\\]Score function:\n\\[\nS_n(p)=  \\frac{dl_n(p)}{dp}= n \\left( \\frac{\\bar{x}}{p}-\\frac{1-\\bar{x}}{1-p} \\right)\n\\]Maximum likelihood estimate: Setting \\(S_n(\\hat{p}_{ML})=0\\) yields solution\n\\[\n\\hat{p}_{ML} = \\bar{x} = \\frac{n_1}{n}\n\\]\n\\(\\frac{dS_n(p)}{dp} = -n \\left( \\frac{\\bar{x}}{p^2} + \\frac{1-\\bar{x}}{(1-p)^2} \\right) <0\\) optimum corresponds indeed maximum (log-)likelihood function negative \\(\\hat{p}_{ML}\\) (indeed \\(p\\)).\nmaximum likelihood estimator \\(p\\) therefore identical frequency\nsuccesses among observations.Maximum likelihood estimate: Setting \\(S_n(\\hat{p}_{ML})=0\\) yields solution\n\\[\n\\hat{p}_{ML} = \\bar{x} = \\frac{n_1}{n}\n\\]\\(\\frac{dS_n(p)}{dp} = -n \\left( \\frac{\\bar{x}}{p^2} + \\frac{1-\\bar{x}}{(1-p)^2} \\right) <0\\) optimum corresponds indeed maximum (log-)likelihood function negative \\(\\hat{p}_{ML}\\) (indeed \\(p\\)).maximum likelihood estimator \\(p\\) therefore identical frequency\nsuccesses among observations.Note analyse coin tossing experiment estimate \\(p\\) may equally well use Binomial distribution \\(\\text{Bin}(n, p)\\) model number successes. case single observation, namely observed \\(k\\) . results MLE \\(p\\) likelihood function based Binomial PMF includes Binomial coefficient \\(\\binom{n}{k}\\) . However, factor depend \\(p\\) disappears score function influence derivation MLE.Example 3.2  Normal distribution unknown mean known variance:\\(x \\sim N(\\mu,\\sigma^2)\\) \\(\\text{E}(x)=\\mu\\) \\(\\text{Var}(x) = \\sigma^2\\)parameter estimated \\(\\mu\\) whereas \\(\\sigma^2\\) known.’s MLE parameter \\(\\mu\\)?data \\(x_1, \\ldots, x_n \\[-\\infty, \\infty]\\) real values.average \\(\\bar{x} = \\frac{1}{n} \\sum_{=1}^n x_i\\) real well.Density: \\[ f(x)=\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]Log-Density:\n\\[\\log f(x) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\]Log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\mu) &= \\sum_{=1}^n \\log f(x_i)\\\\\n&=-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2}\\log(2 \\pi \\sigma^2) }_{\\text{constant term, depend } \\mu \\text{, can removed}}\\\\\n&=-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i^2 - 2 x_i \\mu+\\mu^2)  + C\\\\\n&=\\frac{n}{\\sigma^2}  ( \\bar{x} \\mu  - \\frac{1}{2}\\mu^2)  \\underbrace{ - \\frac{1}{2\\sigma^2}\\sum_{=1}^n   x_i^2 }_{\\text{another constant term}}   + C\\\\\n\\end{split}\n\\]\nNote non-constant terms log-likelihood depend data \\(\\bar{x}\\)!Log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\mu) &= \\sum_{=1}^n \\log f(x_i)\\\\\n&=-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2}\\log(2 \\pi \\sigma^2) }_{\\text{constant term, depend } \\mu \\text{, can removed}}\\\\\n&=-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i^2 - 2 x_i \\mu+\\mu^2)  + C\\\\\n&=\\frac{n}{\\sigma^2}  ( \\bar{x} \\mu  - \\frac{1}{2}\\mu^2)  \\underbrace{ - \\frac{1}{2\\sigma^2}\\sum_{=1}^n   x_i^2 }_{\\text{another constant term}}   + C\\\\\n\\end{split}\n\\]\nNote non-constant terms log-likelihood depend data \\(\\bar{x}\\)!Score function:\n\\[\nS_n(\\mu) = \n\\frac{n}{\\sigma^2} ( \\bar{x}- \\mu)\n\\]Maximum likelihood estimate:\n\\[S_n(\\hat{\\mu}_{ML})=0 \\Rightarrow \\hat{\\mu}_{ML} = \\bar{x}\\]\\(\\frac{dS_n(\\mu)}{d\\mu} = -\\frac{n}{\\sigma^2}<0\\) optimum indeed maximumWith \\(\\frac{dS_n(\\mu)}{d\\mu} = -\\frac{n}{\\sigma^2}<0\\) optimum indeed maximumThe constant term \\(C\\) log-likelihood function collects terms depend parameter. taking first derivative regard parameter term disappears thus \\(C\\) relevant finding MLE parameter.\nfuture often omit constant terms log-likelihood function without mention.Example 3.3  Normal distribution mean variance unknown:\\(x \\sim N(\\mu,\\sigma^2)\\) \\(\\text{E}(x)=\\mu\\) \\(\\text{Var}(x) = \\sigma^2\\)\\(\\mu\\) \\(\\sigma^2\\) need estimated.’s MLE parameter vector \\(\\boldsymbol \\theta= (\\mu,\\sigma^2)^T\\)?data \\(x_1, \\ldots, x_n \\[-\\infty, \\infty]\\) real values.average \\(\\bar{x} = \\frac{1}{n} \\sum_{=1}^n x_i\\) real well.average squared data \\(\\overline{x^2} = \\frac{1}{n} \\sum_{=1}^n x_i^2 \\geq 0\\) non-negative.average squared data \\(\\overline{x^2} = \\frac{1}{n} \\sum_{=1}^n x_i^2 \\geq 0\\) non-negative.Density: \\[ f(x)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]Log-Density:\n\\[\\log f(x) =-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\]Log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\boldsymbol \\theta) & = \\sum_{=1}^n \\log f(x_i)\\\\\n &= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2} \\log(2 \\pi) }_{\\text{constant depending }\\mu \\text{ } \\sigma^2}\\\\\n &= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{n}{2\\sigma^2}  ( \\overline{x^2} -2 \\bar{x} \\mu + \\mu^2)  + C\\\\\n\\end{split}\n\\]\nNote log-likelihood function depends data \\(\\bar{x}\\)\n\\(\\overline{x^2}\\)!Log-likelihood function:\n\\[\n\\begin{split}\nl_n(\\boldsymbol \\theta) & = \\sum_{=1}^n \\log f(x_i)\\\\\n &= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2  \\underbrace{-\\frac{n}{2} \\log(2 \\pi) }_{\\text{constant depending }\\mu \\text{ } \\sigma^2}\\\\\n &= -\\frac{n}{2}\\log(\\sigma^2)-\\frac{n}{2\\sigma^2}  ( \\overline{x^2} -2 \\bar{x} \\mu + \\mu^2)  + C\\\\\n\\end{split}\n\\]\nNote log-likelihood function depends data \\(\\bar{x}\\)\n\\(\\overline{x^2}\\)!Score function \\(\\boldsymbol S\\) (row vector!), gradient \\(l_n(\\boldsymbol \\theta)\\):\n\\[\n\\begin{split}\n\\boldsymbol S(\\boldsymbol \\theta) &= \\nabla l_n(\\boldsymbol \\theta) \\\\\n&=\n\\begin{pmatrix}\n\\frac{n}{\\sigma^2} (\\bar{x}-\\mu) \\\\\n-\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4}   \\left( \\overline{x^2} - 2\\bar{x} \\mu +\\mu^2 \\right)  \\\\\n\\end{pmatrix}^T\\\\\n\\end{split}\n\\]\nNote obtain second component score function partial derivative needs taken regard variance parameter \\(\\sigma^2\\) — regard \\(\\sigma\\)! Hint: replace \\(\\sigma^2 = v\\) log-likelihood function, take partial derivative regard \\(v\\), backsubstitute \\(v=\\sigma^2\\) result.Score function \\(\\boldsymbol S\\) (row vector!), gradient \\(l_n(\\boldsymbol \\theta)\\):\n\\[\n\\begin{split}\n\\boldsymbol S(\\boldsymbol \\theta) &= \\nabla l_n(\\boldsymbol \\theta) \\\\\n&=\n\\begin{pmatrix}\n\\frac{n}{\\sigma^2} (\\bar{x}-\\mu) \\\\\n-\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4}   \\left( \\overline{x^2} - 2\\bar{x} \\mu +\\mu^2 \\right)  \\\\\n\\end{pmatrix}^T\\\\\n\\end{split}\n\\]Note obtain second component score function partial derivative needs taken regard variance parameter \\(\\sigma^2\\) — regard \\(\\sigma\\)! Hint: replace \\(\\sigma^2 = v\\) log-likelihood function, take partial derivative regard \\(v\\), backsubstitute \\(v=\\sigma^2\\) result.Maximum likelihood estimate:\n\\[\n\\boldsymbol S(\\hat{\\boldsymbol \\theta}_{ML})=0 \\Rightarrow \n\\]\n\\[\n\\hat{\\boldsymbol \\theta}_{ML}=\n\\begin{pmatrix}\n \\hat{\\mu}_{ML}  \\\\\n \\widehat{\\sigma^2}_{ML} \\\\\n\\end{pmatrix}\n =\n\\begin{pmatrix}\n\\bar{x} \\\\\n\\overline{x^2} -\\bar{x}^2\\\\\n\\end{pmatrix}\n\\]\nML estimate variance can also write\n\\(\\widehat{\\sigma^2}_{ML} = \\overline{x^2} -\\bar{x}^2 =  \\frac{1}{n}\\sum_{=1}^n (x_i-\\bar{x})^2\\).Maximum likelihood estimate:\n\\[\n\\boldsymbol S(\\hat{\\boldsymbol \\theta}_{ML})=0 \\Rightarrow \n\\]\n\\[\n\\hat{\\boldsymbol \\theta}_{ML}=\n\\begin{pmatrix}\n \\hat{\\mu}_{ML}  \\\\\n \\widehat{\\sigma^2}_{ML} \\\\\n\\end{pmatrix}\n =\n\\begin{pmatrix}\n\\bar{x} \\\\\n\\overline{x^2} -\\bar{x}^2\\\\\n\\end{pmatrix}\n\\]\nML estimate variance can also write\n\\(\\widehat{\\sigma^2}_{ML} = \\overline{x^2} -\\bar{x}^2 =  \\frac{1}{n}\\sum_{=1}^n (x_i-\\bar{x})^2\\).confirm actually maximum need verify eigenvalues\nHessian matrix negative. indeed case, \ndetails see Example 3.6.confirm actually maximum need verify eigenvalues\nHessian matrix negative. indeed case, \ndetails see Example 3.6.","code":""},{"path":"maximum-likelihood-estimation.html","id":"relationship-with-least-squares-estimation","chapter":"3 Maximum likelihood estimation","heading":"3.2.2 Relationship with least squares estimation","text":"Example 3.2\nform log-likelihood function\nfunction sum squared differences. Maximising \\(l_n(\\mu) =-\\frac{1}{2\\sigma^2}\\sum_{=1}^n(x_i-\\mu)^2\\) equivalent minimising \\(\\sum_{=1}^n(x_i-\\mu)^2\\). Hence, finding mean maximum likelihood assuming normal model equivalent least-squares estimation!Note least-squares estimation use least since early 1800s thus predates maximum likelihood (1924). Due simplicity still popular particular regression link maximum likelihood normality allows understand usually works well!","code":""},{"path":"maximum-likelihood-estimation.html","id":"bias-and-maximum-likelihood","chapter":"3 Maximum likelihood estimation","heading":"3.2.3 Bias and maximum likelihood","text":"Example 3.3 interesting shows maximum likelihood can result biased well unbiased estimators.Recall \\(x \\sim N(\\mu, \\sigma^2)\\). result\n\\[\\hat{\\mu}_{ML}=\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\n\\(\\text{E}( \\hat{\\mu}_{ML} ) = \\mu\\)\n\n\\[\\widehat{\\sigma^2}_{ML} \\sim \\frac{\\sigma^2}{n} \\chi^2_{n-1}\\]\n\\(\\text{E}(\\widehat{\\sigma^2}_{ML}) = \\frac{n-1}{n} \\, \\sigma^2\\).Therefore, MLE \\(\\mu\\) unbiased \\[\n\\text{Bias}(\\hat{\\mu}_{ML}) = \\text{E}( \\hat{\\mu}_{ML} ) - \\mu = 0\n\\]\ncontrast, however, MLE \\(\\sigma^2\\) negatively biased \n\\[\n\\text{Bias}(\\widehat{\\sigma^2}_{ML}) = \\text{E}( \\widehat{\\sigma^2}_{ML} ) - \\sigma^2 = -\\frac{1}{n} \\, \\sigma^2\n\\]Thus, case variance parameter normal distribution MLE recovering well-known unbiased estimator variance\\[\n\\widehat{\\sigma^2}_{UB} = \\frac{1}{n-1}\\sum_{=1}^n(x_i-\\bar{x})^2 = \\frac{n}{n-1} \\widehat{\\sigma^2}_{ML}\n\\]\nConversely, unbiased estimator maximum likelihood estimate!Therefore worth keeping mind maximum likelihood can result biased estimates finite \\(n\\).\nlarge \\(n\\), however, bias disappears MLEs consistent.","code":""},{"path":"maximum-likelihood-estimation.html","id":"observed-fisher-information","chapter":"3 Maximum likelihood estimation","heading":"3.3 Observed Fisher information","text":"","code":""},{"path":"maximum-likelihood-estimation.html","id":"motivation-and-definition","chapter":"3 Maximum likelihood estimation","heading":"3.3.1 Motivation and definition","text":"inspection log-likelihood curves apparent log-likelihood function contains information parameter \\(\\boldsymbol \\theta\\) just maximum point \\(\\hat{\\boldsymbol \\theta}_{ML}\\).particular curvature log-likelihood function MLE must somehow related accuracy \\(\\hat{\\boldsymbol \\theta}_{ML}\\): likelihood surface flat near maximum\n(low curvature) difficult find optimal parameter (also numerically!). Conversely, likelihood surface peaked (strong curvature) maximum point clearly defined.curvature described second-order derivatives (Hessian matrix) log-likelihood function.univariate \\(\\theta\\) Hessian scalar:\n\\[\\frac{d^2 l_n(\\theta)}{d\\theta^2}\\]multivariate parameter vector \\(\\boldsymbol \\theta\\) dimension \\(d\\) Hessian matrix size \\(d \\times d\\):\n\\[\\nabla^T\\nabla l_n(\\boldsymbol \\theta)\\]construction Hessian negative definite MLE (.e. eigenvalues negative) ensure function concave MLE (.e. peak shaped).observed Fisher information (matrix) defined \nnegative curvature MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\):\n\\[{\\boldsymbol J_n}(\\hat{\\boldsymbol \\theta}_{ML}) = -\\nabla^T\\nabla l_n(\\hat{\\boldsymbol \\theta}_{ML})\\]Sometimes simply called “observed information”.\navoid confusion expected Fisher information introduced earlier\\[\n\\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta) = -\\text{E}_{F_{\\boldsymbol \\theta}} \\left( \\nabla^T\\nabla\\log f(x|\\boldsymbol \\theta)\\right)\n\\]\nnecessary always use qualifier “observed” referring \\({\\boldsymbol J_n}(\\hat{\\boldsymbol \\theta}_{ML})\\).","code":""},{"path":"maximum-likelihood-estimation.html","id":"examples-of-observed-fisher-information","chapter":"3 Maximum likelihood estimation","heading":"3.3.2 Examples of observed Fisher information","text":"Example 3.4  Bernoulli model \\(\\text{Ber}(p)\\):continue Example 3.1. Recall \n\\(\\hat{p}_{ML} = \\bar{x}=\\frac{n_1}{n}\\) score function\n\\(S_n(p)=n \\left( \\frac{\\bar{x} }{p} - \\frac{1-\\bar{x}}{1-p} \\right)\\). negative second derivative log-likelihood function \n\\[-\\frac{d S_n(p)}{dp}=n \\left( \\frac{ \\bar{x} }{p^2} + \\frac{1 - \\bar{x} }{(1-p)^2} \\right) \\]\nobserved Fisher information therefore\n\\[\n\\begin{split}\nJ_n(\\hat{p}_{ML}) & = n \\left(\\frac{ \\bar{x} }{\\hat{p}_{ML}^2} + \\frac{ 1 - \\bar{x} }{  (1-\\hat{p}_{ML})^2  } \\right) \\\\\n  & = n \\left(\\frac{1}{\\hat{p}_{ML}} + \\frac{1}{1-\\hat{p}_{ML}} \\right) \\\\\n  &= \\frac{n}{\\hat{p}_{ML} (1-\\hat{p}_{ML})} \\\\\n\\end{split}\n\\]inverse observed Fisher information :\n\\[J_n(\\hat{p}_{ML})^{-1}=\\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\\]Compare \\(\\text{Var}\\left(\\frac{x}{n}\\right) = \\frac{p(1-p)}{n}\\) \n\\(x \\sim \\text{Bin}(n, p)\\).Example 3.5  Normal distribution unknown mean known variance:continuation Example 3.2.\nRecall MLE mean\n\\(\\hat{\\mu}_{ML}=\\frac{1}{n}\\sum_{=1}^n x_i=\\bar{x}\\)\nscore function\n\\(\\boldsymbol S_n(\\mu) = \\frac{n}{\\sigma^2} (\\bar{x} -\\mu)\\).\nnegative second derivative score function \n\\[\n-\\frac{d S_n(\\mu)}{d\\mu}= \\frac{n}{\\sigma^2} \n\\]\nobserved Fisher information MLE therefore\n\\[\nJ_n(\\hat{\\mu}_{ML}) = \\frac{n}{\\sigma^2} \n\\]\ninverse observed Fisher information \n\\[\nJ_n(\\hat{\\nu}_{ML})^{-1} = \\frac{\\sigma^2}{n}\n\\]\\(x_i \\sim N(\\mu, \\sigma^2)\\) \\(\\text{Var}(x_i) = \\sigma^2\\)\nhence \\(\\text{Var}(\\bar{x}) = \\frac{\\sigma^2}{n}\\),\nequal inverse observed Fisher information.Example 3.6  Normal distribution mean variance parameter:continuation Example 3.3.\nRecall MLE mean variance:\n\\[\\hat{\\mu}_{ML}=\\frac{1}{n}\\sum_{=1}^n x_i=\\bar{x}\\]\n\\[\\widehat{\\sigma^2}_{ML} = \\frac{1}{n}\\sum_{=1}^n(x_i-\\bar{x})^2 =  \\overline{x^2} - \\bar{x}^2\\]\nscore function\n\\[\\boldsymbol S_n(\\mu,\\sigma^2)=\\nabla l_n(\\mu, \\sigma^2) = \n\\begin{pmatrix}\n\\frac{n}{\\sigma^2}   (\\bar{x}-\\mu) \\\\\n-\\frac{n}{2\\sigma^2}+\\frac{n}{2\\sigma^4} \\left(\\overline{x^2} - 2 \\mu \\bar{x} + \\mu^2\\right) \\\\\n\\end{pmatrix}^T\n\\]\nHessian matrix log-likelihood function \n\\[\\nabla^T\\nabla l_n(\\mu,\\sigma^2) =\n \\begin{pmatrix}\n    - \\frac{n}{\\sigma^2}&  -\\frac{n}{\\sigma^4} (\\bar{x} -\\mu)\\\\\n    - \\frac{n}{\\sigma^4} (\\bar{x} -\\mu) & \\frac{n}{2\\sigma^4}-\\frac{n}{\\sigma^6} \\left(\\overline{x^2} - 2 \\mu \\bar{x} + \\mu^2\\right) \\\\\n    \\end{pmatrix}\n\\]\nnegative Hessian MLE, .e. \\(\\hat{\\mu}_{ML} = \\bar{x}\\)\n\\(\\widehat{\\sigma^2}_{ML} = \\overline{x^2} -\\bar{x}^2\\)\nyields observed Fisher information matrix:\n\\[\n\\boldsymbol J_n(\\hat{\\mu}_{ML},\\widehat{\\sigma^2}_{ML}) = \\begin{pmatrix}\n    \\frac{n}{\\widehat{\\sigma^2}_{ML}}&0 \\\\\n    0 & \\frac{n}{2(\\widehat{\\sigma^2}_{ML})^2}\n    \\end{pmatrix}\n\\]\nNote observed Fisher information matrix diagonal\npositive entries. Therefore \neigenvalues positive required maximum, diagonal matrix eigenvalues simply \nentries diagonal.inverse observed Fisher information matrix \n\\[\n\\boldsymbol J_n(\\hat{\\mu}_{ML},\\widehat{\\sigma^2}_{ML})^{-1} = \\begin{pmatrix}\n    \\frac{\\widehat{\\sigma^2}_{ML}}{n}& 0\\\\\n    0 & \\frac{2(\\widehat{\\sigma^2}_{ML})^2}{n}\n    \\end{pmatrix}\n\\]Recall \\(x \\sim N(\\mu, \\sigma^2)\\) therefore\n\\[\\hat{\\mu}_{ML}=\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\nHence \\(\\text{Var}(\\hat{\\mu}_{ML}) = \\frac{\\sigma^2}{n}\\). compare \n\nfirst diagonal entry inverse observed Fisher information matrix see essentially expression (apart “hat”).empirical variance \\(\\widehat{\\sigma^2}_{ML}\\) follows scaled\nchi-squared distribution\n\\[\\widehat{\\sigma^2}_{ML} \\sim \\frac{\\sigma^2}{n} \\chi^2_{n-1}\\] \nvariance\n\\(\\text{Var}(\\widehat{\\sigma^2}_{ML}) = \\frac{n-1}{n} \\, \\frac{2 \\sigma ^4}{n}\\). large \\(n\\) becomes \\(\\text{Var}(\\widehat{\\sigma^2}_{ML})\\overset{}{=} \\frac{2 \\sigma ^4}{n}\\) essentially (apart “hat”) second diagonal entry inverse observed Fisher information matrix.","code":""},{"path":"maximum-likelihood-estimation.html","id":"relationship-between-observed-and-expected-fisher-information","chapter":"3 Maximum likelihood estimation","heading":"3.3.3 Relationship between observed and expected Fisher information","text":"observed Fisher information \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) expected Fisher information\n\\(\\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)\\) related also two clearly different entities:types Fisher information based computing second order derivative\n(Hessian matrix), thus based curvature function.types Fisher information based computing second order derivative\n(Hessian matrix), thus based curvature function.observed Fisher information computed log-likelihood function.\nTherefore takes observed data account. explicitly depends sample size \\(n\\). contains estimates parameters parameters . curvature log-likelihood function may computed point observed Fisher information specifically refers MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\). linked (asymptotic) variance MLE seen examples discuss detail later.observed Fisher information computed log-likelihood function.\nTherefore takes observed data account. explicitly depends sample size \\(n\\). contains estimates parameters parameters . curvature log-likelihood function may computed point observed Fisher information specifically refers MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\). linked (asymptotic) variance MLE seen examples discuss detail later.contrast, expected Fisher information derived directly log-density. depend observed data, thus dependency sample size. can computed value parameters. describes geometry space models, local approximation relative entropy.contrast, expected Fisher information derived directly log-density. depend observed data, thus dependency sample size. can computed value parameters. describes geometry space models, local approximation relative entropy.Asympotically, large sample size \\(n\\) MLE converges \\(\\hat{\\boldsymbol \\theta}_{ML} \\rightarrow \\boldsymbol \\theta_0\\).\nfollows construction \nobserved Fisher information law large numbers correspondingly \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\rightarrow n \\boldsymbol ^{\\text{Fisher}}( \\boldsymbol \\theta_0 )\\).Asympotically, large sample size \\(n\\) MLE converges \\(\\hat{\\boldsymbol \\theta}_{ML} \\rightarrow \\boldsymbol \\theta_0\\).\nfollows construction \nobserved Fisher information law large numbers correspondingly \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\rightarrow n \\boldsymbol ^{\\text{Fisher}}( \\boldsymbol \\theta_0 )\\).important class models, namely exponential family, find \n\\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) = n \\boldsymbol ^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) also finite sample size \\(n\\). fact case examples discussed (e.g. see\nExamples 2.11 3.4\nBernoulli \nExamples 2.13 3.6\nnormal distribution).important class models, namely exponential family, find \n\\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) = n \\boldsymbol ^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) also finite sample size \\(n\\). fact case examples discussed (e.g. see\nExamples 2.11 3.4\nBernoulli \nExamples 2.13 3.6\nnormal distribution).However, exception. general model \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\neq n \\boldsymbol ^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\)\nfinite sample size \\(n\\). example provided Cauchy distribution median parameter \\(\\theta\\). part exponential family expected Fisher information \\(^{\\text{Fisher}}(\\theta )=\\frac{1}{2}\\) regardless choice\nmedian parameter whereas observed Fisher information \\(J_n(\\hat{\\theta}_{ML})\\) depends \nMLE \\(\\hat{\\theta}_{ML}\\) median parameter simply \\(\\frac{n}{2}\\).However, exception. general model \\(\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \\neq n \\boldsymbol ^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\)\nfinite sample size \\(n\\). example provided Cauchy distribution median parameter \\(\\theta\\). part exponential family expected Fisher information \\(^{\\text{Fisher}}(\\theta )=\\frac{1}{2}\\) regardless choice\nmedian parameter whereas observed Fisher information \\(J_n(\\hat{\\theta}_{ML})\\) depends \nMLE \\(\\hat{\\theta}_{ML}\\) median parameter simply \\(\\frac{n}{2}\\).","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"quadratic-approximation-and-normal-asymptotics","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4 Quadratic approximation and normal asymptotics","text":"","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"multivariate-statistics-for-random-vectors","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.1 Multivariate statistics for random vectors","text":"","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"covariance-and-correlation","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.1.1 Covariance and correlation","text":"Assume scalar random variable \\(x\\) mean \\(\\text{E}(x) = \\mu\\). corresponding variance given \n\\[\n\\begin{split}\n\\text{Var}(x) & = \\text{E}\\left((x-\\mu)^2 \\right) \\\\\n        & =\\text{E}\\left( (x-\\mu)(x-\\mu) \\right) \\\\\n        & = \\text{E}(x^2)-\\mu^2 \\\\\n\\end{split}\n\\]random vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T\\) mean \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\) simply comprised means components, .e. \\(\\boldsymbol \\mu= (\\mu_1, \\ldots, \\mu_d)^T\\). Thus, mean random vector dimension vector length.variance random vector length \\(d\\), however, vector matrix size \\(d\\times d\\). matrix called covariance matrix:\n\\[\n\\begin{split}\n\\text{Var}(\\boldsymbol x) &= \\underbrace{\\boldsymbol \\Sigma}_{d\\times d} = (\\sigma_{ij}) = \\begin{pmatrix}\n    \\sigma_{11} & \\dots & \\sigma_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{d1} & \\dots & \\sigma_{dd}\n\\end{pmatrix} \\\\\n  &=\\text{E}\\left(\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d\\times 1} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1\\times d}\\right) \\\\\n  & = \\text{E}(\\boldsymbol x\\boldsymbol x^T)-\\boldsymbol \\mu\\boldsymbol \\mu^T \\\\\n\\end{split}\n\\]\nentries covariance matrix \\(\\sigma_{ij} =\\text{Cov}(x_i, x_j)\\) describe covariance random variables \\(x_i\\) \\(x_j\\). covariance matrix symmetric, hence \\(\\sigma_{ij}=\\sigma_{ji}\\). diagonal entries \\(\\sigma_{ii} = \\text{Cov}(x_i, x_i) = \\text{Var}(x_i) = \\sigma_i^2\\) correspond variances components \\(\\boldsymbol x\\). covariance matrix positive semi-definite, .e. eigenvalues \\(\\boldsymbol \\Sigma\\) positive equal zero. However, practise one aims use non-singular covariance matrices, eigenvalues positive, invertible.covariance matrix can factorised product\n\\[\n\\boldsymbol \\Sigma= \\boldsymbol V^{\\frac{1}{2}} \\boldsymbol P\\boldsymbol V^{\\frac{1}{2}}\n\\]\n\\(\\boldsymbol V\\) diagonal matrix containing variances\n\\[\n\\boldsymbol V= \\begin{pmatrix}\n    \\sigma_{11} & \\dots & 0\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & \\sigma_{dd}\n\\end{pmatrix}\n\\]\nmatrix \\(\\boldsymbol P\\) (“capital rho”) symmetric correlation matrix\n\\[\n\\boldsymbol P= (\\rho_{ij}) = \\begin{pmatrix}\n    1 & \\dots & \\rho_{1d}\\\\\n     \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{d1} & \\dots & 1\n\\end{pmatrix}   = \\boldsymbol V^{-\\frac{1}{2}} \\boldsymbol \\Sigma\\boldsymbol V^{-\\frac{1}{2}}\n\\]\nThus, correlation \\(x_i\\) \\(x_j\\) defined \n\\[\n\\rho_{ij} = \\text{Cor}(x_i,x_j) = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}\n\\]univariate \\(x\\) scalar constant \\(\\) variance \\(x\\) equals \\(\\text{Var}(x) = ^2 \\text{Var}(x)\\). random vector \\(\\boldsymbol x\\) dimension \\(d\\) matrix \\(\\boldsymbol \\) dimension \\(m \\times d\\) generalises \\(\\text{Var}(\\boldsymbol Ax) = \\boldsymbol \\text{Var}(\\boldsymbol x) \\boldsymbol ^T\\).","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"multivariate-normal-distribution","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.1.2 Multivariate normal distribution","text":"density normally distributed scalar variable \\(x \\sim N(\\mu, \\sigma^2)\\) mean \\(\\text{E}(x) = \\mu\\) variance \\(\\text{Var}(x) = \\sigma^2\\) \n\\[\nf(x |\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)\n\\]univariate normal distribution scalar \\(x\\) generalises multivariate normal distribution vector \\(\\boldsymbol x= (x_1, x_2,...,x_d)^T \\sim N_d(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\) mean \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu\\) covariance matrix \\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma\\). corresponding density \n\\[\nf(\\boldsymbol x| \\boldsymbol \\mu, \\boldsymbol \\Sigma) = (2\\pi)^{-\\frac{d}{2}} \\det(\\boldsymbol \\Sigma)^{-\\frac{1}{2}} \\exp\\left({{-\\frac{1}{2}} \\underbrace{\\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)^T}_{1 \\times d} \\underbrace{\\boldsymbol \\Sigma^{-1}}_{d \\times d} \\underbrace{(\\boldsymbol x-\\boldsymbol \\mu)}_{d \\times 1} }_{1 \\times 1 = \\text{scalar!}}}\\right)\n\\]\\(d=1\\) \\(bx=x\\), \\(\\boldsymbol \\mu= \\mu\\) \\(\\boldsymbol \\Sigma= \\sigma^2\\) multivariate normal density reduces univariate normal density.Example 4.1  Maximum likelihood estimates parameters multivariate normal distribution:Maximising log-likelihood based multivariate normal density yields \nMLEs \\(\\boldsymbol \\mu\\) \\(\\boldsymbol \\Sigma\\). generalisations MLEs mean \\(\\mu\\)\nvariance \\(\\sigma^2\\) univariate normal encountered Example 3.3.estimates can written three different ways:) data vector notationwith \\(\\boldsymbol x_1,\\ldots, \\boldsymbol x_n\\) \\(n\\) vector-valued observations multivariate normal:MLE mean:\n\\[\n\\hat{\\boldsymbol \\mu}_{ML} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k = \\bar{\\boldsymbol x}\n\\]MLE covariance:\n\\[\\underbrace{\\widehat{\\boldsymbol \\Sigma}_{ML}}_{d \\times d} = \\frac{1}{n}\\sum^{n}_{k=1} \\underbrace{\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)}_{d \\times 1} \\; \\underbrace{\\left(\\boldsymbol x_k-\\bar{\\boldsymbol x}\\right)^T}_{1 \\times d}\\]\nNote factor \\(\\frac{1}{n}\\) estimator covariance matrix.\\(\\overline{\\boldsymbol x\\boldsymbol x^T} = \\frac{1}{n}\\sum^{n}_{k=1} \\boldsymbol x_k \\boldsymbol x_k^T\\)\ncan also write\n\\[\n\\widehat{\\boldsymbol \\Sigma}_{ML} = \\overline{\\boldsymbol x\\boldsymbol x^T} - \\bar{\\boldsymbol x} \\bar{\\boldsymbol x}^T\n\\]b) data component notationwith \\(x_{ki}\\) \\(\\)-th component \\(k\\)-th sample \\(\\boldsymbol x_k\\):\\[\\hat{\\mu}_i = \\frac{1}{n}\\sum^{n}_{k=1} x_{ki} \\text{ } \n\\hat{\\boldsymbol \\mu}=\\begin{pmatrix}\n    \\hat{\\mu}_{1}       \\\\\n    \\vdots \\\\\n    \\hat{\\mu}_{d}\n\\end{pmatrix}\\]\\[\\hat{\\sigma}_{ij} = \\frac{1}{n}\\sum^{n}_{k=1} \\left(x_{ki}-\\hat{\\mu}_i\\right) (\\\nx_{kj}-\\hat{\\mu}_j) \\text{ } \\widehat{\\boldsymbol \\Sigma} = (\\hat{\\sigma}_{ij})\n\\]c) data matrix notationwith \\(\\boldsymbol X= \\begin{pmatrix} \\boldsymbol x_1^T \\\\ ... \\\\ \\boldsymbol x_n^T \\\\\\end{pmatrix}\\) data matrix containing samples rows. Note statistics convention — much engineering computer science literature data matrix often transposed samples stored columns. Thus, formulas correct assuming statistics convention.\\[\n\\hat{\\boldsymbol \\mu} = \\frac{1}{n} \\boldsymbol X^T \\boldsymbol 1_n\n\\]\n\\(\\boldsymbol 1_n\\) vector length \\(n\\) containing 1 component.\\[\n\\hat{\\boldsymbol \\Sigma} = \\frac{1}{n} \\boldsymbol X^T \\boldsymbol X- \\hat{\\boldsymbol \\mu} \\hat{\\boldsymbol \\mu}^T\n\\]\nsimplify expression estimate covariance matrix\none often assumes data matrix centered, .e. \\(\\hat{\\boldsymbol \\mu} = 0\\).ambiguity convention (machine learning vs statistics convention) often implicit use\ncentered data matrices matrix notation often confusing. Hence, using two\nnotations generally preferable.","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"approximate-distribution-of-maximum-likelihood-estimates","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2 Approximate distribution of maximum likelihood estimates","text":"","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"quadratic-log-likelihood-resulting-from-normal-model","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2.1 Quadratic log-likelihood resulting from normal model","text":"Assume observe single sample \\(\\boldsymbol x\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma^2)\\) known covariance. corresponding log-likelihood \\(\\boldsymbol \\mu\\) \n\\[\nl_1(\\boldsymbol \\mu) = C - \\frac{1}{2}(\\boldsymbol x-\\boldsymbol \\mu)^T \\boldsymbol \\Sigma^{-1} (\\boldsymbol x-\\boldsymbol \\mu)\n\\]\n\\(C\\) constant depend \\(\\boldsymbol \\mu\\).\nNote log-likelihood exactly quadratic maximum lies\n\\((\\boldsymbol x, C)\\).","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"quadratic-approximation-of-a-log-likelihood-function","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2.2 Quadratic approximation of a log-likelihood function","text":"Now consider quadratic approximation log-likelihood function \\(l_n(\\boldsymbol \\theta)\\) \ngeneral model around MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\).assume model regular \n\\(\\nabla l_n(\\hat{\\boldsymbol \\theta}_{ML} ) = 0\\). Taylor series approximation scalar-valued function \\(f(\\boldsymbol x)\\) around \\(\\boldsymbol x_0\\) \n\\[\nf(\\boldsymbol x) = f(\\boldsymbol x_0) + \\nabla f(\\boldsymbol x_0) (\\boldsymbol x-\\boldsymbol x_0) + \\frac{1}{2} \n(\\boldsymbol x-\\boldsymbol x_0)^T \\nabla^T \\nabla f(\\boldsymbol x_0) (\\boldsymbol x-\\boldsymbol x_0) + \\ldots\n\\]\nApplied log-likelihood function yields\\[l_n(\\boldsymbol \\theta) \\approx l_n(\\hat{\\boldsymbol \\theta}_{ML})- \\frac{1}{2}(\\hat{\\boldsymbol \\theta}_{ML}- \\boldsymbol \\theta)^T J_n(\\hat{\\boldsymbol \\theta}_{ML})(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta)\\]quadratic function maximum \\(( \\hat{\\boldsymbol \\theta}_{ML}, l_n(\\hat{\\boldsymbol \\theta}_{ML}) )\\).\nNote natural appearance\nobserved Fisher information \\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) quadratic term.\nlinear term vanishing gradient MLE.Crucially, realise approximation form \\(\\hat{\\boldsymbol \\theta}_{ML}\\) sample\nmultivariate normal distribution mean \\(\\boldsymbol \\theta\\) covariance given inverse\nobserved Fisher information! Note requires positive definite observed\nFisher information matrix \\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) actually invertible!Example 4.2  Quadratic approximation log-likelihood proportion:Example 3.1 log-likelihood\n\\[\nl_n(p) = n \\left( \\bar{x} \\log p + (1-\\bar{x}) \\log(1-p) \\right)\n\\]\nMLE\n\\[\n\\hat{p}_{ML} = \\bar{x} \n\\]\nExample 3.4 observed Fisher information\n\\[\n\\begin{split}\nJ_n(\\hat{p}_{ML}) = \\frac{n}{\\bar{x} (1-\\bar{x})}\n\\end{split}\n\\]\nlog-likelihood MLE \n\\[\nl_n(\\hat{p}_{ML}) = n \\left( \\bar{x} \\log \\bar{x} + (1-\\bar{x}) \\log(1-\\bar{x}) \\right)\n\\]\nallows us construct quadratic approximation log-likelihood\naround MLE \n\\[\n\\begin{split}\nl_n(p) & \\approx  l_n(\\hat{p}_{ML}) - \\frac{1}{2} J_n(\\hat{p}_{ML}) (p-\\hat{p}_{ML})^2 \\\\\n   &= n \\left( \\bar{x} \\log \\bar{x} + (1-\\bar{x}) \\log(1-\\bar{x}) - \\frac{(p-\\bar{x})^2}{2 \\bar{x} (1-\\bar{x})}  \\right) \\\\\n&=  C + \\frac{ \\bar{x} p -\\frac{1}{2} p^2}{ \\bar{x} (1-\\bar{x})/n} \\\\\n\\end{split}\n\\]\nconstant \\(C\\) depend \\(p\\), purpose match approximate log-likelihood MLE corresponding original log-likelihood. \napproximate log-likelihood takes form normal log-likelihood\n(Example 3.2) one observation\n\\(\\hat{p}_{ML}=\\bar{x}\\) \\(N\\left(p, \\frac{\\bar{x} (1-\\bar{x})}{n} \\right)\\).following figure shows log-likelihood function quadratic approximation\nexample data \\(n = 30\\) \\(\\bar{x} = 0.7\\):","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"asymptotic-normality-of-maximum-likelihood-estimates","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2.3 Asymptotic normality of maximum likelihood estimates","text":"Intuitively, makes sense associate large amount curvature log-likelihood MLE low variance MLE (conversely, low amount curvature high variance).see thatnormality implies quadratic log-likelihood,conversely, taking quadratic approximation log-likelihood implies\napproximate normality, andin quadratic approximation inverse observed Fisher information plays role covariance MLE.suggests following theorem: Asymptotically, MLE normally distributed around true parameter covariance equal inverse observed Fisher information:\\[\\hat{\\boldsymbol \\theta}_{ML} \\overset{}{\\sim}\\underbrace{N_d}_{\\text{multivariate normal}}\\left(\\underbrace{\\boldsymbol \\theta}_{\\text{mean vector}},\\underbrace{\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1}}_{\\text{ covariance matrix}}\\right)\\]theorem distributional properties MLEs greatly enhances usefulness method maximum likelihood. implies regular settings maximum likelihood just method obtaining point estimates also also provides estimates uncertainty.However, need clarify “asymptotic” actually means context theorem:Primarily, means suffient sample size log-likelihood \\(l_n(\\boldsymbol \\theta)\\)\nsufficiently well approximated quadratic function around \\(\\hat{\\boldsymbol \\theta}_{ML}\\).\nbetter local quadratic approximation better normal approximation!Primarily, means suffient sample size log-likelihood \\(l_n(\\boldsymbol \\theta)\\)\nsufficiently well approximated quadratic function around \\(\\hat{\\boldsymbol \\theta}_{ML}\\).\nbetter local quadratic approximation better normal approximation!regular model positive definite observed Fisher information matrix guaranteed large sample size \\(n \\rightarrow \\infty\\) thanks central limit theorem).regular model positive definite observed Fisher information matrix guaranteed large sample size \\(n \\rightarrow \\infty\\) thanks central limit theorem).However, \\(n\\) going infinity fact always required normal approximation hold!\nDepending particular model good local fit quadratic log-likelihood\nmay available also finite \\(n\\). trivial example, normal log-likelihood valid \\(n\\).However, \\(n\\) going infinity fact always required normal approximation hold!\nDepending particular model good local fit quadratic log-likelihood\nmay available also finite \\(n\\). trivial example, normal log-likelihood valid \\(n\\).hand, non-regular models (nondifferentiable log-likelihood MLE /singular Fisher information matrix) amount data, even \\(n\\rightarrow \\infty\\), make quadratic approximation work.hand, non-regular models (nondifferentiable log-likelihood MLE /singular Fisher information matrix) amount data, even \\(n\\rightarrow \\infty\\), make quadratic approximation work.Remarks:technical details considerations worked theory locally asymptotically normal (LAN) models pioneered 1960 Lucien LeCam (1924–2000).technical details considerations worked theory locally asymptotically normal (LAN) models pioneered 1960 Lucien LeCam (1924–2000).also methods obtain higher-order (higher quadratic thus non-normal) asymptotic approximations. relate -called saddle point approximations.also methods obtain higher-order (higher quadratic thus non-normal) asymptotic approximations. relate -called saddle point approximations.","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"asymptotic-optimal-efficiency","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.2.4 Asymptotic optimal efficiency","text":"Assume now \\(\\hat{\\boldsymbol \\theta}\\) arbitrary unbiased estimator \\(\\boldsymbol \\theta\\) \nunderlying data generating model regular density \\(f(\\boldsymbol x| \\boldsymbol \\theta)\\).H. Cramér (1893–1985),\nC. R. Rao (1920–)\nothers demonstrated 1945 -called information inequality,\n\\[\n\\text{Var}(\\hat{\\boldsymbol \\theta}) \\geq \\frac{1}{n} \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)^{-1}\n\\]\nputs lower bound variance estimator \\(\\boldsymbol \\theta\\).\n(Note \\(d>1\\) matrix inequality, meaning difference matrix positive semidefinite).large sample size \\(n \\rightarrow \\infty\\) \\(\\hat{\\boldsymbol \\theta}_{ML} \\rightarrow \\boldsymbol \\theta\\) observed\nFisher information becomes\n\\(J_n(\\hat{\\boldsymbol \\theta}) \\rightarrow n \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)\\)\ntherefore can write asymptotic distribution \\(\\hat{\\boldsymbol \\theta}_{ML}\\) \n\\[\n\\hat{\\boldsymbol \\theta}_{ML} \\overset{}{\\sim} N_d\\left(  \\boldsymbol \\theta,  \\frac{1}{n} \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)^{-1}  \\right)\n\\]\nmeans large \\(n\\) regular models \\(\\hat{\\boldsymbol \\theta}_{ML}\\) achieves lowest variance possible according Cramér-Rao information inequality. words, large sample size maximum likelihood optimally efficient thus best available estimator fact MLE!However, see later hold small sample size indeed possible (necessary) improve MLE (e.g. via Bayesian estimation regularisation).","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"quantifying-the-uncertainty-of-maximum-likelihood-estimates","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.3 Quantifying the uncertainty of maximum likelihood estimates","text":"","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"estimating-the-variance-of-mles","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.3.1 Estimating the variance of MLEs","text":"previous section saw MLEs asymptotically normally distributed,\ninverse Fisher information (expected observed) linked asymptotic variance.leads question whether use observed Fisher information\n\\(J_n(\\hat{\\boldsymbol \\theta}_{ML})\\) expected Fisher information MLE\n\\(n \\boldsymbol ^{\\text{Fisher}}( \\hat{\\boldsymbol \\theta}_{ML} )\\) estimate variance MLE?Clearly, \\(n\\rightarrow \\infty\\) can used interchangeably.However, can different finite \\(n\\)\nparticular models outside exponential family.Also normality may occur well \\(n\\) goes \\(\\infty\\).Therefore one needs choose two, considering also thatthe expected Fisher information MLE average curvature MLE,\nwhereas observed Fisher information actual observed curvature, andthe observed Fisher information naturally occurs quadratic approximation log-likelihood., observed Fisher information estimator variance appropriate\nbased actual observed data also works large \\(n\\) (case yields\nresult using expected Fisher information):\n\\[\n\\widehat{\\text{Var}}(\\hat{\\boldsymbol \\theta}_{ML}) = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1}\n\\]\nsquare-root estimate standard deviation\n\\[\n\\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML}) = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{-1/2}\n\\]\nNote use matrix inversion (inverse) matrix square root.reasons preferring observed Fisher information made mathematically precise classic paper Efron Hinkley (1978).Example 4.3  Estimated variance distribution MLE proportion:Examples 3.1 3.4\nknow MLE\n\\[\n\\hat{p}_{ML} = \\bar{x} = \\frac{k}{n}\n\\]\ncorresponding observed Fisher information\n\\[\nJ_n(\\hat{p}_{ML})=\\frac{n}{\\hat{p}_{ML}(1-\\hat{p}_{ML})}\n\\]\nestimated variance MLE therefore\n\\[\n\\widehat{\\text{Var}}(   \\hat{p}_{ML}  ) = \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\n\\]\ncorresponding asymptotic normal distribution \n\\[\n \\hat{p}_{ML} \\overset{}{\\sim} N\\left(p,   \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}   \\right)\n\\]Example 4.4  Estimated variance distribution MLE mean parameter normal distribution known variance:Examples 3.2 3.5 know \n\\[\\hat{\\mu}_{ML} =\\bar{x}\\]\ncorresponding observed Fisher information \\(\\hat{\\mu}_{ML}\\) \n\\[J_n(\\hat{\\mu}_{ML})=\\frac{n}{\\sigma^2}\\]estimated variance MLE therefore\n\\[\n\\widehat{\\text{Var}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma^2}{n}\n\\]\ncorresponding asymptotic normal distribution \n\\[\n\\hat{\\mu}_{ML} \\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right)\n\\]Note case distribution asymptotic exact, .e. valid\nalso small \\(n\\) (long data \\(x_i\\) actually \\(N(\\mu, \\sigma^2)\\)!).","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"wald-statistic","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.3.2 Wald statistic","text":"Centering MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\) \\(\\boldsymbol \\theta_0\\) followed \nstandardising \\(\\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML})\\) yields Wald statistic\n(named Abraham Wald, 1902–1950):\n\\[\n\\begin{split}\n\\boldsymbol t(\\boldsymbol \\theta_0) & = \\widehat{\\text{SD}}(\\hat{\\boldsymbol \\theta}_{ML})^{-1}(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\\n & = \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})^{1/2}(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\\n\\end{split}\n\\]\nsquared Wald statistic scalar defined \n\\[\n\\begin{split}\nt(\\boldsymbol \\theta_0)^2 &= \\boldsymbol t(\\boldsymbol \\theta_0)^T \\boldsymbol t(\\boldsymbol \\theta_0) \\\\\n&= \n(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)^T\n\\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) \n(\\hat{\\boldsymbol \\theta}_{ML}-\\boldsymbol \\theta_0)\\\\\n\\end{split}\n\\]\nNote literature \\(\\boldsymbol t(\\boldsymbol \\theta_0)\\) \\(t(\\boldsymbol \\theta_0)^2\\) commonly referred Wald statistics. text use qualifier “squared” refer latter.now assume true underlying parameter \\(\\boldsymbol \\theta_0\\). Since MLE asymptotically normal Wald statistic\nasymptotically standard normal distributed:\n\\[\\begin{align*}\n\\begin{array}{cc}\n\\boldsymbol t(\\boldsymbol \\theta_0) \\overset{}{\\sim}\\\\\nt(\\theta_0) \\overset{}{\\sim}\\\\\n\\end{array}\n\\begin{array}{ll}\nN_d(0,\\boldsymbol I_d)\\\\\nN(0,1)\\\\\n\\end{array}\n\\begin{array}{ll}\n  \\text{vector } \\boldsymbol \\theta\\\\\n  \\text{scalar } \\theta\\\\\n\\end{array}\n\\end{align*}\\]\nCorrespondingly, squared Wald statistic chi-squared distributed:\n\\[\\begin{align*}\n\\begin{array}{cc}\nt(\\boldsymbol \\theta_0)^2 \\\\\nt(\\theta_0)^2\\\\\n\\end{array}\n\\begin{array}{ll}\n\\overset{}{\\sim}\\chi^2_d\\\\\n\\overset{}{\\sim}\\chi^2_1\\\\\n\\end{array}\n\\begin{array}{ll}\n  \\text{vector } \\boldsymbol \\theta\\\\\n  \\text{scalar } \\theta\\\\\n\\end{array}\n\\end{align*}\\]\ndegree freedom chi-squared distribution dimension \\(d\\)\nparameter vector \\(\\boldsymbol \\theta\\).Example 4.5  Wald statistic proportion:continue Example 4.3.\n\\(\\hat{p}_{ML} = \\bar{x}\\)\n\n\\(\\widehat{\\text{Var}}( \\hat{p}_{ML} ) = \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n}\\)\nthus \\(\\widehat{\\text{SD}}( \\hat{p}_{ML} ) =\\sqrt{ \\frac{\\hat{p}_{ML}(1-\\hat{p}_{ML})}{n} }\\)\nget Wald statistic:\\[\nt(p_0) = \\frac{\\bar{x}-p_0}{ \\sqrt{\\bar{x}(1-\\bar{x}) / n }  }\\overset{}{\\sim} N(0,1)\n\\]squared Wald statistic :\n\\[t(p_0)^2 = n \\frac{(\\bar{x}-p_0)^2}{ \\bar{x}(1-\\bar{x})   }\\overset{}{\\sim} \\chi^2_1 \\]Example 4.6  Wald statistic mean parameter normal distribution known variance:continue Example 4.4.\n\\(\\hat{\\mu}_{ML} =\\bar{x}\\) \n\\(\\widehat{\\text{Var}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma^2}{n}\\)\nthus \\(\\widehat{\\text{SD}}(\\hat{\\mu}_{ML}) = \\frac{\\sigma}{\\sqrt{n}}\\)\nget Wald statistic:\\[t(\\mu_0) = \\frac{\\bar{x}-\\mu_0}{\\sigma / \\sqrt{n}}\\sim N(0,1)\\]\nNote one sample \\(t\\)-statistic given \\(\\sigma\\).\nsquared Wald statistic :\n\\[t(\\mu_0)^2 = \\frac{(\\bar{x}-\\mu_0)^2}{\\sigma^2 / n}\\sim \\chi^2_1 \\], instance exact distribution, just asymptotic one.Using Wald statistic squared Wald statistic can test whether particular\n\\(\\mu_0\\) can rejected underlying true parameter, can also\nconstruct corresponding confidence intervals.","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"normal-confidence-intervals-using-the-wald-statistic","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.3.3 Normal confidence intervals using the Wald statistic","text":"asymptotic normality MLEs derived regular models enables us construct \ncorresponding normal confidence interval (CI):example, construct asymptotic normal CI MLE \nscalar parameter \\(\\theta\\) use MLE \\(\\hat{\\theta}_{ML}\\) estimate mean\nstandard deviation \\(\\widehat{\\text{SD}}(\\hat{\\theta}_{ML})\\) computed observed Fisher information:\\[\\text{CI}=[\\hat{\\theta}_{ML} \\pm c_{normal} \\widehat{\\text{SD}}(\\hat{\\theta}_{ML})]\\]\\(c_{normal}\\) critical value standard-normal symmetric confidence interval\nchosen achieve desired nominal coverage-\ncritical values computed using inverse standard normal distribution function via\n\\(c_{\\text{normal}}=\\Phi^{-1}\\left(\\frac{1+\\kappa}{2}\\right)\\)\n(cf. refresher section Appendix).example, CI 95% coverage one uses factor 1.96 \n\\[\\text{CI}=[\\hat{\\theta}_{ML} \\pm 1.96\\, \\widehat{\\text{SD}}(\\hat{\\theta}_{ML}) ]\\]normal CI can expressed using Wald statistic follows:\\[\\text{CI}=\\{\\theta_0:  | t(\\theta_0)| < c_{\\text{normal}} \\}\\]Similary, can also expressed using squared Wald statistic:\\[\\text{CI}=\\{\\theta_0:   t(\\boldsymbol \\theta_0)^2 < c_{\\text{chisq}} \\}\\]\nNote form facilitates construction normal confidence intervals\nparameter vector \\(\\boldsymbol \\theta_0\\).following lists containst critical values resulting chi-squared distribution\ndegree freedom \\(m=1\\) three common choices \ncoverage \\(\\kappa\\) normal CI univariate parameter:Example 4.7  Asymptotic normal confidence interval proportion:continue Examples 4.3 4.5.\nAssume observe \\(n=30\\) measurements average \\(\\bar{x} = 0.7\\).\n\\(\\hat{p}_{ML} = \\bar{x} = 0.7\\) \n\\(\\widehat{\\text{SD}}(\\hat{p}_{ML}) = \\sqrt{ \\frac{ \\bar{x}(1-\\bar{x})}{n} } \\approx 0.084\\).symmetric asymptotic normal CI \\(p\\) 95% coverage given \n\\(\\hat{p}_{ML} \\pm 1.96 \\, \\widehat{\\text{SD}}(\\hat{p}_{ML})\\) present data results interval \\([0.536, 0.864]\\).Example 4.8  Normal confidence interval mean:continue Examples 4.4 4.6.\nAssume observe \\(n=25\\) measurements average \\(\\bar{x} = 10\\), normal\nunknown mean variance \\(\\sigma^2=4\\).\\(\\hat{\\mu}_{ML} = \\bar{x} = 10\\) \n\\(\\widehat{\\text{SD}}(\\hat{\\mu}_{ML}) = \\sqrt{ \\frac{ \\sigma^2}{n} } = \\frac{2}{5}\\).symmetric asymptotic normal CI \\(p\\) 95% coverage given \n\\(\\hat{\\mu}_{ML} \\pm 1.96 \\, \\widehat{\\text{SD}}(\\hat{\\mu}_{ML})\\) present data results interval \\([9.216, 10.784]\\).","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"normal-tests-using-the-wald-statistic","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.3.4 Normal tests using the Wald statistic","text":"Finally, recall duality confidence intervals statistical tests. Specifically,\nconfidence interval coverage \\(\\kappa\\) can also used testing follows.every \\(\\theta_0\\) inside CI data allow reject hypothesis \\(\\theta_0\\) true parameter significance level \\(1-\\kappa\\).Conversely, values \\(\\theta_0\\) outside CI can rejected true parameter significance level \\(1-\\kappa\\) .Hence, order test whether \\(\\boldsymbol \\theta_0\\) true underlying parameter value can\ncompute corresponding (squared) Wald statistic, find desired critical\nvalue decide rejection.Example 4.9  Asymptotic normal test proportion:continue Example 4.7.now consider two possible values (\\(p_0=0.5\\) \\(p_0=0.8\\)) potentially true underlying proportion.value \\(p_0=0.8\\) lies inside 95% confidence interval \\([0.536, 0.864]\\). implies reject hypthesis true underlying parameter 5% significance\nlevel. contrast, \\(p_0=0.5\\) outside \nconfidence interval can indeed reject value. words, data plus model\nexlude value statistically implausible.can verified directly computing corresponding (squared) Wald statistics\n(see Example 4.5) comparing relevant critical value (3.84 chi-squared distribution 5% significance level):\\(t(0.5)^2 = 5.71 > 3.84\\) hence \\(p_0=0.5\\) can rejected.\\(t(0.8)^2 = 1.43 < 3.84\\) hence \\(p_0=0.8\\) rejected.Note squared Wald statistic boundaries normal confidence interval\nequal critical value.Example 4.10  Normal confidence interval test mean:continue Example 4.8.now consider two possible values (\\(\\mu_0=9.5\\) \\(\\mu_0=11\\)) potentially true underlying mean parameter.value \\(\\mu_0=9.5\\) lies inside 95% confidence interval \\([9.216, 10.784]\\). implies reject hypthesis true underlying parameter 5% significance\nlevel. contrast, \\(\\mu_0=11\\) outside \nconfidence interval can indeed reject value. words, data plus model\nexlude value statistically implausible.can verified directly computing corresponding (squared) Wald statistics\n(see Example 4.6) comparing relevant critical values:\\(t(9.5)^2 = 1.56 < 3.84\\) hence \\(\\mu_0=9.5\\) rejected.\\(t(11)^2 = 6.25 > 3.84\\) hence \\(\\mu_0=11\\) can rejected.squared Wald statistic boundaries confidence interval\nequals critical value.Note standard one-sample test mean, exact,\napproximation.","code":""},{"path":"quadratic-approximation-and-normal-asymptotics.html","id":"example-of-a-non-regular-model","chapter":"4 Quadratic approximation and normal asymptotics","heading":"4.4 Example of a non-regular model","text":"models allow quadratic approximation log-likelihood function around MLE. case log-likelihood function differentiable MLE. models called non-regular models normal approximation available.Example 4.11  Uniform distribution upper bound \\(\\theta\\):\n\\[x_1,\\dots,x_n \\sim U(0,\\theta)\\]\n\\(x_{[]}\\) denote ordered observations \n\\(0 \\leq x_{[1]} < x_{[2]} < \\ldots < x_{[n]} \\leq \\theta\\) \n\\(x_{[n]} = \\max(x_1,\\dots,x_n)\\).like obtain maximum likelihood estimator\n\\(\\hat{\\theta}_{ML}\\) distribution.probability density function \\(U(0,\\theta)\\) \n\\[f(x|\\theta) =\\begin{cases}\n    \\frac{1}{\\theta} &\\text{} x \\[0,\\theta] \\\\\n    0              & \\text{otherwise.}\n\\end{cases}\n\\]\nlog-scale\n\\[\n\\log f(x|\\theta) =\\begin{cases}\n    - \\log \\theta &\\text{} x \\[0,\\theta] \\\\\n    - \\infty              & \\text{otherwise.}\n\\end{cases}\n\\]Since observed data \\(x_1, \\ldots, x_n\\) lie interval \\([0,\\theta]\\)\nget log-likelihood function\n\\[\nl_n(\\theta) =\\begin{cases}\n    -n\\log \\theta  &\\text{} x_{[n]} \\leq \\theta \\\\\n    - \\infty              & \\text{otherwise}\n\\end{cases}\n\\]Obtaining MLE \\(\\theta\\) straightforward: \\(-n\\log \\theta\\) monotonically decreasing therefore log-likelihood function maximum \\(\\hat{\\theta}_{ML}=x_{[n]}\\).However, discontinuity \\(l_n(\\theta)\\) \\(x_{[n]}\\) therefore\n\\(l_n(\\theta)\\) differentiable \\(\\hat{\\theta}_{ML}\\).\nThus, quadratic approximation around \\(\\hat{\\theta}_{ML}\\)\nobserved Fisher information computed.\nHence, normal approximation distribution \\(\\hat{\\theta}_{ML}\\) valid regardless sample size, .e. even asymptotically \\(n \\rightarrow \\infty\\).Nonetheless, can fact still obtain sampling distribution \\(\\hat{\\theta}_{ML}=x_{[n]}\\). However, via asymptotic arguments instead understanding \\(x_{[n]}\\) order statistic (see https://en.wikipedia.org/wiki/Order_statistic ) following properties:\\[\\begin{align*}\n\\begin{array}{cc}\nx_{[n]}\\sim \\theta \\, \\text{Beta}(n,1)\\\\\n\\\\\n\\text{E}(x_{[n]})=\\frac{n}{n+1} \\theta\\\\\n\\\\\n\\text{Var}(x_{[n]})=\\frac{n}{(n+1)^2(n+2)}\\theta^2\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{\"n-th order statistic\" }\\\\\n\\\\\n\\\\\n\\\\\n\\approx \\frac{\\theta^2}{n^2}\\\\\n\\end{array}\n\\end{align*}\\]Note variance decreases \\(\\frac{1}{n^2}\\) much faster usual \\(\\frac{1}{n}\\) “efficient” estimator. Correspondingly,\n\\(\\hat{\\theta}_{ML}\\) -called “super efficient” estimator.","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"likelihood-based-confidence-interval-and-likelihood-ratio","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5 Likelihood-based confidence interval and likelihood ratio","text":"","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"likelihood-based-confidence-intervals-and-wilks-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1 Likelihood-based confidence intervals and Wilks statistic","text":"","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"general-idea-and-definition-of-wilks-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.1 General idea and definition of Wilks statistic","text":"Instead relying normal / quadratic approximation, can also use log-likelihood directly find called likelihood confidence intervals:Idea: find \\(\\boldsymbol \\theta_0\\) log-likelihood almost good \\(l_n(\\hat{\\boldsymbol \\theta}_{ML})\\).\n\\[\\text{CI}= \\{\\boldsymbol \\theta_0: l_n(\\hat{\\boldsymbol \\theta}_{ML}) - l_n(\\boldsymbol \\theta_0) \\leq \\Delta\\}\\]\n\\(\\Delta\\) tolerated deviation maximum log-likelihood.\nsee determine suitable \\(\\Delta\\) .leads naturally Wilks log likelihood ratio statistic \\(W(\\boldsymbol \\theta_0)\\) defined :\n\\[\n\\begin{split}\nW(\\boldsymbol \\theta_0) & = 2 \\log \\left(\\frac{L(\\hat{\\boldsymbol \\theta}_{ML})}{L(\\boldsymbol \\theta_0)}\\right) \\\\\n& =2(l_n(\\hat{\\boldsymbol \\theta}_{ML})-l_n(\\boldsymbol \\theta_0))\\\\\n\\end{split}\n\\]\nhelp can write likelihood CI follows:\n\\[\\text{CI}= \\{\\boldsymbol \\theta_0: W(\\boldsymbol \\theta_0) \\leq 2 \\Delta\\}\\]Wilks statistic named Samuel S. Wilks (1906–1964).Advantages using likelihood-based CI:restricted symmetricenables construct multivariate CIs parameter vector easily even non-normal casescontains normal CI special caseQuestion: choose \\(\\Delta\\), .e calibrate likelihood interval?\nEssentially, comparing normal CI!Example 5.1  Wilks statistic proportion:log-likelihood parameter \\(p\\) (cf. Example 3.1)\n\\[\nl_n(p) = n ( \\bar{x} \\log p + (1-\\bar{x}) \\log(1-p) )\n\\]\nHence Wilks statistic \n\\[\n\\begin{split}\nW(p_0) & = 2 ( l_n( \\hat{p}_{ML} )  -l_n( p_0  ) )\\\\\n& = 2 n \\left(  \\bar{x} \\log \\left( \\frac{  \\bar{x}  }{p_0}  \\right)  \n                + (1-\\bar{x}) \\log \\left( \\frac{1-\\bar{x} }{1-p_0}  \\right)  \n    \\right) \\\\\n\\end{split}\n\\]Comparing Example 2.8 see case Wilks\nstatistic essentially (apart scale factor \\(2n\\)) KL divergence two\nBernoulli distributions:\n\\[\nW(p_0) =2 n D_{\\text{KL}}( \\text{Ber}( \\hat{p}_{ML} ), \\text{Ber}(p_0)  )\n\\]Example 5.2  Wilks statistic mean parameter normal model:Wilks statistic \n\\[\nW(\\mu_0)^2 = \\frac{(\\bar{x}-\\mu_0)^2}{\\sigma^2 / n}\n\\]See Worksheet 4 derivation\nWilks statistic directly log-likelihood function.Note squared Wald statistic discussed \nExample 4.6.Comparing Example 2.10 see case Wilks\nstatistic essentially (apart scale factor \\(2n\\)) KL divergence two\nnormal distributions different means variance equal \\(\\sigma^2\\):\n\\[\nW(p_0) =2 n D_{\\text{KL}}( N( \\hat{\\mu}_{ML}, \\sigma^2 ), N(\\mu_0, \\sigma^2)  )\n\\]","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"quadratic-approximation-of-wilks-statistic-and-squared-wald-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.2 Quadratic approximation of Wilks statistic and squared Wald statistic","text":"Recall quadratic approximation (= second order Taylor series around MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\)) applied log-likelihood function \\(l_n(\\boldsymbol \\theta_0)\\):\\[l_n(\\boldsymbol \\theta_0)\\approx l_n(\\hat{\\boldsymbol \\theta}_{ML})-\\frac{1}{2}(\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})^T \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML}) (\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})\\]can approximate Wilks statistic:\n\\[\n\\begin{split}\nW(\\boldsymbol \\theta_0) & = 2(l_n(\\hat{\\boldsymbol \\theta}_{ML})-l_n(\\boldsymbol \\theta_0))\\\\\n& \\approx (\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})^T \\boldsymbol J_n(\\hat{\\boldsymbol \\theta}_{ML})(\\boldsymbol \\theta_0-\\hat{\\boldsymbol \\theta}_{ML})\\\\\n& =t(\\boldsymbol \\theta_0)^2 \\\\\n\\end{split}\n\\]Thus quadratic approximation Wilks statistic yields squared Wald statistic!Conversely, Wilks statistic can understood generalisation squared Wald statistic.Example 5.3  Quadratic approximation Wilks statistic proportion (continued Example 5.1):Taylor series second order (\\(p_0\\) around \\(\\bar{x}\\)) yields\n\\[\n\\log \\left( \\frac{  \\bar{x}  }{p_0} \\right) \\approx -\\frac{p_0-\\bar{x}}{\\bar{x}} + \\frac{ ( p_0-\\bar{x} )^2    }{2  \\bar{x}^2   }\n\\]\n\n\\[\n\\log \\left( \\frac{ 1- \\bar{x}  }{1- p_0} \\right) \\approx \\frac{p_0-\\bar{x}}{1-\\bar{x}} + \\frac{ ( p_0-\\bar{x} )^2    }{2  (1-\\bar{x})^2   }\n\\]\ncan approximate Wilks statistic proportion \n\\[\n\\begin{split}\nW(p_0) & \\approx  2 n \\left(  - (p_0-\\bar{x})  +\\frac{ ( p_0-\\bar{x} )^2    }{2  \\bar{x}  } \n+ (p_0-\\bar{x}) + \\frac{ ( p_0-\\bar{x} )^2    }{2  (1-\\bar{x}) } \\right)   \\\\\n& = n \\left(    \\frac{ ( p_0-\\bar{x} )^2    }{  \\bar{x}  } + \\frac{ ( p_0-\\bar{x} )^2    }{  (1-\\bar{x}) } \\right)  \\\\\n& = n \\left(    \\frac{ ( p_0-\\bar{x} )^2    }{  \\bar{x} (1-\\bar{x})  } \\right)   \\\\\n&= t(p_0)^2 \\,.\n\\end{split}\n\\]\nverifies quadratic approximation Wilks statistic leads\nback squared Wald statistic Example 4.5.Example 5.4  Quadratic approximation Wilks statistic mean parameter normal model\n(continued Example 5.2):normal log-likelihood already quadratic mean parameter (cf. Example 3.2).\nCorrespondingly, Wilks statistic quadratic mean parameter well.\nHence particular case quadratic “approximation” fact exact\nWilks statistic squared Wald statistic identical!Correspondingly, confidence intervals tests based Wilks statistic\nidentical obtained using Wald statistic.","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"distribution-of-the-wilks-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.3 Distribution of the Wilks statistic","text":"connection squared Wald statistic implies asympotically \ndistribution.Hence, \\(\\boldsymbol \\theta_0\\) Wilks statistic distributed asymptotically \n\\[W(\\boldsymbol \\theta_0) \\overset{}{\\sim} \\chi^2_d\\]\n\\(d\\) number parameters \\(\\boldsymbol \\theta\\), .e. dimension model.scalar \\(\\theta\\) (.e. single parameter \\(d=1\\)) becomes\n\\[\nW(\\theta_0) \\overset{}{\\sim} \\chi^2_1\n\\]fact known Wilks’ theorem.","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"cutoff-values-for-the-likelihood-ci","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.4 Cutoff values for the likelihood CI","text":"asymptotic distribution \\(W\\) useful choose suitable \\(\\Delta\\) likelihood\nCI — note \\(2 \\Delta = c_{chisq}\\) \\(c_{chisq}\\) critical value specified coverage \\(\\kappa\\). yields table scalar parameterExample 5.5  Likelihood confidence interval proportion:continue Example 5.1, Example 4.7 asssume data \\(n = 30\\) \\(\\bar{x} = 0.7\\).yields (via numerical root finding) 95% likelihood confidence interval\ninterval \\([0.524, 0.843]\\).\nsimilar identical corresponding\nasymptotic normal interval \\([0.536, 0.864]\\) obtained Example 4.7.following figure illustrate relationship normal CI, likelihood\nCI also shows role quadratic approximation (see also Example 4.2). Note :normal CI symmetric around MLE whereas likelihood CI symmetricthe normal CI identical likelihood CI using quadratic approximation!","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"likelihood-ratio-test-lrt-using-wilks-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.5 Likelihood ratio test (LRT) using Wilks statistic","text":"normal case (Wald statistic normal CIs) one can also construct\ntest using Wilks statistic:\\[\\begin{align*}\n\\begin{array}{ll}\nH_0: \\boldsymbol \\theta= \\boldsymbol \\theta_0\\\\\nH_1: \\boldsymbol \\theta\\neq \\boldsymbol \\theta_0\\\\\n\\end{array}\n\\begin{array}{ll}\n  \\text{ True model } \\boldsymbol \\theta_0\\\\\n  \\text{ True model } \\textbf{} \\boldsymbol \\theta_0\\\\\n\\end{array}\n\\begin{array}{ll}\n \\text{  Null hypothesis}\\\\\n \\text{  Alternative hypothesis}\\\\\n\\end{array}\n\\end{align*}\\]test statistic use Wilks log likelihood ratio \\(W(\\boldsymbol \\theta_0)\\).\nExtreme values test statistic imply evidence \\(H_0\\).Note null model “simple” (= single parameter value)\nwhereas alternative model “composite” (= set parameter values).Remarks:composite alternative \\(H_1\\) represented single point (MLE).Reject \\(H_0\\) large values \\(W(\\boldsymbol \\theta_0)\\)\\(H_0\\) large \\(n\\) statistic \\(W(\\boldsymbol \\theta_0)\\) chi-squared distributed, .e. \\(W(\\boldsymbol \\theta_0) \\overset{}{\\sim} \\chi^2_d\\). allows compute\ncritical values (.e tresholds declared rejection given significance level) also \\(p\\)-values corresponding observed test statistics.Models outside CI rejectedModels inside CI rejected, .e. can’t statistically distinguished best alternative model.statistic equivalent \\(W(\\boldsymbol \\theta_0)\\) likelihood ratio\n\\[\\Lambda(\\boldsymbol \\theta_0)  = \\frac{L(\\boldsymbol \\theta_0)}{L(\\hat{\\boldsymbol \\theta}_{ML})}\\]\ntwo statistics can transformed \\(W(\\boldsymbol \\theta_0) = -2\\log \\Lambda(\\boldsymbol \\theta_0)\\)\n\\(\\Lambda(\\boldsymbol \\theta_0) = e^{ - W(\\boldsymbol \\theta_0) / 2 }\\).\nreject \\(H_0\\) small values \\(\\Lambda\\).can shown likelihood ratio test compare two simple model optimal sense given specified type error (=probability wrongly rejecting \\(H_0\\),\n.e. sigificance level) maximise power (=1- type II error, probability correctly\naccepting \\(H_1\\)). known Neyman-Pearson theorem.Example 5.6  Likelihood test proportion:continue Example 5.5 95% likelihood confidence\ninterval \\([0.524, 0.843]\\).value \\(p_0=0.5\\) outside CI hence can rejected whereas \\(p_0=0.8\\)\ninsided CI hence rejected 5% significance level.Wilks statistic \\(p_0=0.5\\) \\(p_0=0.8\\) take following values:\\(W(0.5)^2 = 4.94 > 3.84\\) hence \\(p_0=0.5\\) can rejected.\\(W(0.8)^2 = 1.69 < 3.84\\) hence \\(p_0=0.8\\) rejected.Note Wilks statistic boundaries likelihood confidence interval\nequal critical value (3.84 corresponding 5% significance level chi-squared\ndistribution 1 degree freedom).","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"origin-of-likelihood-ratio-statistic","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.1.6 Origin of likelihood ratio statistic","text":"likelihood ratio statistic asymptotically linked differences KL divergences two compared models underlying true model.Assume \\(F\\) true (unknown) data generating model\n\\(G_{\\boldsymbol \\theta}\\) family models\nlike compare two candidate models \\(G_A\\) \\(G_B\\) corresponding\nparameters \\(\\boldsymbol \\theta_A\\) \\(\\boldsymbol \\theta_B\\) \nbasis observed data \\(x_1, \\ldots, x_n\\).\nKL divergences \\(D_A = D_{\\text{KL}}(F, G_A)\\) \\(D_B=D_{\\text{KL}}(F, G_B)\\) indicate close\nmodels \\(G_A\\) \\(G_B\\) fit true \\(F\\).\ndifference \\(D_B-D_A\\) thus way measure relative fit two models,\ncan computed \n\\[\nD_B-D_A = D_{\\text{KL}}(F, G_B)-D_{\\text{KL}}(F, G_A) = \\text{E}_{F} \\log \\frac{g_A(x)}{g_B(x)}\n\\]\nReplacing \\(F\\) empirical distribution \\(\\hat{F}_n\\) leads \nlarge sample approximation\n\\[\n2 n (D_B-D_A)  \\approx 2 (l_n(\\boldsymbol \\theta_A) - l_n(\\boldsymbol \\theta_B))\n\\]\nHence, difference log-likelihoods provides estimate difference\nKL divergence two models involved.Wilks log likelihood ratio statistic\n\\[\nW(\\boldsymbol \\theta_0) = 2 ( l_n( \\hat{\\boldsymbol \\theta}_{ML} ) - l_n(\\boldsymbol \\theta_0) ) \\approx 2 n (D_{F_{\\boldsymbol \\theta_0}}  - D_{F_{\\hat{\\boldsymbol \\theta}_{ML}}})\n\\]\nthus compares best-fit distribution \\(\\hat{\\boldsymbol \\theta}_{ML}\\)\nparameter distribution parameter \\(\\boldsymbol \\theta_0\\).specific models Wilks statistic\ncan also written form KL divergence:\n\\[\nW(\\boldsymbol \\theta_0) = 2n D_{\\text{KL}}( F_{\\hat{\\boldsymbol \\theta}_{ML}}, F_{\\boldsymbol \\theta_0})\n\\]\ncase examples 5.1 5.2 also generally exponential\nfamily models, true general.","code":""},{"path":"likelihood-based-confidence-interval-and-likelihood-ratio.html","id":"generalised-likelihood-ratio-test-glrt","chapter":"5 Likelihood-based confidence interval and likelihood ratio","heading":"5.2 Generalised likelihood ratio test (GLRT)","text":"Also known maximum likelihood ratio test (MLRT). Generalised Likelihood Ratio Test (GLRT) works like standard likelihood ratio test difference now null model \\(H_0\\) composite model. means denominator test statistic needs optimised well.\n\\[\\begin{align*}\n\\begin{array}{ll}\nH_0: \\boldsymbol \\theta\\\\omega_0 \\subset \\Omega \\\\\nH_1: \\boldsymbol \\theta\\\\omega_1  = \\Omega \\setminus \\omega_0\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{ True model lies restricted model space }\\\\\n\\text{ True model restricted model space } \\\\\n\\end{array}\n\\end{align*}\\]\\(H_0\\) \\(H_1\\) now composite hypotheses.\n\\(\\Omega\\) represents unrestricted model space dimension\n(=number free parameters)\n\\(d = |\\Omega|\\). constrained space \\(\\omega_0\\) degree freedom\n\\(d_0 = |\\omega_0|\\) \\(d_0 < d\\).\nNote standard LRT set \\(\\omega_0\\) simple point\n\\(d_0=0\\)\nnull model simple distribution. Thus, LRT contained GLRT\nspecial case!corresponding generalised (log) likelihood ratio statistic given \\[\nW = 2\\log\\left(\\frac{L(\\hat{\\theta}_{ML})}{L(\\hat{\\theta}_{ML}^0)}\\right)\n\\text{ }\n\\Lambda = \\frac{\\underset{\\theta \\\\omega_0}{\\max}\\, L(\\theta)}{\\underset{\\theta \\\\Omega}{\\max}\\, L(\\theta)}\n\\]\\(L(\\hat{\\theta}_{ML})\\) maximised likelihood assuming full model\n(parameter space \\(\\Omega\\)) \\(L(\\hat{\\theta}_{ML}^0)\\) maximised likelihood restricted model (parameter space \\(\\omega_0\\)).Remarks:MLE restricted model space \\(\\omega_0\\) taken representative \\(H_0\\).likelihood maximised numerator denominator.restriced model special case full model (.e. two models nested).asymptotic distribution \\(W\\) chi-squared degree freedom depending \\(d\\) \\(d_0\\):\\[W \\overset{}{\\sim} \\chi^2_{d-d_0}\\]result due Wilks (1938). Note \nassumes true model contained among investigated models.result due Wilks (1938). Note \nassumes true model contained among investigated models.\\(H_0\\) simple hypothesis (.e. \\(d_0=0\\)) standard LRT (corresponding CI) recovered special case GLRT.\\(H_0\\) simple hypothesis (.e. \\(d_0=0\\)) standard LRT (corresponding CI) recovered special case GLRT.Example 5.7  GLRT example:Case-control study: (e.g. “healthy” vs. “disease”)\nobserve normal data two groups sample size \\(n_1\\) \\(n_2\\)\n(\\(n=n_1+n_2\\)):\\[x_1,\\dots,x_{n_1} \\sim N(\\mu_1, \\sigma^2)\\]\n\n\\[x_{n_1+1},\\dots,x_{n} \\sim N(\\mu_2, \\sigma^2)\\]Question: two means \\(\\mu_1\\) \\(\\mu_2\\) two groups?\\[\\begin{align*}\n\\begin{array}{ll}\nH_0: \\mu_1=\\mu_2  \\text{ (variance unknown nuisance parameter)}\n\\\\\nH_1: \\mu_1\\neq\\mu_2\\\\\n\\end{array}\n\\end{align*}\\]Restricted full models:\\(\\omega_0\\): restricted model two parameters \\(\\mu_0\\) \\(\\sigma^2_0\\)\n(\\(x_{1},\\dots,x_{n} \\sim N(\\mu_0, \\sigma_0^2)\\) ).\\(\\Omega\\): full model three parameters \\(\\mu_1, \\mu_2, \\sigma^2\\).Corresponding log-likelihood functions:Restricted model \\(\\omega_0\\):\n\\[\n\\log L(\\mu_0, \\sigma_0^2) = -\\frac{n}{2} \\log(\\sigma_0^2) \n- \\frac{1}{2\\sigma_0^2} \\sum_{=1}^n (x_i-\\mu_0)^2\n\\]Full model \\(\\Omega\\):\n\\[\n\\begin{split}\n\\log L(\\mu_1, \\mu_2, \\sigma^2) & =\n \\left(-\\frac{n_1}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_{=1}^{n_1} (x_i-\\mu_1)^2   \\right) + \\\\\n& \\phantom{==}\n\\left(-\\frac{n_2}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_{=n_1+1}^{n} (x_i-\\mu_1)^2   \\right)\n \\\\\n&= -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\left( \\sum_{=1}^{n_1} (x_i-\\mu_1)^2 + \\sum_{=n_1+1}^n (x_i-\\mu_2)^2 \\right) \\\\\n\\end{split}\n\\]Corresponding MLEs:\\[\\begin{align*}\n\\begin{array}{ll}\n\\omega_0:\\\\\n\\\\\n\\Omega:\\\\\n\\\\\n\\end{array}\n\\begin{array}{ll}\n\\hat{\\mu}_0 = \\frac{1}{n}\\sum^n_{=1}x_i\\\\\n\\\\\n\\hat{\\mu}_1 = \\frac{1}{n_1}\\sum^{n_1}_{=1}x_i\\\\\n\\hat{\\mu}_2 = \\frac{1}{n_2}\\sum^{n}_{=n_1+1}x_i\\\\\n\\end{array}\n\\begin{array}{ll}\n \\widehat{\\sigma^2_0} = \\frac{1}{n}\\sum^n_{=1}(x_i-\\hat{\\mu}_0)^2\\\\\n\\\\\n \\widehat{\\sigma^2} = \\frac{1}{n}\\left\\{\\sum^{n_1}_{=1}(x_i-\\hat{\\mu}_1)^2+\\sum^n_{=n_1+1}(x_i-\\hat{\\mu}_2)^2\\right\\}\\\\\n\\\\\n\\end{array}\n\\end{align*}\\]note two estimated variances related \n\\[\n\\begin{split}\n\\widehat{\\sigma^2_0} & = \\widehat{\\sigma^2} + \\frac{n_1 n_2}{n^2} (\\hat{\\mu}_1 - \\hat{\\mu}_2)^2\\\\\n& =  \\widehat{\\sigma^2} \\left( 1+   \\frac{1}{n}     \\frac{(\\hat{\\mu}_1 - \\hat{\\mu}_2)^2}{ \\frac{n}{n_1 n_2} \\widehat{\\sigma^2}}\\right) \\\\\n& = \\widehat{\\sigma^2} \\left( 1 +  \\frac{t^2_{ML}}{n}\\right)\n\\end{split}\n\\]\n\n\\[\nt_{ML} = \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{\\sqrt{\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right) \\widehat{\\sigma^2}}}\n\\]\nexample variance decomposition, \n\\(\\widehat{\\sigma^2_0}\\) estimated total variance\n\\(\\widehat{\\sigma^2}\\) estimated within-group variance.Corresponding maximised log-likelihood:Restricted model:\\[\\log L(\\hat{\\mu}_0,\\widehat{\\sigma^2_0}) = -\\frac{n}{2} \\log(\\widehat{\\sigma^2_0}) -\\frac{n}{2} \\]Full model:\\[\n\\log L(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2}) = -\\frac{n}{2} \\log(\\widehat{\\sigma^2}) -\\frac{n}{2}\n\\]Likelihood ratio statistic:\\[\n\\begin{split}\nW & = 2\\log\\left(\\frac{L(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2})}{L(\\hat{\\mu}_0,\\widehat{\\sigma^2_0})}\\right)\\\\\n & = 2 \\log L(\\hat{\\mu}_1,\\hat{\\mu}_2,\\widehat{\\sigma^2}) - 2 \\log L(\\hat{\\mu}_0,\\widehat{\\sigma^2_0}) \\\\\n & = n\\log\\left(\\frac{\\widehat{\\sigma^2_0}}{\\widehat{\\sigma^2}} \\right) \\\\\n & = n\\log\\left(1+\\frac{t^2_{ML}}{n}\\right) \\\\\n\\end{split}\n\\]\nlast step uses decomposition total variance\n\\(\\widehat{\\sigma^2_0}\\).\nunbiased total variance estimate used \n\\[\nW=n\\log\\left(1+\\frac{1}{n-2}t^2\\right)\n\\]\n\n\\[\nt = \\frac{\\hat{\\mu}_1-\\hat{\\mu}_2}{\\sqrt{\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)\\frac{n}{n-2} \\widehat{\\sigma^2}}}\n\\]\\(\\longrightarrow\\) GRLT monotone function (squared) two-sample \\(t\\)-statistic!can shown standard tests normal distributions can interpreted GLRTs!","code":""},{"path":"optimality-properties-and-conclusion.html","id":"optimality-properties-and-conclusion","chapter":"6 Optimality properties and conclusion","heading":"6 Optimality properties and conclusion","text":"","code":""},{"path":"optimality-properties-and-conclusion.html","id":"properties-of-maximum-likelihood-encountered-so-far","chapter":"6 Optimality properties and conclusion","heading":"6.1 Properties of maximum likelihood encountered so far","text":"MLE special case relative entropy minimisation valid large samples.MLE can seen generalisation least squares (conversely, least squares special case ML).\\[\\begin{align*}\n\\begin{array}{cc}\n\\text{Kullback-Leibler 1951}\\\\\n\\textbf{Entropy learning: minimise  } D_{\\text{KL}}(F_{\\text{true}},F_{\\boldsymbol \\theta})\\\\\n\\downarrow\\\\\n\\text{large } n\\\\\n\\downarrow\\\\\n\\text{Fisher 1922}\\\\\n\\textbf{Maximise Likelihood  } L(\\boldsymbol \\theta|x_1, \\dots, x_n)\\\\\n\\downarrow\\\\\n\\text{normal model}\\\\\n\\downarrow\\\\\n\\text{Gauss 1805}\\\\\n\\textbf{Minimise squared error  } \\sum_i (x_i-\\theta)^2\\\\\n\\end{array}\n\\end{align*}\\]Given model, derivation MLE basically automatic (optimisation required)!Given model, derivation MLE basically automatic (optimisation required)!MLEs consistent, .e. true underlying model \\(F_{\\text{true}}\\) parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) contained set specified candidates models \\(F_{\\boldsymbol \\theta}\\)\nMLE converge true model.MLEs consistent, .e. true underlying model \\(F_{\\text{true}}\\) parameter \\(\\boldsymbol \\theta_{\\text{true}}\\) contained set specified candidates models \\(F_{\\boldsymbol \\theta}\\)\nMLE converge true model.Correspondingly, MLEs asympotically unbiased.Correspondingly, MLEs asympotically unbiased.However, MLEs necessarily unbiased finite samples\n(e.g. MLE variance parameter normal distribution).However, MLEs necessarily unbiased finite samples\n(e.g. MLE variance parameter normal distribution).maximum likelihood invariant parameter transformations.maximum likelihood invariant parameter transformations.regular situations (local quadratic approximation possible)\nMLEs asympotically normally distributed, asymptotic variance determined \nobserved Fisher information.regular situations (local quadratic approximation possible)\nMLEs asympotically normally distributed, asymptotic variance determined \nobserved Fisher information.regular situations large sample size MLEs asympotically optimally efficient (Cramer-Rao theorem): large samples MLE achieves lowest possible variance possible estimator — -called Cramer-Rao lower bound. variance decreases zero \\(n \\rightarrow \\infty\\) typically rate \\(1/n\\).regular situations large sample size MLEs asympotically optimally efficient (Cramer-Rao theorem): large samples MLE achieves lowest possible variance possible estimator — -called Cramer-Rao lower bound. variance decreases zero \\(n \\rightarrow \\infty\\) typically rate \\(1/n\\).likelihood ratio can used construct optimal tests (sense Neyman-Pearson theorem).likelihood ratio can used construct optimal tests (sense Neyman-Pearson theorem).","code":""},{"path":"optimality-properties-and-conclusion.html","id":"summarising-data-and-the-concept-of-minimal-sufficiency","chapter":"6 Optimality properties and conclusion","heading":"6.2 Summarising data and the concept of minimal sufficiency","text":"Another important concept statistics likelihood theory (especially applied exponential family)\nminimally sufficient statistic optimally summarise information available data parameter model.Generally, statistic \\(T(x_1, \\ldots, x_n)= T(x_i)\\) function data \\(x_1, \\ldots, x_n\\). following write \\(x_i\\) shorthand complete data set \n\\(n\\) observations.\nstatistic \\(T(x_i)\\) can type value (scalar, vector, matrix etc. — even function). \\(T(x_i)\\) called summary statistic describes important\naspects data location (e.g. average \\(\\text{avg}(x_i) =\\bar{x}\\), median) scale (e.g. standard\ndeviation, interquartile range).statistic \\(T(x_i)\\) said sufficient \nparameter \\(\\boldsymbol \\theta\\) model corresponding likelihood function can written terms \\(T(x_i)\\) \n\\[\nL(\\boldsymbol \\theta| x_i) = h( T(x_i) , \\boldsymbol \\theta) \\, k(x_i) \\,,\n\\]\n\\(h(x)\\) \\(k(x)\\) positive-valued functions, equivalently log-scale\n\\[\nl_n(\\boldsymbol \\theta) = \\log h( T(x_i) , \\boldsymbol \\theta) + \\log k(x_i) \\,.\n\\]\nknown Fisher-Pearson factorisation.\nconstruction, estimation inference \\(\\boldsymbol \\theta\\) based factorised likelihood \\(L(\\boldsymbol \\theta)\\) mediated sufficient statistic \\(T(x_i)\\) require original data \\(x_i\\). Instead, sufficient statistic \\(T(x_i)\\) contains information \\(x_i\\) required learn parameter \\(\\boldsymbol \\theta\\).\nTherefore, MLE \\(\\hat{\\boldsymbol \\theta}_{ML}\\) \\(\\boldsymbol \\theta\\) exists unique MLE unique function sufficient statistic \\(T(x_i)\\). MLE unique can chosen function \\(T(x_i)\\).\nNote sufficient statistic always exists since data \\(x_i\\)\nsufficient statistics, \\(T(x_i) = x_i\\). Furthermore, sufficient statistics unique since applying one--one transformation \n\\(T(x_i)\\) yields another sufficient statistic.Every sufficient statistic \\(T(x_i)\\) induces partitioning space data sets\nclustering hypothetical outcomes statistic \\(T(x_i)\\) assumes value \\(t\\):\n\\[\\mathcal{X}_t = \\{x_i: T(x_i) = t\\}\\]\ndata sets \\(\\mathcal{X}_t\\) equivalent terms sufficient statistic \\(T(x_i)\\). Note dimensions \\(T(x_i)\\) may much smaller \n\\(x_i\\).\nInstead \\(n\\) data points one two summaries may sufficient fully convey information data model parameters.\nThus, transforming data \\(x_i\\) using sufficient statistic \\(T(x_i)\\) may result substantial data reduction.Data sets \\(x_i\\) \\(y_i\\) ratio \nlikelihoods\n\\(L(\\boldsymbol \\theta| x_i )/L(\\boldsymbol \\theta| y_i)\\) depend \\(\\boldsymbol \\theta\\) (two likelihoods\nproportional constant)\ncalled likelihood equivalent likelihood-based procedure\nlearn \\(\\boldsymbol \\theta\\) draw identical conclusions \\(x_i\\) \\(y_i\\).\ndata sets \\(x_i, y_i \\\\mathcal{X}_t\\) equivalent respect\nsufficient statistic \\(T(x_i)\\)\nfollows directly Fisher-Pearson factorisation \nratio\n\\[L(\\boldsymbol \\theta| x_i )/L(\\boldsymbol \\theta| y_i) = k(x_i)/ k(y_i)\\]\nthus constant regard \\(\\boldsymbol \\theta\\). Consequently, data sets \\(\\mathcal{X}_t\\) also likelihood equivalent.\nHowever, converse true: depending sufficient statistics usually many likelihood equivalent data\nsets part set \\(\\mathcal{X}_t\\).particular interest therefore find sufficient statistics achieve coarsest partitioning sample space thus may allow highest data reduction.\nSpecifically, minimal sufficient statistic sufficient statistic \\(T(x_i)\\)\nlikelihood equivalent data sets also equivalent \\(T(x_i)\\).\nTherefore, check whether sufficient statistic \\(T(x_i\\) minimally sufficient verify whether two likelihood equivalent data sets \\(x_i\\) \\(y_i\\)\nalso follows \\(T(x_i) = T(y_i)\\). holds true\n\\(T(x_i)\\) minimally sufficient statistic.equivalent non-operational definition minimal sufficient statistic \\(T(x_i)\\) sufficient statistic can computed sufficient statistic \\(S(x_i)\\). follows directly: assume sufficient statistic \\(S(x_i)\\), defines\ncorresponding set \\(\\mathcal{X}_s\\) likelihood equivalent data sets. implication\n\\(x_i, y_i \\\\mathcal{X}_s\\) ncecessarily also \\(\\mathcal{X}_t\\), thus\nwhenever \\(S(x_i)=S(y_i)\\) also \\(T(x_i)=T(y_i)\\), therefore\n\\(T(x_i)\\) function \\(S(x_i)\\).trivial important example \nminimal sufficient statistic likelihood function \nsince definition can computed set sufficient statistics. Thus likelihood function \\(L(\\boldsymbol \\theta)\\) captures information \\(\\boldsymbol \\theta\\) available data. words, provides optimal summary observed data regard model. Note Bayesian statistics (discussed Part 2 module) likelihood function used proxy/summary data.Example 6.1  Sufficient statistics parameters normal distribution:normal model \\(N(\\mu, \\sigma^2)\\)\nparameter vector \\(\\boldsymbol \\theta= (\\mu, \\sigma^2)^T\\) log-likelihood\n\\[\nl_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2)  - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i-\\mu)^2\n\\]\nOne possible set minimal sufficient statistics \\(\\boldsymbol \\theta\\) \\(\\bar{x}\\)\n\\(\\overline{x^2}\\), can rewrite log-likelihood function\nwithout reference original data \\(x_i\\) follows\n\\[\nl_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2) \n-\\frac{n}{2 \\sigma^2} (\\overline{x^2} - 2 \\bar{x} \\mu + \\mu^2)\n\\]\nalternative set minimal sufficient statistics \\(\\boldsymbol \\theta\\)\nconsists \\(s^2 = \\overline{x^2} - \\bar{x}^2 = \\widehat{\\sigma^2}_{ML}\\) \n\\(\\bar{x} = \\hat{\\mu}_{ML}\\). log-likelihood written terms \\(s^2\\) \\(\\bar{x}\\) \n\\[\nl_n(\\boldsymbol \\theta) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2) \n-\\frac{n}{2 \\sigma^2} (s^2 + (\\bar{x} - \\mu)^2 )\n\\]Note example dimension parameter\nvector \\(\\boldsymbol \\theta\\) equals dimension minimal sufficient statistic,\nfurthermore, MLEs parameters fact minimal sufficient!conclusion Examples 6.1\nholds true generally: exponential family (contains normal distribution special case) MLEs natural parameters minimal sufficient statistics.\nThus, typically substantial dimension reduction raw data sufficient statistics.However, outside exponential family\nMLE necessarily minimal sufficient statistic, may even sufficient statistic.\n(minimal) sufficient statistic dimension \nparameters always exist. classic example Cauchy distribution \nminimal sufficient statistics ordered observations,\nthus MLE parameters constitute sufficient statistics, let alone minimal sufficient statistics.\nHowever, MLE course still function minimal sufficient statistic.summary, likelihood function acts perfect data summariser\n(.e. minimally sufficient statistic),\nexponential families (e.g. Normal distribution) \nMLEs parameters \\(\\hat{\\boldsymbol \\theta}_{ML}\\) minimimally sufficient.Finally, sufficiency clearly useful concept data reduction one needs keep mind always reference specific model Therefore, unless one strongly believes certain model generally good idea keep (discard!) original data.","code":""},{"path":"optimality-properties-and-conclusion.html","id":"concluding-remarks-on-maximum-likelihood","chapter":"6 Optimality properties and conclusion","heading":"6.3 Concluding remarks on maximum likelihood","text":"","code":""},{"path":"optimality-properties-and-conclusion.html","id":"remark-on-kl-divergence","chapter":"6 Optimality properties and conclusion","heading":"6.3.1 Remark on KL divergence","text":"Finding model \\(F_{\\boldsymbol \\theta}\\) best approximates underlying true model \\(F_0\\)\ndone minimising relative entropy \\(D_{\\text{KL}}(F_0,F_{\\boldsymbol \\theta})\\). large sample size \\(n\\)\nmay approximate \\(F_0\\) empirical distribution\\(\\hat{F}_0\\),\nminimising \\(D_{\\text{KL}}(\\hat{F}_0,F_{\\boldsymbol \\theta})\\) yields method maximum likelihood.However, since KL divergence symmetric fact two ways minimise divergence\nfixed \\(F_0\\) optimised \\(F_{\\boldsymbol \\theta}\\), different properties:forward KL, approximation KL: \\(\\min_{\\boldsymbol \\theta} D_{\\text{KL}}(F_0,F_{\\boldsymbol \\theta}\\)\nalso called “M (Moment) projection”. zero avoiding property:\n\\(f_{\\boldsymbol \\theta}(x)>0 \\text{ whenever } f_0(x)>0\\)forward KL, approximation KL: \\(\\min_{\\boldsymbol \\theta} D_{\\text{KL}}(F_0,F_{\\boldsymbol \\theta}\\)also called “M (Moment) projection”. zero avoiding property:\n\\(f_{\\boldsymbol \\theta}(x)>0 \\text{ whenever } f_0(x)>0\\)reverse KL, inference KL: \\(\\min_{\\boldsymbol \\theta} D_{\\text{KL}}(F_{\\boldsymbol \\theta},F_0)\\)\nalso called “(Information) projection”. zero forcing property:\n\\(f_{\\boldsymbol \\theta}(x)=0 \\text{ whenever } f_0(x)=0\\)reverse KL, inference KL: \\(\\min_{\\boldsymbol \\theta} D_{\\text{KL}}(F_{\\boldsymbol \\theta},F_0)\\)also called “(Information) projection”. zero forcing property:\n\\(f_{\\boldsymbol \\theta}(x)=0 \\text{ whenever } f_0(x)=0\\)Maximum likelihood based “forward KL”, whereas Bayesian updating Variational Bayes\napproximations use “reverse KL”.","code":""},{"path":"optimality-properties-and-conclusion.html","id":"what-happens-if-n-is-small","chapter":"6 Optimality properties and conclusion","heading":"6.3.2 What happens if \\(n\\) is small?","text":"long list optimality properties ML\nclear large sample size \\(n\\) best estimator typically MLE.However, small sample size indeed possible (necessary) improve MLE (e.g. via Bayesian estimation regularisation). ideas discussed Part II.Likelihood overfit!Alternative methods need used:regularised/penalised likelihoodBayesian methodswhich essentially two sides coin.Classic example simple non-ML estimator better MLE:\nStein’s example / Stein paradox (C. Stein, 1955):Problem setting: estimation mean multivariate caseMaximum likelihood estimation breaks ! \\(\\rightarrow\\) average (=MLE) worse terms MSE Stein estimator.Maximum likelihood estimation breaks ! \\(\\rightarrow\\) average (=MLE) worse terms MSE Stein estimator.small \\(n\\) asymptotic distributions MLE LRT accurate, inference situations distributions may need obtained simulation\n(e.g. parametric nonparametric bootstrap).small \\(n\\) asymptotic distributions MLE LRT accurate, inference situations distributions may need obtained simulation\n(e.g. parametric nonparametric bootstrap).","code":""},{"path":"optimality-properties-and-conclusion.html","id":"model-selection","chapter":"6 Optimality properties and conclusion","heading":"6.3.3 Model selection","text":"CI sets models statistically distinguishable best ML modelin doubt, choose simplest model compatible databetter prediction, avoids overfittingUseful model exploration model building.Note , construction, model parameters always higher likelihood, implying likelihood favours complex modelsComplex model may overfit!comparison models penalised likelihood Bayesian approaches may necessaryModel selection small samples high dimension challengingModel selection small samples high dimension challengingRecall aim statistics rejecting models (easy large sample size model rejected!)Instead, aim model building, .e. find model explains data well predicts well!Typically, best-fit ML model, rather simpler model close enough best / complex model.Typically, best-fit ML model, rather simpler model close enough best / complex model.","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"essentials-of-bayesian-statistics","chapter":"7 Essentials of Bayesian statistics","heading":"7 Essentials of Bayesian statistics","text":"","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"conditional-probability","chapter":"7 Essentials of Bayesian statistics","heading":"7.1 Conditional probability","text":"Assume two random variables \\(x\\) \\(y\\) joint density (joint PMF) \\(p(x,y)\\).\ndefinition \\(\\int \\int_{x,y} p(x,y) dx dy = 1\\).marginal densities individual \\(x\\) \\(y\\) given \\(p(x) = \\int_y p(x,y) dy\\)\n\\(p(y) = \\int_x f(x,y) dx\\). Thus, computing marginal densities variable removed\njoint density integrating possible states variable.\nfollows also \\(\\int_x p(x) dx = 1\\) \\(\\int_y p(y) dy = 1\\), .e. \nmarginal densities also integrate 1.alternative integrating random variable joint density \\(p(x,y)\\)\nmay wish keep fixed value, say keep \\(y\\) fixed \\(y_0\\).\ncase \\(p(x, y=y_0)\\) proportional conditional density (PMF)\ngiven ratio\n\\[\np(x | y=y_0) = \\frac{p(x, y=y_0)}{p(y=y_0)}\n\\]\ndenominator \\(p(y=y_0) = \\int_x p(x, y=y_0) dx\\) \nneeded ensure \\(\\int_x p(x | y=y_0) dx = 1\\), thus renormalises\n\\(p(x, y=y_0)\\) proper density.simplify notation, specific value variable conditioned often left .mean variance conditional distribution called conditional mean conditional variance.Rearranging see joint density can written \nproduct marginal conditional density two different ways:\n\\[\np(x,y) = p(x| y) f(y) = p(y | x) p(x)\n\\]","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"bayes-theorem","chapter":"7 Essentials of Bayesian statistics","heading":"7.2 Bayes’ theorem","text":"Bayesian statistical learning linked name Thomas Bayes\n(1701-1761) \nfirst state Bayes’ theorem (1763) conditional probability.\nInterestingly, work published Bayes’ death Richard Price (1723-1791)):\\[\np(| B) = p(B | ) \\frac{ p() }{ p(B)}\n\\]theorem relates two possible conditional densities (conditional probability mass functions) \ntwo events \\(\\) \\(B\\).follows directly product rule linking joint density\nmarginal conditional densities.","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"principle-of-bayesian-learning","chapter":"7 Essentials of Bayesian statistics","heading":"7.3 Principle of Bayesian learning","text":"Ingredients:\\(\\theta\\) parameter interest, unknown fixed.prior distribution density \\(p(\\theta)\\) describing uncertainty (randomness!) \\(\\theta\\)data generating process \\(p(x | \\theta)\\) (likelihood!)Question: new information form new observation \\(x\\) arrives - uncertainty \\(\\theta\\) change?Answer: use Bayes’ theorem update prior distribution posterior distribution.\\[\n\\underbrace{p(\\theta | x)}_{\\text{posterior} } = \\frac{p(x | \\theta) }{ p(x)} \\underbrace{p(\\theta)}_{\\text{prior}}\n\\]Note update procedure can repeated : can use posterior new prior update data.denominator Bayes formula need compute \\(p(x)\\).\nobtained \\[\n\\begin{split}\np(x) &=  \\text{E}_{F_{\\theta}}p(x | \\theta) \\\\\n &= \\int_{\\theta} p(x | \\theta) p(\\theta) d\\theta \\\\\n &= \\int_{\\theta} p(x , \\theta) d\\theta \\\\\n\\end{split}\n\\]\n.e. marginalisation parameter \\(\\theta\\) joint\ndistribution \\(\\theta\\) \\(x\\).\n(discrete \\(\\theta\\) replace integral sum).Depending context quantity either called marginal likelihood (underlying model) prior predictive distribution\n(data).Intringuingly, conduct Bayesian statistical analysis typically require integration /averaging (e.g. compute marginal likelihood), contrast maximum likelihood requires optimisation (find maximum likelihood).","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"what-is-exactly-is-the-bayesian-estimate","chapter":"7 Essentials of Bayesian statistics","heading":"7.4 What is exactly is the “Bayesian estimate”?","text":"Bayesian estimate full complete posterior distribution!However, useful summarise aspects posterior distribution:Posterior mean \\(\\text{E}(\\theta | x)\\)Posterior variance \\(\\text{Var}(\\theta | x)\\)Posterior mode\netc.particular mean posterior distribution often taken Bayesian point estimate.posterior distribution also allows define credible regions credible intervals.\nBayesian equivalent confidence intervals constructed \nfinding areas highest probability mass (say 95%) posterior distribution.Bayesian credible intervals (unlike frequentist confidence counterparts) thus easy interpret - simply correspond area parameter space can find parameter given specified probability.\ncontrast, frequentist statistics make sense assign \nprobability parameter value!Note typically many credible intervals given specified coverage \\(\\alpha\\) (say 95%). Therefore, may need criteria\nconstruct intervals.univariate case two-sided equal-tail credible interval obtained finding corresponding lower \\(1-\\alpha/2\\)\nupper \\(\\alpha/2\\) quantiles.highest posterior density (HPD) interval coverage \\(\\alpha\\) found identifying shortest interval (.e. smallest support) given \\(\\alpha\\)\nprobability mass. point within interval higher density resp. probability outside credible interval, density / probability boundaries\nequal. Thus Bayesian HPD credible interval constructed similar like likelihood based confidence interval. posterior multiple modes\nmeans HPD interval may disjoint.","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"computer-implementation-of-bayesian-learning","chapter":"7 Essentials of Bayesian statistics","heading":"7.5 Computer implementation of Bayesian learning","text":"seen Bayesian learning conceptually straightforward:Specify prior uncertainty \\(p(\\theta\\)) parameters interest \\(\\theta\\).Specify data generating process specified parameter: \\(p(x | \\theta)\\).Apply Bayes’ theorem update prior uncertainty light\nnew data.practise, however, computing posterior distribution can computationally demanding, especially\ncomplex models.reason specialised software packages developed computational Bayesian modelling, example:Bayesian statistics R: https://cran.r-project.org/web/views/Bayesian.htmlStan probabilistic programming language (can used R Python) — https://mc-stan.org/Bayesian statistics Python: PyMC3 using Theano,\nPyro using PyTorch,\nNumPyro using JAX,\nTensorFlow Probability using TensorflowBayesian statistics Julia: Turing.jlBayesian hierarchical modelling BUGS, JAGS NIMBLE.addition numerical procedures sample posterior distribution also many procedures aiming approximate Bayesian posterior, employing Laplace approximation, integrated nested Laplace approximation (INLA), variational Bayes etc.","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"bayesian-interpretation-of-probability","chapter":"7 Essentials of Bayesian statistics","heading":"7.6 Bayesian interpretation of probability","text":"","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"what-makes-you-bayesian","chapter":"7 Essentials of Bayesian statistics","heading":"7.6.1 What makes you “Bayesian”?","text":"use Bayes’ theorem therefore automatically Bayesian? !!Bayes’ theorem mathematical fact probability theory.\nHence, Bayes’ theorem valid everyone, whichever form \nstatistical learning subscribing (frequentist ideas,\nlikelihood methods, entropy learning, Bayesian learning).discuss now key difference Bayesian frequentist\nstatistical learning lies differences interpretation probability,\nmathematical formalism probability (includes Bayes’ theorem).","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"mathematics-of-probability","chapter":"7 Essentials of Bayesian statistics","heading":"7.6.2 Mathematics of probability","text":"mathematics probability modern foundation developed Andrey Kolmogorov (1903–1987). book Foundations Theory Probability (1933) establishes probability terms set theory/ measure theory. theory provides coherent mathematical framework work probabilities.However, Kolmogorov’s theory provide interpretation probability!\\(\\rightarrow\\) Kolmogorov framework basis frequentist Bayesian interpretation probability.","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"interpretations-of-probability","chapter":"7 Essentials of Bayesian statistics","heading":"7.6.3 Interpretations of probability","text":"Essentially, two major commonly used interpretation probability statistics - frequentist interpretation Bayesian interpretation.: Frequentist interpretationprobability = frequency (event long-running series identically repeated experiments)ontological view probability (.e. probability “exists” identical something can observed.).also restrictive view probability. example, frequentist probability\nused describe events occur single time.\nFrequentist probability thus can applied asymptotically, large samples!B: Bayesian probability“Probability exist” — famous quote Bruno de Finetti (1906–1985), Bayesian statistician.mean?Probability description state knowledge uncertainty.Probability thus epistemological quantity assigned changes rather something inherent property object.Note require repeated experiments.\nBayesian interpretation probability valid regardless sample size number repetitions experiment.Hence, key difference frequentist Bayesian approaches use Bayes’ theorem.\nRather whether consider probability ontological (frequentist) epistemological entity (Bayesian).","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"historical-developments","chapter":"7 Essentials of Bayesian statistics","heading":"7.7 Historical developments","text":"Thomas Bayes (1701-1761) father Bayesian statistics\ndeath paper Bayes’ theorem published (1763).Thomas Bayes (1701-1761) father Bayesian statistics\ndeath paper Bayes’ theorem published (1763).Laplace (1800) actually first use Bayes’ theorem statistical calculations. activivity called “inverse probability”.Laplace (1800) actually first use Bayes’ theorem statistical calculations. activivity called “inverse probability”.1900 1940 classical mathematical statistics developed field heavily influenced dominated R.. Fisher (invented likelihood theory ANOVA, among things - also working population genetics). Fisher much opposed Bayesian theory.1900 1940 classical mathematical statistics developed field heavily influenced dominated R.. Fisher (invented likelihood theory ANOVA, among things - also working population genetics). Fisher much opposed Bayesian theory.1931 de Finetti publishes “representation theorem”. shows joint distribution sequence exchangeable events (.e. ordering can permuted) can represented mixture distribution can constructed via Bayes’ theorem. (Note exchangeability weaker condition ..d.)\ntheorem often used justification Bayesian statistics (along socalled Dutch book argument, also de Finetti).1931 de Finetti publishes “representation theorem”. shows joint distribution sequence exchangeable events (.e. ordering can permuted) can represented mixture distribution can constructed via Bayes’ theorem. (Note exchangeability weaker condition ..d.)\ntheorem often used justification Bayesian statistics (along socalled Dutch book argument, also de Finetti).1933 publication Kolmogorov’s book probability theory.1933 publication Kolmogorov’s book probability theory.1946 Cox theorem (Richard T. Cox (1898–1991)): aim generalise classical logic (TRUE/FALSE continuous measures uncertainty) inevitably leads probability theory Bayesian learning! justification Bayesian statistics later popularised Edwin T. Jaynes (1922–1998) various books (1959, 2003).1946 Cox theorem (Richard T. Cox (1898–1991)): aim generalise classical logic (TRUE/FALSE continuous measures uncertainty) inevitably leads probability theory Bayesian learning! justification Bayesian statistics later popularised Edwin T. Jaynes (1922–1998) various books (1959, 2003).1955 Stein Paradox - Charles M. Stein (1920–2016) publishes paper Stein estimator - estimator mean dominates ML estimator. estimator always better terms MSE ML estimator, puzzling time!1955 Stein Paradox - Charles M. Stein (1920–2016) publishes paper Stein estimator - estimator mean dominates ML estimator. estimator always better terms MSE ML estimator, puzzling time!1970 onwards Bayesian learning become pervasive!Computers allow complex computations needed Bayesian statisticsMetropolis-Hastings algorithm publishedA lot work interpreting Stein estimators empirical Bayes estimators (Efron Morris 1975)\ndevelopment regularised estimation techniques penalised likelihood regression (e.g. ridge regression)regularisation originally meant make singular systems/matrices invertible - turned regularisation simple Bayesian interpretation!work reference priors (Bernado 1979)penalised likelihood via KL divergence model selection (Akaike 1973)Another boost 1990/2000s science (e.g. genomics) many complex high-dimensional data set becoming widely available. Classical statistical methods used setting (overfitting!) many new methods developed high-dimensional data analysis, many direct link Bayesian statistics:1996 lasso regression (Tibshirani)Machine learning etc (many Bayesians field!)","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"connection-with-entropy-learning","chapter":"7 Essentials of Bayesian statistics","heading":"7.8 Connection with entropy learning","text":"","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"zero-forcing-property","chapter":"7 Essentials of Bayesian statistics","heading":"7.8.1 Zero forcing property","text":"easy see Bayes rule prior probability event set 0, posterior probability event remain 0, regardless data! zero-forcing property Bayes update rule called Cromwell’s rule D. Lindley. Therefore, assigning prior probability 0 event avoided.Note implies assigning prior probability 1 event avoided, , since means assigning 0 alternative events.","code":""},{"path":"essentials-of-bayesian-statistics.html","id":"connection-with-entropy-learning-1","chapter":"7 Essentials of Bayesian statistics","heading":"7.8.2 Connection with entropy learning","text":"Bayesian update rule general form learning new information arrives form data.actually even general principle: principle minimal information update (e.g. Jaynes 1959, 2003) principle minimum information discrimination (MDI) (Kullback 1959):Change beliefs much necessary coherent new evidence!also called entropy learning since KL divergence (\\(F_{\\theta | \\text{new information}};F_{\\theta})\\) employed measure divergence updated\ndistribution distribution prior arrival information.Note update based \\(\\)-projection (see Part , Likelihood), also zero forcing property (hinting Bayes rule special case).Thus, new information arrives uncertainty parameter minimally adjusted, just much needed account new information (“inertia beliefs”).three main special cases follow entropy learning rule:information arrives form data \\(\\rightarrow\\) update T. Bayes’ theorem (1763)information form another distribution \\(\\rightarrow\\) update using R. Jeffrey’s rule (1965)information form constraints \\(\\rightarrow\\) Kullback’s principle minimum MDI (1959),\nE. T. Jaynes MaxEnt principle (1957)Since 1) far common situation clear important study Bayesian learning!shows () fundamentally important KL divergence statistics - leads likelihood inference also Bayesian learning, well forms information updating! Furthermore, relative entropy useful choose priors (e.g. reference priors) experimental design.","code":""},{"path":"beta-binomial-model-for-estimating-a-proportion.html","id":"beta-binomial-model-for-estimating-a-proportion","chapter":"8 Beta-Binomial model for estimating a proportion","heading":"8 Beta-Binomial model for estimating a proportion","text":"chapter discuss estimate portion Bayesian\nframework.","code":""},{"path":"beta-binomial-model-for-estimating-a-proportion.html","id":"binomial-likelihood","chapter":"8 Beta-Binomial model for estimating a proportion","heading":"8.1 Binomial likelihood","text":"order apply Bayes’ theorem first need find suitable\nlikelihood based modeling data generating process. follow\nBinomial model used previously Part :Repeated Bernoulli experiment (Binomial model):\\(x \\\\{0, 1\\}\\) (e.g. “tails” vs. “heads”)proability mass function (pmf): \\(\\text{Pr}(x=1) = p\\), \\(\\text{Pr}(x=0) = 1-p\\)\nMean: \\(\\text{E}(x) = p\\)\nVariance \\(\\text{Var}(x) = p (1-p)\\)\\(\\text{Bin}(n,p)\\) (sum \\(n\\) Bernoulli experiments)\\(x \\\\{0, 1, \\ldots, n\\}\\)\nMean: \\(\\text{E}(x) = n p\\)\nVariance: \\(\\text{Var}(x) = n p (1-p)\\)Standardised Binomial (average \\(n\\) Bernoulli experiments):\\(\\frac{x}{n} \\\\{0, \\frac{1}{n}, \\ldots,1\\}\\)\nMean: \\(\\text{E}(\\frac{x}{n}) = p\\)\nVariance: \\(\\text{Var}(\\frac{x}{n}) = \\frac{p (1-p)}{n}\\)part (likelihood theory) know maximum likelihood estimate proportion\nfrequency \\(\\hat p_{ML} = \\frac{x}{n}\\) given \\(x\\) (number “heads”) observed \\(n\\) repeats.","code":""},{"path":"beta-binomial-model-for-estimating-a-proportion.html","id":"excursion-properties-of-the-beta-distribution","chapter":"8 Beta-Binomial model for estimating a proportion","heading":"8.2 Excursion: Properties of the Beta distribution","text":"density Beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\)\n\\(x \\[0,1]\\) \\(\\alpha>0\\) \\(\\beta>0\\)\n\n\\[f(x | \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}\\]\nmean \\(\\text{E}(x) = \\mu = \\frac{\\alpha}{\\alpha+\\beta}\\) variance\n\\(\\text{Var}(x)=\\frac{\\mu (1-\\mu)}{\\alpha+\\beta+1}\\).density depends Beta function \\(B(, b) = \\frac{ \\Gamma() \\Gamma(b)}{\\Gamma(+ b)}\\)\nturn defined via Euler’s Gamma function\n\\[\n\\Gamma(x) = \\int_0^\\infty t^{x-1} e^{-t} dt\n\\]\nNote \\(\\Gamma(x) = (x-1)!\\) positive integer \\(x\\)useful reparameterisation Beta distribution terms parameters\n\\(\\mu \\[0,1]\\) \\(m > 0\\), yielding original parameters via\n\\(\\alpha= \\mu m\\) \\(\\beta=(1-\\mu)m\\). Conversely, \\(m=\\alpha+\\beta\\) \\(\\mu = \\frac{\\alpha}{\\alpha+\\beta}\\).Beta distribution flexible can assume number different shapes, depending value \\(\\alpha\\) \\(\\beta\\):","code":""},{"path":"beta-binomial-model-for-estimating-a-proportion.html","id":"beta-prior-distribution","chapter":"8 Beta-Binomial model for estimating a proportion","heading":"8.3 Beta prior distribution","text":"Bayesian learning need make explicit uncertainty \\(p\\).\\(p\\) support \\([0,1]\\) \\(\\rightarrow\\) use Beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\) prior \\(p\\) parameters \\(\\alpha \\geq 0\\) \\(\\beta \\geq 0\\):\\[\np \\sim \\text{Beta}(\\alpha, \\beta)\n\\]Note actually mean \\(p\\) random! means model uncertainty \\(p\\) using Beta random variable!flexibility Beta distribution allows accomodate large variety possible scenarious prior knowledge.prior mean \n\\[\\text{E}(p) = \\frac{\\alpha}{m} = \\mu_{\\text{prior}}\\]\nprior variance\n\\[\n\\text{Var}(p)  = \\frac{\\mu_{\\text{prior}} (1-\\mu_{\\text{prior}})}{m + 1}\n\\]\n\\(m = \\alpha + \\beta\\).Note similarity moments standardised Binomial !","code":""},{"path":"beta-binomial-model-for-estimating-a-proportion.html","id":"computing-the-posterior-distribution","chapter":"8 Beta-Binomial model for estimating a proportion","heading":"8.4 Computing the posterior distribution","text":"Bayes’ theorem continous random variables compute posterior density:\n\\[\nf(p | x) = \\frac{f(x | p) f(p) }{\\int_{p^{'}} f(x | p^{'})  f(p^{'}) dp^{'}}\n\\]\nuse analysis Beta-Binomial model:Beta prior:\n\\[\np \\sim \\text{Beta}(\\alpha, \\beta)\n\\]\n\\[\nf(p) = \\frac{1}{B(\\alpha, \\beta)} p^{\\alpha-1} (1-p)^{\\beta-1}\n\\]Binomial likelihood:\n\\[\nx | p \\sim \\text{Bin}(n,p)\n\\]\n\\[\nf(x|p) = \\begin{pmatrix} n \\\\ x \\end{pmatrix} p^x (1-p)^{(n-x)}\n\\]Applying Bayes’ theorem results inBeta posterior distribution\n\\[\np| x \\sim \\text{Beta}(\\alpha+x, \\beta+n-x)\n\\]\n\\[\nf(p| x) = \\frac{1}{B(\\alpha+x, \\beta+n-x)} p^{\\alpha+x-1} (1-p)^{\\beta+n-x-1}\n\\]\n(proof see Worksheet 5!)posterior can summarised first two moments (mean variance):Posterior mean:\n\\[\n\\mu_{\\text{posterior}}= \\text{E}(p | x) = \\frac{x +\\alpha}{n+ m }\n\\]Posterior variance:\n\\[\n\\sigma^2_{\\text{posterior}}= \\text{Var}(p | x) \n= \\frac{\\mu_{\\text{posterior}}(1-\\mu_{\\text{posterior}})}{n+m+1 }\n\\]","code":""},{"path":"properties-of-bayesian-learning.html","id":"properties-of-bayesian-learning","chapter":"9 Properties of Bayesian learning","heading":"9 Properties of Bayesian learning","text":"Beta-Binomial models allows observe number intriguing features\nproperties Bayesian learning. Many extend also models see later.","code":""},{"path":"properties-of-bayesian-learning.html","id":"prior-acting-as-pseudo-data","chapter":"9 Properties of Bayesian learning","heading":"9.1 Prior acting as pseudo-data","text":"expression posterior mean variance can see \n\\(m=\\alpha + \\beta\\) behaves like implicit sample size connected prior information!Specifically, \\(\\alpha\\) \\(\\beta\\) act pseudo-counts influence\nposterior mean posterior variance, exactly way coventional data.example, larger \\(m\\) (thus \\(\\alpha\\) \\(\\beta\\)) smaller posterior variance, variance decreasing proportional inverse \\(m\\). prior highly concentrated, .e. low variance large precision (=inverse variance) implicit data size \\(m\\) large. Conversely, prior large variance, prior vague implicit data size \\(m\\) small.Hence, prior effect one add data – without actually adding data! precisely prior acts regulariser prevents overfitting, increases effective sample size.Another interpretation prior summarises data\nmay available previously observations.","code":""},{"path":"properties-of-bayesian-learning.html","id":"linear-shrinkage-of-mean","chapter":"9 Properties of Bayesian learning","heading":"9.2 Linear shrinkage of mean","text":"posterior mean \\(\\mu_{\\text{posterior}}\\) linearly adjusted \\(\\hat\\mu_{ML}\\). becomes evident writing \\(\\mu_{\\text{posterior}}\\) \\[\n\\mu_{\\text{posterior}} = \\lambda \\mu_{\\text{prior}} + (1-\\lambda) \\hat\\mu_{ML}\n\\]\nweight \\(\\lambda \\[0,1]\\)\n\\[\n\\lambda = \\frac{m}{m+n} \\,.\n\\]\nposterior mean convex combination (.e. weighted average) ML estimate prior mean. factor \\(\\lambda\\) called shrinkage intensity — note ratio “prior sample size” (\\(m\\)) “effective overall sample size” (\\(m+n\\)).called shrinkage, ML estimator “shrunk” towards prior mean (often called “target”, sometimes target zero, terminology “shrinking” makes sense).called shrinkage, ML estimator “shrunk” towards prior mean (often called “target”, sometimes target zero, terminology “shrinking” makes sense).shrinkage intensity zero (\\(\\lambda = 0\\)) ML point estimator recovered. implies \\(\\alpha=0\\) \\(\\beta=0\\), \\(n \\rightarrow \\infty\\).\nNote using maximum likelihood estimate proportion \\(p\\) (moderate small \\(n\\)) Bayesian estimation using Beta-Binomial model prior \\(\\alpha=0\\) \\(\\beta=0\\). prior extremely “u-shaped” implicit prior ML estimation. (use prior intentionally?)shrinkage intensity zero (\\(\\lambda = 0\\)) ML point estimator recovered. implies \\(\\alpha=0\\) \\(\\beta=0\\), \\(n \\rightarrow \\infty\\).Note using maximum likelihood estimate proportion \\(p\\) (moderate small \\(n\\)) Bayesian estimation using Beta-Binomial model prior \\(\\alpha=0\\) \\(\\beta=0\\). prior extremely “u-shaped” implicit prior ML estimation. (use prior intentionally?)shrinkage intensity large (\\(\\lambda \\rightarrow 1\\)) posterior mean corresponds prior.\nhappens \\(n=0\\) \\(m\\) large (implying prior sharply concentrated around prior mean).shrinkage intensity large (\\(\\lambda \\rightarrow 1\\)) posterior mean corresponds prior.\nhappens \\(n=0\\) \\(m\\) large (implying prior sharply concentrated around prior mean).Since ML estimate (=frequency) unbiased Bayesian point estimate biased (finite n)! bias fact prior mean! Bayesian statistics produces default biased estimators (asymptotically unbiased like ML).Since ML estimate (=frequency) unbiased Bayesian point estimate biased (finite n)! bias fact prior mean! Bayesian statistics produces default biased estimators (asymptotically unbiased like ML).posterior mean linear combination ML estimate prior mean coincidence. fact, true distributions exponential family (see e.g. Diaconis Ylvisaker, 1979). Furthermore, possible (indeed quite useful computational reasons!) formulate Bayes theory completely terms linear shrinkage (e.g. Hartigan 1969). resulting theory called “Bayes linear statistics” (Goldstein Wooff, 2007).posterior mean linear combination ML estimate prior mean coincidence. fact, true distributions exponential family (see e.g. Diaconis Ylvisaker, 1979). Furthermore, possible (indeed quite useful computational reasons!) formulate Bayes theory completely terms linear shrinkage (e.g. Hartigan 1969). resulting theory called “Bayes linear statistics” (Goldstein Wooff, 2007).","code":""},{"path":"properties-of-bayesian-learning.html","id":"conjugacy-of-prior-and-posterior-distribution","chapter":"9 Properties of Bayesian learning","heading":"9.3 Conjugacy of prior and posterior distribution","text":"Beta-Binomial model estimating proportion \\(p\\) choice Beta distribution prior distribution along Binomial likelihood resulted Beta distribution posterior distribution well.prior posterior belong distributional family prior called conjugate prior. case prior functional form likelihood.Beta-Binomial likelihood based Binomial distribution following form\n(terms depending parameter \\(p\\) shown):\n\\[\np^x (1-p)^{n-x}\n\\]\nform Beta prior (, showing terms depending \\(p\\)):\n\\[\np^{\\alpha-1} (1-p)^{\\beta-1}\n\\]\nSince posterior proportional product prior\nlikelihood posterior exactly form \nprior:\n\\[\np^{\\alpha+x-1} (1-p)^{\\beta+n-x-1}\n\\]\nChoosing prior distribution family conjugate likelihood\ngreatly simplifies Bayesian analysis since Bayes formula can written form update formula parameters Beta distribution:\n\\[\n\\alpha \\rightarrow \\alpha +x\n\\]\n\\[\n\\beta \\rightarrow \\beta +n-x\n\\]Thus, conjugate prior distributions convenient choices. However, application must ensured prior distribution flexible enough encapsulate prior information may available. cases case alternative priors used (likely require compute posterior distribution numerically rather analytically).","code":""},{"path":"properties-of-bayesian-learning.html","id":"large-sample-asymptotics","chapter":"9 Properties of Bayesian learning","heading":"9.4 Large sample asymptotics","text":"","code":""},{"path":"properties-of-bayesian-learning.html","id":"large-sample-limits-of-mean-and-variance","chapter":"9 Properties of Bayesian learning","heading":"9.4.1 Large sample limits of mean and variance","text":"\\(n\\) large \\(n >> \\alpha, \\beta\\) posterior mean variance become asympotically\\[\n\\mu_{\\text{posterior}} \\overset{}{=} \\frac{x }{n} = \\hat\\mu_{ML}\n\\]\n\n\\[\n\\sigma^2_{\\text{posterior}} \\overset{}{=}   \\frac{\\hat\\mu_{ML} (1-\\hat\\mu_{ML})}{n}\n\\]Thus, sample size large Bayes’ estimator turns ML estimator! Specifically,\nposterior mean becomes ML point estimate, posterior variance equal asymptotic variance computed via observed Fisher information!Thus, large \\(n\\) data dominate details prior (values \\(\\alpha\\) \\(\\beta\\) become irrelevant!","code":""},{"path":"properties-of-bayesian-learning.html","id":"asymptotic-normality-of-the-posterior-distribution","chapter":"9 Properties of Bayesian learning","heading":"9.4.2 Asymptotic Normality of the Posterior distribution","text":"Also known Bayesian Central Limit Theorem (CLT).regularity conditions (regular likelihood positive prior probability \nparameter values, finite number parameters, etc.) large sample size Bayesian posterior distribution converges Normal distribution\ncentered around MLE variance MLE:\\[\n\\text{large $n$:  }  p(\\boldsymbol \\theta| \\boldsymbol x_1, \\boldsymbol x_2, \\ldots, \\boldsymbol x_n) \\N(\\hat{\\boldsymbol \\theta}_{ML}, \\text{Var}(\\hat{\\boldsymbol \\theta}_{ML}) )\n\\]posterior mean variance converging MLE variance MLE\nlarge sample size, also posterior distribution converges sampling distribution!holds generally many regular cases, just example Beta-Bernoulli model.Bayesian CLT generally known\nBernstein-van Mises theorem (discovered around 1920-30), special cases already known Laplace.Worksheet 5 asymptotic convergence posterior distribution normal distribution demonstrated grapically.","code":""},{"path":"properties-of-bayesian-learning.html","id":"posterior-variance-for-finite-n","chapter":"9 Properties of Bayesian learning","heading":"9.5 Posterior variance for finite \\(n\\)","text":"previous chapter derived Bayesian point estimate\nproportion \\(p\\) posterior mean\n\\[\n\\text{E}(p | x ) = \\frac{x+\\alpha}{n+m} = \\hat{p}_{\\text{Bayes}}\n\\]\nposterior variance\n\\[\n\\text{Var}(p | x) = \\frac{\\hat{p}_{\\text{Bayes}} (1-\\hat{p}_{\\text{Bayes}})}{n+m+1}\n\\]Asymptotically, seen large \\(n\\) posterior becomes ML estimator, \nposterior variance becomes asymptotic variance MLE.\nThus, Bayesian estimate indistinguishable MLE large \\(n\\)\nshares favourable properties.addition, finite sample size posterior variance tyically smaller asymptotic\nposterior variance (large \\(n\\)) prior variance, showing combining information\nprior data leads efficient estimate.","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","text":"","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"normal-normal-model-to-estimate-mean","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.1 Normal-Normal model to estimate mean","text":"","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"normal-likelihood","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.1.1 Normal likelihood","text":"likelihood assume data-generating model normal distribution known fixed variance \\(\\sigma^2\\)\n\\[\nx| \\mu \\sim N(\\mu, \\sigma^2)\n\\]\nyields MLE \\(\\hat\\mu_{ML} = \\bar{x}\\).","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"normal-prior-distribution","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.1.2 Normal prior distribution","text":"model uncertainty \\(\\mu\\) use normal distribution \\(N(\\mu, \\sigma^2/k)\\)\nparameterised two parameters \\(\\mu\\) \\(k\\) (remember \\(\\sigma^2\\) fixed).\\(\\mu=\\mu_0\\) \\(k=m\\) get normal prior\n\\[\n\\mu \\sim N(\\mu_0, \\sigma^2/m)\n\\]\nprior mean\n\\(\\text{E}(\\mu) = \\mu_0\\)\nprior variance\n\\(\\text{Var}(\\mu) = \\frac{\\sigma^2}{m}\\)\n\\(m\\) implied sample size prior. Note \\(m\\) need integer value!","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"normal-posterior-distribution","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.1.3 Normal posterior distribution","text":"posterior distribution observing \\(n\\) samples \\(x_1, \\ldots x_n\\)\nnormal \\(\\mu=\\mu_1\\) \\(k=m+n\\)\n\\[\n\\mu | x_1, \\ldots x_n \\sim N(\\mu_1, \\sigma^2/(m+n))\n\\]\nposterior mean\n\\[\n\\text{E}(\\mu |  x_1, \\ldots x_n) = \\mu_1 =\\frac{m \\mu_0 + n \\bar{x}}{n+m}  = \\lambda \\mu_0 + (1-\\lambda) \\hat\\mu_{ML}\n\\]\n\\(\\lambda = \\frac{m}{n+m}\\). Note linear shrinkage \n\\(\\hat\\mu_{ML}\\) towards \\(\\mu_0\\)!corresponding posterior variance \n\\[\n\\text{Var}(\\mu |  x_1, \\ldots x_n) = \\frac{\\sigma^2}{n+m}\n\\]\nThus, normal distribution conjugate distribution mean parameter normal likelihood.","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"large-sample-asymptotics-and-stein-paradox","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.1.4 Large sample asymptotics and Stein paradox","text":"\\(n\\) large \\(n >> m\\) get\n\\[\n\\text{E}(\\mu |  x_1, \\ldots x_n) \\overset{}{=}  \\hat\\mu_{ML}\n\\]\n\\[\n\\text{Var}(\\mu |  x_1, \\ldots x_n) \\overset{}{=} \\frac{\\sigma^2}{n}\n\\]\n.e. MLE asymptotic variance!Note posterior variance \\(\\frac{\\sigma^2}{n+m}\\) smaller asymptotic variance \\(\\frac{\\sigma^2}{n}\\) prior variance \\(\\frac{\\sigma^2}{m}\\).studying frequentist properties posterior mean \\(\\mu_1\\) turns \nappropriate choice \\(m\\) (\\(\\lambda\\)) possible construct estimator \noutperform MLE finite \\(n\\) terms MSE (reduced variance compensating increase bias)!\nCharles Stein one first present estimator (see next chapter), many contemporaries\nconsidered puzzling estimator outperform MLE, hence effect called\nStein paradox.","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"inverse-gamma-normal-model-to-estimate-variance","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.2 Inverse-Gamma-Normal model to estimate variance","text":"","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"inverse-gamma-distribution","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.2.1 Inverse Gamma distribution","text":"Next, study common Bayesian model estimating variance parameter normal distribution. use inverse Gamma distribution:\n\\[\nx \\sim \\text{Inv-Gam}(\\alpha, \\beta)\n\\]\ndistribution closely linked Gamma distribution — inverse \\(x\\) Gamma-distributed inverted scale parameter:\n\\[\\frac{1}{x} \\sim \\text{Gam}(\\alpha, \\beta^{-1})\\]use prior posterior employ different parameterisation \\(k=2(\\alpha-1)\\) \\(v=\\beta/(\\alpha-1)\\):\n\\[\nx \\sim \\text{Inv-Gam}(1+\\frac{k}{2}, \\frac{k}{2} v)\n\\]first two moments inverse Gamma distribution \n\\[\\text{E}(x) = \\frac{\\beta}{\\alpha-1} = v\\]\n\n\\[\\text{Var}(x) = \\frac{\\beta^2}{(\\alpha-1)^2 (\\alpha-2)} =\\frac{2 v^2}{k-2}\\]","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"normal-likelihoood","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.2.2 Normal likelihoood","text":"data likelihood / generating model use\nnormal distribution \\(N(\\mu, \\sigma^2)\\) given fixed mean \\(\\mu\\).yields MLE \\(\\widehat\\sigma^2_{ML}= \\frac{1}{n}\\sum_{=1}^n (x_i-\\mu)^2\\)","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"inverse-gamma-prior-distribution","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.2.3 Inverse Gamma prior distribution","text":"prior distribution use inverse Gamma\ndistribution \\(k=m\\) \\(v=\\sigma^2_0\\)\n\\[\n\\sigma^2 \\sim \\text{Inv-Gam}(k=m, v=\\sigma^2_0) \n\\]\ncorresponding prior mean \n\\[\n\\text{E}(\\sigma^2) = \\sigma^2_0\n\\]\nprior variance \n\\[\n\\text{Var}(\\sigma^2) = \\frac{2 \\sigma_0^4}{m-2}\n\\]\n(note \\(m > 2\\))","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"inverse-gamma-posterior-distribution","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.2.4 Inverse Gamma posterior distribution","text":"inverse Gammma distribution conjugate \nnormal likelihood posterior distribution inverse Gamma well:\n\\[\n\\sigma^2| x_1 \\ldots, x_n \\sim \\text{Inv-Gam}(k=m+n, v=\\sigma^2_1)\n\\]\n\\(\\sigma^2_1 = \\frac{\\sigma^2_0 m + n \\widehat\\sigma^2_{ML}}{m+n}\\).posterior mean \n\\[\n\\text{E}(\\sigma^2 | x_1 \\ldots, x_n) = \\sigma^2_1\n\\]\nposterior variance\n\\[\n\\text{Var}(\\sigma^2 | x_1 \\ldots, x_n) = \\frac{ 2 \\sigma^4_1}{m+n-2}\n\\]\nupdate formula posterior mean variance follows usual linear shrinkage rule:\n\\[\n\\sigma^2_1 =  \\lambda \\sigma^2_0 + (1-\\lambda) \\widehat\\sigma^2_{ML}\n\\]\n\\(\\lambda=\\frac{m}{m+n}\\).","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"large-sample-asymptotics-1","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.2.5 Large sample asymptotics","text":"\\(n\\) large \\(n >> m\\) get\n\\[\n\\text{E}(\\sigma^2 |  x_1, \\ldots x_n) \\overset{}{=}  \\widehat\\sigma^2_{ML}\n\\]\n\\[\n\\text{Var}(\\sigma^2 |  x_1, \\ldots x_n) \\overset{}{=} \\frac{2 \\sigma^4}{n}\n\\]\nindeed MLE \\(\\sigma^2\\) asymptotic variance!","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"estimating-precision","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.2.6 Estimating precision","text":"Instead estimating variance actually bit simpler estimate precision (.e. inverse variance). one use\nGamma prior normal likelihood, resulting Gamma posterior.","code":""},{"path":"normal-normal-and-inverse-gamma-normal-models-for-estimating-the-mean-and-the-variance.html","id":"joint-estimation-of-mean-and-variance","chapter":"10 Normal-Normal and Inverse-Gamma-Normal models for estimating the mean and the variance","heading":"10.2.7 Joint estimation of mean and variance","text":"possible combine Normal-Normal mean\nInverse-Gamma-Normal model joint model mean variance.implies joint prior joint posterior \\(\\mu\\) \\(\\sigma^2\\).Details shown resulting joint point estimators identical individual estimators.","code":""},{"path":"shrinkage-estimation-using-empirical-risk-minimisation.html","id":"shrinkage-estimation-using-empirical-risk-minimisation","chapter":"11 Shrinkage estimation using empirical risk minimisation","heading":"11 Shrinkage estimation using empirical risk minimisation","text":"","code":""},{"path":"shrinkage-estimation-using-empirical-risk-minimisation.html","id":"linear-shrinkage","chapter":"11 Shrinkage estimation using empirical risk minimisation","heading":"11.1 Linear shrinkage","text":"examples Bayesian estimation seen far\nposterior mean parameter interest obtained\nlinear shrinkage\n\\[\n\\hat\\theta_{\\text{shrink}} = \\text{E}( \\theta | x_1, \\ldots, x_n) = \\lambda \\theta_0 + (1-\\lambda) \\hat\\theta_{\\text{ML}}\n\\]\nMLE \\(\\hat\\theta_{\\text{ML}}\\) towards \nprior mean \\(\\theta_0\\), shrinkage intensity \\(\\lambda=\\frac{m}{m+n}\\)\ndetermined pseudo-sample size \\(m\\)\n(turn linked precision prior)\nsample size \\(n\\).resulting point estimate \\(\\hat\\theta_{\\text{shrink}}\\) called shrinkage estimate\nconvex combination \\(\\theta_0\\) \\(\\hat\\theta_{\\text{ML}}\\). prior mean \\(\\theta_0\\) also\ncalled “target”.Bayesian estimation parameter \\(m\\) hence \\(\\lambda\\) given priori, turns possible useful find optimal value \\(\\lambda\\) minimising mean squared error \nestimator \\(\\hat\\theta_{\\text{shrink}}\\).particular, construction, target \\(\\theta_0\\) zero variance\nsubstantial bias, whereas MLE \\(\\hat\\theta_{\\text{ML}}\\) low zero bias non-vanishing variance. combinining two estimators opposite properties aim achieve\nbias-variance tradeoff resulting estimator \\(\\hat\\theta_{\\text{shrink}}\\) lower MSE either\n\\(\\theta_0\\) \\(\\hat\\theta_{\\text{ML}}\\).Specifically, aim find\n\\[\n\\lambda^{\\star} = \\underset{\\lambda}{\\arg \\min \\ }  \n\\text{E}\\left( ( \\theta - \\hat\\theta_{\\text{shrink}} )^2\\right) \n\\]turns can minimised without knowing actual true value \\(\\theta\\)\nresult unbiased \\(\\hat\\theta_{\\text{ML}}\\) \n\\[\n\\lambda^{\\star} = \\frac{\\text{Var}(\\hat\\theta_{\\text{ML}})}{\\text{E}( (\\hat\\theta_{\\text{ML}} - \\theta_0)^2 )}\n\\]\nHence, shrinkage intensity small variance MLE small /target\nMLE differ substantially. hand, variance MLE large /target close MLE shrinkage intensity large.","code":""},{"path":"shrinkage-estimation-using-empirical-risk-minimisation.html","id":"james-stein-estimator","chapter":"11 Shrinkage estimation using empirical risk minimisation","heading":"11.2 James-Stein estimator","text":"can now use empirical risk minimisation estimate shrinkage parameter Normal-Normal model.1955 James Stein propose following estimate \nmultivariate mean \\(\\boldsymbol \\mu\\) using single sample \\(\\boldsymbol x\\)\ndrawn multivariate normal \\(N_d(\\boldsymbol \\mu, \\boldsymbol )\\):\n\\[\n\\hat{\\boldsymbol \\mu}_{JS} = (1-\\frac{d-2}{||\\boldsymbol x||^2}) \\boldsymbol x\n\\]\n, recognise \\(\\hat{\\boldsymbol \\mu}_{ML} = \\boldsymbol x\\), \\(\\boldsymbol \\mu_0=0\\) shrinkage intensity \\(\\lambda^{\\star}=\\frac{d-2}{||\\boldsymbol x||^2}\\).Efron Morris (1972) Lindley Smith (1972)\ngeneralised shrinkage estimator case\nmultiple observations \\(\\boldsymbol x_1, \\ldots \\boldsymbol x_n\\)\ntarget \\(\\boldsymbol \\mu_0\\), yielding empirical Bayes estimate \\(\\mu\\) based Normal-Normal model.","code":""},{"path":"bayesian-model-comparison-using-bayes-factors-and-the-bic.html","id":"bayesian-model-comparison-using-bayes-factors-and-the-bic","chapter":"12 Bayesian model comparison using Bayes factors and the BIC","heading":"12 Bayesian model comparison using Bayes factors and the BIC","text":"","code":""},{"path":"bayesian-model-comparison-using-bayes-factors-and-the-bic.html","id":"the-bayes-factor","chapter":"12 Bayesian model comparison using Bayes factors and the BIC","heading":"12.1 The Bayes factor","text":"like compare two models \\(M_1\\) \\(M_2\\). seeing data \\(D\\) can check Prior odds (= ratio prior probabilities models \\(M_1\\) \\(M_2\\)):\\[\\frac{\\text{Pr}(M_1)}{\\text{Pr}(M_2)}\\]seeing data \\(D\\) arrive Posterior odds (= ratio posterior probabilities):\n\\[\\frac{\\text{Pr}(M_1 | D)}{\\text{Pr}(M_2  | D)}\\]Using Bayes Theorem \\(\\text{Pr}(M_i | D) = \\text{Pr}(M_i) \\frac{p(D | M_i) }{p(D)}\\) can rewrite \nposterior odds \n\\[\n\\underbrace{\\frac{\\text{Pr}(M_1 | D)}{\\text{Pr}(M_2 | D)}}_{\\text{posterior odds}} = \\underbrace{\\frac{p(D | M_1)}{p(D | M_2)}}_{\\text{Bayes factor $B_{12}$}} \\, \n\\underbrace{\\frac{\\text{Pr}(M_1)}{\\text{Pr}(M_2)}}_{\\text{prior odds}}\n\\]Bayes factor multiplicative factor updates prior odds posterior odds, ratio (marginal) likelihoods\ntwo models:\n\\[\nB_{12} = \\frac{p(D | M_1)}{p(D | M_2)}\n\\]log-Bayes factor\n\\(\\log B_{12}\\)\nalso called weight evidence \\(M_1\\) \\(M_2\\). Therefore, see \\[\n\\text{log-posterior odds = weight evidence + log-prior odds}\n\\]","code":""},{"path":"bayesian-model-comparison-using-bayes-factors-and-the-bic.html","id":"connection-with-relative-entropy","chapter":"12 Bayesian model comparison using Bayes factors and the BIC","heading":"12.1.1 Connection with relative entropy","text":"expected weight evidence, expectation taken regard one two models,\nfact KL divergence two models (plus minus sign depending direction):\\[\\text{E}_{M_1}( \\log B_{12} ) = KL(M_1 || M_2)\\]\\[\\text{E}_{M_2}( \\log B_{12} ) = -\\text{E}_{M_2}( \\log B_{21} ) = -KL(M_2 || M_1)\\]","code":""},{"path":"bayesian-model-comparison-using-bayes-factors-and-the-bic.html","id":"interpretation-of-and-scale-for-bayes-factor","chapter":"12 Bayesian model comparison using Bayes factors and the BIC","heading":"12.1.2 Interpretation of and scale for Bayes factor","text":"Following Harold Jeffreys (1961) one may interpret strength Bayes factor follows:recently, Kass Raftery (1995) proposed use following slightly modified scale:","code":""},{"path":"bayesian-model-comparison-using-bayes-factors-and-the-bic.html","id":"computing-pd-m-for-simple-and-composite-models","chapter":"12 Bayesian model comparison using Bayes factors and the BIC","heading":"12.1.3 Computing \\(p(D | M)\\) for simple and composite models","text":"Bayes factor need compute \\(p(D | M)\\), turns\ndifferent simple composite models.model called “simple” directly corresponds specific distribution,\nsay, Normal fixed mean variance, Binomial distribution set probability two classes. Thus, simple model point model space described parameters distribution family (e.g.\n\\(\\mu\\) \\(\\sigma^2\\) normal family \\(N(\\mu, \\sigma^2\\)). simple model \\(M\\) density\n\\(p(D | M)\\) corresponds standard likelihood \\(M\\).hand, model “composite” composed simple models. can finite set, can comprised infinite number models.\nexample, Normal given mean unspecified variance, Binomial model unspecified parameter \\(p\\), composite model.\\(M\\) composite model, underlying simple models indexed \nparameter \\(\\theta\\), probability data given model \nobtained marginalisation \\(\\theta\\):\n\\[\n\\begin{split}\np(D | M) &= \\int_{\\theta} p(D | \\theta, M) p(\\theta| M) d\\theta\\\\\n             &= \\int_{\\theta} p(D , \\theta | M) d\\theta\\\\\n\\end{split}\n\\]\n.e. integrate parameter values \\(\\theta\\). resulting probability called marginal likelihood model \\(M\\). Note marginal likelihood appears also denominator Bayes formula!\nmarginal distribution \\(D\\) also called prior predictive distribution given \\(M\\).distribution \\(\\theta\\) strongly concentrated around specific value composite model degenerates simple point model.worked example (form Beta-Binomial distribution) discussed detail Worksheet 6, Question 3.","code":""},{"path":"bayesian-model-comparison-using-bayes-factors-and-the-bic.html","id":"bayes-factor-versus-likelihood-ratio","chapter":"12 Bayesian model comparison using Bayes factors and the BIC","heading":"12.1.4 Bayes factor versus likelihood ratio","text":"\\(M_1\\) \\(M_2\\) simple models Bayes factor identical likelihood ratio two models.However, one two models composite Bayes factor \ngeneralised likelihood ratio differ:\nBayes factor representative composite model \nmodel average simple models indexed \\(\\theta\\), weights\ntaken prior distribution simple models contained \\(M\\). contrast, contrast generalised likelihood ratio statistic representative composite model chosen maximisation!Thus, composite models, Bayes factor equal corresponding generalised likelihood ratio statistic. see next studying BIC approximation, key difference Bayes factor takes account dimension composite models.","code":""},{"path":"bayesian-model-comparison-using-bayes-factors-and-the-bic.html","id":"approximate-computation-of-the-marginal-likelihood-and-of-the-log-bayes-factor","chapter":"12 Bayesian model comparison using Bayes factors and the BIC","heading":"12.2 Approximate computation of the marginal likelihood and of the log-Bayes factor","text":"marginal likelihood Bayes factor can difficult compute\npractise. Therefore, number approximations Bayesian modeling model selection developed\nimportant -called BIC approximation.","code":""},{"path":"bayesian-model-comparison-using-bayes-factors-and-the-bic.html","id":"schwarz-1978-approximation-of-log-marginal-likelihood","chapter":"12 Bayesian model comparison using Bayes factors and the BIC","heading":"12.2.1 Schwarz (1978) approximation of log-marginal likelihood","text":"logarithm marginal likelihood model can approximated\nusing -called BIC approximation (Schwarz 1978) follow:\n\\[\n\\log p(D | M) \\approx l_n^M(\\hat{\\boldsymbol \\theta}_{ML}^{M}) - \\frac{1}{2} d_M \\log n  \n\\]\n\\(d_M\\) dimension model \\(M\\) (number parameters \\(\\boldsymbol \\theta\\) belonging \\(M\\)) \\(n\\) sample size\n\\(\\hat{\\boldsymbol \\theta}_{ML}^{M}\\) MLE.\nsimple model \\(d_M=0\\) \napproximation case marginal likelihood equals likelihood.formula can obtained quadratic approximation likelihood assuming large \\(n\\) prior uniform around MLE.Note approximation maximum log-likelihood minus penalty depends model complexity (measured dimension \\(d\\)), thus example penalised ML! Also note distribution parameter \\(\\theta\\) required approximation.","code":""},{"path":"bayesian-model-comparison-using-bayes-factors-and-the-bic.html","id":"bayesian-information-criterion-bic","chapter":"12 Bayesian model comparison using Bayes factors and the BIC","heading":"12.2.2 Bayesian information criterion (BIC)","text":"BIC (Bayesian information criterion) model \\(M\\) \napproximated log-marginal likelihood times factor -2:\\[\nBIC(M) = -2 l_n^M(\\hat{\\boldsymbol \\theta}_{ML}^{M}) + d_M \\log n\n\\]Thus, comparing models one aimes maximise marginal likelihood , approximation, minimise BIC.reason factor “-2” simply quantity \nscale Wilks log likelihood ratio. people / software packages also use factor “2”.","code":""},{"path":"bayesian-model-comparison-using-bayes-factors-and-the-bic.html","id":"approximating-the-weight-of-evidence-log-bayes-factor-with-bic","chapter":"12 Bayesian model comparison using Bayes factors and the BIC","heading":"12.2.3 Approximating the weight of evidence (log-Bayes factor) with BIC","text":"Using BIC (twice) log-Bayes factor can approximated \n\\[ \n\\begin{split}\n2 \\log B_{12} &\\approx -BIC(M_1) + BIC(M_2) \\\\\n&=2 \\left( l_n^{M_{1}}(\\hat{\\boldsymbol \\theta}_{ML}^{M_{1}}) - l_n^{M_{2}}(\\hat{\\boldsymbol \\theta}_{ML}^{M_{2}}) \\right) - \\log(n) (d_{M_{1}}-d_{M_{2}}) \\\\\n\\end{split}\n\\]\n.e. penalised log-likelihood ratio model \\(M_1\\) vs. \\(M_2\\).","code":""},{"path":"bayesian-model-comparison-using-bayes-factors-and-the-bic.html","id":"model-complexity-and-occams-razor","chapter":"12 Bayesian model comparison using Bayes factors and the BIC","heading":"12.2.4 Model complexity and Occams razor","text":"demonstrated averaging \\(\\theta\\) marginal likelihood effect automatically penalising complex models.Therefore, comparing models using marginal likelihood, Bayes factor, complex model may ranked simpler models.\ncontrast, selecting model maximum likelihood directly, without averaging, model highest number parameters always wins simpler models.Thus, penalisation implicit marginal likelihood much desired prevents overfitting maximum likelihood. principle preferring less complex model called “Occam’s razor”, natural propery Bayes factor.Note comparing models simpler model often preferably complex model, simpler model typically better suited explaining currently observed data well future data, whereas complex model excel fitting current data perform poorly prediction.","code":""},{"path":"false-discovery-rates.html","id":"false-discovery-rates","chapter":"13 False discovery rates","heading":"13 False discovery rates","text":"","code":""},{"path":"false-discovery-rates.html","id":"general-setup","chapter":"13 False discovery rates","heading":"13.1 General setup","text":"","code":""},{"path":"false-discovery-rates.html","id":"overview-1","chapter":"13 False discovery rates","heading":"13.1.1 Overview","text":"chapter introduce False Discovery Rates (FDR) Bayesian method \ndistinguish null model alternative model. closely linked classical\nfrequentist multiple testing procedures.","code":""},{"path":"false-discovery-rates.html","id":"choosing-between-h_0-and-h_a","chapter":"13 False discovery rates","heading":"13.1.2 Choosing between \\(H_0\\) and \\(H_A\\)","text":"consider two models:\\(H_0:\\) null model, density \\(f_0(x)\\) distribution \\(F_0(x)\\)\\(H_A:\\) alternative model, density \\(f_A(x)\\) distribution \\(F_A(x)\\)Aim: given observations \\(x_1, \\ldots, x_n\\) like decide \\(x_i\\) whether\nbelongs \\(H_0\\) \\(H_A\\).done critical decision threshold \\(x_c\\): \\(x_i > x_c\\) \\(x_i\\) called “significant” otherwise called “significant”.classical statistics one widely used approach find decision threshold computing \\(p\\)-values \\(x_i\\)\n(uses null model alternative model), thresholding \\(p\\)-values certain level (say 5%). \\(n\\) large often test modified adjusting \\(p\\)-values threshold (e.g. Bonferroni correction).Note procedure ignores information may alternative model!","code":""},{"path":"false-discovery-rates.html","id":"true-and-false-positives-and-negatives","chapter":"13 False discovery rates","heading":"13.1.3 True and false positives and negatives","text":"decision threshold \\(x_c\\) can distinguish following errors:False positives (FP), “false alarm”, type error: \\(x_i\\) belongs null called “significant”False negative (FN), “miss”, type II error: \\(x_i\\) belongs alternative, called “significant”addition :True positives (TP), “hits”: belongs alternative called “significant”True negatives (TN), “correct rejections”: belongs null called “significant”","code":""},{"path":"false-discovery-rates.html","id":"specificity-and-sensitivity","chapter":"13 False discovery rates","heading":"13.2 Specificity and Sensitivity","text":"counts TP, TN, FN, FP can derive quantities:True Negative Rate TNR, specificity: \\(TNR= \\frac{TN}{TN+FP} = 1- FPR\\) FPR=False Positive Rate = \\(1-\\alpha_I\\)True Positive Rate TPR, sensitivity, power, recall: \\(TPR= \\frac{TP}{TP+FN} = 1- FNR\\) FNR=False negative rate = \\(1-\\alpha_{II}\\)True Positive Rate TPR, sensitivity, power, recall: \\(TPR= \\frac{TP}{TP+FN} = 1- FNR\\) FNR=False negative rate = \\(1-\\alpha_{II}\\)Accuracy: \\(ACC = \\frac{TP+TN}{TP+TN+FP+FN}\\)Accuracy: \\(ACC = \\frac{TP+TN}{TP+TN+FP+FN}\\)Another common way choose decision threshold \\(x_d\\) classical statistics balance sensitivity/power vs. specificity (maximising power specificity, equivalently, minimising false positive false negative rates). ROC curves plot TPR/sensitivity vs. FPR = 1-specificity.","code":""},{"path":"false-discovery-rates.html","id":"fdr-and-fndr","chapter":"13 False discovery rates","heading":"13.3 FDR and FNDR","text":"possible link observed counts TP, FP, TN, FN:False Discovery Rate (FDR): \\(FDR = \\frac{FP}{FP+TP}\\)False Nondiscovery Rate (FNDR): \\(FNDR = \\frac{FN}{TN+FN}\\)Positive predictive value (PPV), True Discovery Rate (TDR), precision: \\(PPV = \\frac{TP}{FP+TP} = 1-FDR\\)Negative predictive value (NPV): \\(NPV = \\frac{TN}{TN+FN} = 1-FNDR\\)order choose decision threshold natural balance FDR FDNR (PPV NPV), minimising FDR FNDR maximising PPV NPV.machine learning common use “precision-recall plots” plot precision (=PPV, TDR)\nvs. recall (=power, sensitivity).","code":""},{"path":"false-discovery-rates.html","id":"bayesian-perspective","chapter":"13 False discovery rates","heading":"13.4 Bayesian perspective","text":"","code":""},{"path":"false-discovery-rates.html","id":"two-component-mixture-model","chapter":"13 False discovery rates","heading":"13.4.1 Two component mixture model","text":"Bayesian perspective problem choosing decision threshold related computing posterior probability\n\\[\\text{Pr}(H_0 | x_i) , \\]\n.e. probability null model given observation \\(x_i\\), equivalently\ncomputing\n\\[\\text{Pr}(H_A | x_i) = 1- \\text{Pr}(H_0 | x_i)\\]\nprobability alternative model given observation \\(x_i\\).done assuming mixture model\n\\[\nf(x) = \\pi_0 f_0(x) + (1-\\pi_0) f_A(x)\n\\]\n\\(\\pi_0 = \\text{Pr}(H_0)\\) prior probability \\(H_0\\) .\n\\(\\pi_A = 1- \\pi_0 = \\text{Pr}(H_A)\\) prior probabiltiy \\(H_A\\).Note weights \\(\\pi_0\\) can fact estimated observations fitting mixture distribution\nobservations \\(x_1, \\ldots, x_n\\) (implies yields form empirical Bayes method).","code":""},{"path":"false-discovery-rates.html","id":"local-fdr","chapter":"13 False discovery rates","heading":"13.4.2 Local FDR","text":"posterior probability null model given data point given \n\\[\\text{Pr}(H_0 | x_i) = \\frac{\\pi_0 f_0(x_i)}{f(x_i)} = LFDR(x_i)\\]\nquantity also known local FDR local False Discovery Rate.given one-sided setup local FDR large (close 1) small \\(x\\), \nbecome close 0 large \\(x\\). common decision rule given thresholding\nlocal false discovery rates: \\(LFDR(x_i) < 0.1\\) \\(x_i\\) called significant.","code":""},{"path":"false-discovery-rates.html","id":"q-values","chapter":"13 False discovery rates","heading":"13.4.3 q-values","text":"correspondence \\(p\\)-values one can also define tail-area based false discovery rates:\n\\[\nFdr(x_i) = \\text{Pr}(H_0 | X > x_i) = \\frac{\\pi_0 F_0(x_i)}{F(x_i)} \n\\]called q-values, simply False Discovery Rates (FDR). Intriguingly, also frequentist\ninterpretation adjusted p-values (using Benjamini-Hochberg adjustment procedure).","code":""},{"path":"false-discovery-rates.html","id":"software","chapter":"13 False discovery rates","heading":"13.5 Software","text":"number R packages compute (local) FDR values:example:locfdrqvaluefdrtooland many .Using FDR values screening especially useful high-dimensional settings\n(e.g. analysing genomic high-throughput data).FDR values Bayesian well frequentist interpretation, providing evidence \ngood classical statistical methods Bayesian interpretation.","code":""},{"path":"optimality-properties-and-summary.html","id":"optimality-properties-and-summary","chapter":"14 Optimality properties and summary","heading":"14 Optimality properties and summary","text":"","code":""},{"path":"optimality-properties-and-summary.html","id":"bayesian-statistics-in-a-nutshell","chapter":"14 Optimality properties and summary","heading":"14.1 Bayesian statistics in a nutshell","text":"Bayesian statistics explicitly models uncertainty parameters\ninterests probabilityIn light new evidence (observed data) uncertainty updated, .e. prior distribution combined likelihood form posterior distributionExample: Beta-Binomial modelBinomial likelihood\\(n\\) observations: \\(x\\) “heads”, \\(n-x\\) “tails”Frequency \\(\\hat\\theta_{ML} = \\frac{x}{n}\\)Beta prior \\(\\theta \\sim \\text{Beta}(\\alpha_0, \\beta_0)\\) mean \\(\\theta_0=\\frac{\\alpha_0}{m}\\) \\(m=\\alpha_0+\\beta_0\\)Beta posterior \\(\\theta | x,n \\sim \\text{Beta}(\\alpha_1, \\beta_1)\\) mean \\(\\theta_1=\\frac{\\alpha_1}{\\alpha_1+\\beta_1}\\)\n\\(\\alpha_1 = \\alpha_0+x\\) \\(\\beta_1=\\beta_0+n-x\\)Update prior mean posterior mean shrinkage MLE:\n\\[\\theta_1 = \\lambda \\theta_0 + (1-\\lambda) \\hat\\theta_{ML}\\] shrinkage intensity \\(\\lambda=\\frac{m}{n+m}\\)\\(m\\) can interpreted prior sample size","code":""},{"path":"optimality-properties-and-summary.html","id":"remarks","chapter":"14 Optimality properties and summary","heading":"14.1.1 Remarks","text":"posterior family prior \\(\\rightarrow\\) conjugate priorIf posterior family prior \\(\\rightarrow\\) conjugate priorIn exponential family Bayesian update mean always expressible\nlinear shrinkage MLEIn exponential family Bayesian update mean always expressible\nlinear shrinkage MLEFor sample size \\(n \\rightarrow \\infty\\) \\(\\lambda \\rightarrow 0\\) \\(\\theta_1 \\rightarrow \\hat\\theta_{ML}\\) (large samples posterior mean = maximum likelihood estimator)sample size \\(n \\rightarrow \\infty\\) \\(\\lambda \\rightarrow 0\\) \\(\\theta_1 \\rightarrow \\hat\\theta_{ML}\\) (large samples posterior mean = maximum likelihood estimator)\\(n \\rightarrow 0\\) \\(\\lambda \\rightarrow 1\\) \\(\\theta_1 \\rightarrow \\hat\\theta_0\\) (data available fall back prior)\\(n \\rightarrow 0\\) \\(\\lambda \\rightarrow 1\\) \\(\\theta_1 \\rightarrow \\hat\\theta_0\\) (data available fall back prior)Note Bayesian estimator biased finite \\(n\\) construction (asymptotically unbiased like MLE).Note Bayesian estimator biased finite \\(n\\) construction (asymptotically unbiased like MLE).","code":""},{"path":"optimality-properties-and-summary.html","id":"advantages","chapter":"14 Optimality properties and summary","heading":"14.1.2 Advantages","text":"adding prior information regularisation properties. important complex models many parameters, e.g., estimation covariance matrix (avoid singularity).adding prior information regularisation properties. important complex models many parameters, e.g., estimation covariance matrix (avoid singularity).improves small-sample accuracy (e.g. MSE)improves small-sample accuracy (e.g. MSE)Bayesian estimators tend better MLE surprising - use\ndata plus extra information!Bayesian estimators tend better MLE surprising - use\ndata plus extra information!Bayesian credible intervals conceptually much simple frequentist\nconfidence intervalsBayesian credible intervals conceptually much simple frequentist\nconfidence intervals","code":""},{"path":"optimality-properties-and-summary.html","id":"frequentist-properties-of-bayesian-estimators","chapter":"14 Optimality properties and summary","heading":"14.2 Frequentist properties of Bayesian estimators","text":"Bayesian point estimator (e.g. posterior mean) can also assessed frequentist properties.First, know , construction, Bayesian estimator \\(\\hat{p}_{\\text{Bayes}}\\) biased \nfinite \\(n\\) even MLE unbiased (bias posterior mean case).Second, intriguingly turns sampling variance Bayes point estimator (confused posterior variance!) can smaller variance MLE. depends choice shrinkage parameter \\(\\lambda\\) also determines posterior variance.result, Bayesian estimators may smaller MSE (=squared bias + variance) ML estimator finite \\(n\\).statistical decision theory called theorem admissibility Bayes rules.\nstates mild conditions every admissible estimation rule (.e. one dominates \nestimators regard expected loss, MSE) fact Bayes estimator prior.Unfortunately, theorem tell prior needed achive optimality, however optimal estimator minimum MSE can often found tuning \\(\\lambda\\).","code":""},{"path":"optimality-properties-and-summary.html","id":"specifying-the-prior-problem-or-advantage","chapter":"14 Optimality properties and summary","heading":"14.3 Specifying the prior — problem or advantage?","text":"Bayesian statistics analysist needs explicit modeling assumptions:Model = data generating process (likelihood) + prior uncertainty (prior distribution)Note alternative statistical methods can often interpreted Bayesian methods assuming specific implicit prior!example, likelihood estimation Binomial model equivalent Bayes estimation using Beta-Binomial model \\(\\text{Beta}(0,0)\\) prior (=Haldane prior).\nHowever, choosing prior explicitly model, interestingly analysts rather use \nflat prior \\(\\text{Beta}(1,1)\\) (=Laplace prior) implicit sample size \\(m=2\\) transformation-invariant prior \\(\\text{Beta}(1/2, 1/2)\\) (=Jeffreys prior) implicit sample size \\(m=1\\) Haldane prior!\\(\\rightarrow\\) aware implicit priors!!Better acknowledge prior used (even implicit!)Writing assumptions enforced Bayesian approach.Specifying prior thus best understood intrinsic part model specification.\nhelps improve inference may ignored lots data.","code":""},{"path":"optimality-properties-and-summary.html","id":"choosing-a-prior","chapter":"14 Optimality properties and summary","heading":"14.4 Choosing a prior","text":"essential Bayesian analysis specify prior\nuncertainty model parameters. Note simply part modeling process!Typically, location prior determines amount bias, precision (inverse variance)\nprior proportional implied sample size prior.seen large \\(n\\) Bayesian estimate converges ML estimate, large \\(n\\) may ignore specifying prior.However, small \\(n\\) essential prior specified. non-Bayesian approaches (interpreted Bayesian perspective) prior still implicit (e.g. uniform prior likelihood estimation).","code":""},{"path":"optimality-properties-and-summary.html","id":"some-guidelines","chapter":"14 Optimality properties and summary","heading":"14.4.1 Some guidelines","text":"questions remains good ways choose prior? Two useful ways (among many others) :Use weakly informative prior (cf. Gelman). means vague idea suitable values \nparameter interest, use corresponding prior (moderate variance) model uncertainty.\nacknowledges uninformative priors aims ensure prior dominate\nlikelihood.Use weakly informative prior (cf. Gelman). means vague idea suitable values \nparameter interest, use corresponding prior (moderate variance) model uncertainty.\nacknowledges uninformative priors aims ensure prior dominate\nlikelihood.Empirical Bayes methods can often used determine one hyperparameters (.e. parameters prior). several ways , one tune shrinkage parameter \\(\\lambda\\) achieve minimum MSE. discuss .Empirical Bayes methods can often used determine one hyperparameters (.e. parameters prior). several ways , one tune shrinkage parameter \\(\\lambda\\) achieve minimum MSE. discuss .contrast, also exists many proposals advocating select -called “uninformative priors”.\nHowever, easly shown, true unformative priors, since prior looks uninformative (.e. “flat”) one coordinate system can informative another — simple consequence rule transformation \nprobability densities. Furthermore, often priors improper, .e. actually probability distributions.\n(many reasons) search “uniformative” priors just futile fact also undesirable (e.g. prior typically also needs act regulariser)!Instead, specifying prior needs viewed part modelling process, specification prior\nintegral specification likelihood.","code":""},{"path":"optimality-properties-and-summary.html","id":"jeffreys-prior","chapter":"14 Optimality properties and summary","heading":"14.4.2 Jeffreys prior","text":"order complement discussion non-informative priors now look (briefly) proposal Jeffreys (1946).Specifically, prior constructed expected Fisher observation using log-likelihood function thus promises automatic construction objective uninformative priors:\n\\[\np(\\boldsymbol \\theta) \\propto \\sqrt{\\det \\boldsymbol ^{\\text{Fisher}}(\\boldsymbol \\theta)}\n\\]reasoning underlying prior invariance transformation coordinate system parameters.Beta-Binomial model Jeffreys prior corresponds \\(\\text{Beta}(\\frac{1}{2}, \\frac{1}{2})\\).Normal-Normal model corresponds flat improper prior \\(p(\\mu) =1\\).Inverse-Gamma-Norma model Jeffreys prior improper prior \\(p(\\sigma^2) = \\frac{1}{\\sigma^2}\\).already illustrates main problem type prior – namely often improper prior.Another issue Jeffreys priors usually conjugate complicates update prior posterior. alternative Jeffreys prior reference prior developed Bernardo (1979).","code":""},{"path":"optimality-properties-and-summary.html","id":"optimality-of-bayesian-inference","chapter":"14 Optimality properties and summary","heading":"14.5 Optimality of Bayesian inference","text":"optimality Bayesian model making use full model specification (likelihood plus prior) can shown number different perspectives. Correspondingly,\nmany theorems prove (least indicate) optimality:Richard Cox’s theorem: aim generalise classic logic inevitably leads Bayesian inference.Richard Cox’s theorem: aim generalise classic logic inevitably leads Bayesian inference.Entropy perspective: Bayesian inference consequence minimal information update new information arrives form observationsEntropy perspective: Bayesian inference consequence minimal information update new information arrives form observationsde Finetti’s representation theorem: joint distribution exchangeable sequences can viewed posterior distributions computed Bayes theorem)de Finetti’s representation theorem: joint distribution exchangeable sequences can viewed posterior distributions computed Bayes theorem)Frequentist decision theory: admissible decision rules Bayes rules!\n(admissible = always better methods!)Frequentist decision theory: admissible decision rules Bayes rules!\n(admissible = always better methods!)Remark: also excludes (somewhat esoteric) suggestions \npropagating uncertainty (e.g. Fuzzy Logic, imprecise probabilties, etc).","code":""},{"path":"optimality-properties-and-summary.html","id":"conclusion","chapter":"14 Optimality properties and summary","heading":"14.6 Conclusion","text":"Bayesian statistics offers coherent framework statistical learning data, methods forestimationtestingmodel buildingThere number theorems show “optimal” estimators (defined various ways) Bayesian.conceptually simple — can computationally involved!provides coherent generalisation classical TRUE/FALSE logic (therefore suffer inconsistencies prevalent frequentist statistics).Bayesian statistics non-asymptotic theory, works sample size.\nAsympotically (large \\(n\\)) consistent converges true model (like ML!).\nBayesian reasoning can also applied events take place — assumption hypothetical infinitely many repetitions frequentist statistics needed.Moreover, many classical (frequentist) procedures may viewed approximations Bayesian methods estimators, using classical approaches correct application domain perfectly line Bayesian framework.Bayesian estimation inference also automatically regularises (via prior) important complex models problem overfitting.","code":""},{"path":"optimality-properties-and-summary.html","id":"current-directions-of-research","chapter":"14 Optimality properties and summary","heading":"14.6.1 Current directions of research","text":"example: connection Bayesian models algorithmic models widely used machine learning (neural networks, deep learning, convolutional networks, ensemble methods, XGBoost etc).models optimal (Bayesian sense)? Can learn something highly complex, non-parametric statistical models?effective Bayesian learning parameter-rich models? terms computational statistical efficiency.","code":""},{"path":"overview-over-regression-modelling.html","id":"overview-over-regression-modelling","chapter":"15 Overview over regression modelling","heading":"15 Overview over regression modelling","text":"","code":""},{"path":"overview-over-regression-modelling.html","id":"general-setup-1","chapter":"15 Overview over regression modelling","heading":"15.1 General setup","text":"\\(y\\): response variable, also known outcome label\\(y\\): response variable, also known outcome label\\(x_1, x_2, x_3, \\ldots, x_d\\): predictor variables, also known covariates covariables\\(x_1, x_2, x_3, \\ldots, x_d\\): predictor variables, also known covariates covariablesThe relationship outcomes predictor variables assumed follow\n\\[\ny = f(x_1,x_2,\\dots,x_d) + \\varepsilon\n\\]\n\\(f\\) regression function (density) \\(\\varepsilon\\) represents noise.relationship outcomes predictor variables assumed follow\n\\[\ny = f(x_1,x_2,\\dots,x_d) + \\varepsilon\n\\]\n\\(f\\) regression function (density) \\(\\varepsilon\\) represents noise.","code":""},{"path":"overview-over-regression-modelling.html","id":"objectives","chapter":"15 Overview over regression modelling","heading":"15.2 Objectives","text":"Understand relationship response \\(y\\) predictor variables \\(x_i\\) learning regression function \\(f\\) observed data (training data). estimated regression function \\(\\hat{f}\\).Understand relationship response \\(y\\) predictor variables \\(x_i\\) learning regression function \\(f\\) observed data (training data). estimated regression function \\(\\hat{f}\\).Prediction outcomes\n\\[\\underbrace{\\hat{y}}_{\\substack{\\text{predicted response} \\\\ \\text{using fitted $\\hat{f}$}}} = \\hat{f}(x_1,x_2,\\dots,x_d)\\]\ninstead fitted function \\(\\hat{f}\\) known regression function \\(f\\) used denote \n\\[\\underbrace{y^{\\star}}_{\\substack{\\text{predicted response} \\\\ \\text{using known $f$}}} = f(x_1,x_2,\\dots,x_d)\n\\]Prediction outcomes\n\\[\\underbrace{\\hat{y}}_{\\substack{\\text{predicted response} \\\\ \\text{using fitted $\\hat{f}$}}} = \\hat{f}(x_1,x_2,\\dots,x_d)\\]instead fitted function \\(\\hat{f}\\) known regression function \\(f\\) used denote \n\\[\\underbrace{y^{\\star}}_{\\substack{\\text{predicted response} \\\\ \\text{using known $f$}}} = f(x_1,x_2,\\dots,x_d)\n\\]Variable importance\ncovariates relevant predicting outcome?\nallows better understand data model\\(\\rightarrow\\) variable selection\n(build simpler model predictive capability)\nVariable importancewhich covariates relevant predicting outcome?allows better understand data model\\(\\rightarrow\\) variable selection\n(build simpler model predictive capability)","code":""},{"path":"overview-over-regression-modelling.html","id":"regression-as-a-form-of-supervised-learning","chapter":"15 Overview over regression modelling","heading":"15.3 Regression as a form of supervised learning","text":"Regression modeling special case supervised learning.supervised learning make use labeled data, .e. \\(\\boldsymbol x_i\\) associated label\n\\(y_i\\). Thus, data consists pairs \\((\\boldsymbol x_1, y_1),(\\boldsymbol x_2 ,y_2),\\dots,(\\boldsymbol x_n ,y_n)\\).supervision part supervised learning refers fact labels given.regression typically label \\(y_i\\) continuous called response.hand, label \\(y_i\\) discrete/categorical supervised learning called classification.\\[\\begin{align*}\n\\begin{array}{ll}\n\\\\\n\\text{Supervised Learning}\\\\\n\\\\\n\\end{array}\n\\begin{array}{ll}\n\\longrightarrow \\text{Discrete } y\\\\\n\\\\\n\\longrightarrow \\text{Continuous } y\\\\\n\\end{array}\n\\begin{array}{ll}\n\\longrightarrow \\text{Classification Methods}\\\\\n\\\\\n\\longrightarrow \\text{Regression Methods}\\\\\n\\end{array}\n\\end{align*}\\]Another important type statistical learning unsupervised learning labels \\(y\\)\ninferred data \\(\\boldsymbol x\\) (also known clustering). Furthermore, also semi-supervised learning labels partly known.Note regression models (e.g. logistic regression) discrete\nresponse performing classification, one may argue “supervised learning”=“generalised regression”.","code":""},{"path":"overview-over-regression-modelling.html","id":"various-regression-models-used-in-statistics","chapter":"15 Overview over regression modelling","heading":"15.4 Various regression models used in statistics","text":"course study linear multiple regression.\nHowever, aware linear model fact just special cases much general\nregression approaches.General regression model:\n\\[y = f(x_1,\\dots,x_d) + \\text{\"noise\"}\\]function \\(f\\) estimated nonparametrically\n- splines\n- Gaussian processesThe function \\(f\\) estimated nonparametrically\n- splines\n- Gaussian processesGeneralised Additive Models (GAM):\n- function \\(f\\) assumed sum individual functions \\(f_i(x_i)\\)Generalised Additive Models (GAM):\n- function \\(f\\) assumed sum individual functions \\(f_i(x_i)\\)Generalised Linear Models (GLM):\n- \\(f\\) transformed linear predictor \\(h(\\sum b_i x_i)\\), noise assumed exponential familyGeneralised Linear Models (GLM):\n- \\(f\\) transformed linear predictor \\(h(\\sum b_i x_i)\\), noise assumed exponential familyLinear Model (LM):\n- linear predictor \\(\\sum b_i x_i\\), normal noiseLinear Model (LM):\n- linear predictor \\(\\sum b_i x_i\\), normal noiseIn R linear model implemented function lm(), generalised linear models\nfunction glm(). Generalised additive models available package\n“mgcv”.following focus linear regression model continuous response.","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"16 Linear Regression","heading":"16 Linear Regression","text":"","code":""},{"path":"linear-regression.html","id":"the-linear-regression-model","chapter":"16 Linear Regression","heading":"16.1 The linear regression model","text":"module assume \\(f\\) linear function:\n\\[f(x_1, \\ldots, x_d) = \\beta_0 + \\sum^{d}_{j=1} \\beta_j x_j = y^{\\star}\\]vector notation:\n\\[\nf(\\boldsymbol x) = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x= y^{\\star} \n\\]\n\\(\\boldsymbol \\beta=\\begin{pmatrix} \\beta_1 \\\\ \\vdots \\\\ \\beta_d \\end{pmatrix}\\) \\(\\boldsymbol x=\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_d \\end{pmatrix}\\)Therefore, linear regression model \n\\[\n\\begin{split}\ny &= \\beta_0 + \\sum^{d}_{j=1} \\beta_j x_j + \\varepsilon\\\\\n  &= \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x+\\varepsilon \\\\\n  &= y^{\\star} +\\varepsilon\n\\end{split}\n\\]\n:\\(\\beta_0\\) intercept\\(\\boldsymbol \\beta= (\\beta_1,\\ldots,\\beta_d)^T\\) regression coefficients\\(\\boldsymbol x= (x_1,\\ldots,x_d)^T\\) predictor vector containing predictor variables","code":""},{"path":"linear-regression.html","id":"interpretation-of-regression-coefficients-and-intercept","chapter":"16 Linear Regression","heading":"16.2 Interpretation of regression coefficients and intercept","text":"regression coefficient \\(\\beta_i\\) corresponds slope (first partial derivative) regression function direction \\(x_i\\). words,\ngradient \\(f(\\boldsymbol x)\\) regression coefficients: \\(\\nabla f(\\boldsymbol x) = \\boldsymbol \\beta\\)intercept \\(\\beta_0\\) offset origin (\\(x_1=x_2=\\ldots=x_d=0\\)):","code":""},{"path":"linear-regression.html","id":"different-types-of-linear-regression","chapter":"16 Linear Regression","heading":"16.3 Different types of linear regression:","text":"Simple linear regression: \\(y=\\beta_0 + \\beta x + \\varepsilon\\) (=single predictor)Multiple linear regression: \\(y =\\beta_0 + \\sum^{d}_{j=1} \\beta_j x_j + \\varepsilon\\) (= multiple predictor variables)Multivariate regression: multivariate response \\(\\boldsymbol y\\)","code":""},{"path":"linear-regression.html","id":"distributional-assumptions-and-properties","chapter":"16 Linear Regression","heading":"16.4 Distributional assumptions and properties","text":"General assumptions:treat \\(y\\) \\(x_1, \\ldots, x_d\\) primary observables can described random variables.\\(\\beta_0, \\boldsymbol \\beta\\) parameters inferred observations\n\\(y\\) \\(x_1, \\ldots,x_d\\).\\(\\beta_0, \\boldsymbol \\beta\\) parameters inferred observations\n\\(y\\) \\(x_1, \\ldots,x_d\\).Specifically, assume response predictors mean (cov)variance:\nResponse:\\(\\text{E}(y) = \\mu_y\\)\\(\\text{Var}(y) = \\sigma_y^2\\)\nvariance response \\(\\text{Var}(y)\\) also called total variation .\nPredictors:\\(\\text{E}(x_i) = \\mu_{x_i}\\) (\\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu_{\\boldsymbol x}\\))\\(\\text{Var}(x_i) = \\sigma^2_{x_i}\\) \\(\\text{Cor}(x_i, x_j) = \\rho_{ij}\\) (\\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma_{\\boldsymbol x}\\))\nsignal variance \\(\\text{Var}(y^{\\star})=\\text{Var}(\\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x) = \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol \\beta\\) also called explained variation.\nResponse:\\(\\text{E}(y) = \\mu_y\\)\\(\\text{Var}(y) = \\sigma_y^2\\)\nvariance response \\(\\text{Var}(y)\\) also called total variation .Response:\\(\\text{E}(y) = \\mu_y\\)\\(\\text{Var}(y) = \\sigma_y^2\\)\nvariance response \\(\\text{Var}(y)\\) also called total variation .Predictors:\\(\\text{E}(x_i) = \\mu_{x_i}\\) (\\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu_{\\boldsymbol x}\\))\\(\\text{Var}(x_i) = \\sigma^2_{x_i}\\) \\(\\text{Cor}(x_i, x_j) = \\rho_{ij}\\) (\\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma_{\\boldsymbol x}\\))\nsignal variance \\(\\text{Var}(y^{\\star})=\\text{Var}(\\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x) = \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol \\beta\\) also called explained variation.Predictors:\\(\\text{E}(x_i) = \\mu_{x_i}\\) (\\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu_{\\boldsymbol x}\\))\\(\\text{Var}(x_i) = \\sigma^2_{x_i}\\) \\(\\text{Cor}(x_i, x_j) = \\rho_{ij}\\) (\\(\\text{Var}(\\boldsymbol x) = \\boldsymbol \\Sigma_{\\boldsymbol x}\\))\nsignal variance \\(\\text{Var}(y^{\\star})=\\text{Var}(\\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x) = \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x} \\boldsymbol \\beta\\) also called explained variation.assume \\(y\\) \\(\\boldsymbol x\\) jointly distributed correlation \\(\\text{Cor}(y, x_j) = \\rho_{y,x_{j}}\\)\npredictor variable \\(x_j\\) response \\(y\\).assume \\(y\\) \\(\\boldsymbol x\\) jointly distributed correlation \\(\\text{Cor}(y, x_j) = \\rho_{y,x_{j}}\\)\npredictor variable \\(x_j\\) response \\(y\\).contrast \\(y\\) \\(\\boldsymbol x\\) noise variable \\(\\varepsilon\\) indirectly observed\nvia difference \\(\\varepsilon = y - y^{\\star}\\). denote mean variance noise \n\\(\\text{E}(\\varepsilon)\\) \\(\\text{Var}(\\varepsilon)\\).\nnoise variance \\(\\text{Var}(\\varepsilon)\\) also called unexplained variation.contrast \\(y\\) \\(\\boldsymbol x\\) noise variable \\(\\varepsilon\\) indirectly observed\nvia difference \\(\\varepsilon = y - y^{\\star}\\). denote mean variance noise \n\\(\\text{E}(\\varepsilon)\\) \\(\\text{Var}(\\varepsilon)\\).\nnoise variance \\(\\text{Var}(\\varepsilon)\\) also called unexplained variation.Identifiability assumptions:statistical analysis like able separate signal (\\(y^{\\star}\\))\nnoise (\\(\\varepsilon\\)). achieve require distributional assumptions\nensure identifiability avoid confounding:Assumption 1: \\(\\varepsilon\\) \\(y^{\\star}\\) \nindependent. implies \\(\\text{Var}(y) = \\text{Var}(y^{\\star}) + \\text{Var}(\\varepsilon)\\), equivalently\n\\(\\text{Var}(\\varepsilon) = \\text{Var}(y) - \\text{Var}(y^{\\star})\\).\nThus, assumption implies decomposition variance, .e. \ntotal variation \\(\\text{Var}(y)\\) equals sum explained variation\\(\\text{Var}(y^{\\star})\\)\nunexplained variation\\(\\text{Var}(\\varepsilon)\\).Assumption 1: \\(\\varepsilon\\) \\(y^{\\star}\\) \nindependent. implies \\(\\text{Var}(y) = \\text{Var}(y^{\\star}) + \\text{Var}(\\varepsilon)\\), equivalently\n\\(\\text{Var}(\\varepsilon) = \\text{Var}(y) - \\text{Var}(y^{\\star})\\).Thus, assumption implies decomposition variance, .e. \ntotal variation \\(\\text{Var}(y)\\) equals sum explained variation\\(\\text{Var}(y^{\\star})\\)\nunexplained variation\\(\\text{Var}(\\varepsilon)\\).Assumption 2: \\(\\text{E}(\\varepsilon)=0\\). allows identify intercept \\(\\beta_0\\) implies \\(\\text{E}(y) = \\text{E}(y^{\\star})\\).Assumption 2: \\(\\text{E}(\\varepsilon)=0\\). allows identify intercept \\(\\beta_0\\) implies \\(\\text{E}(y) = \\text{E}(y^{\\star})\\).Optional assumptions (often always):noise \\(\\varepsilon\\) normally distributedThe response \\(y\\) predictor\nvariables \\(x_i\\) continuous variablesThe response predictor variables jointly normally distributedFurther properties:result independence assumption 1) can choose two three\nvariances freely:\ngenerative perspective choose signal variance \\(\\text{Var}(y^{\\star})\\) (equivalently variances \\(\\text{Var}(x_j)\\)) noise variance \\(\\text{Var}(\\varepsilon)\\), \nvariance response \\(\\text{Var}(y)\\) follows.\nobservational perspective observe variance reponse\n\\(\\text{Var}(y)\\) variances \\(\\text{Var}(x_j)\\), error variance \\(\\text{Var}(\\varepsilon)\\) follows.\ngenerative perspective choose signal variance \\(\\text{Var}(y^{\\star})\\) (equivalently variances \\(\\text{Var}(x_j)\\)) noise variance \\(\\text{Var}(\\varepsilon)\\), \nvariance response \\(\\text{Var}(y)\\) follows.observational perspective observe variance reponse\n\\(\\text{Var}(y)\\) variances \\(\\text{Var}(x_j)\\), error variance \\(\\text{Var}(\\varepsilon)\\) follows.see later regression coefficients \\(\\beta_j\\) depend correlations response \\(y\\) predictor variables \\(x_j\\).\nThus, choice regression coefficients implies specific correlation pattern,\nvice vera (fact, use correlation pattern infer regression coefficient data!).","code":""},{"path":"linear-regression.html","id":"regression-in-data-matrix-notation","chapter":"16 Linear Regression","heading":"16.5 Regression in data matrix notation","text":"can also write regression terms actual observed data (rather random variables):Data matrix predictors:\n\\[\\boldsymbol X= \\begin{pmatrix} x_{11} & \\dots & x_{1d} \\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} & \\dots & x_{nd} \\end{pmatrix}\\]Note statistics convention: \\(n\\) rows \\(\\boldsymbol X\\) contain samples, \\(d\\) columns contain variables.Response data vector: \\((y_1,\\dots,y_n)^T = \\boldsymbol y\\)regression equation written data matrix notation:\\[\\underbrace{\\boldsymbol y}_{n\\times 1} = \\underbrace{\\boldsymbol 1_n \\beta_0}_{n\\times 1} + \\underbrace{\\boldsymbol X}_{n \\times d} \\underbrace{\\boldsymbol \\beta}_{d\\times 1}+\\underbrace{\\boldsymbol \\varepsilon}_{\\underbrace{n\\times 1}_{\\text{residuals}}}\\]\\(\\boldsymbol 1_n = \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\\) column vector length \\(n\\) (size \\(n \\times 1\\)).Note regression coefficients now multiplied \ndata matrix (compare original vector notation transpose regression coefficients come \nvector predictors).observed noise values (.e. realisations \\(\\varepsilon\\)) called residuals.","code":""},{"path":"linear-regression.html","id":"centering-and-vanishing-of-the-intercept-beta_0","chapter":"16 Linear Regression","heading":"16.6 Centering and vanishing of the intercept \\(\\beta_0\\)","text":"\\(\\boldsymbol x\\) \\(y\\) centered, .e. \\(\\text{E}(\\boldsymbol x) = \\boldsymbol \\mu_{\\boldsymbol x}= 0\\) \\(\\text{E}(y) = \\mu_{y} = 0\\)\nintercept \\(\\beta_0\\) disappears:regression equation \n\\[y=\\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x+\\varepsilon\\]\n\\(E(\\varepsilon)\\). Taking expectation sides get\n\\(\\mu_{y} = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x}\\)\ntherefore\n\\[\n\\beta_0 = \\mu_{y}- \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x}\n\\]\nequal zero means response predictors vanish.\nConversely, assume intercept vanishes (\\(\\beta_0=0\\)) possible general \\(\\boldsymbol \\beta\\) \\(\\boldsymbol \\mu_{\\boldsymbol x}=0\\) \\(\\mu_{y}=0\\).Thus, linear model always possible transform \\(y\\) \\(\\boldsymbol x\\) (data \\(\\boldsymbol y\\) \\(\\boldsymbol X\\)) intercept vanishes!\\(\\Rightarrow\\) therefore often set \\(\\beta_0=0\\).","code":""},{"path":"linear-regression.html","id":"regression-objectives-for-linear-model","chapter":"16 Linear Regression","heading":"16.7 Regression objectives for linear model","text":"Understand functional relationship: find estimates intercept (\\(\\hat{\\beta}_0\\)) regression coefficients (\\(\\hat{\\beta}_j\\))Understand functional relationship: find estimates intercept (\\(\\hat{\\beta}_0\\)) regression coefficients (\\(\\hat{\\beta}_j\\))Prediction:\nKnown coefficients \\(\\beta_0\\) \\(\\boldsymbol \\beta\\):\n\\(y^{\\star} = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x\\)\nEstimated coefficients \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}\\) (note “hat”!):\n\\(\\hat{y} =\\hat{\\beta}_0 + \\sum^{d}_{j=1} \\hat{\\beta}_j x_j = \\hat{\\beta}_0 + \\hat{\\boldsymbol \\beta}^T \\boldsymbol x\\)\nAlso find corresponding prediction errors!Known coefficients \\(\\beta_0\\) \\(\\boldsymbol \\beta\\):\n\\(y^{\\star} = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x\\)Estimated coefficients \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}\\) (note “hat”!):\n\\(\\hat{y} =\\hat{\\beta}_0 + \\sum^{d}_{j=1} \\hat{\\beta}_j x_j = \\hat{\\beta}_0 + \\hat{\\boldsymbol \\beta}^T \\boldsymbol x\\)Also find corresponding prediction errors!Variable importance: predictors \\(x_j\\) relevant?\\(\\rightarrow\\) test whether \\(\\beta_j=0\\)\\(\\rightarrow\\) find measures variable importance\nRemark: see \\(\\beta_j\\) \\(\\hat{\\beta}_j\\) measure importance!Variable importance: predictors \\(x_j\\) relevant?\\(\\rightarrow\\) test whether \\(\\beta_j=0\\)\\(\\rightarrow\\) find measures variable importance\nRemark: see \\(\\beta_j\\) \\(\\hat{\\beta}_j\\) measure importance!","code":""},{"path":"estimating-regression-coefficients.html","id":"estimating-regression-coefficients","chapter":"17 Estimating regression coefficients","heading":"17 Estimating regression coefficients","text":"chapter discuss various ways estimate regression coefficients. First, discuss estimation Ordinary Least Squares (OLS)\nminimising residual sum squares. yields famous\nGauss estimator. Second, derive estimates regression coefficients\nusing methods maximum likelihood assuming normal errors. also leads Gauss estimator. Third, show coefficients \nlinear regression can written interpreted terms two\ncovariance matrices Gauss estimator regression coefficients\nplug-estimator using MLEs covariance matrices.\nFurthermore, show (population version) Gauss estimator\ncan also derived finding best linear predictor conditioning.\nFinally, discuss special cases regression coefficients relationship\nmarginal correlation.","code":""},{"path":"estimating-regression-coefficients.html","id":"ordinary-least-squares-ols-estimator-of-regression-coefficients","chapter":"17 Estimating regression coefficients","heading":"17.1 Ordinary Least Squares (OLS) estimator of regression coefficients","text":"Now show classic way (Gauss 1809; Legendre 1805) estimate regression coefficients method \nordinary least squares (OLS).Idea: choose regression coefficients minimise squared error observations prediction.data matrix notation (note assume \\(\\beta_0=0\\) thus centered data \\(\\boldsymbol X\\) \\(\\boldsymbol y\\)):\\[\\text{RSS}(\\boldsymbol \\beta)=(\\boldsymbol y-\\boldsymbol X\\boldsymbol \\beta)^T(\\boldsymbol y-\\boldsymbol X\\boldsymbol \\beta)\\]RSS abbreviation “Residual Sum Squares” function \\(\\boldsymbol \\beta\\).\nMinimising RSS yields OLS estimate:\\[\\widehat{\\boldsymbol \\beta}_{\\text{OLS}}=\\underset{\\boldsymbol \\beta}{\\arg \\min}\\, \\text{RSS}(\\boldsymbol \\beta)\\]\\[\\text{RSS}(\\boldsymbol \\beta) = \\boldsymbol y^T \\boldsymbol y- 2 \\boldsymbol \\beta^T \\boldsymbol X^T \\boldsymbol y+ \\boldsymbol \\beta^T \\boldsymbol X^T \\boldsymbol X\\boldsymbol \\beta\\]Gradient:\n\\[\\nabla \\text{RSS}(\\boldsymbol \\beta) = -2\\boldsymbol X^T \\boldsymbol y+ 2\\boldsymbol X^T \\boldsymbol X\\boldsymbol \\beta\\]\\[\\nabla \\text{RSS}(\\widehat{\\boldsymbol \\beta}) = 0 \\longrightarrow \\boldsymbol X^T \\boldsymbol y= \\boldsymbol X^T\\boldsymbol X\\widehat{\\boldsymbol \\beta}\\]\\[\\Longrightarrow \\widehat{\\boldsymbol \\beta}_{\\text{OLS}} = \\left(\\boldsymbol X^T\\boldsymbol X\\right)^{-1} \\boldsymbol X^T \\boldsymbol y\\]Note similarities procedure maximum likelihood (ML) estimation (minimisation instead maximisation)! fact, \nsee next chance OLS indeed special case ML!\nalso implies OLS generally good method — sample size \\(n\\) large!Gauss’ estimator fundamental statistics worthwile memorise !","code":""},{"path":"estimating-regression-coefficients.html","id":"maximum-likelihood-estimation-of-regression-coefficients","chapter":"17 Estimating regression coefficients","heading":"17.2 Maximum likelihood estimation of regression coefficients","text":"now show estimate regression coefficients using method\nmaximum likelihood. second method derive \\(\\hat{\\boldsymbol \\beta}\\).recall basic regression equation\n\\[\ny = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x+ \\varepsilon\n\\]\n\\(\\text{E}(\\varepsilon)=0\\) observed data \\(y_1, \\ldots, y_n\\) \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\).\nintercept identified \n\\[\n\\beta_0 = \\mu_{y}- \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x}\n\\]\ncan solve noise variable\n\\[\n\\varepsilon = (y- \\mu_{y}) - \\boldsymbol \\beta^T (\\boldsymbol x-\\boldsymbol \\mu_{\\boldsymbol x})\n\\]Assuming joint (multivariate) normality response \\(y\\) \\(\\boldsymbol x\\) get MLEs \nrespective means (co)variances:\\(\\hat{\\mu}_y=\\hat{\\text{E}}(y)= \\frac{1}{n}\\sum^n_{=1} y_i\\)\\(\\hat{\\sigma}^2_y=\\widehat{\\text{Var}}(y)= \\frac{1}{n}\\sum^n_{=1} (y_i - \\hat{\\mu}_y)^2\\)\\(\\hat{\\boldsymbol \\mu}_{\\boldsymbol x}=\\hat{\\text{E}}(\\boldsymbol x)= \\frac{1}{n}\\sum^n_{=1} \\boldsymbol x_i\\)\\(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}=\\widehat{\\text{Var}}(\\boldsymbol x)= \\frac{1}{n}\\sum^n_{=1} (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_{\\boldsymbol x}) (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_{\\boldsymbol x})^T\\)\\(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}=\\widehat{\\text{Cov}}(\\boldsymbol x, y)= \\frac{1}{n}\\sum^n_{=1} (\\boldsymbol x_i-\\hat{\\boldsymbol \\mu}_{\\boldsymbol x}) (y_i - \\hat{\\mu}_y)\\)noise \\(\\varepsilon \\sim N(0, \\sigma^2_{\\varepsilon})\\)\nnormally distributed mean 0 variance \\(\\text{Var}(\\varepsilon) = \\sigma^2_{\\varepsilon}\\).\nobtained MLEs \\(\\hat{\\mu}_y\\) \\(\\hat{\\boldsymbol \\mu}_{\\boldsymbol x}\\) corresponding (indirect) observations given \n\\[\n(y_i- \\hat{\\mu}_{y}) - \\boldsymbol \\beta^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_{\\boldsymbol x})\n\\]\nleads normal log-likelihood function\n\\[\n\\begin{split}\n\\log L(\\boldsymbol \\beta,\\sigma^2_{\\varepsilon} ) \n&= -\\frac{n}{2} \\log \\sigma^2_{\\varepsilon} - \\frac{1}{2\\sigma^2_{\\varepsilon}} \\sum^n_{=1} \\left((y_i- \\hat{\\mu}_{y}) - \\boldsymbol \\beta^T (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_{\\boldsymbol x})\\right)^2\\\\\n\\end{split}\n\\]now need maximise log-likelihood obtain MLEs \\(\\sigma^2_{\\varepsilon}\\) \\(\\boldsymbol \\beta\\)!Note residual sum squares appears log-likelihood function\n(minus sign), implies ML assuming normal distribution recover\nOLS estimator regression coefficients! OLS special case ML !","code":""},{"path":"estimating-regression-coefficients.html","id":"detailed-derivation-of-the-mles","chapter":"17 Estimating regression coefficients","heading":"17.2.1 Detailed derivation of the MLEs","text":"gradient regard \\(\\boldsymbol \\beta\\) \n\\[\n\\begin{split}\n\\nabla_{\\boldsymbol \\beta} \\log L(\\boldsymbol \\beta,\\sigma^2_{\\varepsilon} ) &= \\frac{1}{\\sigma^2_{\\varepsilon}} \\sum^n_{=1} \\left((\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_{\\boldsymbol x} ) (y_i - \\hat{\\mu}_{y}) - (\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_{\\boldsymbol x} )(\\boldsymbol x_i -\\hat{\\boldsymbol \\mu}_{\\boldsymbol x})^T \\boldsymbol \\beta\\right)  \\\\\n &= \\frac{n}{\\sigma^2_{\\varepsilon}} \\left( \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy} -  \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}\\boldsymbol \\beta\\right)\\\\\n\\end{split}\n\\]\nSetting equal zero yields Gauss estimator\n\\[\n\\hat{\\boldsymbol \\beta} = \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}  \n\\]\nplugin get MLE \\(\\beta_0\\)\n\n\\[\n\\hat{\\beta}_0 = \\hat{\\mu}_{y}- \\hat{\\boldsymbol \\beta}^T \\hat{\\boldsymbol \\mu}_{\\boldsymbol x}\n\\]\nTaking derivative \\(\\log L(\\hat{\\boldsymbol \\beta},\\sigma^2_{\\varepsilon} )\\) regard \\(\\sigma^2_{\\varepsilon}\\) yields\n\\[\n\\frac{\\partial}{\\partial \\sigma^2_{\\varepsilon}} \\log L(\\hat{\\boldsymbol \\beta},\\sigma^2_{\\varepsilon} ) = -\\frac{n}{2\\sigma^2_{\\varepsilon}} +\\frac{1}{2\\sigma^4_{\\varepsilon}} \\sum^n_{=1}  (y_i-\\hat{y}_i)^2\n\\]\n\\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\boldsymbol \\beta}^T \\boldsymbol x_i\\) residuals \\(y_i-\\hat{y}_i\\) resulting fitted linear model.\nleads MLE noise variance\n\\[\n\\widehat{\\sigma^2_{\\varepsilon}}  = \\frac{1}{n}\\sum^n_{=1}(y_i-\\hat{y}_i)^2\n\\]Note MLE \\(\\widehat{\\sigma^2_{\\varepsilon}}\\) biased estimate \\(\\sigma^2_{\\varepsilon}\\). unbiased estimate \\(\\frac{1}{n-d-1}\\sum^n_{=1}(y_i-\\hat{y}_i)^2\\), \\(d\\) dimension \\(\\boldsymbol \\beta\\)\n(.e. number predictors).","code":""},{"path":"estimating-regression-coefficients.html","id":"asymptotics","chapter":"17 Estimating regression coefficients","heading":"17.2.2 Asymptotics","text":"advantage using maximum likelihood also get (asympotic) variance associated estimator typically can also assume asymptotic normality.Specifically, \\(\\hat{\\boldsymbol \\beta}\\) get via observed Fisher information MLE\nasymptotic estimator variance\n\\[\\widehat{\\text{Var}}(\\widehat{\\boldsymbol \\beta})=\\frac{1}{n} \\widehat{\\sigma^2_{\\varepsilon}} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1}\\]\nSimilarly, \\(\\hat{\\beta}_0\\) \n\\[\n\\widehat{\\text{Var}}(\\widehat{\\beta}_0)=\\frac{1}{n} \\widehat{\\sigma^2_{\\varepsilon}} (1 + \\hat{\\boldsymbol \\mu}^T \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1} \\hat{\\boldsymbol \\mu})\n\\]finite sample size \\(n\\) known \\(\\text{Var}(\\varepsilon)\\) one can show variances \n\\[\\text{Var}(\\widehat{\\boldsymbol \\beta})=\\frac{1}{n} \\sigma^2_{\\varepsilon}\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1}\\]\n\n\\[\n\\text{Var}(\\widehat{\\beta}_0)=\\frac{1}{n} \\sigma^2_{\\varepsilon} (1 + \\hat{\\boldsymbol \\mu}^T_{\\boldsymbol x} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1} \\hat{\\boldsymbol \\mu}_{\\boldsymbol x})\n\\]\nregression coefficients intercept normally distributed according \n\\[\n\\widehat{\\boldsymbol \\beta} \\sim N_d(\\boldsymbol \\beta, \\text{Var}(\\widehat{\\boldsymbol \\beta}))\n\\]\n\n\\[\n\\widehat{\\beta}_0 \\sim N(\\beta_0, \\text{Var}(\\widehat{\\beta}_0))\n\\]may use test whether whether \\(\\beta_j = 0\\) \\(\\beta_0 = 0\\).","code":""},{"path":"estimating-regression-coefficients.html","id":"covariance-plug-in-estimator-of-regression-coefficients","chapter":"17 Estimating regression coefficients","heading":"17.3 Covariance plug-in estimator of regression coefficients","text":"now try understand regression coefficients terms covariances (thus obtaining third way compute estimate ).recall Gauss regression coefficients given \\[\\widehat{\\boldsymbol \\beta} = \\left(\\boldsymbol X^T\\boldsymbol X\\right)^{-1}\\boldsymbol X^T \\boldsymbol y\\]\n\\(\\boldsymbol X\\) \\(n \\times d\\) data matrix (statistics convention)\\[\\boldsymbol X= \\begin{pmatrix} x_{11} & \\dots & x_{1d} \\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} & \\dots & x_{nd} \\end{pmatrix}\\]\nNote assume data matrix \\(\\boldsymbol X\\) centered (.e. column sums\n\\(\\boldsymbol X^T \\boldsymbol 1_n = \\boldsymbol 0\\) zero).Likewise \\(\\boldsymbol y= (y_1, \\ldots, y_n)^T\\) response data vector (also centered \\(\\boldsymbol y^T \\boldsymbol 1_n = 0\\)).Noting \n\\[\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}=\\frac{1}{n}(\\boldsymbol X^T\\boldsymbol X)\\]\nMLE covariance matrix among \\(\\boldsymbol x\\)\n\n\\[\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}=\\frac{1}{n}(\\boldsymbol X^T \\boldsymbol y)\\]\nMLE covariance \\(\\boldsymbol x\\) \\(y\\)\nsee OLS estimate regression coefficients can expressed \n\\[\\widehat{\\boldsymbol \\beta} = \\left(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}\\right)^{-1}\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}\\]\ncan write population version (hats!): \\[\\boldsymbol \\beta= \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\]Thus, OLS regression coefficients can interpreted plugin estimator using MLEs covariances!\nfact, may also use unbiased estimates since scale factor (\\(1/n\\) \\(1/(n-1)\\)) cancels matter\none use!","code":""},{"path":"estimating-regression-coefficients.html","id":"importance-of-positive-definiteness-of-estimated-covariance-matrix","chapter":"17 Estimating regression coefficients","heading":"17.3.1 Importance of positive definiteness of estimated covariance matrix","text":"Note \\(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}\\) inverted \\(\\widehat{\\boldsymbol \\beta} = \\left(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}\\right)^{-1}\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}\\).Hence, estimate \\(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}\\) needs positive definite!\\(\\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{\\text{MLE}}\\) positive definite \\(n>d\\)!Therefore can use ML estimate (empirical estimator) large \\(n\\) > \\(d\\),\notherwise need employ different (regularised) estimation approach (e.g. Bayes penalised ML)!Remark: writing \\(\\hat{\\boldsymbol \\beta}\\) explicitly based covariance estimates advantage \ncan construct plug-estimators regression coefficient based regularised\ncovariance estimators improve ML small sample size.\nleads -called SCOUT method (=covariance-regularized regression Witten Tibshirani, 2008).","code":""},{"path":"estimating-regression-coefficients.html","id":"best-linear-predictor","chapter":"17 Estimating regression coefficients","heading":"17.4 Best linear predictor","text":"best linear predictor fourth way arrive linear model. closely related OLS minimising squared residual error.Without assuming normality multiple regression model\ncan shown optimal linear predictor minimum mean squared prediction error:Assumptions:\\(y\\) \\(\\boldsymbol x\\) random variableswe construct new variable (linear predictor)\n\\(y^{\\star\\star} = b_0 + \\boldsymbol b^T \\boldsymbol x\\) optimally approximate \\(y\\)Aim:choose \\(b_0\\) \\(\\boldsymbol b\\) minimize mean squared prediction error\n\\(\\text{E}( (y - y^{\\star\\star})^2 )\\)","code":""},{"path":"estimating-regression-coefficients.html","id":"result","chapter":"17 Estimating regression coefficients","heading":"17.4.1 Result:","text":"mean squared prediction error \\(MSPE\\) dependence \\((b_0, \\boldsymbol b)\\) \n\\[\n\\begin{split}\n\\text{E}( (y - y^{\\star\\star} )^2) & = \\text{Var}(y - y^{\\star\\star}) + \\text{E}(y - y^{\\star\\star})^2 \\\\\n  & = \\text{Var}(y - b_0 -\\boldsymbol b^T \\boldsymbol x) + ( \\text{E}(y) -b_0 - \\boldsymbol b^T \\text{E}(\\boldsymbol x) )^2 \\\\\n & = \\sigma^2_y + \\text{Var}(\\boldsymbol b^T \\boldsymbol x) + 2 \\, \\text{Cov}(y, -\\boldsymbol b^T \\boldsymbol x)  + ( \\mu_y -b_0 - \\boldsymbol b^T \\boldsymbol \\mu_{\\boldsymbol x} )^2 \\\\\n  & = \\sigma^2_y + \\boldsymbol b^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol b- 2 \\, \\boldsymbol b^T \\boldsymbol \\Sigma_{\\boldsymbol xy} + ( \\mu_y -b_0 - \\boldsymbol b^T \\boldsymbol \\mu_{\\boldsymbol x} )^2 \\\\\n  & = MSPE(b_0, \\boldsymbol b) \\\\\n\\end{split}\n\\]look \n\\[\n(\\beta_0, \\boldsymbol \\beta) = \\underset{b_0,\\boldsymbol b}{\\arg\\min} \\,\\, MSPE(b_0, \\boldsymbol b)\n\\]order find minimum compute gradient regard \\((b_0, \\boldsymbol b)\\)\n\\[\n\\nabla MSPE = \n\\begin{pmatrix} \n-2( \\mu_y -b_0 - \\boldsymbol b^T \\boldsymbol \\mu_{\\boldsymbol x} ) \\\\\n2 \\, \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol b- 2 \\, \\boldsymbol \\Sigma_{\\boldsymbol xy} -2 \\boldsymbol \\mu_{\\boldsymbol x} (\\mu_y -b_0 - \\boldsymbol b^T \\boldsymbol \\mu_{\\boldsymbol x}) \\\\\n\\end{pmatrix}\n\\]\nsetting equal zero yields\n\\[\n\\begin{pmatrix}\n\\beta_0\\\\\n\\boldsymbol \\beta\\\\\n\\end{pmatrix}\n= \n\\begin{pmatrix}\n\\mu_y- \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x} \\\\\n\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\\\\n\\end{pmatrix}\n\\]\nThus, optimal values \\(b_0\\) \\(\\boldsymbol b\\) best linear predictor\ncorrespond previously derived coefficients \\(\\beta_0\\) \\(\\boldsymbol \\beta\\)!","code":""},{"path":"estimating-regression-coefficients.html","id":"irreducible-error","chapter":"17 Estimating regression coefficients","heading":"17.4.2 Irreducible Error","text":"minimum achieved MSPE (=irreducible error) \n\\[\nMSPE(\\beta_0,\\boldsymbol \\beta)\n= \\sigma^2_{y} - \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\beta= \\sigma^2_{y} -  \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} \n\\]\nabbreviation\n\\(\\Omega^2 = \\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} = \\sigma_y^{-2} \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\)\ncan simplify \n\\[\nMSPE(\\beta_0,\\boldsymbol \\beta)\n=  \\sigma^2_y (1-\\Omega^2) = \\text{Var}(\\varepsilon)\n\\]Writing \\(b_0=\\beta_0 + \\Delta_0\\) \\(\\boldsymbol b= \\boldsymbol \\beta+ \\boldsymbol \\Delta\\) easy see \nmean squared predictive error quadratic function around minimum:\n\\[\nMSPE(\\beta_0 + \\Delta_0, \\boldsymbol \\beta+ \\boldsymbol \\Delta) = \\text{Var}(\\varepsilon) + \\Delta_0^2 + \\boldsymbol \\Delta^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\Delta\n\\]Note usually \\(y^{\\star} = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x\\) perfectly approximate \\(y\\) irreducible error (= noise variance)\n\\[\\text{Var}(\\varepsilon) =\\sigma^2_y (1-\\Omega^2) > 0\\]\nimplies \\(\\Omega^2 < 1\\).quantity \\(\\Omega^2\\) interpretation population version \nsquared multiple correlation coefficient response predictors\nplays vital role decomposition variance, discussed later.","code":""},{"path":"estimating-regression-coefficients.html","id":"regression-by-conditioning","chapter":"17 Estimating regression coefficients","heading":"17.5 Regression by conditioning","text":"Conditioning fifth way arrive linear model. also general way can used derive many regression models\n(just simple linear model).","code":""},{"path":"estimating-regression-coefficients.html","id":"general-idea","chapter":"17 Estimating regression coefficients","heading":"17.5.1 General idea:","text":"two random variables \\(y\\) (response, scalar) \\(\\boldsymbol x\\) (predictor variables, vector)assume \\(y\\) \\(\\boldsymbol x\\) joint distribution \\(F_{y,\\boldsymbol x}\\)compute conditional random variable \\(y | \\boldsymbol x\\) \ncorresponding distribution \\(F_{y | \\boldsymbol x}\\)","code":""},{"path":"estimating-regression-coefficients.html","id":"multivariate-normal-assumption","chapter":"17 Estimating regression coefficients","heading":"17.5.2 Multivariate normal assumption","text":"Now assume \\(y\\) \\(\\boldsymbol x\\) (jointly) multivariate normal. conditional distribution \\(F_{y | \\boldsymbol x}\\) univariate normal following moments (can verify looking general conditional multivariate normal distribution):) Conditional expectation:\\[ \\text{E}( y | \\boldsymbol x) = y^{\\star} = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x\\]coefficients \\(\\boldsymbol \\beta= \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1}\\boldsymbol \\Sigma_{\\boldsymbol xy}\\) \nintercept \\(\\beta_0 = \\mu_{y} - \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x}\\) .Note \\(y^{\\star}\\) depends \\(\\boldsymbol x\\) random variable \nmean\n\\[\n\\text{E}(y^{\\star}) = \\beta_0 + \\boldsymbol \\beta^T  \\boldsymbol \\mu_{\\boldsymbol x} = \\mu_{y}\n\\]\nvariance\n\\[\n\\begin{split}\n\\text{Var}(y^{\\star}) & = \\text{Var}(\\text{E}( y | \\boldsymbol x)) \\\\\n& = \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\beta= \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} \\\\\n& = \\sigma^2_y \\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} \\\\\n& = \\sigma_y^2 \\Omega^2\\\\\n\\end{split}\n\\]b) Conditional variance:\\[\n\\begin{split}\n\\text{Var}( y | \\boldsymbol x) &=\\sigma^2_y - \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\beta\\\\\n& = \\sigma^2_y - \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}  \\\\\n& = \\sigma^2_y (1-\\Omega^2)\\\\\n\\end{split}\n\\]\nNote constant \\(\\text{E}(\\text{Var}( y | \\boldsymbol x)) = \\sigma^2_y (1-\\Omega^2)\\) well.","code":""},{"path":"estimating-regression-coefficients.html","id":"standardised-regression-coefficients-and-relationship-to-correlation","chapter":"17 Estimating regression coefficients","heading":"17.6 Standardised regression coefficients and relationship to correlation","text":"First note can decompose regression coefficients product\nmarginal correlations correlations among predictors.Using variance-correlation decompositions \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}= \\boldsymbol V_{\\boldsymbol x}^{1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x} \\boldsymbol V_{\\boldsymbol x}^{1/2}\\) \\(\\boldsymbol \\Sigma_{\\boldsymbol xy}= \\boldsymbol V_{\\boldsymbol x}^{1/2} \\boldsymbol P_{\\boldsymbol xy} \\sigma_y\\) rewrite regression coefficients \n\\[\n\\boldsymbol \\beta= {\\underbrace{\\boldsymbol V_{\\boldsymbol x}^{-1/2}}_{\\text{(inverse) scale } x_i}} \\,\\, {\\underbrace{\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1}}_{\\text{ (inverse) correlation among predictors }}} \\underbrace{\\boldsymbol P_{\\boldsymbol xy}}_{\\text{ marginal correlations}}\\,\\,\n\\underbrace{\\sigma_y}_{\\text{ scale }y}\n\\]\nThus regression coefficients \\(\\boldsymbol \\beta\\) contain scale variables, \ntake account correlations among predictors (\\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}\\))\naddition marginal correlations response \\(y\\) predictors \\(x_i\\) (\\(\\boldsymbol P_{\\boldsymbol xy}\\)).decomposition allows understand number special cases regression coefficient simplify :response predictors standardised variance one, .e. \\(\\text{Var}(y)=1\\) \\(\\text{Var}(x_i)\\), \\(\\boldsymbol \\beta\\) becomes equal \nstandardised regression coefficients\n\\[\\boldsymbol \\beta_{\\text{std}} = \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\\]\nNote standardised regression coefficients make use variances thus scale-independent.response predictors standardised variance one, .e. \\(\\text{Var}(y)=1\\) \\(\\text{Var}(x_i)\\), \\(\\boldsymbol \\beta\\) becomes equal \nstandardised regression coefficients\n\\[\\boldsymbol \\beta_{\\text{std}} = \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\\]\nNote standardised regression coefficients make use variances thus scale-independent.correlation among predictors , .e. \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x} = \\boldsymbol \\)\nregression coefficients reduce \n\\[\\boldsymbol \\beta= \\boldsymbol V_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\]\n\\(\\boldsymbol V_{\\boldsymbol x}\\) diagonal matrix containing variances oft predictors.\nalso called marginal regression. Note inversion \\(\\boldsymbol V_{\\boldsymbol x}\\)\ntrival since need invert diagonal element individually.correlation among predictors , .e. \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x} = \\boldsymbol \\)\nregression coefficients reduce \n\\[\\boldsymbol \\beta= \\boldsymbol V_{\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\]\n\\(\\boldsymbol V_{\\boldsymbol x}\\) diagonal matrix containing variances oft predictors.\nalso called marginal regression. Note inversion \\(\\boldsymbol V_{\\boldsymbol x}\\)\ntrival since need invert diagonal element individually.) b) apply simultaneously (.e. correlation among predictors response predictors predictors\nstandardised) \nregression coefficients simplify even \n\\[\n\\boldsymbol \\beta= \\boldsymbol P_{\\boldsymbol xy}\n\\]\nThus, special case regression coefficients identical correlations response predictors!) b) apply simultaneously (.e. correlation among predictors response predictors predictors\nstandardised) \nregression coefficients simplify even \n\\[\n\\boldsymbol \\beta= \\boldsymbol P_{\\boldsymbol xy}\n\\]\nThus, special case regression coefficients identical correlations response predictors!","code":""},{"path":"squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html","id":"squared-multiple-correlation-and-variance-decomposition-in-linear-regression","chapter":"18 Squared multiple correlation and variance decomposition in linear regression","heading":"18 Squared multiple correlation and variance decomposition in linear regression","text":"chapter first introduce (squared) multiple correlation multiple adjusted \\(R^2\\) coefficients estimators. Subsequently\ndiscuss variance decomposition.","code":""},{"path":"squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html","id":"squared-multiple-correlation-omega2-and-the-r2-coefficient","chapter":"18 Squared multiple correlation and variance decomposition in linear regression","heading":"18.1 Squared multiple correlation \\(\\Omega^2\\) and the \\(R^2\\) coefficient","text":"previous chapter encountered following quantity:\n\\[\n\\Omega^2 =  \\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} = \\sigma_y^{-2} \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\n\\]\\(\\boldsymbol \\beta=\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy}\\)\n\n\\(\\beta_0=\\mu_y- \\boldsymbol \\beta^T \\boldsymbol \\mu_{\\boldsymbol x}\\) straightforward verify following:cross-covariance \\(y\\) \\(y^{\\star}\\) \n\\[\n\\begin{split}\n\\text{Cov}(y, y^{\\star}) &= \\boldsymbol \\Sigma_{y  \\boldsymbol x} \\boldsymbol \\beta= \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol \\Sigma_{\\boldsymbol xy} \\\\\n& = \\sigma^2_y \\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} = \\sigma_y^2 \\Omega^2\\\\\n\\end{split}\n\\](signal) variance \\(y^{\\star}\\) \n\\[\n\\begin{split}\n\\text{Var}(y^{\\star}) &= \\boldsymbol \\beta^T \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\beta= \\boldsymbol \\Sigma_{y \\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1}  \\boldsymbol \\Sigma_{\\boldsymbol xy} \\\\\n & = \\sigma^2_y \\boldsymbol P_{y \\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} = \\sigma_y^2 \\Omega^2\\\\\n\\end{split}\n\\]hence correlation \\(\\text{Cor}(y, y^{\\star}) = \\frac{\\text{Cov}(y, y^{\\star})}{\\text{SD}(y) \\text{SD}(y^{\\star})} = \\Omega\\) \\(\\Omega \\geq 0\\).helps understand \\(\\Omega\\) \\(\\Omega^2\\) coefficients:\\(\\Omega\\) linear correlation response (\\(y\\)) prediction \\(y^{\\star}\\).\\(\\Omega\\) linear correlation response (\\(y\\)) prediction \\(y^{\\star}\\).\\(\\Omega^2\\) called squared multiple correlation scalar \\(y\\) vector \\(\\boldsymbol x\\).\\(\\Omega^2\\) called squared multiple correlation scalar \\(y\\) vector \\(\\boldsymbol x\\).Note one predictor (\\(x\\) scalar) \n\\(\\boldsymbol P_{x x} = 1\\) \\(\\boldsymbol P_{y x} = \\rho_{yx}\\) multiple squared correlation coefficient reduces squared correlation\n\\(\\Omega^2 = \\rho_{yx}^2\\) two scalar random variables \\(y\\) \\(x\\).Note one predictor (\\(x\\) scalar) \n\\(\\boldsymbol P_{x x} = 1\\) \\(\\boldsymbol P_{y x} = \\rho_{yx}\\) multiple squared correlation coefficient reduces squared correlation\n\\(\\Omega^2 = \\rho_{yx}^2\\) two scalar random variables \\(y\\) \\(x\\).","code":""},{"path":"squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html","id":"estimation-of-omega2-and-the-multiple-r2-coefficient","chapter":"18 Squared multiple correlation and variance decomposition in linear regression","heading":"18.1.1 Estimation of \\(\\Omega^2\\) and the multiple \\(R^2\\) coefficient","text":"multiple squared correlation coefficient \\(\\Omega^2\\) can estimated plug-empirical estimates corresponding correlation matrices:\n\\[R^2 =  \\hat{\\boldsymbol P}_{y \\boldsymbol x} \\hat{\\boldsymbol P}_{\\boldsymbol x\\boldsymbol x}^{-1} \\hat{\\boldsymbol P}_{\\boldsymbol xy} = \\hat{\\sigma}_y^{-2} \\hat {\\boldsymbol \\Sigma}_{y \\boldsymbol x} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}\\]\nestimator \\(\\Omega^2\\) called multiple \\(R^2\\) coefficient.scale factor \\(1/n\\) \\(1/(n-1)\\) used estimating variance \\(\\sigma^2_y\\) \ncovariances \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}\\) \\(\\boldsymbol \\Sigma_{y \\boldsymbol x}\\) factor cancel .seen \\(\\Omega^2\\) directly linked noise variance via\n\\[\n\\text{Var}(\\varepsilon) =\\sigma^2_y (1-\\Omega^2) \\,.\n\\]\ncan express squared multiple correlation \n\\[\n\\Omega^2 = 1- \\text{Var}(\\varepsilon) / \\sigma^2_y \n\\]maximum likelihood estimate noise variance \\(\\text{Var}(\\varepsilon)\\) (also called residual variance) can computed residual sum squares \\(RSS = \\sum_{=1}^n (y_i -\\hat{y}_i)^2\\)\nfollows:\n\\[\n\\widehat{\\text{Var}}(\\varepsilon)_{ML} = \\frac{RSS}{n}\n\\]\nwhereas unbiased estimate obtained \n\\[\n\\widehat{\\text{Var}}(\\varepsilon)_{UB} = \\frac{RSS}{n-d-1} = \\frac{RSS}{df}\n\\]\ndegree freedom \\(df=n-d-1\\) \\(d\\) number predictors.Similarly, can find maximimum likelihood estimate \\(v_y^{ML}\\) \\(\\sigma^2_y\\)\n(factor \\(1/n\\)) well unbiased estimate \\(v_y^{UB}\\) (scale factor \\(1/(n-1)\\))multiple \\(R^2\\) coefficient can written \n\\[\nR^2 =1- \\widehat{\\text{Var}}(\\varepsilon)_{ML} / v_y^{ML}\n\\]\nNote use MLEs.contrast, -called adjusted multiple \\(R^2\\) coefficient given \n\\[\nR^2_{\\text{adj}}=1- \\widehat{\\text{Var}}(\\varepsilon)_{UB} / v_y^{UB}\n\\]\nunbiased variances used.\\(R^2\\) \\(R^2_{\\text{adj}}\\) estimates \\(\\Omega^2\\) related \n\\[\n1-R^2 = (1- R^2_{\\text{adj}}) \\, \\frac{df}{n-1}\n\\]","code":""},{"path":"squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html","id":"r-commands","chapter":"18 Squared multiple correlation and variance decomposition in linear regression","heading":"18.1.2 R commands","text":"R command lm() fits linear regression model.addition regression cofficients (derived quantities) R function lm() also liststhe multiple R-squared \\(R^2\\),adjusted R-squared \\(R^2_{\\text{adj}}\\),degrees freedom \\(df\\) andthe residual standard error \\(\\sqrt{\\widehat{\\text{Var}}(\\varepsilon)_{UB}}\\) (computed unbiased variance estimate).See also Worksheet 9 provides R code reproduce exact output native lm() R function.","code":""},{"path":"squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html","id":"variance-decomposition-in-regression","chapter":"18 Squared multiple correlation and variance decomposition in linear regression","heading":"18.2 Variance decomposition in regression","text":"squared multiple correlation coefficient useful also plays important role decomposition total variance:total variance: \\(\\text{Var}(y) = \\sigma^2_y\\)unexplained variance (irreducible error): \\(\\sigma^2_y (1-\\Omega^2) = \\text{Var}(\\varepsilon)\\)explained variance complement: \\(\\sigma^2_y \\Omega^2 = \\text{Var}(y^{\\star})\\)summary:\\[\\text{Var}(y)  =  \\text{Var}(y^{\\star}) + \\text{Var}(\\varepsilon)\\]\nbecomes\n\\[\\underbrace{\\sigma^2_y}_{\\text{total variance}}  = \\underbrace{\\sigma_y^2 \\Omega^2}_{\\text{explained variance}}\n + \\underbrace{ \\sigma^2_y (1-\\Omega^2)}_{\\text{unexplained variance}}\\]unexplained variance measures fit introducing predictors model (smaller means better fit).\ntotal variance measures fit model without predictors. explained variance\ndifference total unexplained variance, indicates increase model fit\ndue predictors.","code":""},{"path":"squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html","id":"law-of-total-variance-and-variance-decomposition","chapter":"18 Squared multiple correlation and variance decomposition in linear regression","heading":"18.2.1 Law of total variance and variance decomposition","text":"law total variance \\[\\underbrace{\\text{Var}(y)}_{\\text{total variance}}  = \\underbrace{\\text{Var}( \\text{E}(y | \\boldsymbol x) ) }_{\\text{explained variance}}\n + \\underbrace{ \\text{E}( \\text{Var}( y | \\boldsymbol x) )}_{\\text{unexplained variance}}\\]provides general decomposition explained unexplained parts variance valid regardless form distributions \\(F_{y, \\boldsymbol x}\\) \\(F_{y | \\boldsymbol x}\\).regression conncects\nvariance decomposition conditioning. plug-conditional expections multivariate\nnormal model (cf. previous chapter) recover\\[\\underbrace{\\sigma^2_y}_{\\text{total variance}}  = \\underbrace{\\sigma_y^2 \\Omega^2 }_{\\text{explained variance}}\n + \\underbrace{ \\sigma^2_y (1-\\Omega^2)}_{\\text{unexplained variance}}\\]","code":""},{"path":"squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html","id":"related-quantities","chapter":"18 Squared multiple correlation and variance decomposition in linear regression","heading":"18.2.2 Related quantities","text":"Using three quantities (total variance, explained variance, unexplained variance)\ncan construct number scores:coefficient determination, squared multiple correlation:\\[\n\\frac{\\text{explained var}}{\\text{total var}} = \\frac{\\sigma_y^2 \\Omega^2}{\\sigma_y^2} = \\Omega^2\n\\]\n(range 0 1, 1 indicating perfect fit)coefficient non-determination, coefficient alienation:\\[\n\\frac{\\text{unexplained var}}{\\text{total var}} = \\frac{\\sigma_y^2 (1-\\Omega^2)}{\\sigma_y^2} = 1-\\Omega^2\n\\]\n(range 0 1, 0 indicating perfect fit)\\(F\\) score, \\(t^2\\) score:\\[\n\\frac{\\text{explained var}}{\\text{unexplained var}} = \\frac{\\sigma_y^2 \\Omega^2}{\\sigma_y^2 (1-\\Omega^2)} = \\frac{\\Omega^2}{1-\\Omega^2} = \\mathcal{F} = \\frac{\\tau^2}{n}\n\\]\n(range 0 \\(\\infty\\), \\(\\infty\\) indicating perfect fit)Note \\(\\mathcal{F}\\) \\(\\tau^2\\) scores population versions \n\\(F\\) \\(t^2\\) statistics.Also note \\(\\Omega^2 = \\frac{\\tau^2}{\\tau^2 + n} = \\frac{\\mathcal{F}}{\\mathcal{F} + 1}\\) links squared correlation squared \\(t\\)-scores \\(F\\)-scores.","code":""},{"path":"squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html","id":"sample-version-of-variance-decomposition","chapter":"18 Squared multiple correlation and variance decomposition in linear regression","heading":"18.3 Sample version of variance decomposition","text":"\\(\\Omega^2\\) \\(\\sigma^2_y\\) replaced MLEs can written sample version follows using data points \\(y_i\\), predictions \\(\\hat{y}_i\\) \\(\\bar{y} = \\frac{1}{n}\\sum_{=1}^n {y_i}\\)\\[\\underbrace{\\sum_{=1}^n (y_i-\\bar{y})^2}_{\\text{total sum squares (TSS)}}  = \\underbrace{\\sum_{=1}^n (\\hat{y}_i-\\bar{y})^2 }_{\\text{explained sum squares (ESS)}}\n + \\underbrace{\\sum_{=1}^n (y_i-\\hat{y}_i)^2 }_{\\text{residual sum squares (RSS)}}\\]Note TSS, ESS RSS scale \\(n\\).\nUsing data vector notation sample-based variance decomposition can written form Pythagorean theorem:\n\\[\\underbrace{|| \\boldsymbol y-\\bar{y} \\boldsymbol 1\\ ||^2}_{\\text{total sum squares (TSS)}}  = \n\\underbrace{||\\hat{\\boldsymbol y}-\\bar{y} \\boldsymbol 1||^2 }_{\\text{explained sum squares (ESS)}}\n + \\underbrace{|| \\boldsymbol y-\\hat{\\boldsymbol y} ||^2 }_{\\text{residual sum squares (RSS)}}\\]","code":""},{"path":"squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html","id":"geometric-interpretation-of-regression-as-orthogonal-projection","chapter":"18 Squared multiple correlation and variance decomposition in linear regression","heading":"18.3.1 Geometric interpretation of regression as orthogonal projection:","text":"equation can simplified \\[|| \\boldsymbol y||^2  = \n||\\hat{\\boldsymbol y}||^2 \n + \\underbrace{|| \\boldsymbol y-\\hat{\\boldsymbol y} ||^2 }_{\\text{RSS}}\n\\]Geometrically speaking, implies \\(\\hat{\\boldsymbol y}\\) orthogonal projection \\(\\boldsymbol y\\), since \nresiduals \\(\\boldsymbol y-\\hat{\\boldsymbol y}\\) predictions \\(\\hat{\\boldsymbol y}\\) orthogonal (construction!).also valid centered versions vectors, .e.\n\\(\\hat{\\boldsymbol y}-\\bar{y} \\boldsymbol 1_n\\) orthogonal projection \\(\\boldsymbol y-\\bar{y} \\boldsymbol 1_n\\) (see Figure).Also note angle \\(\\theta\\) two centered vectors directly related (estimated) multiple correlation, \\(R = \\cos(\\theta) = \\frac{||\\hat{\\boldsymbol y}-\\bar{y} \\boldsymbol 1_n ||}{|| \\boldsymbol y-\\bar{y} \\boldsymbol 1_n||}\\), \\(R^2 = \\cos(\\theta)^2 = \\frac{||\\hat{\\boldsymbol y}-\\bar{y} \\boldsymbol 1_n ||^2}{|| \\boldsymbol y-\\bar{y} \\boldsymbol 1_n||^2} = \\frac{\\text{ESS}}{\\text{TSS}}\\).Source Figure: Stack Exchange","code":""},{"path":"prediction-and-variable-selection.html","id":"prediction-and-variable-selection","chapter":"19 Prediction and variable selection","heading":"19 Prediction and variable selection","text":"chapter discuss compute (lower bounds) \nprediction error select variables relevant prediction","code":""},{"path":"prediction-and-variable-selection.html","id":"prediction-and-prediction-intervals","chapter":"19 Prediction and variable selection","heading":"19.1 Prediction and prediction intervals","text":"Learning regression function (training) data first step application regression models.next step actually make prediction future outcomes \\(y^{\\text{test}}\\) given test data\n\\(\\boldsymbol x^{\\text{test}}\\):\n\\[\ny^{\\text{test}} = \\hat{y}(\\boldsymbol x^{\\text{test}}) = \\hat{f}_{\\hat{\\beta}_0, \\hat{\\boldsymbol \\beta}}(\\boldsymbol x^{\\text{test}})\n\\]Note \n\\(y^{\\text{test}}\\) point estimator. possible also construct corresponding interval estimate?answer yes, leads back conditioning approach:\\[y^\\star = \\text{E}(y| \\boldsymbol x) = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x\\]\\[\\text{Var}(\\varepsilon) = \\text{Var}(y|\\boldsymbol x) = \\sigma^2_y (1-\\Omega^2)\\]know mean squared prediction error \\(y^{\\star}\\) \\(\\text{E}((y -y^{\\star})^2)=\\text{Var}(\\varepsilon)\\) minimal irreducible error. Hence, may use \\(\\text{Var}(\\varepsilon)\\) minimum variability prediction.corresponding prediction interval \n\\[\\left[ y^{\\star}(\\boldsymbol x^{\\text{test}}) \\pm c \\times \\text{SD}(\\varepsilon) \\right]\\]\n\\(c\\) suitable constant (e.g. 1.96 symmetric 95% normal intervals).However, please note prediction interval constructed fashion underestimate.\nreason assumes employ \\(y^{\\star} = \\beta_0 + \\boldsymbol \\beta^T \\boldsymbol x\\) reality actually use \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\boldsymbol \\beta}^T \\boldsymbol x\\) prediction — note estimated coefficients! recall earlier chapter (best linear predictor) leads increase MSPE compared using optimal \\(\\beta_0\\) \\(\\boldsymbol \\beta\\).Thus, better prediction intervals need consider mean squared prediction error \\(\\hat{y}\\) can written \\(\\text{E}((y -\\hat{y})^2) = \\text{Var}(\\varepsilon) + \\delta\\) \\(\\delta\\) additional error term due using estimated rather true regression function. \\(\\delta\\) typically declines \\(1/n\\) can substantial small \\(n\\) (particular usually depends number predictors \\(d\\)).details refer later modules regression.","code":""},{"path":"prediction-and-variable-selection.html","id":"variable-importance-and-prediction","chapter":"19 Prediction and variable selection","heading":"19.2 Variable importance and prediction","text":"Another key question regression modeling find \npredictor variables \\(x_1, x_2, \\dots, x_d\\) actually important predicting outcome \\(y\\).\\(\\rightarrow\\) need study variable importance measures (VIM).","code":""},{"path":"prediction-and-variable-selection.html","id":"how-to-quantify-variable-importance","chapter":"19 Prediction and variable selection","heading":"19.2.1 How to quantify variable importance?","text":"variable \\(x_i\\) important improves prediction response \\(y\\).Recall variance decomposition:\\[\\text{Var}(y) = \\sigma_y^2 = \\underbrace{\\sigma^2_y\\Omega^2}_{\\text{explained variance}} + \\underbrace{\\sigma^2_y(1-\\Omega^2)}_{\\text{unexplained/residual variance =} \\text{Var}(\\varepsilon)}\\]\\(\\Omega^2\\) squared multiple correlation \\(\\[0,1]\\)\\(\\Omega^2\\) large \\(\\rightarrow 1\\) predictor variables explain \\(\\sigma_y^2\\)\\(\\Omega^2\\) small \\(\\rightarrow 0\\) linear model fails predictors explain variability\\(\\Rightarrow\\) predictor helps \\(\\begin{array}{ll} \\text{increase explained variance} \\\\ \\text{decrease unexplained variance} \\end{array}\\) important!\\(\\Omega^2 = \\boldsymbol P_{y\\boldsymbol x} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} \\hat{=}\\) function \\(X\\)!VIM: predictors contribute \\(\\Omega^2\\)","code":""},{"path":"prediction-and-variable-selection.html","id":"some-candidates-for-vims","chapter":"19 Prediction and variable selection","heading":"19.2.2 Some candidates for VIMs","text":"regression coefficients \\(\\boldsymbol \\beta\\)\n\\(\\boldsymbol \\beta= \\boldsymbol \\Sigma^{-1}_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol xy}= \\boldsymbol V_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} \\sigma_y\\)\ngood VIM since \\(\\boldsymbol \\beta\\) contains scale!\nLarge \\(\\hat{\\beta}_i\\) indicate \\(x_i\\) important.\nSmall \\(\\hat{\\beta}_i\\) indicate \\(x_i\\) important.\nregression coefficients \\(\\boldsymbol \\beta\\)\\(\\boldsymbol \\beta= \\boldsymbol \\Sigma^{-1}_{\\boldsymbol x\\boldsymbol x} \\boldsymbol \\Sigma_{\\boldsymbol xy}= \\boldsymbol V_{\\boldsymbol x}^{-1/2} \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy} \\sigma_y\\)good VIM since \\(\\boldsymbol \\beta\\) contains scale!Large \\(\\hat{\\beta}_i\\) indicate \\(x_i\\) important.Small \\(\\hat{\\beta}_i\\) indicate \\(x_i\\) important.Standardised regression coefficients \\(\\boldsymbol \\beta_{\\text{std}}\\)\n\\(\\boldsymbol \\beta_{\\text{std}} = \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\\)\nimplies \\(\\text{Var}(y)=1\\), \\(\\text{Var}(x_i)=1\\)\ncontain scale (better \\(\\hat{\\beta}\\))\nstill unclear relates decomposition variance\nStandardised regression coefficients \\(\\boldsymbol \\beta_{\\text{std}}\\)\\(\\boldsymbol \\beta_{\\text{std}} = \\boldsymbol P_{\\boldsymbol x\\boldsymbol x}^{-1} \\boldsymbol P_{\\boldsymbol xy}\\)implies \\(\\text{Var}(y)=1\\), \\(\\text{Var}(x_i)=1\\)contain scale (better \\(\\hat{\\beta}\\))still unclear relates decomposition varianceSquared marginal correlations \\(\\rho_{y, x_i}^2\\)\nConsider case uncorrelated predictors: \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}=\\boldsymbol \\) (correlation among \\(x_i\\))\n\\[\\Rightarrow \\Omega^2 = \\boldsymbol P_{y\\boldsymbol x} \\boldsymbol P_{\\boldsymbol xy} = \\sum^d_{=1} \\rho_{y, x_i}^2\\]\n\\(\\rho_{y, x_i}^2 = \\text{Cor}(y, x_i)\\) marginal correlation \\(y\\) \\(x_i\\), \\(\\Omega^2\\) (uncorrelated predictors) sum squared marginal correlations.\n\\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}=\\boldsymbol \\), ranking predictors \\(\\rho_{y, x_i}^2\\) optimal!\npredictor largest marginal correlation reduces unexplained variance !\ngood news: even weak correlation among predictors marginal correlations still good VIM (perfectly add \\(\\Omega^2\\))\nAdvantage: simple often also effective.\nCaution! strong correlation \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}\\), colinearity (case oftern best remove one strongly correlated variables, merge correlated variables).\nSquared marginal correlations \\(\\rho_{y, x_i}^2\\)Consider case uncorrelated predictors: \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}=\\boldsymbol \\) (correlation among \\(x_i\\))\\[\\Rightarrow \\Omega^2 = \\boldsymbol P_{y\\boldsymbol x} \\boldsymbol P_{\\boldsymbol xy} = \\sum^d_{=1} \\rho_{y, x_i}^2\\]\\(\\rho_{y, x_i}^2 = \\text{Cor}(y, x_i)\\) marginal correlation \\(y\\) \\(x_i\\), \\(\\Omega^2\\) (uncorrelated predictors) sum squared marginal correlations.\\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}=\\boldsymbol \\), ranking predictors \\(\\rho_{y, x_i}^2\\) optimal!predictor largest marginal correlation reduces unexplained variance !good news: even weak correlation among predictors marginal correlations still good VIM (perfectly add \\(\\Omega^2\\))Advantage: simple often also effective.Caution! strong correlation \\(\\boldsymbol P_{\\boldsymbol x\\boldsymbol x}\\), colinearity (case oftern best remove one strongly correlated variables, merge correlated variables).Often, ranking predictors squared marginal correlations done prefiltering step\n(independence screening).","code":""},{"path":"prediction-and-variable-selection.html","id":"regression-t-scores.","chapter":"19 Prediction and variable selection","heading":"19.3 Regression \\(t\\)-scores.","text":"","code":""},{"path":"prediction-and-variable-selection.html","id":"wald-statistic-for-regression-coefficients","chapter":"19 Prediction and variable selection","heading":"19.3.1 Wald statistic for regression coefficients","text":"far, discussed three obvious candidates variable importance measures\n(regression coefficients, standardised regression coefficients, marginal correlations).section consider quantity, \nregression \\(t-\\)score:Recall ML estimation regression coefficients yieldsa point estimate \\(\\hat{\\boldsymbol \\beta}\\)(asymptotic) variance \\(\\widehat{\\text{Var}}(\\hat{\\boldsymbol \\beta})\\)asymptotic normal distribution \\(\\hat{\\boldsymbol \\beta} \\overset{}{\\sim} N_d(\\boldsymbol \\beta, \\widehat{\\text{Var}}(\\hat{\\boldsymbol \\beta}))\\)Corresponding predictor \\(x_i\\) can construct \\(t\\)-score\n\\[\nt_i = \\frac{\\hat{\\beta}_i}{\\widehat{\\text{SD}}(\\hat{\\beta}_i)}\n\\]\nstandard deviations computed \\(\\widehat{\\text{SD}}(\\hat{\\beta}_i) = \\text{Diag}(\\widehat{\\text{Var}}(\\hat{\\boldsymbol \\beta}))_{}\\). corresponds Wald statistic test underlying true\nregression coefficient zero (\\(\\beta_i =0\\)).Correspondingly, null hypthesis \\(\\beta_i=0\\) asymptotically large \\(n\\)\nregression \\(t\\)-score standard normal distributed:\n\\[\nt_i \\overset{}{\\sim} N(0,1)\n\\]\nallows compute (symmetric) \\(p\\)-values \\(p = 2 \\Phi(-|t_i|)\\) \\(\\Phi\\) standard normal distribution function.finite \\(n\\), assuming normality observation using unbiased estimate variance\ncomputing \\(t_i\\), exact distribution \\(i_i\\) given Student-\\(t\\) distribution:\n\\[\nt_i \\sim t_{n-d-1}\n\\]Regression \\(t\\)-scores can thus used test whether regression coefficient zero.\nlarge magnitude \\(t_i\\) score indicates hypothesis \\(\\beta_i=0\\) can rejected.\nThus, small \\(p\\)-value (say smaller 0.05) signals regression coefficient non-zero hence corresponding predictor variable included model.allows rank predictor variables \\(|t_i|\\) corresponding \\(p\\)-values regard relevance linear model. Typically, order simplify model, predictors largest \\(p\\)-values (thus smallest absolute \\(t\\)-scores) may removed model. However, note \\(p\\)-value say larger 0.05 sufficient declare regression coefficient zero (classical statistical testing can reject null hypothesis, accept !).Note construction regression \\(t\\)-scores depend scale, original data rescaled affect corresponding regression \\(t\\)-scores.\nFurthermore, \\(\\widehat{\\text{SD}}(\\hat{\\beta}_i)\\) small, regression \\(t\\)-score\n\\(t_i\\) can still large even \\(\\hat{\\beta}_i\\) small!","code":""},{"path":"prediction-and-variable-selection.html","id":"computing","chapter":"19 Prediction and variable selection","heading":"19.3.2 Computing","text":"perform regression analysis R (another statistical software package) computer return following:\\[\\begin{align*}\n\\begin{array}{cc}\n\\hat{\\beta}_i\\\\\n\\text{Estimated}\\\\\n\\text{repression}\\\\\n\\text{coefficient}\\\\\n\\\\\n\\end{array}\n\\begin{array}{cc}\n\\widehat{\\text{SD}}(\\hat{\\beta}_i)\\\\\n\\text{Error }\\\\\n\\hat{\\beta}_i\\\\\n\\\\\n\\\\\n\\end{array}\n\\begin{array}{cc}\nt_i = \\frac{\\hat{\\beta}_i}{\\widehat{\\text{SD}}(\\hat\\beta_i)}\\\\\n\\text{t-score}\\\\\n\\text{computed }\\\\\n\\text{first two columns}\\\\\n\\\\\n\\end{array}\n\\begin{array}{cc}\n\\text{p-values}\\\\\n\\text{} t_i\\\\\n\\text{based t-distribution}\\\\\n\\\\\n\\\\\n\\end{array}\n\\begin{array}{ll}\n\\text{Indicator }\\\\\n\\text{Significance}\\\\\n\\text{*     } 0.9\\\\\n\\text{**    } 0.95\\\\\n\\text{***   } 0.99\\\\\n\\end{array}\n\\end{align*}\\]lm() function R standard deviation square root unbiased estimate\nvariance (note unbiased!).","code":""},{"path":"prediction-and-variable-selection.html","id":"connection-with-partial-correlation","chapter":"19 Prediction and variable selection","heading":"19.3.3 Connection with partial correlation","text":"deeper reason ranking predictors regression \\(t\\)-scores associated \\(p\\)-values useful link \npartial correlation.particular, (squared) regression \\(t\\)-score can 1:1 transformed \n(estimated) (squared) partial correlation\n\\[\n\\hat{\\rho}_{y, x_i | x_{j \\neq }}^2 = \\frac{t_i^2}{t_i^2 + df}\n\\]\n\\(df=n-d-1\\), can shown \\(p\\)-values testing \\(\\beta_i=0\\) exactly \n\\(p\\)-values testing partial correlation \\(\\rho_{y, x_i | x_{j \\neq }}\\) vanishes!Therefore, ranking predictors \\(x_i\\) regression \\(t\\)-scores leads exactly ranking\n\\(p\\)-values partial correlation!","code":""},{"path":"prediction-and-variable-selection.html","id":"squared-wald-statistic-and-the-f-statistic","chapter":"19 Prediction and variable selection","heading":"19.3.4 Squared Wald statistic and the \\(F\\) statistic","text":"looked individual regression coefficients. However, can also\nconstruct Wald test using complete vector \\(\\hat{\\boldsymbol \\beta}\\). squared Wald statistic\ntest \\(\\boldsymbol \\beta= 0\\) given \n\\[\n\\begin{split}\nt^2 & = \\hat{\\boldsymbol \\beta}^T \\widehat{\\text{Var}}(\\hat{\\boldsymbol \\beta}^{-1}) \\hat{\\boldsymbol \\beta}\\\\\n     & = \\left( \\hat{\\boldsymbol \\Sigma}_{y \\boldsymbol x} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1} \\right)  \n       \\left(  \\frac{n}{ \\widehat{\\sigma^2_{\\varepsilon}} } \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}\\right) \n       \\left(  \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy} \\right)  \\\\\n    & = \\frac{n}{ \\widehat{\\sigma^2_{\\varepsilon}} } \n         \\hat{\\boldsymbol \\Sigma}_{y \\boldsymbol x} \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol x\\boldsymbol x}^{-1}   \n         \\hat{\\boldsymbol \\Sigma}_{\\boldsymbol xy}  \\\\\n    & = \\frac{n}{ \\widehat{\\sigma^2_{\\varepsilon}} } \\hat{\\sigma}_y^{2} R^2\\\\\n\\end{split}\n\\]\n\\(\\widehat{\\sigma^2_{\\varepsilon}} / \\hat{\\sigma}_y^{2} = 1- R^2\\)\nfinally get related \\(F\\) statistic\n\\[\n\\frac{t^2}{n} = \\frac{R^2}{1-R^2} = F\n\\]\nfunction \\(R^2\\). \\(R^2=0\\) \\(F=0\\). \\(R^2\\) large (\\(< 1\\)) \\(F\\) large well (\\(< \\infty\\)) null hypothesis \\(\\boldsymbol \\beta= 0\\) can rejected, implies least one regression coefficient non-zero.\nNote squared Wald statistic \\(t^2\\) asymptotically \\(\\chi^2_d\\) distributed useful\nfind critical values compute \\(p\\)-values.","code":""},{"path":"prediction-and-variable-selection.html","id":"further-approaches-for-variable-selection","chapter":"19 Prediction and variable selection","heading":"19.4 Further approaches for variable selection","text":"addition ranking marginal partial correlation, many approaches variable selection regression!Search-based methods:\nsearch subsets linear models \\(d\\) variables, ranging full model (including\npredictors) empty model (includes predictor) everything inbetween.\nProblem: exhaustive search possible even relatively small \\(d\\) space models large!\nTherefore heuristic approaches forward selection (adding predictors), backward selection (removing predictors), monte-carlo random search employed.\nProblem: maximum likelihood used choosing among models - since ML always pick best model. Therefore, penalised ML criteria AIC Bayesian criteria often employed instead.\nSearch-based methods:search subsets linear models \\(d\\) variables, ranging full model (including\npredictors) empty model (includes predictor) everything inbetween.Problem: exhaustive search possible even relatively small \\(d\\) space models large!Therefore heuristic approaches forward selection (adding predictors), backward selection (removing predictors), monte-carlo random search employed.Problem: maximum likelihood used choosing among models - since ML always pick best model. Therefore, penalised ML criteria AIC Bayesian criteria often employed instead.Integrative estimation variable selection:\nmethods fit regression model perform variable selection simultaneously.\nwell-known approach type “lasso” regression (Tibshirani 1996)\napplies (generalised) linear model ML plus L1 penalty.\nAlternative: Bayesian variable selection estimation procedures\nIntegrative estimation variable selection:methods fit regression model perform variable selection simultaneously.well-known approach type “lasso” regression (Tibshirani 1996)applies (generalised) linear model ML plus L1 penalty.Alternative: Bayesian variable selection estimation proceduresEntropy-based variable selection:\nseen , two popular approaches linear models based correlation, either marginal correlation partial correlation (via regression \\(t\\)-scores).\nCorrelation measures can generalised non-linear settings. One popular measure mutual information computed using KL divergence. case two variables \\(x\\) \\(y\\) joint normal distribution correlation \\(\\rho\\) mutual information function correlation:\n\\[\\text{MI}(x,y) = \\frac{1}{2} \\log (1-\\rho^2)\\]\nregression mutual information response \\(y\\) predictor \\(x_i\\) \\(\\text{MI}(y, x_i)\\), widely used feature selection, particular machine learning.Entropy-based variable selection:seen , two popular approaches linear models based correlation, either marginal correlation partial correlation (via regression \\(t\\)-scores).Correlation measures can generalised non-linear settings. One popular measure mutual information computed using KL divergence. case two variables \\(x\\) \\(y\\) joint normal distribution correlation \\(\\rho\\) mutual information function correlation:\n\\[\\text{MI}(x,y) = \\frac{1}{2} \\log (1-\\rho^2)\\]regression mutual information response \\(y\\) predictor \\(x_i\\) \\(\\text{MI}(y, x_i)\\), widely used feature selection, particular machine learning.","code":""},{"path":"refresher.html","id":"refresher","chapter":"A Refresher","heading":"A Refresher","text":"Statistics mathematical science requires practical use tools probability,\nvector matrices, analysis etc.briefly list essentials needed “Statistical Methods”.\nPlease familiarise () topics.","code":""},{"path":"refresher.html","id":"basic-mathematical-notation","chapter":"A Refresher","heading":"A.1 Basic mathematical notation","text":"Summation:\n\\[\n\\sum_{=1}^n x_i = x_1 + x_2 + \\ldots + x_n\n\\]Multiplication:\n\\[\n\\prod_{=1}^n x_i = x_1 \\times x_2 \\times \\ldots \\times x_n\n\\]","code":""},{"path":"refresher.html","id":"vectors-and-matrices","chapter":"A Refresher","heading":"A.2 Vectors and matrices","text":"Vector matrix notation.Vector algebra.Eigenvectors eigenvalues real symmetric matrix.Eigenvalue (spectral) decomposition real symmetric matrix.Positive negative definiteness real symmetric matrix (containing positive negative eigenvalues).Singularity real symmetric matrix (containing one eigenvalues identical zero).Singular value decomposition real matrix.","code":""},{"path":"refresher.html","id":"functions","chapter":"A Refresher","heading":"A.3 Functions","text":"","code":""},{"path":"refresher.html","id":"gradient","chapter":"A Refresher","heading":"A.3.1 Gradient","text":"nabla operator (also known del operator) row vector\n\\[\n\\nabla =  \\left(\\frac{\\partial}{\\partial x_1}, \\ldots, \n\\frac{\\partial}{\\partial x_d}\\right) = \\frac{\\partial}{\\partial \\boldsymbol x}\n\\]\ncontaining\nfirst order partial derivative operators.gradient scalar-valued function\n\\(h(\\boldsymbol x)\\) vector argument \\(\\boldsymbol x= (x_1, \\ldots, x_d)^T\\)\nalso row vector (\\(d\\) columns) \ncan expressed using nabla operator\n\\[\n\\nabla h(\\boldsymbol x) = \\left( \\frac{\\partial h(\\boldsymbol x)}{\\partial x_1}, \\ldots, \n\\frac{\\partial h(\\boldsymbol x)}{\\partial x_d} \\right) = \n \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\text{grad } h(\\boldsymbol x) \\, .\n\\]\nNote various notations gradient.Example .1  Examples gradient:\\(h(\\boldsymbol x)=\\boldsymbol ^T \\boldsymbol x+ b\\). \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol ^T\\).\\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol x\\). \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = 2 \\boldsymbol x^T\\).\\(h(\\boldsymbol x)=\\boldsymbol x^T \\boldsymbol \\boldsymbol x\\). \\(\\nabla h(\\boldsymbol x) = \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol x^T (\\boldsymbol + \\boldsymbol ^T)\\).","code":""},{"path":"refresher.html","id":"hessian-matrix","chapter":"A Refresher","heading":"A.3.2 Hessian matrix","text":"matrix second order partial derivates scalar-valued\nfunction vector-valued argument called Hessian matrix\ncomputed double application nabla operator:\n\\[\n\\nabla^T \\nabla h(\\boldsymbol x) =\n\\begin{pmatrix}\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1^2}\n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1 \\partial x_2} \n     & \\cdots \n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_1 \\partial x_d} \\\\\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2 \\partial x_1} \n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2^2}\n     & \\cdots \n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_2 \\partial x_d} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d \\partial x_1} \n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d \\partial x_2}  \n     & \\cdots \n     & \\frac{\\partial^2 h(\\boldsymbol x)}{\\partial x_d^2}\n \\end{pmatrix} = \\left(\\frac{\\partial h(\\boldsymbol x)}{\\partial x_i \\partial x_j}\\right)  \n= {\\left(\\frac{\\partial}{\\partial \\boldsymbol x}\\right)}^T \\frac{\\partial h(\\boldsymbol x)}{\\partial \\boldsymbol x}\n\\,.\n\\]\nconstruction square symmetric.","code":""},{"path":"refresher.html","id":"convex-and-concave-functions","chapter":"A Refresher","heading":"A.3.3 Convex and concave functions","text":"function \\(h(x)\\) convex second derivative \\(h''(x) \\geq 0\\) \\(x\\).\ngenerally, function \\(h(\\boldsymbol x)\\) convex Hessian matrix \\(\\nabla^T \\nabla h(\\boldsymbol x)\\)\npositive definite, .e. contains positive eigenvalues.\\(h(\\boldsymbol x)\\) convex, \\(-h(\\boldsymbol x)\\) concave. function concave Hessian matrix negative definite.Example .2  logarithm \\(\\log(x)\\) example concave function whereas \\(x^2\\) convex function.memorise, valley convex.","code":""},{"path":"refresher.html","id":"linear-and-quadratic-approximation","chapter":"A Refresher","heading":"A.3.4 Linear and quadratic approximation","text":"Taylor series first / second order.Applied scalar-valued function scalar:\n\\[\nh(x) \\approx h(x_0) + h'(x_0) (x-x_0) + \\frac{1}{2} h''(x_0) (x-x_0)^2\n\\]\n\\(x = x_0+ \\varepsilon\\) can written \n\\[\nh(x_0+ \\varepsilon) \\approx h(x_0) + h'(x_0) \\, \\varepsilon + \\frac{1}{2} h''(x_0)\\, \\varepsilon^2\n\\]Applied scalar-valued function vector:\n\\[\nh(\\boldsymbol x) \\approx h(\\boldsymbol x_0) + \\nabla h(\\boldsymbol x_0) (\\boldsymbol x-\\boldsymbol x_0) + \\frac{1}{2} \n(\\boldsymbol x-\\boldsymbol x_0)^T \\nabla^T \\nabla h(\\boldsymbol x_0) (\\boldsymbol x-\\boldsymbol x_0)\n\\]\n\\(\\boldsymbol x= \\boldsymbol x_0+ \\boldsymbol \\varepsilon\\) can written \n\\[\nh(\\boldsymbol x_0+ \\boldsymbol \\varepsilon) \\approx h(\\boldsymbol x_0) + \\nabla h(\\boldsymbol x_0)\\boldsymbol \\varepsilon+ \\frac{1}{2} \\boldsymbol \\varepsilon^T \\nabla^T \\nabla h(\\boldsymbol x_0) \\boldsymbol \\varepsilon\n\\]Example .3  Commonly occurring Taylor series approximations second order example\n\\[\n\\log(x_0+\\varepsilon) \\approx \\log(x_0) + \\frac{\\varepsilon}{x_0} - \\frac{\\varepsilon^2}{2 x_0^2}\n\\]\n\n\\[\n\\frac{x_0}{x_0+\\varepsilon} \\approx 1 - \\frac{\\varepsilon}{x_0} + \\frac{\\varepsilon^2}{ x_0^2}\n\\]","code":""},{"path":"refresher.html","id":"conditions-for-local-optimum-of-a-function","chapter":"A Refresher","heading":"A.3.5 Conditions for local optimum of a function","text":"check \\(x_0\\) \\(\\boldsymbol x_0\\) local maximum minimum can use following conditions:function single variable:First derivative zero optimum \\(h'(x_0) = 0\\).second derivative \\(h''(x_0) < 0\\) optimum negative function locally concave optimum maximum.second derivative \\(h''(x_0) > 0\\) positive optimum function locally convex optimum minimum.function several variables:Gradient vanishes maximum, \\(\\nabla h(\\boldsymbol x_0)=0\\).Hessian \\(\\nabla^T \\nabla h(\\boldsymbol x_0)\\) negative definite (= eigenvalues Hessian matrix negative) function locally concave optimum maximum.Hessian positive definite (= eigenvalues Hessian matrix positive) function locally convec optimum minimum.Around local optimum \\(\\boldsymbol x_0\\) can approximate function quadratically using\n\\[\nh(\\boldsymbol x_0+ \\boldsymbol \\varepsilon) \\approx h(\\boldsymbol x_0) +  \\frac{1}{2} \\boldsymbol \\varepsilon^T \\nabla^T \\nabla h(\\boldsymbol x_0) \\boldsymbol \\varepsilon\n\\]\nNote linear term missing due gradient zero \\(\\boldsymbol x_0\\).","code":""},{"path":"refresher.html","id":"functions-of-matrices","chapter":"A Refresher","heading":"A.3.6 Functions of matrices","text":"Matrix inverse,\nmatrix square root etc. symmetric real matrices.Computation via eigenvalue decomposition\n.e. apply function inverse, sqrt etc. eigenvalues.course actually compute matrix functions, use matrix\nnotation matrix square roots, need know exists \ntaking square root matrix entries.Trace determinant square matrix.Connection eigenvalues (trace = sum eigenvalues, determinant = product eigenvalues).","code":""},{"path":"refresher.html","id":"combinatorics","chapter":"A Refresher","heading":"A.4 Combinatorics","text":"","code":""},{"path":"refresher.html","id":"number-of-permutations","chapter":"A Refresher","heading":"A.4.1 Number of permutations","text":"number possible orderings, permutations, \\(n\\) distinct items \nnumber ways put \\(n\\) items \\(n\\) bins exactly one item bin. given \nfactorial\n\\[\nn! = \\prod_{=1}^n = 1 \\times 2 \\times \\ldots \\times n\n\\]\n\\(n\\) positive integer.\n\\(n=0\\) factorial defined \n\\[\n0! = 1 \n\\]\nexactly one permutation zero objects.factorial can also obtained using Gamma function\n\\[\nn! = \\Gamma(n+1)\n\\]\ncan viewed continuous version factorial.","code":""},{"path":"refresher.html","id":"multinomial-and-binomial-coefficient","chapter":"A Refresher","heading":"A.4.2 Multinomial and binomial coefficient","text":"number possible permutation \\(n\\) items \\(K\\) distinct types, \\(n_1\\) type 1, \\(n_2\\) type 2 , equals number ways\nput \\(n\\) items \\(K\\) bins \\(n_1\\) items first bin, \\(n_2\\) second .\ngiven multinomial coefficient\n\\[\n\\binom{n}{n_1, \\ldots, n_K} = \\frac {n!}{n_1! \\times n_2! \\times\\ldots \\times n_K! } \n\\]\n\\(\\sum_{k=1}^K n_k = n\\) \\(K \\leq n\\).\nNote equals number permutation items divided number permutations items bin (type).\\(n_k=1\\) hence \\(K=n\\) multinomial coefficient reduces factorial.two bins / types (\\(K=2\\)) multinomial coefficients becomes \nbinomial coefficient\n\\[\n \\binom{n}{n_1} = \\binom{n}{n_1, n-n_1}    =  \\frac {n!}{n_1! (n - n_1)!} \n\\]\ncounts number ways choose \\(n_1\\) elements set \\(n\\) elements.","code":""},{"path":"refresher.html","id":"de-moivre-sterling-approximation-of-the-factorial","chapter":"A Refresher","heading":"A.4.3 De Moivre-Sterling approximation of the factorial","text":"factorial frequently approximated following formula derived Abraham de Moivre (1667–1754) James Stirling (1692-1770)\n\\[\nn! \\approx \\sqrt{2 \\pi} n^{n+\\frac{1}{2}} e^{-n}\n\\]\nequivalently logarithmic scale\n\\[\n\\log n!  \\approx \\left(n+\\frac{1}{2}\\right) \\log n  -n + \\frac{1}{2}\\log \\left( 2 \\pi\\right)\n\\]\napproximation good small \\(n\\) (fails \\(n=0\\)) becomes\naccurate increasing \\(n\\). large \\(n\\) approximation can simplified \n\\[\n\\log n! \\approx  n \\log n  -n \n\\]","code":""},{"path":"refresher.html","id":"probability","chapter":"A Refresher","heading":"A.5 Probability","text":"","code":""},{"path":"refresher.html","id":"random-variables","chapter":"A Refresher","heading":"A.5.1 Random variables","text":"random variable describes random experiment. set possible outcomes\nsample space state space denoted \n\\(\\Omega = \\{\\omega_1, \\omega_2, \\ldots\\}\\). outcomes \\(\\omega_i\\) elementary events.\nsample space \\(\\Omega\\) can finite infinite. Depending type outcomes\nrandom variable discrete continous.event \\(\\subseteq \\Omega\\) subset \\(\\Omega\\) thus set elementary events \\(= \\{a_1, a_2, \\ldots\\}\\).\nincludes special cases full set \\(= \\Omega\\), empty set \\(= \\emptyset\\), elementary\nevents \\(=\\omega_i\\). complementary event \\(^C\\) complement set \\(\\) set \\(\\Omega\\)\n\\(^C = \\Omega \\setminus = \\{\\omega_i \\\\Omega: \\omega_i \\notin \\}\\).probability event denoted \\(\\text{Pr}()\\).\nassume \\(\\text{Pr}() \\geq 0\\), probabilities positive,\\(\\text{Pr}(\\Omega) = 1\\),\ncertain event probability 1, \\(\\text{Pr}() = \\sum_{a_i \\} \\text{Pr}(a_i)\\), probability \nevent equals sum constituting elementary events \\(a_i\\).implies\\(\\text{Pr}() \\leq 1\\), .e. probabilities lie interval \\([0,1]\\)\\(\\text{Pr}(^C) = 1 - \\text{Pr}()\\), \\(\\text{Pr}(\\emptyset) = 0\\)Assume now two events \\(\\) \\(B\\).\nprobability event “\\(\\) \\(B\\)” given probability set intersection\n\\(\\text{Pr}(\\cap B)\\).\nLikewise probability event “\\(\\) \\(B\\)” given probability set union\n\\(\\text{Pr}(\\cup B)\\).clear probability theory closely linked set theory,\nparticular measure theory. allows unified treatment discrete\ncontinuous random variables (elegant framework needed module).","code":""},{"path":"refresher.html","id":"probability-mass-and-density-function-and-distribution-and-quantile-function","chapter":"A Refresher","heading":"A.5.2 Probability mass and density function and distribution and quantile function","text":"describe random variable \\(x\\) need assign probabilities corresponding elementary outcomes\n\\(x \\\\Omega\\). convenience use name denote random variable elementary outcomes.discrete random variable employ probability mass function (PMF).\ndenote lower case \\(f\\) occasionally also use\n\\(p\\) \\(q\\). discrete case can define \nevent \\(= \\{x: x=\\} = \\{\\}\\) obtain probability directly PMF:\n\\[\\text{Pr}() = \\text{Pr}(x=) =f() \\,.\\]\nPMF property \\(\\sum_{x \\\\Omega} f(x) = 1\\) \n\\(f(x) \\[0,1]\\).continuous random variables need use probability density function (PDF)\ninstead. define event\n\\(= \\{x: < x \\leq + da\\}\\) infinitesimal interval\nassign probability\n\\[\n\\text{Pr}() = \\text{Pr}( < x \\leq + da) = f() da \\,.\n\\]\nPDF property \\(\\int_{x \\\\Omega} f(x) dx = 1\\)\ncontrast PMF density \\(f(x)\\geq 0\\) may take values larger\n1.Assuming ordering\ncan define event \\(= \\{x: x \\leq \\}\\) compute \nprobability\n\\[\nF() = \\text{Pr}() = \\text{Pr}( x \\leq ) =\n\\begin{cases}\n \\sum_{x \\} f(x) & \\text{discrete case} \\\\\n\\int_{x \\} f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\]\nknown distribution function, cumulative distribution function (CDF)\ndenoted upper case \\(F\\) corresponding PDF/PMF \\(f\\)\n(\\(P\\) \\(Q\\) corresponding PDF/PMF \\(p\\) \\(q\\)).\nconstruction distribution function monotonically increasing value ranges 0 1.\nhelp can compute probability general interval sets \n\\[\n\\text{Pr}( < x \\leq b ) = F(b)-F() \\,.\n\\]inverse distribution function \\(y=F(x)\\) quantile function \\(x=F^{-1}(y)\\).\n50% quantile \\(F^{-1}\\left(\\frac{1}{2}\\right)\\) median.random variable \\(x\\) distribution function \\(F\\) write \\(x \\sim F\\).","code":""},{"path":"refresher.html","id":"expection-and-variance-of-a-random-variable","chapter":"A Refresher","heading":"A.5.3 Expection and variance of a random variable","text":"expected value \\(\\text{E}(x)\\) random variable defined \nweighted average possible outcomes:\n\\[\n\\text{E}(x) = \n\\begin{cases}\n \\sum_{x \\\\Omega} x f(x) & \\text{discrete case} \\\\\n\\int_{x \\\\Omega} x f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\]\nexpectation necessarily always defined continuous\nrandom variable integral can diverge.expected value function random variable \\(h(x)\\) \nobtained similarly:\n\\[\n\\text{E}(h(x)) = \n\\begin{cases}\n \\sum_{x \\\\Omega} h(x) f(x) & \\text{discrete case} \\\\\n\\int_{x \\\\Omega} h(x) f(x) dx & \\text{continuous case} \\\\\n\\end{cases}\n\\]\ncalled “law unconscious statistician”, short LOTUS.event \\(\\) can define corresponding indicator function\n\\[\n1_A(x) =\n\\begin{cases}\n1 & x \\\\\\\n0 & x \\notin \\\\\n\\end{cases}\n\\]\nIntriguingly,\n\\[\n\\text{E}(1_A(x) ) = \\text{Pr}()\n\\]\n.e. expectation indicator variable \\(\\) probability\n\\(\\).moments random variables also defined expectation:Zeroth moment: \\(\\text{E}(x^0) = 1\\) definition PDF PMF,First moment: \\(\\text{E}(x^1) = \\text{E}(x) = \\mu\\) , mean,Second moment: \\(\\text{E}(x^2)\\)variance second momented centered mean \\(\\mu\\):\n\\[\\text{Var}(x) = \\text{E}( (x - \\mu)^2 ) = \\sigma^2\\]variance can also computed \\(\\text{Var}(x) = \\text{E}(x^2)-\\text{E}(x)^2\\).distribution necessarily need finite first higher moments.\nexample Cauchy distribution mean variance (higher moment).","code":""},{"path":"refresher.html","id":"transformation-of-random-variables","chapter":"A Refresher","heading":"A.5.4 Transformation of random variables","text":"Linear transformation random variables: \\(\\) \\(b\\) constants \\(x\\) random variable, random variable \\(y= + b x\\) mean \\(\\text{E}(y) = + b \\text{E}(x)\\) variance \\(\\text{Var}(y) = b^2 \\text{Var}(x)\\).general invertible coordinate transformation \\(y = h(x) = y(x)\\) backtransformation \\(x = h^{-1}(y) = x(y)\\).transformation infinitesimal volume element \\(dy = |\\frac{dy}{dx}| dx\\).transformation density \\(f_y(y) =\\left|\\frac{dx}{dy}\\right| f_x(x(y))\\).Note \\(\\left|\\frac{dx}{dy}\\right| = \\left|\\frac{dy}{dx}\\right|^{-1}\\).","code":""},{"path":"refresher.html","id":"law-of-large-numbers","chapter":"A Refresher","heading":"A.5.5 Law of large numbers:","text":"strong law large numbers empirical distribution\n\\(\\hat{F}_n\\) converges true underlying distribution \\(F\\) \\(n \\rightarrow \\infty\\) almost surely:\n\\[\n\\hat{F}_n\\overset{. s.}{\\} F\n\\]\nGlivenko–Cantelli theorem asserts convergence uniform. Since strong law implies weak law also convergence probability:\n\\[\n\\hat{F}_n\\overset{P}{\\} F\n\\]strong law large numbers empirical distribution\n\\(\\hat{F}_n\\) converges true underlying distribution \\(F\\) \\(n \\rightarrow \\infty\\) almost surely:\n\\[\n\\hat{F}_n\\overset{. s.}{\\} F\n\\]\nGlivenko–Cantelli theorem asserts convergence uniform. Since strong law implies weak law also convergence probability:\n\\[\n\\hat{F}_n\\overset{P}{\\} F\n\\]Correspondingly, \\(n \\rightarrow \\infty\\) average \\(\\text{E}_{\\hat{F}_n}(h(X)) = \\frac{1}{n} \\sum_{=1}^n h(x_i)\\) converges expectation \\(\\text{E}_{F}(h(X))\\).Correspondingly, \\(n \\rightarrow \\infty\\) average \\(\\text{E}_{\\hat{F}_n}(h(X)) = \\frac{1}{n} \\sum_{=1}^n h(x_i)\\) converges expectation \\(\\text{E}_{F}(h(X))\\).","code":""},{"path":"refresher.html","id":"jensens-inequality","chapter":"A Refresher","heading":"A.5.6 Jensen’s inequality","text":"\\[\\text{E}(h(\\boldsymbol x)) \\geq h(\\text{E}(\\boldsymbol x))\\]\nconvex function \\(h(\\boldsymbol x)\\).Recall: convex function (\\(x^2\\)) shape “valley”.","code":""},{"path":"refresher.html","id":"distributions","chapter":"A Refresher","heading":"A.6 Distributions","text":"","code":""},{"path":"refresher.html","id":"bernoulli-and-binomial-distribution","chapter":"A Refresher","heading":"A.6.1 Bernoulli and Binomial distribution","text":"Bernoulli distribution \\(\\text{Ber}(p)\\) simplest distribution possible.\nnamed Jacob Bernoulli (1655-1705)\nalso invented law large numbers.describes discrete binary random variable\ntwo states \\(x=0\\) (“failure”) \\(x=1\\) (“success”),\nparameter \\(p \\[0,1]\\) probability “success”.\nOften Bernoulli distribution also referred “coin tossing” model \ntwo outcomes “heads” “tails”.Correspondingly, probability mass function \\(\\text{Ber}(p)\\) \n\\[\nf(x=0) = \\text{Pr}(\\text{\"failure\"}) = 1-p  \n\\]\n\n\\[\nf(x=1) = \\text{Pr}(\\text{\"success\"}) = p \n\\]\ncompact way write PMF Bernoulli distribution \n\\[\nf(x | p ) = p^{x} (1-p)^{1-x}\n\\]random variable \\(x\\) follows Bernoulli distribution \nwrite\n\\[\nx \\sim \\text{Ber}(p) \\,.\n\\]\nexpected value \\(\\text{E}(x) = p\\) variance \\(\\text{Var}(x) = p (1 - p)\\).Closely related Bernoulli distribution Binomial distribution\n\\(\\text{Bin}(m, p)\\) results repeating \nBernoulli experiment \\(m\\) times counting number successes among\n\\(m\\) trials (without keeping track ordering experiments).probability mass function :\n\\[\nf(x | p) = \\binom{m}{x} p^x (1 - p)^{m - x}\n\\]\n\\(x = 0, 1, 2, \\ldots, m\\).\nBinomial coefficient \\(\\binom{m}{x}\\) needed account multiplicity\nways (orderings samples) can observe \\(x\\) sucesses.expected value \\(\\text{E}(x) = mp\\) variance \\(\\text{Var}(x) = mp (1 - p)\\).random variable \\(x\\) follows Binomial distribution \nwrite\n\\[\nx \\sim \\text{Bin}(m, p)\\,\n\\]\n\\(m=1\\) reduces Bernoulli distribution \\(\\text{Ber}(p)\\).R PMF Binomial distribution called dbinom(). Binomial coefficient computed choose().","code":""},{"path":"refresher.html","id":"normal-distribution","chapter":"A Refresher","heading":"A.6.2 Normal distribution","text":"Univariate normal distribution:\\(x \\sim N(\\mu,\\sigma^2)\\) \\(\\text{E}(x)=\\mu\\) \\(\\text{Var}(x) = \\sigma^2\\).Probability density function (PDF): \\[f(x| \\mu, \\sigma^2)=(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]R density function called dnorm().standard normal distribution \\(N(0, 1)\\) mean 1 variance 1.Plot PDF standard normal:cumulative distribution function (CDF) standard normal \\(N(0,1)\\)\n\n\\[\n\\Phi (x ) = \\int_{-\\infty}^{x} f(x'| \\mu=0, \\sigma^2=1) dx' \n\\]\nanalytic expression \\(\\Phi(x)\\). R function called pnorm().inverse \\(\\Phi^{-1}(p)\\) called quantile function standard normal.\nR function called qnorm().sum two normal random variables also normal (appropriate mean variance).","code":""},{"path":"refresher.html","id":"scaled-chi-squared-wishart-gamma-distribution-and-exponential-distribution","chapter":"A Refresher","heading":"A.6.3 Scaled chi-squared / Wishart / gamma distribution and exponential distribution","text":"Assume \\(m\\) independent normal random variables\n\\[z_1,z_2,\\dots,z_m\\sim N(0,\\sigma^2)\\]\nsum squares\n\\[\nx = \\sum_{=1}^{m} z_i^2\n\\]\nfollows scaled chi-squared distribution\n\\[\nx\\sim \\sigma^2 \\text{$\\chi^2_{m}$}\n\\]\ndegree freedom \\(m\\) \\(x \\geq 0\\).\nmean variance scaled chi-squared distributed variable \\(\\text{E}(x)=m \\sigma^2\\) \\(\\text{Var}(x)=2m\\sigma^4\\).Another name scaled chi-squared distribution univariate Wishart distribution \\(W_1(\\sigma^2, m)\\) uses parameters.gamma distribution \\(\\text{Gam}(\\alpha, \\beta)\\) variant scaled chi-squared distribution uses different parameterisation terms shape parameter \\(\\alpha\\) scale parameter \\(\\beta\\). scaled chi-squared distribution \\(\\sigma^2 \\text{$\\chi^2_{m}$}\\) equals \\(\\text{Gam}(\\frac{m}{2}, 2 \\sigma^2)\\). mean \\(\\text{Gam}(\\alpha, \\beta)\\) \\(\\alpha \\beta\\) variance \\(\\alpha \\beta^2\\).chi-squared distribution special case \\(\\sigma^2=1\\) mean \\(\\text{E}(x)=m\\) variance \\(\\text{Var}(x)=2m\\). chi-squared distribution \\(\\text{$\\chi^2_{m}$}\\) equals \\(\\text{Gam}(\\frac{m}{2}, 2)\\).exponential distribution \\(\\text{Exp}(\\beta)\\) scale parameter \\(\\beta\\) (mean \\(\\beta\\) variance \\(\\beta^2\\)) another special case gamma distribution shape parameter \\(\\alpha=1\\). Instead scale parameter exponential distribution also often specified using rate parameter \\(\\lambda= \\frac{1}{\\beta}\\).plot density chi-squared distribution\ndegrees freedom \\(m=1\\) \\(m=3\\):R density chi-squared distribution given dchisq(). cumulative density function pchisq() \nquantile function qchisq().density gamma distribution (aka scaled chi-squared distribution) available R function dgamma(). cumulative density function pgamma() quantile function qgamma().","code":""},{"path":"refresher.html","id":"statistics","chapter":"A Refresher","heading":"A.7 Statistics","text":"","code":""},{"path":"refresher.html","id":"statistical-learning","chapter":"A Refresher","heading":"A.7.1 Statistical learning","text":"aim statistics - data science - machine learning learn data\n(experiments, observations, measurements) learn understand world.Specifically, identify best model(s) data order toto explain current data, andto enable good prediction future dataNote easy get models explain data predict well!called overfitting data happens particular model overparameterized amount data available.Specifically, data \\(x_1, \\ldots, x_n\\) models \\(f(x| \\theta)\\) indexed parameter \\(\\theta\\).Often (always) \\(\\theta\\) can interpreted /associated property model.single parameter write \\(\\theta\\) (scalar parameter). parameter vector write \\(\\boldsymbol \\theta\\) (bold type).","code":""},{"path":"refresher.html","id":"point-and-interval-estimation","chapter":"A Refresher","heading":"A.7.2 Point and interval estimation","text":"parameter \\(\\theta\\) interest modelwe uncertain parameter (.e. don’t know exact value)like learn parameter observing data \\(x_1, \\ldots, x_n\\) modelEstimation:estimator \\(\\theta\\) function \\(\\hat{\\theta}(x_1, \\ldots, x_n)\\) maps data (input) “guess” (output) \\(\\theta\\).point estimator provides single number parameterAn interval estimator provides set possible values parameter.","code":""},{"path":"refresher.html","id":"sampling-properties-of-a-point-estimator-hatboldsymbol-theta","chapter":"A Refresher","heading":"A.7.3 Sampling properties of a point estimator \\(\\hat{\\boldsymbol \\theta}\\)","text":"point estimator \\(\\hat\\theta\\) depends data, hence sampling variation (.e. estimate different new set observations)Thus \\(\\hat\\theta\\) can seen random variable, distribution called sampling distribution (accross different experiments).Properties distribution can used evaluate far estimator\ndeviates (average across different experiments) true value:\\[\\begin{align*}\n\\begin{array}{rr}\n\\text{Bias:}\\\\\n\\text{Variance:}\\\\\n\\text{Mean squared error:}\\\\\n\\\\\n\\end{array}\n\\begin{array}{rr}\n\\text{Bias}(\\hat{\\theta})\\\\\n\\text{Var}(\\hat{\\theta})\\\\\n\\text{MSE}(\\hat{\\theta})\\\\\n\\\\\n\\end{array}\n\\begin{array}{ll}\n=\\text{E}(\\hat{\\theta})-\\theta\\\\\n=\\text{E}\\left((\\hat{\\theta}-\\text{E}(\\hat{\\theta}))^2\\right)\\\\\n=\\text{E}((\\hat{\\theta}-\\theta)^2)\\\\\n=\\text{Var}(\\hat{\\theta})+\\text{Bias}(\\hat{\\theta})^2\\\\\n\\end{array}\n\\end{align*}\\]last identity MSE follows \\(\\text{E}(X^2)=\\text{Var}(X)+\\text{E}(X)^2\\).first sight seems desirable focus unbiased (finite \\(n\\)) estimators.\nHowever, requiring strict unbiasedness always good idea!many situations better allow small bias order achieve smaller variance overall total smaller MSE. called bias-variance tradeoff — bias\ntraded smaller variance (, conversely, less bias traded higher variance)","code":""},{"path":"refresher.html","id":"asymptotics-1","chapter":"A Refresher","heading":"A.7.4 Asymptotics","text":"Typically, \\(\\text{Bias}\\), \\(\\text{Var}\\) \\(\\text{MSE}\\) decrease increasing sample size\ndata \\(n \\\\infty\\) errors become smaller smaller.typical rate decrease variance good estimator \\(\\frac{1}{n}\\).\nThus, sample size doubled variance divided 2\n(standard deviation divided \\(\\sqrt{2}\\)).Consistency: \\(\\hat{\\theta}\\) called consistent \\[\\text{MSE}(\\hat{\\theta}) \\longrightarrow 0 \\text{ $n\\rightarrow \\infty$ }\\].Consistency implies recover true model limit infinite data model class contains true model.Consistency minimum essential requirement reasonable estimator! consistent\nestimators typically prefer estimator efficient (.e. fasted decrease \nMSE) thus smallest variance /MSE given finite \\(n\\).Note model class contain true model strict consistency\nachived still wish get least close possible\ntrue model.","code":""},{"path":"refresher.html","id":"confidence-intervals","chapter":"A Refresher","heading":"A.7.5 Confidence intervals","text":"confidence interval (CI) interval estimate frequentist interpretation.Definition coverage \\(\\kappa\\) CI: often (repeated identical experiment) estimated CI overlap true parameter value \\(\\theta\\)\nEg.: Coverage \\(\\kappa=0.95\\) (95%) means 95 100 case estimated CI contain (unknown) true value (.e. “cover” \\(\\theta\\)).\nEg.: Coverage \\(\\kappa=0.95\\) (95%) means 95 100 case estimated CI contain (unknown) true value (.e. “cover” \\(\\theta\\)).Illustration repeated construction CI \\(\\theta\\):Note CI actually estimate: \\(\\widehat{\\text{CI}}(x_1, \\ldots, x_n)\\), .e. depends data random (sampling) variation.good CI high coverage compact.Note: coverage probability probability true value contained given estimated interval (Bayesian Credible Interval).","code":""},{"path":"refresher.html","id":"symmetric-normal-confidence-interval","chapter":"A Refresher","heading":"A.7.6 Symmetric normal confidence interval","text":"normally distributed univariate random variable\nstraightforward construct symmetric two-sided CI given desired coverage \\(\\kappa\\).normal random variable \\(X \\sim N(\\mu, \\sigma^2)\\) mean \\(\\mu\\) variance \\(\\sigma^2\\) density function \\(f(x)\\) can compute probability\\[\\text{Pr}(X \\leq \\mu + c \\sigma) =  \\int_{-\\infty}^{\\mu+c\\sigma} f(x) dx  = \\Phi (c) = \\frac{1+\\kappa}{2}\\]\nNote \\(\\Phi(c)\\) cumulative distribution function (CDF) standard normal \\(N(0,1)\\):obtain critical point \\(c\\) quantile function, .e. inversion \\(\\Phi\\):\\[c=\\Phi^{-1}\\left(\\frac{1+\\kappa}{2}\\right)\\]following table lists \\(c\\) three commonly used values \\(\\kappa\\) - useful memorise values!symmetric standard normal CI nominal coverage \\(\\kappa\\) fora scalar parameter \\(\\theta\\)normally distributed estimate \\(\\hat{\\theta}\\) andwith estimated standard deviation \\(\\hat{\\text{SD}}(\\hat{\\theta}) = \\hat{\\sigma}\\)given \n\\[\\widehat{\\text{CI}}=[\\hat{\\theta} \\pm c \\hat{\\sigma}]\\]\\(c\\) chosen desired coverage level \\(\\kappa\\).","code":""},{"path":"refresher.html","id":"confidence-interval-for-chi-squared-distribution","chapter":"A Refresher","heading":"A.7.7 Confidence interval for chi-squared distribution","text":"normal CI can compute critical values \nchi-squared distribution use one-sided interval:\n\\[\n\\text{Pr}(X \\leq c) = \\kappa\n\\]\nget \\(c\\) quantile function, .e. inverting CDF chi-squared distribution.following list critical values three common choice \\(\\kappa\\)\n\\(m=1\\) (one degree freedom):one-sided CI nominal coverage \\(\\kappa\\) given \\([0, c ]\\).","code":""},{"path":"further-study.html","id":"further-study","chapter":"B Further study","heading":"B Further study","text":"module can touch surface likelihood Bayes inference.\nstarting point reading following text books recommended.","code":""},{"path":"further-study.html","id":"recommended-reading","chapter":"B Further study","heading":"B.1 Recommended reading","text":"Held Bové (2014) Applied Statistical Inference: Likelihood Bayes. Springer.Held Bové (2014) Applied Statistical Inference: Likelihood Bayes. Springer.Faraway (2015) Linear Models R (second edition). Chapman Hall/CRC.Faraway (2015) Linear Models R (second edition). Chapman Hall/CRC.","code":""},{"path":"further-study.html","id":"additional-references","chapter":"B Further study","heading":"B.2 Additional references","text":"Wood (2015) Core Statistics. Cambridge University Press.Wood (2015) Core Statistics. Cambridge University Press.Gelman et al. (2014) Bayesian data analysis (3rd edition). CRC Press.Gelman et al. (2014) Bayesian data analysis (3rd edition). CRC Press.","code":""},{"path":"bibliography.html","id":"bibliography","chapter":"Bibliography","heading":"Bibliography","text":"Domingos, P. 2015. Master Algorithm: Quest Ultimate Learning Machine Remake World. Basic Books.Efron, B., D. V. Hinkley. 1978. “Assessing Accuracy Maximum Likelihood Estimator: Observed Versus Expected Fisher Information.” Biometrika 65: 457–82. https://doi.org/10.1093/biomet/65.3.457.Faraway, J. J. 2015. Linear Models R. 2nd ed. Chapman; Hall/CRC.Gelman, ., J. B. Carlin, H. . Stern, D. B. Dunson, . Vehtari, D. B. Rubin. 2014. Bayesian Data Analysis. 3rd ed. CRC Press.Held, L., D. S. Bové. 2014. Applied Statistical Inference: Likelihood Bayes. Springer.Wilks, S. S. 1938. “Large-Sample Distribution Likelihood Ratio Testing Composite Hypotheses.” Ann. Math. Statist. 9: 60–62. https://doi.org/10.1214/aoms/1177732360.Wood, S. 2015. Core Statistics. Cambridge University Press.","code":""}]
