<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>14 Overview over regression modelling | Statistical Methods: Likelihood, Bayes and Regression</title>
<meta name="author" content="Korbinian Strimmer">
<meta name="generator" content="bookdown 0.31 with bs4_book()">
<meta property="og:title" content="14 Overview over regression modelling | Statistical Methods: Likelihood, Bayes and Regression">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="14 Overview over regression modelling | Statistical Methods: Likelihood, Bayes and Regression">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="14.1 General setup  \(y\): response variable, also known as outcome or label \(x_1, x_2, x_3, \ldots, x_d\): predictor variables, also known as covariates or covariables The relationship between...">
<meta property="og:description" content="14.1 General setup  \(y\): response variable, also known as outcome or label \(x_1, x_2, x_3, \ldots, x_d\): predictor variables, also known as covariates or covariables The relationship between...">
<meta name="twitter:description" content="14.1 General setup  \(y\): response variable, also known as outcome or label \(x_1, x_2, x_3, \ldots, x_d\): predictor variables, also known as covariates or covariables The relationship between...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Methods: Likelihood, Bayes and Regression</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Likelihood estimation and inference</li>
<li><a class="" href="overview-of-statistical-learning.html"><span class="header-section-number">1</span> Overview of statistical learning</a></li>
<li><a class="" href="from-entropy-to-maximum-likelihood.html"><span class="header-section-number">2</span> From entropy to maximum likelihood</a></li>
<li><a class="" href="maximum-likelihood-estimation.html"><span class="header-section-number">3</span> Maximum likelihood estimation</a></li>
<li><a class="" href="quadratic-approximation-and-normal-asymptotics.html"><span class="header-section-number">4</span> Quadratic approximation and normal asymptotics</a></li>
<li><a class="" href="likelihood-based-confidence-interval-and-likelihood-ratio.html"><span class="header-section-number">5</span> Likelihood-based confidence interval and likelihood ratio</a></li>
<li><a class="" href="optimality-properties-and-conclusion.html"><span class="header-section-number">6</span> Optimality properties and conclusion</a></li>
<li class="book-part">Bayesian Statistics</li>
<li><a class="" href="conditioning-and-bayes-rule.html"><span class="header-section-number">7</span> Conditioning and Bayes rule</a></li>
<li><a class="" href="models-with-latent-variables-and-missing-data.html"><span class="header-section-number">8</span> Models with latent variables and missing data</a></li>
<li><a class="" href="essentials-of-bayesian-statistics.html"><span class="header-section-number">9</span> Essentials of Bayesian statistics</a></li>
<li><a class="" href="bayesian-learning-in-practise.html"><span class="header-section-number">10</span> Bayesian learning in practise</a></li>
<li><a class="" href="bayesian-model-comparison.html"><span class="header-section-number">11</span> Bayesian model comparison</a></li>
<li><a class="" href="choosing-priors-in-bayesian-analysis.html"><span class="header-section-number">12</span> Choosing priors in Bayesian analysis</a></li>
<li><a class="" href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></li>
<li class="book-part">Regression</li>
<li><a class="active" href="overview-over-regression-modelling.html"><span class="header-section-number">14</span> Overview over regression modelling</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">15</span> Linear Regression</a></li>
<li><a class="" href="estimating-regression-coefficients.html"><span class="header-section-number">16</span> Estimating regression coefficients</a></li>
<li><a class="" href="squared-multiple-correlation-and-variance-decomposition-in-linear-regression.html"><span class="header-section-number">17</span> Squared multiple correlation and variance decomposition in linear regression</a></li>
<li><a class="" href="prediction-and-variable-selection.html"><span class="header-section-number">18</span> Prediction and variable selection</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="refresher.html"><span class="header-section-number">A</span> Refresher</a></li>
<li><a class="" href="further-study.html"><span class="header-section-number">B</span> Further study</a></li>
<li><a class="" href="bibliography.html">Bibliography</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="overview-over-regression-modelling" class="section level1" number="14">
<h1>
<span class="header-section-number">14</span> Overview over regression modelling<a class="anchor" aria-label="anchor" href="#overview-over-regression-modelling"><i class="fas fa-link"></i></a>
</h1>
<div id="general-setup" class="section level2" number="14.1">
<h2>
<span class="header-section-number">14.1</span> General setup<a class="anchor" aria-label="anchor" href="#general-setup"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="fig/regression1-p1.png" width="80%" style="display: block; margin: auto;"></div>
<ul>
<li><p><span class="math inline">\(y\)</span>: <strong>response variable</strong>, also known as <strong>outcome</strong> or <strong>label</strong></p></li>
<li><p><span class="math inline">\(x_1, x_2, x_3, \ldots, x_d\)</span>: <strong>predictor variables</strong>, also known as <strong>covariates</strong> or <strong>covariables</strong></p></li>
<li><p>The relationship between the outcomes and the predictor variables is assumed to follow
<span class="math display">\[
y = f(x_1,x_2,\dots,x_d) + \varepsilon
\]</span>
where <span class="math inline">\(f\)</span> is the <strong>regression function</strong> (not a density) and <span class="math inline">\(\varepsilon\)</span> represents <strong>noise</strong>.</p></li>
</ul>
</div>
<div id="objectives" class="section level2" number="14.2">
<h2>
<span class="header-section-number">14.2</span> Objectives<a class="anchor" aria-label="anchor" href="#objectives"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li><p><strong>Understand the relationship</strong> between the response <span class="math inline">\(y\)</span> and the predictor variables <span class="math inline">\(x_i\)</span> by <strong>learning the regression function</strong> <span class="math inline">\(f\)</span> from observed data (training data). The estimated regression function is <span class="math inline">\(\hat{f}\)</span>.</p></li>
<li>
<p><strong>Prediction of outcomes</strong>
<span class="math display">\[\underbrace{\hat{y}}_{\substack{\text{predicted response} \\ \text{using fitted $\hat{f}$}}} = \hat{f}(x_1,x_2,\dots,x_d)\]</span></p>
<p>If instead of the fitted function <span class="math inline">\(\hat{f}\)</span> the known regression function <span class="math inline">\(f\)</span> is used we denote this by
<span class="math display">\[\underbrace{y^{\star}}_{\substack{\text{predicted response} \\ \text{using known $f$}}} = f(x_1,x_2,\dots,x_d)
\]</span></p>
</li>
<li>
<p><strong>Variable importance</strong></p>
<ul>
<li>which covariates are most relevant in predicting the outcome?</li>
<li>allows to better understand the data and model<br><span class="math inline">\(\rightarrow\)</span> variable selection
(to build simpler model with same predictive capability)</li>
</ul>
</li>
</ol>
</div>
<div id="regression-as-a-form-of-supervised-learning" class="section level2" number="14.3">
<h2>
<span class="header-section-number">14.3</span> Regression as a form of supervised learning<a class="anchor" aria-label="anchor" href="#regression-as-a-form-of-supervised-learning"><i class="fas fa-link"></i></a>
</h2>
<p>Regression modelling is a special case of <strong>supervised learning</strong>.</p>
<p>In supervised learning we make use of labelled data, i.e. <span class="math inline">\(\boldsymbol x_i\)</span> has an associated <em>label</em>
<span class="math inline">\(y_i\)</span>. Thus, the data is consists of pairs <span class="math inline">\((\boldsymbol x_1, y_1),(\boldsymbol x_2 ,y_2),\dots,(\boldsymbol x_n ,y_n)\)</span>.</p>
<p>The <em>supervision</em> part of in supervised learning refers to the fact that the labels are given.</p>
<p>In <strong>regression</strong> typically the label <span class="math inline">\(y_i\)</span> is continuous and called the <em>response</em>.</p>
<p>On the other hand, if the label <span class="math inline">\(y_i\)</span> is discrete/categorical then supervised learning is called <strong>classification</strong>.</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\\
\text{Supervised Learning}\\
\\
\end{array}
\begin{array}{ll}
\longrightarrow \text{Discrete } y\\
\\
\longrightarrow \text{Continuous } y\\
\end{array}
\begin{array}{ll}
\longrightarrow \text{Classification Methods}\\
\\
\longrightarrow \text{Regression Methods}\\
\end{array}
\end{align*}\]</span></p>
<p>Another important type of statistical learning is <strong>unsupervised learning</strong> where labels <span class="math inline">\(y\)</span>
are inferred from the data <span class="math inline">\(\boldsymbol x\)</span> (this is also known as <strong>clustering</strong>). Furthermore, there is also <em>semi-supervised learning</em> with labels only partly known.</p>
<p>Note that there are regression models (e.g. logistic regression) with discrete
response that are performing classification, so one may argue that “supervised learning”=“generalised regression”.</p>
</div>
<div id="various-regression-models-used-in-statistics" class="section level2" number="14.4">
<h2>
<span class="header-section-number">14.4</span> Various regression models used in statistics<a class="anchor" aria-label="anchor" href="#various-regression-models-used-in-statistics"><i class="fas fa-link"></i></a>
</h2>
<p>In this course we only study linear multiple regression.
However, you should be aware that the linear model is in fact just a special cases of some much more general
regression approaches.</p>
<p>General regression model:
<span class="math display">\[y = f(x_1,\dots,x_d) + \text{"noise"}\]</span></p>
<ul>
<li><p>The function <span class="math inline">\(f\)</span> is estimated nonparametrically
- splines
- Gaussian processes</p></li>
<li><p>Generalised Additive Models (GAM):
- the function <span class="math inline">\(f\)</span> is assumed to be the sum of individual functions <span class="math inline">\(f_i(x_i)\)</span></p></li>
<li><p>Generalised Linear Models (GLM):
- <span class="math inline">\(f\)</span> is a transformed linear predictor <span class="math inline">\(h(\sum b_i x_i)\)</span>, noise is assumed from an exponential family</p></li>
<li><p>Linear Model (LM):
- linear predictor <span class="math inline">\(\sum b_i x_i\)</span>, normal noise</p></li>
</ul>
<p>In R the linear model is implemented in the function lm(), and generalised linear models
in the function glm(). Generalised additive models are available in the package
“mgcv”.</p>
<p>In the following we focus on the linear regression model with continuous response.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="optimality-properties-and-summary.html"><span class="header-section-number">13</span> Optimality properties and summary</a></div>
<div class="next"><a href="linear-regression.html"><span class="header-section-number">15</span> Linear Regression</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#overview-over-regression-modelling"><span class="header-section-number">14</span> Overview over regression modelling</a></li>
<li><a class="nav-link" href="#general-setup"><span class="header-section-number">14.1</span> General setup</a></li>
<li><a class="nav-link" href="#objectives"><span class="header-section-number">14.2</span> Objectives</a></li>
<li><a class="nav-link" href="#regression-as-a-form-of-supervised-learning"><span class="header-section-number">14.3</span> Regression as a form of supervised learning</a></li>
<li><a class="nav-link" href="#various-regression-models-used-in-statistics"><span class="header-section-number">14.4</span> Various regression models used in statistics</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Methods: Likelihood, Bayes and Regression</strong>" was written by Korbinian Strimmer. It was last built on 20 January 2023.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
